<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: JOIN: Aleksei Riikonen</title>
<meta name="Author" content="Aleksei Riikonen (aariikon@cc.jyu.fi)">
<meta name="Subject" content="JOIN: Aleksei Riikonen">
<meta name="Date" content="2002-12-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>JOIN: Aleksei Riikonen</h1>
<!-- received="Mon Dec 16 14:13:18 2002" -->
<!-- isoreceived="20021216211318" -->
<!-- sent="Mon, 16 Dec 2002 23:23:09 +0200" -->
<!-- isosent="20021216212309" -->
<!-- name="Aleksei Riikonen" -->
<!-- email="aariikon@cc.jyu.fi" -->
<!-- subject="JOIN: Aleksei Riikonen" -->
<!-- id="010201c2a549$7974f960$53c8ea82@ronin.jyu.fi" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Aleksei Riikonen (<a href="mailto:aariikon@cc.jyu.fi?Subject=Re:%20JOIN:%20Aleksei%20Riikonen"><em>aariikon@cc.jyu.fi</em></a>)<br>
<strong>Date:</strong> Mon Dec 16 2002 - 14:23:09 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5924.html">Gary Miller: "Will FAI need simulated empathy to avoid sociopathic tendencies?"</a>
<li><strong>Previous message:</strong> <a href="5922.html">BillK: "Re: Chess and Go study"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5923">[ date ]</a>
<a href="index.html#5923">[ thread ]</a>
<a href="subject.html#5923">[ subject ]</a>
<a href="author.html#5923">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
In accordance with (optional) list customs, I'm making my join post here.
<br>
<p>I'm a lot like many others on this list. Intellectually inclined, and a
<br>
couple of standard deviations away from the average IQ, sure, but probably
<br>
not very close to being among the brightest minds here. Have always liked
<br>
futurism and been interested in all sorts of sciences. Math, evolutionary
<br>
psychology, analytic (and other forms of rational) philosophy - fields of
<br>
this sort top my list, though. A preference to which many of you can relate,
<br>
I'm sure.
<br>
<p>Technically inclined I have not been, though. Think of some stereotypical
<br>
theoretical physicist, and there you have it. I actually switched from
<br>
majoring in theoretical physics to math, because the experimental work even
<br>
the wanna-be theoretical physicists have to do during their first years (at
<br>
least in the university of Jyvaskyla, though I guess this should be pretty
<br>
universal) somewhat turned me off.
<br>
<p>A year or two ago I read Eliezer's &quot;An Introduction to the Singularity&quot;,
<br>
after being directed to it by a friend (Mikko Rauhala, among the readership
<br>
here). That's when the future shock hit me. I was ecstatic.
<br>
<p>Before being introduced to singularity concepts, I actually held the luddite
<br>
view that _extremely_ forceful regulation of technological development would
<br>
be a good idea, as it seemed to me that humanity as it is is just screwing
<br>
things up with all this power we've bestowed on ourselves, and is bound to
<br>
screw up even worse if &quot;progress&quot; isn't very strictly regulated. I had
<br>
briefly toyed with the idea of creating altruistic AI's to solve this
<br>
problem, but with my considerably limited and regrettably uninformed
<br>
intelligence had thought that such things would not hold great enough
<br>
promise for too long a time.
<br>
<p>Eliezer's texts, and the others to which my attention soon spread,
<br>
fortunately dissolved my luddite views. A stereotypical singularitarian,
<br>
that's what I am now. :)
<br>
<p>But to be truthful, I probably differ from many of us in my motivations. I
<br>
am not here because I find all this stuff very cool (which I do), but
<br>
because of my strong ethical principles. I think many would feel that I am a
<br>
somewhat moralist person, and despite me liking technology, I would not have
<br>
a problem turning against it Unabomber-style if I honestly thought that that
<br>
would be the ethically correct decision (which it would be in some
<br>
theoretically possible worlds - fortunately we don't seem to be situated in
<br>
any of them).
<br>
<p>Presently, it is my view that the ethically best course of action for me
<br>
would be to switch to major in computer science (or at least to get out of
<br>
math), and for the most part to proceed to amass wealth to be donated to
<br>
SIAI. I think I am better suited to the task of acquiring funds for the SIAI
<br>
than to the task of becoming a FAI programmer or some such, as I find it
<br>
probable that several underemployed entities here and elsewhere would make
<br>
more talented FAI programmers than what I would make.
<br>
<p>This is becoming a rather long mail, but I'm trying to make it interesting
<br>
enough to meet list standards. At least I myself would be interested in
<br>
this. :) And here I might also note, that English isn't my native language,
<br>
which I can tell reading this. Probably many of you can as well.
<br>
<p>A few more words about the ethical aspects of me, as this is the
<br>
not-boringly-stereotypical part. I am presently comparatively actively
<br>
involved in the movement for the rights of non-human sentients on our
<br>
planet. Many of you know this movement as the animal rights movement, and
<br>
were it not for the singularity, I would actually consider animal rights
<br>
activism to be an ethically very productive pastime even for a rational
<br>
person (my reasoning for this is not as simple as one might think, but is
<br>
nonetheless irrelevant, as we live in a world where it is a high priority to
<br>
pay maximal or near-maximal attention to singularity matters).
<br>
<p>In the future, I'm hoping to shift a larger portion of my resources from AR
<br>
activism to singularity activism. One reason, why I haven't yet done so (not
<br>
very efficiently, at least), is that I prefer to spend my time with people
<br>
who are ethically strong and quite non-self-centered, like myself (in some
<br>
ways, at least). And while the present &quot;singularitarian elite&quot; consists of
<br>
such entities, egoistical persons abound in technophile communities, as they
<br>
do almost everywhere else. And if I were to strive towards effectively
<br>
spreading singularitarianism, for example, this ethically somewhat weak
<br>
majority of SL2 and SL1 audiences would make up the majority of the people I
<br>
would have to deal with.
<br>
<p>With the above I do not mean that I hate self-centered people, or that I
<br>
would act aggressively or in some other irrational manner towards them. I
<br>
do, however, have a strong (and often masked) feeling of superiority towards
<br>
them, and prefer the company of people who I consider to be emotionally and
<br>
ethically well-developed. I admit that this behaviour, as a form of elitism,
<br>
is irrational. Note also, that I do not claim morality to be objective, or
<br>
my feeling of superiority to have non-subjective grounds.
<br>
<p>I was born in the year 1980, in the country of Finland, which is situated in
<br>
Scandinavia. I continue to reside here.
<br>
<p>I haven't yet read all of the SL4 archives. As long as this statement
<br>
remains true, my enthusiasm to participate in the discussion here will
<br>
remain somewhat limited, as it should. And as long as I am not an active
<br>
poster, I probably will not subscribe to this list (except for the time
<br>
which I need to send the occasional post), as I feel it to be more
<br>
convenient for me to skim through the recent posts in the web archives. I am
<br>
mentioning this because I think that everyone here might not realise that a
<br>
large portion of the lurker population of SL4 might not even be subscribed.
<br>
The readership may be substantially larger than the subscriber population.
<br>
(I hope to be reprimanded for improper behaviour if this point has been
<br>
raised on the list before - remember to read the archives before posting,
<br>
everyone! ;)
<br>
<p>Anyway, in the future I might start a discussion regarding the present
<br>
non-human sentient population of our planet and the singularity. I mean, it
<br>
shouldn't be just the humans whom a superintelligent FAI will offer the
<br>
opportunity to transcend etc. It seems that singularitarians often miss this
<br>
point in their writing, even though it is a lucid fact that many non-human
<br>
animals are more sentient than human infants, for example. (I again hope to
<br>
be reprimanded, if this topic has been dealt with exhaustively.)
<br>
<p>I'll leave you with a link to a Prometheus Society article regarding the
<br>
intellectually exceptionally gifted. People, who have felt like &quot;An
<br>
Outsider&quot; because of their intellect might find this piece of writing to be
<br>
somewhat entertaining (though it gets boring around the middle, but there's
<br>
better stuff there too):
<br>
<p><a href="http://www.prometheussociety.org/articles/Outsiders.html">http://www.prometheussociety.org/articles/Outsiders.html</a>
<br>
<p><pre>
--
Aleksei Riikonen - <a href="mailto:aleksei@iki.fi?Subject=Re:%20JOIN:%20Aleksei%20Riikonen">aleksei@iki.fi</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5924.html">Gary Miller: "Will FAI need simulated empathy to avoid sociopathic tendencies?"</a>
<li><strong>Previous message:</strong> <a href="5922.html">BillK: "Re: Chess and Go study"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5923">[ date ]</a>
<a href="index.html#5923">[ thread ]</a>
<a href="subject.html#5923">[ subject ]</a>
<a href="author.html#5923">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
