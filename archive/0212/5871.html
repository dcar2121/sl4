<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: FAI and SSSM</title>
<meta name="Author" content="Bill Hibbard (test@doll.ssec.wisc.edu)">
<meta name="Subject" content="Re: FAI and SSSM">
<meta name="Date" content="2002-12-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: FAI and SSSM</h1>
<!-- received="Thu Dec 12 05:19:45 2002" -->
<!-- isoreceived="20021212121945" -->
<!-- sent="Thu, 12 Dec 2002 06:19:44 -0600 (CST)" -->
<!-- isosent="20021212121944" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@doll.ssec.wisc.edu" -->
<!-- subject="Re: FAI and SSSM" -->
<!-- id="Pine.SOL.4.33.0212120618060.18971-100000@doll.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3DF84D31.40300@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@doll.ssec.wisc.edu?Subject=Re:%20FAI%20and%20SSSM"><em>test@doll.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Thu Dec 12 2002 - 05:19:44 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5872.html">Ben Goertzel: "RE: FAI and SSSM"</a>
<li><strong>Previous message:</strong> <a href="5870.html">Samantha Atkins: "Re: Uploading with current technology"</a>
<li><strong>In reply to:</strong> <a href="5868.html">Eliezer S. Yudkowsky: "FAI and SSSM"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5872.html">Ben Goertzel: "RE: FAI and SSSM"</a>
<li><strong>Reply:</strong> <a href="5872.html">Ben Goertzel: "RE: FAI and SSSM"</a>
<li><strong>Reply:</strong> <a href="5877.html">Eliezer S. Yudkowsky: "Re: FAI and SSSM"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5871">[ date ]</a>
<a href="index.html#5871">[ thread ]</a>
<a href="subject.html#5871">[ subject ]</a>
<a href="author.html#5871">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi Eliezer,
<br>
<p><em>&gt; Bill Hibbard wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; With super-intelligent machines, the key to human safety is
</em><br>
<em>&gt; &gt; in controlling the values that reinforce learning of
</em><br>
<em>&gt; &gt; intelligent behaviors. In machines,
</em><br>
<em>&gt;
</em><br>
<em>&gt; Machines?  Something millions of times smarter than a human cannot be
</em><br>
<em>&gt; thought of as a &quot;machine&quot;.  Such entities, even if they are incarnated as
</em><br>
<em>&gt; physical processes, will not be physical processes that share the
</em><br>
<em>&gt; stereotypical characteristics of those physical processes we now cluster
</em><br>
<em>&gt; as &quot;biological&quot; or &quot;mechanical&quot;.
</em><br>
<p>You've just arguing over the definition of words. I am
<br>
using &quot;machines&quot; to mean artifacts constructed by humans.
<br>
If you read my book you'll see I imagine super-intelligent
<br>
machines as quite different from any other machines humans
<br>
have built.
<br>
<p><em>&gt;  &gt; we can design them so
</em><br>
<em>&gt; &gt; their behaviors are positively reinforced by human happiness
</em><br>
<em>&gt; &gt; and negatively reinforced by human unhappiness.
</em><br>
<em>&gt;
</em><br>
<em>&gt; A Friendly seed AI design, a la:
</em><br>
<em>&gt;   <a href="http://intelligence.org/CFAI/">http://intelligence.org/CFAI/</a>
</em><br>
<em>&gt;   <a href="http://intelligence.org/LOGI/">http://intelligence.org/LOGI/</a>
</em><br>
<em>&gt; doesn't have positive reinforcement or negative reinforcement, not the way
</em><br>
<em>&gt; you're describing them, at any rate.  This makes the implementation of
</em><br>
<em>&gt; your proposal somewhat difficult.
</em><br>
<p>I am well aware of the relation between your approach based
<br>
on planning behavior from goals, and my approach based on
<br>
values for reinforcement learning.
<br>
<p>A robust implementation of reinforcement learning must solve
<br>
the temporal credit assignment problem, which requires a
<br>
simulation model of the world. This simulation model is the
<br>
basis of reasoning based on goals. Planning and goal-based
<br>
reasoning are emergent behaviors of a robust implementation
<br>
of reinforcement learning.
<br>
<p><em>&gt; Positive reinforcement and negative reinforcement are cognitive systems
</em><br>
<em>&gt; that evolved in the absence of deliberative intelligence, via an
</em><br>
<em>&gt; evolutionary incremental crawl up adaptive pathways rather than high-level
</em><br>
<em>&gt; design.  A simple predictive goal system with Bayesian learning and
</em><br>
<em>&gt; Bayesian decisions emergently exhibits most of the functionality that in
</em><br>
<em>&gt; evolved organisms is implemented by separate subsystems for pain and
</em><br>
<em>&gt; pleasure.  See:
</em><br>
<em>&gt;   <a href="http://intelligence.org/friendly/features.html#causal">http://intelligence.org/friendly/features.html#causal</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; A simple goal system that runs on positive reinforcement and negative
</em><br>
<em>&gt; reinforcement would almost instantly short out once it had the ability to
</em><br>
<em>&gt; modify itself.  The systems that implement positive and negative
</em><br>
<em>&gt; reinforcement of goals would automatically be regarded as undesirable,
</em><br>
<em>&gt; since the only possible effect of their functioning is to make *current*
</em><br>
<em>&gt; goals less likely to be achieved, and the current goals at any given point
</em><br>
<em>&gt; are what would determine the perceived desirability of self-modifying
</em><br>
<em>&gt; actions such as &quot;delete the reinforcement system&quot;.  A Friendly AI design
</em><br>
<em>&gt; needs to be stable even given full self-modification.
</em><br>
<p>I think you may be assuming a non-robust implementation of
<br>
reinforcement learning that does not use a simulation model
<br>
to solve the temporal credit assignment problem.
<br>
<p><em>&gt; Finally, you're asking for too little - your proposal seems like a defense
</em><br>
<em>&gt; against fears of AI, rather than asking how far we can take supermorality
</em><br>
<em>&gt; once minds are freed from the constraints of evolutionary design.  This
</em><br>
<em>&gt; isn't a challenge that can be solved through a defensive posture - you
</em><br>
<em>&gt; have to step forward as far as you can.
</em><br>
<p>Not at all. Reinforcement is a two-way street, including both
<br>
negative (what you call defensive) and positive reinforcement.
<br>
My book includes a vivid description of the sort of heaven on
<br>
earth that super-intelligent machines will create for humans,
<br>
assuming that they learn behaviors based on values of human
<br>
happiness, and assuming that they solve the temporal credit
<br>
assignment problem so they can reason about long term happiness.
<br>
<p><em>&gt; &gt; Behaviors are reinforced by much different values in human
</em><br>
<em>&gt; &gt; brains. Human values are mostly self-interest. As social
</em><br>
<em>&gt; &gt; animals humans have some more altruistic values, but these
</em><br>
<em>&gt; &gt; mostly depend on social pressure. Very powerful humans can
</em><br>
<em>&gt; &gt; transcend social pressure and revert to their selfish values,
</em><br>
<em>&gt; &gt; hence the maxim that power corrupts and absolute power
</em><br>
<em>&gt; &gt; corrupts absolutely.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I strongly recommend that you read Steven Pinker's &quot;The Blank Slate&quot;.
</em><br>
<em>&gt; You're arguing from a model of psychology which has today become known as
</em><br>
<em>&gt; the &quot;Standard Social Sciences Model&quot;, and which has since been disproven
</em><br>
<em>&gt; and discarded.  Human cognition, including human altruism, is far more
</em><br>
<em>&gt; complex and includes far more innate complexity than the behaviorists
</em><br>
<em>&gt; believed.
</em><br>
<p>I am quite familiar with Pinker's ideas. He gave a great talk
<br>
on &quot;The Blank Slate&quot; here in Wisconsin last year (I was lucky
<br>
to get a seat, the room was packed). In fact, my ideas about
<br>
human selfishness and altruism are largely based on Pinker's
<br>
How the Mind Works.
<br>
<p>I think you are assuming I am a Skinner behaviorist because you
<br>
are thinking of reinforcement learning without a solution of
<br>
the temporal credit assignment problem.
<br>
<p><em>&gt; If you can't spare the effort for &quot;The Blank Slate&quot;, . . .
</em><br>
<p>That's kind of a cheap shot, Eliezer. I read voraciously. And
<br>
I code voraciously, which you'll see if you visit the URL in
<br>
my sig.
<br>
<p>Cheers,
<br>
Bill
<br>
----------------------------------------------------------
<br>
Bill Hibbard, SSEC, 1225 W. Dayton St., Madison, WI  53706
<br>
<a href="mailto:test@doll.ssec.wisc.edu?Subject=Re:%20FAI%20and%20SSSM">test@doll.ssec.wisc.edu</a>  608-263-4427  fax: 608-263-6738
<br>
<a href="http://www.ssec.wisc.edu/~billh/vis.html">http://www.ssec.wisc.edu/~billh/vis.html</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5872.html">Ben Goertzel: "RE: FAI and SSSM"</a>
<li><strong>Previous message:</strong> <a href="5870.html">Samantha Atkins: "Re: Uploading with current technology"</a>
<li><strong>In reply to:</strong> <a href="5868.html">Eliezer S. Yudkowsky: "FAI and SSSM"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5872.html">Ben Goertzel: "RE: FAI and SSSM"</a>
<li><strong>Reply:</strong> <a href="5872.html">Ben Goertzel: "RE: FAI and SSSM"</a>
<li><strong>Reply:</strong> <a href="5877.html">Eliezer S. Yudkowsky: "Re: FAI and SSSM"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5871">[ date ]</a>
<a href="index.html#5871">[ thread ]</a>
<a href="subject.html#5871">[ subject ]</a>
<a href="author.html#5871">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
