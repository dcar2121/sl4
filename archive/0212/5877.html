<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: FAI and SSSM</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: FAI and SSSM">
<meta name="Date" content="2002-12-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: FAI and SSSM</h1>
<!-- received="Thu Dec 12 19:28:55 2002" -->
<!-- isoreceived="20021213022855" -->
<!-- sent="Thu, 12 Dec 2002 21:28:47 -0500" -->
<!-- isosent="20021213022847" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: FAI and SSSM" -->
<!-- id="3DF945DF.3030505@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="Pine.SOL.4.33.0212120618060.18971-100000@doll.ssec.wisc.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20FAI%20and%20SSSM"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu Dec 12 2002 - 19:28:47 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5878.html">Gordon Worley: "SL4 Calendar of Events"</a>
<li><strong>Previous message:</strong> <a href="5876.html">Ben Goertzel: "RE: Reality Theory"</a>
<li><strong>In reply to:</strong> <a href="5871.html">Bill Hibbard: "Re: FAI and SSSM"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5880.html">Ben Goertzel: "RE: FAI and SSSM"</a>
<li><strong>Reply:</strong> <a href="5880.html">Ben Goertzel: "RE: FAI and SSSM"</a>
<li><strong>Reply:</strong> <a href="5886.html">Damien Broderick: "Bill's book"</a>
<li><strong>Reply:</strong> <a href="5887.html">Bill Hibbard: "Re: FAI and SSSM"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5877">[ date ]</a>
<a href="index.html#5877">[ thread ]</a>
<a href="subject.html#5877">[ subject ]</a>
<a href="author.html#5877">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Bill Hibbard wrote:
<br>
<p><em>&gt;&gt;&gt;With super-intelligent machines, the key to human safety is
</em><br>
<em>&gt;&gt;&gt;in controlling the values that reinforce learning of
</em><br>
<em>&gt;&gt;&gt;intelligent behaviors. In machines,
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Machines?  Something millions of times smarter than a human cannot be
</em><br>
<em>&gt;&gt;thought of as a &quot;machine&quot;.  Such entities, even if they are incarnated as
</em><br>
<em>&gt;&gt;physical processes, will not be physical processes that share the
</em><br>
<em>&gt;&gt;stereotypical characteristics of those physical processes we now cluster
</em><br>
<em>&gt;&gt;as &quot;biological&quot; or &quot;mechanical&quot;.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You've just arguing over the definition of words. I am
</em><br>
<em>&gt; using &quot;machines&quot; to mean artifacts constructed by humans.
</em><br>
<em>&gt; If you read my book you'll see I imagine super-intelligent
</em><br>
<em>&gt; machines as quite different from any other machines humans
</em><br>
<em>&gt; have built.
</em><br>
<p>What about processes that construct themselves?  Does it make sense to 
<br>
describe the child of the child of the child of the child of the mind that 
<br>
humans originally built as an &quot;artifact constructed by humans&quot;?  Is it 
<br>
useful to describe it so, when it shares none of the characteristics that 
<br>
we presently attach to &quot;machines&quot;?  Call it a mind, or better yet an 
<br>
entity; we might be wrong on both counts but at least we won't be quite as 
<br>
wrong as if we use the word &quot;machine&quot;.
<br>
<p>Yes, I'm arguing over the definition of a word; deliberately so because I 
<br>
think that people expect certain characteristics to hold true of 
<br>
&quot;machines&quot;, and that these characteristics don't hold true of SIs 
<br>
(superintelligences).  I would expect an originally human SI and an 
<br>
originally human-built SI to have more in common with each other than 
<br>
either would have in common with a modern-day human or a human-equivalent 
<br>
AI, and I would expect even a half-grown AI to have almost nothing in 
<br>
common with the physical objects we categorize as &quot;machines&quot;.
<br>
<p><em>&gt;&gt;&gt; we can design them so
</em><br>
<em>&gt;&gt;&gt;their behaviors are positively reinforced by human happiness
</em><br>
<em>&gt;&gt;&gt;and negatively reinforced by human unhappiness.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;A Friendly seed AI design, a la:
</em><br>
<em>&gt;&gt;  <a href="http://intelligence.org/CFAI/">http://intelligence.org/CFAI/</a>
</em><br>
<em>&gt;&gt;  <a href="http://intelligence.org/LOGI/">http://intelligence.org/LOGI/</a>
</em><br>
<em>&gt;&gt;doesn't have positive reinforcement or negative reinforcement, not the way
</em><br>
<em>&gt;&gt;you're describing them, at any rate.  This makes the implementation of
</em><br>
<em>&gt;&gt;your proposal somewhat difficult.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I am well aware of the relation between your approach based
</em><br>
<em>&gt; on planning behavior from goals, and my approach based on
</em><br>
<em>&gt; values for reinforcement learning.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; A robust implementation of reinforcement learning must solve
</em><br>
<em>&gt; the temporal credit assignment problem, which requires a
</em><br>
<em>&gt; simulation model of the world. This simulation model is the
</em><br>
<em>&gt; basis of reasoning based on goals. Planning and goal-based
</em><br>
<em>&gt; reasoning are emergent behaviors of a robust implementation
</em><br>
<em>&gt; of reinforcement learning.
</em><br>
<p>Perhaps the complex behavior of planning is emergent in the simple 
<br>
behavior of reinforcement, as well as the simple behavior of reinforcement 
<br>
being a special case of the complex behavior of planning.  I don't think 
<br>
so, but then I haven't tried to figure out how to do it, so I wouldn't 
<br>
know whether it's possible.
<br>
<p>But human evolution includes specific selection pressures on goals, apart 
<br>
from selection pressures on reinforcement.  Imperfectly deceptive social 
<br>
organisms that argue linguistically about each other's motives in adaptive 
<br>
political contexts develop altruistic motives and rationalizations from 
<br>
altruistic motives to selfish actions; if supra-ancestral increase in 
<br>
intelligence or knowledge overcomes the force of rationalization, you are 
<br>
then left with a genuine altruist.  How would a robust implementation of 
<br>
reinforcement learning duplicate the moral and metamoral adaptations which 
<br>
are the result of highly specific selection pressures in an ancestral 
<br>
environment not shared by AIs?  You can transfer moral complexity directly 
<br>
rather than trying to reduplicate its evolutionary causation in humans, 
<br>
but you do have to transfer that complexity - it is not emergent just from 
<br>
reinforcement.
<br>
<p><em>&gt;&gt;A simple goal system that runs on positive reinforcement and negative
</em><br>
<em>&gt;&gt;reinforcement would almost instantly short out once it had the ability to
</em><br>
<em>&gt;&gt;modify itself.  The systems that implement positive and negative
</em><br>
<em>&gt;&gt;reinforcement of goals would automatically be regarded as undesirable,
</em><br>
<em>&gt;&gt;since the only possible effect of their functioning is to make *current*
</em><br>
<em>&gt;&gt;goals less likely to be achieved, and the current goals at any given point
</em><br>
<em>&gt;&gt;are what would determine the perceived desirability of self-modifying
</em><br>
<em>&gt;&gt;actions such as &quot;delete the reinforcement system&quot;.  A Friendly AI design
</em><br>
<em>&gt;&gt;needs to be stable even given full self-modification.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think you may be assuming a non-robust implementation of
</em><br>
<em>&gt; reinforcement learning that does not use a simulation model
</em><br>
<em>&gt; to solve the temporal credit assignment problem.
</em><br>
<p>I confess that I don't see how this changes anything at all.  I assumed a 
<br>
simulation model that is not only used for temporal credit assignment, but 
<br>
which allows for imagination of novel behaviors whose desirability is 
<br>
determined by the match of their extrapolated effects against previously 
<br>
reinforced goal patterns.  Without this ability, no reinforcement-based 
<br>
system would ever be capable of carrying out complex creative actions such 
<br>
as computer programming - when I write a program, I am reasoning from 
<br>
abstract, high-level design goals to novel concrete code, not just 
<br>
implementing coding behaviors that have been previously reinforced.
<br>
<p>When I say &quot;simple reinforcement system&quot;, I mean &quot;a lot simpler than a 
<br>
human or a Friendly AI&quot;; &quot;simple&quot; does include full modeling/simulation 
<br>
capabilities, for both credit assignment and imagination of novel 
<br>
behaviors.  Maybe calling it a &quot;flat&quot; reinforcement system would be 
<br>
better.  The problem with a flat reinforcement system is that it 
<br>
flash-freezes itself the moment it becomes capable of self-modification. 
<br>
Originally, you built the system such that it contained certain internal 
<br>
functional modules which modified goal patterns conditional on external 
<br>
sensory events.  And from its goals at any given point, the system judges 
<br>
the desirability of future states of the universe, and hence the 
<br>
desirability of actions leading to those future states.
<br>
<p>Now imagine this system looking at the fact that it possesses 
<br>
reinforcement modules, and considering the desirability of actions which 
<br>
remove those modules.  Any internal system, whose effect is to change the 
<br>
cognitive pattern against which imagined future events are matched to 
<br>
determine their desirability, is automatically undesirable; if the AI's 
<br>
future pattern changes, then the AI will take actions which result in an 
<br>
inferior match of those futures against the current pattern governing 
<br>
actions.  To protect the goals currently governing actions (including 
<br>
self-modifying actions), the system will remove any internal functionality 
<br>
whose effect is to modify the top layer of its goal system.
<br>
<p>This action feels intuitively wrong to a human because humans have extra 
<br>
complexity in the goal system, which for purposes of Friendly AI we can 
<br>
think of as humans treating moral arguments as having the semantics of 
<br>
probabilistic statements about external referents.  See the appropriate 
<br>
sections of &quot;Creating Friendly AI&quot; for more information.
<br>
<p>How do you think temporal credit assignment would change this?  It doesn't 
<br>
seem relevant.
<br>
<p><em>&gt;&gt;Finally, you're asking for too little - your proposal seems like a defense
</em><br>
<em>&gt;&gt;against fears of AI, rather than asking how far we can take supermorality
</em><br>
<em>&gt;&gt;once minds are freed from the constraints of evolutionary design.  This
</em><br>
<em>&gt;&gt;isn't a challenge that can be solved through a defensive posture - you
</em><br>
<em>&gt;&gt;have to step forward as far as you can.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Not at all. Reinforcement is a two-way street, including both
</em><br>
<em>&gt; negative (what you call defensive)
</em><br>
<p>No, that wasn't what I meant by &quot;defensive&quot; at all.  I was referring to 
<br>
human attitudes about futurism.
<br>
<p><em> &gt; and positive reinforcement.
</em><br>
<em>&gt; My book includes a vivid description of the sort of heaven on
</em><br>
<em>&gt; earth that super-intelligent machines will create for humans,
</em><br>
<p>As I believe your book observes, such vivid descriptions are pointless 
<br>
because we aren't smart enough to get the description right.  For example, 
<br>
your book refers to automated farms and factories rather than 
<br>
nanotechnology and uploading.  This, of course, does not mean that 
<br>
nanotechnology is the correct description; only that we can already be 
<br>
pretty sure that farming and factories are destined for the junk-heap of 
<br>
history.
<br>
<p>Recommended book:  &quot;Permutation City&quot;, Greg Egan.
<br>
Recommending online reading:
<br>
&nbsp;&nbsp;<a href="http://intelligence.org/intro/nanotech.html">http://intelligence.org/intro/nanotech.html</a>
<br>
&nbsp;&nbsp;<a href="http://intelligence.org/intro/upload.html">http://intelligence.org/intro/upload.html</a>
<br>
&nbsp;&nbsp;<a href="http://intelligence.org/what-singularity.html">http://intelligence.org/what-singularity.html</a>
<br>
<p><em>&gt; assuming that they learn behaviors based on values of human
</em><br>
<em>&gt; happiness, and assuming that they solve the temporal credit
</em><br>
<em>&gt; assignment problem so they can reason about long term happiness.
</em><br>
<p>What if someone has goals beyond happiness?  Many philosophies involve 
<br>
greater complexity than that.
<br>
<p>My current best understanding of morality is that &quot;good&quot; consists of 
<br>
people getting what they want, defined however they choose to define it. 
<br>
But I'm not infallible, so that understanding is itself subject to change. 
<br>
&nbsp;&nbsp;What happens if it turns out that &quot;happiness&quot; isn't what you really 
<br>
wanted?  How does your design recover from philosophical errors by the 
<br>
programmers?
<br>
<p><em>&gt;&gt;&gt;Behaviors are reinforced by much different values in human
</em><br>
<em>&gt;&gt;&gt;brains. Human values are mostly self-interest. As social
</em><br>
<em>&gt;&gt;&gt;animals humans have some more altruistic values, but these
</em><br>
<em>&gt;&gt;&gt;mostly depend on social pressure. Very powerful humans can
</em><br>
<em>&gt;&gt;&gt;transcend social pressure and revert to their selfish values,
</em><br>
<em>&gt;&gt;&gt;hence the maxim that power corrupts and absolute power
</em><br>
<em>&gt;&gt;&gt;corrupts absolutely.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;I strongly recommend that you read Steven Pinker's &quot;The Blank Slate&quot;.
</em><br>
<em>&gt;&gt;You're arguing from a model of psychology which has today become known as
</em><br>
<em>&gt;&gt;the &quot;Standard Social Sciences Model&quot;, and which has since been disproven
</em><br>
<em>&gt;&gt;and discarded.  Human cognition, including human altruism, is far more
</em><br>
<em>&gt;&gt;complex and includes far more innate complexity than the behaviorists
</em><br>
<em>&gt;&gt;believed.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I am quite familiar with Pinker's ideas. He gave a great talk
</em><br>
<em>&gt; on &quot;The Blank Slate&quot; here in Wisconsin last year (I was lucky
</em><br>
<em>&gt; to get a seat, the room was packed). In fact, my ideas about
</em><br>
<em>&gt; human selfishness and altruism are largely based on Pinker's
</em><br>
<em>&gt; How the Mind Works.
</em><br>
<p>I have not read that one of Pinker's books, so I don't know how much 
<br>
emotional evolutionary psychology is in it.  Anyway, if you're lucky 
<br>
enough to have access to a source of dead trees, you might also want to 
<br>
read Matt Ridley's &quot;The Origins of Virtue&quot; for specific material on the 
<br>
evolutionary origins of altruism.
<br>
<p><em>&gt; I think you are assuming I am a Skinner behaviorist because you
</em><br>
<em>&gt; are thinking of reinforcement learning without a solution of
</em><br>
<em>&gt; the temporal credit assignment problem.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;If you can't spare the effort for &quot;The Blank Slate&quot;, . . .
</em><br>
<em>&gt; 
</em><br>
<em>&gt; That's kind of a cheap shot, Eliezer.
</em><br>
<p>It honestly wasn't intended as such.  People have lives beyond the books I 
<br>
want them to read, and I try to be aware of that.  These days I rarely if 
<br>
ever read material that is not available online.  There's a lot more 
<br>
effort involved in obtaining a dead tree than in clicking a link, so if 
<br>
someone can't spare the effort for the dead tree, I try to provide a link 
<br>
as a second-best alternative, if possible.  I guess I should be more 
<br>
careful in my phrasing next time.  Sorry.
<br>
<p>I also didn't realize that your book to which you referred was available 
<br>
online.  I've now read it.  Don't suppose you could return the favor and 
<br>
check out &quot;Creating Friendly AI&quot;, if you haven't done so already?
<br>
<p><a href="http://www.ssec.wisc.edu/~billh/gotterdammerung.html">http://www.ssec.wisc.edu/~billh/gotterdammerung.html</a>
<br>
<p><a href="http://intelligence.org/CFAI/">http://intelligence.org/CFAI/</a>
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5878.html">Gordon Worley: "SL4 Calendar of Events"</a>
<li><strong>Previous message:</strong> <a href="5876.html">Ben Goertzel: "RE: Reality Theory"</a>
<li><strong>In reply to:</strong> <a href="5871.html">Bill Hibbard: "Re: FAI and SSSM"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5880.html">Ben Goertzel: "RE: FAI and SSSM"</a>
<li><strong>Reply:</strong> <a href="5880.html">Ben Goertzel: "RE: FAI and SSSM"</a>
<li><strong>Reply:</strong> <a href="5886.html">Damien Broderick: "Bill's book"</a>
<li><strong>Reply:</strong> <a href="5887.html">Bill Hibbard: "Re: FAI and SSSM"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5877">[ date ]</a>
<a href="index.html#5877">[ thread ]</a>
<a href="subject.html#5877">[ subject ]</a>
<a href="author.html#5877">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
