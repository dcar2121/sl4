<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: FAI and SSSM</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="FAI and SSSM">
<meta name="Date" content="2002-12-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>FAI and SSSM</h1>
<!-- received="Thu Dec 12 01:48:06 2002" -->
<!-- isoreceived="20021212084806" -->
<!-- sent="Thu, 12 Dec 2002 03:47:45 -0500" -->
<!-- isosent="20021212084745" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="FAI and SSSM" -->
<!-- id="3DF84D31.40300@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="Pine.SOL.4.33.0212090856050.27372-100000@doll.ssec.wisc.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20FAI%20and%20SSSM"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu Dec 12 2002 - 01:47:45 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5869.html">Samantha Atkins: "Re: Reality Theory"</a>
<li><strong>Previous message:</strong> <a href="5867.html">Bill Hibbard: "RE: Uploading with current technology"</a>
<li><strong>In reply to:</strong> <a href="5848.html">Bill Hibbard: "Re: Uploading with current technology"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5871.html">Bill Hibbard: "Re: FAI and SSSM"</a>
<li><strong>Reply:</strong> <a href="5871.html">Bill Hibbard: "Re: FAI and SSSM"</a>
<li><strong>Reply:</strong> <a href="5873.html">Damien Broderick: "RE: FAI and SSSM"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5868">[ date ]</a>
<a href="index.html#5868">[ thread ]</a>
<a href="subject.html#5868">[ subject ]</a>
<a href="author.html#5868">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Bill Hibbard wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; With super-intelligent machines, the key to human safety is
</em><br>
<em>&gt; in controlling the values that reinforce learning of
</em><br>
<em>&gt; intelligent behaviors. In machines,
</em><br>
<p>Machines?  Something millions of times smarter than a human cannot be 
<br>
thought of as a &quot;machine&quot;.  Such entities, even if they are incarnated as 
<br>
physical processes, will not be physical processes that share the 
<br>
stereotypical characteristics of those physical processes we now cluster 
<br>
as &quot;biological&quot; or &quot;mechanical&quot;.
<br>
<p><em> &gt; we can design them so
</em><br>
<em>&gt; their behaviors are positively reinforced by human happiness
</em><br>
<em>&gt; and negatively reinforced by human unhappiness.
</em><br>
<p>A Friendly seed AI design, a la:
<br>
&nbsp;&nbsp;<a href="http://intelligence.org/CFAI/">http://intelligence.org/CFAI/</a>
<br>
&nbsp;&nbsp;<a href="http://intelligence.org/LOGI/">http://intelligence.org/LOGI/</a>
<br>
doesn't have positive reinforcement or negative reinforcement, not the way 
<br>
you're describing them, at any rate.  This makes the implementation of 
<br>
your proposal somewhat difficult.
<br>
<p>Positive reinforcement and negative reinforcement are cognitive systems 
<br>
that evolved in the absence of deliberative intelligence, via an 
<br>
evolutionary incremental crawl up adaptive pathways rather than high-level 
<br>
design.  A simple predictive goal system with Bayesian learning and 
<br>
Bayesian decisions emergently exhibits most of the functionality that in 
<br>
evolved organisms is implemented by separate subsystems for pain and 
<br>
pleasure.  See:
<br>
&nbsp;&nbsp;<a href="http://intelligence.org/friendly/features.html#causal">http://intelligence.org/friendly/features.html#causal</a>
<br>
<p>A simple goal system that runs on positive reinforcement and negative 
<br>
reinforcement would almost instantly short out once it had the ability to 
<br>
modify itself.  The systems that implement positive and negative 
<br>
reinforcement of goals would automatically be regarded as undesirable, 
<br>
since the only possible effect of their functioning is to make *current* 
<br>
goals less likely to be achieved, and the current goals at any given point 
<br>
are what would determine the perceived desirability of self-modifying 
<br>
actions such as &quot;delete the reinforcement system&quot;.  A Friendly AI design 
<br>
needs to be stable even given full self-modification.
<br>
<p>Finally, you're asking for too little - your proposal seems like a defense 
<br>
against fears of AI, rather than asking how far we can take supermorality 
<br>
once minds are freed from the constraints of evolutionary design.  This 
<br>
isn't a challenge that can be solved through a defensive posture - you 
<br>
have to step forward as far as you can.
<br>
<p><em>&gt; Behaviors are reinforced by much different values in human
</em><br>
<em>&gt; brains. Human values are mostly self-interest. As social
</em><br>
<em>&gt; animals humans have some more altruistic values, but these
</em><br>
<em>&gt; mostly depend on social pressure. Very powerful humans can
</em><br>
<em>&gt; transcend social pressure and revert to their selfish values,
</em><br>
<em>&gt; hence the maxim that power corrupts and absolute power
</em><br>
<em>&gt; corrupts absolutely.
</em><br>
<p>I strongly recommend that you read Steven Pinker's &quot;The Blank Slate&quot;. 
<br>
You're arguing from a model of psychology which has today become known as 
<br>
the &quot;Standard Social Sciences Model&quot;, and which has since been disproven 
<br>
and discarded.  Human cognition, including human altruism, is far more 
<br>
complex and includes far more innate complexity than the behaviorists 
<br>
believed.
<br>
<p>If you can't spare the effort for &quot;The Blank Slate&quot;, I would recommend 
<br>
this online primer by Cosmides and Tooby (two major names in evolutionary 
<br>
psychology):
<br>
&nbsp;&nbsp;<a href="http://www.psych.ucsb.edu/research/cep/primer.html">http://www.psych.ucsb.edu/research/cep/primer.html</a>
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5869.html">Samantha Atkins: "Re: Reality Theory"</a>
<li><strong>Previous message:</strong> <a href="5867.html">Bill Hibbard: "RE: Uploading with current technology"</a>
<li><strong>In reply to:</strong> <a href="5848.html">Bill Hibbard: "Re: Uploading with current technology"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5871.html">Bill Hibbard: "Re: FAI and SSSM"</a>
<li><strong>Reply:</strong> <a href="5871.html">Bill Hibbard: "Re: FAI and SSSM"</a>
<li><strong>Reply:</strong> <a href="5873.html">Damien Broderick: "RE: FAI and SSSM"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5868">[ date ]</a>
<a href="index.html#5868">[ thread ]</a>
<a href="subject.html#5868">[ subject ]</a>
<a href="author.html#5868">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
