<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Uploading with current technology</title>
<meta name="Author" content="Bill Hibbard (test@doll.ssec.wisc.edu)">
<meta name="Subject" content="RE: Uploading with current technology">
<meta name="Date" content="2002-12-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Uploading with current technology</h1>
<!-- received="Mon Dec  9 10:54:51 2002" -->
<!-- isoreceived="20021209175451" -->
<!-- sent="Mon, 9 Dec 2002 11:54:50 -0600 (CST)" -->
<!-- isosent="20021209175450" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@doll.ssec.wisc.edu" -->
<!-- subject="RE: Uploading with current technology" -->
<!-- id="Pine.SOL.4.33.0212091140180.28910-100000@doll.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="000001c29f9d$88b99220$3e553f94@GaryMiller01" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@doll.ssec.wisc.edu?Subject=RE:%20Uploading%20with%20current%20technology"><em>test@doll.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Mon Dec 09 2002 - 10:54:50 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5852.html">Bill Hibbard: "Re: Uploading with current technology"</a>
<li><strong>Previous message:</strong> <a href="5850.html">polysync@pobox.com: "Re: Uploading with current technology"</a>
<li><strong>In reply to:</strong> <a href="5849.html">Gary Miller: "RE: Uploading with current technology"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5853.html">Gary Miller: "RE: Uploading with current technology (Sorry Off topic)"</a>
<li><strong>Reply:</strong> <a href="5853.html">Gary Miller: "RE: Uploading with current technology (Sorry Off topic)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5851">[ date ]</a>
<a href="index.html#5851">[ thread ]</a>
<a href="subject.html#5851">[ subject ]</a>
<a href="author.html#5851">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi Gary,
<br>
<p>On Mon, 9 Dec 2002, Gary Miller wrote:
<br>
<p><em>&gt; Hi Bill,
</em><br>
<em>&gt;
</em><br>
<em>&gt; On Sun, 9 Dec 2002, Bill Hibbard wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt; Furthermore, behaviors learned via their old greedy or xenophobic
</em><br>
<em>&gt; values would be negatively
</em><br>
<em>&gt; &gt;&gt; reinforced and disappear.
</em><br>
<em>&gt;
</em><br>
<em>&gt; How do you give negative reinforcement to someone who has succeeded so
</em><br>
<em>&gt; far beyond the average man that they are both spiritually, emotionally
</em><br>
<em>&gt; and physically untouchable?
</em><br>
<p>Reinforcement values can be built into the basic learning
<br>
architecture of a brain. The communist experiments of the
<br>
twentieth century demonstrated the difficulty of changing
<br>
basic human values.
<br>
<p><em>&gt; Obsessive fear of losing what one has already worked so hard to achieve
</em><br>
<em>&gt; is one of the drivers for achieving ever increasing power and wealth.
</em><br>
<em>&gt; Perhaps it is the recognition and fear of one's eventual mortality today
</em><br>
<em>&gt; that encourages the very rich to share the wealth through philanthropy
</em><br>
<em>&gt; and to invest in their afterlife so to speak.  Once a person has reached
</em><br>
<em>&gt; this level of success and power, I would defy anyone to reeducate them
</em><br>
<em>&gt; to the fact that giving a large portion of their money away is the
</em><br>
<em>&gt; optimal way to further their own self-interests especially if their
</em><br>
<em>&gt; life-spans were hugely extended.
</em><br>
<p>This just seconds what I said in my message: socially
<br>
imposed values can be easily over-powered by the innate
<br>
values of human brains, in humans with the power to ignore
<br>
social pressure.
<br>
<p>Thus to insure human safety in a world populated by super-
<br>
intelligent machines or humans, the basic (hard-wired)
<br>
reinforcement learning values of super-intelligent brains
<br>
must be the happiness of all humans.
<br>
<p><em>&gt; We live in a day again where the middle class is being eroded from the
</em><br>
<em>&gt; top and bottom. The rich do get richer and the poor are becoming more
</em><br>
<em>&gt; numerous.  I have a tremendous respect for people like Bill Gates who
</em><br>
<em>&gt; are spending large amounts of their money in this life to improve living
</em><br>
<em>&gt; conditions in so many parts of the world.  I would pray to see this
</em><br>
<em>&gt; become the norm instead of the exception.  But unfortunately too many
</em><br>
<em>&gt; billionaires still operate under the philosophy that &quot;whoever dies with
</em><br>
<em>&gt; the most toys (or billions) wins the game&quot;.
</em><br>
<p>Bill Gates may not be all that altruistic. Perhaps he is trying
<br>
to counteract the bad publicity of the M$ antitrust case. His
<br>
anti-AIDS campaign is wonderful, but it is interesting that it is
<br>
targeted at India where there are many talented programmers, rather
<br>
than Africa where there are not so many programmers.
<br>
<p>Cheers,
<br>
Bill
<br>
<p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20Uploading%20with%20current%20technology">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20Uploading%20with%20current%20technology">owner-sl4@sl4.org</a>] On Behalf Of Bill
</em><br>
<em>&gt; Hibbard
</em><br>
<em>&gt; Sent: Monday, December 09, 2002 10:00 AM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=RE:%20Uploading%20with%20current%20technology">sl4@sl4.org</a>
</em><br>
<em>&gt; Subject: Re: Uploading with current technology
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Hi Gordon,
</em><br>
<em>&gt;
</em><br>
<em>&gt; On Sun, 8 Dec 2002, Gordon Worley wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; On Sunday, December 8, 2002, at 01:08  PM, Ben Goertzel wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt; <a href="http://users.rcn.com/standley/AI/immortality.htm">http://users.rcn.com/standley/AI/immortality.htm</a>
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; Thoughts?
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; Can anyone with more neuro expertise tell me: Is this guy correct as
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; &gt; regards what is currently technologically plausible?
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The Singularity and, specifically, FAI is a faster, safer way of
</em><br>
<em>&gt; &gt; transcending.  Super *human* intelligence is highly dangerous.  Think
</em><br>
<em>&gt; &gt; male chimp with nuclear feces.  Unless you've got someone way protect
</em><br>
<em>&gt; &gt; the universe from the super *humans*, we're probably better off with
</em><br>
<em>&gt; &gt; our current brains.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I largely agree. But as I point out in my book:
</em><br>
<em>&gt;
</em><br>
<em>&gt;   <a href="http://www.ssec.wisc.edu/~billh/super.html">http://www.ssec.wisc.edu/~billh/super.html</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; after humans meet super-intelligent machines they will want
</em><br>
<em>&gt; to become super-intelligent themselves, and will want the indfinite life
</em><br>
<em>&gt; span of a repairable machine brain supporting their mind.
</em><br>
<em>&gt;
</em><br>
<em>&gt; With super-intelligent machines, the key to human safety is
</em><br>
<em>&gt; in controlling the values that reinforce learning of intelligent
</em><br>
<em>&gt; behaviors. In machines, we can design them so their behaviors are
</em><br>
<em>&gt; positively reinforced by human happiness and negatively reinforced by
</em><br>
<em>&gt; human unhappiness.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Behaviors are reinforced by much different values in human brains. Human
</em><br>
<em>&gt; values are mostly self-interest. As social animals humans have some more
</em><br>
<em>&gt; altruistic values, but these mostly depend on social pressure. Very
</em><br>
<em>&gt; powerful humans can transcend social pressure and revert to their
</em><br>
<em>&gt; selfish values, hence the maxim that power corrupts and absolute power
</em><br>
<em>&gt; corrupts absolutely. Nothing will give a human more power than
</em><br>
<em>&gt; super-intelligence.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Society has a gradual (lots of short-term setbacks, to be
</em><br>
<em>&gt; sure) long-term trend toward equality because human brains
</em><br>
<em>&gt; are distributed quite democratically: the largest IQ (not
</em><br>
<em>&gt; a perfect measure, but widely applied) in history is only
</em><br>
<em>&gt; twice the average. However, the largest computers, buildings, trucks,
</em><br>
<em>&gt; etc are thousands of times their averages. The migration of human minds
</em><br>
<em>&gt; into machine brains theatens to end the even distribution of human
</em><br>
<em>&gt; intelligence, and hence end the gradual long-term trend toward social
</em><br>
<em>&gt; equality.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Given that the combination of super-intelligence and human values is
</em><br>
<em>&gt; dangerous, the solution is to make alteration of reinforcement learning
</em><br>
<em>&gt; values a necessary condition for granting a human super-intelligence.
</em><br>
<em>&gt; That is, when we have the technology to manipulate human intelligence
</em><br>
<em>&gt; then we also need to develop the technology to manipulate human
</em><br>
<em>&gt; reinforcement learning values. Because this change in values would
</em><br>
<em>&gt; affect learning, it would not immediately change the human's old
</em><br>
<em>&gt; behaviors. Hence they would still &quot;be themselves&quot;. But as they learned
</em><br>
<em>&gt; super-intelligent behaviors, their new values would cause those newly
</em><br>
<em>&gt; learned behaviors to serve the happiness of all humans. Furthermore,
</em><br>
<em>&gt; behaviors learned via their old greedy or xenophobic values would be
</em><br>
<em>&gt; negatively reinforced and disappear.
</em><br>
<em>&gt;
</em><br>
<em>&gt; One danger is the temptation to use genetic manipulation as a shortcut
</em><br>
<em>&gt; to super-intelligent humans. This may provide a way to increase human
</em><br>
<em>&gt; intelligence before we understand how it works and before we know how to
</em><br>
<em>&gt; change human reinforcement learning values. This danger is neatly
</em><br>
<em>&gt; parallel with Mary Shelley's Frankestein, in which a human monster is
</em><br>
<em>&gt; created by a scientist tinkering with technology thet he did not really
</em><br>
<em>&gt; understand. We need to understand how human brains work and solve the
</em><br>
<em>&gt; AGI problem before we start manipulating human brains.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Cheers,
</em><br>
<em>&gt; Bill
</em><br>
<em>&gt; ----------------------------------------------------------
</em><br>
<em>&gt; Bill Hibbard, SSEC, 1225 W. Dayton St., Madison, WI  53706
</em><br>
<em>&gt; <a href="mailto:test@doll.ssec.wisc.edu?Subject=RE:%20Uploading%20with%20current%20technology">test@doll.ssec.wisc.edu</a>  608-263-4427  fax: 608-263-6738
</em><br>
<em>&gt; <a href="http://www.ssec.wisc.edu/~billh/vis.html">http://www.ssec.wisc.edu/~billh/vis.html</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5852.html">Bill Hibbard: "Re: Uploading with current technology"</a>
<li><strong>Previous message:</strong> <a href="5850.html">polysync@pobox.com: "Re: Uploading with current technology"</a>
<li><strong>In reply to:</strong> <a href="5849.html">Gary Miller: "RE: Uploading with current technology"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5853.html">Gary Miller: "RE: Uploading with current technology (Sorry Off topic)"</a>
<li><strong>Reply:</strong> <a href="5853.html">Gary Miller: "RE: Uploading with current technology (Sorry Off topic)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5851">[ date ]</a>
<a href="index.html#5851">[ thread ]</a>
<a href="subject.html#5851">[ subject ]</a>
<a href="author.html#5851">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
