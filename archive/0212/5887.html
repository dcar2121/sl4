<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: FAI and SSSM</title>
<meta name="Author" content="Bill Hibbard (test@doll.ssec.wisc.edu)">
<meta name="Subject" content="Re: FAI and SSSM">
<meta name="Date" content="2002-12-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: FAI and SSSM</h1>
<!-- received="Fri Dec 13 11:57:35 2002" -->
<!-- isoreceived="20021213185735" -->
<!-- sent="Fri, 13 Dec 2002 12:57:32 -0600 (CST)" -->
<!-- isosent="20021213185732" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@doll.ssec.wisc.edu" -->
<!-- subject="Re: FAI and SSSM" -->
<!-- id="Pine.SOL.4.33.0212131255330.577-100000@doll.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3DF945DF.3030505@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@doll.ssec.wisc.edu?Subject=Re:%20FAI%20and%20SSSM"><em>test@doll.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Fri Dec 13 2002 - 11:57:32 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5888.html">Gordon Worley: "Re: SL4 Calendar of Events"</a>
<li><strong>Previous message:</strong> <a href="5886.html">Damien Broderick: "Bill's book"</a>
<li><strong>In reply to:</strong> <a href="5877.html">Eliezer S. Yudkowsky: "Re: FAI and SSSM"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5873.html">Damien Broderick: "RE: FAI and SSSM"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5887">[ date ]</a>
<a href="index.html#5887">[ thread ]</a>
<a href="subject.html#5887">[ subject ]</a>
<a href="author.html#5887">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi Eliezer,
<br>
<p><em>&gt; What about processes that construct themselves?  Does it make sense to
</em><br>
<em>&gt; describe the child of the child of the child of the child of the mind that
</em><br>
<em>&gt; humans originally built as an &quot;artifact constructed by humans&quot;?  Is it
</em><br>
<em>&gt; useful to describe it so, when it shares none of the characteristics that
</em><br>
<em>&gt; we presently attach to &quot;machines&quot;?  Call it a mind, or better yet an
</em><br>
<em>&gt; entity; we might be wrong on both counts but at least we won't be quite as
</em><br>
<em>&gt; wrong as if we use the word &quot;machine&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes, I'm arguing over the definition of a word; deliberately so because I
</em><br>
<em>&gt; think that people expect certain characteristics to hold true of
</em><br>
<em>&gt; &quot;machines&quot;, and that these characteristics don't hold true of SIs
</em><br>
<em>&gt; (superintelligences).  I would expect an originally human SI and an
</em><br>
<em>&gt; originally human-built SI to have more in common with each other than
</em><br>
<em>&gt; either would have in common with a modern-day human or a human-equivalent
</em><br>
<em>&gt; AI, and I would expect even a half-grown AI to have almost nothing in
</em><br>
<em>&gt; common with the physical objects we categorize as &quot;machines&quot;.
</em><br>
<p>I get at the same issue in my book by referring to them as
<br>
both machines and gods.
<br>
<p><em>&gt; Perhaps the complex behavior of planning is emergent in the simple
</em><br>
<em>&gt; behavior of reinforcement, as well as the simple behavior of reinforcement
</em><br>
<em>&gt; being a special case of the complex behavior of planning.  I don't think
</em><br>
<em>&gt; so, but then I haven't tried to figure out how to do it, so I wouldn't
</em><br>
<em>&gt; know whether it's possible.
</em><br>
<p>Based partly on Biology of Mind by Deric Bownds, I would say
<br>
that brains evolved in these steps:
<br>
<p>1. sensors -&gt; nerve cells -&gt; actions
<br>
<p>2. learning responses to actions, including the beginnings of
<br>
values such as &quot;eating is good&quot;
<br>
<p>3. simulation (i.e., brains processing experiences that are
<br>
not actually occuring) in order to begin solving the temporal
<br>
credit assignment problem
<br>
<p>4. increasingly sophisticated simulation (planning, simulating
<br>
brains of other animals) and values (social values for teamwork)
<br>
<p>To see how planning fits into learning, consider that when
<br>
humans confront novel situation they consciously plan their
<br>
behavior, based on simulated scenarios. As they repeat the
<br>
situation and it is less novel, those planned behaviors
<br>
become unconscious. Furthermore, those unconscious behaviors
<br>
become part of the repitoire for future planning.
<br>
<p>This relation between planning and learning is illustrated
<br>
by the development of a beginning chess player into a chess
<br>
master. A beginner's plans may include as many alternatives
<br>
as a master's, but the master's plans are in terms of primitive
<br>
units learned through lots of previous plans and reinforcement.
<br>
<p>Planning and reasonning must be grounded in learning, in much
<br>
the way that symbols must be grounded in sensory experience.
<br>
Furthermore, I would say that goals are grounded in values
<br>
(although I admit this last statement depends on how these
<br>
terms are defined).
<br>
<p><em>&gt; But human evolution includes specific selection pressures on goals, apart
</em><br>
<em>&gt; from selection pressures on reinforcement.  Imperfectly deceptive social
</em><br>
<em>&gt; organisms that argue linguistically about each other's motives in adaptive
</em><br>
<em>&gt; political contexts develop altruistic motives and rationalizations from
</em><br>
<em>&gt; altruistic motives to selfish actions; if supra-ancestral increase in
</em><br>
<em>&gt; intelligence or knowledge overcomes the force of rationalization, you are
</em><br>
<em>&gt; then left with a genuine altruist.  How would a robust implementation of
</em><br>
<em>&gt; reinforcement learning duplicate the moral and metamoral adaptations which
</em><br>
<em>&gt; are the result of highly specific selection pressures in an ancestral
</em><br>
<em>&gt; environment not shared by AIs?  You can transfer moral complexity directly
</em><br>
<em>&gt; rather than trying to reduplicate its evolutionary causation in humans,
</em><br>
<em>&gt; but you do have to transfer that complexity - it is not emergent just from
</em><br>
<em>&gt; reinforcement.
</em><br>
<p>Human moral and ethical systems are complex indeed, but are
<br>
all ultimately grounded in human learning values (emotions),
<br>
modifed by social interaction (itself driven by human values
<br>
favoring social interaction).
<br>
<p>Machines will evolve ethical and moral systems based on their
<br>
own learning values, and especially their social interaction
<br>
with humans if their values are for human happiness.
<br>
<p>By the way, Pinker does a great job of analyzing human values
<br>
in How the Mind Works.
<br>
<p><em>&gt; I confess that I don't see how this changes anything at all.  I assumed a
</em><br>
<em>&gt; simulation model that is not only used for temporal credit assignment, but
</em><br>
<em>&gt; which allows for imagination of novel behaviors whose desirability is
</em><br>
<em>&gt; determined by the match of their extrapolated effects against previously
</em><br>
<em>&gt; reinforced goal patterns.  Without this ability, no reinforcement-based
</em><br>
<em>&gt; system would ever be capable of carrying out complex creative actions such
</em><br>
<em>&gt; as computer programming - when I write a program, I am reasoning from
</em><br>
<em>&gt; abstract, high-level design goals to novel concrete code, not just
</em><br>
<em>&gt; implementing coding behaviors that have been previously reinforced.
</em><br>
<p>A robust solution of the temporal credit assignment problem, one
<br>
that finds behaviors to optimize values, includes imagining possible
<br>
futures.
<br>
<p><em>&gt; When I say &quot;simple reinforcement system&quot;, I mean &quot;a lot simpler than a
</em><br>
<em>&gt; human or a Friendly AI&quot;; &quot;simple&quot; does include full modeling/simulation
</em><br>
<em>&gt; capabilities, for both credit assignment and imagination of novel
</em><br>
<em>&gt; behaviors.  Maybe calling it a &quot;flat&quot; reinforcement system would be
</em><br>
<em>&gt; better.  The problem with a flat reinforcement system is that it
</em><br>
<em>&gt; flash-freezes itself the moment it becomes capable of self-modification.
</em><br>
<p>The system will predict the ways contemplated changes affect its
<br>
values, and refrain from changes that negatively affect them. In
<br>
this way the system and its evolution (self-modification) are
<br>
locked into serving human happiness.
<br>
<p>It is interesting that Ben has raised exactly the opposite
<br>
objection: that values might drift as the system evolves.
<br>
<p>I think &quot;happiness of all humans&quot; are the right values, since
<br>
they cause the machines to inherit human values. It does let
<br>
values drift but under human control. Of course, there is no
<br>
way to prove that the value for the happiness of all humans
<br>
won't drift, but on the other hand ask any sane mother if she
<br>
would modify her brain so she no longer loved her children.
<br>
<p><em>&gt; Originally, you built the system such that it contained certain internal
</em><br>
<em>&gt; functional modules which modified goal patterns conditional on external
</em><br>
<em>&gt; sensory events.  And from its goals at any given point, the system judges
</em><br>
<em>&gt; the desirability of future states of the universe, and hence the
</em><br>
<em>&gt; desirability of actions leading to those future states.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Now imagine this system looking at the fact that it possesses
</em><br>
<em>&gt; reinforcement modules, and considering the desirability of actions which
</em><br>
<em>&gt; remove those modules.
</em><br>
<p>Reinforcement learning is fundamental to the way brains work,
<br>
rather than being an optional module. As long as brains cannot
<br>
perfectly predict the universe, they will need reinforcement
<br>
learning.
<br>
<p><em>&gt; Any internal system, whose effect is to change the
</em><br>
<em>&gt; cognitive pattern against which imagined future events are matched to
</em><br>
<em>&gt; determine their desirability, is automatically undesirable; if the AI's
</em><br>
<em>&gt; future pattern changes, then the AI will take actions which result in an
</em><br>
<em>&gt; inferior match of those futures against the current pattern governing
</em><br>
<em>&gt; actions.  To protect the goals currently governing actions (including
</em><br>
<em>&gt; self-modifying actions), the system will remove any internal functionality
</em><br>
<em>&gt; whose effect is to modify the top layer of its goal system.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This action feels intuitively wrong to a human because humans have extra
</em><br>
<em>&gt; complexity in the goal system, which for purposes of Friendly AI we can
</em><br>
<em>&gt; think of as humans treating moral arguments as having the semantics of
</em><br>
<em>&gt; probabilistic statements about external referents.  See the appropriate
</em><br>
<em>&gt; sections of &quot;Creating Friendly AI&quot; for more information.
</em><br>
<em>&gt;
</em><br>
<em>&gt; How do you think temporal credit assignment would change this?  It doesn't
</em><br>
<em>&gt; seem relevant.
</em><br>
<p>By making machine values depend on human happiness, evolution of
<br>
goals remains under human control. The affect of a solution to
<br>
the temporal credit assignment problem is merely to enable the
<br>
system to make predictions about the affect of its contemplated
<br>
behaviors on its values.
<br>
<p><em>&gt; &gt;&gt;Finally, you're asking for too little - your proposal seems like a defense
</em><br>
<em>&gt; &gt;&gt;against fears of AI, rather than asking how far we can take supermorality
</em><br>
<em>&gt; &gt;&gt;once minds are freed from the constraints of evolutionary design.  This
</em><br>
<em>&gt; &gt;&gt;isn't a challenge that can be solved through a defensive posture - you
</em><br>
<em>&gt; &gt;&gt;have to step forward as far as you can.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Not at all. Reinforcement is a two-way street, including both
</em><br>
<em>&gt; &gt; negative (what you call defensive)
</em><br>
<em>&gt;
</em><br>
<em>&gt; No, that wasn't what I meant by &quot;defensive&quot; at all.  I was referring to
</em><br>
<em>&gt; human attitudes about futurism.
</em><br>
<em>&gt;
</em><br>
<em>&gt;  &gt; and positive reinforcement.
</em><br>
<em>&gt; &gt; My book includes a vivid description of the sort of heaven on
</em><br>
<em>&gt; &gt; earth that super-intelligent machines will create for humans,
</em><br>
<em>&gt;
</em><br>
<em>&gt; As I believe your book observes, such vivid descriptions are pointless
</em><br>
<em>&gt; because we aren't smart enough to get the description right.  For example,
</em><br>
<em>&gt; your book refers to automated farms and factories rather than
</em><br>
<em>&gt; nanotechnology and uploading.  This, of course, does not mean that
</em><br>
<em>&gt; nanotechnology is the correct description; only that we can already be
</em><br>
<em>&gt; pretty sure that farming and factories are destined for the junk-heap of
</em><br>
<em>&gt; history.
</em><br>
<p>Good point. Farms and factories as we know them will disappear.
<br>
So I'm using &quot;farms and factories&quot; as generic terms for facilities
<br>
for producing food and artifacts.
<br>
<p><em>&gt; Recommended book:  &quot;Permutation City&quot;, Greg Egan.
</em><br>
<em>&gt; Recommending online reading:
</em><br>
<em>&gt;   <a href="http://intelligence.org/intro/nanotech.html">http://intelligence.org/intro/nanotech.html</a>
</em><br>
<em>&gt;   <a href="http://intelligence.org/intro/upload.html">http://intelligence.org/intro/upload.html</a>
</em><br>
<em>&gt;   <a href="http://intelligence.org/what-singularity.html">http://intelligence.org/what-singularity.html</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; assuming that they learn behaviors based on values of human
</em><br>
<em>&gt; &gt; happiness, and assuming that they solve the temporal credit
</em><br>
<em>&gt; &gt; assignment problem so they can reason about long term happiness.
</em><br>
<em>&gt;
</em><br>
<em>&gt; What if someone has goals beyond happiness?  Many philosophies involve
</em><br>
<em>&gt; greater complexity than that.
</em><br>
<p>Acheiving goals makes people happy.
<br>
<p><em>&gt; My current best understanding of morality is that &quot;good&quot; consists of
</em><br>
<em>&gt; people getting what they want, defined however they choose to define it.
</em><br>
<em>&gt; But I'm not infallible, so that understanding is itself subject to change.
</em><br>
<em>&gt;   What happens if it turns out that &quot;happiness&quot; isn't what you really
</em><br>
<em>&gt; wanted?
</em><br>
<p>Getting what I want makes me happy.
<br>
<p><em>&gt; How does your design recover from philosophical errors by the
</em><br>
<em>&gt; programmers?
</em><br>
<p>This is a major issue for any design. I address it in two sections
<br>
of my book: Failures (in the chapter The Ultimate Engineering
<br>
Challenge) and Mental Illness (in the chapter Good God, Bad God).
<br>
<p>Assuming that through the public policy debate we can avoid
<br>
super-intelligent killing machines, and super-intelligent
<br>
Enron Corporations, then we still face the problem of a
<br>
machine that values the happiness of all humans malfunctioning.
<br>
The real danger comes when the machine advances to the point
<br>
where it is capable of intimately knowing billions of humans,
<br>
so that it can dominate any public policy debate. We need to
<br>
approach that point slowly and carefully.
<br>
<p><em>&gt; I also didn't realize that your book to which you referred was available
</em><br>
<em>&gt; online.  I've now read it.  Don't suppose you could return the favor and
</em><br>
<em>&gt; check out &quot;Creating Friendly AI&quot;, if you haven't done so already?
</em><br>
<em>&gt;
</em><br>
<em>&gt; <a href="http://www.ssec.wisc.edu/~billh/gotterdammerung.html">http://www.ssec.wisc.edu/~billh/gotterdammerung.html</a>
</em><br>
<p>Its an early draft, with a different title. The final print
<br>
edition includes a reference to your work and lots of other
<br>
information not in the on-line draft.
<br>
<p><em>&gt; <a href="http://intelligence.org/CFAI/">http://intelligence.org/CFAI/</a>
</em><br>
<p>By the way, I recognize that these future issues are speculative
<br>
and reasonable people can disagree. Your mailing list, and Ben's
<br>
AGI mailing list, are very valuable resources.
<br>
<p>Cheers,
<br>
Bill
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5888.html">Gordon Worley: "Re: SL4 Calendar of Events"</a>
<li><strong>Previous message:</strong> <a href="5886.html">Damien Broderick: "Bill's book"</a>
<li><strong>In reply to:</strong> <a href="5877.html">Eliezer S. Yudkowsky: "Re: FAI and SSSM"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5873.html">Damien Broderick: "RE: FAI and SSSM"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5887">[ date ]</a>
<a href="index.html#5887">[ thread ]</a>
<a href="subject.html#5887">[ subject ]</a>
<a href="author.html#5887">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
