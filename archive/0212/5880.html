<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: FAI and SSSM</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: FAI and SSSM">
<meta name="Date" content="2002-12-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: FAI and SSSM</h1>
<!-- received="Fri Dec 13 04:34:34 2002" -->
<!-- isoreceived="20021213113434" -->
<!-- sent="Fri, 13 Dec 2002 06:36:39 -0500" -->
<!-- isosent="20021213113639" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: FAI and SSSM" -->
<!-- id="LAEGJLOGJIOELPNIOOAJIEHEDPAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3DF945DF.3030505@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20FAI%20and%20SSSM"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Fri Dec 13 2002 - 04:36:39 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5881.html">Eliezer S. Yudkowsky: "Re: FAI and SSSM"</a>
<li><strong>Previous message:</strong> <a href="5879.html">Cliff Stabbert: "Re: SL4 Calendar of Events"</a>
<li><strong>In reply to:</strong> <a href="5877.html">Eliezer S. Yudkowsky: "Re: FAI and SSSM"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5881.html">Eliezer S. Yudkowsky: "Re: FAI and SSSM"</a>
<li><strong>Reply:</strong> <a href="5881.html">Eliezer S. Yudkowsky: "Re: FAI and SSSM"</a>
<li><strong>Reply:</strong> <a href="5884.html">Bill Hibbard: "RE: FAI and SSSM"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5880">[ date ]</a>
<a href="index.html#5880">[ thread ]</a>
<a href="subject.html#5880">[ subject ]</a>
<a href="author.html#5880">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer wrote (responding to Bill Hibbard):
<br>
<em>&gt; &gt; A robust implementation of reinforcement learning must solve
</em><br>
<em>&gt; &gt; the temporal credit assignment problem, which requires a
</em><br>
<em>&gt; &gt; simulation model of the world. This simulation model is the
</em><br>
<em>&gt; &gt; basis of reasoning based on goals. Planning and goal-based
</em><br>
<em>&gt; &gt; reasoning are emergent behaviors of a robust implementation
</em><br>
<em>&gt; &gt; of reinforcement learning.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Perhaps the complex behavior of planning is emergent in the simple
</em><br>
<em>&gt; behavior of reinforcement, as well as the simple behavior of
</em><br>
<em>&gt; reinforcement
</em><br>
<em>&gt; being a special case of the complex behavior of planning.  I don't think
</em><br>
<em>&gt; so, but then I haven't tried to figure out how to do it, so I wouldn't
</em><br>
<em>&gt; know whether it's possible.
</em><br>
<p>Here is my view on the relationship btw planning &amp; reinforcement learning...
<br>
<p>Firstly, I think that reinforcement learning, if given enough time &amp; space
<br>
to work in, would eventually emergently give rise to planning.  (This could
<br>
probably be shown mathematically, actually, though estimates on the time &amp;
<br>
space required would be hideously exponential.  This relates to Hutter and
<br>
Schmidhuber's work on algorithm information theory based AGI)
<br>
<p>Secondly, I think that in order to make it work in a reasonable
<br>
resource-usage framework, reinforcement learning has got to be modified so
<br>
that it basically emulates some sort of probabilistic planning framework.
<br>
I.e., the solution to the temporal credit assignment problem requires not
<br>
just a simulation of the world but inference &amp; planning based on this
<br>
simulation.
<br>
<p>Finally, I think that any planning process that is going to be effective is
<br>
going to have to be responsive to feedback in roughly the manner of
<br>
reinforcement learning...
<br>
<p>Ultimately, just like &quot;symbolic&quot; and &quot;subsymbolic&quot;, &quot;reinforcement learning&quot;
<br>
and &quot;planning&quot; are crudely drawn categories of an early stage of
<br>
mind-theory, which are not nearly as distinct as they seem to most theorists
<br>
right now...
<br>
<p><em>&gt; How would a robust implementation of
</em><br>
<em>&gt; reinforcement learning duplicate the moral and metamoral
</em><br>
<em>&gt; adaptations which
</em><br>
<em>&gt; are the result of highly specific selection pressures in an ancestral
</em><br>
<em>&gt; environment not shared by AIs?
</em><br>
<p>I don't think it is necessary or even desirable for AI morality to duplicate
<br>
human morality.  (Or, to use your phrasing, human &quot;moral and metamoral
<br>
adaptations&quot;)
<br>
<p>For one thing, I do not have that much faith in the stability of any human's
<br>
moral system, not even that of a so-called &quot;true altruist.&quot;  I would rather
<br>
have the moral system of an AI be significantly more reliable!!!
<br>
<p><em>&gt; You can transfer moral complexity
</em><br>
<em>&gt; directly
</em><br>
<em>&gt; rather than trying to reduplicate its evolutionary causation in humans,
</em><br>
<p>Hmmm.  Well, clearly this transfer can be done via uploading human minds...
<br>
<p>Whether humans' particular brand of moral complexity can be transferred to a
<br>
digital mind that is very nonhuman in character, is a different issue.  It's
<br>
not so clear to me that this can be done effectively.
<br>
<p><em>&gt; but you do have to transfer that complexity - it is not emergent
</em><br>
<em>&gt; just from
</em><br>
<em>&gt; reinforcement.
</em><br>
<p>Really, I think that any state one desires to see in an AI system, can in
<br>
principle be achieved thru reinforcement learning.  That is, reinforcement
<br>
learning is a completely able optimization algorithm, given enough time and
<br>
space.  The problem is, it's terribly terribly slow.
<br>
<p>Bill says this will be solved by solving the temporal credit assignment
<br>
problem, and I say the solution to that may involve *explicit* use of
<br>
planning and inference, as well as the emergence of *further* aspects of
<br>
planning and inference.
<br>
<p><p><em>&gt; I confess that I don't see how this changes anything at all.  I assumed a
</em><br>
<em>&gt; simulation model that is not only used for temporal credit
</em><br>
<em>&gt; assignment, but
</em><br>
<em>&gt; which allows for imagination of novel behaviors whose desirability is
</em><br>
<em>&gt; determined by the match of their extrapolated effects against previously
</em><br>
<em>&gt; reinforced goal patterns.
</em><br>
<p>Right.  And I believe this is actually NEEDED for temporal credit
<br>
assignment!
<br>
<p>The temporal credit assignment problem is so hard that it can't be solved
<br>
within plausible time-constraints, without imagination, comparison of
<br>
imagination to memory, the whole beast called mind...
<br>
<p>Bill, I think that computationally-plausible temporal credit assignment is a
<br>
bigger can of worms than you're realizing ;-)
<br>
<p>There isn't going to be a clever algorithm for solving it.  The solution is
<br>
a well-tuned, integrative mind with many subtle aspects that don't look
<br>
explicitly like reinforcement learning...
<br>
<p>-- Ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5881.html">Eliezer S. Yudkowsky: "Re: FAI and SSSM"</a>
<li><strong>Previous message:</strong> <a href="5879.html">Cliff Stabbert: "Re: SL4 Calendar of Events"</a>
<li><strong>In reply to:</strong> <a href="5877.html">Eliezer S. Yudkowsky: "Re: FAI and SSSM"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5881.html">Eliezer S. Yudkowsky: "Re: FAI and SSSM"</a>
<li><strong>Reply:</strong> <a href="5881.html">Eliezer S. Yudkowsky: "Re: FAI and SSSM"</a>
<li><strong>Reply:</strong> <a href="5884.html">Bill Hibbard: "RE: FAI and SSSM"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5880">[ date ]</a>
<a href="index.html#5880">[ thread ]</a>
<a href="subject.html#5880">[ subject ]</a>
<a href="author.html#5880">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
