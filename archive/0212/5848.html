<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Uploading with current technology</title>
<meta name="Author" content="Bill Hibbard (test@doll.ssec.wisc.edu)">
<meta name="Subject" content="Re: Uploading with current technology">
<meta name="Date" content="2002-12-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Uploading with current technology</h1>
<!-- received="Mon Dec  9 08:00:13 2002" -->
<!-- isoreceived="20021209150013" -->
<!-- sent="Mon, 9 Dec 2002 09:00:08 -0600 (CST)" -->
<!-- isosent="20021209150008" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@doll.ssec.wisc.edu" -->
<!-- subject="Re: Uploading with current technology" -->
<!-- id="Pine.SOL.4.33.0212090856050.27372-100000@doll.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="630DA892-0B05-11D7-9A6B-000A27B4DEFC@rbisland.cx" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@doll.ssec.wisc.edu?Subject=Re:%20Uploading%20with%20current%20technology"><em>test@doll.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Mon Dec 09 2002 - 08:00:08 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5849.html">Gary Miller: "RE: Uploading with current technology"</a>
<li><strong>Previous message:</strong> <a href="5847.html">Eliezer S. Yudkowsky: "Re: Uploading with current technology"</a>
<li><strong>In reply to:</strong> <a href="5842.html">Gordon Worley: "Re: Uploading with current technology"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5849.html">Gary Miller: "RE: Uploading with current technology"</a>
<li><strong>Reply:</strong> <a href="5849.html">Gary Miller: "RE: Uploading with current technology"</a>
<li><strong>Reply:</strong> <a href="5850.html">polysync@pobox.com: "Re: Uploading with current technology"</a>
<li><strong>Reply:</strong> <a href="5859.html">John G. Miller: "Re: Uploading with current technology"</a>
<li><strong>Reply:</strong> <a href="5868.html">Eliezer S. Yudkowsky: "FAI and SSSM"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5848">[ date ]</a>
<a href="index.html#5848">[ thread ]</a>
<a href="subject.html#5848">[ subject ]</a>
<a href="author.html#5848">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi Gordon,
<br>
<p>On Sun, 8 Dec 2002, Gordon Worley wrote:
<br>
<p><em>&gt; On Sunday, December 8, 2002, at 01:08  PM, Ben Goertzel wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; <a href="http://users.rcn.com/standley/AI/immortality.htm">http://users.rcn.com/standley/AI/immortality.htm</a>
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Thoughts?
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Can anyone with more neuro expertise tell me: Is this guy correct as
</em><br>
<em>&gt; &gt; regards
</em><br>
<em>&gt; &gt; what is currently technologically plausible?
</em><br>
<em>&gt;
</em><br>
<em>&gt; The Singularity and, specifically, FAI is a faster, safer way of
</em><br>
<em>&gt; transcending.  Super *human* intelligence is highly dangerous.  Think
</em><br>
<em>&gt; male chimp with nuclear feces.  Unless you've got someone way protect
</em><br>
<em>&gt; the universe from the super *humans*, we're probably better off with
</em><br>
<em>&gt; our current brains.
</em><br>
<p>I largely agree. But as I point out in my book:
<br>
<p>&nbsp;&nbsp;<a href="http://www.ssec.wisc.edu/~billh/super.html">http://www.ssec.wisc.edu/~billh/super.html</a>
<br>
<p>after humans meet super-intelligent machines they will want
<br>
to become super-intelligent themselves, and will want the
<br>
indfinite life span of a repairable machine brain supporting
<br>
their mind.
<br>
<p>With super-intelligent machines, the key to human safety is
<br>
in controlling the values that reinforce learning of
<br>
intelligent behaviors. In machines, we can design them so
<br>
their behaviors are positively reinforced by human happiness
<br>
and negatively reinforced by human unhappiness.
<br>
<p>Behaviors are reinforced by much different values in human
<br>
brains. Human values are mostly self-interest. As social
<br>
animals humans have some more altruistic values, but these
<br>
mostly depend on social pressure. Very powerful humans can
<br>
transcend social pressure and revert to their selfish values,
<br>
hence the maxim that power corrupts and absolute power
<br>
corrupts absolutely. Nothing will give a human more power
<br>
than super-intelligence.
<br>
<p>Society has a gradual (lots of short-term setbacks, to be
<br>
sure) long-term trend toward equality because human brains
<br>
are distributed quite democratically: the largest IQ (not
<br>
a perfect measure, but widely applied) in history is only
<br>
twice the average. However, the largest computers, buildings,
<br>
trucks, etc are thousands of times their averages. The
<br>
migration of human minds into machine brains theatens to
<br>
end the even distribution of human intelligence, and hence
<br>
end the gradual long-term trend toward social equality.
<br>
<p>Given that the combination of super-intelligence and human
<br>
values is dangerous, the solution is to make alteration of
<br>
reinforcement learning values a necessary condition for
<br>
granting a human super-intelligence. That is, when we have
<br>
the technology to manipulate human intelligence then we
<br>
also need to develop the technology to manipulate human
<br>
reinforcement learning values. Because this change in values
<br>
would affect learning, it would not immediately change the
<br>
human's old behaviors. Hence they would still &quot;be themselves&quot;.
<br>
But as they learned super-intelligent behaviors, their new
<br>
values would cause those newly learned behaviors to serve
<br>
the happiness of all humans. Furthermore, behaviors learned
<br>
via their old greedy or xenophobic values would be negatively
<br>
reinforced and disappear.
<br>
<p>One danger is the temptation to use genetic manipulation as a
<br>
shortcut to super-intelligent humans. This may provide a way
<br>
to increase human intelligence before we understand how it
<br>
works and before we know how to change human reinforcement
<br>
learning values. This danger is neatly parallel with Mary
<br>
Shelley's Frankestein, in which a human monster is created by
<br>
a scientist tinkering with technology thet he did not really
<br>
understand. We need to understand how human brains work and
<br>
solve the AGI problem before we start manipulating human brains.
<br>
<p>Cheers,
<br>
Bill
<br>
----------------------------------------------------------
<br>
Bill Hibbard, SSEC, 1225 W. Dayton St., Madison, WI  53706
<br>
<a href="mailto:test@doll.ssec.wisc.edu?Subject=Re:%20Uploading%20with%20current%20technology">test@doll.ssec.wisc.edu</a>  608-263-4427  fax: 608-263-6738
<br>
<a href="http://www.ssec.wisc.edu/~billh/vis.html">http://www.ssec.wisc.edu/~billh/vis.html</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5849.html">Gary Miller: "RE: Uploading with current technology"</a>
<li><strong>Previous message:</strong> <a href="5847.html">Eliezer S. Yudkowsky: "Re: Uploading with current technology"</a>
<li><strong>In reply to:</strong> <a href="5842.html">Gordon Worley: "Re: Uploading with current technology"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5849.html">Gary Miller: "RE: Uploading with current technology"</a>
<li><strong>Reply:</strong> <a href="5849.html">Gary Miller: "RE: Uploading with current technology"</a>
<li><strong>Reply:</strong> <a href="5850.html">polysync@pobox.com: "Re: Uploading with current technology"</a>
<li><strong>Reply:</strong> <a href="5859.html">John G. Miller: "Re: Uploading with current technology"</a>
<li><strong>Reply:</strong> <a href="5868.html">Eliezer S. Yudkowsky: "FAI and SSSM"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5848">[ date ]</a>
<a href="index.html#5848">[ thread ]</a>
<a href="subject.html#5848">[ subject ]</a>
<a href="author.html#5848">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
