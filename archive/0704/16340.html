<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Existential Risk and Fermi's Paradox</title>
<meta name="Author" content="freaken@freaken.dk (freaken@freaken.dk)">
<meta name="Subject" content="Re: Existential Risk and Fermi's Paradox">
<meta name="Date" content="2007-04-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Existential Risk and Fermi's Paradox</h1>
<!-- received="Sun Apr 22 10:37:27 2007" -->
<!-- isoreceived="20070422163727" -->
<!-- sent="Sun, 22 Apr 2007 18:34:17 +0200 (CEST)" -->
<!-- isosent="20070422163417" -->
<!-- name="freaken@freaken.dk" -->
<!-- email="freaken@freaken.dk" -->
<!-- subject="Re: Existential Risk and Fermi's Paradox" -->
<!-- id="29897.85.83.10.102.1177259657.squirrel@mail01.mxhotel.dk" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="c749694b0704211431k7a86be9n7d4fba48563e7569@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> <a href="mailto:freaken@freaken.dk?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox"><em>freaken@freaken.dk</em></a><br>
<strong>Date:</strong> Sun Apr 22 2007 - 10:34:17 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16341.html">Mary Tobias: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>Previous message:</strong> <a href="16339.html">Dagon Gmail: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>In reply to:</strong> <a href="16337.html">Timothy Jennings: "Re: Existential Risk and Fermi's Paradox"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16342.html">Daniel Riaño : "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>Reply:</strong> <a href="16342.html">Daniel Riaño : "Re: Existential Risk and Fermi's Paradox"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16340">[ date ]</a>
<a href="index.html#16340">[ thread ]</a>
<a href="subject.html#16340">[ subject ]</a>
<a href="author.html#16340">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Couldn't it just be, that the aliens in question doesn't want us to see
<br>
them? A la the movie Contact?
<br>
<p><em>&gt; The obvious solution is &quot;we are first&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;We&quot; &quot;[being] first&quot; is only unlikely in the sense of &quot;what is the chance
</em><br>
<em>&gt; of
</em><br>
<em>&gt; that golf ball landing exactly there&quot; said by a teenage caddy pointing at
</em><br>
<em>&gt; a
</em><br>
<em>&gt; random golf ball happening to lie in a certain place when he happens to
</em><br>
<em>&gt; say
</em><br>
<em>&gt; that.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; On 21/04/07, <a href="mailto:apeters2@nd.edu?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">apeters2@nd.edu</a> &lt;<a href="mailto:apeters2@nd.edu?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">apeters2@nd.edu</a>&gt; wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; I'm not sure if &quot;machine rebellion&quot; is a workable concept here.  If we
</em><br>
<em>&gt;&gt; are
</em><br>
<em>&gt;&gt; talking about a civilization able to build whole subrealities at a whim,
</em><br>
<em>&gt;&gt; we are
</em><br>
<em>&gt;&gt; already talking non-biological, uplifted sentience.  Why would they make
</em><br>
<em>&gt;&gt; these
</em><br>
<em>&gt;&gt; (I assume lesser) guardian entities with the capacity to rebel, or even
</em><br>
<em>&gt;&gt; to
</em><br>
<em>&gt;&gt; want
</em><br>
<em>&gt;&gt; to rebel?  Leave them with limited intelligence, perhaps a basic
</em><br>
<em>&gt;&gt; compulsion-program to ensure that they concentrate solely on defense and
</em><br>
<em>&gt;&gt; resource harvesting.
</em><br>
<em>&gt;&gt; Your other point - &quot;bumping up against&quot; other civilizations - seems like
</em><br>
<em>&gt;&gt; a
</em><br>
<em>&gt;&gt; more
</em><br>
<em>&gt;&gt; likely source of problems.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Quoting Dagon Gmail &lt;<a href="mailto:dagonweb@gmail.com?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">dagonweb@gmail.com</a>&gt;:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; &gt; The implication would be, the galactic disk would be seeded with a
</em><br>
<em>&gt;&gt; steadily
</em><br>
<em>&gt;&gt; &gt; growing number of &quot;bombs&quot;,
</em><br>
<em>&gt;&gt; &gt; i.e. extremely defensive automated civilizations solely dedicated to
</em><br>
<em>&gt;&gt; keeping
</em><br>
<em>&gt;&gt; &gt; intact the minds of its original
</em><br>
<em>&gt;&gt; &gt; creators. Just one of these needs to experience a machine rebellion
</em><br>
<em>&gt;&gt; and
</em><br>
<em>&gt;&gt; the
</em><br>
<em>&gt;&gt; &gt; precarious balance is lost. A
</em><br>
<em>&gt;&gt; &gt; machine rebellion may very well not have the sentimental attachment to
</em><br>
<em>&gt;&gt; the
</em><br>
<em>&gt;&gt; &gt; native dream-scape. Machine
</em><br>
<em>&gt;&gt; &gt; civilizations could very well be staunchly objectivist, dedicated to
</em><br>
<em>&gt;&gt; what it
</em><br>
<em>&gt;&gt; &gt; regards as materialist expansion. Any
</em><br>
<em>&gt;&gt; &gt; such rebellion would run into the (alleged) multitudes of &quot;dreaming&quot;
</em><br>
<em>&gt;&gt; or
</em><br>
<em>&gt;&gt; &gt; &quot;virtuamorph&quot; civilizations around.
</em><br>
<em>&gt;&gt; &gt;
</em><br>
<em>&gt;&gt; &gt; And we are talking big timeframes here. If the statistical analysis
</em><br>
<em>&gt;&gt; has
</em><br>
<em>&gt;&gt; any
</em><br>
<em>&gt;&gt; &gt; meaning, virtuamorph civilizations
</em><br>
<em>&gt;&gt; &gt; shouldn't be a de facto dying process; for a dreaming civilization to
</em><br>
<em>&gt;&gt; have
</em><br>
<em>&gt;&gt; &gt; any other meaning than a slow
</em><br>
<em>&gt;&gt; &gt; abortion they have to last millions of years; millions of years means
</em><br>
<em>&gt;&gt; a
</em><br>
<em>&gt;&gt; lot
</em><br>
<em>&gt;&gt; &gt; of galactic shuffling in terms of
</em><br>
<em>&gt;&gt; &gt; stellar trejacteories. There would be many occasions of stars with
</em><br>
<em>&gt;&gt; &gt; &quot;dreamers&quot; drifting into proximity, giving rise to
</em><br>
<em>&gt;&gt; &gt; paranoid, highly protectionist impulses. After all, if all that
</em><br>
<em>&gt;&gt; dreaming
</em><br>
<em>&gt;&gt; is
</em><br>
<em>&gt;&gt; &gt; worth anything in subjective terms the
</em><br>
<em>&gt;&gt; &gt; civilization doing it would fight realworld battles to defend it, and
</em><br>
<em>&gt;&gt; not
</em><br>
<em>&gt;&gt; &gt; just dream about it in metaphorical terms
</em><br>
<em>&gt;&gt; &gt; of +5 vorpal swords.
</em><br>
<em>&gt;&gt; &gt;
</em><br>
<em>&gt;&gt; &gt; Unless the mindscapes have a way of closing off access to reality,
</em><br>
<em>&gt;&gt; i.e.
</em><br>
<em>&gt;&gt; they
</em><br>
<em>&gt;&gt; &gt; materially escape this universe.
</em><br>
<em>&gt;&gt; &gt; But then we introduce new unknows and arbitrary explanations.
</em><br>
<em>&gt;&gt; &gt;
</em><br>
<em>&gt;&gt; &gt; Maybe it's simply easier for civilizations to maintain their
</em><br>
<em>&gt;&gt; consciousness
</em><br>
<em>&gt;&gt; &gt; &gt; in worlds of their own creation rather than expend energy and time
</em><br>
<em>&gt;&gt; in
</em><br>
<em>&gt;&gt; this
</em><br>
<em>&gt;&gt; &gt; &gt; one which is outside of their complete control.  It would seem to me
</em><br>
<em>&gt;&gt; that
</em><br>
<em>&gt;&gt; &gt; &gt; being able to create a paradise of information and experience from
</em><br>
<em>&gt;&gt; the
</em><br>
<em>&gt;&gt; &gt; &gt; substrate of this world would be a better existence than existing in
</em><br>
<em>&gt;&gt; this
</em><br>
<em>&gt;&gt; &gt; &gt; world as is.  Once to this stage, maybe to other civilizations
</em><br>
<em>&gt;&gt; simply
</em><br>
<em>&gt;&gt; do
</em><br>
<em>&gt;&gt; &gt; not
</em><br>
<em>&gt;&gt; &gt; &gt; want to be bothered by lesser beings in this reality who might upset
</em><br>
<em>&gt;&gt; the
</em><br>
<em>&gt;&gt; &gt; &gt; balance and control they desire.  One would only need to be able to
</em><br>
<em>&gt;&gt; &gt; generate
</em><br>
<em>&gt;&gt; &gt; &gt; the prime number sequence in order to create an infinite order of
</em><br>
<em>&gt;&gt; &gt; &gt; probability densities with the next higher prime as the next
</em><br>
<em>&gt;&gt; iterative
</em><br>
<em>&gt;&gt; seed
</em><br>
<em>&gt;&gt; &gt; &gt; value.  In this way, one could mimic true randomness.  A
</em><br>
<em>&gt;&gt; civilization
</em><br>
<em>&gt;&gt; could
</em><br>
<em>&gt;&gt; &gt; &gt; at both times experience truly unique experiences yet have complete
</em><br>
<em>&gt;&gt; control
</em><br>
<em>&gt;&gt; &gt; &gt; over their reality.  The reality they experience would ultimately be
</em><br>
<em>&gt;&gt; &gt; limited
</em><br>
<em>&gt;&gt; &gt; &gt; by the available energy in this reality but hypothetically, they
</em><br>
<em>&gt;&gt; could
</em><br>
<em>&gt;&gt; &gt; &gt; manipulate time in such a way that one second here would be a
</em><br>
<em>&gt;&gt; million
</em><br>
<em>&gt;&gt; years
</em><br>
<em>&gt;&gt; &gt; &gt; in their experienced reality.  Ultimately, their fate would be
</em><br>
<em>&gt;&gt; dependent
</em><br>
<em>&gt;&gt; &gt; &gt; upon the goings on in this universe, but they could develop machines
</em><br>
<em>&gt;&gt; to
</em><br>
<em>&gt;&gt; &gt; &gt; gather energy and other resources to maintain their minds in the
</em><br>
<em>&gt;&gt; &gt; &gt; sub-realities.
</em><br>
<em>&gt;&gt; &gt; &gt;
</em><br>
<em>&gt;&gt; &gt; &gt; They would need to build machines incapable of communicating or
</em><br>
<em>&gt;&gt; avoid
</em><br>
<em>&gt;&gt; &gt; &gt; communicating with minds in this reality while they experience a
</em><br>
<em>&gt;&gt; completely
</em><br>
<em>&gt;&gt; &gt; &gt; unique reality of their own choosing through technology. The
</em><br>
<em>&gt;&gt; machines
</em><br>
<em>&gt;&gt; in
</em><br>
<em>&gt;&gt; &gt; &gt; this time and space are drones programmed to protect the mind(s)
</em><br>
<em>&gt;&gt; living
</em><br>
<em>&gt;&gt; &gt; &gt; within the created world(s).  You could go so far as to model this
</em><br>
<em>&gt;&gt; entire
</em><br>
<em>&gt;&gt; &gt; &gt; existence where each individual mind shapes vis own reality which is
</em><br>
<em>&gt;&gt; &gt; &gt; protected by drones in the higher reality with the ability to
</em><br>
<em>&gt;&gt; transfer
</em><br>
<em>&gt;&gt; &gt; one's
</em><br>
<em>&gt;&gt; &gt; &gt; mind between realities as one sees fit or keep others out as one
</em><br>
<em>&gt;&gt; sees
</em><br>
<em>&gt;&gt; fit.
</em><br>
<em>&gt;&gt; &gt; &gt; Universes could be born by the integration and random sharing of
</em><br>
<em>&gt;&gt; minds
</em><br>
<em>&gt;&gt; &gt; &gt; thereby generating more unique child realities.
</em><br>
<em>&gt;&gt; &gt; &gt;
</em><br>
<em>&gt;&gt; &gt; &gt; The ultimate liberty would be to give each person vis own ideaspace
</em><br>
<em>&gt;&gt; with
</em><br>
<em>&gt;&gt; &gt; &gt; which to construct their own reality and experience it as they see
</em><br>
<em>&gt;&gt; fit.
</em><br>
<em>&gt;&gt; &gt; &gt;
</em><br>
<em>&gt;&gt; &gt; &gt; It would be really cool to be to the level of existence as a
</em><br>
<em>&gt;&gt; universal
</em><br>
<em>&gt;&gt; &gt; &gt; mind integrating with other universal minds creating completely new
</em><br>
<em>&gt;&gt; &gt; &gt; universes.
</em><br>
<em>&gt;&gt; &gt; &gt;
</em><br>
<em>&gt;&gt; &gt; &gt; Why would you want to exchange this kind of ability for the lesser
</em><br>
<em>&gt;&gt; &gt; &gt; existence of an entropic reality?
</em><br>
<em>&gt;&gt; &gt; &gt;
</em><br>
<em>&gt;&gt; &gt; &gt; *Stathis Papaioannou &lt;<a href="mailto:stathisp@gmail.com?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">stathisp@gmail.com</a>&gt;* wrote:
</em><br>
<em>&gt;&gt; &gt; &gt;
</em><br>
<em>&gt;&gt; &gt; &gt;
</em><br>
<em>&gt;&gt; &gt; &gt;
</em><br>
<em>&gt;&gt; &gt; &gt; On 4/20/07, Gordon Worley &lt;<a href="mailto:redbird@mac.com?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">redbird@mac.com</a> &gt; wrote:
</em><br>
<em>&gt;&gt; &gt; &gt;
</em><br>
<em>&gt;&gt; &gt; &gt; The theory of Friendly AI is fully developed and leads to the
</em><br>
<em>&gt;&gt; &gt; &gt; &gt; creation of a Friendly AI path to Singularity first (after all, we
</em><br>
<em>&gt;&gt; &gt; &gt; &gt; may create something that isn't a Friendly AI but that will figure
</em><br>
<em>&gt;&gt; &gt; &gt; &gt; out how to create a Friendly AI).  However, when this path is
</em><br>
<em>&gt;&gt; &gt; &gt; &gt; enacted, what are the chances that something will cause an
</em><br>
<em>&gt;&gt; &gt; &gt; &gt; existential disaster?  Although I suspect it would be less than
</em><br>
<em>&gt;&gt; the
</em><br>
<em>&gt;&gt; &gt; &gt; &gt; chances of a non-Friendly AI path to Singularity, how much less?
</em><br>
<em>&gt;&gt; Is
</em><br>
<em>&gt;&gt; &gt; &gt; &gt; it a large enough difference to warrant the extra time, money, and
</em><br>
<em>&gt;&gt; &gt; &gt; &gt; effort required for Friendly AI?
</em><br>
<em>&gt;&gt; &gt; &gt;
</em><br>
<em>&gt;&gt; &gt; &gt;
</em><br>
<em>&gt;&gt; &gt; &gt; Non-friendly AI might be more likely a cause an existential disaster
</em><br>
<em>&gt;&gt; from
</em><br>
<em>&gt;&gt; &gt; &gt; our point of view, but from its own point of view, unencumbered by
</em><br>
<em>&gt;&gt; concerns
</em><br>
<em>&gt;&gt; &gt; &gt; for anything other than its own well-being, wouldn't it be more
</em><br>
<em>&gt;&gt; rather
</em><br>
<em>&gt;&gt; than
</em><br>
<em>&gt;&gt; &gt; &gt; less likely to survive and colonise the galaxy?
</em><br>
<em>&gt;&gt; &gt; &gt;
</em><br>
<em>&gt;&gt; &gt; &gt; Stathis Papaioannou
</em><br>
<em>&gt;&gt; &gt; &gt;
</em><br>
<em>&gt;&gt; &gt; &gt;
</em><br>
<em>&gt;&gt; &gt; &gt;
</em><br>
<em>&gt;&gt; &gt; &gt; ------------------------------
</em><br>
<em>&gt;&gt; &gt; &gt; Ahhh...imagining that irresistible &quot;new car&quot; smell?
</em><br>
<em>&gt;&gt; &gt; &gt; Check out new cars at Yahoo!
</em><br>
<em>&gt;&gt; &gt;
</em><br>
<em>&gt;&gt; Autos.&lt;
</em><br>
<em>&gt;&gt; <a href="http://us.rd.yahoo.com/evt=48245/*http://autos.yahoo.com/new_cars.html;_ylc=X3oDMTE1YW1jcXJ2BF9TAzk3MTA3MDc2BHNlYwNtYWlsdGFncwRzbGsDbmV3LWNhcnM">http://us.rd.yahoo.com/evt=48245/*http://autos.yahoo.com/new_cars.html;_ylc=X3oDMTE1YW1jcXJ2BF9TAzk3MTA3MDc2BHNlYwNtYWlsdGFncwRzbGsDbmV3LWNhcnM</a>-
</em><br>
<em>&gt;&gt; &gt;
</em><br>
<em>&gt;&gt; &gt; &gt;
</em><br>
<em>&gt;&gt; &gt; &gt;
</em><br>
<em>&gt;&gt; &gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16341.html">Mary Tobias: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>Previous message:</strong> <a href="16339.html">Dagon Gmail: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>In reply to:</strong> <a href="16337.html">Timothy Jennings: "Re: Existential Risk and Fermi's Paradox"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16342.html">Daniel Riaño : "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>Reply:</strong> <a href="16342.html">Daniel Riaño : "Re: Existential Risk and Fermi's Paradox"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16340">[ date ]</a>
<a href="index.html#16340">[ thread ]</a>
<a href="subject.html#16340">[ subject ]</a>
<a href="author.html#16340">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:57 MDT
</em></small></p>
</body>
</html>
