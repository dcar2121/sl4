<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Existential Risk and Fermi's Paradox</title>
<meta name="Author" content="Dagon Gmail (dagonweb@gmail.com)">
<meta name="Subject" content="Re: Existential Risk and Fermi's Paradox">
<meta name="Date" content="2007-04-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Existential Risk and Fermi's Paradox</h1>
<!-- received="Sat Apr 21 11:54:25 2007" -->
<!-- isoreceived="20070421175425" -->
<!-- sent="Sat, 21 Apr 2007 19:51:17 +0200" -->
<!-- isosent="20070421175117" -->
<!-- name="Dagon Gmail" -->
<!-- email="dagonweb@gmail.com" -->
<!-- subject="Re: Existential Risk and Fermi's Paradox" -->
<!-- id="bf3acbfc0704211051v186bd7c2iae86a1ed5c72b405@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="184121.93199.qm@web50807.mail.re2.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Dagon Gmail (<a href="mailto:dagonweb@gmail.com?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox"><em>dagonweb@gmail.com</em></a>)<br>
<strong>Date:</strong> Sat Apr 21 2007 - 11:51:17 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16336.html">apeters2@nd.edu: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>Previous message:</strong> <a href="16334.html">R. W.: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>In reply to:</strong> <a href="16334.html">R. W.: "Re: Existential Risk and Fermi's Paradox"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16336.html">apeters2@nd.edu: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>Reply:</strong> <a href="16336.html">apeters2@nd.edu: "Re: Existential Risk and Fermi's Paradox"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16335">[ date ]</a>
<a href="index.html#16335">[ thread ]</a>
<a href="subject.html#16335">[ subject ]</a>
<a href="author.html#16335">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
The implication would be, the galactic disk would be seeded with a steadily
<br>
growing number of &quot;bombs&quot;,
<br>
i.e. extremely defensive automated civilizations solely dedicated to keeping
<br>
intact the minds of its original
<br>
creators. Just one of these needs to experience a machine rebellion and the
<br>
precarious balance is lost. A
<br>
machine rebellion may very well not have the sentimental attachment to the
<br>
native dream-scape. Machine
<br>
civilizations could very well be staunchly objectivist, dedicated to what it
<br>
regards as materialist expansion. Any
<br>
such rebellion would run into the (alleged) multitudes of &quot;dreaming&quot; or
<br>
&quot;virtuamorph&quot; civilizations around.
<br>
<p>And we are talking big timeframes here. If the statistical analysis has any
<br>
meaning, virtuamorph civilizations
<br>
shouldn't be a de facto dying process; for a dreaming civilization to have
<br>
any other meaning than a slow
<br>
abortion they have to last millions of years; millions of years means a lot
<br>
of galactic shuffling in terms of
<br>
stellar trejacteories. There would be many occasions of stars with
<br>
&quot;dreamers&quot; drifting into proximity, giving rise to
<br>
paranoid, highly protectionist impulses. After all, if all that dreaming is
<br>
worth anything in subjective terms the
<br>
civilization doing it would fight realworld battles to defend it, and not
<br>
just dream about it in metaphorical terms
<br>
of +5 vorpal swords.
<br>
<p>Unless the mindscapes have a way of closing off access to reality, i.e. they
<br>
materially escape this universe.
<br>
But then we introduce new unknows and arbitrary explanations.
<br>
<p>Maybe it's simply easier for civilizations to maintain their consciousness
<br>
<em>&gt; in worlds of their own creation rather than expend energy and time in this
</em><br>
<em>&gt; one which is outside of their complete control.  It would seem to me that
</em><br>
<em>&gt; being able to create a paradise of information and experience from the
</em><br>
<em>&gt; substrate of this world would be a better existence than existing in this
</em><br>
<em>&gt; world as is.  Once to this stage, maybe to other civilizations simply do not
</em><br>
<em>&gt; want to be bothered by lesser beings in this reality who might upset the
</em><br>
<em>&gt; balance and control they desire.  One would only need to be able to generate
</em><br>
<em>&gt; the prime number sequence in order to create an infinite order of
</em><br>
<em>&gt; probability densities with the next higher prime as the next iterative seed
</em><br>
<em>&gt; value.  In this way, one could mimic true randomness.  A civilization could
</em><br>
<em>&gt; at both times experience truly unique experiences yet have complete control
</em><br>
<em>&gt; over their reality.  The reality they experience would ultimately be limited
</em><br>
<em>&gt; by the available energy in this reality but hypothetically, they could
</em><br>
<em>&gt; manipulate time in such a way that one second here would be a million years
</em><br>
<em>&gt; in their experienced reality.  Ultimately, their fate would be dependent
</em><br>
<em>&gt; upon the goings on in this universe, but they could develop machines to
</em><br>
<em>&gt; gather energy and other resources to maintain their minds in the
</em><br>
<em>&gt; sub-realities.
</em><br>
<em>&gt;
</em><br>
<em>&gt; They would need to build machines incapable of communicating or avoid
</em><br>
<em>&gt; communicating with minds in this reality while they experience a completely
</em><br>
<em>&gt; unique reality of their own choosing through technology. The machines in
</em><br>
<em>&gt; this time and space are drones programmed to protect the mind(s) living
</em><br>
<em>&gt; within the created world(s).  You could go so far as to model this entire
</em><br>
<em>&gt; existence where each individual mind shapes vis own reality which is
</em><br>
<em>&gt; protected by drones in the higher reality with the ability to transfer one's
</em><br>
<em>&gt; mind between realities as one sees fit or keep others out as one sees fit.
</em><br>
<em>&gt; Universes could be born by the integration and random sharing of minds
</em><br>
<em>&gt; thereby generating more unique child realities.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The ultimate liberty would be to give each person vis own ideaspace with
</em><br>
<em>&gt; which to construct their own reality and experience it as they see fit.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It would be really cool to be to the level of existence as a universal
</em><br>
<em>&gt; mind integrating with other universal minds creating completely new
</em><br>
<em>&gt; universes.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Why would you want to exchange this kind of ability for the lesser
</em><br>
<em>&gt; existence of an entropic reality?
</em><br>
<em>&gt;
</em><br>
<em>&gt; *Stathis Papaioannou &lt;<a href="mailto:stathisp@gmail.com?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">stathisp@gmail.com</a>&gt;* wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; On 4/20/07, Gordon Worley &lt;<a href="mailto:redbird@mac.com?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">redbird@mac.com</a> &gt; wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt; The theory of Friendly AI is fully developed and leads to the
</em><br>
<em>&gt; &gt; creation of a Friendly AI path to Singularity first (after all, we
</em><br>
<em>&gt; &gt; may create something that isn't a Friendly AI but that will figure
</em><br>
<em>&gt; &gt; out how to create a Friendly AI).  However, when this path is
</em><br>
<em>&gt; &gt; enacted, what are the chances that something will cause an
</em><br>
<em>&gt; &gt; existential disaster?  Although I suspect it would be less than the
</em><br>
<em>&gt; &gt; chances of a non-Friendly AI path to Singularity, how much less?  Is
</em><br>
<em>&gt; &gt; it a large enough difference to warrant the extra time, money, and
</em><br>
<em>&gt; &gt; effort required for Friendly AI?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Non-friendly AI might be more likely a cause an existential disaster from
</em><br>
<em>&gt; our point of view, but from its own point of view, unencumbered by concerns
</em><br>
<em>&gt; for anything other than its own well-being, wouldn't it be more rather than
</em><br>
<em>&gt; less likely to survive and colonise the galaxy?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Stathis Papaioannou
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; ------------------------------
</em><br>
<em>&gt; Ahhh...imagining that irresistible &quot;new car&quot; smell?
</em><br>
<em>&gt; Check out new cars at Yahoo! Autos.&lt;<a href="http://us.rd.yahoo.com/evt=48245/*http://autos.yahoo.com/new_cars.html;_ylc=X3oDMTE1YW1jcXJ2BF9TAzk3MTA3MDc2BHNlYwNtYWlsdGFncwRzbGsDbmV3LWNhcnM">http://us.rd.yahoo.com/evt=48245/*http://autos.yahoo.com/new_cars.html;_ylc=X3oDMTE1YW1jcXJ2BF9TAzk3MTA3MDc2BHNlYwNtYWlsdGFncwRzbGsDbmV3LWNhcnM</a>-&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16336.html">apeters2@nd.edu: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>Previous message:</strong> <a href="16334.html">R. W.: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>In reply to:</strong> <a href="16334.html">R. W.: "Re: Existential Risk and Fermi's Paradox"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16336.html">apeters2@nd.edu: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>Reply:</strong> <a href="16336.html">apeters2@nd.edu: "Re: Existential Risk and Fermi's Paradox"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16335">[ date ]</a>
<a href="index.html#16335">[ thread ]</a>
<a href="subject.html#16335">[ subject ]</a>
<a href="author.html#16335">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:57 MDT
</em></small></p>
</body>
</html>
