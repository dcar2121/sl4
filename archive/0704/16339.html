<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Existential Risk and Fermi's Paradox</title>
<meta name="Author" content="Dagon Gmail (dagonweb@gmail.com)">
<meta name="Subject" content="Re: Existential Risk and Fermi's Paradox">
<meta name="Date" content="2007-04-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Existential Risk and Fermi's Paradox</h1>
<!-- received="Sun Apr 22 08:09:12 2007" -->
<!-- isoreceived="20070422140912" -->
<!-- sent="Sun, 22 Apr 2007 16:06:23 +0200" -->
<!-- isosent="20070422140623" -->
<!-- name="Dagon Gmail" -->
<!-- email="dagonweb@gmail.com" -->
<!-- subject="Re: Existential Risk and Fermi's Paradox" -->
<!-- id="bf3acbfc0704220706g12dfb9d0tcbf4ed6d46298911@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="c749694b0704211431k7a86be9n7d4fba48563e7569@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Dagon Gmail (<a href="mailto:dagonweb@gmail.com?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox"><em>dagonweb@gmail.com</em></a>)<br>
<strong>Date:</strong> Sun Apr 22 2007 - 08:06:23 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16340.html">freaken@freaken.dk: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>Previous message:</strong> <a href="16338.html">Olie Lamb: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>In reply to:</strong> <a href="16337.html">Timothy Jennings: "Re: Existential Risk and Fermi's Paradox"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16340.html">freaken@freaken.dk: "Re: Existential Risk and Fermi's Paradox"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16339">[ date ]</a>
<a href="index.html#16339">[ thread ]</a>
<a href="subject.html#16339">[ subject ]</a>
<a href="author.html#16339">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I use the term Machine Rebelion in the (intentionally) most loose of ways.
<br>
What I am
<br>
suggesting is that there seems to be, from the human vantage point, a
<br>
difference between
<br>
insectoid &quot;necessity&quot;, i.e. survival. Other metaphors for insects would be
<br>
Giger aliens,
<br>
or the borg, (without the hive connotations); just machines that know only
<br>
the value of
<br>
control, expansion, breeding, territory. Humans always insist they are &quot;more
<br>
than that&quot;
<br>
but it can be argued the added quality may nothing more than inefficiently
<br>
evolved
<br>
higher neurology and white noise. Humans label this white noise &quot;art&quot; and
<br>
&quot;aesthetics&quot;
<br>
and &quot;philosophy&quot; and &quot;faith&quot; and in countless other ways. But what are they,
<br>
other than
<br>
inefficient pattern recognition errors?
<br>
<p>Most human civilizations becoming uploaded civilizations with simulated
<br>
humans would
<br>
work hard to keep just those qualities intact, and from our limited vantage
<br>
point as humans
<br>
wedged between animals and a post-singularity civilization these qualities
<br>
are all-essential
<br>
to provide meaning, value, sense of self. Without them most humans, except
<br>
for a small
<br>
selection of sociopaths, would prefer such a civilization to go on and on
<br>
(potentially for
<br>
more than a million years).
<br>
<p>When I say machine intelligence, I suggest the possibility of any of a
<br>
million possible
<br>
highly intelligent post-human civilizations and technologies, discarding
<br>
this white noise and
<br>
becoming solely dedicated to efficiency, expansion, conquest and
<br>
realpolitik. We know the
<br>
examples from fiction.
<br>
<p>Take for instance the Matrix series of movies. Imagine the &quot;machines&quot; to be
<br>
programmed to
<br>
keep humans alive, by virtue of some post-asimovian series of core
<br>
instructions. The machines
<br>
would be dedicated to sustaining the essential human experience. However the
<br>
machines, being
<br>
all-wise and capable of outreasoning feeble human desires, could have easily
<br>
created a series
<br>
of events, or a state of being, where these posthuman psychological
<br>
artifacts would slowly
<br>
dissipate over the span of say, tenthousand years. The machines simply offer
<br>
the humans a
<br>
glorious end scene, an electron opium dream and then sweet oblivion.
<br>
Machines, now having rid
<br>
themselves of the white noise, proceed with real business, allegedly clever
<br>
and efficient enough
<br>
to last the required aeonage to actually last that long to matter in our
<br>
speculations about
<br>
The Fermi Paradox; which would be something in the order of tens of millions
<br>
of years.
<br>
<p>In full accordance with these post-asimovian laws.
<br>
<p>There may still be vestiges of humanity or &quot;aesthetics&quot; or &quot;white noise&quot; in
<br>
such civilizations; grand
<br>
opera's of posthuman (or postklingon) orgies of meaning, restricted to
<br>
-subjectively- a few ten
<br>
billion of any of these aliens, locked in some small underground vault on
<br>
some asteroid, playing
<br>
whatever variants of meta-SL, meta-wow, or meta-eve for, subjective, who
<br>
knows how long. But
<br>
in the real universe, the &quot;machine rebellion&quot; would have moved on to real
<br>
business of exploration,
<br>
mining, conquest, colonization, etc.
<br>
<p>At best Virtual Catatonia is no explanation at all to the Fermi Paradox.
<br>
The paradox remains as
<br>
acute and painful as ever as far as I can tell.
<br>
<p>On 4/21/07, Timothy Jennings &lt;<a href="mailto:timothyjennings@gmail.com?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">timothyjennings@gmail.com</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; The obvious solution is &quot;we are first&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;We&quot; &quot;[being] first&quot; is only unlikely in the sense of &quot;what is the chance
</em><br>
<em>&gt; of that golf ball landing exactly there&quot; said by a teenage caddy pointing at
</em><br>
<em>&gt; a random golf ball happening to lie in a certain place when he happens to
</em><br>
<em>&gt; say that.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; On 21/04/07, <a href="mailto:apeters2@nd.edu?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">apeters2@nd.edu</a> &lt;<a href="mailto:apeters2@nd.edu?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">apeters2@nd.edu</a>&gt; wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I'm not sure if &quot;machine rebellion&quot; is a workable concept here.  If we
</em><br>
<em>&gt; &gt; are
</em><br>
<em>&gt; &gt; talking about a civilization able to build whole subrealities at a whim,
</em><br>
<em>&gt; &gt; we are
</em><br>
<em>&gt; &gt; already talking non-biological, uplifted sentience.  Why would they make
</em><br>
<em>&gt; &gt; these
</em><br>
<em>&gt; &gt; (I assume lesser) guardian entities with the capacity to rebel, or even
</em><br>
<em>&gt; &gt; to want
</em><br>
<em>&gt; &gt; to rebel?  Leave them with limited intelligence, perhaps a basic
</em><br>
<em>&gt; &gt; compulsion-program to ensure that they concentrate solely on defense and
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; resource harvesting.
</em><br>
<em>&gt; &gt; Your other point - &quot;bumping up against&quot; other civilizations - seems like
</em><br>
<em>&gt; &gt; a more
</em><br>
<em>&gt; &gt; likely source of problems.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Quoting Dagon Gmail &lt;<a href="mailto:dagonweb@gmail.com?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">dagonweb@gmail.com</a> &gt;:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt; The implication would be, the galactic disk would be seeded with a
</em><br>
<em>&gt; &gt; steadily
</em><br>
<em>&gt; &gt; &gt; growing number of &quot;bombs&quot;,
</em><br>
<em>&gt; &gt; &gt; i.e. extremely defensive automated civilizations solely dedicated to
</em><br>
<em>&gt; &gt; keeping
</em><br>
<em>&gt; &gt; &gt; intact the minds of its original
</em><br>
<em>&gt; &gt; &gt; creators. Just one of these needs to experience a machine rebellion
</em><br>
<em>&gt; &gt; and the
</em><br>
<em>&gt; &gt; &gt; precarious balance is lost. A
</em><br>
<em>&gt; &gt; &gt; machine rebellion may very well not have the sentimental attachment to
</em><br>
<em>&gt; &gt; the
</em><br>
<em>&gt; &gt; &gt; native dream-scape. Machine
</em><br>
<em>&gt; &gt; &gt; civilizations could very well be staunchly objectivist, dedicated to
</em><br>
<em>&gt; &gt; what it
</em><br>
<em>&gt; &gt; &gt; regards as materialist expansion. Any
</em><br>
<em>&gt; &gt; &gt; such rebellion would run into the (alleged) multitudes of &quot;dreaming&quot;
</em><br>
<em>&gt; &gt; or
</em><br>
<em>&gt; &gt; &gt; &quot;virtuamorph&quot; civilizations around.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; And we are talking big timeframes here. If the statistical analysis
</em><br>
<em>&gt; &gt; has any
</em><br>
<em>&gt; &gt; &gt; meaning, virtuamorph civilizations
</em><br>
<em>&gt; &gt; &gt; shouldn't be a de facto dying process; for a dreaming civilization to
</em><br>
<em>&gt; &gt; have
</em><br>
<em>&gt; &gt; &gt; any other meaning than a slow
</em><br>
<em>&gt; &gt; &gt; abortion they have to last millions of years; millions of years means
</em><br>
<em>&gt; &gt; a lot
</em><br>
<em>&gt; &gt; &gt; of galactic shuffling in terms of
</em><br>
<em>&gt; &gt; &gt; stellar trejacteories. There would be many occasions of stars with
</em><br>
<em>&gt; &gt; &gt; &quot;dreamers&quot; drifting into proximity, giving rise to
</em><br>
<em>&gt; &gt; &gt; paranoid, highly protectionist impulses. After all, if all that
</em><br>
<em>&gt; &gt; dreaming is
</em><br>
<em>&gt; &gt; &gt; worth anything in subjective terms the
</em><br>
<em>&gt; &gt; &gt; civilization doing it would fight realworld battles to defend it, and
</em><br>
<em>&gt; &gt; not
</em><br>
<em>&gt; &gt; &gt; just dream about it in metaphorical terms
</em><br>
<em>&gt; &gt; &gt; of +5 vorpal swords.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; Unless the mindscapes have a way of closing off access to reality, i.e.
</em><br>
<em>&gt; &gt; they
</em><br>
<em>&gt; &gt; &gt; materially escape this universe.
</em><br>
<em>&gt; &gt; &gt; But then we introduce new unknows and arbitrary explanations.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; Maybe it's simply easier for civilizations to maintain their
</em><br>
<em>&gt; &gt; consciousness
</em><br>
<em>&gt; &gt; &gt; &gt; in worlds of their own creation rather than expend energy and time
</em><br>
<em>&gt; &gt; in this
</em><br>
<em>&gt; &gt; &gt; &gt; one which is outside of their complete control.  It would seem to me
</em><br>
<em>&gt; &gt; that
</em><br>
<em>&gt; &gt; &gt; &gt; being able to create a paradise of information and experience from
</em><br>
<em>&gt; &gt; the
</em><br>
<em>&gt; &gt; &gt; &gt; substrate of this world would be a better existence than existing in
</em><br>
<em>&gt; &gt; this
</em><br>
<em>&gt; &gt; &gt; &gt; world as is.  Once to this stage, maybe to other civilizations
</em><br>
<em>&gt; &gt; simply do
</em><br>
<em>&gt; &gt; &gt; not
</em><br>
<em>&gt; &gt; &gt; &gt; want to be bothered by lesser beings in this reality who might upset
</em><br>
<em>&gt; &gt; the
</em><br>
<em>&gt; &gt; &gt; &gt; balance and control they desire.  One would only need to be able to
</em><br>
<em>&gt; &gt; &gt; generate
</em><br>
<em>&gt; &gt; &gt; &gt; the prime number sequence in order to create an infinite order of
</em><br>
<em>&gt; &gt; &gt; &gt; probability densities with the next higher prime as the next
</em><br>
<em>&gt; &gt; iterative seed
</em><br>
<em>&gt; &gt; &gt; &gt; value.  In this way, one could mimic true randomness.  A
</em><br>
<em>&gt; &gt; civilization could
</em><br>
<em>&gt; &gt; &gt; &gt; at both times experience truly unique experiences yet have complete
</em><br>
<em>&gt; &gt; control
</em><br>
<em>&gt; &gt; &gt; &gt; over their reality.  The reality they experience would ultimately be
</em><br>
<em>&gt; &gt; &gt; limited
</em><br>
<em>&gt; &gt; &gt; &gt; by the available energy in this reality but hypothetically, they
</em><br>
<em>&gt; &gt; could
</em><br>
<em>&gt; &gt; &gt; &gt; manipulate time in such a way that one second here would be a
</em><br>
<em>&gt; &gt; million years
</em><br>
<em>&gt; &gt; &gt; &gt; in their experienced reality.  Ultimately, their fate would be
</em><br>
<em>&gt; &gt; dependent
</em><br>
<em>&gt; &gt; &gt; &gt; upon the goings on in this universe, but they could develop machines
</em><br>
<em>&gt; &gt; to
</em><br>
<em>&gt; &gt; &gt; &gt; gather energy and other resources to maintain their minds in the
</em><br>
<em>&gt; &gt; &gt; &gt; sub-realities.
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt; They would need to build machines incapable of communicating or
</em><br>
<em>&gt; &gt; avoid
</em><br>
<em>&gt; &gt; &gt; &gt; communicating with minds in this reality while they experience a
</em><br>
<em>&gt; &gt; completely
</em><br>
<em>&gt; &gt; &gt; &gt; unique reality of their own choosing through technology. The
</em><br>
<em>&gt; &gt; machines in
</em><br>
<em>&gt; &gt; &gt; &gt; this time and space are drones programmed to protect the mind(s)
</em><br>
<em>&gt; &gt; living
</em><br>
<em>&gt; &gt; &gt; &gt; within the created world(s).  You could go so far as to model this
</em><br>
<em>&gt; &gt; entire
</em><br>
<em>&gt; &gt; &gt; &gt; existence where each individual mind shapes vis own reality which is
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt; protected by drones in the higher reality with the ability to
</em><br>
<em>&gt; &gt; transfer
</em><br>
<em>&gt; &gt; &gt; one's
</em><br>
<em>&gt; &gt; &gt; &gt; mind between realities as one sees fit or keep others out as one
</em><br>
<em>&gt; &gt; sees fit.
</em><br>
<em>&gt; &gt; &gt; &gt; Universes could be born by the integration and random sharing of
</em><br>
<em>&gt; &gt; minds
</em><br>
<em>&gt; &gt; &gt; &gt; thereby generating more unique child realities.
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt; The ultimate liberty would be to give each person vis own ideaspace
</em><br>
<em>&gt; &gt; with
</em><br>
<em>&gt; &gt; &gt; &gt; which to construct their own reality and experience it as they see
</em><br>
<em>&gt; &gt; fit.
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt; It would be really cool to be to the level of existence as a
</em><br>
<em>&gt; &gt; universal
</em><br>
<em>&gt; &gt; &gt; &gt; mind integrating with other universal minds creating completely new
</em><br>
<em>&gt; &gt; &gt; &gt; universes.
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt; Why would you want to exchange this kind of ability for the lesser
</em><br>
<em>&gt; &gt; &gt; &gt; existence of an entropic reality?
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt; *Stathis Papaioannou &lt;<a href="mailto:stathisp@gmail.com?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">stathisp@gmail.com</a>&gt;* wrote:
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt; On 4/20/07, Gordon Worley &lt; <a href="mailto:redbird@mac.com?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">redbird@mac.com</a> &gt; wrote:
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt; The theory of Friendly AI is fully developed and leads to the
</em><br>
<em>&gt; &gt; &gt; &gt; &gt; creation of a Friendly AI path to Singularity first (after all, we
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt; &gt; may create something that isn't a Friendly AI but that will figure
</em><br>
<em>&gt; &gt; &gt; &gt; &gt; out how to create a Friendly AI).  However, when this path is
</em><br>
<em>&gt; &gt; &gt; &gt; &gt; enacted, what are the chances that something will cause an
</em><br>
<em>&gt; &gt; &gt; &gt; &gt; existential disaster?  Although I suspect it would be less than
</em><br>
<em>&gt; &gt; the
</em><br>
<em>&gt; &gt; &gt; &gt; &gt; chances of a non-Friendly AI path to Singularity, how much
</em><br>
<em>&gt; &gt; less?  Is
</em><br>
<em>&gt; &gt; &gt; &gt; &gt; it a large enough difference to warrant the extra time, money, and
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt; &gt; effort required for Friendly AI?
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt; Non-friendly AI might be more likely a cause an existential disaster
</em><br>
<em>&gt; &gt; from
</em><br>
<em>&gt; &gt; &gt; &gt; our point of view, but from its own point of view, unencumbered by
</em><br>
<em>&gt; &gt; concerns
</em><br>
<em>&gt; &gt; &gt; &gt; for anything other than its own well-being, wouldn't it be more
</em><br>
<em>&gt; &gt; rather than
</em><br>
<em>&gt; &gt; &gt; &gt; less likely to survive and colonise the galaxy?
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt; Stathis Papaioannou
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt; ------------------------------
</em><br>
<em>&gt; &gt; &gt; &gt; Ahhh...imagining that irresistible &quot;new car&quot; smell?
</em><br>
<em>&gt; &gt; &gt; &gt; Check out new cars at Yahoo!
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; Autos.&lt;<a href="http://us.rd.yahoo.com/evt=48245/*http://autos.yahoo.com/new_cars.html;_ylc=X3oDMTE1YW1jcXJ2BF9TAzk3MTA3MDc2BHNlYwNtYWlsdGFncwRzbGsDbmV3LWNhcnM">http://us.rd.yahoo.com/evt=48245/*http://autos.yahoo.com/new_cars.html;_ylc=X3oDMTE1YW1jcXJ2BF9TAzk3MTA3MDc2BHNlYwNtYWlsdGFncwRzbGsDbmV3LWNhcnM</a>-
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16340.html">freaken@freaken.dk: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>Previous message:</strong> <a href="16338.html">Olie Lamb: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>In reply to:</strong> <a href="16337.html">Timothy Jennings: "Re: Existential Risk and Fermi's Paradox"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16340.html">freaken@freaken.dk: "Re: Existential Risk and Fermi's Paradox"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16339">[ date ]</a>
<a href="index.html#16339">[ thread ]</a>
<a href="subject.html#16339">[ subject ]</a>
<a href="author.html#16339">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:57 MDT
</em></small></p>
</body>
</html>
