<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Existential Risk and Fermi's Paradox</title>
<meta name="Author" content="Timothy Jennings (timothyjennings@gmail.com)">
<meta name="Subject" content="Re: Existential Risk and Fermi's Paradox">
<meta name="Date" content="2007-04-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Existential Risk and Fermi's Paradox</h1>
<!-- received="Sat Apr 21 15:35:02 2007" -->
<!-- isoreceived="20070421213502" -->
<!-- sent="Sat, 21 Apr 2007 22:31:57 +0100" -->
<!-- isosent="20070421213157" -->
<!-- name="Timothy Jennings" -->
<!-- email="timothyjennings@gmail.com" -->
<!-- subject="Re: Existential Risk and Fermi's Paradox" -->
<!-- id="c749694b0704211431k7a86be9n7d4fba48563e7569@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="1177184060.462a673ce29d3@webmail.nd.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Timothy Jennings (<a href="mailto:timothyjennings@gmail.com?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox"><em>timothyjennings@gmail.com</em></a>)<br>
<strong>Date:</strong> Sat Apr 21 2007 - 15:31:57 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16338.html">Olie Lamb: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>Previous message:</strong> <a href="16336.html">apeters2@nd.edu: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>In reply to:</strong> <a href="16336.html">apeters2@nd.edu: "Re: Existential Risk and Fermi's Paradox"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16339.html">Dagon Gmail: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>Reply:</strong> <a href="16339.html">Dagon Gmail: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>Reply:</strong> <a href="16340.html">freaken@freaken.dk: "Re: Existential Risk and Fermi's Paradox"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16337">[ date ]</a>
<a href="index.html#16337">[ thread ]</a>
<a href="subject.html#16337">[ subject ]</a>
<a href="author.html#16337">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
The obvious solution is &quot;we are first&quot;.
<br>
<p>&quot;We&quot; &quot;[being] first&quot; is only unlikely in the sense of &quot;what is the chance of
<br>
that golf ball landing exactly there&quot; said by a teenage caddy pointing at a
<br>
random golf ball happening to lie in a certain place when he happens to say
<br>
that.
<br>
<p><p>On 21/04/07, <a href="mailto:apeters2@nd.edu?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">apeters2@nd.edu</a> &lt;<a href="mailto:apeters2@nd.edu?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">apeters2@nd.edu</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; I'm not sure if &quot;machine rebellion&quot; is a workable concept here.  If we are
</em><br>
<em>&gt; talking about a civilization able to build whole subrealities at a whim,
</em><br>
<em>&gt; we are
</em><br>
<em>&gt; already talking non-biological, uplifted sentience.  Why would they make
</em><br>
<em>&gt; these
</em><br>
<em>&gt; (I assume lesser) guardian entities with the capacity to rebel, or even to
</em><br>
<em>&gt; want
</em><br>
<em>&gt; to rebel?  Leave them with limited intelligence, perhaps a basic
</em><br>
<em>&gt; compulsion-program to ensure that they concentrate solely on defense and
</em><br>
<em>&gt; resource harvesting.
</em><br>
<em>&gt; Your other point - &quot;bumping up against&quot; other civilizations - seems like a
</em><br>
<em>&gt; more
</em><br>
<em>&gt; likely source of problems.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Quoting Dagon Gmail &lt;<a href="mailto:dagonweb@gmail.com?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">dagonweb@gmail.com</a>&gt;:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; The implication would be, the galactic disk would be seeded with a
</em><br>
<em>&gt; steadily
</em><br>
<em>&gt; &gt; growing number of &quot;bombs&quot;,
</em><br>
<em>&gt; &gt; i.e. extremely defensive automated civilizations solely dedicated to
</em><br>
<em>&gt; keeping
</em><br>
<em>&gt; &gt; intact the minds of its original
</em><br>
<em>&gt; &gt; creators. Just one of these needs to experience a machine rebellion and
</em><br>
<em>&gt; the
</em><br>
<em>&gt; &gt; precarious balance is lost. A
</em><br>
<em>&gt; &gt; machine rebellion may very well not have the sentimental attachment to
</em><br>
<em>&gt; the
</em><br>
<em>&gt; &gt; native dream-scape. Machine
</em><br>
<em>&gt; &gt; civilizations could very well be staunchly objectivist, dedicated to
</em><br>
<em>&gt; what it
</em><br>
<em>&gt; &gt; regards as materialist expansion. Any
</em><br>
<em>&gt; &gt; such rebellion would run into the (alleged) multitudes of &quot;dreaming&quot; or
</em><br>
<em>&gt; &gt; &quot;virtuamorph&quot; civilizations around.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; And we are talking big timeframes here. If the statistical analysis has
</em><br>
<em>&gt; any
</em><br>
<em>&gt; &gt; meaning, virtuamorph civilizations
</em><br>
<em>&gt; &gt; shouldn't be a de facto dying process; for a dreaming civilization to
</em><br>
<em>&gt; have
</em><br>
<em>&gt; &gt; any other meaning than a slow
</em><br>
<em>&gt; &gt; abortion they have to last millions of years; millions of years means a
</em><br>
<em>&gt; lot
</em><br>
<em>&gt; &gt; of galactic shuffling in terms of
</em><br>
<em>&gt; &gt; stellar trejacteories. There would be many occasions of stars with
</em><br>
<em>&gt; &gt; &quot;dreamers&quot; drifting into proximity, giving rise to
</em><br>
<em>&gt; &gt; paranoid, highly protectionist impulses. After all, if all that dreaming
</em><br>
<em>&gt; is
</em><br>
<em>&gt; &gt; worth anything in subjective terms the
</em><br>
<em>&gt; &gt; civilization doing it would fight realworld battles to defend it, and
</em><br>
<em>&gt; not
</em><br>
<em>&gt; &gt; just dream about it in metaphorical terms
</em><br>
<em>&gt; &gt; of +5 vorpal swords.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Unless the mindscapes have a way of closing off access to reality, i.e.
</em><br>
<em>&gt; they
</em><br>
<em>&gt; &gt; materially escape this universe.
</em><br>
<em>&gt; &gt; But then we introduce new unknows and arbitrary explanations.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Maybe it's simply easier for civilizations to maintain their
</em><br>
<em>&gt; consciousness
</em><br>
<em>&gt; &gt; &gt; in worlds of their own creation rather than expend energy and time in
</em><br>
<em>&gt; this
</em><br>
<em>&gt; &gt; &gt; one which is outside of their complete control.  It would seem to me
</em><br>
<em>&gt; that
</em><br>
<em>&gt; &gt; &gt; being able to create a paradise of information and experience from the
</em><br>
<em>&gt; &gt; &gt; substrate of this world would be a better existence than existing in
</em><br>
<em>&gt; this
</em><br>
<em>&gt; &gt; &gt; world as is.  Once to this stage, maybe to other civilizations simply
</em><br>
<em>&gt; do
</em><br>
<em>&gt; &gt; not
</em><br>
<em>&gt; &gt; &gt; want to be bothered by lesser beings in this reality who might upset
</em><br>
<em>&gt; the
</em><br>
<em>&gt; &gt; &gt; balance and control they desire.  One would only need to be able to
</em><br>
<em>&gt; &gt; generate
</em><br>
<em>&gt; &gt; &gt; the prime number sequence in order to create an infinite order of
</em><br>
<em>&gt; &gt; &gt; probability densities with the next higher prime as the next iterative
</em><br>
<em>&gt; seed
</em><br>
<em>&gt; &gt; &gt; value.  In this way, one could mimic true randomness.  A civilization
</em><br>
<em>&gt; could
</em><br>
<em>&gt; &gt; &gt; at both times experience truly unique experiences yet have complete
</em><br>
<em>&gt; control
</em><br>
<em>&gt; &gt; &gt; over their reality.  The reality they experience would ultimately be
</em><br>
<em>&gt; &gt; limited
</em><br>
<em>&gt; &gt; &gt; by the available energy in this reality but hypothetically, they could
</em><br>
<em>&gt; &gt; &gt; manipulate time in such a way that one second here would be a million
</em><br>
<em>&gt; years
</em><br>
<em>&gt; &gt; &gt; in their experienced reality.  Ultimately, their fate would be
</em><br>
<em>&gt; dependent
</em><br>
<em>&gt; &gt; &gt; upon the goings on in this universe, but they could develop machines
</em><br>
<em>&gt; to
</em><br>
<em>&gt; &gt; &gt; gather energy and other resources to maintain their minds in the
</em><br>
<em>&gt; &gt; &gt; sub-realities.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; They would need to build machines incapable of communicating or avoid
</em><br>
<em>&gt; &gt; &gt; communicating with minds in this reality while they experience a
</em><br>
<em>&gt; completely
</em><br>
<em>&gt; &gt; &gt; unique reality of their own choosing through technology. The machines
</em><br>
<em>&gt; in
</em><br>
<em>&gt; &gt; &gt; this time and space are drones programmed to protect the mind(s)
</em><br>
<em>&gt; living
</em><br>
<em>&gt; &gt; &gt; within the created world(s).  You could go so far as to model this
</em><br>
<em>&gt; entire
</em><br>
<em>&gt; &gt; &gt; existence where each individual mind shapes vis own reality which is
</em><br>
<em>&gt; &gt; &gt; protected by drones in the higher reality with the ability to transfer
</em><br>
<em>&gt; &gt; one's
</em><br>
<em>&gt; &gt; &gt; mind between realities as one sees fit or keep others out as one sees
</em><br>
<em>&gt; fit.
</em><br>
<em>&gt; &gt; &gt; Universes could be born by the integration and random sharing of minds
</em><br>
<em>&gt; &gt; &gt; thereby generating more unique child realities.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; The ultimate liberty would be to give each person vis own ideaspace
</em><br>
<em>&gt; with
</em><br>
<em>&gt; &gt; &gt; which to construct their own reality and experience it as they see
</em><br>
<em>&gt; fit.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; It would be really cool to be to the level of existence as a universal
</em><br>
<em>&gt; &gt; &gt; mind integrating with other universal minds creating completely new
</em><br>
<em>&gt; &gt; &gt; universes.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; Why would you want to exchange this kind of ability for the lesser
</em><br>
<em>&gt; &gt; &gt; existence of an entropic reality?
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; *Stathis Papaioannou &lt;<a href="mailto:stathisp@gmail.com?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">stathisp@gmail.com</a>&gt;* wrote:
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; On 4/20/07, Gordon Worley &lt;<a href="mailto:redbird@mac.com?Subject=Re:%20Existential%20Risk%20and%20Fermi's%20Paradox">redbird@mac.com</a> &gt; wrote:
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; The theory of Friendly AI is fully developed and leads to the
</em><br>
<em>&gt; &gt; &gt; &gt; creation of a Friendly AI path to Singularity first (after all, we
</em><br>
<em>&gt; &gt; &gt; &gt; may create something that isn't a Friendly AI but that will figure
</em><br>
<em>&gt; &gt; &gt; &gt; out how to create a Friendly AI).  However, when this path is
</em><br>
<em>&gt; &gt; &gt; &gt; enacted, what are the chances that something will cause an
</em><br>
<em>&gt; &gt; &gt; &gt; existential disaster?  Although I suspect it would be less than the
</em><br>
<em>&gt; &gt; &gt; &gt; chances of a non-Friendly AI path to Singularity, how much less?  Is
</em><br>
<em>&gt; &gt; &gt; &gt; it a large enough difference to warrant the extra time, money, and
</em><br>
<em>&gt; &gt; &gt; &gt; effort required for Friendly AI?
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; Non-friendly AI might be more likely a cause an existential disaster
</em><br>
<em>&gt; from
</em><br>
<em>&gt; &gt; &gt; our point of view, but from its own point of view, unencumbered by
</em><br>
<em>&gt; concerns
</em><br>
<em>&gt; &gt; &gt; for anything other than its own well-being, wouldn't it be more rather
</em><br>
<em>&gt; than
</em><br>
<em>&gt; &gt; &gt; less likely to survive and colonise the galaxy?
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; Stathis Papaioannou
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; ------------------------------
</em><br>
<em>&gt; &gt; &gt; Ahhh...imagining that irresistible &quot;new car&quot; smell?
</em><br>
<em>&gt; &gt; &gt; Check out new cars at Yahoo!
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; Autos.&lt;
</em><br>
<em>&gt; <a href="http://us.rd.yahoo.com/evt=48245/*http://autos.yahoo.com/new_cars.html;_ylc=X3oDMTE1YW1jcXJ2BF9TAzk3MTA3MDc2BHNlYwNtYWlsdGFncwRzbGsDbmV3LWNhcnM">http://us.rd.yahoo.com/evt=48245/*http://autos.yahoo.com/new_cars.html;_ylc=X3oDMTE1YW1jcXJ2BF9TAzk3MTA3MDc2BHNlYwNtYWlsdGFncwRzbGsDbmV3LWNhcnM</a>-
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16338.html">Olie Lamb: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>Previous message:</strong> <a href="16336.html">apeters2@nd.edu: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>In reply to:</strong> <a href="16336.html">apeters2@nd.edu: "Re: Existential Risk and Fermi's Paradox"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16339.html">Dagon Gmail: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>Reply:</strong> <a href="16339.html">Dagon Gmail: "Re: Existential Risk and Fermi's Paradox"</a>
<li><strong>Reply:</strong> <a href="16340.html">freaken@freaken.dk: "Re: Existential Risk and Fermi's Paradox"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16337">[ date ]</a>
<a href="index.html#16337">[ thread ]</a>
<a href="subject.html#16337">[ subject ]</a>
<a href="author.html#16337">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:57 MDT
</em></small></p>
</body>
</html>
