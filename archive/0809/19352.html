<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] A model of RSI</title>
<meta name="Author" content="Stuart Armstrong (dragondreaming@googlemail.com)">
<meta name="Subject" content="Re: [sl4] A model of RSI">
<meta name="Date" content="2008-09-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] A model of RSI</h1>
<!-- received="Thu Sep 25 03:25:46 2008" -->
<!-- isoreceived="20080925092546" -->
<!-- sent="Thu, 25 Sep 2008 10:22:40 +0100" -->
<!-- isosent="20080925092240" -->
<!-- name="Stuart Armstrong" -->
<!-- email="dragondreaming@googlemail.com" -->
<!-- subject="Re: [sl4] A model of RSI" -->
<!-- id="38f493f10809250222x67872c2aqd54301de812681e5@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="848250.20160.qm@web51903.mail.re2.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Stuart Armstrong (<a href="mailto:dragondreaming@googlemail.com?Subject=Re:%20[sl4]%20A%20model%20of%20RSI"><em>dragondreaming@googlemail.com</em></a>)<br>
<strong>Date:</strong> Thu Sep 25 2008 - 03:22:40 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19353.html">Andy Miah: "[sl4] Human Futures - Art in an Age of Uncertainty book launch and conference"</a>
<li><strong>Previous message:</strong> <a href="19351.html">Mike Dougherty: "Re: [sl4] A model of RSI"</a>
<li><strong>In reply to:</strong> <a href="19347.html">Matt Mahoney: "Re: [sl4] A model of RSI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19356.html">Matt Mahoney: "Re: [sl4] A model of RSI"</a>
<li><strong>Reply:</strong> <a href="19356.html">Matt Mahoney: "Re: [sl4] A model of RSI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19352">[ date ]</a>
<a href="index.html#19352">[ thread ]</a>
<a href="subject.html#19352">[ subject ]</a>
<a href="author.html#19352">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; Define &quot;improvement&quot;. In the context of a real machine (a finite state machine), we could relax the definition of a goal so that utility is a monotonically increasing function of time but reaches some maximum after finite time. Then we could define improvement as reaching that maximum faster or reaching a greater maximum in the same time. However, a finite state machine can only run a finite number of programs, so there can only be a finite sequence of improvements by any definition.
</em><br>
<p>This is not a problem. I'm pretty sure humans are finite state
<br>
probabilistic machines. &quot;Finite&quot; includes numbers so brutally high
<br>
that they might as well be infinite from the human perspective (and
<br>
possibly from the perspective of the visible universe).
<br>
<p><em>&gt; Self improvement requires a test or goal that cannot be altered through generations. Assuming that goal is &quot;intelligence&quot;, we are not smart enough to test for it above our own level. If we are, then perhaps someone could describe that test.
</em><br>
<p>I've already proposed a gaggle of tests - mainly taking an open ended
<br>
task (running a sucesfull company, organising an election campaign,
<br>
etc...) with a clear relative standard of sucess, and setting the AI's
<br>
head to head. A sucessful test just means a better understanding of
<br>
what we really want.
<br>
<p><em>&gt; We assume (but don't know) that adding more neurons to our brains would make us more intelligent and therefore better. But why do we want to be more intelligent? Because our brains are programmed that way. Intelligence requires both the ability to learn and the desire to learn. Suppose that we engineer our children for bigger brains, but in doing so we accidentally remove the desire to be intelligent. Then our children will engineer our grandchildren according to their interpretation of what it means to improve, not our interpretation.
</em><br>
<p>As much testing for superior intelligence isn't the problem, tradeoffs
<br>
like that will be. We can test for superior intelligence (by our
<br>
definition), but without superior intelligence ourselves, we don't
<br>
really understand the tradeoffs that our tests are forcing on the
<br>
AI's. It might be that a slight modification of our tests,
<br>
insignificant for us, will result in dramatic changes in the resulting
<br>
AI's - but we'll never know that ourselves.
<br>
<p>Stuart
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19353.html">Andy Miah: "[sl4] Human Futures - Art in an Age of Uncertainty book launch and conference"</a>
<li><strong>Previous message:</strong> <a href="19351.html">Mike Dougherty: "Re: [sl4] A model of RSI"</a>
<li><strong>In reply to:</strong> <a href="19347.html">Matt Mahoney: "Re: [sl4] A model of RSI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19356.html">Matt Mahoney: "Re: [sl4] A model of RSI"</a>
<li><strong>Reply:</strong> <a href="19356.html">Matt Mahoney: "Re: [sl4] A model of RSI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19352">[ date ]</a>
<a href="index.html#19352">[ thread ]</a>
<a href="subject.html#19352">[ subject ]</a>
<a href="author.html#19352">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
