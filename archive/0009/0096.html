<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Fwd: Earthweb, from transadmin</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Fwd: Earthweb, from transadmin">
<meta name="Date" content="2000-09-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Fwd: Earthweb, from transadmin</h1>
<!-- received="Mon Sep 18 01:12:21 2000" -->
<!-- isoreceived="20000918071221" -->
<!-- sent="Sun, 17 Sep 2000 22:11:03 -0400" -->
<!-- isosent="20000918021103" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Fwd: Earthweb, from transadmin" -->
<!-- id="39C579B7.44214D85@pobox.com" -->
<!-- charset="US-ASCII" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Fwd:%20Earthweb,%20from%20transadmin"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Sep 17 2000 - 20:11:03 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0097.html">gabriel C: "(no subject)"</a>
<li><strong>Previous message:</strong> <a href="0095.html">Brian Atkins: "Re: [transadmin] Quick Suggestion"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#96">[ date ]</a>
<a href="index.html#96">[ thread ]</a>
<a href="subject.html#96">[ subject ]</a>
<a href="author.html#96">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;
<br>

<br><p><strong>attached mail follows:</strong><hr>
<br><body>
<br>

<!-- |**|begin egp html banner|**| -->

<table border=0 cellspacing=0 cellpadding=2>
<tr bgcolor=#FFFFCC valign=middle>
<td width=77><a href="http://www.egroups.com/"><img border=0 src="http://www.egroups.com/img/logo/logo72.gif" width="72" height="32" alt="eGroups"></a></td>
<td width=388>
<font size="-1">
<a href="http://www.egroups.com/mygroups">My Groups</a> |
<a href="http://www.egroups.com/group/transadmin">transadmin Main Page</a>
| <!-- |@|begin eGroups banner|@| runid: 8150 crid: 4125 --><a target="_blank" href="http://click.egroups.com/1/8150/6/_/_/_/969133843/">Start a new group!</a><!-- |@|end eGroups banner|@| --></font>
</td>
</tr>
</table>
<br>

<!-- |**|end egp html banner|**| -->

<p>

<tt>
Alex Future Bokov wrote:<BR>
&gt; <BR>
&gt; PPS: Oh, one more thing. The 'forgotten' thing I wanted to ask was &quot;How<BR>
&gt; much intelligence is sufficient to meet the objectives? You<BR>
&gt; demonstrated that an &gt;AI would be smarter than an EarthWeb,<BR>
<BR>
(Background for non-attendees:&nbsp; My opinion was that the Earthweb is a<BR>
transhuman, but not a superintelligence.&nbsp; It can form sequences of thoughts<BR>
that are beyond human capability, but any individual thought still has to fit<BR>
inside a single human mind.&nbsp; The Earthweb can be faster than an individual<BR>
human, can have vastly more knowledge, and can even think in genuinely<BR>
transhuman sequences - in those cases where multiple, intersecting experts can<BR>
build upon each others' ideas.&nbsp; An individual human can have one flash of<BR>
genius, or a few flashes of genius; the Earthweb, in theory, can pile<BR>
thousands of flashes of genius one on top of the other, to form sequences<BR>
qualitatively different from those that any single human has ever come up with<BR>
during Earth's previous history.&nbsp; But any individual genius-flash still has to<BR>
come from a single human.&nbsp; Thus the ideal Earthweb is a transhuman but not a<BR>
superintelligence.)<BR>
<BR>
&gt; but would<BR>
&gt; an EarthWeb do just fine for the purposes, and with less likelihood (by<BR>
&gt; definition) of having priorities that conflict with those of humanity,<BR>
&gt; and requiring very little new technology?&nbsp; How does one even go about<BR>
&gt; estimating the level of intelligence needed to safeguard the world from<BR>
&gt; nanodisaster and bring about sentient matter?&quot;<BR>
<BR>
It seems to me that the problem is one of cooperation (or &quot;enforcement&quot;)<BR>
rather than raw intelligence.&nbsp; The transhumanists show up in the 1980s and<BR>
have all these great ideas about how to defend the world from grey goo, then<BR>
the Singularitarians show up in the 1990s and have this great idea about<BR>
bypassing the whole problem via superintelligence.&nbsp; Both of these are<BR>
instances of the exercise of smartness contributing to the safeguarding of the<BR>
world - but the problem is not having the bright idea; the problem is putting<BR>
enough backbone behind it.&nbsp; The Earthweb is great for coming up with ideas,<BR>
but says nothing about backing, or enforcement.<BR>
<BR>
In other words, it looks to me like the Earthweb would say:&nbsp; &quot;Hey, let's go<BR>
build an AI!&quot;&nbsp; Or perhaps the Earthweb is brighter than I am, and would see an<BR>
even simpler and faster way to do it - though I have difficulty imagining what<BR>
one would be.<BR>
<BR>
The neat part about superintelligence isn't just that an SI is really smart;<BR>
it's that an SI can very rapidly build new technologies and use them according<BR>
to a unified set of motives.&nbsp; Before an Earthweb could even begin to replace<BR>
superintelligence as a guardian, it would have to (a) come up with a smart<BR>
plan, (b) invent the technology to implement it, and (c) ensure that the<BR>
technology was used to implement (a).&nbsp; It looks to me like (c) would be the<BR>
major problem, since the Earthweb by its nature is public and distributed. <BR>
But perhaps the Earthweb could come up with a clever solution even to this...<BR>
<BR>
My feeling, though, is that even if the Earthweb does solve all these problems<BR>
and come up with a clever way to build a better world using only the limited<BR>
intelligence of the Earthweb, just going out and building a seed AI will still<BR>
look like an even more superior solution.<BR>
<BR>
In summary, though, the power of Earthweb, as with any transhuman, would lie<BR>
primarily in its smartness, not its brute intelligence.&nbsp; The Earthweb can act<BR>
as guardian if and only if the Earthweb itself comes up with some really<BR>
clever way to act as guardian.<BR>
<BR>
A practical problem is that all the Earthweb techniques I've seen, including<BR>
the idea-futures-on-steroids of Marc Stiegler's _Earthweb_, are still subject<BR>
to distortion by majority prejudice.&nbsp; If a majority of the betting money is in<BR>
the hands of folk with a blind prejudice against AIs, then only a very stable<BR>
culture - only a culture that has had the Earthweb for years or even decades -<BR>
will have the systemic structure whereby informed discussion can overcome<BR>
prejudices.&nbsp; In other words, the Earthweb proposals I've seen have no method<BR>
to distinguish between genius and stupidity except by using the minds of other<BR>
humans, and it will take a while before the system builds up enough internal<BR>
complexity to have a systemic method of distinguishing - independently of the<BR>
human components - the subtle structural differences between a human making a<BR>
good judgement and a human making a bad judgement.<BR>
<BR>
Idea futures are only the beginning of such a method.&nbsp; Idea futures mean<BR>
betting money on the judgements, so that the winners of each round have<BR>
greater weight in successive rounds.&nbsp; Idea futures are better than blind<BR>
majority votes, but they aren't perfect.&nbsp; How do you use idea futures to<BR>
resolve an issue like Friendly AI, even leaving out the payoff problem?&nbsp; It<BR>
would take several iterated rounds on issues of the same order, with similar<BR>
stakes and similar content and similar required knowledge and *resolvable<BR>
predictions*, before the capitalist efficiencies came into play and the bad<BR>
betters started dropping out.<BR>
<BR>
For the Earthweb to resolve a problem like that, it would need a systemic<BR>
structure that systematically resolved each idea into sub-issues, identifying<BR>
each assumption and deduction and sequitur, and discussing and betting on<BR>
these subcomponents separately.&nbsp; In other words, the Earthweb would have to<BR>
actually change the structure of thoughts and thinking and decision-making<BR>
processes, after which it's plausible that the outcome of the decision would<BR>
be better than the sum of the betting humans - if nothing else, because the<BR>
humans who made bets on the final outcome would have the granular resolution<BR>
of the discussion available for examination.<BR>
<BR>
Wanna take this to SL4?&nbsp;&nbsp;&nbsp; ( <a href="http://sysopmind.com/sing/SL4.html">http://sysopmind.com/sing/SL4.html</a> )<BR>
If not, can I forward this message there?&nbsp; And to Extropians, Robin Hanson,<BR>
and Marc Stiegler - I think they'd all be interested.<BR>
<BR>
&gt; PPPS: Anybody who might be wondering, what I mean by EarthWeb is a sort<BR>
&gt; of world-wide 'Slash meets eCommerce meets eBay meets email on<BR>
&gt; steroids' that evolves into an emergent entity in its own right. As for<BR>
&gt; &gt;AI, see SingInst's pages.<BR>
<BR>
And what I mean by &quot;Earthweb&quot; is the entity visualized by Marc Stiegler in the<BR>
book &quot;Earthweb&quot;.<BR>
<BR>
See also:<BR>
<BR>
<a href="http://www.the-earthweb.com">http://www.the-earthweb.com</a><BR>
<a href="http://www.baen.com/chapters/eweb_1.htm">http://www.baen.com/chapters/eweb_1.htm</a> (chapters 1 through 6 available)<BR>
<BR>
--&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- <BR>
Eliezer S. Yudkowsky&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="http://intelligence.org/">http://intelligence.org/</a> <BR>
Research Fellow, Singularity Institute for Artificial Intelligence<BR>
</tt>

<br>
<tt>
To unsubscribe from this group, send an email to:<BR>
transadmin-unsubscribe@egroups.com<BR>
<BR>
</tt>
<br>

</body>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0097.html">gabriel C: "(no subject)"</a>
<li><strong>Previous message:</strong> <a href="0095.html">Brian Atkins: "Re: [transadmin] Quick Suggestion"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#96">[ date ]</a>
<a href="index.html#96">[ thread ]</a>
<a href="subject.html#96">[ subject ]</a>
<a href="author.html#96">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
