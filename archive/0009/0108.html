<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: economic effects of AI (was RE: About that E-mail:...)</title>
<meta name="Author" content="Michael LaTorra (mike99@lascruces.com)">
<meta name="Subject" content="economic effects of AI (was RE: About that E-mail:...)">
<meta name="Date" content="2000-09-30">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>economic effects of AI (was RE: About that E-mail:...)</h1>
<!-- received="Sat Sep 30 22:06:36 2000" -->
<!-- isoreceived="20001001040636" -->
<!-- sent="Sat, 30 Sep 2000 20:02:21 -0600" -->
<!-- isosent="20001001020221" -->
<!-- name="Michael LaTorra" -->
<!-- email="mike99@lascruces.com" -->
<!-- subject="economic effects of AI (was RE: About that E-mail:...)" -->
<!-- id="NEBBJFIIHLGDGJLCGMBIOEACCCAA.mike99@lascruces.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMAEJHEJAA.ben@intelligenesis.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael LaTorra (<a href="mailto:mike99@lascruces.com?Subject=Re:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)"><em>mike99@lascruces.com</em></a>)<br>
<strong>Date:</strong> Sat Sep 30 2000 - 20:02:21 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0109.html">Ben Goertzel: "RE: About that E-mail:..."</a>
<li><strong>Previous message:</strong> <a href="0107.html">Brian Atkins: "Re: About that E-mail:..."</a>
<li><strong>In reply to:</strong> <a href="0105.html">Ben Goertzel: "RE: About that E-mail:..."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0111.html">Ben Goertzel: "RE: economic effects of AI (was RE: About that E-mail:...)"</a>
<li><strong>Reply:</strong> <a href="0111.html">Ben Goertzel: "RE: economic effects of AI (was RE: About that E-mail:...)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#108">[ date ]</a>
<a href="index.html#108">[ thread ]</a>
<a href="subject.html#108">[ subject ]</a>
<a href="author.html#108">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I do agree that commercial AI leading up to SI (at whatever rate of
<br>
progress) would almost certainly be perceived as a great boon because it
<br>
will make many people rich and provide tangible benefits to others in the
<br>
forms of new or cheaper goods and services.
<br>
<p>But this initial &quot;era of good feeling&quot; could change quickly as AI advances
<br>
begin to substitute for more and more &quot;human capital&quot; (i.e., people's jobs).
<br>
I am making this argument not because it feels right to me intuitively, but
<br>
because a very intelligent transhumanist economist has made it. Here's the
<br>
link to, and the abstract of, Robin Hanson's paper:
<br>
<p><a href="http://hanson.gmu.edu/workingpapers.html">http://hanson.gmu.edu/workingpapers.html</a>
<br>
[NOTE: Go to the page and scroll down to the title below then click it to
<br>
open the actual PDF file.]
<br>
<p>Economic Growth Given Machine Intelligence, Aug. '98
<br>
<p>A simple exogenous growth model gives conservative estimates of the economic
<br>
implications of machine intelligence. Machines complement human labor when
<br>
they become more productive at the jobs they perform, but machines also
<br>
substitute for human labor by taking over human jobs. At first, expensive
<br>
hardware and software does only the few jobs where computers have the
<br>
strongest advantage over humans. Eventually, computers do most jobs. At
<br>
first, complementary effects dominate, and human wages rise with computer
<br>
productivity. But eventually substitution can dominate, making wages fall as
<br>
fast as computer prices now do. An intelligence population explosion makes
<br>
per-intelligence consumption fall this fast, while economic growth rates
<br>
rise by an order of magnitude or more. These results are robust to
<br>
automating incrementally, and to distinguishing hardware, software, and
<br>
human capital from other forms of capital.
<br>
<p>Regards,
<br>
Michael LaTorra
<br>
<a href="mailto:mike99@lascruces.com?Subject=Re:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">mike99@lascruces.com</a>
<br>
<p><p>-----Original Message-----
<br>
From: <a href="mailto:owner-sl4@sysopmind.com?Subject=Re:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">owner-sl4@sysopmind.com</a> [mailto:<a href="mailto:owner-sl4@sysopmind.com?Subject=Re:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">owner-sl4@sysopmind.com</a>]On Behalf
<br>
Of Ben Goertzel
<br>
Sent: Saturday, September 30, 2000 7:45 PM
<br>
To: <a href="mailto:sl4@sysopmind.com?Subject=Re:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">sl4@sysopmind.com</a>
<br>
Subject: RE: About that E-mail:...
<br>
<p><p><p>Here's another point
<br>
<p>If the first real AI is a commercial enterprise, it'll be making people
<br>
money
<br>
<p>Everyone will own stock in real AI ... it'll be a huge popular sensation ...
<br>
the financial aspects may drown out any troublesome philosophical aspects in
<br>
the public
<br>
mind...
<br>
<p>if they're making money off it in the short run, not many people will really
<br>
be thinking
<br>
about the long run -- this is typical homo sapiens shortsightedness, which
<br>
will work in the favor
<br>
of cosmic evolution in this case
<br>
<p>-- ben goertzel
<br>
<p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sysopmind.com?Subject=Re:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">owner-sl4@sysopmind.com</a> [mailto:<a href="mailto:owner-sl4@sysopmind.com?Subject=Re:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">owner-sl4@sysopmind.com</a>]On Behalf
</em><br>
<em>&gt; Of Eliezer S. Yudkowsky
</em><br>
<em>&gt; Sent: Saturday, September 30, 2000 9:23 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sysopmind.com?Subject=Re:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">sl4@sysopmind.com</a>
</em><br>
<em>&gt; Subject: Re: About that E-mail:...
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Josh Yotty wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I'm willing to bet the people working toward superhuman
</em><br>
<em>&gt; intelligence will be hunted down. Of course, the people hunting
</em><br>
<em>&gt; us down will be irrational, ignorant, narrowminded and stupid.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Be careful what you fear.  Sufficient amounts of hatred tend to turn into
</em><br>
<em>&gt; self-fulfilling prophecies... and if somebody really did try and
</em><br>
<em>&gt; hunt me down
</em><br>
<em>&gt; I sure wouldn't want to underestimate them.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You'd be amazed at how often witch-hunts don't happen in First World
</em><br>
<em>&gt; countries.  I can't think of anything I ought to be doing in advance to
</em><br>
<em>&gt; prepare for the possibility of violent protesters, so I don't
</em><br>
<em>&gt; intend to worry
</em><br>
<em>&gt; excessively over the possibility until it starts actually
</em><br>
<em>&gt; happening.  There
</em><br>
<em>&gt; are essentially two strategies to deal with anti-technology
</em><br>
<em>&gt; crusades; you can
</em><br>
<em>&gt; try to run quietly and unobtrusively, or you can try for a pro-technology
</em><br>
<em>&gt; crusade.  I've observed that ordinary people tend to grasp the
</em><br>
<em>&gt; Singularity on
</em><br>
<em>&gt; the first try; it's the people who think they're intellectuals
</em><br>
<em>&gt; that you have
</em><br>
<em>&gt; to watch out for - so the second possibility is actually
</em><br>
<em>&gt; plausible.  I don't
</em><br>
<em>&gt; know if running quietly is plausible - it depends on how long it
</em><br>
<em>&gt; takes to get
</em><br>
<em>&gt; to a Singularity.  It's starting to look as if we don't bring the
</em><br>
<em>&gt; issue into
</em><br>
<em>&gt; the public eye, Bill Joy will.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Presently, I think it's not too much to hope for that the future will not
</em><br>
<em>&gt; contain anti-AI terrorist organizations.  There are anti-GM groups and
</em><br>
<em>&gt; antiabortion groups, but it's harder to get public sympathy for a violent
</em><br>
<em>&gt; crusade against something that's only a possibility - I hope.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If we do bring the issue into the public eye, turning it into an
</em><br>
<em>&gt; elitist issue
</em><br>
<em>&gt; isn't really going to help.
</em><br>
<em>&gt;
</em><br>
<em>&gt; --              --              --              --              --
</em><br>
<em>&gt; Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt; Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0109.html">Ben Goertzel: "RE: About that E-mail:..."</a>
<li><strong>Previous message:</strong> <a href="0107.html">Brian Atkins: "Re: About that E-mail:..."</a>
<li><strong>In reply to:</strong> <a href="0105.html">Ben Goertzel: "RE: About that E-mail:..."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0111.html">Ben Goertzel: "RE: economic effects of AI (was RE: About that E-mail:...)"</a>
<li><strong>Reply:</strong> <a href="0111.html">Ben Goertzel: "RE: economic effects of AI (was RE: About that E-mail:...)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#108">[ date ]</a>
<a href="index.html#108">[ thread ]</a>
<a href="subject.html#108">[ subject ]</a>
<a href="author.html#108">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
