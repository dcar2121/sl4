<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=Windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: economic effects of AI (was RE: About that E-mail:...)</title>
<meta name="Author" content="Michael LaTorra (mike99@lascruces.com)">
<meta name="Subject" content="RE: economic effects of AI (was RE: About that E-mail:...)">
<meta name="Date" content="2000-09-30">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: economic effects of AI (was RE: About that E-mail:...)</h1>
<!-- received="Sat Sep 30 22:46:36 2000" -->
<!-- isoreceived="20001001044636" -->
<!-- sent="Sat, 30 Sep 2000 20:41:28 -0600" -->
<!-- isosent="20001001024128" -->
<!-- name="Michael LaTorra" -->
<!-- email="mike99@lascruces.com" -->
<!-- subject="RE: economic effects of AI (was RE: About that E-mail:...)" -->
<!-- id="NEBBJFIIHLGDGJLCGMBICEAECCAA.mike99@lascruces.com" -->
<!-- charset="Windows-1252" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMMEJIEJAA.ben@intelligenesis.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael LaTorra (<a href="mailto:mike99@lascruces.com?Subject=RE:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)"><em>mike99@lascruces.com</em></a>)<br>
<strong>Date:</strong> Sat Sep 30 2000 - 20:41:28 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0113.html">E. Shaun Russell: "Re: About that E-mail:..."</a>
<li><strong>Previous message:</strong> <a href="0111.html">Ben Goertzel: "RE: economic effects of AI (was RE: About that E-mail:...)"</a>
<li><strong>In reply to:</strong> <a href="0111.html">Ben Goertzel: "RE: economic effects of AI (was RE: About that E-mail:...)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0010/0115.html">Ben Goertzel: "RE: economic effects of AI (was RE: About that E-mail:...)"</a>
<li><strong>Reply:</strong> <a href="../0010/0115.html">Ben Goertzel: "RE: economic effects of AI (was RE: About that E-mail:...)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#112">[ date ]</a>
<a href="index.html#112">[ thread ]</a>
<a href="subject.html#112">[ subject ]</a>
<a href="author.html#112">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
What you have described here, Ben, is exactly what I hope will happen. I
<br>
know that I have at least a dozen projects I'd love to work on, projects
<br>
that other people would probably enjoy, but which are not remunerative
<br>
enough in the short run to keep my kids in sneakers. And I can think of
<br>
plenty of services I would like to receive from other people if I had the
<br>
time and money for them.
<br>
<p>In trying to think through the transition to this stage of &quot;low-cost&quot;
<br>
human-to-human service, however, I keep getting stuck at the following
<br>
points:
<br>
<p>1) How will the wealth generated by AI be distributed? We don't want to fall
<br>
into a system with the negative incentives of socialism. But we would
<br>
endanger social stability if, say, 1% of the population controlled 99% of
<br>
the wealth because they own the AIs. (An SI would probably not submit to
<br>
being owned!)
<br>
<p>2) How many &quot;lotus eaters&quot; can a society tolerate? Suppose there were no
<br>
need to work for a basic living (welfare for all and jobs for the few whose
<br>
services are in demand). How many heroin addicts would we want to support?
<br>
<p>3) When the possibility for uploading finally arrives in reality, will there
<br>
be a first-mover advantage? Can the first homesteaders of cyber-mindspace
<br>
gain permanent advantages over those who come later? (If you could control
<br>
the cyber equivalent of the fundamental constants of physics, what would you
<br>
do?)
<br>
<p>Regards,
<br>
Michael LaTorra
<br>
<a href="mailto:mike99@lascruces.com?Subject=RE:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">mike99@lascruces.com</a>
<br>
<p><p><p>-----Original Message-----
<br>
From: <a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">owner-sl4@sysopmind.com</a> [mailto:<a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">owner-sl4@sysopmind.com</a>]On Behalf
<br>
Of Ben Goertzel
<br>
Sent: Saturday, September 30, 2000 8:12 PM
<br>
To: <a href="mailto:sl4@sysopmind.com?Subject=RE:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">sl4@sysopmind.com</a>
<br>
Subject: RE: economic effects of AI (was RE: About that E-mail:...)
<br>
<p><p><p>It's not a ridiculous argument, but I still think it's a wrong one
<br>
<p>People need more than just material goods, people need emotional goodies
<br>
from
<br>
other people, which come in a million different forms
<br>
<p>What this means is that as long as there are humans with human bodies
<br>
similar to the
<br>
ones we have now, there will be plenty of service jobs, because humans want
<br>
to be surrounded
<br>
by humans doing things with and for them
<br>
<p>Also, I predict that when real AI comes about, there will be a long period
<br>
when it is
<br>
complementary to, rather than unmitigatedly superior to, human intelligence.
<br>
Each kind of mind
<br>
will have its niche.  Ultimately AI will surpass us in all ways, but by
<br>
then, the human body
<br>
may also be obsolete due to other technology advances...
<br>
<p>ben
<br>
<p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">owner-sl4@sysopmind.com</a> [mailto:<a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">owner-sl4@sysopmind.com</a>]On Behalf
</em><br>
<em>&gt; Of Michael LaTorra
</em><br>
<em>&gt; Sent: Saturday, September 30, 2000 10:02 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sysopmind.com?Subject=RE:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">sl4@sysopmind.com</a>
</em><br>
<em>&gt; Subject: economic effects of AI (was RE: About that E-mail:...)
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I do agree that commercial AI leading up to SI (at whatever rate of
</em><br>
<em>&gt; progress) would almost certainly be perceived as a great boon because it
</em><br>
<em>&gt; will make many people rich and provide tangible benefits to others in the
</em><br>
<em>&gt; forms of new or cheaper goods and services.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But this initial &quot;era of good feeling&quot; could change quickly as AI advances
</em><br>
<em>&gt; begin to substitute for more and more &quot;human capital&quot; (i.e.,
</em><br>
<em>&gt; people's jobs).
</em><br>
<em>&gt; I am making this argument not because it feels right to me
</em><br>
<em>&gt; intuitively, but
</em><br>
<em>&gt; because a very intelligent transhumanist economist has made it. Here's the
</em><br>
<em>&gt; link to, and the abstract of, Robin Hanson's paper:
</em><br>
<em>&gt;
</em><br>
<em>&gt; <a href="http://hanson.gmu.edu/workingpapers.html">http://hanson.gmu.edu/workingpapers.html</a>
</em><br>
<em>&gt; [NOTE: Go to the page and scroll down to the title below then click it to
</em><br>
<em>&gt; open the actual PDF file.]
</em><br>
<em>&gt;
</em><br>
<em>&gt; Economic Growth Given Machine Intelligence, Aug. '98
</em><br>
<em>&gt;
</em><br>
<em>&gt; A simple exogenous growth model gives conservative estimates of
</em><br>
<em>&gt; the economic
</em><br>
<em>&gt; implications of machine intelligence. Machines complement human labor when
</em><br>
<em>&gt; they become more productive at the jobs they perform, but machines also
</em><br>
<em>&gt; substitute for human labor by taking over human jobs. At first, expensive
</em><br>
<em>&gt; hardware and software does only the few jobs where computers have the
</em><br>
<em>&gt; strongest advantage over humans. Eventually, computers do most jobs. At
</em><br>
<em>&gt; first, complementary effects dominate, and human wages rise with computer
</em><br>
<em>&gt; productivity. But eventually substitution can dominate, making
</em><br>
<em>&gt; wages fall as
</em><br>
<em>&gt; fast as computer prices now do. An intelligence population explosion makes
</em><br>
<em>&gt; per-intelligence consumption fall this fast, while economic growth rates
</em><br>
<em>&gt; rise by an order of magnitude or more. These results are robust to
</em><br>
<em>&gt; automating incrementally, and to distinguishing hardware, software, and
</em><br>
<em>&gt; human capital from other forms of capital.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Regards,
</em><br>
<em>&gt; Michael LaTorra
</em><br>
<em>&gt; <a href="mailto:mike99@lascruces.com?Subject=RE:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">mike99@lascruces.com</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">owner-sl4@sysopmind.com</a> [mailto:<a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">owner-sl4@sysopmind.com</a>]On Behalf
</em><br>
<em>&gt; Of Ben Goertzel
</em><br>
<em>&gt; Sent: Saturday, September 30, 2000 7:45 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sysopmind.com?Subject=RE:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">sl4@sysopmind.com</a>
</em><br>
<em>&gt; Subject: RE: About that E-mail:...
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Here's another point
</em><br>
<em>&gt;
</em><br>
<em>&gt; If the first real AI is a commercial enterprise, it'll be making people
</em><br>
<em>&gt; money
</em><br>
<em>&gt;
</em><br>
<em>&gt; Everyone will own stock in real AI ... it'll be a huge popular
</em><br>
<em>&gt; sensation ...
</em><br>
<em>&gt; the financial aspects may drown out any troublesome philosophical
</em><br>
<em>&gt; aspects in
</em><br>
<em>&gt; the public
</em><br>
<em>&gt; mind...
</em><br>
<em>&gt;
</em><br>
<em>&gt; if they're making money off it in the short run, not many people
</em><br>
<em>&gt; will really
</em><br>
<em>&gt; be thinking
</em><br>
<em>&gt; about the long run -- this is typical homo sapiens shortsightedness, which
</em><br>
<em>&gt; will work in the favor
</em><br>
<em>&gt; of cosmic evolution in this case
</em><br>
<em>&gt;
</em><br>
<em>&gt; -- ben goertzel
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; -----Original Message-----
</em><br>
<em>&gt; &gt; From: <a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">owner-sl4@sysopmind.com</a> [mailto:<a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">owner-sl4@sysopmind.com</a>]On Behalf
</em><br>
<em>&gt; &gt; Of Eliezer S. Yudkowsky
</em><br>
<em>&gt; &gt; Sent: Saturday, September 30, 2000 9:23 PM
</em><br>
<em>&gt; &gt; To: <a href="mailto:sl4@sysopmind.com?Subject=RE:%20economic%20effects%20of%20AI%20(was%20RE:%20About%20that%20E-mail:...)">sl4@sysopmind.com</a>
</em><br>
<em>&gt; &gt; Subject: Re: About that E-mail:...
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Josh Yotty wrote:
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; I'm willing to bet the people working toward superhuman
</em><br>
<em>&gt; &gt; intelligence will be hunted down. Of course, the people hunting
</em><br>
<em>&gt; &gt; us down will be irrational, ignorant, narrowminded and stupid.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Be careful what you fear.  Sufficient amounts of hatred tend to
</em><br>
<em>&gt; turn into
</em><br>
<em>&gt; &gt; self-fulfilling prophecies... and if somebody really did try and
</em><br>
<em>&gt; &gt; hunt me down
</em><br>
<em>&gt; &gt; I sure wouldn't want to underestimate them.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; You'd be amazed at how often witch-hunts don't happen in First World
</em><br>
<em>&gt; &gt; countries.  I can't think of anything I ought to be doing in advance to
</em><br>
<em>&gt; &gt; prepare for the possibility of violent protesters, so I don't
</em><br>
<em>&gt; &gt; intend to worry
</em><br>
<em>&gt; &gt; excessively over the possibility until it starts actually
</em><br>
<em>&gt; &gt; happening.  There
</em><br>
<em>&gt; &gt; are essentially two strategies to deal with anti-technology
</em><br>
<em>&gt; &gt; crusades; you can
</em><br>
<em>&gt; &gt; try to run quietly and unobtrusively, or you can try for a
</em><br>
<em>&gt; pro-technology
</em><br>
<em>&gt; &gt; crusade.  I've observed that ordinary people tend to grasp the
</em><br>
<em>&gt; &gt; Singularity on
</em><br>
<em>&gt; &gt; the first try; it's the people who think they're intellectuals
</em><br>
<em>&gt; &gt; that you have
</em><br>
<em>&gt; &gt; to watch out for - so the second possibility is actually
</em><br>
<em>&gt; &gt; plausible.  I don't
</em><br>
<em>&gt; &gt; know if running quietly is plausible - it depends on how long it
</em><br>
<em>&gt; &gt; takes to get
</em><br>
<em>&gt; &gt; to a Singularity.  It's starting to look as if we don't bring the
</em><br>
<em>&gt; &gt; issue into
</em><br>
<em>&gt; &gt; the public eye, Bill Joy will.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Presently, I think it's not too much to hope for that the
</em><br>
<em>&gt; future will not
</em><br>
<em>&gt; &gt; contain anti-AI terrorist organizations.  There are anti-GM groups and
</em><br>
<em>&gt; &gt; antiabortion groups, but it's harder to get public sympathy for
</em><br>
<em>&gt; a violent
</em><br>
<em>&gt; &gt; crusade against something that's only a possibility - I hope.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; If we do bring the issue into the public eye, turning it into an
</em><br>
<em>&gt; &gt; elitist issue
</em><br>
<em>&gt; &gt; isn't really going to help.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; --              --              --              --              --
</em><br>
<em>&gt; &gt; Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt; &gt; Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0113.html">E. Shaun Russell: "Re: About that E-mail:..."</a>
<li><strong>Previous message:</strong> <a href="0111.html">Ben Goertzel: "RE: economic effects of AI (was RE: About that E-mail:...)"</a>
<li><strong>In reply to:</strong> <a href="0111.html">Ben Goertzel: "RE: economic effects of AI (was RE: About that E-mail:...)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0010/0115.html">Ben Goertzel: "RE: economic effects of AI (was RE: About that E-mail:...)"</a>
<li><strong>Reply:</strong> <a href="../0010/0115.html">Ben Goertzel: "RE: economic effects of AI (was RE: About that E-mail:...)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#112">[ date ]</a>
<a href="index.html#112">[ thread ]</a>
<a href="subject.html#112">[ subject ]</a>
<a href="author.html#112">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
