<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Normative Reasoning: A Siren Song?</title>
<meta name="Author" content="Sebastian Hagen (sebastian_hagen@gmx.de)">
<meta name="Subject" content="Re: Normative Reasoning: A Siren Song?">
<meta name="Date" content="2004-09-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Normative Reasoning: A Siren Song?</h1>
<!-- received="Sat Sep 25 13:23:03 2004" -->
<!-- isoreceived="20040925192303" -->
<!-- sent="Sat, 25 Sep 2004 21:22:56 +0200" -->
<!-- isosent="20040925192256" -->
<!-- name="Sebastian Hagen" -->
<!-- email="sebastian_hagen@gmx.de" -->
<!-- subject="Re: Normative Reasoning: A Siren Song?" -->
<!-- id="4155C590.8070605@gmx.de" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="Normative Reasoning: A Siren Song?" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Sebastian Hagen (<a href="mailto:sebastian_hagen@gmx.de?Subject=Re:%20Normative%20Reasoning:%20A%20Siren%20Song?"><em>sebastian_hagen@gmx.de</em></a>)<br>
<strong>Date:</strong> Sat Sep 25 2004 - 13:22:56 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9854.html">Aleksei Riikonen: "Re: The Future of Human Evolution"</a>
<li><strong>Previous message:</strong> <a href="9852.html">Maru: "Re: My new sf saga featuring the Singularity and FAI concepts is nearing completion"</a>
<li><strong>Maybe in reply to:</strong> <a href="9841.html">Michael Wilson: "Normative Reasoning: A Siren Song?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9873.html">Peter C. McCluskey: "Re: Normative Reasoning: A Siren Song?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9853">[ date ]</a>
<a href="index.html#9853">[ thread ]</a>
<a href="subject.html#9853">[ subject ]</a>
<a href="author.html#9853">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Nick Bostrom wrote:
<br>
<em>&gt; On a vaguely related note, I've had a half-written essay lying around 
</em><br>
<em>&gt; since 2001 on how some &quot;upward&quot; evolutionary trajectories could lead to 
</em><br>
<em>&gt; dystopian outcomes, which I've now finally completed: 
</em><br>
<em>&gt; <a href="http://www.nickbostrom.com/fut/evolution.html">http://www.nickbostrom.com/fut/evolution.html</a>.
</em><br>
<em>&gt; 
</em><br>
note: the following quotes are taken from this essay.
<br>
<p>&quot;And we can surely assume that at least some current human individuals
<br>
would upload if they could, would make many copies of themselves if
<br>
they were uploads, and would happily embrace outsourcing or forego
<br>
leisure if they could thereby increase their fitness.&quot;
<br>
<p>Disclaimer: I would probably forego leisure, and quite possibly embrace
<br>
outsourcing if I had the chance to reliably modify my own source code.
<br>
Making a lot of active copies does not seem like an effective strategy
<br>
to me, aside from making (inactive) backups I would probably use
<br>
available computronium mostly to expand my own mind.
<br>
Copying would probably be effective if there were several &quot;lumps&quot; of
<br>
available computronium that had significant communication delays between
<br>
them, but lacking that just expanding the own mind appears more effective.
<br>
Still, given the chance, self-optimizing myself to a
<br>
&quot;All-Work-And-No-Fun&quot;-mind may well be among the first things I'd do
<br>
after uploading, so some of my possible future selves would likely be
<br>
penalized by an implementation of your suggestions. My opinion on those
<br>
suggestions is therefore probably biased.
<br>
<p>&quot;Some highly complex entities such as multinational corporations and
<br>
nation states contain human beings as constituents. Yet we usually
<br>
assign these high-level complexes only instrumental value.
<br>
Corporations and states are not (it is generally assumed) conscious;
<br>
they cannot feel phenomenal pain or pleasure. We think they are of
<br>
value only to the extent that they serve human needs.&quot;
<br>
<p>I don't think that this is the case simply because these entities have a
<br>
significantly higher complexity than human minds. Since human beings do
<br>
have rather slow and in many ways ineffective input/output channels,
<br>
communication of humans with their environment is a lot more limited
<br>
than communication within their own mind. As human beings are the most
<br>
important constituents of the mentioned large entities, different parts
<br>
of those organizations can only communicate relatively inefficiently 
<br>
with each other, and are often out of sync. This situation is compounded 
<br>
by many humans regarding their involvement in those organizations only 
<br>
as a subgoal, as opposed to a goal on the highest level of their goal 
<br>
system; in other words, they don't really want to have their goal system 
<br>
synced to the goals of the organization either. This severely limits the 
<br>
effectiveness of those organizations. Different parts of them may, and 
<br>
often do, work against each other, based on different availability of 
<br>
information or different goals on the level of those parts.
<br>
Most of these characteristics do not necessarily apply to intelligent
<br>
processes with significantly higher-than-human complexity, and since
<br>
they impair efficiency they are subject to being optimized away.
<br>
<p>&quot;We can thus imagine a technologically highly advanced society,
<br>
containing many sorts of complex structures, some of which are much
<br>
smarter and more intricate than anything that exists today, in which
<br>
there would nevertheless be a complete absence of any type of being
<br>
whose welfare has moral significance.&quot;
<br>
<p>I don't understand this last statement. Why do we have the right to 
<br>
declare absence of moral significance without even being capable of
<br>
understanding either the society described, or understanding objective 
<br>
morality (if there is anything like that)? The society may be decidedly 
<br>
unhuman, but this alone is imho not any justification of declaring it 
<br>
morally insignificant. What is so special about humans that makes them 
<br>
morally significant in contrast to the other systems you describe?
<br>
<p>&quot;Perhaps what will maximize fitness in the future will be nothing but
<br>
non-stop high-intensity drudgery, work of a drab and repetitive
<br>
nature, aimed at improving the eighth decimal of some economic output
<br>
measure. Even if the workers selected for in this scenario were
<br>
conscious, the resulting world would still be radically impoverished in
<br>
terms of the qualities that give value to life.&quot;
<br>
<p>Why? I don't understand why the activities mentioned before the quoted 
<br>
part (&quot;humor, love, game-playing, art, sex, dancing, social 
<br>
conversation, philosophy, literature, scientific discovery, food and 
<br>
drink, friendship, parenting, sport&quot;) are relevant for the value of 
<br>
human life. At least some of them are useful in current human societies 
<br>
for practical reasons, but I don't think that this justifies an 
<br>
assumption of them having objective moral significance.
<br>
<p><p>&nbsp;From the perspective of an agent, that is striving to be
<br>
non-eudaemonic,(me) the proposed implementation looks like something 
<br>
that could destroy a lot of efficiency at problem-solving. If a
<br>
(renormalized) Collective Volition came to the conclusion that this
<br>
is a good idea I'd respect it (since it would have been made by 
<br>
transhuman minds that originally would have been human ones), but 
<br>
human-level minds forcing this kind of two-class-society on transhuman 
<br>
ones appears like a very bad idea to me.
<br>
<p>Sebastian Hagen
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9854.html">Aleksei Riikonen: "Re: The Future of Human Evolution"</a>
<li><strong>Previous message:</strong> <a href="9852.html">Maru: "Re: My new sf saga featuring the Singularity and FAI concepts is nearing completion"</a>
<li><strong>Maybe in reply to:</strong> <a href="9841.html">Michael Wilson: "Normative Reasoning: A Siren Song?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9873.html">Peter C. McCluskey: "Re: Normative Reasoning: A Siren Song?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9853">[ date ]</a>
<a href="index.html#9853">[ thread ]</a>
<a href="subject.html#9853">[ subject ]</a>
<a href="author.html#9853">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:48 MDT
</em></small></p>
</body>
</html>
