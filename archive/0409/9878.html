<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The Future of Human Evolution</title>
<meta name="Author" content="Randall Randall (randall@randallsquared.com)">
<meta name="Subject" content="Re: The Future of Human Evolution">
<meta name="Date" content="2004-09-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The Future of Human Evolution</h1>
<!-- received="Wed Sep 29 01:36:16 2004" -->
<!-- isoreceived="20040929073616" -->
<!-- sent="Wed, 29 Sep 2004 03:36:12 -0400" -->
<!-- isosent="20040929073612" -->
<!-- name="Randall Randall" -->
<!-- email="randall@randallsquared.com" -->
<!-- subject="Re: The Future of Human Evolution" -->
<!-- id="3D137BD5-11EA-11D9-BFF8-000A95A0F1E8@randallsquared.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="41598A9C.40005@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Randall Randall (<a href="mailto:randall@randallsquared.com?Subject=Re:%20The%20Future%20of%20Human%20Evolution"><em>randall@randallsquared.com</em></a>)<br>
<strong>Date:</strong> Wed Sep 29 2004 - 01:36:12 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9879.html">Randall Randall: "Re: Running away"</a>
<li><strong>Previous message:</strong> <a href="9877.html">Robin Lee Powell: "Re: The Future of Human Evolution"</a>
<li><strong>In reply to:</strong> <a href="9872.html">Eliezer Yudkowsky: "Re: The Future of Human Evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9883.html">Thomas Buckner: "Re: The Future of Human Evolution"</a>
<li><strong>Reply:</strong> <a href="9883.html">Thomas Buckner: "Re: The Future of Human Evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9878">[ date ]</a>
<a href="index.html#9878">[ thread ]</a>
<a href="subject.html#9878">[ subject ]</a>
<a href="author.html#9878">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Sep 28, 2004, at 12:00 PM, Eliezer Yudkowsky wrote:
<br>
<em>&gt; Suppose you pack your bags and run away at .99c.  I know too little to 
</em><br>
<em>&gt; compute the fraction of UFAIs randomly selected from the class that 
</em><br>
<em>&gt; meddling dabblers are likely to create, that would run after you at 
</em><br>
<em>&gt; .995c.  But I guess that the fraction is very high.  Why would a 
</em><br>
<em>&gt; paperclip maximizer do this?  Because you might compete with it for 
</em><br>
<em>&gt; paperclip resources if you escaped.  If you have any hope of creating 
</em><br>
<em>&gt; an FAI on board your fleeing vessel, the future of almost any UFAI 
</em><br>
<em>&gt; that doesn't slip out of the universe entirely (and those might not 
</em><br>
<em>&gt; present a danger in the first place) is more secure if it kills you 
</em><br>
<em>&gt; than if it lets you flee.  The faster you run, the less subjective 
</em><br>
<em>&gt; time you have on board the ship before someone catches up with you, 
</em><br>
<em>&gt; owing to lightspeed effects.
</em><br>
<p>This assumes that there is a significant chance of being able to
<br>
find and kill all escapees.  Given the apparent ease of blending
<br>
in with the cosmic background over a medium-size angle (say, one
<br>
radian), the chances of finding all escapees seem quite slim.  A
<br>
maximizer of whatever is therefore only going to increase the
<br>
chances of blowback by killing many or most escapees.  This game
<br>
has interesting parallels with current international games.
<br>
<p><em>&gt; Suppose it doesn't run after you.  In that case, if more than one 
</em><br>
<em>&gt; group escapes, say, 10 groups, then any one of them can also 
</em><br>
<em>&gt; potentially create an UFAI that will chase after you at .995c.
</em><br>
<p>But UFAIs created light years away are not huge threats, since
<br>
they need to know you're there, and that information would seem
<br>
scarce.
<br>
<p><em>&gt; Suppose only one group escapes.  If you have any kind of potential for 
</em><br>
<em>&gt; growth, any ability to colonize the galaxy and turn into something 
</em><br>
<em>&gt; interesting, you *still* have to solve the FAI problem before you can 
</em><br>
<em>&gt; do it.
</em><br>
<p>Given a graphic example of the dangers of AI development, an
<br>
escapee group would probably pursue other approaches, such as
<br>
upload enhancement, which can, at least, start with a known
<br>
ethical upload (oneself, in the limit).
<br>
<p><em>&gt; Running away is a good strategy for dealing with bioviruses and 
</em><br>
<em>&gt; military nanotech.  AI rather less so.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I also dispute that you would have .99c-capable escape vehicles 
</em><br>
<em>&gt; *immediately* after nanotech is developed.  It seems likely to me that 
</em><br>
<em>&gt; years, perhaps a decade or more, would lapse between the development 
</em><br>
<em>&gt; of absurdly huge nanocomputers and workable escape vehicles.
</em><br>
<p>I actually was only thinking .10c vehicles.  Get beyond the
<br>
mass-dense part of the solar area and coast, radiating waste
<br>
heat forward as much as possible.  If we pause in the Oort
<br>
or asteroid belt, producing a few million decoys per actual
<br>
escape vehicle seems well within the capability of relatively
<br>
dumb software.
<br>
<p><em>&gt; It's not just the design, it's the debugging.  Computers you can tile. 
</em><br>
<em>&gt;  Of course there'll also be a lag between delivery of nanocomputers 
</em><br>
<em>&gt; and when an UFAI pops out.  I merely point out the additional problem.
</em><br>
<p>One of my assumptions is that generic optimizers are difficult
<br>
enough that some sort of genetic algorithm will be required to
<br>
produce the first one.  I realize we differ on this, since you
<br>
believe you have a solution that doesn't require GA.
<br>
<p><pre>
--
Randall Randall &lt;<a href="mailto:randall@randallsquared.com?Subject=Re:%20The%20Future%20of%20Human%20Evolution">randall@randallsquared.com</a>&gt;
&quot;And no practical definition of freedom would be complete
  without the freedom to take the consequences. Indeed, it
  is the freedom upon which all the others are based.&quot;
  - Terry Pratchett, _Going Postal_
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9879.html">Randall Randall: "Re: Running away"</a>
<li><strong>Previous message:</strong> <a href="9877.html">Robin Lee Powell: "Re: The Future of Human Evolution"</a>
<li><strong>In reply to:</strong> <a href="9872.html">Eliezer Yudkowsky: "Re: The Future of Human Evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9883.html">Thomas Buckner: "Re: The Future of Human Evolution"</a>
<li><strong>Reply:</strong> <a href="9883.html">Thomas Buckner: "Re: The Future of Human Evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9878">[ date ]</a>
<a href="index.html#9878">[ thread ]</a>
<a href="subject.html#9878">[ subject ]</a>
<a href="author.html#9878">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:49 MDT
</em></small></p>
</body>
</html>
