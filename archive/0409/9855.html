<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The Future of Human Evolution</title>
<meta name="Author" content="Sebastian Hagen (sebastian_hagen@gmx.de)">
<meta name="Subject" content="Re: The Future of Human Evolution">
<meta name="Date" content="2004-09-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The Future of Human Evolution</h1>
<!-- received="Sun Sep 26 06:47:24 2004" -->
<!-- isoreceived="20040926124724" -->
<!-- sent="Sun, 26 Sep 2004 14:47:18 +0200" -->
<!-- isosent="20040926124718" -->
<!-- name="Sebastian Hagen" -->
<!-- email="sebastian_hagen@gmx.de" -->
<!-- subject="Re: The Future of Human Evolution" -->
<!-- id="4156BA56.1090204@gmx.de" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="1096148901.4155e7a5e32f2@www1.helsinki.fi" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Sebastian Hagen (<a href="mailto:sebastian_hagen@gmx.de?Subject=Re:%20The%20Future%20of%20Human%20Evolution"><em>sebastian_hagen@gmx.de</em></a>)<br>
<strong>Date:</strong> Sun Sep 26 2004 - 06:47:18 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9856.html">Eliezer Yudkowsky: "Re: The Future of Human Evolution"</a>
<li><strong>Previous message:</strong> <a href="9854.html">Aleksei Riikonen: "Re: The Future of Human Evolution"</a>
<li><strong>In reply to:</strong> <a href="9854.html">Aleksei Riikonen: "Re: The Future of Human Evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9856.html">Eliezer Yudkowsky: "Re: The Future of Human Evolution"</a>
<li><strong>Reply:</strong> <a href="9856.html">Eliezer Yudkowsky: "Re: The Future of Human Evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9855">[ date ]</a>
<a href="index.html#9855">[ thread ]</a>
<a href="subject.html#9855">[ subject ]</a>
<a href="author.html#9855">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Aleksei Riikonen wrote:
<br>
<em>&gt; As an agent striving to be non-eudaemonic, could you elaborate on what are 
</em><br>
<em>&gt; the things you value? (Non-instrumentally, that is.)
</em><br>
The best answer I can give is 'whatever has objective moral relevance'.
<br>
Unfortunately I don't know what exactly qualifies for that, so currently
<br>
the active subgoal is to get more intelligence applied to the task of
<br>
finding out. Should there be in fact nothing with objective moral
<br>
relevance, what I do is per definition morally irrelevant, so I don't
<br>
have to consider this possibility in calculating the expected utility of
<br>
my actions.
<br>
This rationale has been copied from
<br>
&lt;<a href="http://yudkowsky.net/tmol-faq/logic.html#meaning">http://yudkowsky.net/tmol-faq/logic.html#meaning</a>&gt;, but (since I haven't
<br>
found anything that appears to be better) it does represent my current
<br>
opinion on the matter.
<br>
This opinion may well be horribly flawed; correctly solving a complex
<br>
problem requires getting everything (important) right, while disproving
<br>
a proposed solution only requires finding one critical mistake.
<br>
<p><em>&gt; Note that in Bostrom's essay, even consciousness itself was classified as 
</em><br>
<em>&gt; eudaemonic. (At least in the case where the supposition, that consciousness 
</em><br>
<em>&gt; isn't necessary for maximizing the efficiency of any optimizing or problem-
</em><br>
<em>&gt; solving process, is true.) Assuming that we are all using the same 
</em><br>
<em>&gt; terminology here, it would seem that consciousness is morally irrelevant to 
</em><br>
<em>&gt; you.
</em><br>
I don't sufficiently understand consciousness to meaningfully refer to
<br>
it when talking about moral. I don't think that what I would
<br>
(intuitively) call 'consciousness' is by definition eudaemonic, but
<br>
since I don't have any clear ideas about the concept that's a moot point.
<br>
<p><em>&gt; The only somewhat relevant drawback with regard to his suggestions that 
</em><br>
<em>&gt; comes to my mind as of now, is that we would indeed be sacrificing some 
</em><br>
<em>&gt; problem-solving efficiency by being eudaumonic. This might pose a problem 
</em><br>
<em>&gt; in some scenarios in which we are competing with external agents presently 
</em><br>
<em>&gt; unknown to us (e.g. extraterrestrial post-singularity civilizations). It 
</em><br>
<em>&gt; would seem like quite the non-trivial question, whether the probability 
</em><br>
<em>&gt; that we are in fact situated in such a scenario, is non-infinitesimal.
</em><br>
Additionally, even if that were the case, it would be questionable
<br>
whether the difference in efficiency is relevant for the outcome of the
<br>
conflict, assuming there is one. Considering cosmic timescales it seems
<br>
highly unlikely that the two civilizations would reach superintelligence
<br>
at roughly the same time (say, within a few hundred years).
<br>
Since one of them likely has a lot more time to establish an
<br>
infrastructure and perform research before the SI-complexes encounter
<br>
each other, lack of efficiency caused by preferring eudaemonic agents
<br>
may well be completely irrelevant to the outcome.
<br>
There are other relevant aspects like possibilities to enforce a
<br>
stand-off given a minimum technology level, but this has been discussed
<br>
before on sl4 and isn't really the topic of this thread.
<br>
<p><em>&gt; So let's strive to build something FAIish and find out ;)
</em><br>
Sure - though considering the differences in world-models and goals
<br>
between present-day humans we may not find anything that we can agree on
<br>
qualifying as FAIish.
<br>
<p>Sebastian Hagen
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9856.html">Eliezer Yudkowsky: "Re: The Future of Human Evolution"</a>
<li><strong>Previous message:</strong> <a href="9854.html">Aleksei Riikonen: "Re: The Future of Human Evolution"</a>
<li><strong>In reply to:</strong> <a href="9854.html">Aleksei Riikonen: "Re: The Future of Human Evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9856.html">Eliezer Yudkowsky: "Re: The Future of Human Evolution"</a>
<li><strong>Reply:</strong> <a href="9856.html">Eliezer Yudkowsky: "Re: The Future of Human Evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9855">[ date ]</a>
<a href="index.html#9855">[ thread ]</a>
<a href="subject.html#9855">[ subject ]</a>
<a href="author.html#9855">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:48 MDT
</em></small></p>
</body>
</html>
