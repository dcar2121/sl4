<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The Future of Human Evolution</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: The Future of Human Evolution">
<meta name="Date" content="2004-09-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The Future of Human Evolution</h1>
<!-- received="Tue Sep 28 10:00:32 2004" -->
<!-- isoreceived="20040928160032" -->
<!-- sent="Tue, 28 Sep 2004 12:00:28 -0400" -->
<!-- isosent="20040928160028" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: The Future of Human Evolution" -->
<!-- id="41598A9C.40005@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="F8313EA8-1159-11D9-ADC2-000A95A0F1E8@randallsquared.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20The%20Future%20of%20Human%20Evolution"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Sep 28 2004 - 10:00:28 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9873.html">Peter C. McCluskey: "Re: Normative Reasoning: A Siren Song?"</a>
<li><strong>Previous message:</strong> <a href="9871.html">Randall Randall: "Re: The Future of Human Evolution"</a>
<li><strong>In reply to:</strong> <a href="9871.html">Randall Randall: "Re: The Future of Human Evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9874.html">Dani Eder: "Re: Running away"</a>
<li><strong>Reply:</strong> <a href="9874.html">Dani Eder: "Re: Running away"</a>
<li><strong>Reply:</strong> <a href="9878.html">Randall Randall: "Re: The Future of Human Evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9872">[ date ]</a>
<a href="index.html#9872">[ thread ]</a>
<a href="subject.html#9872">[ subject ]</a>
<a href="author.html#9872">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Randall Randall wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I agree that the debate is about the relative order of
</em><br>
<em>&gt; working examples, but I think that the relative dangers
</em><br>
<em>&gt; involved are quite relevant.  In particular, while newly
</em><br>
<em>&gt; built nanofactories will certainly allow a brute forcing
</em><br>
<em>&gt; of the AI problem at some point, it seems clear that
</em><br>
<em>&gt; building vehicles sufficient to leave the vicinity will
</em><br>
<em>&gt; be effective immediately (essentially), as that problem
</em><br>
<em>&gt; is well understood, unlike the AI problem.  In any case,
</em><br>
<em>&gt; it seems like a simple grid choice to me, where one axis
</em><br>
<em>&gt; is limit on travel (FTL or STL), and the other which
</em><br>
<em>&gt; technology comes to fruition first (MNT or AI).  In an
</em><br>
<em>&gt; FTL world, FAI is the only apparent hope of surviving the
</em><br>
<em>&gt; advent of AI.  In an STL world, however, MNT can be a
</em><br>
<em>&gt; sufficient technology for surviving unFriendly AI, for
</em><br>
<em>&gt; some.  Since we appear to live in an STL world, I prefer
</em><br>
<em>&gt; MNT first.
</em><br>
<p>Suppose you pack your bags and run away at .99c.  I know too little to 
<br>
compute the fraction of UFAIs randomly selected from the class that 
<br>
meddling dabblers are likely to create, that would run after you at .995c. 
<br>
&nbsp;&nbsp;But I guess that the fraction is very high.  Why would a paperclip 
<br>
maximizer do this?  Because you might compete with it for paperclip 
<br>
resources if you escaped.  If you have any hope of creating an FAI on board 
<br>
your fleeing vessel, the future of almost any UFAI that doesn't slip out of 
<br>
the universe entirely (and those might not present a danger in the first 
<br>
place) is more secure if it kills you than if it lets you flee.  The faster 
<br>
you run, the less subjective time you have on board the ship before someone 
<br>
catches up with you, owing to lightspeed effects.
<br>
<p>Suppose it doesn't run after you.  In that case, if more than one group 
<br>
escapes, say, 10 groups, then any one of them can also potentially create 
<br>
an UFAI that will chase after you at .995c.
<br>
<p>Suppose only one group escapes.  If you have any kind of potential for 
<br>
growth, any ability to colonize the galaxy and turn into something 
<br>
interesting, you *still* have to solve the FAI problem before you can do it.
<br>
<p>Running away is a good strategy for dealing with bioviruses and military 
<br>
nanotech.  AI rather less so.
<br>
<p>I also dispute that you would have .99c-capable escape vehicles 
<br>
*immediately* after nanotech is developed.  It seems likely to me that 
<br>
years, perhaps a decade or more, would lapse between the development of 
<br>
absurdly huge nanocomputers and workable escape vehicles.  It's not just 
<br>
the design, it's the debugging.  Computers you can tile.  Of course 
<br>
there'll also be a lag between delivery of nanocomputers and when an UFAI 
<br>
pops out.  I merely point out the additional problem.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9873.html">Peter C. McCluskey: "Re: Normative Reasoning: A Siren Song?"</a>
<li><strong>Previous message:</strong> <a href="9871.html">Randall Randall: "Re: The Future of Human Evolution"</a>
<li><strong>In reply to:</strong> <a href="9871.html">Randall Randall: "Re: The Future of Human Evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9874.html">Dani Eder: "Re: Running away"</a>
<li><strong>Reply:</strong> <a href="9874.html">Dani Eder: "Re: Running away"</a>
<li><strong>Reply:</strong> <a href="9878.html">Randall Randall: "Re: The Future of Human Evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9872">[ date ]</a>
<a href="index.html#9872">[ thread ]</a>
<a href="subject.html#9872">[ subject ]</a>
<a href="author.html#9872">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:49 MDT
</em></small></p>
</body>
</html>
