<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The Future of Human Evolution</title>
<meta name="Author" content="Jef Allbright (jef@jefallbright.net)">
<meta name="Subject" content="Re: The Future of Human Evolution">
<meta name="Date" content="2004-09-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The Future of Human Evolution</h1>
<!-- received="Sun Sep 26 13:07:39 2004" -->
<!-- isoreceived="20040926190739" -->
<!-- sent="Sun, 26 Sep 2004 12:07:35 -0700" -->
<!-- isosent="20040926190735" -->
<!-- name="Jef Allbright" -->
<!-- email="jef@jefallbright.net" -->
<!-- subject="Re: The Future of Human Evolution" -->
<!-- id="41571377.5090609@jefallbright.net" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="4156F3CA.1050008@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Jef Allbright (<a href="mailto:jef@jefallbright.net?Subject=Re:%20The%20Future%20of%20Human%20Evolution"><em>jef@jefallbright.net</em></a>)<br>
<strong>Date:</strong> Sun Sep 26 2004 - 13:07:35 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9858.html">Sebastian Hagen: "Re: The Future of Human Evolution"</a>
<li><strong>Previous message:</strong> <a href="9856.html">Eliezer Yudkowsky: "Re: The Future of Human Evolution"</a>
<li><strong>In reply to:</strong> <a href="9856.html">Eliezer Yudkowsky: "Re: The Future of Human Evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9858.html">Sebastian Hagen: "Re: The Future of Human Evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9857">[ date ]</a>
<a href="index.html#9857">[ thread ]</a>
<a href="subject.html#9857">[ subject ]</a>
<a href="author.html#9857">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer Yudkowsky wrote:
<br>
<p><em>&gt; Sebastian Hagen wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; Aleksei Riikonen wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; As an agent striving to be non-eudaemonic, could you elaborate on 
</em><br>
<em>&gt;&gt;&gt; what are the things you value? (Non-instrumentally, that is.)
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; The best answer I can give is 'whatever has objective moral relevance'.
</em><br>
<em>&gt;&gt; Unfortunately I don't know what exactly qualifies for that, so currently
</em><br>
<em>&gt;&gt; the active subgoal is to get more intelligence applied to the task of
</em><br>
<em>&gt;&gt; finding out. Should there be in fact nothing with objective moral
</em><br>
<em>&gt;&gt; relevance, what I do is per definition morally irrelevant, so I don't
</em><br>
<em>&gt;&gt; have to consider this possibility in calculating the expected utility of
</em><br>
<em>&gt;&gt; my actions.
</em><br>
<em>&gt;&gt; This rationale has been copied from
</em><br>
<em>&gt;&gt; &lt;<a href="http://yudkowsky.net/tmol-faq/logic.html#meaning">http://yudkowsky.net/tmol-faq/logic.html#meaning</a>&gt;, but (since I haven't
</em><br>
<em>&gt;&gt; found anything that appears to be better) it does represent my current
</em><br>
<em>&gt;&gt; opinion on the matter.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; TMOL is superceded by CFAI which in turn is superceded by 
</em><br>
<em>&gt; <a href="http://sl4.org/wiki/DialogueOnFriendliness">http://sl4.org/wiki/DialogueOnFriendliness</a> and 
</em><br>
<em>&gt; <a href="http://sl4.org/wiki/CollectiveVolition">http://sl4.org/wiki/CollectiveVolition</a>.
</em><br>
<p><p>Which will likely be superseded by something along the lines of 
<br>
&quot;Rational Actualization&quot;.
<br>
<p>Eliezer, your current stumbling blocks involve (1) extrapolation fails 
<br>
to predict the future environment, including our own place in it, due to 
<br>
combinatorial explosion, cumulative error, and fundamentally bounded 
<br>
knowledge; and (2) while humanity is the standard by which we judge 
<br>
morality, this is encompassed by the bigger picture &quot;morality&quot; which is 
<br>
simply that what works, survives and grows.
<br>
<p>The next step in the journey will indeed involve the increasing 
<br>
awareness of the multi-vectored &quot;collective volition&quot; of humanity, but 
<br>
in the context of what works, focusing on effective principles to 
<br>
actualize our collective vision and drive toward an inherently 
<br>
unknowable future. 
<br>
<p>While humanity is the measure of our morality, current humanity is only 
<br>
a milepost along the way, and our current values will be seen as 
<br>
rational only within their limited context, not to be applied outside 
<br>
that context due to their limitations.  And we can't successfully 
<br>
extrapolate from limited data if we want solutions that apply to a 
<br>
larger context.
<br>
<p>We can influence the direction, but not the destination of our path 
<br>
according to our &quot;moral&quot; choices.  We can do this by applying increasing 
<br>
awareness of ourselves, our environment, and the principles that 
<br>
describe growth, but it's open-ended, not something amenable to 
<br>
extrapolation and control.
<br>
<p>Why is this important?  Because we can significantly improve the quality 
<br>
of the journey by preparing the right tools now.
<br>
<p>- Jef
<br>
<a href="http://www.jefallbright.net">http://www.jefallbright.net</a>
<br>
<p><p><p><p><p><em>&gt;
</em><br>
<em>&gt; Roughly, the problem with the 'objective' criterion is that to get an 
</em><br>
<em>&gt; objective answer you need a well-defined question, where any question 
</em><br>
<em>&gt; that is the output of a well-defined algorithm may be taken as 
</em><br>
<em>&gt; well-defined.  In the TMOL FAQ you've got an algorithm that searches 
</em><br>
<em>&gt; for a solution and has no criterion for recognizing a solution.  This, 
</em><br>
<em>&gt; in retrospect, was kinda silly.  If you actually implement the stated 
</em><br>
<em>&gt; logic or something as close to the stated logic as you can get, what 
</em><br>
<em>&gt; it will *actually* do (assuming you get it to work at all) is treat, 
</em><br>
<em>&gt; as its effective supergoal, the carrying out of operations that have 
</em><br>
<em>&gt; been shown to be generally useful for subproblems, relative to some 
</em><br>
<em>&gt; generalization operator.  For example, it might sort everything in the 
</em><br>
<em>&gt; universe into alphabetical order (as a supergoal) because sorting 
</em><br>
<em>&gt; things into alphabetical order was found to often be useful on 
</em><br>
<em>&gt; algorithmic problems (as subgoals).  In short, the damn thing don't work.
</em><br>
<em>&gt;
</em><br>
<em>&gt; We know how morality evolved.  We know how it's implemented in 
</em><br>
<em>&gt; humans.  We know how human psychology treats it.  ('We' meaning 
</em><br>
<em>&gt; humanity's accumulated scientific knowledge - I understand that not 
</em><br>
<em>&gt; everyone is familiar with the details.)  What's left to figure out?  
</em><br>
<em>&gt; What information is missing?  If a superintelligence could figure out 
</em><br>
<em>&gt; an answer, why shouldn't we?
</em><br>
<em>&gt;
</em><br>
<em>&gt; The rough idea behind the 'volitional extrapolation' concept is that 
</em><br>
<em>&gt; you can get an answer to a question that, to a human, appears poorly 
</em><br>
<em>&gt; defined, so long as the human actually contains the roots of an 
</em><br>
<em>&gt; answer; criteria that aren't obviously relevant - that haven't yet 
</em><br>
<em>&gt; reached out and connected themselves to the problem - but that would 
</em><br>
<em>&gt; connect themselves to the problem given the right realizations.  Like, 
</em><br>
<em>&gt; you know 'even numbers are green', and you want to know 'what color is 
</em><br>
<em>&gt; 14?', but you haven't applied the computing power to figure out that 
</em><br>
<em>&gt; 14 is even.  That kind of question can turn out to be well-defined, 
</em><br>
<em>&gt; even if, at the moment, you're staring at 14 with absolutely no idea 
</em><br>
<em>&gt; how to determine what color it is.  But the question, if not the 
</em><br>
<em>&gt; answer, has to be implicit in the asker.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If you claim that you know absolutely nothing about objective 
</em><br>
<em>&gt; morality, how would you look at an AI, or any other well-defined 
</em><br>
<em>&gt; process, and claim that it did (or for that matter, did not) compute 
</em><br>
<em>&gt; an objective morality?  If you know absolutely nothing about objective 
</em><br>
<em>&gt; morality, where it comes from, what sort of thing it is, and so on, 
</em><br>
<em>&gt; then how do you know that 'Look in the Bible' is an unsatisfactory 
</em><br>
<em>&gt; justification for a morality?
</em><br>
<em>&gt;
</em><br>
<em>&gt; It all starts with humans.  You're the one who'd have to determine any 
</em><br>
<em>&gt; criterion for recognizing a morality, an algorithm for producing 
</em><br>
<em>&gt; morality, or an algorithm for producing an algorithm for producing 
</em><br>
<em>&gt; morality.  Why would any answer you recognized as reasonable be 
</em><br>
<em>&gt; non-eudaemonic?
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9858.html">Sebastian Hagen: "Re: The Future of Human Evolution"</a>
<li><strong>Previous message:</strong> <a href="9856.html">Eliezer Yudkowsky: "Re: The Future of Human Evolution"</a>
<li><strong>In reply to:</strong> <a href="9856.html">Eliezer Yudkowsky: "Re: The Future of Human Evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9858.html">Sebastian Hagen: "Re: The Future of Human Evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9857">[ date ]</a>
<a href="index.html#9857">[ thread ]</a>
<a href="subject.html#9857">[ subject ]</a>
<a href="author.html#9857">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:48 MDT
</em></small></p>
</body>
</html>
