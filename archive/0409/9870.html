<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Good old-fashioned CFAI</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Good old-fashioned CFAI">
<meta name="Date" content="2004-09-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Good old-fashioned CFAI</h1>
<!-- received="Tue Sep 28 07:17:02 2004" -->
<!-- isoreceived="20040928131702" -->
<!-- sent="Tue, 28 Sep 2004 09:16:57 -0400" -->
<!-- isosent="20040928131657" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Good old-fashioned CFAI" -->
<!-- id="41596449.2050907@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="4158F782.9070004@tutopia.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Good%20old-fashioned%20CFAI"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Sep 28 2004 - 07:16:57 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9871.html">Randall Randall: "Re: The Future of Human Evolution"</a>
<li><strong>Previous message:</strong> <a href="9869.html">Eliezer Yudkowsky: "Re: The Future of Human Evolution"</a>
<li><strong>In reply to:</strong> <a href="9868.html">Christian Rovner: "Good old-fashioned CFAI (was: The Future of Human Evolution)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9876.html">Christian Rovner: "Re: Good old-fashioned CFAI"</a>
<li><strong>Reply:</strong> <a href="9876.html">Christian Rovner: "Re: Good old-fashioned CFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9870">[ date ]</a>
<a href="index.html#9870">[ thread ]</a>
<a href="subject.html#9870">[ subject ]</a>
<a href="author.html#9870">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Christian Rovner wrote:
<br>
<em>&gt; Eliezer Yudkowsky wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; If it's not well-specified, you can't do it.  
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I understand that an AI steers its environment towards an optimal state 
</em><br>
<em>&gt; according to some utility function. Apparently you say that this 
</em><br>
<em>&gt; function must be well-specified if we want the AI to be predictably 
</em><br>
<em>&gt; Friendly.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This contradicts what I understood from CFAI, where you proposed the 
</em><br>
<em>&gt; creation of an AI that would improve its own utility function according 
</em><br>
<em>&gt; to the feedback provided by its programmers. This feedback implies a 
</em><br>
<em>&gt; meta-utility function which is unknown even to them, and it would be 
</em><br>
<em>&gt; gradually made explict thanks to the AI's superior intelligence. This 
</em><br>
<em>&gt; sounded like a good plan to me. What exactly is wrong about it?
</em><br>
<p>You would still need a well-specified description of what you were trying 
<br>
to do.  To give a flavor-only example (i.e., don't try this at home, it 
<br>
won't work even if you do everything right), suppose that the programmers 
<br>
are expected utility maximizers and Bayesians, and that the utility 
<br>
function U-h(x) of a human can be factored into the additive sum of 
<br>
subfunctions V1-h(x), V2-h(x)... V10-h(x).  We will suppose that U-h(x) is 
<br>
identical for all humans including the programmers, obviating many of the 
<br>
complex questions that enter into collective volition.  Suppose also that 
<br>
humans have no explicit knowledge of their own utility functions, and can 
<br>
only infer them by observing their own choices, or imagining choices, and 
<br>
trying to build a model of their own utility function.  They do, however, 
<br>
know the abstract fact that they are expected utility maximizers and that 
<br>
they possess some utility function U-h(x) that is the same for all humans 
<br>
and that is factorizable into a set of utility functions V-h(x), etc.
<br>
<p>And the humans are also capable (this is an extra specification, because a 
<br>
standard expected utility maximizer does not include this ability) of 
<br>
*abstracting* over uncertain states with uncertain utilities, and taking 
<br>
this into account in their planning.  For example, one might have an action 
<br>
A that leads to a future F about which we know nothing except that it has a 
<br>
utility of 32 - it might be any possible future such that it has a utility 
<br>
of 32.  As an example, human Fred must choose whether to place human Larry 
<br>
or alien Motie Jerry at the controls of a car.  By earlier assumption, all 
<br>
humans share the same utility function, and this is known to humans; Moties 
<br>
have a different utility function which is similar but not identical to the 
<br>
human utility function.  Even without visualizing all the possibilities 
<br>
that Larry or Jerry might encounter, even without visualizing *any* 
<br>
specific possibility, Fred will prefer to place Larry rather than Jerry in 
<br>
the driver's seat.  This sounds straightforward enough, but it requires 
<br>
something above and beyond standard expected utility theory, some of which 
<br>
I'm still working out how to handle.  Standard expected utility theory just 
<br>
operates over completely specified states of the universe; it doesn't 
<br>
include any way of handling abstraction.
<br>
<p>Now, here's the thing.  Given all that, and given that Fred knows all that, 
<br>
Fred might try to construct a Friendly AI that is a good thing from the 
<br>
perspective of Fred's and humanity's utility function, which is not known 
<br>
to Fred.  Fred might try to devise an optimization process such that 
<br>
feedback from Fred fine-tunes the effective utility function of the 
<br>
optimization process, or try to devise an optimization process such that it 
<br>
will scan in Fred or some other human and read out the effective utility 
<br>
function from the physical state.  Those are the two strategies I can think 
<br>
of offhand, or Fred might try to combine them.  Neither strategy is simple 
<br>
and both contain all sorts of hidden gotchas.
<br>
<p>*But* - and this is the key point - Fred has got to *know* all this stuff. 
<br>
&nbsp;&nbsp;He has got to know the nature of the problem in order to solve it.  Fred 
<br>
may be able to build an FAI without a complete printout of U-h(x) in hand. 
<br>
&nbsp;&nbsp;Fred can't possibly build an FAI without knowing that he is an &quot;expected 
<br>
utility maximizer&quot; or that there *is* a U-h(x) behind his choices.
<br>
<p>This is what I mean by saying that you cannot build an FAI to accomplish an 
<br>
end for which you do not have a well-specified abstract description.
<br>
<p>On this planet, humans *aren't* expected utility maximizers, which makes 
<br>
things a bit more difficult for Eliezer than for Fred.  But I want to 
<br>
figure out how to solve Fred's simpler problem first, which would still be 
<br>
a huge step forward.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9871.html">Randall Randall: "Re: The Future of Human Evolution"</a>
<li><strong>Previous message:</strong> <a href="9869.html">Eliezer Yudkowsky: "Re: The Future of Human Evolution"</a>
<li><strong>In reply to:</strong> <a href="9868.html">Christian Rovner: "Good old-fashioned CFAI (was: The Future of Human Evolution)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9876.html">Christian Rovner: "Re: Good old-fashioned CFAI"</a>
<li><strong>Reply:</strong> <a href="9876.html">Christian Rovner: "Re: Good old-fashioned CFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9870">[ date ]</a>
<a href="index.html#9870">[ thread ]</a>
<a href="subject.html#9870">[ subject ]</a>
<a href="author.html#9870">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:49 MDT
</em></small></p>
</body>
</html>
