<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The Future of Human Evolution</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: The Future of Human Evolution">
<meta name="Date" content="2004-09-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The Future of Human Evolution</h1>
<!-- received="Sun Sep 26 10:52:30 2004" -->
<!-- isoreceived="20040926165230" -->
<!-- sent="Sun, 26 Sep 2004 12:52:26 -0400" -->
<!-- isosent="20040926165226" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: The Future of Human Evolution" -->
<!-- id="4156F3CA.1050008@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="4156BA56.1090204@gmx.de" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20The%20Future%20of%20Human%20Evolution"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Sep 26 2004 - 10:52:26 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9857.html">Jef Allbright: "Re: The Future of Human Evolution"</a>
<li><strong>Previous message:</strong> <a href="9855.html">Sebastian Hagen: "Re: The Future of Human Evolution"</a>
<li><strong>In reply to:</strong> <a href="9855.html">Sebastian Hagen: "Re: The Future of Human Evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9857.html">Jef Allbright: "Re: The Future of Human Evolution"</a>
<li><strong>Reply:</strong> <a href="9857.html">Jef Allbright: "Re: The Future of Human Evolution"</a>
<li><strong>Reply:</strong> <a href="9858.html">Sebastian Hagen: "Re: The Future of Human Evolution"</a>
<li><strong>Reply:</strong> <a href="../0410/9908.html">Samantha Atkins: "Re: The Future of Human Evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9856">[ date ]</a>
<a href="index.html#9856">[ thread ]</a>
<a href="subject.html#9856">[ subject ]</a>
<a href="author.html#9856">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Sebastian Hagen wrote:
<br>
<em>&gt; Aleksei Riikonen wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; As an agent striving to be non-eudaemonic, could you elaborate on what 
</em><br>
<em>&gt;&gt; are the things you value? (Non-instrumentally, that is.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The best answer I can give is 'whatever has objective moral relevance'.
</em><br>
<em>&gt; Unfortunately I don't know what exactly qualifies for that, so currently
</em><br>
<em>&gt; the active subgoal is to get more intelligence applied to the task of
</em><br>
<em>&gt; finding out. Should there be in fact nothing with objective moral
</em><br>
<em>&gt; relevance, what I do is per definition morally irrelevant, so I don't
</em><br>
<em>&gt; have to consider this possibility in calculating the expected utility of
</em><br>
<em>&gt; my actions.
</em><br>
<em>&gt; This rationale has been copied from
</em><br>
<em>&gt; &lt;<a href="http://yudkowsky.net/tmol-faq/logic.html#meaning">http://yudkowsky.net/tmol-faq/logic.html#meaning</a>&gt;, but (since I haven't
</em><br>
<em>&gt; found anything that appears to be better) it does represent my current
</em><br>
<em>&gt; opinion on the matter.
</em><br>
<p>TMOL is superceded by CFAI which in turn is superceded by 
<br>
<a href="http://sl4.org/wiki/DialogueOnFriendliness">http://sl4.org/wiki/DialogueOnFriendliness</a> and 
<br>
<a href="http://sl4.org/wiki/CollectiveVolition">http://sl4.org/wiki/CollectiveVolition</a>.
<br>
<p>Roughly, the problem with the 'objective' criterion is that to get an 
<br>
objective answer you need a well-defined question, where any question that 
<br>
is the output of a well-defined algorithm may be taken as well-defined.  In 
<br>
the TMOL FAQ you've got an algorithm that searches for a solution and has 
<br>
no criterion for recognizing a solution.  This, in retrospect, was kinda 
<br>
silly.  If you actually implement the stated logic or something as close to 
<br>
the stated logic as you can get, what it will *actually* do (assuming you 
<br>
get it to work at all) is treat, as its effective supergoal, the carrying 
<br>
out of operations that have been shown to be generally useful for 
<br>
subproblems, relative to some generalization operator.  For example, it 
<br>
might sort everything in the universe into alphabetical order (as a 
<br>
supergoal) because sorting things into alphabetical order was found to 
<br>
often be useful on algorithmic problems (as subgoals).  In short, the damn 
<br>
thing don't work.
<br>
<p>We know how morality evolved.  We know how it's implemented in humans.  We 
<br>
know how human psychology treats it.  ('We' meaning humanity's accumulated 
<br>
scientific knowledge - I understand that not everyone is familiar with the 
<br>
details.)  What's left to figure out?  What information is missing?  If a 
<br>
superintelligence could figure out an answer, why shouldn't we?
<br>
<p>The rough idea behind the 'volitional extrapolation' concept is that you 
<br>
can get an answer to a question that, to a human, appears poorly defined, 
<br>
so long as the human actually contains the roots of an answer; criteria 
<br>
that aren't obviously relevant - that haven't yet reached out and connected 
<br>
themselves to the problem - but that would connect themselves to the 
<br>
problem given the right realizations.  Like, you know 'even numbers are 
<br>
green', and you want to know 'what color is 14?', but you haven't applied 
<br>
the computing power to figure out that 14 is even.  That kind of question 
<br>
can turn out to be well-defined, even if, at the moment, you're staring at 
<br>
14 with absolutely no idea how to determine what color it is.  But the 
<br>
question, if not the answer, has to be implicit in the asker.
<br>
<p>If you claim that you know absolutely nothing about objective morality, how 
<br>
would you look at an AI, or any other well-defined process, and claim that 
<br>
it did (or for that matter, did not) compute an objective morality?  If you 
<br>
know absolutely nothing about objective morality, where it comes from, what 
<br>
sort of thing it is, and so on, then how do you know that 'Look in the 
<br>
Bible' is an unsatisfactory justification for a morality?
<br>
<p>It all starts with humans.  You're the one who'd have to determine any 
<br>
criterion for recognizing a morality, an algorithm for producing morality, 
<br>
or an algorithm for producing an algorithm for producing morality.  Why 
<br>
would any answer you recognized as reasonable be non-eudaemonic?
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9857.html">Jef Allbright: "Re: The Future of Human Evolution"</a>
<li><strong>Previous message:</strong> <a href="9855.html">Sebastian Hagen: "Re: The Future of Human Evolution"</a>
<li><strong>In reply to:</strong> <a href="9855.html">Sebastian Hagen: "Re: The Future of Human Evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9857.html">Jef Allbright: "Re: The Future of Human Evolution"</a>
<li><strong>Reply:</strong> <a href="9857.html">Jef Allbright: "Re: The Future of Human Evolution"</a>
<li><strong>Reply:</strong> <a href="9858.html">Sebastian Hagen: "Re: The Future of Human Evolution"</a>
<li><strong>Reply:</strong> <a href="../0410/9908.html">Samantha Atkins: "Re: The Future of Human Evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9856">[ date ]</a>
<a href="index.html#9856">[ thread ]</a>
<a href="subject.html#9856">[ subject ]</a>
<a href="author.html#9856">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:48 MDT
</em></small></p>
</body>
</html>
