<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Proving the Impossibility of Stable Goal Systems</title>
<meta name="Author" content="Peter Voss (peter@optimal.org)">
<meta name="Subject" content="Proving the Impossibility of Stable Goal Systems">
<meta name="Date" content="2006-03-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Proving the Impossibility of Stable Goal Systems</h1>
<!-- received="Sun Mar  5 15:37:22 2006" -->
<!-- isoreceived="20060305223722" -->
<!-- sent="Sun, 5 Mar 2006 14:37:14 -0800" -->
<!-- isosent="20060305223714" -->
<!-- name="Peter Voss" -->
<!-- email="peter@optimal.org" -->
<!-- subject="Proving the Impossibility of Stable Goal Systems" -->
<!-- id="200603052237.k25MbKe11231@tick.javien.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="42F8E35A.9060705@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Peter Voss (<a href="mailto:peter@optimal.org?Subject=Re:%20Proving%20the%20Impossibility%20of%20Stable%20Goal%20Systems"><em>peter@optimal.org</em></a>)<br>
<strong>Date:</strong> Sun Mar 05 2006 - 15:37:14 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14296.html">Eliezer S. Yudkowsky: "Re: Proving the Impossibility of Stable Goal Systems"</a>
<li><strong>Previous message:</strong> <a href="14294.html">Richard Loosemore: "Re: Friendliness not an Add-on"</a>
<li><strong>In reply to:</strong> <a href="../0508/11866.html">Eliezer S. Yudkowsky: "Re: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14296.html">Eliezer S. Yudkowsky: "Re: Proving the Impossibility of Stable Goal Systems"</a>
<li><strong>Reply:</strong> <a href="14296.html">Eliezer S. Yudkowsky: "Re: Proving the Impossibility of Stable Goal Systems"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14295">[ date ]</a>
<a href="index.html#14295">[ thread ]</a>
<a href="subject.html#14295">[ subject ]</a>
<a href="author.html#14295">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eli,
<br>
<p>&nbsp;
<br>
<p>Have you seriously considered putting focused effort into proving that
<br>
practical self-modifying systems can *not* have predictably stable goal
<br>
systems?
<br>
<p>&nbsp;
<br>
<p>I don't recall specific discussion on that point.
<br>
<p>&nbsp;
<br>
<p>I mention 'practical' here, because one must then assume live interaction
<br>
with the world, finite processing power, plus a number of other constraints.
<br>
<p>&nbsp;
<br>
<p>I strongly suspect that such a proof would be relatively simple. (Obviously,
<br>
at this stage you don't agree with this sentiment).
<br>
<p>&nbsp;
<br>
<p>Naturally the implication for SIAI (and the FAI/AGI community in general)
<br>
would be substantial.
<br>
<p>&nbsp;
<br>
<p>Peter
<br>
<p>&nbsp;
<br>
<p>&nbsp;
<br>
<p>PS. Off the top of my head, I would imagine that the following
<br>
considerations may be included:
<br>
<p>&nbsp;
<br>
<p>- Any practical high-level AGI has to use its knowledge to interpret (and
<br>
question?) its given goals
<br>
<p>&nbsp;
<br>
<p>- Such a system would gain improved knowledge from interactions with the
<br>
real world. The content of this knowledge and conclusions reached by the AGI
<br>
are not predictable.
<br>
<p>&nbsp;
<br>
<p>- By the nature of its source of information, much knowledge would be based
<br>
on induction and/or statistics, and be inherently fallible.
<br>
<p>&nbsp;
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14296.html">Eliezer S. Yudkowsky: "Re: Proving the Impossibility of Stable Goal Systems"</a>
<li><strong>Previous message:</strong> <a href="14294.html">Richard Loosemore: "Re: Friendliness not an Add-on"</a>
<li><strong>In reply to:</strong> <a href="../0508/11866.html">Eliezer S. Yudkowsky: "Re: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14296.html">Eliezer S. Yudkowsky: "Re: Proving the Impossibility of Stable Goal Systems"</a>
<li><strong>Reply:</strong> <a href="14296.html">Eliezer S. Yudkowsky: "Re: Proving the Impossibility of Stable Goal Systems"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14295">[ date ]</a>
<a href="index.html#14295">[ thread ]</a>
<a href="subject.html#14295">[ subject ]</a>
<a href="author.html#14295">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
