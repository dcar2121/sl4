<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Why playing it safe is the most dangerous thing</title>
<meta name="Author" content="Olie Lamb (neomorphy@gmail.com)">
<meta name="Subject" content="Re: Why playing it safe is the most dangerous thing">
<meta name="Date" content="2006-03-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Why playing it safe is the most dangerous thing</h1>
<!-- received="Tue Mar 14 16:48:23 2006" -->
<!-- isoreceived="20060314234823" -->
<!-- sent="Wed, 15 Mar 2006 09:48:21 +1000" -->
<!-- isosent="20060314234821" -->
<!-- name="Olie Lamb" -->
<!-- email="neomorphy@gmail.com" -->
<!-- subject="Re: Why playing it safe is the most dangerous thing" -->
<!-- id="f3afeba0603141548g4b4d44dewfdb1ef4818896a7d@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="Why playing it safe is the most dangerous thing" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Olie Lamb (<a href="mailto:neomorphy@gmail.com?Subject=Re:%20Why%20playing%20it%20safe%20is%20the%20most%20dangerous%20thing"><em>neomorphy@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Mar 14 2006 - 16:48:21 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14389.html">Ben Goertzel: "META: Ooops"</a>
<li><strong>Previous message:</strong> <a href="14387.html">Joel Pitt: "Re: ICCS/CogSci-2006 Toward Social Mechanisms of Android Science"</a>
<li><strong>Maybe in reply to:</strong> <a href="../0602/14219.html">Philip Goetz: "Why playing it safe is the most dangerous thing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14394.html">Philip Goetz: "Re: Why playing it safe is the most dangerous thing"</a>
<li><strong>Reply:</strong> <a href="14394.html">Philip Goetz: "Re: Why playing it safe is the most dangerous thing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14388">[ date ]</a>
<a href="index.html#14388">[ thread ]</a>
<a href="subject.html#14388">[ subject ]</a>
<a href="author.html#14388">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Main Topic:  Negative utilitarianism
<br>
<p>(Apparently, this didn't get to SL4, as it was sent from a non-subscribed
<br>
adress.  Silly me.)
<br>
<p><em>&gt;From: &quot;Philip Goetz&quot; &lt;<a href="mailto:philgoetz@gmail.com?Subject=Re:%20Why%20playing%20it%20safe%20is%20the%20most%20dangerous%20thing">philgoetz@gmail.com</a>&gt;
</em><br>
<em>&gt;Subject: Re: Why playing it safe is the most dangerous thing
</em><br>
<em>&gt;Date: Fri, 24 Feb 2006 10:26:58 -0500
</em><br>
<em>&gt;...
</em><br>
<em>&gt;We could add the notion of negative utility.  &quot;Negative utility&quot; is my
</em><br>
<em>&gt;explanation for why lotteries are so popular in poor communities,
</em><br>
<em>&gt;despite the fact that the expected ROI of a lottery ticket is &lt; 1;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Suppose, contemplating whether to buy a lottery ticket, a person sums
</em><br>
<em>&gt;up the expected utility of their entire future life without buying the
</em><br>
<em>&gt;lottery ticket, and concludes it is below the &quot;zero utility level&quot;
</em><br>
<em>&gt;below which they would be better off dead.  They then consider the
</em><br>
<em>&gt;expected utility on buying the lottery ticket.  This gives them two
</em><br>
<em>&gt;possible outcomes: one of very high probability, and a slightly lower
</em><br>
<em>&gt;negative utilty; one of small probability, with positive utilty.
</em><br>
<p>Yes, this is a reasonably common extention of utilitarianism.  From my
<br>
experience, a fair portion of  undergraduate normative ethics classes will
<br>
toss over a couple of negative-utilitarian models.
<br>
<p>One problem with applying your variety of negative-utility funtion outlook
<br>
is that it relies on the starting outlook that future life=shit.  This
<br>
outlook is also known as &quot;extreme pessimism&quot;.
<br>
<p>Your call for such a pessimistic outlook puts the onus of proof heavily upon
<br>
you.  I don't think that your generalisations about the nature of &quot;jerks in
<br>
power&quot; constitutes much of an argument for believing that caution leads
<br>
probability of negative utility of &quot;near 1&quot;.  You can't get much more
<br>
extreme pessimism than that.
<br>
<p>I don't think that it's possible to be confident about the positive or
<br>
negative outcomes of actions that support the status quo.  The only way to
<br>
have much faith in the course of the future is to have a have a hand in
<br>
shaping it.
<br>
<p>== Negative utility systems don't work ==
<br>
<p>The bigger problem is that some of the other implications of
<br>
negative-utility operations lead to a number of absurdities.
<br>
<p>Firstly, your version:  saying that any negative-utility-state can revert to
<br>
null, thanks to suicide, it implies that any risky activity can be
<br>
justified.  (&quot;Don't dance on the rail!  You could fall and become
<br>
quadriplegic!&quot;  &quot;That's OK, I could just kill myself if that happened!&quot;)
<br>
<p>In fact, the only risky activities that need to be avoided are those with
<br>
minor bad consequences - check the math, it works out that way.
<br>
<p>(more)
<br>
<p><em>&gt;Rather than combining these two, the person reasons that they can kill
</em><br>
<em>&gt;themselves any time they choose, and thus replaces each of the
</em><br>
<em>&gt;negative-utility outcomes with a zero &quot;suicide utility&quot;.  The
</em><br>
<em>&gt;low-probability positive outcome, averaged together with the
</em><br>
<em>&gt;high-probability suicide utility of zero, produces an average utility,
</em><br>
<em>&gt;which is higher than the suicide utility (zero) of their life without
</em><br>
<em>&gt;the lottery ticket.
</em><br>
<em>&gt;
</em><br>
<em>&gt;(Note that finding oneself with a losing lottery ticket doesn't then
</em><br>
<em>&gt;require one to commit suicide.  One merely begins looking for other
</em><br>
<em>&gt;low-probability branches - future lottery tickets - leading towards
</em><br>
<em>&gt;positive utility.)
</em><br>
<em>&gt;
</em><br>
<em>&gt;More specifically, this negative utility theory says that, when
</em><br>
<em>&gt;comparing possible actions, you compare the expected utilities only of
</em><br>
<em>&gt;the portions of the probability distributions with positive utility.
</em><br>
<em>&gt;If you consider the probability distribution on future expected summed
</em><br>
<em>&gt;life utilities, and let
</em><br>
<em>&gt;
</em><br>
<em>&gt;     - U0 be the positive area for the no-ticket distribution (the
</em><br>
<em>&gt;integral of utility over all outcomes under which utility is positive)
</em><br>
<em>&gt;     - UT be the positive area for the bought-a-ticket distribution
</em><br>
<em>&gt;
</em><br>
<em>&gt;then UT &gt; U0 =&gt; you should buy a ticket.
</em><br>
<em>&gt;
</em><br>
<em>&gt;We can apply similar logic to possible outcomes of the Singularity.
</em><br>
<em>&gt;If, as I've argued, the careful approach provides us with a near-1
</em><br>
<em>&gt;probability of negative utility, and the damn-the-torpedoes approach
</em><br>
<em>&gt;provides us with a greater-than-epsilon probability of positive
</em><br>
<em>&gt;utility, then we seem to be in a situation where the summed positive
</em><br>
<em>&gt;utility of damn-the-torpedos is greater than the summed positive
</em><br>
<em>&gt;utility of the cautious approach, EVEN if the expected utility of the
</em><br>
<em>&gt;cautious approach is greater.
</em><br>
<p>Apart from the fact that this negative-utilitarian-model doesn't work, you
<br>
can't reasonably apply a suicide function in the case of UFAI.
<br>
<p>In a Nasty-Powerful-Intelligence scenario, there is no guarantee that
<br>
suicide would be a possible escape for sentients at the butt end of the
<br>
Nasty-P-I's malice.  Why would a nasty-intelligence let its subjects escape
<br>
through suicide, any more than a dancing bear's owner would let the bear
<br>
kill itself?
<br>
<p>== other negative utility models don't work, either ==
<br>
<p>One typical model of negative utility that comes up is that only suffering
<br>
counts, or that it counts more than pleasure.  This model of utilitarianism
<br>
is appealing, in that it avoids the Slavery-pitfall from which the standard
<br>
aggregate-utility model suffers.
<br>
<p>That is: the standard model says that it's ok to make one slave suffer 10
<br>
points if the slave brings 20 points of happiness to their master(s).
<br>
<p>The suffering-utility model avoids this.  However, if you count
<br>
auto-utility, it makes it immoral to clean one's own bathroom (delayed
<br>
gratification), which is absurd.  It's also immoral to work to buy your
<br>
partner a gift, (suffering for greater gain) which is also absurd, although
<br>
the neg-utility advocate can say that decisions to suffer don't count.
<br>
There's still a problem in getting someone to pass you the salt, since they
<br>
are suffering and the reward doesn't count, or is discounted.
<br>
<p>Another, contested implication of neg-utility models is the pinprick
<br>
argument <a href="http://www.utilitarianism.com/pinprick-argument.html">http://www.utilitarianism.com/pinprick-argument.html</a>
<br>
<p>A much more sensible approach, which gets the benefits of neg-utility, but
<br>
doesn't suffer its pitfalls, is maxi-min utilitarianism, where the goal is
<br>
to make the least-well-off person in a group as happy as possible.  This
<br>
model also has flaws, but they're not /quite/ as absurd.
<br>
<p><em>&gt;
</em><br>
<p><p>... And another response to a different topic w/in that post:
<br>
<p><em>&gt;On 2/24/06, Ben Goertzel &lt;<a href="mailto:ben@goertzel.org?Subject=Re:%20Why%20playing%20it%20safe%20is%20the%20most%20dangerous%20thing">ben@goertzel.org</a>&gt; wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Peter, two points:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; 1)
</em><br>
<em>&gt; &gt; Eliezer has sometimes proposed that a Singularity not properly planned
</em><br>
<em>&gt; &gt; with regard to Friendly AI is almost certain to lead to human
</em><br>
<em>&gt; &gt; extinction.  But this has not been convincingly argued for.  He has
</em><br>
<em>&gt; &gt; merely shown why this is a significant possibility.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Human extinction might be a likely outcome.  I was speaking of
</em><br>
<em>&gt;extinction of life, which I regard as a definitely bad thing, and an
</em><br>
<em>&gt;unlikely outcome.
</em><br>
<p>Hmm... Pan-Computronium seems a fair-likely outcome to me for any scenario
<br>
involving an &quot;unwise&quot; seed-AI.  Pan-Computronium, would seem to imply the
<br>
appropriation of the biosphere's carbon supply.
<br>
<p>Sounds to me as though the rest of life is just as likely to get the boot as
<br>
humans are...
<br>
<p>Not that I'm sure that UFAI
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14389.html">Ben Goertzel: "META: Ooops"</a>
<li><strong>Previous message:</strong> <a href="14387.html">Joel Pitt: "Re: ICCS/CogSci-2006 Toward Social Mechanisms of Android Science"</a>
<li><strong>Maybe in reply to:</strong> <a href="../0602/14219.html">Philip Goetz: "Why playing it safe is the most dangerous thing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14394.html">Philip Goetz: "Re: Why playing it safe is the most dangerous thing"</a>
<li><strong>Reply:</strong> <a href="14394.html">Philip Goetz: "Re: Why playing it safe is the most dangerous thing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14388">[ date ]</a>
<a href="index.html#14388">[ thread ]</a>
<a href="subject.html#14388">[ subject ]</a>
<a href="author.html#14388">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
