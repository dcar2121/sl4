<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Proving the Impossibility of Stable Goal Systems</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="Re: Proving the Impossibility of Stable Goal Systems">
<meta name="Date" content="2006-03-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Proving the Impossibility of Stable Goal Systems</h1>
<!-- received="Sun Mar  5 17:20:53 2006" -->
<!-- isoreceived="20060306002053" -->
<!-- sent="Sun, 5 Mar 2006 19:20:52 -0500" -->
<!-- isosent="20060306002052" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="Re: Proving the Impossibility of Stable Goal Systems" -->
<!-- id="638d4e150603051620m22f66508jb85df1f89d6e94c2@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="440B7A79.5070303@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=Re:%20Proving%20the%20Impossibility%20of%20Stable%20Goal%20Systems"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sun Mar 05 2006 - 17:20:52 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14298.html">dave last: "RE: Too much talk not enough action"</a>
<li><strong>Previous message:</strong> <a href="14296.html">Eliezer S. Yudkowsky: "Re: Proving the Impossibility of Stable Goal Systems"</a>
<li><strong>In reply to:</strong> <a href="14296.html">Eliezer S. Yudkowsky: "Re: Proving the Impossibility of Stable Goal Systems"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14297">[ date ]</a>
<a href="index.html#14297">[ thread ]</a>
<a href="subject.html#14297">[ subject ]</a>
<a href="author.html#14297">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Peter,
<br>
<p>I think your general line of argument is probably right.
<br>
<p>However, it seems to me that a rigorous proof of your argument is
<br>
going to be verrrry difficult to come by.
<br>
<p>Look at how much math Marcus Hutter had to generate to prove some
<br>
theorems that amount to, in essence, the statement that &quot;given any
<br>
computable goal, with near-infinite processing power and memory one
<br>
can make a software program that will achieve this goal just about as
<br>
well as is possible.&quot;  Intuitively this is almost obvious but hundreds
<br>
of pages of hard math had to be done to prove it rigorously.  And this
<br>
is obviously much simpler than the intuition you've expressed
<br>
regarding Friendliness of AI systems with finite resources.
<br>
<p>I feel that our current mathematical concepts are very ill-suited to
<br>
proving theorems of this nature.  Hutter's work is great but it
<br>
reminds me of things like
<br>
<p>* a 50-page geometric derivation of the formula for the slope of a
<br>
cubic polynomial, done before Newton and Leibniz invented the rules of
<br>
calculus
<br>
<p>* old-fashioned differential geometry calculations, done using
<br>
coordinate geometry and endless elementary algebra -- which is how
<br>
this stuff was done back before the invention of manifold theory
<br>
<p>In each of these cases, long and nasty proofs were replaced by short
<br>
elegant ones after the correct set of new concepts was introduced.
<br>
<p>In the case of issues regarding AGI, Friendliness and
<br>
self-modification and the like, I think we don't yet have the correct
<br>
set of new concepts.  Once we do, some analogue of Hutter's theorems
<br>
will be provable almost trivially, and quite possibly some analogue of
<br>
your hypothesis about the difficulty of guaranteeing Friendliness for
<br>
AGI systems under finite resources will also be provable (though
<br>
probably not quite as trivially).
<br>
<p>An interesting topic is what these new concepts might be.  Of course
<br>
they will end up being related to current theoretical notions like
<br>
probability and algorithmic information, but I expect they will
<br>
involve a fundamentally different perspective as well.
<br>
<p>-- Ben G
<br>
<p><p><p><p><p>On 3/5/06, Eliezer S. Yudkowsky &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Proving%20the%20Impossibility%20of%20Stable%20Goal%20Systems">sentience@pobox.com</a>&gt; wrote:
<br>
<em>&gt; Peter Voss wrote:
</em><br>
<em>&gt; &gt; Eli,
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Have you seriously considered putting focused effort into proving that
</em><br>
<em>&gt; &gt; practical self-modifying systems can *not* have predictably stable goal
</em><br>
<em>&gt; &gt; systems?
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I don't recall specific discussion on that point.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I often consider that problem; if I could prove the problem impossible
</em><br>
<em>&gt; in theory I would probably be quite close to solving it in practice.  My
</em><br>
<em>&gt; attention tends to focus on Godelian concerns, though I think of them as
</em><br>
<em>&gt; &quot;Lobian&quot; after Lob's Theorem.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; I strongly suspect that such a proof would be relatively simple.
</em><br>
<em>&gt; &gt; (Obviously, at this stage you don't agree with this sentiment).
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Naturally the implication for SIAI (and the FAI/AGI community in
</em><br>
<em>&gt; &gt; general) would be substantial.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Please go right ahead and do it; don't let me stop you!
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; - Any practical high-level AGI has to use its knowledge to interpret
</em><br>
<em>&gt; &gt; (and question?) its given goals
</em><br>
<em>&gt;
</em><br>
<em>&gt; Any AGI must use its model of the real world to decide which real-world
</em><br>
<em>&gt; actions lead to which real-world consequences, and evaluate its utility
</em><br>
<em>&gt; function or other decision system against the model's predicted
</em><br>
<em>&gt; real-world consequences.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This does *not* necessarily involve changing the utility function.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; - Such a system would gain improved knowledge from interactions with the
</em><br>
<em>&gt; &gt; real world. The content of this knowledge and conclusions reached by the
</em><br>
<em>&gt; &gt; AGI are not predictable.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Agreed.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; - By the nature of its source of information, much knowledge would be
</em><br>
<em>&gt; &gt; based on induction and/or statistics, and be inherently fallible.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Also agreed.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Please note however, that accidentally killing a human is not a
</em><br>
<em>&gt; catastrophic failure.  One evil deed does not turn the FAI evil, like a
</em><br>
<em>&gt; character in a bad movie.  *Catastrophic* failures, which change what
</em><br>
<em>&gt; the FAI is *trying* to do, require that the FAI fail on the task of
</em><br>
<em>&gt; recursive self-improvement.
</em><br>
<em>&gt;
</em><br>
<em>&gt; So an impossibility proof would have to say:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1)  The AI cannot reproduce onto new hardware, or modify itself on
</em><br>
<em>&gt; current hardware, with knowable stability of the decision system (that
</em><br>
<em>&gt; which determines what the AI is *trying* to accomplish in the external
</em><br>
<em>&gt; world) and bounded low cumulative failure probability over many rounds
</em><br>
<em>&gt; of self-modification.
</em><br>
<em>&gt;
</em><br>
<em>&gt; or
</em><br>
<em>&gt;
</em><br>
<em>&gt; 2)  The AI's decision function (as it exists in abstract form across
</em><br>
<em>&gt; self-modifications) cannot be knowably stably bound with bounded low
</em><br>
<em>&gt; cumulative failure probability to programmer-targeted consequences as
</em><br>
<em>&gt; represented within the AI's changing, inductive world-model.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If I could rigorously prove such an impossibility, my understanding
</em><br>
<em>&gt; would probably have advanced to the point where I could also go ahead
</em><br>
<em>&gt; and pull it off in practice.
</em><br>
<em>&gt;
</em><br>
<em>&gt; --
</em><br>
<em>&gt; Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt; Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14298.html">dave last: "RE: Too much talk not enough action"</a>
<li><strong>Previous message:</strong> <a href="14296.html">Eliezer S. Yudkowsky: "Re: Proving the Impossibility of Stable Goal Systems"</a>
<li><strong>In reply to:</strong> <a href="14296.html">Eliezer S. Yudkowsky: "Re: Proving the Impossibility of Stable Goal Systems"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14297">[ date ]</a>
<a href="index.html#14297">[ thread ]</a>
<a href="subject.html#14297">[ subject ]</a>
<a href="author.html#14297">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
