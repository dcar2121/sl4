<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Shocklevel 5</title>
<meta name="Author" content="Jeff Bone (jbone@jump.net)">
<meta name="Subject" content="Shocklevel 5">
<meta name="Date" content="2001-12-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Shocklevel 5</h1>
<!-- received="Fri Dec 07 21:17:39 2001" -->
<!-- isoreceived="20011208041739" -->
<!-- sent="Fri, 07 Dec 2001 19:39:50 -0600" -->
<!-- isosent="20011208013950" -->
<!-- name="Jeff Bone" -->
<!-- email="jbone@jump.net" -->
<!-- subject="Shocklevel 5" -->
<!-- id="3C116F66.AAEAA6A3@jump.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3C1127D7.78AD1719@jump.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Jeff Bone (<a href="mailto:jbone@jump.net?Subject=Re:%20Shocklevel%205"><em>jbone@jump.net</em></a>)<br>
<strong>Date:</strong> Fri Dec 07 2001 - 18:39:50 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2427.html">Alden Jurling: "Re: Shocklevel 5"</a>
<li><strong>Previous message:</strong> <a href="2425.html">Eliezer S. Yudkowsky: "Direct neuron interface via quantum dots"</a>
<li><strong>In reply to:</strong> <a href="2422.html">Jeff Bone: "Re: New website: The Simulation Argument"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2427.html">Alden Jurling: "Re: Shocklevel 5"</a>
<li><strong>Reply:</strong> <a href="2427.html">Alden Jurling: "Re: Shocklevel 5"</a>
<li><strong>Reply:</strong> <a href="2428.html">Eliezer S. Yudkowsky: "The inevitability of death, or the death of inevitability?"</a>
<li><strong>Reply:</strong> <a href="2431.html">Gordon Worley: "Re: Shocklevel 5"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2426">[ date ]</a>
<a href="index.html#2426">[ thread ]</a>
<a href="subject.html#2426">[ subject ]</a>
<a href="author.html#2426">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
The question of risk and risk reduction is, IMO, a very interesting one
<br>
and essential to any long-range planning activity.  The point I've been
<br>
trying to illustrate is obscured, perhaps, by a kind of &quot;shocklevel&quot;
<br>
problem --- and though I've been accused of anthropomorphic reasoning and
<br>
because Eli has himself made that claim in the past, I feel I need to
<br>
clarify a few points.
<br>
<p>The &quot;risk&quot; argument has nothing to do with any anthropomorphic
<br>
assumptions;  to the contrary, the counterarguments --- and indeed
<br>
perhaps the whole notion of &quot;Friendliness&quot; --- is grounded in a kind of
<br>
anthropomorphic reasoning and constrained by shocklevel-deficient &quot;event
<br>
horizons.&quot;  The examples I've used have probably compounded the
<br>
misunderstanding, so let me try to put together an argument that is
<br>
totally (or nearly so) divorced from an anthropomorphic context.  I'll
<br>
try to be very clear about the assumptions.
<br>
<p>We are explicitly ignoring the following topics:
<br>
<p>(1)  Whether an immediately pre-Power mind is likely to develop along
<br>
Friendly or malevolent lines.
<br>
(2)  Whether any particular course of action is likely to result in
<br>
Friendliness or malevolence.
<br>
(3)  Whether an emergent Power has any interest at all in its precursors'
<br>
individual survival.
<br>
<p>Let us assume that you are the first Power that results from human
<br>
technological advance.  Let us assume that, like all living beings that
<br>
we are aware of, you are concerned about &quot;survival.&quot;  Let us define
<br>
survival to be &quot;continuity of awareness over time, perhaps punctuated.&quot;
<br>
You might be concerned about this for your own sake, if you had an
<br>
individual sense of self, or you might be an altruistic uberbeing
<br>
concerned only with the survival of your own constituents.  It doesn't
<br>
matter which:  the latter case implies / requires the former, as the
<br>
altruistic uberbeing must ensure its own survival in order to ensure the
<br>
survival of its constituents.  Regardless, the essential challenge is
<br>
simple:  continue to function and pursue your goals indefinitely over
<br>
time.
<br>
<p>---&gt;  NO ANTHROPOMORPHIC BIAS
<br>
<p>Let's call you &quot;Alpha.&quot;
<br>
<p>You (Alpha) are a Mind, but your substrate (at least initially) is normal
<br>
matter.  Let's assume that you cannot use spacetime itself as a
<br>
computational substrate --- there is no particular reason to believe you
<br>
can't, but there's also no particular reason to assume you can.  Let's
<br>
assume that you can turn normal matter into perfectly efficient
<br>
computronium.  Even so, whatever mass / volume makes up your substrate is
<br>
subject to physical risks in the long-term in the form of disastrous
<br>
events:  planetary events like collisions, stellar events like novas,
<br>
interstellar events like supernovas, reality-changing events like
<br>
collapse of a metastable vacuum state, universal events like the heat
<br>
death of the universe, etc.  There are three general strategies for
<br>
dealing with those risks:  minimization of your &quot;profile&quot; relative to
<br>
such events by minimizing the volume / mass involved in the substrate or
<br>
changing the characteristics of your interaction with other normal
<br>
timespace / matter, hardening physical security from such events if
<br>
possible, or making yourself as distributed across timespace as possible
<br>
to amortize risk from single-point failures.
<br>
<p>There is a minimal volume / mass substrate required for you to
<br>
perpetuate.
<br>
There is a maximum amount of physical security that can be achieved.
<br>
The lightcone constrains maximum distribution.
<br>
The speed of light constrains interaction across a distributed body.
<br>
The risk of annihilation approaches unity over time no matter what.
<br>
<p>How do you achieve the penultimate objective, which is to ensure your
<br>
*own* survival so that you can continue to perform your other functions,
<br>
which might be protection and perpetuation of some external constituency?
<br>
<p>Bottom line, in the limit:  you cannot.  Extinction of the &quot;individual&quot;
<br>
--- even a distributed, omnipotent ubermind --- is 100% certain at some
<br>
future point, if for no other reason than the entropic progress of the
<br>
universe.
<br>
<p>---&gt;  ABSOLUTE SAFETY FOR ANY SYSTEM, MIND, OR INDIVIDUAL IS A PHYSICAL
<br>
IMPOSSIBILITY UNLESS YOU CAN REWRITE THE SECOND LAW OF THERMODYNAMICS.
<br>
<p>Welcome to Shocklevel 5.  Infinite survival is an impossibility, even
<br>
(especially) in an infinite universe.
<br>
<p>You can *maximize* survival probability over a finite time, but you
<br>
cannot guarantee immortality even given wildly optimistic assumptions ---
<br>
if the universe is open.  (If it's closed, it might be a different story
<br>
--- but that's a longshot.)  There are specific things you *can* do ---
<br>
moving the substrate to a dark matter basis minimizes its interaction
<br>
with normal matter and energy, minimizing many of the risks (planetary,
<br>
stellar, interstellar) but still leaves you exposed to reality failure or
<br>
universal catastrophes.
<br>
<p>---&gt;  SUBSTRATE &quot;MATTERS,&quot; I.E. HAS QUANTIFIABLE RISK IMPACT
<br>
<p>So what does this have to do with Sysops?  Well, admittedly, we're on the
<br>
far reaches of implication -wrt- Sysops.  The point I'm making is really
<br>
about risk;  my fear is that we are too anthropomorphically constrained
<br>
to evaluate risks in longer-than-human timescales.  The notion of
<br>
Friendly seems to assume a particular set of imperatives for such a Mind
<br>
that may, in fact, be unduly influenced and constrained by those notions
<br>
of &quot;safety,&quot; what's desirable, etc.  Evolution has numerous deadends;  it
<br>
would be horrible to condemn the human line (not physically, but
<br>
considering all our possible antecedents) to such a deadend through
<br>
mistakes that trade temporary security for long-term viability.
<br>
<p>Long-term species viability and long-term individual viability may not be
<br>
compatible.  If we build a system that ensures the latter, we may deny
<br>
ourselves the former.  And without long-term species viability, the
<br>
long-term prospects for the viability of intelligence in the universe go
<br>
to zero, as it will certainly take engineering effort on a massive scale
<br>
to minimize some of the large-scale extinction risks.
<br>
<p>$0.02,
<br>
<p>jb
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2427.html">Alden Jurling: "Re: Shocklevel 5"</a>
<li><strong>Previous message:</strong> <a href="2425.html">Eliezer S. Yudkowsky: "Direct neuron interface via quantum dots"</a>
<li><strong>In reply to:</strong> <a href="2422.html">Jeff Bone: "Re: New website: The Simulation Argument"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2427.html">Alden Jurling: "Re: Shocklevel 5"</a>
<li><strong>Reply:</strong> <a href="2427.html">Alden Jurling: "Re: Shocklevel 5"</a>
<li><strong>Reply:</strong> <a href="2428.html">Eliezer S. Yudkowsky: "The inevitability of death, or the death of inevitability?"</a>
<li><strong>Reply:</strong> <a href="2431.html">Gordon Worley: "Re: Shocklevel 5"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2426">[ date ]</a>
<a href="index.html#2426">[ thread ]</a>
<a href="subject.html#2426">[ subject ]</a>
<a href="author.html#2426">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
