<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Sysop yet again Re: New website: The Simulation Argument</title>
<meta name="Author" content="Jeff Bone (jbone@jump.net)">
<meta name="Subject" content="Re: Sysop yet again Re: New website: The Simulation Argument">
<meta name="Date" content="2001-12-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Sysop yet again Re: New website: The Simulation Argument</h1>
<!-- received="Sat Dec 08 16:55:41 2001" -->
<!-- isoreceived="20011208235541" -->
<!-- sent="Sat, 08 Dec 2001 15:43:01 -0600" -->
<!-- isosent="20011208214301" -->
<!-- name="Jeff Bone" -->
<!-- email="jbone@jump.net" -->
<!-- subject="Re: Sysop yet again Re: New website: The Simulation Argument" -->
<!-- id="3C128965.B1A367A@jump.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3C11B760.3CB181EE@posthuman.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Jeff Bone (<a href="mailto:jbone@jump.net?Subject=Re:%20Sysop%20yet%20again%20Re:%20New%20website:%20The%20Simulation%20Argument"><em>jbone@jump.net</em></a>)<br>
<strong>Date:</strong> Sat Dec 08 2001 - 14:43:01 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2435.html">Jeff Bone: "Re: Shocklevel 5"</a>
<li><strong>Previous message:</strong> <a href="2433.html">Jeff Bone: "Re: The inevitability of death, or the death of inevitability?"</a>
<li><strong>In reply to:</strong> <a href="2430.html">Brian Atkins: "Re: Sysop yet again Re: New website: The Simulation Argument"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2392.html">Gordon Worley: "Simulation aware"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2434">[ date ]</a>
<a href="index.html#2434">[ thread ]</a>
<a href="subject.html#2434">[ subject ]</a>
<a href="author.html#2434">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Brian Atkins wrote:
<br>
<p><em>&gt; Nothing. Superintelligences being unable to stop or predict earthquakes
</em><br>
<em>&gt; is very unlikely. Which is what I said.
</em><br>
<p>Okay, got it --- why is *that* unlikely?  (Forget earthquakes, think larger scale.  The
<br>
point is that unless the Power controls all of spacetime, there are risks that cannot
<br>
be avoided, only predicted --- and preciction at the top end of the scale requires very
<br>
fine-grained simulation of reality, which has natural limits (Beckenstein Bound, etc.)
<br>
<p><em>&gt; But the Sysop Scenario
</em><br>
<em>&gt; comes from the idea that at least for a while we are going to be stuck
</em><br>
<em>&gt; running on computronium or worse. We will not &quot;prefer&quot; that, it simply
</em><br>
<em>&gt; is expected (barring magic physics) to be what we end up with. So I have
</em><br>
<em>&gt; to shoot that remark of yours down.
</em><br>
<p>???
<br>
<p>How does this &quot;shoot down&quot; any remark I've made?  I have absolutely no contest with
<br>
your comment --- I think we'll be running on less-than-computronium for a very long
<br>
time.  I don't, however, think that means we can't extrapolate, and I don't think it
<br>
means we *shouldn't* extrapolate --- particularly when balancing risk / reward
<br>
equations.
<br>
<p><em>&gt; I will also shoot down your death
</em><br>
<em>&gt; vs. copies remark since no matter how many copies you have floating
</em><br>
<em>&gt; around the solar system, if all the atoms get taken over by a Blight you
</em><br>
<em>&gt; will reach a state of &quot;complete&quot; death.
</em><br>
<p>???
<br>
<p>Again, I'm not sure how this &quot;shoots down&quot; anything, as we're in complete agreement
<br>
here.
<br>
<p><em>&gt; There are things that even SIs
</em><br>
<em>&gt; probably have to worry about. If you can accept that then you can accept
</em><br>
<em>&gt; that some form of Sysop /may/ be needed even in that future time.
</em><br>
<p>Hold on, pard.  I'm not arguing that superintelligence isn't necessary.  IMO, it's an
<br>
ABSOLUTE necessity in order to manage some of the larger-scale risks.  But that DOES
<br>
NOT logically lead to the conclusion that a &quot;Sysop&quot; --- i.e., potentially coercive
<br>
external intelligence --- is needed.
<br>
<p><em>&gt; What is much more important is /getting there/ in the first place. So
</em><br>
<em>&gt; I have to agree with Gordon you seem to be stuck on something that has
</em><br>
<em>&gt; little importance to pre-Singularity goings-on.
</em><br>
<p>I disagree --- if you're building a moral machine that has a different notion of
<br>
morals, then you had best hope that those morals are consistent with at least your
<br>
longest-term goals.
<br>
<p><em>&gt; Friendliness is
</em><br>
<em>&gt; not about expecting any kind of certain outcome other than the one that
</em><br>
<em>&gt; is logically and rationally best for everyone, based upon what they want.
</em><br>
<p>And there is the crux of the debate, such as it is:  I absolutely do not believe that
<br>
any rational being free from sentiment and cultural programming can believe in any
<br>
single outcome that is &quot;logically and rationally best for everyone, based upon what
<br>
they want.&quot;  IMO, that notion is helplessly naive, prima facie inconsistent with the
<br>
observable world (and not just human society, so don't go off on the &quot;anthropomorphic&quot;
<br>
tangent.)  Given that you guys are transcending the bounds of traditional computer
<br>
science and AI and dabbling in what may be an incredibly powerful kind of &quot;applied
<br>
philosophy&quot; I think that the discussion is at a minimum worth having.
<br>
<p><em>&gt; I don't see what you're saying. In the case of earthquakes for instance
</em><br>
<em>&gt; the Sysop would already know they exist. So it likely would immediately
</em><br>
<em>&gt; upon noticing that &quot;earthquakes exist, and I have no way to predict or stop
</em><br>
<em>&gt; them&quot; begin notifying the people and providing any alternatives it had
</em><br>
<em>&gt; to them. Like I said, if they decide to stay around it's their own fault
</em><br>
<em>&gt; when something bad happens.
</em><br>
<p>&quot;The System wishes to inform you that the collapse of the metastable vacuum state will
<br>
occur at an undetermined time in the next year with a possibility of 1 in 1x10^-60.
<br>
Such an event will result in certain annihilation of your consciousness and all
<br>
available backups.  I have no alternatives to offer you at this time.&quot;  (Or substitute
<br>
inescapable supernova event, etc.)
<br>
<p><em>&gt; The only other class
</em><br>
<p>Wow, you sound really certain of that.  I've already named a number of them, and I
<br>
believe Nick has a kind of prospective taxonomy for different existential risks.
<br>
Running up against these kinds of things --- statements of denial in the face of
<br>
provided information --- are why I wonder from time to time how thick those
<br>
rose-coloured glasses you're wearing actually are.  ;-)
<br>
<p><em>&gt; of no-wins are surprise situations like say a near-
</em><br>
<em>&gt; light-speed black hole comes zooming into the solar system. Well as soon
</em><br>
<em>&gt; as the Sysop's external sensors pick it up it would let everyone know to
</em><br>
<em>&gt; clear out of the area.
</em><br>
<p>What if simple physics (or other concerns, like logistics) make the catastrophe
<br>
unavoidable?  &quot;E.g., the Sun will go nova in the next in the next 10 years with 100%
<br>
certainty, and the radioactive blast wave will proceed at the speed of light outward,
<br>
eventually catching all the sublight vehicles we have, destroying them.  Evacuation is
<br>
futile.&quot;
<br>
<p><em>&gt; This kind of thing would of course not be perfect safety, it is simply
</em><br>
<em>&gt; the best possible under physical limits. Still, can't beat that.
</em><br>
<p>Sure, I agree.  But I think it's clear --- if you look at this stuff --- that a Sysop
<br>
is going to be very concerned about gathering all available information about the
<br>
environment its constituents have to live in, and will be required to make predictions
<br>
and forecasts from detailed models.
<br>
<p><em>&gt; That latter part of my statement appears tautological, but the idea that
</em><br>
<em>&gt; an AI can be designed such that it will stay Friendly is not.
</em><br>
<p>I'm not so sure.  Maybe second- or third-order tautological.  ;-)
<br>
<p><em>&gt; I don't think we've heard any criticism from you yet regarding either
</em><br>
<em>&gt; CFAI or GISAI. If you have comments about the feasibility of either then
</em><br>
<em>&gt; by all means let's drop the Sysop thread and get to the meat.
</em><br>
<p>I've made some comments about those topics in the past...  I'm trying to take a &quot;wait
<br>
and see&quot; approach, here.
<br>
<p><em>&gt; Right, well like I said, I trust a Friendly SI to be able to figure out
</em><br>
<em>&gt; pretty easily whether it is practical or not.
</em><br>
<p>Right, but here's the tautology:  &quot;I trust a Friendly SI to figure this stuff out,
<br>
because a Friendly SI is by definition trustworthy in such matters.&quot;
<br>
<p><em>&gt; &gt; The crux of my issue is this:  &quot;most perfect universe&quot; is underdefined, and
</em><br>
<em>&gt; &gt; indeed perhaps undefinable in any universally mutually agreeable fashion.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It's on a person by person basis with the Sysop breaking ties :-) That's
</em><br>
<em>&gt; my story, and I'm sticking to it :-)
</em><br>
<p>And I'm okay with that, I'm just *deeply* concerned with how a Sysop might break such
<br>
ties, and on what basis.
<br>
<p><em>&gt; I think your claim that there's always a tradeoff is wrong.
</em><br>
<p>Okay, fine --- there's *almost* always a tradeoff between liberty and safety.
<br>
<p><em>&gt; Friendliness also BTW is not necessarily about making the world a safe
</em><br>
<em>&gt; place. As I said, it is a completely different topic and aim from the
</em><br>
<em>&gt; Sysop discussion. Friendliness is strictly about how do you build an AI
</em><br>
<em>&gt; that will be &quot;nice&quot;.
</em><br>
<p>&quot;Nice&quot; and &quot;safety&quot; are hopelessly entangled.  An SI that says &quot;how do you do?&quot; and
<br>
&quot;excuse me&quot; and &quot;looking sharp today, Brian&quot; and so forth while turning your
<br>
neighborhood into a nanogoo breeding tank can't really be called &quot;nice,&quot; can it?
<br>
<p><em>&gt; You read this, right? <a href="http://www.intelligence.org/CFAI/info/indexfaq.html#q_1">http://www.intelligence.org/CFAI/info/indexfaq.html#q_1</a>
</em><br>
<p>Yup, right when it hit the bitstream a while back.
<br>
<p>jb
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2435.html">Jeff Bone: "Re: Shocklevel 5"</a>
<li><strong>Previous message:</strong> <a href="2433.html">Jeff Bone: "Re: The inevitability of death, or the death of inevitability?"</a>
<li><strong>In reply to:</strong> <a href="2430.html">Brian Atkins: "Re: Sysop yet again Re: New website: The Simulation Argument"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2392.html">Gordon Worley: "Simulation aware"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2434">[ date ]</a>
<a href="index.html#2434">[ thread ]</a>
<a href="subject.html#2434">[ subject ]</a>
<a href="author.html#2434">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
