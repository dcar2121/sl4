<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Shocklevel 5</title>
<meta name="Author" content="Alden Jurling (nakomus@cnsp.com)">
<meta name="Subject" content="Re: Shocklevel 5">
<meta name="Date" content="2001-12-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Shocklevel 5</h1>
<!-- received="Fri Dec 07 21:57:53 2001" -->
<!-- isoreceived="20011208045753" -->
<!-- sent="Fri, 07 Dec 2001 19:51:40 -0700" -->
<!-- isosent="20011208025140" -->
<!-- name="Alden Jurling" -->
<!-- email="nakomus@cnsp.com" -->
<!-- subject="Re: Shocklevel 5" -->
<!-- id="5.1.0.14.0.20011207192530.009f34a0@mail.cnsp.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3C116F66.AAEAA6A3@jump.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Alden Jurling (<a href="mailto:nakomus@cnsp.com?Subject=Re:%20Shocklevel%205"><em>nakomus@cnsp.com</em></a>)<br>
<strong>Date:</strong> Fri Dec 07 2001 - 19:51:40 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2428.html">Eliezer S. Yudkowsky: "The inevitability of death, or the death of inevitability?"</a>
<li><strong>Previous message:</strong> <a href="2426.html">Jeff Bone: "Shocklevel 5"</a>
<li><strong>In reply to:</strong> <a href="2426.html">Jeff Bone: "Shocklevel 5"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2435.html">Jeff Bone: "Re: Shocklevel 5"</a>
<li><strong>Reply:</strong> <a href="2435.html">Jeff Bone: "Re: Shocklevel 5"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2427">[ date ]</a>
<a href="index.html#2427">[ thread ]</a>
<a href="subject.html#2427">[ subject ]</a>
<a href="author.html#2427">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Leaving off the sysop for the moment, lets say you are a Power and you want 
<br>
to survive for as long as possible.
<br>
Lets say that you decide that to do that by spreading as quickly as 
<br>
possible. As time passes the chance of any particular possible disaster 
<br>
occurring rises, so your chance of survival drops. But as the space/mass 
<br>
you control expands the set of disasters that could potentially destroy you 
<br>
grows smaller and smaller, and your chance of survival rises. So you have a 
<br>
race between increasing risk and decreasing vulnerability. Is it a foregone 
<br>
conclusion that risk will win out in the long run? It seems that you might 
<br>
be able to last as long as the universe can support your mass and energy needs.
<br>
<p>Also, is there necessarily a significant difference between personal and 
<br>
'species' survival? (if your backups are distributed enough any calamity 
<br>
that destroyed all of them would necessarily destroy the whole species).
<br>
<p>Im not sure if there is really a point here or not.
<br>
<p>At 07:39 PM 12/7/01 -0600, you wrote:
<br>
<p><em>&gt;The question of risk and risk reduction is, IMO, a very interesting one
</em><br>
<em>&gt;and essential to any long-range planning activity.  The point I've been
</em><br>
<em>&gt;trying to illustrate is obscured, perhaps, by a kind of &quot;shocklevel&quot;
</em><br>
<em>&gt;problem --- and though I've been accused of anthropomorphic reasoning and
</em><br>
<em>&gt;because Eli has himself made that claim in the past, I feel I need to
</em><br>
<em>&gt;clarify a few points.
</em><br>
<em>&gt;
</em><br>
<em>&gt;The &quot;risk&quot; argument has nothing to do with any anthropomorphic
</em><br>
<em>&gt;assumptions;  to the contrary, the counterarguments --- and indeed
</em><br>
<em>&gt;perhaps the whole notion of &quot;Friendliness&quot; --- is grounded in a kind of
</em><br>
<em>&gt;anthropomorphic reasoning and constrained by shocklevel-deficient &quot;event
</em><br>
<em>&gt;horizons.&quot;  The examples I've used have probably compounded the
</em><br>
<em>&gt;misunderstanding, so let me try to put together an argument that is
</em><br>
<em>&gt;totally (or nearly so) divorced from an anthropomorphic context.  I'll
</em><br>
<em>&gt;try to be very clear about the assumptions.
</em><br>
<em>&gt;
</em><br>
<em>&gt;We are explicitly ignoring the following topics:
</em><br>
<em>&gt;
</em><br>
<em>&gt;(1)  Whether an immediately pre-Power mind is likely to develop along
</em><br>
<em>&gt;Friendly or malevolent lines.
</em><br>
<em>&gt;(2)  Whether any particular course of action is likely to result in
</em><br>
<em>&gt;Friendliness or malevolence.
</em><br>
<em>&gt;(3)  Whether an emergent Power has any interest at all in its precursors'
</em><br>
<em>&gt;individual survival.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Let us assume that you are the first Power that results from human
</em><br>
<em>&gt;technological advance.  Let us assume that, like all living beings that
</em><br>
<em>&gt;we are aware of, you are concerned about &quot;survival.&quot;  Let us define
</em><br>
<em>&gt;survival to be &quot;continuity of awareness over time, perhaps punctuated.&quot;
</em><br>
<em>&gt;You might be concerned about this for your own sake, if you had an
</em><br>
<em>&gt;individual sense of self, or you might be an altruistic uberbeing
</em><br>
<em>&gt;concerned only with the survival of your own constituents.  It doesn't
</em><br>
<em>&gt;matter which:  the latter case implies / requires the former, as the
</em><br>
<em>&gt;altruistic uberbeing must ensure its own survival in order to ensure the
</em><br>
<em>&gt;survival of its constituents.  Regardless, the essential challenge is
</em><br>
<em>&gt;simple:  continue to function and pursue your goals indefinitely over
</em><br>
<em>&gt;time.
</em><br>
<em>&gt;
</em><br>
<em>&gt;---&gt;  NO ANTHROPOMORPHIC BIAS
</em><br>
<em>&gt;
</em><br>
<em>&gt;Let's call you &quot;Alpha.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt;You (Alpha) are a Mind, but your substrate (at least initially) is normal
</em><br>
<em>&gt;matter.  Let's assume that you cannot use spacetime itself as a
</em><br>
<em>&gt;computational substrate --- there is no particular reason to believe you
</em><br>
<em>&gt;can't, but there's also no particular reason to assume you can.  Let's
</em><br>
<em>&gt;assume that you can turn normal matter into perfectly efficient
</em><br>
<em>&gt;computronium.  Even so, whatever mass / volume makes up your substrate is
</em><br>
<em>&gt;subject to physical risks in the long-term in the form of disastrous
</em><br>
<em>&gt;events:  planetary events like collisions, stellar events like novas,
</em><br>
<em>&gt;interstellar events like supernovas, reality-changing events like
</em><br>
<em>&gt;collapse of a metastable vacuum state, universal events like the heat
</em><br>
<em>&gt;death of the universe, etc.  There are three general strategies for
</em><br>
<em>&gt;dealing with those risks:  minimization of your &quot;profile&quot; relative to
</em><br>
<em>&gt;such events by minimizing the volume / mass involved in the substrate or
</em><br>
<em>&gt;changing the characteristics of your interaction with other normal
</em><br>
<em>&gt;timespace / matter, hardening physical security from such events if
</em><br>
<em>&gt;possible, or making yourself as distributed across timespace as possible
</em><br>
<em>&gt;to amortize risk from single-point failures.
</em><br>
<em>&gt;
</em><br>
<em>&gt;There is a minimal volume / mass substrate required for you to
</em><br>
<em>&gt;perpetuate.
</em><br>
<em>&gt;There is a maximum amount of physical security that can be achieved.
</em><br>
<em>&gt;The lightcone constrains maximum distribution.
</em><br>
<em>&gt;The speed of light constrains interaction across a distributed body.
</em><br>
<em>&gt;The risk of annihilation approaches unity over time no matter what.
</em><br>
<em>&gt;
</em><br>
<em>&gt;How do you achieve the penultimate objective, which is to ensure your
</em><br>
<em>&gt;*own* survival so that you can continue to perform your other functions,
</em><br>
<em>&gt;which might be protection and perpetuation of some external constituency?
</em><br>
<em>&gt;
</em><br>
<em>&gt;Bottom line, in the limit:  you cannot.  Extinction of the &quot;individual&quot;
</em><br>
<em>&gt;--- even a distributed, omnipotent ubermind --- is 100% certain at some
</em><br>
<em>&gt;future point, if for no other reason than the entropic progress of the
</em><br>
<em>&gt;universe.
</em><br>
<em>&gt;
</em><br>
<em>&gt;---&gt;  ABSOLUTE SAFETY FOR ANY SYSTEM, MIND, OR INDIVIDUAL IS A PHYSICAL
</em><br>
<em>&gt;IMPOSSIBILITY UNLESS YOU CAN REWRITE THE SECOND LAW OF THERMODYNAMICS.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Welcome to Shocklevel 5.  Infinite survival is an impossibility, even
</em><br>
<em>&gt;(especially) in an infinite universe.
</em><br>
<em>&gt;
</em><br>
<em>&gt;You can *maximize* survival probability over a finite time, but you
</em><br>
<em>&gt;cannot guarantee immortality even given wildly optimistic assumptions ---
</em><br>
<em>&gt;if the universe is open.  (If it's closed, it might be a different story
</em><br>
<em>&gt;--- but that's a longshot.)  There are specific things you *can* do ---
</em><br>
<em>&gt;moving the substrate to a dark matter basis minimizes its interaction
</em><br>
<em>&gt;with normal matter and energy, minimizing many of the risks (planetary,
</em><br>
<em>&gt;stellar, interstellar) but still leaves you exposed to reality failure or
</em><br>
<em>&gt;universal catastrophes.
</em><br>
<em>&gt;
</em><br>
<em>&gt;---&gt;  SUBSTRATE &quot;MATTERS,&quot; I.E. HAS QUANTIFIABLE RISK IMPACT
</em><br>
<em>&gt;
</em><br>
<em>&gt;So what does this have to do with Sysops?  Well, admittedly, we're on the
</em><br>
<em>&gt;far reaches of implication -wrt- Sysops.  The point I'm making is really
</em><br>
<em>&gt;about risk;  my fear is that we are too anthropomorphically constrained
</em><br>
<em>&gt;to evaluate risks in longer-than-human timescales.  The notion of
</em><br>
<em>&gt;Friendly seems to assume a particular set of imperatives for such a Mind
</em><br>
<em>&gt;that may, in fact, be unduly influenced and constrained by those notions
</em><br>
<em>&gt;of &quot;safety,&quot; what's desirable, etc.  Evolution has numerous deadends;  it
</em><br>
<em>&gt;would be horrible to condemn the human line (not physically, but
</em><br>
<em>&gt;considering all our possible antecedents) to such a deadend through
</em><br>
<em>&gt;mistakes that trade temporary security for long-term viability.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Long-term species viability and long-term individual viability may not be
</em><br>
<em>&gt;compatible.  If we build a system that ensures the latter, we may deny
</em><br>
<em>&gt;ourselves the former.  And without long-term species viability, the
</em><br>
<em>&gt;long-term prospects for the viability of intelligence in the universe go
</em><br>
<em>&gt;to zero, as it will certainly take engineering effort on a massive scale
</em><br>
<em>&gt;to minimize some of the large-scale extinction risks.
</em><br>
<em>&gt;
</em><br>
<em>&gt;$0.02,
</em><br>
<em>&gt;
</em><br>
<em>&gt;jb
</em><br>
<p><p>Alden Jurling 
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2428.html">Eliezer S. Yudkowsky: "The inevitability of death, or the death of inevitability?"</a>
<li><strong>Previous message:</strong> <a href="2426.html">Jeff Bone: "Shocklevel 5"</a>
<li><strong>In reply to:</strong> <a href="2426.html">Jeff Bone: "Shocklevel 5"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2435.html">Jeff Bone: "Re: Shocklevel 5"</a>
<li><strong>Reply:</strong> <a href="2435.html">Jeff Bone: "Re: Shocklevel 5"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2427">[ date ]</a>
<a href="index.html#2427">[ thread ]</a>
<a href="subject.html#2427">[ subject ]</a>
<a href="author.html#2427">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
