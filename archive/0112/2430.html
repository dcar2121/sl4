<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Sysop yet again Re: New website: The Simulation Argument</title>
<meta name="Author" content="Brian Atkins (brian@posthuman.com)">
<meta name="Subject" content="Re: Sysop yet again Re: New website: The Simulation Argument">
<meta name="Date" content="2001-12-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Sysop yet again Re: New website: The Simulation Argument</h1>
<!-- received="Sat Dec 08 08:28:10 2001" -->
<!-- isoreceived="20011208152810" -->
<!-- sent="Sat, 08 Dec 2001 01:46:56 -0500" -->
<!-- isosent="20011208064656" -->
<!-- name="Brian Atkins" -->
<!-- email="brian@posthuman.com" -->
<!-- subject="Re: Sysop yet again Re: New website: The Simulation Argument" -->
<!-- id="3C11B760.3CB181EE@posthuman.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3C1126D7.D82278D4@jump.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brian Atkins (<a href="mailto:brian@posthuman.com?Subject=Re:%20Sysop%20yet%20again%20Re:%20New%20website:%20The%20Simulation%20Argument"><em>brian@posthuman.com</em></a>)<br>
<strong>Date:</strong> Fri Dec 07 2001 - 23:46:56 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2431.html">Gordon Worley: "Re: Shocklevel 5"</a>
<li><strong>Previous message:</strong> <a href="2429.html">tonyg@kcbbs.gen.nz: "Re: entropy and heat-death"</a>
<li><strong>In reply to:</strong> <a href="2421.html">Jeff Bone: "Re: Sysop yet again Re: New website: The Simulation Argument"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2434.html">Jeff Bone: "Re: Sysop yet again Re: New website: The Simulation Argument"</a>
<li><strong>Reply:</strong> <a href="2434.html">Jeff Bone: "Re: Sysop yet again Re: New website: The Simulation Argument"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2430">[ date ]</a>
<a href="index.html#2430">[ thread ]</a>
<a href="subject.html#2430">[ subject ]</a>
<a href="author.html#2430">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Jeff Bone wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Brian Atkins wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; Your scenario is very unlikely,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What, exactly, is unlikely about bad weather or earthquakes or any of a large
</em><br>
<p>Nothing. Superintelligences being unable to stop or predict earthquakes
<br>
is very unlikely. Which is what I said.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; The real question is the relationship between bits and atoms.  The less future
</em><br>
<em>&gt; civilizations rely on atoms --- the more bit-based we are --- the less we need
</em><br>
<em>&gt; consider physical actuarial threats.  I find the Sysop idea rather amusing in
</em><br>
<em>&gt; many ways;  the name of this list refers to the notion of future shock, but IMO
</em><br>
<em>&gt; there's a built-in amount of shock in assuming that we will prefer to interact
</em><br>
<em>&gt; with the physical universe to the extent we are forced to do so today;  also that
</em><br>
<em>&gt; we remain individuals, that our value system will place the concept of risk
</em><br>
<em>&gt; elimination over that of those things we will have to give up to achieve it,
</em><br>
<em>&gt; etc.  Also, to what extent does the concept of physical death really matter
</em><br>
<em>&gt; assuming the possibility of backups, etc?  I.e., all the concerns that we suppose
</em><br>
<em>&gt; a Friendly Sysop would have are built on a value system and set of assumptions
</em><br>
<em>&gt; that we Unascended have today.  There's no way to know how all that will appear
</em><br>
<em>&gt; to Posthuman perspective, but I'd wager that they're going to find it all quaint
</em><br>
<em>&gt; and amusing.
</em><br>
<p>Well Jeff maybe you'll be the one to figure out how to warp into the
<br>
Universe next door and keep it all to yourself :-) But the Sysop Scenario
<br>
comes from the idea that at least for a while we are going to be stuck
<br>
running on computronium or worse. We will not &quot;prefer&quot; that, it simply
<br>
is expected (barring magic physics) to be what we end up with. So I have
<br>
to shoot that remark of yours down. I will also shoot down your death
<br>
vs. copies remark since no matter how many copies you have floating
<br>
around the solar system, if all the atoms get taken over by a Blight you
<br>
will reach a state of &quot;complete&quot; death. There are things that even SIs
<br>
probably have to worry about. If you can accept that then you can accept
<br>
that some form of Sysop /may/ be needed even in that future time. But at
<br>
any rate, the whole Sysop thing is not exactly central here.
<br>
<p>What is much more important is /getting there/ in the first place. So
<br>
I have to agree with Gordon you seem to be stuck on something that has
<br>
little importance to pre-Singularity goings-on. The facts are that
<br>
sentiences of here-and-now do have certain things they want kept safe,
<br>
and ways must be found to accomplish this, preferrably with the least
<br>
amount of risk. It is quite possible as I said that the Transition
<br>
Guide, as it matures, decides that a Sysop is the wrong way to go and
<br>
it goes off and does something completely different. Friendliness is
<br>
not about expecting any kind of certain outcome other than the one that
<br>
is logically and rationally best for everyone, based upon what they want.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; but at any rate should a no-win situation
</em><br>
<em>&gt; &gt; become apparent the Sysop likely would simply inform the affected individuals
</em><br>
<em>&gt; &gt; of the situation and the lack of absolute safety, and let them decide what
</em><br>
<em>&gt; &gt; they want to do.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, that's fine, but doing so with ample time for the potentially effected
</em><br>
<em>&gt; individual ample lead time to avoid certain classes of disaster will require
</em><br>
<em>&gt; fine-grained simulation at a significant level.  And there're physical limits to
</em><br>
<em>&gt; the possible accuracy of simulation.
</em><br>
<p>I don't see what you're saying. In the case of earthquakes for instance
<br>
the Sysop would already know they exist. So it likely would immediately
<br>
upon noticing that &quot;earthquakes exist, and I have no way to predict or stop
<br>
them&quot; begin notifying the people and providing any alternatives it had
<br>
to them. Like I said, if they decide to stay around it's their own fault
<br>
when something bad happens.
<br>
<p>The only other class of no-wins are surprise situations like say a near-
<br>
light-speed black hole comes zooming into the solar system. Well as soon
<br>
as the Sysop's external sensors pick it up it would let everyone know to
<br>
clear out of the area.
<br>
<p>This kind of thing would of course not be perfect safety, it is simply
<br>
the best possible under physical limits. Still, can't beat that.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; Just to clarify, SIAI is not about building a Sysop. We are trying to
</em><br>
<em>&gt; &gt; build a seed mind that has low risk of unFriendliness, but what it chooses
</em><br>
<em>&gt; &gt; to develop into is up to it, and if stays Friendly then it will not
</em><br>
<em>&gt; &gt; choose to develop into something that is &quot;not worth the risks&quot;.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So you say.  IMO, that appears tautological.
</em><br>
<p>That latter part of my statement appears tautological, but the idea that
<br>
an AI can be designed such that it will stay Friendly is not.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; Your
</em><br>
<em>&gt; &gt; other choice BTW is to wait around until some other organization lets
</em><br>
<em>&gt; &gt; loose either by design or accident some sort of higher-risk A-mind.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Hey, despite the criticism --- which is intended to be constructive --- I'm
</em><br>
<em>&gt; rooting for you guys.  That is, UNTIL somebody comes along with a better plan
</em><br>
<em>&gt; that's less tightly tautological and more pragmatic. ;-)
</em><br>
<p>I don't think we've heard any criticism from you yet regarding either
<br>
CFAI or GISAI. If you have comments about the feasibility of either then
<br>
by all means let's drop the Sysop thread and get to the meat.
<br>
<p><em>&gt; &gt; I find your earthquake
</em><br>
<em>&gt; &gt; example to be very unconvincing... I was just reading in New Scientist
</em><br>
<em>&gt; &gt; a couple weeks ago about a new prediction theory for earthquakes and
</em><br>
<em>&gt; &gt; other sudden events so I think a superintelligence will be able to find
</em><br>
<em>&gt; &gt; a way to predict these events, or even if it has to simulate the whole
</em><br>
<em>&gt; &gt; damn planet it can do that too quite easily, in fact it could probably
</em><br>
<em>&gt; &gt; use a very small chunk of the Earth for the needed hardware assuming
</em><br>
<em>&gt; &gt; computronium really is the best way to compute. Of course why bother
</em><br>
<em>&gt; &gt; when it probably has the technology to completely eliminate earthquakes.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Okay, forget about earthquakes.  What about interstellar neighborhood stellar
</em><br>
<em>&gt; disaster chains?  Etc. etc. etc.  The risk to perpetual existance across the
</em><br>
<em>&gt; board approaches unity;  attempting to build a rational agent who has as a goal
</em><br>
<em>&gt; elimination of involuntary risk assumption is a quixotic and perhaps irrational
</em><br>
<em>&gt; task.  OTOH, very different proposition if the goal is minimization of risk and
</em><br>
<em>&gt; the scope of such activity is constrained.
</em><br>
<p>The only /real/ problem I see is heat death. By replicating myself and mailing
<br>
off copies to different areas of the Universe I can avoid any kind of
<br>
problems with random nasty events. Plus you can send out nano space probes
<br>
to map each and every nasty little bit out there so you can predict it
<br>
all 10 billion years in advance. It's really not that difficult. The real
<br>
risks come from within, from unFriendly SIs. Depending on what the physics
<br>
are like, you might end up with an intelligence arms race where if you
<br>
don't keep up with Jones' garage of 3 jupiter brains then your nanodefenses
<br>
become no match for theirs and you get eaten. Or something like that. I
<br>
would personally prefer that rather than force everyone in the Universe
<br>
be constantly maxed out in intelligence and defenses, we could instead
<br>
develop some system to notice when someone goes rogue and have the Sysop
<br>
take them out of circulation before they are a threat to anyone. But like
<br>
I said who can really say how it all will turn out.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; Personally I don't see how preventing 99.999% of bad stuff is an
</em><br>
<em>&gt; &gt; unworthy goal.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It's not --- it's an extremely noble goal.  The question isn't the goal, it's the
</em><br>
<em>&gt; practicality of the path.  &quot;The road to Hell is paved with good intentions&quot; and
</em><br>
<em>&gt; all that.  My perspective is that pursuing this coarse is very desirable given
</em><br>
<em>&gt; the alternatives, but IMO we should be careful to be realistic and not pollyannas
</em><br>
<em>&gt; about it.
</em><br>
<p>Right, well like I said, I trust a Friendly SI to be able to figure out
<br>
pretty easily whether it is practical or not. The Sysop thing is just a
<br>
thought experiment that is rather unlikely to be exactly how it actually
<br>
turns out.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; A Sysop is not about providing perfect safety, it is
</em><br>
<em>&gt; &gt; about creating the most perfect universe possible
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The crux of my issue is this:  &quot;most perfect universe&quot; is underdefined, and
</em><br>
<em>&gt; indeed perhaps undefinable in any universally mutually agreeable fashion.
</em><br>
<p>It's on a person by person basis with the Sysop breaking ties :-) That's
<br>
my story, and I'm sticking to it :-)
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; while still within
</em><br>
<em>&gt; &gt; the physical laws we are all apparently stuck with. Even if it turns
</em><br>
<em>&gt; &gt; out it can only prevent 10% of the bad stuff then that is worth
</em><br>
<em>&gt; &gt; doing- why wouldn't it be?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There's always a trade-off between safety and liberty.  Consider how security
</em><br>
<em>&gt; fears over 9-11 are impacting civil liberties already.  One of my biggest fears
</em><br>
<em>&gt; re: the social impact of accelerating technology isn't that a Power takes over
</em><br>
<em>&gt; --- which IMO is unavoidable, really --- but that fear and security concerns
</em><br>
<em>&gt; trump concerns over progress.  Friendliness IMO seems largely to be about making
</em><br>
<em>&gt; the world a safe place --- but &quot;safe&quot; is a subjective value judgement, and IMO it
</em><br>
<em>&gt; may be dangerous to task (or even just ask) a Power to provide it.
</em><br>
<p>I think your claim that there's always a tradeoff is wrong. For instance I
<br>
can inject myself with some nanobots that prevent all kinds of internal
<br>
diseases which would increase my safety without hurting my liberty one
<br>
iota- in fact it would likely increase my liberty because I could then
<br>
eat more bad food :-)
<br>
<p>Friendliness also BTW is not necessarily about making the world a safe
<br>
place. As I said, it is a completely different topic and aim from the
<br>
Sysop discussion. Friendliness is strictly about how do you build an AI
<br>
that will be &quot;nice&quot;. If all AI designers build their AIs this way then
<br>
yes it will make the world safer than it would have been otherwise, but
<br>
in terms of directly increasing the safety of people's lives past that
<br>
you are now talking about individual decisions being made by the FAI.
<br>
For all we know the FAI may determine that the best thing to do would
<br>
be to fly off the planet and never talk to us again, although of course
<br>
that looks unlikely. It will likely decide that making the Universe safer
<br>
is a Good Thing, but who knows how it will accomplish that? It may
<br>
decide that the best way is to upgrade everyone to superintelligence
<br>
so they can perceive the objective morality it has discovered. Or it
<br>
may decide a Sysop is needed. Whatever it decides you can be sure it
<br>
will make the decision only after fully understanding what you want,
<br>
because what /it/ will want is to help you.
<br>
<p>You read this, right? <a href="http://www.intelligence.org/CFAI/info/indexfaq.html#q_1">http://www.intelligence.org/CFAI/info/indexfaq.html#q_1</a>
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; P.S. I reiterate no one is tasking a Power to do anything. Powers decide
</em><br>
<em>&gt; &gt; for themselves what they want to do :-)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You'd better believe it, buddy!  I am in total agreement with this statement.
</em><br>
<em>&gt; The fundamental failure of Friendliness (as I understand it, based on a few
</em><br>
<em>&gt; reads) is that it attempts to constrain such a Power, and tautologically brushes
</em><br>
<em>&gt; off any arguments for why such constraints might or might not be desirable,
</em><br>
<em>&gt; achievable, etc.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Still, like I said, it's the best shot we've got today, so my criticism should be
</em><br>
<em>&gt; taken exactly as it's intended --- constructively, informatively.
</em><br>
<em>&gt; 
</em><br>
<p>Well we can't respond unless you want to point out specific instances of
<br>
the alleged failures.
<br>
<pre>
-- 
Brian Atkins
Singularity Institute for Artificial Intelligence
<a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2431.html">Gordon Worley: "Re: Shocklevel 5"</a>
<li><strong>Previous message:</strong> <a href="2429.html">tonyg@kcbbs.gen.nz: "Re: entropy and heat-death"</a>
<li><strong>In reply to:</strong> <a href="2421.html">Jeff Bone: "Re: Sysop yet again Re: New website: The Simulation Argument"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2434.html">Jeff Bone: "Re: Sysop yet again Re: New website: The Simulation Argument"</a>
<li><strong>Reply:</strong> <a href="2434.html">Jeff Bone: "Re: Sysop yet again Re: New website: The Simulation Argument"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2430">[ date ]</a>
<a href="index.html#2430">[ thread ]</a>
<a href="subject.html#2430">[ subject ]</a>
<a href="author.html#2430">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
