<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: The inevitability of death, or the death of inevitability?</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="The inevitability of death, or the death of inevitability?">
<meta name="Date" content="2001-12-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>The inevitability of death, or the death of inevitability?</h1>
<!-- received="Fri Dec 07 22:15:27 2001" -->
<!-- isoreceived="20011208051527" -->
<!-- sent="Fri, 07 Dec 2001 22:14:07 -0500" -->
<!-- isosent="20011208031407" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="The inevitability of death, or the death of inevitability?" -->
<!-- id="3C11857F.9A8C2691@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3C116F66.AAEAA6A3@jump.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20The%20inevitability%20of%20death,%20or%20the%20death%20of%20inevitability?"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri Dec 07 2001 - 20:14:07 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2429.html">tonyg@kcbbs.gen.nz: "Re: entropy and heat-death"</a>
<li><strong>Previous message:</strong> <a href="2427.html">Alden Jurling: "Re: Shocklevel 5"</a>
<li><strong>In reply to:</strong> <a href="2426.html">Jeff Bone: "Shocklevel 5"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2433.html">Jeff Bone: "Re: The inevitability of death, or the death of inevitability?"</a>
<li><strong>Reply:</strong> <a href="2433.html">Jeff Bone: "Re: The inevitability of death, or the death of inevitability?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2428">[ date ]</a>
<a href="index.html#2428">[ thread ]</a>
<a href="subject.html#2428">[ subject ]</a>
<a href="author.html#2428">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Jeff Bone wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; How do you achieve the penultimate objective, which is to ensure your
</em><br>
<em>&gt; *own* survival so that you can continue to perform your other functions,
</em><br>
<em>&gt; which might be protection and perpetuation of some external constituency?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Bottom line, in the limit:  you cannot.  Extinction of the &quot;individual&quot;
</em><br>
<em>&gt; --- even a distributed, omnipotent ubermind --- is 100% certain at some
</em><br>
<em>&gt; future point, if for no other reason than the entropic progress of the
</em><br>
<em>&gt; universe.
</em><br>
<p>Don't you think we're too young, as a species, to be making judgements
<br>
about that?  I do think there's a possibility that, in the long run, the
<br>
probability of extinction approaches unity - for both individual and
<br>
species, albeit with different time constants.  I think this simply
<br>
because forever is such a very, very long time.  I don't think that what
<br>
zaps us will be the second law of thermodynamics, the heat death of the
<br>
Universe, the Big Crunch, et cetera, because these all seem like the kind
<br>
of dooms that keep getting revised every time our model of the laws of
<br>
physics changes.  It seems pretty likely to me that we can outlast 10^31
<br>
years.  Living so long that it has to be expressed in Knuth notation is a
<br>
separate issue.  Our current universe may let us get away with big
<br>
exponents, but it just doesn't seem to be really suited to doing things
<br>
that require Knuth notation.
<br>
<p>But I can't see this as something to lose sweat about.  Lasting
<br>
cognitively for 10^31 years is something I can at least touch with my
<br>
imagination; 3^^^^3 years is simply beyond me.
<br>
<p><em>&gt; ---&gt;  ABSOLUTE SAFETY FOR ANY SYSTEM, MIND, OR INDIVIDUAL IS A PHYSICAL
</em><br>
<em>&gt; IMPOSSIBILITY UNLESS YOU CAN REWRITE THE SECOND LAW OF THERMODYNAMICS.
</em><br>
<p>I don't think the laws of physics have settled down yet.  I admit of the
<br>
possibility that the limits which appear under current physical law are
<br>
absolute, even though most of them have highly speculative and
<br>
controversial workarounds scattered through the physics literature.  I'm
<br>
not trying to avoid confronting the possibility; I'm just saying that the
<br>
real probability we need to worry about is probably less than 30%, and
<br>
that the foregoing statement could easily wind up looking completely
<br>
ridiculous (&quot;How could any sane being assign that a probability of more
<br>
than 1%?&quot;) in a few years.
<br>
<p><em>&gt; So what does this have to do with Sysops?
</em><br>
<p>Yes, that is the question...
<br>
<p><em>&gt; The point I'm making is really
</em><br>
<em>&gt; about risk;  my fear is that we are too anthropomorphically constrained
</em><br>
<em>&gt; to evaluate risks in longer-than-human timescales.
</em><br>
<p>I agree, especially if we have to move from exponential notation to Knuth
<br>
notation in order to express the quantities involved.  (I usually talk
<br>
about Knuth notation rather than infinity because Knuth notation seems
<br>
larger.)
<br>
<p><em>&gt; The notion of
</em><br>
<em>&gt; Friendly seems to assume a particular set of imperatives for such a Mind
</em><br>
<em>&gt; that may, in fact, be unduly influenced and constrained by those notions
</em><br>
<em>&gt; of &quot;safety,&quot; what's desirable, etc.
</em><br>
<p>And here, of course, is where the real disagreement lies.
<br>
<p>Under what circumstances is a Sysop Scenario necessary and desirable?  It
<br>
is not necessary to protect individuals from the environment, except
<br>
during the very early phases of those individuals' lifespan; even if we
<br>
wished to prolong our merely human phase, we would still eventually
<br>
outgrow the need for any external protection, and would become capable of
<br>
protecting ourselves to whatever degree we found desirable.  That's been
<br>
going on for millennia already and we're already a lot better at it than
<br>
we were a few centuries ago.  It may or may not be necessary to protect
<br>
transhuman individuals from one another; it could be that under higher
<br>
levels of technology defense massively outweighs offense.  However, even
<br>
within an impregnable barrier, there is still the possibility of the
<br>
violation of sentient rights; a mind can take matter under its strict
<br>
control and transform it into a simulation of a mind advanced enough to be
<br>
deserving of citizenship rights.  If, as is quite possible, any
<br>
sufficiently advanced mind has zero probability of engaging in such
<br>
shenanigans, then the whole intelligent substrate proposal - Unix Reality,
<br>
Sysop Scenario, whatever - would be completely unnecessary except for
<br>
humans and during the very early stages of transhumanity.
<br>
<p>I think that ruling out the possibility of an unimaginable number of
<br>
sentient entities being deprived of citizenship rights, and/or the
<br>
possibility of species extinction due to inter-entity warfare, would both
<br>
be sufficient cause for intelligent substrate, if intelligent substrate
<br>
were the best means to accomplish those ends.  Is it actually the best? 
<br>
Who knows?  All I know is that at my current level of intelligence, this
<br>
is the means which I can imagine.  Maybe you can just delete certain
<br>
sections of the branching tree of probabilities, or change the nature of
<br>
reality so that the emergent character of the universe shifts from
<br>
&quot;neutral&quot; to &quot;benevolent&quot;, but I have absolutely no idea how to go about
<br>
doing those things.
<br>
<p>Whether we are all DOOMED in the long run seems to me like an orthogonal
<br>
issue.  If everyone is doomed, then everyone will still be doomed under an
<br>
intelligent substrate scenario, but at least everyone will be doomed by
<br>
inevitable environmental conditions, not by warfare or unethical creators.
<br>
<p>Eliminate negative events if possible.  Minimize their probability or
<br>
delay them otherwise.  Why would a correctly created Friendly AI be unable
<br>
to understand this?  Why would any real AI, Friendly or otherwise, break
<br>
down on dealing with nonabsolutes?
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2429.html">tonyg@kcbbs.gen.nz: "Re: entropy and heat-death"</a>
<li><strong>Previous message:</strong> <a href="2427.html">Alden Jurling: "Re: Shocklevel 5"</a>
<li><strong>In reply to:</strong> <a href="2426.html">Jeff Bone: "Shocklevel 5"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2433.html">Jeff Bone: "Re: The inevitability of death, or the death of inevitability?"</a>
<li><strong>Reply:</strong> <a href="2433.html">Jeff Bone: "Re: The inevitability of death, or the death of inevitability?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2428">[ date ]</a>
<a href="index.html#2428">[ thread ]</a>
<a href="subject.html#2428">[ subject ]</a>
<a href="author.html#2428">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
