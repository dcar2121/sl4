<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The inevitability of death, or the death of inevitability?</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: The inevitability of death, or the death of inevitability?">
<meta name="Date" content="2001-12-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The inevitability of death, or the death of inevitability?</h1>
<!-- received="Sat Dec 08 18:26:10 2001" -->
<!-- isoreceived="20011209012610" -->
<!-- sent="Sat, 08 Dec 2001 18:21:09 -0500" -->
<!-- isosent="20011208232109" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: The inevitability of death, or the death of inevitability?" -->
<!-- id="3C12A065.A1594C6E@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="JBEPKOGDDIKKAHFPOEFIOEPGCLAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20The%20inevitability%20of%20death,%20or%20the%20death%20of%20inevitability?"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Dec 08 2001 - 16:21:09 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2440.html">Jeff Bone: "Re: The inevitability of death, or the death of inevitability?"</a>
<li><strong>Previous message:</strong> <a href="2438.html">Jeff Bone: "A more concise and quantifiable definition of Friendliness?"</a>
<li><strong>In reply to:</strong> <a href="2437.html">Ben Goertzel: "RE: The inevitability of death, or the death of inevitability?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2441.html">Jeff Bone: "Re: The inevitability of death, or the death of inevitability?"</a>
<li><strong>Reply:</strong> <a href="2441.html">Jeff Bone: "Re: The inevitability of death, or the death of inevitability?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2439">[ date ]</a>
<a href="index.html#2439">[ thread ]</a>
<a href="subject.html#2439">[ subject ]</a>
<a href="author.html#2439">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; For what it's worth, my intuition agrees with Eli's on this.  We have yet
</em><br>
<em>&gt; explored only a very small part of the &quot;known universe&quot;, and there are big
</em><br>
<em>&gt; aspects of particle physics that we don't yet come close to understanding
</em><br>
<em>&gt; (e.g. quantum gravity, quantum measurement -- yes, there are claims of
</em><br>
<em>&gt; understanding, but nothing well-substantiated).  To assume that the Big Bang
</em><br>
<em>&gt; /Big Crunch model or any other fragment of modern science is going to
</em><br>
<em>&gt; survive untouched 1000 years from now is just plain silly.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The same intuition leads me to believe that the notion of &quot;designing
</em><br>
<em>&gt; Friendly AI&quot; is only slightly relevant to the final outcome of the grand
</em><br>
<em>&gt; &quot;superhuman AI engineering&quot; experiment.  I agree that it's a worthwhile
</em><br>
<em>&gt; pursuit, because it has a clearly &gt; 0 chance of making a difference.  But as
</em><br>
<em>&gt; with fundamental physics, there's a hell of a lot we don't understand about
</em><br>
<em>&gt; minds...
</em><br>
<p>Well, the CFAI model doesn't require that the creators know everything
<br>
about the human mind.  It requires a certain bounded amount of complexity
<br>
which is used to construct an unambiguous pointer to unknown facts about
<br>
the human mind, facts which may not be known now, but which are expected
<br>
to be accessible to a transhuman intelligence.
<br>
<p>In other words, under the CFAI model, you can say:  &quot;I have this vague
<br>
feeling that liberty and life and love and laughter are important, but I'm
<br>
not sure about it, and I don't know where the feeling comes from.  Count
<br>
that in, okay?&quot;  The physical causation behind this statement - in your
<br>
accumulated experience, in your brainware, in your genes - is in principle
<br>
accessible to a transhuman intelligence, even one that has to extrapolate
<br>
the causes after the event.  The Friendly AI can then intelligently use
<br>
existing philosophical complexity to decide which of these causes are
<br>
valid and should be absorbed.  The Friendly AI can then &quot;repeat&quot; the above
<br>
statement at a higher level of intelligence - that is, having absorbed the
<br>
moral baseline behind the statement, it can re-produce the statement as
<br>
you would have produced it at a higher intelligence level.
<br>
<p>So what's needed is the threshold level of moral complexity to understand
<br>
how to correctly use pointers like the one described above - not a
<br>
complete diagram of human moral complexity, or a complete understanding of
<br>
transhuman philosophy.  That threshold level of complexity - which is big,
<br>
but bounded, and hopefully accessible to merely human understanding - is
<br>
what CFAI attempts to describe.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2440.html">Jeff Bone: "Re: The inevitability of death, or the death of inevitability?"</a>
<li><strong>Previous message:</strong> <a href="2438.html">Jeff Bone: "A more concise and quantifiable definition of Friendliness?"</a>
<li><strong>In reply to:</strong> <a href="2437.html">Ben Goertzel: "RE: The inevitability of death, or the death of inevitability?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2441.html">Jeff Bone: "Re: The inevitability of death, or the death of inevitability?"</a>
<li><strong>Reply:</strong> <a href="2441.html">Jeff Bone: "Re: The inevitability of death, or the death of inevitability?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2439">[ date ]</a>
<a href="index.html#2439">[ thread ]</a>
<a href="subject.html#2439">[ subject ]</a>
<a href="author.html#2439">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
