<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Sysop yet again Re: New website: The Simulation Argument</title>
<meta name="Author" content="Jeff Bone (jbone@jump.net)">
<meta name="Subject" content="Re: Sysop yet again Re: New website: The Simulation Argument">
<meta name="Date" content="2001-12-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Sysop yet again Re: New website: The Simulation Argument</h1>
<!-- received="Fri Dec 07 15:36:55 2001" -->
<!-- isoreceived="20011207223655" -->
<!-- sent="Fri, 07 Dec 2001 14:30:15 -0600" -->
<!-- isosent="20011207203015" -->
<!-- name="Jeff Bone" -->
<!-- email="jbone@jump.net" -->
<!-- subject="Re: Sysop yet again Re: New website: The Simulation Argument" -->
<!-- id="3C1126D7.D82278D4@jump.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3C1067C2.B2E504D0@posthuman.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Jeff Bone (<a href="mailto:jbone@jump.net?Subject=Re:%20Sysop%20yet%20again%20Re:%20New%20website:%20The%20Simulation%20Argument"><em>jbone@jump.net</em></a>)<br>
<strong>Date:</strong> Fri Dec 07 2001 - 13:30:15 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2422.html">Jeff Bone: "Re: New website: The Simulation Argument"</a>
<li><strong>Previous message:</strong> <a href="2420.html">Brian Atkins: "Sysop yet again Re: New website: The Simulation Argument"</a>
<li><strong>In reply to:</strong> <a href="2420.html">Brian Atkins: "Sysop yet again Re: New website: The Simulation Argument"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2430.html">Brian Atkins: "Re: Sysop yet again Re: New website: The Simulation Argument"</a>
<li><strong>Reply:</strong> <a href="2430.html">Brian Atkins: "Re: Sysop yet again Re: New website: The Simulation Argument"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2421">[ date ]</a>
<a href="index.html#2421">[ thread ]</a>
<a href="subject.html#2421">[ subject ]</a>
<a href="author.html#2421">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Brian Atkins wrote:
<br>
<p><em>&gt; Your scenario is very unlikely,
</em><br>
<p>What, exactly, is unlikely about bad weather or earthquakes or any of a large
<br>
number of other actuarial threats?  To put this in context:  I saw an analysis
<br>
(done by an actual actuarial ;-) some time ago which concluded that even if
<br>
disease, old age, and intentional violence were eliminated as causes of human
<br>
death, the average lifespan of a human being would still only be approximately
<br>
600 years.  That is, the mortality rate at about 1200 years approaches 100% due
<br>
to the year-by-year likelihood of dying in an &quot;accident&quot; (i.e., being physically
<br>
destroyed) given current accident rates.  Assuming that we continue to exist
<br>
primarily physically, the maximum amount we can increase longevity that is
<br>
constrained by physics.  At the boundaries, it's constrained by 2LT.
<br>
<p>The real question is the relationship between bits and atoms.  The less future
<br>
civilizations rely on atoms --- the more bit-based we are --- the less we need
<br>
consider physical actuarial threats.  I find the Sysop idea rather amusing in
<br>
many ways;  the name of this list refers to the notion of future shock, but IMO
<br>
there's a built-in amount of shock in assuming that we will prefer to interact
<br>
with the physical universe to the extent we are forced to do so today;  also that
<br>
we remain individuals, that our value system will place the concept of risk
<br>
elimination over that of those things we will have to give up to achieve it,
<br>
etc.  Also, to what extent does the concept of physical death really matter
<br>
assuming the possibility of backups, etc?  I.e., all the concerns that we suppose
<br>
a Friendly Sysop would have are built on a value system and set of assumptions
<br>
that we Unascended have today.  There's no way to know how all that will appear
<br>
to Posthuman perspective, but I'd wager that they're going to find it all quaint
<br>
and amusing.
<br>
<p><em>&gt; but at any rate should a no-win situation
</em><br>
<em>&gt; become apparent the Sysop likely would simply inform the affected individuals
</em><br>
<em>&gt; of the situation and the lack of absolute safety, and let them decide what
</em><br>
<em>&gt; they want to do.
</em><br>
<p>Well, that's fine, but doing so with ample time for the potentially effected
<br>
individual ample lead time to avoid certain classes of disaster will require
<br>
fine-grained simulation at a significant level.  And there're physical limits to
<br>
the possible accuracy of simulation.
<br>
<p><em>&gt; Just to clarify, SIAI is not about building a Sysop. We are trying to
</em><br>
<em>&gt; build a seed mind that has low risk of unFriendliness, but what it chooses
</em><br>
<em>&gt; to develop into is up to it, and if stays Friendly then it will not
</em><br>
<em>&gt; choose to develop into something that is &quot;not worth the risks&quot;.
</em><br>
<p>So you say.  IMO, that appears tautological.
<br>
<p><em>&gt; Your
</em><br>
<em>&gt; other choice BTW is to wait around until some other organization lets
</em><br>
<em>&gt; loose either by design or accident some sort of higher-risk A-mind.
</em><br>
<p>Hey, despite the criticism --- which is intended to be constructive --- I'm
<br>
rooting for you guys.  That is, UNTIL somebody comes along with a better plan
<br>
that's less tightly tautological and more pragmatic. ;-)
<br>
<p><em>&gt; The &quot;let's just don't ever build an A-mind&quot; choice is almost certainly
</em><br>
<em>&gt; a fantasy so it is not up for consideration.
</em><br>
<p>Not suggesting it, would never suggest it, IMO SAI can't happen fast enough.  I'm
<br>
just suspicious both of the Friendliness argument itself (mechanically) and the
<br>
notion in general.
<br>
<p><em>&gt; I think you face potential disasters from external events no matter what
</em><br>
<em>&gt; substrate so I don't see that it matters much.
</em><br>
<p>Yes, but if your required physical substrate is minimized in volume, mass, and
<br>
other physical characteristics then it is minimally exposed to actuarial risks.
<br>
Any civ running as a simulation on a &quot;well protected&quot; physical substrate is at
<br>
lower risk than, say, a planet-based civ.  And choice of substrate (and physical
<br>
architecture) for running such a sim has significant impact on risks;  a
<br>
world-size normal-matter singleton simulator running a civ is much more at-risk
<br>
from a number of different classes of disaster than a civ running on, say, a
<br>
distributed swarm of independent computational units made from dark matter.  (I'm
<br>
not supposing anything about the practicality of either, just trying to
<br>
illustrate why substrate matters.)
<br>
<p><em>&gt; I find your earthquake
</em><br>
<em>&gt; example to be very unconvincing... I was just reading in New Scientist
</em><br>
<em>&gt; a couple weeks ago about a new prediction theory for earthquakes and
</em><br>
<em>&gt; other sudden events so I think a superintelligence will be able to find
</em><br>
<em>&gt; a way to predict these events, or even if it has to simulate the whole
</em><br>
<em>&gt; damn planet it can do that too quite easily, in fact it could probably
</em><br>
<em>&gt; use a very small chunk of the Earth for the needed hardware assuming
</em><br>
<em>&gt; computronium really is the best way to compute. Of course why bother
</em><br>
<em>&gt; when it probably has the technology to completely eliminate earthquakes.
</em><br>
<p>Okay, forget about earthquakes.  What about interstellar neighborhood stellar
<br>
disaster chains?  Etc. etc. etc.  The risk to perpetual existance across the
<br>
board approaches unity;  attempting to build a rational agent who has as a goal
<br>
elimination of involuntary risk assumption is a quixotic and perhaps irrational
<br>
task.  OTOH, very different proposition if the goal is minimization of risk and
<br>
the scope of such activity is constrained.
<br>
<p><em>&gt; Unforseen surprises from outside the solar system seem like the only
</em><br>
<em>&gt; real threats, but perhaps you have some other ideas.
</em><br>
<p>Collapse of a metastable vacuum state triggered by local particle accelerator
<br>
experiments.  Passing through a large interstellar cloud of dark matter.
<br>
Supernova in the neighborhood.  Accidental creation of a black hole.
<br>
&quot;Information clogging&quot; of the universe due to the observation effects of
<br>
computronium on the quantum fabric.  Etc. etc. etc.  Most of these are probably
<br>
pure fantasy, but just as surely as that's true there are huge classes of risk we
<br>
haven't even considered.
<br>
<p><em>&gt; Personally I don't see how preventing 99.999% of bad stuff is an
</em><br>
<em>&gt; unworthy goal.
</em><br>
<p>It's not --- it's an extremely noble goal.  The question isn't the goal, it's the
<br>
practicality of the path.  &quot;The road to Hell is paved with good intentions&quot; and
<br>
all that.  My perspective is that pursuing this coarse is very desirable given
<br>
the alternatives, but IMO we should be careful to be realistic and not pollyannas
<br>
about it.
<br>
<p><em>&gt; A Sysop is not about providing perfect safety, it is
</em><br>
<em>&gt; about creating the most perfect universe possible
</em><br>
<p>The crux of my issue is this:  &quot;most perfect universe&quot; is underdefined, and
<br>
indeed perhaps undefinable in any universally mutually agreeable fashion.
<br>
<p><em>&gt; while still within
</em><br>
<em>&gt; the physical laws we are all apparently stuck with. Even if it turns
</em><br>
<em>&gt; out it can only prevent 10% of the bad stuff then that is worth
</em><br>
<em>&gt; doing- why wouldn't it be?
</em><br>
<p>There's always a trade-off between safety and liberty.  Consider how security
<br>
fears over 9-11 are impacting civil liberties already.  One of my biggest fears
<br>
re: the social impact of accelerating technology isn't that a Power takes over
<br>
--- which IMO is unavoidable, really --- but that fear and security concerns
<br>
trump concerns over progress.  Friendliness IMO seems largely to be about making
<br>
the world a safe place --- but &quot;safe&quot; is a subjective value judgement, and IMO it
<br>
may be dangerous to task (or even just ask) a Power to provide it.
<br>
<p><em>&gt; P.S. I reiterate no one is tasking a Power to do anything. Powers decide
</em><br>
<em>&gt; for themselves what they want to do :-)
</em><br>
<p>You'd better believe it, buddy!  I am in total agreement with this statement.
<br>
The fundamental failure of Friendliness (as I understand it, based on a few
<br>
reads) is that it attempts to constrain such a Power, and tautologically brushes
<br>
off any arguments for why such constraints might or might not be desirable,
<br>
achievable, etc.
<br>
<p>Still, like I said, it's the best shot we've got today, so my criticism should be
<br>
taken exactly as it's intended --- constructively, informatively.
<br>
<p>jb
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2422.html">Jeff Bone: "Re: New website: The Simulation Argument"</a>
<li><strong>Previous message:</strong> <a href="2420.html">Brian Atkins: "Sysop yet again Re: New website: The Simulation Argument"</a>
<li><strong>In reply to:</strong> <a href="2420.html">Brian Atkins: "Sysop yet again Re: New website: The Simulation Argument"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2430.html">Brian Atkins: "Re: Sysop yet again Re: New website: The Simulation Argument"</a>
<li><strong>Reply:</strong> <a href="2430.html">Brian Atkins: "Re: Sysop yet again Re: New website: The Simulation Argument"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2421">[ date ]</a>
<a href="index.html#2421">[ thread ]</a>
<a href="subject.html#2421">[ subject ]</a>
<a href="author.html#2421">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
