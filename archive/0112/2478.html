<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Deconstructing Eli:  A Final Cautionary Note</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Deconstructing Eli:  A Final Cautionary Note">
<meta name="Date" content="2001-12-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Deconstructing Eli:  A Final Cautionary Note</h1>
<!-- received="Sun Dec 09 16:41:07 2001" -->
<!-- isoreceived="20011209234107" -->
<!-- sent="Sun, 09 Dec 2001 16:39:53 -0500" -->
<!-- isosent="20011209213953" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Deconstructing Eli:  A Final Cautionary Note" -->
<!-- id="3C13DA29.27F80831@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3C13C9E0.3887CB68@jump.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Deconstructing%20Eli:%20%20A%20Final%20Cautionary%20Note"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Dec 09 2001 - 14:39:53 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2479.html">Damien Broderick: "Re: A billion years later..."</a>
<li><strong>Previous message:</strong> <a href="2477.html">Eliezer S. Yudkowsky: "Re: META: Off topic, of course"</a>
<li><strong>In reply to:</strong> <a href="2476.html">Jeff Bone: "Deconstructing Eli:  A Final Cautionary Note"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2466.html">Jeff Bone: "Re: The inevitability of death, or the death of inevitability?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2478">[ date ]</a>
<a href="index.html#2478">[ thread ]</a>
<a href="subject.html#2478">[ subject ]</a>
<a href="author.html#2478">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Rather than try to reply to all of this, I'm just going to try and correct
<br>
the specific dangling points where I think an idea is being
<br>
misrepresented, and I don't want to leave it that way.  Rather than taking
<br>
the argument any further, that is.  Some of these are things I do care
<br>
about, such as an accusation of being anti-logic in general, when I was
<br>
asserting distrust of a certain ontology.
<br>
<p>Some people complain about list quality, some people complain about my
<br>
leaving arguments unfinished, and everyone has so many other interesting
<br>
uses for my time.  Sigh.  Some days you just can't win, can't break even,
<br>
and can't get out of the game.
<br>
<p>Jeff Bone wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &quot;Eliezer S. Yudkowsky&quot; wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'm not saying you are incorrect about any of your assertions, just that it's sloppy to assume (as
</em><br>
<em>&gt; you have above) that the availability of some speculative loopholes in some (classes of)
</em><br>
<em>&gt; currently- accepted physical laws implies there will similarly significant loopholes in all
</em><br>
<em>&gt; (classes of) currently- accepted physical laws.
</em><br>
<p>You are transforming a sensible probabilistic argument into a nonsensical
<br>
absolute.  I do not argue that the past history of science, or the current
<br>
state of research papers, definitely show that useful loopholes exist for
<br>
all laws.  I am saying that they constitute significant but not definite
<br>
evidence against the assertion that a given law is knowably absolute.  I
<br>
am not claiming proof.  I am adding a single weight to the scales.
<br>
<p><em>&gt; &gt; I don't trust human conceptions
</em><br>
<em>&gt; &gt; about &quot;logic&quot; because past experience has shown that the universe often
</em><br>
<em>&gt; &gt; defies our intuitive conception of logic.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ELI DROPS THE BALL #3:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, I think that about wraps things
</em><br>
<em>&gt; up;  if the results of our best efforts to use mathematical (logical) reasoning to discuss various
</em><br>
<em>&gt; issues cannot be trusted simply because human conceptions of &quot;logic&quot; are invalid, then we're
</em><br>
<em>&gt; done.  I'll proceed to parse through the rest of this and bat away objections, but Eli --- you
</em><br>
<em>&gt; just shot yourself in the foot -wrt- ever again having a &quot;rational&quot; conversation with a skeptic.
</em><br>
<em>&gt; :-(  And I'm not even a skeptic, I'm a believer playing devil's advocate.
</em><br>
<p>Oh, come now!  I don't mean that I distrust logic in the classical sense
<br>
of simple rationality, or that I prefer emotion to logic.  I mean that I
<br>
distrust human logical formalisms as an alleged foundation of our
<br>
universe's basic ontology.  For example, a Turing machine has a single
<br>
space of simultaneity and our universe does not (although this does not
<br>
alone affect computability).  For example, our innate conception of cause
<br>
and effect runs into an infinite recursion problem which our universe
<br>
clearly manages to resolve one way or another.  And Penrose once made the
<br>
interesting point, with which I agree, that mathematics should be treated
<br>
as a very, very well-confirmed physical theory rather than as a logical
<br>
truth.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/\/\
<br>
But I still \  / the Bayesian Probability Theorem.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\/
<br>
<p>Incidentally, I don't think it's irrational to distrust rationality.  Real
<br>
skepticism should apply to everything, including skepticism.  The point at
<br>
which my alarms go off is when I hear someone claiming that there is some
<br>
specific thing that they trust *more* than rationality.
<br>
<p><em>&gt; &gt; Only if you think that &quot;Moore's Law is likely to continue for the next ten
</em><br>
<em>&gt; &gt; years&quot; should be analogized to &quot;no effective workaround to the second law
</em><br>
<em>&gt; &gt; of thermodynamics will be discovered over the next ten billion years&quot;.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ELI DROPS THE BALL #4:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You're having a scaling problem.  You have to consider Moore's Law relative to the history of
</em><br>
<em>&gt; human existance, then consider the observable implications of 2LT relative to the current age of
</em><br>
<em>&gt; the universe.  And BTW, I'm not speaking of Moore's Law in its strict form, I believe that in
</em><br>
<em>&gt; order to justify prediction of Singularity you must, as Kurzweil does, look for a Moore's Law-like
</em><br>
<em>&gt; principle operating throughout the age of humanity.
</em><br>
<p>Okay, now this is exactly what I was talking about earlier.  I don't care
<br>
about Moore's Law as a grand sweep across history as long as it holds up
<br>
just long enough to deliver me the computing power I need.  You say that
<br>
&quot;in order to justify prediction of Singularity I must&quot;, generalize Moore's
<br>
Law, and then attack that generalization as if I had asserted it, which I
<br>
do not.  If I wish to limit my reliance on Moore's Law to ten years, you
<br>
cannot claim that I am guilty of logical contradiction by virtue of the
<br>
fact that I *would* be guilty of logical contradiction if I said that
<br>
Moore's Law would hold for the next ten billion years, even if you think
<br>
that's something I &quot;must&quot; assert.
<br>
<p><em>&gt; &gt; But
</em><br>
<em>&gt; &gt; unless you have a physics degree you've been concealing, and I'm pretty
</em><br>
<em>&gt; &gt; sure you would have mentioned it by now, both of us are simply listing
</em><br>
<em>&gt; &gt; which laws of physics we like and dislike based on their character...
</em><br>
<em>&gt; &gt; rather a silly activity, really.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ELI DROPS THE BALL #5, #6, #7:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Now really, Eli.  You of all people should know that it doesn't take a degree to make significant
</em><br>
<em>&gt; advances in a field
</em><br>
<p>True.  Consider me corrected.  What I meant was that I thought that both
<br>
of us had overreached our expertise.  I didn't mean that all people not
<br>
possessing degrees should be excluded, just that it might take a roughly
<br>
degree-equivalent amount of expertise to continue the argument any
<br>
further.
<br>
<p><em>&gt; &quot;Jeff doesn't have a physics degree&quot; --&gt; both of us are simply listing favorite laws
</em><br>
<em>&gt;
</em><br>
<em>&gt; doesn't make sense on many dimensions.  Why should my degree or lack thereof effect the process by
</em><br>
<em>&gt; which *you* are engaging in this discussion?  Why should it impact *my* criteria?
</em><br>
<p>Because I felt myself making a mental reach for the specific mathematics,
<br>
and the mental operation failed due to lack of knowledge - I moved from
<br>
physics to cognitive science before I started getting into the
<br>
differential equations.  Maybe you can operate without them, but that
<br>
would imply that you're much better at this than I am; and while that
<br>
*could* be true, if I believed that, I wouldn't still be arguing with you.
<br>
<p><em>&gt; Is the
</em><br>
<em>&gt; implication that only a degreed person can have selection criteria other than an apparently
</em><br>
<em>&gt; baseless emotional &quot;like&quot; or &quot;dislike?&quot;
</em><br>
<p>Well... I wouldn't use the term &quot;baseless&quot;, but yeah.  This conversation
<br>
has basically degenerated into &quot;I like this law&quot;, &quot;I don't like this
<br>
law&quot;.  Perhaps one of us is right and the other wrong, but I don't think
<br>
we can take the argument any further.
<br>
<p><em>&gt; Muddy thinking, wrapped up with a pat
</em><br>
<em>&gt; value judgement intended to devalue the whole endeavor:  &quot;rather a silly activity, really.&quot;
</em><br>
<p>This *is* rather silly.  Don't you think so?
<br>
<p><em>&gt; &gt; If it's a decision predicated on the truth of physical law, then what a
</em><br>
<em>&gt; &gt; Friendly AI requires is the ability to make the correct decision based on
</em><br>
<em>&gt; &gt; the truth as known to it at that time.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ELI DROPS THE BALL #8:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But herein lies the handwaving.  That's tautological.  &quot;What's needed for the Friendly to make the
</em><br>
<em>&gt; correct decision is the ability to make the correct decision based on what is known at the time.&quot;
</em><br>
<p>I did NOT say that.  What I said was that the DESIGN REQUIREMENT was that
<br>
the Friendly AI have the basis to LATER make the correct decision based on
<br>
later knowledge, as OPPOSED to the design requirement being that WE make
<br>
the correct decision NOW based on CURRENT knowledge.
<br>
<p><em>&gt; &gt; Even if you argue that our current model of
</em><br>
<em>&gt; &gt; physics will affect how we now make moral decisions that establish basic
</em><br>
<em>&gt; &gt; values (supergoals) which are then not dependent on physics, a Friendly AI
</em><br>
<em>&gt; &gt; with causal validity semantics would probably re-model the moral decision
</em><br>
<em>&gt; &gt; we would have made at this point as if we had had accurate knowledge of
</em><br>
<em>&gt; &gt; physics.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Explain the implications of the latter.
</em><br>
<p>We can screw up our understanding of physics without screwing up the
<br>
Friendly AI.  Causal validity semantics describes the cognitive processes
<br>
needed for the Friendly AI to find and fix mistakes in its own creation;
<br>
to heal the consequences of those points in its past history where the
<br>
programmers took actions based on incorrect models of reality.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2479.html">Damien Broderick: "Re: A billion years later..."</a>
<li><strong>Previous message:</strong> <a href="2477.html">Eliezer S. Yudkowsky: "Re: META: Off topic, of course"</a>
<li><strong>In reply to:</strong> <a href="2476.html">Jeff Bone: "Deconstructing Eli:  A Final Cautionary Note"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2466.html">Jeff Bone: "Re: The inevitability of death, or the death of inevitability?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2478">[ date ]</a>
<a href="index.html#2478">[ thread ]</a>
<a href="subject.html#2478">[ subject ]</a>
<a href="author.html#2478">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
