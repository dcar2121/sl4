<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The inevitability of death, or the death of inevitability?</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: The inevitability of death, or the death of inevitability?">
<meta name="Date" content="2001-12-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The inevitability of death, or the death of inevitability?</h1>
<!-- received="Sun Dec 09 15:30:20 2001" -->
<!-- isoreceived="20011209223020" -->
<!-- sent="Sun, 09 Dec 2001 14:55:55 -0500" -->
<!-- isosent="20011209195555" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: The inevitability of death, or the death of inevitability?" -->
<!-- id="3C13C1CB.297A6840@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="JBEPKOGDDIKKAHFPOEFIIEPPCLAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20The%20inevitability%20of%20death,%20or%20the%20death%20of%20inevitability?"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Dec 09 2001 - 12:55:55 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2474.html">aominux: "Off topic, of course"</a>
<li><strong>Previous message:</strong> <a href="2472.html">Gordon Worley: "Re: Dissection of a Flame"</a>
<li><strong>In reply to:</strong> <a href="2469.html">Ben Goertzel: "RE: The inevitability of death, or the death of inevitability?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2474.html">aominux: "Off topic, of course"</a>
<li><strong>Reply:</strong> <a href="2474.html">aominux: "Off topic, of course"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2473">[ date ]</a>
<a href="index.html#2473">[ thread ]</a>
<a href="subject.html#2473">[ subject ]</a>
<a href="author.html#2473">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Eli wrote:
</em><br>
<em>&gt; &gt; &gt; I'm not accusing you of that.  I'm merely pointing out that
</em><br>
<em>&gt; &gt; it's very hard for humans to
</em><br>
<em>&gt; &gt; &gt; accurately assess their priors and reason about things in a
</em><br>
<em>&gt; &gt; manner divorced from sentiment.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I agree, it's very hard.  But you have provided no evidence supporting the
</em><br>
<em>&gt; &gt; assertion that I am guilty of this flaw in any of the specific cases in
</em><br>
<em>&gt; &gt; question; *first* you must establish this, *then* you may sigh over the
</em><br>
<em>&gt; &gt; fallibility of humans in general and me in particular.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; OK, so it's &quot;psychoanalyze Eli&quot; time is it?  What could be more exciting,
</em><br>
<em>&gt; and more relevant to the future of AI and the universe?  ;)
</em><br>
<p>Our annual imports and exports of dental floss?  No, just kidding.  I'm a
<br>
volunteer; I have no right to complain.
<br>
<p><em>&gt; I would propose that our friend Eliezer has a strong personal desire to be
</em><br>
<em>&gt; useful and helpful, on a grand scale.
</em><br>
<p>And this is bad... because of why?
<br>
<p>It's true that up to, say, age fourteen or so, I had an emotional desire
<br>
to be helpful on a grand scale; this was later rationalized (that's in the
<br>
sense of &quot;made normative&quot;, not &quot;irrationally justified&quot;) into a desire to
<br>
accomplish the largest possible amount of good through my actions. 
<br>
Practically the definition of a goal system, really.
<br>
<p>I want to accomplish as much good as possible with my life.  The grand
<br>
scale happens to be available, so I go for that.
<br>
<p>In emotional terms, you might phrase the transition as follows, from:
<br>
<p>&quot;I want to do a huge amount of good&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;to
<br>
&quot;I have the capability to do a huge amount of good, so of course I want to
<br>
do it; who wouldn't?&quot;
<br>
<p><em>&gt; (I won't go so far as to call it a
</em><br>
<em>&gt; &quot;savior complex&quot; ;).  I would propose that this desire is biasing (in a +
</em><br>
<em>&gt; direction) his estimate of the probability that it's possible for *any*
</em><br>
<em>&gt; human being to be useful and helpful in terms of the Singularity and
</em><br>
<em>&gt; associated events.
</em><br>
<p>One should be aware that there are psychological forces pushing in the
<br>
opposite direction as well.  There are psychological forces pushing for
<br>
passivity and refusal of responsibility - if the Singularity is beyond
<br>
your ability to affect, you don't have to do anything about it.  There are
<br>
social forces pushing toward modesty.  There is a prevailing memetic
<br>
environment designed to provide consolation for insignificance instead of
<br>
motivation to attempt significance.
<br>
<p>The truth is that whatever we as humans may be, the last thing we are is
<br>
insignificant.  Humanity's entire future hinges on us, and this future
<br>
contains an unimaginable number of sentient beings.  Simple division says
<br>
that if the future contains at least six quintillion beings, then each
<br>
living human carries the responsibility for at least a billion lives.  The
<br>
moral weight flowing through pre-Singularity Earth is so heavy that it
<br>
doesn't even begin to diminish when divided by a paltry number like six
<br>
billion.
<br>
<p>I can only assume that you are measuring 'usefulness' as a percentage of
<br>
the Singularity, rather than weighing it in absolute terms.  Very well. 
<br>
There's no point in blowing our fuses by trying to measure all moral
<br>
quantities in units of trillions.  Nonetheless, I can still hardly be
<br>
accused of irrationality for focusing on the Singularity as opposed to
<br>
other things.
<br>
<p><em>&gt; But, the thing is, our probability estimates as regarding such things are
</em><br>
<em>&gt; *inevitably* pretty damn shaky anyway, and hence very highly susceptible to
</em><br>
<em>&gt; bias....  Where do you draw the line between bias and intuition?
</em><br>
<p>In normative terms, the line is very definite.  If your desires influence
<br>
your conclusions, that's bias.  If not, that's intuition.
<br>
<p>The processes by which desires influence conclusions (in humans) is not
<br>
invisible.  They have distinct mental 'feels'.  It's just that these
<br>
feelings usually goes unrecognized.
<br>
<p><em>&gt; Where there is very little data, we have to use sentiment to guide our
</em><br>
<em>&gt; intuitions.  There's not enough data to proceed by pure conscious reason.
</em><br>
<p>What's wrong with trying to remain calm and letting our intuitions run on
<br>
their own rails?
<br>
<p><em>&gt; [By the way, in my view, &quot;sentiment&quot; may be analyzed as &quot;a lot of little
</em><br>
<em>&gt; tiny unconfident reasoning steps merged together, many of them analogies
</em><br>
<em>&gt; based on personal experience&quot;.  so you can say it's a kind of reason, but
</em><br>
<em>&gt; it's different than &quot;conscious reason&quot;, which is based on a smaller number
</em><br>
<em>&gt; of reasoning steps, most of which are fairly confident, and each of which
</em><br>
<em>&gt; can be scrutinized extensively.]
</em><br>
<p>I rarely have the experience of having an intuition, or any subjective
<br>
feeling, without being able to figure out where it comes from.  I guess I
<br>
basically see intuition as an extension of rationality by other means.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2474.html">aominux: "Off topic, of course"</a>
<li><strong>Previous message:</strong> <a href="2472.html">Gordon Worley: "Re: Dissection of a Flame"</a>
<li><strong>In reply to:</strong> <a href="2469.html">Ben Goertzel: "RE: The inevitability of death, or the death of inevitability?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2474.html">aominux: "Off topic, of course"</a>
<li><strong>Reply:</strong> <a href="2474.html">aominux: "Off topic, of course"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2473">[ date ]</a>
<a href="index.html#2473">[ thread ]</a>
<a href="subject.html#2473">[ subject ]</a>
<a href="author.html#2473">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
