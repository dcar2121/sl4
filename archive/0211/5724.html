<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Knowability of Friendly AI (was: ethics of argument)</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Knowability of Friendly AI (was: ethics of argument)">
<meta name="Date" content="2002-11-11">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Knowability of Friendly AI (was: ethics of argument)</h1>
<!-- received="Mon Nov 11 05:54:44 2002" -->
<!-- isoreceived="20021111125444" -->
<!-- sent="Mon, 11 Nov 2002 07:54:36 -0500" -->
<!-- isosent="20021111125436" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Knowability of Friendly AI (was: ethics of argument)" -->
<!-- id="3DCFA88C.4050101@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJGECGDNAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Knowability%20of%20Friendly%20AI%20(was:%20ethics%20of%20argument)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Nov 11 2002 - 05:54:36 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5725.html">Ben Goertzel: "RE: Knowability of Friendly AI (was: ethics of argument)"</a>
<li><strong>Previous message:</strong> <a href="5723.html">Ben Goertzel: "RE: The ethics of argument (was: AGI funding)"</a>
<li><strong>In reply to:</strong> <a href="5722.html">Ben Goertzel: "RE: The ethics of argument (was: AGI funding)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5725.html">Ben Goertzel: "RE: Knowability of Friendly AI (was: ethics of argument)"</a>
<li><strong>Reply:</strong> <a href="5725.html">Ben Goertzel: "RE: Knowability of Friendly AI (was: ethics of argument)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5724">[ date ]</a>
<a href="index.html#5724">[ thread ]</a>
<a href="subject.html#5724">[ subject ]</a>
<a href="author.html#5724">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em> &gt;
</em><br>
<em> &gt; Eliezer wrote:
</em><br>
<em> &gt;
</em><br>
<em> &gt;&gt; You're arguing from fully general uncertainty again; can you give a
</em><br>
<em> &gt;&gt; specific X in Friendly AI theory that you do not think it is possible
</em><br>
<em> &gt;&gt; to usefully consider in advance?)
</em><br>
<em> &gt;
</em><br>
<em> &gt; There are many examples.  One example is, the stability of AGI
</em><br>
<em> &gt; goal-systems under self-modification.  To understand this at all in
</em><br>
<em> &gt; advance of having simple self-modifying AGI's to experiment with, one
</em><br>
<em> &gt; would need a tremendously, immeasurably more sophisticated mathematical
</em><br>
<em> &gt; dynamical systems theory than we now possess (or than seems feasible to
</em><br>
<em> &gt; create in the near term).  Yet you seem to be making some very
</em><br>
<em> &gt; confident assertions in this regard in CFAI.
</em><br>
<p>That's because I'm not viewing the problem as &quot;the stability of AGI goal 
<br>
systems under self-modification&quot;.  Rather there are certain *particular 
<br>
kinds* of self-modification which humans exhibit, which have particular 
<br>
*meanings* in terms of human philosophy and morality, which need to be 
<br>
embodied in a Friendly AI on purely moral grounds, and which also turn out 
<br>
to play critical roles in describing both the empirical transfer process 
<br>
and those moral considerations which, from a human perspective, govern the 
<br>
creation of AI.  &quot;My supergoals could be wrong&quot; is simultaneously a valid 
<br>
part of human philosophical thinking, a consequence of the &quot;external 
<br>
reference semantics&quot; needed for an AI to treat any supergoal-modifying 
<br>
causal process (especially including feedback from the programmers) as 
<br>
desirable, and a major consideration affecting the morality of AI creation 
<br>
itself.
<br>
<p>Ergo:
<br>
<p>1)  The structural dynamics described in CFAI have moral meaning in human 
<br>
terms - they are not a theory of all possible self-modifying goal systems 
<br>
but a specific theory of a moral Friendly AI.
<br>
<p>2)  The structural dynamics described in CFAI are implemented in humans 
<br>
and can be examined there.
<br>
<p>3)  What matters is not (A) &quot;Are AGI goal systems stable under 
<br>
self-modification?&quot; but (B) &quot;Can AGI goal systems be at least as &quot;stable&quot; 
<br>
under self-modification as an evolved human, where &quot;stability&quot; is defined 
<br>
in a specific and morally relevant way?&quot;
<br>
<p>I'd also ask you to consider narrowing your focus from the extremely 
<br>
general issue of &quot;the stability of self-modifying goal systems&quot; to 
<br>
statements of the order found in CFAI, such as &quot;A goal system with 
<br>
external reference semantics and probabilistic supergoals exhibits certain 
<br>
behaviors that are morally relevant to Friendly AI and necessary to 
<br>
Friendly AI construction, and is therefore a superior design choice by 
<br>
comparison with more commonly proposed goal system structures under which 
<br>
supergoals are treated as correct by definition.&quot;  Why do you believe 
<br>
that, e.g., this specific question cannot be considered in advance?
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5725.html">Ben Goertzel: "RE: Knowability of Friendly AI (was: ethics of argument)"</a>
<li><strong>Previous message:</strong> <a href="5723.html">Ben Goertzel: "RE: The ethics of argument (was: AGI funding)"</a>
<li><strong>In reply to:</strong> <a href="5722.html">Ben Goertzel: "RE: The ethics of argument (was: AGI funding)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5725.html">Ben Goertzel: "RE: Knowability of Friendly AI (was: ethics of argument)"</a>
<li><strong>Reply:</strong> <a href="5725.html">Ben Goertzel: "RE: Knowability of Friendly AI (was: ethics of argument)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5724">[ date ]</a>
<a href="index.html#5724">[ thread ]</a>
<a href="subject.html#5724">[ subject ]</a>
<a href="author.html#5724">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
