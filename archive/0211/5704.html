<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: The ethics of argument (was: AGI funding)</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="The ethics of argument (was: AGI funding)">
<meta name="Date" content="2002-11-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>The ethics of argument (was: AGI funding)</h1>
<!-- received="Sun Nov 10 10:07:50 2002" -->
<!-- isoreceived="20021110170750" -->
<!-- sent="Sun, 10 Nov 2002 12:07:41 -0500" -->
<!-- isosent="20021110170741" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="The ethics of argument (was: AGI funding)" -->
<!-- id="3DCE925D.8060904@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJEEBBDNAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20The%20ethics%20of%20argument%20(was:%20AGI%20funding)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Nov 10 2002 - 10:07:41 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5705.html">Ben Goertzel: "RE: The ethics of argument (was: AGI funding)"</a>
<li><strong>Previous message:</strong> <a href="5703.html">Ben Goertzel: "RE: AGI funding (was Re: Some bad news)"</a>
<li><strong>In reply to:</strong> <a href="5702.html">Ben Goertzel: "RE: AGI funding (was Re: Some bad news)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5705.html">Ben Goertzel: "RE: The ethics of argument (was: AGI funding)"</a>
<li><strong>Reply:</strong> <a href="5705.html">Ben Goertzel: "RE: The ethics of argument (was: AGI funding)"</a>
<li><strong>Maybe reply:</strong> <a href="5711.html">doug.bailey@ey.com: "Re: The ethics of argument (was: AGI funding)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5704">[ date ]</a>
<a href="index.html#5704">[ thread ]</a>
<a href="subject.html#5704">[ subject ]</a>
<a href="author.html#5704">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; Eliezer,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't see doing well-thought-out PR campaign in favor of the Singularity
</em><br>
<em>&gt; as necessarily being a moral compromise.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Some PR campaigns *are* immoral; others, in my view, are not.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What moral principle do you think that a well-thought-out PR campaign
</em><br>
<em>&gt; violates?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Think about it this way.  Imagine you have 10 seconds to convince entity Y
</em><br>
<em>&gt; of conclusion X.  But to tell Y even a reasonable fraction of what you know
</em><br>
<em>&gt; about X, will take 60 seconds.  So you have to be selective.  There is one
</em><br>
<em>&gt; 10-second statement about X (call it X1) that you think really captures
</em><br>
<em>&gt; what's most central and important about X; but there is another 10-second
</em><br>
<em>&gt; statement about X (call it X2) that you think will be more convincing to Y
</em><br>
<em>&gt; (ie more likely to convince Y of X).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What is the most rational way to spend your 10 seconds?  Telling Y X1, or
</em><br>
<em>&gt; telling Y X2?
</em><br>
<p>Okay... 10 reasons off the top of my head to pick X1:
<br>
<p>1)  You are not faced with a choice of convincing Y of X using means X1 or 
<br>
means X2.  You rather have a choice of convincing Y of X1 or convincing Y 
<br>
of X2.
<br>
<p>2)  Y has individual rights.  Perhaps &quot;I have 10 seconds to convince 
<br>
entity Y of conclusion X&quot; is not the best way to frame the problem.  Maybe 
<br>
the correct way to frame the problem is:  &quot;How can I help Y?  What 
<br>
information do I possess that Y desires?&quot;
<br>
<p>3)  How hard have you *tried* to convey X1?  Did you try at least once or 
<br>
just give up immediately because it seemed too hard?  If you do make the 
<br>
moral compromise, will you spend at least as much time practicing how to 
<br>
convey X1 as to convey X2?  Is it a temporary compromise or a permanent 
<br>
one?  The usual answer I encounter is that it is a permanent compromise, 
<br>
made without even trying it the ethical way.  People who ask those 
<br>
questions know enough about ethics not to make the mistake in the first place.
<br>
<p>4)  Pick a lifepath.  You can assiduously practice, and become an expert 
<br>
at, conveying the real truth of things in as few words as possible.  Or 
<br>
you can become an expert at telling people what they want to hear.  Who do 
<br>
you want to be?
<br>
<p>5)  The universe is an uncertain place.  Good intentions are not 
<br>
sufficient.  They do, however, count for something.  Mistakes made from 
<br>
the best of intentions are still mistakes, but they are easier to recover 
<br>
from because you can immediately, honestly, and openly admit to them, 
<br>
rather than needing to conceal the error and its consequences.
<br>
<p>6)  Your beliefs themselves are uncertain.  Obscuring beliefs about which 
<br>
you yourself could be wrong adds a double layer of indirection.  If you 
<br>
later discover that you are wrong, it's very likely that there will be no 
<br>
way to recover - no way to get from what you did convince the audience of, 
<br>
to what you now believe to be the real truth.  I've made mistakes, such as 
<br>
insisting between 1996 and 2000 that ethical minds can knowably operate at 
<br>
a moral optimum using blank-slate goal systems.  Because I provided my 
<br>
real thoughts underlying that conclusion, when my thoughts changed it was 
<br>
not an impossible distance to argue the new conclusion.  Had I chosen 
<br>
persuasive-sounding arguments that were not my own original reasons for 
<br>
believing, it is unlikely that the old arguments would have had anything 
<br>
at all in common with the new conclusion.
<br>
<p>7)  If you start out by immediately compromising your principles, it isn't 
<br>
likely there'll be anything at all left of them by the time you're 
<br>
finished.  When you're starting out is exactly the time to be most strict.
<br>
<p>8)  Why is it that people don't even seem to realize that X2 is a risk? 
<br>
You can argue about whether it's a necessary risk or an ethical risk to 
<br>
take, but it's most certainly a risk.  Have you spent as much time 
<br>
thinking about all the ways X2 could go wrong - whether or not it's 
<br>
morally acceptable - as you have arguing yourself into the idea that you 
<br>
should be allowed to do it?
<br>
<p>9)  What will people who repeat X2 and themselves add extra distance for 
<br>
persuasive power create in the way of X2.2?  How about X2.2.2?  Maybe you 
<br>
want to convey X1 yourself so that people who repeat the statement will 
<br>
convey X2 rather than Ifni-knows-what.
<br>
<p>10)  And above all:  There are known bugs in the human mind that make it 
<br>
likely that you are underestimating the size of any given moral 
<br>
compromise, including the moral compromise represented by arguing X2.  So 
<br>
be careful, dammit!  I am reminded of the following quote:
<br>
<p>&quot;Adultery always begins with the adulterer(s) claiming to themselves and 
<br>
to others that the relationship is &quot;harmless&quot; because it hasn't crossed a 
<br>
certain line. The line where it becomes wrong is the line where you start 
<br>
having to rationalize like that.&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-- Gelfin
<br>
<p><em>&gt; Unfortunately, this is the situation one is in, when trying to get complex
</em><br>
<em>&gt; ideas across to the mass of humanity, at this stage.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Is it immoral to tell Y X2 rather than X1, in order to convince them?
</em><br>
<p>At the very least it's damn risky, and I'd take the coward's way out 
<br>
myself.  The reason why ethics exist is that, in morality, plus times plus 
<br>
times plus times plus times minus equals minus.  And minus times minus 
<br>
equals minus.  Good intentions aren't enough; it is necessary to be on 
<br>
your toes.
<br>
<p><em>&gt; The moral dilemma, for me, comes up when there's another statement X3, which
</em><br>
<em>&gt; you know to be FALSE, but which you calculate has an even higher chance than
</em><br>
<em>&gt; X2 of convincing Y of X.  In this case, you have the well-known moral
</em><br>
<em>&gt; dilemma of whether to lie to Y for its own good.
</em><br>
<p>Answer:  That's up to Y.  As the default for human-level intelligences, Y 
<br>
should be presumed not to wish this unless you hear a direct statement 
<br>
otherwise (&quot;Please lie to me for my own good.&quot;)
<br>
<p><em> &gt; But in the X1 vs. X2
</em><br>
<em>&gt; choice, I see no moral dilemma.  [Though I do see a source of frustration,
</em><br>
<em>&gt; because as a truth-seeking individual one would always *rather* say X1 than
</em><br>
<em>&gt; X2...]
</em><br>
<p>Then this is yet another good reason to choose X1.  It gives you safety 
<br>
margin.  A moment of moral weakness will result in saying X2, not X3.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5705.html">Ben Goertzel: "RE: The ethics of argument (was: AGI funding)"</a>
<li><strong>Previous message:</strong> <a href="5703.html">Ben Goertzel: "RE: AGI funding (was Re: Some bad news)"</a>
<li><strong>In reply to:</strong> <a href="5702.html">Ben Goertzel: "RE: AGI funding (was Re: Some bad news)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5705.html">Ben Goertzel: "RE: The ethics of argument (was: AGI funding)"</a>
<li><strong>Reply:</strong> <a href="5705.html">Ben Goertzel: "RE: The ethics of argument (was: AGI funding)"</a>
<li><strong>Maybe reply:</strong> <a href="5711.html">doug.bailey@ey.com: "Re: The ethics of argument (was: AGI funding)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5704">[ date ]</a>
<a href="index.html#5704">[ thread ]</a>
<a href="subject.html#5704">[ subject ]</a>
<a href="author.html#5704">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
