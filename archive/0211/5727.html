<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Knowability of Friendly AI (was: ethics of argument)</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Knowability of Friendly AI (was: ethics of argument)">
<meta name="Date" content="2002-11-11">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Knowability of Friendly AI (was: ethics of argument)</h1>
<!-- received="Mon Nov 11 09:51:24 2002" -->
<!-- isoreceived="20021111165124" -->
<!-- sent="Mon, 11 Nov 2002 11:51:05 -0500" -->
<!-- isosent="20021111165105" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Knowability of Friendly AI (was: ethics of argument)" -->
<!-- id="LAEGJLOGJIOELPNIOOAJGEDADNAA.ben@goertzel.org" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3DCFD1AE.1030101@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Knowability%20of%20Friendly%20AI%20(was:%20ethics%20of%20argument)"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Mon Nov 11 2002 - 09:51:05 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5728.html">Gordon Worley: "Re: AGI funding (was Re: Some bad news)"</a>
<li><strong>Previous message:</strong> <a href="5726.html">Eliezer S. Yudkowsky: "Re: Knowability of Friendly AI (was: ethics of argument)"</a>
<li><strong>In reply to:</strong> <a href="5726.html">Eliezer S. Yudkowsky: "Re: Knowability of Friendly AI (was: ethics of argument)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5710.html">Slawomir Paliwoda: "Re: The ethics of argument (was: AGI funding)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5727">[ date ]</a>
<a href="index.html#5727">[ thread ]</a>
<a href="subject.html#5727">[ subject ]</a>
<a href="author.html#5727">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer wrote:
<br>
<em>&gt; &gt; Although I've spent much of my life creating heuristic
</em><br>
<em>&gt; conceptual arguments
</em><br>
<em>&gt; &gt; about topics of interest, the three forms of knowledge I trust most are:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; -- mathematical
</em><br>
<em>&gt; &gt; -- empirical
</em><br>
<em>&gt; &gt; -- experiential [when referring to subjective domains]
</em><br>
<em>&gt;
</em><br>
<em>&gt; It looks to me like you're missing out on the entire domain of
</em><br>
<em>&gt; nonmathematical abstract reasoning, e.g. by combining
</em><br>
<em>&gt; generalizations from
</em><br>
<em>&gt; previous empirical experience, or by nonmathematical reasoning about the
</em><br>
<em>&gt; properties of abstract systems that are simple enough to be modeled
</em><br>
<em>&gt; deductively.
</em><br>
<p>I am not missing out on this domain, I just trust it less than the three I
<br>
mentioned.
<br>
<p>I spent most of my time doing nonmathematical abstract reasoning, actually.
<br>
<p><em>&gt; You are an individual, you can always go off and build AI regardless of
</em><br>
<em>&gt; what the heuristic arguments say, but right now the heuristic arguments
</em><br>
<em>&gt; say that Novamente as it stands, if it works, will destroy the world,
</em><br>
<p>Right now YOUR heuristic arguments say that...
<br>
<p>... and, MINE say otherwise ;-)
<br>
<p><em>&gt; Saying &quot;I distrust all heuristic arguments&quot; doesn't really cut it here.
</em><br>
<p>Of course I don't distrust all heuristic arguments equally, and I didn't
<br>
mean to imply that I did.
<br>
<p><em>&gt; One, you don't distrust your *own* heuristic arguments,
</em><br>
<p>I trust my own heuristic arguments less than my own mathematical, empirical
<br>
or experiential evidence.
<br>
<p>I trust my own newborn heuristic arguments only slightly; but if an
<br>
heuristic argument of mine has been around a while and I haven't been
<br>
convinced of a flaw in it, then yeah, I trust it somewhat.  My research is
<br>
guided by my heuristic arguments, after all.
<br>
<p><em>&gt;  It seems to me
</em><br>
<em>&gt; that your claim that nothing can be known about Friendly AI in advance
</em><br>
<em>&gt; would be, if it were true, a strong (though not knockdown) argument
</em><br>
<em>&gt; against developing AI in the first place.
</em><br>
<p>A better statement of  my perspective would be: &quot;The amount that can be
<br>
confidently known about Friendly AI in advance of having real AGI's to
<br>
experiment with, or a huge mathematical advance, is very small.&quot;
<br>
<p><em>&gt; Would it be fair to summarize your argument so far as:  &quot;Novamente is a
</em><br>
<em>&gt; good Singularity project because nothing useful can be known about
</em><br>
<em>&gt; Friendly AI in advance, which unknowability is itself knowable on the
</em><br>
<em>&gt; grounds that Friendly AI is neither empirically demonstrated nor
</em><br>
<em>&gt; mathematically proven knowledge.  It is correct Singularity strategy to
</em><br>
<em>&gt; invest in AI projects when nothing is known about Friendly AI, since the
</em><br>
<em>&gt; only way to find out is to try it.  The amount of concern I've shown for
</em><br>
<em>&gt; Friendly AI so far is around the right proportional amount of concern
</em><br>
<em>&gt; desired in the leader of a Singularity AI project.&quot;
</em><br>
<p>No, that's a somewhat twisted summary of my argument ;)
<br>
<p>The last sentence is one I agree with; but the previous sentences I do not
<br>
agree with as stated.  They all seem like unconscious or conscious attempts
<br>
to &quot;spin&quot; my perspective in an unfavorable way.
<br>
<p>For instance, when you say
<br>
<p><em>&gt;It is correct Singularity strategy to
</em><br>
<em>&gt; invest in AI projects when nothing is known about Friendly AI, since the
</em><br>
<em>&gt; only way to find out is to try it.
</em><br>
<p>I'd say, rather:
<br>
<p>&quot;It is correct Singularity strategy to invest in AGI project when very
<br>
little is known about Friendly AI, since serious knowledge about Friendly AI
<br>
is only going to evolve organically along with practical knowledge about
<br>
building and teaching AGI's.&quot;
<br>
<p><em>&gt; Are you even going
</em><br>
<em>&gt; to *have*
</em><br>
<em>&gt; a Friendliness Failure Lab?
</em><br>
<p>We will have a Novababy testing lab, and Friendliness will be one among many
<br>
things tested there.
<br>
<p>As I stated on this list a couple months ago, I will write &amp; post a plan for
<br>
Friendliness testing and other aspects of Novababy testing, after completing
<br>
the current draft of the Novamente book.  Looks like a few more months.
<br>
<p><em>&gt; I
</em><br>
<em>&gt; think - as you seem to deny - that it's possible to take a *lot* of
</em><br>
<em>&gt; territory on the Friendly AI part of Singularity strategy, over and above
</em><br>
<em>&gt; that represented by a generic recursively self-improving AI project.
</em><br>
<p>Yes, this is a significant area of disagreement between us.
<br>
<p>Also, I know I'm not alone in my views on this, within the small group of
<br>
Singularity-focused AGI researchers.  Peter Voss, for instance, has
<br>
expressed very similar views to mine on this topic, in the past, on this
<br>
list.  He has said that he feels it's just &quot;too early&quot; to be thinking in
<br>
such detail about Friendly AI; but he is spending his time trying to
<br>
actually build an AGI capable of launching the Singularity.
<br>
<p><p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5728.html">Gordon Worley: "Re: AGI funding (was Re: Some bad news)"</a>
<li><strong>Previous message:</strong> <a href="5726.html">Eliezer S. Yudkowsky: "Re: Knowability of Friendly AI (was: ethics of argument)"</a>
<li><strong>In reply to:</strong> <a href="5726.html">Eliezer S. Yudkowsky: "Re: Knowability of Friendly AI (was: ethics of argument)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5710.html">Slawomir Paliwoda: "Re: The ethics of argument (was: AGI funding)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5727">[ date ]</a>
<a href="index.html#5727">[ thread ]</a>
<a href="subject.html#5727">[ subject ]</a>
<a href="author.html#5727">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
