<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Knowability of Friendly AI (was: ethics of argument)</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Knowability of Friendly AI (was: ethics of argument)">
<meta name="Date" content="2002-11-11">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Knowability of Friendly AI (was: ethics of argument)</h1>
<!-- received="Mon Nov 11 08:50:09 2002" -->
<!-- isoreceived="20021111155009" -->
<!-- sent="Mon, 11 Nov 2002 10:50:06 -0500" -->
<!-- isosent="20021111155006" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Knowability of Friendly AI (was: ethics of argument)" -->
<!-- id="3DCFD1AE.1030101@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJGECIDNAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Knowability%20of%20Friendly%20AI%20(was:%20ethics%20of%20argument)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Nov 11 2002 - 08:50:06 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5727.html">Ben Goertzel: "RE: Knowability of Friendly AI (was: ethics of argument)"</a>
<li><strong>Previous message:</strong> <a href="5725.html">Ben Goertzel: "RE: Knowability of Friendly AI (was: ethics of argument)"</a>
<li><strong>In reply to:</strong> <a href="5725.html">Ben Goertzel: "RE: Knowability of Friendly AI (was: ethics of argument)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5727.html">Ben Goertzel: "RE: Knowability of Friendly AI (was: ethics of argument)"</a>
<li><strong>Reply:</strong> <a href="5727.html">Ben Goertzel: "RE: Knowability of Friendly AI (was: ethics of argument)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5726">[ date ]</a>
<a href="index.html#5726">[ thread ]</a>
<a href="subject.html#5726">[ subject ]</a>
<a href="author.html#5726">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt;&gt;I'd also ask you to consider narrowing your focus from the extremely
</em><br>
<em>&gt;&gt;general issue of &quot;the stability of self-modifying goal systems&quot; to
</em><br>
<em>&gt;&gt;statements of the order found in CFAI, such as &quot;A goal system with
</em><br>
<em>&gt;&gt;external reference semantics and probabilistic supergoals
</em><br>
<em>&gt;&gt;exhibits certain
</em><br>
<em>&gt;&gt;behaviors that are morally relevant to Friendly AI and necessary to
</em><br>
<em>&gt;&gt;Friendly AI construction, and is therefore a superior design choice by
</em><br>
<em>&gt;&gt;comparison with more commonly proposed goal system structures under which
</em><br>
<em>&gt;&gt;supergoals are treated as correct by definition.&quot;  Why do you believe
</em><br>
<em>&gt;&gt;that, e.g., this specific question cannot be considered in advance?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Although I've spent much of my life creating heuristic conceptual arguments
</em><br>
<em>&gt; about topics of interest, the three forms of knowledge I trust most are:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; -- mathematical
</em><br>
<em>&gt; -- empirical
</em><br>
<em>&gt; -- experiential [when referring to subjective domains]
</em><br>
<p>It looks to me like you're missing out on the entire domain of 
<br>
nonmathematical abstract reasoning, e.g. by combining generalizations from 
<br>
previous empirical experience, or by nonmathematical reasoning about the 
<br>
properties of abstract systems that are simple enough to be modeled 
<br>
deductively.
<br>
<p><em>&gt; My own arguments as to why Novamente will work as an AGI are also in the
</em><br>
<em>&gt; category of &quot;moderately convincing, suggestive, heuristic arguments&quot; at this
</em><br>
<em>&gt; stage.  But we're working hard to turn them into empirical demonstrations ;)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; One characteristic of &quot;moderately convincing, suggestive, heuristic
</em><br>
<em>&gt; arguments&quot; is that intelligent, informed, fair-minded people can rationally
</em><br>
<em>&gt; disagree on their validity....
</em><br>
<p>Another interesting characteristic of such arguments is that they can be 
<br>
communicated between individuals.  Of course I can't stop you from saying 
<br>
&quot;I, Ben Goertzel, choose to trust my experiential arguments over your 
<br>
heuristic ones&quot;.  However, you are not a single individual in isolation. 
<br>
You have, on this mailing list and others, argued that funding Novamente 
<br>
is a Singularitarian endeavor.  So it may well be, but only if its theory 
<br>
of Friendly AI improves, and I am willing to offer specific arguments in 
<br>
support of the latter statement.  The domain of &quot;moderately convincing, 
<br>
suggestive, heuristic arguments&quot; is exactly that which governs Singularity 
<br>
strategy, including questions such as &quot;Should we try to build AI, at all, 
<br>
in the first place?&quot; and &quot;Is this particular AI project likely to destroy 
<br>
the world?&quot;  It's also, as I believe you point out, the way that both of 
<br>
us intend to use to build AI in the first place.  So if the kind of 
<br>
reasoning I'm using to think about Friendly AI can't really be any more 
<br>
useful than the kind of thinking that will let you, say, save the world or 
<br>
build an intelligent being, then I think that's probably useful enough for 
<br>
me.  If anything it shows that the right caliber of reasoning is being 
<br>
used, since these are all tasks in the same domain.
<br>
<p>You are an individual, you can always go off and build AI regardless of 
<br>
what the heuristic arguments say, but right now the heuristic arguments 
<br>
say that Novamente as it stands, if it works, will destroy the world, and 
<br>
all else being equal I'd rather not rely on the pleasant but basically 
<br>
ungrounded possibility that Ben Goertzel or someone else on the Novamente 
<br>
project will learn at such a rate to arrive at a workable theory of 
<br>
Friendly AI before it's much, much too late.  This isn't to say it can't 
<br>
happen.  It's just that with the entire world at stake, I'd rather not be 
<br>
reduced to relying on pure ungrounded hope, all else being equal.
<br>
<p>Saying &quot;I distrust all heuristic arguments&quot; doesn't really cut it here. 
<br>
One, you don't distrust your *own* heuristic arguments, so it seems rather 
<br>
solipsistic to apply different standards of evidence to arguments that you 
<br>
like versus arguments you don't.  And two, I don't see even a good 
<br>
heuristic argument - let alone a mathematical one - that developing AI in 
<br>
the total absence of even heuristic knowledge as to whether it will be 
<br>
Friendly is really all that good a strategy for humanity.  It seems to me 
<br>
that your claim that nothing can be known about Friendly AI in advance 
<br>
would be, if it were true, a strong (though not knockdown) argument 
<br>
against developing AI in the first place.
<br>
<p>Really, I'm stuck in the same position with you that I am with the various 
<br>
people who argue against, say, the proposition that AI can ever work in 
<br>
the first place; no matter how much evidence I present, they can always 
<br>
claim to feel uncertain about it.  On the other hand, convincing the third 
<br>
parties in the audience is an entirely separate issue.  Basically, you've 
<br>
placed me in a position where I can simultaneously see that Novamente's 
<br>
current described structure would prima facie result in an SI existential 
<br>
catastrophe if Novamente began recursive self-improvement, and where I 
<br>
have no particular empirical reason - apart from pure hope - to suppose 
<br>
that matters will improve over the next years.  Now of course this could 
<br>
simply be my private nightmare, but since I am *not* arguing from my own 
<br>
uncertainty, it looks to me like my heuristic nightmare is communicable 
<br>
between rational thinkers and your selective uncertainty is not.
<br>
<p>Would it be fair to summarize your argument so far as:  &quot;Novamente is a 
<br>
good Singularity project because nothing useful can be known about 
<br>
Friendly AI in advance, which unknowability is itself knowable on the 
<br>
grounds that Friendly AI is neither empirically demonstrated nor 
<br>
mathematically proven knowledge.  It is correct Singularity strategy to 
<br>
invest in AI projects when nothing is known about Friendly AI, since the 
<br>
only way to find out is to try it.  The amount of concern I've shown for 
<br>
Friendly AI so far is around the right proportional amount of concern 
<br>
desired in the leader of a Singularity AI project.&quot;
<br>
<p>One of the major problems I have with this is that I have a plan for 
<br>
learning various pieces of empirical information about Friendly AI.  I may 
<br>
make observations that I had no way of anticipating, but there are also 
<br>
plenty of specific things that I already want to find out about - that I 
<br>
know enough to look for.  I'm not confident in the Novamente project's 
<br>
ability to learn about Friendly AI empirically without a model.  Why would 
<br>
you acquire empirical knowledge about what happens when a goal system has 
<br>
the ability to reflectively model itself at a level where it can reason 
<br>
about the abstract desirability of the presence or absence of a causal 
<br>
system whose effect is to modify the supergoals?  Why would you create 
<br>
lots of models like this in the Friendliness Failure Lab and test them to 
<br>
see if they work, and more importantly how they fail, if you don't know 
<br>
that the question above is an important one?  Are you even going to *have* 
<br>
a Friendliness Failure Lab?
<br>
<p>It looks to me like Novamente will start out with a such a vague theory of 
<br>
AI morality that adjusting the theory to fit evidence coincidentally 
<br>
encountered while doing other things will produce an only slightly less 
<br>
vague theory.  Let's suppose you're right and CFAI turns out to be 
<br>
completely wrong because it's not mathematics.  I'd still expect that 
<br>
empirically investigating everything that CFAI suggests should be 
<br>
investigated would produce a hell of a lot of knowledge about AI morality. 
<br>
&nbsp;&nbsp;Maybe enough that pure generalization from empirical evidence would be 
<br>
sufficient to give birth to a new coherent theory that was knowably 
<br>
adequate to produce Friendly AI, and if not, we could always just fold up 
<br>
shop and stop working on AI.  CFAI has a lot of hypotheses that can be 
<br>
investigated early on, so we'd know long before reaching a danger point if 
<br>
the theory was junk by virtue of not being mathematics.  It looks to me 
<br>
like your theory of AI morality is sufficiently vague that, if Novamente 
<br>
became capable of recursive self-improvement at the point you say you 
<br>
expect, it seems likely that - at that point in the development process - 
<br>
you would have modified a vague theory of AI morality into a different 
<br>
vague theory of AI morality, but not acquired the detailed knowledge 
<br>
needed for Friendly AI.  Now I could be wrong, of course - from your 
<br>
perspective, the statement is suspect because I am basing it on a 
<br>
CFAI-based visualization of what you're likely to encounter.  But there is 
<br>
still a strategic problem in bulling ahead when not only do you not know, 
<br>
you also don't have a specific theory of what you want to know or when you 
<br>
need to know it.
<br>
<p>In Singularity strategy terms, what Novamente is doing seems to verge on 
<br>
ignoring the Friendly AI question entirely - it currently makes sense to 
<br>
invest in Novamente if and only if you believe that investing in a generic 
<br>
recursively self-improving AI project is a good thing.  Now there are a 
<br>
number of reasons why this might be a good idea, such as that the project 
<br>
doesn't actually succeed but produces critical knowledge or even tools of 
<br>
Singularity relevance (such as an Earthweb), or that you expect it is 
<br>
knowably the case that AI developers learn enough about Friendly AI 
<br>
development to get by even if they aren't looking, or you think that you 
<br>
*have* to rely on the previous chance because otherwise the world is gonna 
<br>
blow up.  But I don't think it's *necessary* to go down that route.  I 
<br>
think - as you seem to deny - that it's possible to take a *lot* of 
<br>
territory on the Friendly AI part of Singularity strategy, over and above 
<br>
that represented by a generic recursively self-improving AI project.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5727.html">Ben Goertzel: "RE: Knowability of Friendly AI (was: ethics of argument)"</a>
<li><strong>Previous message:</strong> <a href="5725.html">Ben Goertzel: "RE: Knowability of Friendly AI (was: ethics of argument)"</a>
<li><strong>In reply to:</strong> <a href="5725.html">Ben Goertzel: "RE: Knowability of Friendly AI (was: ethics of argument)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5727.html">Ben Goertzel: "RE: Knowability of Friendly AI (was: ethics of argument)"</a>
<li><strong>Reply:</strong> <a href="5727.html">Ben Goertzel: "RE: Knowability of Friendly AI (was: ethics of argument)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5726">[ date ]</a>
<a href="index.html#5726">[ thread ]</a>
<a href="subject.html#5726">[ subject ]</a>
<a href="author.html#5726">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
