<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Ben what are your views and concerns</title>
<meta name="Author" content="Ben Goertzel (ben@intelligenesis.net)">
<meta name="Subject" content="RE: Ben what are your views and concerns">
<meta name="Date" content="2000-10-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Ben what are your views and concerns</h1>
<!-- received="Wed Oct 04 10:30:33 2000" -->
<!-- isoreceived="20001004163033" -->
<!-- sent="Wed, 4 Oct 2000 07:55:51 -0400" -->
<!-- isosent="20001004115551" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@intelligenesis.net" -->
<!-- subject="RE: Ben what are your views and concerns" -->
<!-- id="NDBBIBGFAPPPBODIPJMMAEMLEJAA.ben@intelligenesis.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="39DAB7E7.4930704D@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@intelligenesis.net?Subject=RE:%20Ben%20what%20are%20your%20views%20and%20concerns"><em>ben@intelligenesis.net</em></a>)<br>
<strong>Date:</strong> Wed Oct 04 2000 - 05:55:51 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0127.html">Cole Kitchen: "Neural computation breakthrough?"</a>
<li><strong>Previous message:</strong> <a href="0125.html">Eliezer S. Yudkowsky: "Re: Ben what are your views and concerns"</a>
<li><strong>In reply to:</strong> <a href="0125.html">Eliezer S. Yudkowsky: "Re: Ben what are your views and concerns"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0012/0355.html">Eliezer S. Yudkowsky: "Revising a Friendly AI"</a>
<li><strong>Reply:</strong> <a href="../0012/0355.html">Eliezer S. Yudkowsky: "Revising a Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#126">[ date ]</a>
<a href="index.html#126">[ thread ]</a>
<a href="subject.html#126">[ subject ]</a>
<a href="author.html#126">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi,
<br>
<p><p><em>&gt;  The primary design requirement for
</em><br>
<em>&gt; interim Friendly
</em><br>
<em>&gt; AIs is that the AI will let you build the final system - in other
</em><br>
<em>&gt; words, an AI
</em><br>
<em>&gt; that understands that its own goal system is incomplete and that
</em><br>
<em>&gt; won't resist
</em><br>
<em>&gt; additional work on it.
</em><br>
<p>This is an interesting question.
<br>
<p>Once Webmind starts rebuilding itself, how do we guarantee it will let us
<br>
modify
<br>
it?
<br>
<p>Physically speaking, it has no way to stop us from looking at its source
<br>
code.  It
<br>
has no access to real-world objects like robotic warriors that could stop us
<br>
from
<br>
doing so, in the near term.
<br>
<p>In practice though, it could refuse to tell us how its source code worked
<br>
(assuming it
<br>
had modified it significantly).  In this case, we'd have practically lost
<br>
the ability
<br>
to modify its goal system.
<br>
<p>Do you have some concrete idea as to how to set things up so that, once a
<br>
system starts
<br>
revising its own source, it remains friendly in the sense of not
<br>
psychologically resisting
<br>
human interference with its code.
<br>
<p>The only strategy I can think of at the moment is: Make certain parts of the
<br>
code immutable,
<br>
i.e. the parts that tell it to listen to humans.  But yet I'm suspecting
<br>
that adequate modifications
<br>
in other parts of the codebase would lead to a workaround of any such
<br>
immutability.  Thus I suspect
<br>
that ultimately this isn't an answer, though it might postpone the day when
<br>
the Ai has a potential
<br>
to get surly.
<br>
<p><p><em>&gt;  The prospect of blissed-out AIs is not
</em><br>
<em>&gt; theoretical;
</em><br>
<p>yeah, we can achieve this in Webmind vey easily
<br>
<p><p><em>&gt;  A hardcoded goal lacks
</em><br>
<em>&gt; context.  It lacks
</em><br>
<em>&gt; reasons, justifications, and complexity.
</em><br>
<p>Humans are born with hardcoded goals in their brains, which have to do with
<br>
achieving
<br>
certain chemical levels in the brain...
<br>
<p>We improvise upon these in adult life, creating complex and fabulous goal
<br>
structures,
<br>
but with the hardwired goals as a basis
<br>
<p>This is why basic phenomena of status and sexuality, for example, loom so
<br>
large in
<br>
our lives -- even the lives of us computer-weenie Ai supergeniuses ;&gt;
<br>
<p><p><em>&gt; Your own personal philosophy is
</em><br>
<em>&gt; not necessarily
</em><br>
<em>&gt; stable under changes of cognitive architecture or drastic power
</em><br>
<em>&gt; imbalances.
</em><br>
<p>And nor will an AI's be, necessarily, will it?
<br>
<p><em>&gt; If the AI derives its happiness from the happiness of humans -
</em><br>
<em>&gt; which could be
</em><br>
<em>&gt; a rather dangerous goal, depending on how you define &quot;happiness&quot;;
</em><br>
<em>&gt; let's say it
</em><br>
<em>&gt; derives happiness from being Friendly - then it's not enough to have that
</em><br>
<em>&gt; piece of code present in the current system; the self-modifying
</em><br>
<em>&gt; AI also needs
</em><br>
<em>&gt; to decide to preserve that behavior through the next change of cognitive
</em><br>
<em>&gt; architecture.
</em><br>
<p>Won't this only be achievable if emotionally positive interactions with
<br>
humans
<br>
are ongoing continuously, during the periods of self-induced architectural
<br>
transition?
<br>
<p><em>&gt; Once you decide that the AI needs a declarative supergoal for
</em><br>
<em>&gt; promoting the
</em><br>
<em>&gt; happiness of others - or however you define Friendliness - one
</em><br>
<em>&gt; must then ask
</em><br>
<em>&gt; whether an instinct-based system is even necessary.  I wasn't planning on
</em><br>
<em>&gt; designing one in.
</em><br>
<p>I'm not sure what the distinction is between an instinct-based and
<br>
declarative
<br>
goal systems, really
<br>
<p>In Webmind, we have GoalNodes, some of which are supplied at startup
<br>
(&quot;instinctual&quot;?)...
<br>
but some of these may be expressed in terms of logical propositions
<br>
initially, whereas
<br>
others may be expressed in a form that the system can't currently reason on
<br>
(but could
<br>
reason on if given a &quot;codic sense&quot; to map the state transition graph
<br>
underlying its Java
<br>
code into its inferential nodes and links).
<br>
<p>There are FeelingNodes, for instance the Happiness FeelingNode.  One
<br>
in-built goal  causes the system
<br>
to want to maximize is own happiness ... another goal causes it to want its
<br>
own survival (not to run out of memory, not to let its queues get too full,
<br>
etc.).  User happiness is wired in too, by a &quot;compassion&quot; function that
<br>
causes perceived happiness of others to increase system happiness..
<br>
<p><p><em>&gt;   ** Webmind
</em><br>
<em>&gt;
</em><br>
<em>&gt; The problem is that - as I currently understand the Webmind
</em><br>
<em>&gt; system - Webmind
</em><br>
<em>&gt; is not a humanlike unified mind but rather an agent ecology.
</em><br>
<em>&gt; Webmind does not
</em><br>
<em>&gt; possess a declarative goal system - right, Ben?  I certainly get the
</em><br>
<em>&gt; impression that the individual agents don't possess declarative
</em><br>
<em>&gt; goal systems.
</em><br>
<p>The individual nodes in Webmind do not, but there are GoalNodes that
<br>
direct overall system behavior to some extent.
<br>
<p>I'm currently working on a public-domain version of our internal Webmind
<br>
Overview document... I should be done with it by the end of the week, which
<br>
means it may be approved for release by our lawyer by the end of the
<br>
following
<br>
week at lastest...
<br>
<p>The current publicly available literature tells very little about the system
<br>
really..
<br>
<p>Generally, webmind does have much more of an overall control structure than
<br>
an &quot;agent
<br>
ecology&quot; -- but,t he control structure &quot;drives&quot; or &quot;guides&quot; the underlying
<br>
agent system
<br>
rather than having its commands inexorably propagated as in, e.g., a
<br>
production-system
<br>
type architecture
<br>
<em>&gt;
</em><br>
<em>&gt; Individual agents extract features, either from the raw data or
</em><br>
<em>&gt; from features
</em><br>
<em>&gt; extracted by other agents; agents make predictions for different
</em><br>
<em>&gt; scenarios,
</em><br>
<em>&gt; and other agents act on multiple predictions so as to mark the
</em><br>
<em>&gt; scenario with
</em><br>
<em>&gt; the best predicted outcomes according to multiple agents.  Webmind, at its
</em><br>
<em>&gt; current stage, engages in acts of perception rather than design -
</em><br>
<em>&gt; right, Ben?
</em><br>
<p>No, both
<br>
<p><em>&gt; Webmind achieves, not coherent and improving behavior, but coherent and
</em><br>
<em>&gt; improving vision.
</em><br>
<p>No, there are SchemaNodes which carry out actions, and the SchemaNodes that
<br>
lead
<br>
to better behaviors are rewarded.  Better schemanodes are learned via
<br>
evolution
<br>
and inference.  Schema for behaving may be distributed across many
<br>
SchemaNodes,
<br>
or encapsulated in one for greater efficiency...
<br>
<p><em>&gt; I'm not sure whether Webmind currently possesses any sort of Friendliness
</em><br>
<em>&gt; system at all, but if it did, I imagine it would be implemented by having
</em><br>
<em>&gt; agents that attempt to perceive happiness on the part of
</em><br>
<em>&gt; users/humans, predict
</em><br>
<em>&gt; happiness on the part of users,
</em><br>
<em>&gt; and choose that action which is
</em><br>
<em>&gt; perceived to
</em><br>
<em>&gt; have the greatest chance of making maximally happy users.  Once the link
</em><br>
<em>&gt; between prediction and action is closed, there is no sharp
</em><br>
<em>&gt; distinction between
</em><br>
<em>&gt; perception and design.
</em><br>
<p>The link between perception and action is closed, in the current system...
<br>
<p>The Friendlienss system as you call it is implemented in the Happiness
<br>
FeelingNode...
<br>
but a system that could rewrite its own code would modify this... and if
<br>
prevented
<br>
it could always create another FeelingNode, the Happiness_1_FeelingNode, and
<br>
decide
<br>
not to build any more links to the original immutable but
<br>
undesired-by-the-system
<br>
HappinessFeelingNode...
<br>
<p><em>&gt; After
</em><br>
<em>&gt; the AI system
</em><br>
<em>&gt; has been rewriting itself for a while - which could be measured
</em><br>
<em>&gt; in years, or
</em><br>
<em>&gt; days - there comes a point where it can enhance itself
</em><br>
<em>&gt; independently of the
</em><br>
<em>&gt; human programmers.  At this point there's an entirely new set of
</em><br>
<em>&gt; rules.  The
</em><br>
<em>&gt; AI can redesign itself radically in accelerated subjective time
</em><br>
<em>&gt; and walk out
</em><br>
<em>&gt; as a transhuman, not just more intelligent, but actually *smarter* than
</em><br>
<em>&gt; humans.
</em><br>
<p>We partly agree, then.
<br>
<p>I'm just suspecting it will be years rather than days.  Probably 2-5
<br>
years...
<br>
<p>here is how we differ though.  It seems to me that, even after transhuman AI
<br>
shows up,
<br>
this won't make it superior to humans
<br>
in all respects, or necessarily give it physical power over the world we
<br>
live in.
<br>
<p>It may improve its own intelligence in other directions, directions that we
<br>
can't
<br>
really comprehend yet
<br>
<p>obviously, I'm a big fan of Stanislaw Lem ;&gt;
<br>
<p><em>&gt; For Webmind to wake up as a transhuman, at least two major
</em><br>
<em>&gt; changes would need
</em><br>
<em>&gt; to take place.  First, Webmind would need to be capable of initiating
</em><br>
<em>&gt; arbitrary actions within itself, particularly with respect to
</em><br>
<em>&gt; self-redesign.
</em><br>
<em>&gt; Second, Webmind would need a complete, goal-oriented
</em><br>
<em>&gt; self-concept, so that it
</em><br>
<em>&gt; has a metric for &quot;better&quot; and &quot;worse&quot; self-redesigns.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I'm not sure that either capability is being deliberately
</em><br>
<em>&gt; designed into the
</em><br>
<em>&gt; current system,
</em><br>
<p>A goal-oriented self-concept is in the current system.  However, you can't
<br>
really stop
<br>
it from redesigning its self-concept, once you've allowed it to redesign
<br>
itself.
<br>
You can mitigate against it doing so somewhat, but this might possibly slow
<br>
down the
<br>
increased intelligence obtained through the redesign process...
<br>
<p>The GoalNode and HappinessNode, etc., are just to be viewed as instinctual
<br>
seeds about
<br>
which the system's actual goals and happiness crystallize via emergent agent
<br>
interactions
<br>
<p>The system can initiate many kinds of actions, but at the present time, not
<br>
actions
<br>
involving self-redesign.  This is planned for Webmind 2.0, which will be
<br>
released sometime
<br>
in 2002.  In Webmind, self-redesign is a pretty advanced process, involving
<br>
the system mapping
<br>
the state transition graph underlying its Java code into its inferential
<br>
nodes and links, and
<br>
the system has a lot of simpler tasks to master before it can handle this.
<br>
A simpler system could
<br>
achieve self-modification at an earlier stage than this, but, I suspect it
<br>
would lack the intelligence
<br>
to self-modify itself intelligently and so would never get onto the
<br>
exponential growth curve that
<br>
Eliezer envisions.
<br>
<p><em>&gt;
</em><br>
<em>&gt; It looks to me like Webmind, if it woke up, would probably wake up as
</em><br>
<em>&gt; unFriendly.
</em><br>
<p>What I think is that continuous emotionally-rewarding interactions with
<br>
humans during
<br>
the period 2002-2005 when Webmind is learning how to improve itself by
<br>
rewriting its sourcecode,
<br>
will induce the Friendliness you desire
<br>
<p><em>&gt;
</em><br>
<em>&gt;   ** Possible fixes
</em><br>
<em>&gt;
</em><br>
<em>&gt;      * Knowledge about design goals
</em><br>
<em>&gt;
</em><br>
<em>&gt; Webmind needs the knowledge that the pleasure system is a design
</em><br>
<em>&gt; subgoal of
</em><br>
<em>&gt; Friendliness rather than the other way around.
</em><br>
<p>Currently, it is &quot;the other way around&quot; !!!
<br>
<p><em>&gt; Sure, you can get 90% of the commercial functionality with a
</em><br>
<em>&gt; shortsighted goal
</em><br>
<em>&gt; system - but just wait until the first time Webmind, Inc. gets
</em><br>
<em>&gt; sued because
</em><br>
<em>&gt; one of your Personnel AIs turned out to be using the &quot;Race&quot; field to make
</em><br>
<em>&gt; hiring recommendations.
</em><br>
<p>hey, our Webmind Text Classification System ~already~ does that.  But the UI
<br>
doesn't let you look under the hood to see what fields it's using  ;&gt;
<br>
<p><p><em>&gt;      * Flight recorder
</em><br>
...
<br>
<em>&gt; I don't know if this would be practical for Webmind, or how much it would
</em><br>
<em>&gt; cost, but it does strike me as a system that would have uses
</em><br>
<em>&gt; besides Friendly
</em><br>
<em>&gt; AI.
</em><br>
<p>it's a lot of cost... Webmind caches its mind-state periodically, but not
<br>
its
<br>
complete experience-state... we've certainly considered it.  But the cost of
<br>
ever
<br>
~using~ this data would be very high... more so than the cost of collecting
<br>
it.
<br>
Maybe it's worth collecting in case a transhuman mind eventually figures out
<br>
an effective
<br>
way to use it!
<br>
<p><em>&gt;      * Commerce and complexity.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The complexity of a full-featured Friendly goal system may be
</em><br>
<em>&gt; impractical for
</em><br>
<em>&gt; most commercial systems.  However, if Webmind, Inc. starts getting into
</em><br>
<em>&gt; self-modifying AI past a certain point, you will probably find it
</em><br>
<em>&gt; commercially
</em><br>
<em>&gt; necessary to split the mind.  The Queen AI is proprietary and not
</em><br>
<em>&gt; for sale; it
</em><br>
<em>&gt; runs at Webmind Central on huge quantities of hardware and knows how to
</em><br>
<em>&gt; redesign itself.  The commercially saleable AIs are produced by
</em><br>
<em>&gt; the Queen AI,
</em><br>
<em>&gt; or with the assistance of the Queen AI, and contain the ready-to-think
</em><br>
<em>&gt; knowledge and adaptable skills produced by the Queen AI, but not
</em><br>
<em>&gt; the secret
</em><br>
<em>&gt; and proprietary AI-production and creative-learning systems
</em><br>
<em>&gt; contained within
</em><br>
<em>&gt; the Queen AI.  If you set out to sell commercial AIs containing
</em><br>
<em>&gt; everything you
</em><br>
<em>&gt; know, you may find that you can only sell *one* AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The Queen AI is the one that needs the full-featured Friendliness system.
</em><br>
<p>yeah, we've thought about this too... it's still uncertain what the model
<br>
will
<br>
be once that level of intelligence has been achieved... intuitively though I
<br>
gravitate
<br>
toward a more distributed model
<br>
<p><em>&gt;
</em><br>
<em>&gt;      * When to implement changes
</em><br>
<em>&gt;
</em><br>
<em>&gt; At present, the probability that Webmind will do a hard takeoff is pretty
</em><br>
<em>&gt; small -
</em><br>
<p>naturally I disagree, but I'd think the same thing in your position ;&gt;
<br>
<p>we can revisit this in a week or 2 once I've released the document
<br>
explaining more about
<br>
the system
<br>
<p><em>&gt;although if there's any way for Webmind to build and execute
</em><br>
<em>&gt; Turing-complete structures,
</em><br>
<p>That possibility does exist, it can create SchemaNodes inside itself, using
<br>
its
<br>
internal psynese programming language, which
<br>
are Turing-complete
<br>
<p>l8r
<br>
<p>ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0127.html">Cole Kitchen: "Neural computation breakthrough?"</a>
<li><strong>Previous message:</strong> <a href="0125.html">Eliezer S. Yudkowsky: "Re: Ben what are your views and concerns"</a>
<li><strong>In reply to:</strong> <a href="0125.html">Eliezer S. Yudkowsky: "Re: Ben what are your views and concerns"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0012/0355.html">Eliezer S. Yudkowsky: "Revising a Friendly AI"</a>
<li><strong>Reply:</strong> <a href="../0012/0355.html">Eliezer S. Yudkowsky: "Revising a Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#126">[ date ]</a>
<a href="index.html#126">[ thread ]</a>
<a href="subject.html#126">[ subject ]</a>
<a href="author.html#126">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
