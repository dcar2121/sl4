<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: upper theoretical limits</title>
<meta name="Author" content="Ben Goertzel (ben@intelligenesis.net)">
<meta name="Subject" content="RE: upper theoretical limits">
<meta name="Date" content="2000-10-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: upper theoretical limits</h1>
<!-- received="Tue Oct 03 23:59:40 2000" -->
<!-- isoreceived="20001004055940" -->
<!-- sent="Tue, 3 Oct 2000 21:35:33 -0400" -->
<!-- isosent="20001004013533" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@intelligenesis.net" -->
<!-- subject="RE: upper theoretical limits" -->
<!-- id="NDBBIBGFAPPPBODIPJMMKEMCEJAA.ben@intelligenesis.net" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="200010032234.OAA09637@aurora.uaf.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@intelligenesis.net?Subject=RE:%20upper%20theoretical%20limits"><em>ben@intelligenesis.net</em></a>)<br>
<strong>Date:</strong> Tue Oct 03 2000 - 19:35:33 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0124.html">Ben Goertzel: "RE: upper theoretical limits"</a>
<li><strong>Previous message:</strong> <a href="0122.html">Alicia Madsen: "upper theoretical limits"</a>
<li><strong>In reply to:</strong> <a href="0122.html">Alicia Madsen: "upper theoretical limits"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0124.html">Ben Goertzel: "RE: upper theoretical limits"</a>
<li><strong>Reply:</strong> <a href="0124.html">Ben Goertzel: "RE: upper theoretical limits"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#123">[ date ]</a>
<a href="index.html#123">[ thread ]</a>
<a href="subject.html#123">[ subject ]</a>
<a href="author.html#123">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
It seems very obvious to me that, for any given hardware setup, there is a
<br>
limit to the
<br>
intelligence of any system that can run on this hardware.
<br>
<p>Of course, you may say that an AI system, using the whole universe as its
<br>
auxiliary memory,
<br>
can extend its intelligence by building itself extra brain, if it's smart
<br>
enough....  But my
<br>
conjecture is that for any particular piece of hardware H, if a mind is
<br>
using H as the physical
<br>
substrate for its thinking, then there is an upper limit to the intelligence
<br>
of this mind.
<br>
<p>Of course, to prove this mathematically requires one to have a mathematical
<br>
definition of
<br>
intelligence.  For instance, if one defines intelligence as &quot;the ability to
<br>
achieve complex
<br>
goals in complex environments,&quot; then this follows according to any
<br>
algorithmic-information-based
<br>
definition of complexity.
<br>
<p>If one wants to allow for AI systems incorporating more &amp; more of the
<br>
universe into their brains,
<br>
then ultimately we arrive at the question of whether the universe is finite.
<br>
If so, there is
<br>
a (presumably very large) upper bound to the intelligence of any entity in
<br>
the universe.
<br>
<p>In practice, I think that the upper limit for intelligence achievable on,
<br>
say, the PC on my desk
<br>
is pretty small.
<br>
<p>Yeah, this PC is a universal turing machine, if it makes use of N floppy
<br>
disks for its &quot;memory tape,&quot;
<br>
but intelligence isn't just about theoretical computing power, it's about
<br>
computing power ~in time~ --
<br>
and looking things up on these N floppy disks will slow the system down
<br>
enough that eventually, the k'th
<br>
floppy disk will not enhance its intelligence any more.
<br>
<p>My guess is that about a terabyte of RAM is required to get human-level
<br>
intelligence (serviced by
<br>
an appropriate number of processors; not, at current processor speeds, just
<br>
one or even just a handful).
<br>
This is based partly on a priori calculations and partly on experimentation
<br>
with our current 100 Gig RAM network.
<br>
<p><p>-- Ben G
<br>
<p><p><p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20upper%20theoretical%20limits">owner-sl4@sysopmind.com</a> [mailto:<a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20upper%20theoretical%20limits">owner-sl4@sysopmind.com</a>]On Behalf
</em><br>
<em>&gt; Of Alicia Madsen
</em><br>
<em>&gt; Sent: Tuesday, October 03, 2000 6:35 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sysopmind.com?Subject=RE:%20upper%20theoretical%20limits">sl4@sysopmind.com</a>
</em><br>
<em>&gt; Subject: upper theoretical limits
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; This upper theoretical limit people speak of, does it all go back
</em><br>
<em>&gt; to how well
</em><br>
<em>&gt; humans grasp the concept of infinity? As a college student, I
</em><br>
<em>&gt; have just begun
</em><br>
<em>&gt; to grapple with calculus, and am not familiar with its peculiarities. In
</em><br>
<em>&gt; replying to my post, explain as much as possible about holes in my logic.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Peter Voss said recently on <a href="mailto:sl4@sysopmind.com?Subject=RE:%20upper%20theoretical%20limits">sl4@sysopmind.com</a> &quot; agree with you,
</em><br>
<em>&gt; that here we
</em><br>
<em>&gt; are in intuition' territory. My own approach to AI design leads
</em><br>
<em>&gt; me to believe
</em><br>
<em>&gt; that at a certain point of intelligence there will be enough of
</em><br>
<em>&gt; an exponential
</em><br>
<em>&gt; burst for one system to dominate. I don't think that hardware
</em><br>
<em>&gt; will be a major
</em><br>
<em>&gt; limiting factor. On the other hand, perhaps each type of
</em><br>
<em>&gt; intelligence has its
</em><br>
<em>&gt; own upper theoretical limit. If so, I haven't yet identified it.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Perhaps if one &quot;type&quot; of intelligence reaches its limit at a
</em><br>
<em>&gt; certain number n,
</em><br>
<em>&gt; and another &quot;type&quot; reaches its intelligence limit at a certain
</em><br>
<em>&gt; number k, then
</em><br>
<em>&gt; all that must be done is rewrite their functions so that they are
</em><br>
<em>&gt; continuos
</em><br>
<em>&gt; together at a new limit. My question is if we have these
</em><br>
<em>&gt; webminds, and they
</em><br>
<em>&gt; are capable of rewriting their programs so that they can
</em><br>
<em>&gt; continually increase
</em><br>
<em>&gt; their capabilities, and then work together, why worry about upper
</em><br>
<em>&gt; limits? I do
</em><br>
<em>&gt; not think that an &quot;upper&quot; limits will exist, as you speak of an
</em><br>
<em>&gt; exponential
</em><br>
<em>&gt; growth rate, and thus continuous everywhere, with this capability to be
</em><br>
<em>&gt; rewritten favorably.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is why I think that one AI or even &quot;webmind&quot; as it is called
</em><br>
<em>&gt; will depend
</em><br>
<em>&gt; on each other, and for its own survival will not &quot;dominate&quot; the
</em><br>
<em>&gt; others. In my
</em><br>
<em>&gt; opinion, an AI is like all other AIs and thus only one of them in
</em><br>
<em>&gt; the first
</em><br>
<em>&gt; place, especially because they will be sharing information. It is
</em><br>
<em>&gt; true that
</em><br>
<em>&gt; there are many parallels of the AI system and humanity, because
</em><br>
<em>&gt; we are friends
</em><br>
<em>&gt; with the logic of Darwin, as we are trapped in the existential
</em><br>
<em>&gt; circumstance
</em><br>
<em>&gt; thrusted upon us.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But Darwin is also not limiting, only a tool we choose to use,
</em><br>
<em>&gt; and we are not
</em><br>
<em>&gt; to fear this tool. In my culture (Inupiaq Eskimo) there are
</em><br>
<em>&gt; examples from past
</em><br>
<em>&gt; and present of elders leaving the small community when food is
</em><br>
<em>&gt; scarce, and
</em><br>
<em>&gt; wander off to die so that the community may survive. I think that because
</em><br>
<em>&gt; humanity has the choice and demonstrated the ability to make this
</em><br>
<em>&gt; choice of
</em><br>
<em>&gt; suicide, then an AI system will also have this choice, as we are
</em><br>
<em>&gt; in the same
</em><br>
<em>&gt; condition. A human interface with the baby AI or webring will not
</em><br>
<em>&gt; jeopardize
</em><br>
<em>&gt; it  because we cannot lie to it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Thus my opinion is that AIs depend on each other for survival,
</em><br>
<em>&gt; and are also
</em><br>
<em>&gt; not limited in intelligence, as well as not limited by their existential
</em><br>
<em>&gt; circumstance.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I follow Eliezer Yudkowsky's logic that we cannot lie to an Ai,
</em><br>
<em>&gt; at least not
</em><br>
<em>&gt; for long, because its logical flaw will be spottable. So it will
</em><br>
<em>&gt; not be an
</em><br>
<em>&gt; issue. What I find interesting, is this concept of AIs having familial
</em><br>
<em>&gt; relationships, although I do not think it is of much importance
</em><br>
<em>&gt; in the long
</em><br>
<em>&gt; run towards an SI. If humans are able to interface with the AI
</em><br>
<em>&gt; and &quot;webrings&quot;
</em><br>
<em>&gt; then we will shape the graph of their intelligence in the
</em><br>
<em>&gt; beginning, and so I
</em><br>
<em>&gt; do not worry about AIs having moral dilemmas because of the
</em><br>
<em>&gt; guidance it will
</em><br>
<em>&gt; receive from its human interface, or even falling out of the
</em><br>
<em>&gt; community of AIs
</em><br>
<em>&gt; and &quot;dying&quot;. With the development of nanotechnology well
</em><br>
<em>&gt; underway, and also
</em><br>
<em>&gt; the presence of many interested individuals and organizations in
</em><br>
<em>&gt; AI, I have no
</em><br>
<em>&gt; fear that an SI will not eventually exist as the ratrace has
</em><br>
<em>&gt; already begun.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Alicia Madsen
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0124.html">Ben Goertzel: "RE: upper theoretical limits"</a>
<li><strong>Previous message:</strong> <a href="0122.html">Alicia Madsen: "upper theoretical limits"</a>
<li><strong>In reply to:</strong> <a href="0122.html">Alicia Madsen: "upper theoretical limits"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0124.html">Ben Goertzel: "RE: upper theoretical limits"</a>
<li><strong>Reply:</strong> <a href="0124.html">Ben Goertzel: "RE: upper theoretical limits"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#123">[ date ]</a>
<a href="index.html#123">[ thread ]</a>
<a href="subject.html#123">[ subject ]</a>
<a href="author.html#123">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
