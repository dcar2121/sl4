<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI timeframes</title>
<meta name="Author" content="J. Andrew Rogers (andrew@ceruleansystems.com)">
<meta name="Subject" content="Re: AI timeframes">
<meta name="Date" content="2004-04-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI timeframes</h1>
<!-- received="Sat Apr 10 00:40:02 2004" -->
<!-- isoreceived="20040410064002" -->
<!-- sent="Fri, 9 Apr 2004 23:39:54 -0700" -->
<!-- isosent="20040410063954" -->
<!-- name="J. Andrew Rogers" -->
<!-- email="andrew@ceruleansystems.com" -->
<!-- subject="Re: AI timeframes" -->
<!-- id="E08E7399-8AB9-11D8-A065-003065C9EC00@ceruleansystems.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="40774309.8010405@cse.ucsc.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> J. Andrew Rogers (<a href="mailto:andrew@ceruleansystems.com?Subject=Re:%20AI%20timeframes"><em>andrew@ceruleansystems.com</em></a>)<br>
<strong>Date:</strong> Sat Apr 10 2004 - 00:39:54 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8475.html">Ben Goertzel: "RE: AI timeframes"</a>
<li><strong>Previous message:</strong> <a href="8473.html">J. Andrew Rogers: "Re: AI timeframes"</a>
<li><strong>In reply to:</strong> <a href="8469.html">Elias Sinderson: "Re: AI timeframes"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8484.html">Philip Sutton: "Re: AI timeframes"</a>
<li><strong>Reply:</strong> <a href="8484.html">Philip Sutton: "Re: AI timeframes"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8474">[ date ]</a>
<a href="index.html#8474">[ thread ]</a>
<a href="subject.html#8474">[ subject ]</a>
<a href="author.html#8474">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Apr 9, 2004, at 5:42 PM, Elias Sinderson wrote:
<br>
<em>&gt; My reflecting on the idea, however, has brought me to another, 
</em><br>
<em>&gt; potentially more important, question: Is it reasonable to assume that 
</em><br>
<em>&gt; a successful SAI project can be controlled? ... James made the 
</em><br>
<em>&gt; assertion [2] that one could not control the results and I imagine 
</em><br>
<em>&gt; that this may be a road well traveled for him - if you have the time, 
</em><br>
<em>&gt; could you expand some more on this point (or provide some appropriate 
</em><br>
<em>&gt; references)?
</em><br>
<p><p>Actually, all that the arguments and discussions over time have shown 
<br>
is that &quot;control&quot; means different things to different people, though 
<br>
certain things are generally agreed upon.  It is highly arguably as to 
<br>
the limits of control, partially depending on what one means by 
<br>
&quot;control&quot;.  However, in the context of my comment that you were 
<br>
responding to:
<br>
<p>AI, like money, doesn't have any value unless you use it.  And the 
<br>
stronger the AI, the more uses and more value it has.  One end of the 
<br>
spectrum, there is the argument that a sufficiently intelligent AI is 
<br>
not controllable, which is a pretty strong argument from theory.  But 
<br>
even if you exclude this case, you end up with an interesting and 
<br>
complicated scenario.
<br>
<p>The complication comes from competing forces:  You exploit your AI by 
<br>
using it, with a stronger AI being more valuable, yet the stronger the 
<br>
AI and the more you use it, the more likely you will find yourself 
<br>
competing with someone else's implementation.  It is no more 
<br>
controllable than any other technology, and in this case, it is a 
<br>
particularly cheap advanced technology to implement and relatively easy 
<br>
to steal.
<br>
<p>You end up generating an AI arms race.  Someone with more resources can 
<br>
crush your AI advantage by brute force through capitalizing a stronger 
<br>
AI unless you grow the strength of your AI as fast as possible.  It is 
<br>
a first-mover advantage writ large, and because the barrier to entry is 
<br>
very low as such things go, you can be leapfrogged very quickly if you 
<br>
do not follow an aggressive growth curve.  All the while, you are 
<br>
rapidly approaching the cliff of general capability where the AI is no 
<br>
longer even controllable because it has grown too strong.
<br>
<p>In this scenario, there are two modes of losing control, depending on  
<br>
your strategy.  You can keep things very secret and controlled, but you 
<br>
are only delaying the inevitable take-off as other implementations 
<br>
eventually get built, and getting leapfrogged as more aggressive 
<br>
implementations come online.  On the other hand, you can put the pedal 
<br>
to the metal and grow the AI as fast as possible so that no other 
<br>
implementation can catch you, but with the enormous caveat that you 
<br>
will rapidly hit the point where your AI is uncontrollable.
<br>
<p>In both cases, you will lose control sooner than later.  The average 
<br>
outcome will be almost the same in both cases.  This is what I was 
<br>
referring to in my previous comment.
<br>
<p>There is a more technically rigorous argument about the limits of 
<br>
control, but that is a lot dryer and, as you surmised, has been 
<br>
discussed in relative length in the past.  I wasn't making a technical 
<br>
case here, though you will find that as such things go, I generally 
<br>
believe that AI is theoretically more controllable than some others on 
<br>
this list, if only by matters of degree.
<br>
<p>j. andrew rogers
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8475.html">Ben Goertzel: "RE: AI timeframes"</a>
<li><strong>Previous message:</strong> <a href="8473.html">J. Andrew Rogers: "Re: AI timeframes"</a>
<li><strong>In reply to:</strong> <a href="8469.html">Elias Sinderson: "Re: AI timeframes"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8484.html">Philip Sutton: "Re: AI timeframes"</a>
<li><strong>Reply:</strong> <a href="8484.html">Philip Sutton: "Re: AI timeframes"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8474">[ date ]</a>
<a href="index.html#8474">[ thread ]</a>
<a href="subject.html#8474">[ subject ]</a>
<a href="author.html#8474">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
