<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendliness, Vagueness, self-modifying AGI, etc</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="Re: Friendliness, Vagueness, self-modifying AGI, etc">
<meta name="Date" content="2004-04-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendliness, Vagueness, self-modifying AGI, etc</h1>
<!-- received="Wed Apr  7 02:03:18 2004" -->
<!-- isoreceived="20040407080318" -->
<!-- sent="Wed, 7 Apr 2004 09:03:15 +0100 (BST)" -->
<!-- isosent="20040407080315" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="Re: Friendliness, Vagueness, self-modifying AGI, etc" -->
<!-- id="20040407080315.50204.qmail@web25103.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="Friendliness, Vagueness, self-modifying AGI, etc" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20Friendliness,%20Vagueness,%20self-modifying%20AGI,%20etc"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Wed Apr 07 2004 - 02:03:15 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8438.html">Thomas Buckner: "Re: escape from simulation"</a>
<li><strong>Previous message:</strong> <a href="8436.html">Marc Geddes: "Re: Friendliness, Vagueness, self-modifying AGI, etc."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8439.html">Ben Goertzel: "RE: Friendliness, Vagueness, self-modifying AGI, etc"</a>
<li><strong>Reply:</strong> <a href="8439.html">Ben Goertzel: "RE: Friendliness, Vagueness, self-modifying AGI, etc"</a>
<li><strong>Maybe reply:</strong> <a href="8445.html">J. Andrew Rogers: "Re: Friendliness, Vagueness, self-modifying AGI, etc"</a>
<li><strong>Reply:</strong> <a href="8446.html">Paul Hughes: "Call for Friendliness PR Campaign"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8437">[ date ]</a>
<a href="index.html#8437">[ thread ]</a>
<a href="subject.html#8437">[ subject ]</a>
<a href="author.html#8437">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I apologise if my remarks to you have appeared aggresive Ben; despite
<br>
my best efforts the crushing mental pressure of existential risks
<br>
still causes me to tend towards extremism. To some extent this just
<br>
reflects how disappointed I am with myself for being so short-sighted
<br>
over Christmas.
<br>
<p>Ben Goertzel wrote;
<br>
<em>&gt; I think that documents on the level of CFAI and LOGI are important
</em><br>
<em>&gt; and necessary, but that they need to come along with more formal,
</em><br>
<em>&gt; detailed and precise documents.
</em><br>
<p>Neither document is a constructive account, as I understand it for
<br>
three reasons; risk, time and expertise. The risk factor is the most
<br>
important; handing out detailed blueprints for building a seed AI is
<br>
dangerous even if they're seriously flawed. From experience I know
<br>
that trawling large numbers of papers for local solutions and
<br>
intelligent use of DE can patch a lot of holes. Dangerously competent
<br>
people can and will go ahead and implement a constructive version of
<br>
LOGI, and a constructive version of CFAI would make things worse by
<br>
giving them a broken version of Friendliness to soothe any lingering
<br>
doubts about destroying the world. Remember that this is exactly what
<br>
Eliezer wanted to do in 2001; before that he had the breathtakingly
<br>
irresponsible plan to implement an AGI with /no/ Friendliness at
<br>
all and just hope that objective morality existed and that the AGI
<br>
found it before doing anything terminal.
<br>
<p>The other issues are that Eliezer doesn't have a lot of time and has
<br>
relatively little actual coding or architecture experience. You may
<br>
need to write a million words and throw them away to be a good
<br>
writer, but you certainly need to write a million lines of code and
<br>
throw them away to be an excellent programmer. I'm sure you know Ben
<br>
that effective AGI architecture requires absolute focus and tends to
<br>
absorb your every waking hour; the constant interference of financial
<br>
and personnel concerns with my ability to technically direct was one
<br>
of the factors that killed my first startup at the end of last year.
<br>
<p><em>&gt; There are a lot of ideas that *look* sensible on the abstract
</em><br>
<em>&gt; conceptual level of CFAI/LOGI, but don't actually correspond to any
</em><br>
<em>&gt; sensible detailed design.
</em><br>
<p>Without going into detail, I found I had to bend but not break most
<br>
of LOGI to make it work. I also put in a lot of novel low-level
<br>
structure, some of which had trickle-up implications for the concept,
<br>
thought and deliberation layers. I was operating under the New
<br>
Bayesian Paradigm (tm) (though I wasn't sold on strict utility back
<br>
then), hacked things a bit for easy DE application and also for
<br>
efficient loose clustering based on the Destiny Star encrypted grid
<br>
computing platform.
<br>
<p><em>&gt; And then, one level down, there are a lot of ideas that look
</em><br>
<em>&gt; sensible on both the conceptual level and the detailed-design
</em><br>
<em>&gt; level, but don't actually work for reasons that are only seen in
</em><br>
<em>&gt; the course of experimenting with the implementation of the design.
</em><br>
<p>There are quite a few things in LOGI and a lot of things in CFAI I
<br>
didn't get as far as implementing before sanity intervened, so I may
<br>
just have left out the hard bits. The only thing I had a huge amount
<br>
of trouble implementing and failed to get to work in any meaningful
<br>
fashion is CFAI's shaper networks. Admittadly I was trying to
<br>
generate pathetically simple moralities grounded in microworlds,
<br>
but still the concept looks unworkable as written to me.
<br>
<p><em>&gt; The only way to avoid this latter problem is to do a full formal,
</em><br>
<em>&gt; mathematical analysis of one's detailed design, but I doubt that
</em><br>
<em>&gt; is possible.
</em><br>
<p>Certainly formal analysis of non-trivial software doesn't have a good
<br>
track record. I'm prepared to keep an open mind until Eliezer
<br>
publishes he revised cognitive theories though; there appears to be
<br>
some impressive mathematical grounding in progress. In the mean time
<br>
detailed cognitive competence verification is the watchword for AGI
<br>
design; for Friendliness I'm working on verifyably emergence-free
<br>
utilitarian descriptive calculus substrates.
<br>
<p><em>&gt; My worry is that, even if the successor to CFAI provides
</em><br>
<em>&gt; Friendliness principles that are not obviously flawed on the
</em><br>
<em>&gt; *conceptual* level, they'll still fall apart in the experimental
</em><br>
<em>&gt; phase.
</em><br>
<p>If my limited progress in understanding Friendliness has revealed one
<br>
thing, it is that a verifyably correct theory puts fairly strict
<br>
constraints on the architecture (and even some on the code layer). I
<br>
don't think conceptual fudging will be necessary to implement it, but
<br>
of course in the face of the SIAI's track record it would be foolish
<br>
to ignore the possiblity of inexplicable early results. In that case
<br>
it will presumably be back to the drawing board (with the requirement
<br>
of explaining not just what the earlier failure was, but also why we
<br>
made it).
<br>
<p><em>&gt; Thus my oft-repeated statement that the true theory of Friendliness
</em><br>
<em>&gt; will be arrived at only: A) by experimentation with fairly smart
</em><br>
<em>&gt; AGI's that can modify themselves to a limited degree.
</em><br>
<p>This is very close to what I thought prior to getting some real
<br>
experience with non-traditional (ie usefully complex) AI. Basically
<br>
once I'd accepted that there was no escape from the logic of the
<br>
Singularity, I tried to help in any way possible. Unlike the average
<br>
SIAI volunteer I was used to producing 50,000+ lines of working,
<br>
tested code a month, had some experience of GOFAI and was used to
<br>
designing complex, asynchronous systems. When my company failed I
<br>
also had a lot of free time, a spare 40 gigaflop compute cluster and
<br>
frankly a motive to engage in obsessive avoidance behaviour. I
<br>
decided to try and produce some experimental data on LOGI and CFAI
<br>
to assist the SIAI research, while also developing relevant low-level
<br>
optimisation techniques and doing design-ahead for an emergency
<br>
last-ditch takeoff seed in case nanowar is about to break out.
<br>
Trying to gather data on obsolete techniques was mistake #1, failing
<br>
to implement a positive safety strategy for takeoff protection was
<br>
mistake #2, considering an illegal and dangerously tempting takeoff
<br>
plan was mistake #3, and using every directed evolution trick I could
<br>
research or invent in order to cut down development time was mistake
<br>
#4. I sincerely hope that relating this reduces the likelyhood that
<br>
anyone else makes the same mistakes, as frankly I have nightmares
<br>
about someone else replicating this without Eliezer catching them in
<br>
time and convincing them how stupid they're being.
<br>
<p><em>&gt; B) by the development of a very strong mathematical theory of
</em><br>
<em>&gt; self-modifying AGI's and their behavior in uncertain environments
</em><br>
<em>&gt;
</em><br>
<em>&gt; My opinion is that we will NOT be able to do B within the next few
</em><br>
<em>&gt; decades, because the math is just too hard.
</em><br>
<p>I'm not qualified to give an opinion on this; I haven't spent years
<br>
staring at it. I suspect that a lot of progress could be made if lots
<br>
of genius researchers were working on it, but you and Eliezer seem to
<br>
be it.
<br>
<p><em>&gt; However, A is obviously pretty dangerous from a Friendliness
</em><br>
<em>&gt; perspective.
</em><br>
<p>ie an existential risk perspective. Developing positive-safety
<br>
takeoff protection is just difficult, not near impossible, and is
<br>
our duty as AGI researchers. I am not too worried about Novamente at
<br>
the moment, but you may well hire a bright spark or two who revises
<br>
the architecture in the direction of AI-completeness (I would've
<br>
volunteered, if you'd asked a few months back). I think everyone
<br>
affiliated with the SIAI would be a lot happier if you adopted a
<br>
draconian, triple-redundant and preemptive takeoff prevention policy.
<br>
<p><em>&gt; My best guess of the correct path is a combination of A and B.
</em><br>
<p>I agree; I have been working on a new design that is mostly DE-free
<br>
with this in mind. However I don't know if anything useful will be
<br>
inferrable from wisdom tournaments occuring under such strict takeoff
<br>
protection. Still, it's worth a try, and there should be a lot of
<br>
useful low-level design results on optimising complex, partial,
<br>
overlaid Bayesian networks and fast reversible perception (achieving
<br>
stochastic-like results from non-stochastic algorithms).
<br>
<p>I am also working on a commercial AI application primarily as the
<br>
most obvious way to raise cash for the SIAI project, but also in some
<br>
part due to the realisations I came to when doing design-ahead for
<br>
the Emergency Takeoff Device (tm). I'm thus working on an 'expert
<br>
system' (actually cut-down LOGI, but don't tell anyone :) for network
<br>
security (penetration testing and active defence) applications. The
<br>
field is red hot in VC circles right now and things are looking
<br>
fairly promising. Of course there's no way (and possibly no point)
<br>
to defend the Internet against a transhuman AGI, but widespread
<br>
deployment of these kind of systems might make it a lot more
<br>
difficult for a viral proto-AGI to get a foothold.
<br>
<p><pre>
----
Footnote: embarassingly I've just discovered that Daniel Dennett
pre-empted the material in my last post by about 14 years; it
could've been a rip-off of 'Consciousness Explained' if the book
had been slightly earlier in my reading list. This does tend to
happen from time to time when you read through five feet of AGI
relevant literature in the space of four months ;&gt; Anyway, here's
a particularly relevant section;
'I have grown accustomed to the disrespect expressed by some of
 the participants for their colleagues in the other disciplines.
 &quot;Why, Dan,&quot; ask the people in artificial intelligence, &quot;do you
 waste your time conferring with those neuroscientists? They
 wave their hands about 'information processing' and worry about
 /where/ it happens, and which neurotransmitters are involved,
 but they haven't a clue about the computational requirements of
 higher cognitive functions.&quot; &quot;Why,&quot; ask the neuroscientists,
 &quot;do you waste your time on the fantasies of artificial
 intelligence? They just invent whatever machinery they wan, and
 say unpardonably ignorant things about the brain. The cognitive
 psychologists, meanwhile, are accused of concocting models with
 /neither/ biological plauisiblity /nor/ proven computational
 powers; the anthropologists wouldn't know a model if they saw
 one, and the philosophers, as we all know, just take in each
 other's laundry, warning about confusions they themselves have
 created, in an arena bereft of both data and empirically
 testable theories. With so many idiots working on the problem,
 no wonder consciousness is still a mystery.
 All these charges are true, and more besides, but I have yet to
 encounter any idiots. Mostly the theorists I have drawn from
 strike me as very smart people - even brilliant people, with the
 arrogance and impatience that often comes with brilliance - but
 with limited perspectives and agendeas, trying to make progress
 on the hard problems by taking whatever shortcuts they can see,
 while deploring other people's shortcuts. No one can keep all
 the problems and details clear, including me, and everyone has
 to mumble, guess and handwave about large parts of the problem.'
Put that chapter together with some choice cuts from 'What
Computers Still Can't Do' and you have have a good account of why
AI hasn't made much progress. This may be for the best;
'mumbling, guessing and handwaving' about takeoff prevention and
Friendliness is a really bad plan. In any case, 'Consciousness
Explained' has the highest concentration of LOGI-relevant material
of anything I've read to date, so if anyone is having a hard time
understanding Eliezer's ideas I highly recommend it.
 * Michael Wilson
 
'Elegance is more than just a frill in life; it is one of the driving
 criteria behind survival.' - Douglas Hofstadter, 'Metamagical Themas'
.
	
	
		
____________________________________________________________
Yahoo! Messenger - Communicate instantly...&quot;Ping&quot; 
your friends today! Download Messenger Now 
<a href="http://uk.messenger.yahoo.com/download/index.html">http://uk.messenger.yahoo.com/download/index.html</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8438.html">Thomas Buckner: "Re: escape from simulation"</a>
<li><strong>Previous message:</strong> <a href="8436.html">Marc Geddes: "Re: Friendliness, Vagueness, self-modifying AGI, etc."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8439.html">Ben Goertzel: "RE: Friendliness, Vagueness, self-modifying AGI, etc"</a>
<li><strong>Reply:</strong> <a href="8439.html">Ben Goertzel: "RE: Friendliness, Vagueness, self-modifying AGI, etc"</a>
<li><strong>Maybe reply:</strong> <a href="8445.html">J. Andrew Rogers: "Re: Friendliness, Vagueness, self-modifying AGI, etc"</a>
<li><strong>Reply:</strong> <a href="8446.html">Paul Hughes: "Call for Friendliness PR Campaign"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8437">[ date ]</a>
<a href="index.html#8437">[ thread ]</a>
<a href="subject.html#8437">[ subject ]</a>
<a href="author.html#8437">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
