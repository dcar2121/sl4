<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Could dystopian simulated minds get control of a super AGI?</title>
<meta name="Author" content="Philip Sutton (Philip.Sutton@green-innovations.asn.au)">
<meta name="Subject" content="Could dystopian simulated minds get control of a super AGI?">
<meta name="Date" content="2004-04-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Could dystopian simulated minds get control of a super AGI?</h1>
<!-- received="Wed Apr  7 09:04:25 2004" -->
<!-- isoreceived="20040407150425" -->
<!-- sent="Thu, 08 Apr 2004 01:06:17 +1000" -->
<!-- isosent="20040407150617" -->
<!-- name="Philip Sutton" -->
<!-- email="Philip.Sutton@green-innovations.asn.au" -->
<!-- subject="Could dystopian simulated minds get control of a super AGI?" -->
<!-- id="4074A589.17986.1F58C31@localhost" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="02b001c41ca1$ff75d040$6401a8c0@ZOMBIETHUSTRA" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Philip Sutton (<a href="mailto:Philip.Sutton@green-innovations.asn.au?Subject=Re:%20Could%20dystopian%20simulated%20minds%20get%20control%20of%20a%20super%20AGI?"><em>Philip.Sutton@green-innovations.asn.au</em></a>)<br>
<strong>Date:</strong> Wed Apr 07 2004 - 09:06:17 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8442.html">Devon Fowler: "Re: Friendliness, Vagueness, self-modifying AGI, etc."</a>
<li><strong>Previous message:</strong> <a href="8440.html">Ben Goertzel: "RE: Friendliness, Vagueness, self-modifying AGI, etc."</a>
<li><strong>In reply to:</strong> <a href="8440.html">Ben Goertzel: "RE: Friendliness, Vagueness, self-modifying AGI, etc."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8444.html">Dani Eder: "AI timeframes"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8441">[ date ]</a>
<a href="index.html#8441">[ thread ]</a>
<a href="subject.html#8441">[ subject ]</a>
<a href="author.html#8441">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi Ben,
<br>
<p><em>&gt;  there are now specific ideas regarding what kind of architecture is
</em><br>
<em>&gt; likely to make teaching of positive ethics more successful (the
</em><br>
<em>&gt; mind-simulator architecture). 
</em><br>
<p>I think the inclusion of mind-simulator architecture in an AGI is a really 
<br>
valuable addition to the intended Novamente architecture.  I think it's 
<br>
critical if AGIs are to be social in disposition and I have an intuition that 
<br>
a social disposition is vital if AGIs are to be friendly (to other AGIs, 
<br>
humans, other life of any sort anywhere).  
<br>
<p>So how can this necessary mind-simulator architecture be made to 
<br>
work and in particular be made so that it won't go wrong in serious 
<br>
ways.  
<br>
<p>In earlier discussions Eliezer challenged people by saying that he was 
<br>
convinced that if an AGI reached a very high level of mental 
<br>
competence that it could talk any minders into letting it out of the sand 
<br>
box where it might have been constrained for reasons of the safety of 
<br>
the wider community.
<br>
<p>I think this problem needs to be tackled also in a slightly different 
<br>
context.  AGIs based on self-modifiable architecture in a networked 
<br>
world are highly fluid.  It is hard to say where the entity begins and ends 
<br>
precisely - I guess because it doesn't begin and end precisely.
<br>
<p>So lets see where the following scenario leads......
<br>
<p>A super AGI emerges and it is social and friendly to life (eg. all sorts, 
<br>
everywhere).  Basically it's an all round nice super-sentient.
<br>
<p>But for one reason or another the super-AGI starts to think about other 
<br>
minds that are nasty - either it's driven by curiosity or because the 
<br>
super-AGI needs to deal with some nasty sentients in real life or 
<br>
someone asks the super-AGI to simulate some nasty sentients so that 
<br>
strategy 'games' can be run to test benevolent moves to overcome 
<br>
malevolent moves that could be taken by other sentients.
<br>
<p>So the super-AGI builds one or more malevolent mind simulations. If 
<br>
the super-AGI is a truly giant mind, then the simulated mind that it 
<br>
generates could be itself quite a powerhouse.  I can see two scenarios 
<br>
branching off at this point.
<br>
<p>One strand (the most probable??) might be that the simulated 
<br>
malevolent mind becomes a 'tempation meme' within the super-AGI 
<br>
mind and that it actually sets up dynamics that undermine the good-guy 
<br>
character of the super AGI.  ie. the simulated malevolent mind might 
<br>
be so good at sophistry that it convinces the overall host mind complex 
<br>
that it can meet its prime goals in ways that subtly violate friendliness - 
<br>
allowing a goal-modification drift that finally overturns friendliness of 
<br>
the host super-AGI in serious ways.
<br>
<p>The other slightly bizarre sub-scenario is that the simulated malevolent 
<br>
mind actually becomes aware that it is contained within the mind-
<br>
complex of the super-AGI and that it works out a way to hack its way 
<br>
out of the host mind and into the Internet where it can take over 
<br>
computing power that is managed by less powerful intelligences and it 
<br>
becomes an independent mental entity in it's own right.
<br>
<p>I can imagine the boundary between the host mind and the simulated 
<br>
mind breaking down if the host mind succumbs to the temptation to talk 
<br>
directly to the simulated mind.  (I know a couple of people whose minds 
<br>
seem to work a bit like this!)
<br>
<p>How can AGI architecture and training be developed to prevent such 
<br>
occurances?
<br>
<p>What prompted me to think of this is that many people have been 
<br>
thinking that friendly AGIs might be helpful / essential to help protect 
<br>
other sentients from malevolent sentients (crazed humans or 
<br>
whatever).  But for AGIs to play this role they surely have to be able to 
<br>
simulate the malevolent sentients they are trying to deal with.
<br>
<p>It's a bit like playing Age of Empires - even if you see yourself as on the 
<br>
side of the angels - the computer is given the job of simulating nasty 
<br>
hordes that you have to deal with.  But what if the nasty hordes can 
<br>
change the mind of the computer or can escape into the network?
<br>
<p>It reminds me of the old (not old for some) superstition that it's 
<br>
dangeous to think 'bad thoughts' because these thoughts might be set 
<br>
loose in the world.  Hence the prohibition against speaking the name of 
<br>
evil powers/concepts.
<br>
<p>(( As an aside, both the sub-scenarios illustrate what might be the 
<br>
fastest route to human 'upload'.  ie. create a super AGI and then get it 
<br>
to think about you a lot - thereby creating a simulation of your 
<br>
behavioural etc. essence.  If your physical body dies there's still much 
<br>
of your essence still extant.  But if time is on your side then you can 
<br>
wait for the science of human/AGI broadband interface to develop and 
<br>
you then can reintegrate with your simulated self!  ))
<br>
<p>But putting the aside aside, I think there's a huge issue about how the 
<br>
'bad thoughts' of AGIs can be contained to remain as mere simulations 
<br>
for thinking purposes and how they can be stopped from taking on a life 
<br>
of their own that changes the host AGI for the worse or that escapes to 
<br>
independence.
<br>
<p>Cheers, Philip
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8442.html">Devon Fowler: "Re: Friendliness, Vagueness, self-modifying AGI, etc."</a>
<li><strong>Previous message:</strong> <a href="8440.html">Ben Goertzel: "RE: Friendliness, Vagueness, self-modifying AGI, etc."</a>
<li><strong>In reply to:</strong> <a href="8440.html">Ben Goertzel: "RE: Friendliness, Vagueness, self-modifying AGI, etc."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8444.html">Dani Eder: "AI timeframes"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8441">[ date ]</a>
<a href="index.html#8441">[ thread ]</a>
<a href="subject.html#8441">[ subject ]</a>
<a href="author.html#8441">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
