<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Friendliness, Vagueness, self-modifying AGI, etc</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Friendliness, Vagueness, self-modifying AGI, etc">
<meta name="Date" content="2004-04-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Friendliness, Vagueness, self-modifying AGI, etc</h1>
<!-- received="Wed Apr  7 06:58:35 2004" -->
<!-- isoreceived="20040407125835" -->
<!-- sent="Wed, 7 Apr 2004 08:58:06 -0400" -->
<!-- isosent="20040407125806" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Friendliness, Vagueness, self-modifying AGI, etc" -->
<!-- id="02a601c41c9f$fe8a22a0$6401a8c0@ZOMBIETHUSTRA" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20040407080315.50204.qmail@web25103.mail.ukl.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Friendliness,%20Vagueness,%20self-modifying%20AGI,%20etc"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Wed Apr 07 2004 - 06:58:06 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8440.html">Ben Goertzel: "RE: Friendliness, Vagueness, self-modifying AGI, etc."</a>
<li><strong>Previous message:</strong> <a href="8438.html">Thomas Buckner: "Re: escape from simulation"</a>
<li><strong>In reply to:</strong> <a href="8437.html">Michael Wilson: "Re: Friendliness, Vagueness, self-modifying AGI, etc"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8445.html">J. Andrew Rogers: "Re: Friendliness, Vagueness, self-modifying AGI, etc"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8439">[ date ]</a>
<a href="index.html#8439">[ thread ]</a>
<a href="subject.html#8439">[ subject ]</a>
<a href="author.html#8439">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; Ben Goertzel wrote;
</em><br>
<em>&gt; &gt; I think that documents on the level of CFAI and LOGI are 
</em><br>
<em>&gt; important and 
</em><br>
<em>&gt; &gt; necessary, but that they need to come along with more 
</em><br>
<em>&gt; formal, detailed 
</em><br>
<em>&gt; &gt; and precise documents.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Neither document is a constructive account, as I understand 
</em><br>
<em>&gt; it for three reasons; risk, time and expertise. The risk 
</em><br>
<em>&gt; factor is the most important; handing out detailed blueprints 
</em><br>
<em>&gt; for building a seed AI is dangerous even if they're seriously 
</em><br>
<em>&gt; flawed. 
</em><br>
<p>Yes, you have a good point, as a general principle.  
<br>
<p>However, I do *not* believe Eliezer is hiding a correct design for an
<br>
AGI due to security concerns.
<br>
<p>What you are describing is one of the several reasons that Novamente is
<br>
not open-source.  Right now the code is only moderately closely held
<br>
(i.e. it's proprietary but we don't take outlandish security
<br>
precautions), but we will get more paranoid about this as more of the
<br>
design is implemented and the kinks are worked out.
<br>
<p><em>&gt; Remember that 
</em><br>
<em>&gt; this is exactly what Eliezer wanted to do in 2001; before 
</em><br>
<em>&gt; that he had the breathtakingly irresponsible plan to 
</em><br>
<em>&gt; implement an AGI with /no/ Friendliness at all and just hope 
</em><br>
<em>&gt; that objective morality existed and that the AGI found it 
</em><br>
<em>&gt; before doing anything terminal.
</em><br>
<p>I'm not sure his plan was THAT extreme as you describe it, was it?
<br>
<p>Perhaps it was more like my original plan, which was to rely on TEACHING
<br>
exclusively to imbue positive ethics in my AI system.  My work has
<br>
evolved since, and there are now specific ideas regarding what kind of
<br>
architecture is likely to make teaching of positive ethics more
<br>
successful (the mind-simulator architecture).
<br>
&nbsp;
<br>
<em>&gt; The other issues are that Eliezer doesn't have a lot of time 
</em><br>
<p>Hey, that is patently not the case.  Eliezer has more time than any of
<br>
us, as -- last I checked -- he has no family to take care of, and no
<br>
job, and no other interests nearly rivaling the Singularity in
<br>
intensity.  If he has no time for concrete AI design it's because he has
<br>
prioritized other types of Singularity-oriented work.
<br>
<p><em>&gt; and has relatively little actual coding or architecture 
</em><br>
<em>&gt; experience. 
</em><br>
<p>This one is a good point.
<br>
<p><em>&gt; I'm sure you know Ben that effective 
</em><br>
<em>&gt; AGI architecture requires absolute focus and tends to absorb 
</em><br>
<em>&gt; your every waking hour; the constant interference of 
</em><br>
<em>&gt; financial and personnel concerns with my ability to 
</em><br>
<em>&gt; technically direct was one of the factors that killed my 
</em><br>
<em>&gt; first startup at the end of last year.
</em><br>
<p>I wish I had absolute focus on AGI these days.  I have about 50% focus
<br>
on it, but fortunately the other 50% of my working-time is focused on
<br>
Novamente-based commercial software projects, so there's not so much
<br>
cognitive dissonance.
<br>
&nbsp;
<br>
<em>&gt; There are quite a few things in LOGI and a lot of things in 
</em><br>
<em>&gt; CFAI I didn't get as far as implementing before sanity 
</em><br>
<em>&gt; intervened, so I may just have left out the hard bits. The 
</em><br>
<em>&gt; only thing I had a huge amount of trouble implementing and 
</em><br>
<em>&gt; failed to get to work in any meaningful fashion is CFAI's 
</em><br>
<em>&gt; shaper networks. Admittadly I was trying to generate 
</em><br>
<em>&gt; pathetically simple moralities grounded in microworlds, but 
</em><br>
<em>&gt; still the concept looks unworkable as written to me.
</em><br>
<p>I believe the &quot;shaper networks&quot; idea is sensible and meaningful, BUT, I
<br>
believe that in order to get shaper networks to work, a whole bunch of
<br>
intermediary dynamics are needed, which are not touched in Eliezer's
<br>
documents.  A lot of  mind dynamics is left out, and a lot of issues in
<br>
knowledge representation.
<br>
<p>CFAI defines
<br>
<p>&quot;Shaper:  A shaper is a philosophical affector, a source of supergoal
<br>
content or a modifier for other shapers; a belief in the AI's
<br>
philosophy; a node in the causal network that produces supergoal
<br>
content.&quot;
<br>
<p>Essentially, a shaper network requires a workable, learnable,
<br>
reason-able representation of abstract content, which allows abstract
<br>
bits of uncertain knowledge to interact with each other, to modify each
<br>
other, to spawn actions, etc. 
<br>
<p>So far as I can tell there is nothing in Eli's AI framework that
<br>
suggests a knowledge representation capable of being coupled with
<br>
sufficiently powerful learning and reasoning algorithms to be used in
<br>
this way.
<br>
<p>I think this CAN be done with a neural net architecture of some sort,
<br>
and in my paper on &quot;Hebbian Logic&quot; I gave a sketchy idea of how.
<br>
Novamente takes a different approach, using a &quot;probabilistic combinatory
<br>
term logic&quot; approach to knowledge representation, and then using a
<br>
special kind of probabilistic inference (with some relation to Hebbian
<br>
learning) synthesized with evolutionary learning for learning/reasoning.
<br>
<p><p>But my point is that Eli's architecture gives a grand overall picture,
<br>
but doesn't actually give a workable and *learnable and modifiable* way
<br>
to represent complex knowledge.  Of course it's easy to represent
<br>
complex knowledge -- predicate logic does that, but it does so in a very
<br>
brittle, non-learnable way.  And it's easy to get learning to work on
<br>
SIMPLE knowledge; standard neural net architectures do that.  But
<br>
representing complex knowledge in a way that flexible, adaptive learning
<br>
can work on -- that is hard, and that is the crux of making AGI; it is
<br>
required for making &quot;shaper networks&quot; work, and it is one thing that
<br>
Eli's writings never come close to addressing IMO.
<br>
<p><p><em>&gt; &gt; B) by the development of a very strong mathematical theory of 
</em><br>
<em>&gt; &gt; self-modifying AGI's and their behavior in uncertain environments
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; My opinion is that we will NOT be able to do B within the next few 
</em><br>
<em>&gt; &gt; decades, because the math is just too hard.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'm not qualified to give an opinion on this; I haven't spent 
</em><br>
<em>&gt; years staring at it. I suspect that a lot of progress could 
</em><br>
<em>&gt; be made if lots of genius researchers were working on it, but 
</em><br>
<em>&gt; you and Eliezer seem to be it.
</em><br>
<p>That could be; but a lot of genius researchers are working on related
<br>
but apparently EASIER questions in complex systems dynamics, without
<br>
making very rapid progress...
<br>
&nbsp;
<br>
<em>&gt; ie an existential risk perspective. Developing 
</em><br>
<em>&gt; positive-safety takeoff protection is just difficult, not 
</em><br>
<em>&gt; near impossible, and is our duty as AGI researchers. I am not 
</em><br>
<em>&gt; too worried about Novamente at the moment, but you may well 
</em><br>
<em>&gt; hire a bright spark or two who revises the architecture in 
</em><br>
<em>&gt; the direction of AI-completeness 
</em><br>
<p>;-)
<br>
<p>We may revise the architecture in future, but probably not before we
<br>
complete implementing the current one and seeing how easy or hard it is
<br>
to teach stuff in an appropriate simulated world
<br>
<p>Anyway, frankly, you do not know nearly enough about the architecture to
<br>
know if it needs revision or not.
<br>
<p>If you're interested to become involved in Novamente, you can email me
<br>
off-list, of course.  
<br>
<p><em>&gt; (I would've volunteered, if 
</em><br>
<em>&gt; you'd asked a few months back). I think everyone affiliated 
</em><br>
<em>&gt; with the SIAI would be a lot happier if you adopted a 
</em><br>
<em>&gt; draconian, triple-redundant and preemptive takeoff prevention policy.
</em><br>
<p>We will do that when the time comes ... Novamente is not at a stage
<br>
where this is relevant right now, but it will be in a matter of 1-5
<br>
years depending on a number of factors.
<br>
&nbsp;
<br>
<em>&gt; I'm 
</em><br>
<em>&gt; thus working on an 'expert system' (actually cut-down LOGI, 
</em><br>
<em>&gt; but don't tell anyone :) for network security (penetration 
</em><br>
<em>&gt; testing and active defence) applications. The field is red 
</em><br>
<em>&gt; hot in VC circles right now and things are looking fairly 
</em><br>
<em>&gt; promising.
</em><br>
<p>Interesting!  I'll email you off-list some stuff about network security
<br>
that I wrote about a year ago, when I was thinking about getting into
<br>
that area.
<br>
<p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8440.html">Ben Goertzel: "RE: Friendliness, Vagueness, self-modifying AGI, etc."</a>
<li><strong>Previous message:</strong> <a href="8438.html">Thomas Buckner: "Re: escape from simulation"</a>
<li><strong>In reply to:</strong> <a href="8437.html">Michael Wilson: "Re: Friendliness, Vagueness, self-modifying AGI, etc"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8445.html">J. Andrew Rogers: "Re: Friendliness, Vagueness, self-modifying AGI, etc"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8439">[ date ]</a>
<a href="index.html#8439">[ thread ]</a>
<a href="subject.html#8439">[ subject ]</a>
<a href="author.html#8439">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
