<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: &quot;SIMULATIONS: A Singularitarian Primer&quot;</title>
<meta name="Author" content="Mitch Howe (mitch_howe@yahoo.com)">
<meta name="Subject" content="&quot;SIMULATIONS: A Singularitarian Primer&quot;">
<meta name="Date" content="2001-10-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>&quot;SIMULATIONS: A Singularitarian Primer&quot;</h1>
<!-- received="Sat Oct 06 18:06:47 2001" -->
<!-- isoreceived="20011007000647" -->
<!-- sent="Sat, 6 Oct 2001 15:25:56 -0600" -->
<!-- isosent="20011006212556" -->
<!-- name="Mitch Howe" -->
<!-- email="mitch_howe@yahoo.com" -->
<!-- subject="&quot;SIMULATIONS: A Singularitarian Primer&quot;" -->
<!-- id="001301c14ead$7d5f7080$c110e63f@mitch" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Mitch Howe (<a href="mailto:mitch_howe@yahoo.com?Subject=Re:%20&quot;SIMULATIONS:%20A%20Singularitarian%20Primer&quot;"><em>mitch_howe@yahoo.com</em></a>)<br>
<strong>Date:</strong> Sat Oct 06 2001 - 15:25:56 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2285.html">Eliezer S. Yudkowsky: "Re: &quot;SIMULATIONS: A Singularitarian Primer&quot;"</a>
<li><strong>Previous message:</strong> <a href="2283.html">Spudboy100@aol.com: "Re: as a member of this list..sI4"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2285.html">Eliezer S. Yudkowsky: "Re: &quot;SIMULATIONS: A Singularitarian Primer&quot;"</a>
<li><strong>Reply:</strong> <a href="2285.html">Eliezer S. Yudkowsky: "Re: &quot;SIMULATIONS: A Singularitarian Primer&quot;"</a>
<li><strong>Reply:</strong> <a href="2286.html">Gordon Worley: "Re: &quot;SIMULATIONS: A Singularitarian Primer&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2284">[ date ]</a>
<a href="index.html#2284">[ thread ]</a>
<a href="subject.html#2284">[ subject ]</a>
<a href="author.html#2284">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
The following is the result of my efforts to summarize and make accessible
<br>
the discussions that have occurred on the SL4 list regarding living in
<br>
simulations.  I wrote as though it were part of a future FAQ for newcomers
<br>
to the list, my target audience being general readers, especiialy those who
<br>
are ready to start gulping down some SL4 material.
<br>
<p>Please let me know if I have left out obviously important issues or used
<br>
certain terms out of context.  I have assumed that certain key terms (sysop,
<br>
uploading, etc.) would be linked to other explanatory articles.  I have
<br>
additionally assumed that nobody on the list is looking for specific credit
<br>
for ideas regarding simulations, so while I have not directly quoted anyone,
<br>
neither have I cited any one person as a source.  (Also, many of my own
<br>
ideas inevitably filled areas where list content seemed lacking in juicy
<br>
material -- but I think these are pretty sound.)
<br>
<p>All editing suggestions are welcome.
<br>
<p>------
<br>
SIMULATIONS: A Singularitarian Primer
<br>
(based on the musings of SL4 mailing list participants)
<br>
by Mitch Howe
<br>
<p>IMAGINATION MEETS REALITY
<br>
<p>Mankind has been creating simulations from its earliest beginnings. When
<br>
deciding whether to climb a tree to retrieve a fruit, the brain simulates
<br>
the tree, the fruit, the human, and forces such as gravity. An experiment is
<br>
run. Whether the simulation ends with the consumption of sweet fruit, or
<br>
concludes with being picked apart by a large pack of wolves after a
<br>
paralyzing back injury, a decision can be made.
<br>
<p>In more recent times, artificial simulations performed by computers have
<br>
provided scientists with opportunities to model situations difficult to test
<br>
in the field, such as the formation of galaxies or the intricate chain of
<br>
events in a nuclear detonation. Other simulators have provided training for
<br>
dangerous or expensive tasks, such as military combat and space travel.
<br>
These same types of simulations are also adapted to entertainment, in games
<br>
that model everything from gladiatorial combat with rocket launchers to the
<br>
design and management of amusement parks.
<br>
<p>When discussing the future, especially the possibilities that lie beyond the
<br>
Singularity, the idea of simulation greatly exceeds the detail of the most
<br>
advanced military flight simulators. It even goes far beyond the immersing
<br>
ability of today's most advanced &quot;virtual reality&quot; gear.  The simulations
<br>
ahead could be so advanced that the uninformed individual would not even
<br>
recognize being in a simulation. These simulations could be so versatile
<br>
that they would become the preferred reality of mankind's future.
<br>
<p>Simulations of such fidelity would, of course, require incredibly abundant
<br>
and powerful technology. While such systems could not be built today, they
<br>
are easily imagined and seem perfectly possible for a superintelligence
<br>
(SI) -- the expected eventual result of any artificial intelligence (AI)
<br>
system able to repeatedly improve on its own design.  High fidelity
<br>
simulations also seem to be a logical tool for a superintelligence acting as
<br>
mankind's benevolent guardian.
<br>
<p>The imagined future where a concerned SI regulates human affairs, preventing
<br>
as a minimum everything from murder to nuclear holocaust, is also referred
<br>
to as the Sysop scenario, in reference to a type of computer program that
<br>
regulates the activities of other programs. Truly life-like simulations
<br>
would almost certainly require the management of an impressively powerful
<br>
intelligence, and for this reason simulations and the Sysop concept are
<br>
seldom discussed independently.
<br>
<p>LAYERS OF SIMULATION
<br>
<p>Given the phenomenal capabilities an SI could have, one could question
<br>
whether a Sysop would actually need to use virtual, computerized simulations
<br>
to fully protect and serve mankind.  Nanotechnology, the widely anticipated
<br>
ability to manufacture by manipulating individual atoms, could give a Sysop
<br>
nearly infinite reach in the real world. Constants like gravity and inertia
<br>
could almost seem to be manipulated at will by invisible, omnipresent
<br>
nanotech devices that respond instantly to deflect bullets or cushion falls.
<br>
Nanodoctors could render all illness obsolete.  Poverty and hunger could be
<br>
conquered by limitless manufacturing capacity tailored to every individual
<br>
need.  It is easily argued, however, that a reality so casually interrupted
<br>
by an outside force is, in effect, a simulation. Controlled reality -- this
<br>
broadest type of simulation -- should, however, be inherently less efficient
<br>
than other possibilities, and so would likely never be implemented.
<br>
<p>Basic limitations in human senses are what allow a life-like simulation to
<br>
be far more efficient than controlled reality.  The human eyes, for example,
<br>
cannot discern the individual pixels of a high resolution digital image from
<br>
a few feet away; Unaided, they cannot make out skin cells at any distance.
<br>
In a simulation, reality does not need to be modeled except as it could be
<br>
perceived.  Skin cells do not need to exist unless they are being examined
<br>
under a microscope.  A tree falling in a forest need not bother making a
<br>
sound if nobody is around to hear it.
<br>
<p>Free from simulating or controlling reality in all its complexity, watching
<br>
over and serving humanity in an artificial environment becomes a
<br>
comparatively simple question of content delivery.  Specifically, how
<br>
directly would the sensory data of an artificial environment be sent to the
<br>
human brain?  Several possibilities present themselves.
<br>
<p>The simulation might be projected externally, through special clothing or in
<br>
dedicated rooms.  This could be similar to the &quot;holodeck&quot; from Star Trek:
<br>
The Next Generation.  While external systems would be the least intrusive on
<br>
the human body, they would also require a great deal of equipment and
<br>
energy.
<br>
<p>Simulations could also be delivered directly to the brain through the
<br>
brain's own sensory pathways - the neural lines of communication that tell
<br>
it what the eyes are seeing, the ears hearing, etc.  This would be akin to
<br>
the human condition seen in the movie &quot;The Matrix&quot;.  The human body would be
<br>
preserved, but the brain would receive artificially created content
<br>
indistinguishable from that which the body would provide in a similar, but
<br>
real, environment.  Direct-to-brain delivery of simulations would require
<br>
fewer resources than external systems but would still be shackled by the
<br>
need to maintain human bodies - or brains, at the very least.
<br>
<p>Replacing the brain with an artificial system would completely eliminate the
<br>
need for infrastructure dedicated to preserving biological bodies.  This
<br>
idea, known commonly as &quot;uploading&quot;, holds that the human brain is
<br>
essentially a complex but reproducible computer.  If the pathways of thought
<br>
and the mechanisms of memory can be accurately mapped then human
<br>
individuality might be transferred and preserved within a computer system,
<br>
presumably a superintelligent one.  Simulations could then be delivered to
<br>
individuals as internally run programs, making the Sysop title for the
<br>
controlling entity especially appropriate.
<br>
<p>It remains to be seen whether uploading will be possible.  The brain still
<br>
holds many mysteries, not the least of which is the uncertain nature and
<br>
origin of consciousness; That which makes us self-aware may not prove
<br>
transferable.  Nevertheless, of all currently imaginable types of
<br>
simulation, this last, uploaded type has the most potential.  It would give
<br>
a Sysop the greatest freedom to alter the environment to ensure the survival
<br>
and fulfillment of the human race.  It would also allow for a tremendously
<br>
high population, since the only limiting resources would be energy and
<br>
computing material - and any self-improving superintelligence would
<br>
eventually optimize the material from which it is made.  The densest, most
<br>
efficient computing material that can be made out of a given unit of matter,
<br>
often called &quot;computronium&quot;, would likely become the preferred state for
<br>
most matter under the Sysop's control in an uploaded future;  A ham sandwich
<br>
gives a single biological human the ability to continue functioning for a
<br>
few hours, but the matter available in a ham sandwich, when converted to
<br>
computronium, might provide for the minds and worlds of a million uploaded
<br>
humans.
<br>
<p>DOUBTS AND CONCERNS
<br>
<p>Before moving on to some conjectures that can be made about life in a
<br>
simulation, it must be said that many do not believe large-scale simulation
<br>
is likely or even desirable.  For instance, it is logical to conclude that
<br>
if humans were uploaded to an artificial substrate they would be free to
<br>
expand their own capabilities, becoming superintelligences in their own
<br>
right.   Humans that go on to obtain god-like powers (a concept
<br>
traditionally called &quot;apotheosis&quot;) may not have any use for simulated
<br>
environments.
<br>
<p>Also, it is possible that a chaotic and unprotected life would be more
<br>
fulfilling than an existence in even the most paradisaical simulation - a
<br>
dilemma sometimes referred to as the &quot;gilded cage&quot; problem.  There is every
<br>
reason to think that a traditional life could be provided, in simulation, to
<br>
those who desire it, but knowing it is still a simulation might be hollow
<br>
and depressing just the same.
<br>
<p>Of course, in an environment where most any desire could be provided, the
<br>
mind itself could likely be altered so that it did not realize it was living
<br>
in a simulation.  But this opens the door to a related concern, for it might
<br>
be simplest to merely reconfigure the brain or uploaded mind to a
<br>
continually happy state.  Creating a stagnant, blissed-out condition is also
<br>
referred to as &quot;wireheading&quot;, and is potentially destructive of human
<br>
individuality and progress.
<br>
<p>Not to be ignored, also, is the fear that moving the bulk of human
<br>
civilization into a simulation would increase its vulnerability to external
<br>
disaster or extinction.  If the human race exists as billions of individuals
<br>
per cubic foot of computronium, an errant meteorite could prove more
<br>
catastrophic than all of the wars and disasters of the last five centuries.
<br>
Also, if humanity is reliant on a single superintelligence for its
<br>
existence, then should this entity ever become incapacitated the entire
<br>
species would be jeopardized.  It stands to reason that anything
<br>
superintelligent would have prepared for these possibilities to the point
<br>
where unsimulated life would be far more dangerous, but the worst disasters
<br>
are, almost by definition, unanticipated.
<br>
<p>LIFE IN A SIMULATION
<br>
<p>Upon first consideration, it may seem unlikely that any meaningful
<br>
predictions can be made about what it would be like to live in a simulation,
<br>
especially as an uploaded individual.  However, some logical conclusions can
<br>
be made based on one simple fact: Resources will always be finite.  Even if
<br>
a Sysop converted all the matter in the solar system to computronium and the
<br>
energy to power it, there would still be limits to what it could do.  The
<br>
same would hold true if the entire galaxy were consumed this way.  The
<br>
capabilities of a superintelligence so endowed may seem unimaginably huge,
<br>
but they could still be defined.  As a result, there must be decisions made
<br>
as to how resources are allocated.
<br>
<p>Certain tasks are guaranteed to consume matter and energy.  Bodies, brains,
<br>
or uploaded minds will probably be common and intensive operations to
<br>
maintain.  It stands to reason, then, that there would be a maximum
<br>
population that a Sysop could support, necessitating the need for
<br>
regulations regarding population growth.  But if humanity retains any of its
<br>
most human characteristics, there will be the desire to reproduce.  A
<br>
benevolent Sysop would understand this, and probably provide a way for it to
<br>
occur.  But it might set limits based on the rate at which it increases its
<br>
own capacity to maintain a growing population.  These could be very
<br>
accommodating early on when capacity would likely be many orders of
<br>
magnitude greater than needed.  These limits may become very strict,
<br>
however, as new computing material ceases to become available.  There is,
<br>
after all, no reason to believe that uploaded minds, or even biological
<br>
brains in the care of a superintelligence, would ever free up resources by
<br>
dying of &quot;natural&quot; causes.
<br>
<p>Traditional human reproduction would not necessarily be the only means of
<br>
population growth.  It should be possible to create simulated human minds
<br>
from scratch.  These would consume the same resources as uploaded minds, and
<br>
ethically they should be entitled to the same rights and privileges as
<br>
anyone else.  They would certainly be no less human or intelligent by any
<br>
measurable trait.  Hence, there would likely be regulations regarding the
<br>
creation of new minds from scratch at least as restrictive as those
<br>
governing traditional kinds of reproduction.
<br>
<p>It has already been mentioned that the infrastructure needed to support a
<br>
single biological human in a traditional environment is far greater than
<br>
that needed to maintain a brain or uploaded mind in a simulation.  Although
<br>
it seems unlikely that a benevolent, friendly Sysop would force uploading or
<br>
simulation on anyone not born that way, these options are sure to be
<br>
strongly encouraged.  This should not be too difficult, since the lifestyle
<br>
available in simulation promises to be far more enjoyable than the
<br>
traditional life so famously described by the thinker Hobbes as short,
<br>
brutish, and chaotic.  Pain, hunger, illness, poverty, crime, war, and even
<br>
death need not enter the pearly gates of simulation.
<br>
<p>Simulations themselves would also consume resources, however, meaning
<br>
communal simulated environments should be more efficient than private
<br>
simulations for all.  Humans are evolved social creatures, and there is
<br>
every reason to believe that they would generally prefer to share large
<br>
simulations anyway.  Communal simulations require a more complex system of
<br>
Sysop intervention, however, to insure that nobody's basic rights are
<br>
involuntarily infringed upon.  Murder and rape would be wholly prohibited.
<br>
Nevertheless, a benevolent Sysop would likely wish to provide some sort of
<br>
venue for those who wish to experiment with forces and actions that would
<br>
not involve harm to sentient beings but would still infringe upon the rights
<br>
of others.  So it seems reasonable that life in simulation would involve
<br>
some combination of large, communal environments and specialized, private
<br>
workshops.  The Sysop could determine some means of equitably doling out
<br>
available resources to allow for those who crave the power of private
<br>
simulation.
<br>
<p>CONTROVERSIAL SPECULATIONS
<br>
<p>Beyond the finite nature of resources, little can be known for sure about
<br>
the nature of life in simulation, but speculations are plentiful and make
<br>
for interesting discussions.  One that has probably been covered more than
<br>
any other, for thousands of years in fact, is the assertion that all of us
<br>
are already living in a high-fidelity simulation, or some kind of dream that
<br>
does not represent &quot;true&quot; reality.  Though impossible to disprove, this
<br>
concept is typically weakened by the inability of its proponents to
<br>
adequately explain the purpose of a simulation that includes seemingly
<br>
pointless pain and suffering in abundance.  (Religious arguments are often
<br>
introduced into this discussion, usually resulting in inconsistent or
<br>
circuitous logic.)  A common sentiment is that any entity uncaring enough to
<br>
allow simulation participants to experience such generally brutish
<br>
conditions would probably not take enough interest in humanity to bother
<br>
creating and maintaining a simulation in the first place;  An unconcerned SI
<br>
would be better off eliminating or ignoring humankind entirely, making it
<br>
unlikely that we are currently living in a simulation.
<br>
<p>Pondering life in a simulation leads many to wonder what kinds of
<br>
institutions might still prove useful to humanity.  For instance, some
<br>
speculate that a barter or capitalist economy might thrive, with computing
<br>
resources as the ultimate medium of exchange.  The idea is that a Sysop
<br>
might equitably distribute computing power to all its dependents (often
<br>
called &quot;citizens&quot;), but that these individuals might in turn trade this
<br>
precious commodity for goods and services that cannot be simulated.  The
<br>
most common counterpoint made is the assumption that there would be no
<br>
obtainable good or service that the Sysop could not simulate, making trade
<br>
between citizens pointless.  However, given the general understanding that
<br>
sentient persons could not be simulated without becoming sentient wards of
<br>
the Sysop in their own right, it seems probable that certain pleasures or
<br>
services could only be satisfactorily provided by other citizens: those
<br>
calling for the most subtle human touch or requiring voluntary infringement
<br>
of protected rights.
<br>
<p>The nature of time in simulation also lends itself to contradictory
<br>
viewpoints, especially in the context of uploaded individuals.  Some see
<br>
perceived time as being accelerated by the speed of the hardware upon which
<br>
mind and simulations are run - an uploaded person living a hundred years,
<br>
for example, in ten minutes of &quot;real&quot; time.  Others see it possible that the
<br>
speed of uploaded thought and simulations could be slowed to conserve
<br>
energy - an uploaded mind experiencing ten seconds, for instance, in 100
<br>
years of real time.  Naturally, still others predict a middle solution where
<br>
simulated life is clocked to match real time, perhaps in homage to history
<br>
or tradition.
<br>
<p>Related debates concern the longevity of uploaded citizens, such as whether
<br>
it might be appropriate for otherwise immortal minds to be terminated in
<br>
order to allow everyone interested the experience of reproducing another
<br>
sentient - an interesting and controversial tradeoff should the Sysop
<br>
exhaust all means of obtaining new resources.  The right of an individual to
<br>
reproduce might be tied to an obligation to &quot;die&quot; after a predetermined
<br>
amount of time.  Ethical dilemmas complicate this seemingly straightforward
<br>
approach, such as the possibility that someone could enter into a
<br>
reproduction/death contract without fully appreciating the consequences -
<br>
after all, the consequences of death are not understood now and may not be
<br>
in the future, either.  Logically, a superintelligence ought to be able to
<br>
make competent judgements about the maturity of each citizen, but it might
<br>
still determine that the fairest course of action would be to universally
<br>
allow reproduction and universally terminate all citizens upon reaching a
<br>
certain age - or to deny reproduction entirely if resource limits are
<br>
reached.
<br>
<p>Harking back to earlier discussions regarding the security of civilization
<br>
in a simulation, another interesting line of thought concerns the idea of
<br>
&quot;backups&quot; -  identical copies of a citizen's mind, presumably for the
<br>
purpose of redundancy in case of disaster.  The logical counterpoint to this
<br>
concept is that a duplicate of a mind would, unless exposed to the exact
<br>
same environment, diverge from its original in memory and function.  The
<br>
backup would thus not be a backup at all, but a new and unique sentient.  Of
<br>
course, when discussing simulations it may be possible to exactly replicate
<br>
the original's environment for the sake of the backup, but many would argue
<br>
that these minds would still be different people that could not be ethically
<br>
interchanged.  This problem has very deep philosophical roots and may never
<br>
be resolved even if the nature of consciousness is fully understood.
<br>
<p>Also reviving a previously mentioned concern, some feel that even if an SI
<br>
and mankind jointly determine to move to a completely simulated environment,
<br>
there may still be the ability of uploaded minds to expand and utilize more
<br>
resources for &quot;thought&quot;.  One possible scenario has the Sysop allocating
<br>
resources to everyone equally, with every individual free to use these for
<br>
any purpose, whether that be to create large and complex simulations or to
<br>
enhance the power of their own minds.  This vision allows for uploaded minds
<br>
that become superintelligent prisoners, suffocated of an environment after
<br>
having converted all their available resources into mental capacity.  The
<br>
ability of minds to expand their own abilities may thus need to be regulated
<br>
in order to avoid the unsavory, presumably unethical confinement of
<br>
superintelligent beings.  But would restricting the capacity for growth be
<br>
any more ethical?
<br>
<p>It should be obvious now that many a moral dilemma punctuates the envisioned
<br>
potential of simulations.  Would a simulated mind's feelings be any less
<br>
valid than those found in a biological brain?  Could a superintelligence
<br>
actually be entitled to more rights and resources than an individual of
<br>
traditional capacity?  How much intelligence does an artificial mind need to
<br>
be considered sentient?
<br>
<p>BRINGING SIMULATION INTO FOCUS
<br>
<p>Easy answers to such questions do not exist, but contradictory opinions
<br>
abound.   As with many controversial issues, however, there are benefits to
<br>
continued discussion.  More debate may not result in consensus regarding
<br>
simulations, but should help to inspire guidelines that people can be
<br>
comfortable with.  Exploring the possibilities of life in simulation can
<br>
help prepare individuals and communities for the mind-boggling potential
<br>
benefits of artificial environments while acknowledging their limitations
<br>
and requirements.
<br>
<p><p>_________________________________________________________
<br>
Do You Yahoo!?
<br>
Get your free @yahoo.com address at <a href="http://mail.yahoo.com">http://mail.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2285.html">Eliezer S. Yudkowsky: "Re: &quot;SIMULATIONS: A Singularitarian Primer&quot;"</a>
<li><strong>Previous message:</strong> <a href="2283.html">Spudboy100@aol.com: "Re: as a member of this list..sI4"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2285.html">Eliezer S. Yudkowsky: "Re: &quot;SIMULATIONS: A Singularitarian Primer&quot;"</a>
<li><strong>Reply:</strong> <a href="2285.html">Eliezer S. Yudkowsky: "Re: &quot;SIMULATIONS: A Singularitarian Primer&quot;"</a>
<li><strong>Reply:</strong> <a href="2286.html">Gordon Worley: "Re: &quot;SIMULATIONS: A Singularitarian Primer&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2284">[ date ]</a>
<a href="index.html#2284">[ thread ]</a>
<a href="subject.html#2284">[ subject ]</a>
<a href="author.html#2284">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
