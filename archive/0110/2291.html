<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The Simulation Risk</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: The Simulation Risk">
<meta name="Date" content="2001-10-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The Simulation Risk</h1>
<!-- received="Mon Oct 08 02:07:11 2001" -->
<!-- isoreceived="20011008080711" -->
<!-- sent="Sun, 07 Oct 2001 20:10:34 -0400" -->
<!-- isosent="20011008001034" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: The Simulation Risk" -->
<!-- id="3BC0EEFA.4A234C89@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3BC0E322.2050703@pacbell.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20The%20Simulation%20Risk"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Oct 07 2001 - 18:10:34 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2292.html">Mitch Howe: "Sysop and Population (was: &quot;SIMULATIONS...&quot;)"</a>
<li><strong>Previous message:</strong> <a href="2290.html">Edwin Evans: "The Simulation Risk"</a>
<li><strong>In reply to:</strong> <a href="2290.html">Edwin Evans: "The Simulation Risk"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2291">[ date ]</a>
<a href="index.html#2291">[ thread ]</a>
<a href="subject.html#2291">[ subject ]</a>
<a href="author.html#2291">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Edwin Evans wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Creating a simulation is one of the most dangerous things an AI could
</em><br>
<em>&gt; do. Conceivably, it could simulate pain and suffering (1) on a
</em><br>
<em>&gt; hyperastronomical scale. Whereas I find this very unlikely (2), the
</em><br>
<em>&gt; sheer magnitude of the disaster dictates that people trying to build
</em><br>
<em>&gt; superpowerful AIs should be concerned about this danger. I think the
</em><br>
<em>&gt; only thing that can justify taking this risk is the goal of creating
</em><br>
<em>&gt; positive benefits of a similar magnitude and creating net positive
</em><br>
<em>&gt; objective value (3).
</em><br>
<p>I agree that this is the best reason for creating SI.  However, it is not
<br>
the only conceivable reason.  For example, it might be the case that it is
<br>
not a question of *whether* to take the risk, but when and who; in this
<br>
scenario it may be logical, although perhaps not exactly the smartest
<br>
thing to do, to trade off a 70% risk against a 90% risk.  Of course in
<br>
this case another logical course of action is to try and wipe humanity off
<br>
the planet, counting the whole thing as a bad deal and leaving the
<br>
creation of Singularities to some other, more organized species.  This is
<br>
also not perhaps the smartest thing to do.  Call me a starry-eyed
<br>
impractical idealist, but I tend to think that intelligence is the tool
<br>
that lets us avoid these dilemmas in the first place...
<br>
<p><em>&gt; If it wasn't for the risk of losing the positive
</em><br>
<em>&gt; benefit, we should be patient beyond any personal and ordinary human
</em><br>
<em>&gt; suffering to reduce the risk of this disaster. Fortunately I think the
</em><br>
<em>&gt; odds are heavily in our favor. Unfortunately, we cannot risk waiting too
</em><br>
<em>&gt; long to try to push them more in our favor.
</em><br>
<p>Maybe the best summary is that the fundamental odds determine whether or
<br>
not it's a good idea to have a Singularity some time in the next fifty
<br>
thousand years, while it's the timing issue relative to other technologies
<br>
and other projects - and, of course, the ongoing planetwide death rate -
<br>
which make it a good idea to do it some time in, say, the next decade.
<br>
<p>For the record, I would think the chance of a CFAI-architecture failure of
<br>
Friendliness resulting in an exponential number of ancestral simulations
<br>
should actually be fairly small (no obvious path from point A to point B),
<br>
and like many specific negative scenarios can be reduced in probability
<br>
even further by the use of ethical injunctions and negative anchors. 
<br>
Because of the Bayesian arguments, I'm always on the lookout for a way
<br>
that a malfunctioning Singularity (CFAI architecture or not) could wind up
<br>
creating a large number of ancestral simulations - or, given my own
<br>
experienced identity, a large number of programming-team simulations - but
<br>
I haven't found any particularly alarming-looking ones as yet.  (Not that
<br>
there's much I could do about it if I did spot one... by hypothesis, if
<br>
the Bayesian argument is valid, it would be too late to do anything about
<br>
it.)
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2292.html">Mitch Howe: "Sysop and Population (was: &quot;SIMULATIONS...&quot;)"</a>
<li><strong>Previous message:</strong> <a href="2290.html">Edwin Evans: "The Simulation Risk"</a>
<li><strong>In reply to:</strong> <a href="2290.html">Edwin Evans: "The Simulation Risk"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2291">[ date ]</a>
<a href="index.html#2291">[ thread ]</a>
<a href="subject.html#2291">[ subject ]</a>
<a href="author.html#2291">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
