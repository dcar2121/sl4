<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] to-do list for strong, nice AI</title>
<meta name="Author" content="Pavitra (celestialcognition@gmail.com)">
<meta name="Subject" content="Re: [sl4] to-do list for strong, nice AI">
<meta name="Date" content="2009-10-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] to-do list for strong, nice AI</h1>
<!-- received="Sat Oct 17 00:46:29 2009" -->
<!-- isoreceived="20091017064629" -->
<!-- sent="Sat, 17 Oct 2009 01:45:59 -0500" -->
<!-- isosent="20091017064559" -->
<!-- name="Pavitra" -->
<!-- email="celestialcognition@gmail.com" -->
<!-- subject="Re: [sl4] to-do list for strong, nice AI" -->
<!-- id="4AD96827.3010802@gmail.com" -->
<!-- charset="UTF-8" -->
<!-- inreplyto="F8049862-6D06-4292-8F58-B04AF3464905@ceruleansystems.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Pavitra (<a href="mailto:celestialcognition@gmail.com?Subject=Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI"><em>celestialcognition@gmail.com</em></a>)<br>
<strong>Date:</strong> Sat Oct 17 2009 - 00:45:59 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20573.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Previous message:</strong> <a href="20571.html">J. Andrew Rogers: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>In reply to:</strong> <a href="20571.html">J. Andrew Rogers: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20573.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20572">[ date ]</a>
<a href="index.html#20572">[ thread ]</a>
<a href="subject.html#20572">[ subject ]</a>
<a href="author.html#20572">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
J. Andrew Rogers wrote:
<br>
<em>&gt; On Oct 16, 2009, at 10:54 PM, Pavitra wrote:
</em><br>
<em>&gt;&gt; Matt Mahoney wrote:
</em><br>
<em>&gt;&gt;&gt; To satisfy conflicts between people
</em><br>
<em>&gt;&gt;&gt; (e.g. I want your money), AI has to know what everyone knows. Then it
</em><br>
<em>&gt;&gt;&gt; could calculate what an ideal secrecy-free market would do and
</em><br>
<em>&gt;&gt;&gt; allocate resources accordingly.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Assuming an ideal secrecy-free market generates the best possible
</em><br>
<em>&gt;&gt; allocation of resources. Unless there's a relevant theorem of ethics  
</em><br>
<em>&gt;&gt; I'm
</em><br>
<em>&gt;&gt; not aware of, that seems a nontrivial assumption.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What is your definition of &quot;best possible allocation&quot;?
</em><br>
<p>That's kind of the point. It seems premature to assert that a particular
<br>
strategy is the best until we have a working definition of &quot;best&quot; (step A).
<br>
<p><em>&gt; Matt is making  
</em><br>
<em>&gt; a pretty pedestrian decision theoretic assertion about AGI. It would  
</em><br>
<em>&gt; out-perform real markets in terms of distribution of resources, but  
</em><br>
<em>&gt; the allocation would still be market-like because resources would  
</em><br>
<em>&gt; still be scarce.  It would be as though a smarter version of you was  
</em><br>
<em>&gt; making decisions for you.
</em><br>
<p>Umm. I don't see how you get from &quot;ideal market &gt; real market&quot; to &quot;ideal
<br>
market &gt;= X, for any X&quot;.
<br>
<p>In particular, I see why making _my_ decisions ideal and fully-informed
<br>
would be good for _me_, but I don't see why it's good that _other_
<br>
people's values should be trusted. Are you really advocating that we
<br>
should allow people who want to eat babies (the law of large number
<br>
implies that at least one exists somewhere on Earth) to be full agents
<br>
in our ideal secrecy-free market, with just as much power over the fate
<br>
of the universe as you and me?
<br>
<p>Coherent extrapolated volition sounds nice politically, but
<br>
realistically, I want the AI to act according to *my* values. To the
<br>
extent that I care what other people think (which is more than the tone
<br>
of this sentence might suggest), I've already updated my desires
<br>
accordingly. By definition, I believe that an AI implementing my
<br>
personal values exclusively is the best possible AI.
<br>
<p><em>&gt; Ethics has little to do with it.
</em><br>
<p>Perhaps 'morality' would have been more accurate than 'ethics'.
<br>
<p><em>&gt; How much sub-optimality should the  
</em><br>
<em>&gt; AGI intentionally insert into decisions, and how does one objectively  
</em><br>
<em>&gt; differentiate nominally &quot;bad&quot; suboptimality and nominally &quot;good&quot;  
</em><br>
<em>&gt; suboptimality?
</em><br>
<p>None, of course; it should behave as close to optimally as possible. My
<br>
objection to the market strategy is that it may optimize partially for
<br>
other people's values at the expense of my own.
<br>
<p><p>
<br><p>
<p><hr>
<ul>
<li>application/pgp-signature attachment: <a href="../att-20572/01-signature.asc">OpenPGP digital signature</a>
</ul>
<!-- attachment="01-signature.asc" -->
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20573.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Previous message:</strong> <a href="20571.html">J. Andrew Rogers: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>In reply to:</strong> <a href="20571.html">J. Andrew Rogers: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20573.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20572">[ date ]</a>
<a href="index.html#20572">[ thread ]</a>
<a href="subject.html#20572">[ subject ]</a>
<a href="author.html#20572">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
