<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: [sl4] Example, and request for a practical experiment (was Re: How big is an FAI solution?) nice AI)</title>
<meta name="Author" content="Tim Freeman (tim@fungible.com)">
<meta name="Subject" content="[sl4] Example, and request for a practical experiment (was Re: How big is an FAI solution?) nice AI)">
<meta name="Date" content="2009-10-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>[sl4] Example, and request for a practical experiment (was Re: How big is an FAI solution?) nice AI)</h1>
<!-- received="Sat Oct 24 10:30:14 2009" -->
<!-- isoreceived="20091024163014" -->
<!-- sent="Sat, 24 Oct 2009 08:51:12 -0700" -->
<!-- isosent="20091024155112" -->
<!-- name="Tim Freeman" -->
<!-- email="tim@fungible.com" -->
<!-- subject="[sl4] Example, and request for a practical experiment (was Re: How big is an FAI solution?) nice AI)" -->
<!-- id="20091024163012.B4681D293C@fungible.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="b7a9e8680910231325s5168212et2f849f8218bb7be0@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tim Freeman (<a href="mailto:tim@fungible.com?Subject=Re:%20[sl4]%20Example,%20and%20request%20for%20a%20practical%20experiment%20(was%20Re:%20How%20big%20is%20an%20FAI%20solution?)%20nice%20AI)"><em>tim@fungible.com</em></a>)<br>
<strong>Date:</strong> Sat Oct 24 2009 - 09:51:12 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20604.html">Tim Freeman: "Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>Previous message:</strong> <a href="20602.html">Thomas McCabe: "Re: How big is an FAI solution? (was Re: [sl4] to-do list for strong,  nice AI)"</a>
<li><strong>In reply to:</strong> <a href="20602.html">Thomas McCabe: "Re: How big is an FAI solution? (was Re: [sl4] to-do list for strong,  nice AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20581.html">Kevin: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20603">[ date ]</a>
<a href="index.html#20603">[ thread ]</a>
<a href="subject.html#20603">[ subject ]</a>
<a href="author.html#20603">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Wed, Oct 21, 2009 at 9:59 AM, Tim Freeman &lt;<a href="mailto:tim@fungible.com?Subject=Re:%20[sl4]%20Example,%20and%20request%20for%20a%20practical%20experiment%20(was%20Re:%20How%20big%20is%20an%20FAI%20solution?)%20nice%20AI)">tim@fungible.com</a>&gt; wrote:
<br>
<em>&gt; The PDF file at <a href="http://www.fungible.com/respect/talk-mar-2009.pdf">http://www.fungible.com/respect/talk-mar-2009.pdf</a> is
</em><br>
<em>&gt; &lt;1MB.  A real definition would include training data that would
</em><br>
<em>&gt; probably be a few GB's.  Start reading at
</em><br>
<em>&gt; <a href="http://www.fungible.com/respect/index.html">http://www.fungible.com/respect/index.html</a>.
</em><br>
<p>I keep quoting that so it's clear which proposed AI we're talking
<br>
about.
<br>
<p>By the way, I've been interpreting &quot;an FAI solution&quot; to mean a
<br>
specification of what an FAI would do.  This doesn't include a
<br>
practical impelementation, so an FAI solution may not include an AI
<br>
solution.  A practical AI solution might be huge, and I don't know
<br>
what it would take.  My apologies to everyone if we've been talking
<br>
about different problems.
<br>
<p>From: Thomas McCabe &lt;<a href="mailto:pphysics141@gmail.com?Subject=Re:%20[sl4]%20Example,%20and%20request%20for%20a%20practical%20experiment%20(was%20Re:%20How%20big%20is%20an%20FAI%20solution?)%20nice%20AI)">pphysics141@gmail.com</a>&gt;
<br>
<em>&gt;Suppose I'm in a burning building, and my
</em><br>
<em>&gt;legs are crushed beneath a one-ton iron bar. My desires are, by the
</em><br>
<em>&gt;standards of such things, reasonably simple: I do not want to die a
</em><br>
<em>&gt;horrible, fiery death. My actions are to attempt to lift the bar off
</em><br>
<em>&gt;of me, but I cannot lift something which is more than ten times my
</em><br>
<em>&gt;weight, and so, without intervention, I *will* die a horrible, fiery
</em><br>
<em>&gt;death. If an FAI tried to mimic my actions, it would exert an upward
</em><br>
<em>&gt;force on the bar which is grossly insufficient to actually get it off
</em><br>
<em>&gt;of me, and I would *still* die a horrible, fiery death.
</em><br>
<p>It's good to have concrete examples.  Thanks for posting that.
<br>
<p>However, I get a different result when I work through the scenario.
<br>
The proposed AI makes one model of what all humans want and believe,
<br>
so the fact that everyone else has consistently taken action to avoid
<br>
horrible fiery death when possible makes it much easier for the AI to
<br>
believe that you also want to avoid horrible fiery death.  Otherwise
<br>
the AI has to think you're someone special, and the added code that
<br>
says &quot;Thomas McCabe, unlike everyone else, wants to die a fiery death&quot;
<br>
makes those explanations of human motivation require more code and
<br>
therefore have less a-priori probability than the ones where you, like
<br>
everyone else, don't want a fiery death.  Thus the AI could decide to
<br>
lift the iron bar off of you without having to pay attention to your
<br>
behavior at all.
<br>
<p>The AI might get the right answer if even you were the only person it
<br>
knows about and the AI hasn't observed enough past to see that you
<br>
generally prefer to avoid fiery death.  It infers your beliefs and
<br>
your motivation simultneously.  You attempted to lift the bar and
<br>
failed.  The AI needs to explain that.  Perhaps you thought the bar
<br>
was lighter than it really was (belief), and you wanted to lift the
<br>
bar and escape from the fire (motivation).  If that explanation seems
<br>
more likely than the competing explanations, the AI would then try to
<br>
give you what you wanted (it would lift the bar, put out the fire, or
<br>
both).
<br>
<p>If, on the other hand, the AI has observed you to repeatedly attempt
<br>
and fail to lift large objects in the past, along with limited joyful
<br>
experiments with self-immolation, then the AI would be liekely to
<br>
assume that you are doing what you want to do and leave you to your
<br>
pastimes as you burn alive.
<br>
<p>The AI disregards your actual beliefs, BTW.  This leads to an
<br>
interesting experiment that I haven't done yet.  Suppose the AI
<br>
determines that Christianity is unlikely; the AI is, pragmatically, an
<br>
Athiest.  If this AI encountered a fervent Christian who wanted to get
<br>
to Heaven, it would try to arrange for this person to get what he
<br>
would want if he had the same beliefs (athiesm) as the AI.  I don't
<br>
know what these people would want if athiesm were true.
<br>
<p>Here's the experiment: somebody please go find a friendly devout
<br>
Christian and ask them:
<br>
<p>&nbsp;&nbsp;&nbsp;If God appeared before you and said &quot;I am entitled to change the
<br>
&nbsp;&nbsp;&nbsp;rules, and I am doing so now.  I quit.  Now you are your own.  You
<br>
&nbsp;&nbsp;&nbsp;no longer have a soul -- your thoughts will henceforth be an
<br>
&nbsp;&nbsp;&nbsp;ordinary physical consequence of neurons firing in in your brain,
<br>
&nbsp;&nbsp;&nbsp;and if the information there is lost, you are gone.  There is no
<br>
&nbsp;&nbsp;&nbsp;Heaven or Hell&quot; along with other tenants of athiesm, and then God
<br>
&nbsp;&nbsp;&nbsp;kept his word and vanished, and furthermore you believed Him, what
<br>
&nbsp;&nbsp;&nbsp;would you want in that situation?
<br>
<p>If they would want bizarre self-destructive things, we have a problem:
<br>
the AI might easily decide to murder all Christians.  If they would
<br>
want the same things that other athiests want (decent food, health,
<br>
family, monster trucks, whatever) then at this proposal doesn't have
<br>
that specific bug.
<br>
<p><pre>
-- 
Tim Freeman               <a href="http://www.fungible.com">http://www.fungible.com</a>           <a href="mailto:tim@fungible.com?Subject=Re:%20[sl4]%20Example,%20and%20request%20for%20a%20practical%20experiment%20(was%20Re:%20How%20big%20is%20an%20FAI%20solution?)%20nice%20AI)">tim@fungible.com</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20604.html">Tim Freeman: "Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>Previous message:</strong> <a href="20602.html">Thomas McCabe: "Re: How big is an FAI solution? (was Re: [sl4] to-do list for strong,  nice AI)"</a>
<li><strong>In reply to:</strong> <a href="20602.html">Thomas McCabe: "Re: How big is an FAI solution? (was Re: [sl4] to-do list for strong,  nice AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20581.html">Kevin: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20603">[ date ]</a>
<a href="index.html#20603">[ thread ]</a>
<a href="subject.html#20603">[ subject ]</a>
<a href="author.html#20603">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
