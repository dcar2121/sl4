<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] to-do list for strong, nice AI</title>
<meta name="Author" content="Pavitra (celestialcognition@gmail.com)">
<meta name="Subject" content="Re: [sl4] to-do list for strong, nice AI">
<meta name="Date" content="2009-10-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] to-do list for strong, nice AI</h1>
<!-- received="Sun Oct 18 16:18:51 2009" -->
<!-- isoreceived="20091018221851" -->
<!-- sent="Sun, 18 Oct 2009 17:18:16 -0500" -->
<!-- isosent="20091018221816" -->
<!-- name="Pavitra" -->
<!-- email="celestialcognition@gmail.com" -->
<!-- subject="Re: [sl4] to-do list for strong, nice AI" -->
<!-- id="4ADB9428.5000304@gmail.com" -->
<!-- charset="UTF-8" -->
<!-- inreplyto="51635.52138.qm@web51902.mail.re2.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Pavitra (<a href="mailto:celestialcognition@gmail.com?Subject=Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI"><em>celestialcognition@gmail.com</em></a>)<br>
<strong>Date:</strong> Sun Oct 18 2009 - 16:18:16 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20575.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Previous message:</strong> <a href="20573.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>In reply to:</strong> <a href="20573.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20575.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Reply:</strong> <a href="20575.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20574">[ date ]</a>
<a href="index.html#20574">[ thread ]</a>
<a href="subject.html#20574">[ subject ]</a>
<a href="author.html#20574">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; I define &quot;best&quot; to mean the result that an ideal secrecy-free market
</em><br>
<em>&gt; would produce. Do you have a better definition, for some definition
</em><br>
<em>&gt; of &quot;better definition&quot;?
</em><br>
<p>Yes: the result that an ideal secrecy-free dictatorship run by me would
<br>
produce. (Secrecy-free in this context means that there are no secrets
<br>
from the dictator. I can choose what to divulge to which of the rest of
<br>
the citizens.)
<br>
<p><em>&gt;&gt;&gt; One human knows 10^9 bits (Landauer's estimate of human long term
</em><br>
<em>&gt;&gt;&gt;  memory). 10^10 humans know 10^17 to 10^18 bits, allowing for
</em><br>
<em>&gt;&gt;&gt; some overlapping knowledge.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; Again, where are you obtaining your estimates of
</em><br>
<em>&gt;&gt; degree-of-compressibility?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The U.S. Dept. of Labor estimates it costs on average $15K to replace
</em><br>
<em>&gt; an employee. This is about 4 months of U.S. per capita income, or
</em><br>
<em>&gt; 0.5% of life expectancy. This mean on average that nobody knows more
</em><br>
<em>&gt; than 99.5% of what you need to know to do your job. It is reasonable
</em><br>
<em>&gt; to assume that as the economy grows and machines do our more mundane
</em><br>
<em>&gt; tasks, that jobs will become more specialized and that the fraction
</em><br>
<em>&gt; of shared knowledge will decrease. It is already the case that higher
</em><br>
<em>&gt; paying jobs cost more to replace, e.g. 1-2 years.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Turnover cost is relevant because the primary function of AI will be
</em><br>
<em>&gt; to make humans more productive, at least initially. Our interest is
</em><br>
<em>&gt; in the cost of work-related knowledge.
</em><br>
<p>This feels wrong in several ways.
<br>
<p>Why is redundancy in employment utility a good indicator of redundancy
<br>
in the aspects of human experience that we will care about preserving
<br>
through the Singularity?
<br>
<p>Doesn't the variance in redundancy by job type imply that the
<br>
cost-to-replace reflects the nature of the job more than it reflects the
<br>
nature of the person?
<br>
<p>Why will &quot;the primary function of AI ... be to make humans more
<br>
productive, at least initially&quot;? Shouldn't the AI handle
<br>
productivity/production more or less unilaterally, and make humans more
<br>
happy/eudaimonic?
<br>
<p><em>&gt;&gt; The Turing test is probably not suitable.
</em><br>
<em>&gt; ...
</em><br>
<em>&gt;&gt; Chatterbots have been found to improve their Turing Test
</em><br>
<em>&gt;&gt; performance significantly by committing deliberate errors of
</em><br>
<em>&gt;&gt; spelling and avoiding topics that require intelligent or coherent
</em><br>
<em>&gt;&gt; discourse.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I agree. Turing was aware of the problem in 1950 when he gave an
</em><br>
<em>&gt; example of a computer taking 30 seconds to give the wrong answer to
</em><br>
<em>&gt; an arithmetic problem. I proposed text compression as one
</em><br>
<em>&gt; alternative. <a href="http://mattmahoney.net/dc/rationale.html">http://mattmahoney.net/dc/rationale.html</a>
</em><br>
<p>That seems like a pretty good definition, but I'm not convinced that a
<br>
gigabyte of Wikipedia is _the best_ possible corpus. In particular,
<br>
Wikipedia is very thin on fiction. I want AI to be able to grok the arts.
<br>
<p><em>&gt;&gt;&gt;&gt; C-&gt;D[ ] Develop an automated comparison test that returns the
</em><br>
<em>&gt;&gt;&gt;&gt; more intelligent of two given systems.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;&gt; How? The test giver has to know more than the test taker.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; Again, this seems more a criticism of C than of D.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It depends on what you mean by &quot;intelligence&quot;. A more general
</em><br>
<em>&gt; definition might be making more accurate predictions, or making them
</em><br>
<em>&gt; faster. But it raises the question of how the evaluator can know the
</em><br>
<em>&gt; correct answers unless it is more intelligent than the evaluated. If
</em><br>
<em>&gt; your goal is to predict human behavior (a prerequisite for
</em><br>
<em>&gt; friendliness), then humans have to do the testing.
</em><br>
<p>This still sounds like you're talking about step C.
<br>
<p>Step D says, &quot;Assuming we already have a formal definition of
<br>
intelligence, develop a computable comparison test for intelligence&quot;. I
<br>
don't see why the comparison test requires greater-than-tested
<br>
intelligence _in addition to_ whatever level of intelligence the formal
<br>
definition created in C constitutes.
<br>
<p>The incomputability of Kolgomorov complexity is likely to be the largest
<br>
obstacle in step D.
<br>
<p>Your website that you linked to above is likely to be about as close as
<br>
we'll ever get to completing steps C and D.
<br>
<p><em>&gt;&gt; The whole point of Singularity-level AGI is that it's a nonhuman 
</em><br>
<em>&gt;&gt; intelligence. By hypothesis, &quot;humanity&quot; âŠ‰ &quot;intelligence&quot;.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Nonhuman intelligence = human extinction.
</em><br>
<p>What about intelligence that is a proper superset of the human?
<br>
Molecular matter != destruction of all atoms.
<br>
<p><em>&gt; I don't mean this in a good or bad way, as &quot;good&quot; and &quot;bad&quot; are
</em><br>
<em>&gt; relative to whatever populates the world after humans are gone. It
</em><br>
<em>&gt; might be your goal to have these agents preserve human memories, but
</em><br>
<em>&gt; it might not be *their* goals, and it's their goals that count. They
</em><br>
<em>&gt; might rationally conclude that if your memories were those of
</em><br>
<em>&gt; somebody else's, you wouldn't notice.
</em><br>
<p>It's my goals that count right now, because I'm the one deciding my
<br>
actions. Hopefully, current human goals can shape what the goals of the
<br>
future-beings will be.
<br>
<p><em>&gt; You could argue that with somebody else's memories you wouldn't be
</em><br>
<em>&gt; &quot;you&quot;. But what are you arguing? If a machine simulates you well
</em><br>
<em>&gt; enough that nobody can tell the difference, is it really you? Would
</em><br>
<em>&gt; you kill yourself and expect your soul to transfer to the machine?
</em><br>
<p>I agree that it may (depending on the nature of the Singularity) be
<br>
largely irrelevant what my goals are going to be after the rise of the
<br>
Machines. But it matters very much what my goals are now, because the
<br>
Machines will be created by pre-Singularity humans.
<br>
<p><em>&gt;&gt; The goal, then, would be to ensure that the Singularity will be
</em><br>
<em>&gt;&gt; Friendly.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Define &quot;Friendly&quot; in 10^17 bits or less.
</em><br>
<p>Cheating answer: shares my value system.
<br>
<p><p>
<br><p>
<p><hr>
<ul>
<li>application/pgp-signature attachment: <a href="../att-20574/01-signature.asc">OpenPGP digital signature</a>
</ul>
<!-- attachment="01-signature.asc" -->
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20575.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Previous message:</strong> <a href="20573.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>In reply to:</strong> <a href="20573.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20575.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Reply:</strong> <a href="20575.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20574">[ date ]</a>
<a href="index.html#20574">[ thread ]</a>
<a href="subject.html#20574">[ subject ]</a>
<a href="author.html#20574">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
