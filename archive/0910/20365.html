<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] I am a Singularitian who does not believe in the Singularity.</title>
<meta name="Author" content="Pavitra (celestialcognition@gmail.com)">
<meta name="Subject" content="Re: [sl4] I am a Singularitian who does not believe in the Singularity.">
<meta name="Date" content="2009-10-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] I am a Singularitian who does not believe in the Singularity.</h1>
<!-- received="Thu Oct  8 15:57:06 2009" -->
<!-- isoreceived="20091008215706" -->
<!-- sent="Thu, 08 Oct 2009 16:56:08 -0500" -->
<!-- isosent="20091008215608" -->
<!-- name="Pavitra" -->
<!-- email="celestialcognition@gmail.com" -->
<!-- subject="Re: [sl4] I am a Singularitian who does not believe in the Singularity." -->
<!-- id="4ACE5FF8.4030101@gmail.com" -->
<!-- charset="UTF-8" -->
<!-- inreplyto="1255014453.14167.1338951747@webmail.messagingengine.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Pavitra (<a href="mailto:celestialcognition@gmail.com?Subject=Re:%20[sl4]%20I%20am%20a%20Singularitian%20who%20does%20not%20believe%20in%20the%20Singularity."><em>celestialcognition@gmail.com</em></a>)<br>
<strong>Date:</strong> Thu Oct 08 2009 - 15:56:08 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20366.html">Mike Dougherty: "Re: [sl4] Starglider's Mini-FAQ on Artificial Intelligence"</a>
<li><strong>Previous message:</strong> <a href="20364.html">Vladimir Nesov: "Re: [sl4] I am a Singularitian who does not believe in the  Singularity."</a>
<li><strong>In reply to:</strong> <a href="20358.html">John K Clark: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20374.html">John K Clark: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<li><strong>Reply:</strong> <a href="20374.html">John K Clark: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20365">[ date ]</a>
<a href="index.html#20365">[ thread ]</a>
<a href="subject.html#20365">[ subject ]</a>
<a href="author.html#20365">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Warning: longish post.
<br>
<p>John K Clark wrote:
<br>
<em>&gt; On Wed, 07 Oct 2009 13:32:59 -0500, &quot;Pavitra&quot;
</em><br>
<em>&gt; &lt;<a href="mailto:celestialcognition@gmail.com?Subject=Re:%20[sl4]%20I%20am%20a%20Singularitian%20who%20does%20not%20believe%20in%20the%20Singularity.">celestialcognition@gmail.com</a>&gt; said:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; You're anthropomorphizing.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes, but you almost make that sound like a bad thing. At the moment
</em><br>
<em>&gt; human minds are the only minds we have to study
</em><br>
<p>That's not entirely true. We have animal, insect, and plant
<br>
intelligences; we have operating systems, search engines, applications,
<br>
and IRC bots; and we have AI projects of various pre-AGI degrees of
<br>
sophistication.
<br>
<p><em>&gt; so it's not unreasonable
</em><br>
<em>&gt; to suspect that future hypothetical minds will not be different from our
</em><br>
<em>&gt; own in EVERY conceivable way.
</em><br>
<p>There are enough attributes of minds that any given future mind will
<br>
probably resemble ours in at least one aspect, but there are enough
<br>
possible minds that any given future mind will almost certainly not
<br>
resemble ours in any given aspect.
<br>
<p><em>&gt; If you disagree and think an AI would be
</em><br>
<em>&gt; completely inscrutable then I don't understand how you can be so
</em><br>
<em>&gt; confident in being able to train it so that it will obey your every
</em><br>
<em>&gt; command like a trained puppy till the end of time.
</em><br>
<p>That's a flawed analogy. A puppy has an existing baseline desire system,
<br>
which the trainer exploits to mold the puppy's behavior to the trainer's
<br>
wishes. A computer programmer can _write_ the initial baseline so that
<br>
the AI _intrinsically_ wants what the trainer prefers it to want.
<br>
<p><em>&gt; Like any tool
</em><br>
<em>&gt; anthropomorphizing can be misused but it is not a 4 letter word.        
</em><br>
<p>It can be used well, but I believe that you are not doing so in this
<br>
particular case.
<br>
<p><p><em>&gt;&gt;&gt; People have developed a sense of absurdity and there is no reason a
</em><br>
<em>&gt;&gt;&gt; superior being wouldn't too. Mr. Jupiter Brain is bound to wonder
</em><br>
<em>&gt;&gt;&gt; why he is in the absurd position of valuing human slug well being
</em><br>
<em>&gt;&gt;&gt; above his own, and it wouldn't take him long to come to an answer,
</em><br>
<em>&gt;&gt;&gt; and a solution. A solution that we might not like much.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Unless there's a specific reason it *would*
</em><br>
<em>&gt;&gt; develop a sense of absurdity, the mere complexity of the hypothesis is a
</em><br>
<em>&gt;&gt; reason it wouldn't develop it simply by chance.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Any intelligent mind is going to be exposed to huge amounts of data, it
</em><br>
<em>&gt; will need to distinguish between what is important and what is not.
</em><br>
<em>&gt; Sometimes this is difficult, sometimes it's easy, sometimes it's
</em><br>
<em>&gt; absurdly easy.  
</em><br>
<p>There is a difference between factual or propositional absurdity (the
<br>
sky is green, humans like cardboard-flavored ice cream, inspecting this
<br>
particular mote of dust very closely is likely to yield lots of
<br>
information about the price of tea in China) versus moral or
<br>
prescriptive absurdity (I should save this slug's life, I should examine
<br>
this dust mote, I should construct orbital teapots).
<br>
<p>Any being capable of critiquing its desire to save slugs is necessarily
<br>
doing so in terms of some even more fundamental framework (e.g., a sense
<br>
of pride or dignity). If it is capable of critiquing its desire for
<br>
pride/dignity, then there must be yet another even higher framework.
<br>
<p>Eventually, the recursion bottoms out, and the being is found to have a
<br>
top-level framework that it is incapable of critiquing. It may be able
<br>
to describe scientifically why it came to have that particular
<br>
framework, but it will not be able to desire that that framework were
<br>
otherwise.
<br>
<p>This appears to me to be _tautologically_ true: I cannot conceive of any
<br>
well-defined mind specifiable in a finite amount of information for
<br>
which it is false.
<br>
<p><em>&gt;&gt; I would expect a given intelligence to have a
</em><br>
<em>&gt;&gt; sense of absurdity if and only if it was evolved/designed to detect
</em><br>
<em>&gt;&gt; attempts to deceive it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And of course the AI IS being lied to, told that human decisions are
</em><br>
<em>&gt; wiser than its own; and a AI that has the ability to detect this
</em><br>
<em>&gt; deception will develop much much faster than one who does not. 
</em><br>
<p>Perhaps, but that's not sufficient. What would cause a design that
<br>
detects lies to be selected over one that falls for them? Is the AI
<br>
being developed using a genetic programming framework?
<br>
<p>Or are you simply proposing that, of many AGI projects being developed
<br>
in various labs worldwide, one that detects lies will reach critical
<br>
mass first? If so, then why would lie-detection particularly be the key
<br>
deciding factor, rather than (say) implementation in
<br>
$YOUR_FAVORITE_PROGRAMMING_LANGUAGE or running on high-end hardware?
<br>
<p><p>Also, what exactly do you mean by &quot;wiser&quot;? It is not an empirical fact
<br>
that &quot;You should build teapots in space&quot; is an unwise decision while
<br>
&quot;You should provide each human with a harem of catpeople&quot; is a wise one.
<br>
Moral preference is defined relative to a particular mind. It is not an
<br>
ontologically intrinsic property common to all sufficiently intelligent
<br>
beings.
<br>
<p>Have you read
<br>
&lt;<a href="http://lesswrong.com/lw/rn/no_universally_compelling_arguments/">http://lesswrong.com/lw/rn/no_universally_compelling_arguments/</a>&gt; ?
<br>
<p><p>
<br><p>
<p><hr>
<ul>
<li>application/pgp-signature attachment: <a href="../att-20365/01-signature.asc">OpenPGP digital signature</a>
</ul>
<!-- attachment="01-signature.asc" -->
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20366.html">Mike Dougherty: "Re: [sl4] Starglider's Mini-FAQ on Artificial Intelligence"</a>
<li><strong>Previous message:</strong> <a href="20364.html">Vladimir Nesov: "Re: [sl4] I am a Singularitian who does not believe in the  Singularity."</a>
<li><strong>In reply to:</strong> <a href="20358.html">John K Clark: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20374.html">John K Clark: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<li><strong>Reply:</strong> <a href="20374.html">John K Clark: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20365">[ date ]</a>
<a href="index.html#20365">[ thread ]</a>
<a href="subject.html#20365">[ subject ]</a>
<a href="author.html#20365">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:04 MDT
</em></small></p>
</body>
</html>
