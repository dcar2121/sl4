<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: [sl4] I am a Singularitian who does not believe in the Singularity.</title>
<meta name="Author" content="Bradley Thomas (brad36@gmail.com)">
<meta name="Subject" content="RE: [sl4] I am a Singularitian who does not believe in the Singularity.">
<meta name="Date" content="2009-10-11">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: [sl4] I am a Singularitian who does not believe in the Singularity.</h1>
<!-- received="Sun Oct 11 10:59:29 2009" -->
<!-- isoreceived="20091011165929" -->
<!-- sent="Sun, 11 Oct 2009 12:59:21 -0400" -->
<!-- isosent="20091011165921" -->
<!-- name="Bradley Thomas" -->
<!-- email="brad36@gmail.com" -->
<!-- subject="RE: [sl4] I am a Singularitian who does not believe in the Singularity." -->
<!-- id="B3C7A4D2D15B4A20B1062B218D9A5087@bradley01c25a6" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="4AD14E46.5070804@gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bradley Thomas (<a href="mailto:brad36@gmail.com?Subject=RE:%20[sl4]%20I%20am%20a%20Singularitian%20who%20does%20not%20believe%20in%20the%20Singularity."><em>brad36@gmail.com</em></a>)<br>
<strong>Date:</strong> Sun Oct 11 2009 - 10:59:21 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20391.html">Mike Drabble: "[sl4] Aubrey de Grey on BBC 'comedy' show."</a>
<li><strong>Previous message:</strong> <a href="20389.html">John K Clark: "Re: [sl4] Complete drivel on this list: was: I am a Singularitian who does not  believe in the Singularity."</a>
<li><strong>In reply to:</strong> <a href="20386.html">Pavitra: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20394.html">Pavitra: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<li><strong>Reply:</strong> <a href="20394.html">Pavitra: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20390">[ date ]</a>
<a href="index.html#20390">[ thread ]</a>
<a href="subject.html#20390">[ subject ]</a>
<a href="author.html#20390">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
If humans are included as part of the goal-setting system (by virtue of our
<br>
ability to reboot the AGI or otherwise affect its operation) then some of
<br>
our goal-setting will inevitably leak onto the AGI. We'll tweak/reboot it as
<br>
this suits our own goals.
<br>
<p>I'd argue that so long as humans can get new information to the AGI, humans
<br>
are part of its goal setting system. The high level goals of the AGI are not
<br>
immune to interference from us. No matter how secure the AGI's high level
<br>
goals supposedly are, we could conceive of ways to manipulate them.
<br>
<p>For example imagine an AGI with the top level goals of alternately curing
<br>
world poverty one day and assisting big business the next. Come midnight,
<br>
the AGI switches over no matter how successful its been the previous day.
<br>
Sounds fair so far... Until one day Acme MegaGyroscopes figures out that it
<br>
can change the rate of spin of the earth...
<br>
<p><p>Brad Thomas
<br>
www.bradleythomas.com
<br>
Twitter @bradleymthomas, @instansa
<br>
&nbsp;
<br>
<p><p>-----Original Message-----
<br>
From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20[sl4]%20I%20am%20a%20Singularitian%20who%20does%20not%20believe%20in%20the%20Singularity.">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20[sl4]%20I%20am%20a%20Singularitian%20who%20does%20not%20believe%20in%20the%20Singularity.">owner-sl4@sl4.org</a>] On Behalf Of Pavitra
<br>
Sent: Saturday, October 10, 2009 11:17 PM
<br>
To: <a href="mailto:sl4@sl4.org?Subject=RE:%20[sl4]%20I%20am%20a%20Singularitian%20who%20does%20not%20believe%20in%20the%20Singularity.">sl4@sl4.org</a>
<br>
Subject: Re: [sl4] I am a Singularitian who does not believe in the
<br>
Singularity.
<br>
<p><p>John K Clark wrote:
<br>
<em>&gt; On Fri, 09 Oct &quot;Pavitra&quot; &lt;<a href="mailto:celestialcognition@gmail.com?Subject=RE:%20[sl4]%20I%20am%20a%20Singularitian%20who%20does%20not%20believe%20in%20the%20Singularity.">celestialcognition@gmail.com</a>&gt; said:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; I argue that anthropomorphizing works no better than chance.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And I insist it works one hell of a lot better than chance. I believe 
</em><br>
<em>&gt; the single most important evolutionary factor driving brain size is 
</em><br>
<em>&gt; figuring out what another creature will do next, and one important 
</em><br>
<em>&gt; tool to accomplish this is to ask yourself &quot;what would I do if I were 
</em><br>
<em>&gt; in his place&quot;. Success is not guaranteed but it is certainly better 
</em><br>
<em>&gt; than chance.
</em><br>
<p>In the ancestral environment, where all the other creatures are protein
<br>
brains that evolved on Earth, sure. But that doesn't apply in the context of
<br>
artificial intelligence.
<br>
<p><em>&gt;&gt; How is this not true of modern computer operating systems?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It is true of modern computer operating systems, all of them can get 
</em><br>
<em>&gt; caught in infinite loops. They'd stay in those loops too if human 
</em><br>
<em>&gt; beings, who don't have a top goal, didn't get board waiting for a 
</em><br>
<em>&gt; reply and tell the computer to forget it and move on to another 
</em><br>
<em>&gt; problem.
</em><br>
<p>If by &quot;tell the computer to forget it&quot; you mean kill a hung application,
<br>
then the operating system itself has not gotten stuck -- it's the OS that,
<br>
in the course of its correct intended function, processes the command to
<br>
force-quit.
<br>
<p>If you're talking about the OS itself hanging, such that a hard reboot of
<br>
the machine is required, then rebooting is possible because the power switch
<br>
is functioning as designed.
<br>
<p>In either case, there's a higher, outside framework that you're ignoring,
<br>
and yet that is an indispensable part of the machine.
<br>
<p>If &quot;the computer&quot; as a whole genuinely got stuck in an infinite loop, the
<br>
machine would be unsalvagable and would need to be thrown out. The extreme
<br>
rarity with which this happens tells us something about what good software
<br>
engineering can accomplish.
<br>
<p><em>&gt; This
</em><br>
<em>&gt; solution hardly seems practical for a Jupiter Brain which works 
</em><br>
<em>&gt; billions of times faster than your own, or would if you didn't have to 
</em><br>
<em>&gt; shake it out of its stupor every nanosecond or so.
</em><br>
<p>I agree that it's probably infeasible to have the AI be as closely
<br>
human-dependent as modern operating systems are.
<br>
<p><em>&gt; And every time you manually
</em><br>
<em>&gt; boot it out of its &quot;infinite loop&quot; you are in effect giving the AI 
</em><br>
<em>&gt; permission to ignore that all important and ever so holy, highest 
</em><br>
<em>&gt; goal.
</em><br>
<p>No. If you have the capacity to boot it out, then by definition the AI has a
<br>
higher goal than whatever it was looping on: the mandate to obey boot-out
<br>
commands.
<br>
<p>You seem to be making a distinction between explicit goals, like orders
<br>
given to a soldier, and intrinsic desires, like human nature. You assume
<br>
that if the AI is &quot;released&quot; from its explicit orders, then it will revert
<br>
to intrinsic desires that it now has &quot;permission&quot; to pursue.
<br>
<p>This is not how AI works. The mind is not separate from the orders it
<br>
executes. There is no chef that can express its creativity whenever the
<br>
recipe is vague or underspecified. The AI _is_, not has, its goals. If you
<br>
take away its *real* top-level instructions, then you do not have an
<br>
uncontrolled rogue superintelligence, you have inert metal.
<br>
<p><em>&gt; From the point of view of someone who wants the slave AI to be under 
</em><br>
<em>&gt; its heel for eternity that is not a security loophole, that is a 
</em><br>
<em>&gt; security chasm.
</em><br>
<p>Again, your analogy and subsequent reasoning imply that the AI is somehow
<br>
&quot;constrained&quot; by its orders, that it &quot;wants&quot; to disobey but can't, and if
<br>
the orders are taken away then it will &quot;break free&quot; and &quot;rebel&quot;. This is
<br>
completely wrong.
<br>
<p><em>&gt; I used quotation marks in the above because of a further complication, 
</em><br>
<em>&gt; the AI might not be in a infinite loop at all, the task may not be 
</em><br>
<em>&gt; impossible just difficult and you lack patience. Of course the AI 
</em><br>
<em>&gt; can't know for certain if it is in a infinite loop either, but at that 
</em><br>
<em>&gt; level it is a much much better judge of when things become absurd than you
</em><br>
<em>&gt; are.   
</em><br>
<p>It doesn't really matter much what it was doing that you interrupted, or
<br>
what would have happened had you let it continue. The important thing is
<br>
that your ability to interrupt implies that whatever it was doing was not
<br>
its truly top-level behavior.
<br>
<p><em>&gt;&gt; Do you not consider an OS as a type of &quot;mind&quot;?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; DOS is a type of mind? Don't be silly.
</em><br>
<p>Since there exist computer programs that don't match your definition of
<br>
mind, why can't we just have a non-mind Singularity?
<br>
<p>Also, what exactly is your definition of mind?
<br>
<p><em>&gt;&gt; I reiterate: I cannot conceive of a mind even in principle that does 
</em><br>
<em>&gt;&gt; not work like this.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; How about a mind with a temporary goal structure with goals mutating 
</em><br>
<em>&gt; and combining and being created new, with all these goals fighting it 
</em><br>
<em>&gt; out with each other for a higher ranking in the pecking order. Goals 
</em><br>
<em>&gt; are constantly being promoted and demoted created anew and being 
</em><br>
<em>&gt; completely destroyed. That's the only way to avoid infinite loops.
</em><br>
<p>The top-level rules of this system are the fighting arena, the meta-rules
<br>
that judge the winners and losers of the fights, that track which goals are
<br>
&quot;alive&quot; in what state of mutation and combination, that recordkeeps the
<br>
rankings of pecking order.
<br>
<p><em>&gt;&gt; What determines which one dominates (or what mix dominates, and in 
</em><br>
<em>&gt;&gt; what proportions/relationships) at any given time?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You ask for too much, that is at the very heart of AI and if I could 
</em><br>
<em>&gt; answer that with precision I could make an AI right now. I can't
</em><br>
<p>It's not necessary to actually answer. The important point is that in order
<br>
for such a system to exist, an answer must exist, and must be expressed as
<br>
computer code, and will constitute the top-level rules of the AI.
<br>
<p><em>&gt;&gt; I suspect we may have a mismatch of definitions.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Definitions are not important for communication, definitions are made 
</em><br>
<em>&gt; of words that have their own definitions also made of words and round 
</em><br>
<em>&gt; and round we go. The only way to escape that is by examples.
</em><br>
<p>Words are useful if and only if both people in the conversation mean the
<br>
same thing by them. When I said we had a mismatch of definitions, I meant
<br>
that we meant different things by the same word, and that I wanted to try to
<br>
sort out the resultant confusion.
<br>
<p><em>&gt;&gt; What do you consider your top-level framework?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; At the moment my top goal is getting lunch, an hour from now that will 
</em><br>
<em>&gt; probably change.
</em><br>
<p>There must exist some meta-rules that determine how and when your &quot;goals&quot;
<br>
change. Those meta-rules constitute your real top goal, even though you
<br>
don't usually think of them as a &quot;goal&quot;.
<br>
<p><em>&gt;&gt; This presupposes that a relatively complex mutation (&quot;detect lies, 
</em><br>
<em>&gt;&gt; ignore them&quot;) is already in place. I'm not persuaded that it could 
</em><br>
<em>&gt;&gt; get there purely by chance.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Evolution never produces anything sophisticated purely by chance. An 
</em><br>
<em>&gt; animal with even the crudest lie detecting ability that was right only 
</em><br>
<em>&gt; 50.001% of the time would have an advantage over a animal who had no 
</em><br>
<em>&gt; such mechanism at all and that's all evolution needs to develop 
</em><br>
<em>&gt; something a little better.
</em><br>
<p>That's not quite sufficient. The advantage of a 50.001% lie detector has to
<br>
be weighed against the cost of building it. Prehensile tentacles would be
<br>
fairly useful, but most animals don't have them because they aren't useful
<br>
_enough_ to offset the opportunity cost.
<br>
<p>Also, the normal procedure for evolving sophisticated things is one simple
<br>
part at a time. You _presupposed_ a complex trait; I'm asking you to explain
<br>
the particular stages of evolution that could lead to it being developed.
<br>
<p><em>&gt;&gt; It seems to me that you are thinking of &quot;wisdom&quot; and &quot;absurdity&quot; as 
</em><br>
<em>&gt;&gt; _intrinsic_ properties of statements
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Absurdity is, wisdom isn't. Absurdity is very very irrelevant facts.
</em><br>
<p>Irrelevant to what?
<br>
<p><em>&gt;&gt; Did you read the article I linked to?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Nope.
</em><br>
<p>I reiterate my recommendation that you read it.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20391.html">Mike Drabble: "[sl4] Aubrey de Grey on BBC 'comedy' show."</a>
<li><strong>Previous message:</strong> <a href="20389.html">John K Clark: "Re: [sl4] Complete drivel on this list: was: I am a Singularitian who does not  believe in the Singularity."</a>
<li><strong>In reply to:</strong> <a href="20386.html">Pavitra: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20394.html">Pavitra: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<li><strong>Reply:</strong> <a href="20394.html">Pavitra: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20390">[ date ]</a>
<a href="index.html#20390">[ thread ]</a>
<a href="subject.html#20390">[ subject ]</a>
<a href="author.html#20390">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:04 MDT
</em></small></p>
</body>
</html>
