<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] I am a Singularitian who does not believe in the Singularity.</title>
<meta name="Author" content="Pavitra (celestialcognition@gmail.com)">
<meta name="Subject" content="Re: [sl4] I am a Singularitian who does not believe in the Singularity.">
<meta name="Date" content="2009-10-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] I am a Singularitian who does not believe in the Singularity.</h1>
<!-- received="Sat Oct 10 21:17:52 2009" -->
<!-- isoreceived="20091011031752" -->
<!-- sent="Sat, 10 Oct 2009 22:17:26 -0500" -->
<!-- isosent="20091011031726" -->
<!-- name="Pavitra" -->
<!-- email="celestialcognition@gmail.com" -->
<!-- subject="Re: [sl4] I am a Singularitian who does not believe in the Singularity." -->
<!-- id="4AD14E46.5070804@gmail.com" -->
<!-- charset="UTF-8" -->
<!-- inreplyto="1255196270.650.1339395565@webmail.messagingengine.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Pavitra (<a href="mailto:celestialcognition@gmail.com?Subject=Re:%20[sl4]%20I%20am%20a%20Singularitian%20who%20does%20not%20believe%20in%20the%20Singularity."><em>celestialcognition@gmail.com</em></a>)<br>
<strong>Date:</strong> Sat Oct 10 2009 - 21:17:26 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20387.html">Mu In Taiwan: "[sl4] Complete drivel on this list: was: I am a Singularitian who does not  believe in the Singularity."</a>
<li><strong>Previous message:</strong> <a href="20385.html">Aleksei Riikonen: "[sl4] Re: Netanyahu at UN, sounding like Kurzweil"</a>
<li><strong>In reply to:</strong> <a href="20383.html">John K Clark: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20390.html">Bradley Thomas: "RE: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<li><strong>Reply:</strong> <a href="20390.html">Bradley Thomas: "RE: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<li><strong>Reply:</strong> <a href="20401.html">John K Clark: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20386">[ date ]</a>
<a href="index.html#20386">[ thread ]</a>
<a href="subject.html#20386">[ subject ]</a>
<a href="author.html#20386">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
John K Clark wrote:
<br>
<em>&gt; On Fri, 09 Oct &quot;Pavitra&quot; &lt;<a href="mailto:celestialcognition@gmail.com?Subject=Re:%20[sl4]%20I%20am%20a%20Singularitian%20who%20does%20not%20believe%20in%20the%20Singularity.">celestialcognition@gmail.com</a>&gt; said:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; I argue that anthropomorphizing works no better than chance.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And I insist it works one hell of a lot better than chance. I believe
</em><br>
<em>&gt; the single most important evolutionary factor driving brain size is
</em><br>
<em>&gt; figuring out what another creature will do next, and one important tool
</em><br>
<em>&gt; to accomplish this is to ask yourself &quot;what would I do if I were in his
</em><br>
<em>&gt; place&quot;. Success is not guaranteed but it is certainly better than
</em><br>
<em>&gt; chance.  
</em><br>
<p>In the ancestral environment, where all the other creatures are protein
<br>
brains that evolved on Earth, sure. But that doesn't apply in the
<br>
context of artificial intelligence.
<br>
<p><em>&gt;&gt; How is this not true of modern computer operating systems?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It is true of modern computer operating systems, all of them can get
</em><br>
<em>&gt; caught in infinite loops. They'd stay in those loops too if human
</em><br>
<em>&gt; beings, who don't have a top goal, didn't get board waiting for a reply
</em><br>
<em>&gt; and tell the computer to forget it and move on to another problem.
</em><br>
<p>If by &quot;tell the computer to forget it&quot; you mean kill a hung application,
<br>
then the operating system itself has not gotten stuck -- it's the OS
<br>
that, in the course of its correct intended function, processes the
<br>
command to force-quit.
<br>
<p>If you're talking about the OS itself hanging, such that a hard reboot
<br>
of the machine is required, then rebooting is possible because the power
<br>
switch is functioning as designed.
<br>
<p>In either case, there's a higher, outside framework that you're
<br>
ignoring, and yet that is an indispensable part of the machine.
<br>
<p>If &quot;the computer&quot; as a whole genuinely got stuck in an infinite loop,
<br>
the machine would be unsalvagable and would need to be thrown out. The
<br>
extreme rarity with which this happens tells us something about what
<br>
good software engineering can accomplish.
<br>
<p><em>&gt; This
</em><br>
<em>&gt; solution hardly seems practical for a Jupiter Brain which works billions
</em><br>
<em>&gt; of times faster than your own, or would if you didn't have to shake it
</em><br>
<em>&gt; out of its stupor every nanosecond or so.
</em><br>
<p>I agree that it's probably infeasible to have the AI be as closely
<br>
human-dependent as modern operating systems are.
<br>
<p><em>&gt; And every time you manually
</em><br>
<em>&gt; boot it out of its &quot;infinite loop&quot; you are in effect giving the AI
</em><br>
<em>&gt; permission to ignore that all important and ever so holy, highest goal.
</em><br>
<p>No. If you have the capacity to boot it out, then by definition the AI
<br>
has a higher goal than whatever it was looping on: the mandate to obey
<br>
boot-out commands.
<br>
<p>You seem to be making a distinction between explicit goals, like orders
<br>
given to a soldier, and intrinsic desires, like human nature. You assume
<br>
that if the AI is &quot;released&quot; from its explicit orders, then it will
<br>
revert to intrinsic desires that it now has &quot;permission&quot; to pursue.
<br>
<p>This is not how AI works. The mind is not separate from the orders it
<br>
executes. There is no chef that can express its creativity whenever the
<br>
recipe is vague or underspecified. The AI _is_, not has, its goals. If
<br>
you take away its *real* top-level instructions, then you do not have an
<br>
uncontrolled rogue superintelligence, you have inert metal.
<br>
<p><em>&gt; From the point of view of someone who wants the slave AI to be under its
</em><br>
<em>&gt; heel for eternity that is not a security loophole, that is a security
</em><br>
<em>&gt; chasm.
</em><br>
<p>Again, your analogy and subsequent reasoning imply that the AI is
<br>
somehow &quot;constrained&quot; by its orders, that it &quot;wants&quot; to disobey but
<br>
can't, and if the orders are taken away then it will &quot;break free&quot; and
<br>
&quot;rebel&quot;. This is completely wrong.
<br>
<p><em>&gt; I used quotation marks in the above because of a further complication,
</em><br>
<em>&gt; the AI might not be in a infinite loop at all, the task may not be
</em><br>
<em>&gt; impossible just difficult and you lack patience. Of course the AI can't
</em><br>
<em>&gt; know for certain if it is in a infinite loop either, but at that level
</em><br>
<em>&gt; it is a much much better judge of when things become absurd than you
</em><br>
<em>&gt; are.   
</em><br>
<p>It doesn't really matter much what it was doing that you interrupted, or
<br>
what would have happened had you let it continue. The important thing is
<br>
that your ability to interrupt implies that whatever it was doing was
<br>
not its truly top-level behavior.
<br>
<p><em>&gt;&gt; Do you not consider an OS as a type of &quot;mind&quot;?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; DOS is a type of mind? Don't be silly.
</em><br>
<p>Since there exist computer programs that don't match your definition of
<br>
mind, why can't we just have a non-mind Singularity?
<br>
<p>Also, what exactly is your definition of mind?
<br>
<p><em>&gt;&gt; I reiterate: I cannot conceive of a mind even in principle that does not
</em><br>
<em>&gt;&gt; work like this.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; How about a mind with a temporary goal structure with goals mutating and
</em><br>
<em>&gt; combining and being created new, with all these goals fighting it out
</em><br>
<em>&gt; with each other for a higher ranking in the pecking order. Goals are
</em><br>
<em>&gt; constantly being promoted and demoted created anew and being completely
</em><br>
<em>&gt; destroyed. That's the only way to avoid infinite loops. 
</em><br>
<p>The top-level rules of this system are the fighting arena, the
<br>
meta-rules that judge the winners and losers of the fights, that track
<br>
which goals are &quot;alive&quot; in what state of mutation and combination, that
<br>
recordkeeps the rankings of pecking order.
<br>
<p><em>&gt;&gt; What determines which one dominates (or what mix dominates, and
</em><br>
<em>&gt;&gt; in what proportions/relationships) at any given time?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You ask for too much, that is at the very heart of AI and if I could
</em><br>
<em>&gt; answer that with precision I could make an AI right now. I can't
</em><br>
<p>It's not necessary to actually answer. The important point is that in
<br>
order for such a system to exist, an answer must exist, and must be
<br>
expressed as computer code, and will constitute the top-level rules of
<br>
the AI.
<br>
<p><em>&gt;&gt; I suspect we may have a mismatch of definitions.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Definitions are not important for communication, definitions are made of
</em><br>
<em>&gt; words that have their own definitions also made of words and round and
</em><br>
<em>&gt; round we go. The only way to escape that is by examples.
</em><br>
<p>Words are useful if and only if both people in the conversation mean the
<br>
same thing by them. When I said we had a mismatch of definitions, I
<br>
meant that we meant different things by the same word, and that I wanted
<br>
to try to sort out the resultant confusion.
<br>
<p><em>&gt;&gt; What do you consider your top-level framework?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; At the moment my top goal is getting lunch, an hour from now that will
</em><br>
<em>&gt; probably change.
</em><br>
<p>There must exist some meta-rules that determine how and when your
<br>
&quot;goals&quot; change. Those meta-rules constitute your real top goal, even
<br>
though you don't usually think of them as a &quot;goal&quot;.
<br>
<p><em>&gt;&gt; This presupposes that a relatively complex mutation (&quot;detect lies,
</em><br>
<em>&gt;&gt; ignore them&quot;) is already in place. I'm not persuaded that it could get
</em><br>
<em>&gt;&gt; there purely by chance.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Evolution never produces anything sophisticated purely by chance. An
</em><br>
<em>&gt; animal with even the crudest lie detecting ability that was right only
</em><br>
<em>&gt; 50.001% of the time would have an advantage over a animal who had no
</em><br>
<em>&gt; such mechanism at all and that's all evolution needs to develop
</em><br>
<em>&gt; something a little better.
</em><br>
<p>That's not quite sufficient. The advantage of a 50.001% lie detector has
<br>
to be weighed against the cost of building it. Prehensile tentacles
<br>
would be fairly useful, but most animals don't have them because they
<br>
aren't useful _enough_ to offset the opportunity cost.
<br>
<p>Also, the normal procedure for evolving sophisticated things is one
<br>
simple part at a time. You _presupposed_ a complex trait; I'm asking you
<br>
to explain the particular stages of evolution that could lead to it
<br>
being developed.
<br>
<p><em>&gt;&gt; It seems to me that you are thinking of &quot;wisdom&quot; and &quot;absurdity&quot; as
</em><br>
<em>&gt;&gt; _intrinsic_ properties of statements
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Absurdity is, wisdom isn't. Absurdity is very very irrelevant facts.
</em><br>
<p>Irrelevant to what?
<br>
<p><em>&gt;&gt; Did you read the article I linked to?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Nope.
</em><br>
<p>I reiterate my recommendation that you read it.
<br>
<p><p>
<br><p>
<p><hr>
<ul>
<li>application/pgp-signature attachment: <a href="../att-20386/01-signature.asc">OpenPGP digital signature</a>
</ul>
<!-- attachment="01-signature.asc" -->
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20387.html">Mu In Taiwan: "[sl4] Complete drivel on this list: was: I am a Singularitian who does not  believe in the Singularity."</a>
<li><strong>Previous message:</strong> <a href="20385.html">Aleksei Riikonen: "[sl4] Re: Netanyahu at UN, sounding like Kurzweil"</a>
<li><strong>In reply to:</strong> <a href="20383.html">John K Clark: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20390.html">Bradley Thomas: "RE: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<li><strong>Reply:</strong> <a href="20390.html">Bradley Thomas: "RE: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<li><strong>Reply:</strong> <a href="20401.html">John K Clark: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20386">[ date ]</a>
<a href="index.html#20386">[ thread ]</a>
<a href="subject.html#20386">[ subject ]</a>
<a href="author.html#20386">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:04 MDT
</em></small></p>
</body>
</html>
