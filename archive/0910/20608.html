<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)</title>
<meta name="Author" content="Tim Freeman (tim@fungible.com)">
<meta name="Subject" content="Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)">
<meta name="Date" content="2009-10-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)</h1>
<!-- received="Sat Oct 24 19:04:33 2009" -->
<!-- isoreceived="20091025010433" -->
<!-- sent="Sat, 24 Oct 2009 16:54:37 -0700" -->
<!-- isosent="20091024235437" -->
<!-- name="Tim Freeman" -->
<!-- email="tim@fungible.com" -->
<!-- subject="Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)" -->
<!-- id="20091025010431.E532FD293C@fungible.com" -->
<!-- inreplyto="20091024203148.GF18098@digitalkingdom.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tim Freeman (<a href="mailto:tim@fungible.com?Subject=Re:%20Why%20extrapolate?%20(was%20Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI)"><em>tim@fungible.com</em></a>)<br>
<strong>Date:</strong> Sat Oct 24 2009 - 17:54:37 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20609.html">Robin Lee Powell: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>Previous message:</strong> <a href="20607.html">Robin Lee Powell: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>In reply to:</strong> <a href="20607.html">Robin Lee Powell: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20609.html">Robin Lee Powell: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>Reply:</strong> <a href="20609.html">Robin Lee Powell: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20608">[ date ]</a>
<a href="index.html#20608">[ thread ]</a>
<a href="subject.html#20608">[ subject ]</a>
<a href="author.html#20608">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
From: Robin Lee Powell &lt;<a href="mailto:rlpowell@digitalkingdom.org?Subject=Re:%20Why%20extrapolate?%20(was%20Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI)">rlpowell@digitalkingdom.org</a>&gt;
<br>
<em>&gt;I would hope that the extrapolation would include extrapolating the
</em><br>
<em>&gt;actions of the AI; like saying, &quot;Hey, there's a bug that's going to
</em><br>
<em>&gt;make you suicidal in a few years; you want I shoud fix that?&quot;.
</em><br>
<p>I would hope that too, but what you and I hope for isn't relevant.
<br>
The question we're discussing is, would CEV do what we want?
<br>
<p>Arguments of the form &quot;We want X so CEV must do it&quot; aren't part of
<br>
answering that question.  That argument presupposes that CEV would do
<br>
what we want, which is the question we started with.
<br>
<p>I don't know what you mean by &quot;include extrapolating the actions of
<br>
the AI&quot; in extrapolating human volition.  Specifically, I have no
<br>
meaning for &quot;include&quot; that makes sense in this context -- what does it
<br>
mean to include one extrapolation in another?  Furthermore, I see no
<br>
similarity between extrapolating (the consequences of?) actions and
<br>
extrapolating volition, so I get confused when you use the word
<br>
&quot;extrapolating&quot; for both.
<br>
<p><em>&gt;Wait, what?  That's a total failure to enact the result; that's
</em><br>
<em>&gt;sub-goal stomp of the worst kind.  Doing that guarantees that Mpebna
</em><br>
<em>&gt;will never get to the Nice Place To Live that CEV envisioned, so
</em><br>
<em>&gt;it's a stupid action, contrary to the point of CEV.
</em><br>
<p>I see you aren't quoting the definition of CEV when you're arguing
<br>
about what it means.  Is the proposed scenario consistent with the
<br>
definition, or not?
<br>
<p>Maybe the answer is &quot;we don't know&quot;, which points at another basic
<br>
problem with CEV.  It's expressed in English and we can have pointless
<br>
arguments forever about what it means.
<br>
<p><em>&gt;The point of extrapolating is to stop CEV from doing things to make
</em><br>
<em>&gt;people happy now that would prevent them from getting the best
</em><br>
<em>&gt;outcome in the future.
</em><br>
<p>Well, if that makes sense then that's what you want.  I've heard other
<br>
people claim to want the same thing.  I suppose an AI that's giving
<br>
people a weighted average of what they want now would do that if
<br>
people like you got a high enough weight in the average.  Be aware
<br>
that other people really do value the present more than the future,
<br>
and you'll be in conflict with them.
<br>
<p>I don't understand how to define an ethical system that doesn't
<br>
sometimes trade the future for the present.  If I prefer outcome X
<br>
over outcome Y, then there has to be some observation made at a
<br>
specific time that lets me know if X or Y happened, and the future
<br>
after that point doesn't matter.  I can care about the infinite future
<br>
if I have weighted collection of preferences for events at different
<br>
times, but the sums have to converge otherwise it makes no sense, and
<br>
if you don't trade the future for the present at some point the sums
<br>
don't converge.  Peter de Blanc gave a talk about this at one of those
<br>
SIAI intern's dinners -- it looks like his paper, which I have not
<br>
understood in detail, is here:
<br>
<p>&nbsp;&nbsp;&nbsp;<a href="http://adsabs.harvard.edu/abs/2009arXiv0907.5598D">http://adsabs.harvard.edu/abs/2009arXiv0907.5598D</a>
<br>
<p>I'm not sure if you meant &quot;the best&quot; or &quot;a good enough&quot; there.  If you
<br>
really meant &quot;the best&quot;, we disagree.  One problem with &quot;the best&quot; is
<br>
that we don't have a definition for it.  Another problem is that the
<br>
best is the enemy of good enough.  I'd rather get a good-enough
<br>
outcome with a well-defined and understandable procedure than try to
<br>
get &quot;the best&quot; outcome with something that seems vague and
<br>
unpredictable.  If you want to solve it, you must regard it as an
<br>
engineering problem, not a math or science problem.
<br>
<p><em>&gt;If it turns out, for example, that future humans will have a shared
</em><br>
<em>&gt;morality of tolerance and mercy; if CEV simply did whatever current
</em><br>
<em>&gt;humans want, then it might (for example) find and utterly destroy all
</em><br>
<em>&gt;the remaining Nazi officers hiding out in various places, an action
</em><br>
<em>&gt;that future humans would predictably abhor, and that cannot be
</em><br>
<em>&gt;un-done.  A crappy example, admittedly, but the point is just that
</em><br>
<em>&gt;without extrapolation we can't avoid permanent affects we might regret
</em><br>
<em>&gt;later.
</em><br>
<p>I agree that my procedure is prone to this sort of bug, where if
<br>
enough people want person X dead now, the AI will go murder X for
<br>
them.  I'd like to see a solution to that, but I can't make enough
<br>
sense of Extrpolation in CEV to use it as a solution.
<br>
<pre>
-- 
Tim Freeman               <a href="http://www.fungible.com">http://www.fungible.com</a>           <a href="mailto:tim@fungible.com?Subject=Re:%20Why%20extrapolate?%20(was%20Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI)">tim@fungible.com</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20609.html">Robin Lee Powell: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>Previous message:</strong> <a href="20607.html">Robin Lee Powell: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>In reply to:</strong> <a href="20607.html">Robin Lee Powell: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20609.html">Robin Lee Powell: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>Reply:</strong> <a href="20609.html">Robin Lee Powell: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20608">[ date ]</a>
<a href="index.html#20608">[ thread ]</a>
<a href="subject.html#20608">[ subject ]</a>
<a href="author.html#20608">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
