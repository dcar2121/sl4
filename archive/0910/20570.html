<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] to-do list for strong, nice AI</title>
<meta name="Author" content="Pavitra (celestialcognition@gmail.com)">
<meta name="Subject" content="Re: [sl4] to-do list for strong, nice AI">
<meta name="Date" content="2009-10-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] to-do list for strong, nice AI</h1>
<!-- received="Fri Oct 16 23:54:43 2009" -->
<!-- isoreceived="20091017055443" -->
<!-- sent="Sat, 17 Oct 2009 00:54:19 -0500" -->
<!-- isosent="20091017055419" -->
<!-- name="Pavitra" -->
<!-- email="celestialcognition@gmail.com" -->
<!-- subject="Re: [sl4] to-do list for strong, nice AI" -->
<!-- id="4AD95C0B.5080907@gmail.com" -->
<!-- charset="UTF-8" -->
<!-- inreplyto="151179.12197.qm@web51911.mail.re2.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Pavitra (<a href="mailto:celestialcognition@gmail.com?Subject=Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI"><em>celestialcognition@gmail.com</em></a>)<br>
<strong>Date:</strong> Fri Oct 16 2009 - 23:54:19 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20571.html">J. Andrew Rogers: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Previous message:</strong> <a href="20569.html">Matt Mahoney: "Re: [sl4] prediction markets"</a>
<li><strong>In reply to:</strong> <a href="20568.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20571.html">J. Andrew Rogers: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Reply:</strong> <a href="20571.html">J. Andrew Rogers: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Reply:</strong> <a href="20573.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20570">[ date ]</a>
<a href="index.html#20570">[ thread ]</a>
<a href="subject.html#20570">[ subject ]</a>
<a href="author.html#20570">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Matt Mahoney wrote:
<br>
<em>&gt; Pavitra wrote:
</em><br>
<em>&gt;&gt; A[ ] Develop a mathematically formal definition of Friendliness.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In order for AI to do what you want (as opposed to what you tell it),
</em><br>
<em>&gt; it has to at least know what you know, and use that knowledge at
</em><br>
<em>&gt; least as fast as your brain does.
</em><br>
<p>Doesn't this imply that the relevant data and algorithms are
<br>
incompressible? In particular, it's possible that _lossy_ compression
<br>
may be acceptable, provided edge cases are properly handled; I can
<br>
predict the trajectory of a cannonball to an acceptable precision
<br>
without knowing the position or trajectory of any of its individual atoms.
<br>
<p><em>&gt; To satisfy conflicts between people
</em><br>
<em>&gt; (e.g. I want your money), AI has to know what everyone knows. Then it
</em><br>
<em>&gt; could calculate what an ideal secrecy-free market would do and
</em><br>
<em>&gt; allocate resources accordingly.
</em><br>
<p>Assuming an ideal secrecy-free market generates the best possible
<br>
allocation of resources. Unless there's a relevant theorem of ethics I'm
<br>
not aware of, that seems a nontrivial assumption.
<br>
<p><em>&gt; One human knows 10^9 bits (Landauer's estimate of human long term
</em><br>
<em>&gt; memory). 10^10 humans know 10^17 to 10^18 bits, allowing for some
</em><br>
<em>&gt; overlapping knowledge.
</em><br>
<p>Again, where are you obtaining your estimates of degree-of-compressibility?
<br>
<p><em>&gt;&gt; A-&gt;B[ ] Develop an automated test for Friendliness with a 0% false 
</em><br>
<em>&gt;&gt; positive rate and a reasonably low false negative rate.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Unlikely. Using an iterative approach, each time that a human gives
</em><br>
<em>&gt; feedback to the AI (good or bad), one bit of information is added to
</em><br>
<em>&gt; the model. Development will be slow.
</em><br>
<p>This sounds relevant to step A, but not to B-assuming-A-is-solved.
<br>
Solving A doesn't necessarily rely on iterating over a binary predicate;
<br>
I agree with you that it probably shouldn't.
<br>
<p><em>&gt;&gt; C[ ] Develop a mathematically formal definition of intelligence.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Legg and Hutter propose to define universal intelligence as the
</em><br>
<em>&gt; expected reward given a universal (Solomonoff) distribution of
</em><br>
<em>&gt; environments. <a href="http://www.hutter1.net/ai/sior.pdf">http://www.hutter1.net/ai/sior.pdf</a> However it is not
</em><br>
<em>&gt; computable because the number of environments is infinite. Other
</em><br>
<em>&gt; definitions are possible of course, e.g. the Turing test.
</em><br>
<p>The Turing test is probably not suitable.
<br>
<p>What about computing an approximation? Is it possible to determine that
<br>
a given precision of approximation is &quot;good enough&quot; for a given
<br>
situation, or would that have to be part of the AI itself and am I
<br>
mixing levels?
<br>
<p><em>&gt;&gt; C-&gt;D[ ] Develop an automated comparison test that returns the more 
</em><br>
<em>&gt;&gt; intelligent of two given systems.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; How? The test giver has to know more than the test taker.
</em><br>
<p>Again, this seems more a criticism of C than of D. However, reading it
<br>
as such:
<br>
<p>I am not a mathematician, but it feels like the validity of your
<br>
argument may be equivalent (or at least analogous) to P=NP.
<br>
<p><em>&gt; However, you don't need C and D. If you solve B then you already have
</em><br>
<em>&gt; a model of all human minds, and therefore have already solved
</em><br>
<em>&gt; intelligence, at least by the Turing test.
</em><br>
<p>The whole point of Singularity-level AGI is that it's a nonhuman
<br>
intelligence. By hypothesis, &quot;humanity&quot; âŠ‰ &quot;intelligence&quot;.
<br>
<p>I don't hold with the Turing test. It's too fuzzy, subjective, and
<br>
fallible, and above all it tests for humanity rather than intelligence.
<br>
Chatterbots have been found to improve their Turing Test performance
<br>
significantly by committing deliberate errors of spelling and avoiding
<br>
topics that require intelligent or coherent discourse.
<br>
<p><em>&gt;&gt; B,D-&gt;E[ ] Develop prototype systems and apply these tests to them 
</em><br>
<em>&gt;&gt; iteratively until the Singularity occurs.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Let's keep in mind that a Singularity is *not* the goal. The goal is
</em><br>
<em>&gt; friendly AI. The Singularity is what happens when we lose control of
</em><br>
<em>&gt; it.
</em><br>
<p>I thought the Singularity was defined as &quot;AGI occurs and goes foom&quot;, so
<br>
that the Singularity would be either Friendly or unFriendly according to
<br>
the nature of the particular AI that first reaches the magic threshhold.
<br>
<p>The goal, then, would be to ensure that the Singularity will be Friendly.
<br>
<p>If Singularity is defined as &quot;unFriendly foom&quot;, then of course E should
<br>
instead read &quot;...until we get Friendly AGI or die trying.&quot;
<br>
<p><p>
<br><p>
<p><hr>
<ul>
<li>application/pgp-signature attachment: <a href="../att-20570/01-signature.asc">OpenPGP digital signature</a>
</ul>
<!-- attachment="01-signature.asc" -->
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20571.html">J. Andrew Rogers: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Previous message:</strong> <a href="20569.html">Matt Mahoney: "Re: [sl4] prediction markets"</a>
<li><strong>In reply to:</strong> <a href="20568.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20571.html">J. Andrew Rogers: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Reply:</strong> <a href="20571.html">J. Andrew Rogers: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Reply:</strong> <a href="20573.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20570">[ date ]</a>
<a href="index.html#20570">[ thread ]</a>
<a href="subject.html#20570">[ subject ]</a>
<a href="author.html#20570">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
