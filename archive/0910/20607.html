<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)</title>
<meta name="Author" content="Robin Lee Powell (rlpowell@digitalkingdom.org)">
<meta name="Subject" content="Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)">
<meta name="Date" content="2009-10-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)</h1>
<!-- received="Sat Oct 24 14:31:51 2009" -->
<!-- isoreceived="20091024203151" -->
<!-- sent="Sat, 24 Oct 2009 13:31:48 -0700" -->
<!-- isosent="20091024203148" -->
<!-- name="Robin Lee Powell" -->
<!-- email="rlpowell@digitalkingdom.org" -->
<!-- subject="Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)" -->
<!-- id="20091024203148.GF18098@digitalkingdom.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20091024201700.0CD8DD293C@fungible.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Robin Lee Powell (<a href="mailto:rlpowell@digitalkingdom.org?Subject=Re:%20Why%20extrapolate?%20(was%20Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI)"><em>rlpowell@digitalkingdom.org</em></a>)<br>
<strong>Date:</strong> Sat Oct 24 2009 - 14:31:48 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20608.html">Tim Freeman: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>Previous message:</strong> <a href="20606.html">Tim Freeman: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>In reply to:</strong> <a href="20606.html">Tim Freeman: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20608.html">Tim Freeman: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>Reply:</strong> <a href="20608.html">Tim Freeman: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20607">[ date ]</a>
<a href="index.html#20607">[ thread ]</a>
<a href="subject.html#20607">[ subject ]</a>
<a href="author.html#20607">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Sat, Oct 24, 2009 at 12:35:29PM -0700, Tim Freeman wrote:
<br>
<em>&gt; Here's an example that bothers me: Nobody has lived 1000 years.
</em><br>
<em>&gt; Maybe the human mind has bugs in that untested scenario.  One
</em><br>
<em>&gt; possible bug is that all 1000 year olds are suicidal.  I'm
</em><br>
<em>&gt; concerned that the Extrapolation step would figure out what people
</em><br>
<em>&gt; would want if they &quot;had grown up farther together&quot;, where
</em><br>
<em>&gt; &quot;farther&quot; means 1000 years, and then correctly infer that everyone
</em><br>
<em>&gt; would want to die in that situation.  The AI gives them what they
</em><br>
<em>&gt; would want if they had grown up farther together by killing them
</em><br>
<em>&gt; all now.  I'd prefer that it let the situation evolve naturally --
</em><br>
<em>&gt; that way maybe people would kill themselves at age 900 but they'd
</em><br>
<em>&gt; still get a decent life for a while.
</em><br>
<p>I would hope that the extrapolation would include extrapolating the
<br>
actions of the AI; like saying, &quot;Hey, there's a bug that's going to
<br>
make you suicidal in a few years; you want I shoud fix that?&quot;.
<br>
<p><em>&gt; Here's another example that bothers me: Mpebna in Somalia is
</em><br>
<em>&gt; starving. If Mpebna weren't starving, and had grown up in a more
</em><br>
<em>&gt; gentle environment, he would like, among other things, to have a
</em><br>
<em>&gt; virtual reality system that allowed him to communicate visually
</em><br>
<em>&gt; with his friends without the trouble of travelling to visit them.
</em><br>
<em>&gt; The FAI comes into power, and poof! Mpebna is presented with a
</em><br>
<em>&gt; fancy VR system.  Mpebna doesn't know WTF it is, Mpebna is still
</em><br>
<em>&gt; starving, and now Mpebna hates the people who deployed the FAI,
</em><br>
<em>&gt; since they could have fed him and they chose not to.  How exactly
</em><br>
<em>&gt; did the people who deployed the FAI benefit from getting into
</em><br>
<em>&gt; conflict with Mpebna here? The alternative was to give him food
</em><br>
<em>&gt; and wait patiently for him to want something else.
</em><br>
<p>Wait, what?  That's a total failure to enact the result; that's
<br>
sub-goal stomp of the worst kind.  Doing that guarantees that Mpebna
<br>
will never get to the Nice Place To Live that CEV envisioned, so
<br>
it's a stupid action, contrary to the point of CEV.
<br>
<p>The point isn't to extrapolate what people would want and then just
<br>
give the end result to them; the point is to extrapolate what people
<br>
would want and the create a world that leads naturally to, or at
<br>
least easily allows, those things to occur.  That is: to find out
<br>
what humanity might really want some day as A Nice Place To Live,
<br>
and then make sure that that's possible.  Not even to force it,
<br>
necessarily; just make sure it's possible.
<br>
<p>IOW, the end result of CEV isn't &quot;poof, here's a VR system; you'd
<br>
eventually want that if you could&quot;, it's &quot;here's a box that produces
<br>
infinite, and some educational material; I'll come back in a year
<br>
and we'll talk about computers&quot;, because that's how you get Mpebna
<br>
started down the road towards being in a Nice Place To Live.
<br>
<p>Very loosely; I'm not a super-intelligent FAI, and I haven't
<br>
seriously analyzed that particular example even with my weak-ass
<br>
brain; do not pick holes in that particular example please, it's not
<br>
the point.  Maybe CEV leads to Mpebna suddenly understanding vast
<br>
amounts of information in a microsecond, and finding himself
<br>
immortal and massively wise; I dunno.  But certainly &quot;here's a VR
<br>
suit, sorry your starving, bye&quot; would mean CEV was an abject
<br>
failure.
<br>
<p>The point of extrapolating is to stop CEV from doing things to make
<br>
people happy now that would prevent them from getting the best
<br>
outcome in the future.  If it turns out, for example, that future
<br>
humans will have a shared morality of tolerance and mercy; if CEV
<br>
simply did whatever current humans want, then it might (for example)
<br>
find and utterly destroy all the remaining Nazi officers hiding out
<br>
in various places, an action that future humans would predictably
<br>
abhor, and that cannot be un-done.  A crappy example, admittedly,
<br>
but the point is just that without extrapolation we can't avoid
<br>
permanent affects we might regret later.
<br>
<p>-Robin
<br>
<p><pre>
-- 
They say:  &quot;The first AIs will be built by the military as weapons.&quot;
And I'm  thinking:  &quot;Does it even occur to you to try for something
other  than  the default  outcome?&quot;  See <a href="http://shrunklink.com/cdiz">http://shrunklink.com/cdiz</a>
<a href="http://www.digitalkingdom.org/~rlpowell/">http://www.digitalkingdom.org/~rlpowell/</a> *** <a href="http://www.lojban.org/">http://www.lojban.org/</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20608.html">Tim Freeman: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>Previous message:</strong> <a href="20606.html">Tim Freeman: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>In reply to:</strong> <a href="20606.html">Tim Freeman: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20608.html">Tim Freeman: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>Reply:</strong> <a href="20608.html">Tim Freeman: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20607">[ date ]</a>
<a href="index.html#20607">[ thread ]</a>
<a href="subject.html#20607">[ subject ]</a>
<a href="author.html#20607">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
