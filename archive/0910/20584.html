<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] to-do list for strong, nice AI</title>
<meta name="Author" content="Matt Mahoney (matmahoney@yahoo.com)">
<meta name="Subject" content="Re: [sl4] to-do list for strong, nice AI">
<meta name="Date" content="2009-10-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] to-do list for strong, nice AI</h1>
<!-- received="Tue Oct 20 18:48:03 2009" -->
<!-- isoreceived="20091021004803" -->
<!-- sent="Tue, 20 Oct 2009 17:47:58 -0700 (PDT)" -->
<!-- isosent="20091021004758" -->
<!-- name="Matt Mahoney" -->
<!-- email="matmahoney@yahoo.com" -->
<!-- subject="Re: [sl4] to-do list for strong, nice AI" -->
<!-- id="373999.10408.qm@web51903.mail.re2.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="12902e900910201116m7563b037x63c38c3433c2f82c@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Matt Mahoney (<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI"><em>matmahoney@yahoo.com</em></a>)<br>
<strong>Date:</strong> Tue Oct 20 2009 - 18:47:58 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20585.html">Pavitra: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Previous message:</strong> <a href="20583.html">Mu In Taiwan: "[sl4] This post contains basilisks. View at your own risk."</a>
<li><strong>In reply to:</strong> <a href="20579.html">Luke: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20585.html">Pavitra: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Reply:</strong> <a href="20585.html">Pavitra: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Reply:</strong> <a href="20604.html">Tim Freeman: "Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20584">[ date ]</a>
<a href="index.html#20584">[ thread ]</a>
<a href="subject.html#20584">[ subject ]</a>
<a href="author.html#20584">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Luke wrote:
<br>
<em>&gt; (1) With regard to a definition of friendly AI, and how it needs to encompass all those bits, I've got a big problem there.  (a) That's impossible.  So we either need to find a way around that intractable problem (i.e. a smaller definition, within the 10^5 bits range or something, 10^2 would be great but that's obviously wishful thinking), or we need to accept that we're not going to be able to proceed, and start &quot;saying our prayers&quot; or seeking &quot;enlightenment&quot; or stocking up on heroin or whatever else we need to do to face death.  This is a completely serious point:  if we decide we cannot hope to produce this friendly AI, it's better to accept that as quickly as possible and decide what we want to do with this short stay between the birth canal and the grave.  
</em><br>
<p>For another proposed definition of Friendliness, see <a href="http://intelligence.org/upload/CEV.html">http://intelligence.org/upload/CEV.html</a>
<br>
<p>This is a much more detailed definition, but it suffers from the same flaw as my simple description. It does not define &quot;human&quot;. It's an important question in edge cases such as embryos, animals, and people who are terminally ill and in a coma. It becomes even more important when you consider future human-software hybrids and simulations. There are no &quot;correct&quot; answers to these questions, just opinions based on cultural beliefs. But a complete definition of Friendliness *must* answer these questions.
<br>
<p><em>&gt; (2) You said that a test-giver has to be more intelligent than a test-taker.  I don't think that's necessarily the case.  For instance, what if the test consisted of:  &quot;We're dealing with RSA.  Here's an encrypted message, and here's the public key that encrypted it.  What is the private key?&quot;
</em><br>
<p>Suppose the AI says &quot;I can't tell you for 10^100 years, because there is no faster algorithm for solving RSA than those already known&quot;. Is this the correct answer? You don't know.
<br>
<p>In fact there are *no* classes of problems that are known to be hard to solve but easy to check. That's true even if it turns out that P != NP, because you still don't know *which* NP-complete problems are hard, just that some of them are.
<br>
&nbsp;-- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI">matmahoney@yahoo.com</a>
<br>
<p><p><p><p>________________________________
<br>
From: Luke &lt;<a href="mailto:wlgriffiths@gmail.com?Subject=Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI">wlgriffiths@gmail.com</a>&gt;
<br>
To: <a href="mailto:sl4@sl4.org?Subject=Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI">sl4@sl4.org</a>
<br>
Sent: Tue, October 20, 2009 2:16:49 PM
<br>
Subject: Re: [sl4] to-do list for strong, nice AI
<br>
<p>@Pavitra:  thanks for reminding me of that.  It's true - there's a lot of talking that needs to get done before we can throw out the haiku of FGAI design.  I accept this fate, this large-scale discussion, though I can't promise I'll read everything before I respond.  Not enough time for that.
<br>
<p>@Matt Mahoney:  Two points, as follows:
<br>
<p>(1) With regard to a definition of friendly AI, and how it needs to encompass all those bits, I've got a big problem there.  (a) That's impossible.  So we either need to find a way around that intractable problem (i.e. a smaller definition, within the 10^5 bits range or something, 10^2 would be great but that's obviously wishful thinking), or we need to accept that we're not going to be able to proceed, and start &quot;saying our prayers&quot; or seeking &quot;enlightenment&quot; or stocking up on heroin or whatever else we need to do to face death.  This is a completely serious point:  if we decide we cannot hope to produce this friendly AI, it's better to accept that as quickly as possible and decide what we want to do with this short stay between the birth canal and the grave.  
<br>
<p>However, as a programmer I'm tempted to point out that often you don't need to see the bits that represent an object, but merely the bits that represent its interface.  Let someone else worry about implementation.
<br>
<p>(2) You said that a test-giver has to be more intelligent than a test-taker.  I don't think that's necessarily the case.  For instance, what if the test consisted of:  &quot;We're dealing with RSA.  Here's an encrypted message, and here's the public key that encrypted it.  What is the private key?&quot;  It might take massive computational power to &quot;take&quot; that test, i.e. break the code.  But it takes orders of magnitude less to both generate the encrypted message, and confirm any answer the test-taker provides.  This is quite similar to the problem of theorem-provers mentioned above.  Another example of a test could be:  &quot;Here's a lab full of standard stock ingredients.  Create something that will make me trip.  I will give you your grade one hour after you deliver your answer.&quot;
<br>
<p><p>As a final point:  I'm going to go ahead and put the to-do list up online.  I warn I'm going to lean heavily on real-world applicability, so we might see a constant resonance between mathematical definitions and what I consider &quot;executable&quot; actions.  I'll be putting up steps like &quot;raise $20,000,000 to fund research&quot; and &quot;create a computer with 700 TF to perform tests&quot; and the like.  Others can focus on the mathematical rigor necessary at various junctures.  Defining waypoints as mathematical objects, and the interconnecting strategies as meatspace man-hours, may be our best bet.
<br>
<p>&nbsp;- Luke
<br>
<p><p><p><p>On Tue, Oct 20, 2009 at 11:22 AM, Matt Mahoney &lt;<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI">matmahoney@yahoo.com</a>&gt; wrote:
<br>
<p>Luke wrote:
<br>
<em>&gt;&gt; Alright, it is no wonder you guys can't get anything done.  I start a single thread, with a single, simple purpose:  to trade versions of a single document: the to-do list.  And you all can't resist the urge to get into the most arcane, esoteric mathematical bullshit imaginable.  &quot;Degree of compressibility&quot;.  &quot;Test giver must have more information than test-taker&quot;.  wank wank wank.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Because your checklist is wrong. Specifically, the first 3 steps are wrong. This invalidates the last 2 steps that depend on them. To quote:
</em><br>
<em>&gt;&gt;&gt;&gt;
</em><br>
<em>&gt;This document implies dependencies only insofar as each step's dependencies should appear before that step.  Note that other sequential orderings are possible while maintaining this constraint. 
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;[ ] Compile design requirements for &quot;friendly AI&quot;.  When will we know we have succeeded?
</em><br>
<em>&gt;
</em><br>
<em>&gt;[ ] Develop automated tests which will determine whether a given system is friendly to humans or not
</em><br>
<em>&gt;[ ] Develop automated tests which will determine whether a given system is intelligent or not (IQ, whatever)
</em><br>
<em>&gt;	(these tests should reflect the requirements laid out in the first step: &quot;compile design requirements&quot;)
</em><br>
<em>&gt;
</em><br>
<em>&gt;[ ] Develop prototype systems and apply these tests to them.  Refactor tests as necessary in the case we find that some requirement is not specified in the tests.
</em><br>
<em>&gt;
</em><br>
<em>&gt;[ ] Continue refactoring prototypes until we have a system which passes both the intelligence tests and friendliness tests.  
</em><br>
<em>&gt;&lt;&lt;&lt;1. The definition of &quot;Friendly AI&quot; has an algorithmic complexity of 10^17 bits. Roughly, it means to do what people want, with conflicts resolved as an ideal secrecy-free market would resolve them. So your definition has to describe what 10^10 people want, and how much they want it, which means your definition must describe what they know, and each person knows about 10^7 bits that nobody else knows. My definition is cheating, of course, because I am pointing to human brains instead of describing what they contain.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Also, I haven't defined &quot;people&quot;. Does it include embryos, animals, slaves, women, and illegal immigrants? (Don't give me an answer that depends on your cultural beliefs). Does it include future
</em><br>
<em>&gt; human-animal-robot-software hybrids? Do all people have equal rights or do we weight rights by how much money you have like in a real market?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;2. You can't test for friendliness unless you already know that the tester is friendly. How do you know it isn't lying?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;3. You can't test for intelligence unless you are smarter than the test taker. Otherwise, how do you know that it is giving the right answers?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;So the result is we will find another way to build AI. There is a US$1 quadrillion incentive to get it done. That's the value of global human labor divided by market interest rates.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Just in case you haven't noticed, the internet is getting smarter.
</em><br>
<em>&gt;
</em><br>
<em>&gt;-- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI">matmahoney@yahoo.com</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20585.html">Pavitra: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Previous message:</strong> <a href="20583.html">Mu In Taiwan: "[sl4] This post contains basilisks. View at your own risk."</a>
<li><strong>In reply to:</strong> <a href="20579.html">Luke: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20585.html">Pavitra: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Reply:</strong> <a href="20585.html">Pavitra: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Reply:</strong> <a href="20604.html">Tim Freeman: "Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20584">[ date ]</a>
<a href="index.html#20584">[ thread ]</a>
<a href="subject.html#20584">[ subject ]</a>
<a href="author.html#20584">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
