<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] to-do list for strong, nice AI</title>
<meta name="Author" content="Luke (wlgriffiths@gmail.com)">
<meta name="Subject" content="Re: [sl4] to-do list for strong, nice AI">
<meta name="Date" content="2009-10-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] to-do list for strong, nice AI</h1>
<!-- received="Mon Oct 19 23:33:53 2009" -->
<!-- isoreceived="20091020053353" -->
<!-- sent="Tue, 20 Oct 2009 01:33:46 -0400" -->
<!-- isosent="20091020053346" -->
<!-- name="Luke" -->
<!-- email="wlgriffiths@gmail.com" -->
<!-- subject="Re: [sl4] to-do list for strong, nice AI" -->
<!-- id="12902e900910192233x104303f4j70585202432cad11@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="797318.20923.qm@web51908.mail.re2.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Luke (<a href="mailto:wlgriffiths@gmail.com?Subject=Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI"><em>wlgriffiths@gmail.com</em></a>)<br>
<strong>Date:</strong> Mon Oct 19 2009 - 23:33:46 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20577.html">Pavitra: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Previous message:</strong> <a href="20575.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>In reply to:</strong> <a href="20575.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20577.html">Pavitra: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Reply:</strong> <a href="20577.html">Pavitra: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Reply:</strong> <a href="20578.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20576">[ date ]</a>
<a href="index.html#20576">[ thread ]</a>
<a href="subject.html#20576">[ subject ]</a>
<a href="author.html#20576">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Alright, it is no wonder you guys can't get anything done.  I start a single
<br>
thread, with a single, simple purpose:  to trade versions of a single
<br>
document: the to-do list.  And you all can't resist the urge to get into the
<br>
most arcane, esoteric mathematical bullshit imaginable.  &quot;Degree of
<br>
compressibility&quot;.  &quot;Test giver must have more information than test-taker&quot;.
<br>
&nbsp;wank wank wank.
<br>
I've noticed that conversations on this list consistently do one thing:
<br>
grow.
<br>
<p>The to-do list was an effort to contract the conversation.  An English
<br>
teacher of mine in high school once laid out the process of writing a paper
<br>
like this:
<br>
<p>&lt;&gt;&lt;&gt;&lt;=
<br>
<p>It's supposed to be a graphic:  it shows the process of brainstorming,
<br>
refinement, outlining, composition, and editing, as successive periods of
<br>
expansion and contraction.  Any evolutionary process does this.
<br>
&nbsp;Diversification (expansion), selection (contraction), diversification
<br>
within the subset that was selected (expansion), and so on.  Editing comes
<br>
in where the thing in front of you doesn't grow or contract; merely changes
<br>
through substitution (hence the parallel lines of the equals sign).  The
<br>
to-do list was supposed to be a refinement process, but it merely became the
<br>
seed for more uncontrolled growth.
<br>
<p>Oh, and as for &quot;mathematical&quot; definitions of friendliness, I'd say to hell
<br>
with economics (which never models more than one entity and has no concept
<br>
of allies).  I'd ask the military:  how do you gauge whether another entity
<br>
is friendly?  I'm sure the CIA's been working on that problem for a
<br>
looooooong time.  Maybe they won't share their findings though.  Maybe
<br>
they're not friendly to our goals.  But being humans, who also stand to die
<br>
at the hands of circular saw-wielding terminators, they might just see the
<br>
benefit in sharing knowledge.  Anyone here do military theory?
<br>
<p>Sorry if I offended anyone.  I'm only trying to up the mutation rate;
<br>
nothing personal.
<br>
<p>&nbsp;- Luke
<br>
<p>On Sun, Oct 18, 2009 at 8:07 PM, Matt Mahoney &lt;<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI">matmahoney@yahoo.com</a>&gt; wrote:
<br>
<p><em>&gt; From: Pavitra &lt;<a href="mailto:celestialcognition@gmail.com?Subject=Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI">celestialcognition@gmail.com</a>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt; I define &quot;best&quot; to mean the result that an ideal secrecy-free market
</em><br>
<em>&gt; &gt;&gt; would produce. Do you have a better definition, for some definition
</em><br>
<em>&gt; &gt;&gt; of &quot;better definition&quot;?
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Yes: the result that an ideal secrecy-free dictatorship run by me would
</em><br>
<em>&gt; &gt; produce. (Secrecy-free in this context means that there are no secrets
</em><br>
<em>&gt; &gt; from the dictator. I can choose what to divulge to which of the rest of
</em><br>
<em>&gt; &gt; the citizens.)
</em><br>
<em>&gt;
</em><br>
<em>&gt; No problem. The AGI puts your brain in a simulator where you can be
</em><br>
<em>&gt; dictator.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt;&gt;&gt; One human knows 10^9 bits (Landauer's estimate of human long term
</em><br>
<em>&gt; &gt;&gt;&gt;&gt;  memory). 10^10 humans know 10^17 to 10^18 bits, allowing for
</em><br>
<em>&gt; &gt;&gt;&gt; some overlapping knowledge.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt;&gt; Again, where are you obtaining your estimates of
</em><br>
<em>&gt; &gt;&gt;&gt; degree-of-compressibility?
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt; The U.S. Dept. of Labor estimates it costs on average $15K to replace
</em><br>
<em>&gt; &gt;&gt; an employee. This is about 4 months of U.S. per capita income, or
</em><br>
<em>&gt; &gt;&gt; 0.5% of life expectancy. This mean on average that nobody knows more
</em><br>
<em>&gt; &gt;&gt; than 99.5% of what you need to know to do your job. It is reasonable
</em><br>
<em>&gt; &gt;&gt; to assume that as the economy grows and machines do our more mundane
</em><br>
<em>&gt; &gt;&gt; tasks, that jobs will become more specialized and that the fraction
</em><br>
<em>&gt; &gt;&gt; of shared knowledge will decrease. It is already the case that higher
</em><br>
<em>&gt; &gt;&gt; paying jobs cost more to replace, e.g. 1-2 years.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt; Turnover cost is relevant because the primary function of AI will be
</em><br>
<em>&gt; &gt;&gt; to make humans more productive, at least initially. Our interest is
</em><br>
<em>&gt; &gt;&gt; in the cost of work-related knowledge.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;This feels wrong in several ways.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Why is redundancy in employment utility a good indicator of redundancy
</em><br>
<em>&gt; &gt; in the aspects of human experience that we will care about preserving
</em><br>
<em>&gt; &gt; through the Singularity?
</em><br>
<em>&gt;
</em><br>
<em>&gt; People learn at a fairly constant rate. If you spend 1/3 of your life at
</em><br>
<em>&gt; work, then 1/3 of what you know is work related. The cost of replacing you
</em><br>
<em>&gt; is the cost of the new employee re-learning all of the information that is
</em><br>
<em>&gt; found in your brain and nowhere else. An AGI can guess most of what you know
</em><br>
<em>&gt; from what other people know. It is critically important to measure how much
</em><br>
<em>&gt; you know that nobody else does. When you multiply that by the world's
</em><br>
<em>&gt; population, you have the number of bits needed to model all of the world's
</em><br>
<em>&gt; brains.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Doesn't the variance in redundancy by job type imply that the
</em><br>
<em>&gt; &gt; cost-to-replace reflects the nature of the job more than it reflects the
</em><br>
<em>&gt; &gt; nature of the person?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes. As jobs become more specialized, the cost of automating all work goes
</em><br>
<em>&gt; up.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Why will &quot;the primary function of AI ... be to make humans more
</em><br>
<em>&gt; &gt; productive, at least initially&quot;? Shouldn't the AI handle
</em><br>
<em>&gt; &gt; productivity/production more or less unilaterally, and make humans more
</em><br>
<em>&gt; &gt; happy/eudaimonic?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Because AGI is expensive and people want a return on their investment. So
</em><br>
<em>&gt; people will invest in ways to automate work and increase productivity. This
</em><br>
<em>&gt; happens to require solving hard problems in language, vision, and modeling
</em><br>
<em>&gt; human behavior.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt; I agree. Turing was aware of the problem in 1950 when he gave an
</em><br>
<em>&gt; &gt;&gt; example of a computer taking 30 seconds to give the wrong answer to
</em><br>
<em>&gt; &gt;&gt; an arithmetic problem. I proposed text compression as one
</em><br>
<em>&gt; &gt;&gt; alternative. <a href="http://mattmahoney.net/dc/rationale.html">http://mattmahoney.net/dc/rationale.html</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; That seems like a pretty good definition, but I'm not convinced that a
</em><br>
<em>&gt; &gt; gigabyte of Wikipedia is _the best_ possible corpus. In particular,
</em><br>
<em>&gt; &gt; Wikipedia is very thin on fiction. I want AI to be able to grok the arts.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It's not the ideal corpus, but that doesn't exist yet. But it's very
</em><br>
<em>&gt; similar to the problem we want to solve.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt;&gt;&gt;&gt; C-&gt;D[ ] Develop an automated comparison test that returns the
</em><br>
<em>&gt; &gt;&gt;&gt;&gt;&gt; more intelligent of two given systems.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt;&gt;&gt; How? The test giver has to know more than the test taker.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt;&gt; Again, this seems more a criticism of C than of D.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt; It depends on what you mean by &quot;intelligence&quot;. A more general
</em><br>
<em>&gt; &gt;&gt; definition might be making more accurate predictions, or making them
</em><br>
<em>&gt; &gt;&gt; faster. But it raises the question of how the evaluator can know the
</em><br>
<em>&gt; &gt;&gt; correct answers unless it is more intelligent than the evaluated. If
</em><br>
<em>&gt; &gt;&gt; your goal is to predict human behavior (a prerequisite for
</em><br>
<em>&gt; &gt;&gt; friendliness), then humans have to do the testing.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; This still sounds like you're talking about step C.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Step D says, &quot;Assuming we already have a formal definition of
</em><br>
<em>&gt; &gt; intelligence, develop a computable comparison test for intelligence&quot;. I
</em><br>
<em>&gt; &gt; don't see why the comparison test requires greater-than-tested
</em><br>
<em>&gt; &gt; intelligence _in addition to_ whatever level of intelligence the formal
</em><br>
<em>&gt; &gt; definition created in C constitutes.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Suppose the test is text compression. The test works because the correct
</em><br>
<em>&gt; answer to the question &quot;what is the next bit?&quot; was decided by humans that
</em><br>
<em>&gt; are smarter than the best predictors we now have. When these predictors get
</em><br>
<em>&gt; as good as humans, what will you use for your test then?
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; The incomputability of Kolgomorov complexity is likely to be the largest
</em><br>
<em>&gt; &gt; obstacle in step D.
</em><br>
<em>&gt;
</em><br>
<em>&gt; No. It's that knowledge can't come from nowhere. If it were possible for an
</em><br>
<em>&gt; agent in a closed system to test proposed modifications of itself for
</em><br>
<em>&gt; intelligence, then it could recursively self improve. But if intelligence
</em><br>
<em>&gt; means knowing more, then clearly that can't happen.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt; Nonhuman intelligence = human extinction.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; What about intelligence that is a proper superset of the human?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Now you're talking about superhuman intelligence, as in AI that knows what
</em><br>
<em>&gt; all humans know. This it is a question of whether the AI &quot;is&quot; us. It's a
</em><br>
<em>&gt; purely philosophical question because there is no test to tell whether a
</em><br>
<em>&gt; program that simulates you &quot;is&quot; you.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt; Define &quot;Friendly&quot; in 10^17 bits or less.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Cheating answer: shares my value system.
</em><br>
<em>&gt;
</em><br>
<em>&gt; What if the AI can reprogram your value system?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Suppose your value system rejects the idea of your value system being
</em><br>
<em>&gt; reprogrammed. But you also value being absolute dictator of the world. The
</em><br>
<em>&gt; AI has a model of your brain and knows this. It simulates a world where you
</em><br>
<em>&gt; can have everything you want. What do you think will happen to you? What
</em><br>
<em>&gt; happens to any reinforcement learner that receives only positive
</em><br>
<em>&gt; reinforcement no matter what it does?
</em><br>
<em>&gt;
</em><br>
<em>&gt; -- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI">matmahoney@yahoo.com</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20577.html">Pavitra: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Previous message:</strong> <a href="20575.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>In reply to:</strong> <a href="20575.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20577.html">Pavitra: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Reply:</strong> <a href="20577.html">Pavitra: "Re: [sl4] to-do list for strong, nice AI"</a>
<li><strong>Reply:</strong> <a href="20578.html">Matt Mahoney: "Re: [sl4] to-do list for strong, nice AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20576">[ date ]</a>
<a href="index.html#20576">[ thread ]</a>
<a href="subject.html#20576">[ subject ]</a>
<a href="author.html#20576">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
