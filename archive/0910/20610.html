<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)</title>
<meta name="Author" content="Matt Mahoney (matmahoney@yahoo.com)">
<meta name="Subject" content="Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)">
<meta name="Date" content="2009-10-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)</h1>
<!-- received="Sun Oct 25 16:10:02 2009" -->
<!-- isoreceived="20091025221002" -->
<!-- sent="Sun, 25 Oct 2009 15:09:55 -0700 (PDT)" -->
<!-- isosent="20091025220955" -->
<!-- name="Matt Mahoney" -->
<!-- email="matmahoney@yahoo.com" -->
<!-- subject="Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)" -->
<!-- id="957475.95660.qm@web51903.mail.re2.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20091025210841.GH18098@digitalkingdom.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Matt Mahoney (<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Why%20extrapolate?%20(was%20Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI)"><em>matmahoney@yahoo.com</em></a>)<br>
<strong>Date:</strong> Sun Oct 25 2009 - 16:09:55 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20611.html">Johnicholas Hines: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>Previous message:</strong> <a href="20609.html">Robin Lee Powell: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>In reply to:</strong> <a href="20609.html">Robin Lee Powell: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20611.html">Johnicholas Hines: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>Reply:</strong> <a href="20611.html">Johnicholas Hines: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>Reply:</strong> <a href="20613.html">Tim Freeman: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20610">[ date ]</a>
<a href="index.html#20610">[ thread ]</a>
<a href="subject.html#20610">[ subject ]</a>
<a href="author.html#20610">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Suppose we create AI that is capable of modeling any human brain. Perhaps it captures the information by brain scanning, or perhaps by observing us and learning to predict our actions. Whatever technique is used, a model of your brain would be like a function that could be called from a higher level program. Given your mental state and input, it will predict your future mental state and output. Suppose also that the AI does this for every person on earth, as well as anything that might be considered intelligent, such as babies, animals, and computers.
<br>
<p>There are many things one could do with such an AI. For example, I could impersonate you and tell your bank to transfer all of your money to me. Or if I wanted to wipe out all human life, I could simulate various scenarios to figure out the quickest way to do it. For example, I might convince world leaders to launch nuclear attacks. (I can predict which stimuli will have this result). Or I could convince people to build armies of killer robots to defend them, then reprogram them to kill everyone. Or I could convince people to upload by running convincing simulations of dead people describing how happy they are. Once everyone is convinced and have discarded their physical bodies, I turn off the simulation.
<br>
<p>But lets say we want the AI to be friendly. How do we program it?
<br>
<p>The AI could run simulations to predict the consequences of its actions. It could be given a search problem: what actions will result in the greatest total happiness for all humans?
<br>
<p>This requires answering the questions &quot;what is human?&quot; and &quot;what is happiness?&quot;
<br>
<p>Suppose that you have Alzheimer's disease and you need a new brain, so the AI removes your malfunctioning brain and replaces it with a functionally identical computer. (It can do this because it already has a model of your brain). Or it replaces half of your brain. Is it &quot;you&quot;?
<br>
<p>Suppose that your body is old so the AI replaces it too, with a robot body. Does its form matter? Is it &quot;you&quot; if it doesn't look like the original? You may choose to be embodied in many robots distributed all over the world, or not have a body at all.
<br>
<p>Suppose that a simulation of your brain was sped up in a simulated environment so that you lived a year in 1 second. Suppose there were multiple copies with the same initial memories but run in different simulated worlds in parallel. Is each of them &quot;you&quot;? Remember that we assumed that the AI could already run simulations of you (without your knowledge) to predict your actions.
<br>
<p>I know these topics have been discussed, but as far as I know they have not been answered in any way that settles the question of &quot;what is friendly?&quot;
<br>
<p>And this raises the question &quot;what is happiness?&quot; If happiness can be modeled by utility, then the AI can compute your utility for any mental state. It does a search, finds the state of maximum utility, and if your brain has been replaced with a computer, puts you directly into this state. This state is fixed. How does it differ from death?
<br>
<p>Or if utility is not a good model of happiness, then what is?
<br>
<p>These questions are important because we are actually building AI that models human brains. The more it observes of us, the better it will be able to predict our actions. If we don't answer these questions, then by default the AI will be programmed to do whatever its builders program it to do.
<br>
&nbsp;-- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Why%20extrapolate?%20(was%20Re:%20[sl4]%20to-do%20list%20for%20strong,%20nice%20AI)">matmahoney@yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20611.html">Johnicholas Hines: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>Previous message:</strong> <a href="20609.html">Robin Lee Powell: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>In reply to:</strong> <a href="20609.html">Robin Lee Powell: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20611.html">Johnicholas Hines: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>Reply:</strong> <a href="20611.html">Johnicholas Hines: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<li><strong>Reply:</strong> <a href="20613.html">Tim Freeman: "Re: Why extrapolate? (was Re: [sl4] to-do list for strong, nice AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20610">[ date ]</a>
<a href="index.html#20610">[ thread ]</a>
<a href="subject.html#20610">[ subject ]</a>
<a href="author.html#20610">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
