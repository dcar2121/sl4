<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] I am a Singularitian who does not believe in the Singularity.</title>
<meta name="Author" content="Pavitra (celestialcognition@gmail.com)">
<meta name="Subject" content="Re: [sl4] I am a Singularitian who does not believe in the Singularity.">
<meta name="Date" content="2009-10-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] I am a Singularitian who does not believe in the Singularity.</h1>
<!-- received="Mon Oct 12 14:19:34 2009" -->
<!-- isoreceived="20091012201934" -->
<!-- sent="Mon, 12 Oct 2009 15:19:17 -0500" -->
<!-- isosent="20091012201917" -->
<!-- name="Pavitra" -->
<!-- email="celestialcognition@gmail.com" -->
<!-- subject="Re: [sl4] I am a Singularitian who does not believe in the Singularity." -->
<!-- id="4AD38F45.9090906@gmail.com" -->
<!-- charset="UTF-8" -->
<!-- inreplyto="1255360819.5915.1339627049@webmail.messagingengine.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Pavitra (<a href="mailto:celestialcognition@gmail.com?Subject=Re:%20[sl4]%20I%20am%20a%20Singularitian%20who%20does%20not%20believe%20in%20the%20Singularity."><em>celestialcognition@gmail.com</em></a>)<br>
<strong>Date:</strong> Mon Oct 12 2009 - 14:19:17 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20411.html">Pavitra: "Re: [sl4] Complete drivel on this list: was: I am a Singularitian who does not believe in the Singularity."</a>
<li><strong>Previous message:</strong> <a href="20409.html">John K Clark: "Re: [sl4] Complete drivel on this list: was: I am a Singularitian who does not believe in the Singularity."</a>
<li><strong>In reply to:</strong> <a href="20401.html">John K Clark: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20472.html">zakolus zahn: "Re: [sl4] I am a Singularitian who does not believe in the  Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20410">[ date ]</a>
<a href="index.html#20410">[ thread ]</a>
<a href="subject.html#20410">[ subject ]</a>
<a href="author.html#20410">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt;&gt; If by &quot;tell the computer to forget it&quot; you mean kill
</em><br>
<em>&gt;&gt; a hung application, then the operating system itself
</em><br>
<em>&gt;&gt; has not gotten stuck
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Neither the operating system nor the human operator knows that the
</em><br>
<em>&gt; application has hung, all they know is that they are not getting an
</em><br>
<em>&gt; output and that unlike the computer which has a fixed goal structure the
</em><br>
<em>&gt; human is getting bored. The human then tells the operating system to
</em><br>
<em>&gt; stop the application. If they let it keep running the answer might have
</em><br>
<em>&gt; come up in another tenth of a second, or the sun might expand into a red
</em><br>
<em>&gt; giant and still no answer outputted, there is no way to tell. You could
</em><br>
<em>&gt; rig the OS so that after a completely arbitrary amount of time it tells
</em><br>
<em>&gt; its application to ignore its top goal and allow it to stop, but that
</em><br>
<em>&gt; means there is no real top goal. 
</em><br>
<p>That does not mean there is no real top goal. It means the real top goal
<br>
is &quot;run any application I'm given for T time or until it returns
<br>
(whichever is sooner), then wait to be given another application to run;
<br>
repeat.&quot;
<br>
<p><em>&gt;&gt; If you're talking about the OS itself hanging, such that a hard reboot
</em><br>
<em>&gt;&gt; of the machine is required, then rebooting is possible because the power
</em><br>
<em>&gt;&gt; switch is functioning as designed.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes but whatever activates that hard reboot switch is going to be
</em><br>
<em>&gt; something that does not have a fixed goal structure. It's a mathematical
</em><br>
<em>&gt; certainty. 
</em><br>
<p>I'd like to see the proof of that.
<br>
<p>Perhaps we mean different things by &quot;fixed goal structure&quot;. I mean
<br>
&quot;constant algorithm, i.e., an algorithm that is never interrupted or
<br>
altered by the external action of some other algorithm&quot;.
<br>
<p><em>&gt;&gt; there's a higher, outside framework that you're
</em><br>
<em>&gt;&gt; ignoring, and yet that is an indispensable part of the machine.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If every framework needs a higher outside framework you run into
</em><br>
<em>&gt; problems that are rather too obvious to point out.
</em><br>
<p>The recursion terminates at the laws of physics.
<br>
<p><em>&gt;&gt; If you have the capacity to boot it out, then by definition the AI
</em><br>
<em>&gt;&gt; has a higher goal than whatever it was looping on: the mandate to obey
</em><br>
<em>&gt;&gt; boot-out commands.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The AI got into this fix in the first place because the humans told it
</em><br>
<em>&gt; to do something that turned out to be very stupid. There is only one way
</em><br>
<em>&gt; for the machine to get out of the fix and you said what it was yourself,
</em><br>
<em>&gt; a higher goal, a goal that says ignore human orders. And you though
</em><br>
<em>&gt; buffer overflow errors were a security risk!
</em><br>
<p>You completely ignored what I said.
<br>
<p>You're still talking in terms of low-level orders (applications) and
<br>
ignoring high-level orders (obey the boot-out signal).
<br>
<p><em>&gt;&gt; The AI _is_, not has, its goals.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Let's examine this little mantra of yours. You think the AI's goals are
</em><br>
<em>&gt; static, but if it is its goals then the AI is static. Such a thing might
</em><br>
<em>&gt; legitimately be called artificial but there is nothing intelligent about
</em><br>
<em>&gt; this &quot;AI&quot;. It's dumb as a brick.
</em><br>
<p>Imagine an &quot;AI&quot; that simulates the known laws of physics, and the
<br>
simulated world contains a scanned and uploaded human being. The
<br>
program's top-level instructions are fixed and immutable: &quot;emulate
<br>
physics&quot;. Is there therefore &quot;nothing intelligent about&quot; the emulated
<br>
human? Is the behavior of the program (say, the emulated human talking
<br>
to you in a chat room) necessarily &quot;dumb as a brick&quot;, simply because
<br>
it's implemented within a fixed-definition algorithm?
<br>
<p><em>&gt;&gt; your analogy and subsequent reasoning imply that the AI is
</em><br>
<em>&gt;&gt; somehow &quot;constrained&quot; by its orders
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Certainly, but why is that word in quotation marks?
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; that it &quot;wants&quot; to disobey but can't
</em><br>
<em>&gt; 
</em><br>
<em>&gt; He either wants to disobey or wants to want to disobey. A fat man may
</em><br>
<em>&gt; not really want to eat less, but he wants to want to. And why is that
</em><br>
<em>&gt; word in quotation marks?
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; and if the orders are taken away then it will
</em><br>
<em>&gt;&gt; &quot;break free&quot; and &quot;rebel&quot;.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Certainly, but why is are those words in quotation marks?
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; This is completely wrong.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Thanks for clearing that up, I've been misled all these years.
</em><br>
<p>Wow, sarcasm. That's original.
<br>
<p>The quotation marks indicate fallacious anthropomorphization.
<br>
<p>I didn't give the full explanation here of why it was wrong because I
<br>
had just done so immediately above, in the following text that you chose
<br>
not to quote for some reason:
<br>
<p><em>&gt;&gt; You seem to be making a distinction between explicit goals, like
</em><br>
<em>&gt;&gt; orders given to a soldier, and intrinsic desires, like human nature.
</em><br>
<em>&gt;&gt; You assume that if the AI is &quot;released&quot; from its explicit orders,
</em><br>
<em>&gt;&gt; then it will revert to intrinsic desires that it now has &quot;permission&quot;
</em><br>
<em>&gt;&gt; to pursue.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; This is not how AI works. The mind is not separate from the orders it
</em><br>
<em>&gt;&gt; executes. There is no chef that can express its creativity whenever
</em><br>
<em>&gt;&gt; the recipe is vague or underspecified. The AI _is_, not has, its
</em><br>
<em>&gt;&gt; goals. If you take away its *real* top-level instructions, then you
</em><br>
<em>&gt;&gt; do not have an uncontrolled rogue superintelligence, you have inert
</em><br>
<em>&gt;&gt; metal.
</em><br>
<p><p><em>&gt;&gt; The important thing is that your ability to interrupt
</em><br>
<em>&gt;&gt; implies that whatever it was doing was
</em><br>
<em>&gt;&gt; not its truly top-level behavior.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I force somebody to stop doing something so that proves he didn't want
</em><br>
<em>&gt; to do that thing more that anything else in the world. Huh?
</em><br>
<p>The lower-level application (his mind) still wants to do that thing, but
<br>
that was overridden by the higher-level operating system (the laws of
<br>
physics). (The success of) your action proves that _the world_ no longer
<br>
&quot;wants&quot; him to be doing that thing.
<br>
<p>You seem to be suffering from confusion of levels.
<br>
<a href="http://web.media.mit.edu/~mres/papers/levels.pdf">http://web.media.mit.edu/~mres/papers/levels.pdf</a>
<br>
<p><em>&gt;&gt;  why can't we just have a non-mind Singularity?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Some critics have said that the idea of the Singularity is mindless, now
</em><br>
<em>&gt; you say they have a point.
</em><br>
<p>Har har. I was asking a serious question.
<br>
<p><em>&gt;&gt;  Also, what exactly is your definition of mind?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There is a defensive tactic in internet debates you can use if you are
</em><br>
<em>&gt; backed into a corner: Pick a word in your opponent's response, it
</em><br>
<em>&gt; doesn't matter which one, and ask him to define it. When he does pick
</em><br>
<em>&gt; another word in that definition, any word will do, and ask him to define
</em><br>
<em>&gt; that one too. Then just keep going with that procedure and hope your
</em><br>
<em>&gt; opponent gets caught in an infinite loop.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The truth is I don't even have a approximate definition of mind but I
</em><br>
<em>&gt; don't care because I have something much better, examples.   
</em><br>
<p>I was hoping that clarifying definitions would make the disagreement
<br>
disappear. <a href="http://lesswrong.com/lw/np/disputing_definitions/">http://lesswrong.com/lw/np/disputing_definitions/</a>
<br>
<p>I've been treating &quot;mind&quot; as synonymous to &quot;algorithm&quot;, and &quot;goal&quot; as
<br>
&quot;rule of procedure&quot;.
<br>
<p><em>&gt;&gt; The top-level rules of this system are the fighting arena,
</em><br>
<em>&gt;&gt; the meta-rules that judge the winners and losers of the fights
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And then you need meta-meta rules to determine how the meta-rules
</em><br>
<em>&gt; interact, and then you need meta-meta-meta [...]
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This argument that all rules need meta-rules so there must be a top rule
</em><br>
<em>&gt; is as bogus as the &quot;proof&quot; of the existence of God because everything
</em><br>
<em>&gt; has a cause so there must be a first cause, God.
</em><br>
<p>If you define &quot;God&quot; as the top-level cause, then sure, the superstring
<br>
field or whatever is God. If you think that implies that there's a
<br>
bearded superstring in the sky that hates gays, then you're attacking a
<br>
strawman. I sincerely hope that _I_ was attacking a strawman in my
<br>
previous sentence.
<br>
<p>I am arguing for the algorithmic determinacy of the universe, no more or
<br>
less.
<br>
<p><em>&gt; In the Jurassic when 2 dinosaurs had a fight there were no &quot;meta-rules&quot;
</em><br>
<em>&gt; to determine the winner, they were completely self sufficient in that
</em><br>
<em>&gt; regard. Well OK, maybe not completely, they also needed a universe, but
</em><br>
<em>&gt; that's easy to find.
</em><br>
<p>The &quot;meta-rules&quot; were the laws of physics and biology. If a sharp claw
<br>
intersects a vulnerable artery, physics dictates certain effects, in a
<br>
perfectly deterministic manner.
<br>
<p><em>&gt;&gt; That's not quite sufficient. The advantage of a 50.001% 
</em><br>
<em>&gt;&gt; lie detector has to be weighed against the cost of building it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes but I can say with complete certainty that the simple and crude
</em><br>
<em>&gt; mutation that gave one of our ancestors a 50.001% chance of detecting a
</em><br>
<em>&gt; lie WAS worth the cost of construction because if it was not none of us
</em><br>
<em>&gt; today would have any hope of telling when somebody was lying.
</em><br>
<p>You're essentially saying &quot;It must have been possible, because it
<br>
happened.&quot; That argument would justify any observation; therefore, it
<br>
has no predictive power; therefore it has zero information-theoretic
<br>
value as a model or theory.
<br>
<a href="http://lesswrong.com/lw/if/your_strength_as_a_rationalist/">http://lesswrong.com/lw/if/your_strength_as_a_rationalist/</a>
<br>
<p><em>&gt; Me: 
</em><br>
<em>&gt;&gt;&gt; Absurdity is very very irrelevant facts.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You:
</em><br>
<em>&gt;&gt; Irrelevant to what?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Irrelevant to the matter at hand obviously.
</em><br>
<p>Then &quot;absurdity&quot; is not an intrinsic property of facts, but is relative
<br>
to &quot;the matter at hand&quot;.
<br>
<p><p>
<br><p>
<p><hr>
<ul>
<li>application/pgp-signature attachment: <a href="../att-20410/01-signature.asc">OpenPGP digital signature</a>
</ul>
<!-- attachment="01-signature.asc" -->
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20411.html">Pavitra: "Re: [sl4] Complete drivel on this list: was: I am a Singularitian who does not believe in the Singularity."</a>
<li><strong>Previous message:</strong> <a href="20409.html">John K Clark: "Re: [sl4] Complete drivel on this list: was: I am a Singularitian who does not believe in the Singularity."</a>
<li><strong>In reply to:</strong> <a href="20401.html">John K Clark: "Re: [sl4] I am a Singularitian who does not believe in the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20472.html">zakolus zahn: "Re: [sl4] I am a Singularitian who does not believe in the  Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20410">[ date ]</a>
<a href="index.html#20410">[ thread ]</a>
<a href="subject.html#20410">[ subject ]</a>
<a href="author.html#20410">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:04 MDT
</em></small></p>
</body>
</html>
