<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Definition of strong recursive self-improvement</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Definition of strong recursive self-improvement">
<meta name="Date" content="2004-12-31">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Definition of strong recursive self-improvement</h1>
<!-- received="Fri Dec 31 12:26:41 2004" -->
<!-- isoreceived="20041231192641" -->
<!-- sent="Fri, 31 Dec 2004 14:26:44 -0500" -->
<!-- isosent="20041231192644" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Definition of strong recursive self-improvement" -->
<!-- id="41D5A7F4.6060205@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="8d71341e041231015038346de5@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Definition%20of%20strong%20recursive%20self-improvement"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri Dec 31 2004 - 12:26:44 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10494.html">Thomas Buckner: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Previous message:</strong> <a href="10492.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>In reply to:</strong> <a href="10492.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10494.html">Thomas Buckner: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10494.html">Thomas Buckner: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10495.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10493">[ date ]</a>
<a href="index.html#10493">[ thread ]</a>
<a href="subject.html#10493">[ subject ]</a>
<a href="author.html#10493">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Russell Wallace wrote:
<br>
<em>&gt; On Thu, 30 Dec 2004 19:15:36 -0800 (PST), Thomas Buckner
</em><br>
<em>&gt; &lt;<a href="mailto:tcbevolver@yahoo.com?Subject=Re:%20Definition%20of%20strong%20recursive%20self-improvement">tcbevolver@yahoo.com</a>&gt; wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;Evolutionary trial-and-error tournaments between
</em><br>
<em>&gt;&gt;subsystems of itself, with winning strategies
</em><br>
<em>&gt;&gt;globally adopted, but still under control of the
</em><br>
<em>&gt;&gt;singleton.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well yes, that's what I believe in too. Evolution as a tool under
</em><br>
<em>&gt; control of something that is not itself evolved.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Or put another way: A &quot;self-modifying&quot; entity must, to produce useful
</em><br>
<em>&gt; results in the long run, consist of a static part that modifies the
</em><br>
<em>&gt; dynamic part, but is not itself modified in the process.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; (Biological evolution is not a counterexample (the static part being
</em><br>
<em>&gt; the laws of physics + the terrestrial environment) nor is human
</em><br>
<em>&gt; culture (the static part being the laws of physics + the terrestrial
</em><br>
<em>&gt; environment + the human genome).)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; However, that won't &quot;fold the graph in on itself&quot; to make a magic FOOM
</em><br>
<em>&gt; as Eliezer appears to believe.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; As I understand him to mean it, &quot;recursive self-improvement&quot; means
</em><br>
<em>&gt; modifying the whole stack. That's the part I don't believe in; more to
</em><br>
<em>&gt; the point, that's the part that would have to work in order for a
</em><br>
<em>&gt; &quot;hard takeoff&quot; scenario to be realistic.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't think that can happen, but if it could, it would make a
</em><br>
<em>&gt; difference to rational planning at the present time, which is why I'm
</em><br>
<em>&gt; asking whether there's a reason to believe it could.
</em><br>
<p>I know of no way for a recursive optimizer to change the laws of physics. 
<br>
(That doesn't mean no way exists; but it's not necessary.)  Strong RSI 
<br>
means that the part of the process that, structurally speaking, performs 
<br>
optimization, is open to restructuring.
<br>
<p>When you look at a system on a level of &quot;the laws of physics&quot;, then the 
<br>
laws of physics, as such, are not structurally responsible for optimizing 
<br>
either genomes (in the case of natural selection) or thoughts (in the case 
<br>
of humans).  In both cases, we can model the optimization on a higher level 
<br>
without directly modeling the laws of physics.  Now this doesn't mean the 
<br>
optimization takes place outside the laws of physics.  What it does mean is 
<br>
that the optimizer can be fully recursive within the laws of physics, 
<br>
because we don't have to modify the laws of physics to modify the structure 
<br>
embedded within physics that performs optimization.
<br>
<p>Let's say we start with a coin that might be either heads or tails.  A 
<br>
human being looks at this coin.  If the coin is heads, the human being does 
<br>
nothing.  If the coin is tails, the human being performs a FLIP operation. 
<br>
&nbsp;&nbsp;After this is carried out, the coin becomes heads regardless of its 
<br>
initial condition.  This is an optimization process (albeit a very weak and 
<br>
uninteresting one), compressing the future so that the coin ends up heads 
<br>
regardless of its initial condition.  If an AI were unleashed upon the 
<br>
problem with a utility function of tails=0 heads=1, then arbitrarily great 
<br>
efforts might go into finding and flipping the coin, up to the limit of the 
<br>
AI's intelligence.  Actions would be chosen on the basis of whether they 
<br>
were predicted to lead to a coin in state HEADS.  If the AI were smart 
<br>
enough, it might assemble a ship and send it to a distant galaxy to find 
<br>
the coin, because that was the action predicted to lead to state HEADS with 
<br>
the highest probability.
<br>
<p>The important thing to note is that it is inconvenient to regard &quot;the laws 
<br>
of physics&quot; as flipping the coin, because if you abstract the laws of 
<br>
physics from the particular initial configuration of the universe, and 
<br>
examine the laws as such, then the laws do not say that the coin must go to 
<br>
HEADS.  With alternate initial conditions, the universe might contain an 
<br>
optimizer that steers the future to TAILS.  So the laws of physics, 
<br>
although unmodifiable as far as we know, are not structurally responsible 
<br>
for the optimization.  That's why an optimizer can reach around and modify 
<br>
itself, reasoning that if the optimizer has a different form it will be 
<br>
more efficient in solving the problem of choosing actions that lead to 
<br>
HEADS and therefore the self-modification probably leads to HEADS.
<br>
<p>Human beings using their intelligence to directly and successfully modify 
<br>
human brains for higher intelligence, would be a very weak example of 
<br>
Strong RSI - *if* the resultant enhanced humans were smarter than the 
<br>
smartest researchers previously working on human intelligence enhancement.
<br>
<p>Human beings using their intelligence to collect favorable mutations into 
<br>
the human genome, or programming DNA directly, for the purpose of producing 
<br>
smarter children, would be a weak case of RSI arising (after a hell of a 
<br>
long time) from natural selection.  That is, natural selection is an 
<br>
optimization process that produces optimizers very unlike itself (vehicles 
<br>
for genes with their own nervous systems and built-in goals), but these 
<br>
secondary optimizers don't change the structure of the primary optimization 
<br>
process; they don't use their intelligence to modify DNA.  Eventually one 
<br>
of these optimizers became capable of reaching back and transcending the 
<br>
optimization process of natural selection, choosing genes on a criterion 
<br>
other than reproductive efficiency of the vehicles constructed by the 
<br>
genes.  But it's not Strong RSI until the first genetically modified 
<br>
supergenius is born who is better at genetic engineering than the previous 
<br>
researchers.  And by that time, some existing supergenius like myself will 
<br>
have long since built a Strong RSI that doesn't pass through the bottleneck 
<br>
of 200Hz neurons.
<br>
<p>To sum up, if an optimizer is capable of restructuring the part of itself 
<br>
that does the optimizing - capable of changing the dynamics that 
<br>
distinguish, evaluate, and choose between possible plans and designs; and 
<br>
if this restructuring is broad enough to permit moving between, e.g., 
<br>
optimizations structured like natural selection and optimizations 
<br>
structured like a mammalian brain; and if the restructuring produces 
<br>
substantial increases in the power of the optimization process, including 
<br>
the power to commit further restructurings; then I would call that Strong 
<br>
Recursive Self-Improvement.
<br>
<p>Please note that this definition excludes natural selection, ordinary human 
<br>
intelligence, genetic engineering that does not produce new researchers who 
<br>
are better at genetic engineering, memetic 'evolution' that can't modify 
<br>
human brains to produce enhanced humans who are better at modifying human 
<br>
brains, the progress of the global economy, and many other weak little 
<br>
optimization processes commonly offered up as precedent for the literally 
<br>
unimaginable power of a superintelligence.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10494.html">Thomas Buckner: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Previous message:</strong> <a href="10492.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>In reply to:</strong> <a href="10492.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10494.html">Thomas Buckner: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10494.html">Thomas Buckner: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10495.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10493">[ date ]</a>
<a href="index.html#10493">[ thread ]</a>
<a href="subject.html#10493">[ subject ]</a>
<a href="author.html#10493">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
