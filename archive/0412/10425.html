<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: More MWI implications: Altruism and the 'Quantum Insurance Policy'</title>
<meta name="Author" content="Marc Geddes (marc_geddes@yahoo.co.nz)">
<meta name="Subject" content="Re: More MWI implications: Altruism and the 'Quantum Insurance Policy'">
<meta name="Date" content="2004-12-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: More MWI implications: Altruism and the 'Quantum Insurance Policy'</h1>
<!-- received="Sun Dec 12 22:25:24 2004" -->
<!-- isoreceived="20041213052524" -->
<!-- sent="Mon, 13 Dec 2004 18:25:22 +1300 (NZDT)" -->
<!-- isosent="20041213052522" -->
<!-- name="Marc Geddes" -->
<!-- email="marc_geddes@yahoo.co.nz" -->
<!-- subject="Re: More MWI implications: Altruism and the 'Quantum Insurance Policy'" -->
<!-- id="20041213052522.23532.qmail@web20227.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="e6ee0419041212200230c6f81b@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marc Geddes (<a href="mailto:marc_geddes@yahoo.co.nz?Subject=Re:%20More%20MWI%20implications:%20Altruism%20and%20the%20'Quantum%20Insurance%20Policy'"><em>marc_geddes@yahoo.co.nz</em></a>)<br>
<strong>Date:</strong> Sun Dec 12 2004 - 22:25:22 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10426.html">David Massoglia: "RE: life extension - genomic tests"</a>
<li><strong>Previous message:</strong> <a href="10424.html">Edmund Schaefer: "Re: More MWI implications: Altruism and the 'Quantum Insurance Policy'"</a>
<li><strong>In reply to:</strong> <a href="10424.html">Edmund Schaefer: "Re: More MWI implications: Altruism and the 'Quantum Insurance Policy'"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10428.html">Edmund Schaefer: "Re: More MWI implications: Altruism and the 'Quantum Insurance Policy'"</a>
<li><strong>Reply:</strong> <a href="10428.html">Edmund Schaefer: "Re: More MWI implications: Altruism and the 'Quantum Insurance Policy'"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10425">[ date ]</a>
<a href="index.html#10425">[ thread ]</a>
<a href="subject.html#10425">[ subject ]</a>
<a href="author.html#10425">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;--- Edmund Schaefer &lt;<a href="mailto:edmund.schaefer@gmail.com?Subject=Re:%20More%20MWI%20implications:%20Altruism%20and%20the%20'Quantum%20Insurance%20Policy'">edmund.schaefer@gmail.com</a>&gt; 
<br>
<em>&gt;  
</em><br>
<em>&gt; &gt; For instance
</em><br>
<em>&gt; &gt; suppose Eliezer was hit by a truck walking to
</em><br>
<em>&gt; work.
</em><br>
<em>&gt; &gt; Suppose he'd been linking the decision about which
</em><br>
<em>&gt; &gt; route to walk to work to a 'quantum coin flip'. 
</em><br>
<em>&gt; Then
</em><br>
<em>&gt; &gt; half the alternative versions of himself would
</em><br>
<em>&gt; have
</em><br>
<em>&gt; &gt; taken another route to work and avoided the truck.
</em><br>
<em>&gt;  So
</em><br>
<em>&gt; &gt; in 50% of QM branches he'd live on.  Compare that
</em><br>
<em>&gt; to
</em><br>
<em>&gt; &gt; the case where Eli's decision about which route to
</em><br>
<em>&gt; &gt; walk to work was being made mostly according to
</em><br>
<em>&gt; &gt; classical physics.  If something bad happened to
</em><br>
<em>&gt; him
</em><br>
<em>&gt; &gt; he'd be dead in say 99% of QM branches.  The
</em><br>
<em>&gt; effect of
</em><br>
<em>&gt; &gt; the quantum decision making is to re-distribute
</em><br>
<em>&gt; risk
</em><br>
<em>&gt; &gt; across the multiverse.  Therefore the altruist
</em><br>
<em>&gt; &gt; strategy has to be to deploy the 'quantum
</em><br>
<em>&gt; decisions'
</em><br>
<em>&gt; &gt; scheme to break the classical physics symmetry
</em><br>
<em>&gt; across
</em><br>
<em>&gt; &gt; the multiverse.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This only works because our fictional Eli assigned a
</em><br>
<em>&gt; 99% probability
</em><br>
<em>&gt; to the lethal path being more desirable. Your
</em><br>
<em>&gt; &quot;insurance policy&quot; boils
</em><br>
<em>&gt; down to the following piece of advice: If you make a
</em><br>
<em>&gt; decision that
</em><br>
<em>&gt; you're really sure about, and happen to be wrong,
</em><br>
<em>&gt; you're better off
</em><br>
<em>&gt; flipping a coin. Sure, that's sound advice, but it
</em><br>
<em>&gt; doesn't do me any
</em><br>
<em>&gt; good. If I knew I was wrong, it wouldn't be very
</em><br>
<em>&gt; sane of me to keep
</em><br>
<em>&gt; that 99% estimate of desirability. You started with
</em><br>
<em>&gt; a *really* bad
</em><br>
<em>&gt; decision, scrapped the decision in favor of a
</em><br>
<em>&gt; fifty-fifty method, saw
</em><br>
<em>&gt; that it drastically improved the survival of your
</em><br>
<em>&gt; quantum descendants,
</em><br>
<em>&gt; and said &quot;behold the life-saving power of
</em><br>
<em>&gt; randomness&quot;. Sorry, but it
</em><br>
<em>&gt; doesn't work like that. Intelligence works better
</em><br>
<em>&gt; than flipping coins.
</em><br>
<em>&gt; If you trust coin flips instead of intelligence,
</em><br>
<em>&gt; you're more likely to
</em><br>
<em>&gt; get killed. Applying this to MWI translates it to
</em><br>
<em>&gt; &quot;If you go with
</em><br>
<em>&gt; intelligence, you survive in a greater number of
</em><br>
<em>&gt; quantum realities.&quot;
</em><br>
<p>Hang on there.  The fictional Eli in my exmaple didn't
<br>
*assume* anything.  The example amounted to the Eli
<br>
asking:  *What if* I'm living in a branch with a 99%
<br>
chance of my death soon.  I don't know for certain
<br>
that the decision I'm taking is really really bad. 
<br>
I'm asking *what if* the decision is that bad.  
<br>
<p>Then the reasoning goes:  Is there something that
<br>
could be done to 'spread' the risk between all the
<br>
sentient copies of myself that diverge across the
<br>
multiverse from now on?
<br>
<p>The quantum coin flip can then be used to 'insure'
<br>
some of the alternative copies of Eli against the said
<br>
bad decision.  The quantum coin does not have be
<br>
50-50.  It can be 'weighted' according to a factoring
<br>
in of all rational information.
<br>
&nbsp;&nbsp;
<br>
<p><em>&gt;  
</em><br>
<em>&gt; &gt; In fact the scheme can be used to redistribute the
</em><br>
<em>&gt; &gt; risk of Unfriendly A.I across the multiverse. 
</em><br>
<em>&gt; There
</em><br>
<em>&gt; &gt; is a certain probability that leading A.I
</em><br>
<em>&gt; researchers
</em><br>
<em>&gt; &gt; will screw up and create Unfriendly A.I.  Again,
</em><br>
<em>&gt; if
</em><br>
<em>&gt; &gt; the human brain is largely operating off classical
</em><br>
<em>&gt; &gt; physics, a dumb decision by an A.I researcher in
</em><br>
<em>&gt; this
</em><br>
<em>&gt; &gt; QM branch is largely correlated with the same dumb
</em><br>
<em>&gt; &gt; decision by alternative versions of that
</em><br>
<em>&gt; researcher in
</em><br>
<em>&gt; &gt; all the QM branches divergent from that time on. 
</em><br>
<em>&gt; As
</em><br>
<em>&gt; &gt; an example:  Let's say Ben Goertzel screwed up and
</em><br>
<em>&gt; &gt; created and Unfriendly A.I because of a dumb
</em><br>
<em>&gt; decision.
</em><br>
<em>&gt; &gt;  The same thing happens in most of the alternative
</em><br>
<em>&gt; &gt; branches if his decisions were caused by classical
</em><br>
<em>&gt; &gt; physics!  But suppose Ben had been deploying my
</em><br>
<em>&gt; &gt; 'quantum insurance scheme', whereby he had been
</em><br>
<em>&gt; basing
</em><br>
<em>&gt; &gt; some of his daily decisions off quantum random
</em><br>
<em>&gt; &gt; numbers.  Then there would be more variation in
</em><br>
<em>&gt; the
</em><br>
<em>&gt; &gt; alternative versions of Ben across the Multiverse.
</em><br>
<em>&gt;  At
</em><br>
<em>&gt; &gt; least some versions of Ben would be less likely to
</em><br>
<em>&gt; &gt; make that dumb decision, and there would be an
</em><br>
<em>&gt; assured
</em><br>
<em>&gt; &gt; minimum percentage of QM branches avoiding
</em><br>
<em>&gt; Unfriendly
</em><br>
<em>&gt; &gt; A.I.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And if he doesn't screw up the AI? What if Ben was
</em><br>
<em>&gt; right? Your
</em><br>
<em>&gt; insurance scheme just killed half of that branch of
</em><br>
<em>&gt; the multiverse
</em><br>
<em>&gt; because a lot of Bens decided to go on coin flips
</em><br>
<em>&gt; instead of a correct
</em><br>
<em>&gt; theory, and I don't see why the second batch of
</em><br>
<em>&gt; fifty-gazillion
</em><br>
<em>&gt; sentients is less valuable than the first batch.
</em><br>
<em>&gt; Also, keep in mind,
</em><br>
<em>&gt; there's going to be some that hit the ultimately
</em><br>
<em>&gt; desirable state.
</em><br>
<em>&gt; Somewhere out there there's a quantum reality where
</em><br>
<em>&gt; Friendly AI
</em><br>
<em>&gt; spontaneously materialized out of a gas cloud. You
</em><br>
<em>&gt; can't really drive
</em><br>
<em>&gt; the number of desirable quantum realities down to
</em><br>
<em>&gt; zero, any more than
</em><br>
<em>&gt; you can accurately assign something a Bayesian
</em><br>
<em>&gt; probability of zero.
</em><br>
<em>&gt;  
</em><br>
<p>The qauntum coin flip does not have to be 50-50.  It
<br>
can be 'weighted' to factor in all rational data as
<br>
per Bayes theorem.
<br>
<p>Suppose Ben had performed all the rational analysis he
<br>
could.  He would still be left with some probability
<br>
distribution for certain courses of action.  He could
<br>
then 'bias' the qauntum coin according to this
<br>
probability distribution.
<br>
<p>In the fictional exmaple Ben is *not* assuming that
<br>
the decision he is about to take is bad.  He saying it
<br>
*might* be bad.  He is then asking if there is
<br>
anything he can do to 'spread the risk' across
<br>
alternative versions of himself in the multiverse, so
<br>
as to ensure that some minimum fraction of hs
<br>
alternative selves experience a good outcome.  
<br>
<p>=====
<br>
&quot;Live Free or Die, Death is not the Worst of Evils.&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Gen. John Stark
<br>
<p>&quot;The Universe...or nothing!&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-H.G.Wells
<br>
<p><p>Please visit my web-sites.
<br>
<p>Sci-Fi and Fantasy                : <a href="http://www.prometheuscrack.com">http://www.prometheuscrack.com</a>
<br>
Mathematics, Mind and Matter      : <a href="http://www.riemannai.org">http://www.riemannai.org</a>
<br>
<p>Find local movie times and trailers on Yahoo! Movies.
<br>
<a href="http://au.movies.yahoo.com">http://au.movies.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10426.html">David Massoglia: "RE: life extension - genomic tests"</a>
<li><strong>Previous message:</strong> <a href="10424.html">Edmund Schaefer: "Re: More MWI implications: Altruism and the 'Quantum Insurance Policy'"</a>
<li><strong>In reply to:</strong> <a href="10424.html">Edmund Schaefer: "Re: More MWI implications: Altruism and the 'Quantum Insurance Policy'"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10428.html">Edmund Schaefer: "Re: More MWI implications: Altruism and the 'Quantum Insurance Policy'"</a>
<li><strong>Reply:</strong> <a href="10428.html">Edmund Schaefer: "Re: More MWI implications: Altruism and the 'Quantum Insurance Policy'"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10425">[ date ]</a>
<a href="index.html#10425">[ thread ]</a>
<a href="subject.html#10425">[ subject ]</a>
<a href="author.html#10425">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
