<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Conservative Estimation of the Economic Impact of Artificial Intelligence</title>
<meta name="Author" content="Thomas Buckner (tcbevolver@yahoo.com)">
<meta name="Subject" content="Re: Conservative Estimation of the Economic Impact of Artificial Intelligence">
<meta name="Date" content="2004-12-30">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Conservative Estimation of the Economic Impact of Artificial Intelligence</h1>
<!-- received="Thu Dec 30 20:15:39 2004" -->
<!-- isoreceived="20041231031539" -->
<!-- sent="Thu, 30 Dec 2004 19:15:36 -0800 (PST)" -->
<!-- isosent="20041231031536" -->
<!-- name="Thomas Buckner" -->
<!-- email="tcbevolver@yahoo.com" -->
<!-- subject="Re: Conservative Estimation of the Economic Impact of Artificial Intelligence" -->
<!-- id="20041231031536.11392.qmail@web60007.mail.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="8d71341e0412300857378c8f72@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Thomas Buckner (<a href="mailto:tcbevolver@yahoo.com?Subject=Re:%20Conservative%20Estimation%20of%20the%20Economic%20Impact%20of%20Artificial%20Intelligence"><em>tcbevolver@yahoo.com</em></a>)<br>
<strong>Date:</strong> Thu Dec 30 2004 - 20:15:36 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10492.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Previous message:</strong> <a href="10490.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>In reply to:</strong> <a href="10490.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10492.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Reply:</strong> <a href="10492.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10491">[ date ]</a>
<a href="index.html#10491">[ thread ]</a>
<a href="subject.html#10491">[ subject ]</a>
<a href="author.html#10491">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
--- Russell Wallace &lt;<a href="mailto:russell.wallace@gmail.com?Subject=Re:%20Conservative%20Estimation%20of%20the%20Economic%20Impact%20of%20Artificial%20Intelligence">russell.wallace@gmail.com</a>&gt;
<br>
wrote:
<br>
<p><em>&gt; On Wed, 29 Dec 2004 18:27:43 -0500, Eliezer
</em><br>
<em>&gt; Yudkowsky
</em><br>
<em>&gt; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Conservative%20Estimation%20of%20the%20Economic%20Impact%20of%20Artificial%20Intelligence">sentience@pobox.com</a>&gt; wrote:
</em><br>
<em>&gt; &gt; Recursive self-improvement seems to be
</em><br>
<em>&gt; missing in this discussion.  Just a
</em><br>
<em>&gt; &gt; band of humans gradually improving an AI that
</em><br>
<em>&gt; slowly acquires more and more
</em><br>
<em>&gt; &gt; abilities.  It makes for a nice fantasy of
</em><br>
<em>&gt; slow, relatively safe
</em><br>
<em>&gt; &gt; transcendence where you always have plenty of
</em><br>
<em>&gt; time to see threats coming
</em><br>
<em>&gt; &gt; before they hit.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Recursive self-improvement is a nice idea, but
</em><br>
<em>&gt; I'm still curious as to
</em><br>
<em>&gt; why you believe it can work, even in principle?
</em><br>
Because it's been demonstrated in human society
<br>
for quite some time! (Individual humans
<br>
themselves may not have gotten radically smarter
<br>
in the last 10K years, but human society's
<br>
information-processing and power over the world
<br>
have been on an exponential growth curve the
<br>
whole time). 
<br>
<em>&gt; Suppose an AI hits on a way to create an
</em><br>
<em>&gt; improved version of itself,
</em><br>
<em>&gt; using heuristic methods (i.e. the same way
</em><br>
<em>&gt; human engineers do things).
</em><br>
<em>&gt; How does it know whether the modified version
</em><br>
<em>&gt; will in fact be an
</em><br>
<em>&gt; improvement? There are only two methods:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; - Formal proof. In general, for interesting
</em><br>
<em>&gt; values of A and B, there
</em><br>
<em>&gt; isn't any formal proof that A is better than B,
</em><br>
<em>&gt; even when it is in
</em><br>
<em>&gt; fact better.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; - Trial and error. This is how human engineers
</em><br>
<em>&gt; work. The problem when
</em><br>
<em>&gt; you're dealing with self-replicating entities
</em><br>
<em>&gt; is that this gets you
</em><br>
<em>&gt; into open-ended evolution. Maybe this would
</em><br>
<em>&gt; work, but if so it would
</em><br>
<em>&gt; be for evolution's value of &quot;work&quot;, which would
</em><br>
<em>&gt; select for an optimal
</em><br>
<em>&gt; self-replicator; it would just turn the
</em><br>
<em>&gt; universe into copies of
</em><br>
<em>&gt; itself. We wouldn't even get a supply of
</em><br>
<em>&gt; paperclips out of it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Is there a third possibility that I'm missing?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; - Russell
</em><br>
<em>&gt; 
</em><br>
Evolutionary trial-and-error tournaments between
<br>
subsystems of itself, with winning strategies
<br>
globally adopted, but still under control of the
<br>
singleton. The SAI 'wants' to optimize
<br>
information-processing power, not
<br>
self-replication per se. I have never been
<br>
convinced that a SAI would want paperclips any
<br>
more than we do. It might want something we can't
<br>
understand, but if it merely creates endless
<br>
copies of something essentially stupid, then it
<br>
is stupid also, even if it is clever about
<br>
achieving a stupid goal. On the other hand, if I
<br>
saw a SAI on a computronium-creating binge, I
<br>
would proceed on the assumption that it had a
<br>
good reason I couldn't grasp.
<br>
<p>The whole biz-school angle in the original post
<br>
puts me in mind of Theodore Sturgeon's novella
<br>
Microcosmic God (see google search:
<br>
<a href="http://www.google.com/search?hl=en&amp;q=Microcosmic+God&amp;btnG=Google+Search">http://www.google.com/search?hl=en&amp;q=Microcosmic+God&amp;btnG=Google+Search</a><br>
A scientist named Kidder partners with a banker
<br>
named Conant. Kidder, to accelerate his research,
<br>
creates the Neoterics, tiny creatures who live
<br>
fast, die fast, and develop a culture of their
<br>
own so fast that they soon outstrip the humans.
<br>
Conant makes more money than Croesus off Kidder's
<br>
inventions, but it is Kidder who has the
<br>
Neoterics' allegiance, and they throw up an
<br>
impenetrable dome over his island before the Air
<br>
Force can bomb it. The story ends with the
<br>
enigmatic dome, and the knowledge that they will
<br>
come out sometime soon...
<br>
<p>Tom Buckner
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
__________________________________ 
<br>
Do you Yahoo!? 
<br>
The all-new My Yahoo! - Get yours free! 
<br>
<a href="http://my.yahoo.com">http://my.yahoo.com</a> 
<br>
&nbsp;
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10492.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Previous message:</strong> <a href="10490.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>In reply to:</strong> <a href="10490.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10492.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Reply:</strong> <a href="10492.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10491">[ date ]</a>
<a href="index.html#10491">[ thread ]</a>
<a href="subject.html#10491">[ subject ]</a>
<a href="author.html#10491">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
