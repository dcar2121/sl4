<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Conservative Estimation of the Economic Impact of Artificial Intelligence</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Conservative Estimation of the Economic Impact of Artificial Intelligence">
<meta name="Date" content="2004-12-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Conservative Estimation of the Economic Impact of Artificial Intelligence</h1>
<!-- received="Wed Dec 29 16:27:41 2004" -->
<!-- isoreceived="20041229232741" -->
<!-- sent="Wed, 29 Dec 2004 18:27:43 -0500" -->
<!-- isosent="20041229232743" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Conservative Estimation of the Economic Impact of Artificial Intelligence" -->
<!-- id="41D33D6F.90404@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="3ad827f30412280329221f0922@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Conservative%20Estimation%20of%20the%20Economic%20Impact%20of%20Artificial%20Intelligence"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Dec 29 2004 - 16:27:43 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10489.html">Yan King Yin: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Previous message:</strong> <a href="10487.html">BillK: "Google Directory AI links"</a>
<li><strong>In reply to:</strong> <a href="10485.html">justin corwin: "Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10490.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Reply:</strong> <a href="10490.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10488">[ date ]</a>
<a href="index.html#10488">[ thread ]</a>
<a href="subject.html#10488">[ subject ]</a>
<a href="author.html#10488">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
justin corwin wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; What these people are really asking, via the agency of hypothetical
</em><br>
<em>&gt; investors, is &quot;what is AI good for, and how can that be delivered from
</em><br>
<em>&gt; your research.&quot;
</em><br>
<p>One of the most subtle and pernicious biases against rationality - one that 
<br>
I struggle with myself - is the tendency for your answers to change 
<br>
depending on the questions people ask you.  The more you have to argue 
<br>
against some particular question people bug you with, the more your 
<br>
intellectual focus shifts to that question - even if it's the wrong 
<br>
question.  Would I have been so mistaken earlier in my life about the 
<br>
inevitable benevolence of SIs, if people on the Extropians list hadn't 
<br>
bugged me about it?  Yes, I would have.  But I suspect I wouldn't have been 
<br>
so mistaken about that question, if earlier in my life, my parents and a 
<br>
number of annoying other people hadn't repeated to me over and over the 
<br>
cliche wisdom that intelligence has nothing to do with morality or 
<br>
benevolence or emotional maturity.  Which is not, in fact, how things work 
<br>
in humans.  From defending against that earlier fallacy, I set myself up to 
<br>
make the later mistake in the case of AIs.
<br>
<p>All those relatives and venture capitalists you so desperately want to 
<br>
convince, will always ask you the wrong questions, demand you to defend 
<br>
points that are not the most important points.  In this case, what matters 
<br>
is the survival of humane life.  This requires that someone build a 
<br>
Friendly AI (difficult problem) before someone hacks together a 
<br>
self-improving thing that goes FOOM (not an easy problem, but easier than 
<br>
FAI).  Venture capitalists won't ask you about that.  But you'll argue with 
<br>
venture capitalists, and develop all sorts of clever arguments for 
<br>
persuading them (and yourself), and your focus will change to proving that 
<br>
AI *really can* be commercially viable, and you'll lose track of the one 
<br>
most important thing.
<br>
<p><em>&gt; Artificial Intelligence, even very weakly achieved, is not just
</em><br>
<em>&gt; another technology. It represents, at the very least, a complete
</em><br>
<em>&gt; industry, and most likely, is one of those events that redefines the
</em><br>
<em>&gt; landscape of human activity.
</em><br>
<p>I like the &quot;most likely&quot; qualification you place on that.  It certainly 
<br>
helps maintain the image of your rationality if you try to make so 
<br>
startling a case to venture capitalists - it sounds like you're questioning 
<br>
yourself, even though you're not.  But if something is &quot;most likely&quot; true, 
<br>
you have to plan for it; to plan for it, you have to accept it as a 
<br>
starting point and not be embarassed to think about it or talk about it. 
<br>
Otherwise you'll flinch away from the crazy-sounding true thoughts and 
<br>
stick with the moderate, publicly acceptable, false thoughts.
<br>
<p>If conditional probabilities, p(X|A), p(Y|A), p(Z|A), all depend on A; then 
<br>
to *plan* for the &quot;most likely&quot; eventuality of A, we have to work with 
<br>
those conditional probabilities, and not be continually distracted by talk 
<br>
of &quot;but of course, all this talk of X assumes that A holds&quot;.  Suppose A 
<br>
holds.  It can happen, you know, you even called it &quot;most likely&quot;.  If so, 
<br>
then you can't factor the doubt of A independently into your plans for X, 
<br>
Y, Z that follow from A.  When you're developing your conditional plan for 
<br>
dealing with A, you have to take A as an accomplished fact so that you 
<br>
don't think that P(X|A) is necessarily at least as uncertain as P(A).
<br>
<p>When we plan for that &quot;most likely&quot; eventuality, let us take for granted 
<br>
that AI has an impact large enough to rewrite the solar system in arbitrary 
<br>
ways conforming to the (actual) laws of physics.  Let us not repeat our 
<br>
uncertainty on this point over and over when we are developing contingency 
<br>
plans for dealing with that &quot;most likely&quot; outcome.  We take that 
<br>
uncertainty into account once, at the beginning, when we weight the 
<br>
importance of the contingency plan; and that you already called &quot;most 
<br>
likely&quot;.  You may sound crazier to venture capitalists, VCs to whom the 
<br>
point is still shocking, if you don't repeat your uncertainty over and over 
<br>
again to reassure them you're not a cultist.  But if you want to follow 
<br>
Bayesian decision theory - if you want to arrive at a rational plan - you 
<br>
can't factor the uncertainty of A multiple times into plans that depend on 
<br>
P(X|A), P(Y|A), and P(Z|A).
<br>
<p>What I'm trying to avoid here is the tendency of people to say:  &quot;Well, 
<br>
suppose that this coin is 90% biased towards heads.  In that contingency, 
<br>
should we bet on the next three flips coming up HHH?  It might seem like we 
<br>
should, but consider:  On the first round, the coin is very likely to come 
<br>
up H, but only if the coin is indeed biased, which is very uncertain.  Then 
<br>
on the second round, it might come up H again, but again, this depends on 
<br>
the coin being biased.  And the third round has the same objection.  So we 
<br>
can see that HHH is actually an exceedingly unlikely outcome, even though 
<br>
I've said it's possible and even probable that the coin is biased.&quot;
<br>
<p>I sometimes look on people's thoughts and think that they have only a 
<br>
limited supply of daring.  Once an AI academic uses up all their daring on 
<br>
suggesting that human-equivalent intelligence is possible in the next 
<br>
thirty years, they can't consider further daring thoughts like transhuman 
<br>
intelligence in the next 20 years, recursive self-improvement in the next 
<br>
10 years, FAI being more difficult than AI, or the human species being in 
<br>
imminent danger of extinction.  They've used up their daring and won't be 
<br>
able to say anything future-shocky until they refill their daring tank at a 
<br>
daring station.
<br>
<p>To arrive at the correct answer means arriving at some theory that runs on 
<br>
its own rails to a prediction, regardless of what the prediction &quot;sounds 
<br>
like&quot;.  Not all theories with this property are correct, but all correct 
<br>
theories have this property.
<br>
<p><em>&gt; Any transhuman intelligence, of course, represents an absolute
</em><br>
<em>&gt; departure from human prediction,
</em><br>
<p>I used to go around saying that, but that was when intelligence was a 
<br>
sacred mystery to me.  If I set an FAI in motion, it will be because I have 
<br>
made some kind of very strong prediction about the FAI's effects - even if 
<br>
that prediction is abstract, it will be a prediction; it will constrain the 
<br>
space of outcomes.
<br>
<p><em>&gt; but for the time being, let us speak of what we can.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The unfortunate thing, from my point of view, is that generating a
</em><br>
<em>&gt; conservative estimation of the economic impact of AI is nearly
</em><br>
<em>&gt; impossible.
</em><br>
<p>Indeed so.
<br>
<p><em>&gt; It pre-supposes several things.
</em><br>
<p><em>&gt; -First, that AI impacts economically before it changes the entire
</em><br>
<em>&gt; landscape, this seems quite possible, AI will take some time to
</em><br>
<em>&gt; develop, and even once complete will require some time to run. Even if
</em><br>
<em>&gt; it's just inflation hitting the roof as everyone with any money does
</em><br>
<em>&gt; whatever they think will avert the apocalypse during the last week of
</em><br>
<em>&gt; the Final Program, that counts as economic impact.
</em><br>
<p>Sounds like a catastrophic scenario that it becomes your responsibility to 
<br>
avoid, if you fulfill your dream of having a say in the matter.  Besides, 
<br>
the Final Week isn't the same as the five-year runup or twenty-year gradual 
<br>
economic development that people like to fantasize about.
<br>
<p><em>&gt; -Second, that there is some period of stability in the development of
</em><br>
<em>&gt; AI that allows for AI 'products' to be evaluated in terms of
</em><br>
<em>&gt; relatively cognizant economic terms. This is very tricky. It has been
</em><br>
<em>&gt; popularly supposed by some that human-commensurate intelligence
</em><br>
<em>&gt; represents the top level, or a hard barrier, that AI research will
</em><br>
<em>&gt; continue to that point and then stop, or at least be slowed. It is
</em><br>
<em>&gt; likely that a certain level of intelligence represents the maximum
</em><br>
<em>&gt; effective potential of a particular design, due to scaling laws,
</em><br>
<em>&gt; architectural support requirements, or flaws in the design to start
</em><br>
<em>&gt; with. Unfortunately, an AI will not be using the same design as a
</em><br>
<em>&gt; human. It is, in my estimation, just as likely to top out at the
</em><br>
<em>&gt; commensurate intelligence of a mouse, or a dolphin, or so far above us
</em><br>
<em>&gt; that the intelligence is not measurable. It seems clear to me that
</em><br>
<em>&gt; minds need not follow a uniform plan with uniform strengths, although
</em><br>
<em>&gt; they may be very correlated. This makes design-independent analysis
</em><br>
<em>&gt; complicated.
</em><br>
<p>Okay, this follows LOGI so far, but...
<br>
<p><em>&gt; Some hope in the form of computer power requirements, assuming
</em><br>
<em>&gt; biologicals and previous experience with unintelligent mechanical
</em><br>
<em>&gt; computation hold, the physical task of running an intelligence may
</em><br>
<em>&gt; limit it to certain levels of potential until larger/faster computers
</em><br>
<em>&gt; can be built.  Unfortunately even Kurzweil's rather charming little
</em><br>
<em>&gt; graphs
</em><br>
<p>(What about Ilkka Tuomi's countergraphs?)
<br>
<p><em>&gt; give us little time before available computation far outstrips
</em><br>
<em>&gt; human level, leaving us in the same boat. The stability given us there
</em><br>
<em>&gt; is fleeting, but does allow enough years to be evaluated on the
</em><br>
<em>&gt; economic scale.
</em><br>
<p>And here we see the start of what leapt out at me as the chief mistake in 
<br>
the whole analysis - ignoring the possibility of recursive 
<br>
self-improvement.  If AI scales nicely and neatly with the speed of 
<br>
human-produced computing power, then you can have a nice little decade when 
<br>
AI makes money.  If the AI reaches an infrahuman threshold level where RSI 
<br>
becomes tractable and then immediately goes FOOM, this decade does not 
<br>
exist.  If the AI reaches a threshold level and absorbs the Internet, it 
<br>
goes FOOM.  If the AI reaches a threshold level and makes a billion dollars 
<br>
and the directors of your company decide to buy it a shiny new 
<br>
supercomputer, it goes FOOM.
<br>
<p>If you don't point this out to venture capitalists, they won't point it out 
<br>
for you.  You can fool venture capitalists, if that is your wish.
<br>
<p>But you wrote a whole essay about the &quot;economic potential&quot; of AI, and you 
<br>
didn't say anything about recursive self-improvement.  That's a pretty 
<br>
severe omission.  This is the blindness that comes of turning your focus of 
<br>
attention to persuading venture capitalists.
<br>
<p><em>&gt; -Third, that our status, as AI researchers and developers, will give
</em><br>
<em>&gt; us a privileged and controllable stake in the construction and
</em><br>
<em>&gt; deployment of AI products and resources, allowing us to capitalize on
</em><br>
<em>&gt; our investment, as per the standard industrial research model. This
</em><br>
<em>&gt; seems fairly safe, until one realizes that there are many forces that
</em><br>
<em>&gt; oppose such status, merely because of the nature of AI. Governments
</em><br>
<em>&gt; may not allow technology of this kind to remain concentrated in the
</em><br>
<em>&gt; hands of private corporations. AI may follow the same path as other
</em><br>
<em>&gt; technologies, with many parallel breakthroughs at the same time,
</em><br>
<em>&gt; leaving us as merely members of a population of AI projects suddenly
</em><br>
<em>&gt; getting results. The information nature of this development increases
</em><br>
<em>&gt; this problem a great deal. I have no reason to imagine that AI
</em><br>
<em>&gt; development requires specialized hardware, or is impossible to employ
</em><br>
<em>&gt; without the experience gained in the research of said AI software. So
</em><br>
<em>&gt; piracy, industrial espionage, and simple reverse-engineering may
</em><br>
<em>&gt; render our position very tenuous indeed.
</em><br>
<p>&quot;Tenuous&quot; is an interesting word for the uncontrolled proliferation of a 
<br>
technology easily capable of wiping out the world.  Or worse, but that 
<br>
still seems to me unlikely - one of the technology thiefs would have to 
<br>
master true FAI techniques for that.
<br>
<p><em>&gt; I have no easy answers for this assumption,
</em><br>
<p>I don't intend to build technology that is user-friendly enough to be 
<br>
stolen, or maybe &quot;kidnapped&quot; would be a better word.  An FAI build solely 
<br>
for one purpose will not easily be turned to another.  If I steal Tolkien's 
<br>
original manuscript of _Lord of the Rings_, it doesn't mean that I can flip 
<br>
a little built-in switch to make a Gray Lensman the hero instead of Frodo - 
<br>
not unless I'm a good enough author to rewrite the novel from scratch.
<br>
<p>Though the FAI-nappers might still be able to blow up the world.
<br>
<p><em>&gt; save that while worrying, little evidence exists
</em><br>
<em>&gt; either way. I personally believe that our position is privileged and
</em><br>
<em>&gt; will remain so until the formation of other AI projects with
</em><br>
<em>&gt; commensurate theory, developed technology, and talent, at that point
</em><br>
<em>&gt; it becomes more problematic.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Assuming we have answers to all these questions, we may find that AI
</em><br>
<em>&gt; is indeed a good way to make money, or at least in the near term.
</em><br>
<p>Congratulations on persuading the venture capitalists you might make money! 
<br>
&nbsp;&nbsp;What about the survival of the human species?  Is that being considered? 
<br>
&nbsp;&nbsp;Whoops, too late!  It's too late to introduce that consideration into 
<br>
your essay.  It can't be pasted on afterward.  If you were going to care, 
<br>
the time to care was right at the beginning, before you knew what your plan 
<br>
would be.  Now all you can do is rationalize a consideration that wasn't 
<br>
there when the actual plan was devised.  Unless you actually change your 
<br>
plan in some way to accomodate the new requirement, but I've never seen any 
<br>
other AI researcher do that so I'm not spending much time hoping for it.
<br>
<p><em>&gt; I have a story I can tell here, but the supporting evidence is
</em><br>
<em>&gt; abstract, and indirect. Artificial Intelligence is likely, in my
</em><br>
<em>&gt; opinion, to follow an accelerating series of plateaus of development,
</em><br>
<em>&gt; starting with the low animal intelligence which is the focus of our
</em><br>
<em>&gt; research now. Progress will be slow, and spin off products limited in
</em><br>
<em>&gt; their scope. As intelligence increases, the more significant
</em><br>
<em>&gt; bottleneck will be trainability and transfer of learned content
</em><br>
<em>&gt; between AIs. This period represents the most fruitful opportunity for
</em><br>
<em>&gt; standard economic gain. The AI technology at this point will create
</em><br>
<em>&gt; three divisions across most industry, in terms of decision technology.
</em><br>
<em>&gt; You will have tasks that require human decision-making, tasks that can
</em><br>
<em>&gt; be fully mechanized, performed by standard programmatic
</em><br>
<em>&gt; approaches(normal coding, specialized hardware, special purpose
</em><br>
<em>&gt; products), and a new category, AI decision-making. This will be any
</em><br>
<em>&gt; task too general or too expensive to be solved algorithmically, and
</em><br>
<em>&gt; not complex enough to require human intervention. Both borders will
</em><br>
<em>&gt; expand, as it gets cheaper to throw AI at the problem than to go
</em><br>
<em>&gt; through and solve it mechanically, and as the upper bound of decision
</em><br>
<em>&gt; making gets more and more capable.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'm afraid I have no real evidence as to how long this period will
</em><br>
<em>&gt; last. It depends entirely on the difficulty of increasing the
</em><br>
<em>&gt; intelligence of the AI, which may reside in design, hardware, and to a
</em><br>
<em>&gt; certain extent, motivation(goal systems are a thesis in themselves,
</em><br>
<em>&gt; ask EY).  I suspect, based on my experiences thus far, that early AI
</em><br>
<em>&gt; designs will be very lossy and faulty and poorly optimized for
</em><br>
<em>&gt; increasing in intelligence.
</em><br>
<p>I used to think like that, before I understood that FAI had to be done 
<br>
deliberately rather than by accident, which meant using only algorithms 
<br>
where I understood why they worked, working out a complete and principled 
<br>
theoretical framework into which everything would need to fit, and avoiding 
<br>
probabilistic self-modification.
<br>
<p>I yearn for the old days when I thought I could just throw cool-seeming 
<br>
algorithms at the problem, but that's what makes FAI harder than AI.
<br>
<p><em>&gt; This may mean that a complete redesign of
</em><br>
<em>&gt; AI theory will be necessary to get to the next series of plateaus.
</em><br>
<em>&gt; Unless this is simply beyond human capability, there is no reason to
</em><br>
<em>&gt; think this will take any longer than the development of AI theory
</em><br>
<em>&gt; sufficient to get us to this point.
</em><br>
<p>Recursive self-improvement seems to be missing in this discussion.  Just a 
<br>
band of humans gradually improving an AI that slowly acquires more and more 
<br>
abilities.  It makes for a nice fantasy of slow, relatively safe 
<br>
transcendence where you always have plenty of time to see threats coming 
<br>
before they hit.
<br>
<p>In LOGI, there's also a discussion of plateaus and breakthroughs, but it 
<br>
forms the background of a graph of ability against 
<br>
efficiency/hardware/knowledge, *NOT* a graph of ability against *time*. 
<br>
The graph against *time* has to take into account what happens when we fold 
<br>
the prior graph in on itself to describe recursively self-improving AI.
<br>
<p><em>&gt; Sometime after this, economic aspirations become fleeting in the
</em><br>
<em>&gt; general upheaval and reconstitution caused by the arrival of another
</em><br>
<em>&gt; kind of intelligence. Some might say this is rather the point of AI
</em><br>
<em>&gt; research.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Projecting into the future is always dangerous. I think that any
</em><br>
<em>&gt; attempt, especially the one above, to characterize the trajectory of
</em><br>
<em>&gt; any technology is doomed to be largely irrelevant. But some choices
</em><br>
<em>&gt; must be made on best available guesses, so here are mine. AI research
</em><br>
<em>&gt; will change a lot of things. In the near term, it will remain a fringe
</em><br>
<em>&gt; activity, and people will still ask the strange question 'what will
</em><br>
<em>&gt; those AIs be good for, anyway?'. But some investors will come, and the
</em><br>
<em>&gt; clearest way I can communicate with them what the goals and value of
</em><br>
<em>&gt; AI research is that it is vastly enabling. I don't know what the first
</em><br>
<em>&gt; task an AI will perform is. I know that it will be something that
</em><br>
<em>&gt; can't be done with anything else. It represents, in the near term, an
</em><br>
<em>&gt; investment in future capability. If money is what you're after
</em><br>
<em>&gt; primarily, I don't know how to defend an investment in AI research
</em><br>
<em>&gt; from the perspective of, say, venture capital. I can point to examples
</em><br>
<em>&gt; of enabling technology, like CAD, or tooling, or electrical power,
</em><br>
<em>&gt; which did not fit into the world they arrived in, but created their
</em><br>
<em>&gt; own industries.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'm not saying I can't make up clever uses for AI technologies that
</em><br>
<em>&gt; could make a gazillion dollars, if I had designs for them in my hand.
</em><br>
<em>&gt; There are obvious and clear storytelling ideas. But that would be
</em><br>
<em>&gt; intellectually dishonest.
</em><br>
<p>*nods*
<br>
<p>I wish this were more widely appreciated.  And I'm sorry that rationalists 
<br>
must be penalized for knowing what constitutes a lie.
<br>
<p><em>&gt; I'm looking for a way to express, in terms
</em><br>
<em>&gt; of investment return, what AI is likely to actually do
</em><br>
<p>Turn the solar system into paperclips.
<br>
<p><em>&gt; for us,
</em><br>
<p>*That* takes a little more work.
<br>
<p><em>&gt; in a conservative, defensible sense.
</em><br>
<p>Sometimes the most factually likely outcome is just not something that 
<br>
sounds all nicey-nicey and sane to the unenlightened.  You want to try 
<br>
drawing a &quot;conservative, defensible&quot; picture of the early 21st century in 
<br>
17th-century England?
<br>
<p><em>&gt; This must be separated from, for example, safety concerns, in which it
</em><br>
<em>&gt; is perhaps useful to imagine, as some do on this forum, what the
</em><br>
<em>&gt; failure modes, what the fastest take off, what the actual capability
</em><br>
<em>&gt; of such developments may be. That isn't helpful in this kind of
</em><br>
<em>&gt; planning.
</em><br>
<p>This sounds like a complete non-sequitur to me.  I'm planning to prevent 
<br>
the solar system from being turned into paperclips.  I don't know of any 
<br>
other consideration that ought to be overriding that.  It sounds to me like 
<br>
you just took all the obvious considerations that would scrap your essay, 
<br>
and quickly tried to shove them under the carpet by saying, &quot;But that isn't 
<br>
helpful in this kind of planning.&quot;  My good sir, what are you planning to 
<br>
do, and why should anyone help you with it, if safety concerns (not having 
<br>
your AI turn the solar system into paperclips) and failure modes (you 
<br>
seriously think you can get away with not thinking about those, in *any* 
<br>
essay?) and recursive self-improvement (which you didn't even mention) 
<br>
don't enter into it?
<br>
<p>It sounds like you already know why your essay is indefensible, but you 
<br>
think that if you admit it really quickly and move on immediately, you 
<br>
won't have to notice.
<br>
<p>I am genuinely confused about what you regard as the point of your essay. 
<br>
Is it to persuade venture capitalists of a conclusion beneficial to you, 
<br>
predetermined before you started writing the essay?  Is it to determine the 
<br>
most likely answer on a question of fact on which you are presently 
<br>
uncertain?  Is it to advocate an alternate strategy, compared to the 
<br>
Singularity Institute or some other line of thinking?  It seems that I'm 
<br>
hearing considerations that would be appropriate to all three purposes. 
<br>
But the first consideration, at least, is something that shouldn't mix with 
<br>
considerations two and three.  If you're thinking about how to persuade 
<br>
venture capitalists that they'll make money, you'd better not let even a 
<br>
shred of thought carry over from that to your Singularity planning.
<br>
<p><em>&gt; I must anticipate a response suggesting that non-profit, private
</em><br>
<em>&gt; efforts to research AI, such as the Singularity Institute, AGIRI, etc
</em><br>
<em>&gt; are better suited for this subject matter, and in fact invalidate my
</em><br>
<em>&gt; queries as relevant at all. I remain very doubtful that this is the
</em><br>
<em>&gt; case. AI is not something to be solved quickly, nor something to be
</em><br>
<em>&gt; solved with few people with no money.
</em><br>
<p>Here, for example, is an answer that sounds like it's appropriate to 
<br>
arguing for some other strategy than the Singularity Institute pursues. 
<br>
It's a non-sequitur to the previous paragraph, about why you don't need to 
<br>
worry about safety concerns - something I'm not clear on, no matter what 
<br>
you think you're arguing.  That wasn't a request for you to think quickly 
<br>
and rationalize a better reason, by the way, it was a request for you to 
<br>
give up the fight and start thinking about inconvenient safety questions. 
<br>
If you believe AI is not something that can be solved with few people, then 
<br>
figure out a way to do it safely with many people.  Don't defend your plan 
<br>
by claiming that someone else's plan is worse.  Your plan has to work for 
<br>
itself, regardless of what anyone else is doing.  As I keep repeating to 
<br>
myself, having learned the lesson the hard way, &quot;Being the best counts for 
<br>
absolutely nothing; you have to be adequate, which is much harder.&quot;  This 
<br>
is another fallacy that comes of arguing with people - for if you want 
<br>
funding, you will argue that you are the best effort.  Realistically, mere 
<br>
comparison is probably the most complex issue that we can hope to discuss 
<br>
on medium as mailing lists.  But it doesn't change the necessity of 
<br>
adequacy.  You can't cover a problem in your own project by saying that 
<br>
someone else has a different problem.
<br>
<p>Oh, and another lesson I learned the hard way:  You can't say how difficult 
<br>
a problem is to solve unless you know exactly how to solve it.  Saying, 
<br>
&quot;This problem is NP-complete&quot; means it's knowably hard (assuming P!=NP), 
<br>
requires at least X cycles to compute.  Saying &quot;this problem mystifies and 
<br>
baffles me&quot; does not license you to estimate the number of people required 
<br>
to implement a solution once you have one.
<br>
<p><em>&gt; AI is in its first stages of
</em><br>
<em>&gt; real development, and a massive amount of research and data needs to
</em><br>
<em>&gt; be collected, if AI theories are to be informed by more than
</em><br>
<em>&gt; introspection and biological analogue.
</em><br>
<p>If you're not familiar with the massive amount of research and data already 
<br>
gathered, what's the use of asking for more?  We already have more research 
<br>
and data than a human being could assimilate in a lifetime.  I have studied 
<br>
a narrow fraction of existing knowledge which is nonetheless pretty damn 
<br>
wide by academic standards, and I have found that sufficient to my needs.
<br>
<p><em>&gt; Like so many things in our
</em><br>
<em>&gt; modern world, AI will be done long before we can properly evaluate and
</em><br>
<em>&gt; prepare ourselves for the results, however long it takes.
</em><br>
<p>Who's &quot;we&quot;?  Eliezer Yudkowsky?  Leon Kass?
<br>
<p>This sort of plaint is not an excuse.  Prepare or die.  No, it's not easy. 
<br>
&nbsp;&nbsp;Do it anyway.  Am I the only AI wannabe who knows this?
<br>
<p><em>&gt; But people
</em><br>
<em>&gt; need to have reasons to join AI efforts, to fund them, and to support
</em><br>
<em>&gt; them, in levels thus far not seen. I submit this is at least partially
</em><br>
<em>&gt; because this kind of analysis is either not publicised, or has simply
</em><br>
<em>&gt; not been done.
</em><br>
<p>We'll see how much funding your essay generates for A2I2, but I'm betting 
<br>
on not much.  For one thing, it was aimed at an audience that, you seem to 
<br>
think, currently assumes a hard takeoff; you spend most of your essay 
<br>
defending the assertion that you'll have enough of a breathing space to 
<br>
make a profit.
<br>
<p>It's not clear to me whether you're trying to write persuasively or settle 
<br>
a question to fact.  In either case, it seems that you're appealing to the 
<br>
need to persuade people, or something, to explain why you're ignoring 
<br>
safety concerns.  You say, &quot;No one can prepare&quot;, then, &quot;But people need to 
<br>
have reasons to join AI efforts&quot;.  Why does sentence B follow sentence A? 
<br>
Why do you presume your audience already agrees that people need reasons to 
<br>
join (your) AI effort, and that this is an acceptable justification for... 
<br>
what?
<br>
<p>Who are you trying to persuade?  Of what?  This essay's point is unclear.
<br>
<p><em>&gt; This kind of analysis also raises the rather uncomfortable spectre of
</em><br>
<em>&gt; doubt, that I have jumped into a field of study without sufficient
</em><br>
<em>&gt; research and investigation, or have unrealistic (at least ungrounded)
</em><br>
<em>&gt; expectations for the fruits of my work. I submit that my primary
</em><br>
<em>&gt; interest in AI is at least partially unrelated to gain of these kinds,
</em><br>
<em>&gt; and secondarily informed by the safety concerns, asymmetric potential,
</em><br>
<em>&gt; and increasing importance investigated much more clearly by other
</em><br>
<em>&gt; authors (Vinge, Yudkowsky, Good).
</em><br>
<p>SECONDARILY informed?  What in Belldandy's name is your PRIMARY concern?
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10489.html">Yan King Yin: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Previous message:</strong> <a href="10487.html">BillK: "Google Directory AI links"</a>
<li><strong>In reply to:</strong> <a href="10485.html">justin corwin: "Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10490.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Reply:</strong> <a href="10490.html">Russell Wallace: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10488">[ date ]</a>
<a href="index.html#10488">[ thread ]</a>
<a href="subject.html#10488">[ subject ]</a>
<a href="author.html#10488">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
