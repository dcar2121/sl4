<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Conservative Estimation of the Economic Impact of Artificial Intelligence</title>
<meta name="Author" content="justin corwin (outlawpoet@gmail.com)">
<meta name="Subject" content="Conservative Estimation of the Economic Impact of Artificial Intelligence">
<meta name="Date" content="2004-12-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Conservative Estimation of the Economic Impact of Artificial Intelligence</h1>
<!-- received="Tue Dec 28 04:29:15 2004" -->
<!-- isoreceived="20041228112915" -->
<!-- sent="Tue, 28 Dec 2004 03:29:12 -0800" -->
<!-- isosent="20041228112912" -->
<!-- name="justin corwin" -->
<!-- email="outlawpoet@gmail.com" -->
<!-- subject="Conservative Estimation of the Economic Impact of Artificial Intelligence" -->
<!-- id="3ad827f30412280329221f0922@mail.gmail.com" -->
<!-- charset="US-ASCII" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> justin corwin (<a href="mailto:outlawpoet@gmail.com?Subject=Re:%20Conservative%20Estimation%20of%20the%20Economic%20Impact%20of%20Artificial%20Intelligence"><em>outlawpoet@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Dec 28 2004 - 04:29:12 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10486.html">Ben Goertzel: "RE: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Previous message:</strong> <a href="10484.html">Thomas Buckner: "Santa Bot flunks the Turing test (humor)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10486.html">Ben Goertzel: "RE: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Reply:</strong> <a href="10486.html">Ben Goertzel: "RE: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Reply:</strong> <a href="10488.html">Eliezer Yudkowsky: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Maybe reply:</strong> <a href="10489.html">Yan King Yin: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Reply:</strong> <a href="../0501/10522.html">Stephen Reed: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10485">[ date ]</a>
<a href="index.html#10485">[ thread ]</a>
<a href="subject.html#10485">[ subject ]</a>
<a href="author.html#10485">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
In my work, I concentrate largely on the practical, necessary research
<br>
to allow us to build an artificial intelligence. It is the central
<br>
fascination of most of the people in my company. I would imagine that
<br>
most people in this community have great interest(if not actual
<br>
involvement) in the research, development, and application of novel
<br>
intelligent systems, IA, AI, or otherwise.
<br>
<p>However, in the process of explaining my involvement to other people,
<br>
as well as attempting to instill enthusiasm (or at least explain
<br>
potential impact) for my research, I've been encountering the
<br>
following baffling response with increasing frequency. My fearless
<br>
leader here at A2I2, Peter Voss, has been encountering the same thing.
<br>
<p>&quot;How do you intend to productize your research, what is it that you
<br>
expect to come out of this company, and how can you make money?&quot;
<br>
<p>I would like to take a moment to say that I am only indirectly
<br>
interested in money. My involvement in A2I2 and AI research in general
<br>
is motivated both by interest and idealism, with a backdrop of
<br>
pragmatic analysis. AI is a very large lever. It will count, in game
<br>
theoretic terms, an inconceivable amount, before changing the
<br>
landscape entirely. Money, for me, is just a poorly normalized utility
<br>
token, and inasmuch as I am interested in it, I expect to be more than
<br>
able to backwards exchange the influence I have on the creation of AI
<br>
for it. Digression ended.
<br>
<p>What these people are really asking, via the agency of hypothetical
<br>
investors, is &quot;what is AI good for, and how can that be delivered from
<br>
your research.&quot;
<br>
<p>Initially, much like Peter had in previous discussion, I discounted
<br>
this reaction as simply the miscategorization of AI as 'yet another
<br>
techno-widget', to be judged in the marketplace and priced for proper
<br>
placement on shelves. It has become clear to me, however, that it runs
<br>
somewhat deeper than this.
<br>
<p>Artificial Intelligence, even very weakly achieved, is not just
<br>
another technology. It represents, at the very least, a complete
<br>
industry, and most likely, is one of those events that redefines the
<br>
landscape of human activity.
<br>
<p>Any transhuman intelligence, of course, represents an absolute
<br>
departure from human prediction, but for the time being, let us speak
<br>
of what we can.
<br>
<p>The unfortunate thing, from my point of view, is that generating a
<br>
conservative estimation of the economic impact of AI is nearly
<br>
impossible. It pre-supposes several things.
<br>
<p>-First, that AI impacts economically before it changes the entire
<br>
landscape, this seems quite possible, AI will take some time to
<br>
develop, and even once complete will require some time to run. Even if
<br>
it's just inflation hitting the roof as everyone with any money does
<br>
whatever they think will avert the apocalypse during the last week of
<br>
the Final Program, that counts as economic impact.
<br>
<p>-Second, that there is some period of stability in the development of
<br>
AI that allows for AI 'products' to be evaluated in terms of
<br>
relatively cognizant economic terms. This is very tricky. It has been
<br>
popularly supposed by some that human-commensurate intelligence
<br>
represents the top level, or a hard barrier, that AI research will
<br>
continue to that point and then stop, or at least be slowed. It is
<br>
likely that a certain level of intelligence represents the maximum
<br>
effective potential of a particular design, due to scaling laws,
<br>
architectural support requirements, or flaws in the design to start
<br>
with. Unfortunately, an AI will not be using the same design as a
<br>
human. It is, in my estimation, just as likely to top out at the
<br>
commensurate intelligence of a mouse, or a dolphin, or so far above us
<br>
that the intelligence is not measurable. It seems clear to me that
<br>
minds need not follow a uniform plan with uniform strengths, although
<br>
they may be very correlated. This makes design-independent analysis
<br>
complicated.
<br>
<p>Some hope in the form of computer power requirements, assuming
<br>
biologicals and previous experience with unintelligent mechanical
<br>
computation hold, the physical task of running an intelligence may
<br>
limit it to certain levels of potential until larger/faster computers
<br>
can be built.  Unfortunately even Kurzweil's rather charming little
<br>
graphs give us little time before available computation far outstrips
<br>
human level, leaving us in the same boat. The stability given us there
<br>
is fleeting, but does allow enough years to be evaluated on the
<br>
economic scale.
<br>
<p>-Third, that our status, as AI researchers and developers, will give
<br>
us a privileged and controllable stake in the construction and
<br>
deployment of AI products and resources, allowing us to capitalize on
<br>
our investment, as per the standard industrial research model. This
<br>
seems fairly safe, until one realizes that there are many forces that
<br>
oppose such status, merely because of the nature of AI. Governments
<br>
may not allow technology of this kind to remain concentrated in the
<br>
hands of private corporations. AI may follow the same path as other
<br>
technologies, with many parallel breakthroughs at the same time,
<br>
leaving us as merely members of a population of AI projects suddenly
<br>
getting results. The information nature of this development increases
<br>
this problem a great deal. I have no reason to imagine that AI
<br>
development requires specialized hardware, or is impossible to employ
<br>
without the experience gained in the research of said AI software. So
<br>
piracy, industrial espionage, and simple reverse-engineering may
<br>
render our position very tenuous indeed. I have no easy answers for
<br>
this assumption, save that while worrying, little evidence exists
<br>
either way. I personally believe that our position is privileged and
<br>
will remain so until the formation of other AI projects with
<br>
commensurate theory, developed technology, and talent, at that point
<br>
it becomes more problematic.
<br>
<p>Assuming we have answers to all these questions, we may find that AI
<br>
is indeed a good way to make money, or at least in the near term.
<br>
<p>I have a story I can tell here, but the supporting evidence is
<br>
abstract, and indirect. Artificial Intelligence is likely, in my
<br>
opinion, to follow an accelerating series of plateaus of development,
<br>
starting with the low animal intelligence which is the focus of our
<br>
research now. Progress will be slow, and spin off products limited in
<br>
their scope. As intelligence increases, the more significant
<br>
bottleneck will be trainability and transfer of learned content
<br>
between AIs. This period represents the most fruitful opportunity for
<br>
standard economic gain. The AI technology at this point will create
<br>
three divisions across most industry, in terms of decision technology.
<br>
You will have tasks that require human decision-making, tasks that can
<br>
be fully mechanized, performed by standard programmatic
<br>
approaches(normal coding, specialized hardware, special purpose
<br>
products), and a new category, AI decision-making. This will be any
<br>
task too general or too expensive to be solved algorithmically, and
<br>
not complex enough to require human intervention. Both borders will
<br>
expand, as it gets cheaper to throw AI at the problem than to go
<br>
through and solve it mechanically, and as the upper bound of decision
<br>
making gets more and more capable.
<br>
<p>I'm afraid I have no real evidence as to how long this period will
<br>
last. It depends entirely on the difficulty of increasing the
<br>
intelligence of the AI, which may reside in design, hardware, and to a
<br>
certain extent, motivation(goal systems are a thesis in themselves,
<br>
ask EY).  I suspect, based on my experiences thus far, that early AI
<br>
designs will be very lossy and faulty and poorly optimized for
<br>
increasing in intelligence. This may mean that a complete redesign of
<br>
AI theory will be necessary to get to the next series of plateaus.
<br>
Unless this is simply beyond human capability, there is no reason to
<br>
think this will take any longer than the development of AI theory
<br>
sufficient to get us to this point.
<br>
<p>Sometime after this, economic aspirations become fleeting in the
<br>
general upheaval and reconstitution caused by the arrival of another
<br>
kind of intelligence. Some might say this is rather the point of AI
<br>
research.
<br>
<p>Projecting into the future is always dangerous. I think that any
<br>
attempt, especially the one above, to characterize the trajectory of
<br>
any technology is doomed to be largely irrelevant. But some choices
<br>
must be made on best available guesses, so here are mine. AI research
<br>
will change a lot of things. In the near term, it will remain a fringe
<br>
activity, and people will still ask the strange question 'what will
<br>
those AIs be good for, anyway?'. But some investors will come, and the
<br>
clearest way I can communicate with them what the goals and value of
<br>
AI research is that it is vastly enabling. I don't know what the first
<br>
task an AI will perform is. I know that it will be something that
<br>
can't be done with anything else. It represents, in the near term, an
<br>
investment in future capability. If money is what you're after
<br>
primarily, I don't know how to defend an investment in AI research
<br>
from the perspective of, say, venture capital. I can point to examples
<br>
of enabling technology, like CAD, or tooling, or electrical power,
<br>
which did not fit into the world they arrived in, but created their
<br>
own industries.
<br>
<p>I'm not saying I can't make up clever uses for AI technologies that
<br>
could make a gazillion dollars, if I had designs for them in my hand.
<br>
There are obvious and clear storytelling ideas. But that would be
<br>
intellectually dishonest. I'm looking for a way to express, in terms
<br>
of investment return, what AI is likely to actually do for us, in a
<br>
conservative, defensible sense.
<br>
<p>This must be separated from, for example, safety concerns, in which it
<br>
is perhaps useful to imagine, as some do on this forum, what the
<br>
failure modes, what the fastest take off, what the actual capability
<br>
of such developments may be. That isn't helpful in this kind of
<br>
planning.
<br>
<p>I must anticipate a response suggesting that non-profit, private
<br>
efforts to research AI, such as the Singularity Institute, AGIRI, etc
<br>
are better suited for this subject matter, and in fact invalidate my
<br>
queries as relevant at all. I remain very doubtful that this is the
<br>
case. AI is not something to be solved quickly, nor something to be
<br>
solved with few people with no money. It is in it's first stages of
<br>
real development, and a massive amount of research and data needs to
<br>
be collected,  if AI theories are to be informed by more than
<br>
introspection and biological analogue. Like so many things in our
<br>
modern world, AI will be done long before we can properly evaluate and
<br>
prepare ourselves for the results, however long it takes. But people
<br>
need to have reasons to join AI efforts, to fund them, and to support
<br>
them, in levels thus far not seen. I submit this is at least partially
<br>
because this kind of analysis is either not publicised, or has simply
<br>
not been done.
<br>
<p>...
<br>
<p>This kind of analysis also raises the rather uncomfortable spectre of
<br>
doubt, that I have jumped into a field of study without sufficient
<br>
research and investigation, or have unrealistic(at least ungrounded)
<br>
expectations for the fruits of my work. I submit that my primary
<br>
interest in AI is at least partially unrelated to gain of these kinds,
<br>
and secondarily informed by the safety concerns, asymmetric potential,
<br>
and increasing importance investigated much more clearly by other
<br>
authors(Vinge, Yudkowsky, Good).
<br>
<p>Any responses or questions can be asked on the sl4 mailing list, to
<br>
which this is posted, to me privately, or on my blog.
<br>
<p>Justin Corwin
<br>
<a href="mailto:outlawpoet@hell.com?Subject=Re:%20Conservative%20Estimation%20of%20the%20Economic%20Impact%20of%20Artificial%20Intelligence">outlawpoet@hell.com</a>
<br>
<a href="http://outlawpoet.blogspot.com">http://outlawpoet.blogspot.com</a>
<br>
<a href="http://www.adaptiveai.com">http://www.adaptiveai.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10486.html">Ben Goertzel: "RE: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Previous message:</strong> <a href="10484.html">Thomas Buckner: "Santa Bot flunks the Turing test (humor)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10486.html">Ben Goertzel: "RE: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Reply:</strong> <a href="10486.html">Ben Goertzel: "RE: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Reply:</strong> <a href="10488.html">Eliezer Yudkowsky: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Maybe reply:</strong> <a href="10489.html">Yan King Yin: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<li><strong>Reply:</strong> <a href="../0501/10522.html">Stephen Reed: "Re: Conservative Estimation of the Economic Impact of Artificial Intelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10485">[ date ]</a>
<a href="index.html#10485">[ thread ]</a>
<a href="subject.html#10485">[ subject ]</a>
<a href="author.html#10485">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
