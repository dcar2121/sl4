<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The Future of Human Evolution</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="Re: The Future of Human Evolution">
<meta name="Date" content="2004-10-01">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The Future of Human Evolution</h1>
<!-- received="Fri Oct  1 06:43:17 2004" -->
<!-- isoreceived="20041001124317" -->
<!-- sent="Fri, 1 Oct 2004 13:43:13 +0100 (BST)" -->
<!-- isosent="20041001124313" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="Re: The Future of Human Evolution" -->
<!-- id="20041001124313.59327.qmail@web25302.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="The Future of Human Evolution" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20The%20Future%20of%20Human%20Evolution"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Fri Oct 01 2004 - 06:43:13 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9899.html">Dani Eder: "Re: The Future of Human Evolution"</a>
<li><strong>Previous message:</strong> <a href="../0409/9897.html">Keith Henson: "Re: The Future of Human Evolution"</a>
<li><strong>Maybe in reply to:</strong> <a href="../0409/9854.html">Aleksei Riikonen: "Re: The Future of Human Evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9901.html">Ben Goertzel: "RE: The Future of Human Evolution"</a>
<li><strong>Reply:</strong> <a href="9901.html">Ben Goertzel: "RE: The Future of Human Evolution"</a>
<li><strong>Reply:</strong> <a href="9902.html">Sebastian Hagen: "Re: The Future of Human Evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9898">[ date ]</a>
<a href="index.html#9898">[ thread ]</a>
<a href="subject.html#9898">[ subject ]</a>
<a href="author.html#9898">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Nick Bostrom wrote:
<br>
<em>&gt; On a vaguely related note, I've had a half-written essay lying around 
</em><br>
<em>&gt; since 2001 on how some &quot;upward&quot; evolutionary trajectories could lead to 
</em><br>
<em>&gt; dystopian outcomes, which I've now finally completed: 
</em><br>
<em>&gt; <a href="http://www.nickbostrom.com/fut/evolution.html">http://www.nickbostrom.com/fut/evolution.html</a>.
</em><br>
<p>Thanks for making that available Nick, it was both insightful and
<br>
well-written and covered a large part of what I was trying to convey in
<br>
substantially greater detail. I look forward to your presentation at that
<br>
the ExtroBritannia event next weekend; hopefully I will be able to attend.
<br>
<p>Jeff Medina wrote:
<br>
<em>&gt;&gt; &quot;This is on top of the already known issues with qualia and the
</em><br>
<em>&gt;&gt; illusion of free will; both are results of specific (adaptive)
</em><br>
<em>&gt;&gt; flaws in human introspective capability
</em><br>
<em>&gt;
</em><br>
<em>&gt; Nothing of value is gained in engineering qualia out of the mind.
</em><br>
<em>&gt; The blind are not more evolutionary fit or competitive than the sighted,
</em><br>
<em>&gt; and neither are the color-blind in relation to those with color-vision.
</em><br>
<p>This is incorrect. I characterise qualia and subjectivity in general as
<br>
flaws because they are both irrational (don't map onto a consistent
<br>
Bayesian prior) and based on lack of information (poor human
<br>
introspective capability). Rational utilitarians will always achieve
<br>
their goals more efficiently than irrational agents with ad-hoc decision
<br>
functions over the long run, and complete introspection is actually a
<br>
tremendously powerful ability (it's about a third of the basis for seed
<br>
AI and the Singularity in the first place). Restricting the design of an
<br>
intelligent agent to be based on what humans would characterise as
<br>
subjective experience will make it substantially, possibly massively,
<br>
less efficient than a normative design with the same computing resources.
<br>
<p><em>&gt; The only examples I can imagine that might lead someone to think
</em><br>
<em>&gt; qualia were a bad thing are ones which mention a negative result of a
</em><br>
<em>&gt; phenomenal experience, such as cognitive processing severely hampered
</em><br>
<em>&gt; by pain, loss of efficiency resulting from an inexplicable desire to
</em><br>
<em>&gt; stare at shiny objects, and the like.
</em><br>
<p>That's lousy cognitive design /on top of/ the restrictions imposed by
<br>
subjectivity in the first place. My above statement holds for even
<br>
renormalised transhumans; the brain is a horribly computationally
<br>
inefficient and broken design (evolution is not your friend). I take
<br>
comfort in the fact that there is massive, possibly indefinite, scope
<br>
for improvement without removing subjectivity (even if it will never
<br>
be as efficient as normative reasoning).
<br>
<p><em>&gt; But this sort of example misappropriates responsibility to qualia
</em><br>
<em>&gt; for the unwanted result, when the problem is the cognitive
</em><br>
<em>&gt; association between a particular phenomenal experience and a
</em><br>
<em>&gt; particular negative behavior or loss of ability.
</em><br>
<p>That statement is a Gordian knot of philosophical and cognitive
<br>
confusion. Briefly, you can't reengineer associations without changing
<br>
the qualia involved; the associations are a large part of the
<br>
definition. Nor are the consequences of such engineering reliably
<br>
constrained to forward propagation, at least not on the current 
<br>
amalgamation of nasty hacks we call the human brain. That's not a
<br>
reason not to do it; it's just a reason to be absolutely sure what
<br>
you're doing before you start self-improving. Regardless messing about
<br>
with qualia isn't the same as removing qualia and no matter how much
<br>
you rearrange the latter will always be more efficient and effective.
<br>
Right now I don't know if merely taking qualia out of the primary
<br>
(but not secondary-introspective) reasoning path counts as 'removing';
<br>
it's hard to envision perfect rationalists at the best of times, never
<br>
mind attempting to verify if hybrid reasoners have subjectivity.
<br>
wrong approach.
<br>
<p><em>&gt; Aside: It remains to be seen whether it is even theoretically possible
</em><br>
<em>&gt; to remove phenomenal consciousness from a sufficiently intelligent,
</em><br>
<em>&gt; metacognitive being.
</em><br>
<p>It's possible (if silly), but you're looking at this the wrong way.
<br>
perfect rationalists have /more/ introspective capability and are thus
<br>
more aware of what's going on their own minds than human-like reasoners
<br>
are. We're not talking about stripping out bits of reflectivity here;
<br>
normative reasoners simply don't have the complicated set of mental
<br>
blocks, biases and arbitrary priors that constitutes human subjectivity
<br>
(sensory and introspective qualia, the latter including a whole bundle
<br>
of crazy and often irrational but adaptive self-imaging hacks that
<br>
create the global sensation of a unified volitional self). EvPsych built
<br>
us (from inappropriate parts) to play social games, not solve logic
<br>
problems, and it made a hell of a mess in the process. However one of the
<br>
side-effects was something we consider uniquely valuable, the conscious
<br>
volitional self, so don't be too quick to strip that stuff out and
<br>
re-extrapolate yourself from Eliezer Yudkowsky's pocket calculator.
<br>
<p><em>&gt; Some form of panpsychism may yet be true, meaning qualia may be an
</em><br>
<em>&gt; intrinsic property of all existence, even though it's only recognizable
</em><br>
<em>&gt; by a certain class of mind-like, reflective structure.
</em><br>
<p>I used to be that open minded once, but then I learned that philosophy
<br>
is just confused psychology and cosmology. Repeat after me; 'qualia are
<br>
not ontologically fundamental' (or rather not more ontologically
<br>
fundamental than any other class of causal process).
<br>
<p><em>&gt; Moving beyond &quot;the illusion of free will&quot; requires no engineering on
</em><br>
<em>&gt; the part of transhumans. Current humans who have realized that the
</em><br>
<em>&gt; folk concept of free will (which most closely resembles what is known
</em><br>
<em>&gt; in the philosophical literature as the libertarian view of the will)
</em><br>
<em>&gt; is a necessary impossibility (*) demonstrate all that is needed to
</em><br>
<em>&gt; overcome the illusion is rational reflection.
</em><br>
<p>There is a huge difference between knowing declaratively that free will
<br>
is illusory and having that fact (or rather the absence of declarations
<br>
to the contrary) redundantly and pervasively hardwired into your
<br>
cognitive architecture. I don't know if there are alternative
<br>
interpretations of free will that make more physical sense but can still
<br>
play the same role in building a human-like subjectivity; I hope so, but
<br>
even if there are that would only solve part of the problem.
<br>
<p><em>&gt; because, roughly, if the universe is deterministic, we cannot have
</em><br>
<em>&gt; libertarian free will, our choices being determined by physical law
</em><br>
<em>&gt; and &quot;beyond our control&quot; (according to the folk concept), and if the
</em><br>
<em>&gt; universe is indeterministic, we cannot have libertarian free will,
</em><br>
<em>&gt; because undetermined choices are not determined by our goals,
</em><br>
<em>&gt; preferences, or will.
</em><br>
<p>Congratulations. Now try thinking about global renormalisation of
<br>
probability amplitudes across timeless phase space and free will as
<br>
implicit in the shape of your self-reference class's distribution
<br>
across universes as derived from the root cause for anything existing
<br>
at all (yeah, I'm still working on that one too :).
<br>
<p><em>&gt; Now even assuming we could and did get rid of both qualia and the
</em><br>
<em>&gt; illusion of free will, would this really threaten our moral and legal
</em><br>
<em>&gt; foundations?
</em><br>
<p>Legal systems are a red herring as they will be either nonexistent or
<br>
changed beyond recognition after a Singularity anyway, and legality is
<br>
functionally irrelevant to the desirability of post-Singularity
<br>
outcomes. Speculation on the legal consequences of transhumanism and
<br>
AI is an SL3- topic.
<br>
<p><em>&gt; And qualia? Would you lose your moral sensibilities if you could no
</em><br>
<em>&gt; longer hear or see? Neither would I.
</em><br>
<p>Slow down. If you strip the qualia out of a human being, empathy no
<br>
longer works (abstract cognitive simulation of external agents is still
<br>
an option, given enough computing power, but the link between their
<br>
subjectivity and yours has been cut). This is a /huge/ issue for
<br>
practical morality and since almost all our more abstract morality is
<br>
derived from practical morality it's a major issue full stop. If
<br>
we're going to discriminate between goal-seeking agents on any kind of
<br>
qualitative grounds (if not, you have the same rights as a toaster with
<br>
the same processing power) then we need to define what it is about
<br>
the agent's cognitive architecture that makes them volitional and thus
<br>
morally valuable. For CFAI-era FAI we'd have to define it very, perhaps
<br>
impossibly specifically; CV may allow a working definition at startup
<br>
time.
<br>
<p><em>&gt; There is no reason to think sentient beings are the only objects that
</em><br>
<em>&gt; matter to other sentient beings, and the question dissolves on this
</em><br>
<em>&gt; realization.
</em><br>
<p>No, it doesn't. Morality exists because goal-seeking agents can have
<br>
conflicting goals. It's purpose is to determine who's will takes priority
<br>
whenever a disagreement occurs. Physics provides a simple baseline;
<br>
survival of the fittest, whichever process has more relevant resources
<br>
wins. Morality imposes a different decision function through external
<br>
mechanisms or additional internal goals in order to achieve a more
<br>
globally desirable outcome. It's possible to construct morality just
<br>
from game theory, but humans generally want to see more human-specific
<br>
preference complexity in moral systems they have to live within. The
<br>
problem is that our preferences are defined against features of our
<br>
cognitive architecture that are locally adaptive but generally
<br>
irrational.
<br>
<p>Peter McCluskey wrote:
<br>
<em>&gt; I'm having trouble figuring out why we should worry about this kind of
</em><br>
<em>&gt; problem. We have goal-systems which are sometimes inconsistent, and when
</em><br>
<em>&gt; we need to choose between conflicting goals, we figure out which goal is
</em><br>
<em>&gt; least important and discard it as an obsolete sub-goal.
</em><br>
<p>We're not that rational; we're not expected-utility optimisers. Besides,
<br>
subjectivity isn't about goal conflict, it's about the global shape and
<br>
causal connectivity of our cognitive process. Removing subjectivity isn't
<br>
an explicit goal-system change; it's porting the goal system to a new
<br>
implementation architecture which doesn't have all of the implicit goals
<br>
and inconsistencies of the old one. I used to think it was possible to
<br>
abstract out all of our implicit goals and preserve them through such a
<br>
transition, but because subjectivity is irrational it can't be abstracted
<br>
to a consistent goal system that doesn't simply embed all the complexity
<br>
of the original process.
<br>
<p><em>&gt;&gt; The CV question could be glibly summarised as 'is there a likely
</em><br>
incremental
<br>
<em>&gt;&gt; self-improvement path from me to a paperclip optimiser?'.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Or were you suggesting that we should be upset with the end result if
</em><br>
<em>&gt; fully informed people would decide to follow that path?
</em><br>
<p>'Fully-informed' isn't as simple as it sounds. To be fully informed about
<br>
issues too complex for you to work out for yourself, something more
<br>
intelligent
<br>
than you has to inform you. If they're too complex for you even to
<br>
comprehend,
<br>
something more intelligent than you has to take your preferences and apply
<br>
them to generate an opinion on the complex issue. CV works by extrapolating
<br>
you forward to get the more intelligent entity that advises the earlier
<br>
versions. The problem is that the extrapolation might snuff out something
<br>
you would consider important if you knew about it in a way that the
<br>
non-extrapolated version of you can't recognise and the extrapolated version
<br>
of you doesn't report. My conclusion is that to avoid this the extrapolated
<br>
volitional entities in CV need to interact with the (Power-class) AGI running
<br>
the process as an effective 'neutral party' to detect these sort of
<br>
discontinuities, but that further complicates the problem of not letting the
<br>
knowledge that the CV process is a simulation affect the result.
<br>
<p>Marc Geddes wrote;
<br>
<em>&gt; For instance the concept of 'wetness' is an emergent property which
</em><br>
<em>&gt; cannot be *explained* in terms of the individual behaviour of
</em><br>
<em>&gt; hydrogen and oxygen atoms.
</em><br>
<p>This is correct in that you also need a definition of the human cognitive
<br>
architecture to explain the human sensation of wetness.
<br>
<p><em>&gt; The *explanation* of 'wetness' does NOT NEED to be *derived* from
</em><br>
<em>&gt; physics.
</em><br>
<p>Yes, it does. If wetness wasn't physically implementable, it would not
<br>
exist.
<br>
<p><em>&gt; But a casual description of something is NOT the same thing as an actual
</em><br>
<em>&gt; *understanding* of that something.  
</em><br>
<p>A full causal description constitutes complete understanding of any
<br>
process, assuming it is defined all the way down to primitive physical/
<br>
mathematical operators, though such a description does not necessarily
<br>
include knowledge of relationships to other, similar processes possible
<br>
under those physics that we might include as part of our concept of
<br>
understanding the process.
<br>
<p>The world will not end in a dramatic fight between seed AIs(1), with pithy
<br>
quotes and bullet time. If we fail there may be a brief moment of terror,
<br>
or more likely confusion, but that is all. I concede that pithy quotes
<br>
and bullet time are fun and sell novels; I hope that readers will also be
<br>
motivated to think about the real issues in the process.
<br>
<p>(1) Unless of course we get a critical failure on our Friendliness roll,
<br>
in which case I expect to Tokyo getting levelled by giant tentacle monsters.
<br>
<p>Randall Randall wrote:
<br>
<em>&gt;&gt; It's not just the design, it's the debugging.  Computers you can tile. 
</em><br>
<em>&gt;&gt; Of course there'll also be a lag between delivery of nanocomputers 
</em><br>
<em>&gt;&gt; and when an UFAI pops out.  I merely point out the additional problem.
</em><br>
<em>&gt;
</em><br>
<em>&gt; One of my assumptions is that generic optimizers are difficult
</em><br>
<em>&gt; enough that some sort of genetic algorithm will be required to
</em><br>
<em>&gt; produce the first one.  I realize we differ on this, since you
</em><br>
<em>&gt; believe you have a solution that doesn't require GA.
</em><br>
<p>Generic optimisers are moderately hard to write. An inductive bias
<br>
comprehensive enough to allow takeoff is extremely hard to write for current
<br>
hardware and progressively less difficult to write as the available
<br>
hardware improves. You can use genetic algorithms to trade gigaflops for
<br>
programmer time and understanding, at a ratio determined by your
<br>
understanding of directed evolution theory. Anyone trying to build an AGI
<br>
on purpose will probably code the engine and a bit of the bias and use DE
<br>
to fill in the rest. Doing so will hopelessly break any Friendliness system
<br>
they have, which is probably already hopelessly broken given that they
<br>
think that using DE on an AGI is a good idea in the first place. But
<br>
anyway the net result is that silly amounts of computing power (i.e
<br>
nanocomputers) decrease the level of both programmer time and understanding
<br>
required to produce UFAI, eventually to the point where the world could be
<br>
wiped out at any moment by hopeful fools messing about with GAs in their
<br>
basement (or university offices).
<br>
<p><em>&gt; Since we appear to live in an STL world, I prefer MNT first.
</em><br>
<p>FTL looks a lot more possible if you have the technology and resources to
<br>
create exotic configurations of matter, not to mention perform experiments
<br>
of near-arbitrary size and complexity and the intelligence to advance the
<br>
theory rapidly. Thus while FTL may or may not be possible, if it is a UFAI
<br>
is much more likely to develop it after you leave than you are at any point.
<br>
<p>Eliezer Yudkosky wrote:
<br>
<em>&gt; If you have any hope of creating an FAI on board your fleeing vessel, the
</em><br>
<em>&gt; future of almost any UFAI that doesn't slip out of the universe entirely
</em><br>
<em>&gt; (and those might not present a danger in the first place) is more secure
</em><br>
<em>&gt; if it kills you than if it lets you flee.
</em><br>
<p>Or even a UFAI; the paperclip optimiser does not want to compete with a
<br>
wingnut optimiser. Given the amount of computing power required to run a
<br>
nanotech spacecraft, it seems highly unlikely that the probability of the
<br>
escapees building one could get that low, so running is almost certainly
<br>
futile (a Power with the resources of at least the solar system /will/
<br>
find you and render you harmless, probably by the 'zap with an exawatt
<br>
graser' approach). Ha, amusing passtime for transhumans; compete against
<br>
your friends to design a seed AI that can turn a (lossily) simulated
<br>
universe into your favourite desktop implement before they can turn it
<br>
into theirs.
<br>
<p><em>&gt; Don't try this at home, it won't work even if you do everything right.
</em><br>
<p>(Don't try this at your neighbourhood AGI project either.)
<br>
<p><em>&gt; The problem word is &quot;objective&quot;.  There's a very deep problem here, a
</em><br>
<em>&gt; place where the mind processes the world in such a way as to create the
</em><br>
<em>&gt; appearance of an impossible question. 
</em><br>
<p>We're roughly equivalent to a seed AI trying to fix a bug in its source code
<br>
that it has been programmed (by evolution in our case) not to see. There is
<br>
clearly an inconsistency here, but we can't see it directly.
<br>
<p>Jeff Albright wrote:
<br>
<em>&gt; We can influence the direction, but not the destination of our path 
</em><br>
<em>&gt; according to our &quot;moral&quot; choices.  We can do this by applying increasing 
</em><br>
<em>&gt; awareness of ourselves, our environment, and the principles that 
</em><br>
<em>&gt; describe growth, but it's open-ended, not something amenable to 
</em><br>
<em>&gt; extrapolation and control.
</em><br>
<p>The behavioural recommendation is a meaningless generalism; the conclusion
<br>
is simply wrong. The absolute tractability of extrapolation is unknown but
<br>
dependent on simplifying assumptions, which may be enforced by a Power with
<br>
the means to do so. The enforcement of simplifying assumptions such as
<br>
'all sentient life is wiped out' is an example of 'control' that we'd like
<br>
to avoid. The 'let's just create a Singularity and see what happens'
<br>
sentiment is deprecated; those people likely to influence the fate of
<br>
humanity must create a consensus preference function for possible futures
<br>
and then a tractable means to positively verify our choices.
<br>
<p><em>&gt; The next step in the journey will indeed involve the increasing 
</em><br>
<em>&gt; awareness of the multi-vectored &quot;collective volition&quot; of humanity, but 
</em><br>
<em>&gt; in the context of what works, focusing on effective principles to 
</em><br>
<em>&gt; actualize our collective vision and drive toward an inherently 
</em><br>
<em>&gt; unknowable future. 
</em><br>
<p>I agree with this part in so much as I think that Eliezer's model of CV as
<br>
a black box the programmers don't mess about with (for ethical reasons) is
<br>
likely to be unworkable; I suspect the AI running the process will need a
<br>
lot of trial runs and programmer feedback which will unavoidably entail a
<br>
peak at the spectrum of possible futures (though certainty thresholds for
<br>
implementation should all be bootstrappable out of CV itself).
<br>
<p>Sebastian Hagen wrote:
<br>
<em>&gt; The best answer I can give is 'whatever has objective moral relevance'.
</em><br>
<em>&gt; Unfortunately I don't know what exactly qualifies for that, so currently
</em><br>
<em>&gt; the active subgoal is to get more intelligence applied to the task of
</em><br>
<em>&gt; finding out. 
</em><br>
<p>Currently the only thing I can imagine you mean is working out all the
<br>
possible ways that the physics we're embedded in (or possibly, all logically
<br>
consistent physics) could have produced intelligent agents and generalising
<br>
across their goal systems. The only preference function for realities built
<br>
into physics is their actual probability amplitude; expecting there to be a
<br>
notion of desire for goal-seeking agents built into the structure of the
<br>
universe itself is simply a layer confusion (excepting the possibility that
<br>
our universe was designed by an intelligent agent; we could be someone else's
<br>
simulation or alpha-point computing experiment, but that just backs up the
<br>
problem to a wider scope).
<br>
<p><em>&gt; Should there be in fact nothing with objective moral relevance, what I do
</em><br>
<em>&gt; is per definition morally irrelevant, so I don't have to consider this
</em><br>
<em>&gt; possibility in calculating the expected utility of my actions.
</em><br>
<p>This class of goal-seeking agent (the class you would be if you actually
<br>
meant that) will probably be considered a time bomb by normative reasoners.
<br>
Regardless, all the people I know of who are actually developing AGI and
<br>
FAI do have subjective morality built into their goal systems, so it's not
<br>
a terribly relevant statement.
<br>
<p><em>&gt; Considering cosmic timescales it seems highly unlikely that the two
</em><br>
<em>&gt; civilizations would reach superintelligence at roughly the same time...
</em><br>
<em>&gt; since one of them likely has a lot more time to establish an
</em><br>
<em>&gt; infrastructure and perform research before the SI-complexes encounter
</em><br>
<em>&gt; each other, lack of efficiency caused by preferring eudaemonic agents
</em><br>
<em>&gt; may well be completely irrelevant to the outcome.
</em><br>
<p>As you point out the problem isn't competition between alien Powers, it's
<br>
competition between agents within a single civilisation (2). Even with a
<br>
Sysop that prevents sentients doing nasty things to each other, there will
<br>
still be meaningful forms of competition unless the general intelligences
<br>
with subjectivity are isolated from those that lack it (an upload/flesher
<br>
divide could be seen as a very primitive version of such a partitioning).
<br>
<p>(2) Assuming we avoid the 'all souls sucked up into a swirly red sphere in
<br>
low earth orbit' critical failure scenario. :)
<br>
<p><em>&gt; &quot;All-Work-And-No-Fun&quot;-mind may well be among the first things I'd do
</em><br>
<em>&gt; after uploading, so some of my possible future selves would likely be
</em><br>
<em>&gt; penalized by an implementation of your suggestions. My opinion on those
</em><br>
<em>&gt; suggestions is therefore probably biased...
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't think that what I would (intuitively) call 'consciousness' is
</em><br>
<em>&gt; by definition eudaemonic, but since I don't have any clear ideas about
</em><br>
<em>&gt; the concept that's a moot point.
</em><br>
<p>Argh. Firstly, intuition is worse than useless for dealing with these
<br>
concepts. Secondly, you shouldn't even be considering self-modification
<br>
if you don't have a clue about the basic issues involved. Without that
<br>
you might as well adopt the principle of indifference regarding what you
<br>
might do, possibly modified by the opinion of people who have put serious
<br>
thought into it. &quot;All-Work-And-No-Fun&quot; can be interpreted two ways; an
<br>
Egan-style outlook that merely makes leisure undesirable (incidentally
<br>
what else do you care about so much that you want to use your mind to
<br>
achieve?) and actually removing subjectivity. The former is a goal system
<br>
change; the later is a reasoning architecture change (as opposed to a
<br>
simple substrate change that doesn't affect the agent's decision
<br>
function). You apparent cheerful willingness to turn yourself into a
<br>
paperclip optimiser, dropping us to game-theoretic morality that
<br>
ultimately fills the universe with selfish replicators, is /exactly/ what
<br>
I was concerned about. Without CV it's a justification for the desirability
<br>
of Bostrom's singleton (Sysop); with CV it's a reason to make sure that
<br>
you can't do this (in the CV simulation and reality) without everyone who
<br>
might be affected understanding and approving of the consequences of allowing
<br>
non-eudaemonic reasonsers to assert a fundamental claim resources.
<br>
<p><em>&gt; Why? I don't understand why the activities mentioned before the quoted part
</em><br>
<em>&gt; (&quot;humor, love, game-playing, art, sex, dancing, social conversation,
</em><br>
<em>&gt; philosophy, literature, scientific discovery, food and drink, friendship,
</em><br>
<em>&gt; parenting, sport&quot;) are relevant for the value of human life.
</em><br>
<p>What else does human life (as distinguished from say a theorem proving
<br>
program running on your PC) consist of?
<br>
<p><em>&gt; From the perspective of an agent, that is striving to be non-eudaemonic,
</em><br>
<em>&gt; (me) the proposed implementation looks like something that could destroy a
</em><br>
<em>&gt; lot of efficiency at problem-solving. 
</em><br>
<p><em>&gt;From the perspective of the majority of humanity, why should we care? Being
</em><br>
less efficient just means that things take longer. Assuming we've dealt with
<br>
the competition problem, what's the hurry?
<br>
<p><em>&gt; Why do we have the right to declare absence of moral significance without
</em><br>
<em>&gt; even being capable of understanding either the society described, or
</em><br>
<em>&gt; understanding objective  morality (if there is anything like that)? The
</em><br>
<em>&gt; society may be decidedly unhuman, but this alone is imho not any
</em><br>
<em>&gt; justification of declaring it  morally insignificant.
</em><br>
<p>Singularity strategy is not something where political correctness is relevant
<br>
(or politics in general; several Singularitarians including myself have
<br>
already had our former libertarian ideals gutted by the realities of
<br>
posthuman
<br>
existence and arguably any remaining long-term libertarians just don't
<br>
understand the issues yet). These possible futures are in competition; we're
<br>
already trying to drag the probability distribution away from extinction
<br>
scenarios and to do that we also need to decide where we want to drag it
<br>
to. Allowing the current will of humanity to shape the probability
<br>
distribution
<br>
of possible racial futures is just as important as allowing your current
<br>
volition
<br>
to shape the probability distribution of your possible future selves.
<br>
<p>Aleksei Riikonen wrote:
<br>
<em>&gt; So let's strive to build something FAIish and find out ;)
</em><br>
<p>That's the other siren song, at least for us implementers; the desire to
<br>
try it and see. However impatience is no excuse for unnecessarily
<br>
endangering extant billions and subjunctive quadrillions of lives; 
<br>
Goertzel-style (attempted) experimental recklessness remains unforgivable.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><a href="http://www.sl4.org/bin/wiki.pl?Starglider">http://www.sl4.org/bin/wiki.pl?Starglider</a>
<br>
<p><p><p><p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ALL-NEW Yahoo! Messenger - all new features - even more fun!  <a href="http://uk.messenger.yahoo.com">http://uk.messenger.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9899.html">Dani Eder: "Re: The Future of Human Evolution"</a>
<li><strong>Previous message:</strong> <a href="../0409/9897.html">Keith Henson: "Re: The Future of Human Evolution"</a>
<li><strong>Maybe in reply to:</strong> <a href="../0409/9854.html">Aleksei Riikonen: "Re: The Future of Human Evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9901.html">Ben Goertzel: "RE: The Future of Human Evolution"</a>
<li><strong>Reply:</strong> <a href="9901.html">Ben Goertzel: "RE: The Future of Human Evolution"</a>
<li><strong>Reply:</strong> <a href="9902.html">Sebastian Hagen: "Re: The Future of Human Evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9898">[ date ]</a>
<a href="index.html#9898">[ thread ]</a>
<a href="subject.html#9898">[ subject ]</a>
<a href="author.html#9898">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:49 MDT
</em></small></p>
</body>
</html>
