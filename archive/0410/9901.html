<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: The Future of Human Evolution</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: The Future of Human Evolution">
<meta name="Date" content="2004-10-01">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: The Future of Human Evolution</h1>
<!-- received="Fri Oct  1 14:16:21 2004" -->
<!-- isoreceived="20041001201621" -->
<!-- sent="Fri, 1 Oct 2004 16:18:12 -0400" -->
<!-- isosent="20041001201812" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: The Future of Human Evolution" -->
<!-- id="JNEIJCJJHIEAILJBFHILIELICFAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20041001124313.59327.qmail@web25302.mail.ukl.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20The%20Future%20of%20Human%20Evolution"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Fri Oct 01 2004 - 14:18:12 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9902.html">Sebastian Hagen: "Re: The Future of Human Evolution"</a>
<li><strong>Previous message:</strong> <a href="9900.html">Aleksei Riikonen: "Re: The Future of Human Evolution"</a>
<li><strong>In reply to:</strong> <a href="9898.html">Michael Wilson: "Re: The Future of Human Evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9902.html">Sebastian Hagen: "Re: The Future of Human Evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9901">[ date ]</a>
<a href="index.html#9901">[ thread ]</a>
<a href="subject.html#9901">[ subject ]</a>
<a href="author.html#9901">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michael,
<br>
<p>I'll just respond to a couple scattered points in this long exchange; not
<br>
because they're the only interesting ones but because I'm highly busy
<br>
lately...
<br>
<p><em>&gt; That's the other siren song, at least for us implementers; the desire to
</em><br>
<em>&gt; try it and see. However impatience is no excuse for unnecessarily
</em><br>
<em>&gt; endangering extant billions and subjunctive quadrillions of lives;
</em><br>
<em>&gt; Goertzel-style (attempted) experimental recklessness remains unforgivable.
</em><br>
<p>It gets redundant to keep clarifying my position on this list, but I feel
<br>
obliged to do so at least briefly in case there are newbies to the list who
<br>
haven't heard all this before.
<br>
<p>My position is that I have much less faith than Eliezer or you in the power
<br>
of philosophical, semi-rigorous thinking to clarify issues regarding
<br>
advanced AI morality (or the dangers versus benefits of advanced biotech,
<br>
nanotech, etc.).  Even mathematical reasoning -- and we're verrrry far from
<br>
any kind of rigorous math understanding of anything to do with AI
<br>
morality -- is only as good as the axioms you put into it, and we never
<br>
quite know for sure if our axioms agree with the experienced world until we
<br>
do practical experimentation in the domain of interest...
<br>
<p>My skepticism about the solidity of this kind of philosophical thinking
<br>
seems to be borne out by the history of Eliezer's thinking so far -- each
<br>
year he argues quite vehemently and convincingly for his current
<br>
perspective; then, a year later, he's on to a different perspective....  I
<br>
don't think he's wrong to change his views as he learns and grows, but I do
<br>
think he's wrong to think any kind of near-definite conclusion about
<br>
Friendly AI is going to be arrived at without significant empirical
<br>
experimentation with serious AGI's...  Until then, opinions will shift and
<br>
grow and retreat, as in any data-poor area of inquiry.
<br>
<p>Look at dynamical systems theory.  The theorists came with a lot of
<br>
interesting ideas -- but until computers came along and let us experiment
<br>
with a bunch of dynamical systems, all the theorists missed SO MANY THINGS
<br>
that seem obvious to us now in the light of experiment (&quot;chaos theory&quot; for
<br>
one).  Not that the theorists were totally wrong, just that their views were
<br>
limited, and then the experimental work opened up their minds to new
<br>
realities and new avenues for theory.
<br>
<p>In short, I really don't think we're going to have a decent understanding of
<br>
the morality of advanced AI's until we've experimented with some AGI systems
<br>
acting in various environments with various goal systems.  You can call it
<br>
&quot;reckless&quot; to advocate this sort of experimentation -- but then, I'd argue
<br>
that it's reckless NOT to proceed in the most rapid and sensible way toward
<br>
AI because the alternative is pretty likely the destruction of the human
<br>
race via other nascent technologies.
<br>
<p>Based on this philosophy I am proceeding -- as fast as possible given the
<br>
limitations on my funding for the project and my own time and my need to
<br>
earn a living -- to complete the Novamente AI system which I believe will
<br>
result in an AGI worth of the label, one which can be used to experiment
<br>
with AGI morality among many other things.
<br>
<p>I'm not looking to repeat prior arguments with Eliezer and you; my point in
<br>
writing these paragraphs is just to clarify my position in the light of your
<br>
somewhat deprecating allusion to my work in your message.
<br>
<p><p><em>&gt; &gt; The *explanation* of 'wetness' does NOT NEED to be *derived* from
</em><br>
<em>&gt; &gt; physics.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes, it does. If wetness wasn't physically implementable, it would not
</em><br>
<em>&gt; exist.
</em><br>
<p>The limitations of this point of view should be obvious to anyone who's
<br>
studied Buddhist psychology or Western phenomenology ;-)
<br>
<p>You say that only the physical world &quot;exists&quot; -- but what is this physical
<br>
world?  It's an abstraction that you learned about through conversations and
<br>
textbooks.  I.e., it's a structure in your mind that you built out of
<br>
simpler structures in your mind -- simpler structures corresponding to
<br>
sensations coming in through sights, sounds and other sensations.
<br>
<p>You can take a point of view that the physical world is primary and your
<br>
mind is just a consequence of the physical domain.  Or you can take a point
<br>
of view that mind is primary and the very concept of the physical is
<br>
something you learn and build based on various mental experiences.  Neither
<br>
approach is more correct than the other; each approach seems to be useful
<br>
for different purposes.  I have found that a simultaneous awareness of both
<br>
views is useful in the context of AI design.
<br>
<p>-- Ben Goertzel
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9902.html">Sebastian Hagen: "Re: The Future of Human Evolution"</a>
<li><strong>Previous message:</strong> <a href="9900.html">Aleksei Riikonen: "Re: The Future of Human Evolution"</a>
<li><strong>In reply to:</strong> <a href="9898.html">Michael Wilson: "Re: The Future of Human Evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9902.html">Sebastian Hagen: "Re: The Future of Human Evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9901">[ date ]</a>
<a href="index.html#9901">[ thread ]</a>
<a href="subject.html#9901">[ subject ]</a>
<a href="author.html#9901">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:49 MDT
</em></small></p>
</body>
</html>
