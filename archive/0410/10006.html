<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: SIAI:  Donate Today and Tomorrow</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="SIAI:  Donate Today and Tomorrow">
<meta name="Date" content="2004-10-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>SIAI:  Donate Today and Tomorrow</h1>
<!-- received="Thu Oct 21 04:21:38 2004" -->
<!-- isoreceived="20041021102138" -->
<!-- sent="Thu, 21 Oct 2004 06:21:31 -0400" -->
<!-- isosent="20041021102131" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="SIAI:  Donate Today and Tomorrow" -->
<!-- id="41778DAB.8080202@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20SIAI:%20%20Donate%20Today%20and%20Tomorrow"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu Oct 21 2004 - 04:21:31 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10007.html">Eliezer Yudkowsky: "Re: [agi] A difficulty with AI reflectivity"</a>
<li><strong>Previous message:</strong> <a href="10005.html">Christian Szegedy: "Re: [agi] A difficulty with AI reflectivity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10017.html">Slawomir Paliwoda: "Re: Donate Today and Tomorrow"</a>
<li><strong>Reply:</strong> <a href="10017.html">Slawomir Paliwoda: "Re: Donate Today and Tomorrow"</a>
<li><strong>Reply:</strong> <a href="10021.html">Marc Geddes: "Re: SIAI:  Donate Today and Tomorrow"</a>
<li><strong>Maybe reply:</strong> <a href="10030.html">Aikin, Robert: "RE: SIAI:  Donate Today and Tomorrow"</a>
<li><strong>Maybe reply:</strong> <a href="10041.html">Aikin, Robert: "RE: SIAI:  Donate Today and Tomorrow"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10006">[ date ]</a>
<a href="index.html#10006">[ thread ]</a>
<a href="subject.html#10006">[ subject ]</a>
<a href="author.html#10006">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
The Singularity Institute is approaching the end of its five-year
<br>
provisional tax-exempt period.  At the end of this year, December 31st
<br>
2004, the IRS will request information from us that they then use to
<br>
determine whether the Singularity Institute will get permanent status as a
<br>
public charity.  The critical test they apply is called the &quot;public
<br>
support&quot; test, and like most of what the IRS does, it's complicated.
<br>
Roughly speaking, the IRS adds up all the donations we receive in a
<br>
four-year period, takes the grand total, and divides by 50; this amount (2%
<br>
of total support) is the most of any one individual's donation that counts
<br>
as &quot;public support&quot;.  The rest of the donations from that individual,
<br>
anything over and above 2% of your total support over that four-year
<br>
period, are nonpublic support.
<br>
<p>Leaving aside the exact details of the calculation, the IRS asks whether a
<br>
nonprofit is a *publicly* supported organization - whether their donations
<br>
come from a broad base, or a few big donors.
<br>
<p>Right now, most of SIAI's support comes from a few big donors.  To pass the
<br>
public support test automatically, we would need 33.3% public support
<br>
(1/3).  At present, according to our calculations, we're at roughly 25%
<br>
public support.  This doesn't automatically fail us, but it does mean that
<br>
the IRS applies something called the &quot;facts and circumstances&quot; test, which
<br>
we might not pass - or the IRS might demand unworkable changes in our
<br>
governing structure, like having members of the Board appointed by public
<br>
officials (yes, this is part of the &quot;facts and circumstances&quot; test).  In
<br>
the worst case the IRS might determine that we were a private foundation,
<br>
which would severely cripple SIAI.  If we can get 6 *new* donors - not
<br>
existing major donors - to donate $5000 apiece, we should pass
<br>
automatically.  If we can't do that, the IRS still pays attention to how
<br>
much public support we *do* have, how many donors we have, and from how
<br>
wide a base.
<br>
<p>The Singularity Institute also currently has $7,690 unfilled in the
<br>
Fellowship Challenge - yes, $7,690 unfilled; Brian Cartmell increased the
<br>
total Challenge Grant to $15,000.  So if you donate now, and earmark your
<br>
donation for the Fellowship Challenge, Cartmell will match your donation.
<br>
<p>Even if you can only afford a hundred bucks, the Fellowship Challenge will
<br>
still match the donation, it still will be one more person who donated, and
<br>
it will still help show the IRS that we have a broad base of public
<br>
support.  In short, now - yes, *now* - is a good time.  And we need *new*
<br>
donors.  That means *you*.  Not the people who already donated more than
<br>
$5000 over 2001-2004, *you*.
<br>
<p>The Singularity Institute now announces a 72-hour donations campaign,
<br>
starting at 5AM Eastern Time on Thursday, October 21st, and continuing
<br>
until 5AM Eastern Time, Sunday October 24th.  The title of the campaign is
<br>
&quot;Donate Today and Tomorrow&quot;.  The 72-hour time limit is intended to give
<br>
people &quot;today and tomorrow&quot; even if they don't check their email more than
<br>
once a day.  The theme of this campaign is donating early and fast, so if
<br>
you've already decided to send us a check for $5000, you can go ahead and
<br>
mail it right now, then continue reading at your leisure.  Otherwise, a
<br>
somewhat longish essay follows, meant for people who think that SIAI is a
<br>
fun, neat, cool idea, but who haven't gotten around to donating yet.  If
<br>
you've never heard of us, don't start here - visit our website at
<br>
<a href="http://intelligence.org/">http://intelligence.org/</a>.
<br>
<p>--- Today And Tomorrow --
<br>
<p>Once upon a time, long ago and far away, ever so long ago...
<br>
<p>In the beginning, saith Richard Dawkins, the world was populated by stable
<br>
things:  Patterns that arose frequently, or patterns that lasted a long
<br>
time.  Raindrops that fell from the sky, or mountains that rose from the
<br>
crust.  This was the era of chemistry, the era of dynamic stability, an age
<br>
before optimization.  By chance a star might have dynamics that let it burn
<br>
longer; and you would be more likely to see such a long-burning star than
<br>
to see an unlucky star that went nova shortly after coalescing.  But the
<br>
star has no properties that are there *because* they help the star burn
<br>
longer.  A star may have intricate stellar dynamics, sunspots and corona
<br>
and flares - but a star is not optimized, has no complex functional design.
<br>
A star is an accident, something that just happens when interstellar dust
<br>
comes together.  In the beginning, the universe was populated by accidents.
<br>
<p>Today we see many patterns, from butterflies to kittens, with far more
<br>
complexity than mere stars or raindrops.  Not that stars are simple, but
<br>
stars can't compare to the intricate order of a kitten.  The kitten arises
<br>
from a different kind of pattern-maker, a different source of structure.
<br>
When we look around us, we find not just stable accidents, like mountains,
<br>
or frequent accidents, like water molecules, but also patterns that copy
<br>
themselves successfully.  The process that structures these patterns is
<br>
called evolution, and evolution produces complex structure enormously
<br>
faster than accident alone.  The first ten billion years of the universe
<br>
were relatively boring, with little real complexity, because there was no
<br>
way for complexity to accumulate.  If by luck one star burns unusually
<br>
long, that makes it no more likely that future stars will also last longer
<br>
- the successful pattern isn't copied into other stars.  Evolution builds
<br>
on its successes and turns them into bigger and more complex successes.  In
<br>
an information-theoretic sense, there might be less complexity in the
<br>
entire Milky Way outside Earth than there is in a single butterfly.
<br>
<p>Once upon a time, currently thought to be around 3.85 billion years ago,
<br>
the era of accidents ended.  Perhaps by sheer chance the processes of
<br>
chemistry coughed up a single self-replicating molecule.  Perhaps the
<br>
transition was more subtle, an attractor in a swirling loop of catalytic
<br>
chemistry.  Even if it was most exceedingly unlikely for any given accident
<br>
of chemistry to create a replicating pattern, it only had to happen once.
<br>
So it all began, once upon a time, ever so long ago: the ultimate
<br>
grandparent of all life, the very first replicator.  From our later
<br>
perspective, that first replicator must have looked like one of the oddest
<br>
and most awkward things in the universe - a replicator with the pattern of
<br>
a stable thing; a reproducing structure that arose by accident, without
<br>
evolutionary optimization.  The era of stable things ended with a special
<br>
stable thing that was also a replicator, and only in this way could the era
<br>
of biology bootstrap into existence.  Had you been around at the time, you
<br>
might have mistaken the first replicator for an exceptionally stable thing
<br>
that just happened to be very common in the primordial soup.  A great
<br>
success as stable things go, but nothing fundamentally different.
<br>
<p>The true nature of the next era would not become clear until you saw the
<br>
second replicator ever to exist, your penultimate grandparent, an optimized
<br>
pattern that would never have arisen in the primordial soup without a
<br>
billion copies of that first replicator around to potentially mutate.  It
<br>
was the second replicator that was a new kind of structure, a pattern that
<br>
would never be found in a world of stable things alone.
<br>
<p>And what a strange sight a human must be - an intelligence inscribed
<br>
entirely by evolution, a mind not constructed by another mind.  We are one
<br>
of the oddest and most awkward sights in the universe - an intelligence
<br>
with the pattern of an evolved thing.  Like that first replicator, we only
<br>
had to evolve once, but we did have to evolve.  The transition from biology
<br>
to intelligence, from survival of the fittest to recursive
<br>
self-improvement, could never have happened otherwise.
<br>
<p>Even today, if you look into the core of the replicators in today's world,
<br>
you can see the trace of that distant stable thing in their ancestry.  What
<br>
a miracle, what fortuity, that Urey and Miller's experiment of running
<br>
electricity through a primordial atmosphere of methane and ammonia should
<br>
produce the amino acids making up all proteins!  What fortuity, that later
<br>
experiments based on new models of the primordial atmosphere produced DNA
<br>
bases as well!  How lucky!  No, it is not luck at all; of course the
<br>
elements of biology are molecules that would tend to arise by chance
<br>
chemistry on primordial Earth.  How else would life get started?  Strange,
<br>
that RNA should be capable of both carrying information, and twisting up
<br>
into catalysts and enzymes.  What a fortuitous property, in a molecule so
<br>
close to DNA!  But it is not luck; how else would you expect life to start,
<br>
but with a molecule capable of both carrying information and carrying out
<br>
chemical operations?  And because that pattern of RNA was there at the
<br>
beginning, it still forms part of the uttermost core, tiny strands of
<br>
single-stranded RNA ubiquitous in the chemistry of the cell.  Look at the
<br>
pattern of any living thing in the world and you can see that there must
<br>
have been a stable thing in its ancestry, far far back, ever so long ago.
<br>
<p>And someday, if we do our jobs right, our distant grandchildren will study
<br>
evolutionary biology, and shake their heads in wonder.  For though it is
<br>
understandable enough that kindness should beget kindness and loving minds
<br>
construct more loving minds, it is passing strange that the simple
<br>
equations of evolutionary biology should do likewise.  Who would ever have
<br>
thought that natural selection, bloody in tooth and claw, should give rise
<br>
to a sense of fun?  Who would expect that the winning alleles would embody
<br>
the love of beauty, taking joy in helping others, the ability to be swayed
<br>
by moral argument, the wish to be better people?
<br>
<p>Evolution is an optimization process devoid of conscience and compassion.
<br>
And yet we have conscience.  We have compassion.  Alleles for conscience
<br>
and compassion somehow drove competing alleles to extinction.  I do not
<br>
wish to hint at mystery where no mystery exists: science does have a good
<br>
idea as to how alleles for mercy wiped out the genetic competition.  All
<br>
our human natures are patterns that evolution can produce, and furthermore,
<br>
patterns that carry the unique characteristic signature of evolution.  The
<br>
original archetype of Romeo and Juliet would not arise without sexual
<br>
reproduction.  Some emotions are less obvious at first blush than others,
<br>
but there is always a reason, and there is always evolution's unique
<br>
signature.  That evolution coughed up friendship is amazing, but not
<br>
mysterious.
<br>
<p>I still nominate the evolution of kindness as the most wonderful event in
<br>
the history of the universe.  The light that is born into our species is a
<br>
precious thing, which must not be lost.
<br>
<p>Less surprising is that bloodlust, hatred, prejudice, tribalism, and
<br>
rationalization are products of evolution.  They are not humane, but they
<br>
are human.  They are part of us, not by our choice, but by evolution's
<br>
design, and the heritage of billions of years of lethal combat.  That is
<br>
the tragedy of the human condition, that we are not what we wish we were.
<br>
There shall come a time, I think, when humanity sees itself reflected, and
<br>
burns the darkness clean.  Yet the same impartial optimization process
<br>
inscribed both our light and our darkness.  The forces that first
<br>
constructed intelligence were without intelligence.  The dynamics that
<br>
birthed morality were without morality.
<br>
<p>So it all began, once upon a time, long ago and far away, ever so long ago,
<br>
in an age so distant that only a tiny handful of minds in all the countless
<br>
galaxies remember...
<br>
<p>But that is for the future.  First we must survive this century.
<br>
<p>It is not exactly trivial, to try and master fully the art of Friendly AI
<br>
before anyone succeeds in cobbling together a half-understood hack that
<br>
recursively self-improves.  Creating Friendly AI is an engineering
<br>
challenge, and it will take full-time people and specialized skills, and
<br>
donors to support them.  It has to be done fast, because we need a
<br>
difficult cure to arrive before an easy disease.  I still think it's
<br>
possible to win, though the hour grows later.  But you can't change the
<br>
future by relaxing and letting things go on the way they have.  You can't
<br>
change the future by doing whatever you planned to do anyway with a newer
<br>
and cleverer excuse.  Changing the future takes people willing to change
<br>
even themselves, if that is the price demanded.
<br>
<p>And even among those who understand what is at stake, it seems that the
<br>
most common reaction is to sit back and cheer the Singularity Institute on,
<br>
the way one might cheer for a football team - passively.  Maybe it's
<br>
watching movies that engrains the habit into people's minds.  They would
<br>
look at you oddly, in the movie theatre, if when the world was threatened
<br>
you tried to jump into the silver screen to help.  Maybe it's watching
<br>
movies that teaches people that when the fate of the human species is at
<br>
stake, the thing to do is wait with baited breath for the good guys to win,
<br>
as they always do.  Too few seem to realize that the outcome is yet in
<br>
doubt, and that they might have their own parts in the unfinished tale,
<br>
waiting for them to leap onto the stage.
<br>
<p>There's research in social psychology, starting in 1968 with a famous
<br>
series of experiments by Latane and Darley, on the phenomenon now known as
<br>
the &quot;bystander effect&quot;.  When more people are present at an emergency, it
<br>
can reduce the probability that *anyone* will help.  One of Latane and
<br>
Darley's original experiments had subjects fill out questionnaries in a
<br>
room when they began to add smoke.  In one condition, the subject was
<br>
alone.  In another condition, three subjects were in the room.  In the
<br>
final condition, one subject and two confederate experimenters (apparently
<br>
other students) were in the room.  75% of alone subjects calmly noticed the
<br>
smoke and left the room to report it.  When three subjects were in a room
<br>
together, only 38% of the time did anyone leave to report the smoke.  When
<br>
a subject was placed with two confederates who deliberately ignored the
<br>
smoke, the subject reported the smoke only 10% of the time.  Other
<br>
experiments in Latane and Darley's original series included the
<br>
experimenter apparently breaking a leg and a student apparently having a
<br>
seizure.  In every case, the subjects who were alone reacted to the
<br>
emergency faster than the subjects who witnessed the emergency in a group.
<br>
<p>Since 1968, Latane and Darley's original experiments have been extensively
<br>
replicated, varied, tested in real-world conditions, et cetera, and the
<br>
original result still holds: people in groups are much slower to react to
<br>
emergencies than individuals, if they react at all.  The current consensus
<br>
in social psychology is that the bystander effect stems from two primary
<br>
causes, diffusion of responsibility and a phenomenon called &quot;pluralistic
<br>
ignorance&quot;.  Diffusion of responsibility works like this:  If three people
<br>
are present, each one thinks, &quot;Well, *I* don't need to do anything, those
<br>
other two will handle it,&quot; and no one does anything.  People who are alone
<br>
know that they alone are responsible, and if they don't do something, no
<br>
one will - and, yes, that does make a huge, experimentally verifiable
<br>
difference; one person, alone, is literally more than twice as likely to
<br>
act in an emergency as a group of three people.  &quot;Pluralistic ignorance&quot; is
<br>
the name that social psychologists give to group underreaction:  When
<br>
people in groups see an emergency, they look around to see if anyone else
<br>
is responding.  If no one else is responding, they assume it's not a real
<br>
emergency.  The problem is that *before* people have decided something is
<br>
an emergency, they instinctively try to appear calm and unconcerned while
<br>
they dart their eyes about to see if anyone else is responding - and what
<br>
they see are other people appearing calm and unconcerned.
<br>
<p>If you ever find yourself with a broken leg or some other emergency, and
<br>
you're unlucky enough to have many people about, social psychologist Robert
<br>
Cialdini recommends that you point at *one particular* person and tell him
<br>
or her to call 911, or carry out some other definite action.  Singling out
<br>
a particular person reduces the diffusion of responsibility, and once a
<br>
single person steps up to help, other people may also stop ignoring the
<br>
emergency.
<br>
<p>Experimental manipulations that reduce group apathy and the bystander
<br>
effect include (1) a bystander who thinks that the emergency requires
<br>
intervention from them *personally*, (2) bystanders with considerable
<br>
training in emergency intervention, and, last but not least, (3) bystanders
<br>
who know about the bystander effect.  So the next time you're in a large
<br>
group when someone has a heart attack, don't try to look calm and
<br>
unconcerned while you dart your eyes around to see if anyone else is
<br>
panicking.  Take out your cellphone, call 911, point to other people
<br>
specifically and recruit them to help you.  And yes, it has to be you,
<br>
because only you know about the bystander effect.
<br>
<p>The bystander effect takes odd forms when it comes to people
<br>
procrastinating about when to donate to the Singularity Institute.  My
<br>
current observation is that most nondonors do honestly plan to donate,
<br>
eventually, just as soon as X - where X varies across donors, but always
<br>
lies at least a year in the future.  Non-donors also expect that lots of
<br>
other people are donating.  This is not so.  Rather, lots of other people
<br>
expect that lots of other people are donating.  Also interesting is the way
<br>
people choose X, the future condition that will finally allow them to
<br>
donate.  High school students say they want to wait until they can get into
<br>
college on a scholarship.  People in college on scholarships want to wait
<br>
until they graduate and get jobs.  People with jobs want to wait until they
<br>
pay off their student loans.  People whose student loans are paid off, want
<br>
to wait another five years until their stock options vest.  People whose
<br>
stock options have vested, want to wait until they can support both the
<br>
Singularity Institute and the startup venture they're currently funding...
<br>
<p>Meanwhile, the Singularity Institute recently received, in the mail, an
<br>
envelope with no return address, containing an anonymous donation for ten
<br>
dollars.  We know, because he or she told us so, that whoever sent this
<br>
donation is a high school student - and that's all we know.  On the
<br>
shoulders of such people rests the fate of the world.
<br>
<p>Maybe someday this person will show up on the SL4 mailing list.  Maybe
<br>
we'll find out who sent that letter after the Singularity.  Maybe the donor
<br>
will never choose to reveal himself or herself.  Maybe someday there will
<br>
be a monument to this person, the Anonymous Donor, and it will say:  &quot;We
<br>
never knew who the one was, or why the one helped us, but when the one was
<br>
needed, the one was there.&quot;  I wonder how many names will be on the other
<br>
monument, the monument to that entire band which once conducted the last
<br>
defense of humankind.  Less than ten thousand names?  Less than a thousand?
<br>
At the end of 2003 the roster of donors was less than a hundred.  Let us
<br>
be optimistic, and hope there will be five thousand and twenty-four names
<br>
on this monument, and that they put forth enough effort to win.  Five
<br>
thousand and twenty-four names would still be fewer than one in a million,
<br>
and there would never be any more.  Somewhere on that monument will be the
<br>
name of someone who donated a single dollar, and humanity will be glad that
<br>
he did, for it is strange enough that there are only five thousand and
<br>
twenty-four names on that monument; how much sadder if there should be only
<br>
five thousand twenty-three.  For as long as Earth-originating intelligent
<br>
life continues that monument shall exist, and it shall still have only five
<br>
thousand twenty-four names...
<br>
<p>And I had this thought, and I wondered how many SIAI volunteers would have
<br>
their names on the monument, and I knew that at the present rate it would
<br>
be fewer than one in ten.  As a wise volunteer recently observed - I can't
<br>
find the quote, so I'm paraphrasing - &quot;The problem is that we're
<br>
indoctrinated into believing that you can make a big difference by helping
<br>
out just a little.  But the sad truth is that you can't do AI on two hours
<br>
a month.&quot;  These are words of deep wisdom, and I wish I could remember who
<br>
said them (if you're reading this, write me).  As Christine Peterson of
<br>
Foresight said on a similar occasion for similar reasons, donating is a
<br>
*lot* more helpful than volunteering.  I'm sorry, but that's the way it is. 
<br>
&nbsp;&nbsp;If you're waiting for the Singularity Institute to come up with a
<br>
desperately urgent problem that can be solved with ten hours of Flash
<br>
coding over five months, you'll probably wait forever.
<br>
<p>I had that thought about the monument, and I wondered what it would be like
<br>
to spend the next several centuries explaining that, yes, you were one of
<br>
the tiny fraction of humanity that knew the Singularity Institute existed,
<br>
and you were on the volunteers list, and you even hung out on the SIAI
<br>
volunteers IRC channel, but you never actually got around to donating a
<br>
hundred bucks and that's why your name isn't on the monument.
<br>
<p>So I mentioned this thought in the #siaiv IRC channel, for it seemed to me
<br>
like a dreadful and possible doom against which people ought to be warned.
<br>
And someone who was not a donor said: but we aren't doing it for the
<br>
fame.  And I replied: it's all well and good to act on pure altruism if you
<br>
can attain that level, but there's something wrong with deriding fame-lust
<br>
when fame-lust would produce pragmatically better behavior.  For one
<br>
motivated by lust for fame would send in the hundred bucks, and that is
<br>
more help than receiving nothing from an altruist.
<br>
<p>I probably should have phrased that rejoinder more tactfully.  I was
<br>
feeling a tad frustrated at the time.  But tactless or not, it happens to
<br>
be true.
<br>
<p>When I started up the Singularitarian movement, I wished (in my youthful
<br>
idealism) to appeal to pure altruism, unmixed with lesser motives like the
<br>
lure of everlasting fame.  My original reasoning was that we might all zip
<br>
off directly to superintelligence without lingering in the human spaces
<br>
where things like monuments made sense, and for this reason I could promise
<br>
nothing for the future.  Today I do not think I would choose such rapid
<br>
growth.  I think I will prefer to grow at whatever healthy rate keeps my
<br>
mind from going stale, and smell the roses along the way.  But I might be
<br>
mistaken.  There may be no monument.  There certainly won't be a monument
<br>
if the human species dies without ever having its chance at the light.  The
<br>
future is uncertain and I cannot honestly promise anything; and so it
<br>
disturbs me to even offer pleasant prospects, because there is something in
<br>
human nature that makes us treat prospects as if they were promises.
<br>
Today, it still strikes me as wrong and perhaps dangerous to tell someone
<br>
that they should donate to SIAI for the uncertain prospect of fame.  But I
<br>
would also be deeply annoyed if the human species died off because its
<br>
defenders were too proud to stoop to pointing out some of the specific
<br>
impure motives that of course should not motivate you to help save the
<br>
world.  Right now, most people are waiting into the indefinite future to
<br>
donate, and that's not an acceptable outcome.  It means we're doing
<br>
something wrong, and there's something about our strategy that we need to
<br>
reconsider, and maybe this is it.
<br>
<p>I worry about the evolutionary psychology of the bystander effect.  People
<br>
dealing with emergencies in groups stand by and do nothing.  Now experiment
<br>
also shows that people who find themselves the sole source of help in
<br>
emergencies, *do* act.  This implies that ancestors who acted when alone
<br>
did not do consistently worse than ancestors who walked away.  Maybe the
<br>
ancestors who walked away had lousy reputations and no one wanted to be
<br>
their friends.  Maybe the ancestors who helped, tended to end up helping
<br>
someone who was more likely than average to share the helpful allele.  The
<br>
point is that individual helping behavior was not selected out.  So why the
<br>
bystander effect?  If the selection pressure favors (or at least doesn't
<br>
punish) acting in emergencies when you're standing there alone, why would
<br>
this change if three people are present?
<br>
<p>My thought is that the presence of a group creates an arms race between
<br>
alleles in which the goal is to avoid being the first person to help.
<br>
Suppose that we start out with an allele A for helping someone in trouble
<br>
right away.  Allele A might maintain itself in the gene pool because
<br>
spatial variance in the distribution of allele A meant that an ancestor who
<br>
carried allele A and encountered someone in need of emergency assistance,
<br>
or a threat to the tribe, usually helped beneficiaries with a higher
<br>
proportion of A alleles than the general population pool.  For whatever
<br>
reason, allele A hasn't been driven to extinction.  But now suppose that
<br>
there's a group of three people watching someone in need of help.  We'll
<br>
call the person who needs helping Harry.  Suppose one of the group of three
<br>
has an alternative allele B that runs the algorithm, &quot;Wait 20 seconds, then
<br>
help Harry.&quot;  If all three people present carry allele B, then Harry is
<br>
still helped, albeit after a delay of 20 seconds.  If one or more of the
<br>
others present has allele A that helps immediately, then the A-carriers
<br>
bear most of the cost of helping, while the B-carriers freeload.  This
<br>
holds true whether Harry carries allele A or B.
<br>
<p>Perhaps the bystander effect results from an evolutionary arms race to not
<br>
be the *first* to help.  Even if helping someone in need tended to be a net
<br>
evolutionary benefit in the ancestral environment, if there happened to be
<br>
a *group* present, there might have been a fitness advantage to not being
<br>
the *first* to help.  There would have been an arms race between alleles, a
<br>
race of apathy and delay and hoping that someone else would handle the
<br>
problem instead.  And this arms race has no obvious upper bound.  Latane
<br>
and Darley performed their original series of experiments in the aftermath
<br>
of the Kitty Genovese incident in Queens, New York, 1964.  Kitty Genovese
<br>
was stabbed, raped, robbed, and murdered over the course of half an hour.
<br>
Later investigations showed that more than 38 people had witnessed parts of
<br>
the attack.  None called police.  In 1995, Deletha Word was beaten with a
<br>
tire iron on a bridge in Detroit; she jumped from the bridge to escape and
<br>
died; none of the people crossing the bridge that morning stopped to save
<br>
her.  And though these are but anecdotes, they are anecdotes which
<br>
illustrate solid experimental results.  Individuals help, and people in
<br>
groups hang back and wait for someone else to help.
<br>
<p>The thought also occurred to me that if you help everyone in the entire
<br>
human species, it is an evolutionary null-op.  Evolution runs on allele
<br>
substitution rates in a population pool.  What matters isn't whether you
<br>
reproduce, it's whether you outreproduce peers who carry different alleles
<br>
- whether an allele increases its proportion in the gene pool and
<br>
eventually becomes fixed.  A benefit that everyone in your species shares
<br>
equally, benefits all alleles equally, and hence is an evolutionary
<br>
null-op; it produces zero genetic information.  Luckily, human beings are
<br>
adaptation-executers, not fitness-maximizers.  There were no existential
<br>
risks in the ancestral environment, nor any Friendly AIs, no way for an
<br>
individual to harm or benefit the entire human species at once.  Our
<br>
psychologies are already inscribed, solely by those selection pressures
<br>
that acted on our ancestors.  The psychology of existential risk is likely
<br>
to fit the &quot;threat to the tribe&quot; template, a problem that was around in
<br>
ancestral times and that involved noticeable selection pressures.
<br>
<p>(Unfortunately, &quot;threats to the tribe&quot; tended to be those evil Other Guys
<br>
from the Tribe That Lives Across The Water, not hard-to-understand threats
<br>
like recursively self-improving optimization processes.  Which is why it's
<br>
so difficult to keep people focused on boring old saving the world instead
<br>
of fun politics.  The sad truth is that if the Singularity were
<br>
recognizably a Democrat or Republican project, we'd get a lot more funding.)
<br>
<p>I've done some math, and I have not yet found any obvious evolutionary
<br>
reason why an action that benefits both others and yourself should create
<br>
less selection pressure favoring your alleles than an action that benefits
<br>
only yourself.  (We assume the same individual cost and the same individual
<br>
benefit in both cases, the only question being whether others receive
<br>
duplicates of the individual benefit.)  But I intend to keep looking for
<br>
explanatory math, because there's a difference of *psychology* that is
<br>
downright bizarre.  If you ask how much people are willing to pay not to
<br>
get shot, they name the entire amount of money in their bank account.  If
<br>
you ask people how much they're willing to pay for the entire human species
<br>
to survive, most of them name the amount of money in their pockets, minus
<br>
whatever they need to pay for their accustomed lunch.  If it was *only*
<br>
their own life at stake, not them plus the rest of the human species,
<br>
they'd drop everything to handle it.  There's an eerie echo here with the
<br>
observation that anything that benefits or harms the entire human species
<br>
is an evolutionary null-op.  But I looked, and I didn't find any plausible
<br>
direct connection, so it probably happens for other reasons.  Maybe
<br>
something that threatens everyone is something that someone else might
<br>
handle - so hang back and wait another 20 seconds, or another 20 years.
<br>
<p>This kind of evolutionary arms race between individuals can promote alleles
<br>
to universality that cause major group disasters.  Individual selection can
<br>
promote alleles to universality that result in the extinction of the entire
<br>
species.  Natural selection exercises no foresight, no extrapolation, does
<br>
not ask &quot;if this goes on&quot;.  Natural selection is a tautology: alleles that
<br>
increase their proportions in the gene pool become universal.  Often the
<br>
winning alleles look to a human like clever design, but sometimes the same
<br>
math can promote downright suicidal alleles.  George Williams's classic
<br>
&quot;Adaptation and Natural Selection&quot; (published in 1966 and still an
<br>
excellent read) debunked the then-popular notion that evolution worked for
<br>
the good of species or ecologies.  Williams discusses how individual
<br>
selection can create group disasters or, indeed, wipe out an entire
<br>
species.  Happens all the time, apparently.
<br>
<p>I don't think that procrastination deserves the death penalty.  But I'm
<br>
human and I have wacky human notions about mercy, kindness, second chances,
<br>
fair warnings, consequences proportional to acts.  Maybe Nature has other
<br>
ideas.
<br>
<p>So before that arms race of individual procrastination causes a species
<br>
catastrophe, I want to get past this weird psychology that distinguishes
<br>
between benefits to only yourself, and benefits to you *plus* everyone
<br>
else.  It really bugs me that if there was some kind of legitimate reason
<br>
why the Singularity Institute *had* to build a Friendly AI that benefited
<br>
only SIAI donors, we'd probably have a lot more donors.  It's the *same
<br>
benefit*!  Does it not count if other people get it too?
<br>
<p>Of course you want to help.  It's not like you're a bad person or anything.
<br>
But there are these perfectly reasonable reasons why it makes sense to
<br>
wait another year before helping, that somehow don't apply to buying a
<br>
movie ticket or a cheeseburger.  Even though, when you think about it, it's
<br>
not the same order of personal benefit we're talking about here.
<br>
Evolutionary psychology is subtle and sometimes downright stupid when it
<br>
messes with your head.
<br>
<p>Here's a thought experiment:  If I offered people, for ten dollars, a pill
<br>
that let them instantly lose five pounds of fat or gain five pounds of
<br>
muscle, they'd buy it, right?  They'd buy it today, and not sometime in the
<br>
indefinite future when their student loans were paid off.  Now, why do so
<br>
few people get around to sending even ten dollars to the Singularity
<br>
Institute?  Do people care more about losing five pounds than the survival
<br>
of the human species?  For that matter, do you care more about losing five
<br>
pounds than you care about extending your healthy lifespan, or about not
<br>
dying of an existential risk?  When you make the comparison explicitly, it
<br>
sounds wrong - but how do people behave when they consider the two problems
<br>
in isolation?  People spend more on two-hour movies than they ever get
<br>
around to donating to the Singularity Institute.  Cripes, even in pure
<br>
entertainment we provide a larger benefit than that!
<br>
<p>The two questions seem to be handled by different areas of the brain.
<br>
There are self-benefiting actions, that we go out and do right now using
<br>
any necessary resources.  And there's philanthropy, which we'll get to at
<br>
some point in the indefinite future, if there are any free resources that
<br>
we aren't using for something else.  And, as only the Singularity Institute
<br>
could demonstrate, this difference in emotional psychology doesn't even
<br>
seem to depend on whether the philanthropic benefit that lands on *you
<br>
personally* is *larger* than the selfish benefit.  If I had an fMRI machine
<br>
I could probably show that the two questions activate different brain
<br>
areas.  One emotional module procrastinates indefinitely, hoping that
<br>
someone else will do it instead.  The other emotional module goes out and
<br>
does it right away before anyone else gets there first.
<br>
<p>If you knew you were going to get your name on a monument and get awed
<br>
looks at social functions for the next thousand years, you'd probably make
<br>
certain you seized the moment and sent in *some* kind of donation.
<br>
Preferably one you weren't embarrassed to talk about a hundred years later,
<br>
but yeah, fifty bucks if it came down to that, just to make sure you sent
<br>
in *something*.  For the honor of the human species, to bump the count up
<br>
to five thousand twenty-five.  Because it sure would be embarrassing to
<br>
forget, and not get around to it in time, and spend the next thousand years
<br>
kicking yourself.  And, y'know, according to my current understanding, this
<br>
scenario isn't really all that unlikely.  So if fame is what it takes to
<br>
get you moving, then by all means go for it.  But whatever it takes to kick
<br>
yourself out of bystander mode, please do!  For it is also awful to forget
<br>
and not get around to it in time, if what's at stake is the survival of
<br>
humankind, and you don't have a thousand years to kick yourself afterward
<br>
because the human species lost.  And yet somehow the psychology seems to be
<br>
different; for in the first case people do it today, and in the second case
<br>
they plan to do it next year.
<br>
<p>There is a failure I would warn you against, a bug in the human
<br>
architecture.  Since the Singularity Institute booted up, I have observed
<br>
this surprising fact: donors donate, and nondonors don't.  People who
<br>
donate this year will, very likely, donate again next year.  People who
<br>
plan to donate next year will, next year, still be planning to donate next
<br>
year.  If you want to give to the Singularity Institute someday, the best
<br>
thing you can do to ensure that is to pull up
<br>
<a href="http://intelligence.org/donate.html">http://intelligence.org/donate.html</a> and send a hundred dollars, right now.
<br>
That makes you a donor instead of a non-donor.  But if you donate only a
<br>
hundred dollars, won't that prevent you from donating five thousand
<br>
dollars, which you were planning to do any time now?  No.  Non sequitur.
<br>
Just because you've donated a hundred dollars doesn't mean you can't donate
<br>
more.  What it does is transform you from a non-donor into a donor.  That
<br>
is progress, for donors donate, and non-donors don't.
<br>
<p>The title of this message is &quot;Donate Today and Tomorrow&quot;.  The usual motto
<br>
for fighting procrastination is &quot;today, not tomorrow&quot;.  Yet it seems to me
<br>
that this is not the way the human mind works.  It's either &quot;today and
<br>
tomorrow&quot;, or &quot;neither today nor tomorrow&quot;.  The difficult part of keeping
<br>
the Singularity Institute alive isn't finding people who want us to win,
<br>
it's getting you to do something about it - to throw that mental switch
<br>
from &quot;someday in the indefinite future&quot; to &quot;in the next 48 hours after
<br>
reading this message&quot;.
<br>
<p>Yes, the Singularity Institute is running a *very brief* fundraising
<br>
campaign.  Otherwise everyone waits until the last minute, hoping someone
<br>
else will donate, and then they forget.  Our new campaign lasts 72 hours, 
<br>
terminating at 5AM Eastern Time, Sunday, October 24th.  Hopefully this will 
<br>
give almost everyone a chance to donate in the next 48 hours after reading 
<br>
this message.  No one finds out how much someone else donated until 
<br>
afterward.  So act now, before it's too late.  Just like real life, 
<br>
compressed into a slightly shorter timescale.
<br>
<p>If you didn't even see this message until too late, I suppose you could
<br>
take 24 hours from whenever you read it.  A day may not seem like a large
<br>
unit of time, but it's made up of hours, and an hour is made up of minutes.
<br>
Even a minute is enough time to think, if you think now instead of later.
<br>
Athletes and police officers and martial artists make huge choices in
<br>
seconds because they don't think of themselves as slow.   What are you
<br>
waiting for?  You're faster than this.  Don't think you are, know you are.
<br>
<p>You can always donate afterward too, of course.  Donate today and tomorrow.
<br>
<p>But now would be a good time.  Now is when the Challenge Grant runs.  Now
<br>
is when the IRS review of our public charity status approaches.  The next
<br>
72 hours is when we're running our &quot;Donate Today and Tomorrow&quot; campaign.
<br>
If you can't be a major donor, we're looking for a typical donation of one
<br>
hundred dollars.  Everyone who reads this and hasn't already donated,
<br>
please.  More than a hundred dollars would be wonderful.  If it helps,
<br>
think of the monument and what you'll be telling people at social functions
<br>
for the next thousand years.  But we need a broader donor base, so please
<br>
send something.  If you just can't handle a hundred dollars, and you're at
<br>
least as well off as that unknown high school student, match that
<br>
ten-dollar donation.  Think of it as voting to tell the IRS that you
<br>
approve of the Singularity Institute and you want SIAI to have permanent
<br>
public charity status.  Think of it as making sure that the human species
<br>
doesn't end up with an embarrassingly small monument.  Think of it as
<br>
letting us know that you exist and you care and we don't have to do this alone.
<br>
<p>It's your responsibility, you, yes, you personally.  If I could insert
<br>
&lt;your name here&gt; into this email I'd do it.  There are six billion people
<br>
in this world.  An infinitesimal fraction have the faintest inkling of
<br>
what's at stake.  If you don't step up, that's it.  No one will.  It's you
<br>
or no one.
<br>
<p>This essay began when someone wrote to me, explaining why he didn't think
<br>
his potential donations were important:  &quot;Trying to support the Institute
<br>
financially, I believe that I wouldn't be able to offer much more than
<br>
other professional contributors (you know, contributors that are
<br>
professionals).&quot;  That was the anvil that broke the camel's back.  Only 8
<br>
donors in the history of the Singularity Institute have given more than
<br>
$5000.  Right now it doesn't take much to make you a big name in the
<br>
history of the Singularity Institute.  Being a major donor may not seem
<br>
like a glamorous, rare part to play in the unfolding of history, but it
<br>
*is*.  And if it weren't glamorous, that wouldn't make the tiniest
<br>
difference.  It's probably going to take something like ten major, steady
<br>
repeat donors for each full-time specialist on the Friendly AI programming
<br>
team.  That's what it takes to get the job done.
<br>
<p>When this essay was done, I sent it to an SIAI volunteer, a non-donor who'd
<br>
been hanging around since 2001.  He read it all, offered a number of
<br>
comments and suggestions for getting past the bystander effect, and then
<br>
casually added:  &quot;But I have an active reason for not donating regularly.&quot;
<br>
When I was done banging my head against the keyboard, I asked why.  This
<br>
was an error: his reasons started to get more elaborate as he explained.
<br>
That's another failure mode - developing intricate justifications for not
<br>
donating.  If you decide not to donate, leave your options open for the
<br>
future.  Don't expend great effort trying to justify your decision to
<br>
yourself, lest you succeed.  You have no need to justify your decision to
<br>
me, or to anyone.  If you dislike your choice, change it!  If you approve
<br>
your choice, do it without apology!  That volunteer did realize, after
<br>
cogitating further, that despite his reasons it didn't make sense to donate
<br>
*nothing* - he signed up for a twenty-dollar monthly donation.  And I
<br>
rejoiced, for there was one more donor, and a monthly donor beside.  One
<br>
more when the IRS asks how many people care about this effort.  One more
<br>
who might donate more someday.  One more who will be able to say afterward:
<br>
&quot;I was there.&quot;  I want everyone's name on the monument.  Seriously.
<br>
<p>People assume that someone else is taking care of it.  That professional
<br>
contributors are donating.  They're not!  Other people are not taking care
<br>
of it for you, and you shouldn't wish that!  This is *your* chance to make
<br>
a difference!  Not someone else, you!  You you you!  You, born into this
<br>
generation; you, one of the first intelligent beings ever to exist; you,
<br>
one of only six billion sentients that can potentially intervene in this
<br>
matter, out of all the countless minds that will someday (we hope) come
<br>
into existence.  And as if that isn't enough, you're among the tiny
<br>
fraction that knows what's going on and is in a position to do something
<br>
about it.  That's as targeted as it gets, the finger pointing to you and
<br>
you alone.  You can't get any more personally responsible than that.  No
<br>
more diffusion of responsibility.  No more bystander apathy.  This problem
<br>
landed in *your* lap.
<br>
<p>What if you want to throw that mental switch from &quot;someday&quot; to &quot;now&quot;, but
<br>
you're having trouble actually doing it?  I know the feeling.  Here's my
<br>
suggestion:  If you're a potential major donor, pick up your checkbook and
<br>
write out a check for at least one thousand dollars, now.  There's nothing
<br>
irrevocable about that.  Later you can rip up the check and not mail it, or
<br>
rip it up and write a check for five thousand instead.  But perform the
<br>
action.  Break the mental inertia.  Give yourself a little reminder to
<br>
stare at you.  If it stares at you for too long, rid yourself of the
<br>
problem by tearing it up or mailing it.  If you're thinking of a lesser
<br>
donation and you haven't gotten around to it, open up
<br>
<a href="http://intelligence.org/donate.html">http://intelligence.org/donate.html</a>, and let it stare at you until you decide
<br>
how to get rid of it.  But do something *now*.  Defy paralysis.  Take the
<br>
first step, and the second step will be easier.  The scariest part isn't
<br>
leaping onstage - it's standing up in the audience.
<br>
<p><p>Yours,
<br>
Eliezer Yudkowsky,
<br>
for the Singularity Institute for Artificial Intelligence, Inc.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10007.html">Eliezer Yudkowsky: "Re: [agi] A difficulty with AI reflectivity"</a>
<li><strong>Previous message:</strong> <a href="10005.html">Christian Szegedy: "Re: [agi] A difficulty with AI reflectivity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10017.html">Slawomir Paliwoda: "Re: Donate Today and Tomorrow"</a>
<li><strong>Reply:</strong> <a href="10017.html">Slawomir Paliwoda: "Re: Donate Today and Tomorrow"</a>
<li><strong>Reply:</strong> <a href="10021.html">Marc Geddes: "Re: SIAI:  Donate Today and Tomorrow"</a>
<li><strong>Maybe reply:</strong> <a href="10030.html">Aikin, Robert: "RE: SIAI:  Donate Today and Tomorrow"</a>
<li><strong>Maybe reply:</strong> <a href="10041.html">Aikin, Robert: "RE: SIAI:  Donate Today and Tomorrow"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10006">[ date ]</a>
<a href="index.html#10006">[ thread ]</a>
<a href="subject.html#10006">[ subject ]</a>
<a href="author.html#10006">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:49 MDT
</em></small></p>
</body>
</html>
