<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Donate Today and Tomorrow</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Donate Today and Tomorrow">
<meta name="Date" content="2004-10-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Donate Today and Tomorrow</h1>
<!-- received="Fri Oct 22 04:00:14 2004" -->
<!-- isoreceived="20041022100014" -->
<!-- sent="Fri, 22 Oct 2004 06:00:13 -0400" -->
<!-- isosent="20041022100013" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Donate Today and Tomorrow" -->
<!-- id="4178DA2D.8030806@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="BAY22-DAV16SFsxf4yp000001eb@hotmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Donate%20Today%20and%20Tomorrow"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri Oct 22 2004 - 04:00:13 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10024.html">Wei Dai: "Re: [agi] A difficulty with AI reflectivity"</a>
<li><strong>Previous message:</strong> <a href="10022.html">Yan King Yin: "Re: [agi] A difficulty with AI reflectivity"</a>
<li><strong>In reply to:</strong> <a href="10017.html">Slawomir Paliwoda: "Re: Donate Today and Tomorrow"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10106.html">Slawomir Paliwoda: "Re: Donate Today and Tomorrow"</a>
<li><strong>Reply:</strong> <a href="10106.html">Slawomir Paliwoda: "Re: Donate Today and Tomorrow"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10023">[ date ]</a>
<a href="index.html#10023">[ thread ]</a>
<a href="subject.html#10023">[ subject ]</a>
<a href="author.html#10023">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Slawomir Paliwoda wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt;&gt; Here's a thought experiment:  If I offered people, for ten dollars, a
</em><br>
<em>&gt;&gt; pill that let them instantly lose five pounds of fat or gain five
</em><br>
<em>&gt;&gt; pounds of muscle, they'd buy it, right?  They'd buy it today, and not
</em><br>
<em>&gt;&gt; sometime in the indefinite future when their student loans were paid
</em><br>
<em>&gt;&gt; off.  Now, why do so few people get around to sending even ten dollars
</em><br>
<em>&gt;&gt; to the Singularity Institute?  Do people care more about losing five
</em><br>
<em>&gt;&gt; pounds than the survival of the human species?  For that matter, do
</em><br>
<em>&gt;&gt; you care more about losing five pounds than you care about extending
</em><br>
<em>&gt;&gt; your healthy lifespan, or about not dying of an existential risk?
</em><br>
<em>&gt;&gt; When you make the comparison explicitly, it sounds wrong - but how do
</em><br>
<em>&gt;&gt; people behave when they consider the two problems in isolation?
</em><br>
<em>&gt;&gt; People spend more on two-hour movies than they ever get around to
</em><br>
<em>&gt;&gt; donating to the Singularity Institute.  Cripes, even in pure 
</em><br>
<em>&gt;&gt; entertainment we provide a larger benefit than that!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer, I think your involvement in this project has caused you to lose
</em><br>
<em>&gt; a bit of the sense of objectivity necessary to evaluate true options 
</em><br>
<em>&gt; included in your thought experiment, and I infer that from your 
</em><br>
<em>&gt; question: &quot;Do people care more about losing five pounds than the 
</em><br>
<em>&gt; survival of the human species?&quot; What this question implies is the 
</em><br>
<em>&gt; assumption that donating to SIAI equates to preventing existential risks
</em><br>
<em>&gt; from happening. Your question has an obvious answer. Of course people 
</em><br>
<em>&gt; care more about survival of human species than losing five pounds, but 
</em><br>
<em>&gt; how do we know that SIAI, despite its intentions, is on a straight path
</em><br>
<em>&gt; to implementing humanity-saving technology?
</em><br>
<p>Do I have to point out that people spend a heck of a lot more than ten 
<br>
dollars trying to lose five pounds, based on schemes with a heck of a lot 
<br>
less rational justification than SIAI has offered?  My puzzle still stands. 
<br>
&nbsp;&nbsp;The possibility of humanity being wiped out seems to have less 
<br>
psychological force than the opportunity to lose five pounds.  No matter 
<br>
how much we grow, I don't think we'll match the membership or resource 
<br>
expenditure of any major weight-loss meme.  That's just not psychologically 
<br>
realistic given human nature.  Now I do not think that so much resources 
<br>
should be required.  I'll be surprised if we need more than two percent of 
<br>
the cost of a B-2 bomber.  But my puzzle stands.
<br>
<p><em>&gt; What makes your
</em><br>
<em>&gt; organization different from, say, an organization that also claims to
</em><br>
<em>&gt; save the world, but by different means, like prayer, for instance?
</em><br>
<p>Rationality.  Prayer doesn't work given our universe's laws of physics, and 
<br>
that makes it an invalid solution no matter what the morality.
<br>
<p>Isn't this *exactly* the same argument that people use against cryonics or 
<br>
nanotechnology?
<br>
<p><em>&gt; And
</em><br>
<em>&gt; no, I'm not trying to imply anything about cults here, but I'm trying to
</em><br>
<em>&gt; point out the common factor between the two organizations which is that,
</em><br>
<em>&gt; assuming it's next to impossible to truly understand CFAI and LOGI,
</em><br>
<em>&gt; commitment to these projects requires faith in implementation and belief
</em><br>
<em>&gt; that the means will lead to intended end. One cannot aspire to
</em><br>
<em>&gt; rationalism and rely on faith at the same time.
</em><br>
<p>Bayesians may and must look at what other Bayesians think and account it as 
<br>
evidence.  (&quot;Must&quot;, because a Bayesian is commanded to take every scrap of 
<br>
available information into account, ignoring none upon peril of paradox. 
<br>
Jaynes 1:14.)   Robin Hanson wrote a fascinating paper on meta-rationality 
<br>
which proves from reasonable assumptions that Bayesians cannot agree to 
<br>
disagree; they must have the same probability judgment once they both know 
<br>
the other's, both know the other knows theirs, etc.  Nick Bostrom and I, on 
<br>
the way back from Extro 5, tried to split a taxi ride and found an extra 
<br>
$20 bill in our contributions.  I thought the $20 was his, he thought it 
<br>
was mine.  We had both read Hanson on meta-rationality, and we both knew 
<br>
what we had to do.  He named the probability he thought it was his (20%), I 
<br>
named the probability I thought it was mine (15%), and we split it in 20:15 
<br>
ratio.
<br>
<p><a href="http://hanson.gmu.edu/deceive.pdf">http://hanson.gmu.edu/deceive.pdf</a>
<br>
<p>Guessing how much other people know relative to you is not faith, so long 
<br>
as you pursue it as a question of simple fact, taking into account neither 
<br>
personal likes nor personal dislikes.
<br>
<p><em>&gt; I've noticed a Matrix quote in your essay, (&quot;Don't think you are, know 
</em><br>
<em>&gt; you are&quot;). There is an equally interesting quote from Reloaded you might
</em><br>
<em>&gt; agree with, and it is when Cornel West responds to one of Zion's 
</em><br>
<em>&gt; commanders, &quot;Comprehension is not requisite for cooperation.&quot; And even 
</em><br>
<em>&gt; though I'm convinced that Matrix trilogy is an overlooked masterpiece, 
</em><br>
<em>&gt; much farther ahead of its time than Blade Runner ever hoped to be, I 
</em><br>
<em>&gt; don't think Mr. West was correct. Comprehension is indeed a requisite 
</em><br>
<em>&gt; for cooperation, and as long as you are unable to find a way to overcome
</em><br>
<em>&gt; the &quot;comprehension&quot; requirement, I don't think you should expect to
</em><br>
<em>&gt; find donors who don't understand exactly what you are doing and how.
</em><br>
<p>Which existential risks reality throws at you is completely independent of 
<br>
your ability to understand them; you have no right to expect the two 
<br>
variables to correlate.  Do you think it the best policy for Earth's 
<br>
survival that nobody ever support an existential-risk-management project 
<br>
unless they can comprehend all the science involved?  We'd better hope 
<br>
there's not a single existential risk out there that's hard to understand. 
<br>
&nbsp;&nbsp;If it requires an entire semester of college to explain, we're doomed.
<br>
<p>I've tried very hard to explain what we're doing and how, but I also have 
<br>
to do the actual work, and I'm becoming increasingly nervous about time. 
<br>
No matter how much I write, it will always be possible for people to demand 
<br>
more.  At some point I have to say, &quot;I've written something but not 
<br>
everything, and no matter what else I write, it will still be 'something 
<br>
but not everything'.&quot;
<br>
<p><em>&gt; Let's say I'm a potential donor. How do I know, despite sincere 
</em><br>
<em>&gt; intentions of the organization to save the world, that the world won't 
</em><br>
<em>&gt; &quot;drift toward tragedy&quot; as a result of FAI research made possible in part
</em><br>
<em>&gt; by my donation? How do I know what you know to be certain without 
</em><br>
<em>&gt; spending next 5 years studying?
</em><br>
<p>You guess, choosing a policy such that you would expect Earth to reliably 
<br>
survive technically intricate existential threats if everyone followed your 
<br>
rule.  It's irrational to allocate billions of dollars to publicly 
<br>
understandable but slight risks, and less than a million dollars to a much 
<br>
worse risk where it's harder for a member of the general public to 
<br>
understand the internals.
<br>
<p>The fact that the risk exists and that it's very severe should both be 
<br>
comprehensible - not easily, maybe, but you should still be able to see 
<br>
that, rationally, on the basis of what I've already written.  And if it's 
<br>
still hard to understand, what the hell am I supposed to do?  Turn a little 
<br>
dial to decrease the intrinsic difficulty of the problem?  Flip the switch 
<br>
on the back of my head from &quot;bad explainer&quot; to &quot;good explainer&quot;?  I do the 
<br>
best I can.  People can always generate more and more demands, and they do, 
<br>
because it feels reasonable and they don't realize the result of following 
<br>
that policy is an inevitable loss.
<br>
<p>I should point out that your argument incorporates a known, experimentally 
<br>
verified irrational bias against uncertainty.  People prefer to bet on a 
<br>
coin with known 50% odds, than to bet on a variable completely unknown to 
<br>
them, like a match between foreign sports teams.  From an expected utility 
<br>
standpoint, you should assign the two bets the same value, especially if 
<br>
you flip a coin to decide which team to bet upon (a manipulation that makes 
<br>
the problem transparent).  But people have a visceral dislike of 
<br>
uncertainty; they want to know *all* the details.  Even a single unknown 
<br>
detail can feel like an unscratched itch.
<br>
<p>I sympathize, of course.  Yay curiosity!  Go on studying!  But don't hold 
<br>
off your support until you've achieved a full technical understanding of 
<br>
AI, because that's a policy guaranteed to doom Earth if everyone follows 
<br>
it.  Though you didn't wait, of course; you are a prior SIAI donor, for 
<br>
which we thank you.  You asked legitimate questions with legitimate 
<br>
answers, but you still donated - you didn't do *nothing*.  I likewise hope 
<br>
you don't choose to wait on future donations.
<br>
<p>The idea that absolute proof is required to deal with an existential risk 
<br>
is another case of weird psychology.  Would you drive in a car that had a 
<br>
10% chance of crashing on every trip?  There's no *absolute proof* that 
<br>
you'll crash, so you can safely ignore the threat, right?  If people 
<br>
require absolute proof for existential risks before they'll deal with them, 
<br>
while reacting very badly to a 1% personal risk, then that is another case 
<br>
of weird psychology that needs explaining.
<br>
<p>As we all know, there's nothing worse in this world than losing face.  The 
<br>
most important thing in an emergency is to look cool and suave.  That's 
<br>
why, when Gandalf first suspected that Frodo carried the One Ring, he had 
<br>
to make *absolutely sure* that his dreadful guess was correct, 
<br>
interrogating Gollum, searching the archives at Gondor, before carrying out 
<br>
the tiniest safety precaution.  Like, say, sending Frodo to Rivendell the 
<br>
instant the thought crossed his mind.  What weight the conquest of all 
<br>
Middle-Earth, compared to the possibility of Gandalf with egg on his face? 
<br>
&nbsp;&nbsp;And the interesting thing is, I've never heard anyone else notice that 
<br>
there's something wrong with this.  It just seems like ordinary human 
<br>
nature.  Tolkien isn't depicting Gandalf as a bad guy.  Gandalf is just 
<br>
following the ordinary procedure of taking no precaution against an 
<br>
existential risk until it has been confirmed with absolute certainty, lest 
<br>
face be lost.
<br>
<p>I don't think it's a tiny probability, mind you.  I've already identified 
<br>
the One Ring to my satisfaction... and looked back, and saw that I'd been 
<br>
stupid and demanded too much evidence, and vowed not to make the mistake again.
<br>
<p>Imagine if everyone at the Council of Elrond had to call a six-month halt 
<br>
so they could also go check the archives at Gondor.  By all means send for 
<br>
some Xeroxes and study them at your first opportunity, but get on with 
<br>
saving the world meanwhile.  Sauron waits for no Xerox machine.
<br>
<p><em>&gt; Other questions: Why SIAI team would need so much money to continue
</em><br>
<em>&gt; building FAI if the difficulty of creating it does not lie in hardware?
</em><br>
<em>&gt; What are the real costs?
</em><br>
<p>Extremely smart programmers.  Who said anything about needing &quot;so much 
<br>
money&quot;?  I expect to save the world on a ridiculously low budget, but it 
<br>
will still have to be one to two orders of magnitude higher than it is now. 
<br>
&nbsp;&nbsp;Hiring extremely smart programmers does take more money than the trickle 
<br>
we have currently.
<br>
<p><em>&gt; Why the pursuit of fame has now become a just reason to support SIAI? 
</em><br>
<em>&gt; Are you suggesting that SIAI has acknowledged that ends justify means?
</em><br>
<p>I think better of someone who lusts after fame and contributes a hundred 
<br>
bucks than a pure altruist who never gets around to it.  I don't think that 
<br>
counts as saying that the end justifies the means.  The other way around: 
<br>
By their fruits ye shall know them.
<br>
<p><em>&gt; Increased donations give you greater power to influence the world. Do 
</em><br>
<em>&gt; you see anything wrong in entrusting a small group of people with the 
</em><br>
<em>&gt; fate of entire human race?
</em><br>
<p>I see something wrong with giving a small group of people the ability to 
<br>
command the rest of the human race, hence the collective volition model. 
<br>
As for *entrusting* the future - not to exercise humanity's decisions, but 
<br>
to make sure humanity exercises them - I will use whichever strategy seems 
<br>
to offer the greatest probability of success, including blinding the 
<br>
programmers as to the extrapolated future, keeping the programmers isolated 
<br>
from a Last Judge who can only return one bit of information, etc.  Or not, 
<br>
if I think of a better way.
<br>
<p>The alternative appears to be entrusting small groups of people who aren't 
<br>
even trying to solve the problem with the fate of the entire human race. 
<br>
That looks to me like a guaranteed loss and I'm not willing to accept that 
<br>
ending.
<br>
<p><em>&gt; What would you tell people objecting to that idea?
</em><br>
<p>&quot;I'm sorry.  Someone has to do something if any of us are going to survive, 
<br>
and this is the best way I've been able to find.  You object but I have not 
<br>
heard you say a better alternative, unless it is letting catastrophe go its 
<br>
way unhindered.  You can't argue fine points of moral dilemmas if you're dead.&quot;
<br>
<p><em>&gt; Do we have the right to end the world as we know it without their
</em><br>
<em>&gt; approval?
</em><br>
<p>There are no rights, only responsibilities.  I'll turn the question over to 
<br>
a collective volition if I can, but even then the moral dilemma remains, 
<br>
it's just not me who has to decide it.
<br>
<p>The question is not whether the world &quot;as we know it&quot; ends, for it always 
<br>
does, generation after generation, and each new generation acts surprised 
<br>
by this.  The question is what comes after.
<br>
<p><em>&gt; These are difficult questions which perhaps illustrate the difficulty of
</em><br>
<em>&gt; the deceptively simple choice to support, or not to support SIAI.
</em><br>
<p>Deciding whether to try to save the human species is an extremely 
<br>
complicated question.  You can get so tangled up in the intricacies that 
<br>
you forget the answer is obviously yes.
<br>
<p>Lest we lose momentum, would any of SIAI's new donors care to post some 
<br>
positive remarks on the Today and Tomorrow campaign?  Part of the problem 
<br>
that transhumanist organizations have in organizing, I think, is that when 
<br>
a new effort tries to launch, we hear from the critics but not all the 
<br>
people who approve; it creates a bias against anything getting started.
<br>
<p>SIAI *is* getting new donors as a result of this effort - though I won't 
<br>
tell you how many until it's over.  It's *your* opportunity, not anyone else's.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10024.html">Wei Dai: "Re: [agi] A difficulty with AI reflectivity"</a>
<li><strong>Previous message:</strong> <a href="10022.html">Yan King Yin: "Re: [agi] A difficulty with AI reflectivity"</a>
<li><strong>In reply to:</strong> <a href="10017.html">Slawomir Paliwoda: "Re: Donate Today and Tomorrow"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10106.html">Slawomir Paliwoda: "Re: Donate Today and Tomorrow"</a>
<li><strong>Reply:</strong> <a href="10106.html">Slawomir Paliwoda: "Re: Donate Today and Tomorrow"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10023">[ date ]</a>
<a href="index.html#10023">[ thread ]</a>
<a href="subject.html#10023">[ subject ]</a>
<a href="author.html#10023">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:49 MDT
</em></small></p>
</body>
</html>
