<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Approaching the end of Today and Tomorrow</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Approaching the end of Today and Tomorrow">
<meta name="Date" content="2004-10-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Approaching the end of Today and Tomorrow</h1>
<!-- received="Sat Oct 23 19:08:06 2004" -->
<!-- isoreceived="20041024010806" -->
<!-- sent="Sat, 23 Oct 2004 21:08:00 -0400" -->
<!-- isosent="20041024010800" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Approaching the end of Today and Tomorrow" -->
<!-- id="417B0070.50301@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Approaching%20the%20end%20of%20Today%20and%20Tomorrow"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Oct 23 2004 - 19:08:00 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10097.html">Ben Goertzel: "Ben vs. the AI academics..."</a>
<li><strong>Previous message:</strong> <a href="10095.html">Ben Goertzel: "RE: Weaknesses in FAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0411/10195.html">Robin Lee Powell: "Re: Approaching the end of Today and Tomorrow"</a>
<li><strong>Reply:</strong> <a href="../0411/10195.html">Robin Lee Powell: "Re: Approaching the end of Today and Tomorrow"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10096">[ date ]</a>
<a href="index.html#10096">[ thread ]</a>
<a href="subject.html#10096">[ subject ]</a>
<a href="author.html#10096">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
SIAI's 72-hour lightning campaign ends in eight hours - 5AM Eastern time on 
<br>
Sunday morning.  It's down to the last ninth.
<br>
<p>I'm sorry I wasn't able to answer all the questions asked in the time 
<br>
available.  Hopefully people chose a sensible criterion in which they 
<br>
realize I can't answer everything, but reasonable answers to some questions 
<br>
are a good sign for those not yet answered.  I hope to answer more, but 
<br>
that won't happen until after Today and Tomorrow.
<br>
<p>I think that transhumanism has some ways yet to go before we can compete 
<br>
with flying-saucer cults, but people still donated despite the naysaying, 
<br>
choosing to be strong without certainty.  It looks to me like the stream of 
<br>
donations slowed substantially after people began sharing their 
<br>
justifications for not donating, and did not quite gain back the momentum 
<br>
even after donors also began speaking up.  But the campaign had a fair run 
<br>
before the naysaying started.
<br>
<p>The right to disagree is both morally and pragmatically necessary.  A bias 
<br>
in which thoughts we choose to communicate is a bias in collective 
<br>
reasoning.  Still, just because our ethics mandate an act doesn't mean the 
<br>
act is without consequences.  We've learned to show our disagreements with 
<br>
one another.  I don't think we're quite as good at consciously choosing 
<br>
that it is possible to act coherently despite disagreements, even 
<br>
disagreements that seem important; or consciously correcting for the peer 
<br>
pressure felt when disagreement is more likely to be publicly aired than 
<br>
agreement.  It takes work.  It isn't natural to us.  But I think we can do 
<br>
it if we try.
<br>
<p>Some of the justification for not donating to the Singularity Institute 
<br>
took the form, &quot;Why aren't you further along / doing more?&quot;  Well, that's 
<br>
rather a Catch-22, isn't it?  If you think the Singularity Institute should 
<br>
be doing XYZ... go ahead, don't let me stop you.  It's your planet too. 
<br>
No, seriously, it's your planet too.  We took responsibility.  We didn't 
<br>
take responsibility away from you.
<br>
<p>The Singularity Institute is a banner planted in the ground, a line drawn 
<br>
in the sands of time:  This is where humanity stops being a victim and 
<br>
starts fighting back.  Sometimes humans wander up to the banner, see that 
<br>
not much of an army has gathered, and then wander away.  It can be hard to 
<br>
get the party rolling, if people only want to join after the room is 
<br>
already crowded.  No matter who else comes and goes, you will find Eliezer 
<br>
Yudkowsky standing by that banner, gnawing steadily away at the challenge 
<br>
of Friendly AI, which is one of the things that humanity needs to be doing 
<br>
at this point.  For SIAI to grow another step we need three things: enough 
<br>
steady funding to pay one more person, one more person to pay, and a 
<br>
worthwhile job that person can do.  It's the second requirement that's the 
<br>
most difficult, and what makes the second requirement difficult is the 
<br>
third requirement.  It isn't easy to find people who can do worthwhile 
<br>
jobs.  SIAI doesn't want to invent make-work, token efforts to show we're 
<br>
doing something.  But even if there were *no* active workers yet present at 
<br>
the banner, not even Eliezer Yudkowsky, there would still need to be a 
<br>
Singularity Institute.  There would still have to be a rallying point, a 
<br>
banner planted in the ground, a gathering place for the people who wanted 
<br>
to make it happen.  It would have to begin somewhere, and how else would it 
<br>
ever begin?
<br>
<p>One year ago we didn't have Tyler Emerson or Michael Wilson or Michael 
<br>
Anissimov.  Progress is being made.  If it's too slow to suit you, get out 
<br>
and push.  It's your planet and your problem.  We took responsibility but 
<br>
we didn't take it from you.
<br>
<p>Those who still haven't donated anything at all - ask yourself whether the 
<br>
Singularity Institute has been worth more to you, and to the transhumanist 
<br>
community, than the price of a movie ticket.  Our suggested donation was a 
<br>
hundred dollars, but if you can't afford that, ten dollars is better than 
<br>
nothing.
<br>
<p><a href="http://intelligence.org/donate.html">http://intelligence.org/donate.html</a>
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10097.html">Ben Goertzel: "Ben vs. the AI academics..."</a>
<li><strong>Previous message:</strong> <a href="10095.html">Ben Goertzel: "RE: Weaknesses in FAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0411/10195.html">Robin Lee Powell: "Re: Approaching the end of Today and Tomorrow"</a>
<li><strong>Reply:</strong> <a href="../0411/10195.html">Robin Lee Powell: "Re: Approaching the end of Today and Tomorrow"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10096">[ date ]</a>
<a href="index.html#10096">[ thread ]</a>
<a href="subject.html#10096">[ subject ]</a>
<a href="author.html#10096">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:49 MDT
</em></small></p>
</body>
</html>
