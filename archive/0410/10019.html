<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: [agi] A difficulty with AI reflectivity</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: [agi] A difficulty with AI reflectivity">
<meta name="Date" content="2004-10-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: [agi] A difficulty with AI reflectivity</h1>
<!-- received="Thu Oct 21 19:52:35 2004" -->
<!-- isoreceived="20041022015235" -->
<!-- sent="Thu, 21 Oct 2004 21:52:37 -0400" -->
<!-- isosent="20041022015237" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: [agi] A difficulty with AI reflectivity" -->
<!-- id="JNEIJCJJHIEAILJBFHILCEMDCIAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="00af01c4b7d2$a67de460$6401a8c0@MRA02" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20[agi]%20A%20difficulty%20with%20AI%20reflectivity"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Thu Oct 21 2004 - 19:52:37 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10020.html">Marc Geddes: "Another one of my  'big picture' philosophical schemata"</a>
<li><strong>Previous message:</strong> <a href="10018.html">Michael Roy Ames: "Re: [agi] A difficulty with AI reflectivity"</a>
<li><strong>In reply to:</strong> <a href="10018.html">Michael Roy Ames: "Re: [agi] A difficulty with AI reflectivity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10012.html">Eliezer Yudkowsky: "Re: [agi] A difficulty with AI reflectivity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10019">[ date ]</a>
<a href="index.html#10019">[ thread ]</a>
<a href="subject.html#10019">[ subject ]</a>
<a href="author.html#10019">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi,
<br>
<p>About the comment that humans are doing something different from Godel
<br>
machines and other theorem-proving-type AI systems.
<br>
<p>Indeed, we are!
<br>
<p>Godel's theorem discusses the possibility (or otherwise) of systems of logic
<br>
that are both complete and consistent and powerful...
<br>
<p>However, humans would seem NOT to be logically consistent, so the
<br>
applicability of this theorem to humans is a bit suspect...
<br>
<p>We have sophisticated, nonlinear attention-allocation systems that allow us
<br>
to manage our inconsistencies, in a way that works OK given the environments
<br>
we've evolved for...
<br>
<p>But the problems faced by an inconsistency-embracing intelligence
<br>
architecture are quite different from the ones faced by a
<br>
mathematically-consistency-based intelligence architecture
<br>
<p>Mathematical consistency would seem to require massively more computational
<br>
resources than human-cognition-like strategies for
<br>
inconsistency-embracing...
<br>
<p>The way the human mind deals with &quot;This sentence is false&quot;, on an initial
<br>
intuitive basis, is to embrace the inconsistency, in a manner similar to how
<br>
we embrace many of our internal inconsistencies.  Of course this is
<br>
different than how a fully consistent AI system would experience such a
<br>
paradox.
<br>
<p>Whether consistency is &quot;better&quot; in an AI system is a complicated question,
<br>
unless one assumes infinite or essentially infinite resources.  Consistency
<br>
is better &quot;all else equal&quot; but the computational cost of maintaining
<br>
consistency is probably very high, and the effort spent on this may be
<br>
better spent on other things (as is my suspicion).
<br>
<p>Novamente, as you may have guesses, is not guaranteed to be consistent,
<br>
though it can strive for consistency in particular domains when this is
<br>
judged important by it.
<br>
<p>-- Ben G
<br>
<p><p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20[agi]%20A%20difficulty%20with%20AI%20reflectivity">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20[agi]%20A%20difficulty%20with%20AI%20reflectivity">owner-sl4@sl4.org</a>]On Behalf Of Michael
</em><br>
<em>&gt; Roy Ames
</em><br>
<em>&gt; Sent: Thursday, October 21, 2004 9:01 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=RE:%20[agi]%20A%20difficulty%20with%20AI%20reflectivity">sl4@sl4.org</a>
</em><br>
<em>&gt; Subject: Re: [agi] A difficulty with AI reflectivity
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Eliezer wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Schimdhuber's original claim, that the
</em><br>
<em>&gt; &gt; Godel Machine could rewrite every part
</em><br>
<em>&gt; &gt; of its own code including tossing out
</em><br>
<em>&gt; &gt; the need for a theorum-prover, I would
</em><br>
<em>&gt; &gt; consider wrap-around reflectivity.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Fair enough, thank you.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; A human's ability to have this kind of
</em><br>
<em>&gt; &gt; email conversation - reflect on different
</em><br>
<em>&gt; &gt; possible architectures for reflectivity -
</em><br>
<em>&gt; &gt; I would consider an even higher kind of
</em><br>
<em>&gt; &gt; wrap-around, and the kind I'm most
</em><br>
<em>&gt; &gt; interested in.
</em><br>
<em>&gt;
</em><br>
<em>&gt; A &quot;higher kind?  Different, certainly, as we can rewrite little of our
</em><br>
<em>&gt; cognitive processes.  But use of the word 'higher' leads me to suspect
</em><br>
<em>&gt; phantoms.  As neither of us know yet how humans do reflectivity,
</em><br>
<em>&gt; the best we
</em><br>
<em>&gt; can do is discuss our guesses.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; We are algorithms, but I don't think we're
</em><br>
<em>&gt; &gt; doing the same sort of thing that a
</em><br>
<em>&gt; &gt; reflective theorem-prover would do. For
</em><br>
<em>&gt; &gt; example, humans can actually be temporarily
</em><br>
<em>&gt; &gt; confused by &quot;This statement is false&quot;,
</em><br>
<em>&gt; &gt; rather than using a language which refuses
</em><br>
<em>&gt; &gt; to form the sentence.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The human experience of confusion manifests when we encounter
</em><br>
<em>&gt; data that has
</em><br>
<em>&gt; very poor correspondence with our internal model of that part of
</em><br>
<em>&gt; the world.
</em><br>
<em>&gt; A theorum prover decides on the truth or falsehood of a
</em><br>
<em>&gt; statements based on
</em><br>
<em>&gt; its axioms, and that is all it can do. Humans can additionally
</em><br>
<em>&gt; decide if the
</em><br>
<em>&gt; statement fits their current model of reality.  If a theorem prover were
</em><br>
<em>&gt; complex enough to contain a non-trivial model of reality against which it
</em><br>
<em>&gt; could decide 'does it fit' questions, then it would come much closer to a
</em><br>
<em>&gt; human's abilities in wrap-around reflectivity.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; We're doing something else.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Agreed, though I would phrase it: &quot;We're doing more&quot;.  What is it
</em><br>
<em>&gt; you think
</em><br>
<em>&gt; we are doing?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Michael Roy Ames
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10020.html">Marc Geddes: "Another one of my  'big picture' philosophical schemata"</a>
<li><strong>Previous message:</strong> <a href="10018.html">Michael Roy Ames: "Re: [agi] A difficulty with AI reflectivity"</a>
<li><strong>In reply to:</strong> <a href="10018.html">Michael Roy Ames: "Re: [agi] A difficulty with AI reflectivity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10012.html">Eliezer Yudkowsky: "Re: [agi] A difficulty with AI reflectivity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10019">[ date ]</a>
<a href="index.html#10019">[ thread ]</a>
<a href="subject.html#10019">[ subject ]</a>
<a href="author.html#10019">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:49 MDT
</em></small></p>
</body>
</html>
