<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Donate Today and Tomorrow</title>
<meta name="Author" content="Ralph Cerchione (figment@boone.net)">
<meta name="Subject" content="Re: Donate Today and Tomorrow">
<meta name="Date" content="2004-10-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Donate Today and Tomorrow</h1>
<!-- received="Thu Oct 28 14:44:08 2004" -->
<!-- isoreceived="20041028204408" -->
<!-- sent="Thu, 28 Oct 2004 16:43:49 -0400" -->
<!-- isosent="20041028204349" -->
<!-- name="Ralph Cerchione" -->
<!-- email="figment@boone.net" -->
<!-- subject="Re: Donate Today and Tomorrow" -->
<!-- id="008601c4bd2e$d6098570$6501a8c0@ralph" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20041027062051.89733.qmail@web20228.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ralph Cerchione (<a href="mailto:figment@boone.net?Subject=Re:%20Donate%20Today%20and%20Tomorrow"><em>figment@boone.net</em></a>)<br>
<strong>Date:</strong> Thu Oct 28 2004 - 14:43:49 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10179.html">David Clark: "Re: Universal ethics"</a>
<li><strong>Previous message:</strong> <a href="10177.html">Damien Broderick: "Re: Psychodynamics cont..."</a>
<li><strong>In reply to:</strong> <a href="10155.html">Marc Geddes: "Re: Donate Today and Tomorrow"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10180.html">Slawomir Paliwoda: "Re: Donate Today and Tomorrow"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10178">[ date ]</a>
<a href="index.html#10178">[ thread ]</a>
<a href="subject.html#10178">[ subject ]</a>
<a href="author.html#10178">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Slawomir, everybody, an alternative viewpoint...
<br>
<p>&quot;Marc Geddes&quot; &lt;<a href="mailto:marc_geddes@yahoo.co.nz?Subject=Re:%20Donate%20Today%20and%20Tomorrow">marc_geddes@yahoo.co.nz</a>&gt; wrote...
<br>
<em>&gt; --- Slawomir Paliwoda &lt;<a href="mailto:velvethum@hotmail.com?Subject=Re:%20Donate%20Today%20and%20Tomorrow">velvethum@hotmail.com</a>&gt; wrote:
</em><br>
<em>&gt; &gt; Not when you start thinking about the consequences
</em><br>
<em>&gt; &gt; of the cause you are supporting. SIAI failing to build safe and humane
</em><br>
SI
<br>
<em>&gt; &gt; is not the worst thing that can happen. The worst thing that can happen
</em><br>
is,
<br>
<em>&gt; &gt; actually, SIAI succeeding at making SI that would later turn
</em><br>
<em>&gt; &gt; Unfriendly. It makes sense to support the cause when it is shown how the
</em><br>
project
<br>
<em>&gt; &gt; won't lead to UFAI in a way potential donors can understand. In absence
</em><br>
of
<br>
<em>&gt; &gt; comprehension, the only thing left is trust.
</em><br>
<em>&gt;
</em><br>
Okay, I'm not utterly convinced that creating fully sentient AI is nearly as
<br>
simple or inevitable as some enthusiasts like to insist. In fact, I think
<br>
it's telling that people closely involved in this research tend to make a
<br>
point of moderating their predictive statements after a while, even if they
<br>
remain optimistic. Personally, I suspect that given our present levels of
<br>
technology and intelligence (or close to them), creating AI could take many
<br>
decades.
<br>
<p>The key point, however, is &quot;given our present levels...(or close to them).&quot;
<br>
I don't think we're going to be stuck in our present cognitive range for
<br>
several more decades. We're making too many advances in areas such as
<br>
genetics, nootropics, non-invasive enhancement methods, accelerated
<br>
learning, etc. Not to mention that too many technological advances apply
<br>
directly to areas that can enhance that above research (the Human Genome
<br>
Project, new computers systematically engaged in basic independent
<br>
scientific research, improvements in scanning human brain functions, etc).
<br>
<p>To be quite frank, I think we're apt to have transhumans running around
<br>
before too much longer. And if we have superhuman intelligence in actual
<br>
human researchers, that makes many technological possibilities that much
<br>
more likely.
<br>
<p>In short, we're apt to have some form of AI available in the not-too-distant
<br>
future anyway. My own perspective is: If Friendly Artificial Intelligence
<br>
research doesn't cost that much, why not lay some groundwork out for it now?
<br>
Even if we don't get an actual FAI in the next couple of decades, if we have
<br>
intensively pursued several different routes to that goal for the next
<br>
twenty years, we'll have laid down a beaten path for future researchers.
<br>
Which means that a newly emerging transhuman or technological savant is more
<br>
apt to follow an effective route to FAI than to UFAI, all things being
<br>
equal.
<br>
<p><em>&gt;From my perspective, then, Friendly Artificial Intelligence research is a
</em><br>
bit of an insurance policy. It doesn't have to be an existential
<br>
all-or-nothing gamble. I'll settle for hedging my bets, personally. =)
<br>
<p><em>&gt; That's another point possibly deterring potential
</em><br>
<em>&gt; donars.  If Sing Inst is capable of actually 'saving
</em><br>
<em>&gt; the world', the flip side is that they are also
</em><br>
<em>&gt; capable of actually destroying it if their approach
</em><br>
<em>&gt; fails.
</em><br>
<em>&gt;
</em><br>
Actually, this is true. I take comfort in the belief that even if AI doesn't
<br>
emerge as a result of incremental stages of progress -- an improved assembly
<br>
line 'bot here, a masterful expert system there, a simple &quot;AI&quot; scientific
<br>
researcher there -- but in a sudden, blinding burst, I still think that
<br>
we'll be seeing such incremental progress in other areas of computer
<br>
research as well as in the development of potential transhumans. Meaning: I
<br>
suspect we'll have a lot more intelligence and resources to apply to this
<br>
problem before we actually solve it.
<br>
<p>Incidentally, the human race technically already faces a number of
<br>
existential threats. Nuclear weapons, various potential environmental or
<br>
natural catastrophes, a potential super-plague or nanowar... we're quite
<br>
capable of getting wiped out without AIs.
<br>
<p><em>&gt; Someone donating to Sing Inst may actually be
</em><br>
<em>&gt; *accelerating the destruction of the world* if Sing
</em><br>
<em>&gt; Inst creates an UFAI.
</em><br>
<em>&gt;
</em><br>
Duly noted. =) But seriously, people are going to continue computer
<br>
research, and they're going to continue AI research. Is an Institute
<br>
dedicated to creating a &quot;warmer, fuzzier AI&quot; that terrifying an option?
<br>
Given the alternative of people creating an AI who've never considered the
<br>
problem, or who assume that transcendent intelligence will make their
<br>
machine not only omniscient and omnipotent, but omnibenevolent as well?
<br>
<p><em>&gt; You can understand the dilemna of someone who agrees
</em><br>
<em>&gt; with basic Singulatarian concepts, but has serious
</em><br>
<em>&gt; doubts about the specific approach of Sing Inst...
</em><br>
<em>&gt;
</em><br>
<em>&gt; For instance take my stubborn claims:
</em><br>
<em>&gt;
</em><br>
<em>&gt; *No pratical (real-time) general intelligence without
</em><br>
<em>&gt; sentience is possible
</em><br>
<em>&gt;
</em><br>
This may be correct, but as I've pointed out before, we already have
<br>
computers conducting basic research in biotech/pharmaceuticals without
<br>
having come close to full sentience. Which means we may be able to
<br>
manufacture and commit vast amounts of effective brainpower to these
<br>
problems very soon indeed. Failing that, we're already applying that
<br>
intellect to problems related to creating full-fledged transhumans.
<br>
<p><em>&gt; *No completely selfless AI is possible
</em><br>
<em>&gt;
</em><br>
Ack. Depending on the level of intelligence... Hmm, I'm really not sure how
<br>
firm this point is. I'll hear out your arguments, of course. But subsentient
<br>
AI may not even be sufficiently self-aware to be self-centered. And higher
<br>
intelligences... I think anything we say about some hypothetical being with
<br>
a basic intellect that's a mere thousand times superior to our own, even
<br>
before counting its vastly swifter processing speeds, is probably rather
<br>
speculative at this point. =)
<br>
<p><em>&gt; *Collective Volition is impossible for Singleton AI to
</em><br>
<em>&gt; calculate and can't be imposed from the top-down
</em><br>
<em>&gt;
</em><br>
<em>&gt; What should I do?
</em><br>
<em>&gt;
</em><br>
<em>&gt; (a)  Trust that Eli is right and I'm simply mistaken?
</em><br>
<em>&gt;
</em><br>
He is the Great Leader.
<br>
<p><em>&gt; (b)  Stick to my guns and doubt Eli?
</em><br>
<em>&gt;
</em><br>
Do you question the _Great_Leader_?! =)
<br>
<p><em>&gt; (c)  Snap and start scribbling incomprehensible
</em><br>
<em>&gt; diagrams and spouting gibberish?
</em><br>
<em>&gt;
</em><br>
This would be my option. =)
<br>
<p>A very interesting post, Slawomir. I won't tell you what to do, actually. In
<br>
fact, my (still limited) resources are presently far more focused on
<br>
developing transhumans, because that means research into self-enhancement,
<br>
which has proven rather beneficial to my efforts to improve my financial
<br>
status and thus to acquire the significant resources to effect these other
<br>
issues.
<br>
<p>Choose the strategy that makes the most sense to you. Obviously. =)
<br>
<p>Ralph
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10179.html">David Clark: "Re: Universal ethics"</a>
<li><strong>Previous message:</strong> <a href="10177.html">Damien Broderick: "Re: Psychodynamics cont..."</a>
<li><strong>In reply to:</strong> <a href="10155.html">Marc Geddes: "Re: Donate Today and Tomorrow"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10180.html">Slawomir Paliwoda: "Re: Donate Today and Tomorrow"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10178">[ date ]</a>
<a href="index.html#10178">[ thread ]</a>
<a href="subject.html#10178">[ subject ]</a>
<a href="author.html#10178">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:49 MDT
</em></small></p>
</body>
</html>
