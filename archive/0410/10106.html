<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Donate Today and Tomorrow</title>
<meta name="Author" content="Slawomir Paliwoda (velvethum@hotmail.com)">
<meta name="Subject" content="Re: Donate Today and Tomorrow">
<meta name="Date" content="2004-10-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Donate Today and Tomorrow</h1>
<!-- received="Sun Oct 24 07:18:32 2004" -->
<!-- isoreceived="20041024131832" -->
<!-- sent="Sun, 24 Oct 2004 09:16:50 -0400" -->
<!-- isosent="20041024131650" -->
<!-- name="Slawomir Paliwoda" -->
<!-- email="velvethum@hotmail.com" -->
<!-- subject="Re: Donate Today and Tomorrow" -->
<!-- id="BAY22-DAV13llNY30kk0000554d@hotmail.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="4178DA2D.8030806@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Slawomir Paliwoda (<a href="mailto:velvethum@hotmail.com?Subject=Re:%20Donate%20Today%20and%20Tomorrow"><em>velvethum@hotmail.com</em></a>)<br>
<strong>Date:</strong> Sun Oct 24 2004 - 07:16:50 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10107.html">Eliezer Yudkowsky: "Re: Weaknesses in FAI"</a>
<li><strong>Previous message:</strong> <a href="10105.html">Ben Goertzel: "RE: Weaknesses in FAI"</a>
<li><strong>In reply to:</strong> <a href="10023.html">Eliezer Yudkowsky: "Re: Donate Today and Tomorrow"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10111.html">TC: "RE: Donate Today and Tomorrow"</a>
<li><strong>Reply:</strong> <a href="10111.html">TC: "RE: Donate Today and Tomorrow"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10106">[ date ]</a>
<a href="index.html#10106">[ thread ]</a>
<a href="subject.html#10106">[ subject ]</a>
<a href="author.html#10106">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt;&gt; Eliezer, I think your involvement in this project has caused you to lose
</em><br>
<em>&gt;&gt; a bit of the sense of objectivity necessary to evaluate true options
</em><br>
<em>&gt;&gt; included in your thought experiment, and I infer that from your question:
</em><br>
<em>&gt;&gt; &quot;Do people care more about losing five pounds than the survival of the
</em><br>
<em>&gt;&gt; human species?&quot; What this question implies is the assumption that
</em><br>
<em>&gt;&gt; donating to SIAI equates to preventing existential risks
</em><br>
<em>&gt;&gt; from happening. Your question has an obvious answer. Of course people
</em><br>
<em>&gt;&gt; care more about survival of human species than losing five pounds, but
</em><br>
<em>&gt;&gt; how do we know that SIAI, despite its intentions, is on a straight path
</em><br>
<em>&gt;&gt; to implementing humanity-saving technology?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Do I have to point out that people spend a heck of a lot more than ten
</em><br>
<em>&gt; dollars trying to lose five pounds, based on schemes with a heck of a lot
</em><br>
<em>&gt; less rational justification than SIAI has offered?  My puzzle still
</em><br>
<em>&gt; stands.
</em><br>
<p>True, SIAI has offered more rational justification, but at the end of the
<br>
day, people will either support the project or not. If your justification 
<br>
sounds
<br>
about 50% right to a donor, does it mean you should expect her to part with
<br>
her $5? I suspect that convincing someone half-way usually buys $0 in 
<br>
support. The point is
<br>
that $10 fat-reducing pill and $10 donation to an uncertain cause are not 
<br>
that different than a $10 lottery ticket. If you replace &quot;donation&quot; with a 
<br>
&quot;lottery ticket&quot;, then the choices in the puzzle might become more clear. 
<br>
Besides, this is not a typical lottery
<br>
ticket because it might either win you nothing less than paradise, but also 
<br>
hell or a quick death if you're lucky. There's a lot at stake in supporting 
<br>
SIAI than many people realize. We have everything to win, but also 
<br>
everything to lose. You've mentioned a monument, but someday, somewhere, 
<br>
there might be the wall of shame with our names on it in case things don't 
<br>
go as planned.
<br>
<p><p><em>&gt;&gt; What makes your
</em><br>
<em>&gt;&gt; organization different from, say, an organization that also claims to
</em><br>
<em>&gt;&gt; save the world, but by different means, like prayer, for instance?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Rationality.  Prayer doesn't work given our universe's laws of physics,
</em><br>
<em>&gt; and that makes it an invalid solution no matter what the morality.
</em><br>
<p><p>Okay, I'm all for rationality, but even rational, good people make mistakes. 
<br>
Why should I trust you that you won't make a mistake that snowballs into 
<br>
UFAI?
<br>
<p><p><em>&gt;&gt; And
</em><br>
<em>&gt;&gt; no, I'm not trying to imply anything about cults here, but I'm trying to
</em><br>
<em>&gt;&gt; point out the common factor between the two organizations which is that,
</em><br>
<em>&gt;&gt; assuming it's next to impossible to truly understand CFAI and LOGI,
</em><br>
<em>&gt;&gt; commitment to these projects requires faith in implementation and belief
</em><br>
<em>&gt;&gt; that the means will lead to intended end. One cannot aspire to
</em><br>
<em>&gt;&gt; rationalism and rely on faith at the same time.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Bayesians may and must look at what other Bayesians think and account it
</em><br>
<em>&gt; as evidence.
</em><br>
<p><p>Well, I'm sorry, but that's unacceptable. The views of self-proclaimed 
<br>
rationalists are not allowed to count as evidence. That seems like cheating. 
<br>
Well, no, that is cheating. The truth can only be verified by reality, not 
<br>
by minds. You can't perform experiments inside a mind and say, &quot;Well, I 
<br>
don't see anything wrong with my thesis so it must be correct.&quot;  Besides, 
<br>
how do we even know that a self-proclaimed rationalist is a true rationalist 
<br>
in the first place?
<br>
<p><p><em>&gt;&gt; Comprehension is indeed a requisite for cooperation, and as long as you
</em><br>
<em>&gt;&gt; are unable to find a way to overcome
</em><br>
<em>&gt;&gt; the &quot;comprehension&quot; requirement, I don't think you should expect to
</em><br>
<em>&gt;&gt; find donors who don't understand exactly what you are doing and how.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Which existential risks reality throws at you is completely independent of
</em><br>
<em>&gt; your ability to understand them; you have no right to expect the two
</em><br>
<em>&gt; variables to correlate.
</em><br>
<p><p>I never expected them to correlate. &quot;Comprehension&quot; here does not refer to
<br>
existential risks - these are easily understandable - but, rather, to 
<br>
technical
<br>
means designed to avoid these risks. Currently, I do not fully comprehend
<br>
them and remain unconvinced about how they help to avoid the UFAI risk.
<br>
<p><p><em>&gt; I've tried very hard to explain what we're doing and how, but I also have
</em><br>
<em>&gt; to do the actual work, and I'm becoming increasingly nervous about time.
</em><br>
<em>&gt; No matter how much I write, it will always be possible for people to
</em><br>
<em>&gt; demand more.  At some point I have to say, &quot;I've written something but not
</em><br>
<em>&gt; everything, and no matter what else I write, it will still be 'something
</em><br>
<em>&gt; but not everything'.&quot;
</em><br>
<p><p>I think the vast majority *gets* the &quot;why&quot; part of what you are doing, and, 
<br>
as your
<br>
casual reader, I'm confident you've written enough. (Mr. Bostrom's paper 
<br>
about existential risks is a classic in the genre, of course) It's the &quot;how&quot; 
<br>
part that hasn't gotten much coverage. &quot;How,&quot; as in, &quot;How is your project 
<br>
safe from UFAI risk?&quot; I suspect that, among all the people on this list, 
<br>
there is only one person besides Eliezer who might comprehend the answer to 
<br>
that question if that answer even exists. The rest can only justify their 
<br>
support for SIAI by having faith in the desirable outcome.
<br>
<p><p><em>&gt; And if it's still hard to understand, what the hell am I supposed to do?
</em><br>
<em>&gt; Turn a little dial to decrease the intrinsic difficulty of the problem?
</em><br>
<em>&gt; Flip the switch on the back of my head from &quot;bad explainer&quot; to &quot;good
</em><br>
<em>&gt; explainer&quot;?  I do the best I can.  People can always generate more and
</em><br>
<em>&gt; more demands, and they do, because it feels reasonable and they don't
</em><br>
<em>&gt; realize the result of following that policy is an inevitable loss.
</em><br>
<p><p>Your frustration is understandable. Nobody says it's your fault that your
<br>
explanations are not convincing enough for some. Part of the blame should be
<br>
assigned to inherently complex nature of the problem which is both extremely
<br>
difficult to comprehend and explain.
<br>
<p><p><em>&gt;&gt; Other questions: Why SIAI team would need so much money to continue
</em><br>
<em>&gt;&gt; building FAI if the difficulty of creating it does not lie in hardware?
</em><br>
<em>&gt;&gt; What are the real costs?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Extremely smart programmers.
</em><br>
<p><p>This is what I don't understand. If these singularitarian programmers were
<br>
true believers in the purpose of FAI, fully aware of the stakes involved,
<br>
why would they object to not receiving any compensation for their work? What
<br>
would a true singularitarian choose - living in poverty while saving the
<br>
world or waiting for the conditions such that he or she
<br>
won't have to live in poverty when the work on saving humanity begins?
<br>
<p>If 10 brilliant Seed AI programmers moved into a cheap house in Georgia and 
<br>
worked passionately on FAI despite poor living conditions, it would go a 
<br>
long way to persuade others of the sincerity of the project leaders as well 
<br>
as strength of conviction in their project. Posting programmers' biographies 
<br>
wouldn't hurt either.
<br>
<p><p><em>&gt;&gt; Why the pursuit of fame has now become a just reason to support SIAI? Are
</em><br>
<em>&gt;&gt; you suggesting that SIAI has acknowledged that ends justify means?
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think better of someone who lusts after fame and contributes a hundred
</em><br>
<em>&gt; bucks than a pure altruist who never gets around to it.  I don't think
</em><br>
<em>&gt; that counts as saying that the end justifies the means.  The other way
</em><br>
<em>&gt; around: By their fruits ye shall know them.
</em><br>
<p><p>Then how would you respond to a slogan, &quot;FAI cures cancer?&quot; I remember
<br>
you protesting against these kinds of tactics few years ago when I suggested
<br>
it as one of the ways to promote SIAI's work to a wider audience.
<br>
<p><p><em>&gt;&gt; Increased donations give you greater power to influence the world. Do you
</em><br>
<em>&gt;&gt; see anything wrong in entrusting a small group of people with the fate of
</em><br>
<em>&gt;&gt; entire human race?
</em><br>
<em>&gt;
</em><br>
<em>&gt; I see something wrong with giving a small group of people the ability to
</em><br>
<em>&gt; command the rest of the human race, hence the collective volition model.
</em><br>
<em>&gt; As for *entrusting* the future - not to exercise humanity's decisions, but
</em><br>
<em>&gt; to make sure humanity exercises them - I will use whichever strategy seems
</em><br>
<em>&gt; to offer the greatest probability of success, including blinding the
</em><br>
<em>&gt; programmers as to the extrapolated future, keeping the programmers
</em><br>
<em>&gt; isolated from a Last Judge who can only return one bit of information,
</em><br>
<em>&gt; etc.  Or not, if I think of a better way.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The alternative appears to be entrusting small groups of people who aren't
</em><br>
<em>&gt; even trying to solve the problem with the fate of the entire human race.
</em><br>
<em>&gt; That looks to me like a guaranteed loss and I'm not willing to accept that
</em><br>
<em>&gt; ending.
</em><br>
<p><p>I've never understood why doing nothing &quot;guarantees&quot; a loss. Last
<br>
time I checked, grey goo was unrealistic. What is the nature of the imminent 
<br>
threat SIAI tries to avoid? Is it not UFAI itself, i.e., a threat that might 
<br>
emerge from the work on FAI?
<br>
<p><p><em>&gt;&gt; Do we have the right to end the world as we know it without their
</em><br>
<em>&gt;&gt; approval?
</em><br>
<em>&gt;
</em><br>
<em>&gt; There are no rights, only responsibilities.  I'll turn the question over
</em><br>
<em>&gt; to a collective volition if I can, but even then the moral dilemma
</em><br>
<em>&gt; remains, it's just not me who has to decide it.
</em><br>
<p><p>People have rights. I don't understand how you can plan to honor these
<br>
rights, but only after Singularity.
<br>
<p><p><em>&gt; The question is not whether the world &quot;as we know it&quot; ends, for it always
</em><br>
<em>&gt; does, generation after generation, and each new generation acts surprised
</em><br>
<em>&gt; by this.  The question is what comes after.
</em><br>
<p><p>The consequences of emergence of FAI for humanity would be infinitely more 
<br>
profound than those caused by new generations. Generations have had only a 
<br>
temporary power
<br>
over humanity to steer it in different directions. In contrast, FAI will 
<br>
gain absolute and
<br>
eternal power over humanity. After FAI happens, no future generation will be
<br>
capable of undoing it.
<br>
<p>SIAI is attempting to create God which I do not object to. The scary part, 
<br>
though, is that nobody knows if that is going to be a benevolent God, and as 
<br>
long as its benevolence can't be proven, supporting creation of a God could 
<br>
lead to an eternal loss of humanity's potential. Choose wisely for this is 
<br>
my universe and potential too.
<br>
<p>Finally, Iet me share with you all my idea for increasing donations to SIAI. 
<br>
How about giving SIAI enthusiasts an opportunity to pay for essays and 
<br>
papers published by the institute? Even though I'm cautious about supporting 
<br>
SIAI by donating money, I would definitely see myself spending $15 for 
<br>
Eliezer's next versions of CFAI (Creating Humane Artificial Intelligence?) 
<br>
or LOGI. At least some of us would feel like we *bought* something to 
<br>
alleviate the sense of being a bit gullible for donating. Obviously, SIAI 
<br>
publications would offer more sympathetic enthusiasts, who clearly handle 
<br>
uncertainty better than I do, even more opportunities to donate or to even 
<br>
go beyond the minimum price of a publication as a sign of stronger support 
<br>
for the cause.
<br>
<p>Slawomir 
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10107.html">Eliezer Yudkowsky: "Re: Weaknesses in FAI"</a>
<li><strong>Previous message:</strong> <a href="10105.html">Ben Goertzel: "RE: Weaknesses in FAI"</a>
<li><strong>In reply to:</strong> <a href="10023.html">Eliezer Yudkowsky: "Re: Donate Today and Tomorrow"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10111.html">TC: "RE: Donate Today and Tomorrow"</a>
<li><strong>Reply:</strong> <a href="10111.html">TC: "RE: Donate Today and Tomorrow"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10106">[ date ]</a>
<a href="index.html#10106">[ thread ]</a>
<a href="subject.html#10106">[ subject ]</a>
<a href="author.html#10106">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:49 MDT
</em></small></p>
</body>
</html>
