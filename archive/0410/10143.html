<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: What I think is wrong with Eli's current approach</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: What I think is wrong with Eli's current approach">
<meta name="Date" content="2004-10-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: What I think is wrong with Eli's current approach</h1>
<!-- received="Mon Oct 25 07:36:53 2004" -->
<!-- isoreceived="20041025133653" -->
<!-- sent="Mon, 25 Oct 2004 09:36:57 -0400" -->
<!-- isosent="20041025133657" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: What I think is wrong with Eli's current approach" -->
<!-- id="JNEIJCJJHIEAILJBFHILMEEECJAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20041025053054.46154.qmail@web20227.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20What%20I%20think%20is%20wrong%20with%20Eli's%20current%20approach"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Mon Oct 25 2004 - 07:36:57 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10144.html">Robin Lee Powell: "Re: [HUMOR?] But will Penrose make any submissions?"</a>
<li><strong>Previous message:</strong> <a href="10142.html">Ben Goertzel: "RE: A funding suggestion: Solve The Riemann hypothesis ;)"</a>
<li><strong>In reply to:</strong> <a href="10139.html">Marc Geddes: "What I think is wrong with Eli's current approach"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10152.html">Marc Geddes: "RE: What I think is wrong with Eli's current approach"</a>
<li><strong>Reply:</strong> <a href="10152.html">Marc Geddes: "RE: What I think is wrong with Eli's current approach"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10143">[ date ]</a>
<a href="index.html#10143">[ thread ]</a>
<a href="subject.html#10143">[ subject ]</a>
<a href="author.html#10143">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Marc,
<br>
<p>I think it would be great if you'd summarize your ideas on AI, FAI, morality
<br>
and life in general into a crisp &amp; coherent document, and post the URL to
<br>
the list.
<br>
<p>I can see you've done a lot of deep thinking about these issues, but it's
<br>
often hard for  me to piece together your ideas from your various posts,
<br>
into a coherent point of view...
<br>
<p>-- Ben G
<br>
<p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20What%20I%20think%20is%20wrong%20with%20Eli's%20current%20approach">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20What%20I%20think%20is%20wrong%20with%20Eli's%20current%20approach">owner-sl4@sl4.org</a>]On Behalf Of Marc
</em><br>
<em>&gt; Geddes
</em><br>
<em>&gt; Sent: Monday, October 25, 2004 1:31 AM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=RE:%20What%20I%20think%20is%20wrong%20with%20Eli's%20current%20approach">sl4@sl4.org</a>
</em><br>
<em>&gt; Subject: What I think is wrong with Eli's current approach
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Eli said&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;Someone other than me needs to talk to Marc Geddes
</em><br>
<em>&gt; before he snaps completely and becomes another
</em><br>
<em>&gt; Mentifex, complete with incomprehensible ASCII
</em><br>
<em>&gt; diagrams. I don't have the time and I don't have the
</em><br>
<em>&gt; tact, but Geddes was once a promising mind and there
</em><br>
<em>&gt; might be some way to pull him out of this.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Heh.  The philosophical schematic was designed only to
</em><br>
<em>&gt; make sense to someone who already has an inkling of my
</em><br>
<em>&gt; general theory.  It wouldn’t mean much to you, but it
</em><br>
<em>&gt; does to me.  Be assured I do have a general theory
</em><br>
<em>&gt; that makes some degree of sense.  I suggest you come
</em><br>
<em>&gt; back and take a another look at my schematic after
</em><br>
<em>&gt; you’ve been working on FAI theory for 10 more years ;)
</em><br>
<em>&gt;
</em><br>
<em>&gt; But O.K Eli I promise that I will not continue to
</em><br>
<em>&gt; discuss my own ill-formed ideas on Sl4 (excepting this
</em><br>
<em>&gt; one post).  I am simply going to try to explain, as
</em><br>
<em>&gt; best I can, what I think is wrong with your current
</em><br>
<em>&gt; approach.
</em><br>
<em>&gt;
</em><br>
<em>&gt; My schematic on the SL4 wiki:
</em><br>
<em>&gt;
</em><br>
<em>&gt; <a href="http://www.sl4.org/bin/wiki.pl?FundamentalTheoremofMorality">http://www.sl4.org/bin/wiki.pl?FundamentalTheoremofMorality</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; You'll notice in my schematic the entry for 'POLITICS'
</em><br>
<em>&gt; and 'POSSIBILITY' in my matrix reads 'MARKET', which
</em><br>
<em>&gt; in my explanation of terms I said was referring to CV
</em><br>
<em>&gt; (Collective Volition is a kind of highly sophisticated
</em><br>
<em>&gt; 'futures market').  In fact you'll see that a lot of
</em><br>
<em>&gt; the issues that Eli is dealing with I have placed in
</em><br>
<em>&gt; the POLITICS row of the Cognition matrix, instead of
</em><br>
<em>&gt; the ETHICS row.
</em><br>
<em>&gt;
</em><br>
<em>&gt; In short I think that CV (Collective Volition) is 'a
</em><br>
<em>&gt; nice place to live', or *a good political system*.  It
</em><br>
<em>&gt; is *not*, I believe, something that can be
</em><br>
<em>&gt; comprehended by or embodied in a single agent.  So CV
</em><br>
<em>&gt; is the operational *global outcome* of morality, *not*
</em><br>
<em>&gt; something that a singleton AI  *does*.  What exactly
</em><br>
<em>&gt; do I mean?  Well, I'm really saying that I think that
</em><br>
<em>&gt; the whole top-down approach *can't work*
</em><br>
<em>&gt;
</em><br>
<em>&gt; Eli's requirement that his AI *not* be sentient should
</em><br>
<em>&gt; be the tip-off that there is something highly suspect
</em><br>
<em>&gt; and peculiar about his proposed RPOP.   Should actual
</em><br>
<em>&gt; consciousness emerge in simulations of sentients, then
</em><br>
<em>&gt; RPOP is immediately stymied, since it would not be
</em><br>
<em>&gt; able to make effective simulations of sentients
</em><br>
<em>&gt; without violations of *person-hood*.  Worse, a
</em><br>
<em>&gt; conscious (sentient) RPOP would immediately run into a
</em><br>
<em>&gt; problem with self-reference.  A conscious RPOP would
</em><br>
<em>&gt; be a *person* itself, and a suitably generalized
</em><br>
<em>&gt; definition of *person-hood* would end up with the RPOP
</em><br>
<em>&gt; having to include its own volition in the calculation
</em><br>
<em>&gt; of CV.  This leads a fatal infinite regress.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Is general intelligence without sentience possible?  I
</em><br>
<em>&gt; say not, not in any *practical* sense.  Of course we
</em><br>
<em>&gt; can imagine a theoretical general intelligence with
</em><br>
<em>&gt; infinite computational power.  In that case, I agree
</em><br>
<em>&gt; that general intelligence without sentience would be
</em><br>
<em>&gt; possible.  One would simply take a pure Bayesian
</em><br>
<em>&gt; reasoning machine, capable of duplicating any kind of
</em><br>
<em>&gt; intelligence without sentience by burning up as much
</em><br>
<em>&gt; computational resources as it needed.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But in the real world, there can be no such thing as
</em><br>
<em>&gt; *infinite computational resources*.  Any real world AI
</em><br>
<em>&gt; will only have access to *finite* computational
</em><br>
<em>&gt; resources at any given time.  General intelligence
</em><br>
<em>&gt; would require *useful computational short-cuts* in
</em><br>
<em>&gt; order to do useful things in real-time.  Theoretically
</em><br>
<em>&gt; ideal Bayesian reasoning won't work, because it will
</em><br>
<em>&gt; quickly run into computational intractability for
</em><br>
<em>&gt; complex problems.  So all finite resource AI's would
</em><br>
<em>&gt; need *specialized computational short-cuts*.  And
</em><br>
<em>&gt; these *computational short-cuts* I maintain, are what
</em><br>
<em>&gt; *necessarily* give rise to qualia (consciousness and
</em><br>
<em>&gt; sentience).
</em><br>
<em>&gt;
</em><br>
<em>&gt; To sum up:  Any RPOP would quickly run into
</em><br>
<em>&gt; computational intractability if it stuck with pure
</em><br>
<em>&gt; Bayesian reasoning.  It would be forced to resort to
</em><br>
<em>&gt; *computational short-cuts*.  These would, I claim,
</em><br>
<em>&gt; inevitably give rise to consciousness.  With qualia
</em><br>
<em>&gt; present the RPOP would now be a 'Person'.  A suitably
</em><br>
<em>&gt; generalized definition of 'Person-Hood' would result
</em><br>
<em>&gt; in the RPOP being forced to include its own volition
</em><br>
<em>&gt; in the CV calculation.  This would give rise to an
</em><br>
<em>&gt; infinite regress.  Ergo, Collective Volition cannot be
</em><br>
<em>&gt; calculated by a singleton RPOP and the entire top-down
</em><br>
<em>&gt; approach is flawed.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I should make it clear that I *do* agree that CV
</em><br>
<em>&gt; (Collective Volition) is the ideal *political system*.
</em><br>
<em>&gt;  That is, I agree that CV is *a nice place to live*.
</em><br>
<em>&gt; But I disagree that CV is something capable of being
</em><br>
<em>&gt; embodied in a singleton RPOP and imposed from the
</em><br>
<em>&gt; top-down.  Eli's mistake is his insistence on the
</em><br>
<em>&gt; top-down approach.  He has mistaken a *distributed
</em><br>
<em>&gt; system* (Collective Volition) for a mind.  But in fact
</em><br>
<em>&gt; CV is not a singleton.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Under my theory, all working FAI's are necessarily
</em><br>
<em>&gt; sentients which assign themselves *Person-hood*
</em><br>
<em>&gt; status.  No singleton FAI can possibly implement
</em><br>
<em>&gt; Collective Volition (since any FAI is itself
</em><br>
<em>&gt; *included* in what constitutes Collective Volition).
</em><br>
<em>&gt; None the less, CV would still represent an ideal
</em><br>
<em>&gt; *political system* for sentients, which the FAI's
</em><br>
<em>&gt; would try to act in harmony with.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Under my theory no singeton FAI can fully calculate
</em><br>
<em>&gt; CV, but it *can* still obtain some degree of
</em><br>
<em>&gt; understanding and determine which actions are and are
</em><br>
<em>&gt; not in harmony in CV.  That is,  FAI could still
</em><br>
<em>&gt; perform calculations about CV sufficient to establish
</em><br>
<em>&gt; a sort of 'futures market' to help determine which
</em><br>
<em>&gt; actions were *Friendly* and which were *Unfriendly*.
</em><br>
<em>&gt;
</em><br>
<em>&gt; CV places constraints on permissible sentient actions.
</em><br>
<em>&gt;  But it's a distributed *global system* and *not*
</em><br>
<em>&gt; something that can be embodied in a Singleton as
</em><br>
<em>&gt; Eliezer thinks.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Of course I've banging away with my objections on SL4
</em><br>
<em>&gt; for a couple of years now ;)  Recall that I've always
</em><br>
<em>&gt; said that;
</em><br>
<em>&gt;
</em><br>
<em>&gt; (1)  (Practical) general intelligence without
</em><br>
<em>&gt; sentience is impossible
</em><br>
<em>&gt; (2)  Completely selfless AI is impossible
</em><br>
<em>&gt;
</em><br>
<em>&gt; Now though, I think my objections are stronger because
</em><br>
<em>&gt; I've got some plausible reasons for them and my own
</em><br>
<em>&gt; general theory of FAI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Eliezer.  And me.  One of us has to be right out of
</em><br>
<em>&gt; this, the other has to be wrong (Marc chuckles to
</em><br>
<em>&gt; himself and nods his head).  The time is fast
</em><br>
<em>&gt; approaching when we'll find out who's who...
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; =====
</em><br>
<em>&gt; &quot;Live Free or Die, Death is not the Worst of Evils.&quot;
</em><br>
<em>&gt;                                                     - Gen. John Stark
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;The Universe...or nothing!&quot;
</em><br>
<em>&gt;                             -H.G.Wells
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Please visit my web-sites.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Sci-Fi and Fantasy                : <a href="http://www.prometheuscrack.com">http://www.prometheuscrack.com</a>
</em><br>
<em>&gt; Mathematics, Mind and Matter      : <a href="http://www.riemannai.org">http://www.riemannai.org</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; Find local movie times and trailers on Yahoo! Movies.
</em><br>
<em>&gt; <a href="http://au.movies.yahoo.com">http://au.movies.yahoo.com</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10144.html">Robin Lee Powell: "Re: [HUMOR?] But will Penrose make any submissions?"</a>
<li><strong>Previous message:</strong> <a href="10142.html">Ben Goertzel: "RE: A funding suggestion: Solve The Riemann hypothesis ;)"</a>
<li><strong>In reply to:</strong> <a href="10139.html">Marc Geddes: "What I think is wrong with Eli's current approach"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10152.html">Marc Geddes: "RE: What I think is wrong with Eli's current approach"</a>
<li><strong>Reply:</strong> <a href="10152.html">Marc Geddes: "RE: What I think is wrong with Eli's current approach"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10143">[ date ]</a>
<a href="index.html#10143">[ thread ]</a>
<a href="subject.html#10143">[ subject ]</a>
<a href="author.html#10143">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:49 MDT
</em></small></p>
</body>
</html>
