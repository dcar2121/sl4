<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: JOIN</title>
<meta name="Author" content="ps udoname (ps.udoname@gmail.com)">
<meta name="Subject" content="Re: JOIN">
<meta name="Date" content="2006-09-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: JOIN</h1>
<!-- received="Wed Sep 20 06:52:34 2006" -->
<!-- isoreceived="20060920125234" -->
<!-- sent="Wed, 20 Sep 2006 13:51:24 +0100" -->
<!-- isosent="20060920125124" -->
<!-- name="ps udoname" -->
<!-- email="ps.udoname@gmail.com" -->
<!-- subject="Re: JOIN" -->
<!-- id="28553f510609200551n568b167ub752aed02604807e@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="f5238c30609081240r2ac1e413xa91cdfe66c157b4a@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> ps udoname (<a href="mailto:ps.udoname@gmail.com?Subject=Re:%20JOIN"><em>ps.udoname@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Sep 20 2006 - 06:51:24 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16019.html">maru dubshinki: "Re: JOIN"</a>
<li><strong>Previous message:</strong> <a href="16017.html">George Dvorsky: "[Fwd: [x-risk] Cerulo: Cultural Challenges to Envisioning the Worst]"</a>
<li><strong>In reply to:</strong> <a href="15979.html">maru dubshinki: "Re: JOIN"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16019.html">maru dubshinki: "Re: JOIN"</a>
<li><strong>Reply:</strong> <a href="16019.html">maru dubshinki: "Re: JOIN"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16018">[ date ]</a>
<a href="index.html#16018">[ thread ]</a>
<a href="subject.html#16018">[ subject ]</a>
<a href="author.html#16018">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; Well, so what? If consciousness is purely impossible without quantum
</em><br>
<em>&gt; effects, and consciousness really is the simplest way of producing all
</em><br>
<em>&gt; the observable consequences we attribute to intelligence and
</em><br>
<em>&gt; consciousness, then we can simply make a quantum computer. Progress is
</em><br>
<em>&gt; steady and sure in that domain these days anyway.
</em><br>
<p><p>True, I did not mean that quantum effects would make AI impossible, but what
<br>
it would mean is that work on how to program AI on a classical computer
<br>
might not translate to how to create an AI that uses these quantum effects.
<br>
Secondly creating the hardware for an AI would be harder if Penrose is
<br>
right, as microtubules are far more complex then other theories of how
<br>
neurones work. This means that AI and uploading come later and so DNI,
<br>
nanotech etc come first, which means we have to worry about grey goo etc.
<br>
<p><p>Well, that's act utilitarianism you're thinking of there, not rule or
<br>
<em>&gt; any of the more exotic variants. Eliezer's latest thinking is
</em><br>
<em>&gt; apparently on the lines of his
</em><br>
<em>&gt; <a href="http://www.intelligence.org/friendly/extrapolated-volition.html">http://www.intelligence.org/friendly/extrapolated-volition.html</a> essay;
</em><br>
<em>&gt; it's worth reading.
</em><br>
<p><p>It's good to see that thought is going into this. However some things worry
<br>
me (and I have looked at the PAQ's but i'm still not happy)
<br>
<p>&quot;The worrying question is:
<br>
What if only 20% of the planetary population is nice, or cares about
<br>
niceness, or *falls into the niceness attractor* when their volition is
<br>
extrapolated?&quot;
<br>
<p>&quot;If I later find I'm one of the 5% of humanity whose personal philosophies
<br>
predictably work out to pure libertarianism, and I threw away my one chance
<br>
to free humanity - the hell with it.&quot;
<br>
<p>Yep, assuming humanity as a whole is capable of making good choices in a bad
<br>
idea I think, as shown by all the things democratises have done in the past
<br>
which we find repugnant today. Taking account of volition helps this,
<br>
hopefully democracy based on volition would never have had slavery for
<br>
instance.
<br>
However, given the number of Christians in the world, what if the world
<br>
falls into a Christianity attractor for instance? (Just an example, I don't
<br>
mean that Christianity is the ultimate evil)
<br>
<p>&quot;The reason for the collective dynamic is that you can go collective -&gt;
<br>
individual, but not the other way 'round. If you could go individual -&gt;
<br>
collective but not collective -&gt; individual, I'd advocate an individual
<br>
dynamic.&quot;
<br>
<p>Why can it not go the other way round? If collective is fundamentally the
<br>
right way, I would assume that if everyone kept growing mentally eventually
<br>
all individuals would elect to join the collective.
<br>
<p>&quot;What about infants? What about brain-damage cases? What about people with
<br>
Alzheimer's disease?&quot;
<br>
<p>Wouldn't the long term extrapolated volition of an infant be the same as the
<br>
long term extrapolated volition of an adult?
<br>
<p>&quot;Maybe everyone wearing a Japanese schoolgirl uniform at the time of
<br>
Singularity will be attacked by tentacle monsters.&quot;
<br>
<p>WTF?
<br>
<p>Do they want to be attacked? If not, it's a good argument against collective
<br>
volition.
<br>
<p>In general, I would far prefer Individual volition, because I trust my
<br>
volition, but not the rest of humanity's.
<br>
<p>Also, am I correct in understanding that the volition of non-human sentients
<br>
does not count?
<br>
<p><p><p><p><em>&gt; &gt;  Instead, I think brain-computer interfacing might be a better idear. AI
</em><br>
<em>&gt; &gt; attempts should not give the AI direct control over anything, and the AI
</em><br>
<em>&gt; &gt; should be asked how to bootstrap humans. A big off button would also be
</em><br>
<em>&gt; a
</em><br>
<em>&gt; &gt; good idear.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Your second line is a suggestion for AI boxing. See
</em><br>
<em>&gt; <a href="http://sl4.org/archive/0207/4935.html">http://sl4.org/archive/0207/4935.html</a> and <a href="http://sl4.org/wiki/AI_Jail">http://sl4.org/wiki/AI_Jail</a>
</em><br>
<em>&gt; Long story short, not giving the AI direct control over anything would
</em><br>
<em>&gt; probably only work in the early stages and so would be of minimal use.
</em><br>
<em>&gt; Ditto for the big off button.
</em><br>
<p><p>I realise it might only work in the early stages, but that could still be
<br>
useful.
<br>
<p>The links are worrying and very surprising, especially as a real AI could be
<br>
far better at persuading people to let them out. Perhaps AI boxing is a bad
<br>
idear in view of that.
<br>
I would like to take part in an AI boxing experiment sometime.
<br>
<p><p>Uploading brains doesn't seem to be a popular suggestion on this list
<br>
<em>&gt; either, since their reasoning goes that humans are not the stablest
</em><br>
<em>&gt; and sanest mentalities you can get, and would probably become
</em><br>
<em>&gt; unfriendly in an upload.
</em><br>
<p><p>By 'bootstrapping' I didn't just mean uploading but also DNI and all forms
<br>
of intelligence enhancement, some of which will come before AI. I think AI
<br>
will come before uploading, as  it must be easier to reverse engineer a
<br>
generic brain then upload a specific person. However this would not
<br>
necessarily mean FAI comes before uploading.
<br>
I realise humans are likely to be fairly unfriendly. The best thing to do
<br>
would be to bootstrap as many as possible simultaneously. They would not
<br>
fight each other because of game theory (I hope). Then you would have to
<br>
hope that at least one of the people bootstrapped would be friendly enough
<br>
to upload everyone else.
<br>
<p>Sorry about the spelling on my previous post.
<br>
<p><p><em>&gt;  I could say more about myself, but I might as well do the questionaire
</em><br>
<em>&gt; ......
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; ~maru
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16019.html">maru dubshinki: "Re: JOIN"</a>
<li><strong>Previous message:</strong> <a href="16017.html">George Dvorsky: "[Fwd: [x-risk] Cerulo: Cultural Challenges to Envisioning the Worst]"</a>
<li><strong>In reply to:</strong> <a href="15979.html">maru dubshinki: "Re: JOIN"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16019.html">maru dubshinki: "Re: JOIN"</a>
<li><strong>Reply:</strong> <a href="16019.html">maru dubshinki: "Re: JOIN"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16018">[ date ]</a>
<a href="index.html#16018">[ thread ]</a>
<a href="subject.html#16018">[ subject ]</a>
<a href="author.html#16018">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:57 MDT
</em></small></p>
</body>
</html>
