<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: One for the history books</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: One for the history books">
<meta name="Date" content="2001-08-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: One for the history books</h1>
<!-- received="Sun Aug 26 20:31:23 2001" -->
<!-- isoreceived="20010827023123" -->
<!-- sent="Sun, 26 Aug 2001 19:49:55 -0400" -->
<!-- isosent="20010826234955" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: One for the history books" -->
<!-- id="3B898B23.3B2688F3@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="JEEOJKGAIKFKEHJONHMPMEFACHAA.lcorbin@tsoft.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20One%20for%20the%20history%20books"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Aug 26 2001 - 17:49:55 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2146.html">Dani Eder: "Re: One for the history books"</a>
<li><strong>Previous message:</strong> <a href="2144.html">Gordon Worley: "[HUMOR] Sluggy Singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2146.html">Dani Eder: "Re: One for the history books"</a>
<li><strong>Reply:</strong> <a href="2146.html">Dani Eder: "Re: One for the history books"</a>
<li><strong>Reply:</strong> <a href="2149.html">Simon McClenahan: "Re: One for the history books"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2145">[ date ]</a>
<a href="index.html#2145">[ thread ]</a>
<a href="subject.html#2145">[ subject ]</a>
<a href="author.html#2145">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Lee Corbin wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer writes
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; Getting a place in the history books isn't a good reason to do
</em><br>
<em>&gt; &gt; something.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Of course.  But it would seem almost unbelievably difficult for
</em><br>
<em>&gt; anyone to get truly beyond that need.  My own best theory as to
</em><br>
<em>&gt; why a number of famous people who admit that cryonics would work,
</em><br>
<em>&gt; e.g. Issac Asimov, never bothered to get themselves frozen, is
</em><br>
<em>&gt; that they had begun to live for their fame.  I am seriously
</em><br>
<em>&gt; doubtful of denials of the appeal of fame and fortune; to some
</em><br>
<em>&gt; degree they remind me of the calls to altruism made by failed
</em><br>
<em>&gt; economic systems.  Perhaps it's only deeply unconsciously, but
</em><br>
<em>&gt; I don't think that there is anyone who can stand up and claim
</em><br>
<em>&gt; that no *comparitively mundane* aren't also a factor in the
</em><br>
<em>&gt; working of his or her mind.  We just have to learn to live with
</em><br>
<em>&gt; this, and, indeed make the best use of it.
</em><br>
<p>I have made the decision to go for one hundred point zero zero percent
<br>
altruism and complete rationality.  The decision is not synonymous with
<br>
the achievement, but is a necessary precondition of that achievement.  To
<br>
make a deliberate compromise is to halt your progress at that point,
<br>
because even the best you can achieve and the most ambitious form of
<br>
sanity you can imagine is only another step on a long road where only the
<br>
next single step is visible at any given time.  There are a variety of
<br>
emotions that could act as sources of mental energy for my Singularity
<br>
work, the desire for fame among them.  The excuse is there if I choose to
<br>
use it.  I do not so choose.  I choose to relinquish those emotions rather
<br>
than compromise rationality.
<br>
<p>I made that choice at around age fifteen or sixteen, shortly after I
<br>
became aware of evolutionary psychology and the likelihood that the
<br>
emotions designed to underly altruism would not be consistent with the
<br>
declared goals of altruism.  Because emotions like the desire for fame are
<br>
fairly clear-cut - are &quot;exceptional conditions&quot; within the event-loop of
<br>
the mind - it is possible to learn to identify the emotion's subjective
<br>
feel, notice it, and disbelieve the mental imagery that causes it.  I've
<br>
since moved on to more interesting areas of mental cleanup.  As far as
<br>
things like the desire for fame go, I am finished.  If you were to
<br>
identify the brain-level hardware support for fameseeking and insert a
<br>
little alarm that went off whenever the brainware activated, I think the
<br>
activity level would be essentially zero; if the alarm ever went off, it
<br>
would be a small, choked cough as the emotion was triggered, I noticed the
<br>
subjective feel, and the emotion was subsequently wiped out of existence. 
<br>
(I am still validating my desire for routine social respect, which is
<br>
quite a different thing from a desire for general fame - although I am
<br>
recently starting to believe that this emotion may be &quot;considered harmful&quot;
<br>
as well.)
<br>
<p>In short, I'm now confident enough about my mental cleanup that I can talk
<br>
about it without making nervous little disclaimers such as &quot;But, of
<br>
course, who really knows what's inside their mind?&quot;  As far as the
<br>
specific cognitive force &quot;desire for fame&quot; is concerned, I predict that my
<br>
plans will exhibit no more sign of it than plans made by an AI.  I will
<br>
not make excuses in advance for failures because I am done cleaning up
<br>
that emotion and I do not expect there to be any failures.
<br>
<p>I realize that this is a claim for such an extraordinarily high level of
<br>
ability that Bayesian reasoners, reasoning from the prior expected
<br>
population levels of self-overestimators and extremely sane people, may
<br>
find that such a claim (considered as an unadorned abstract) is more
<br>
reason to doubt sanity than to believe it.  That's probably the reason why
<br>
a lot of people who are interested in the cognitive science of rationality
<br>
manage to sound self-deprecating when talking about it; not just as a
<br>
signal to others, I think, but as a signal to themselves, because they
<br>
*know* that high confidence in sanity is often a signal of insanity.  But
<br>
that in itself is unsanity.  It's saying, &quot;I observe that I'm damned good
<br>
at being sane, but I won't admit it even to myself, because if I sent
<br>
myself the signal of confidence in sanity, I might have to interpret that
<br>
signal as evidence of insanity.&quot;
<br>
<p>And yes, for some years, I gave myself the runaround this way.  It's an
<br>
emergent flaw in the way that the human mind processes pleasureable and
<br>
painful anticipations, and not a single emotion that can be identified and
<br>
dealt with.  I'm also sad to admit that my giving myself the runaround was
<br>
eventually crushed under the buildup of experience, rather than noticed
<br>
via pure abstract logic.  I only started working with the general problem
<br>
of pleasurable and painful anticipations while writing &quot;Creating Friendly
<br>
AI&quot;, and am actually still working on cleaning that general class of
<br>
emergent problems out of my own mind.
<br>
<p><em>&gt; I never feel any the worse about someone when I come to have
</em><br>
<em>&gt; reason to think that either fame or fortune is a factor in their
</em><br>
<em>&gt; behavior.  The only thing that annoys me is when they're so
</em><br>
<em>&gt; transparent about it that it comes accross as petty and blind,
</em><br>
<em>&gt; and seems to bespeak an inability to see the situation from
</em><br>
<em>&gt; others' points of view.
</em><br>
<p>Fame, as a motive to do good, is certainly preferable to the various
<br>
motives to do things that aren't good.  So, as noxious emotions go,
<br>
fameseeking is fairly far down on my list of emotions to target in
<br>
others.  The main reason for me to be concerned about observed fameseeking
<br>
is if I see it in someone who I think would like to be a rational
<br>
altruist, and who's already gotten fairly far in cleaning up the mental
<br>
landscape.  Even so, fameseeking acts as an approximation to rational
<br>
altruism under some circumstances, and I am thus willing to use this
<br>
emotion as memetic shorthand - provided that the arguments are truthful,
<br>
the flaws in the approximation to altruistic rationality are either unused
<br>
or explicitly dealt with, and pure rationality is closer than under the
<br>
previous status quo.  But there's no way that you could use the argument
<br>
from post-Singularity fame, because the underlying statement is probably
<br>
not truthful (do transhumans care?) and also because Singularitarians are
<br>
generally supposed to *be* conscious rationalists and the use of emotional
<br>
shorthand may lower the standard.
<br>
<p>On the other hand, it's acceptable to say:  &quot;You are one of only six
<br>
billion entities, in a galaxy of four hundred billion stars and decillions
<br>
of sentient beings, whose lives predate the Singularity; you are one of
<br>
the oldest of all living things.  I don't know whether you or anyone else
<br>
will respect that in the future, because it's difficult to predict what
<br>
transhumans will care about, but it does say something about how we should
<br>
feel *now*.  We are not just the six billion people who were around before
<br>
the Singularity; we are the six billion people who created the
<br>
Singularity.  An incomprehensibly huge future rests on the consequences of
<br>
our present-day actions, and 'the impact we have on the universe' is
<br>
essentially 'the impact we have on the Singularity'.&quot;
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2146.html">Dani Eder: "Re: One for the history books"</a>
<li><strong>Previous message:</strong> <a href="2144.html">Gordon Worley: "[HUMOR] Sluggy Singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2146.html">Dani Eder: "Re: One for the history books"</a>
<li><strong>Reply:</strong> <a href="2146.html">Dani Eder: "Re: One for the history books"</a>
<li><strong>Reply:</strong> <a href="2149.html">Simon McClenahan: "Re: One for the history books"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2145">[ date ]</a>
<a href="index.html#2145">[ thread ]</a>
<a href="subject.html#2145">[ subject ]</a>
<a href="author.html#2145">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
