<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Article: The coming superintelligence: who will be in control?</title>
<meta name="Author" content="Ben Goertzel (ben@webmind.com)">
<meta name="Subject" content="RE: Article: The coming superintelligence: who will be in control?">
<meta name="Date" content="2001-08-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Article: The coming superintelligence: who will be in control?</h1>
<!-- received="Thu Aug 02 13:17:25 2001" -->
<!-- isoreceived="20010802191725" -->
<!-- sent="Thu, 2 Aug 2001 07:19:57 -0400" -->
<!-- isosent="20010802111957" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@webmind.com" -->
<!-- subject="RE: Article: The coming superintelligence: who will be in control?" -->
<!-- id="NDBBIBGFAPPPBODIPJMMCEOPFNAA.ben@webmind.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3B68E2C9.D29552FE@posthuman.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@webmind.com?Subject=RE:%20Article:%20The%20coming%20superintelligence:%20who%20will%20be%20in%20control?"><em>ben@webmind.com</em></a>)<br>
<strong>Date:</strong> Thu Aug 02 2001 - 05:19:57 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2043.html">Arona Ndiaye: "Re: Augmenting humans is a better way"</a>
<li><strong>Previous message:</strong> <a href="2041.html">Mark Walker: "Re: Article: The coming superintelligence: who will be in control?"</a>
<li><strong>In reply to:</strong> <a href="2035.html">Brian Atkins: "Re: Article: The coming superintelligence: who will be in control?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2048.html">James Higgins: "RE: Article: The coming superintelligence: who will be in control?"</a>
<li><strong>Reply:</strong> <a href="2048.html">James Higgins: "RE: Article: The coming superintelligence: who will be in control?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2042">[ date ]</a>
<a href="index.html#2042">[ thread ]</a>
<a href="subject.html#2042">[ subject ]</a>
<a href="author.html#2042">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Brian, I think your analysis of the peculiarities of Kurzweil's world-view
<br>
is spot-on, and I also appreciate your clarification that Kurzweil himself
<br>
is not the point here -- he's just a particular example of the general
<br>
phenomenon that even people who see what's coming, for some reason only are
<br>
able to see 90% of it.  This is related to what, in my question to Kurzweil
<br>
at his talk, I called &quot;hard takeoff denial&quot;.  What you're pointing out is
<br>
something like &quot;Singularity malleability denial&quot; -- refusal to confront the
<br>
fact that we, by our intentional actions, may significantly morph the
<br>
outcome of this huge transformation that's coming.  The combination of these
<br>
denials is what I less generously referred to in my post as a kind of
<br>
&quot;narrowmindedness&quot;, but you said it more pleasantly and more clearly.
<br>
<p>Personally, I don't share the confidence of some that the Singularity will
<br>
necessarily be good for the human race.  I think it has the potential to be
<br>
great for us, and also the potential to exterminate us.  I'm with Eli, in
<br>
believing that we need to specifically work to make it good.  I don't
<br>
entirely agree with him on the specific AI-engineering mechanisms that will
<br>
succeed in this regard, but this is a pretty minor quibble in the big
<br>
picture (and perhaps he'll bring me around to his view once he's articulated
<br>
it more clearly and fully).
<br>
<p>Another point is that there are LOTS of other people who basically see it
<br>
our way, but don't feel it's useful to post and read a lot of e-mail
<br>
messages about the topic.  For instance, most of my Brazilian WM development
<br>
team sees these things the same way, roughly, as me and Eli and Brian, but
<br>
they prefer to spend their time working on computer science rather than
<br>
chatting about philosophy.  I felt the same way, and was working toward the
<br>
same goals *before* I started posting on this list.  So don't assume that
<br>
just because some big spokespeople don't fully &quot;get it&quot;, those of us who
<br>
babble about it a lot are the only ones who get it!!
<br>
<p>-- Ben G
<br>
<p><p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20Article:%20The%20coming%20superintelligence:%20who%20will%20be%20in%20control?">owner-sl4@sysopmind.com</a> [mailto:<a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20Article:%20The%20coming%20superintelligence:%20who%20will%20be%20in%20control?">owner-sl4@sysopmind.com</a>]On Behalf
</em><br>
<em>&gt; Of Brian Atkins
</em><br>
<em>&gt; Sent: Thursday, August 02, 2001 1:19 AM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sysopmind.com?Subject=RE:%20Article:%20The%20coming%20superintelligence:%20who%20will%20be%20in%20control?">sl4@sysopmind.com</a>
</em><br>
<em>&gt; Subject: Re: Article: The coming superintelligence: who will be in
</em><br>
<em>&gt; control?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Amara D. Angelica&quot; wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Brian, an intriguing idea. Can you or anyone else elaborate?
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt; -----Original Message-----
</em><br>
<em>&gt; &gt; &gt; From: <a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20Article:%20The%20coming%20superintelligence:%20who%20will%20be%20in%20control?">owner-sl4@sysopmind.com</a>
</em><br>
<em>&gt; [mailto:<a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20Article:%20The%20coming%20superintelligence:%20who%20will%20be%20in%20control?">owner-sl4@sysopmind.com</a>]On Behalf
</em><br>
<em>&gt; &gt; &gt; Of Brian Atkins
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt; going beyond trend tracking and guessing what will happen.
</em><br>
<em>&gt; I'd like to see
</em><br>
<em>&gt; &gt; &gt; people take the issue farther and try to figure out a) can we
</em><br>
<em>&gt; manipulate
</em><br>
<em>&gt; &gt; &gt; the timing and character of the Singularity significantly b)
</em><br>
<em>&gt; if so, should
</em><br>
<em>&gt; &gt; &gt; we accelerate it?
</em><br>
<em>&gt;
</em><br>
<em>&gt; I'm not sure if you wanted me to elaborate on the fact that no one is
</em><br>
<em>&gt; talking about this issue, or to elaborate on possible answers to A and B.
</em><br>
<em>&gt; I'll do the former, because as you can see below I think Ray and the
</em><br>
<em>&gt; people on this list have already decided for themselves that the answer
</em><br>
<em>&gt; to B is Yes. The answer to A IMO is Yes, and I think Ray would also agree
</em><br>
<em>&gt; with that at least to a limited extent. What I am frustrated by is seeing
</em><br>
<em>&gt; people realize that both A and B are Yes, but then not helping to push the
</em><br>
<em>&gt; Singularity closer to us in time. So I'd like to get the word out about
</em><br>
<em>&gt; this issue, since (again IMO) this is even more important than simply
</em><br>
<em>&gt; realizing a Singularity is coming. Once you see it's coming then you have
</em><br>
<em>&gt; to go a step farther and &quot;pick a side&quot;. Sitting on the fence like
</em><br>
<em>&gt; a reporter
</em><br>
<em>&gt; (no offense :-) is not a rational choice in this situation. If you believe
</em><br>
<em>&gt; the Singularity will be good (and I mean good in the sense of saving your
</em><br>
<em>&gt; life, not in the sense of whether congress lowers your tax rate
</em><br>
<em>&gt; 3%) for you
</em><br>
<em>&gt; then you should try to advance it. If you believe the Singularity will be
</em><br>
<em>&gt; bad for you, then you should try to prevent it. This is a world changing
</em><br>
<em>&gt; bit of history.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Here's some examples of the &quot;blindspot&quot; that I'm talking about. Go look at
</em><br>
<em>&gt; part 4 of the Extro-5 Kurzweil talk here:
</em><br>
<em>&gt;
</em><br>
<em>&gt; <a href="http://www.kurzweilai.net/meme/frame.html?main=/articles/art0235.html">http://www.kurzweilai.net/meme/frame.html?main=/articles/art0235.html</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; skip ahead to around 13:15 at which point Eliezer asks Ray something
</em><br>
<em>&gt; along the lines of &quot;Well, you've described the Singularity and our
</em><br>
<em>&gt; progress to it so far, but you haven't said what kind of Singularity
</em><br>
<em>&gt; you would like to see or what time you would /prefer/ it to happen&quot;.
</em><br>
<em>&gt; Then Ray sits there for like 7 seconds (which makes me think he might
</em><br>
<em>&gt; not have thought about this much) before someone in the back says
</em><br>
<em>&gt; something that causes him to then go off on a tangent without answering
</em><br>
<em>&gt; the question. Very frustrating since that was the one question I wanted
</em><br>
<em>&gt; an answer to! :-)
</em><br>
<em>&gt;
</em><br>
<em>&gt; If you or anyone else here has ever seen him address this I'd like to
</em><br>
<em>&gt; know about it. He seems to have chosen a clinical observer style when
</em><br>
<em>&gt; it comes to the Singularity, which lets him make predictions about what
</em><br>
<em>&gt; the future might be like, but yet not consider the fact that someone
</em><br>
<em>&gt; with his excellent grasp of the situation would be exactly the right
</em><br>
<em>&gt; kind of person to help guide and support the actual development. Here's
</em><br>
<em>&gt; a quote from his book precis:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Technology will remain a double edged sword, and the story of the Twenty
</em><br>
<em>&gt;  First century has not yet been written. It represents vast power to be
</em><br>
<em>&gt;  used for all humankind's purposes. We have no choice but to work hard to
</em><br>
<em>&gt;  apply these quickening technologies to advance our human values, despite
</em><br>
<em>&gt;  what often appears to be a lack of consensus on what those values should
</em><br>
<em>&gt;  be.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Confusing to say the least, unless he's running a secret AI or brain
</em><br>
<em>&gt; scanning project we don't know about :-) He advocates working to
</em><br>
<em>&gt; achieve the
</em><br>
<em>&gt; Singularity, and points out that the history is not made yet, but provides
</em><br>
<em>&gt; no advice (outside of some possible future scenarios) on what might be the
</em><br>
<em>&gt; best way to achieve it, whether we should try to accelerate the arrival of
</em><br>
<em>&gt; it (is it &quot;ethical&quot; to accelerate the Singularity), and how we might do so
</em><br>
<em>&gt; if we do decide we want to accelerate it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; He goes on to talk about the purpose of life which he sees as evolving
</em><br>
<em>&gt; to the Singularity. He says that, but then almost immediately goes back
</em><br>
<em>&gt; towards pointing out that the real reason the Singularity will happen
</em><br>
<em>&gt; is due to economics. However, if he really feels like achieving a
</em><br>
<em>&gt; Singularity is the goal of life, then I'd like to ask him: what are your
</em><br>
<em>&gt; plans for after you finish your book? How will you help to achieve this
</em><br>
<em>&gt; goal? I don't see any answers to that, or even anyone else asking this
</em><br>
<em>&gt; question of themselves (besides people who hang around here) or Ray.
</em><br>
<em>&gt;
</em><br>
<em>&gt; He ends his precis with the admonition that we all should &quot;stick around
</em><br>
<em>&gt; so you might see the Singularity&quot;. Which really clenches it for me- if
</em><br>
<em>&gt; he had really internalized the &quot;Singularity is the goal of life&quot; idea
</em><br>
<em>&gt; then he would instead be telling people to go out and help get the
</em><br>
<em>&gt; Singularity here more quickly so that even more people now living will
</em><br>
<em>&gt; be able to survive to see it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Now I don't want to look like I'm stuck on Ray. He just is the most
</em><br>
<em>&gt; glaring example to me because I've read so much of his stuff lately.
</em><br>
<em>&gt; He also is the biggest proponent of the Singularity in the mainstream
</em><br>
<em>&gt; world, and yet still seems to be missing the final pieces of the picture.
</em><br>
<em>&gt; --
</em><br>
<em>&gt; Brian Atkins
</em><br>
<em>&gt; Director, Singularity Institute for Artificial Intelligence
</em><br>
<em>&gt; <a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2043.html">Arona Ndiaye: "Re: Augmenting humans is a better way"</a>
<li><strong>Previous message:</strong> <a href="2041.html">Mark Walker: "Re: Article: The coming superintelligence: who will be in control?"</a>
<li><strong>In reply to:</strong> <a href="2035.html">Brian Atkins: "Re: Article: The coming superintelligence: who will be in control?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2048.html">James Higgins: "RE: Article: The coming superintelligence: who will be in control?"</a>
<li><strong>Reply:</strong> <a href="2048.html">James Higgins: "RE: Article: The coming superintelligence: who will be in control?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2042">[ date ]</a>
<a href="index.html#2042">[ thread ]</a>
<a href="subject.html#2042">[ subject ]</a>
<a href="author.html#2042">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
