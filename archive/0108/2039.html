<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: ESSAY:  AI and Effective Sagacity</title>
<meta name="Author" content="Mitch Howe (mitch_howe@yahoo.com)">
<meta name="Subject" content="ESSAY:  AI and Effective Sagacity">
<meta name="Date" content="2001-08-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>ESSAY:  AI and Effective Sagacity</h1>
<!-- received="Thu Aug 02 04:06:13 2001" -->
<!-- isoreceived="20010802100613" -->
<!-- sent="Thu, 2 Aug 2001 01:48:34 -0600" -->
<!-- isosent="20010802074834" -->
<!-- name="Mitch Howe" -->
<!-- email="mitch_howe@yahoo.com" -->
<!-- subject="ESSAY:  AI and Effective Sagacity" -->
<!-- id="000a01c11b27$88c3eaa0$d612e63f@mitch" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Mitch Howe (<a href="mailto:mitch_howe@yahoo.com?Subject=Re:%20ESSAY:%20%20AI%20and%20Effective%20Sagacity"><em>mitch_howe@yahoo.com</em></a>)<br>
<strong>Date:</strong> Thu Aug 02 2001 - 01:48:34 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2040.html">Mitch Howe: "JOIN: Mitch Howe"</a>
<li><strong>Previous message:</strong> <a href="2038.html">Eliezer S. Yudkowsky: "Re: Article: The coming superintelligence: who will be in control?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2050.html">Peter Voss: "RE: ESSAY:  AI and Effective Sagacity"</a>
<li><strong>Reply:</strong> <a href="2050.html">Peter Voss: "RE: ESSAY:  AI and Effective Sagacity"</a>
<li><strong>Reply:</strong> <a href="2055.html">Gordon Worley: "Re: ESSAY:  AI and Effective Sagacity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2039">[ date ]</a>
<a href="index.html#2039">[ thread ]</a>
<a href="subject.html#2039">[ subject ]</a>
<a href="author.html#2039">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
[The following is an essay I've written on human thought as it relates to
<br>
AI.  I am not a neuroscientist, but base my conclusions on hopefully logical
<br>
analysis of experiences I believe are common to us all.  I frame these
<br>
conclusions in the context of AI to see if they shed any light on the
<br>
requirements for creating a seed AI and nurturing it into a
<br>
superintelligence.  I believe they do and would like to know if anyone
<br>
agrees.  'Join' post to follow.]
<br>
<p><p><p>**AI and Effective Sagacity** - Mitchell Howe 8/2/01
<br>
<p>In the field of AI, the supergoal is to create an information processing
<br>
system that does something truly significant.  (Whether this something is
<br>
good, bad, of financial worth to a few, of world-ending importance to many,
<br>
etc, depends upon who is doing the programming and how successful they are
<br>
at it.)  The seemingly essential subgoal that defines AI research is to
<br>
create a system that can both learn and improve itself in self-reinforcing
<br>
manner to eventually meet the end objective of significant action.  Some
<br>
minimal yet critical combination of software elegance and hardware
<br>
capability is required to get to this point.
<br>
<p>Discussion often lingers on the questions of how near to the capacity of the
<br>
human brain such a system would need to be in order to meet this goal, or
<br>
even to what degree of human brain might be required.  I believe such
<br>
questions are largely meaningless because they lose sight of the only
<br>
supergoal - that such a system sustainably learn and improve, leading to
<br>
eventual significant action.
<br>
<p>Consider this in light of the debate about whether a person with 50 IQ can
<br>
ever hope to achieve the results of someone with a 100 IQ by remembering
<br>
that within the wide range of IQ scores held by capable adults there are
<br>
many with high IQ's who have failed to contribute anything insightful or
<br>
even useful, just as there are many with lower IQ's who have come up with
<br>
world-changing ideas and become leaders in business.  (While far from
<br>
scientific, an issue of TIME from early this year had fun with this idea.)
<br>
The ability to solve simple problems and make logical conclusions from given
<br>
data, as measured by IQ scores, does not directly correlate to the AI
<br>
supergoal of doing something truly significant.  Somebody may know how to
<br>
design a better mousetrap yet never do anything with this knowledge.  We
<br>
would hope that an AI not likewise 'fizzle' (unless its better mousetrap
<br>
design was a grey goo that would wipe out all mammalian life).
<br>
<p>I believe that a large part of the surprisingly common discord between IQ
<br>
scores and societal significance can be explained by my simple theory of
<br>
'Effective Sagacity'.  It begins with the idea that there are various levels
<br>
of thought experienced in the human mind, and that only the time spent at
<br>
the highest level contributes to genuinely productive intelligence.  I
<br>
prefer to identify just two levels of thought with the disclaimer that there
<br>
is no hard line between them.  I like to call them Fidget and Sage.
<br>
<p>Fidget is the level of thought that involves making numerous small, trivial
<br>
decisions and enacting any routine physical actions these decisions require.
<br>
Many activities, once learned, become Fidgetized.  Card shuffling and
<br>
dealing.  Assembly line tasks.  Simple arithmetic.  Brushing your teeth.
<br>
You know that they are Fidgetized because you can think about something else
<br>
entirely while doing them.  But you don't always think about something else
<br>
because Fidget is often capable of bringing Sage mind behind it lock-step.
<br>
(I'll talk about that more about interplay between these two in a second.)
<br>
Fidget cannot intentionally change your life, but it is very useful and
<br>
powerful nonetheless.
<br>
<p>Sage is the level of thought that involves conscious consideration and
<br>
complex decision-making.  It is the level you are at when you not only hear
<br>
what your professor is saying, but also think about it, relate it to your
<br>
model of the universe, and implement it accordingly - *learning*  Sage is
<br>
responsible for pondering the deeper questions of life, sustaining
<br>
meaningful conversation, and making conclusions about your identity.  It was
<br>
hopefully the level you were at if/when you decided on a career, spouse,
<br>
etc.  Sage is not all-powerful, though.  For starters, it has very low
<br>
endurance when most actively engaged, like someone who can walk for miles
<br>
but can barely run a lap around the track.  It is also easily distracted by
<br>
inconsequential tasks, like a dog happily entertained for hours by a simple
<br>
game of catch.  In fact, given the choice between running a lap and
<br>
repeatedly grabbing a stick in its mouth, Sage will usually bring you a
<br>
drool-covered stick.
<br>
<p>Because of the complimentary talents of Fidget and Sage, they have a very
<br>
friendly relationship.  People are often most satisfied when both are
<br>
simultaneously occupied at a low-to-middle stress level.  Solitaire on the
<br>
computer is mostly a thoughtless exercise of mouse clicks under Fidget
<br>
control, with occasional input from Sage when an actual strategic decision
<br>
needs to be made.  Neither mind is working terribly hard but both are
<br>
occupied and satisfied - a condition of well-being some researchers have
<br>
called  &quot;flow&quot;.   Fidget is just as happy to spend hours throwing a stick as
<br>
Sage is to chase it and bring it back -- the seductive addiction of video
<br>
games and jigsaw puzzles is explained.
<br>
<p>The poor endurance of Sage, and its desire to rest at an optimum
<br>
lower-stress activity level also sheds light on many kinds of
<br>
procrastination, since the thing you put off doing is often some special
<br>
case that requires a higher Sage activity level.  &quot;I can't study anymore for
<br>
my final.  I must go for a swim and work on my tan.&quot;  &quot;I can't finish
<br>
writing about levels of thought right now.  I must play Diablo II for a
<br>
couple of hours.&quot;
<br>
<p>(Five hours later)
<br>
<p>There are times though, when one level of thought operates almost
<br>
independently from the other.  If you have ever been putting staples in
<br>
hundreds documents when you realized that you had run out of staples a dozen
<br>
slams of the stapler ago, you know what I am talking about.  The fully
<br>
Fidgetized task did not require the attention of Sage, who found something
<br>
else to do and failed to notice and report the absence of staples.  It is
<br>
either called &quot;daydreaming&quot; or &quot;spacing out&quot;, depending on whether Sage was
<br>
meandering through the park or asleep on the bench when it was discovered.
<br>
Driving is an activity that unfortunately lends itself to inappropriate
<br>
Fidgetization.  While first learning to drive, few can really think about
<br>
much else besides driving, but over time the procedures become more routine.
<br>
Many, many traffic accidents have occurred because people allowed Sage to
<br>
leave driving completely up to Fidget, who does not react promptly when
<br>
something unexpected occurs.  Perhaps Sage was talking to his stock broker
<br>
on the cell phone, or perhaps just carrying on an imaginary conversation
<br>
with an ex-lover who would be oh-so jealous about seeing him with so-and so
<br>
behind the truck that just stopped suddenly in front of -WHAM!-.  (I mean,
<br>
honestly, there are few excusable reasons to rear-end someone.)
<br>
<p>Sage can also be deliberately put out to pasture, and this is frequently
<br>
done when Fidget is busy and can't play.  Many drivers and workers in
<br>
repetitive jobs either consciously or unconsciously silence Sage by
<br>
listening to music - an activity that for many gets Sage absently swaying to
<br>
the beat. (This is not always the case when listening to music, but a use to
<br>
which it is frequently put.)
<br>
<p>Even if Fidget is not busy, Sage can be intentionally suppressed.  For some,
<br>
like angst-ridden teenagers, conversations with Sage may be so disturbing
<br>
that loud music is the best way to drown them out.  For others, chatting
<br>
with Sage may simply be dull and unsatisfying.  Alcohol and Marijuana are
<br>
known Sage-suppressants.  TV offers many levels of basic thought occupation
<br>
catered mostly to minds ranging from the &quot;moronic&quot; to the &quot;typical
<br>
American&quot; - which is why many noticeably intelligent people have just one or
<br>
two favorite shows and renounce the rest as a worthless morass of glandular
<br>
titillation.
<br>
<p>So what do I mean by &quot;Effective Sagacity&quot;?  Well, by now it should be
<br>
obvious that humans, on average, spend very little time with Sage hard at
<br>
work.  Sage is usually engaged in trivial games with Fidget, deliberately
<br>
distracted while Fidget is busy, or intentionally suppressed because of
<br>
boring or uncomfortable mental dialogue.  It may even be that Sage, when
<br>
allowed to slack off so much, becomes even more out of shape and incapable
<br>
of running laps. (I reluctantly make this conclusion knowing that I give
<br>
ammunition to those who derail mine and subsequent generations as having no
<br>
attention span thanks to today's ubiquitous entertainment technology.)  The
<br>
problem is, high-level Sage-thought is the only kind that fosters true
<br>
learning, creativity, experimentation, etc.   Therefore, even the most
<br>
high-IQ human may never produce anything new or useful to society if she is
<br>
unable or unwilling to regularly put her lanky-but-lazy Sage through its
<br>
paces.  The low-IQ underdog may climb to the top of his field because his
<br>
awkward-but-fit Sage is continually running marathons.  The formula is as
<br>
follows:
<br>
<p>**The amount previously invested and currently spent in highest-level
<br>
thought combine to form one's &quot;Effective Sagacity.&quot;  In the end, this is the
<br>
*only* measurement of mental capacity an AI researcher ought to be
<br>
interested in.**
<br>
<p>Note that I did not say that Effective Sagacity was the proportion of high
<br>
Sage thought to other thought, nor did I say that it was the average height
<br>
of one's thoughts.  Only highest-level 'Sage' thoughts count.  Only thoughts
<br>
already completed (which by definition have enriched the mind) or currently
<br>
undertaken count.  This means that a mind too unsophisticated to think any
<br>
deep thoughts will automatically be disqualified from having a high
<br>
Effective Sagacity.  It also means that a high-IQ -- the mere potential to
<br>
think really big thoughts, is meaningless.
<br>
<p>When we talk about AI, it must be said that a self-improving seed
<br>
intelligence has the potential to have an Effective Sagacity score
<br>
completely off the charts compared to humans.  This is fine.  If, due to
<br>
faster-than-neuron circuitry and clever software, the AI thinks through the
<br>
equivalent of 1,000 human years of high Sage thought in just two weeks, the
<br>
scale is not broken - just embarrassing to humans.  It may also be that this
<br>
same AI is thinking thoughts of far higher Sage than humans are capable of.
<br>
This is more of a stretch for the Effective Sagacity scale, but if such is
<br>
demonstrably the case than the machine is already a superintelligence that
<br>
is probably doing something very significant.  Hope it's friendly.
<br>
<p>An AI researcher, then, should also take heart in the knowledge that most of
<br>
the human mind's activity may not need to be replicated in order to create a
<br>
machine that thinks high Sage thoughts.  Others have already stated well the
<br>
reality of the human mind's origins and its preoccupation with biological
<br>
drives.  These same forces undoubtedly worked in some way that I do not
<br>
fully understand to create the range of generally low-endurance Sage most of
<br>
us rely upon to learn and create.  An artificial intelligence would not only
<br>
be free of the bio-burdens of survival, but also of the human limitations on
<br>
sustained high-level thought.  It may not be necessary to come even close to
<br>
matching human neural capacity in silicon, not only because so much of the
<br>
brain's body-minded tasks need not be wired for, but because the primary
<br>
thought tasks that are programmed will be consistently carried out.  If a
<br>
software engineer spends just 30 minutes a day actually entering code, she
<br>
is probably not spending the other 7.5 hours thinking about that code, but
<br>
rather some 2.5 hours thinking about the code, 2 hours thinking about food,
<br>
sex, or social status, and 2 hours &quot;spaced out&quot; or otherwise incapacitated
<br>
by Sage lazily chasing down or soaking up trivial thoughts of some kind or
<br>
other.  An AI should be able to tweak strongly in favor of the on-target
<br>
thought.
<br>
<p>It is possible that this conclusion is wrong; It could be that there is some
<br>
fundamental limitation inherent in the brain's level computational capacity
<br>
that makes it possible to learn effectively for short periods of time but
<br>
impossible to do it for weeks on end - but I doubt it.  It could also be
<br>
that an AI would have its own crippling correlates to human Fidget
<br>
activities - exhaustive memory or data-stream management, perhaps.  These
<br>
Fidget distractions could easily demand so much attention that little
<br>
capacity is left for Sage thought.  (This metaphor may very crudely apply to
<br>
Ben Goertzel's early incarnation of Webmind.)  More efficient coding and
<br>
more powerful hardware seem very likely to overcome this potential
<br>
bottleneck soon, however.
<br>
<p>All these happy conclusions seem to support the view of a hard, fast AI
<br>
takeoff sooner rather than later.  I'm all too happy to stand by that, but
<br>
the Effective Sagacity view suggests an additional hurdle a growing
<br>
seed-AI - the limits of human knowledge obtained thus far.  A highly
<br>
Sagacious AI would be very adept at learning new material, at internalizing
<br>
input to create a more accurate model of the universe, and using this model
<br>
to produce insightful output.  The problem potentially arises after the
<br>
young AI has devoured all available texts and treatises on computer science
<br>
along with all examples of program code - and perhaps managed to make only
<br>
modest improvement on its own design.  Further progress could be very slow
<br>
without additional instructional materials.  Fortunately, the truly
<br>
Sagacious AI could also effectively find its way out of this cul-de-sac of
<br>
human thought.  It could do so the same way outstanding scientists do today:
<br>
by identifying the limits of current understanding and coming up with the
<br>
right questions to ask in order to expand those limits.  The AI could either
<br>
come up with great experiments to advance human knowledge, or, more
<br>
efficiently in the software field, create and perform experiments on its
<br>
own.  Even if the AI is -merely- capable of directing humans in bold new
<br>
experiments, it has already done something truly significant.  This would
<br>
also increase the likelihood that it would continue to be capable of
<br>
improvement and further ultimate significance.
<br>
<p>The Effective Sagacity view suggests that the goal of AI is simpler than it
<br>
is often made out to be.  Not only does AI not require replication of the
<br>
human brain, it should not prove as susceptible to subtle weaknesses that
<br>
sap the capacity of even the most brilliant humans to sustain high-level
<br>
thought.  It would be naive, however, to suggest that creating an AI is a
<br>
simple task.  Coding and wiring for a truly significant new intelligence
<br>
demands both daring creativity and enviable perseverance.  It will require
<br>
thinkers of the highest Sagacity.
<br>
<p>****
<br>
<p><p>_________________________________________________________
<br>
Do You Yahoo!?
<br>
Get your free @yahoo.com address at <a href="http://mail.yahoo.com">http://mail.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2040.html">Mitch Howe: "JOIN: Mitch Howe"</a>
<li><strong>Previous message:</strong> <a href="2038.html">Eliezer S. Yudkowsky: "Re: Article: The coming superintelligence: who will be in control?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2050.html">Peter Voss: "RE: ESSAY:  AI and Effective Sagacity"</a>
<li><strong>Reply:</strong> <a href="2050.html">Peter Voss: "RE: ESSAY:  AI and Effective Sagacity"</a>
<li><strong>Reply:</strong> <a href="2055.html">Gordon Worley: "Re: ESSAY:  AI and Effective Sagacity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2039">[ date ]</a>
<a href="index.html#2039">[ thread ]</a>
<a href="subject.html#2039">[ subject ]</a>
<a href="author.html#2039">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
