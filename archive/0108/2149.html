<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: One for the history books</title>
<meta name="Author" content="Simon McClenahan (peepsplat@yahoo.com)">
<meta name="Subject" content="Re: One for the history books">
<meta name="Date" content="2001-08-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: One for the history books</h1>
<!-- received="Mon Aug 27 15:48:32 2001" -->
<!-- isoreceived="20010827214832" -->
<!-- sent="Mon, 27 Aug 2001 14:51:54 -0500" -->
<!-- isosent="20010827195154" -->
<!-- name="Simon McClenahan" -->
<!-- email="peepsplat@yahoo.com" -->
<!-- subject="Re: One for the history books" -->
<!-- id="001e01c12f31$b97637c0$140b640a@stellentied.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="3B898B23.3B2688F3@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Simon McClenahan (<a href="mailto:peepsplat@yahoo.com?Subject=Re:%20One%20for%20the%20history%20books"><em>peepsplat@yahoo.com</em></a>)<br>
<strong>Date:</strong> Mon Aug 27 2001 - 13:51:54 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2150.html">Eliezer S. Yudkowsky: "Re: One for the history books"</a>
<li><strong>Previous message:</strong> <a href="2148.html">Jimmy Wales: "Re: One for the history books"</a>
<li><strong>In reply to:</strong> <a href="2145.html">Eliezer S. Yudkowsky: "Re: One for the history books"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2149">[ date ]</a>
<a href="index.html#2149">[ thread ]</a>
<a href="subject.html#2149">[ subject ]</a>
<a href="author.html#2149">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
From: &quot;Eliezer S. Yudkowsky&quot; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20One%20for%20the%20history%20books">sentience@pobox.com</a>&gt;
<br>
<p><em>&gt; I have made the decision to go for one hundred point zero zero percent
</em><br>
<em>&gt; altruism and complete rationality.  The decision is not synonymous with
</em><br>
<em>&gt; the achievement, but is a necessary precondition of that achievement.  To
</em><br>
<em>&gt; make a deliberate compromise is to halt your progress at that point,
</em><br>
<em>&gt; because even the best you can achieve and the most ambitious form of
</em><br>
<em>&gt; sanity you can imagine is only another step on a long road where only the
</em><br>
<em>&gt; next single step is visible at any given time.  There are a variety of
</em><br>
<em>&gt; emotions that could act as sources of mental energy for my Singularity
</em><br>
<em>&gt; work, the desire for fame among them.  The excuse is there if I choose to
</em><br>
<em>&gt; use it.  I do not so choose.  I choose to relinquish those emotions rather
</em><br>
<em>&gt; than compromise rationality.
</em><br>
<p>You may choose, have the will, or the volition, to be absolutely altruistic
<br>
and rational, but so long as you have a conciousness and a sub-conciousness,
<br>
your concious self will still have less than total control over your whole
<br>
self. Your neural pathways are hard-wired to make emotionally irrational
<br>
signals, and if you did somehow manage to re-train or re-wire to achieve
<br>
true rationality, surely your self would cease to be human? Would a
<br>
transhuman be free of emotions? Would a Sysop think it is in our best
<br>
interests to be 100% altruistic and rational?
<br>
<p>Despite a lot of nay-saying from most Western Culture and most of those
<br>
people who conciously choose to be 100% logical and rational, it is quite
<br>
possible to &quot;see&quot; and affect your sub-concious both actively and passively.
<br>
Actively through self-hypnosis, meditation, and other techniques. Passively
<br>
by subliminial messages, positive reinforcement (&quot;I think I can&quot;), hanging
<br>
out with a certain crowd, memes, etc. Sub-concious behaviour can also be
<br>
externally read as well through body language, hypnosis, etc., and of course
<br>
affected by subliminal advertising, persuasion techniques, etc.
<br>
<p>I guess I'm nit-picking because Eli's choice is to be as close to 100% as
<br>
possible, but since no human can be at 100%, the measurement of altruism and
<br>
rationality becomes a relative one. Eli may achieve 95%, and I may achieve
<br>
91%, other SL4's may be in the same range and SL1's may achieve up to 20%.
<br>
Eli chooses not to use a resource of &quot;fame&quot; to reach 95% because it somehow
<br>
compromises all the other resources he uses to reach Rational Nirvana.
<br>
<p><p><em>&gt; I made that choice at around age fifteen or sixteen, shortly after I
</em><br>
<em>&gt; became aware of evolutionary psychology and the likelihood that the
</em><br>
<em>&gt; emotions designed to underly altruism would not be consistent with the
</em><br>
<em>&gt; declared goals of altruism.  Because emotions like the desire for fame are
</em><br>
<em>&gt; fairly clear-cut - are &quot;exceptional conditions&quot; within the event-loop of
</em><br>
<em>&gt; the mind - it is possible to learn to identify the emotion's subjective
</em><br>
<em>&gt; feel, notice it, and disbelieve the mental imagery that causes it.  I've
</em><br>
<em>&gt; since moved on to more interesting areas of mental cleanup.  As far as
</em><br>
<em>&gt; things like the desire for fame go, I am finished.
</em><br>
<p>My question is, why? Why not use fame in a positive manner rather than a
<br>
negative one? Me, I would have no problem with being a poster-boy for my
<br>
beliefs and special interests. I would revel in it and use the power of fame
<br>
to spread the good word with as much power as any other revolutionary in
<br>
history. I think the signup message when joining this list says something
<br>
about discouraging people from being humble. What could be more opposite of
<br>
humility than fame?
<br>
<p>What are the declared goals of altruism anyway? (I'm guessing you've already
<br>
written a tome on this subject, so the executive summary would be
<br>
appreciated)
<br>
<p><p><p><em>&gt; (I am still validating my desire for routine social respect, which is
</em><br>
<em>&gt; quite a different thing from a desire for general fame - although I am
</em><br>
<em>&gt; recently starting to believe that this emotion may be &quot;considered harmful&quot;
</em><br>
<em>&gt; as well.)
</em><br>
<p>All emotions are beneficial as well as harmful. Social respect and fame are
<br>
both external affectors. You can be affected negatively to become a victim,
<br>
or you can be affected positively to become a better person. Whether an
<br>
effect is positive or negative depends on how you receive the events. Did
<br>
the lovable/loving robot in A.I. get depressed when he could not find a way
<br>
to get love back from? No, he kept waiting, he maybe even died temporarily
<br>
(he could not know it would be &quot;temporary&quot; surely). He was resurrected and
<br>
was able to resume his &quot;love person X&quot; program at the end of the film. I
<br>
think if the story was to be continued, he would probably die or terminate
<br>
himself if he realized his purpose in life was gone (although why they
<br>
couldn't resurrect person X indefinitely doesn't make sense, but that's
<br>
Hollywood anthromorphism for you).
<br>
<p>I know, Eli wrote a tome on The Meaning of Life too. If you devote your self
<br>
to the unreachable goal of 100% to X (altruism and rationality), you will
<br>
use up to 100% of your resources to achieve this unreachable goal and there
<br>
will be no resources left for other things. For being human. For desiring
<br>
social respect, or fame. Maybe if one desires fame amongst other things, it
<br>
allows you to keep yourself in check and not go overboard and become an
<br>
&quot;unbalanced&quot; individual. Desire for these things is not harmful, it's the
<br>
side effect of the result of achieving different levels of the goal that are
<br>
important.
<br>
<p><p><p><em>&gt; In short, I'm now confident enough about my mental cleanup that I can talk
</em><br>
<em>&gt; about it without making nervous little disclaimers such as &quot;But, of
</em><br>
<em>&gt; course, who really knows what's inside their mind?&quot;
</em><br>
<p>Why &quot;nervous little disclaimer&quot;? It sounds like you're ashamed if you admit
<br>
to yourself or others that you actually don't know what goes on in your
<br>
mind. It really doesn't matter. What matters is how you use your mind, and
<br>
how you train it to become a better person.
<br>
<p><p>&nbsp;&nbsp;As far as the
<br>
<em>&gt; specific cognitive force &quot;desire for fame&quot; is concerned, I predict that my
</em><br>
<em>&gt; plans will exhibit no more sign of it than plans made by an AI.  I will
</em><br>
<em>&gt; not make excuses in advance for failures because I am done cleaning up
</em><br>
<em>&gt; that emotion and I do not expect there to be any failures.
</em><br>
<p>Sounds like you're in denial ;-)
<br>
<p><p><p><p><em>&gt;
</em><br>
<em>&gt; I realize that this is a claim for such an extraordinarily high level of
</em><br>
<em>&gt; ability that Bayesian reasoners, reasoning from the prior expected
</em><br>
<em>&gt; population levels of self-overestimators and extremely sane people, may
</em><br>
<em>&gt; find that such a claim (considered as an unadorned abstract) is more
</em><br>
<em>&gt; reason to doubt sanity than to believe it.  That's probably the reason why
</em><br>
<em>&gt; a lot of people who are interested in the cognitive science of rationality
</em><br>
<em>&gt; manage to sound self-deprecating when talking about it; not just as a
</em><br>
<em>&gt; signal to others, I think, but as a signal to themselves, because they
</em><br>
<em>&gt; *know* that high confidence in sanity is often a signal of insanity.  But
</em><br>
<em>&gt; that in itself is unsanity.  It's saying, &quot;I observe that I'm damned good
</em><br>
<em>&gt; at being sane, but I won't admit it even to myself, because if I sent
</em><br>
<em>&gt; myself the signal of confidence in sanity, I might have to interpret that
</em><br>
<em>&gt; signal as evidence of insanity.&quot;
</em><br>
<p>Is this like trying to figure out if we live in real life or is our reality
<br>
just a simulation? This is a Strange Loop of reasoning is it not? Are you
<br>
claiming to have broken free?
<br>
<p><p><p>&nbsp;&nbsp;The main reason for me to be concerned about observed fameseeking
<br>
<em>&gt; is if I see it in someone who I think would like to be a rational
</em><br>
<em>&gt; altruist, and who's already gotten fairly far in cleaning up the mental
</em><br>
<em>&gt; landscape.  Even so, fameseeking acts as an approximation to rational
</em><br>
<em>&gt; altruism under some circumstances, and I am thus willing to use this
</em><br>
<em>&gt; emotion as memetic shorthand - provided that the arguments are truthful,
</em><br>
<em>&gt; the flaws in the approximation to altruistic rationality are either unused
</em><br>
<em>&gt; or explicitly dealt with, and pure rationality is closer than under the
</em><br>
<em>&gt; previous status quo.
</em><br>
<p>OK, so there is a use for fame! Why go out of your way to avoid it? With
<br>
this above knowledge, if one was to actively seek fame, it can be useful as
<br>
another resource to achieving your goals (whatever they are). If you can use
<br>
it, use it. Don't waste resources. Ever! That's my philosophy at least.
<br>
<p><p><em>&gt; On the other hand, it's acceptable to say:  &quot;You are one of only six
</em><br>
<em>&gt; billion entities, in a galaxy of four hundred billion stars and decillions
</em><br>
<em>&gt; of sentient beings, whose lives predate the Singularity; you are one of
</em><br>
<em>&gt; the oldest of all living things.  I don't know whether you or anyone else
</em><br>
<em>&gt; will respect that in the future, because it's difficult to predict what
</em><br>
<em>&gt; transhumans will care about, but it does say something about how we should
</em><br>
<em>&gt; feel *now*.  We are not just the six billion people who were around before
</em><br>
<em>&gt; the Singularity; we are the six billion people who created the
</em><br>
<em>&gt; Singularity.  An incomprehensibly huge future rests on the consequences of
</em><br>
<em>&gt; our present-day actions, and 'the impact we have on the universe' is
</em><br>
<em>&gt; essentially 'the impact we have on the Singularity'.&quot;
</em><br>
<p>This sounds like a backup plan for what happens if effects from the
<br>
Singularity comes close to destroying us all (obviously because we didn't
<br>
implement SingInst's Friendly AI strategy). We could hopefully repopulate
<br>
with original Humans, while of course keeping the unfriendly
<br>
transhumans/sysops/computers at bay. That must be what it feels like to be a
<br>
neo-luddite ;-)
<br>
<p><p>Well, this email took longer than expected. Back to the daily grind, where
<br>
I'm not famous and I am occasionally emotionally irrational, even though I
<br>
do not conciously desire it to be that way.
<br>
<p>cheers,
<br>
&nbsp;&nbsp;&nbsp;&nbsp;Simon
<br>
<p><p>_________________________________________________________
<br>
Do You Yahoo!?
<br>
Get your free @yahoo.com address at <a href="http://mail.yahoo.com">http://mail.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2150.html">Eliezer S. Yudkowsky: "Re: One for the history books"</a>
<li><strong>Previous message:</strong> <a href="2148.html">Jimmy Wales: "Re: One for the history books"</a>
<li><strong>In reply to:</strong> <a href="2145.html">Eliezer S. Yudkowsky: "Re: One for the history books"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2149">[ date ]</a>
<a href="index.html#2149">[ thread ]</a>
<a href="subject.html#2149">[ subject ]</a>
<a href="author.html#2149">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
