<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Floppy take-off</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Floppy take-off">
<meta name="Date" content="2001-08-01">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Floppy take-off</h1>
<!-- received="Wed Aug 01 21:11:17 2001" -->
<!-- isoreceived="20010802031117" -->
<!-- sent="Wed, 01 Aug 2001 21:08:09 -0400" -->
<!-- isosent="20010802010809" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Floppy take-off" -->
<!-- id="3B68A7F9.42950318@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="4.3.2.7.2.20010801175852.022ed0b8@mail.earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Floppy%20take-off"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Aug 01 2001 - 19:08:09 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2032.html">Eliezer S. Yudkowsky: "Re: Floppy take-off"</a>
<li><strong>Previous message:</strong> <a href="2030.html">James Higgins: "Re: Dumbasses vs. AI researchers"</a>
<li><strong>In reply to:</strong> <a href="2027.html">James Higgins: "Re: Floppy take-off"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2015.html">Peter Voss: "Article: The coming superintelligence: who will be in control?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2031">[ date ]</a>
<a href="index.html#2031">[ thread ]</a>
<a href="subject.html#2031">[ subject ]</a>
<a href="author.html#2031">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
James Higgins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I don't think this should worry you.  Think of it as the emergency backup
</em><br>
<em>&gt; plan.  If all else fails, run the AI researcher software on a billion
</em><br>
<em>&gt; computers and see what it comes up with.  Given enough time and resources I
</em><br>
<em>&gt; believe even a million or less human equivalent scientists could solve
</em><br>
<em>&gt; virtually any scientific problem.  If this were our worse case scenario
</em><br>
<em>&gt; (and I don't think it is) I'd be having a celebration right now!
</em><br>
<p>Intelligence can almost certainly be brute-forced.  Friendliness rather
<br>
less so.  Although a grand challenge when considered in isolation,
<br>
Friendliness should involve less effort than general intelligence *if* you
<br>
understand the principles involved in both.
<br>
<p>Brute-force AI is a way of solving the problem without understanding it. 
<br>
Furthermore, brute-forcing is a way of solving the problem without much
<br>
programmer effort.  With Friendliness there is a certain amount of human
<br>
input, needed to provide the raw material, that cannot be easily
<br>
compressed.  On a nanocomputer, a non-Friendliness-aware AI project run by
<br>
uncomprehending researchers can beat a Friendly AI project even if the
<br>
latter has a vastly better understanding of the nature of intelligence.
<br>
<p>Any method that allows for a successfully self-improving AI where the
<br>
researchers don't really understand what's going on is a very serious
<br>
danger.  In fact, given CFAI theory, I've made a total turnaround from my
<br>
position of few years back and now regard non-Friendliness-aware AI as a
<br>
disaster scenario.  Not just an uncertain win, but a disaster scenario.
<br>
<p>A basically Friendly seed - if previously developed over the course of
<br>
years - may be something you can drop into an ultra-nanocomputer and watch
<br>
a Friendly SI explode outward a few minutes later, although the lack of
<br>
continuing human input through the initial takeoff is an added significant
<br>
risk.  But without that basically Friendly seed, brute-force AI on a
<br>
nanocomputer is a total disaster scenario.  And probably a relatively easy
<br>
one to implement (relative to grey goo, military goo, or even biotech
<br>
plagues).
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2032.html">Eliezer S. Yudkowsky: "Re: Floppy take-off"</a>
<li><strong>Previous message:</strong> <a href="2030.html">James Higgins: "Re: Dumbasses vs. AI researchers"</a>
<li><strong>In reply to:</strong> <a href="2027.html">James Higgins: "Re: Floppy take-off"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2015.html">Peter Voss: "Article: The coming superintelligence: who will be in control?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2031">[ date ]</a>
<a href="index.html#2031">[ thread ]</a>
<a href="subject.html#2031">[ subject ]</a>
<a href="author.html#2031">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
