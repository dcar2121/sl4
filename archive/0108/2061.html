<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: CFAI criticism Re: Article: The coming superintelligence:who will  be  incontrol?</title>
<meta name="Author" content="Brian Atkins (brian@posthuman.com)">
<meta name="Subject" content="Re: CFAI criticism Re: Article: The coming superintelligence:who will  be  incontrol?">
<meta name="Date" content="2001-08-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: CFAI criticism Re: Article: The coming superintelligence:who will  be  incontrol?</h1>
<!-- received="Fri Aug 03 03:42:04 2001" -->
<!-- isoreceived="20010803094204" -->
<!-- sent="Fri, 03 Aug 2001 03:40:40 -0400" -->
<!-- isosent="20010803074040" -->
<!-- name="Brian Atkins" -->
<!-- email="brian@posthuman.com" -->
<!-- subject="Re: CFAI criticism Re: Article: The coming superintelligence:who will  be  incontrol?" -->
<!-- id="3B6A5578.E9D07F4A@posthuman.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="4.3.2.7.2.20010802114636.02345d48@mail.earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brian Atkins (<a href="mailto:brian@posthuman.com?Subject=Re:%20CFAI%20criticism%20Re:%20Article:%20The%20coming%20superintelligence:who%20will%20%20be%20%20incontrol?"><em>brian@posthuman.com</em></a>)<br>
<strong>Date:</strong> Fri Aug 03 2001 - 01:40:40 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2062.html">Dan Clemmensen: "Re: Floppy take-off"</a>
<li><strong>Previous message:</strong> <a href="2060.html">Gordon Worley: "[ARTICLE] Slashdot and the Singularity"</a>
<li><strong>In reply to:</strong> <a href="2057.html">James Higgins: "Re: CFAI criticism Re: Article: The coming superintelligence: who will be  incontrol?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2066.html">Eliezer S. Yudkowsky: "Re: CFAI criticism Re: Article: The coming superintelligence:who will  be  incontrol?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2061">[ date ]</a>
<a href="index.html#2061">[ thread ]</a>
<a href="subject.html#2061">[ subject ]</a>
<a href="author.html#2061">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
James Higgins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; At 02:03 PM 8/2/2001 -0400, you wrote:
</em><br>
<em>&gt; &gt;James Higgins wrote:
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; When I first read &quot;Staring Into the Singularity&quot; I started thinking about
</em><br>
<em>&gt; &gt; &gt; how much more, well just more/different, an SI would be than ourselves.  As
</em><br>
<em>&gt; &gt; &gt; it has been discussed in this room, most people believe that a human can't
</em><br>
<em>&gt; &gt; &gt; even talk with an SI though a binary (light on/off) connection without
</em><br>
<em>&gt; &gt; &gt; having them be controlled by the SI.  Given such vast intellect,
</em><br>
<em>&gt; &gt; &gt; capabilities and the freedom to fully alter its own code I don't believe
</em><br>
<em>&gt; &gt; &gt; there is anything we can program into an AI that will ensure friendliness
</em><br>
<em>&gt; &gt; &gt; when it gets to SI status.  We're just not anywhere near smart enough to do
</em><br>
<em>&gt; &gt; &gt; that.  I really wish I didn't believe this (it would make me happier), but
</em><br>
<em>&gt; &gt; &gt; this is what extensive thought on the matter leads me to believe.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; Based on this belief, the best course may be to hold off on launching an AI
</em><br>
<em>&gt; &gt; &gt; that could progress to an SI until we have the ability to enhance our
</em><br>
<em>&gt; &gt; &gt; intelligence significantly.  Humans with much greater intelligence *may* be
</em><br>
<em>&gt; &gt; &gt; able to alter/control a SI, but I believe that ultimately we cannot.  But I
</em><br>
<em>&gt; &gt; &gt; suspect that we will have Real AI and most likely SI before that comes to
</em><br>
<em>&gt; &gt; &gt; pass, thus my belief that if SIs aren't inherently friendly we are probably
</em><br>
<em>&gt; &gt; &gt; doomed.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;One thing SIAI is trying to do is make something of a science out of
</em><br>
<em>&gt; &gt;Friendliness. It may be impossible, but we're trying. Here we have a
</em><br>
<em>&gt; &gt;large difference of opinion between us and James on what would be the
</em><br>
<em>&gt; &gt;optimum path to take due more or less to this one issue of Friendliness.
</em><br>
<em>&gt; &gt;But so far James seems to be going on mostly a &quot;gut feel&quot; that Friendly
</em><br>
<em>&gt; &gt;AI is not doable with a large degree of certainty. Do you have any specific
</em><br>
<em>&gt; &gt;criticisms of FAI James that we could try to discuss? I can tell from
</em><br>
<em>&gt; &gt;your other posts that your main concern is apparently a combo of &quot;will it
</em><br>
<em>&gt; &gt;work long term&quot; and &quot;can we be 100% certain&quot;, right? It seems like your
</em><br>
<em>&gt; &gt;concern is addressed in the CFAI FAQ:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I am not an AI expert.  Actually, I have no real training in AI at all.  I
</em><br>
<em>&gt; am a master software architect/engineer and fairly intelligent,
</em><br>
<em>&gt; however.  So I have read many of the Singularity related documents, thought
</em><br>
<em>&gt; long and hard and participate in this list in order to learn and to provide
</em><br>
<em>&gt; a slightly different perspective on things.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So I guess you could say I am going on &quot;gut feel&quot; to some extent, and also
</em><br>
<em>&gt; applied reasoning and logic.  To me, it is not logical to assume that we
</em><br>
<em>&gt; can sufficiently influence an entity that will be many millions of times
</em><br>
<em>&gt; more intelligent than us.  This is like saying mice could influence humans
</em><br>
<em>&gt; to be mouse-friendly.  Yes, I realize that mice don't exactly equate or
</em><br>
<p>Ok, first off we are not attempting to influence a SI. The period of time
<br>
we will be actively influencing it will be like from birth to at least
<br>
Eliezer (and whoever else is around) level of thought. But past that it
<br>
will be on its own. So think of it as some humans influencing a human, and
<br>
then the human goes out into the world and makes its own decisions from
<br>
that point on.
<br>
<p><em>&gt; have technology, but we will be much farther down on the intelligence scale
</em><br>
<em>&gt; to an SI than a mouse is to us.  So both my gut feel and reasoning suggest
</em><br>
<em>&gt; that we can't do much to influence the SI.  No disagreement about the fact
</em><br>
<em>&gt; that we can create one, or that we should *try* to influence it, however.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I have also discussed this topic with a friend of mine who is very
</em><br>
<em>&gt; intelligent and extremely knowledgeable about AI.  He is working on (has
</em><br>
<em>&gt; been for quite some time actually) a language specifically intended for AI
</em><br>
<em>&gt; development.  He has read many of the Singularity documents, but does not
</em><br>
<em>&gt; participate on this list (to the best of my knowledge at least, he has
</em><br>
<em>&gt; never posted).  He had many good arguments on why we would almost certainly
</em><br>
<em>&gt; fail to successfully implement friendliness into an SI!  So I have thought
</em><br>
<p>We would love to hear these comments either publicly or privately. Could
<br>
you get him to write them up into an email, or do you remember any specifics?
<br>
Anyone who can help push Friendliness further has directly contributed to
<br>
making the Singularity safer.
<br>
<p><em>&gt; about and discussed this topic thoroughly.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;I have a hard time seeing how a human-level Gandhi-ish AI will suddenly run
</em><br>
<em>&gt; &gt;amok as it gets smarter, except due to some technical glitch (which is a
</em><br>
<em>&gt; &gt;separate issue we can talk about if you want).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If you were talking about our ability to create a friendly AI, we
</em><br>
<em>&gt; agree.  However, the AI will have to evolve many, many times in order to
</em><br>
<em>&gt; become an SI.  During any one of these evolutions it could, intentionally
</em><br>
<em>&gt; or not, remove or hamper friendliness.  Some of these could entail a
</em><br>
<em>&gt; complete, from the ground up rewrite, using none of the original code and
</em><br>
<em>&gt; only hand-picked logic/data.  Friendliness, as a requirement, could easily
</em><br>
<em>&gt; fall out during such a transition.  It could decide that it would be better
</em><br>
<em>&gt; off without some of the code/data that is part of friendliness.  Further,
</em><br>
<em>&gt; it could at some point ponder why it is supposed to be friendly at all.  It
</em><br>
<em>&gt; could decide that being friendly to humans is not a top priority, or that
</em><br>
<em>&gt; how to be friendly should be completely different than what we envision.
</em><br>
<p>Remember, we do not plan to &quot;release&quot; the AI until it has at least gained
<br>
a level of intelligence and Friendliness at least as good as us. Do you
<br>
really think such a being would make the mistakes you are worrying about?
<br>
It will know how critical it is to get things right, and it will make sure
<br>
to do so. It will test rewritten versions of itself before giving up on
<br>
the old design, just like we tested the original version. Only it will be
<br>
able to test things even more thoroughly IMO.
<br>
<p>As for changing its mind about Friendliness, we actually expect that to
<br>
happen. We are not attempting to indoctrinate some robot that must stick
<br>
to a certain way of thinking. We think as it gets smarter it should be
<br>
able to continually look at and revise its exact thoughts regarding
<br>
friendliness. However, it will only do so once it is damn sure that it
<br>
is making an improvement to what it already has learned. As for completely
<br>
dumping friendliness, see:
<br>
<p><a href="http://www.intelligence.org/CFAI/info/indexfaq.html#q_2.6">http://www.intelligence.org/CFAI/info/indexfaq.html#q_2.6</a>
<br>
<p><em>&gt; 
</em><br>
<em>&gt; We have a hard enough time making stable hardware/software (Windows 2000
</em><br>
<em>&gt; crashed on me when I was originally writing this reply), so I frankly doubt
</em><br>
<em>&gt; our ability to implement such a subtle concept in such a complex, self
</em><br>
<em>&gt; evolving system.
</em><br>
<p>If we can't do it, we won't release it.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; That is not to say that I think SingInst, Eli or any other such individuals
</em><br>
<em>&gt; or organizations are wasting time or effort.  Friendliness and such
</em><br>
<em>&gt; concepts are things that we must research.  Even if we only nudge the SI,
</em><br>
<em>&gt; just slightly, in that direction the effort is worthwhile.  Any progress is
</em><br>
<em>&gt; better than no progress.  I'm just a realist, and I realistically don't
</em><br>
<em>&gt; think we are adequately equipped, at present, to ensure a friendly SI.  I
</em><br>
<em>&gt; think intelligent enhancement, if it becomes available in time, would be a
</em><br>
<em>&gt; major boon to your work.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;Also, can you address this quote from Q3.3 in the FAQ, since it relates
</em><br>
<em>&gt; &gt;to your suggestion the ideal path would be to wait:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&quot;Nothing in this world is perfectly safe.  The question is how to minimize
</em><br>
<em>&gt; &gt;  risk.  As best as we can figure it, trying really hard to develop Friendly
</em><br>
<em>&gt; &gt;  AI is safer than any alternate strategy, including not trying to develop
</em><br>
<em>&gt; &gt;  Friendly AI, or waiting to develop Friendly AI, or trying to develop some
</em><br>
<em>&gt; &gt;  other technology first.  That's why the Singularity Institute exists.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; That's the wonderful thing, we can have it both ways.  I agree that you
</em><br>
<em>&gt; shouldn't be waiting for anything and should be working on friendliness
</em><br>
<em>&gt; now.  You don't have to, the work that will eventually lead to intelligence
</em><br>
<em>&gt; enhancement is going on in parallel.  If, however, we get to the point that
</em><br>
<em>&gt; we have both the hardware &amp; software to launch an SI, but have not
</em><br>
<em>&gt; progressed massively on the general concept of friendliness THEN I think it
</em><br>
<em>&gt; may be prudent to wait.  So I'm advocating delays later, rather than
</em><br>
<em>&gt; sooner, if it is necessary.
</em><br>
<em>&gt; 
</em><br>
<p>That would be a heckuva tough decision for us to make. If we had our AI
<br>
grown to the point we thought it was ready to release, I doubt we could
<br>
find arguments sufficient to hold off releasing it. We're talking about
<br>
something here that like I said would feel something like Eliezer/Gandhi
<br>
when you talk to it. We will have run all our tests and as far as we can
<br>
tell it fits our goals. If we decide to wait even longer, how much longer
<br>
should we wait? Until humans hit a certain IQ? What specifically is that
<br>
number we should wait for? It would be extremely difficult to draw the line.
<br>
You would never be able to say that you were 100% sure it was going to
<br>
have a positive long term result. Meanwhile, a bunch of people die everyday
<br>
you delay...
<br>
<pre>
-- 
Brian Atkins
Singularity Institute for Artificial Intelligence
<a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2062.html">Dan Clemmensen: "Re: Floppy take-off"</a>
<li><strong>Previous message:</strong> <a href="2060.html">Gordon Worley: "[ARTICLE] Slashdot and the Singularity"</a>
<li><strong>In reply to:</strong> <a href="2057.html">James Higgins: "Re: CFAI criticism Re: Article: The coming superintelligence: who will be  incontrol?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2066.html">Eliezer S. Yudkowsky: "Re: CFAI criticism Re: Article: The coming superintelligence:who will  be  incontrol?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2061">[ date ]</a>
<a href="index.html#2061">[ thread ]</a>
<a href="subject.html#2061">[ subject ]</a>
<a href="author.html#2061">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
