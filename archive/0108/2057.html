<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: CFAI criticism Re: Article: The coming superintelligence: who will be  incontrol?</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="Re: CFAI criticism Re: Article: The coming superintelligence: who will be  incontrol?">
<meta name="Date" content="2001-08-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: CFAI criticism Re: Article: The coming superintelligence: who will be  incontrol?</h1>
<!-- received="Thu Aug 02 17:37:03 2001" -->
<!-- isoreceived="20010802233703" -->
<!-- sent="Thu, 02 Aug 2001 14:48:08 -0700" -->
<!-- isosent="20010802214808" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="Re: CFAI criticism Re: Article: The coming superintelligence: who will be  incontrol?" -->
<!-- id="4.3.2.7.2.20010802114636.02345d48@mail.earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3B69960C.842A1F28@posthuman.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=Re:%20CFAI%20criticism%20Re:%20Article:%20The%20coming%20superintelligence:%20who%20will%20be%20%20incontrol?"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Thu Aug 02 2001 - 15:48:08 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2058.html">Mitch Howe: "Re: ESSAY:  AI and Effective Sagacity"</a>
<li><strong>Previous message:</strong> <a href="2056.html">Brian Atkins: "NEWS: IBM moves into the supercomputer rental biz"</a>
<li><strong>In reply to:</strong> <a href="2051.html">Brian Atkins: "CFAI criticism Re: Article: The coming superintelligence: who will be  incontrol?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2059.html">Eliezer S. Yudkowsky: "Re: CFAI criticism Re: Article: The coming superintelligence:who will  be  incontrol?"</a>
<li><strong>Reply:</strong> <a href="2059.html">Eliezer S. Yudkowsky: "Re: CFAI criticism Re: Article: The coming superintelligence:who will  be  incontrol?"</a>
<li><strong>Reply:</strong> <a href="2061.html">Brian Atkins: "Re: CFAI criticism Re: Article: The coming superintelligence:who will  be  incontrol?"</a>
<li><strong>Reply:</strong> <a href="2066.html">Eliezer S. Yudkowsky: "Re: CFAI criticism Re: Article: The coming superintelligence:who will  be  incontrol?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2057">[ date ]</a>
<a href="index.html#2057">[ thread ]</a>
<a href="subject.html#2057">[ subject ]</a>
<a href="author.html#2057">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 02:03 PM 8/2/2001 -0400, you wrote:
<br>
<em>&gt;James Higgins wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; When I first read &quot;Staring Into the Singularity&quot; I started thinking about
</em><br>
<em>&gt; &gt; how much more, well just more/different, an SI would be than ourselves.  As
</em><br>
<em>&gt; &gt; it has been discussed in this room, most people believe that a human can't
</em><br>
<em>&gt; &gt; even talk with an SI though a binary (light on/off) connection without
</em><br>
<em>&gt; &gt; having them be controlled by the SI.  Given such vast intellect,
</em><br>
<em>&gt; &gt; capabilities and the freedom to fully alter its own code I don't believe
</em><br>
<em>&gt; &gt; there is anything we can program into an AI that will ensure friendliness
</em><br>
<em>&gt; &gt; when it gets to SI status.  We're just not anywhere near smart enough to do
</em><br>
<em>&gt; &gt; that.  I really wish I didn't believe this (it would make me happier), but
</em><br>
<em>&gt; &gt; this is what extensive thought on the matter leads me to believe.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Based on this belief, the best course may be to hold off on launching an AI
</em><br>
<em>&gt; &gt; that could progress to an SI until we have the ability to enhance our
</em><br>
<em>&gt; &gt; intelligence significantly.  Humans with much greater intelligence *may* be
</em><br>
<em>&gt; &gt; able to alter/control a SI, but I believe that ultimately we cannot.  But I
</em><br>
<em>&gt; &gt; suspect that we will have Real AI and most likely SI before that comes to
</em><br>
<em>&gt; &gt; pass, thus my belief that if SIs aren't inherently friendly we are probably
</em><br>
<em>&gt; &gt; doomed.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;One thing SIAI is trying to do is make something of a science out of
</em><br>
<em>&gt;Friendliness. It may be impossible, but we're trying. Here we have a
</em><br>
<em>&gt;large difference of opinion between us and James on what would be the
</em><br>
<em>&gt;optimum path to take due more or less to this one issue of Friendliness.
</em><br>
<em>&gt;But so far James seems to be going on mostly a &quot;gut feel&quot; that Friendly
</em><br>
<em>&gt;AI is not doable with a large degree of certainty. Do you have any specific
</em><br>
<em>&gt;criticisms of FAI James that we could try to discuss? I can tell from
</em><br>
<em>&gt;your other posts that your main concern is apparently a combo of &quot;will it
</em><br>
<em>&gt;work long term&quot; and &quot;can we be 100% certain&quot;, right? It seems like your
</em><br>
<em>&gt;concern is addressed in the CFAI FAQ:
</em><br>
<p>I am not an AI expert.  Actually, I have no real training in AI at all.  I 
<br>
am a master software architect/engineer and fairly intelligent, 
<br>
however.  So I have read many of the Singularity related documents, thought 
<br>
long and hard and participate in this list in order to learn and to provide 
<br>
a slightly different perspective on things.
<br>
<p>So I guess you could say I am going on &quot;gut feel&quot; to some extent, and also 
<br>
applied reasoning and logic.  To me, it is not logical to assume that we 
<br>
can sufficiently influence an entity that will be many millions of times 
<br>
more intelligent than us.  This is like saying mice could influence humans 
<br>
to be mouse-friendly.  Yes, I realize that mice don't exactly equate or 
<br>
have technology, but we will be much farther down on the intelligence scale 
<br>
to an SI than a mouse is to us.  So both my gut feel and reasoning suggest 
<br>
that we can't do much to influence the SI.  No disagreement about the fact 
<br>
that we can create one, or that we should *try* to influence it, however.
<br>
<p>I have also discussed this topic with a friend of mine who is very 
<br>
intelligent and extremely knowledgeable about AI.  He is working on (has 
<br>
been for quite some time actually) a language specifically intended for AI 
<br>
development.  He has read many of the Singularity documents, but does not 
<br>
participate on this list (to the best of my knowledge at least, he has 
<br>
never posted).  He had many good arguments on why we would almost certainly 
<br>
fail to successfully implement friendliness into an SI!  So I have thought 
<br>
about and discussed this topic thoroughly.
<br>
<p><em>&gt;I have a hard time seeing how a human-level Gandhi-ish AI will suddenly run
</em><br>
<em>&gt;amok as it gets smarter, except due to some technical glitch (which is a
</em><br>
<em>&gt;separate issue we can talk about if you want).
</em><br>
<p>If you were talking about our ability to create a friendly AI, we 
<br>
agree.  However, the AI will have to evolve many, many times in order to 
<br>
become an SI.  During any one of these evolutions it could, intentionally 
<br>
or not, remove or hamper friendliness.  Some of these could entail a 
<br>
complete, from the ground up rewrite, using none of the original code and 
<br>
only hand-picked logic/data.  Friendliness, as a requirement, could easily 
<br>
fall out during such a transition.  It could decide that it would be better 
<br>
off without some of the code/data that is part of friendliness.  Further, 
<br>
it could at some point ponder why it is supposed to be friendly at all.  It 
<br>
could decide that being friendly to humans is not a top priority, or that 
<br>
how to be friendly should be completely different than what we envision.
<br>
<p>We have a hard enough time making stable hardware/software (Windows 2000 
<br>
crashed on me when I was originally writing this reply), so I frankly doubt 
<br>
our ability to implement such a subtle concept in such a complex, self 
<br>
evolving system.
<br>
<p>That is not to say that I think SingInst, Eli or any other such individuals 
<br>
or organizations are wasting time or effort.  Friendliness and such 
<br>
concepts are things that we must research.  Even if we only nudge the SI, 
<br>
just slightly, in that direction the effort is worthwhile.  Any progress is 
<br>
better than no progress.  I'm just a realist, and I realistically don't 
<br>
think we are adequately equipped, at present, to ensure a friendly SI.  I 
<br>
think intelligent enhancement, if it becomes available in time, would be a 
<br>
major boon to your work.
<br>
<p><em>&gt;Also, can you address this quote from Q3.3 in the FAQ, since it relates
</em><br>
<em>&gt;to your suggestion the ideal path would be to wait:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&quot;Nothing in this world is perfectly safe.  The question is how to minimize
</em><br>
<em>&gt;  risk.  As best as we can figure it, trying really hard to develop Friendly
</em><br>
<em>&gt;  AI is safer than any alternate strategy, including not trying to develop
</em><br>
<em>&gt;  Friendly AI, or waiting to develop Friendly AI, or trying to develop some
</em><br>
<em>&gt;  other technology first.  That's why the Singularity Institute exists.&quot;
</em><br>
<p>That's the wonderful thing, we can have it both ways.  I agree that you 
<br>
shouldn't be waiting for anything and should be working on friendliness 
<br>
now.  You don't have to, the work that will eventually lead to intelligence 
<br>
enhancement is going on in parallel.  If, however, we get to the point that 
<br>
we have both the hardware &amp; software to launch an SI, but have not 
<br>
progressed massively on the general concept of friendliness THEN I think it 
<br>
may be prudent to wait.  So I'm advocating delays later, rather than 
<br>
sooner, if it is necessary.
<br>
<p>James Higgins
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2058.html">Mitch Howe: "Re: ESSAY:  AI and Effective Sagacity"</a>
<li><strong>Previous message:</strong> <a href="2056.html">Brian Atkins: "NEWS: IBM moves into the supercomputer rental biz"</a>
<li><strong>In reply to:</strong> <a href="2051.html">Brian Atkins: "CFAI criticism Re: Article: The coming superintelligence: who will be  incontrol?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2059.html">Eliezer S. Yudkowsky: "Re: CFAI criticism Re: Article: The coming superintelligence:who will  be  incontrol?"</a>
<li><strong>Reply:</strong> <a href="2059.html">Eliezer S. Yudkowsky: "Re: CFAI criticism Re: Article: The coming superintelligence:who will  be  incontrol?"</a>
<li><strong>Reply:</strong> <a href="2061.html">Brian Atkins: "Re: CFAI criticism Re: Article: The coming superintelligence:who will  be  incontrol?"</a>
<li><strong>Reply:</strong> <a href="2066.html">Eliezer S. Yudkowsky: "Re: CFAI criticism Re: Article: The coming superintelligence:who will  be  incontrol?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2057">[ date ]</a>
<a href="index.html#2057">[ thread ]</a>
<a href="subject.html#2057">[ subject ]</a>
<a href="author.html#2057">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
