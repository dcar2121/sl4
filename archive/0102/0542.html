<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Beyond evolution</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Beyond evolution">
<meta name="Date" content="2001-02-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Beyond evolution</h1>
<!-- received="Mon Feb 05 12:59:05 2001" -->
<!-- isoreceived="20010205195905" -->
<!-- sent="Mon, 05 Feb 2001 12:57:20 -0500" -->
<!-- isosent="20010205175720" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Beyond evolution" -->
<!-- id="3A7EE980.EC93EDF0@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3A7E5883.4A572FB6@objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Beyond%20evolution"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Feb 05 2001 - 10:57:20 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0543.html">Samantha Atkins: "Re: Beyond evolution"</a>
<li><strong>Previous message:</strong> <a href="0541.html">Samantha Atkins: "Re: Beyond evolution"</a>
<li><strong>In reply to:</strong> <a href="0541.html">Samantha Atkins: "Re: Beyond evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0543.html">Samantha Atkins: "Re: Beyond evolution"</a>
<li><strong>Reply:</strong> <a href="0543.html">Samantha Atkins: "Re: Beyond evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#542">[ date ]</a>
<a href="index.html#542">[ thread ]</a>
<a href="subject.html#542">[ subject ]</a>
<a href="author.html#542">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Samantha Atkins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &gt; Multiple entities are multiple points of failure of Friendliness and even
</em><br>
<em>&gt; &gt; greater dangers.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes, but we seem to get along pretty well being able to more or less
</em><br>
<em>&gt; balance one another's power and to some extent limit each other's
</em><br>
<em>&gt; possibility of running totally amok in an un-stoppable way.
</em><br>
<p>Part of the reason why I think a Friendly SI will go with the
<br>
Sysop scenario is that I think the system of checks and balances breaks
<br>
down completely under high technology.
<br>
<p>Humans are a special case on at least two counts:  First, we exist in the
<br>
sort of intermediate technological society where, right up until fifty
<br>
years ago, it was *technically impossible* to run completely amok and
<br>
destroy the world, and even now, the checks and balances work fairly
<br>
well.  Second, we're evolved entities who usually aren't trustworthy, or
<br>
knowably trustworthy at any rate, in the absence of checks and balances.
<br>
<p><em>&gt; A single
</em><br>
<em>&gt; sysop is missing that sort of checks and balances.  The assumption is
</em><br>
<em>&gt; that we can design it so well that it will automatically check and
</em><br>
<em>&gt; balance.  I confess to having a lot of doubt that this can be done.
</em><br>
<p>Humans in general are balanced: balanced against each other; balanced in
<br>
terms of internal cognitive ability levels; balanced between nature and
<br>
nurture; part of one big Gaussian curve.  It doesn't apply to the
<br>
transhuman spaces.
<br>
<p><em>&gt; &gt; A failure of Friendliness in a transcending seed AI results in a total
</em><br>
<em>&gt; &gt; takeover regardless of what a Friendly AI thinks about the Sysop
</em><br>
<em>&gt; &gt; Scenario.  Once an AI has *reached* the Sysop point you're either screwed
</em><br>
<em>&gt; &gt; or saved, so forking off more Sysops after that is a particularly
</em><br>
<em>&gt; &gt; pointless risk.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But a Sysop does &quot;take over&quot; and govern the entire space.  The AIs can
</em><br>
<em>&gt; balance each other out.
</em><br>
<p>I really don't think so.  First AI to transcend moves into accelerated
<br>
subjective time and wins all the marbles.  Unless ve decides not to, in
<br>
which case you have the &quot;flipping through the deck&quot; problem.
<br>
<p><em>&gt; I have a large worry with the idea of their
</em><br>
<em>&gt; only being one of them and with it perhaps having too limited a notion
</em><br>
<em>&gt; of what &quot;Friendliness&quot; entails.
</em><br>
<p>Having too limited a notion?  Sounds unFriendly to me.
<br>
<p><em>&gt; The question is why I should allow myself to be
</em><br>
<em>&gt; limited by your notion of a Sysop.
</em><br>
<p>Mine?  I may not be able to shove off all responsibility onto the
<br>
shoulders of an SI but I sure intend to try.  Letting Samantha Atkins be
<br>
limited by &quot;Eliezer Yudkowsky's notion&quot; (that is, a notion which is unique
<br>
to Eliezer Yudkowsky and not an inevitable consequence of panhuman ethics)
<br>
sounds unFriendly to me.
<br>
<p><em>&gt; If we decide it is not a good
</em><br>
<em>&gt; solution but the Sysop disagrees, then what?
</em><br>
<p>Then either &quot;we&quot; (who's &quot;we&quot;?) are mistaken, or somebody (me) really
<br>
screwed up the definition of Friendliness.
<br>
<p><em>&gt; I can see in theory how such a being could not be in the way but I think
</em><br>
<em>&gt; my notion of that is a bit different than yours.
</em><br>
<p>Well, I've yet to hear a concrete proposal that would result in less
<br>
summated suffering than a Sysop Scenario - under *any* definition of
<br>
morality.
<br>
<p><em>&gt; &gt; Yes, and yes.  The risks inherent in material omnipotence are inherent in
</em><br>
<em>&gt; &gt; rapid transcendence and thus inherent in AI.  The Sysop Scenario adds
</em><br>
<em>&gt; &gt; nothing to that.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; However, your solution is to make one AI and SI with the right
</em><br>
<em>&gt; moral/ethical grounding to have this power without running amok.  What
</em><br>
<em>&gt; of the other billions of beings?  Is there an evolutionary path for them
</em><br>
<em>&gt; up to and beyond this level of transcendence (assuming they wish it)?
</em><br>
<p>Yes!  But strike &quot;evolutionary&quot; from the record, please.
<br>
<p><em>&gt; What of other beings reaching full trancendence and having to learn
</em><br>
<em>&gt; wisdom and morality along the way?  Is there room enough for them to do
</em><br>
<em>&gt; so?
</em><br>
<p>Sure!
<br>
<p><em>&gt; &gt; And, incidentally, the Sysop Scenario isn't what I'm telling a Friendly
</em><br>
<em>&gt; &gt; seed AI to do.  It's what I think a Friendly superintelligence will decide
</em><br>
<em>&gt; &gt; to do.  I'm neither taking on the building of such, nor need to be
</em><br>
<em>&gt; &gt; convinced that it's the Only Answer.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; OK.  I don't think it will decide quite along the Sysop lines, but you
</em><br>
<em>&gt; could well be right.
</em><br>
<p>OK.
<br>
<p><em>&gt; OK.  As long as the first Sysop doesn't insist Friendliness is only
</em><br>
<em>&gt; compatible with roughly its own solutions to the very complex questions
</em><br>
<em>&gt; involved.  My intuition is that there are many possible solution spaces
</em><br>
<em>&gt; that cannot all be explored  by any one SI.  Some of them may not even
</em><br>
<em>&gt; seem all that &quot;Friendly&quot; from other particular solution spaces.
</em><br>
<p>Which parts of &quot;Friendliness&quot; are more and less arbitrary is itself part
<br>
of the understanding that constitutes a Friendship system.  Any
<br>
sufficiently arbitrary answer shouldn't be part of Friendliness at all; it
<br>
should probably just be delegated back to the volitional decisions of
<br>
individuals, or at least be overridable by the decisions of individuals. 
<br>
Even the primacy of pleasure over pain is subject to the volitional
<br>
override of static masochists.
<br>
<p><em>&gt; I guess I have a hard time expecting many people to do this.  Or at
</em><br>
<em>&gt; least it is doubtful that they wouldn't choose to upgrade pretty soon.
</em><br>
<em>&gt; So what is the significance of &quot;static&quot;.  I think I am missing something
</em><br>
<em>&gt; there.
</em><br>
<p>The significance of &quot;static&quot; is that it's the only part of the Universe
<br>
about which we can have meaningful discussions.
<br>
<p><em>&gt; &gt; &gt; For entities in a VR who are playing with
</em><br>
<em>&gt; &gt; &gt; designer universes of simulated beings they experience from inside, is
</em><br>
<em>&gt; &gt; &gt; it really harm that in this universe these simulated beings maim and
</em><br>
<em>&gt; &gt; &gt; kill one another?  In other words, does the SysOp prevent real harm or
</em><br>
<em>&gt; &gt; &gt; all appearance of harm?  What is and isn't real needs answering also,
</em><br>
<em>&gt; &gt; &gt; obviously.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I don't see how this moral issue is created by the Sysop Scenario.  It's
</em><br>
<em>&gt; &gt; something that we need to decide, as a fundamental moral issue, no matter
</em><br>
<em>&gt; &gt; which future we walk into.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; OK, but I am exploring what you think the Sysop answer is or should be
</em><br>
<em>&gt; to be compatible with Friendliness.
</em><br>
<p>Will you take &quot;I don't know, I'll ask the Sysop&quot; as a legitimate answer
<br>
here?
<br>
<p><em>&gt; &gt; You can still become a better person, as measured by what you'd do if the
</em><br>
<em>&gt; &gt; Sysop suddenly vanished.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But will you ever know unless it does?
</em><br>
<p>Sure; have your exoself run a predictive scan on your simulated cortex. 
<br>
Minds are real in themselves and can be understood in themselves; the
<br>
external reality is the expression of it, not the test.
<br>
<p><em>&gt; No.  Others can go outside who may have more nefarious motives.  Are you
</em><br>
<em>&gt; claiming they would never tire of being bloody tyrants, never feel
</em><br>
<em>&gt; remorse, never seek to undo some part of the painful ugly creatin they
</em><br>
<em>&gt; made?
</em><br>
<p>Some would, some wouldn't.
<br>
<p><em>&gt; Without experiencing the consequences, how do beings actually
</em><br>
<em>&gt; learn these things?
</em><br>
<p>I think that maybe one of the underlying disagreements is that we disagree
<br>
on how much &quot;real experience&quot; is necessary.  My own position is that the
<br>
human brain has two settings:  &quot;Sympathize, using all available hardwired
<br>
neurons,&quot; and &quot;Project abstractly, using high-level thoughts.&quot;  For us,
<br>
there's a very sharp border between really experiencing something and
<br>
thinking about it abstractly, because we can't do enough abstract thought
<br>
to simulate all the pixels in a visual cortex.  For us, the behaviors that
<br>
we abstractly imagine on hearing the phrase &quot;four-dimensional visual
<br>
cortex&quot; will never be as sharp, as real, as the experiences of an entity
<br>
with a true 4D visual cortex.  But this is a distinction that breaks down
<br>
for self-modifying entities, like seed AIs or transcendent humans, who can
<br>
abstractly think about every pixel and feature extractor in a 4D visual
<br>
cortex, and thus understand every facet of intuition and behavior that
<br>
would be exhibited by a being with a true 4D visual cortex, even if he or
<br>
she or ve retains their original 3D visual cortex the whole while.  A seed
<br>
AI or a transcendent human with a 3D cortex can look at a 4D Escher
<br>
painting and understand it by virtue of their ability to understand a 4D
<br>
cortex.
<br>
<p>So, without experiencing the consequences, beings learn by using their
<br>
very vivid imaginations.
<br>
<p><em>&gt; Sure.  Make a space (probably VR) where entities can do whatever they
</em><br>
<em>&gt; wish including making their own VR spaces controlled by theselves which
</em><br>
<em>&gt; are as miserable or wonderful as they wish and as their skill and wisdom
</em><br>
<em>&gt; allows.  Keep the Sysop as an entity that insures all conscious beings
</em><br>
<em>&gt; created or who become involved come to no permanent or irreparable
</em><br>
<em>&gt; harm.  Otherwise they are free to be as good or horrible to one another
</em><br>
<em>&gt; as they wish.  And they are free to not know that they can come to no
</em><br>
<em>&gt; irreparable harm or cause any.  Would this be compatible?
</em><br>
<p>OK, but it sounds like you're talking an &quot;unescapable&quot; Sysop, which I
<br>
really thought was your whole point in the first place.  I mean, if I
<br>
understand this scenario correctly, I can't go Outside for fear that I'll
<br>
bring an entity to permanent or irreparable harm.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0543.html">Samantha Atkins: "Re: Beyond evolution"</a>
<li><strong>Previous message:</strong> <a href="0541.html">Samantha Atkins: "Re: Beyond evolution"</a>
<li><strong>In reply to:</strong> <a href="0541.html">Samantha Atkins: "Re: Beyond evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0543.html">Samantha Atkins: "Re: Beyond evolution"</a>
<li><strong>Reply:</strong> <a href="0543.html">Samantha Atkins: "Re: Beyond evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#542">[ date ]</a>
<a href="index.html#542">[ thread ]</a>
<a href="subject.html#542">[ subject ]</a>
<a href="author.html#542">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
