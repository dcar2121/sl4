<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Beyond evolution</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Beyond evolution">
<meta name="Date" content="2001-02-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Beyond evolution</h1>
<!-- received="Mon Feb 05 11:50:06 2001" -->
<!-- isoreceived="20010205185006" -->
<!-- sent="Sun, 04 Feb 2001 23:38:43 -0800" -->
<!-- isosent="20010205073843" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Beyond evolution" -->
<!-- id="3A7E5883.4A572FB6@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3A7E30A9.B6E60FDF@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Beyond%20evolution"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Mon Feb 05 2001 - 00:38:43 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0542.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>Previous message:</strong> <a href="0540.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>In reply to:</strong> <a href="0539.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0542.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>Reply:</strong> <a href="0542.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#541">[ date ]</a>
<a href="index.html#541">[ thread ]</a>
<a href="subject.html#541">[ subject ]</a>
<a href="author.html#541">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&quot;Eliezer S. Yudkowsky&quot; wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Samantha Atkins wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; and will not allow
</em><br>
<em>&gt; &gt; disagreement that leads to possible actions that it decides are possibly
</em><br>
<em>&gt; &gt; harmful to the sentiences in its care?  Where is the freedom?  I see
</em><br>
<em>&gt; &gt; freedom to disagree but not to fully act on one's disagreement?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The Sysop rules won't allow you to kill someone without vis permission.
</em><br>
<em>&gt; You can advocate killing people to your heart's content.
</em><br>
<p>It is not about killing and your answers that involve killing or the
<br>
threat to kill when I am talking about the more general issue of freedom
<br>
are both over-simplifying the question and suggestive of some either-or
<br>
limitations that are bogus.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; &gt; Build another SI of equal intelligence - sure, as long as you build ver
</em><br>
<em>&gt; &gt; &gt; inside the Sysop.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; What for?  That would rather defeat the purpose of having more than one
</em><br>
<em>&gt; &gt; local Entity of such power.  A single entitity is a single point of
</em><br>
<em>&gt; &gt; failure of Friendliness and a great danger.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Multiple entities are multiple points of failure of Friendliness and even
</em><br>
<em>&gt; greater dangers.
</em><br>
<em>&gt; 
</em><br>
<p>Yes, but we seem to get along pretty well being able to more or less
<br>
balance one another's power and to some extent limit each other's
<br>
possibility of running totally amok in an unstoppable way.  A single
<br>
sysop is missing that sort of checks and balances.  The assumption is
<br>
that we can design it so well that it will automatically check and
<br>
balance.  I confess to having a lot of doubt that this can be done.  
<br>
<p><em>&gt; A failure of Friendliness in a transcending seed AI results in a total
</em><br>
<em>&gt; takeover regardless of what a Friendly AI thinks about the Sysop
</em><br>
<em>&gt; Scenario.  Once an AI has *reached* the Sysop point you're either screwed
</em><br>
<em>&gt; or saved, so forking off more Sysops after that is a particularly
</em><br>
<em>&gt; pointless risk.
</em><br>
<em>&gt; 
</em><br>
<p>But a Sysop does &quot;take over&quot; and govern the entire space.  The AIs can
<br>
balance each other out.  So it is not &quot;regardless of what a friend AI
<br>
thinks&quot;.   I have no problem at all with the idea of teaching the AIs 
<br>
ahout Friendliness or the path of the Boddhisattwa or however else you
<br>
might wish to express it.  I have a large worry with the idea of their
<br>
only being one of them and with it perhaps having too limited a notion
<br>
of what &quot;Friendliness&quot; entails.
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt; So what *do* you intend that you can't do with a Sysop?  No tautological
</em><br>
<em>&gt; answers like &quot;Build something outside the Sysop&quot;; name some specific fun
</em><br>
<em>&gt; or meaningful thing that you should, morally, be able to do, but which the
</em><br>
<em>&gt; Sysop won't let you do.
</em><br>
<em>&gt; 
</em><br>
<p>Sorry, it is a pretty tautological or abstract (and yet not unimportant
<br>
at all) question.  The question is why I should allow myself to be
<br>
limited by your notion of a Sysop.  And why we should believe this is a
<br>
really good state or the best solution.  If we decide it is not a good
<br>
solution but the Sysop disagrees, then what?  Looking for a specific
<br>
example of the Sysop being in the way is beside the point.  
<br>
<p>I can see in theory how such a being could not be in the way but I think
<br>
my notion of that is a bit different than yours.  
<br>
<p><em>&gt; &gt;
</em><br>
<em>&gt; &gt; Again, the Sysop abrogates all decisions and all wisdom to itself.  How
</em><br>
<em>&gt; &gt; about upgrading its uploads to their own ever-increasing wisdom.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 15: Sysop:  &quot;OK.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; &gt; Under absolutely none of these circumstances does the Sysop need to strike
</em><br>
<em>&gt; &gt; &gt; back at you.  Ve just gives you an API error.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Err.  Already assumes precisely my point.  This being is effectively
</em><br>
<em>&gt; &gt; God.  You exist only within it and as it allows.  Are you really willing
</em><br>
<em>&gt; &gt; to take on the building of such?  Are you so convinced it is the Only
</em><br>
<em>&gt; &gt; Answer?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes, and yes.  The risks inherent in material omnipotence are inherent in
</em><br>
<em>&gt; rapid transcendence and thus inherent in AI.  The Sysop Scenario adds
</em><br>
<em>&gt; nothing to that.
</em><br>
<em>&gt; 
</em><br>
<p>However, your solution is to make one AI and SI with the right
<br>
moral/ethical grounding to have this power without running amok.  What
<br>
of the other billions of beings?  Is there an evolutionary path for them
<br>
up to and beyond this level of transcendence (assuming they wish it)? 
<br>
What of other beings reaching full trancendence and having to learn
<br>
wisdom and morality along the way?  Is there room enough for them to do
<br>
so?
<br>
<p><p><em>&gt; And, incidentally, the Sysop Scenario isn't what I'm telling a Friendly
</em><br>
<em>&gt; seed AI to do.  It's what I think a Friendly superintelligence will decide
</em><br>
<em>&gt; to do.  I'm neither taking on the building of such, nor need to be
</em><br>
<em>&gt; convinced that it's the Only Answer.
</em><br>
<em>&gt; 
</em><br>
<p>OK.  I don't think it will decide quite along the Sysop lines, but you
<br>
could well be right.
<br>
<p><em>&gt; &gt;
</em><br>
<em>&gt; &gt; But this Sysop can't be built by your earlier response except totally
</em><br>
<em>&gt; &gt; within the Sysop so in no real sense is it independent.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; No, I'm pointing out a possible variation on my earlier response (albeit
</em><br>
<em>&gt; one that I personally think improbable), under which it's possible to
</em><br>
<em>&gt; construct an independent Sysop as long as it's an independent Friendly
</em><br>
<em>&gt; Sysop.
</em><br>
<p><p>OK.  As long as the first Sysop doesn't insist Friendliness is only
<br>
compatible with roughly its own solutions to the very complex questions
<br>
involved.  My intuition is that there are many possible solution spaces
<br>
that cannot all be explored  by any one SI.  Some of them may not even
<br>
seem all that &quot;Friendly&quot; from other particular solution spaces.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; I am concerned
</em><br>
<em>&gt; &gt; by the phrase &quot;static uploads&quot;.  Do you mean by this that uploads cannot
</em><br>
<em>&gt; &gt; grow indefinitely in capability?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; No, I mean modern-day humans who choose to upload but not to upgrade.
</em><br>
<em>&gt; 
</em><br>
<p>I guess I have a hard time expecting many people to do this.  Or at
<br>
least it is doubtful that they wouldn't choose to upgrade pretty soon. 
<br>
So what is the significance of &quot;static&quot;.  I think I am missing something
<br>
there.
<br>
<p><p><em>&gt; &gt;
</em><br>
<em>&gt; &gt; Let's see.  The SysOp is a super-intelligence.  Therefore it has its own
</em><br>
<em>&gt; &gt; agenda and interests.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; NON SEQUITUR
</em><br>
<em>&gt; 
</em><br>
<p>How so?  
<br>
<p><em>&gt; &gt; It controls all aspects of material reality and
</em><br>
<em>&gt; &gt; all virtual ones that we have access to.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; This is a good deal more than
</em><br>
<em>&gt; &gt; just an operating system.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Why?  The laws of physics control all aspects of material reality too.
</em><br>
<p>The laws of physics are not part of or at the bidding of a conscious
<br>
super-intelligent entity as far as we know.  This is a large difference.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; What precisely constitutes harm of another
</em><br>
<em>&gt; &gt; citizen to the Sysop?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Each citizen would define the way in which other entities can interact
</em><br>
<em>&gt; with matter and computronium which that citizen owns.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; For entities in a VR who are playing with
</em><br>
<em>&gt; &gt; designer universes of simulated beings they experience from inside, is
</em><br>
<em>&gt; &gt; it really harm that in this universe these simulated beings maim and
</em><br>
<em>&gt; &gt; kill one another?  In other words, does the SysOp prevent real harm or
</em><br>
<em>&gt; &gt; all appearance of harm?  What is and isn't real needs answering also,
</em><br>
<em>&gt; &gt; obviously.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't see how this moral issue is created by the Sysop Scenario.  It's
</em><br>
<em>&gt; something that we need to decide, as a fundamental moral issue, no matter
</em><br>
<em>&gt; which future we walk into.
</em><br>
<em>&gt; 
</em><br>
<p>OK, but I am exploring what you think the Sysop answer is or should be
<br>
to be compatible with Friendliness.  I have a suspicion that it is not
<br>
possible for many types of being to evolve without at least being under
<br>
the impression that they can harm and be harmed.  It would be nice if I
<br>
was wrong about that but I don't think I am.  If it is necessary in
<br>
certain types of being-spaces, then Friendliness would entail soemthing
<br>
that doesn't particularly look friendly. 
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt; &gt; &gt; Of course not.  You could be right and I could be wrong, in which case -
</em><br>
<em>&gt; &gt; &gt; if I've built well - the Sysop will do something else, or the seed AI will
</em><br>
<em>&gt; &gt; &gt; do something other than become Sysop.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; OK.  If it is not the Sysop what are some of the alternate scenarios
</em><br>
<em>&gt; &gt; that you could see occurring that are desirable outcomes?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1)  It turns out that humanity's destiny is to have an overall GroupMind
</em><br>
<em>&gt; that runs the Solar System.  The Sysop creates the infrastructure for the
</em><br>
<em>&gt; GroupMind, invites everyone in who wants in, transfers control of API
</em><br>
<em>&gt; functions to the GroupMind's volition, and either terminates verself or
</em><br>
<em>&gt; joins the GroupMind.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 2)  Preventing citizens from torturing one another doesn't require
</em><br>
<em>&gt; continuous enforcement by a sentient entity; the Sysop invokes some kind
</em><br>
<em>&gt; of ontotechnological Word of Command that rules out the negative set of
</em><br>
<em>&gt; possibilities, then terminates verself, or sticks around being helpful
</em><br>
<em>&gt; until more SIs show up.
</em><br>
<em>&gt; 
</em><br>
<p>Both of these scenarios miss a possibility that I think is crucial. 
<br>
Which is that individual beings in all their variety have to evolve
<br>
their own solution without some super-being (in their context) solving
<br>
the problem for them.  From past interactions I know you disown this as
<br>
a likely scenario.  But I think it might be the only one that has the
<br>
beings fully impelled and able to grow up. 
<br>
<p><em>&gt; &gt; &gt; Yes.  I think that, if the annoyance resulting from pervasive forbiddance
</em><br>
<em>&gt; &gt; &gt; is a necessary subgoal of ruling out the space of possibilities in which
</em><br>
<em>&gt; &gt; &gt; citizenship rights are violated, then it's an acceptable tradeoff.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; If the citizens have no choice then there is no morality.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; That sounds to me like one more variation on &quot;It's the struggle that's
</em><br>
<em>&gt; important, not the goal.&quot;  What's desirable is that people not hurt one
</em><br>
<em>&gt; another.  It's also desirable that they not choose to hurt one another,
</em><br>
<em>&gt; but that's totally orthagonal to the first point.
</em><br>
<em>&gt; 
</em><br>
<p>If I cannot choose to be hurtful then I cannot choose not to be. I have
<br>
no chose but to be harmless.  I did not grow into choosing wisely but
<br>
was chosen for by something Other.  I am thus a very different kind of
<br>
being than one that grew by learning to choose wisely.  People not
<br>
hurting one other does not trump people learning not to hurt each other
<br>
and why it is important.  We could lock everyone in strait-jackets
<br>
(physical, mental or chemical) and metaphorically feed them
<br>
intravenously and accomplish them not hurting one another.
<br>
<p><em>&gt; You can still become a better person, as measured by what you'd do if the
</em><br>
<em>&gt; Sysop suddenly vanished.
</em><br>
<em>&gt; 
</em><br>
<p>But will you ever know unless it does?
<br>
<p><em>&gt; Are we less moral because we live in a society with police officers?
</em><br>
<em>&gt; Would we suddenly become more moral if all law enforcement and all social
</em><br>
<em>&gt; disapprobation and all other consequences of murder suddenly vanished?
</em><br>
<em>&gt; 
</em><br>
<p>Not at all a good analogy.  I am not talking about consequences
<br>
disappearing. I am talking about freedom to choose and to face
<br>
consequences remaining.
<br>
<p><p><em>&gt; &gt;
</em><br>
<em>&gt; &gt; The Sysop is refusing to let me out of Sysop space.  Truthfully we have
</em><br>
<em>&gt; &gt; no idea how various sentiences will react to being in Sysop space no
</em><br>
<em>&gt; &gt; matter how benign you think it is.  Your hypothetical space where I
</em><br>
<em>&gt; &gt; torture sentients is an utter strawman.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Is it still a strawman scenario when integrated over the six billion
</em><br>
<em>&gt; current residents of Earth?  Or is only Samantha allowed to go Outside?
</em><br>
<em>&gt; 
</em><br>
<p>No.  Others can go outside who may have more nefarious motives.  Are you
<br>
claiming they would never tire of being bloody tyrants, never feel
<br>
remorse, never seek to undo some part of the painful ugly creatin they
<br>
made?  Without experiencing the consequences, how do beings actually
<br>
learn these things?  
<br>
<p><em>&gt; The Friendly seed AI turned Friendly superintelligence makes the final
</em><br>
<em>&gt; decision, and ve *does* have an idea of how various sentiences will
</em><br>
<em>&gt; react.  If the Sysop scenario really results in more summated misery than
</em><br>
<em>&gt; letting every Hitler have vis own planet, or if there's some brilliant
</em><br>
<em>&gt; third alternative, then the Sysop scenario will undoubtedly be quietly
</em><br>
<em>&gt; ditched.
</em><br>
<p>Sure.  Make a space (probably VR) where entities can do whatever they
<br>
wish including making their own VR spaces controlled by theselves which
<br>
are as miserable or wonderful as they wish and as their skill and wisdom
<br>
allows.  Keep the Sysop as an entity that insures all conscious beings
<br>
created or who become involved come to no permanent or irreparable
<br>
harm.  Otherwise they are free to be as good or horrible to one another
<br>
as they wish.  And they are free to not know that they can come to no
<br>
irreparable harm or cause any.  Would this be compatible?
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0542.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>Previous message:</strong> <a href="0540.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>In reply to:</strong> <a href="0539.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0542.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>Reply:</strong> <a href="0542.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#541">[ date ]</a>
<a href="index.html#541">[ thread ]</a>
<a href="subject.html#541">[ subject ]</a>
<a href="author.html#541">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
