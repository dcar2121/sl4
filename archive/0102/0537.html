<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Beyond evolution</title>
<meta name="Author" content="Ben Goertzel (ben@webmind.com)">
<meta name="Subject" content="RE: Beyond evolution">
<meta name="Date" content="2001-02-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Beyond evolution</h1>
<!-- received="Sun Feb 04 22:29:20 2001" -->
<!-- isoreceived="20010205052920" -->
<!-- sent="Sun, 4 Feb 2001 22:24:45 -0500" -->
<!-- isosent="20010205032445" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@webmind.com" -->
<!-- subject="RE: Beyond evolution" -->
<!-- id="JBEPKOGDDIKKAHFPOEFIEEPACEAA.ben@webmind.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3A7E0BFF.325FDD19@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@webmind.com?Subject=RE:%20Beyond%20evolution"><em>ben@webmind.com</em></a>)<br>
<strong>Date:</strong> Sun Feb 04 2001 - 20:24:45 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0538.html">Samantha Atkins: "Re: Beyond evolution"</a>
<li><strong>Previous message:</strong> <a href="0536.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>In reply to:</strong> <a href="0536.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0577.html">jpp22: "Re: Beyond evolution (late reply...)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#537">[ date ]</a>
<a href="index.html#537">[ thread ]</a>
<a href="subject.html#537">[ subject ]</a>
<a href="author.html#537">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Typically, one distinguishes
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-- evolution, which occurs in populations of entities
<br>
<p>from
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-- adaptation, which occurs in a single entity
<br>
<p>It seems like what Christian is talking about is really adaptation, not
<br>
evolution.
<br>
<p>There is a large CS literature on adaptive learning, which is powerful,
<br>
often competitive
<br>
with evolutionary learning.
<br>
<p>Of course, adaptive learning, in general, also has the potential to go awry.
<br>
(Just like evolution.)
<br>
<p>Eliezer's claim is that self-modification of a superhumanly intelligent
<br>
system is a special kind of
<br>
adaptive learning that's very unlikely to go seriously awry, if it's
<br>
initiated in the right way.
<br>
He has not proved this precisely, but, his argument seem plausible to me.
<br>
<p>(My only remaining major disagreement with him has to do with whether
<br>
superhumanly intelligent systems
<br>
can be expected to retain a significant interest in humans or not....  But I
<br>
have not happened on any
<br>
really scientific way to resolve this at the moment.)
<br>
<p>ben
<br>
<p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20Beyond%20evolution">owner-sl4@sysopmind.com</a> [mailto:<a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20Beyond%20evolution">owner-sl4@sysopmind.com</a>]On Behalf
</em><br>
<em>&gt; Of Eliezer S. Yudkowsky
</em><br>
<em>&gt; Sent: Sunday, February 04, 2001 9:12 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sysopmind.com?Subject=RE:%20Beyond%20evolution">sl4@sysopmind.com</a>
</em><br>
<em>&gt; Subject: Re: Beyond evolution
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Discussing things at this level of abstraction is pointless.  If you
</em><br>
<em>&gt; believe you've found a specific selection pressure that will necessarily
</em><br>
<em>&gt; produce specific behaviors in a singleton seed AI that undergoes
</em><br>
<em>&gt; successive rounds of self-modification, then say so, describe why, and
</em><br>
<em>&gt; explain how you are extending the phrase &quot;selection pressure&quot; to usefully
</em><br>
<em>&gt; apply in the absence of a population of replicators.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Be concrete.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Christian Weisgerber wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; You can't outrun evolution.  Not in this universe at least.
</em><br>
<em>&gt;
</em><br>
<em>&gt; How would you know?  Humanity is a very, very young species.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; It is a very fundamental principle, more fundamental than physical law.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Evolution certainly is more fundamental than our physical laws, since you
</em><br>
<em>&gt; can get evolution under a variety of physical-law scenarios; however, all
</em><br>
<em>&gt; we know about evolution is that we find a lot of it in the absence of
</em><br>
<em>&gt; control by intelligence.  This is not enough to make deductions about a
</em><br>
<em>&gt; technological world.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; - A population of replicators.
</em><br>
<em>&gt; &gt; - Mutation.
</em><br>
<em>&gt; &gt;   (In its widest meaning, i.e. some change to the replicators.)
</em><br>
<em>&gt; &gt; - A fitness function.
</em><br>
<em>&gt; &gt; - Limited survivability, typically by resource limitation.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; This may look like a lot of conditions, but good luck trying to
</em><br>
<em>&gt; &gt; find circumstances where they don't apply.
</em><br>
<em>&gt;
</em><br>
<em>&gt; No population of replicators.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; All you are suggesting with your &quot;casting aside evolution&quot; is a
</em><br>
<em>&gt; &gt; replacement of the mutation mechanism, from random change to
</em><br>
<em>&gt; &gt; engineered change by the replicators themselves.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If mutations that lead to undesirable effects are deliberately excluded by
</em><br>
<em>&gt; an intelligent mutation mechanism, then selection pressures for that
</em><br>
<em>&gt; behavior are irrelevant - or, more accurately, the apparent selection
</em><br>
<em>&gt; pressures do not exist, since the fitness metric includes &quot;survivability&quot;
</em><br>
<em>&gt; under the observing mutation mechanism, as well as any trials in the
</em><br>
<em>&gt; external world.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is all we care about from the Friendly AI perspective, so whether or
</em><br>
<em>&gt; not you can really call it &quot;evolution&quot; is wholly irrelevant.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; This may affect
</em><br>
<em>&gt; &gt; the mutation speed, but it doesn't change one iota about the
</em><br>
<em>&gt; &gt; applicability of the principle of evolution.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Evolution has certain characteristics by which it can be recognized.  If
</em><br>
<em>&gt; designed things look absolutely nothing like it, then it doesn't matter
</em><br>
<em>&gt; whether or not you've expanded &quot;evolution&quot; to describe everything in the
</em><br>
<em>&gt; Universe; it just means that you've expanded &quot;evolution&quot; to the point
</em><br>
<em>&gt; where it becomes useless.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Evolution describes a specific subset of the Universe.  It describes
</em><br>
<em>&gt; bacteria and people, but not stars (random) or surge protectors
</em><br>
<em>&gt; (designed).  Calling something &quot;evolved&quot; is a useful statement, especially
</em><br>
<em>&gt; in Friendly AI, because it enables us to predict certain characteristics
</em><br>
<em>&gt; that appear in evolved things but not random things or designed things.
</em><br>
<em>&gt; If you mutate the term beyond its fitness, it will die.
</em><br>
<em>&gt;
</em><br>
<em>&gt; --              --              --              --              --
</em><br>
<em>&gt; Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt; Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0538.html">Samantha Atkins: "Re: Beyond evolution"</a>
<li><strong>Previous message:</strong> <a href="0536.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>In reply to:</strong> <a href="0536.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0577.html">jpp22: "Re: Beyond evolution (late reply...)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#537">[ date ]</a>
<a href="index.html#537">[ thread ]</a>
<a href="subject.html#537">[ subject ]</a>
<a href="author.html#537">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
