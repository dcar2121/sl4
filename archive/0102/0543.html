<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Beyond evolution</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Beyond evolution">
<meta name="Date" content="2001-02-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Beyond evolution</h1>
<!-- received="Mon Feb 05 16:35:33 2001" -->
<!-- isoreceived="20010205233533" -->
<!-- sent="Mon, 05 Feb 2001 13:32:56 -0800" -->
<!-- isosent="20010205213256" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Beyond evolution" -->
<!-- id="3A7F1C08.63002E6D@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3A7EE980.EC93EDF0@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Beyond%20evolution"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Mon Feb 05 2001 - 14:32:56 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0544.html">Gordon Worley: "Learning to be evil"</a>
<li><strong>Previous message:</strong> <a href="0542.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>In reply to:</strong> <a href="0542.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0546.html">Durant Schoon: "Re: Beyond evolution"</a>
<li><strong>Reply:</strong> <a href="0546.html">Durant Schoon: "Re: Beyond evolution"</a>
<li><strong>Reply:</strong> <a href="0547.html">Durant Schoon: "Re: Beyond evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#543">[ date ]</a>
<a href="index.html#543">[ thread ]</a>
<a href="subject.html#543">[ subject ]</a>
<a href="author.html#543">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&quot;Eliezer S. Yudkowsky&quot; wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Humans are a special case on at least two counts:  First, we exist in the
</em><br>
<em>&gt; sort of intermediate technological society where, right up until fifty
</em><br>
<em>&gt; years ago, it was *technically impossible* to run completely amok and
</em><br>
<em>&gt; destroy the world, and even now, the checks and balances work fairly
</em><br>
<em>&gt; well.  Second, we're evolved entities who usually aren't trustworthy, or
</em><br>
<em>&gt; knowably trustworthy at any rate, in the absence of checks and balances.
</em><br>
<em>&gt; 
</em><br>
<p>Can any sort of entity capable of change over time be absolutely
<br>
trustworthy by such a criteria?  
<br>
<p><em>&gt; &gt; A single
</em><br>
<em>&gt; &gt; sysop is missing that sort of checks and balances.  The assumption is
</em><br>
<em>&gt; &gt; that we can design it so well that it will automatically check and
</em><br>
<em>&gt; &gt; balance.  I confess to having a lot of doubt that this can be done.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Humans in general are balanced: balanced against each other; balanced in
</em><br>
<em>&gt; terms of internal cognitive ability levels; balanced between nature and
</em><br>
<em>&gt; nurture; part of one big Gaussian curve.  It doesn't apply to the
</em><br>
<em>&gt; transhuman spaces.
</em><br>
<em>&gt; 
</em><br>
<p>Why not?  I do not see why transhumans could not also balance one
<br>
another's excesses to some degree. 
<br>
<p><em>&gt; &gt;
</em><br>
<em>&gt; &gt; But a Sysop does &quot;take over&quot; and govern the entire space.  The AIs can
</em><br>
<em>&gt; &gt; balance each other out.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I really don't think so.  First AI to transcend moves into accelerated
</em><br>
<em>&gt; subjective time and wins all the marbles.  Unless ve decides not to, in
</em><br>
<em>&gt; which case you have the &quot;flipping through the deck&quot; problem.
</em><br>
<em>&gt; 
</em><br>
<p>What is this &quot;flipping through the deck&quot; problem?  That we will turn up
<br>
a joker now and then?  Then having at least one transcendent AI who is
<br>
reasonably trustworthy already in place provides a necessary balance. 
<br>
But there are pluses as well as potential minuses to allowing more than
<br>
one.
<br>
<p><em>&gt; &gt; I have a large worry with the idea of their
</em><br>
<em>&gt; &gt; only being one of them and with it perhaps having too limited a notion
</em><br>
<em>&gt; &gt; of what &quot;Friendliness&quot; entails.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Having too limited a notion?  Sounds unFriendly to me.
</em><br>
<em>&gt; 
</em><br>
<p>There are possible solutions to the Friendliness problem of what being
<br>
Friendly to lesser beings does and does not entail that would not be at
<br>
all pleasant for said lesser beings.  If there is only one SI class
<br>
intelligence solving the problem it is possible that one of these less
<br>
happy Friendly solutions would be the only game in town.
<br>
<p><em>&gt; &gt; The question is why I should allow myself to be
</em><br>
<em>&gt; &gt; limited by your notion of a Sysop.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Mine?  I may not be able to shove off all responsibility onto the
</em><br>
<em>&gt; shoulders of an SI but I sure intend to try.  Letting Samantha Atkins be
</em><br>
<em>&gt; limited by &quot;Eliezer Yudkowsky's notion&quot; (that is, a notion which is unique
</em><br>
<em>&gt; to Eliezer Yudkowsky and not an inevitable consequence of panhuman ethics)
</em><br>
<em>&gt; sounds unFriendly to me.
</em><br>
<em>&gt; 
</em><br>
<p>I thought you had come to question objective ethics.  If you are the
<br>
primary designer of this SI's goal system then your notion of what such
<br>
a system should be will strongly determine what this SI will come up
<br>
with or at least what type of problem it is attempting to solve.  But I
<br>
am being side-tracked from the point I was attempting to make.  Which is
<br>
simply by what right should you or I or some SI of the future force all
<br>
beings to subject their freedom to our grand design?
<br>
<p><em>&gt; &gt; If we decide it is not a good
</em><br>
<em>&gt; &gt; solution but the Sysop disagrees, then what?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Then either &quot;we&quot; (who's &quot;we&quot;?) are mistaken, or somebody (me) really
</em><br>
<em>&gt; screwed up the definition of Friendliness.
</em><br>
<em>&gt; 
</em><br>
<p>But blame would not be the point.  The point is that with a single SI
<br>
scenario there is no counter-balance and we are simply and utterly
<br>
stuck.
<br>
<p><em>&gt; &gt; I can see in theory how such a being could not be in the way but I think
</em><br>
<em>&gt; &gt; my notion of that is a bit different than yours.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, I've yet to hear a concrete proposal that would result in less
</em><br>
<em>&gt; summated suffering than a Sysop Scenario - under *any* definition of
</em><br>
<em>&gt; morality.
</em><br>
<em>&gt; 
</em><br>
<p><p>Assuming &quot;summated suffering&quot; is even a valid criteria, suffering might
<br>
be most minimized over time by the continued growth in understanding and
<br>
ability of the beings suffering.  Thus an SI tasks with reducing
<br>
suffering would have the subgoal of maximizing the growth of the beings
<br>
it is to help.  But short term suffering may be essential for these
<br>
beings to grasp the consequences of certain lines of development.  Thus
<br>
a strategy of great freedom but with an invisible safety net (having
<br>
backups of beings for instance) might be much more Friendly than simply
<br>
throwing an API error whenever anything at all untoward was attempted by
<br>
any being. 
<br>
<p><em>&gt; &gt; &gt; Yes, and yes.  The risks inherent in material omnipotence are inherent in
</em><br>
<em>&gt; &gt; &gt; rapid transcendence and thus inherent in AI.  The Sysop Scenario adds
</em><br>
<em>&gt; &gt; &gt; nothing to that.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; However, your solution is to make one AI and SI with the right
</em><br>
<em>&gt; &gt; moral/ethical grounding to have this power without running amok.  What
</em><br>
<em>&gt; &gt; of the other billions of beings?  Is there an evolutionary path for them
</em><br>
<em>&gt; &gt; up to and beyond this level of transcendence (assuming they wish it)?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes!  But strike &quot;evolutionary&quot; from the record, please.
</em><br>
<em>&gt; 
</em><br>
<p>Why?  Simple preference?  WOuld you prefer adaptation?  Growth?  what?
<br>
<p><em>&gt; &gt; What of other beings reaching full trancendence and having to learn
</em><br>
<em>&gt; &gt; wisdom and morality along the way?  Is there room enough for them to do
</em><br>
<em>&gt; &gt; so?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Sure!
</em><br>
<em>&gt; 
</em><br>
<p>I don't see how if the sysop prohibits all actions that do not appear
<br>
friendly.  How would a being learn the suffering inherent in such
<br>
actions if it cannot perform them and experience the consequences?
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; OK.  As long as the first Sysop doesn't insist Friendliness is only
</em><br>
<em>&gt; &gt; compatible with roughly its own solutions to the very complex questions
</em><br>
<em>&gt; &gt; involved.  My intuition is that there are many possible solution spaces
</em><br>
<em>&gt; &gt; that cannot all be explored  by any one SI.  Some of them may not even
</em><br>
<em>&gt; &gt; seem all that &quot;Friendly&quot; from other particular solution spaces.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Which parts of &quot;Friendliness&quot; are more and less arbitrary is itself part
</em><br>
<em>&gt; of the understanding that constitutes a Friendship system.  Any
</em><br>
<em>&gt; sufficiently arbitrary answer shouldn't be part of Friendliness at all; it
</em><br>
<em>&gt; should probably just be delegated back to the volitional decisions of
</em><br>
<em>&gt; individuals, or at least be overridable by the decisions of individuals.
</em><br>
<em>&gt; Even the primacy of pleasure over pain is subject to the volitional
</em><br>
<em>&gt; override of static masochists.
</em><br>
<em>&gt; 
</em><br>
<p>OK, thanks.
<br>
<p>&nbsp;&nbsp;
<br>
<em>&gt; &gt; I guess I have a hard time expecting many people to do this.  Or at
</em><br>
<em>&gt; &gt; least it is doubtful that they wouldn't choose to upgrade pretty soon.
</em><br>
<em>&gt; &gt; So what is the significance of &quot;static&quot;.  I think I am missing something
</em><br>
<em>&gt; &gt; there.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The significance of &quot;static&quot; is that it's the only part of the Universe
</em><br>
<em>&gt; about which we can have meaningful discussions.
</em><br>
<em>&gt; 
</em><br>
<p>I am not so sure of that but OK. 
<br>
<p><em>&gt; &gt; OK, but I am exploring what you think the Sysop answer is or should be
</em><br>
<em>&gt; &gt; to be compatible with Friendliness.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Will you take &quot;I don't know, I'll ask the Sysop&quot; as a legitimate answer
</em><br>
<em>&gt; here?
</em><br>
<em>&gt; 
</em><br>
<p>Yes and no.  I am as yet unclear on how much the Sysop answer is
<br>
determined by the input of its designers, programmers and early
<br>
trainers.  So your own answer is interesting and possibly crucial to
<br>
evaluating the likely Friendliness of the Sysop you are working on.
<br>
<p><em>&gt; &gt; &gt; You can still become a better person, as measured by what you'd do if the
</em><br>
<em>&gt; &gt; &gt; Sysop suddenly vanished.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; But will you ever know unless it does?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Sure; have your exoself run a predictive scan on your simulated cortex.
</em><br>
<em>&gt; Minds are real in themselves and can be understood in themselves; the
</em><br>
<em>&gt; external reality is the expression of it, not the test.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; No.  Others can go outside who may have more nefarious motives.  Are you
</em><br>
<em>&gt; &gt; claiming they would never tire of being bloody tyrants, never feel
</em><br>
<em>&gt; &gt; remorse, never seek to undo some part of the painful ugly creation they
</em><br>
<em>&gt; &gt; made?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Some would, some wouldn't.
</em><br>
<em>&gt; 
</em><br>
<p>I think that every one of them would if given enough time to get
<br>
thoroughly bored and disgusted.  And that learning of what is not
<br>
worthwhile to do and what it leads to is, I submit, quite important.  If
<br>
some beings do not learn but stay in a loop of their own making forever,
<br>
that is a small price to pay for freedom in my view. 
<br>
<p><em>&gt; &gt; Without experiencing the consequences, how do beings actually
</em><br>
<em>&gt; &gt; learn these things?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think that maybe one of the underlying disagreements is that we disagree
</em><br>
<em>&gt; on how much &quot;real experience&quot; is necessary.  My own position is that the
</em><br>
<em>&gt; human brain has two settings:  &quot;Sympathize, using all available hardwired
</em><br>
<em>&gt; neurons,&quot; and &quot;Project abstractly, using high-level thoughts.&quot;  For us,
</em><br>
<em>&gt; there's a very sharp border between really experiencing something and
</em><br>
<em>&gt; thinking about it abstractly, because we can't do enough abstract thought
</em><br>
<em>&gt; to simulate all the pixels in a visual cortex.  For us, the behaviors that
</em><br>
<em>&gt; we abstractly imagine on hearing the phrase &quot;four-dimensional visual
</em><br>
<em>&gt; cortex&quot; will never be as sharp, as real, as the experiences of an entity
</em><br>
<em>&gt; with a true 4D visual cortex.  But this is a distinction that breaks down
</em><br>
<em>&gt; for self-modifying entities, like seed AIs or transcendent humans, who can
</em><br>
<em>&gt; abstractly think about every pixel and feature extractor in a 4D visual
</em><br>
<em>&gt; cortex, and thus understand every facet of intuition and behavior that
</em><br>
<em>&gt; would be exhibited by a being with a true 4D visual cortex, even if he or
</em><br>
<em>&gt; she or ve retains their original 3D visual cortex the whole while.  A seed
</em><br>
<em>&gt; AI or a transcendent human with a 3D cortex can look at a 4D Escher
</em><br>
<em>&gt; painting and understand it by virtue of their ability to understand a 4D
</em><br>
<em>&gt; cortex.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So, without experiencing the consequences, beings learn by using their
</em><br>
<em>&gt; very vivid imaginations.
</em><br>
<em>&gt; 
</em><br>
<p>A truly vivid imagination is an actual experience in some space in such
<br>
a manner that one will survive the experience (although perhaps not from
<br>
the point of view held within the space) and carry memories and
<br>
therefore learning forward (perhaps subject to volition).  Without
<br>
experiencing the consequences the imagining is seriously flawed and
<br>
teaches much less. It is not truly interactive unless the imagined
<br>
reality &quot;pushes back&quot;.  This does not mean the experience has to be
<br>
real-time/space fatal to be effective as a learning device. 
<br>
<p><em>&gt; &gt; Sure.  Make a space (probably VR) where entities can do whatever they
</em><br>
<em>&gt; &gt; wish including making their own VR spaces controlled by theselves which
</em><br>
<em>&gt; &gt; are as miserable or wonderful as they wish and as their skill and wisdom
</em><br>
<em>&gt; &gt; allows.  Keep the Sysop as an entity that insures all conscious beings
</em><br>
<em>&gt; &gt; created or who become involved come to no permanent or irreparable
</em><br>
<em>&gt; &gt; harm.  Otherwise they are free to be as good or horrible to one another
</em><br>
<em>&gt; &gt; as they wish.  And they are free to not know that they can come to no
</em><br>
<em>&gt; &gt; irreparable harm or cause any.  Would this be compatible?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; OK, but it sounds like you're talking an &quot;unescapable&quot; Sysop, which I
</em><br>
<em>&gt; really thought was your whole point in the first place.  I mean, if I
</em><br>
<em>&gt; understand this scenario correctly, I can't go Outside for fear that I'll
</em><br>
<em>&gt; bring an entity to permanent or irreparable harm.
</em><br>
<p>It is unescapable but so lightly involved in the apparent Universe that
<br>
most beings experience as to not be a limitation.  Which means we are
<br>
not suggesting that much that is different from each other.  
<br>
Interesting.  Thank you very much for the conversation.  I have learned
<br>
from it.
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0544.html">Gordon Worley: "Learning to be evil"</a>
<li><strong>Previous message:</strong> <a href="0542.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>In reply to:</strong> <a href="0542.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0546.html">Durant Schoon: "Re: Beyond evolution"</a>
<li><strong>Reply:</strong> <a href="0546.html">Durant Schoon: "Re: Beyond evolution"</a>
<li><strong>Reply:</strong> <a href="0547.html">Durant Schoon: "Re: Beyond evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#543">[ date ]</a>
<a href="index.html#543">[ thread ]</a>
<a href="subject.html#543">[ subject ]</a>
<a href="author.html#543">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
