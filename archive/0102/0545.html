<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Learning to be evil</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Learning to be evil">
<meta name="Date" content="2001-02-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Learning to be evil</h1>
<!-- received="Wed Feb 07 23:45:55 2001" -->
<!-- isoreceived="20010208064555" -->
<!-- sent="Wed, 07 Feb 2001 23:09:16 -0500" -->
<!-- isosent="20010208040916" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Learning to be evil" -->
<!-- id="3A821BEC.B29E3DFE@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="f05001900b6a7805c96cc@[10.0.1.3]" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Learning%20to%20be%20evil"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Feb 07 2001 - 21:09:16 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0546.html">Durant Schoon: "Re: Beyond evolution"</a>
<li><strong>Previous message:</strong> <a href="0544.html">Gordon Worley: "Learning to be evil"</a>
<li><strong>In reply to:</strong> <a href="0544.html">Gordon Worley: "Learning to be evil"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0551.html">Gordon Worley: "Re: Learning to be evil"</a>
<li><strong>Reply:</strong> <a href="0551.html">Gordon Worley: "Re: Learning to be evil"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#545">[ date ]</a>
<a href="index.html#545">[ thread ]</a>
<a href="subject.html#545">[ subject ]</a>
<a href="author.html#545">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Gordon Worley wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; An assumption that many seemed to be making in the recent discussion
</em><br>
<em>&gt; was that there will inevitable be evil AIs, thus the need for
</em><br>
<em>&gt; Friendliness and a sysop (or not, depending on whom you ask) to make
</em><br>
<em>&gt; sure that the AIs don't do anything evil.  I'm wondering if they
</em><br>
<em>&gt; would ever become evil in the first place?
</em><br>
<p>It's a free Universe.  Everyone has the right to design evil AIs, unless
<br>
that is found to constitute child abuse or something.  Certainly people
<br>
will have the right to become extremely powerful and intelligent and,
<br>
unless objective morality steps in, extremely evil, so yes, there may well
<br>
be ultrapowered incredibly evil entities in the Universe, and, thanks to
<br>
the Sysop paradigm, Amishfolk on Earth going *thbbpt* at them.
<br>
<p>*Unless* altruism is a convergent subgoal powerful enough to suck in any
<br>
sufficiently intelligent entity, regardless of how programmed, which is a
<br>
pleasant and significant possibility but *not* one that I'm currently
<br>
relying on.
<br>
<p><em>&gt; The consequences of evil
</em><br>
<em>&gt; actions always come back on the evil doer, having a negative effect
</em><br>
<em>&gt; on them.
</em><br>
<p>A romantic and rather impractical view.  Sometimes the consequences of
<br>
evil come back on the evildoer, sometimes they don't.  Highly competent
<br>
evildoers have gone on to die in bed, surrounded by many loving, newly
<br>
wealthy great-grandchildren, and somewhere along the line, you've got
<br>
their genes.
<br>
<p><em>&gt; To that extent, Friendliness seems to me like an
</em><br>
<em>&gt; inherent trait in SIs, since unlike humans they will be smart enough
</em><br>
<em>&gt; to consider all of the consequences.
</em><br>
<p>And if the SI is the only one around, and powerful enough that there are
<br>
no consequences?
<br>
<p>Anyhoo, Friendliness isn't intended to suppress evil impulses.  AIs don't
<br>
have 'em unless you put them there.  Correspondingly, although other
<br>
possibilities exist, the default, engineering-*conservative* assumption is
<br>
that goodness also doesn't materialize in source code.
<br>
<p>Friendliness is a means whereby a genuinely, willingly altruistic
<br>
programming team transmits genuine, willing altruism to an AI, packaged to
<br>
avoid damage in transmission, and infused in such form as to eventually
<br>
become independent of the original programmers.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0546.html">Durant Schoon: "Re: Beyond evolution"</a>
<li><strong>Previous message:</strong> <a href="0544.html">Gordon Worley: "Learning to be evil"</a>
<li><strong>In reply to:</strong> <a href="0544.html">Gordon Worley: "Learning to be evil"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0551.html">Gordon Worley: "Re: Learning to be evil"</a>
<li><strong>Reply:</strong> <a href="0551.html">Gordon Worley: "Re: Learning to be evil"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#545">[ date ]</a>
<a href="index.html#545">[ thread ]</a>
<a href="subject.html#545">[ subject ]</a>
<a href="author.html#545">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
