<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Six theses on superintelligence</title>
<meta name="Author" content="Anders Sandberg (asa@nada.kth.se)">
<meta name="Subject" content="Re: Six theses on superintelligence">
<meta name="Date" content="2001-02-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Six theses on superintelligence</h1>
<!-- received="Sat Feb 10 16:29:01 2001" -->
<!-- isoreceived="20010210232901" -->
<!-- sent="10 Feb 2001 22:26:08 +0100" -->
<!-- isosent="20010210212608" -->
<!-- name="Anders Sandberg" -->
<!-- email="asa@nada.kth.se" -->
<!-- subject="Re: Six theses on superintelligence" -->
<!-- id="b49itmib573.fsf@sans04.nada.kth.se" -->
<!-- inreplyto="Sat, 10 Feb 2001 05:12:40&quot;" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Anders Sandberg (<a href="mailto:asa@nada.kth.se?Subject=Re:%20Six%20theses%20on%20superintelligence"><em>asa@nada.kth.se</em></a>)<br>
<strong>Date:</strong> Sat Feb 10 2001 - 14:26:08 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0559.html">Gordon Worley: "Re: Learning to be evil"</a>
<li><strong>Previous message:</strong> <a href="0557.html">Eliezer S. Yudkowsky: "Re: Learning to be evil"</a>
<li><strong>Maybe in reply to:</strong> <a href="0548.html">Mitchell Porter: "Six theses on superintelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0571.html">Durant Schoon: "Codic Cortex WAS: Six theses on superintelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#558">[ date ]</a>
<a href="index.html#558">[ thread ]</a>
<a href="subject.html#558">[ subject ]</a>
<a href="author.html#558">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&quot;Mitchell Porter&quot; &lt;<a href="mailto:mitchtemporarily@hotmail.com?Subject=Re:%20Six%20theses%20on%20superintelligence">mitchtemporarily@hotmail.com</a>&gt; writes:
<br>
<p><em>&gt; Anders said
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;Why is this isomorphic to Chaitin approximations? I
</em><br>
<em>&gt; &gt;might have had too
</em><br>
<em>&gt; &gt;little sleep for the last nights, but it doesn't
</em><br>
<em>&gt; &gt;seem clear to me.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If you know the halting probability for a Turing
</em><br>
<em>&gt; machine, you can solve the halting problem for
</em><br>
<em>&gt; any program on that machine. (&quot;... knowing Omega_N
</em><br>
<em>&gt; [first N bits of the halting probability] enables
</em><br>
<em>&gt; one to solve the halting problem for all N-bit
</em><br>
<em>&gt; programs&quot; --<a href="http://www.cs.umaine.edu/~chaitin/nv.html">http://www.cs.umaine.edu/~chaitin/nv.html</a>)
</em><br>
<p>Aha! 
<br>
&nbsp;
<br>
<em>&gt; The idea is that a superintelligence would have
</em><br>
<em>&gt; a 'computational core' which spends its time
</em><br>
<em>&gt; approximating Omega, and modules which take general
</em><br>
<em>&gt; problems, encode them as halting problems, and look
</em><br>
<em>&gt; them up in Approximate Omega.
</em><br>
<p>Ah, that's where the rub is: how do you convert a general problem into
<br>
a halting problem in an efficient way? For example, how does &quot;What
<br>
actions will give me the largest probability of having more than one
<br>
million dollars within ten years?&quot; or &quot;How do I build a
<br>
nanoassembler?&quot; convert into halting problems?
<br>
<p>I would guess that the sum of work often remains constant in the
<br>
general case: the amount of work needed to encode a problem into a
<br>
form solvable by an algorithm and the amount of work in using the
<br>
algorithm tend to be fairly constant. Intelligence is about finding
<br>
ways of getting around this by exploiting patterns that make the
<br>
problem non-general, such as in mathematical tricks where a simple
<br>
transformation makes a hard problem simple. 
<br>
<p><em>&gt; &gt;I'm not as certain as you are that there exists an
</em><br>
<em>&gt; &gt;unique optimal
</em><br>
<em>&gt; &gt;strategy. Without working within a certain problem
</em><br>
<em>&gt; &gt;domain the no free
</em><br>
<em>&gt; &gt;lunch theorems get you. Taking the problem domain to
</em><br>
<em>&gt; &gt;be 'the entire
</em><br>
<em>&gt; &gt;physical universe' doesn't really help, since you
</em><br>
<em>&gt; &gt;also have to include
</em><br>
<em>&gt; &gt;the probability distribution of the environment, and
</em><br>
<em>&gt; &gt;this will be very
</em><br>
<em>&gt; &gt;dependent not just on the interests but also actions
</em><br>
<em>&gt; &gt;of the being.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think approximating Omega is precisely the sort of
</em><br>
<em>&gt; task where a no-free-lunch theorem is likely to apply.
</em><br>
<em>&gt; The optimal strategy probably involves nothing more
</em><br>
<em>&gt; intelligent than simulating all possible programs, and
</em><br>
<em>&gt; incrementing Approximate Omega appropriately when one
</em><br>
<em>&gt; is seen to terminate. The no-free-lunch theorem might
</em><br>
<em>&gt; be: even if you have an approximation strategy which
</em><br>
<em>&gt; outperforms blind simulation in calculating some finite
</em><br>
<em>&gt; number of Omega bits, its asymptotic performance can't
</em><br>
<em>&gt; beat blind simulation.
</em><br>
<p>Sounds possible.
<br>
<p><em>&gt; &gt;What if this strategy is hard to compute
</em><br>
<em>&gt; &gt;efficiently, and different
</em><br>
<em>&gt; &gt;choices in initial conditions will produce
</em><br>
<em>&gt; &gt;noticeable differences in
</em><br>
<em>&gt; &gt;performance?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If the No-Free-Omega Hypothesis :) is correct, then
</em><br>
<em>&gt; such differences in performance will disappear
</em><br>
<em>&gt; asymptotically (assuming hardware equality, and assuming
</em><br>
<em>&gt; no-one pursues a *sub*optimal strategy).
</em><br>
<p>Ah, egalitarian transcendence! I wonder what we libertarians on the
<br>
list should make of it :-)
<br>
&nbsp;
<br>
<em>&gt; &gt;Some goals are not much helped by intelligence
</em><br>
<em>&gt; &gt;beyond a certain level
</em><br>
<em>&gt; &gt;(like, say, gardening), so the self-enhancement
</em><br>
<em>&gt; &gt;process would peter
</em><br>
<em>&gt; &gt;out before it reached any strong limits.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Only if self-enhancement was strictly a subgoal of
</em><br>
<em>&gt; the gardening goal. But perhaps this is more precise:
</em><br>
<em>&gt; self-enhancement will not be hindered if it is a
</em><br>
<em>&gt; subgoal of an open-ended goal, or a co-goal of just
</em><br>
<em>&gt; about anything.
</em><br>
<p>Beings with closed goals will eventually run out of expansion, I
<br>
think. Only beings with open-ended goals will be motivated to grow and
<br>
persist indefinitely. Playing Carse's &quot;infinite games&quot; might be a
<br>
survival trait for posthumans. 
<br>
<p><em>&gt; (Okay, that's a retreat from 'You don't have to do
</em><br>
<em>&gt; anything *but* approximate Omega!' But this is what
</em><br>
<em>&gt; I want a general theory of self-enhancement to tell me -
</em><br>
<em>&gt; in what sort of environments will you *always* need
</em><br>
<em>&gt; domain-specific modules that do something more than
</em><br>
<em>&gt; consult the Omega module? Maybe this will even prove
</em><br>
<em>&gt; to be true in the majority of environments.)
</em><br>
<p>A very interesting question. I'll have to think hard on that one, it
<br>
seems to relate to some of my own issues with how to set learning
<br>
parameters dependent on the information learned from the environment. 
<br>
<p><pre>
-- 
-----------------------------------------------------------------------
Anders Sandberg                                      Towards Ascension!
<a href="mailto:asa@nada.kth.se?Subject=Re:%20Six%20theses%20on%20superintelligence">asa@nada.kth.se</a>                            <a href="http://www.nada.kth.se/~asa/">http://www.nada.kth.se/~asa/</a>
GCS/M/S/O d++ -p+ c++++ !l u+ e++ m++ s+/+ n--- h+/* f+ g+ w++ t+ r+ !y
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0559.html">Gordon Worley: "Re: Learning to be evil"</a>
<li><strong>Previous message:</strong> <a href="0557.html">Eliezer S. Yudkowsky: "Re: Learning to be evil"</a>
<li><strong>Maybe in reply to:</strong> <a href="0548.html">Mitchell Porter: "Six theses on superintelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0571.html">Durant Schoon: "Codic Cortex WAS: Six theses on superintelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#558">[ date ]</a>
<a href="index.html#558">[ thread ]</a>
<a href="subject.html#558">[ subject ]</a>
<a href="author.html#558">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
