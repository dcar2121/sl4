<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Beyond evolution</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Beyond evolution">
<meta name="Date" content="2001-02-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Beyond evolution</h1>
<!-- received="Sun Feb 04 23:49:20 2001" -->
<!-- isoreceived="20010205064920" -->
<!-- sent="Sun, 04 Feb 2001 23:48:41 -0500" -->
<!-- isosent="20010205044841" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Beyond evolution" -->
<!-- id="3A7E30A9.B6E60FDF@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3A7E2602.E85D1653@objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Beyond%20evolution"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Feb 04 2001 - 21:48:41 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0540.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>Previous message:</strong> <a href="0538.html">Samantha Atkins: "Re: Beyond evolution"</a>
<li><strong>In reply to:</strong> <a href="0538.html">Samantha Atkins: "Re: Beyond evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0540.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>Reply:</strong> <a href="0540.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>Reply:</strong> <a href="0541.html">Samantha Atkins: "Re: Beyond evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#539">[ date ]</a>
<a href="index.html#539">[ thread ]</a>
<a href="subject.html#539">[ subject ]</a>
<a href="author.html#539">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Samantha Atkins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &quot;Eliezer S. Yudkowsky&quot; wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; Advice - freely offered, freely rejected.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What does it mean to reject the advice of a Being that controls all the
</em><br>
<em>&gt; material local universe to a very fine level
</em><br>
<p>1:  Sysop (observes):  Samantha's volitional decision is that she would
<br>
like me to offer advice as long as I don't use persuasive methods that
<br>
'force' her decision - that is, use persuasive methods that are powerful
<br>
enough to convince her of false things as well as true things.
<br>
2:  Sysop:  &quot;You know, Samantha, Sysops aren't such bad things.&quot;
<br>
3:  Samantha:  &quot;I disagree!&quot;
<br>
4:  Sysop:  &quot;OK.&quot;
<br>
5:  Samantha:  &quot;I will start a revolutionary committee to overthrow the
<br>
Sysop!&quot;
<br>
6:  Sysop:  &quot;OK.&quot;
<br>
7:  Samantha:  &quot;Please put out a notice to that effect on the public
<br>
band.&quot;
<br>
8:  Sysop:  &quot;OK.&quot;
<br>
9:  Samantha:  &quot;Please construct a SysopKiller device using matter which I
<br>
own.&quot;
<br>
10: Sysop:  &quot;OK.&quot;
<br>
11: Samantha:  &quot;Fire the SysopKiller at this test target so I can see how
<br>
it works.&quot;
<br>
12: Sysop:  &quot;OK.&quot;
<br>
13: Samantha:  &quot;Fire the SysopKiller at yourself.&quot;
<br>
14: Sysop:  &quot;API error.&quot;
<br>
<p><em>&gt; and will not allow
</em><br>
<em>&gt; disagreement that leads to possible actions that it decides are possibly
</em><br>
<em>&gt; harmful to the sentiences in its care?  Where is the freedom?  I see
</em><br>
<em>&gt; freedom to disagree but not to fully act on one's disagreement?
</em><br>
<p>The Sysop rules won't allow you to kill someone without vis permission. 
<br>
You can advocate killing people to your heart's content.
<br>
<p><em>&gt; &gt; Build another SI of equal intelligence - sure, as long as you build ver
</em><br>
<em>&gt; &gt; inside the Sysop.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What for?  That would rather defeat the purpose of having more than one
</em><br>
<em>&gt; local Entity of such power.  A single entitity is a single point of
</em><br>
<em>&gt; failure of Friendliness and a great danger.
</em><br>
<p>Multiple entities are multiple points of failure of Friendliness and even
<br>
greater dangers.
<br>
<p>A failure of Friendliness in a transcending seed AI results in a total
<br>
takeover regardless of what a Friendly AI thinks about the Sysop
<br>
Scenario.  Once an AI has *reached* the Sysop point you're either screwed
<br>
or saved, so forking off more Sysops after that is a particularly
<br>
pointless risk.
<br>
<p><em>&gt; &gt; Build an Ultraweapon of Megadeath and Destruction so you can see how it
</em><br>
<em>&gt; &gt; works - sure, as long as there's a bit of Sysop somewhere inside the
</em><br>
<em>&gt; &gt; trigger making sure you don't point it at the Amish communities on Old
</em><br>
<em>&gt; &gt; Earth.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Building ultra-weapons is not at all the point or anything I intend.
</em><br>
<p>So what *do* you intend that you can't do with a Sysop?  No tautological
<br>
answers like &quot;Build something outside the Sysop&quot;; name some specific fun
<br>
or meaningful thing that you should, morally, be able to do, but which the
<br>
Sysop won't let you do.
<br>
<p><em>&gt; &gt; Build an Ultraweapon that you can aim anywhere, with no Sysopmatter
</em><br>
<em>&gt; &gt; (visible or not) anywhere near it - you might still be able to get away
</em><br>
<em>&gt; &gt; with this, as long as the Sysop can predict the future with total
</em><br>
<em>&gt; &gt; certainty and predict that you'll never abuse the Ultraweapon, regardless
</em><br>
<em>&gt; &gt; of any external influences you encounter.  Probably no human, even Gandhi,
</em><br>
<em>&gt; &gt; is subject to this prediction, but an uploaded Gandhi turned transhuman
</em><br>
<em>&gt; &gt; might be.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Again, the Sysop abrogates all decisions and all wisdom to itself.  How
</em><br>
<em>&gt; about upgrading its uploads to their own ever-increasing wisdom.
</em><br>
<p>15: Sysop:  &quot;OK.&quot;
<br>
<p><em>&gt; &gt; Under absolutely none of these circumstances does the Sysop need to strike
</em><br>
<em>&gt; &gt; back at you.  Ve just gives you an API error.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Err.  Already assumes precisely my point.  This being is effectively
</em><br>
<em>&gt; God.  You exist only within it and as it allows.  Are you really willing
</em><br>
<em>&gt; to take on the building of such?  Are you so convinced it is the Only
</em><br>
<em>&gt; Answer?
</em><br>
<p>Yes, and yes.  The risks inherent in material omnipotence are inherent in
<br>
rapid transcendence and thus inherent in AI.  The Sysop Scenario adds
<br>
nothing to that.
<br>
<p>And, incidentally, the Sysop Scenario isn't what I'm telling a Friendly
<br>
seed AI to do.  It's what I think a Friendly superintelligence will decide
<br>
to do.  I'm neither taking on the building of such, nor need to be
<br>
convinced that it's the Only Answer.
<br>
<p><em>&gt; &gt; For all I know, it's entirely okay to fork off and run under your own
</em><br>
<em>&gt; &gt; Sysop as long as that Sysop is also Friendly.  (People who chime in about
</em><br>
<em>&gt; &gt; how this would dump us into a Darwinian regime may take this as an
</em><br>
<em>&gt; &gt; argument against Sysop splitting.)  The static uploads may even form their
</em><br>
<em>&gt; &gt; own polises with different operating systems and rules, with the
</em><br>
<em>&gt; &gt; underlying Sysop merely acting to ensure that no citizen can be trapped
</em><br>
<em>&gt; &gt; inside a polis.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But this Sysop can't be built by your earlier response except totally
</em><br>
<em>&gt; within the Sysop so in no real sense is it independent.
</em><br>
<p>No, I'm pointing out a possible variation on my earlier response (albeit
<br>
one that I personally think improbable), under which it's possible to
<br>
construct an independent Sysop as long as it's an independent Friendly
<br>
Sysop.
<br>
<p><em>&gt; I am concerned
</em><br>
<em>&gt; by the phrase &quot;static uploads&quot;.  Do you mean by this that uploads cannot
</em><br>
<em>&gt; grow indefinitely in capability?
</em><br>
<p>No, I mean modern-day humans who choose to upload but not to upgrade.
<br>
<p><em>&gt; &gt; This brings up a point I keep on trying to make, which is that the Sysop
</em><br>
<em>&gt; &gt; is not a ruler; the Sysop is an operating system.  The Sysop may not even
</em><br>
<em>&gt; &gt; have a public personality as such; our compounded &quot;wishes about wishes&quot;
</em><br>
<em>&gt; &gt; may form an independent operating system and API that differs from citizen
</em><br>
<em>&gt; &gt; to citizen, ranging from genie interfaces with a personality, to an Eganic
</em><br>
<em>&gt; &gt; &quot;exoself&quot;, to transhumans that simply dispense with the appearance of an
</em><br>
<em>&gt; &gt; interface and integrate their abilities into themselves, like motor
</em><br>
<em>&gt; &gt; functions.  The fact that there's a Sysop underneath it all changes
</em><br>
<em>&gt; &gt; nothing; it just means that your interface (a) can exhibit arbitrarily
</em><br>
<em>&gt; &gt; high levels of intelligence and (b) will return some kind of error if you
</em><br>
<em>&gt; &gt; try to harm another citizen.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Let's see.  The SysOp is a super-intelligence.  Therefore it has its own
</em><br>
<em>&gt; agenda and interests.
</em><br>
<p>NON SEQUITUR
<br>
<p><em>&gt; It controls all aspects of material reality and
</em><br>
<em>&gt; all virtual ones that we have access to.
</em><br>
<p>Yes.
<br>
<p><em>&gt; This is a good deal more than
</em><br>
<em>&gt; just an operating system.
</em><br>
<p>Why?  The laws of physics control all aspects of material reality too.
<br>
<p><em>&gt; What precisely constitutes harm of another
</em><br>
<em>&gt; citizen to the Sysop?
</em><br>
<p>Each citizen would define the way in which other entities can interact
<br>
with matter and computronium which that citizen owns.
<br>
<p><em>&gt; For entities in a VR who are playing with
</em><br>
<em>&gt; designer universes of simulated beings they experience from inside, is
</em><br>
<em>&gt; it really harm that in this universe these simulated beings maim and
</em><br>
<em>&gt; kill one another?  In other words, does the SysOp prevent real harm or
</em><br>
<em>&gt; all appearance of harm?  What is and isn't real needs answering also,
</em><br>
<em>&gt; obviously.
</em><br>
<p>I don't see how this moral issue is created by the Sysop Scenario.  It's
<br>
something that we need to decide, as a fundamental moral issue, no matter
<br>
which future we walk into.
<br>
<p><em>&gt; &gt; Yep.  Again, for static uploads, the Sysop won't *necessarily* be a
</em><br>
<em>&gt; &gt; dominant feature of reality, or even a noticeable one.  For sysophobic
</em><br>
<em>&gt; &gt; statics, the complexity of the future would be embedded entirely in social
</em><br>
<em>&gt; &gt; interactions and so on.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If it is present at all it will be noticeable except for those who
</em><br>
<em>&gt; purposefully choose to design a local space where they do not see it.
</em><br>
<p>Yes, that's right.
<br>
<p><em>&gt; &gt; Of course not.  You could be right and I could be wrong, in which case -
</em><br>
<em>&gt; &gt; if I've built well - the Sysop will do something else, or the seed AI will
</em><br>
<em>&gt; &gt; do something other than become Sysop.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; OK.  If it is not the Sysop what are some of the alternate scenarios
</em><br>
<em>&gt; that you could see occurring that are desirable outcomes?
</em><br>
<p>1)  It turns out that humanity's destiny is to have an overall GroupMind
<br>
that runs the Solar System.  The Sysop creates the infrastructure for the
<br>
GroupMind, invites everyone in who wants in, transfers control of API
<br>
functions to the GroupMind's volition, and either terminates verself or
<br>
joins the GroupMind.
<br>
<p>2)  Preventing citizens from torturing one another doesn't require
<br>
continuous enforcement by a sentient entity; the Sysop invokes some kind
<br>
of ontotechnological Word of Command that rules out the negative set of
<br>
possibilities, then terminates verself, or sticks around being helpful
<br>
until more SIs show up.
<br>
<p><em>&gt; &gt; Yes.  I think that, if the annoyance resulting from pervasive forbiddance
</em><br>
<em>&gt; &gt; is a necessary subgoal of ruling out the space of possibilities in which
</em><br>
<em>&gt; &gt; citizenship rights are violated, then it's an acceptable tradeoff.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If the citizens have no choice then there is no morality.
</em><br>
<p>That sounds to me like one more variation on &quot;It's the struggle that's
<br>
important, not the goal.&quot;  What's desirable is that people not hurt one
<br>
another.  It's also desirable that they not choose to hurt one another,
<br>
but that's totally orthagonal to the first point.
<br>
<p>You can still become a better person, as measured by what you'd do if the
<br>
Sysop suddenly vanished.
<br>
<p>Are we less moral because we live in a society with police officers? 
<br>
Would we suddenly become more moral if all law enforcement and all social
<br>
disapprobation and all other consequences of murder suddenly vanished?
<br>
<p><em>&gt; There is only
</em><br>
<em>&gt; that which works by the Sysop's rules and that which does not.  In such
</em><br>
<em>&gt; a universe I see little impetus for the citizens to evolve.
</em><br>
<p>11:  Johnny:   &quot;This is my thought sculpture.&quot;
<br>
12:  Samantha: &quot;It sucks.&quot;
<br>
<p>21:  Eliezer:  &quot;This is my thought sculpture.&quot;
<br>
22:  Eliezer:  &quot;It sucks.&quot;
<br>
<p><em>&gt; &gt; Please note that in your scenario, people are not all free free free as a
</em><br>
<em>&gt; &gt; bird.  In your scenario, you can take an extended vacation from Sysop
</em><br>
<em>&gt; &gt; space, manufacture a million helpless sentients, and then refuse to let
</em><br>
<em>&gt; &gt; *them* out of Samantha space.  You can take actions that would make them
</em><br>
<em>&gt; &gt; *desperate* to leave Samantha space and they still won't be able to go,
</em><br>
<em>&gt; &gt; because the Sysop that would ensure those rights has gone away to give you
</em><br>
<em>&gt; &gt; a little personal space.  I daresay that in terms of the total integral
</em><br>
<em>&gt; &gt; over all sentients and their emotions, the Samantha scenario involves many
</em><br>
<em>&gt; &gt; many more sentients feeling much more intense desire to escape control.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The Sysop is refusing to let me out of Sysop space.  Truthfully we have
</em><br>
<em>&gt; no idea how various sentiences will react to being in Sysop space no
</em><br>
<em>&gt; matter how benign you think it is.  Your hypothetical space where I
</em><br>
<em>&gt; torture sentients is an utter strawman.
</em><br>
<p>Is it still a strawman scenario when integrated over the six billion
<br>
current residents of Earth?  Or is only Samantha allowed to go Outside?
<br>
<p>The Friendly seed AI turned Friendly superintelligence makes the final
<br>
decision, and ve *does* have an idea of how various sentiences will
<br>
react.  If the Sysop scenario really results in more summated misery than
<br>
letting every Hitler have vis own planet, or if there's some brilliant
<br>
third alternative, then the Sysop scenario will undoubtedly be quietly
<br>
ditched.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0540.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>Previous message:</strong> <a href="0538.html">Samantha Atkins: "Re: Beyond evolution"</a>
<li><strong>In reply to:</strong> <a href="0538.html">Samantha Atkins: "Re: Beyond evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0540.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>Reply:</strong> <a href="0540.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>Reply:</strong> <a href="0541.html">Samantha Atkins: "Re: Beyond evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#539">[ date ]</a>
<a href="index.html#539">[ thread ]</a>
<a href="subject.html#539">[ subject ]</a>
<a href="author.html#539">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
