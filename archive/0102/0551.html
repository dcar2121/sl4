<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Learning to be evil</title>
<meta name="Author" content="Gordon Worley (redbird@rbisland.cx)">
<meta name="Subject" content="Re: Learning to be evil">
<meta name="Date" content="2001-02-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Learning to be evil</h1>
<!-- received="Fri Feb 09 16:01:11 2001" -->
<!-- isoreceived="20010209230111" -->
<!-- sent="Fri, 09 Feb 2001 15:29:47 -0500" -->
<!-- isosent="20010209202947" -->
<!-- name="Gordon Worley" -->
<!-- email="redbird@rbisland.cx" -->
<!-- subject="Re: Learning to be evil" -->
<!-- id="f05001900b6a9fce0ab3b@[10.0.1.2]" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3A821BEC.B29E3DFE@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Gordon Worley (<a href="mailto:redbird@rbisland.cx?Subject=Re:%20Learning%20to%20be%20evil"><em>redbird@rbisland.cx</em></a>)<br>
<strong>Date:</strong> Fri Feb 09 2001 - 13:29:47 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0552.html">Eliezer S. Yudkowsky: "Re: Learning to be evil"</a>
<li><strong>Previous message:</strong> <a href="0550.html">Anders Sandberg: "Re: Six theses on superintelligence"</a>
<li><strong>In reply to:</strong> <a href="0545.html">Eliezer S. Yudkowsky: "Re: Learning to be evil"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0552.html">Eliezer S. Yudkowsky: "Re: Learning to be evil"</a>
<li><strong>Reply:</strong> <a href="0552.html">Eliezer S. Yudkowsky: "Re: Learning to be evil"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#551">[ date ]</a>
<a href="index.html#551">[ thread ]</a>
<a href="subject.html#551">[ subject ]</a>
<a href="author.html#551">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 11:09 PM -0500 2/7/01, Eliezer S. Yudkowsky wrote:
<br>
<em>&gt;  &gt; The consequences of evil
</em><br>
<em>&gt;  &gt; actions always come back on the evil doer, having a negative effect
</em><br>
<em>&gt;  &gt; on them.
</em><br>
<em>&gt;
</em><br>
<em>&gt;A romantic and rather impractical view.  Sometimes the consequences of
</em><br>
<em>&gt;evil come back on the evildoer, sometimes they don't.  Highly competent
</em><br>
<em>&gt;evildoers have gone on to die in bed, surrounded by many loving, newly
</em><br>
<em>&gt;wealthy great-grandchildren, and somewhere along the line, you've got
</em><br>
<em>&gt;their genes.
</em><br>
<p>This may depend on what you choose to consider evil.  I would 
<br>
consider anything to be evil if it has a net negative effect.  In 
<br>
such a case, the evil doer may not directly feel the consequences, 
<br>
and in fact ve may benefit, but there will be no net benefit. 
<br>
Consider, for example, that an SI decides to create a planet eating 
<br>
fog that proceedes to consume planets and turn their matter into 
<br>
something more useful to the SI.  Along the way, some species will 
<br>
have to die.  Included amongst these will probably be some 
<br>
intelligent ones.  Had the SI not done an evil thing by wiping out 
<br>
intelligent life, ve could have benefited from the intelligences on 
<br>
that planet because there is the possibility they would have created 
<br>
something new that the SI had never encountered before.  For the SI 
<br>
that seems to know everything (ignoring forgetting information so 
<br>
that ve can experience for the first time again), something new would 
<br>
be very interesting, even life changing.  I can't be totally sure on 
<br>
this, though, since I don't know what it will be like to be an SI. 
<br>
I'll report back here in a few decades to let you know what it's 
<br>
like.  :-)  Ultimately, it may turn out that my example is not evil 
<br>
at all, and I would have to look for another one.
<br>
<p>Now, an intelligence's personal idea of what is evil is something 
<br>
different.  Just as there may be a universal standard for beauty, yet 
<br>
many people have a certain conception of beauty, there may be a 
<br>
universal concept of what is evil, but many people who toss in morals 
<br>
on top to create new guidlines for evil.  A PETA member might think 
<br>
that it's evil when I eat a hamburger, but I don't since, AFAIK, the 
<br>
cow that died to make it is worth more as a sandwitch than as a dumb 
<br>
(i.e. not intelligent) beast that consumes resources to be wasted 
<br>
when it dies.
<br>
<p><em>&gt;  &gt; To that extent, Friendliness seems to me like an
</em><br>
<em>&gt;&gt;  inherent trait in SIs, since unlike humans they will be smart enough
</em><br>
<em>&gt;  &gt; to consider all of the consequences.
</em><br>
<em>&gt;
</em><br>
<em>&gt;And if the SI is the only one around, and powerful enough that there are
</em><br>
<em>&gt;no consequences?
</em><br>
<p>That's why we have to make sure that more than one personal is 
<br>
uploaded at once and that there is more than one AI.  No one should 
<br>
be the first upload, no matter how much money they pay, though I am 
<br>
content with them being one of the first *uploads*.  Just the same, a 
<br>
single AI should not be released on the universe (unless there are 
<br>
already some post humans around) without some other AIs.
<br>
<p><em>&gt;Anyhoo, Friendliness isn't intended to suppress evil impulses.  AIs don't
</em><br>
<em>&gt;have 'em unless you put them there.  Correspondingly, although other
</em><br>
<em>&gt;possibilities exist, the default, engineering-*conservative* assumption is
</em><br>
<em>&gt;that goodness also doesn't materialize in source code.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Friendliness is a means whereby a genuinely, willingly altruistic
</em><br>
<em>&gt;programming team transmits genuine, willing altruism to an AI, packaged to
</em><br>
<em>&gt;avoid damage in transmission, and infused in such form as to eventually
</em><br>
<em>&gt;become independent of the original programmers.
</em><br>
<p>Okay, now I see what Friendliness is supposed to be.  Or wait, maybe 
<br>
I don't.  What do you mean by altruistic?  Are you refering the 
<br>
common definition as something good found in more dictionaries, the 
<br>
Ayn Rand definition, or some other, personal definiton?  As you might 
<br>
imagine, altruism has become a loaded word due to its differing uses, 
<br>
so what you consider it to mean is pivitol to understanding 
<br>
Friendliness, as well as some of your other ideas that I have seen 
<br>
make reference to it.  Sorry if you've answered this somewhere 
<br>
before, but I can't remember ever seeing a link about it or an 
<br>
explination.
<br>
<pre>
-- 
Gordon Worley
<a href="http://www.rbisland.cx/">http://www.rbisland.cx/</a>
mailto:<a href="mailto:redbird@rbisland.cx?Subject=Re:%20Learning%20to%20be%20evil">redbird@rbisland.cx</a>
PGP:  C462 FA84 B811 3501 9010  20D2 6EF3 77F7 BBD3 B003
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0552.html">Eliezer S. Yudkowsky: "Re: Learning to be evil"</a>
<li><strong>Previous message:</strong> <a href="0550.html">Anders Sandberg: "Re: Six theses on superintelligence"</a>
<li><strong>In reply to:</strong> <a href="0545.html">Eliezer S. Yudkowsky: "Re: Learning to be evil"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0552.html">Eliezer S. Yudkowsky: "Re: Learning to be evil"</a>
<li><strong>Reply:</strong> <a href="0552.html">Eliezer S. Yudkowsky: "Re: Learning to be evil"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#551">[ date ]</a>
<a href="index.html#551">[ thread ]</a>
<a href="subject.html#551">[ subject ]</a>
<a href="author.html#551">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
