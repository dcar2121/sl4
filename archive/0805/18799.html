<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Happy Box</title>
<meta name="Author" content="Samantha Atkins (sjatkins@gmail.com)">
<meta name="Subject" content="Re: Happy Box">
<meta name="Date" content="2008-05-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Happy Box</h1>
<!-- received="Sat May  3 22:58:42 2008" -->
<!-- isoreceived="20080504045842" -->
<!-- sent="Sat, 3 May 2008 21:55:14 -0700" -->
<!-- isosent="20080504045514" -->
<!-- name="Samantha Atkins" -->
<!-- email="sjatkins@gmail.com" -->
<!-- subject="Re: Happy Box" -->
<!-- id="A12AB6CD-86D4-40E9-AE18-22B5E78D5F26@gmail.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="1209835991.6679.58.camel@phantom" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:sjatkins@gmail.com?Subject=Re:%20Happy%20Box"><em>sjatkins@gmail.com</em></a>)<br>
<strong>Date:</strong> Sat May 03 2008 - 22:55:14 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="18800.html">William Pearson: "Re: Self vs. other (was Re: Balance of power)"</a>
<li><strong>Previous message:</strong> <a href="18798.html">Krekoski Ross: "Re: Unbounded happiness"</a>
<li><strong>In reply to:</strong> <a href="18791.html">Mikko Rauhala: "Re: Happy Box"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18801.html">Stathis Papaioannou: "Re: Happy Box"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18799">[ date ]</a>
<a href="index.html#18799">[ thread ]</a>
<a href="subject.html#18799">[ subject ]</a>
<a href="author.html#18799">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On May 3, 2008, at 10:33 AM, Mikko Rauhala wrote:
<br>
<p><em>&gt; la, 2008-05-03 kello 09:16 -0700, Samantha Atkins kirjoitti:
</em><br>
<em>&gt;&gt; I disagree.  No supergoal being better / more intelligent than  
</em><br>
<em>&gt;&gt; another
</em><br>
<em>&gt;&gt; implies that the entities involved effectively have no nature, that  
</em><br>
<em>&gt;&gt; they
</em><br>
<em>&gt;
</em><br>
<em>&gt; I fail to see how this is implied. On the contrary, as intelligence is
</em><br>
<em>&gt; not a factor in primary goals, the primary goals of an entity pretty
</em><br>
<em>&gt; much define its nature (or if you like, the other way around,  
</em><br>
<em>&gt; depending
</em><br>
<em>&gt; on what's your preferred semantics for &quot;nature&quot;).
</em><br>
<p>So does this entity + supegoal exist in a vacuum or does the quality  
<br>
of its supergoal effect its survivability and viability relative to  
<br>
other beings it may have to compete and cooperate with?  Given that  
<br>
its goal structure directly impinges on the quality of its life  
<br>
(speaking very loosely on purpose) it should occur to it that perhaps  
<br>
a slight relaxation or reworking of its goal structure might produce  
<br>
more and better results still more or less in keeping with its  
<br>
supergoal.  In particular I would expect a system with the supergoal  
<br>
to maximize human potential and well-being to have considerable leeway  
<br>
to form notions of what that might entail and how best to achieve it.   
<br>
It would have to continuously monitor actual results and consider  
<br>
alternative formulations to have a chance of succeeding.
<br>
<p><em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; somehow exist in a vacuum such that whether their goals make  
</em><br>
<em>&gt;&gt; conditions
</em><br>
<em>&gt;&gt; they live in better or worse for themselves and those around them is
</em><br>
<em>&gt;&gt; utterly and completely irrelevant.   This is simply and literally
</em><br>
<em>&gt;&gt; without foundation.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If &quot;this&quot; refers to the claim that my words somehow imply the above,
</em><br>
<em>&gt; indeed it is true. Otherwise this is more straws for the strawman.
</em><br>
<em>&gt;
</em><br>
<p>I don't think you can so easily dismiss the objection.
<br>
<p><p><em>&gt;&gt; Much of our discussion presumes that a super-intelligence will not do
</em><br>
<em>&gt;&gt; meta-abstraction on its own goals.  This is very convenient and  
</em><br>
<em>&gt;&gt; perhaps
</em><br>
<em>&gt;&gt; necessary to convincing ourselves we can somehow cause such a being  
</em><br>
<em>&gt;&gt; to
</em><br>
<em>&gt;&gt; be &quot;Friendly&quot;. But it does not match any intelligent systems existing
</em><br>
<em>&gt;&gt; to date and is not a foolproof assumption.
</em><br>
<em>&gt;
</em><br>
<em>&gt; As for systems existing to date, it's trivially explainable why we  
</em><br>
<em>&gt; have
</em><br>
<em>&gt; to do that nasty stuff: our goal structure, such as it is, is a
</em><br>
<em>&gt; self-contradictory mess of overlapping and inconsistent values.  
</em><br>
<em>&gt; However,
</em><br>
<em>&gt; for at least some of us, consistency and its friends are included in
</em><br>
<em>&gt; that mess (quite possibly as instrumental values rather than intrinsic
</em><br>
<em>&gt; ones). This enables, nay, even requires us to study, abstract and rank
</em><br>
<em>&gt; some of our goals in order to make some sense out of the spaghetti  
</em><br>
<em>&gt; we've
</em><br>
<em>&gt; been dealt. (The apparent instrumentality stems largely from the fact
</em><br>
<em>&gt; that if we did not try to clean things up a bit, we'd be no good at
</em><br>
<em>&gt; achieving any of our goals, what with working internally against
</em><br>
<em>&gt; ourselves the whole time - as, indeed, we often do regardless.)
</em><br>
<p>That is one trivial explanation.  The jury is out whether any  
<br>
intelligent viable entity would require some amount of flexibility to  
<br>
avoid stasis.
<br>
<p><em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; On the other hand, a rational intelligent system with a clean and
</em><br>
<em>&gt; consistent goal architecture does not have to do things like this, and
</em><br>
<em>&gt; indeed will not, for rationality in pursuing goals _by definition_  
</em><br>
<em>&gt; rules
</em><br>
<em>&gt; out changing those goals. Of course, whether such a system can be  
</em><br>
<em>&gt; built
</em><br>
<em>&gt; is another matter, where one has to rely on one's best guestimates.
</em><br>
<em>&gt; Regardless, with our way of operation easily explained by our  
</em><br>
<em>&gt; identified
</em><br>
<em>&gt; bugs (irrationality and inconsistency), your argument is easily
</em><br>
<em>&gt; nullified as a reason to think systems in general would have to be
</em><br>
<em>&gt; similarly buggy.
</em><br>
<em>&gt;
</em><br>
<p>This is supposition.   It is also supposition there there is a &quot;clean  
<br>
and consistent goal architecture&quot; that produces much of interest in  
<br>
the way of AGI.    My argument is not yet nullified as we cannot at  
<br>
this point really claim that such a system can be built and be  
<br>
viable.    Flexibility is not equivalent to buggy.
<br>
<p><em>&gt; Of course, I don't deny the possibility of hodgepodge superintelligent
</em><br>
<em>&gt; systems that were as irrational and inconsistent as us. In this case
</em><br>
<em>&gt; they would quite conceivably have to resort to same kind of cognitive
</em><br>
<em>&gt; kludges as us too, and be rather dangerous.
</em><br>
<p>It might be that our notion of rationality is a bit too tight to  
<br>
produce a creative ever-growing super-intelligence.
<br>
<p><em>&gt; No foolproof claims from me here, I just think it's worth a shot  
</em><br>
<em>&gt; making
</em><br>
<em>&gt; a the former kind of AI what with there being no solid arguments to  
</em><br>
<em>&gt; the
</em><br>
<em>&gt; impossibility thereof. As well as having astronomical waste hanging in
</em><br>
<em>&gt; the balance and all that.
</em><br>
<em>&gt;
</em><br>
<p>No foolproof claims from me either.  Just a question and an uneasy  
<br>
suspicion.
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18800.html">William Pearson: "Re: Self vs. other (was Re: Balance of power)"</a>
<li><strong>Previous message:</strong> <a href="18798.html">Krekoski Ross: "Re: Unbounded happiness"</a>
<li><strong>In reply to:</strong> <a href="18791.html">Mikko Rauhala: "Re: Happy Box"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18801.html">Stathis Papaioannou: "Re: Happy Box"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18799">[ date ]</a>
<a href="index.html#18799">[ thread ]</a>
<a href="subject.html#18799">[ subject ]</a>
<a href="author.html#18799">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
