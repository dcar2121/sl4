<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Slow-fast singularity</title>
<meta name="Author" content="charles griffiths (cs.griffiths@yahoo.com)">
<meta name="Subject" content="Re: Slow-fast singularity">
<meta name="Date" content="2008-05-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Slow-fast singularity</h1>
<!-- received="Fri May  9 03:17:13 2008" -->
<!-- isoreceived="20080509091713" -->
<!-- sent="Fri, 9 May 2008 02:14:32 -0700 (PDT)" -->
<!-- isosent="20080509091432" -->
<!-- name="charles griffiths" -->
<!-- email="cs.griffiths@yahoo.com" -->
<!-- subject="Re: Slow-fast singularity" -->
<!-- id="597649.61845.qm@web33508.mail.mud.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="87478b5d0805081223w7cb26749i3bf30ecb7df800ab@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> charles griffiths (<a href="mailto:cs.griffiths@yahoo.com?Subject=Re:%20Slow-fast%20singularity"><em>cs.griffiths@yahoo.com</em></a>)<br>
<strong>Date:</strong> Fri May 09 2008 - 03:14:32 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="18864.html">Stuart Armstrong: "Re: Signaling after a singularity"</a>
<li><strong>Previous message:</strong> <a href="18862.html">Matt Mahoney: "Re: Error detecting, error correcting, error predicting"</a>
<li><strong>In reply to:</strong> <a href="18859.html">Krekoski Ross: "Re: Slow-fast singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18867.html">Matt Mahoney: "Re: Slow-fast singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18863">[ date ]</a>
<a href="index.html#18863">[ thread ]</a>
<a href="subject.html#18863">[ subject ]</a>
<a href="author.html#18863">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
math
<br>
<p><p>Krekoski Ross &lt;<a href="mailto:rosskrekoski@gmail.com?Subject=Re:%20Slow-fast%20singularity">rosskrekoski@gmail.com</a>&gt; wrote: Actually, now that I think about it-- this precludes a fast take-off with no end. 
<br>
<p>Any intelligent system cannot meaningfully increase its own complexity without a lot of input, or otherwise assimilating complexity and adding it to its own. 
<br>
&nbsp;
<br>
Consider I have an AI of a given complexity, lets call it takeoff(n) where n is the number of iterations we would like it to run for, conceivably a very large number.  takeoff() itself has a specific complexity. 
<br>
<p>&nbsp;however, without a large number of inputs, takeoff() cannot increase its own complexity, since the complexity it reaches at any point in time is describable with its own function, and a specific integer as an argument. 
<br>
&nbsp;
<br>
any AI can therefore really only at most, assimilate the complexity of the sum of all human knowledge. Without the ability to meaningfully interact with its environment and effectively assimilate information that is otherwise external to the sum of human knowledge, it will plateau. 
<br>
&nbsp;
<br>
Ross
<br>
<p><p><p>On Thu, May 8, 2008 at 6:43 PM, Krekoski Ross &lt;<a href="mailto:rosskrekoski@gmail.com?Subject=Re:%20Slow-fast%20singularity">rosskrekoski@gmail.com</a>&gt; wrote:
<br>
&nbsp;
<br>
<p>On Thu, May 8, 2008 at 8:59 AM, Stuart Armstrong &lt;<a href="mailto:dragondreaming@googlemail.com?Subject=Re:%20Slow-fast%20singularity">dragondreaming@googlemail.com</a>&gt; wrote:  
<br>
&nbsp;
<br>
&nbsp;
<br>
What makes you claim that? We have little understanding of
<br>
&nbsp;intelligence; we don't know how easy or hard increases in intelligence
<br>
&nbsp;will turn out to be; we're not even certain how high the advantages of
<br>
&nbsp;increased intelligence will turn out to be.
<br>
&nbsp;
<br>
&nbsp;It could be a series of increasing returns, and the advantages could
<br>
&nbsp;be huge - but we really don't know that. &quot;Most likely scenario&quot; is
<br>
&nbsp;much to strong a thing to say.
<br>
&nbsp;
<br>
<p><p>Yes. 
<br>
<p>I personally dont have a strong opinion on the probability of either scenario just because there are so many unknowns, and we have an effective sample size of 1 (ourselves) with which to base all of our understanding of intelligence.  But I think we should realize one thing--- is it only by incredible coincidence that our intelligence is at a level such that we can understand the formal properties of our brain, but are just below some 'magical' threshold that would allow us to mentally simulate what differences in subjective experience and intelligence a slight change in our architecture would entail, but just above the threshold where it would be possible to do so for 'lower' entities? 
<br>
&nbsp;&nbsp;
<br>
I've mentioned this before in various forms but in general I think its a fairly under-addressed topic: Can an intelligent system of complexity A, perfectly emulate (perform a test run of) an intelligent system of complexity A? (for fairly obvious reasons it cannot emulate one of higher complexity). It seems possible that an intelligent system of complexity A can emulate one of complexity A-K where K is the output of some function that describes some proportion of A (we dont know specifically how complexity in an intelligent system affects intelligence, except that in a perfectly designed machine, an increase in complexity will entail an increase in intelligence).  I think that because of natural systemic overhead, it is impossible for any perfectly designed intelligent system to properly model another system of equal complexity. (and indeed no effective way to evaluate the model if it could)
<br>
&nbsp;&nbsp;
<br>
This has implications on the rate at which any AI can self-improve-- if K is a reasonably significant proportion of A, even a godlike AI would have difficulty improving its own intelligence in an efficient and rapid way. 
<br>
&nbsp;&nbsp;
<br>
This is also why evolution by random mutation is a slow, but actually quite efficient way of increasing intelligence--- we dont want a progressively larger, but structurally homogenous system (which actually is not an efficient increase in complexity, only size). We want structural diversity in an intelligent system, and its not clear how a system can 'invent' novel structures that is completely foreign to it. Many of our own advances in science, by analogy, arise from mimicry of for example non-human biological systems. 
<br>
&nbsp;&nbsp;
<br>
Ross
<br>
<p>&nbsp;
<br>
<p>&nbsp;Stuart
<br>
&nbsp;
<br>
<p>&nbsp;
<br>
<p>&nbsp;
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
---------------------------------
<br>
Be a better friend, newshound, and know-it-all with Yahoo! Mobile.  Try it now.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18864.html">Stuart Armstrong: "Re: Signaling after a singularity"</a>
<li><strong>Previous message:</strong> <a href="18862.html">Matt Mahoney: "Re: Error detecting, error correcting, error predicting"</a>
<li><strong>In reply to:</strong> <a href="18859.html">Krekoski Ross: "Re: Slow-fast singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18867.html">Matt Mahoney: "Re: Slow-fast singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18863">[ date ]</a>
<a href="index.html#18863">[ thread ]</a>
<a href="subject.html#18863">[ subject ]</a>
<a href="author.html#18863">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
