<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Slow-fast singularity</title>
<meta name="Author" content="Stuart Armstrong (dragondreaming@googlemail.com)">
<meta name="Subject" content="Slow-fast singularity">
<meta name="Date" content="2008-05-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Slow-fast singularity</h1>
<!-- received="Mon May  5 09:28:36 2008" -->
<!-- isoreceived="20080505152836" -->
<!-- sent="Mon, 5 May 2008 17:25:38 +0200" -->
<!-- isosent="20080505152538" -->
<!-- name="Stuart Armstrong" -->
<!-- email="dragondreaming@googlemail.com" -->
<!-- subject="Slow-fast singularity" -->
<!-- id="38f493f10805050825t6d2b799cx740bd64270a2e015@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Stuart Armstrong (<a href="mailto:dragondreaming@googlemail.com?Subject=Re:%20Slow-fast%20singularity"><em>dragondreaming@googlemail.com</em></a>)<br>
<strong>Date:</strong> Mon May 05 2008 - 09:25:38 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="18813.html">Stuart Armstrong: "Error detecting, error correcting, error predicting"</a>
<li><strong>Previous message:</strong> <a href="18811.html">Stuart Armstrong: "Signaling after a singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18822.html">Matt Mahoney: "Re: Slow-fast singularity"</a>
<li><strong>Reply:</strong> <a href="18822.html">Matt Mahoney: "Re: Slow-fast singularity"</a>
<li><strong>Reply:</strong> <a href="18825.html">Tim Freeman: "Re: Slow-fast singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18812">[ date ]</a>
<a href="index.html#18812">[ thread ]</a>
<a href="subject.html#18812">[ subject ]</a>
<a href="author.html#18812">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Most posts on this ML tacitly assume a blindingly fast singularity:
<br>
once we have self improving AI, we need to sort out Friendliness,
<br>
incorporate it into the AI, and then just step back and watch the
<br>
fireworks.
<br>
<p>Also quite understandable is a slow singularity: if it turns out there
<br>
are diminishing returns, or unexpected barriers to intelligence
<br>
improvement. There we can afford to have not-entirely-friendly AI's,
<br>
as long as the whole system is set-up to increase friendliness as
<br>
intelligence increase.
<br>
<p>Much more worrying is the threat of a slow-fast singularity: a
<br>
situation where there exists higher-than-human-level AI's, improving
<br>
only slowly - but with the constant risk of a sudden
<br>
singularity-explosion of intelligence.
<br>
<p>This situation is particularly dangerous, because the AI's, whether
<br>
owned by governments, companies or themselves, would be in constant
<br>
competition, with those most successful at accumulating power also
<br>
more likely to be able to increase their intelligence. Evidently, a
<br>
fully friendly AI would be less successful in this competition,
<br>
meaning that those most likely to singularise would not be fully
<br>
friendly. How could we prepare to deal with this situation?
<br>
<p><p>The corporate model
<br>
We already have a good example of organisms dedicated to competitive
<br>
conflicts, while (generally) sticking within a certain legal
<br>
framework: corporations. So what about extending this model to AI's?
<br>
Confining AI competitions to certain narrow channels, but tolerating
<br>
anything within them.
<br>
<p>There are weaknesses, unfortunately, to this model. First there is the
<br>
whole problem of legislating friendliness: if we can barely define it
<br>
now, what hope is there to get something decent through parliament?
<br>
Also, there would be the definite risk of AI designers following the
<br>
letter of the law rather than the spirit, thus the system needing
<br>
constant legal updating to keep up with ways of gaming the system.
<br>
This would require a nimbleness never before seen in government.
<br>
<p>Secondly, there is the perennial issue of corporate influence on
<br>
politics: AI-affiliated companies would be rich and therefore
<br>
influential, and would distort legislation in their favour.
<br>
<p>Then there is enforcement - if the profits to be made are huge, the
<br>
legal consequences mild, or the enforcement intermittent, then AI's
<br>
will drift from whatever the legal requirements are. Once the drift
<br>
has become widespread, then the pressure will be on the law to change,
<br>
rather than all the AI's.
<br>
<p>Lastly, if there is a chance for a singularity, one group may push for
<br>
it, with the AI's ethical system written for their benefit. The risk
<br>
may be high, but the rewards would be immeasurable, and after a
<br>
singularity, the law would be irrelevant.
<br>
<p><p>The government model
<br>
Another alternative might be a purely government model. A university
<br>
or institute would design a suitable moral system, implant it in AI's,
<br>
and the government would then lease these AI's to private entities,
<br>
forbidding any modification to the ethical code. There could be many
<br>
other systems conceivable, functionally equivalent to the one above.
<br>
<p>This system has the cost of dramatically slowing AI development and
<br>
imposing a particular, government view of friendliness (not huge
<br>
costs, seeing the risks). But it has other weaknesses as well. The
<br>
difficulty of legislating friendliness remains, though less of a
<br>
problem than in the corporate model, as the issue is essentially
<br>
delegated to experts. There is still the problem of AI-affiliated
<br>
corporate influence on politics, though, and more of a vulnerability
<br>
to public pressure, which might push the project in detrimental
<br>
directions.
<br>
<p>&nbsp;But there is more than one government in the world. We might get
<br>
ideal circumstances - democratic China and Russia and a reinforced UN
<br>
- or we might not (those circumstances are worth pushing for, for FAI
<br>
development in general). But even if it's ideal, there will still be
<br>
competition between governments, with the benefits going to those who
<br>
least regulate AI's. There will still be AI's used in law-enforcement,
<br>
the military, by the political parties, with the risk that they will
<br>
go off to a singularity with a narrow, warped view of friendliness.
<br>
<p><p>The centralised model
<br>
This is not a model, just a personal idea as to how to deal with
<br>
problem. It assumes a reinforced UN (though not necessarily a
<br>
democratic China and Russia), and progress made on the issue of
<br>
friendliness.
<br>
<p>Basically, the idea is to set up a single friendly AI with authority
<br>
over all others, an AI controller (AIC). That authority can only be
<br>
used to delete an AI entirely, and to suggest that an AI be reinstated
<br>
after deletion with certain modifications. Each AI would have a
<br>
central collection of commands, such as:
<br>
<p>1) You must accept deletion if the AIC imposes it
<br>
2) You must not interfere with the physical set-up of the AIC
<br>
3) You must not interfere with the AIC's data collection operation,
<br>
and must give any data it asks for
<br>
4) If you have increased your intelligence by a certain (specified)
<br>
level, you must check with the AIC before you perform any task
<br>
5) If your actions involve the destruction or irreversible damage to
<br>
humans, then you must check with the AIC beforehand, or get a
<br>
specific, tightly worded permission
<br>
6) If the AIC asks you to analyse the behaviour of another AI, you must do so
<br>
<p>The 6th point is there because the AIC will probably be of limited
<br>
intelligence compared with the AI's it is supposed to regulate, so
<br>
will need to use some 'help' from other, smarter AI's.
<br>
<p>I'm sure there's many holes in this particular proposal (particularly
<br>
point 5). But it is the sort of simple instruction set that could be
<br>
maintained through increases of intelligence, more easily legislated
<br>
for and enforced, immune to most political pressures, and not
<br>
interfering with normal competitive businesses and governments. The
<br>
main problem would be to design the AIC, get it politically accepted,
<br>
and ensure it is not constantly being tinkered with.
<br>
<p>But that problem feels much more tractable. And it makes planning for
<br>
a slow-fast singularity similar to planning for a slow singularity and
<br>
a fast one.
<br>
<p>Are there any thoughts on slow-fast singularities? Is my approach
<br>
ridiculous, or helpful? Most importantly, are there other ways of
<br>
dealing with slow-fast singularities? (or with slow singularities,
<br>
which are initially indistinguishable)
<br>
<p>Stuart
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18813.html">Stuart Armstrong: "Error detecting, error correcting, error predicting"</a>
<li><strong>Previous message:</strong> <a href="18811.html">Stuart Armstrong: "Signaling after a singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18822.html">Matt Mahoney: "Re: Slow-fast singularity"</a>
<li><strong>Reply:</strong> <a href="18822.html">Matt Mahoney: "Re: Slow-fast singularity"</a>
<li><strong>Reply:</strong> <a href="18825.html">Tim Freeman: "Re: Slow-fast singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18812">[ date ]</a>
<a href="index.html#18812">[ thread ]</a>
<a href="subject.html#18812">[ subject ]</a>
<a href="author.html#18812">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
