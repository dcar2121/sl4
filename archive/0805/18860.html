<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Slow-fast singularity</title>
<meta name="Author" content="Krekoski Ross (rosskrekoski@gmail.com)">
<meta name="Subject" content="Re: Slow-fast singularity">
<meta name="Date" content="2008-05-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Slow-fast singularity</h1>
<!-- received="Thu May  8 21:52:40 2008" -->
<!-- isoreceived="20080509035240" -->
<!-- sent="Thu, 8 May 2008 19:26:18 +0000" -->
<!-- isosent="20080508192618" -->
<!-- name="Krekoski Ross" -->
<!-- email="rosskrekoski@gmail.com" -->
<!-- subject="Re: Slow-fast singularity" -->
<!-- id="87478b5d0805081226i3a45b1f3xf87b677fc6146648@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="87478b5d0805081223w7cb26749i3bf30ecb7df800ab@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Krekoski Ross (<a href="mailto:rosskrekoski@gmail.com?Subject=Re:%20Slow-fast%20singularity"><em>rosskrekoski@gmail.com</em></a>)<br>
<strong>Date:</strong> Thu May 08 2008 - 13:26:18 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="18861.html">Matt Mahoney: "Re: Error detecting, error correcting, error predicting"</a>
<li><strong>Previous message:</strong> <a href="18859.html">Krekoski Ross: "Re: Slow-fast singularity"</a>
<li><strong>In reply to:</strong> <a href="18859.html">Krekoski Ross: "Re: Slow-fast singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18863.html">charles griffiths: "Re: Slow-fast singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18860">[ date ]</a>
<a href="index.html#18860">[ thread ]</a>
<a href="subject.html#18860">[ subject ]</a>
<a href="author.html#18860">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
And, if we want to throttle the speed of intelligence increase, all we need
<br>
to do is limit the input.
<br>
<p>Ross
<br>
<p>On Thu, May 8, 2008 at 7:23 PM, Krekoski Ross &lt;<a href="mailto:rosskrekoski@gmail.com?Subject=Re:%20Slow-fast%20singularity">rosskrekoski@gmail.com</a>&gt;
<br>
wrote:
<br>
<p><em>&gt; Actually, now that I think about it-- this precludes a fast take-off with
</em><br>
<em>&gt; no end.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Any intelligent system cannot meaningfully increase its own complexity
</em><br>
<em>&gt; without a lot of input, or otherwise assimilating complexity and adding it
</em><br>
<em>&gt; to its own.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Consider I have an AI of a given complexity, lets call it takeoff(n) where
</em><br>
<em>&gt; n is the number of iterations we would like it to run for, conceivably a
</em><br>
<em>&gt; very large number.  takeoff() itself has a specific complexity.
</em><br>
<em>&gt;
</em><br>
<em>&gt; however, without a large number of inputs, takeoff() cannot increase its
</em><br>
<em>&gt; own complexity, since the complexity it reaches at any point in time is
</em><br>
<em>&gt; describable with its own function, and a specific integer as an argument.
</em><br>
<em>&gt;
</em><br>
<em>&gt; any AI can therefore really only at most, assimilate the complexity of the
</em><br>
<em>&gt; sum of all human knowledge. Without the ability to meaningfully interact
</em><br>
<em>&gt; with its environment and effectively assimilate information that is
</em><br>
<em>&gt; otherwise external to the sum of human knowledge, it will plateau.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ross
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; On Thu, May 8, 2008 at 6:43 PM, Krekoski Ross &lt;<a href="mailto:rosskrekoski@gmail.com?Subject=Re:%20Slow-fast%20singularity">rosskrekoski@gmail.com</a>&gt;
</em><br>
<em>&gt; wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; On Thu, May 8, 2008 at 8:59 AM, Stuart Armstrong &lt;
</em><br>
<em>&gt;&gt; <a href="mailto:dragondreaming@googlemail.com?Subject=Re:%20Slow-fast%20singularity">dragondreaming@googlemail.com</a>&gt; wrote:
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; What makes you claim that? We have little understanding of
</em><br>
<em>&gt;&gt;&gt; intelligence; we don't know how easy or hard increases in intelligence
</em><br>
<em>&gt;&gt;&gt; will turn out to be; we're not even certain how high the advantages of
</em><br>
<em>&gt;&gt;&gt; increased intelligence will turn out to be.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; It could be a series of increasing returns, and the advantages could
</em><br>
<em>&gt;&gt;&gt; be huge - but we really don't know that. &quot;Most likely scenario&quot; is
</em><br>
<em>&gt;&gt;&gt; much to strong a thing to say.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Yes.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; I personally dont have a strong opinion on the probability of either
</em><br>
<em>&gt;&gt; scenario just because there are so many unknowns, and we have an effective
</em><br>
<em>&gt;&gt; sample size of 1 (ourselves) with which to base all of our understanding of
</em><br>
<em>&gt;&gt; intelligence.  But I think we should realize one thing--- is it only by
</em><br>
<em>&gt;&gt; incredible coincidence that our intelligence is at a level such that we can
</em><br>
<em>&gt;&gt; understand the formal properties of our brain, but are just below some
</em><br>
<em>&gt;&gt; 'magical' threshold that would allow us to mentally simulate what
</em><br>
<em>&gt;&gt; differences in subjective experience and intelligence a slight change in our
</em><br>
<em>&gt;&gt; architecture would entail, but just above the threshold where it would be
</em><br>
<em>&gt;&gt; possible to do so for 'lower' entities?
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; I've mentioned this before in various forms but in general I think its a
</em><br>
<em>&gt;&gt; fairly under-addressed topic: Can an intelligent system of complexity A,
</em><br>
<em>&gt;&gt; perfectly emulate (perform a test run of) an intelligent system of
</em><br>
<em>&gt;&gt; complexity A? (for fairly obvious reasons it cannot emulate one of higher
</em><br>
<em>&gt;&gt; complexity). It seems possible that an intelligent system of complexity A
</em><br>
<em>&gt;&gt; can emulate one of complexity A-K where K is the output of some function
</em><br>
<em>&gt;&gt; that describes some proportion of A (we dont know specifically how
</em><br>
<em>&gt;&gt; complexity in an intelligent system affects intelligence, except that in a
</em><br>
<em>&gt;&gt; perfectly designed machine, an increase in complexity will entail an
</em><br>
<em>&gt;&gt; increase in intelligence).  I think that because of natural systemic
</em><br>
<em>&gt;&gt; overhead, it is impossible for any perfectly designed intelligent system to
</em><br>
<em>&gt;&gt; properly model another system of equal complexity. (and indeed no effective
</em><br>
<em>&gt;&gt; way to evaluate the model if it could)
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; This has implications on the rate at which any AI can self-improve-- if K
</em><br>
<em>&gt;&gt; is a reasonably significant proportion of A, even a godlike AI would have
</em><br>
<em>&gt;&gt; difficulty improving its own intelligence in an efficient and rapid way.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; This is also why evolution by random mutation is a slow, but actually
</em><br>
<em>&gt;&gt; quite efficient way of increasing intelligence--- we dont want a
</em><br>
<em>&gt;&gt; progressively larger, but structurally homogenous system (which actually is
</em><br>
<em>&gt;&gt; not an efficient increase in complexity, only size). We want structural
</em><br>
<em>&gt;&gt; diversity in an intelligent system, and its not clear how a system can
</em><br>
<em>&gt;&gt; 'invent' novel structures that is completely foreign to it. Many of our own
</em><br>
<em>&gt;&gt; advances in science, by analogy, arise from mimicry of for example non-human
</em><br>
<em>&gt;&gt; biological systems.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Ross
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; Stuart
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18861.html">Matt Mahoney: "Re: Error detecting, error correcting, error predicting"</a>
<li><strong>Previous message:</strong> <a href="18859.html">Krekoski Ross: "Re: Slow-fast singularity"</a>
<li><strong>In reply to:</strong> <a href="18859.html">Krekoski Ross: "Re: Slow-fast singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18863.html">charles griffiths: "Re: Slow-fast singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18860">[ date ]</a>
<a href="index.html#18860">[ thread ]</a>
<a href="subject.html#18860">[ subject ]</a>
<a href="author.html#18860">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
