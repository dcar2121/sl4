<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Slow-fast singularity</title>
<meta name="Author" content="Krekoski Ross (rosskrekoski@gmail.com)">
<meta name="Subject" content="Re: Slow-fast singularity">
<meta name="Date" content="2008-05-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Slow-fast singularity</h1>
<!-- received="Thu May  8 21:50:05 2008" -->
<!-- isoreceived="20080509035005" -->
<!-- sent="Thu, 8 May 2008 19:23:07 +0000" -->
<!-- isosent="20080508192307" -->
<!-- name="Krekoski Ross" -->
<!-- email="rosskrekoski@gmail.com" -->
<!-- subject="Re: Slow-fast singularity" -->
<!-- id="87478b5d0805081223w7cb26749i3bf30ecb7df800ab@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="87478b5d0805081143o5b952350ua0128a64e4a385f6@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Krekoski Ross (<a href="mailto:rosskrekoski@gmail.com?Subject=Re:%20Slow-fast%20singularity"><em>rosskrekoski@gmail.com</em></a>)<br>
<strong>Date:</strong> Thu May 08 2008 - 13:23:07 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="18860.html">Krekoski Ross: "Re: Slow-fast singularity"</a>
<li><strong>Previous message:</strong> <a href="18858.html">Krekoski Ross: "Re: Slow-fast singularity"</a>
<li><strong>In reply to:</strong> <a href="18858.html">Krekoski Ross: "Re: Slow-fast singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18860.html">Krekoski Ross: "Re: Slow-fast singularity"</a>
<li><strong>Reply:</strong> <a href="18860.html">Krekoski Ross: "Re: Slow-fast singularity"</a>
<li><strong>Reply:</strong> <a href="18863.html">charles griffiths: "Re: Slow-fast singularity"</a>
<li><strong>Reply:</strong> <a href="18867.html">Matt Mahoney: "Re: Slow-fast singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18859">[ date ]</a>
<a href="index.html#18859">[ thread ]</a>
<a href="subject.html#18859">[ subject ]</a>
<a href="author.html#18859">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Actually, now that I think about it-- this precludes a fast take-off with no
<br>
end.
<br>
<p>Any intelligent system cannot meaningfully increase its own complexity
<br>
without a lot of input, or otherwise assimilating complexity and adding it
<br>
to its own.
<br>
<p>Consider I have an AI of a given complexity, lets call it takeoff(n) where n
<br>
is the number of iterations we would like it to run for, conceivably a very
<br>
large number.  takeoff() itself has a specific complexity.
<br>
<p>however, without a large number of inputs, takeoff() cannot increase its own
<br>
complexity, since the complexity it reaches at any point in time is
<br>
describable with its own function, and a specific integer as an argument.
<br>
<p>any AI can therefore really only at most, assimilate the complexity of the
<br>
sum of all human knowledge. Without the ability to meaningfully interact
<br>
with its environment and effectively assimilate information that is
<br>
otherwise external to the sum of human knowledge, it will plateau.
<br>
<p>Ross
<br>
<p><p><p>On Thu, May 8, 2008 at 6:43 PM, Krekoski Ross &lt;<a href="mailto:rosskrekoski@gmail.com?Subject=Re:%20Slow-fast%20singularity">rosskrekoski@gmail.com</a>&gt;
<br>
wrote:
<br>
<p><em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; On Thu, May 8, 2008 at 8:59 AM, Stuart Armstrong &lt;
</em><br>
<em>&gt; <a href="mailto:dragondreaming@googlemail.com?Subject=Re:%20Slow-fast%20singularity">dragondreaming@googlemail.com</a>&gt; wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; What makes you claim that? We have little understanding of
</em><br>
<em>&gt;&gt; intelligence; we don't know how easy or hard increases in intelligence
</em><br>
<em>&gt;&gt; will turn out to be; we're not even certain how high the advantages of
</em><br>
<em>&gt;&gt; increased intelligence will turn out to be.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; It could be a series of increasing returns, and the advantages could
</em><br>
<em>&gt;&gt; be huge - but we really don't know that. &quot;Most likely scenario&quot; is
</em><br>
<em>&gt;&gt; much to strong a thing to say.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I personally dont have a strong opinion on the probability of either
</em><br>
<em>&gt; scenario just because there are so many unknowns, and we have an effective
</em><br>
<em>&gt; sample size of 1 (ourselves) with which to base all of our understanding of
</em><br>
<em>&gt; intelligence.  But I think we should realize one thing--- is it only by
</em><br>
<em>&gt; incredible coincidence that our intelligence is at a level such that we can
</em><br>
<em>&gt; understand the formal properties of our brain, but are just below some
</em><br>
<em>&gt; 'magical' threshold that would allow us to mentally simulate what
</em><br>
<em>&gt; differences in subjective experience and intelligence a slight change in our
</em><br>
<em>&gt; architecture would entail, but just above the threshold where it would be
</em><br>
<em>&gt; possible to do so for 'lower' entities?
</em><br>
<em>&gt;
</em><br>
<em>&gt; I've mentioned this before in various forms but in general I think its a
</em><br>
<em>&gt; fairly under-addressed topic: Can an intelligent system of complexity A,
</em><br>
<em>&gt; perfectly emulate (perform a test run of) an intelligent system of
</em><br>
<em>&gt; complexity A? (for fairly obvious reasons it cannot emulate one of higher
</em><br>
<em>&gt; complexity). It seems possible that an intelligent system of complexity A
</em><br>
<em>&gt; can emulate one of complexity A-K where K is the output of some function
</em><br>
<em>&gt; that describes some proportion of A (we dont know specifically how
</em><br>
<em>&gt; complexity in an intelligent system affects intelligence, except that in a
</em><br>
<em>&gt; perfectly designed machine, an increase in complexity will entail an
</em><br>
<em>&gt; increase in intelligence).  I think that because of natural systemic
</em><br>
<em>&gt; overhead, it is impossible for any perfectly designed intelligent system to
</em><br>
<em>&gt; properly model another system of equal complexity. (and indeed no effective
</em><br>
<em>&gt; way to evaluate the model if it could)
</em><br>
<em>&gt;
</em><br>
<em>&gt; This has implications on the rate at which any AI can self-improve-- if K
</em><br>
<em>&gt; is a reasonably significant proportion of A, even a godlike AI would have
</em><br>
<em>&gt; difficulty improving its own intelligence in an efficient and rapid way.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is also why evolution by random mutation is a slow, but actually quite
</em><br>
<em>&gt; efficient way of increasing intelligence--- we dont want a progressively
</em><br>
<em>&gt; larger, but structurally homogenous system (which actually is not an
</em><br>
<em>&gt; efficient increase in complexity, only size). We want structural diversity
</em><br>
<em>&gt; in an intelligent system, and its not clear how a system can 'invent' novel
</em><br>
<em>&gt; structures that is completely foreign to it. Many of our own advances in
</em><br>
<em>&gt; science, by analogy, arise from mimicry of for example non-human biological
</em><br>
<em>&gt; systems.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ross
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Stuart
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18860.html">Krekoski Ross: "Re: Slow-fast singularity"</a>
<li><strong>Previous message:</strong> <a href="18858.html">Krekoski Ross: "Re: Slow-fast singularity"</a>
<li><strong>In reply to:</strong> <a href="18858.html">Krekoski Ross: "Re: Slow-fast singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18860.html">Krekoski Ross: "Re: Slow-fast singularity"</a>
<li><strong>Reply:</strong> <a href="18860.html">Krekoski Ross: "Re: Slow-fast singularity"</a>
<li><strong>Reply:</strong> <a href="18863.html">charles griffiths: "Re: Slow-fast singularity"</a>
<li><strong>Reply:</strong> <a href="18867.html">Matt Mahoney: "Re: Slow-fast singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18859">[ date ]</a>
<a href="index.html#18859">[ thread ]</a>
<a href="subject.html#18859">[ subject ]</a>
<a href="author.html#18859">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
