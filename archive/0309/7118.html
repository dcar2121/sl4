<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: My doubts about Libertarianism and volitional morality</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: My doubts about Libertarianism and volitional morality">
<meta name="Date" content="2003-09-11">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: My doubts about Libertarianism and volitional morality</h1>
<!-- received="Thu Sep 11 14:55:37 2003" -->
<!-- isoreceived="20030911205537" -->
<!-- sent="Thu, 11 Sep 2003 16:55:24 -0400" -->
<!-- isosent="20030911205524" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: My doubts about Libertarianism and volitional morality" -->
<!-- id="3F60E13C.4080004@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20030911065158.33398.qmail@web20206.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20My%20doubts%20about%20Libertarianism%20and%20volitional%20morality"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu Sep 11 2003 - 14:55:24 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7119.html">James Rogers: "RE: Progress, and One road or many to AI?"</a>
<li><strong>Previous message:</strong> <a href="7117.html">Eliezer S. Yudkowsky: "Re: Progress, and One road or many to AI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7137.html">Wei Dai: "Re: My doubts about Libertarianism and volitional morality"</a>
<li><strong>Reply:</strong> <a href="7137.html">Wei Dai: "Re: My doubts about Libertarianism and volitional morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7118">[ date ]</a>
<a href="index.html#7118">[ thread ]</a>
<a href="subject.html#7118">[ subject ]</a>
<a href="author.html#7118">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Marc Geddes wrote:
<br>
<p><em>&gt; I've recently realized what I think the major problem
</em><br>
<em>&gt; with Libertarianism and volitional morality is.
</em><br>
<em>&gt; [snip]
</em><br>
<em>&gt; In fact, I fear that the problem of incomplete
</em><br>
<em>&gt; information throws the whole theory of volitional
</em><br>
<em>&gt; morality into doubt.  People cannot be said be making
</em><br>
<em>&gt; free choices when full relevant information is not
</em><br>
<em>&gt; available.
</em><br>
<em>&gt; [snip]
</em><br>
<em>&gt; I think the problem of incomplete information is a far
</em><br>
<em>&gt; greater problem for Libertarianism and volitional
</em><br>
<em>&gt; morality than I had first thought. 
</em><br>
<p>You're confusing Libertarianism and volitionism.  They are quite different 
<br>
things.
<br>
<p>Volitionism is a philosophy whose prime purpose is to explicitly raise and 
<br>
address challenges like incomplete information, the fact that people's 
<br>
moral codes change over time, the inability of people to predict their own 
<br>
preferences, preference reversals, nontransitive preference orderings, 
<br>
duration insensitivity, and so on.  &quot;How, given a human, do you say what 
<br>
he or she 'wants'?&quot; is the FAI-complete question that volitionism is meant 
<br>
to address.
<br>
<p>In the case of incomplete information, the problem is relatively 
<br>
straightforward.  The von Neumann-Morgenstern expected utility equation is:
<br>
<p>D(a) = Sum over all x:  U(x)P(x|a).
<br>
<p>If your decisionmaking obeys a few plausible-sounding axioms (which, 
<br>
naturally, real people violate all over the place), the desirability of an 
<br>
action will equal the sum, over all X of interest, of &quot;the utility of X, 
<br>
times the probability of X given that the decision system chooses A&quot;.
<br>
<p>Suppose we assume that X has an objective frequency given A, F(x|a).  If 
<br>
the subjective frequency assigned by the person to P(x|a) doesn't match 
<br>
the objective frequency F(x|a), then to a first approximation we can say 
<br>
that the subject's &quot;volition&quot; as a moral desideratum should be computed 
<br>
using U(x)F(x|a), while the subject's actual decisions will in fact be 
<br>
computed using U(x)P(x|a).  In other words, your &quot;volition&quot; is an abstract 
<br>
entity which your actual decisions only approximate; your volition is the 
<br>
decision you would make if you had perfect information.
<br>
<p>Really people are a lot more complicated than this, so for Belldandy's 
<br>
sake don't go plugging the above into a Friendly AI.  People have complex 
<br>
structure in the utility computation U(x), which is something that most 
<br>
accounts of expected utility don't take into account at all.  All kinds of 
<br>
assumptions, such as the time invariance of U(x) or separability of U(x) 
<br>
or perfect knowledge of U(x), have been invisibly glossed over by one 
<br>
philosopher or another.  But for the particular question of incomplete 
<br>
information affecting the binding of ends to means, the construction of 
<br>
volition is *relatively* straightforward; the volition is the decision 
<br>
that people would make if they had the missing information.
<br>
<p>It rapidly gets more complex, but for the *particular case* of the 
<br>
means-end mismatch you cite:
<br>
<p><em>&gt; Suppose I pointed to shelf of boxes and said that in
</em><br>
<em>&gt; one box was millions of dollars, and the other boxes
</em><br>
<em>&gt; were empty.  I then allowed someone to 'choose' which
</em><br>
<em>&gt; box they wanted, telling him or her that if they pick
</em><br>
<em>&gt; the box containing the money they can keep it.   They
</em><br>
<em>&gt; pick a box - it's empty.  All fair?  Suppose that it
</em><br>
<em>&gt; turned out that the box with the money had been
</em><br>
<em>&gt; deliberately placed in shadow, so that the person
</em><br>
<em>&gt; doing the choosing didn't see it.  So I didn't lie
</em><br>
<em>&gt; exactly, but clearly information was concealed from
</em><br>
<em>&gt; the person doing the choosing.
</em><br>
<p>Here the missing information is clear; the &quot;correct&quot; choice that the 
<br>
person's decision system is trying and failing to approximate is clear; 
<br>
the math involved is clear (the person's P($1M|box13) is 0.05, while 
<br>
F($1M|box13) is 1, and the person's decision process converges to box13 as 
<br>
P converges to F); and nobody is likely to object if you point out the 
<br>
correct box to them, or even if you preemptively choose the &quot;correct&quot; box 
<br>
for them on the basis of your more informed world-model and your guess as 
<br>
to their utility function; though they might start to worry if you made a 
<br>
habit of it.
<br>
<p>Regardless of which box the person chooses, the person wants the box with 
<br>
the money, and in a volitional sense can be said to &quot;want&quot; box 13 even 
<br>
though no specific representation of box 13 appears in the person's mind. 
<br>
&nbsp;&nbsp;The person is not likely to object to your construal of that abstract 
<br>
fact, or to your assistance in obtaining the goal, at least if you're an 
<br>
ordinary human-level friend.  And this answer is nicely consonant with 
<br>
both our intuitions about what it means to help someone, and our intutions 
<br>
about what it means to be helped.
<br>
<p>So the *particular* problem you cite is straightforward in volitionism. 
<br>
Obviously there are more complicated ones.
<br>
<p><em>&gt; Just how far do you carry volitional morality?
</em><br>
<p>I don't know that I would say you can draw the line wherever you like, but 
<br>
since you are yourself and I am not, you *will in fact* draw the line 
<br>
wherever you choose, whether I like it or not.  You may decide that 
<br>
homosexuality is immoral, for example.  But why would I, as a third party, 
<br>
pay attention to you, as a fourth party, saying that you've decided it's 
<br>
wrong, if the first and second parties, Susan and Debby, decide to get 
<br>
married, and they both appear happy with their choices?  How can 
<br>
desiderata that violate volitional morality take wings and become 
<br>
transpersonal?
<br>
<p><em>&gt; Suppose a group of people decided that they wanted to
</em><br>
<em>&gt; start making snuff movies in the middle of the road. 
</em><br>
<em>&gt; Every person in the group agrees to it of their own
</em><br>
<em>&gt; free will (the subjects are happy to commit suicide). 
</em><br>
<em>&gt; Volitional morality is obeyed.  Should we let them
</em><br>
<em>&gt; start making the snuff movies in the street?  To me
</em><br>
<em>&gt; the answer is an obvious no, and in this extreme
</em><br>
<em>&gt; example volitional morality looks absurd.
</em><br>
<p>Why is the answer obviously no?  Who are you to determine whether someone 
<br>
else lives or dies?  I don't think the answer is obviously yes, but 
<br>
&quot;obviously no&quot;?
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7119.html">James Rogers: "RE: Progress, and One road or many to AI?"</a>
<li><strong>Previous message:</strong> <a href="7117.html">Eliezer S. Yudkowsky: "Re: Progress, and One road or many to AI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7137.html">Wei Dai: "Re: My doubts about Libertarianism and volitional morality"</a>
<li><strong>Reply:</strong> <a href="7137.html">Wei Dai: "Re: My doubts about Libertarianism and volitional morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7118">[ date ]</a>
<a href="index.html#7118">[ thread ]</a>
<a href="subject.html#7118">[ subject ]</a>
<a href="author.html#7118">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
