<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: All sentient have to be observer-centered!  My theory of FAI morality</title>
<meta name="Author" content="Marc Geddes (marc_geddes@yahoo.co.nz)">
<meta name="Subject" content="Re: All sentient have to be observer-centered!  My theory of FAI morality">
<meta name="Date" content="2004-02-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: All sentient have to be observer-centered!  My theory of FAI morality</h1>
<!-- received="Sat Feb 28 22:49:00 2004" -->
<!-- isoreceived="20040229054900" -->
<!-- sent="Sun, 29 Feb 2004 18:48:53 +1300 (NZDT)" -->
<!-- isosent="20040229054853" -->
<!-- name="Marc Geddes" -->
<!-- email="marc_geddes@yahoo.co.nz" -->
<!-- subject="Re: All sentient have to be observer-centered!  My theory of FAI morality" -->
<!-- id="20040229054853.31929.qmail@web20203.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20040227114819.96329.qmail@web11701.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marc Geddes (<a href="mailto:marc_geddes@yahoo.co.nz?Subject=Re:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality"><em>marc_geddes@yahoo.co.nz</em></a>)<br>
<strong>Date:</strong> Sat Feb 28 2004 - 22:48:53 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8082.html">Marc Geddes: "The Fundamental Theorem of Morality"</a>
<li><strong>Previous message:</strong> <a href="8080.html">Marc Geddes: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>In reply to:</strong> <a href="8077.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8088.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8088.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8081">[ date ]</a>
<a href="index.html#8081">[ thread ]</a>
<a href="subject.html#8081">[ subject ]</a>
<a href="author.html#8081">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;--- Tommy McCabe &lt;<a href="mailto:rocketjet314@yahoo.com?Subject=Re:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality">rocketjet314@yahoo.com</a>&gt; wrote: &gt;
<br>
You say that moralities 'consistent' with each other
<br>
<em>&gt; don't have to be identical. They do. Morality isn't
</em><br>
<em>&gt; mathematics. In order for them to be consistent,
</em><br>
<em>&gt; they
</em><br>
<em>&gt; have to give the same result in every situation, in
</em><br>
<em>&gt; other words, they must be identical. 'I like X'
</em><br>
<em>&gt; isn't
</em><br>
<em>&gt; really a consistent morality with 'Do not kill',
</em><br>
<em>&gt; since
</em><br>
<em>&gt; given the former, one would kill to get X. I don't
</em><br>
<em>&gt; like the idea of an AI acting like a human, ie, of
</em><br>
<em>&gt; having heuristics of 'Coke is better tha Pepsi' for
</em><br>
<em>&gt; no
</em><br>
<em>&gt; good reason. Of course, if their is a good reason, a
</em><br>
<em>&gt; Yudkowskian FAI would have that anyway. You may take
</em><br>
<em>&gt; the 'personal component of morality is necessary'
</em><br>
<em>&gt; thing as an axiom, but I don't and I need to see
</em><br>
<em>&gt; some
</em><br>
<em>&gt; proof.
</em><br>
<p>O.K, 'conisistent with' wasn't a good word to use as
<br>
regards moralities.  But I think you know what I
<br>
meant.  Perhaps 'congruent with' would be a better
<br>
term.  
<br>
<p>I could define morality Y as being congruent with
<br>
moralitity X, if in most situations, Y did not
<br>
conflict with X.  And if in the situations where Y did
<br>
conflict, X took priority.
<br>
<p>So for instance, say morality X was 'Thou shall not
<br>
kill', and morality Y was 'Coke is Good, Pepsi is
<br>
Evil'.  Y is congruent with X if a sentient can pursue
<br>
Y without conflicting with X  (The sentient looks to
<br>
promote Coke, but without killing anyone).
<br>
<p>The reason I think a 'Personal Morality' component is
<br>
neccessery, is that WE DON'T KNOW what the Universal
<br>
Morality component is.  It might be 'Volitional
<br>
Morality', but that's just Eliezer's guess.  FAI's are
<br>
designed to try to reason out Universal Morality for
<br>
themselves.  Programmers don't know what it is in
<br>
advance.  It's unlikely they'd get it exactly right to
<br>
begin with.  So, in the beginning some of what we
<br>
teach an FAI will be wrong.  The part which is wrong
<br>
will be just arbitrary (Personal Morality).  So you
<br>
see, all FAI's WILL have a 'Personal Morality'
<br>
component to start with.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &quot;Well yeah true, a Yudkowskian FAI would of course
</em><br>
<em>&gt; refuse requests to hurt other people.  But it would
</em><br>
<em>&gt; aim to fulfil ALL requests consistent with volition.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; (All requests which don't involve violating other
</em><br>
<em>&gt; peoples right).&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And that's a bad thing? You really don't want an AI
</em><br>
<em>&gt; deciding not to fulfill Pepsi requests because it
</em><br>
<em>&gt; thinks Coke is better for no good reason- that leads
</em><br>
<em>&gt; to an AI not wanting to fulfill Singularity requests
</em><br>
<em>&gt; because suffering is better.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &quot;For instance, 'I want to go ice skating', 'I want a
</em><br>
<em>&gt; Pepsi', 'I want some mountain climbing qquipment'
</em><br>
<em>&gt; and
</em><br>
<em>&gt; so on and so on.  A Yudkowskian FAI can't draw any
</em><br>
<em>&gt; distinctions between these, and would see all of
</em><br>
<em>&gt; them
</em><br>
<em>&gt; as equally 'good'.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It wouldn't- at all. A Yudkowskian FAI, especially a
</em><br>
<em>&gt; transhuman one, could easily apply Bayes' Theorem
</em><br>
<em>&gt; and
</em><br>
<em>&gt; such, and see what the possible outcomes are, and
</em><br>
<em>&gt; their porbabilities, for each event. They certainly
</em><br>
<em>&gt; aren't identical!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &quot;But an FAI with a 'Personal Morality' component,
</em><br>
<em>&gt; would
</em><br>
<em>&gt; not neccesserily fulfil all of these requests.  For
</em><br>
<em>&gt; instance an FAI that had a personal morality
</em><br>
<em>&gt; component
</em><br>
<em>&gt; 'Coke is good, Pepsi is evil' would refuse to fulfil
</em><br>
<em>&gt; a
</em><br>
<em>&gt; request for Pepsi.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; That is a bad thing!!! AIs shouldn't arbitrarily
</em><br>
<em>&gt; decide to refuse Pepsi- eventually the AI is then
</em><br>
<em>&gt; going to arbitrarily refuse survival. And yes, it is
</em><br>
<em>&gt; arbitrary, because if it isn't arbitrary than the
</em><br>
<em>&gt; Yudkowskian FAI would have it in the first place!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &quot;The 'Personal morality' component
</em><br>
<em>&gt; would tell an FAI what it SHOULD do, the 'Universal
</em><br>
<em>&gt; morality' componanet is concerned with what an FAI
</em><br>
<em>&gt; SHOULDN'T do.  A Yudkowskian FAI would be unable to
</em><br>
<em>&gt; draw this distinction, since it would have no
</em><br>
<em>&gt; 'Personal Morality'  (Remember a Yudkowskian FAI is
</em><br>
<em>&gt; entirely non-observer centerd, and so it could only
</em><br>
<em>&gt; have Universal Morality).&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Quite wrong. Even Eurisko could tell the difference
</em><br>
<em>&gt; between &quot;Don't do A&quot; and &quot;Do A&quot;. And check your
</em><br>
<em>&gt; spelling.
</em><br>
<p>Sorry.  What I meant was that the FAI can't
<br>
distinguigh between 'Acts and Omissions' (read up on
<br>
moral philosophy for an explanation).  
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &quot;You could say that a
</em><br>
<em>&gt; Yudkowskian FAI just views everything that doesn't
</em><br>
<em>&gt; hurt others as equal, where as an FAI with an extra
</em><br>
<em>&gt; oberver centered component would have some extra
</em><br>
<em>&gt; personal principles.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1. No one ever said that. Straw man.
</em><br>
<em>&gt; 2. Arbitrary principles thrown in with morality are
</em><br>
<em>&gt; bad things.
</em><br>
<p><p><em>&gt; 
</em><br>
<em>&gt; &quot;Yeah, yeah, true, but an FAI with a 'Personal
</em><br>
<em>&gt; Morality' would have some additional goals on top of
</em><br>
<em>&gt; this.  A Yudkowskian FAI does of course have the
</em><br>
<em>&gt; goals
</em><br>
<em>&gt; 'aim to do things that help with the fulfilment of
</em><br>
<em>&gt; sentient requests'.  But that's all.  An FAI with an
</em><br>
<em>&gt; additional 'Personal Morality' component, would also
</em><br>
<em>&gt; have the Yudkowskian goals, but it would have some
</em><br>
<em>&gt; additional goals.  For instance the additinal
</em><br>
<em>&gt; personal
</em><br>
<em>&gt; morality 'Coke is good, Pepsi is evil' would lead
</em><br>
<em>&gt; the
</em><br>
<em>&gt; FAI to personally support 'Coke' goals (provided
</em><br>
<em>&gt; such
</em><br>
<em>&gt; goals did not contradict the Yudkowskian goals).&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It isn't a good thing to arbitarily stick moralities
</em><br>
<em>&gt; and goals into goal systems without justification.
</em><br>
<em>&gt; If
</em><br>
<em>&gt; there was justification, then it would be present in
</em><br>
<em>&gt; a
</em><br>
<em>&gt; Yudkowskian FAI. And 'Coke' goals would contradict
</em><br>
<em>&gt; Yudkowskian goals every time someone asked for a
</em><br>
<em>&gt; Pepsi.
</em><br>
<p>But ARE all 'arbitary' goals really a bad thing? 
<br>
Aren't such extra goals what makes life interesting? 
<br>
Do you prefer rock music or heavy metal?  Do you like
<br>
Chinese food or Sea food best?  What do you prefer: 
<br>
Modern art or Classical?  You could say that these
<br>
preferences are probably 'arbitrary', but they're
<br>
actually what marks us out as individuals and makes us
<br>
unique.
<br>
<p>If all of us simply pursued 'true' (normative,
<br>
Universal) morality, then all of us would be identical
<br>
(because all sentients by definition converge on the
<br>
same normative morality).
<br>
<p>Now in the example of an FAI with the additional
<br>
'arbitary' goal 'Coke is Good, Pepsi is Evil') in most
<br>
situations this would not conflict with Volition.  In
<br>
the specific circumstances where it did, Volition
<br>
could take precedence.  You can then say that the
<br>
additional morality is 'congruent with' (does not
<br>
conflict with) Volition.
<br>
<p>Why would an FAI refusing someone a Pepsi be bad?  The
<br>
FAI would not stop anyone drinking Pepsi if that's
<br>
what they wanted.  It would simply be refusing to
<br>
actively help them.  'Coke is good, Pepsi is bad'
<br>
would only contradict Volitional Morality if the FAI
<br>
actually tried to use force to stop people drinking
<br>
Pepsi.  So long as the FAI continued to tolerate
<br>
people drinking Pepsi, there is no conflict with
<br>
Volition.  You see the distinction between 'Acts and
<br>
Omissions'?  Actively helping someone do x, is not the
<br>
same thing as simply tolerating someone doing x.  
<br>
&nbsp;&nbsp;
<br>
<em>&gt; 
</em><br>
<em>&gt; &quot;I've given the general solution to the problem of
</em><br>
<em>&gt; FAI
</em><br>
<em>&gt; morality.  We don't know that 'Personal Morality'
</em><br>
<em>&gt; set
</em><br>
<em>&gt; to unity would be stable.  Therefore we have to
</em><br>
<em>&gt; consider the case where FAI's have to have a
</em><br>
<em>&gt; non-trival 'Personal Morality' component.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Non sequitur. That's like saying &quot;We don't know if
</em><br>
<em>&gt; car
</em><br>
<em>&gt; A will be stable with 100% certainty, so we have to
</em><br>
<em>&gt; take a look at car B that has large heaps of trash
</em><br>
<em>&gt; on
</em><br>
<em>&gt; it for no good reason&quot;
</em><br>
<em>&gt; 
</em><br>
See what I said above.  The programmers don't know in
<br>
advance what true (Universal) morality is.  So some of
<br>
what a FAI learns in the beginning will be wrong (and
<br>
so all FAI's will start with some arbitrary 'Personal
<br>
Morality' components thrown in).  
<br>
<p><p><p>=====
<br>
Please visit my web-site at:  <a href="http://www.prometheuscrack.com">http://www.prometheuscrack.com</a>
<br>
<p>Find local movie times and trailers on Yahoo! Movies.
<br>
<a href="http://au.movies.yahoo.com">http://au.movies.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8082.html">Marc Geddes: "The Fundamental Theorem of Morality"</a>
<li><strong>Previous message:</strong> <a href="8080.html">Marc Geddes: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>In reply to:</strong> <a href="8077.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8088.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8088.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8081">[ date ]</a>
<a href="index.html#8081">[ thread ]</a>
<a href="subject.html#8081">[ subject ]</a>
<a href="author.html#8081">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
