<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: All sentient have to be observer-centered!  My theory of FAI morality</title>
<meta name="Author" content="Marc Geddes (marc_geddes@yahoo.co.nz)">
<meta name="Subject" content="Re: All sentient have to be observer-centered!  My theory of FAI morality">
<meta name="Date" content="2004-02-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: All sentient have to be observer-centered!  My theory of FAI morality</h1>
<!-- received="Thu Feb 26 23:43:48 2004" -->
<!-- isoreceived="20040227064348" -->
<!-- sent="Fri, 27 Feb 2004 19:43:46 +1300 (NZDT)" -->
<!-- isosent="20040227064346" -->
<!-- name="Marc Geddes" -->
<!-- email="marc_geddes@yahoo.co.nz" -->
<!-- subject="Re: All sentient have to be observer-centered!  My theory of FAI morality" -->
<!-- id="20040227064346.14617.qmail@web20211.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="403EC54E.1010206@acceleratingfuture.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marc Geddes (<a href="mailto:marc_geddes@yahoo.co.nz?Subject=Re:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality"><em>marc_geddes@yahoo.co.nz</em></a>)<br>
<strong>Date:</strong> Thu Feb 26 2004 - 23:43:46 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8074.html">Marc Geddes: "Oops.  Slight correction to my last post..."</a>
<li><strong>Previous message:</strong> <a href="8072.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>In reply to:</strong> <a href="8071.html">Michael Anissimov: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8074.html">Marc Geddes: "Oops.  Slight correction to my last post..."</a>
<li><strong>Reply:</strong> <a href="8074.html">Marc Geddes: "Oops.  Slight correction to my last post..."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8073">[ date ]</a>
<a href="index.html#8073">[ thread ]</a>
<a href="subject.html#8073">[ subject ]</a>
<a href="author.html#8073">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;--- Michael Anissimov
<br>
&lt;<a href="mailto:michael@acceleratingfuture.com?Subject=Re:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality">michael@acceleratingfuture.com</a>&gt; wrote: &gt; Marc, have
<br>
you carefully read
<br>
<em>&gt; <a href="http://www.intelligence.org/CFAI/anthro.html">http://www.intelligence.org/CFAI/anthro.html</a>?  
</em><br>
<em>&gt; It poses very convincing rebuttals to what you are
</em><br>
<em>&gt; currently arguing.  
</em><br>
<p>I read them Mike.  I didn't find these 'rebuttals'
<br>
very convincing.
<br>
<p><em>&gt; Quote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &quot;There is no reason why an evolved goal system would
</em><br>
<em>&gt; be anything /but/ 
</em><br>
<em>&gt; observer-focused.  Since the days when we were
</em><br>
<em>&gt; competing chemical blobs, 
</em><br>
<em>&gt; the primary focus of selection has been the
</em><br>
<em>&gt; individual.  Even in cases 
</em><br>
<em>&gt; where fitness or inclusive fitness 
</em><br>
<em>&gt;
</em><br>
&lt;<a href="http://www.intelligence.org/CFAI/info/glossary.html#gloss_inclusive_reproductive_fitness">http://www.intelligence.org/CFAI/info/glossary.html#gloss_inclusive_reproductive_fitness</a>&gt;
<br>
<em>&gt; 
</em><br>
<em>&gt; is augmented by behaving nicely towards your
</em><br>
<em>&gt; children, your close 
</em><br>
<em>&gt; relatives, or your reciprocal-altruism trade
</em><br>
<em>&gt; partners, the selection 
</em><br>
<em>&gt; pressures are still spilling over onto /your/ kin,
</em><br>
<em>&gt; /your/ children, 
</em><br>
<em>&gt; /your/ partners.  We started out as competing blobs
</em><br>
<em>&gt; in a sea, each blob 
</em><br>
<em>&gt; with its own measure of fitness.  We grew into
</em><br>
<em>&gt; competing players in a 
</em><br>
<em>&gt; social network, each player with a different set of
</em><br>
<em>&gt; goals and subgoals, 
</em><br>
<em>&gt; sometimes overlapping, sometimes not.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; When selection pressures no longer adhere to single
</em><br>
<em>&gt; organisms strongly 
</em><br>
<em>&gt; (as in ant colonies), &quot;selfish&quot; behavior gets
</em><br>
<em>&gt; distributed across the 
</em><br>
<em>&gt; entire colony rather than each unique individual. 
</em><br>
<em>&gt; Many simple forms of 
</em><br>
<em>&gt; life share most or all of their genetic material
</em><br>
<em>&gt; with their bretheren, 
</em><br>
<em>&gt; and therefore behave in purely selfless ways.  This
</em><br>
<em>&gt; happens in a very 
</em><br>
<em>&gt; predictable way... if a selection pressure came into
</em><br>
<em>&gt; existence that 
</em><br>
<em>&gt; selected for genuine benevolence and/or pure
</em><br>
<em>&gt; selflessness, then 
</em><br>
<em>&gt; eventually the species in question would evolve that
</em><br>
<em>&gt; way.  But no such 
</em><br>
<em>&gt; selection pressures exist.  Here is something I once
</em><br>
<em>&gt; wrote on another list;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 'Try to imagine an animal that evolving for millions
</em><br>
<em>&gt; of years in an 
</em><br>
<em>&gt; environment where benevolence is the only effective
</em><br>
<em>&gt; survival strategy, 
</em><br>
<em>&gt; and while a certain amount of psuedoselfish behavior
</em><br>
<em>&gt; exists as a 
</em><br>
<em>&gt; coincidental subgoal of efficiency. I'm not saying
</em><br>
<em>&gt; that all beings in 
</em><br>
<em>&gt; the future should be forced to be like this, but
</em><br>
<em>&gt; it's just a thought 
</em><br>
<em>&gt; experiment to show that the existence of perfectly
</em><br>
<em>&gt; selfless beings could 
</em><br>
<em>&gt; be possible. If the selection pressures towards
</em><br>
<em>&gt; altruism were intense 
</em><br>
<em>&gt; enough, not only would benevolence be the only
</em><br>
<em>&gt; externally observable 
</em><br>
<em>&gt; behavior amongst these entities, but the *tendencies
</em><br>
<em>&gt; to resort to 
</em><br>
<em>&gt; egotism or notice opportunities for selfish deeds*
</em><br>
<em>&gt; would not be absent - 
</em><br>
<em>&gt; they would not even be cognitively capable of being
</em><br>
<em>&gt; egoistic unless they 
</em><br>
<em>&gt; performed neurosurgery on themselves (or whatever.)
</em><br>
<em>&gt; And why would they 
</em><br>
<em>&gt; want to do such a thing? They might use
</em><br>
<em>&gt; computational models to see what 
</em><br>
<em>&gt; it would have been like they had evolved more
</em><br>
<em>&gt; &quot;selfishly&quot;, (a vague, 
</em><br>
<em>&gt; theoretical, abstract concept to them) and see only
</em><br>
<em>&gt; war, 
</em><br>
<em>&gt; negative-sumness, and counterproductivity. One of
</em><br>
<em>&gt; the most disturbing 
</em><br>
<em>&gt; things they might notice is that such  a culture
</em><br>
<em>&gt; could develop memetic 
</em><br>
<em>&gt; patterns which act strongly to preserve the existing
</em><br>
<em>&gt; cognitive template, 
</em><br>
<em>&gt; and disbelieve proposed designs for minds reliably
</em><br>
<em>&gt; possessing 
</em><br>
<em>&gt; selflessness, even in the context of a highly
</em><br>
<em>&gt; selfish world.&quot;
</em><br>
<p>Arguments from evolution don't say that much. 
<br>
Firstly, there's a big difference between observation
<br>
and experiment.  Arguments basesd solely on
<br>
observation are notoriously unreliable.
<br>
<p>Secondly, the class of beings important for FAI
<br>
hypothesis are the SENTIENT beings  (Capable of
<br>
intelligent, abstract reflection).  It's only very
<br>
recently that sentients (humans) evolved. 
<br>
Observations based on non-sentients (like ants) aren't
<br>
likely to be that revelent.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Maybe you are conflating the idea of an
</em><br>
<em>&gt; observer-*instantiated* morality 
</em><br>
<em>&gt; and an observer-biased one.  Personal moralities
</em><br>
<em>&gt; will be instantiated in 
</em><br>
<em>&gt; some observer, by definition, but this doesn't mean
</em><br>
<em>&gt; that the observer is 
</em><br>
<em>&gt; necessarily *biased* towards him/herself.  Instead
</em><br>
<em>&gt; of asking whether 
</em><br>
<em>&gt; genuinely selfless beliefs and actions are
</em><br>
<em>&gt; &quot;logically possible&quot; 
</em><br>
<em>&gt; (philosophical appealing), maybe you should ask if
</em><br>
<em>&gt; they are physically 
</em><br>
<em>&gt; possible; given a long enough time, could a
</em><br>
<em>&gt; neuroscientist modify a 
</em><br>
<em>&gt; human brain such that the resulting human was almost
</em><br>
<em>&gt; entirely selfless?  
</em><br>
<em>&gt; Heck, there is already evidence that the drug E can
</em><br>
<em>&gt; compel people to act 
</em><br>
<em>&gt; selflessly, and that approach is incredibly
</em><br>
<em>&gt; untargeted in comparison to 
</em><br>
<em>&gt; what an advanced neurosurgeon could do, never mind
</em><br>
<em>&gt; an AGI researcher 
</em><br>
<em>&gt; designing an AI from scratch...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Michael Anissimov
</em><br>
<em>&gt; 
</em><br>
<em>&gt;  
</em><br>
<p>In the real world, all the sentients we know about
<br>
(humans) have both non-observer centered (altrustic)
<br>
components to their morality AND observer centered
<br>
(self centerd) components to their morality.  If we
<br>
define 'Universal Morality' to mean the non-observer
<br>
centered components, and 'Personal Morality' to mean
<br>
the observer-centered components, then human morality
<br>
as a whole is described by the following equation:
<br>
<p>Universal Morality x Personal Morality
<br>
<p>In other words, the altruistic side of our morality
<br>
(Universal Morality) interacts (is transformed by,
<br>
hence the multipication sign) with our self-centered
<br>
goals (Personal Morality).  
<br>
<p>Note I don't dispute the existence of an entirely non
<br>
self-centered (altrusitic) morality.  I agree that
<br>
such a morality exists.  It's normative, in the sense
<br>
that all ethical sentients would converge on it, if
<br>
they thought about morality for long enough.  That's
<br>
why I called it 'Universal Morality'.    
<br>
<p>However, as an empirical fact, I note that all the
<br>
sentients we know about (humans) do have self-centered
<br>
components to their morality as well as altruistic. 
<br>
These self-centered components are not normative. 
<br>
There is no unique observer centered morality, and
<br>
many different possible kinds are possible.  That's
<br>
why I called this side of morality 'Personal
<br>
Morality'.  So clearly all the sentients we know about
<br>
(humans) have a morality which is a mixture of
<br>
Universal (altruistic) and Personal (observer
<br>
centered) morality.  Since such sentients (of which
<br>
humans are a specific example) are clearly possible
<br>
the general solution to FAI morality is given by the
<br>
equation:
<br>
<p>Universal Morality x Personal Morality
<br>
<p>A 100% altruistic sentient is a special case of this
<br>
general equation.  If we eliminated all Self-Centered
<br>
goals from our morality, then this would be equivalent
<br>
to setting the 'Personal Morality' component to Unity
<br>
(1).
<br>
<p>Thus 
<br>
<p>Universal Morality x 1 =Universal Morality
<br>
<p>and we are left with a 100% altruistic sentient
<br>
(equivalent to a Yudkowskian FAI).
<br>
<p>Whether a 100% altuistic sentient is logically or
<br>
empirically possible is a debatable point.  But I do
<br>
not, in any event, regard such a sentient as
<br>
desireable.
<br>
<p>If you completely strip out observer centered goals
<br>
('Personal Morality'), you are left with Universal
<br>
Morality.  But this (by definition) is normative
<br>
(unique, all altruistic sentients converge on it).  So
<br>
if all sentients followed this morality alone, you are
<br>
left with totally bland uniformity.  Everything
<br>
unique, everything interesting, everything personal,
<br>
everything creative, would have been stripped away. 
<br>
Would you really like to see everything reduced to
<br>
this? 
<br>
<p>Of course, to be moral we should strive to follow
<br>
Universal Morality, BUT THIS DOES IMPLY THAT WE HAVE
<br>
TO COMPLETELY STRIP OUT OBSERVER CENTERED GOALS!  Some
<br>
observer centered goals will certainly contradict
<br>
Universal Morality, but we have no reason for
<br>
believing that ALL observer centered goals contradict
<br>
Universal Morality.  That is, we can imagine ourselves
<br>
behaving in a manner totally CONSISTENT with altruism
<br>
(Universal Morality), whilst at the same time having
<br>
additional observer centered goals.  Universal
<br>
Morality (altruism) is just a very general set of
<br>
contraints. For instance we could have personal
<br>
moralities like ('Coke is good for me , Pepsi is bad
<br>
for me') , ('Ice skating is good for me', 'Running is
<br>
bad for me') and so, which don't contradict Universal
<br>
morality.  In other words:  Not all observer-centered
<br>
biases are bad!  SOME of them are bad (for instance
<br>
behaviour which hurts others), and yes, we want to
<br>
strip out those parts of our observer centered
<br>
morality which contradict altruism.  But I see no
<br>
reason why we should want to strip out ALL of our
<br>
observer centered goals.
<br>
<p>In fact, it's the observer centered goals that make
<br>
indivuals unique!  It's the observer centered goals
<br>
that allow for individual creatively and uniqueness. 
<br>
If you ever stripped away all your observer centered
<br>
goals, you would be an empty husk, devoid of
<br>
individually or uniqueness (Remember, all you would be
<br>
left with is Universal Morality, which is normative). 
<br>
And that's exactly what a Yudkowskian FAI would be: 
<br>
An empty husk.  
<br>
<p>So, even if the stripping of all observer centered
<br>
goals was possible (which I strongly doubt), I don't
<br>
even regard it as desireable. 
<br>
<p>&nbsp;&nbsp;
<br>
<p><p><p>=====
<br>
Please visit my web-site at:  <a href="http://www.prometheuscrack.com">http://www.prometheuscrack.com</a>
<br>
<p><a href="http://personals.yahoo.com.au">http://personals.yahoo.com.au</a> - Yahoo! Personals
<br>
New people, new possibilities. FREE for a limited time.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8074.html">Marc Geddes: "Oops.  Slight correction to my last post..."</a>
<li><strong>Previous message:</strong> <a href="8072.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>In reply to:</strong> <a href="8071.html">Michael Anissimov: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8074.html">Marc Geddes: "Oops.  Slight correction to my last post..."</a>
<li><strong>Reply:</strong> <a href="8074.html">Marc Geddes: "Oops.  Slight correction to my last post..."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8073">[ date ]</a>
<a href="index.html#8073">[ thread ]</a>
<a href="subject.html#8073">[ subject ]</a>
<a href="author.html#8073">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
