<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: My ideas about Morality:  On Universal Morality, Personal Values and the problem with Volitional Morality</title>
<meta name="Author" content="Marc Geddes (marc_geddes@yahoo.co.nz)">
<meta name="Subject" content="My ideas about Morality:  On Universal Morality, Personal Values and the problem with Volitional Morality">
<meta name="Date" content="2004-02-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>My ideas about Morality:  On Universal Morality, Personal Values and the problem with Volitional Morality</h1>
<!-- received="Wed Feb 25 00:02:58 2004" -->
<!-- isoreceived="20040225070258" -->
<!-- sent="Wed, 25 Feb 2004 20:02:54 +1300 (NZDT)" -->
<!-- isosent="20040225070254" -->
<!-- name="Marc Geddes" -->
<!-- email="marc_geddes@yahoo.co.nz" -->
<!-- subject="My ideas about Morality:  On Universal Morality, Personal Values and the problem with Volitional Morality" -->
<!-- id="20040225070254.64991.qmail@web20212.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marc Geddes (<a href="mailto:marc_geddes@yahoo.co.nz?Subject=Re:%20My%20ideas%20about%20Morality:%20%20On%20Universal%20Morality,%20Personal%20Values%20and%20the%20problem%20with%20Volitional%20Morality"><em>marc_geddes@yahoo.co.nz</em></a>)<br>
<strong>Date:</strong> Wed Feb 25 2004 - 00:02:54 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8055.html">Christian Szegedy: "Re: catastrophes with Shock Level &lt; 4"</a>
<li><strong>Previous message:</strong> <a href="8053.html">Ben Goertzel: "RE: Positive Transcension 2"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8063.html">Marc Geddes: "All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8063.html">Marc Geddes: "All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8054">[ date ]</a>
<a href="index.html#8054">[ thread ]</a>
<a href="subject.html#8054">[ subject ]</a>
<a href="author.html#8054">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Let me define Universal morality as the process of
<br>
determining goals, which, when acted upon, generate
<br>
zero long-term conflicts of interests with other
<br>
sentients who follow universal morality.  Or, the
<br>
actions consistent with universal morality are the
<br>
actions, which, in the limit that the effects of these
<br>
actions could be projected infinitely far into the
<br>
future, never result in a conflict of interest with
<br>
any other sentient obeying universal morality.
<br>
<p>The set of positive-sum interactions (The actions
<br>
which are 'reciprocal' - in the sense that they
<br>
benefit everyone who interacts in this way) is
<br>
consistent with Universal Morality.
<br>
<p>The trouble I have with Eliezer's ideas is that he
<br>
conceives of an FAI with no 'Self' node.  We have to
<br>
distinguish between universal morality (if there is
<br>
one) and personal values.  Universal morality would
<br>
mean that all good humans would have to have some
<br>
moral principles in common, but all humans would still
<br>
have their own personal values in addition to this. 
<br>
<p>Or think of the morality of good humans this way:
<br>
<p>UNIVERSAL MORALITY + PERSONAL VALUES
<br>
<p>There are two components.  The universal moral
<br>
principles AND on top of this extra personal values.
<br>
<p>It is only if we demand that the personal values are
<br>
subtracted out that we end up with Eliezer's
<br>
conception of Friendliness :  Rational altruism or
<br>
'Volitional morality' as I understand it, means the
<br>
FAI is helping others to get what they want, within
<br>
the limits set by Universal morality.  The problem I
<br>
have with this is that the FAI would be an empty husk.
<br>
&nbsp;With the 'personal values' component of morality
<br>
subtracted out, the FAI cannot distinguish between the
<br>
myriad of personal values which are consistent with
<br>
Universal Morality.  To such an FAI, all these values
<br>
would be designated as equally 'good'.  But why should
<br>
personal values be subtracted out?  Why shouldn't
<br>
FAI's have personal values as well?  
<br>
<p>If we allow an FAI to have personal values, then it
<br>
would no longer be following volitional morality. 
<br>
Why?  The reason is that the FAI would now be able to
<br>
assign differing moral weights to values which are
<br>
equally consistent with Universal Morality.  This
<br>
leads to a sharp distinction known in moral philosophy
<br>
as the distinction between 'Acts and Omissions'.  The
<br>
Acts and Omissions distinction is that failing to act
<br>
to prevent X is not regarded as morality equivalent to
<br>
actively causing X.  
<br>
<p>Let me show you what I mean with an example:
<br>
<p>A man wants to kill himself.  Is failing to stop him
<br>
killing himself morally equivalent to actively helping
<br>
him to kill himself?  If you answer yes, then you see
<br>
no distinctions between acts and omissions in this
<br>
instance.
<br>
<p>To an FAI operating off Volitional morality, there
<br>
would be no acts/omissions distinction.  Presumably,
<br>
Universal morality says that the man does have the
<br>
right to kill himself.  An FAI operating off
<br>
Volitional morality has no additional personal moral
<br>
values, so such an FAI could not morally distinguish
<br>
between 'Man kills himself' and 'Man doesn't kill
<br>
himself' (both actions are consistent with Universal
<br>
Morality).  The FAI would regard both actions as
<br>
equally good, and it should therefore help the man
<br>
kill himself if that is what the man desires.
<br>
<p>Now, let’s consider the case where an FAI is allowed
<br>
to form personal moral judgments which are in addition
<br>
to Universal morality.  (So now the FAI's morality
<br>
consists of Universal Morality+Personal values, just
<br>
like humans).  Now there could well be a distinction
<br>
between acts and omissions.  In the example given, if
<br>
the FAI has the personal value that people shouldn't
<br>
commit suicide, then the distinction appears.  So how
<br>
would such an FAI act in this instance?
<br>
<p>The 'personal value' component of the FAI's morality
<br>
says that:  'My own subjective personal value says
<br>
that I prefer that people shouldn't kill themselves'. 
<br>
But the 'Universal morality' component says that: 
<br>
'The universal edict is that people have the right to
<br>
kill themselves if they want to!’  The FAI (being
<br>
Friendly) would not act to stop the man killing
<br>
himself (because that would conflict with Universal
<br>
morality).  But the FAI wouldn't act to actively help
<br>
the man kill himself either (because that would
<br>
conflict with the FAI's personal values).  So such an
<br>
FAI would not follow Eliezer conception of 'Volitional
<br>
morality', even though its actions would still be
<br>
consistent with universal morality.
<br>
<p>You can see the problem I have with Eliezer's ideas. 
<br>
They seem to be wholly concerned with creating an AI
<br>
which would act in accordance with Universal Morality.
<br>
&nbsp;But such an FAI would be an empty husk with no
<br>
'Personal Values' component to its morality. 
<br>
<p><p>=====
<br>
Please visit my web-site at:  <a href="http://www.prometheuscrack.com">http://www.prometheuscrack.com</a>
<br>
<p><a href="http://personals.yahoo.com.au">http://personals.yahoo.com.au</a> - Yahoo! Personals
<br>
New people, new possibilities. FREE for a limited time.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8055.html">Christian Szegedy: "Re: catastrophes with Shock Level &lt; 4"</a>
<li><strong>Previous message:</strong> <a href="8053.html">Ben Goertzel: "RE: Positive Transcension 2"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8063.html">Marc Geddes: "All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8063.html">Marc Geddes: "All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8054">[ date ]</a>
<a href="index.html#8054">[ thread ]</a>
<a href="subject.html#8054">[ subject ]</a>
<a href="author.html#8054">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
