<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Friendly AI in &quot;Positive Transcension&quot;</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Friendly AI in &quot;Positive Transcension&quot;">
<meta name="Date" content="2004-02-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Friendly AI in &quot;Positive Transcension&quot;</h1>
<!-- received="Sun Feb 15 13:14:03 2004" -->
<!-- isoreceived="20040215201403" -->
<!-- sent="Sun, 15 Feb 2004 15:20:47 -0500" -->
<!-- isosent="20040215202047" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Friendly AI in &quot;Positive Transcension&quot;" -->
<!-- id="BMECIIDGKPGNFPJLIDNPOEMLCMAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="402FC946.9090803@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Friendly%20AI%20in%20&quot;Positive%20Transcension&quot;"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sun Feb 15 2004 - 13:20:47 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7947.html">Eliezer S. Yudkowsky: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Previous message:</strong> <a href="7945.html">Ben Goertzel: "RE: qualia and orgasmium"</a>
<li><strong>In reply to:</strong> <a href="7941.html">Eliezer S. Yudkowsky: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7947.html">Eliezer S. Yudkowsky: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Reply:</strong> <a href="7947.html">Eliezer S. Yudkowsky: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7946">[ date ]</a>
<a href="index.html#7946">[ thread ]</a>
<a href="subject.html#7946">[ subject ]</a>
<a href="author.html#7946">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi,
<br>
<p><em>&gt; &gt; Before responding at all I'm going to make a request of you.  Please
</em><br>
<em>&gt; &gt; summarize,
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; FIRST, in a single, clear, not-too-long sentence
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; SECOND, in a single, clear, not-too-long paragraph
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; your own view of your theory of Friendly AI.  I will then
</em><br>
<em>&gt; paraphrase your
</em><br>
<em>&gt; &gt; own summaries, in my own words, in my revised essay.  I have no
</em><br>
<em>&gt; desire to
</em><br>
<em>&gt; &gt; misrepresent your ideas, of course.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This cannot possibly be done.  What you're asking is undoable.
</em><br>
<p>OK, well if you can't even summarize your own work compactly, then how the
<br>
heck do you expect me to be able to do so??? ;-)
<br>
<p><em>&gt; Now, please first, in a single, clear, not-too-long sentence, and second,
</em><br>
<em>&gt; in a single, clear, not-too-long paragraph, summarize, to me,
</em><br>
<em>&gt; your view of
</em><br>
<em>&gt; all the fundamental concepts that went into Novamente - not just for me,
</em><br>
<em>&gt; mind you, but for readers completely unfamiliar with your ideas who will
</em><br>
<em>&gt; read my interpretation of your ideas.
</em><br>
<p><p>At the end of this email I will paste the intro from an overview paper on
<br>
Novamente that appeared in the proceedings of the IJCAI '03 conference.
<br>
Yeah, it doesn't really get at the essence of the theory underlying the
<br>
system, but it summarizes the work in a not-too-misleading way.
<br>
<p>Pardon the LaTex ;-)
<br>
<p><em>&gt; I'll be sure to add a similar disclaimer to my forthcoming essay about
</em><br>
<em>&gt; &quot;Novamente: Cyc with activation levels added&quot;.
</em><br>
<p>My essay was not principally about your ideas, it was about my own ideas,
<br>
and mentioned yours along with those of several other folks.
<br>
<p>Regarding Novamente and Cyc, Cyc lacks
<br>
<p>-- grounding of concepts in experiential, nonlinguistic data
<br>
-- evolutionary learning as a means for concept creation
<br>
-- attention allocation dynamics aimed at forming a &quot;moving bubble of
<br>
awareness&quot;
<br>
-- introspection aimed at having the system analyze its own thought
<br>
processes and create new concepts accordingly
<br>
-- probabilistic inference spanning multiple subdomains of knowledge
<br>
-- learning of procedures (for carrying out external-world actions, or
<br>
cognitive actions)
<br>
-- even in principle, the ability to modify its own cognitive processes or
<br>
data structures
<br>
<p>and a large number of other features of Novamente.  The differentiation is
<br>
not hard to understand.
<br>
<p>Cyc consists of a knowledge base of abstract knowledge, and a collection of
<br>
reasoning engines running on top of it -- mostly crisp ones, but some that
<br>
do probababilistic inference in narrow subdomains.  Novamente explicitly
<br>
contains much more than that.
<br>
<p><em>&gt; &gt; So far as I can tell, the biggest difference you see between my
</em><br>
<em>&gt; rendition of
</em><br>
<em>&gt; &gt; your views, and your actual views, is that instead of &quot;programming or
</em><br>
<em>&gt; &gt; otherwise inculcating benevolence to humans&quot;, you'd rather speak about
</em><br>
<em>&gt; &gt; &quot;programming or otherwise inculcating humane morality in an AI&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You're missing upward of a dozen fundamental concepts here.
</em><br>
<p>Look, any brief summary is going to miss a lot of fundamental concepts.
<br>
That is the nature of summary.  In summarizing something, one has to choose
<br>
what to include and what to leave out.
<br>
<p><em>&gt; First, let's delete &quot;programming or otherwise inculcating&quot; and replacing
</em><br>
<em>&gt; with &quot;choosing&quot;, which is the correct formulation under the basic theory
</em><br>
<em>&gt; of FAI, which makes extensive use of the expected utility principle.
</em><br>
<em>&gt; Choice subsumes choice over programming, choice over environmental
</em><br>
<em>&gt; information, and any other design options of which we might prefer one to
</em><br>
<em>&gt; another.
</em><br>
<p>Fine, we can refer to &quot;choosing&quot;, while noting that programming and teaching
<br>
are the apparently most likely forms of choosing in this context...
<br>
<p><em>&gt; Next, more importantly, &quot;humane&quot; is not being given its intuitive sense
</em><br>
<em>&gt; here!  Humane is here a highly technical concept, &quot;renormalized
</em><br>
<em>&gt; humanity&quot;.
</em><br>
<p>So far as I can tell this is a fuzzy and ill-defined and obscure concept,
<br>
lacking a clear and compact definition
<br>
<p>Feel free to give one, or refer me to a specific paragraph in one of your
<br>
online writings where such is given.
<br>
<p><p><em>&gt; It's not that you're misunderstanding *what specifically* I'm saying, but
</em><br>
<em>&gt; that you're misunderstanding the *sort of thing* I'm attempting to
</em><br>
<em>&gt; describe.  Not apples versus oranges, more like apples versus the
</em><br>
<em>&gt; equation
</em><br>
<em>&gt; x'' = -kx.
</em><br>
<p>OK, so please clearly explain what sort of thing you're attempting to
<br>
describe.
<br>
<p><em>&gt; Your most serious obstacle here is your inability to see anything except
</em><br>
<em>&gt; the specific content of an ethical system - you see &quot;Joyous Growth&quot; as a
</em><br>
<em>&gt; specific ethical system, you see &quot;benevolence&quot; as specific content, your
</em><br>
<em>&gt; mental model of &quot;humaneness&quot; is something-or-other with specific ethical
</em><br>
<em>&gt; content.  &quot;Humaneness&quot; as I'm describing it *produces* specific ethical
</em><br>
<em>&gt; content but *is not composed of* specific ethical content.  Imagine the
</em><br>
<em>&gt; warm fuzzy feeling that you get when considering &quot;Joyous Growth&quot;.  Now,
</em><br>
<em>&gt; throughout history and across the globe, do you think that only
</em><br>
<em>&gt; 21st-century Americans get warm fuzzy feelings when considering their
</em><br>
<em>&gt; personal moral philosophies?
</em><br>
<p>Actually, abstractions like that don't give me &quot;warm fuzzy feelings&quot;... but
<br>
maybe that's a quirk of my personal psychology.  I get warm fuzzy feelings
<br>
toward humans and animals, for instance, but not toward abstract principles.
<br>
<p>And, I *don't* see abstract ethical principles as being specific ethical
<br>
systems, I tried to very clearly draw that distinction in my essay, by
<br>
defining abstract ethical principles as tools for judging specific ethical
<br>
systems, and defining ethical systems as factories for producing ethical
<br>
rules.
<br>
<p>I can understand if you're positing some kind of &quot;humaneness&quot; as an abstract
<br>
ethical principle for producing specific human ethical systems.  It still
<br>
seems to me like it's a messy, overcomplex, needlessly ill-defined ethical
<br>
principle which is unlikely to be implantable in an AI or to survive a
<br>
Transcension.
<br>
<p><em>&gt; The dynamics of the thinking you do when you consider that question would
</em><br>
<em>&gt; form part of the &quot;renormalization&quot; step, step 4, the volition examining
</em><br>
<em>&gt; itself under reflection.  It is improper to speak of a vast morass of
</em><br>
<em>&gt; &quot;humane morality&quot; which needs to be renormalized, because the word
</em><br>
<em>&gt; &quot;humane&quot; was not introduced until after step 4.  You could speak
</em><br>
<em>&gt; of a vast
</em><br>
<em>&gt; contradictory morass of the summated outputs of human moralities, but if
</em><br>
<em>&gt; you add the &quot;e&quot; on the end, then in FAI theory it has the connotation of
</em><br>
<em>&gt; something already renormalized.  Furthermore, it is improper to speak of
</em><br>
<em>&gt; renormalizing the vast contradictory morass as such, because it's a
</em><br>
<em>&gt; superposition of outputs, not a dynamic process capable of renormalizing
</em><br>
<em>&gt; itself.  You can speak of renormalizing a given individual, or
</em><br>
<em>&gt; renormalizing a model based on a typical individual.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is all already taken into account in FAI theory.  At length.
</em><br>
<p>Well, I'm not sure I believe there is a clear, consistent, meaningful,
<br>
usable entity corresponding to your two-word phrase &quot;humane morality.&quot;  I'm
<br>
not so sure this beast exists.  Maybe all there is, in the human-related
<br>
moral sphere, is a complex mess of interrelated, largely self-contradictory
<br>
ethical systems, guided by some general principles of complex systems
<br>
dynamics and by our biological habits and heritage.
<br>
<p>-- Ben G
<br>
<p><p><p><p><p><p>************
<br>
\begin{abstract}
<br>
The {\em Novamente AI Engine}, a novel AI software system, is briefly
<br>
reviewed.  Unlike the majority of contemporary AI projects, Novamente
<br>
is aimed at artificial {\em general} intelligence, rather than being
<br>
restricted by design to one particular application domain, or to a
<br>
narrow range of cognitive functions.  Novamente integrates aspects of
<br>
many prior AI projects and paradigms, including symbolic,
<br>
neural-network, evolutionary programming and reinforcement learning
<br>
approaches; but its overall architecture is unique, drawing on
<br>
system-theoretic ideas regarding complex mental dynamics and
<br>
associated emergent patterns.
<br>
\end{abstract}
<br>
<p>\section{Introduction}
<br>
<p>We describe here an in-development AI software system that confronts
<br>
the ``grand problem of artificial intelligence'': Artificial General
<br>
Intelligence (AGI).  This software system is the {\em Novamente AI
<br>
Engine}, or more compactly {\em Novamente}.
<br>
<p>The Novamente design incorporates aspects of many previous AI
<br>
paradigms such as genetic programming, neural networks, agent systems,
<br>
evolutionary programming, reinforcement learning, and probabilistic
<br>
reasoning.  However, it is unique in its overall architecture, which
<br>
confronts the problem of creating a holistic digital mind in a direct
<br>
way that has not been done before.
<br>
<p>The fundamental principles underlying the system design derive from a
<br>
novel complex-systems-based theory of mind called the ``psynet
<br>
model'', which was developed in a series of cross-disciplinary
<br>
research treatises published during 1993-2001
<br>
\cite{goe93,goe93b,goe94,goe97,goe02}.
<br>
What the psynet model has led us to is not
<br>
a conventional AI program, nor a conventional multi-agent-system
<br>
framework.  Rather, Novamente aims to be an autonomous,
<br>
self-organizing, self-evolving AGI system, with its own understanding
<br>
of the world, and the ability to relate to humans on a
<br>
``mind-to-mind'' rather than a ``software-program-to-mind''
<br>
level.  The Novamente project is based on many of the same ideas that
<br>
underlay the Webmind AI Engine project carried out at Webmind
<br>
Inc. during 1997-2001 \cite{goertzel00}; and it also draws to some extent
<br>
on ideas from Pei Wang's Non-Axiomatic Reasoning System (NARS)
<br>
\cite{wang95phd}.
<br>
<p>At the moment, Novamente is partially implemented as a C++ software
<br>
system, currently customized for Linux
<br>
clusters, with a few externally-facing components written in Java.
<br>
The overall mathematical and conceptual design of the system is
<br>
described in a forthcoming paper \cite{NovPaper} and book
<br>
\cite{NovBook}.  While the implementation is not yet complete, the
<br>
design has matured throughout the years, and draws upon the many
<br>
lessons learned by the authors in the design, implementation and
<br>
testing of the Webmind AI Engine.  The current, partially-complete
<br>
codebase is being
<br>
used by the startup firm Biomind LLC, to analyze genetics and
<br>
proteomics data in the context of information integrated from numerous
<br>
biological databases.  Once the system is fully engineered, the
<br>
project will begin a phase of interactively teaching the Novamente
<br>
system how to respond to user queries, and how to usefully analyze and
<br>
organize data.  The end result of this teaching process will be an
<br>
autonomous AGI system, oriented toward assisting humans in
<br>
collectively solving pragmatic problems.
<br>
*******************
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7947.html">Eliezer S. Yudkowsky: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Previous message:</strong> <a href="7945.html">Ben Goertzel: "RE: qualia and orgasmium"</a>
<li><strong>In reply to:</strong> <a href="7941.html">Eliezer S. Yudkowsky: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7947.html">Eliezer S. Yudkowsky: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Reply:</strong> <a href="7947.html">Eliezer S. Yudkowsky: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7946">[ date ]</a>
<a href="index.html#7946">[ thread ]</a>
<a href="subject.html#7946">[ subject ]</a>
<a href="author.html#7946">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
