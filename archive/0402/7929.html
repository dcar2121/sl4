<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=Windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendly AI in &quot;Positive Transcension&quot;</title>
<meta name="Author" content="Metaqualia (metaqualia@mynichi.com)">
<meta name="Subject" content="Re: Friendly AI in &quot;Positive Transcension&quot;">
<meta name="Date" content="2004-02-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendly AI in &quot;Positive Transcension&quot;</h1>
<!-- received="Sun Feb 15 02:05:55 2004" -->
<!-- isoreceived="20040215090555" -->
<!-- sent="Sun, 15 Feb 2004 17:50:17 +0900" -->
<!-- isosent="20040215085017" -->
<!-- name="Metaqualia" -->
<!-- email="metaqualia@mynichi.com" -->
<!-- subject="Re: Friendly AI in &quot;Positive Transcension&quot;" -->
<!-- id="00a701c3f3a2$d5ebae70$0d01a8c0@CURZIOL2" -->
<!-- charset="Windows-1252" -->
<!-- inreplyto="402ED90B.3080504@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Metaqualia (<a href="mailto:metaqualia@mynichi.com?Subject=Re:%20Friendly%20AI%20in%20&quot;Positive%20Transcension&quot;"><em>metaqualia@mynichi.com</em></a>)<br>
<strong>Date:</strong> Sun Feb 15 2004 - 01:50:17 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7930.html">Alejandro Dubrovsky: "Re: In defense of physics (was: Encouraging a Positive Transcension)"</a>
<li><strong>Previous message:</strong> <a href="7928.html">Metaqualia: "Re: In defense of physics"</a>
<li><strong>In reply to:</strong> <a href="7922.html">Eliezer S. Yudkowsky: "Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7939.html">Ben Goertzel: "RE: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7929">[ date ]</a>
<a href="index.html#7929">[ thread ]</a>
<a href="subject.html#7929">[ subject ]</a>
<a href="author.html#7929">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; I wish these were the old days, so I could look over what I just wrote
</em><br>
<em>&gt; with satisfaction, rather than the dreadful sinking knowledge that
</em><br>
<em>&gt; everything I just said sounded like complete gibberish to anyone honest
</em><br>
<em>&gt; enough to admit it.
</em><br>
<p>I must say that although your prose is very entertaining there are clearer
<br>
ways of explaining what you are explaining!
<br>
<p>Basically you are making the distinction between
<br>
<p>A. hardwiring the outcome of human morality into an AI (don't do this, do
<br>
that)
<br>
<p>and
<br>
<p>B. replicating the whole framework that created the outcome (the AI spits
<br>
out &quot;do this&quot; and &quot;don't do that&quot;s autonomously)
<br>
<p>Asimov went for #1, but that is very dangerous as it is a brittle system.
<br>
This is not friendly AI.
<br>
<p>It is like the difference between 1. writing an xml parser to look for
<br>
specific substrings and extracting the text between the tags blindly (if it
<br>
comes after &lt;ZipCode&gt; it must be a zipcode), AND 2. making an AI that
<br>
understands the nature of the document (looks like a letter, this must be
<br>
the recipient's address, 5 digit numbers in an address are a zipcode). This
<br>
is friendly.
<br>
<p>Further, you add self improvement to the mix, so that according to Friendly
<br>
AI with the uppercase F, 3. the AI must be able to get better at doing what
<br>
it does, always checking if it is really right in its assumptions or if it
<br>
can learn more (ex. I have always thought that 5 digit numbers in an address
<br>
are a zipcode but I have some spare cycles so I am going to gather all the
<br>
data available and check whether street numbers can get to be 5 digits long,
<br>
whether street names can be expressed with 5 digits in any notation system,
<br>
whether this could be something else other than a letter, etc.)
<br>
<p>It is not an extremely hard concept to understand.
<br>
It may be hard to implement, but not that hard to understand.
<br>
<p>Now I don't think Ben thinks that Joyous Growth is a primitive that can be
<br>
taught to silicon easily; we were discussing about human morality outcomes
<br>
among humans so what he means is perfectly clear to us (or at least me).
<br>
Even though his primitives are different from mine (they are more numerous).
<br>
<p>To get outside of the XML analogy and better summarize once again,
<br>
<p>Your work with friendly ai consists in building a set of rules by which to
<br>
code an AI, which if followed would allow the AI to perform the following:
<br>
<p>1. extract human statements/behaviors as [partial? incomplete?
<br>
contradictory? possibly plain wrong?] data about human morality and create a
<br>
model for an ideal morality that synthesizes all this data as accurately as
<br>
possible
<br>
<p>2. visualize itself as a tool that accomplishes n.1, but an imperfect tool,
<br>
always in need of refinement to better accomplish the goal
<br>
<p><em>&gt; a)  A Friendly AI improving itself approaches as a limit the AI you'd have
</em><br>
<em>&gt; built if you knew what you were doing, *provided that* you *did* know what
</em><br>
<em>&gt; you were doing when you defined the limiting process.
</em><br>
<p>System tends to become friendlier regardless of initial configuration
<br>
provided it can correctly evaluate its degree of friendliness at each step.
<br>
Agree.
<br>
<p><em>&gt; b)  A Friendly AI does not look old and busted when the civilization that
</em><br>
<em>&gt; created it has grown up a few million years.  FAIs grow up too - very
</em><br>
<em>&gt; rapidly, where the course is obvious (entropy low), in other areas waiting
</em><br>
<em>&gt; for the civilization to actually make its choices.
</em><br>
<p>AIs evolve. Agree.
<br>
<p><em>&gt; c)  If you are in the middle of constructing an FAI, and you make a
</em><br>
<em>&gt; mistake about what you really wanted, but you got the fundamental
</em><br>
<em>&gt; architecture right, you can say &quot;Oops&quot; and the FAI listens.  This is
</em><br>
<em>&gt; really really REALLY nontrivial.
</em><br>
<p>AI does not take data as direct observations of the ideal morality, but as
<br>
possibly incorrect data pointing to the ideal morality. Agree. Very
<br>
nontrivial.
<br>
<p><em>&gt; d)  Friendly AI doesn't run on verbal principles or moral philosophies.
</em><br>
<em>&gt; If you said to an FAI, &quot;Joyous Growth&quot;, its architecture would attempt to
</em><br>
<em>&gt; suck out the warm fuzzy feeling that &quot;Joyous Growth&quot; gives you and is the
</em><br>
<em>&gt; actual de facto reason you feel fond of &quot;Joyous Growth&quot;.
</em><br>
<p>AI will try to understand what programmers really mean by simulating their
<br>
neural structure, not just take their words are programming primitives.
<br>
Agree.
<br>
<p><em>&gt; f)  Friendly AI is frickin' complicated, so please stop summarizing it as
</em><br>
<em>&gt; &quot;hardwiring benevolence to humans&quot;.  I ain't bloody Asimov.  This isn't
</em><br>
<em>&gt; even near the galaxy of the solar system that has the planet where the
</em><br>
<em>&gt; ballpark is located.
</em><br>
<p>Most people, when hearing *hardwiring*, would
<br>
not indeed suspect that the task is so hard; nevertheless I think that is
<br>
just what it is; whatever means you choose to use, as long as you are not
<br>
making an AI that will dissect the laws of the universe to find if good and
<br>
evil have a really truly objective meaning, as long as you are most
<br>
preoccupied with what happens to humans, that is what you are doing,
<br>
hardwiring benevolence to humans. You are doing it well and in a very hard
<br>
way. But you ARE exploiting your brain's simulation power to constrain the
<br>
AI into a specific shape with a specific set of behaviors (whether
<br>
immediately foreseeable or not).
<br>
Which is not necessarily a bad thing, it depends on your ethical system. Who
<br>
can tell a human that building a human friendly AI is wrong? Who can say
<br>
that a universal morality really exists anyway?
<br>
Personally, because I feel I am at the most basic level a qualia stream and
<br>
not a human, I would like the AI to figure out qualia and if there is a
<br>
universally justifiable ethical system use THAT; if there isn't one, default
<br>
to be human friendly. But I'll go with the crowd as long as you don't make
<br>
it unfriendly to rabbits on purpose. :-)
<br>
<p>I shall add that chimps (since you mentioned them) are much more similar to
<br>
us than we are comfortable believing; they show ethical behaviour in many
<br>
instances although I cannot give references here! From caring for children
<br>
to scratching each other's back to condemning excessively selfish behavior,
<br>
animals exhibit a wide range of social smarts which we call human evolved
<br>
morality. The reason humans are the only ones talking about morality is not
<br>
that they are the only ones &quot;feeling stuff about morality&quot;, but that they
<br>
are the only ones that can talk.
<br>
<p><em>&gt; There is no light in this world except that embodied in humanity.  Even my
</em><br>
<p>I agree. The existence of sentients is the only thing that matters, since
<br>
everything else is inert, dead, might as well not exist at all. This seems
<br>
to be contradicting my earlier statements but it is not.
<br>
I think that the *light* that humanity embodies is not a brain that weighs a
<br>
few pounds or a max-4-hops logical inference system, or a randomly arisen
<br>
good/evil preference system, but qualia, and a link between qualia and the
<br>
physical world. When qualia arise, that makes a difference. Without them,
<br>
have whatever universe you will, nobody knows about it!
<br>
<p><em>&gt; I shut up about Shiva-Singularities.  See, I even gave it a name, back in
</em><br>
<em>&gt; my wild and reckless youth.  You've sometimes presumed to behave toward me
</em><br>
<em>&gt; in a sage and elderly fashion, Ben, so allow me to share one of the
</em><br>
<em>&gt; critical lessons from my own childhood:  No, you do not want humanity to
</em><br>
<em>&gt; go extinct.  Trust me on this, because I've been there, and I know from
</em><br>
<em>&gt; experience that it isn't obvious.
</em><br>
<p>I think I was the one to suggest that the existence of humans may not be the
<br>
n.1 priority in universal terms. Now that the difference between a human and
<br>
a qualia stream is clear I suppose I won't look like a crazy assassin
<br>
anymore :)
<br>
There is a lot to a human besides being human! We are qualia streams inside
<br>
a mind inside a brain inside a human. The human is the least important part.
<br>
<p><p>mq
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7930.html">Alejandro Dubrovsky: "Re: In defense of physics (was: Encouraging a Positive Transcension)"</a>
<li><strong>Previous message:</strong> <a href="7928.html">Metaqualia: "Re: In defense of physics"</a>
<li><strong>In reply to:</strong> <a href="7922.html">Eliezer S. Yudkowsky: "Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7939.html">Ben Goertzel: "RE: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7929">[ date ]</a>
<a href="index.html#7929">[ thread ]</a>
<a href="subject.html#7929">[ subject ]</a>
<a href="author.html#7929">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
