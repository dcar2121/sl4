<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: All sentient have to be observer-centered!  My theory of FAI morality</title>
<meta name="Author" content="Marc Geddes (marc_geddes@yahoo.co.nz)">
<meta name="Subject" content="All sentient have to be observer-centered!  My theory of FAI morality">
<meta name="Date" content="2004-02-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>All sentient have to be observer-centered!  My theory of FAI morality</h1>
<!-- received="Wed Feb 25 23:45:53 2004" -->
<!-- isoreceived="20040226064553" -->
<!-- sent="Thu, 26 Feb 2004 19:45:51 +1300 (NZDT)" -->
<!-- isosent="20040226064551" -->
<!-- name="Marc Geddes" -->
<!-- email="marc_geddes@yahoo.co.nz" -->
<!-- subject="All sentient have to be observer-centered!  My theory of FAI morality" -->
<!-- id="20040226064551.61380.qmail@web20207.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20040225070254.64991.qmail@web20212.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marc Geddes (<a href="mailto:marc_geddes@yahoo.co.nz?Subject=Re:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality"><em>marc_geddes@yahoo.co.nz</em></a>)<br>
<strong>Date:</strong> Wed Feb 25 2004 - 23:45:51 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8064.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Previous message:</strong> <a href="8062.html">Christopher Healey: "RE: Intelligence is exploitative (RE: Zen singularity)"</a>
<li><strong>In reply to:</strong> <a href="8054.html">Marc Geddes: "My ideas about Morality:  On Universal Morality, Personal Values and the problem with Volitional Morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8064.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8064.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8069.html">Rafal Smigrodzki: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8063">[ date ]</a>
<a href="index.html#8063">[ thread ]</a>
<a href="subject.html#8063">[ subject ]</a>
<a href="author.html#8063">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
My main worry with Eliezer's ideas is that I don't
<br>
think that a non observer-centered sentient is
<br>
logically possible.  Or if it's possible, such a
<br>
sentient would not be stable.  Can I prove this?  No. 
<br>
But all the examples of stable sentients (humans) that
<br>
we have are observer centered.  I can only point to
<br>
this, combined with the fact that so many people
<br>
posting to sl4 agree with me.  I can only strongly
<br>
urge Eliezer and others working on AI NOT to attempt
<br>
the folly of trying to create a non observer centered
<br>
AI.  For goodness sake don't try it!  It could mean
<br>
the doom of us all.
<br>
<p>I do agree that some kind of 'Universal Morality' is
<br>
possible. i.e I agree that there exists a non-observer
<br>
centered morality which all friendly sentients would
<br>
aspire to.  However, as I said, I don't think that
<br>
non-observer sentients would be stable so any friendly
<br>
stable sentient cannot follow Universal Morality
<br>
exactly.
<br>
<p>If AI morality were just:
<br>
<p>Universal Morality
<br>
<p>then I postulate that the AI would fail  (either it
<br>
could never be created in the first place, or else it
<br>
would not be stable and it would under go friendliness
<br>
failure).
<br>
<p>But there's a way to make AI's stable: add a small
<br>
observer-centered component.  Such an AI could still
<br>
be MOSTLY altruistic, but now it would only be
<br>
following Universal Morality as an approximation,
<br>
since there would be an additional observer-centered
<br>
component.
<br>
<p>So I postulate that all stable FAI's have to have
<br>
moralities of the form:
<br>
<p>Universal Morality x Personal Morality
<br>
<p>Now Universal Morality (by definition) is not
<br>
arbitrary or observer centered.  There is one and only
<br>
one Universal Morality and it must be symmetric across
<br>
all sentients (it has to work if everyone does it -
<br>
positive sum interactions).
<br>
<p>But Personal morality (by definition) can have many
<br>
degrees of freedom and is observer centered.  There
<br>
are many different possible kinds of personal morality
<br>
and the morality is subjective and observer centered. 
<br>
The only constraint is that Personal Morality has to
<br>
be consistent with Universal Morality to be Friendly. 
<br>
That's why I say that stable FAI's follow Universal
<br>
Morality transformed by (multipication sign) Personal
<br>
Morality.
<br>
<p>Now an FAI operating off Universal Morality alone
<br>
(which I'm postulating is impossible or unstable)
<br>
would to one and only one (unique) Singularity.  There
<br>
would be only one possible form a successful
<br>
Singularity could take.  A reasonable guess (due to
<br>
Eliezer) is that:
<br>
<p>Universal Morality = Volitional Morality
<br>
<p>That is, it was postulated by Eli that Universal
<br>
Morality is respect for sentient volition (free will).
<br>
&nbsp;With no observer centered component, an FAI following
<br>
this morality would aim to fulfil sentient requests
<br>
(consistent with sentient volition).  But I think that
<br>
such an AI is impossible or unstable.
<br>
<p>I was postulating that all stable FAI's have a
<br>
morality of the form:
<br>
<p>Universal Morality x Personal Morality
<br>
<p>If I am right, then there are many different kinds of
<br>
successul (Friendly) Singularities.  Although
<br>
Universal Morality is unique, Personal Morality can
<br>
have many degrees of freedom.  So the precise form a
<br>
successful Singularity takes would depend on the
<br>
'Personal Morality' componant of the FAI's morality.
<br>
<p>Assuming that:
<br>
<p>Universal Morality = Volition based Morality
<br>
<p>we see that:
<br>
<p>Universal Morality x Personal Morality
<br>
<p>leads to something quite different.  Respect for
<br>
sentient volition (Universal Morality) gets
<br>
transformed (mulipication sign) by Personal Morality. 
<br>
This leads to a volition based morality with an
<br>
Acts/Omissions distinction (See my previous post for
<br>
an explanation of the Moral Acts/Omissions
<br>
distinctions).  
<br>
<p>FAI's with morality of this form would still respect
<br>
sentient volition, but they would not neccesserily
<br>
fulfil sentient requests.  Sentient requests would
<br>
only be fulfilled when such requests are consistent
<br>
with the FAI's Personal Morality.  So the 'Personal
<br>
Morality' component would act like a filter stopping
<br>
some sentient requests from being fulfilled.  In
<br>
addition, such FAI's would be pursuing goals of their
<br>
own (so long as such goals did not violate sentient
<br>
volition).  So you see, my form of FAI is a far more
<br>
interesting and complex beast than an FAI which just
<br>
followed Universal Morality.
<br>
<p>Eliezer's 'Friendliness' theory (whereby the AI is
<br>
reasoning about morality and can modify its own goals
<br>
to try to close in on normalized 'Universal Morality')
<br>
is currently only dealing with the 'Universal
<br>
Morality' component of morality.
<br>
<p>But if I am right, then all stable FAI have to have an
<br>
observer-centered (Personal Morality) componant to
<br>
their morality as well.  
<br>
<p>So it's vital that FAI programmers give consideration
<br>
to just what the 'Personal Morality' of an FAI should
<br>
be.  The question of personal values cannot be evaded
<br>
if non observer centered FAI's are impossible.  Even
<br>
with Universal Morality, there would have to be a
<br>
'Personal Morality' componant which would have to be
<br>
chosen directly by the programmers (this 'Personal
<br>
Morality' componant is arbitrary and
<br>
non-renormalizable).
<br>
<p>To sum up: my theory is that all stable FAI have
<br>
moralitites of the form:
<br>
<p>Universal Morality x Personal Morality
<br>
<p>Only the 'Universal Morality' can be normalized.     
<br>
<p>=====
<br>
Please visit my web-site at:  <a href="http://www.prometheuscrack.com">http://www.prometheuscrack.com</a>
<br>
<p>Find local movie times and trailers on Yahoo! Movies.
<br>
<a href="http://au.movies.yahoo.com">http://au.movies.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8064.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Previous message:</strong> <a href="8062.html">Christopher Healey: "RE: Intelligence is exploitative (RE: Zen singularity)"</a>
<li><strong>In reply to:</strong> <a href="8054.html">Marc Geddes: "My ideas about Morality:  On Universal Morality, Personal Values and the problem with Volitional Morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8064.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8064.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8069.html">Rafal Smigrodzki: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8063">[ date ]</a>
<a href="index.html#8063">[ thread ]</a>
<a href="subject.html#8063">[ subject ]</a>
<a href="author.html#8063">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
