<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: All sentient have to be observer-centered!  My theory of FAI morality</title>
<meta name="Author" content="Tommy McCabe (rocketjet314@yahoo.com)">
<meta name="Subject" content="RE: All sentient have to be observer-centered!  My theory of FAI morality">
<meta name="Date" content="2004-02-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: All sentient have to be observer-centered!  My theory of FAI morality</h1>
<!-- received="Sun Feb 29 05:58:40 2004" -->
<!-- isoreceived="20040229125840" -->
<!-- sent="Sun, 29 Feb 2004 04:58:34 -0800 (PST)" -->
<!-- isosent="20040229125834" -->
<!-- name="Tommy McCabe" -->
<!-- email="rocketjet314@yahoo.com" -->
<!-- subject="RE: All sentient have to be observer-centered!  My theory of FAI morality" -->
<!-- id="20040229125834.47454.qmail@web11707.mail.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20040229051444.74196.qmail@web20211.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tommy McCabe (<a href="mailto:rocketjet314@yahoo.com?Subject=RE:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality"><em>rocketjet314@yahoo.com</em></a>)<br>
<strong>Date:</strong> Sun Feb 29 2004 - 05:58:34 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8088.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Previous message:</strong> <a href="8086.html">Tomaz Kristan: "Re: [SL4] AI --&gt; Jobless Economy"</a>
<li><strong>In reply to:</strong> <a href="8080.html">Marc Geddes: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8087">[ date ]</a>
<a href="index.html#8087">[ thread ]</a>
<a href="subject.html#8087">[ subject ]</a>
<a href="author.html#8087">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
--- Marc Geddes &lt;<a href="mailto:marc_geddes@yahoo.co.nz?Subject=RE:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality">marc_geddes@yahoo.co.nz</a>&gt; wrote:
<br>
<em>&gt;  --- Rafal Smigrodzki &lt;<a href="mailto:rafal@smigrodzki.org?Subject=RE:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality">rafal@smigrodzki.org</a>&gt; wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; Marc wrote:
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; &gt; I don't regard the evolutionary arguments as
</em><br>
<em>&gt; very
</em><br>
<em>&gt; &gt; &gt; convincing.  They're based on observation, not
</em><br>
<em>&gt; &gt; &gt; experiment.  Besides, it's only very recently in
</em><br>
<em>&gt; &gt; &gt; evolutionary history that the first sentients
</em><br>
<em>&gt; &gt; (humans)
</em><br>
<em>&gt; &gt; &gt; appeared.  It's the class of sentients that is
</em><br>
<em>&gt; &gt; &gt; revelent to FAI work.  Evolutionary observations
</em><br>
<em>&gt; &gt; about
</em><br>
<em>&gt; &gt; &gt; non-sentients is not likely to say much of
</em><br>
<em>&gt; &gt; relevence.
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; ### You might wish to read some evolutionary
</em><br>
<em>&gt; &gt; psychology texts.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Um..well O.K sure I don't doubt that evolutionary
</em><br>
<em>&gt; psychology is very relevent to HUMAN psychology, but
</em><br>
<em>&gt; it is of much revelevence to the general class of
</em><br>
<em>&gt; SENTIENT psychology?  I'm not sure evolutionary
</em><br>
<em>&gt; psychology says much one or the other.
</em><br>
<p>Is evolutionary psychology really relevant to sentient
<br>
psychology in general? No!!! And that's why you can't
<br>
go tagging sentients-in-general with purely
<br>
evolutionary traits like observer-centered moralities
<br>
taht are hardwired in.
<br>
&nbsp;
<br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; -----------------------------
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; In any event, I don't regard non observer based
</em><br>
<em>&gt; &gt; &gt; sentients as even desireable (See my other
</em><br>
<em>&gt; &gt; replies).
</em><br>
<em>&gt; &gt; &gt; If you strip out all observer centered goals,
</em><br>
<em>&gt; &gt; you're
</em><br>
<em>&gt; &gt; &gt; left with normative altruism.  All sentients
</em><br>
<em>&gt; would
</em><br>
<em>&gt; &gt; &gt; converge on this, and all individual uniqueness
</em><br>
<em>&gt; &gt; would
</em><br>
<em>&gt; &gt; &gt; be stripped away.  You'd be left with bland
</em><br>
<em>&gt; &gt; &gt; uniformity.  An empty husk.  Universal morality
</em><br>
<em>&gt; is
</em><br>
<em>&gt; &gt; &gt; probably just a very general set of contrainsts,
</em><br>
<em>&gt; &gt; and
</em><br>
<em>&gt; &gt; &gt; FAI's following this alone would be qute unable
</em><br>
<em>&gt; to
</em><br>
<em>&gt; &gt; &gt; distinguish between the myraid of interesting
</em><br>
<em>&gt; &gt; personal
</em><br>
<em>&gt; &gt; &gt; goals that are consistent with it.  Everything
</em><br>
<em>&gt; &gt; that
</em><br>
<em>&gt; &gt; &gt; didn't hurt others (assuming that Universal
</em><br>
<em>&gt; &gt; Morality
</em><br>
<em>&gt; &gt; &gt; is volition based) whould be equally 'Good' to
</em><br>
<em>&gt; &gt; such an
</em><br>
<em>&gt; &gt; &gt; FAI.  There would be no possibility of anything
</em><br>
<em>&gt; &gt; &gt; unquinely human or personal.  For instance the
</em><br>
<em>&gt; two
</em><br>
<em>&gt; &gt; &gt; outcomes 'Rafal kills himself', 'Rafal doesn't
</em><br>
<em>&gt; &gt; kill
</em><br>
<em>&gt; &gt; &gt; humself' would be designated as morally
</em><br>
<em>&gt; equivalent
</em><br>
<em>&gt; &gt; &gt; under Volitional Morality.
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; ### I don't understand the first part of your
</em><br>
<em>&gt; &gt; paragraph. As to your claim
</em><br>
<em>&gt; &gt; about what would and would not be equivalent under
</em><br>
<em>&gt; &gt; volitional morality, I
</em><br>
<em>&gt; &gt; have to disagree. Since I am opposed to killing
</em><br>
<em>&gt; &gt; myself, all other being
</em><br>
<em>&gt; &gt; equal, one of the outcomes is regarded as inferior
</em><br>
<em>&gt; &gt; in any moral system
</em><br>
<em>&gt; &gt; striving to fulfill the wishes of sentients,
</em><br>
<em>&gt; &gt; including mine.
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; Rafal
</em><br>
<em>&gt; &gt;  
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, let me try to explain the first part of the
</em><br>
<em>&gt; paragraph.  As I understand it, Eliezer believes
</em><br>
<em>&gt; that
</em><br>
<em>&gt; there exists a morality which is normative (all
</em><br>
<em>&gt; ethical sentients would converge on it if they
</em><br>
<em>&gt; thought
</em><br>
<em>&gt; about it for long enough).  That's why I called it a
</em><br>
<em>&gt; 'Universal Morality'  (It's morally symmetric).  And
</em><br>
<em>&gt; he's trying to come up an FAI which converges on
</em><br>
<em>&gt; this
</em><br>
<em>&gt; morality.  But, if all sentient morality was this
</em><br>
<em>&gt; Universal Morality alone, then all sentient
</em><br>
<em>&gt; moralities
</em><br>
<em>&gt; would be identical  (Because the universal morality
</em><br>
<em>&gt; is
</em><br>
<em>&gt; normative and morally symmetric).  So I'm asking why
</em><br>
<em>&gt; it's desirable to build an FAI which just follows
</em><br>
<em>&gt; this
</em><br>
<em>&gt; morality alone.  Why shouldn't FAI's have some
</em><br>
<em>&gt; personal goals on top of the Universal Morality? 
</em><br>
<em>&gt; (So
</em><br>
<em>&gt; long as these personal goals didn't contradict the
</em><br>
<em>&gt; Universal Morality).  Do you see what I'm saying?
</em><br>
<p>Perhaps there is room for differentiation (maybe a lot
<br>
of differentiation) among sentients, but you would
<br>
really want the Sysop (or whatever you call it) to be
<br>
not-observer centered at all, and the Sysop, or the
<br>
first AI to initiate the Singularity, or whatever you
<br>
want to call it, is the being we're trying to develop.
<br>
<p><em>&gt; As regards the second part of what I was saying, in
</em><br>
<em>&gt; the example given of course IF you think that
</em><br>
<em>&gt; killing
</em><br>
<em>&gt; yourself would not be desireable, then Eliezer's FAI
</em><br>
<em>&gt; agrees to designate your choice as 'good'.  But the
</em><br>
<em>&gt; FAI can't morally distinguigh between any of the
</em><br>
<em>&gt; choices you do in fact make.  For instance IF you
</em><br>
<em>&gt; did
</em><br>
<em>&gt; decide that you wanted to kill yourself one day,
</em><br>
<em>&gt; then
</em><br>
<em>&gt; the FAI would see this as 'just another choice', no
</em><br>
<em>&gt; better or worse than your previous choices that you
</em><br>
<em>&gt; wanted to live  (It would in general see all
</em><br>
<em>&gt; requests
</em><br>
<em>&gt; consistent with 'volition' as equal).  In order to
</em><br>
<em>&gt; have an FAI which valued transhumanist goals you'd
</em><br>
<em>&gt; probably have to directly program some 'Personal
</em><br>
<em>&gt; Values' into the FAI, in addition to having an FAI
</em><br>
<em>&gt; which could reason about Universal Morality (the
</em><br>
<em>&gt; class
</em><br>
<em>&gt; of morally symmetric interactions).  You see what
</em><br>
<em>&gt; I'm
</em><br>
<em>&gt; saying?   
</em><br>
<p>I just don't get how Yudkowskian Friendliness = Unable
<br>
To Distinguish Event A From Event B.
<br>
<p>__________________________________
<br>
Do you Yahoo!?
<br>
Get better spam protection with Yahoo! Mail.
<br>
<a href="http://antispam.yahoo.com/tools">http://antispam.yahoo.com/tools</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8088.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Previous message:</strong> <a href="8086.html">Tomaz Kristan: "Re: [SL4] AI --&gt; Jobless Economy"</a>
<li><strong>In reply to:</strong> <a href="8080.html">Marc Geddes: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8087">[ date ]</a>
<a href="index.html#8087">[ thread ]</a>
<a href="subject.html#8087">[ subject ]</a>
<a href="author.html#8087">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
