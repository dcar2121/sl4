<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Humane-ness</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Humane-ness">
<meta name="Date" content="2004-02-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Humane-ness</h1>
<!-- received="Tue Feb 17 11:28:14 2004" -->
<!-- isoreceived="20040217182814" -->
<!-- sent="Tue, 17 Feb 2004 13:35:04 -0500" -->
<!-- isosent="20040217183504" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Humane-ness" -->
<!-- id="BMECIIDGKPGNFPJLIDNPGEBPCNAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="BMECIIDGKPGNFPJLIDNPAEBMCNAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Humane-ness"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Tue Feb 17 2004 - 11:35:04 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7961.html">Ben Goertzel: "RE: Humane-ness"</a>
<li><strong>Previous message:</strong> <a href="7959.html">Ben Goertzel: "Humane-ness"</a>
<li><strong>In reply to:</strong> <a href="7959.html">Ben Goertzel: "Humane-ness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7961.html">Ben Goertzel: "RE: Humane-ness"</a>
<li><strong>Reply:</strong> <a href="7961.html">Ben Goertzel: "RE: Humane-ness"</a>
<li><strong>Reply:</strong> <a href="7966.html">Metaqualia: "Re: Humane-ness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7960">[ date ]</a>
<a href="index.html#7960">[ thread ]</a>
<a href="subject.html#7960">[ subject ]</a>
<a href="author.html#7960">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
following up...
<br>
<p>So, in sum, the difficulties with Humane AI *as I understand it* are
<br>
<p>1.	The difficulty of defining humane-ness
<br>
2.	The presence of delusions that I judge ethically undesirable, in the
<br>
near-consensus worldview of humanity
<br>
<p>The second point here may seem bizarrely egomaniacal – who am I to judge the
<br>
vast mass of humanity as being ethically wrong on major points?  And yet, it
<br>
has to be observed that the vast mass of humanity has shifted its ethical
<br>
beliefs many times over history.  At many points in history, the vast mass
<br>
of humans believed slavery was ethical, for instance.  Now, you could argue
<br>
that if they’d had enough information, and carried out enough discussion and
<br>
deliberation, they might have decided it was bad.  Perhaps this is the case.
<br>
But to lead the human race through a process of discussion, deliberation and
<br>
discovery adequate to free it from its collective delusions – this is a very
<br>
large task.  I see no evidence that any existing political institution is up
<br>
to this task.  Perhaps an AGI could carry out this process – but then what
<br>
is the goal system of this AGI?  Do we begin this goal system with the
<br>
current ethical systems of the human race – as Eliezer seems to suggest in
<br>
the quote I gave (“Human nature is not a bad place to start…”)?  In that
<br>
case, does the AGI begin by believing in God and reincarnation, which are
<br>
beliefs of the vast majority of humans?  Or does the AGI begin with some
<br>
other guiding principle, such as Voluntary Joyous Growth?  My hypothesis is
<br>
that an AGI beginning with Voluntary Joyous Growth as a guiding principle is
<br>
more likely to help humanity along a path of increasing wisdom and
<br>
humane-ness than an AGI beginning with current human nature as a guiding
<br>
principle.
<br>
<p>One can posit, as a goal, the creation of a Humane AI that embodies
<br>
humane-ness as discovered by humanity via interaction with an appropriately
<br>
guided AGI.  However, I’m not sure what this adds, beyond what one gets from
<br>
creating an AGI that follows the principle of Voluntary Joyous Growth and
<br>
leaving it to interact with humanity.  If the creation of the Humane AI is
<br>
going to make humans happier, and going to help humans to grow, and going to
<br>
be something that humans choose, then the Voluntary Joyous Growth based AGI
<br>
is going to choose it anyway.  On the other hand, maybe after humans become
<br>
wiser, they’ll realize that the creation of an AGI embodying the average of
<br>
human wishes is not such a great goal anyway.  As an alternative, perhaps a
<br>
host of different AGI’s will be created, embodying different aspects of
<br>
human nature and humane-ness, and allowed to evolve radically in different
<br>
directions.
<br>
<p>-- Ben G
<br>
<p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: Ben Goertzel [mailto:<a href="mailto:ben@goertzel.org?Subject=RE:%20Humane-ness">ben@goertzel.org</a>]
</em><br>
<em>&gt; Sent: Tuesday, February 17, 2004 12:53 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=RE:%20Humane-ness">sl4@sl4.org</a>
</em><br>
<em>&gt; Subject: Humane-ness
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Eliezer,
</em><br>
<em>&gt;
</em><br>
<em>&gt; Trolling the Net briefly, I found this quote from you (from the
</em><br>
<em>&gt; WTA list in Aug. 2003):
</em><br>
<em>&gt;
</em><br>
<em>&gt; ***
</em><br>
<em>&gt; The important thing is not to be human but to be humane. ...
</em><br>
<em>&gt;
</em><br>
<em>&gt; Though we might wish to believe that Hitler was an inhuman
</em><br>
<em>&gt; monster, he was, in fact, a human monster; and Gandhi is noted
</em><br>
<em>&gt; not for being remarkably human but for being remarkably humane.
</em><br>
<em>&gt; The attributes of our species are not exempt from ethical
</em><br>
<em>&gt; examination in virtue of being &quot;natural&quot; or &quot;human&quot;. Some human
</em><br>
<em>&gt; attributes, such as empathy and a sense of fairness, are
</em><br>
<em>&gt; positive; others, such as a tendency toward tribalism or
</em><br>
<em>&gt; groupishness, have left deep scars on human history. If there is
</em><br>
<em>&gt; value in being human, it comes, not from being &quot;normal&quot; or
</em><br>
<em>&gt; &quot;natural&quot;, but from having within us the raw material for
</em><br>
<em>&gt; humaneness: compassion, a sense of humor, curiosity, the wish to
</em><br>
<em>&gt; be a better person. Trying to preserve &quot;humanness&quot;, rather than
</em><br>
<em>&gt; cultivating humaneness, would idolize the bad along with the
</em><br>
<em>&gt; good. One might say that if &quot;human&quot; is what we are, then &quot;humane&quot;
</em><br>
<em>&gt; is what we, as humans, wish we were. Human nature is not a bad
</em><br>
<em>&gt; place to start that journey, but we can't fulfill that potential
</em><br>
<em>&gt; if we reject any progress past the starting point.
</em><br>
<em>&gt; ***
</em><br>
<em>&gt;
</em><br>
<em>&gt; If the goal of your &quot;Friendly AI&quot; project is to create an AI that
</em><br>
<em>&gt; is &quot;humane&quot; in this sense, then perhaps &quot;Humane AI&quot; would be a
</em><br>
<em>&gt; better name for the project...
</em><br>
<em>&gt;
</em><br>
<em>&gt; I have a few comments here.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1)
</em><br>
<em>&gt; I am not sure that humane-ness, in the sense that you propose, is
</em><br>
<em>&gt; a well-defined concept.  Doesn't the specific set of properties
</em><br>
<em>&gt; called &quot;humaneness&quot; you get depend on the specific algorithm that
</em><br>
<em>&gt; you use to sum together the wishes of various individuals in the
</em><br>
<em>&gt; world?  If so, then how do you propose to choose among the
</em><br>
<em>&gt; different algorithms?
</em><br>
<em>&gt;
</em><br>
<em>&gt; 2)
</em><br>
<em>&gt; How do you propose to distinguish the &quot;positive&quot; from the
</em><br>
<em>&gt; &quot;negative&quot; aspects of human nature ... e.g. compassion versus
</em><br>
<em>&gt; tribalism?  I guess you want to distinguish these by a kind of
</em><br>
<em>&gt; near-consensus process -- e.g. you're hoping that most people, on
</em><br>
<em>&gt; careful consideration and discussion, will agree that tribalism
</em><br>
<em>&gt; although humanly universal, isn't good?  I'm not so confident
</em><br>
<em>&gt; that people's &quot;wishes regarding what they were&quot; are good ones...
</em><br>
<em>&gt; (which is another way of saying: I think my own ethic differs
</em><br>
<em>&gt; considerably from the mean of humanity's)
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Do you propose to evaluate
</em><br>
<em>&gt;
</em><br>
<em>&gt; P(X is humane) = P(X is considered good by H after careful
</em><br>
<em>&gt; reflection and discussion | H is human)
</em><br>
<em>&gt;
</em><br>
<em>&gt; I guess you're thinking of something more complicated along these
</em><br>
<em>&gt; lines (?)
</em><br>
<em>&gt;
</em><br>
<em>&gt; One runs into serious issues with cultural and individual
</em><br>
<em>&gt; relativity here.
</em><br>
<em>&gt;
</em><br>
<em>&gt; For instance, the vast majority of humans believe that
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Belief in God&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; is a good and important aspect of human nature.  Thus, it seems
</em><br>
<em>&gt; to me, &quot;Belief in God&quot; should be considered humane according to
</em><br>
<em>&gt; your definition -- it's part of what we humans are, AND, part of
</em><br>
<em>&gt; what we humans wish we were.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Nevertheless, I think that belief in God -- though it has some
</em><br>
<em>&gt; valuable spiritual intuitions at its core -- basically sucks.
</em><br>
<em>&gt; Thus, I consider it MY moral responsibilty to work so that belief
</em><br>
<em>&gt; in God is NOT projected beyond the human race into any AGI's we
</em><br>
<em>&gt; may create.  Unless (and I really doubt it) it's shown that the
</em><br>
<em>&gt; only way to achieve other valuable things is to create an AGi
</em><br>
<em>&gt; that's deluded in this way.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Of course, there are many other examples besides &quot;belief in God&quot;
</em><br>
<em>&gt; that could be used to illustrate this point.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You could try to define humaneness as something like &quot;What humans
</em><br>
<em>&gt; WOULD wish they were, if they were wiser humans&quot; -- but we humans
</em><br>
<em>&gt; are fucking UNwise creatures, and this is really quite essential
</em><br>
<em>&gt; to our humanity... and of course, defining this requires some
</em><br>
<em>&gt; ethical or metaethical standard beyond what humans are or wish they were.
</em><br>
<em>&gt;
</em><br>
<em>&gt; ??
</em><br>
<em>&gt;
</em><br>
<em>&gt; -- Ben G
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7961.html">Ben Goertzel: "RE: Humane-ness"</a>
<li><strong>Previous message:</strong> <a href="7959.html">Ben Goertzel: "Humane-ness"</a>
<li><strong>In reply to:</strong> <a href="7959.html">Ben Goertzel: "Humane-ness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7961.html">Ben Goertzel: "RE: Humane-ness"</a>
<li><strong>Reply:</strong> <a href="7961.html">Ben Goertzel: "RE: Humane-ness"</a>
<li><strong>Reply:</strong> <a href="7966.html">Metaqualia: "Re: Humane-ness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7960">[ date ]</a>
<a href="index.html#7960">[ thread ]</a>
<a href="subject.html#7960">[ subject ]</a>
<a href="author.html#7960">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
