<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Humane-ness</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Humane-ness">
<meta name="Date" content="2004-02-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Humane-ness</h1>
<!-- received="Tue Feb 17 11:36:11 2004" -->
<!-- isoreceived="20040217183611" -->
<!-- sent="Tue, 17 Feb 2004 13:43:00 -0500" -->
<!-- isosent="20040217184300" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Humane-ness" -->
<!-- id="BMECIIDGKPGNFPJLIDNPAECACNAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="BMECIIDGKPGNFPJLIDNPGEBPCNAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Humane-ness"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Tue Feb 17 2004 - 11:43:00 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7962.html">Robin Lee Powell: "Re: Humane-ness"</a>
<li><strong>Previous message:</strong> <a href="7960.html">Ben Goertzel: "RE: Humane-ness"</a>
<li><strong>In reply to:</strong> <a href="7960.html">Ben Goertzel: "RE: Humane-ness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7964.html">Chris Healey: "RE: Humane-ness"</a>
<li><strong>Reply:</strong> <a href="7964.html">Chris Healey: "RE: Humane-ness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7961">[ date ]</a>
<a href="index.html#7961">[ thread ]</a>
<a href="subject.html#7961">[ subject ]</a>
<a href="author.html#7961">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
And one more point
<br>
<p>I said in my essay that “Be nice to humans” or “Obey your human masters” are
<br>
simply too concrete and low-level ethical prescriptions to be expected to
<br>
survive the Transcension.
<br>
<p>However, I suggest that a highly complex and messy network of beliefs like
<br>
Eliezer’s “humane-ness” is insufficiently crisp, elegant and abstract to be
<br>
expected to survive the Transcension.
<br>
<p>I still suspect that abstract principles like &quot;Voluntary Joyous Growth&quot; have
<br>
a greater chance of survival.  Initially these are grounded in human
<br>
concepts and feelings -- in aspects of &quot;humane-ness&quot; -- as as the
<br>
Transcension proceeds they will gain other, related groundings.
<br>
<p>-- Ben G
<br>
<p><p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20Humane-ness">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20Humane-ness">owner-sl4@sl4.org</a>]On Behalf Of Ben
</em><br>
<em>&gt; Goertzel
</em><br>
<em>&gt; Sent: Tuesday, February 17, 2004 1:35 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=RE:%20Humane-ness">sl4@sl4.org</a>
</em><br>
<em>&gt; Subject: RE: Humane-ness
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; following up...
</em><br>
<em>&gt;
</em><br>
<em>&gt; So, in sum, the difficulties with Humane AI *as I understand it* are
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1.	The difficulty of defining humane-ness
</em><br>
<em>&gt; 2.	The presence of delusions that I judge ethically undesirable, in the
</em><br>
<em>&gt; near-consensus worldview of humanity
</em><br>
<em>&gt;
</em><br>
<em>&gt; The second point here may seem bizarrely egomaniacal – who am I
</em><br>
<em>&gt; to judge the
</em><br>
<em>&gt; vast mass of humanity as being ethically wrong on major points?
</em><br>
<em>&gt; And yet, it
</em><br>
<em>&gt; has to be observed that the vast mass of humanity has shifted its ethical
</em><br>
<em>&gt; beliefs many times over history.  At many points in history, the vast mass
</em><br>
<em>&gt; of humans believed slavery was ethical, for instance.  Now, you
</em><br>
<em>&gt; could argue
</em><br>
<em>&gt; that if they’d had enough information, and carried out enough
</em><br>
<em>&gt; discussion and
</em><br>
<em>&gt; deliberation, they might have decided it was bad.  Perhaps this
</em><br>
<em>&gt; is the case.
</em><br>
<em>&gt; But to lead the human race through a process of discussion,
</em><br>
<em>&gt; deliberation and
</em><br>
<em>&gt; discovery adequate to free it from its collective delusions –
</em><br>
<em>&gt; this is a very
</em><br>
<em>&gt; large task.  I see no evidence that any existing political
</em><br>
<em>&gt; institution is up
</em><br>
<em>&gt; to this task.  Perhaps an AGI could carry out this process – but then what
</em><br>
<em>&gt; is the goal system of this AGI?  Do we begin this goal system with the
</em><br>
<em>&gt; current ethical systems of the human race – as Eliezer seems to suggest in
</em><br>
<em>&gt; the quote I gave (“Human nature is not a bad place to start…”)?  In that
</em><br>
<em>&gt; case, does the AGI begin by believing in God and reincarnation, which are
</em><br>
<em>&gt; beliefs of the vast majority of humans?  Or does the AGI begin with some
</em><br>
<em>&gt; other guiding principle, such as Voluntary Joyous Growth?  My
</em><br>
<em>&gt; hypothesis is
</em><br>
<em>&gt; that an AGI beginning with Voluntary Joyous Growth as a guiding
</em><br>
<em>&gt; principle is
</em><br>
<em>&gt; more likely to help humanity along a path of increasing wisdom and
</em><br>
<em>&gt; humane-ness than an AGI beginning with current human nature as a guiding
</em><br>
<em>&gt; principle.
</em><br>
<em>&gt;
</em><br>
<em>&gt; One can posit, as a goal, the creation of a Humane AI that embodies
</em><br>
<em>&gt; humane-ness as discovered by humanity via interaction with an
</em><br>
<em>&gt; appropriately
</em><br>
<em>&gt; guided AGI.  However, I’m not sure what this adds, beyond what
</em><br>
<em>&gt; one gets from
</em><br>
<em>&gt; creating an AGI that follows the principle of Voluntary Joyous Growth and
</em><br>
<em>&gt; leaving it to interact with humanity.  If the creation of the Humane AI is
</em><br>
<em>&gt; going to make humans happier, and going to help humans to grow,
</em><br>
<em>&gt; and going to
</em><br>
<em>&gt; be something that humans choose, then the Voluntary Joyous Growth
</em><br>
<em>&gt; based AGI
</em><br>
<em>&gt; is going to choose it anyway.  On the other hand, maybe after
</em><br>
<em>&gt; humans become
</em><br>
<em>&gt; wiser, they’ll realize that the creation of an AGI embodying the
</em><br>
<em>&gt; average of
</em><br>
<em>&gt; human wishes is not such a great goal anyway.  As an alternative,
</em><br>
<em>&gt; perhaps a
</em><br>
<em>&gt; host of different AGI’s will be created, embodying different aspects of
</em><br>
<em>&gt; human nature and humane-ness, and allowed to evolve radically in different
</em><br>
<em>&gt; directions.
</em><br>
<em>&gt;
</em><br>
<em>&gt; -- Ben G
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; -----Original Message-----
</em><br>
<em>&gt; &gt; From: Ben Goertzel [mailto:<a href="mailto:ben@goertzel.org?Subject=RE:%20Humane-ness">ben@goertzel.org</a>]
</em><br>
<em>&gt; &gt; Sent: Tuesday, February 17, 2004 12:53 PM
</em><br>
<em>&gt; &gt; To: <a href="mailto:sl4@sl4.org?Subject=RE:%20Humane-ness">sl4@sl4.org</a>
</em><br>
<em>&gt; &gt; Subject: Humane-ness
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Eliezer,
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Trolling the Net briefly, I found this quote from you (from the
</em><br>
<em>&gt; &gt; WTA list in Aug. 2003):
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; ***
</em><br>
<em>&gt; &gt; The important thing is not to be human but to be humane. ...
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Though we might wish to believe that Hitler was an inhuman
</em><br>
<em>&gt; &gt; monster, he was, in fact, a human monster; and Gandhi is noted
</em><br>
<em>&gt; &gt; not for being remarkably human but for being remarkably humane.
</em><br>
<em>&gt; &gt; The attributes of our species are not exempt from ethical
</em><br>
<em>&gt; &gt; examination in virtue of being &quot;natural&quot; or &quot;human&quot;. Some human
</em><br>
<em>&gt; &gt; attributes, such as empathy and a sense of fairness, are
</em><br>
<em>&gt; &gt; positive; others, such as a tendency toward tribalism or
</em><br>
<em>&gt; &gt; groupishness, have left deep scars on human history. If there is
</em><br>
<em>&gt; &gt; value in being human, it comes, not from being &quot;normal&quot; or
</em><br>
<em>&gt; &gt; &quot;natural&quot;, but from having within us the raw material for
</em><br>
<em>&gt; &gt; humaneness: compassion, a sense of humor, curiosity, the wish to
</em><br>
<em>&gt; &gt; be a better person. Trying to preserve &quot;humanness&quot;, rather than
</em><br>
<em>&gt; &gt; cultivating humaneness, would idolize the bad along with the
</em><br>
<em>&gt; &gt; good. One might say that if &quot;human&quot; is what we are, then &quot;humane&quot;
</em><br>
<em>&gt; &gt; is what we, as humans, wish we were. Human nature is not a bad
</em><br>
<em>&gt; &gt; place to start that journey, but we can't fulfill that potential
</em><br>
<em>&gt; &gt; if we reject any progress past the starting point.
</em><br>
<em>&gt; &gt; ***
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; If the goal of your &quot;Friendly AI&quot; project is to create an AI that
</em><br>
<em>&gt; &gt; is &quot;humane&quot; in this sense, then perhaps &quot;Humane AI&quot; would be a
</em><br>
<em>&gt; &gt; better name for the project...
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I have a few comments here.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; 1)
</em><br>
<em>&gt; &gt; I am not sure that humane-ness, in the sense that you propose, is
</em><br>
<em>&gt; &gt; a well-defined concept.  Doesn't the specific set of properties
</em><br>
<em>&gt; &gt; called &quot;humaneness&quot; you get depend on the specific algorithm that
</em><br>
<em>&gt; &gt; you use to sum together the wishes of various individuals in the
</em><br>
<em>&gt; &gt; world?  If so, then how do you propose to choose among the
</em><br>
<em>&gt; &gt; different algorithms?
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; 2)
</em><br>
<em>&gt; &gt; How do you propose to distinguish the &quot;positive&quot; from the
</em><br>
<em>&gt; &gt; &quot;negative&quot; aspects of human nature ... e.g. compassion versus
</em><br>
<em>&gt; &gt; tribalism?  I guess you want to distinguish these by a kind of
</em><br>
<em>&gt; &gt; near-consensus process -- e.g. you're hoping that most people, on
</em><br>
<em>&gt; &gt; careful consideration and discussion, will agree that tribalism
</em><br>
<em>&gt; &gt; although humanly universal, isn't good?  I'm not so confident
</em><br>
<em>&gt; &gt; that people's &quot;wishes regarding what they were&quot; are good ones...
</em><br>
<em>&gt; &gt; (which is another way of saying: I think my own ethic differs
</em><br>
<em>&gt; &gt; considerably from the mean of humanity's)
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Do you propose to evaluate
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; P(X is humane) = P(X is considered good by H after careful
</em><br>
<em>&gt; &gt; reflection and discussion | H is human)
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I guess you're thinking of something more complicated along these
</em><br>
<em>&gt; &gt; lines (?)
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; One runs into serious issues with cultural and individual
</em><br>
<em>&gt; &gt; relativity here.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; For instance, the vast majority of humans believe that
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &quot;Belief in God&quot;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; is a good and important aspect of human nature.  Thus, it seems
</em><br>
<em>&gt; &gt; to me, &quot;Belief in God&quot; should be considered humane according to
</em><br>
<em>&gt; &gt; your definition -- it's part of what we humans are, AND, part of
</em><br>
<em>&gt; &gt; what we humans wish we were.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Nevertheless, I think that belief in God -- though it has some
</em><br>
<em>&gt; &gt; valuable spiritual intuitions at its core -- basically sucks.
</em><br>
<em>&gt; &gt; Thus, I consider it MY moral responsibilty to work so that belief
</em><br>
<em>&gt; &gt; in God is NOT projected beyond the human race into any AGI's we
</em><br>
<em>&gt; &gt; may create.  Unless (and I really doubt it) it's shown that the
</em><br>
<em>&gt; &gt; only way to achieve other valuable things is to create an AGi
</em><br>
<em>&gt; &gt; that's deluded in this way.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Of course, there are many other examples besides &quot;belief in God&quot;
</em><br>
<em>&gt; &gt; that could be used to illustrate this point.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; You could try to define humaneness as something like &quot;What humans
</em><br>
<em>&gt; &gt; WOULD wish they were, if they were wiser humans&quot; -- but we humans
</em><br>
<em>&gt; &gt; are fucking UNwise creatures, and this is really quite essential
</em><br>
<em>&gt; &gt; to our humanity... and of course, defining this requires some
</em><br>
<em>&gt; &gt; ethical or metaethical standard beyond what humans are or wish
</em><br>
<em>&gt; they were.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; ??
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; -- Ben G
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7962.html">Robin Lee Powell: "Re: Humane-ness"</a>
<li><strong>Previous message:</strong> <a href="7960.html">Ben Goertzel: "RE: Humane-ness"</a>
<li><strong>In reply to:</strong> <a href="7960.html">Ben Goertzel: "RE: Humane-ness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7964.html">Chris Healey: "RE: Humane-ness"</a>
<li><strong>Reply:</strong> <a href="7964.html">Chris Healey: "RE: Humane-ness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7961">[ date ]</a>
<a href="index.html#7961">[ thread ]</a>
<a href="subject.html#7961">[ subject ]</a>
<a href="author.html#7961">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
