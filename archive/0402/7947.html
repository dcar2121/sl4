<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendly AI in &quot;Positive Transcension&quot;</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Friendly AI in &quot;Positive Transcension&quot;">
<meta name="Date" content="2004-02-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendly AI in &quot;Positive Transcension&quot;</h1>
<!-- received="Sun Feb 15 14:31:13 2004" -->
<!-- isoreceived="20040215213113" -->
<!-- sent="Sun, 15 Feb 2004 16:30:52 -0500" -->
<!-- isosent="20040215213052" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Friendly AI in &quot;Positive Transcension&quot;" -->
<!-- id="402FE50C.3030505@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="BMECIIDGKPGNFPJLIDNPOEMLCMAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Friendly%20AI%20in%20&quot;Positive%20Transcension&quot;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Feb 15 2004 - 14:30:52 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7948.html">Keith Henson: "Re: Ethics was In defense of physics"</a>
<li><strong>Previous message:</strong> <a href="7946.html">Ben Goertzel: "RE: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>In reply to:</strong> <a href="7946.html">Ben Goertzel: "RE: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7949.html">Metaqualia: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Reply:</strong> <a href="7949.html">Metaqualia: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Reply:</strong> <a href="7955.html">Ben Goertzel: "RE: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7947">[ date ]</a>
<a href="index.html#7947">[ thread ]</a>
<a href="subject.html#7947">[ subject ]</a>
<a href="author.html#7947">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; Hi,
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt; This cannot possibly be done.  What you're asking is undoable.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; OK, well if you can't even summarize your own work compactly, then how
</em><br>
<em>&gt; the heck do you expect me to be able to do so??? ;-)
</em><br>
<p>I don't, of course.  However, I should hope that you would be aware of 
<br>
your own lack of understanding and/or inability to compactly summarize, 
<br>
and avoid giving summaries of FAI that are the diametric opposite of what 
<br>
I mean by the term.  I don't understand Novamente, and I say so.  I 
<br>
wouldn't want to make your job of explaining Novamente even harder by 
<br>
giving my own &quot;explanation&quot; of it.
<br>
<p>To make progress on explaining FAI is my own burden, not yours; I only ask 
<br>
that you avoid making anti-progress.
<br>
<p><em>&gt;&gt; You're missing upward of a dozen fundamental concepts here.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Look, any brief summary is going to miss a lot of fundamental concepts.
</em><br>
<em>&gt; That is the nature of summary.  In summarizing something, one has to
</em><br>
<em>&gt; choose what to include and what to leave out.
</em><br>
<p>By &quot;missing&quot; I don't mean &quot;failing to include&quot; but &quot;making statements in 
<br>
contradiction of&quot;.
<br>
<p><em>&gt;&gt; First, let's delete &quot;programming or otherwise inculcating&quot; and
</em><br>
<em>&gt;&gt; replacing with &quot;choosing&quot;, which is the correct formulation under the
</em><br>
<em>&gt;&gt; basic theory of FAI, which makes extensive use of the expected
</em><br>
<em>&gt;&gt; utility principle. Choice subsumes choice over programming, choice
</em><br>
<em>&gt;&gt; over environmental information, and any other design options of which
</em><br>
<em>&gt;&gt; we might prefer one to another.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Fine, we can refer to &quot;choosing&quot;, while noting that programming and
</em><br>
<em>&gt; teaching are the apparently most likely forms of choosing in this
</em><br>
<em>&gt; context...
</em><br>
<p>Well, you could note that, if that were your opinion, I suppose.
<br>
<p><em>&gt;&gt; Next, more importantly, &quot;humane&quot; is not being given its intuitive
</em><br>
<em>&gt;&gt; sense here!  Humane is here a highly technical concept, &quot;renormalized
</em><br>
<em>&gt;&gt; humanity&quot;.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So far as I can tell this is a fuzzy and ill-defined and obscure
</em><br>
<em>&gt; concept, lacking a clear and compact definition
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Feel free to give one, or refer me to a specific paragraph in one of
</em><br>
<em>&gt; your online writings where such is given.
</em><br>
<p>On the contrary, I gave a fuzzy and ill-defined and obscure exposition of 
<br>
a clear and compact concept.  To make it clear and compact would require a 
<br>
lot of technical background and numerous new concepts, after which I could 
<br>
give a clear and compact definition.  I am not going to be able to define 
<br>
this thing in one paragraph.  I am not asking you to define it for me, 
<br>
either.  I am asking you, if I haven't finished explaining it, to avoid 
<br>
misexplaining it on my behalf, because it is complicated.
<br>
<p><em>&gt;&gt; It's not that you're misunderstanding *what specifically* I'm saying,
</em><br>
<em>&gt;&gt; but that you're misunderstanding the *sort of thing* I'm attempting
</em><br>
<em>&gt;&gt; to describe.  Not apples versus oranges, more like apples versus the 
</em><br>
<em>&gt;&gt; equation x'' = -kx.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; OK, so please clearly explain what sort of thing you're attempting to 
</em><br>
<em>&gt; describe.
</em><br>
<p>Why do you expect me to be able to do this off-the-cuff?  You once claimed 
<br>
that understanding AI ethics was work for decades and a Manhattan Project. 
<br>
&nbsp;&nbsp;If I have made progress, why do you expect that progress to be 
<br>
explainable in one paragraph?
<br>
<p>I am going to take a stab below, but don't be surprised if I fail.
<br>
<p>Presently, you seem to be in a state of feeling that there is nothing to 
<br>
understand.  The most I can seriously hope to do is to show you that there 
<br>
exists a thing-to-understand which is not similar to previously understood 
<br>
things.  Explaining the thing-to-understand seems to be beyond hope of our 
<br>
current communications bandwidth.
<br>
<p>Obviously I am still in the middle of working some things out.  The 
<br>
problem arises when you present Friendly AI to your readers as a finished 
<br>
theory that contradicts this thing that I am in the process of working 
<br>
out.  Additional reasons why I am emotionally annoyed is that you are 
<br>
presenting FAI as the advocacy of a class of theories that I took specific 
<br>
offense at in 1996, thus starting me down the road that in 2000 would lead 
<br>
to my first faltering attempts at doing real work in FAI.  You are 
<br>
presenting FAI as the problem I created FAI to solve.  You accuse me of 
<br>
advocating exactly the position that I reacted against by creating FAI. 
<br>
(I can provide historical references to document this on request.)
<br>
<p>That you also dismiss FAI because of supposed failure address exactly 
<br>
those problems that FAI was created specifically to solve, is also 
<br>
personally annoying, but much more forgiveable, since if I have not solved 
<br>
those problems to your satisfaction, I have not.  At most I would object 
<br>
to the implicit connotation that I haven't thought of the problems.
<br>
<p>It is just the active misrepresentation of myself as holding the diametric 
<br>
opposite of my actual views that I am complaining about here.  I quite 
<br>
understand that you cannot be expected to explain FAI; that is my job.
<br>
<p><em>&gt;&gt; Your most serious obstacle here is your inability to see anything
</em><br>
<em>&gt;&gt; except the specific content of an ethical system - you see &quot;Joyous
</em><br>
<em>&gt;&gt; Growth&quot; as a specific ethical system, you see &quot;benevolence&quot; as
</em><br>
<em>&gt;&gt; specific content, your mental model of &quot;humaneness&quot; is
</em><br>
<em>&gt;&gt; something-or-other with specific ethical content.  &quot;Humaneness&quot; as
</em><br>
<em>&gt;&gt; I'm describing it *produces* specific ethical content but *is not
</em><br>
<em>&gt;&gt; composed of* specific ethical content.  Imagine the warm fuzzy
</em><br>
<em>&gt;&gt; feeling that you get when considering &quot;Joyous Growth&quot;.  Now, 
</em><br>
<em>&gt;&gt; throughout history and across the globe, do you think that only 
</em><br>
<em>&gt;&gt; 21st-century Americans get warm fuzzy feelings when considering their
</em><br>
<em>&gt;&gt;  personal moral philosophies?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And, I *don't* see abstract ethical principles as being specific
</em><br>
<em>&gt; ethical systems, I tried to very clearly draw that distinction in my
</em><br>
<em>&gt; essay, by defining abstract ethical principles as tools for judging
</em><br>
<em>&gt; specific ethical systems, and defining ethical systems as factories for
</em><br>
<em>&gt; producing ethical rules.
</em><br>
<p>To me these are subcategories of essentially the same kind of content, 
<br>
humans thinking in philosophical mode.  The commensurability of abstract 
<br>
ethical principles, specific ethical systems, and ethical rules, as 
<br>
cognitive content, are why we have no trouble in reasoning from verbally 
<br>
stated ethical principles to specific ethical systems and so on.  They are 
<br>
produced by human philosophers; they are argued all in the same midnight 
<br>
bull sessions; they are written down in books and transmitted memetically. 
<br>
&nbsp;&nbsp;Children receive them from parents as instructions, understand them as 
<br>
social expectations, feel their force using the emotions and social models 
<br>
of frontal cortex.  Principles, systems, and rules are subcategories of 
<br>
the same kind of mental thing manipulated in the mind - they've got the 
<br>
same representation.  Humans argue about principles, systems, and rules 
<br>
fairly intuitively - the debate goes back to Plato at the least.
<br>
<p>FAI::humaneness is a different *kind of thing* from this.  There is no way 
<br>
to argue FAI::humaneness to someone; it's not cognitive content, it's a 
<br>
what-it-does of a set of dynamics.  It's not the sort of thing you could 
<br>
describe in a philosophy book; you would describe it by pointing to a 
<br>
complete FAI system that implements FAI::humaneness.  It's not an 
<br>
arguable-thing, or a verbal-language-thing.  Consider the difference 
<br>
between a picture, the visual cortex, the genetic information that 
<br>
specifies how to wire a visual cortex in a context of incoming visual 
<br>
stimuli, and the selection pressure that produces the genetic information. 
<br>
&nbsp;&nbsp;&nbsp;These are all *different kinds of things*.  I cannot explain 
<br>
FAI::humaneness by simple analogy to any of them, but it illustrates what 
<br>
I mean by incommensurability.
<br>
<p><em>&gt; I can understand if you're positing some kind of &quot;humaneness&quot; as an
</em><br>
<em>&gt; abstract ethical principle for producing specific human ethical
</em><br>
<em>&gt; systems.  It still seems to me like it's a messy, overcomplex,
</em><br>
<em>&gt; needlessly ill-defined ethical principle which is unlikely to be
</em><br>
<em>&gt; implantable in an AI or to survive a Transcension.
</em><br>
<p>FAI::humaneness is not an abstract ethical principle, or anything 
<br>
commensurate with abstract ethical principles.  (There does seem to be a 
<br>
*very* strong tendency for people to try to interpret all issues of AI 
<br>
morality in terms of that class of cognitive content, which is probably 
<br>
the number one problem I run into in explaining things.)  As for messy, 
<br>
overcomplex, needlessly ill-defined - I would have no problem with your 
<br>
paper if you accused FAI of that!  It is a common enough sentiment and not 
<br>
actively misrepresentative.  However, since I have failed to explain the 
<br>
&quot;sort of stuff&quot; that FAI::humaneness is made of, it is not surprising that 
<br>
you would consider it &quot;messy&quot;, &quot;overcomplex&quot;, &quot;unstable&quot; - or 
<br>
&quot;non-commutative&quot;, &quot;purplish-brown&quot;, or &quot;less than 0&quot;.  Heaven knows what 
<br>
sins the model in your mind is guilty of, since it's formed of stuff not 
<br>
commensurate with what I'm trying to describe.
<br>
<p><em>&gt;&gt; The dynamics of the thinking you do when you consider that question
</em><br>
<em>&gt;&gt; would form part of the &quot;renormalization&quot; step, step 4, the volition
</em><br>
<em>&gt;&gt; examining itself under reflection.  It is improper to speak of a vast
</em><br>
<em>&gt;&gt; morass of &quot;humane morality&quot; which needs to be renormalized, because
</em><br>
<em>&gt;&gt; the word &quot;humane&quot; was not introduced until after step 4.  You could
</em><br>
<em>&gt;&gt; speak of a vast contradictory morass of the summated outputs of human
</em><br>
<em>&gt;&gt; moralities, but if you add the &quot;e&quot; on the end, then in FAI theory it
</em><br>
<em>&gt;&gt; has the connotation of something already renormalized.  Furthermore,
</em><br>
<em>&gt;&gt; it is improper to speak of renormalizing the vast contradictory
</em><br>
<em>&gt;&gt; morass as such, because it's a superposition of outputs, not a
</em><br>
<em>&gt;&gt; dynamic process capable of renormalizing itself.  You can speak of
</em><br>
<em>&gt;&gt; renormalizing a given individual, or renormalizing a model based on a
</em><br>
<em>&gt;&gt; typical individual.
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt; This is all already taken into account in FAI theory.  At length.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, I'm not sure I believe there is a clear, consistent, meaningful, 
</em><br>
<em>&gt; usable entity corresponding to your two-word phrase &quot;humane morality.&quot;
</em><br>
<em>&gt; I'm not so sure this beast exists.  Maybe all there is, in the
</em><br>
<em>&gt; human-related moral sphere, is a complex mess of interrelated, largely
</em><br>
<em>&gt; self-contradictory ethical systems, guided by some general principles
</em><br>
<em>&gt; of complex systems dynamics and by our biological habits and heritage.
</em><br>
<p>If you think FAI is messy, self-contradictory, etc., I have no problems 
<br>
with you putting those criticisms into the paper.
<br>
<p>What I wish you to avoid is presenting FAI as if it were a specific 
<br>
ethical principle, or anything commensurate with a specific ethical 
<br>
principle, because that's the *wrong sort of stuff* to describe either 
<br>
FAI::humaneness or FAI architectural principles.  It is, in fact, 
<br>
diametrically opposed to FAI, which was created to address inherent and 
<br>
fundamental problems with trying to imbue specific ethical principles into 
<br>
an AI - this is what I consider to be the generalized Asimov Law fallacy. 
<br>
&nbsp;&nbsp;I have the same objections to Joyous Growth, but that's a separate 
<br>
issue.  It's not surprising that you see no superiority of FAI over Joyous 
<br>
Growth if you attempt to somehow interpret FAI as a specific ethical 
<br>
principle.  This is actually impossible, like trying to interpret a 
<br>
mathematical system as the quantity 15, but after you've invented a 
<br>
specific ethical principle such as &quot;benevolence toward humans&quot; and 
<br>
attached that concept to Goertzel::FAI, then Goertzel::FAI would indeed 
<br>
have no advantage over Joyous Growth.  It omits the entire problem FAI was 
<br>
intended to solve.  In fact, it misrepresents FAI as something which 
<br>
happens to be a clear instance of the problem FAI was intended to solve. 
<br>
Do you see why I'm objecting here?
<br>
<p>If you say that the specific ethical principle underlying FAI is 
<br>
&quot;benevolence toward humans&quot;, this is objectionable because it 
<br>
misrepresents FAI as a specific ethical principle, and no amount of 
<br>
rephrasing will get rid of that.  Criticisms of FAI on the basis of being 
<br>
too specific and concrete as an ethical principle implicitly misrepresent 
<br>
FAI as an ethical principle; I would much prefer that these criticisms be 
<br>
amended to state that FAI involves too much specific and concrete 
<br>
&quot;information&quot;, which, if taken as implying Shannon information about the 
<br>
space of possible outcomes, would not imply anything actively misleading 
<br>
about the nature of FAI.  And so on.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7948.html">Keith Henson: "Re: Ethics was In defense of physics"</a>
<li><strong>Previous message:</strong> <a href="7946.html">Ben Goertzel: "RE: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>In reply to:</strong> <a href="7946.html">Ben Goertzel: "RE: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7949.html">Metaqualia: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Reply:</strong> <a href="7949.html">Metaqualia: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Reply:</strong> <a href="7955.html">Ben Goertzel: "RE: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7947">[ date ]</a>
<a href="index.html#7947">[ thread ]</a>
<a href="subject.html#7947">[ subject ]</a>
<a href="author.html#7947">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
