<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: All sentient have to be observer-centered!  My theory of FAI morality</title>
<meta name="Author" content="Marc Geddes (marc_geddes@yahoo.co.nz)">
<meta name="Subject" content="RE: All sentient have to be observer-centered!  My theory of FAI morality">
<meta name="Date" content="2004-02-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: All sentient have to be observer-centered!  My theory of FAI morality</h1>
<!-- received="Sat Feb 28 22:14:45 2004" -->
<!-- isoreceived="20040229051445" -->
<!-- sent="Sun, 29 Feb 2004 18:14:44 +1300 (NZDT)" -->
<!-- isosent="20040229051444" -->
<!-- name="Marc Geddes" -->
<!-- email="marc_geddes@yahoo.co.nz" -->
<!-- subject="RE: All sentient have to be observer-centered!  My theory of FAI morality" -->
<!-- id="20040229051444.74196.qmail@web20211.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="OJEHKDIANIFPAJPDBDGLKEFBDAAA.rafal@smigrodzki.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marc Geddes (<a href="mailto:marc_geddes@yahoo.co.nz?Subject=RE:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality"><em>marc_geddes@yahoo.co.nz</em></a>)<br>
<strong>Date:</strong> Sat Feb 28 2004 - 22:14:44 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8081.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Previous message:</strong> <a href="8079.html">Philip Sutton: "UNU report 2003 identified human-machine intelligence as key issue"</a>
<li><strong>In reply to:</strong> <a href="8078.html">Rafal Smigrodzki: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8087.html">Tommy McCabe: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8087.html">Tommy McCabe: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8080">[ date ]</a>
<a href="index.html#8080">[ thread ]</a>
<a href="subject.html#8080">[ subject ]</a>
<a href="author.html#8080">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;--- Rafal Smigrodzki &lt;<a href="mailto:rafal@smigrodzki.org?Subject=RE:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality">rafal@smigrodzki.org</a>&gt; wrote: &gt;
<br>
Marc wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &gt; I don't regard the evolutionary arguments as very
</em><br>
<em>&gt; &gt; convincing.  They're based on observation, not
</em><br>
<em>&gt; &gt; experiment.  Besides, it's only very recently in
</em><br>
<em>&gt; &gt; evolutionary history that the first sentients
</em><br>
<em>&gt; (humans)
</em><br>
<em>&gt; &gt; appeared.  It's the class of sentients that is
</em><br>
<em>&gt; &gt; revelent to FAI work.  Evolutionary observations
</em><br>
<em>&gt; about
</em><br>
<em>&gt; &gt; non-sentients is not likely to say much of
</em><br>
<em>&gt; relevence.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ### You might wish to read some evolutionary
</em><br>
<em>&gt; psychology texts.
</em><br>
<p>Um..well O.K sure I don't doubt that evolutionary
<br>
psychology is very relevent to HUMAN psychology, but
<br>
it is of much revelevence to the general class of
<br>
SENTIENT psychology?  I'm not sure evolutionary
<br>
psychology says much one or the other.
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt; -----------------------------
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; In any event, I don't regard non observer based
</em><br>
<em>&gt; &gt; sentients as even desireable (See my other
</em><br>
<em>&gt; replies).
</em><br>
<em>&gt; &gt; If you strip out all observer centered goals,
</em><br>
<em>&gt; you're
</em><br>
<em>&gt; &gt; left with normative altruism.  All sentients would
</em><br>
<em>&gt; &gt; converge on this, and all individual uniqueness
</em><br>
<em>&gt; would
</em><br>
<em>&gt; &gt; be stripped away.  You'd be left with bland
</em><br>
<em>&gt; &gt; uniformity.  An empty husk.  Universal morality is
</em><br>
<em>&gt; &gt; probably just a very general set of contrainsts,
</em><br>
<em>&gt; and
</em><br>
<em>&gt; &gt; FAI's following this alone would be qute unable to
</em><br>
<em>&gt; &gt; distinguish between the myraid of interesting
</em><br>
<em>&gt; personal
</em><br>
<em>&gt; &gt; goals that are consistent with it.  Everything
</em><br>
<em>&gt; that
</em><br>
<em>&gt; &gt; didn't hurt others (assuming that Universal
</em><br>
<em>&gt; Morality
</em><br>
<em>&gt; &gt; is volition based) whould be equally 'Good' to
</em><br>
<em>&gt; such an
</em><br>
<em>&gt; &gt; FAI.  There would be no possibility of anything
</em><br>
<em>&gt; &gt; unquinely human or personal.  For instance the two
</em><br>
<em>&gt; &gt; outcomes 'Rafal kills himself', 'Rafal doesn't
</em><br>
<em>&gt; kill
</em><br>
<em>&gt; &gt; humself' would be designated as morally equivalent
</em><br>
<em>&gt; &gt; under Volitional Morality.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ### I don't understand the first part of your
</em><br>
<em>&gt; paragraph. As to your claim
</em><br>
<em>&gt; about what would and would not be equivalent under
</em><br>
<em>&gt; volitional morality, I
</em><br>
<em>&gt; have to disagree. Since I am opposed to killing
</em><br>
<em>&gt; myself, all other being
</em><br>
<em>&gt; equal, one of the outcomes is regarded as inferior
</em><br>
<em>&gt; in any moral system
</em><br>
<em>&gt; striving to fulfill the wishes of sentients,
</em><br>
<em>&gt; including mine.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Rafal
</em><br>
<em>&gt;  
</em><br>
<p>Well, let me try to explain the first part of the
<br>
paragraph.  As I understand it, Eliezer believes that
<br>
there exists a morality which is normative (all
<br>
ethical sentients would converge on it if they thought
<br>
about it for long enough).  That's why I called it a
<br>
'Universal Morality'  (It's morally symmetric).  And
<br>
he's trying to come up an FAI which converges on this
<br>
morality.  But, if all sentient morality was this
<br>
Universal Morality alone, then all sentient moralities
<br>
would be identical  (Because the universal morality is
<br>
normative and morally symmetric).  So I'm asking why
<br>
it's desirable to build an FAI which just follows this
<br>
morality alone.  Why shouldn't FAI's have some
<br>
personal goals on top of the Universal Morality?  (So
<br>
long as these personal goals didn't contradict the
<br>
Universal Morality).  Do you see what I'm saying?
<br>
<p>As regards the second part of what I was saying, in
<br>
the example given of course IF you think that killing
<br>
yourself would not be desireable, then Eliezer's FAI
<br>
agrees to designate your choice as 'good'.  But the
<br>
FAI can't morally distinguigh between any of the
<br>
choices you do in fact make.  For instance IF you did
<br>
decide that you wanted to kill yourself one day, then
<br>
the FAI would see this as 'just another choice', no
<br>
better or worse than your previous choices that you
<br>
wanted to live  (It would in general see all requests
<br>
consistent with 'volition' as equal).  In order to
<br>
have an FAI which valued transhumanist goals you'd
<br>
probably have to directly program some 'Personal
<br>
Values' into the FAI, in addition to having an FAI
<br>
which could reason about Universal Morality (the class
<br>
of morally symmetric interactions).  You see what I'm
<br>
saying?   
<br>
<p>&nbsp;&nbsp;
<br>
<p>=====
<br>
Please visit my web-site at:  <a href="http://www.prometheuscrack.com">http://www.prometheuscrack.com</a>
<br>
<p><a href="http://personals.yahoo.com.au">http://personals.yahoo.com.au</a> - Yahoo! Personals
<br>
New people, new possibilities. FREE for a limited time.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8081.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Previous message:</strong> <a href="8079.html">Philip Sutton: "UNU report 2003 identified human-machine intelligence as key issue"</a>
<li><strong>In reply to:</strong> <a href="8078.html">Rafal Smigrodzki: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8087.html">Tommy McCabe: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8087.html">Tommy McCabe: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8080">[ date ]</a>
<a href="index.html#8080">[ thread ]</a>
<a href="subject.html#8080">[ subject ]</a>
<a href="author.html#8080">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
