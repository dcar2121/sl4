<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Ethics was In defense of physics</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Ethics was In defense of physics">
<meta name="Date" content="2004-02-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Ethics was In defense of physics</h1>
<!-- received="Sun Feb 15 20:37:29 2004" -->
<!-- isoreceived="20040216033729" -->
<!-- sent="Sun, 15 Feb 2004 22:37:14 -0500" -->
<!-- isosent="20040216033714" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Ethics was In defense of physics" -->
<!-- id="40303AEA.2000007@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="5.1.0.14.0.20040215174123.02f57ec0@pop.bloor.is.net.cable.rogers.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Ethics%20was%20In%20defense%20of%20physics"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Feb 15 2004 - 20:37:14 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7951.html">Ben Goertzel: "RE: Positive Transcension"</a>
<li><strong>Previous message:</strong> <a href="7949.html">Metaqualia: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>In reply to:</strong> <a href="7948.html">Keith Henson: "Re: Ethics was In defense of physics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7971.html">Keith Henson: "Re: Ethics was In defense of physics"</a>
<li><strong>Reply:</strong> <a href="7971.html">Keith Henson: "Re: Ethics was In defense of physics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7950">[ date ]</a>
<a href="index.html#7950">[ thread ]</a>
<a href="subject.html#7950">[ subject ]</a>
<a href="author.html#7950">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Keith Henson wrote:
<br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; The point is that you can perform all learning necessary to the task 
</em><br>
<em>&gt;&gt; of transforming everything in sight into paperclips, and you won't 
</em><br>
<em>&gt;&gt; have conflicts with distant parts of yourself that also want to 
</em><br>
<em>&gt;&gt; transform everything into paperclips - the target is constant, only 
</em><br>
<em>&gt;&gt; the aim gets updated.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And the weapons, and the gravity field, and if it starts thinking about 
</em><br>
<em>&gt; what it is doing and what paper clips are used for, it might switch from 
</em><br>
<em>&gt; metal to plastic or branch out into report covers (which do a better 
</em><br>
<em>&gt; job) and then reconsider the whole business of sticking papers together 
</em><br>
<em>&gt; and start making magnetic media.
</em><br>
<p>What does it matter so long as there are paperclips?
<br>
<p>Seriously, what *does* it matter from the perspective of a mind that only 
<br>
wants paperclips?  If you yourself want something besides paperclips, you 
<br>
should not build a paperclip optimization process, of course.
<br>
<p><em>&gt;&gt; Hm... I infer that you're thinking of some algorithm, such as 
</em><br>
<em>&gt;&gt; reinforcement on neural nets, that doesn't cleanly separate model 
</em><br>
<em>&gt;&gt; information and utility computation.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Even if you cleanly split out utility computation, widely separated AIs 
</em><br>
<em>&gt; are going to be working off rather different data bases.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Take shifting a galaxy to avoid the worst consequences of collisions 
</em><br>
<em>&gt; (whatever they are).  That's an obvious project for a friendly and very 
</em><br>
<em>&gt; patient AI.  Say galaxy A needs to go right or left direction and galaxy 
</em><br>
<em>&gt; B needs to go left or right depending on what A does to modify the 
</em><br>
<em>&gt; collision.  If the AIs figure this out when they are separated by 
</em><br>
<em>&gt; several million light years, they are going to have a heck of a time 
</em><br>
<em>&gt; deciding which way each should cause their local galaxy to dodge.  If 
</em><br>
<em>&gt; they both decide the same way, you are going to get one of those 
</em><br>
<em>&gt; sidewalk episodes of people dodging into each other's path--with really 
</em><br>
<em>&gt; lamentable results for anybody nearby if the black holes merge.
</em><br>
<p>If you anticipate this problem in advance, you can keep a simple reference 
<br>
mind on offline storage somewhere, and some set of agreed-on protocols for 
<br>
reducing your local data to the subset of the local data that would be 
<br>
visible to a distant self.  Both copies of yourself feed the reference 
<br>
mind identical copies of the intersection of the data that would be known 
<br>
to both entities.  The reference mind then outputs a set of coordinated 
<br>
high-level strategies on the level where coordination is necessary.  The 
<br>
rest is up to the local minds and they can use full knowledge in 
<br>
implementing it.
<br>
<p>In general, the ability to carry out optimal plans with multiple actions, 
<br>
whether simultaneous spatially distributed actions or temporally 
<br>
distributed local actions, depends on your ability to reliably predict 
<br>
spatially or temporally distant actions.  The solution I gave above is an 
<br>
extreme case of the answer, &quot;in thinking through coordinated plans, don't 
<br>
use data your other self can't access&quot;.  This answer is not necessarily 
<br>
optimal, but it's simple.  A more complex answer would involve optimizing 
<br>
over probability distributions for the distant mind's action.  The more 
<br>
important it is to be perfectly coordinated, the more unshared information 
<br>
you should throw away in order to be predictable.
<br>
<p><em>&gt; To the extent humans share goals it is because humans share genes. Males 
</em><br>
<em>&gt; in particular are optimize to act and to take risks for others on the 
</em><br>
<em>&gt; basis of the average relationship in a tribe a few hundred thousand 
</em><br>
<em>&gt; years ago (averaging to something like second cousin).
</em><br>
<p>Sometimes humans share goals, not because they have high relatedness to 
<br>
one another, but because humans share the genes that construct the goals 
<br>
and the goals are cognitively implemented in non-deictic form (the goal 
<br>
template doesn't use the &quot;this&quot; variable).  For example, humans like 
<br>
particular kinds of environments, so if you were to propose a workable way 
<br>
of transforming Toronto into the tree-city of Lothlorien, there'd be 
<br>
widely distributed support for that proposal not because everyone in 
<br>
Toronto is related to you, but because the parts of our brains that 
<br>
process the pretty flowers (signs of fertile territory) are constructed by 
<br>
species-typical genes.  Shared utility functions exist because of shared 
<br>
genes, but not necessarily because of Hamiltonian relatedness.
<br>
<p>Likewise, you can get selection pressures derived from iterated Prisoner's 
<br>
Dilemma between not necessarily related partners, and selection pressures 
<br>
on more complex social interactions if language is around.  If you had an 
<br>
evolved intelligent species whose spawning process scrambled zygotes 
<br>
spatially before they grew up, so that they weren't related to nearby 
<br>
individuals, I'd still expect them to evolve social coordination 
<br>
mechanisms in the process of evolving intelligence.  We behave honorably 
<br>
toward unrelated individuals.
<br>
<p><em>&gt; I have a real problem of part of my brain being subjective months our of 
</em><br>
<em>&gt; sync.  When you have to communicate, even with your twin brother, via 
</em><br>
<em>&gt; sailing ship you might have the same interest and goals but you darn 
</em><br>
<em>&gt; sure are going to be different individuals.
</em><br>
<p>The human side of this is one issue; making lots of paperclips, or 
<br>
creating a stable FAI, is another.  Obviously you can't have brain lobes 
<br>
millions of ticks distant from each other and remain a classical human. 
<br>
I'm just saying it doesn't obviously introduce insoluble stability 
<br>
problems for an FAI.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7951.html">Ben Goertzel: "RE: Positive Transcension"</a>
<li><strong>Previous message:</strong> <a href="7949.html">Metaqualia: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>In reply to:</strong> <a href="7948.html">Keith Henson: "Re: Ethics was In defense of physics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7971.html">Keith Henson: "Re: Ethics was In defense of physics"</a>
<li><strong>Reply:</strong> <a href="7971.html">Keith Henson: "Re: Ethics was In defense of physics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7950">[ date ]</a>
<a href="index.html#7950">[ thread ]</a>
<a href="subject.html#7950">[ subject ]</a>
<a href="author.html#7950">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
