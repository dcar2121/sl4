<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: All sentient have to be observer-centered!  My theory of FAI morality</title>
<meta name="Author" content="Marc Geddes (marc_geddes@yahoo.co.nz)">
<meta name="Subject" content="Re: All sentient have to be observer-centered!  My theory of FAI morality">
<meta name="Date" content="2004-02-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: All sentient have to be observer-centered!  My theory of FAI morality</h1>
<!-- received="Thu Feb 26 21:49:15 2004" -->
<!-- isoreceived="20040227044915" -->
<!-- sent="Fri, 27 Feb 2004 17:49:13 +1300 (NZDT)" -->
<!-- isosent="20040227044913" -->
<!-- name="Marc Geddes" -->
<!-- email="marc_geddes@yahoo.co.nz" -->
<!-- subject="Re: All sentient have to be observer-centered!  My theory of FAI morality" -->
<!-- id="20040227044913.6701.qmail@web20212.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20040226120446.6439.qmail@web11702.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marc Geddes (<a href="mailto:marc_geddes@yahoo.co.nz?Subject=Re:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality"><em>marc_geddes@yahoo.co.nz</em></a>)<br>
<strong>Date:</strong> Thu Feb 26 2004 - 21:49:13 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8073.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Previous message:</strong> <a href="8071.html">Michael Anissimov: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>In reply to:</strong> <a href="8064.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8076.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8076.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8077.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8072">[ date ]</a>
<a href="index.html#8072">[ thread ]</a>
<a href="subject.html#8072">[ subject ]</a>
<a href="author.html#8072">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;--- Tommy McCabe &lt;<a href="mailto:rocketjet314@yahoo.com?Subject=Re:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality">rocketjet314@yahoo.com</a>&gt; wrote: &gt; 
<br>
<em>&gt; --- Marc Geddes &lt;<a href="mailto:marc_geddes@yahoo.co.nz?Subject=Re:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality">marc_geddes@yahoo.co.nz</a>&gt; wrote:
</em><br>
<em>&gt; &gt; My main worry with Eliezer's ideas is that I don't
</em><br>
<em>&gt; &gt; think that a non observer-centered sentient is
</em><br>
<em>&gt; &gt; logically possible.  Or if it's possible, such a
</em><br>
<em>&gt; &gt; sentient would not be stable.  Can I prove this? 
</em><br>
<em>&gt; &gt; No.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Maybe not, but you can provde some evidence beyond
</em><br>
<em>&gt; 'everyone says so'.
</em><br>
<p>Actually, I do have some evidence, but the arguments
<br>
are long complex and philosophical.  Trying to prove
<br>
it was not the purpose of my post.  Here all I'm
<br>
saying is this:  ASSUMING that 100% non-observer
<br>
centered sentients are impossible, what would the
<br>
consequences be?  And then I'm reasoning out the
<br>
consequences.  It's just an exploration of FAI
<br>
morality IF the assumption is correct.  I'm just
<br>
reasoning through a possible scenario here.
<br>
<p><p><p><em>&gt; 
</em><br>
<em>&gt; &gt; But all the examples of stable sentients (humans)
</em><br>
<em>&gt; &gt; that
</em><br>
<em>&gt; &gt; we have are observer centered.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Humans are non-central special cases. Humans were
</em><br>
<em>&gt; built by Darwinian evolution, the worst possible
</em><br>
<em>&gt; case
</em><br>
<em>&gt; of design-and-test. Out in the jungle, it certainly
</em><br>
<em>&gt; helps to have a goal system centered around 'I'-
</em><br>
<em>&gt; that
</em><br>
<em>&gt; doesn't prove that it's necessary or even desirable.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; I can only point to
</em><br>
<em>&gt; &gt; this, combined with the fact that so many people
</em><br>
<em>&gt; &gt; posting to sl4 agree with me.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes, and if you lived 2000 years ago, most people
</em><br>
<em>&gt; would have agreed with you that the Earth was flat.
</em><br>
<em>&gt; The few that didn't believe that, however, had good
</em><br>
<em>&gt; reasons for it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; I can only strongly
</em><br>
<em>&gt; &gt; urge Eliezer and others working on AI NOT to
</em><br>
<em>&gt; attempt
</em><br>
<em>&gt; &gt; the folly of trying to create a non observer
</em><br>
<em>&gt; &gt; centered
</em><br>
<em>&gt; &gt; AI.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Saying that something is 'folly' doesn't mean it's
</em><br>
<em>&gt; impossible- just look at how many achievements in
</em><br>
<em>&gt; human history were laughed at as being 'folly'!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; For goodness sake don't try it!  It could mean
</em><br>
<em>&gt; &gt; the doom of us all.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And so could brushing your teeth in the morning.
</em><br>
<em>&gt; (Really!)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; I do agree that some kind of 'Universal Morality'
</em><br>
<em>&gt; is
</em><br>
<em>&gt; &gt; possible. i.e I agree that there exists a
</em><br>
<em>&gt; &gt; non-observer
</em><br>
<em>&gt; &gt; centered morality which all friendly sentients
</em><br>
<em>&gt; would
</em><br>
<em>&gt; &gt; aspire to. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Agreed.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; However, as I said, I don't think that
</em><br>
<em>&gt; &gt; non-observer sentients would be stable so any
</em><br>
<em>&gt; &gt; friendly
</em><br>
<em>&gt; &gt; stable sentient cannot follow Universal Morality
</em><br>
<em>&gt; &gt; exactly.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Saying it doesn't make it so. You have offered no
</em><br>
<em>&gt; evidence for this besides the logically fallacious
</em><br>
<em>&gt; generalizing from a small, non-central sample and
</em><br>
<em>&gt; the
</em><br>
<em>&gt; argument from popularity.
</em><br>
<p>Again, I just want to assume it for now.  See what I
<br>
said above.  All I want to do is say:  ASSUMING it's
<br>
true, what would the consequences for FAI morality be?
<br>
&nbsp;And then I'm reasoning through the consequences. 
<br>
It's just like someone exploring a given scenario.
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt; &gt; If AI morality were just:
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; Universal Morality
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; then I postulate that the AI would fail  (either
</em><br>
<em>&gt; it
</em><br>
<em>&gt; &gt; could never be created in the first place, or else
</em><br>
<em>&gt; &gt; it
</em><br>
<em>&gt; &gt; would not be stable and it would under go
</em><br>
<em>&gt; &gt; friendliness
</em><br>
<em>&gt; &gt; failure).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Saying doesn't make it so. Evidence, please?
</em><br>
<p>See above.  I just want to assume it as an axiom, and
<br>
see what the consequences for FAI would be.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; But there's a way to make AI's stable: add a small
</em><br>
<em>&gt; &gt; observer-centered component.  Such an AI could
</em><br>
<em>&gt; still
</em><br>
<em>&gt; &gt; be MOSTLY altruistic, but now it would only be
</em><br>
<em>&gt; &gt; following Universal Morality as an approximation,
</em><br>
<em>&gt; &gt; since there would be an additional
</em><br>
<em>&gt; observer-centered
</em><br>
<em>&gt; &gt; component.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; That's like taking a perfectly good bicycle and
</em><br>
<em>&gt; putting gum in the chain.
</em><br>
<p>We don't know whether I'm right or not.  Again, I'm
<br>
just considering the possibility and then looking at
<br>
the consequences.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; So I postulate that all stable FAI's have to have
</em><br>
<em>&gt; &gt; moralities of the form:
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; Universal Morality x Personal Morality
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Saying it doesn't make it so, as much as humans are
</em><br>
<em>&gt; prone to believing something when it is repeated.
</em><br>
<em>&gt; Evidence?
</em><br>
<p>See above.  This is a consequence of my assumption. 
<br>
It's just an exploration of a possible scenario.  I'm
<br>
not saying it has to be so.
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt; &gt; Now Universal Morality (by definition) is not
</em><br>
<em>&gt; &gt; arbitrary or observer centered.  There is one and
</em><br>
<em>&gt; &gt; only
</em><br>
<em>&gt; &gt; one Universal Morality and it must be symmetric
</em><br>
<em>&gt; &gt; across
</em><br>
<em>&gt; &gt; all sentients (it has to work if everyone does it
</em><br>
<em>&gt; -
</em><br>
<em>&gt; &gt; positive sum interactions).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is quite possibly true (though many on SL4
</em><br>
<em>&gt; would
</em><br>
<em>&gt; argue against that)
</em><br>
<p>It has to be true by definition.  That's what
<br>
'normative altruism' means - IF there is an
<br>
non-observer centered morality THEN all sentient FAI's
<br>
have to converge on this uniqie morality in the limit
<br>
that they thought about morality for long enough.
<br>
&nbsp;
<br>
<em>&gt; 
</em><br>
<em>&gt; &gt; But Personal morality (by definition) can have
</em><br>
<em>&gt; many
</em><br>
<em>&gt; &gt; degrees of freedom and is observer centered. 
</em><br>
<em>&gt; There
</em><br>
<em>&gt; &gt; are many different possible kinds of personal
</em><br>
<em>&gt; &gt; morality
</em><br>
<em>&gt; &gt; and the morality is subjective and observer
</em><br>
<em>&gt; &gt; centered.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Agreed.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; The only constraint is that Personal Morality has
</em><br>
<em>&gt; to
</em><br>
<em>&gt; &gt; be consistent with Universal Morality to be
</em><br>
<em>&gt; &gt; Friendly. 
</em><br>
<em>&gt; &gt; That's why I say that stable FAI's follow
</em><br>
<em>&gt; Universal
</em><br>
<em>&gt; &gt; Morality transformed by (multipication sign)
</em><br>
<em>&gt; &gt; Personal
</em><br>
<em>&gt; &gt; Morality.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Moralities can't be 'consistent' if they aren't
</em><br>
<em>&gt; identical.
</em><br>
<p><p>Not true!  Are you familiar with mathematics?   take a
<br>
very simple equation:
<br>
<p>x^2 = 4  (x squared equals 4)
<br>
<p>What values of x are consistent with this equation?
<br>
<p>There are two different answers:  -2 and 2
<br>
<p>-2 squared = -2 x -2 =4
<br>
<p>and
<br>
<p>2 squared = 2 x 2 =4
<br>
<p><p>Now, by analogy, there could be many different
<br>
personal moralities consistent with universal
<br>
morality.  When I say that two moralities are
<br>
'Consistent with' each other, all I mean is that they
<br>
don't contradict each other.
<br>
<p>Here's an example:
<br>
<p>Suppose Universal Morality just said:  'Thou shall not
<br>
kill'
<br>
<p>There are many different personal moralities
<br>
consistent with that.  Here are 3 examples of
<br>
extremely simple personal moralities:
<br>
<p>'Ice skating is good'
<br>
'Coke is good, Pepsi is evil'
<br>
'Mountain climbing is good'
<br>
<p>Each of these 3 personal moralities is consistent with
<br>
'Thou Shall not kill', provided that the people
<br>
following the moralities don't kill anyone in the
<br>
proccess. 
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; Now an FAI operating off Universal Morality alone
</em><br>
<em>&gt; &gt; (which I'm postulating is impossible or unstable)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Saying, even repeated saying, doesn't make it so. I
</em><br>
<em>&gt; need evidence!
</em><br>
<p>All I'm doing here is exploring the scenario, not
<br>
trying to prove it.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; would to one and only one (unique) Singularity.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Non sequitur. AIs, even if they all have the same
</em><br>
<em>&gt; morality, can be quite different.
</em><br>
<p>They wouldn't be 'quite different' if they were
<br>
entirely non-observer centered.  That was the very
<br>
point I was making!  FAI's which were 100%
<br>
non-observer centered, would all be converging on the
<br>
same unique morality.  It is only if we added an
<br>
observer centered component (a 'personal morality' as
<br>
I explained above) that the FAI's would be different.
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt; &gt; There
</em><br>
<em>&gt; &gt; would be only one possible form a successful
</em><br>
<em>&gt; &gt; Singularity could take.  A reasonable guess (due
</em><br>
<em>&gt; to
</em><br>
<em>&gt; &gt; Eliezer) is that:
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; Universal Morality = Volitional Morality
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Quite possibly true.
</em><br>
<p>O.K, so all the FAI's would be going around fulfilling
<br>
volitional requests, so long as these requests didn't
<br>
hurt anyone else.  That's a unique outcome to the
<br>
Singularity.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; That is, it was postulated by Eli that Universal
</em><br>
<em>&gt; &gt; Morality is respect for sentient volition (free
</em><br>
<em>&gt; &gt; will).
</em><br>
<em>&gt; &gt;  With no observer centered component, an FAI
</em><br>
<em>&gt; &gt; following
</em><br>
<em>&gt; &gt; this morality would aim to fulfil sentient
</em><br>
<em>&gt; requests
</em><br>
<em>&gt; &gt; (consistent with sentient volition).  But I think
</em><br>
<em>&gt; &gt; that
</em><br>
<em>&gt; &gt; such an AI is impossible or unstable.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Repeating it doesn't make it so. Where is the
</em><br>
<em>&gt; evidence?
</em><br>
<p>I'm exploring the scenario, not trying to prove it.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; I was postulating that all stable FAI's have a
</em><br>
<em>&gt; &gt; morality of the form:
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; Universal Morality x Personal Morality
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Repeating it doesn't make it correct. Where is the
</em><br>
<em>&gt; evidence?
</em><br>
<p>I'm exploring the scenario.  The equation follows IF
<br>
an observer centered (personal morality) component has
<br>
to be added.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; If I am right, then there are many different kinds
</em><br>
<em>&gt; &gt; of
</em><br>
<em>&gt; &gt; successul (Friendly) Singularities. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Agreed.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; Although
</em><br>
<em>&gt; &gt; Universal Morality is unique, Personal Morality
</em><br>
<em>&gt; can
</em><br>
<em>&gt; &gt; have many degrees of freedom.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Agreed.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; So the precise form a
</em><br>
<em>&gt; &gt; successful Singularity takes would depend on the
</em><br>
<em>&gt; &gt; 'Personal Morality' componant of the FAI's
</em><br>
<em>&gt; morality.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is like the statement 'Have you stopped beating
</em><br>
<em>&gt; your wife?' - it implies which has not been proven,
</em><br>
<em>&gt; or
</em><br>
<em>&gt; even strongly suggested by evidence.
</em><br>
<p>It's just a consequence of my original axiom.  I'm
<br>
just exploring the scenario to see what I would imply.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; Assuming that:
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; Universal Morality = Volition based Morality
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; we see that:
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; Universal Morality x Personal Morality
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; leads to something quite different. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Agreed.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; Respect for
</em><br>
<em>&gt; &gt; sentient volition (Universal Morality) gets
</em><br>
<em>&gt; &gt; transformed (mulipication sign) by Personal
</em><br>
<em>&gt; &gt; Morality. 
</em><br>
<em>&gt; &gt; This leads to a volition based morality with an
</em><br>
<em>&gt; &gt; Acts/Omissions distinction (See my previous post
</em><br>
<em>&gt; for
</em><br>
<em>&gt; &gt; an explanation of the Moral Acts/Omissions
</em><br>
<em>&gt; &gt; distinctions).  
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; FAI's with morality of this form would still
</em><br>
<em>&gt; respect
</em><br>
<em>&gt; &gt; sentient volition, but they would not neccesserily
</em><br>
<em>&gt; &gt; fulfil sentient requests.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Neither would a Yudkowskian FAI, for example, if
</em><br>
<em>&gt; Saddam Hussein wants to kill everybody.
</em><br>
<p>Well yeah true, a Yudkowskian FAI would of course
<br>
refuse requests to hurt other people.  But it would
<br>
aim to fulfil ALL requests consistent with volition. 
<br>
(All requests which don't involve violating other
<br>
peoples right).  But the point I was making was that
<br>
there are many such requests consistent with this. 
<br>
For instance, 'I want to go ice skating', 'I want a
<br>
Pepsi', 'I want some mountain climbing qquipment' and
<br>
so on and so on.  A Yudkowskian FAI can't draw any
<br>
distinctions between these, and would see all of them
<br>
as equally 'good'.
<br>
<p>But an FAI with a 'Personal Morality' component, would
<br>
not neccesserily fulfil all of these requests.  For
<br>
instance an FAI that had a personal morality component
<br>
'Coke is good, Pepsi is evil' would refuse to fulfil a
<br>
request for Pepsi.  That is NOT saying that the FAI
<br>
would stop anyone from drinking Pepsi if they wanted
<br>
to.  Remember, any Personal morality has to be
<br>
consistent with Universal morality.  If Universal
<br>
morality said that people should be allowed to do what
<br>
they want so long as they are not hurting anyone, then
<br>
the FAI is not allowed to stop people drinking Pepsi,
<br>
even though the FAI's personal morality doesn't agree
<br>
with it.  You see?  The 'Personal morality' component
<br>
would tell an FAI what it SHOULD do, the 'Universal
<br>
morality' componanet is concerned with what an FAI
<br>
SHOULDN'T do.  A Yudkowskian FAI would be unable to
<br>
draw this distinction, since it would have no
<br>
'Personal Morality'  (Remember a Yudkowskian FAI is
<br>
entirely non-observer centerd, and so it could only
<br>
have Universal Morality).  You could say that a
<br>
Yudkowskian FAI just views everything that doesn't
<br>
hurt others as equal, where as an FAI with an extra
<br>
oberver centered component would have some extra
<br>
personal principles.
<br>
<p><p><p><em>&gt; 
</em><br>
<em>&gt; &gt; Sentient requests would
</em><br>
<em>&gt; &gt; only be fulfilled when such requests are
</em><br>
<em>&gt; consistent
</em><br>
<em>&gt; &gt; with the FAI's Personal Morality.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; A good reason had better be supplied along with the
</em><br>
<em>&gt; rejections.
</em><br>
<p>See above.  I'm just exploring the scenario.  This
<br>
would be a consequence of my assumption that all FAI's
<br>
have to have a 'Personal Morality' componenet.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; So the 'Personal
</em><br>
<em>&gt; &gt; Morality' component would act like a filter
</em><br>
<em>&gt; stopping
</em><br>
<em>&gt; &gt; some sentient requests from being fulfilled.  In
</em><br>
<em>&gt; &gt; addition, such FAI's would be pursuing goals of
</em><br>
<em>&gt; &gt; their
</em><br>
<em>&gt; &gt; own (so long as such goals did not violate
</em><br>
<em>&gt; sentient
</em><br>
<em>&gt; &gt; volition). 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So would a Yudkowskian or entirely volition-based
</em><br>
<em>&gt; AI-
</em><br>
<em>&gt; it would form goals that affected itself instead of
</em><br>
<em>&gt; humans, as long as the goals would lead to helping
</em><br>
<em>&gt; humanity (or sentients in general, after the
</em><br>
<em>&gt; Singularity).
</em><br>
<p>Yeah, yeah, true, but an FAI with a 'Personal
<br>
Morality' would have some additional goals on top of
<br>
this.  A Yudkowskian FAI does of course have the goals
<br>
'aim to do things that help with the fulfilment of
<br>
sentient requests'.  But that's all.  An FAI with an
<br>
additional 'Personal Morality' component, would also
<br>
have the Yudkowskian goals, but it would have some
<br>
additional goals.  For instance the additinal personal
<br>
morality 'Coke is good, Pepsi is evil' would lead the
<br>
FAI to personally support 'Coke' goals (provided such
<br>
goals did not contradict the Yudkowskian goals).
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; So you see, my form of FAI is a far more
</em><br>
<em>&gt; &gt; interesting and complex beast than an FAI which
</em><br>
<em>&gt; just
</em><br>
<em>&gt; &gt; followed Universal Morality.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 'Interesting' doesn't mean better, or even possible.
</em><br>
<p>My quation is actually the general solution to FAI
<br>
morality.  See what I say below.
<br>
<p><em>&gt;  
</em><br>
<em>&gt; &gt; Eliezer's 'Friendliness' theory (whereby the AI is
</em><br>
<em>&gt; &gt; reasoning about morality and can modify its own
</em><br>
<em>&gt; &gt; goals
</em><br>
<em>&gt; &gt; to try to close in on normalized 'Universal
</em><br>
<em>&gt; &gt; Morality')
</em><br>
<em>&gt; &gt; is currently only dealing with the 'Universal
</em><br>
<em>&gt; &gt; Morality' component of morality.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; True- and is there any reason why it shouldn't?
</em><br>
<p>No, but I'm just exploring the consequences of FAI's
<br>
with an extra 'Personal Morality' component.  
<br>
<p>My own theory is actually the more general solution of
<br>
FAI morality, because my equation has Eliezer's FAI as
<br>
a special case.
<br>
<p>Take the equation:
<br>
<p>Universal Morality x Personal Morality
<br>
<p>and set Personal Morality equal to unity (1)
<br>
<p>Then Universal Morality x 1 = Universal Morality
<br>
<p>Such an FAI would be equivalent to the Yudkowskian one
<br>
(it only has Universal Morality).  So you see, my
<br>
solution has the Yudkowskian FAI as a special case.
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt; &gt; But if I am right, then all stable FAI have to
</em><br>
<em>&gt; have
</em><br>
<em>&gt; &gt; an
</em><br>
<em>&gt; &gt; observer-centered (Personal Morality) componant to
</em><br>
<em>&gt; &gt; their morality as well.  
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Why?
</em><br>
<p>At point I will try to prove it.  Here I am just
<br>
giving the general solution to the problem of FAI
<br>
morality, and postulating that an FAI with no Personal
<br>
Morality component (Personal Morality set to unity in
<br>
my equation) would be unstable.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; So it's vital that FAI programmers give
</em><br>
<em>&gt; &gt; consideration
</em><br>
<em>&gt; &gt; to just what the 'Personal Morality' of an FAI
</em><br>
<em>&gt; &gt; should
</em><br>
<em>&gt; &gt; be.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Another statement based on an unproven assumption.
</em><br>
<p>I've given the general solution to the problem of FAI
<br>
morality.  We don't know that 'Personal Morality' set
<br>
to unity would be stable.  Therefore we have to
<br>
consider the case where FAI's have to have a
<br>
non-trival 'Personal Morality' component.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; The question of personal values cannot be
</em><br>
<em>&gt; &gt; evaded
</em><br>
<em>&gt; &gt; if non observer centered FAI's are impossible. 
</em><br>
<em>&gt; Even
</em><br>
<em>&gt; &gt; with Universal Morality, there would have to be a
</em><br>
<em>&gt; &gt; 'Personal Morality' componant which would have to
</em><br>
<em>&gt; be
</em><br>
<em>&gt; &gt; chosen directly by the programmers (this 'Personal
</em><br>
<em>&gt; &gt; Morality' componant is arbitrary and
</em><br>
<em>&gt; &gt; non-renormalizable).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Why, again?
</em><br>
<p>What I said was: 'the question of personal values
<br>
cannot be evaaded' IF 'non observer centered FAI's are
<br>
impossible'.  Personal morality would not be normative
<br>
(there would be no unique personal morality
<br>
'solution', so an AI could not use reason to get a
<br>
solution).  We have to consider the possibility.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; To sum up: my theory is that all stable FAI have
</em><br>
<em>&gt; &gt; moralitites of the form:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Evidence? You have provided no evidence.
</em><br>
<p>My equation is the general solution to FAI morality,
<br>
with the Yudkowskian FAI represented by the special
<br>
case : Personal Morality set to unity (1).
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; Universal Morality x Personal Morality
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; Only the 'Universal Morality' can be normalized.  
</em><br>
<em>&gt;  
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; =====
</em><br>
<em>&gt; &gt; Please visit my web-site at: 
</em><br>
<em>&gt; &gt; <a href="http://www.prometheuscrack.com">http://www.prometheuscrack.com</a>
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; Find local movie times and trailers on Yahoo!
</em><br>
<em>&gt; &gt; Movies.
</em><br>
<em>&gt; &gt; <a href="http://au.movies.yahoo.com">http://au.movies.yahoo.com</a>
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; __________________________________
</em><br>
<em>&gt; Do you Yahoo!?
</em><br>
<em>&gt; Get better spam protection with Yahoo! Mail.
</em><br>
<em>&gt; <a href="http://antispam.yahoo.com/tools">http://antispam.yahoo.com/tools</a> 
</em><br>
<p>=====
<br>
Please visit my web-site at:  <a href="http://www.prometheuscrack.com">http://www.prometheuscrack.com</a>
<br>
<p><a href="http://personals.yahoo.com.au">http://personals.yahoo.com.au</a> - Yahoo! Personals
<br>
New people, new possibilities. FREE for a limited time.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8073.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Previous message:</strong> <a href="8071.html">Michael Anissimov: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>In reply to:</strong> <a href="8064.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8076.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8076.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8077.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8072">[ date ]</a>
<a href="index.html#8072">[ thread ]</a>
<a href="subject.html#8072">[ subject ]</a>
<a href="author.html#8072">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
