<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Ethics was In defense of physics</title>
<meta name="Author" content="Keith Henson (hkhenson@rogers.com)">
<meta name="Subject" content="Re: Ethics was In defense of physics">
<meta name="Date" content="2004-02-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Ethics was In defense of physics</h1>
<!-- received="Wed Feb 18 23:00:14 2004" -->
<!-- isoreceived="20040219060014" -->
<!-- sent="Thu, 19 Feb 2004 00:53:37 -0500" -->
<!-- isosent="20040219055337" -->
<!-- name="Keith Henson" -->
<!-- email="hkhenson@rogers.com" -->
<!-- subject="Re: Ethics was In defense of physics" -->
<!-- id="5.1.0.14.0.20040218235738.02f73e30@pop.bloor.is.net.cable.rogers.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="40303AEA.2000007@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Keith Henson (<a href="mailto:hkhenson@rogers.com?Subject=Re:%20Ethics%20was%20In%20defense%20of%20physics"><em>hkhenson@rogers.com</em></a>)<br>
<strong>Date:</strong> Wed Feb 18 2004 - 22:53:37 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7972.html">Chris Healey: "RE: Humane-ness (resend due to addressing error)"</a>
<li><strong>Previous message:</strong> <a href="7970.html">Ben Goertzel: "free will and consciousness"</a>
<li><strong>In reply to:</strong> <a href="7950.html">Eliezer S. Yudkowsky: "Re: Ethics was In defense of physics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7930.html">Alejandro Dubrovsky: "Re: In defense of physics (was: Encouraging a Positive Transcension)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7971">[ date ]</a>
<a href="index.html#7971">[ thread ]</a>
<a href="subject.html#7971">[ subject ]</a>
<a href="author.html#7971">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 10:37 PM 15/02/04 -0500, you wrote:
<br>
<em>&gt;Keith Henson wrote:
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;The point is that you can perform all learning necessary to the task of 
</em><br>
<em>&gt;&gt;&gt;transforming everything in sight into paperclips, and you won't have 
</em><br>
<em>&gt;&gt;&gt;conflicts with distant parts of yourself that also want to transform 
</em><br>
<em>&gt;&gt;&gt;everything into paperclips - the target is constant, only the aim gets updated.
</em><br>
<em>&gt;&gt;And the weapons, and the gravity field, and if it starts thinking about 
</em><br>
<em>&gt;&gt;what it is doing and what paper clips are used for, it might switch from 
</em><br>
<em>&gt;&gt;metal to plastic or branch out into report covers (which do a better job) 
</em><br>
<em>&gt;&gt;and then reconsider the whole business of sticking papers together and 
</em><br>
<em>&gt;&gt;start making magnetic media.
</em><br>
<em>&gt;
</em><br>
<em>&gt;What does it matter so long as there are paperclips?
</em><br>
<em>&gt;
</em><br>
<em>&gt;Seriously, what *does* it matter from the perspective of a mind that only 
</em><br>
<em>&gt;wants paperclips?  If you yourself want something besides paperclips, you 
</em><br>
<em>&gt;should not build a paperclip optimization process, of course.
</em><br>
<p>Agreed.  But &quot;mind&quot; is by definition more than an automaton.  I.e., we 
<br>
already have paperclip making machines.  The goals of giving a mind a 
<br>
single focused nature *and* great intellect would seem to be in conflict.
<br>
<p><em>&gt;&gt;&gt;Hm... I infer that you're thinking of some algorithm, such as 
</em><br>
<em>&gt;&gt;&gt;reinforcement on neural nets, that doesn't cleanly separate model 
</em><br>
<em>&gt;&gt;&gt;information and utility computation.
</em><br>
<em>&gt;&gt;Even if you cleanly split out utility computation, widely separated AIs 
</em><br>
<em>&gt;&gt;are going to be working off rather different data bases.
</em><br>
<em>&gt;&gt;Take shifting a galaxy to avoid the worst consequences of collisions 
</em><br>
<em>&gt;&gt;(whatever they are).  That's an obvious project for a friendly and very 
</em><br>
<em>&gt;&gt;patient AI.  Say galaxy A needs to go right or left direction and galaxy 
</em><br>
<em>&gt;&gt;B needs to go left or right depending on what A does to modify the 
</em><br>
<em>&gt;&gt;collision.  If the AIs figure this out when they are separated by several 
</em><br>
<em>&gt;&gt;million light years, they are going to have a heck of a time deciding 
</em><br>
<em>&gt;&gt;which way each should cause their local galaxy to dodge.  If they both 
</em><br>
<em>&gt;&gt;decide the same way, you are going to get one of those sidewalk episodes 
</em><br>
<em>&gt;&gt;of people dodging into each other's path--with really lamentable results 
</em><br>
<em>&gt;&gt;for anybody nearby if the black holes merge.
</em><br>
<em>&gt;
</em><br>
<em>&gt;If you anticipate this problem in advance, you can keep a simple reference 
</em><br>
<em>&gt;mind on offline storage somewhere, and some set of agreed-on protocols for 
</em><br>
<em>&gt;reducing your local data to the subset of the local data that would be 
</em><br>
<em>&gt;visible to a distant self.  Both copies of yourself feed the reference 
</em><br>
<em>&gt;mind identical copies of the intersection of the data that would be known 
</em><br>
<em>&gt;to both entities.  The reference mind then outputs a set of coordinated 
</em><br>
<em>&gt;high-level strategies on the level where coordination is necessary.  The 
</em><br>
<em>&gt;rest is up to the local minds and they can use full knowledge in 
</em><br>
<em>&gt;implementing it.
</em><br>
<em>&gt;
</em><br>
<em>&gt;In general, the ability to carry out optimal plans with multiple actions, 
</em><br>
<em>&gt;whether simultaneous spatially distributed actions or temporally 
</em><br>
<em>&gt;distributed local actions, depends on your ability to reliably predict 
</em><br>
<em>&gt;spatially or temporally distant actions.  The solution I gave above is an 
</em><br>
<em>&gt;extreme case of the answer, &quot;in thinking through coordinated plans, don't 
</em><br>
<em>&gt;use data your other self can't access&quot;.  This answer is not necessarily 
</em><br>
<em>&gt;optimal, but it's simple.  A more complex answer would involve optimizing 
</em><br>
<em>&gt;over probability distributions for the distant mind's action.  The more 
</em><br>
<em>&gt;important it is to be perfectly coordinated, the more unshared information 
</em><br>
<em>&gt;you should throw away in order to be predictable.
</em><br>
<p>Thought experiment:
<br>
<p>Twin brothers living far apart and out of communication each constructs the 
<br>
first (or second) automobiles.  They complete their lethally fast toys on 
<br>
the same hour of the same day and each decides to visit the other.  In 
<br>
spite of these being the first two cars, there is a well paved, two vehicle 
<br>
wide road between the homes of the two brothers.  Each figures out that 
<br>
driving in the center would be a disaster if someone else also had a 
<br>
car.  But which side of the road do they pick?
<br>
<p>(A random pick results in 50% chance of killing both of them in a fatal 
<br>
head on collision.)
<br>
<p><em>&gt;&gt;To the extent humans share goals it is because humans share genes. Males 
</em><br>
<em>&gt;&gt;in particular are optimize to act and to take risks for others on the 
</em><br>
<em>&gt;&gt;basis of the average relationship in a tribe a few hundred thousand years 
</em><br>
<em>&gt;&gt;ago (averaging to something like second cousin).
</em><br>
<em>&gt;
</em><br>
<em>&gt;Sometimes humans share goals, not because they have high relatedness to 
</em><br>
<em>&gt;one another, but because humans share the genes that construct the goals
</em><br>
<p>Or memes are selected in the environment of brains which seem to give a 
<br>
group a common goal.  Cult memes for example.
<br>
<p><em>&gt;and the goals are cognitively implemented in non-deictic form (the goal 
</em><br>
<em>&gt;template doesn't use the &quot;this&quot; variable).
</em><br>
<p><em>&gt;  For example, humans like particular kinds of environments, so if you 
</em><br>
<em>&gt; were to propose a workable way of transforming Toronto into the tree-city 
</em><br>
<em>&gt; of Lothlorien, there'd be widely distributed support for that proposal 
</em><br>
<em>&gt; not because everyone in Toronto is related to you, but because the parts 
</em><br>
<em>&gt; of our brains that process the pretty flowers (signs of fertile 
</em><br>
<em>&gt; territory) are constructed by species-typical genes.  Shared utility 
</em><br>
<em>&gt; functions exist because of shared genes, but not necessarily because of 
</em><br>
<em>&gt; Hamiltonian relatedness.
</em><br>
<p>A lot of this already happens.  Our liking for parks of mowed grass and 
<br>
nearby trees was obtained honestly by our remote ancestors.
<br>
<p><em>&gt;Likewise, you can get selection pressures derived from iterated Prisoner's 
</em><br>
<em>&gt;Dilemma between not necessarily related partners, and selection pressures 
</em><br>
<em>&gt;on more complex social interactions if language is around.  If you had an 
</em><br>
<em>&gt;evolved intelligent species whose spawning process scrambled zygotes 
</em><br>
<em>&gt;spatially before they grew up, so that they weren't related to nearby 
</em><br>
<em>&gt;individuals, I'd still expect them to evolve social coordination 
</em><br>
<em>&gt;mechanisms in the process of evolving intelligence.  We behave honorably 
</em><br>
<em>&gt;toward unrelated individuals.
</em><br>
<p>That's true, but I really wonder if it didn't take close proximity to 
<br>
relatives to shape up the ability to behave honorably toward 
<br>
unrelateds?  On the other hand, octopi spawn that way.  Anyone have a 
<br>
pointer to them acting honorably toward each other?
<br>
<p>The point is perhaps moot unless someone uses an evolutionary process to 
<br>
generate AIs.
<br>
<p><em>&gt;&gt;I have a real problem of part of my brain being subjective months our of 
</em><br>
<em>&gt;&gt;sync.  When you have to communicate, even with your twin brother, via 
</em><br>
<em>&gt;&gt;sailing ship you might have the same interest and goals but you darn sure 
</em><br>
<em>&gt;&gt;are going to be different individuals.
</em><br>
<em>&gt;
</em><br>
<em>&gt;The human side of this is one issue; making lots of paperclips, or 
</em><br>
<em>&gt;creating a stable FAI, is another.  Obviously you can't have brain lobes 
</em><br>
<em>&gt;millions of ticks distant from each other and remain a classical human. 
</em><br>
<em>&gt;I'm just saying it doesn't obviously introduce insoluble stability 
</em><br>
<em>&gt;problems for an FAI.
</em><br>
<p>One reason this concerns me is personal boosted intelligence.  If we are 
<br>
going to deal with AIs as something more than pets, we are probably going 
<br>
to need enhancement.  But talking to an AI that was spread out over light 
<br>
minutes might be a bit disconcerting.
<br>
<p>A point I understand but slightly question is the assumption that there 
<br>
well be only once and the others will be clones.  I can see this if the 
<br>
takeoff and spreading out is extremely fast, but it if is not you have the 
<br>
potential for more than one AI.  Does more than one creat a problem?
<br>
<p>Keith Henson
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7972.html">Chris Healey: "RE: Humane-ness (resend due to addressing error)"</a>
<li><strong>Previous message:</strong> <a href="7970.html">Ben Goertzel: "free will and consciousness"</a>
<li><strong>In reply to:</strong> <a href="7950.html">Eliezer S. Yudkowsky: "Re: Ethics was In defense of physics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7930.html">Alejandro Dubrovsky: "Re: In defense of physics (was: Encouraging a Positive Transcension)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7971">[ date ]</a>
<a href="index.html#7971">[ thread ]</a>
<a href="subject.html#7971">[ subject ]</a>
<a href="author.html#7971">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
