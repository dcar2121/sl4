<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Friendly AI in &quot;Positive Transcension&quot;</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Friendly AI in &quot;Positive Transcension&quot;">
<meta name="Date" content="2004-02-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Friendly AI in &quot;Positive Transcension&quot;</h1>
<!-- received="Mon Feb 16 08:58:20 2004" -->
<!-- isoreceived="20040216155820" -->
<!-- sent="Mon, 16 Feb 2004 11:05:08 -0500" -->
<!-- isosent="20040216160508" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Friendly AI in &quot;Positive Transcension&quot;" -->
<!-- id="BMECIIDGKPGNFPJLIDNPIEOHCMAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="402FE50C.3030505@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Friendly%20AI%20in%20&quot;Positive%20Transcension&quot;"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Mon Feb 16 2004 - 09:05:08 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7956.html">Christian Szegedy: "Re: Positive Transcension"</a>
<li><strong>Previous message:</strong> <a href="7954.html">Mikko Särelä: "RE: Encouraging a Positive Transcension"</a>
<li><strong>In reply to:</strong> <a href="7947.html">Eliezer S. Yudkowsky: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7944.html">Jef Allbright: "Positive Transcension"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7955">[ date ]</a>
<a href="index.html#7955">[ thread ]</a>
<a href="subject.html#7955">[ subject ]</a>
<a href="author.html#7955">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer,
<br>
<p>So far I've been very conciliatory in this dialogue, but I feel the need to
<br>
be a little more defensive of my position now.  I really don't think my
<br>
descriptions of your views were THAT far off from what you said in CFAI.  I
<br>
may not have read CFAI for a couple years, but I didn't read it *that*
<br>
sloppily.
<br>
<p>In CFAI you wrote, as a characterization of Friendly AI
<br>
<p>&quot;All that is required is that the initial shaper network of the Friendly AI
<br>
converge to normative altruism. &quot;
<br>
<p>You also wrote a lot about programmers teaching early-stage AI's to be
<br>
Friendly.
<br>
<p>You also noted that you believe the hard takeoff to be the most likely
<br>
possibility, so you assume it as a default case in your discussion.
<br>
<p>Then, you complain when, in my essay, I say you envision a hard takeoff...
<br>
(but this is clearly stated in CFAI!)
<br>
<p>And you complain when I say that you propose AI's that will be taught to be
<br>
&quot;benevolent.&quot;  To me, &quot;benevolent&quot; is not so far off from &quot;normatively
<br>
altruistic&quot;....  And teaching AI's to be benevolent is not so far off from
<br>
teaching them in a way intended to cause them to converge to a condition of
<br>
benevolence/normative-altruism.
<br>
<p>It still seems to me that my summary of your views is NOT so far off from
<br>
what you explicitly say in CFAI.
<br>
<p>The difference seems to be that I think of &quot;normative altruism&quot; as being a
<br>
specific ethical rule or ethical system, whereas you are thinking of
<br>
&quot;normative altruism&quot; as being something much different and more cosmic...
<br>
<p>In practical terms, about my essay... either tonight or later this week,
<br>
I'll revise my essay in a way that should make it inoffensive to you, at
<br>
least in terms of its mention of your work.
<br>
<p>I'll
<br>
<p>* mention your theory of Friendly AI, and a few points from these email
<br>
discussions, and then
<br>
<p>* introduce a theory called &quot;Benevolent AI&quot;, which I'll describe as loosely
<br>
related to but simpler than your ideas.  Most of the discussion currently
<br>
framed in my essay as being about Friendly AI will be rephrased as being
<br>
about &quot;Benevolent AI.&quot;
<br>
<p><p><em>&gt; I don't, of course.  However, I should hope that you would be aware of
</em><br>
<em>&gt; your own lack of understanding and/or inability to compactly summarize,
</em><br>
<em>&gt; and avoid giving summaries of FAI that are the diametric opposite of what
</em><br>
<em>&gt; I mean by the term.
</em><br>
<p>This is rather an overstatement, no?   The diametric opposite of your theory
<br>
would be more like
<br>
<p>&quot;Create an AI with the most inhumane moral and goal system possible.&quot;
<br>
<p>-- or else perhaps something like
<br>
<p>&quot;It doesn't matter what the hell we do, all AI's of sufficient intelligence
<br>
will do what God tells them to anyway&quot; ;-)
<br>
<p><em>&gt; Presently, you seem to be in a state of feeling that there is nothing to
</em><br>
<em>&gt; understand.
</em><br>
<p>Nah, I definitely get the feeling you've progressed a lot beyond what is
<br>
explicitly stated in CFAI, in terms of your own thinking on these subjects.
<br>
<p>I got that feeling when we met last in person, as well.
<br>
<p>I believe that my summary is a lot closer to what you said in CFAI, than in
<br>
what you think currently....  I think that now you interpret CFAI in terms
<br>
of your current ideas, even when this interpretation is not the most direct
<br>
interpretation of the actual text in CFAI.
<br>
<p><em>&gt; To me these are subcategories of essentially the same kind of content,
</em><br>
<em>&gt; humans thinking in philosophical mode.  The commensurability of abstract
</em><br>
<em>&gt; ethical principles, specific ethical systems, and ethical rules, as
</em><br>
<em>&gt; cognitive content, are why we have no trouble in reasoning from verbally
</em><br>
<em>&gt; stated ethical principles to specific ethical systems and so on.
</em><br>
<em>&gt; They are
</em><br>
<em>&gt; produced by human philosophers; they are argued all in the same midnight
</em><br>
<em>&gt; bull sessions; they are written down in books and transmitted
</em><br>
<em>&gt; memetically.
</em><br>
<em>&gt;   Children receive them from parents as instructions, understand them as
</em><br>
<em>&gt; social expectations, feel their force using the emotions and
</em><br>
<em>&gt; social models
</em><br>
<em>&gt; of frontal cortex.  Principles, systems, and rules are subcategories of
</em><br>
<em>&gt; the same kind of mental thing manipulated in the mind - they've got the
</em><br>
<em>&gt; same representation.  Humans argue about principles, systems, and rules
</em><br>
<em>&gt; fairly intuitively - the debate goes back to Plato at the least.
</em><br>
<em>&gt;
</em><br>
<em>&gt; FAI::humaneness is a different *kind of thing* from this.  There
</em><br>
<em>&gt; is no way
</em><br>
<em>&gt; to argue FAI::humaneness to someone; it's not cognitive content, it's a
</em><br>
<em>&gt; what-it-does of a set of dynamics.
</em><br>
<p>Hmmm...  I'm not sure why &quot;joy&quot; and &quot;growth&quot; and &quot;choice&quot; aren't also in the
<br>
category of
<br>
<p>&quot;what-it-does of a set of dynamics&quot;
<br>
<p>And I'm not sure why &quot;humane-ness&quot; isn't also something to be written about
<br>
and discussed.
<br>
<p>In fact, aren't you planning on writing something about it at some point int
<br>
he future, which will then be discussed?
<br>
<p>I'm not asserting there's NO distinction between &quot;humaneness&quot; and these
<br>
other principles, but I don't yet see why the distinction is as profound as
<br>
you say it is...
<br>
<p><em>&gt; It's not the sort of thing you could
</em><br>
<em>&gt; describe in a philosophy book; you would describe it by pointing to a
</em><br>
<em>&gt; complete FAI system that implements FAI::humaneness.
</em><br>
<p>But aren't choice, growth and joy also far better described via example than
<br>
via verbal discussions?  Of course they are...
<br>
<p>It seems to me that humaneness is a messier, more complex, and less crisp
<br>
sorta thing than growth, choice or joy -- yet I don't see why you think it's
<br>
of a fundamentally different character.  Frankly, each of these other three
<br>
principles is also a big nasty, hard-to-define-mess...
<br>
<p>I think that growth, joy and choice are three relatively-well-defined
<br>
principles that are *important components* of &quot;humaneness&quot;.  They represent
<br>
some of the better components of humaneness in my view -- as opposed to the
<br>
also-large components embodying &quot;man's inhumanity to man&quot; ...
<br>
<p><p><em>&gt; What I wish you to avoid is presenting FAI as if it were a specific
</em><br>
<em>&gt; ethical principle, or anything commensurate with a specific ethical
</em><br>
<em>&gt; principle, because that's the *wrong sort of stuff* to describe either
</em><br>
<em>&gt; FAI::humaneness or FAI architectural principles.  It is, in fact,
</em><br>
<em>&gt; diametrically opposed to FAI, which was created to address inherent and
</em><br>
<em>&gt; fundamental problems with trying to imbue specific ethical
</em><br>
<em>&gt; principles into
</em><br>
<em>&gt; an AI - this is what I consider to be the generalized Asimov Law fallacy.
</em><br>
<p>OK, that's a clear statement -- I can revise my mention of your theory to
<br>
account for this sentiment on your part
<br>
<p>Personally, of course, I disagree that there are fundamental problems with
<br>
imbuing abstract ethical principles into an AI.
<br>
<p>I think that there are problems with imbuing overly narrow ethical
<br>
principles into an AI, which don't necessarily exist if the principles are
<br>
abstract and &quot;natural&quot; enough.  (Yet grounded in specifics in spite of their
<br>
abstraction... with the understanding that the specifics will change over
<br>
time but the abstractions will remain relatively constant)
<br>
<p>I think it may be easier to put an abstract ethical principle into an AI
<br>
than to put a big messy thing like &quot;humaneness&quot; into one, actually...
<br>
<p>-- Ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7956.html">Christian Szegedy: "Re: Positive Transcension"</a>
<li><strong>Previous message:</strong> <a href="7954.html">Mikko Särelä: "RE: Encouraging a Positive Transcension"</a>
<li><strong>In reply to:</strong> <a href="7947.html">Eliezer S. Yudkowsky: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7944.html">Jef Allbright: "Positive Transcension"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7955">[ date ]</a>
<a href="index.html#7955">[ thread ]</a>
<a href="subject.html#7955">[ subject ]</a>
<a href="author.html#7955">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
