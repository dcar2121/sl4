<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: All sentient have to be observer-centered!  My theory of FAI morality</title>
<meta name="Author" content="Tommy McCabe (rocketjet314@yahoo.com)">
<meta name="Subject" content="Re: All sentient have to be observer-centered!  My theory of FAI morality">
<meta name="Date" content="2004-02-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: All sentient have to be observer-centered!  My theory of FAI morality</h1>
<!-- received="Thu Feb 26 05:04:53 2004" -->
<!-- isoreceived="20040226120453" -->
<!-- sent="Thu, 26 Feb 2004 04:04:46 -0800 (PST)" -->
<!-- isosent="20040226120446" -->
<!-- name="Tommy McCabe" -->
<!-- email="rocketjet314@yahoo.com" -->
<!-- subject="Re: All sentient have to be observer-centered!  My theory of FAI morality" -->
<!-- id="20040226120446.6439.qmail@web11702.mail.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20040226064551.61380.qmail@web20207.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tommy McCabe (<a href="mailto:rocketjet314@yahoo.com?Subject=Re:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality"><em>rocketjet314@yahoo.com</em></a>)<br>
<strong>Date:</strong> Thu Feb 26 2004 - 05:04:46 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8065.html">Stephen Tattum: "Re: catastrophes with Shock Level &lt; 4"</a>
<li><strong>Previous message:</strong> <a href="8063.html">Marc Geddes: "All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>In reply to:</strong> <a href="8063.html">Marc Geddes: "All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8072.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8072.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8064">[ date ]</a>
<a href="index.html#8064">[ thread ]</a>
<a href="subject.html#8064">[ subject ]</a>
<a href="author.html#8064">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
--- Marc Geddes &lt;<a href="mailto:marc_geddes@yahoo.co.nz?Subject=Re:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality">marc_geddes@yahoo.co.nz</a>&gt; wrote:
<br>
<em>&gt; My main worry with Eliezer's ideas is that I don't
</em><br>
<em>&gt; think that a non observer-centered sentient is
</em><br>
<em>&gt; logically possible.  Or if it's possible, such a
</em><br>
<em>&gt; sentient would not be stable.  Can I prove this? 
</em><br>
<em>&gt; No.
</em><br>
<p>Maybe not, but you can provde some evidence beyond
<br>
'everyone says so'.
<br>
<p><em>&gt; But all the examples of stable sentients (humans)
</em><br>
<em>&gt; that
</em><br>
<em>&gt; we have are observer centered.
</em><br>
<p>Humans are non-central special cases. Humans were
<br>
built by Darwinian evolution, the worst possible case
<br>
of design-and-test. Out in the jungle, it certainly
<br>
helps to have a goal system centered around 'I'- that
<br>
doesn't prove that it's necessary or even desirable.
<br>
<p><em>&gt; I can only point to
</em><br>
<em>&gt; this, combined with the fact that so many people
</em><br>
<em>&gt; posting to sl4 agree with me.
</em><br>
<p>Yes, and if you lived 2000 years ago, most people
<br>
would have agreed with you that the Earth was flat.
<br>
The few that didn't believe that, however, had good
<br>
reasons for it.
<br>
<p><em>&gt; I can only strongly
</em><br>
<em>&gt; urge Eliezer and others working on AI NOT to attempt
</em><br>
<em>&gt; the folly of trying to create a non observer
</em><br>
<em>&gt; centered
</em><br>
<em>&gt; AI.
</em><br>
<p>Saying that something is 'folly' doesn't mean it's
<br>
impossible- just look at how many achievements in
<br>
human history were laughed at as being 'folly'!
<br>
<p><em>&gt; For goodness sake don't try it!  It could mean
</em><br>
<em>&gt; the doom of us all.
</em><br>
<p>And so could brushing your teeth in the morning.
<br>
(Really!)
<br>
<p><em>&gt; I do agree that some kind of 'Universal Morality' is
</em><br>
<em>&gt; possible. i.e I agree that there exists a
</em><br>
<em>&gt; non-observer
</em><br>
<em>&gt; centered morality which all friendly sentients would
</em><br>
<em>&gt; aspire to. 
</em><br>
<p>Agreed.
<br>
<p><em>&gt; However, as I said, I don't think that
</em><br>
<em>&gt; non-observer sentients would be stable so any
</em><br>
<em>&gt; friendly
</em><br>
<em>&gt; stable sentient cannot follow Universal Morality
</em><br>
<em>&gt; exactly.
</em><br>
<p>Saying it doesn't make it so. You have offered no
<br>
evidence for this besides the logically fallacious
<br>
generalizing from a small, non-central sample and the
<br>
argument from popularity.
<br>
<p><em>&gt; If AI morality were just:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Universal Morality
</em><br>
<em>&gt; 
</em><br>
<em>&gt; then I postulate that the AI would fail  (either it
</em><br>
<em>&gt; could never be created in the first place, or else
</em><br>
<em>&gt; it
</em><br>
<em>&gt; would not be stable and it would under go
</em><br>
<em>&gt; friendliness
</em><br>
<em>&gt; failure).
</em><br>
<p>Saying doesn't make it so. Evidence, please?
<br>
<p><em>&gt; But there's a way to make AI's stable: add a small
</em><br>
<em>&gt; observer-centered component.  Such an AI could still
</em><br>
<em>&gt; be MOSTLY altruistic, but now it would only be
</em><br>
<em>&gt; following Universal Morality as an approximation,
</em><br>
<em>&gt; since there would be an additional observer-centered
</em><br>
<em>&gt; component.
</em><br>
<p>That's like taking a perfectly good bicycle and
<br>
putting gum in the chain.
<br>
<p><em>&gt; So I postulate that all stable FAI's have to have
</em><br>
<em>&gt; moralities of the form:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Universal Morality x Personal Morality
</em><br>
<p>Saying it doesn't make it so, as much as humans are
<br>
prone to believing something when it is repeated.
<br>
Evidence?
<br>
<p><em>&gt; Now Universal Morality (by definition) is not
</em><br>
<em>&gt; arbitrary or observer centered.  There is one and
</em><br>
<em>&gt; only
</em><br>
<em>&gt; one Universal Morality and it must be symmetric
</em><br>
<em>&gt; across
</em><br>
<em>&gt; all sentients (it has to work if everyone does it -
</em><br>
<em>&gt; positive sum interactions).
</em><br>
<p>This is quite possibly true (though many on SL4 would
<br>
argue against that)
<br>
<p><em>&gt; But Personal morality (by definition) can have many
</em><br>
<em>&gt; degrees of freedom and is observer centered.  There
</em><br>
<em>&gt; are many different possible kinds of personal
</em><br>
<em>&gt; morality
</em><br>
<em>&gt; and the morality is subjective and observer
</em><br>
<em>&gt; centered.
</em><br>
<p>Agreed.
<br>
<p><em>&gt; The only constraint is that Personal Morality has to
</em><br>
<em>&gt; be consistent with Universal Morality to be
</em><br>
<em>&gt; Friendly. 
</em><br>
<em>&gt; That's why I say that stable FAI's follow Universal
</em><br>
<em>&gt; Morality transformed by (multipication sign)
</em><br>
<em>&gt; Personal
</em><br>
<em>&gt; Morality.
</em><br>
<p>Moralities can't be 'consistent' if they aren't
<br>
identical.
<br>
<p><em>&gt; Now an FAI operating off Universal Morality alone
</em><br>
<em>&gt; (which I'm postulating is impossible or unstable)
</em><br>
<p>Saying, even repeated saying, doesn't make it so. I
<br>
need evidence!
<br>
<p><em>&gt; would to one and only one (unique) Singularity.
</em><br>
<p>Non sequitur. AIs, even if they all have the same
<br>
morality, can be quite different.
<br>
<p><em>&gt; There
</em><br>
<em>&gt; would be only one possible form a successful
</em><br>
<em>&gt; Singularity could take.  A reasonable guess (due to
</em><br>
<em>&gt; Eliezer) is that:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Universal Morality = Volitional Morality
</em><br>
<p>Quite possibly true.
<br>
<p><em>&gt; That is, it was postulated by Eli that Universal
</em><br>
<em>&gt; Morality is respect for sentient volition (free
</em><br>
<em>&gt; will).
</em><br>
<em>&gt;  With no observer centered component, an FAI
</em><br>
<em>&gt; following
</em><br>
<em>&gt; this morality would aim to fulfil sentient requests
</em><br>
<em>&gt; (consistent with sentient volition).  But I think
</em><br>
<em>&gt; that
</em><br>
<em>&gt; such an AI is impossible or unstable.
</em><br>
<p>Repeating it doesn't make it so. Where is the
<br>
evidence?
<br>
<p><em>&gt; I was postulating that all stable FAI's have a
</em><br>
<em>&gt; morality of the form:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Universal Morality x Personal Morality
</em><br>
<p>Repeating it doesn't make it correct. Where is the
<br>
evidence?
<br>
<p><em>&gt; If I am right, then there are many different kinds
</em><br>
<em>&gt; of
</em><br>
<em>&gt; successul (Friendly) Singularities. 
</em><br>
<p>Agreed.
<br>
<p><em>&gt; Although
</em><br>
<em>&gt; Universal Morality is unique, Personal Morality can
</em><br>
<em>&gt; have many degrees of freedom.
</em><br>
<p>Agreed.
<br>
<p><em>&gt; So the precise form a
</em><br>
<em>&gt; successful Singularity takes would depend on the
</em><br>
<em>&gt; 'Personal Morality' componant of the FAI's morality.
</em><br>
<p>This is like the statement 'Have you stopped beating
<br>
your wife?' - it implies which has not been proven, or
<br>
even strongly suggested by evidence.
<br>
<p><em>&gt; Assuming that:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Universal Morality = Volition based Morality
</em><br>
<em>&gt; 
</em><br>
<em>&gt; we see that:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Universal Morality x Personal Morality
</em><br>
<em>&gt; 
</em><br>
<em>&gt; leads to something quite different. 
</em><br>
<p>Agreed.
<br>
<p><em>&gt; Respect for
</em><br>
<em>&gt; sentient volition (Universal Morality) gets
</em><br>
<em>&gt; transformed (mulipication sign) by Personal
</em><br>
<em>&gt; Morality. 
</em><br>
<em>&gt; This leads to a volition based morality with an
</em><br>
<em>&gt; Acts/Omissions distinction (See my previous post for
</em><br>
<em>&gt; an explanation of the Moral Acts/Omissions
</em><br>
<em>&gt; distinctions).  
</em><br>
<em>&gt; 
</em><br>
<em>&gt; FAI's with morality of this form would still respect
</em><br>
<em>&gt; sentient volition, but they would not neccesserily
</em><br>
<em>&gt; fulfil sentient requests.
</em><br>
<p>Neither would a Yudkowskian FAI, for example, if
<br>
Saddam Hussein wants to kill everybody.
<br>
<p><em>&gt; Sentient requests would
</em><br>
<em>&gt; only be fulfilled when such requests are consistent
</em><br>
<em>&gt; with the FAI's Personal Morality.
</em><br>
<p>A good reason had better be supplied along with the
<br>
rejections.
<br>
<p><em>&gt; So the 'Personal
</em><br>
<em>&gt; Morality' component would act like a filter stopping
</em><br>
<em>&gt; some sentient requests from being fulfilled.  In
</em><br>
<em>&gt; addition, such FAI's would be pursuing goals of
</em><br>
<em>&gt; their
</em><br>
<em>&gt; own (so long as such goals did not violate sentient
</em><br>
<em>&gt; volition). 
</em><br>
<p>So would a Yudkowskian or entirely volition-based AI-
<br>
it would form goals that affected itself instead of
<br>
humans, as long as the goals would lead to helping
<br>
humanity (or sentients in general, after the
<br>
Singularity).
<br>
<p><em>&gt; So you see, my form of FAI is a far more
</em><br>
<em>&gt; interesting and complex beast than an FAI which just
</em><br>
<em>&gt; followed Universal Morality.
</em><br>
<p>'Interesting' doesn't mean better, or even possible.
<br>
&nbsp;
<br>
<em>&gt; Eliezer's 'Friendliness' theory (whereby the AI is
</em><br>
<em>&gt; reasoning about morality and can modify its own
</em><br>
<em>&gt; goals
</em><br>
<em>&gt; to try to close in on normalized 'Universal
</em><br>
<em>&gt; Morality')
</em><br>
<em>&gt; is currently only dealing with the 'Universal
</em><br>
<em>&gt; Morality' component of morality.
</em><br>
<p>True- and is there any reason why it shouldn't?
<br>
<p><em>&gt; But if I am right, then all stable FAI have to have
</em><br>
<em>&gt; an
</em><br>
<em>&gt; observer-centered (Personal Morality) componant to
</em><br>
<em>&gt; their morality as well.  
</em><br>
<p>Why?
<br>
<p><em>&gt; So it's vital that FAI programmers give
</em><br>
<em>&gt; consideration
</em><br>
<em>&gt; to just what the 'Personal Morality' of an FAI
</em><br>
<em>&gt; should
</em><br>
<em>&gt; be.
</em><br>
<p>Another statement based on an unproven assumption.
<br>
<p><em>&gt; The question of personal values cannot be
</em><br>
<em>&gt; evaded
</em><br>
<em>&gt; if non observer centered FAI's are impossible.  Even
</em><br>
<em>&gt; with Universal Morality, there would have to be a
</em><br>
<em>&gt; 'Personal Morality' componant which would have to be
</em><br>
<em>&gt; chosen directly by the programmers (this 'Personal
</em><br>
<em>&gt; Morality' componant is arbitrary and
</em><br>
<em>&gt; non-renormalizable).
</em><br>
<p>Why, again?
<br>
<p><em>&gt; To sum up: my theory is that all stable FAI have
</em><br>
<em>&gt; moralitites of the form:
</em><br>
<p>Evidence? You have provided no evidence.
<br>
<p><em>&gt; Universal Morality x Personal Morality
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Only the 'Universal Morality' can be normalized.    
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; =====
</em><br>
<em>&gt; Please visit my web-site at: 
</em><br>
<em>&gt; <a href="http://www.prometheuscrack.com">http://www.prometheuscrack.com</a>
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Find local movie times and trailers on Yahoo!
</em><br>
<em>&gt; Movies.
</em><br>
<em>&gt; <a href="http://au.movies.yahoo.com">http://au.movies.yahoo.com</a>
</em><br>
<p><p>__________________________________
<br>
Do you Yahoo!?
<br>
Get better spam protection with Yahoo! Mail.
<br>
<a href="http://antispam.yahoo.com/tools">http://antispam.yahoo.com/tools</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8065.html">Stephen Tattum: "Re: catastrophes with Shock Level &lt; 4"</a>
<li><strong>Previous message:</strong> <a href="8063.html">Marc Geddes: "All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>In reply to:</strong> <a href="8063.html">Marc Geddes: "All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8072.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8072.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8064">[ date ]</a>
<a href="index.html#8064">[ thread ]</a>
<a href="subject.html#8064">[ subject ]</a>
<a href="author.html#8064">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
