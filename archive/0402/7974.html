<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Positive Transcension 2</title>
<meta name="Author" content="Philip Sutton (Philip.Sutton@green-innovations.asn.au)">
<meta name="Subject" content="Re: Positive Transcension 2">
<meta name="Date" content="2004-02-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Positive Transcension 2</h1>
<!-- received="Thu Feb 19 14:22:53 2004" -->
<!-- isoreceived="20040219212253" -->
<!-- sent="Fri, 20 Feb 2004 08:29:43 +1100" -->
<!-- isosent="20040219212943" -->
<!-- name="Philip Sutton" -->
<!-- email="Philip.Sutton@green-innovations.asn.au" -->
<!-- subject="Re: Positive Transcension 2" -->
<!-- id="4035C577.30880.128EAC@localhost" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="BMECIIDGKPGNFPJLIDNPGEKOCMAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Philip Sutton (<a href="mailto:Philip.Sutton@green-innovations.asn.au?Subject=Re:%20Positive%20Transcension%202"><em>Philip.Sutton@green-innovations.asn.au</em></a>)<br>
<strong>Date:</strong> Thu Feb 19 2004 - 14:29:43 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7975.html">Ben Goertzel: "RE: Humane-ness (resend due to addressing error)"</a>
<li><strong>Previous message:</strong> <a href="7973.html">Rafal Smigrodzki: "RE: Ethical theories"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7976.html">Ben Goertzel: "RE: Positive Transcension 2"</a>
<li><strong>Reply:</strong> <a href="7976.html">Ben Goertzel: "RE: Positive Transcension 2"</a>
<li><strong>Reply:</strong> <a href="8011.html">Peter C. McCluskey: "Re: Positive Transcension 2"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7974">[ date ]</a>
<a href="index.html#7974">[ thread ]</a>
<a href="subject.html#7974">[ subject ]</a>
<a href="author.html#7974">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben,
<br>
<p>I've just finished reading your 14 February version of &quot;Encouraging a 
<br>
Positive Transcension&quot;.
<br>
<p>It's taken me two reads of the paper to become clear on a few issues.
<br>
<p>It seems to me that there are really three separate ethical issues at the 
<br>
heart of the paper that have been conflated and they are: how can we 
<br>
ensure that the next big advance in cognitive capacity in our neck of 
<br>
the universe-
<br>
<p>-   is not a disaster for existing sentient beings (humans being the
<br>
&nbsp;&nbsp;&nbsp;&nbsp;only ones we know of presently), 
<br>
<p>-   doesn't fail to carry forward the gains made so far by existing
<br>
&nbsp;&nbsp;&nbsp;&nbsp;creative sentient beings (including humans) and 
<br>
<p>-   helps to drive (and does not prevent) further wondrous flowering of
<br>
&nbsp;&nbsp;&nbsp;&nbsp;the universe. 
<br>
<p>While these issues clearly interrelate (will protecting existing sentient 
<br>
beings lead to a stagnation in the flowering of the universe?) I think 
<br>
there is something to be gained from being clear about each one.
<br>
<p>And there is a special aspect to the first issue that shouldn't be 
<br>
overlooked. The emergence of AGI is not some inevitable process that 
<br>
Fate deals up to us.  On the earth at least, it is the outcome of 
<br>
deliberate actions by a few humans that could impact on the rest of 
<br>
humanity (and perhaps a lot of the rest of the universe as well).  So 
<br>
while we discuss the ethics we want to see AGIs apply, we also need to 
<br>
also think about the ethics of what we ourselves are doing.  If we can't 
<br>
get our own ethics sorted out then I'm not too hopeful we'll be able to 
<br>
generate appropriate and adequate ethics in our AGI progeny.
<br>
<p>So let's start with how some humans might feel about some other 
<br>
humans creating a 'thing' which could wipe out humans without their 
<br>
agreement.  
<br>
<p>Ben you said: &quot;And this may or may not lead to the demise of humanity 
<br>
- which may or may not be a terrible thing.&quot;  At best loose language like 
<br>
this means one thing to most people - somebody else is being cavalier 
<br>
about their future - at worst they are likely to perceive an active threat 
<br>
to their existence.
<br>
<p>Frankly I doubt if anyone will care if humanity evolves or transcends to 
<br>
a higher state of being so long as it's voluntary.  To a timeless observer 
<br>
it might be arguable that the humanity of 2004 (or whatever) is no 
<br>
longer to be found - but the people who have evolved/transcended will 
<br>
still feel like humanity of the new era - they will not have been 
<br>
obliterated.  To mix this sort of change up with the death of humanity 
<br>
via, for example, rather un-necessary discussions of Nietzsche's 
<br>
notions of &quot;a good death&quot; and &quot;Man is something to be overcome&quot; 
<br>
seems to me to be pointless and dangerous. After the &quot;bad death&quot; of 
<br>
many thousands of people in the Twin Towers the US has rained death 
<br>
on many more thousands of people in the rest of the world.  For AGI-
<br>
advocates to be cavalier about the lives of billions of people is to my 
<br>
mind to, very understandably, invite similar very nasty reactions.
<br>
<p>To withhold concern for other humans lives because theoretically some 
<br>
AGI might form the view that our mass/energy could be deployed more 
<br>
beautifully/usefully seems simply silly.  The universe is a big place with, 
<br>
most likely, a mind bogglingly large amount of mass/energy not used 
<br>
by any sentient beings - so having a few billion humans on the Earth or 
<br>
the nearby planets is hardly going to cramp the style of any self-
<br>
respecting AGI with a big brain.
<br>
<p>I think the first step in creating safe AGI is for the would-be creators of 
<br>
AGI to themselves make an ethical commitment to the protection of 
<br>
humans - not because humans are the peak of creation or all that 
<br>
stunningly special from the perspective of the universe as a whole but 
<br>
simply because they exist and they deserve respect - especially from 
<br>
their fellow humans.  If AGI developers cannot give their fellow humans 
<br>
that commitment or that level of respect, then I think they demonstrate 
<br>
they are not safe parents for growing AGIs!  I was actually rather 
<br>
disturbed by your statement towards the end of your paper where you 
<br>
said: &quot;In spite of my own affection for Voluntary Joyous Growth, 
<br>
however, I have strong inclinations toward both the Joyous Growth 
<br>
Guided Voluntarism and pure Joyous Growth variants as well.&quot;  My 
<br>
reading of this is that you would be prepared to inflict Joyous Growth 
<br>
future on people whether they wanted it or not and even if this resulted 
<br>
in the involuntary elimination of people or other sentients that somehow 
<br>
were seen by the AGI or AGIs pursuing Joyous Growth as being an 
<br>
impediment in the way of the achievement of joyous growth.  If I've 
<br>
interpreted what you are saying correctly that's pretty scary stuff!
<br>
<p>I think the next step is to consider what values we would like AGIs to 
<br>
hold in order for them to be sound citizens in a community of sentients.  
<br>
I think the minimum that is needed is for them to have a tolerant, 
<br>
respectful, compassionate, live-and-let-live attitude.  This is what I 
<br>
personally would hope for from all sentients - no matter how low or 
<br>
mighty their intellectual powers.  This doesn't mean that all human 
<br>
behaviours or all AGI behaviours should be accepted.  Cruel or 
<br>
exploitative or oppressive behaviours by any sentient or group of 
<br>
sentients would seem to me to be behaviours that should be resisted or 
<br>
prevented.
<br>
<p>I think AGIs that had a tolerant, respectful, compassionate, live-and-let-
<br>
live ethic would not intrude excessively on human society.  They might, 
<br>
for example, try to discourage female circumcision or even go so far as 
<br>
stopping capital punishment in human societies (I can't see that these 
<br>
actions would conform to the ethics that the AGIs were given [under my 
<br>
scenario] their human creators/carers).  As far as I can see I don't think 
<br>
that AGIs need to have ported into them a sort of general digest of 
<br>
human-ness or even an idiosyncratic (renormalised) essence of general 
<br>
humane-ness.  I think we should be able to be more transparent than 
<br>
that and to identify the key ethical drivers that lead to tolerant, 
<br>
respectful, compassionate, live-and-let-live behaviour.
<br>
<p>I think these notions are sufficiently abstract to be able to pass your test 
<br>
of being likely to &quot;survive successive self-modification&quot;.  They are not 
<br>
tied to a form of humanity that is frozen in time and they not tied 
<br>
conceptually to any particular form of life or sentience.  And I think this 
<br>
base ethic would be useful in guiding how AGIs relate to each other.
<br>
<p>If AGIs adopted a tolerant, respectful, compassionate, live-and-let-live 
<br>
ethic then I think that we would have pretty good assurance that the 
<br>
emergence of AGIs was not going to be a disaster for any existing 
<br>
sentient beings (including humans beings) and that the gains made so 
<br>
far by existing creative sentient beings would not be lost due to the 
<br>
cavalier (or otherwise) annihilation of sentient societies by more 
<br>
powerful AGIs.
<br>
<p>Now I want to move on to the issue how ethical systems might ensure 
<br>
that AGIs help to drive (and do not prevent) further wondrous flowering 
<br>
of the universe.
<br>
<p>Ben, you proposed that AGIs should have an ethic of promoting 
<br>
voluntary, joyous growth.  The way you discussed this issue it made it 
<br>
sound as if all AGIs should have this goal/ethical structure.  It's not 
<br>
clear to me that all AGIs need such a goal structure for there to be a 
<br>
wondrous flowering of the universe.  The development of art/science 
<br>
etc. that we love so much on the earth was the work of one species in 
<br>
20 million.  Perhaps only a small minority of AGIs need to be creative 
<br>
or promoting &quot;voluntary joyous growth&quot; for there to be the unfolding that 
<br>
you are hoping for.
<br>
<p>My guess is that if we avoid human or AGI dictatorship, then humans of 
<br>
all sorts will facilitate the creation of all sorts of AGIs.  So long as these 
<br>
AGIs all practice a tolerant, respectful, compassionate, live-and-let-live 
<br>
ethic and so long as *some* AGIs pursue 'voluntary joyous growth' or 
<br>
'growth in knowledge and development and application of creativity' 
<br>
then I think a positive transcension will occur.  I think we should look to 
<br>
a plurality of AGI ethics as much or more than we should expect and 
<br>
support a plurality of human goals and ethics.  If people or AGIs are 
<br>
happy with relatively unchanging lives (as we perceive it), then good on 
<br>
them if that's what makes them happy or transcendent. It only takes a 
<br>
small percentage of 'driven creatives' (whether human or AGI) to keep 
<br>
evolution moving along.  In a healthy mix of sentients it's probably also 
<br>
a good idea if at least a reasonable percentage (5-10%???) are driven 
<br>
by an urge to improve wellbeing for themselves and others - ie. not 
<br>
driven pricipally by growth in knowledge/patterns or hedonistic joy!  
<br>
This nead for a complementary mix of motivations within a 
<br>
human/AGI/other sentient meta population is another reason for having 
<br>
a plurality of AGIs rather than just one.  (I know that mindplexes could 
<br>
be formed by groups of AGIs - but I still think that even a mindplex will 
<br>
be better quality/wiser if we encourage the creation of many, diverse 
<br>
AGIs with distinct perspectives.)
<br>
<p>In your paper you suggest that we need AGIs to save humanity from 
<br>
our destructive urges (applied via advanced technology).  If having 
<br>
AGIs around could increase the risk of humanity being wiped out to 
<br>
achieve a more beautiful deployment of mass/energy then it might be a 
<br>
good idea to go back and check to see just exactly how dangerous the 
<br>
other feared technologies are. While nanotech and genetic engineering 
<br>
could produce some pretty virulent and deadly entities I'm not sure that 
<br>
they are likely to be much more destructive than bubonic plague, 
<br>
eboloa virus, small pox have been in their time etc.  There are a lot of 
<br>
people around so that even if these threats killed millions? billions? 
<br>
they are unlikely to wipe out even most people.  So should we seek 
<br>
help from this scale of threat by creating something that might arbitarily 
<br>
decide to wipe out the lot of us on a whim?
<br>
<p>I don't think that AGIs are inevitably a threat, but this will only be if 
<br>
AGIs are imbued with tolerant, respectful, compassionate, live-and-let-
<br>
live ethics and I think these will only be imbued if their human makers 
<br>
are similarly moved by the same ethic.
<br>
<p>-------
<br>
<p>By the way, your interpretation of my idea of 'sustainability' as a form of 
<br>
notalgia misses the point I was trying to make. I don't think one has to 
<br>
be a Luddite or a back-to-the land/cave person to find some value in 
<br>
the concept of sustainability.  I think life for any creative sentient is 
<br>
made up of three processes:
<br>
<p>-   changing things for the better driven by need or delight 
<br>
<p>-   engaging in a journey of life where change occurs but the change
<br>
&nbsp;&nbsp;&nbsp;&nbsp;cannot be characterised as better or worse than what went before 
<br>
<p>-   retaining conditions/things that are valued because they are needed
<br>
&nbsp;&nbsp;&nbsp;&nbsp;(from a utilitarian point of view) or they are valued
<br>
&nbsp;&nbsp;&nbsp;&nbsp;existentially/morally etc. 
<br>
<p>These three processes combine a concern for both change and 
<br>
continuity.  At any particular moment not *everything* is changed nor is 
<br>
*everything* retained (sustained).
<br>
<p>The notion of combining continuity and change is particularly important 
<br>
if you think that the purpose of change includes creating better 
<br>
situations/things.  If things can be improved then it is sensible to protect 
<br>
these improvements from back-sliding when yet further changes are 
<br>
made.
<br>
<p>Nostalgia plays a part in this for some people but generally I suspect a 
<br>
fairly small part in this whole process of managing/fostering continuity 
<br>
and change.
<br>
<p><pre>
----
Oh well, I hope what I've said is of some use!
Cheers, Philip
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7975.html">Ben Goertzel: "RE: Humane-ness (resend due to addressing error)"</a>
<li><strong>Previous message:</strong> <a href="7973.html">Rafal Smigrodzki: "RE: Ethical theories"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7976.html">Ben Goertzel: "RE: Positive Transcension 2"</a>
<li><strong>Reply:</strong> <a href="7976.html">Ben Goertzel: "RE: Positive Transcension 2"</a>
<li><strong>Reply:</strong> <a href="8011.html">Peter C. McCluskey: "Re: Positive Transcension 2"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7974">[ date ]</a>
<a href="index.html#7974">[ thread ]</a>
<a href="subject.html#7974">[ subject ]</a>
<a href="author.html#7974">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
