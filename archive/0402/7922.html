<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Friendly AI in &quot;Positive Transcension&quot;</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Friendly AI in &quot;Positive Transcension&quot;">
<meta name="Date" content="2004-02-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Friendly AI in &quot;Positive Transcension&quot;</h1>
<!-- received="Sat Feb 14 19:27:33 2004" -->
<!-- isoreceived="20040215022733" -->
<!-- sent="Sat, 14 Feb 2004 21:27:23 -0500" -->
<!-- isosent="20040215022723" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Friendly AI in &quot;Positive Transcension&quot;" -->
<!-- id="402ED90B.3080504@pobox.com" -->
<!-- charset="windows-1252" -->
<!-- inreplyto="BMECIIDGKPGNFPJLIDNPEEKOCMAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Friendly%20AI%20in%20&quot;Positive%20Transcension&quot;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Feb 14 2004 - 19:27:23 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7923.html">Keith Henson: "Ethics was In defense of physics (was: Encouraging a Positive Transcension)"</a>
<li><strong>Previous message:</strong> <a href="7921.html">Metaqualia: "Re: In defense of physics (was: Encouraging a Positive Transcension)"</a>
<li><strong>In reply to:</strong> <a href="7915.html">Ben Goertzel: "Positive Transcension"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7929.html">Metaqualia: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Reply:</strong> <a href="7929.html">Metaqualia: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Reply:</strong> <a href="7939.html">Ben Goertzel: "RE: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7922">[ date ]</a>
<a href="index.html#7922">[ thread ]</a>
<a href="subject.html#7922">[ subject ]</a>
<a href="author.html#7922">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Here is a revised version of my recent futurist essay, improved 
</em><br>
<em>&gt; significantly in parts due to the feedback of several readers
</em><br>
<em>&gt; 
</em><br>
<em>&gt; www.goertzel.org/papers/PositiveTranscension.htm
</em><br>
<p>Ben, looking over this paper, I would like you to attach a disclaimer that
<br>
runs something like this:
<br>
<p>&quot;Note:  Eliezer Yudkowsky has looked over the sections of this paper
<br>
dealing with Friendly AI, and says that not only did I not understand FAI,
<br>
but I wasn't even in the remote ballpark - Yudkowsky says he couldn't even
<br>
have figured out I was talking about his theory if I hadn't mentioned it
<br>
by name, and that all of my representations are absolutely antithetical to
<br>
spirit of Friendly AI as he meant it.&quot;
<br>
<p>No offense is meant.  Okay, so you utterly failed to get FAI; so did
<br>
everyone else.  Anyway, here are some examples of specific problems:
<br>
<p><em>&gt; For Eliezer Yudkowsky, the preservation of human life and human will is
</em><br>
<em>&gt;  of almost supreme importance.
</em><br>
<p>What I wish to preserve is sentient life and humane morality.  Presently, 
<br>
humans are the only sentients (I don't think it includes chimps, but I'm 
<br>
not sure), and it is *only* humans that embody humane morality.  Lose the
<br>
humans and you lose everything that you value.  I know you don't believe
<br>
this, but it's why I place the emphasis.
<br>
<p>You know how economists are always talking about utility functions?  If 
<br>
you wanted to take a very very VERY rough stab at FAI, it would be 
<br>
something like this:
<br>
<p>1)  This is a human.
<br>
<p>2)  The human being embodies a utility function, preferences, 
<br>
things-that-output-choices.  (No, not really, but we're taking rough stabs 
<br>
here.)
<br>
<p>3)  This is what the human being's preferences would be, taking the limit 
<br>
as knowledge approaches a perfect model of reality, and computing power 
<br>
available goes to infinity.  Call this a &quot;volition&quot;.  The limit is not 
<br>
even remotely well-defined.  On to step 4.
<br>
<p>4)  Let the volition examine itself.  This is the renormalized volition, 
<br>
or the volition under reflection.  Take the limit to logical omniscience 
<br>
on this too.  Now it's even less well-defined then before.
<br>
<p>5)  Instead of using the renormalized volition from a specific human, use 
<br>
a &quot;typical&quot; human starting point derived from the evolutionary psychology 
<br>
specific to the species.  This is a &quot;humane&quot; morality.  Oh, and the limit? 
<br>
&nbsp;&nbsp;It ain't getting any better-defined.
<br>
<p>6)  Take all the confusion involved in taking the limit of humaneness and 
<br>
call it &quot;entropy&quot;.  This is much more impressive.
<br>
<p>7) Actually calculate the amount of entropy in the system.  Be more 
<br>
reluctant to guess when the entropy is high.
<br>
<p>8)  Jump back to systems that are not logically omniscient by 
<br>
reintroducing probabilism into the calculations.  This kind of 
<br>
probabilistic uncertainty also adds to the entropy.  We are now 
<br>
approximating humaneness.
<br>
<p>9)  Let an ideally Friendly AI be the AI that would be constructed by a 
<br>
humane morality.
<br>
<p>10) Build an approximately Friendly AI with an architecture that 
<br>
explicitly treats itself as an approximation to the AI that would be 
<br>
constructed by a humane morality.  If it were me doing the first 
<br>
approximation, I'd start by guessing that this involved embodying a humane 
<br>
morality within the FAI itself, i.e., a humane FAI.
<br>
<p>I wish these were the old days, so I could look over what I just wrote 
<br>
with satisfaction, rather than the dreadful sinking knowledge that 
<br>
everything I just said sounded like complete gibberish to anyone honest 
<br>
enough to admit it.
<br>
<p>Some key derived concepts are these:
<br>
<p>a)  A Friendly AI improving itself approaches as a limit the AI you'd have 
<br>
built if you knew what you were doing, *provided that* you *did* know what 
<br>
you were doing when you defined the limiting process.
<br>
<p>b)  A Friendly AI does not look old and busted when the civilization that 
<br>
created it has grown up a few million years.  FAIs grow up too - very 
<br>
rapidly, where the course is obvious (entropy low), in other areas waiting 
<br>
for the civilization to actually make its choices.
<br>
<p>c)  If you are in the middle of constructing an FAI, and you make a 
<br>
mistake about what you really wanted, but you got the fundamental 
<br>
architecture right, you can say &quot;Oops&quot; and the FAI listens.  This is 
<br>
really really REALLY nontrivial.  It requires practically the entire 
<br>
discipline of FAI to do this one thing.
<br>
<p>d)  Friendly AI doesn't run on verbal principles or moral philosophies. 
<br>
If you said to an FAI, &quot;Joyous Growth&quot;, its architecture would attempt to 
<br>
suck out the warm fuzzy feeling that &quot;Joyous Growth&quot; gives you and is the 
<br>
actual de facto reason you feel fond of &quot;Joyous Growth&quot;.
<br>
<p>e)  The architecture that does this cool stuff is where the technical meat 
<br>
comes in, and is the interesting part of Friendly AI that goes beyond bull 
<br>
sessions about what kind of perfect world we'd like.  The bull sessions 
<br>
are useless.  You'll know what you want when you know how to do it.  All 
<br>
the really *good* options are phrased in the deep language of FAI.
<br>
<p>f)  Friendly AI is frickin' complicated, so please stop summarizing it as 
<br>
&quot;hardwiring benevolence to humans&quot;.  I ain't bloody Asimov.  This isn't 
<br>
even near the galaxy of the solar system that has the planet where the 
<br>
ballpark is located.
<br>
<p><em>&gt; He goes even further than most Singularity believers, postulating a 
</em><br>
<em>&gt; “hard takeoff” in which a self-modifying AI program moves from 
</em><br>
<em>&gt; near-human to superhuman intelligence within hours or minutes – instant
</em><br>
<em>&gt; Singularity!
</em><br>
<p>Ben, you *know* I've read my Tversky and Kahneman.  You *know* I'm not
<br>
stupid enough to say that as a specific prediction of real-world events.
<br>
<p>What I do say is that a hard takeoff looks to be theoretically possible
<br>
and is a real practical possibility as well.  Moreover, I say that it is 
<br>
both reasonable, and a necessary exercise in AI craftsmanship, to work as 
<br>
if a hard takeoff was an immediate possibility at all times - it forces 
<br>
you to do things that are the right things in any case, and to address 
<br>
necessary theoretical issues.
<br>
<p><em>&gt; With this in mind, he prioritizes the creation of “Friendly AI’s” – 
</em><br>
<em>&gt; artificial intelligence programs with “beneficence to human life” 
</em><br>
<em>&gt; programmed in or otherwise inculcated as a primary value.
</em><br>
<p>Absolutely wrong.  Ben, you never understood Friendly AI from the
<br>
beginning.  I say it without malice, because I never understood Novamente
<br>
from the beginning, and it took me too many interchanges with you before I
<br>
realized it.  Communicating about AI is fscking hard.  I don't understand
<br>
Novamente's parts, design philosophy, or where the work gets done; as far
<br>
as I can tell the parts should sit there and brood and not do a damn
<br>
thing, which is presumably not the case.  If you actually wanted to
<br>
communicate to me how Novamente works, it would probably take hands-on
<br>
experience with the system, or maybe a university course with textbooks,
<br>
experiments, teaching assistants, and experienced professors.  That's what
<br>
it takes to teach most subjects, and AI is fscking hard.  Friendly AI is
<br>
two orders of magnitude fscking harder.  If there were college courses on
<br>
the subject, people still wouldn't get it, because FAI is too fscking
<br>
hard.  So I mean no malice by saying that you didn't get Friendly AI at all.
<br>
<p>The idea of programming the *output* of a human's moral philosophy into an
<br>
FAI as a &quot;primary value&quot; is absolutely antithetical to the spirit of
<br>
Friendly AI, because you lose all the information and dynamics the human
<br>
used to decide that &quot;beneficence to human life&quot; (for example) was even a
<br>
good idea to begin with.  You therefore lose the content of &quot;beneficence&quot;,
<br>
the dynamics that would decide what constituted &quot;life&quot;, and so on.
<br>
<p><em>&gt; The creation of Friendly AI, he proposes, is the path most likely to 
</em><br>
<em>&gt; lead to a human-friendly post-Singularity world.[11]
</em><br>
<p>Smells vaguely of tautology, given definition (9) above.
<br>
<p><em>&gt; This perspective raises a major issue regarding the notion of AI 
</em><br>
<em>&gt; Friendliness.  Perhaps “Be nice to humans” or “Obey your human masters”
</em><br>
<em>&gt; are simply too concrete and low-level ethical prescriptions to be 
</em><br>
<em>&gt; expected to survive the Transcension.  Perhaps it’s more reasonable to 
</em><br>
<em>&gt; expect highly abstract ethical principles to survive.  Perhaps it’s 
</em><br>
<em>&gt; more sensible to focus on ensuring the Principle of Voluntary Joyous 
</em><br>
<em>&gt; Growth to survive the Transcension, than to focus on specific ethical 
</em><br>
<em>&gt; rules (which have meaning only within specific ethical systems, which 
</em><br>
<em>&gt; are highly context and culture bound).
</em><br>
<p>FAI does not make specific ethical rules the basis of hardwired
<br>
programming.  I seem to recall addressing you on this specific point
<br>
long since, actually... so for heaven's sake, if you stop nothing else, 
<br>
please stop this particular misrepresentation.
<br>
<p>The problem with expecting &quot;highly abstract ethical principles&quot; to survive
<br>
is that even if they only contain 30 bits of Kolmogorov complexity, which
<br>
is far too little, the odds of them arising by chance are still a billion
<br>
to one.  Your instincts give you a totally off estimate of the actual
<br>
complexity of your &quot;highly abstract&quot; ethical principles.  You say
<br>
something like &quot;be beneficent to human life&quot;, and your mind recalls Buddha
<br>
and Gandhi, and all the cognitive complexity you've ever developed
<br>
associated with &quot;beneficence&quot; over a lifetime of living with human
<br>
emotions, a human limbic system, a human empathic architecture for
<br>
predicting your conspecifics by using yourself as a model, a human
<br>
sympathetic architecture for evaluating fairness and other
<br>
brainware-supported emotional concepts by putting yourself in other
<br>
people's shoes.
<br>
<p><em>&gt; So, my essential complaint against Yudkowsky’s Friendly AI notion is 
</em><br>
<em>&gt; that – quite apart from ethical issues regarding the wisdom of using 
</em><br>
<em>&gt; mass-energy on humans rather than some other form of existence -- I 
</em><br>
<em>&gt; strongly suspect that it’s impossible to create AGI’s that will 
</em><br>
<em>&gt; progressively radically self-improve and yet retain belief in the “Be 
</em><br>
<em>&gt; nice to and preserve humans” maxim.  I think this “Friendly AI” 
</em><br>
<em>&gt; principle is just too concrete and too non-universal to survive the 
</em><br>
<em>&gt; successive radical-self-improvement process and the Transcension.  On 
</em><br>
<em>&gt; the other hand, I think a more abstract and universally-attractive 
</em><br>
<em>&gt; principle like Voluntary Joyous Growth might well make it.
</em><br>
<p>The core of Friendly AI is the sort of thing you seem to call &quot;abstract&quot;, 
<br>
indeed, far more abstract than &quot;Joyous Growth&quot; (though FAI must become 
<br>
entirely concrete in my thinking, if it is ever to come into existence). 
<br>
Yudkowsky's Friendly AI notion does indeed say that you have to understand 
<br>
how to transfer large amounts of moral complexity from point A to point B. 
<br>
&nbsp;&nbsp;If you do not understand how to transfer &quot;concrete&quot; and &quot;non-universal&quot; 
<br>
complexity, you will fail, because absolutely everything you want to do 
<br>
has concrete non-universal complexity in it.  &quot;Voluntary Joyous Growth&quot;, 
<br>
as you understand that and would apply it, has kilobits and kilobits of 
<br>
complexity bound up in it.  You're evaluating the simplicity of this 
<br>
concept using a brain that makes things like empathy and sympathy and 
<br>
benevolence into emotional primitives that can be chunked and manipulated 
<br>
as if they were ontologically basic, and they're not.  You're estimating 
<br>
the complexity of things like &quot;Voluntary Joyous Growth&quot; as if it were 
<br>
three words long, when actually they're words that call up complexity-rich 
<br>
concepts that key into your entire existing emotional architecture and 
<br>
have implications &quot;obvious&quot; under that emotional architecture and that 
<br>
emotional architecture only.  Try explaining your Voluntary Joyous Growth 
<br>
to a !Kung tribesman, who's *got* all your brainware already, and you'll 
<br>
get a better picture of the complexity inherent in it.  Then try 
<br>
explaining it to a chimpanzee.  Then try explaining it to a Motie.  And 
<br>
then maybe you'll be ready to explain it to silicon.
<br>
<p>Friendly AI is farging difficult!  If you cannot do farging difficult
<br>
things, you cannot do Friendly AI!  Why is this so bloody hard to explain
<br>
to people?  Why does everyone expect this to be some kind of cakewalk?
<br>
Why does everyone turn around and flee at the merest hint that any kind of 
<br>
real effort might be involved?
<br>
<p>I was guilty of this too, by the way, which is why I'm now so intolerant 
<br>
of it.
<br>
<p><em>&gt; On the other hand, the Friendly AI principle does not seem to harmonize
</em><br>
<em>&gt; naturally with the evolutionary nature of the universe at all.
</em><br>
<em>&gt; Rather, it seems to contradict a key aspect of the nature of the
</em><br>
<em>&gt; universe -- which is that the old gives way to the new when the time
</em><br>
<em>&gt; has come for this to occur.
</em><br>
<p>And all societies inevitably progress toward Communism.
<br>
<p>This sort of thinking is guaranteed to fail.  Everyone would like to think 
<br>
the universe is on their side.  It is the naturalistic fallacy committed 
<br>
with respect to mystical gibberish.
<br>
<p>The universe I live in is neither for me nor against me.  When I am lucky, 
<br>
it presents to me an acceptable outcome as an accessible option.
<br>
<p><em>&gt; It’s an interesting question whether speciecide contradicts the
</em><br>
<em>&gt; universal-attractor nature of Compassion.  Under the Voluntary Joyous
</em><br>
<em>&gt; Growth principle, it’s not favored to extinguish beings without their
</em><br>
<em>&gt; permission.  But if a species wants to annihilate itself, because it
</em><br>
<em>&gt; feels its mass-energy can be used for something better, then it’s
</em><br>
<em>&gt; perfectly Compassionate to allow it to do so.
</em><br>
<p>Yeah, yeah, been there, done that, wrote the damn book, tried to have all 
<br>
the copies of the damn book burned after I actually figured out what the 
<br>
hell I was talking about.  See <a href="http://hanson.gmu.edu/vc.html">http://hanson.gmu.edu/vc.html</a> for an 
<br>
example of who Eliezer used to be.
<br>
<p>There is no light in this world except that embodied in humanity.  Even my 
<br>
old thoughts of species self-sacrifice were things that only a human would 
<br>
ever have thought.  If you lose the information bound up in humanity, you 
<br>
lose everything of any value, and it won't ever come back.  Everything we 
<br>
care about is specific to humanity, even as our (rather odd) moral 
<br>
instincts drive us to argue that it is universal.  When I understood that, 
<br>
I shut up about Shiva-Singularities.  See, I even gave it a name, back in 
<br>
my wild and reckless youth.  You've sometimes presumed to behave toward me 
<br>
in a sage and elderly fashion, Ben, so allow me to share one of the 
<br>
critical lessons from my own childhood:  No, you do not want humanity to 
<br>
go extinct.  Trust me on this, because I've been there, and I know from 
<br>
experience that it isn't obvious.
<br>
<p><em>&gt; In either case: Hope for the best!
</em><br>
<p>I think I covered this at length a few weeks ago.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7923.html">Keith Henson: "Ethics was In defense of physics (was: Encouraging a Positive Transcension)"</a>
<li><strong>Previous message:</strong> <a href="7921.html">Metaqualia: "Re: In defense of physics (was: Encouraging a Positive Transcension)"</a>
<li><strong>In reply to:</strong> <a href="7915.html">Ben Goertzel: "Positive Transcension"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7929.html">Metaqualia: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Reply:</strong> <a href="7929.html">Metaqualia: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Reply:</strong> <a href="7939.html">Ben Goertzel: "RE: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7922">[ date ]</a>
<a href="index.html#7922">[ thread ]</a>
<a href="subject.html#7922">[ subject ]</a>
<a href="author.html#7922">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
