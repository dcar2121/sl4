<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: All sentient have to be observer-centered!  My theory of FAI morality</title>
<meta name="Author" content="Tommy McCabe (rocketjet314@yahoo.com)">
<meta name="Subject" content="Re: All sentient have to be observer-centered!  My theory of FAI morality">
<meta name="Date" content="2004-02-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: All sentient have to be observer-centered!  My theory of FAI morality</h1>
<!-- received="Sun Feb 29 06:16:08 2004" -->
<!-- isoreceived="20040229131608" -->
<!-- sent="Sun, 29 Feb 2004 05:16:07 -0800 (PST)" -->
<!-- isosent="20040229131607" -->
<!-- name="Tommy McCabe" -->
<!-- email="rocketjet314@yahoo.com" -->
<!-- subject="Re: All sentient have to be observer-centered!  My theory of FAI morality" -->
<!-- id="20040229131607.928.qmail@web11705.mail.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20040229054853.31929.qmail@web20203.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tommy McCabe (<a href="mailto:rocketjet314@yahoo.com?Subject=Re:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality"><em>rocketjet314@yahoo.com</em></a>)<br>
<strong>Date:</strong> Sun Feb 29 2004 - 06:16:07 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8089.html">Philip Sutton: "Re: UNU report 2003 identified human-machine intelligence as key issue"</a>
<li><strong>Previous message:</strong> <a href="8087.html">Tommy McCabe: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>In reply to:</strong> <a href="8081.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8095.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8095.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8088">[ date ]</a>
<a href="index.html#8088">[ thread ]</a>
<a href="subject.html#8088">[ subject ]</a>
<a href="author.html#8088">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
--- Marc Geddes &lt;<a href="mailto:marc_geddes@yahoo.co.nz?Subject=Re:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality">marc_geddes@yahoo.co.nz</a>&gt; wrote:
<br>
<em>&gt;  --- Tommy McCabe &lt;<a href="mailto:rocketjet314@yahoo.com?Subject=Re:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality">rocketjet314@yahoo.com</a>&gt; wrote: &gt;
</em><br>
<em>&gt; You say that moralities 'consistent' with each other
</em><br>
<em>&gt; &gt; don't have to be identical. They do. Morality
</em><br>
<em>&gt; isn't
</em><br>
<em>&gt; &gt; mathematics. In order for them to be consistent,
</em><br>
<em>&gt; &gt; they
</em><br>
<em>&gt; &gt; have to give the same result in every situation,
</em><br>
<em>&gt; in
</em><br>
<em>&gt; &gt; other words, they must be identical. 'I like X'
</em><br>
<em>&gt; &gt; isn't
</em><br>
<em>&gt; &gt; really a consistent morality with 'Do not kill',
</em><br>
<em>&gt; &gt; since
</em><br>
<em>&gt; &gt; given the former, one would kill to get X. I don't
</em><br>
<em>&gt; &gt; like the idea of an AI acting like a human, ie, of
</em><br>
<em>&gt; &gt; having heuristics of 'Coke is better tha Pepsi'
</em><br>
<em>&gt; for
</em><br>
<em>&gt; &gt; no
</em><br>
<em>&gt; &gt; good reason. Of course, if their is a good reason,
</em><br>
<em>&gt; a
</em><br>
<em>&gt; &gt; Yudkowskian FAI would have that anyway. You may
</em><br>
<em>&gt; take
</em><br>
<em>&gt; &gt; the 'personal component of morality is necessary'
</em><br>
<em>&gt; &gt; thing as an axiom, but I don't and I need to see
</em><br>
<em>&gt; &gt; some
</em><br>
<em>&gt; &gt; proof.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; O.K, 'conisistent with' wasn't a good word to use as
</em><br>
<em>&gt; regards moralities.  But I think you know what I
</em><br>
<em>&gt; meant.  Perhaps 'congruent with' would be a better
</em><br>
<em>&gt; term.  
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I could define morality Y as being congruent with
</em><br>
<em>&gt; moralitity X, if in most situations, Y did not
</em><br>
<em>&gt; conflict with X.  And if in the situations where Y
</em><br>
<em>&gt; did
</em><br>
<em>&gt; conflict, X took priority.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So for instance, say morality X was 'Thou shall not
</em><br>
<em>&gt; kill', and morality Y was 'Coke is Good, Pepsi is
</em><br>
<em>&gt; Evil'.  Y is congruent with X if a sentient can
</em><br>
<em>&gt; pursue
</em><br>
<em>&gt; Y without conflicting with X  (The sentient looks to
</em><br>
<em>&gt; promote Coke, but without killing anyone).
</em><br>
<p>Here's an idea: Perhaps (although I have no idea how
<br>
you would relate them) you could have a supergoal of
<br>
what you call Universal Morality, and then, if
<br>
supporting Coke over Pepsi somehow supported Universal
<br>
Morality, you could have it as a subgoal. That way,
<br>
<p>1. You support Coke over Pepsi
<br>
2. Your support is justified
<br>
3. If, at any time in the future, supporting Coke
<br>
contradicts Universal Morality, it can be easily
<br>
dropped
<br>
<p><em>&gt; The reason I think a 'Personal Morality' component
</em><br>
<em>&gt; is
</em><br>
<em>&gt; neccessery, is that WE DON'T KNOW what the Universal
</em><br>
<em>&gt; Morality component is.
</em><br>
<p>That's like saying, &quot;I don't know what the perfect car
<br>
is, so that means I'm going to assume that having gum
<br>
in the engine is necessary&quot;. Makes no sense at all. If
<br>
you don't know how to build an engine, substituting
<br>
sticks of gum isn't going to work.
<br>
<p><em>&gt; It might be 'Volitional
</em><br>
<em>&gt; Morality', but that's just Eliezer's guess.
</em><br>
<p>A 'guess' implies that Eliezer's ideas about morality
<br>
aren't justified. I'm sure that Eli has good reasons,
<br>
whatever they are, for thinking that Volitional
<br>
Morality is the objective morality, or at least good.
<br>
Anyway, AIs don't have moralities hardwired into them-
<br>
they can correct programmer deficiencies later.
<br>
<p><em>&gt; FAI's
</em><br>
<em>&gt; are
</em><br>
<em>&gt; designed to try to reason out Universal Morality for
</em><br>
<em>&gt; themselves.  Programmers don't know what it is in
</em><br>
<em>&gt; advance.  It's unlikely they'd get it exactly right
</em><br>
<em>&gt; to
</em><br>
<em>&gt; begin with.  So, in the beginning some of what we
</em><br>
<em>&gt; teach an FAI will be wrong.  The part which is wrong
</em><br>
<em>&gt; will be just arbitrary (Personal Morality).  So you
</em><br>
<em>&gt; see, all FAI's WILL have a 'Personal Morality'
</em><br>
<em>&gt; component to start with.
</em><br>
<p>I'll have to agree with you there. Programmers aren't
<br>
perfect, and moral mistakes are bound to get into the
<br>
AI. However, the AI can certainly correct these. And
<br>
that's not even 'all FAIs'- it's just all FAIs built
<br>
by humans.
<br>
<p><em>&gt; &gt; &quot;Well yeah true, a Yudkowskian FAI would of course
</em><br>
<em>&gt; &gt; refuse requests to hurt other people.  But it
</em><br>
<em>&gt; would
</em><br>
<em>&gt; &gt; aim to fulfil ALL requests consistent with
</em><br>
<em>&gt; volition.
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; (All requests which don't involve violating other
</em><br>
<em>&gt; &gt; peoples right).&quot;
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; And that's a bad thing? You really don't want an
</em><br>
<em>&gt; AI
</em><br>
<em>&gt; &gt; deciding not to fulfill Pepsi requests because it
</em><br>
<em>&gt; &gt; thinks Coke is better for no good reason- that
</em><br>
<em>&gt; leads
</em><br>
<em>&gt; &gt; to an AI not wanting to fulfill Singularity
</em><br>
<em>&gt; requests
</em><br>
<em>&gt; &gt; because suffering is better.
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; &quot;For instance, 'I want to go ice skating', 'I want
</em><br>
<em>&gt; a
</em><br>
<em>&gt; &gt; Pepsi', 'I want some mountain climbing qquipment'
</em><br>
<em>&gt; &gt; and
</em><br>
<em>&gt; &gt; so on and so on.  A Yudkowskian FAI can't draw any
</em><br>
<em>&gt; &gt; distinctions between these, and would see all of
</em><br>
<em>&gt; &gt; them
</em><br>
<em>&gt; &gt; as equally 'good'.&quot;
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; It wouldn't- at all. A Yudkowskian FAI, especially
</em><br>
<em>&gt; a
</em><br>
<em>&gt; &gt; transhuman one, could easily apply Bayes' Theorem
</em><br>
<em>&gt; &gt; and
</em><br>
<em>&gt; &gt; such, and see what the possible outcomes are, and
</em><br>
<em>&gt; &gt; their porbabilities, for each event. They
</em><br>
<em>&gt; certainly
</em><br>
<em>&gt; &gt; aren't identical!
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; &quot;But an FAI with a 'Personal Morality' component,
</em><br>
<em>&gt; &gt; would
</em><br>
<em>&gt; &gt; not neccesserily fulfil all of these requests. 
</em><br>
<em>&gt; For
</em><br>
<em>&gt; &gt; instance an FAI that had a personal morality
</em><br>
<em>&gt; &gt; component
</em><br>
<em>&gt; &gt; 'Coke is good, Pepsi is evil' would refuse to
</em><br>
<em>&gt; fulfil
</em><br>
<em>&gt; &gt; a
</em><br>
<em>&gt; &gt; request for Pepsi.&quot;
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; That is a bad thing!!! AIs shouldn't arbitrarily
</em><br>
<em>&gt; &gt; decide to refuse Pepsi- eventually the AI is then
</em><br>
<em>&gt; &gt; going to arbitrarily refuse survival. And yes, it
</em><br>
<em>&gt; is
</em><br>
<em>&gt; &gt; arbitrary, because if it isn't arbitrary than the
</em><br>
<em>&gt; &gt; Yudkowskian FAI would have it in the first place!
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; &quot;The 'Personal morality' component
</em><br>
<em>&gt; &gt; would tell an FAI what it SHOULD do, the
</em><br>
<em>&gt; 'Universal
</em><br>
<em>&gt; &gt; morality' componanet is concerned with what an FAI
</em><br>
<em>&gt; &gt; SHOULDN'T do.  A Yudkowskian FAI would be unable
</em><br>
<em>&gt; to
</em><br>
<em>&gt; &gt; draw this distinction, since it would have no
</em><br>
<em>&gt; &gt; 'Personal Morality'  (Remember a Yudkowskian FAI
</em><br>
<em>&gt; is
</em><br>
<em>&gt; &gt; entirely non-observer centerd, and so it could
</em><br>
<em>&gt; only
</em><br>
<em>&gt; &gt; have Universal Morality).&quot;
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; Quite wrong. Even Eurisko could tell the
</em><br>
<em>&gt; difference
</em><br>
<em>&gt; &gt; between &quot;Don't do A&quot; and &quot;Do A&quot;. And check your
</em><br>
<em>&gt; &gt; spelling.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Sorry.  What I meant was that the FAI can't
</em><br>
<em>&gt; distinguigh between 'Acts and Omissions' (read up on
</em><br>
<em>&gt; moral philosophy for an explanation).  
</em><br>
<p>The FAI can't distinguish between heuristic A that
<br>
says 'do B' and heuristic C that says 'don't do B'?
<br>
<p><em>&gt; &gt; 
</em><br>
<em>&gt; &gt; &quot;You could say that a
</em><br>
<em>&gt; &gt; Yudkowskian FAI just views everything that doesn't
</em><br>
<em>&gt; &gt; hurt others as equal, where as an FAI with an
</em><br>
<em>&gt; extra
</em><br>
<em>&gt; &gt; oberver centered component would have some extra
</em><br>
<em>&gt; &gt; personal principles.&quot;
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; 1. No one ever said that. Straw man.
</em><br>
<em>&gt; &gt; 2. Arbitrary principles thrown in with morality
</em><br>
<em>&gt; are
</em><br>
<em>&gt; &gt; bad things.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; &quot;Yeah, yeah, true, but an FAI with a 'Personal
</em><br>
<em>&gt; &gt; Morality' would have some additional goals on top
</em><br>
<em>&gt; of
</em><br>
<em>&gt; &gt; this.  A Yudkowskian FAI does of course have the
</em><br>
<em>&gt; &gt; goals
</em><br>
<em>&gt; &gt; 'aim to do things that help with the fulfilment of
</em><br>
<em>&gt; &gt; sentient requests'.  But that's all.  An FAI with
</em><br>
<em>&gt; an
</em><br>
<em>&gt; &gt; additional 'Personal Morality' component, would
</em><br>
<em>&gt; also
</em><br>
<em>&gt; &gt; have the Yudkowskian goals, but it would have some
</em><br>
<em>&gt; &gt; additional goals.  For instance the additinal
</em><br>
<em>&gt; &gt; personal
</em><br>
<em>&gt; &gt; morality 'Coke is good, Pepsi is evil' would lead
</em><br>
<em>&gt; &gt; the
</em><br>
<em>&gt; &gt; FAI to personally support 'Coke' goals (provided
</em><br>
<em>&gt; &gt; such
</em><br>
<em>&gt; &gt; goals did not contradict the Yudkowskian goals).&quot;
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; It isn't a good thing to arbitarily stick
</em><br>
<em>&gt; moralities
</em><br>
<em>&gt; &gt; and goals into goal systems without justification.
</em><br>
<em>&gt; &gt; If
</em><br>
<em>&gt; &gt; there was justification, then it would be present
</em><br>
<em>&gt; in
</em><br>
<em>&gt; &gt; a
</em><br>
<em>&gt; &gt; Yudkowskian FAI. And 'Coke' goals would contradict
</em><br>
<em>&gt; &gt; Yudkowskian goals every time someone asked for a
</em><br>
<em>&gt; &gt; Pepsi.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But ARE all 'arbitary' goals really a bad thing? 
</em><br>
<em>&gt; Aren't such extra goals what makes life interesting?
</em><br>
<p>It may make life 'interesting' (even this isn't
<br>
proven), but it's sure not something you would want in
<br>
the original AI that starts the Singularity. 
<br>
&quot;First come the Guardians or the Transition Guide,
<br>
then come the friends and drinking companions&quot;-
<br>
Eliezer, CFAI
<br>
<p><em>&gt; Do you prefer rock music or heavy metal?  Do you
</em><br>
<em>&gt; like
</em><br>
<em>&gt; Chinese food or Sea food best?  What do you prefer: 
</em><br>
<em>&gt; Modern art or Classical?  You could say that these
</em><br>
<em>&gt; preferences are probably 'arbitrary', but they're
</em><br>
<em>&gt; actually what marks us out as individuals and makes
</em><br>
<em>&gt; us
</em><br>
<em>&gt; unique.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If all of us simply pursued 'true' (normative,
</em><br>
<em>&gt; Universal) morality, then all of us would be
</em><br>
<em>&gt; identical
</em><br>
<em>&gt; (because all sentients by definition converge on the
</em><br>
<em>&gt; same normative morality).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Now in the example of an FAI with the additional
</em><br>
<em>&gt; 
</em><br>
<em>&gt; === message truncated ===
</em><br>
<p><p>__________________________________
<br>
Do you Yahoo!?
<br>
Get better spam protection with Yahoo! Mail.
<br>
<a href="http://antispam.yahoo.com/tools">http://antispam.yahoo.com/tools</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8089.html">Philip Sutton: "Re: UNU report 2003 identified human-machine intelligence as key issue"</a>
<li><strong>Previous message:</strong> <a href="8087.html">Tommy McCabe: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>In reply to:</strong> <a href="8081.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8095.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8095.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8088">[ date ]</a>
<a href="index.html#8088">[ thread ]</a>
<a href="subject.html#8088">[ subject ]</a>
<a href="author.html#8088">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
