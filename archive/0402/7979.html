<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Positive Transcension 2</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Positive Transcension 2">
<meta name="Date" content="2004-02-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Positive Transcension 2</h1>
<!-- received="Fri Feb 20 09:01:20 2004" -->
<!-- isoreceived="20040220160120" -->
<!-- sent="Fri, 20 Feb 2004 11:08:23 -0500" -->
<!-- isosent="20040220160823" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Positive Transcension 2" -->
<!-- id="BMECIIDGKPGNFPJLIDNPIELNCNAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="BMECIIDGKPGNFPJLIDNPAEKMCNAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Positive%20Transcension%202"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Fri Feb 20 2004 - 09:08:23 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7980.html">Christian Szegedy: "Re: Positive Transcension 2"</a>
<li><strong>Previous message:</strong> <a href="7978.html">Philip Sutton: "RE: Positive Transcension 2"</a>
<li><strong>In reply to:</strong> <a href="7976.html">Ben Goertzel: "RE: Positive Transcension 2"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7980.html">Christian Szegedy: "Re: Positive Transcension 2"</a>
<li><strong>Reply:</strong> <a href="7980.html">Christian Szegedy: "Re: Positive Transcension 2"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7979">[ date ]</a>
<a href="index.html#7979">[ thread ]</a>
<a href="subject.html#7979">[ subject ]</a>
<a href="author.html#7979">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Philip,
<br>
<p>The &quot;democracy&quot; question is an interesting one...
<br>
<p>Let's suppose that some person creates a technology capable of launching a
<br>
Singularity, and needs to make decisions about how to use it or whether to
<br>
use it, etc. -- based on a careful consideration of various sorts of costs
<br>
and benefits, alternative risks to humanity, etc. etc.
<br>
<p>I note that I am very far from being in this position, and I assume everyone
<br>
else in the world is also very far from being in this position.  If a lot of
<br>
my guesses about AI design and teaching turn out right I may be in this
<br>
position one day... but for now this is a totally speculative conversation.
<br>
<p>What decision process should they use?
<br>
<p>Clearly some kind of process inbetween the extremes of:
<br>
<p>a) the technologist and their cronies makes the decisions themselves
<br>
<p>b) a majority vote of the human race is taken
<br>
<p>would be appropriate.  But I'm not sure in practice what kind of
<br>
&quot;intermediate process&quot; would work best.
<br>
<p>Perhaps the appropriate process will become more apparent once the issue
<br>
becomes more relevant and less speculative...
<br>
<p><p>-- Ben G
<br>
<p><p><p><p>&nbsp;&nbsp;-----Original Message-----
<br>
&nbsp;&nbsp;From: Ben Goertzel [mailto:<a href="mailto:ben@goertzel.org?Subject=RE:%20Positive%20Transcension%202">ben@goertzel.org</a>]
<br>
&nbsp;&nbsp;Sent: Friday, February 20, 2004 12:32 AM
<br>
&nbsp;&nbsp;To: <a href="mailto:sl4@sl4.org?Subject=RE:%20Positive%20Transcension%202">sl4@sl4.org</a>
<br>
&nbsp;&nbsp;Subject: RE: Positive Transcension 2
<br>
<p><p>&nbsp;&nbsp;Philip,
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;***
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;So let's start with how some humans might feel about some other humans
<br>
creating a 'thing' which could wipe out humans without their agreement.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;Well, I don't think that most humans understand the predicament of the
<br>
human race very well.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;If people don't understand the existential risks posed by other
<br>
technologies, how are they going to be able to participate in a serious
<br>
cost-benefit analysis regarding the creation of AGI's of various types?
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;I'm a fan of the democratic process, and yet, I'm also a bit skeptical
<br>
of the ability of this process to make the right decisions in this kind of
<br>
area....
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;So much of the world population is religious ... of course they are
<br>
going to feel TOTALLY DIFFERENTLY about the various existential risks and
<br>
the benefits of transhumanity, than nonreligious folks of transhumanist
<br>
bent...
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;Do you really think that we should proceed with these technologies via
<br>
some kind of global majority vote?  Bear in mind that around 80% of the
<br>
world population believes in reincarnation...
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;***
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ben you said: &quot;And this may or may not lead to the demise of
<br>
humanity - which may or may not be a terrible thing.&quot;  At best loose
<br>
language like this means one thing to most people - somebody else is being
<br>
cavalier about their future - at worst they are likely to perceive an active
<br>
threat to their existence.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;You can call it cavalier -- I call it honest and open-minded.  I guess
<br>
if you take that quote out of context it can sound scary, but why do you
<br>
need to take it out of context?
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;***
<br>
&nbsp;&nbsp;&nbsp;&nbsp;Frankly I doubt if anyone will care if humanity evolves or transcends to
<br>
a higher state of being so long as it's voluntary.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;This is very naive -- very many people mind voluntary transhumanist
<br>
actions even if they're milder than transcension.   Psychedelic drugs are
<br>
illegal, as are smart drugs, homebrew neuromodifications, etc. etc. etc.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;Experimentation with stems cells is barely legal, for Chrissake !!!!
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;Again, you seem to overestimate the rationality and wisdom of the &quot;mass
<br>
mind&quot;
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;To withhold concern for other humans lives because theoretically some
<br>
AGI might form the view that our mass/energy could be deployed more
<br>
beautifully/usefully seems simply silly.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;I do not advocate witholding concern for other humans.  I'm sorry if
<br>
what I wrote was misinterpreted that way.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* **
<br>
&nbsp;&nbsp;&nbsp;&nbsp;I think the first step in creating safe AGI is for the would-be creators
<br>
of AGI to themselves make an ethical commitment to the protection of
<br>
humans - not because humans are the peak of creation or all that stunningly
<br>
special from the perspective of the universe as a whole but simply because
<br>
they exist and they deserve respect - especially from their fellow humans.
<br>
If AGI developers cannot give their fellow humans that commitment or that
<br>
level of respect, then I think they demonstrate they are not safe parents
<br>
for growing AGIs!
<br>
&nbsp;&nbsp;&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;In other words, you are stating that only people who agree with your
<br>
personal ethics should be allowed to create AGI's -- your personal ethics
<br>
being that the preservation of humans is paramount.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;I think that the preservation of humans is very, very important -- but
<br>
I'm not willing to assert that it's absolutely paramount just to sound
<br>
&quot;politically correct.&quot;
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;***
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I was actually rather disturbed by your statement towards the end of
<br>
your paper where you said: &quot;In spite of my own affection for Voluntary
<br>
Joyous Growth, however, I have strong inclinations toward both the Joyous
<br>
Growth Guided Voluntarism and pure Joyous Growth variants as well.&quot;  My
<br>
reading of this is that you would be prepared to inflict Joyous Growth
<br>
future on people whether they wanted it or not and even if this resulted in
<br>
the involuntary elimination of people or other sentients that somehow were
<br>
seen by the AGI or AGIs pursuing Joyous Growth as being an impediment in the
<br>
way of the achievement of joyous growth.  If I've interpreted what you are
<br>
saying correctly that's pretty scary stuff!
<br>
&nbsp;&nbsp;&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;It seems you are oddly misinterpreting my statement here.  I said that
<br>
my primary affection was for VOLUNTARY Joyous Growth, which is an ethical
<br>
principle that places *free choice* as a primary value.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;Free choice means not forcing humans to transcend, and not forcing
<br>
humans not to transcend.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;What you are advocating is a Joyous Growth Biased Voluntarism, in which
<br>
AS AN ABSOLUTE RULE no one is to be forced to transcend (or annihilated, or
<br>
forced to do anything).   I think this is more problematic, but is also
<br>
worthy of consideration.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* **
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;I think the next step is to consider what values we would like AGIs to
<br>
hold in order for them to be sound citizens in a community of sentients. I
<br>
think the minimum that is needed is for them to have a tolerant, respectful,
<br>
compassionate, live-and-let-live attitude.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;It seems to me that you're just rephrasing what I call &quot;Voluntary
<br>
Joyousity&quot; here, in language that you like better for some reason.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;compasionate = valuing Joy of others
<br>
&nbsp;&nbsp;&nbsp;&nbsp;tolerant, live-and-let-live = valuing others' ability to choose
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;All you've left out is the &quot;growth&quot; part.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;If you prefer the verbiage of &quot;compassionate and tolerant&quot; as opposed to
<br>
&quot;joy and choice&quot;, that's fine with me....  None of these English words
<br>
really captures what needs to be said exactly, anyway...
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;***
<br>
&nbsp;&nbsp;&nbsp;&nbsp;I think AGIs that had a tolerant, respectful, compassionate,
<br>
live-and-let- live ethic would not intrude excessively on human society.
<br>
They might, for example, try to discourage female circumcision or even go so
<br>
far as stopping capital punishment in human societies (I can't see that
<br>
these actions would conform to the ethics that the AGIs were given [under my
<br>
scenario] their human creators/carers).  As far as I can see I don't think
<br>
that AGIs need to have ported into them a sort of general digest of
<br>
human-ness or even an idiosyncratic (renormalised) essence of general
<br>
humane-ness.  I think we should be able to be more transparent than that and
<br>
to identify the key ethical drivers that lead to tolerant, respectful,
<br>
compassionate, live-and-let-live behaviour.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;What's odd is that you seem to agree with me almost completely --- you
<br>
agree with me that Eliezer's idea of embodying humane-ness in AI's is
<br>
overcomplicated, and you agree that it's good to supply AGI's with general
<br>
ethical principles.  The only difference is that you choose different words
<br>
to describe what I call Joy and Choice, and you appear not to value what I
<br>
call Growth enough to want to make it a basic value.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* **
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;I think these notions are sufficiently abstract to be able to pass your
<br>
test of being likely to &quot;survive successive self-modification&quot;.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;Yes -- becuase they're the SAME as the notions I proposed, merely worded
<br>
in a way that evokes more pleasant associations for you...
<br>
&nbsp;&nbsp;&nbsp;&nbsp;* **
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;In your paper you suggest that we need AGIs to save humanity from our
<br>
destructive urges (applied via advanced technology).  If having AGIs around
<br>
could increase the risk of humanity being wiped out to achieve a more
<br>
beautiful deployment of mass/energy then it might be a good idea to go back
<br>
and check to see just exactly how dangerous the other feared technologies
<br>
are. While nanotech and genetic engineering could produce some pretty
<br>
virulent and deadly entities I'm not sure that they are likely to be much
<br>
more destructive than bubonic plague, eboloa virus, small pox have been in
<br>
their time etc.  There are a lot of people around so that even if these
<br>
threats killed millions? billions? they are unlikely to wipe out even most
<br>
people.  So should we seek help from this scale of threat by creating
<br>
something that might arbitarily decide to wipe out the lot of us on a whim?
<br>
&nbsp;&nbsp;&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;I'm afraid you are being woefully naive on this particular topic.  The
<br>
existential risks of MNT and bioweapons are very very real -- not today, but
<br>
within centuries for sure, and decades quite probably.  But I don't have
<br>
time to trot out the arguments for this point tonight.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7980.html">Christian Szegedy: "Re: Positive Transcension 2"</a>
<li><strong>Previous message:</strong> <a href="7978.html">Philip Sutton: "RE: Positive Transcension 2"</a>
<li><strong>In reply to:</strong> <a href="7976.html">Ben Goertzel: "RE: Positive Transcension 2"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7980.html">Christian Szegedy: "Re: Positive Transcension 2"</a>
<li><strong>Reply:</strong> <a href="7980.html">Christian Szegedy: "Re: Positive Transcension 2"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7979">[ date ]</a>
<a href="index.html#7979">[ thread ]</a>
<a href="subject.html#7979">[ subject ]</a>
<a href="author.html#7979">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
