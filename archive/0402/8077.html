<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: All sentient have to be observer-centered!  My theory of FAI morality</title>
<meta name="Author" content="Tommy McCabe (rocketjet314@yahoo.com)">
<meta name="Subject" content="Re: All sentient have to be observer-centered!  My theory of FAI morality">
<meta name="Date" content="2004-02-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: All sentient have to be observer-centered!  My theory of FAI morality</h1>
<!-- received="Fri Feb 27 04:48:26 2004" -->
<!-- isoreceived="20040227114826" -->
<!-- sent="Fri, 27 Feb 2004 03:48:19 -0800 (PST)" -->
<!-- isosent="20040227114819" -->
<!-- name="Tommy McCabe" -->
<!-- email="rocketjet314@yahoo.com" -->
<!-- subject="Re: All sentient have to be observer-centered!  My theory of FAI morality" -->
<!-- id="20040227114819.96329.qmail@web11701.mail.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20040227044913.6701.qmail@web20212.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tommy McCabe (<a href="mailto:rocketjet314@yahoo.com?Subject=Re:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality"><em>rocketjet314@yahoo.com</em></a>)<br>
<strong>Date:</strong> Fri Feb 27 2004 - 04:48:19 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8078.html">Rafal Smigrodzki: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Previous message:</strong> <a href="8076.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>In reply to:</strong> <a href="8072.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8081.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8081.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8077">[ date ]</a>
<a href="index.html#8077">[ thread ]</a>
<a href="subject.html#8077">[ subject ]</a>
<a href="author.html#8077">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
You say that moralities 'consistent' with each other
<br>
don't have to be identical. They do. Morality isn't
<br>
mathematics. In order for them to be consistent, they
<br>
have to give the same result in every situation, in
<br>
other words, they must be identical. 'I like X' isn't
<br>
really a consistent morality with 'Do not kill', since
<br>
given the former, one would kill to get X. I don't
<br>
like the idea of an AI acting like a human, ie, of
<br>
having heuristics of 'Coke is better tha Pepsi' for no
<br>
good reason. Of course, if their is a good reason, a
<br>
Yudkowskian FAI would have that anyway. You may take
<br>
the 'personal component of morality is necessary'
<br>
thing as an axiom, but I don't and I need to see some
<br>
proof.
<br>
<p>&quot;Well yeah true, a Yudkowskian FAI would of course
<br>
refuse requests to hurt other people.  But it would
<br>
aim to fulfil ALL requests consistent with volition. 
<br>
(All requests which don't involve violating other
<br>
peoples right).&quot;
<br>
<p>And that's a bad thing? You really don't want an AI
<br>
deciding not to fulfill Pepsi requests because it
<br>
thinks Coke is better for no good reason- that leads
<br>
to an AI not wanting to fulfill Singularity requests
<br>
because suffering is better.
<br>
<p>&quot;For instance, 'I want to go ice skating', 'I want a
<br>
Pepsi', 'I want some mountain climbing qquipment' and
<br>
so on and so on.  A Yudkowskian FAI can't draw any
<br>
distinctions between these, and would see all of them
<br>
as equally 'good'.&quot;
<br>
<p>It wouldn't- at all. A Yudkowskian FAI, especially a
<br>
transhuman one, could easily apply Bayes' Theorem and
<br>
such, and see what the possible outcomes are, and
<br>
their porbabilities, for each event. They certainly
<br>
aren't identical!
<br>
<p>&quot;But an FAI with a 'Personal Morality' component,
<br>
would
<br>
not neccesserily fulfil all of these requests.  For
<br>
instance an FAI that had a personal morality component
<br>
'Coke is good, Pepsi is evil' would refuse to fulfil a
<br>
request for Pepsi.&quot;
<br>
<p>That is a bad thing!!! AIs shouldn't arbitrarily
<br>
decide to refuse Pepsi- eventually the AI is then
<br>
going to arbitrarily refuse survival. And yes, it is
<br>
arbitrary, because if it isn't arbitrary than the
<br>
Yudkowskian FAI would have it in the first place!
<br>
<p>&quot;The 'Personal morality' component
<br>
would tell an FAI what it SHOULD do, the 'Universal
<br>
morality' componanet is concerned with what an FAI
<br>
SHOULDN'T do.  A Yudkowskian FAI would be unable to
<br>
draw this distinction, since it would have no
<br>
'Personal Morality'  (Remember a Yudkowskian FAI is
<br>
entirely non-observer centerd, and so it could only
<br>
have Universal Morality).&quot;
<br>
<p>Quite wrong. Even Eurisko could tell the difference
<br>
between &quot;Don't do A&quot; and &quot;Do A&quot;. And check your
<br>
spelling.
<br>
<p>&quot;You could say that a
<br>
Yudkowskian FAI just views everything that doesn't
<br>
hurt others as equal, where as an FAI with an extra
<br>
oberver centered component would have some extra
<br>
personal principles.&quot;
<br>
<p>1. No one ever said that. Straw man.
<br>
2. Arbitrary principles thrown in with morality are
<br>
bad things.
<br>
<p>&quot;Yeah, yeah, true, but an FAI with a 'Personal
<br>
Morality' would have some additional goals on top of
<br>
this.  A Yudkowskian FAI does of course have the goals
<br>
'aim to do things that help with the fulfilment of
<br>
sentient requests'.  But that's all.  An FAI with an
<br>
additional 'Personal Morality' component, would also
<br>
have the Yudkowskian goals, but it would have some
<br>
additional goals.  For instance the additinal personal
<br>
morality 'Coke is good, Pepsi is evil' would lead the
<br>
FAI to personally support 'Coke' goals (provided such
<br>
goals did not contradict the Yudkowskian goals).&quot;
<br>
<p>It isn't a good thing to arbitarily stick moralities
<br>
and goals into goal systems without justification. If
<br>
there was justification, then it would be present in a
<br>
Yudkowskian FAI. And 'Coke' goals would contradict
<br>
Yudkowskian goals every time someone asked for a
<br>
Pepsi.
<br>
<p>&quot;I've given the general solution to the problem of FAI
<br>
morality.  We don't know that 'Personal Morality' set
<br>
to unity would be stable.  Therefore we have to
<br>
consider the case where FAI's have to have a
<br>
non-trival 'Personal Morality' component.&quot;
<br>
<p>Non sequitur. That's like saying &quot;We don't know if car
<br>
A will be stable with 100% certainty, so we have to
<br>
take a look at car B that has large heaps of trash on
<br>
it for no good reason&quot;
<br>
<p><p><p>__________________________________
<br>
Do you Yahoo!?
<br>
Get better spam protection with Yahoo! Mail.
<br>
<a href="http://antispam.yahoo.com/tools">http://antispam.yahoo.com/tools</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8078.html">Rafal Smigrodzki: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Previous message:</strong> <a href="8076.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>In reply to:</strong> <a href="8072.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8081.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<li><strong>Reply:</strong> <a href="8081.html">Marc Geddes: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8077">[ date ]</a>
<a href="index.html#8077">[ thread ]</a>
<a href="subject.html#8077">[ subject ]</a>
<a href="author.html#8077">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
