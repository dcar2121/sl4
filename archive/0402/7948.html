<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Ethics was In defense of physics</title>
<meta name="Author" content="Keith Henson (hkhenson@rogers.com)">
<meta name="Subject" content="Re: Ethics was In defense of physics">
<meta name="Date" content="2004-02-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Ethics was In defense of physics</h1>
<!-- received="Sun Feb 15 19:57:37 2004" -->
<!-- isoreceived="20040216025737" -->
<!-- sent="Sun, 15 Feb 2004 22:01:00 -0500" -->
<!-- isosent="20040216030100" -->
<!-- name="Keith Henson" -->
<!-- email="hkhenson@rogers.com" -->
<!-- subject="Re: Ethics was In defense of physics" -->
<!-- id="5.1.0.14.0.20040215174123.02f57ec0@pop.bloor.is.net.cable.rogers.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="402F9F3A.20502@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Keith Henson (<a href="mailto:hkhenson@rogers.com?Subject=Re:%20Ethics%20was%20In%20defense%20of%20physics"><em>hkhenson@rogers.com</em></a>)<br>
<strong>Date:</strong> Sun Feb 15 2004 - 20:01:00 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7949.html">Metaqualia: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Previous message:</strong> <a href="7947.html">Eliezer S. Yudkowsky: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>In reply to:</strong> <a href="7932.html">Eliezer S. Yudkowsky: "Re: Ethics was In defense of physics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7950.html">Eliezer S. Yudkowsky: "Re: Ethics was In defense of physics"</a>
<li><strong>Reply:</strong> <a href="7950.html">Eliezer S. Yudkowsky: "Re: Ethics was In defense of physics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7948">[ date ]</a>
<a href="index.html#7948">[ thread ]</a>
<a href="subject.html#7948">[ subject ]</a>
<a href="author.html#7948">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 11:32 AM 15/02/04 -0500, Eliezer S. Yudkowsky wrote:
<br>
<em>&gt;Keith Henson wrote:
</em><br>
<em>&gt;&gt;It seems to me that the core would have to be absolutely impervious to 
</em><br>
<em>&gt;&gt;outside influences--which is in conflict with intelligence--to the extent 
</em><br>
<em>&gt;&gt;that intelligence has to do with learning. Otherwise units at the ends of 
</em><br>
<em>&gt;&gt;communication delays would diverge.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Okay, as a proof of principle, let's take a generic optimization process 
</em><br>
<em>&gt;(i.e., a paperclip SI) and decompartmentalize learning into Bayesian 
</em><br>
<em>&gt;learning of world-models and the expected utility equation with a constant 
</em><br>
<em>&gt;utility function.  See:
</em><br>
<em>&gt;
</em><br>
<em>&gt;<a href="http://intelligence.org/friendly/features.html#causal_bayesian">http://intelligence.org/friendly/features.html#causal_bayesian</a>
</em><br>
<em>&gt;<a href="http://intelligence.org/CFAI/design/clean.html#reinforcement">http://intelligence.org/CFAI/design/clean.html#reinforcement</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt;(CFAI doesn't call anything by its proper name - &quot;cleanly causal&quot; should 
</em><br>
<em>&gt;be translated as &quot;expected utility&quot;, &quot;Bayesian Probability Theorem&quot; is 
</em><br>
<em>&gt;Bayes' Theorem.)
</em><br>
<em>&gt;
</em><br>
<em>&gt;The point is that you can perform all learning necessary to the task of 
</em><br>
<em>&gt;transforming everything in sight into paperclips, and you won't have 
</em><br>
<em>&gt;conflicts with distant parts of yourself that also want to transform 
</em><br>
<em>&gt;everything into paperclips - the target is constant, only the aim gets updated.
</em><br>
<p>And the weapons, and the gravity field, and if it starts thinking about 
<br>
what it is doing and what paper clips are used for, it might switch from 
<br>
metal to plastic or branch out into report covers (which do a better job) 
<br>
and then reconsider the whole business of sticking papers together and 
<br>
start making magnetic media.
<br>
<p>But after reading the articles I do see what you are getting at and why it 
<br>
is really important.  It also might really tough to do it right.  In the 
<br>
long run, I agree with your point that we will have to depend on the &quot;good 
<br>
will,&quot; friendliness, toward us so it makes a lot of sense to do it right in 
<br>
the first place.  (0r die trying.)
<br>
<p>This is one place where the analogies of what drives people to be friendly 
<br>
might be of interest.
<br>
<p><em>&gt;Programming in explicit cooperation with distant self-parts, or 
</em><br>
<em>&gt;maintaining integrity of a philosophically growing Friendly AI, are more 
</em><br>
<em>&gt;complex subjects.  The former looks doable and theoretically 
</em><br>
<em>&gt;straightforward; the latter looks doable and theoretically complex.
</em><br>
<p>The latter may be pointless if the AIs are far enough apart.  Like several 
<br>
billion light years.
<br>
<p><em>&gt;&gt;I suppose every AI could be broadcasting its total information stream 
</em><br>
<em>&gt;&gt;into memory and receiving the memory dumps from every other AI.  It would 
</em><br>
<em>&gt;&gt;have to treat the experience (memory) of other AIs with equal weight to 
</em><br>
<em>&gt;&gt;its own.  That would keep at least close ones in sync, but if there are 
</em><br>
<em>&gt;&gt;growing numbers of these things, the storage problem will get out of hand 
</em><br>
<em>&gt;&gt;no matter what media is being used.  (In fact, it might make the case for 
</em><br>
<em>&gt;&gt;very few AIs.  Even on per star would get out of hand.)
</em><br>
<em>&gt;
</em><br>
<em>&gt;Hm... I infer that you're thinking of some algorithm, such as 
</em><br>
<em>&gt;reinforcement on neural nets, that doesn't cleanly separate model 
</em><br>
<em>&gt;information and utility computation.
</em><br>
<p>Even if you cleanly split out utility computation, widely separated AIs are 
<br>
going to be working off rather different data bases.
<br>
<p>Take shifting a galaxy to avoid the worst consequences of collisions 
<br>
(whatever they are).  That's an obvious project for a friendly and very 
<br>
patient AI.  Say galaxy A needs to go right or left direction and galaxy B 
<br>
needs to go left or right depending on what A does to modify the 
<br>
collision.  If the AIs figure this out when they are separated by several 
<br>
million light years, they are going to have a heck of a time deciding which 
<br>
way each should cause their local galaxy to dodge.  If they both decide the 
<br>
same way, you are going to get one of those sidewalk episodes of people 
<br>
dodging into each other's path--with really lamentable results for anybody 
<br>
nearby if the black holes merge.
<br>
<p><em>&gt;&gt;The problems this creates are bad enough that far apart AI cores would be 
</em><br>
<em>&gt;&gt;forced to consider themselves as different &quot;individuals&quot; just by weight 
</em><br>
<em>&gt;&gt;of different (unsync'ed) post creation experiences.  I think this is true 
</em><br>
<em>&gt;&gt;even if closer ones engaged in total mind melding.
</em><br>
<em>&gt;
</em><br>
<em>&gt;In human beings all the axes of &quot;individual&quot; versus &quot;group&quot; are conflated: 
</em><br>
<em>&gt;many memories versus one memory, many clusters versus one processor, 
</em><br>
<em>&gt;different goals versus same goals, different plans versus same plans, and 
</em><br>
<em>&gt;so on.
</em><br>
<p>To the extent humans share goals it is because humans share genes. Males in 
<br>
particular are optimize to act and to take risks for others on the basis of 
<br>
the average relationship in a tribe a few hundred thousand years ago 
<br>
(averaging to something like second cousin).
<br>
<p><em>&gt;Different memories stored in local nodes of an optimization process 
</em><br>
<em>&gt;sprawled over long interprocessor communication delays does not equate to 
</em><br>
<em>&gt;conflict of interest.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;With FTL there doesn't seem to be an obvious limit.  Without . . . 
</em><br>
<em>&gt;&gt;eventually your brain undergoes a gravitational singularity.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Only if you want to keep each computing element within small-N clock ticks 
</em><br>
<em>&gt;of every other computing element.  This is the case with the human brain, 
</em><br>
<em>&gt;for which Anders Sandberg calculated S = (single instruction time / 
</em><br>
<em>&gt;communication delay) ~ 1.  See &quot;The Physics of Information Processing 
</em><br>
<em>&gt;Superobjects&quot;.
</em><br>
<p>That's an interesting number.  I wonder what William Calvin would say about 
<br>
that?  (He knows one heck of a lot about problems of this class.)
<br>
<p><em>&gt;Actually, with FTL or without FTL, if you try to keep S ~ 1 or S &lt; bound, 
</em><br>
<em>&gt;you run into problems with your brain collapsing gravitationally.  Without 
</em><br>
<em>&gt;FTL, because of the lightspeed delay; with FTL, because the necessary 
</em><br>
<em>&gt;density of FTL relays to keep all processors within N hops also grows, 
</em><br>
<em>&gt;albeit logarithmically (I guess).  In either case, you can either slow 
</em><br>
<em>&gt;down your processors or accept a lower S.
</em><br>
<p>I have a real problem of part of my brain being subjective months our of 
<br>
sync.  When you have to communicate, even with your twin brother, via 
<br>
sailing ship you might have the same interest and goals but you darn sure 
<br>
are going to be different individuals.
<br>
<p>Keith Henson
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7949.html">Metaqualia: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Previous message:</strong> <a href="7947.html">Eliezer S. Yudkowsky: "Re: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>In reply to:</strong> <a href="7932.html">Eliezer S. Yudkowsky: "Re: Ethics was In defense of physics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7950.html">Eliezer S. Yudkowsky: "Re: Ethics was In defense of physics"</a>
<li><strong>Reply:</strong> <a href="7950.html">Eliezer S. Yudkowsky: "Re: Ethics was In defense of physics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7948">[ date ]</a>
<a href="index.html#7948">[ thread ]</a>
<a href="subject.html#7948">[ subject ]</a>
<a href="author.html#7948">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
