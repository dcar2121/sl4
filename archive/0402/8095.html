<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: All sentient have to be observer-centered!  My theory of FAI morality</title>
<meta name="Author" content="Marc Geddes (marc_geddes@yahoo.co.nz)">
<meta name="Subject" content="Re: All sentient have to be observer-centered!  My theory of FAI morality">
<meta name="Date" content="2004-02-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: All sentient have to be observer-centered!  My theory of FAI morality</h1>
<!-- received="Sun Feb 29 22:15:56 2004" -->
<!-- isoreceived="20040301051556" -->
<!-- sent="Mon, 1 Mar 2004 18:15:55 +1300 (NZDT)" -->
<!-- isosent="20040301051555" -->
<!-- name="Marc Geddes" -->
<!-- email="marc_geddes@yahoo.co.nz" -->
<!-- subject="Re: All sentient have to be observer-centered!  My theory of FAI morality" -->
<!-- id="20040301051555.10438.qmail@web20203.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20040229131607.928.qmail@web11705.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marc Geddes (<a href="mailto:marc_geddes@yahoo.co.nz?Subject=Re:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality"><em>marc_geddes@yahoo.co.nz</em></a>)<br>
<strong>Date:</strong> Sun Feb 29 2004 - 22:15:55 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8096.html">Marc Geddes: "RE: The Fundamental Theorem of Morality"</a>
<li><strong>Previous message:</strong> <a href="8094.html">mike99: "RE: [SL4] AI --&gt; Jobless Economy"</a>
<li><strong>In reply to:</strong> <a href="8088.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8069.html">Rafal Smigrodzki: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8095">[ date ]</a>
<a href="index.html#8095">[ thread ]</a>
<a href="subject.html#8095">[ subject ]</a>
<a href="author.html#8095">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;--- Tommy McCabe &lt;<a href="mailto:rocketjet314@yahoo.com?Subject=Re:%20All%20sentient%20have%20to%20be%20observer-centered!%20%20My%20theory%20of%20FAI%20morality">rocketjet314@yahoo.com</a>&gt; wrote: &gt; 
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Here's an idea: Perhaps (although I have no idea how
</em><br>
<em>&gt; you would relate them) you could have a supergoal of
</em><br>
<em>&gt; what you call Universal Morality, and then, if
</em><br>
<em>&gt; supporting Coke over Pepsi somehow supported
</em><br>
<em>&gt; Universal
</em><br>
<em>&gt; Morality, you could have it as a subgoal. That way,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1. You support Coke over Pepsi
</em><br>
<em>&gt; 2. Your support is justified
</em><br>
<em>&gt; 3. If, at any time in the future, supporting Coke
</em><br>
<em>&gt; contradicts Universal Morality, it can be easily
</em><br>
<em>&gt; dropped
</em><br>
<p><p>Well, that's a possibility but if supporting Coke over
<br>
Pepsi supported Universal Morality, it would, by
<br>
definition be a part of Universal Morality.  It
<br>
wouldn't be a part of Personal Morality, by my
<br>
definition.
<br>
<p>The point I'm making is that being moral DOESN'T
<br>
require that all goals support Universal Morality. 
<br>
All that is required for 'friendliness' (instead of
<br>
'Friendliness') is that all goals don't actually
<br>
contradict Universal Morality.  There are many
<br>
possible personal goals, which, whilst not actually a
<br>
part of Universal Morality, can still be pursued
<br>
without conflicting with Universal Morality.  I define
<br>
these 'Personal Moralities' as being congruent with
<br>
Universal Morality.
<br>
<p><p><p><em>&gt; 
</em><br>
<em>&gt; That's like saying, &quot;I don't know what the perfect
</em><br>
<em>&gt; car
</em><br>
<em>&gt; is, so that means I'm going to assume that having
</em><br>
<em>&gt; gum
</em><br>
<em>&gt; in the engine is necessary&quot;. Makes no sense at all.
</em><br>
<em>&gt; If
</em><br>
<em>&gt; you don't know how to build an engine, substituting
</em><br>
<em>&gt; sticks of gum isn't going to work.
</em><br>
<p>That's not a fair analogy.  See what I said below. 
<br>
Anything at all not which isn't a part of Universal
<br>
Morality falls under the 'Personal Morality' category
<br>
(by my definition).  Since the programmers won't get
<br>
everything exactly right to start with, my equation
<br>
accurately describes all human created FAI's.  (Since
<br>
all such AI's will have a 'Personal Morality'
<br>
componenet to start with).  I'm just pointing out that
<br>
in the real world no cars are perfect, then asking
<br>
what real world (non perfect) cars in general look
<br>
like.
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt; A 'guess' implies that Eliezer's ideas about
</em><br>
<em>&gt; morality
</em><br>
<em>&gt; aren't justified. I'm sure that Eli has good
</em><br>
<em>&gt; reasons,
</em><br>
<em>&gt; whatever they are, for thinking that Volitional
</em><br>
<em>&gt; Morality is the objective morality, or at least
</em><br>
<em>&gt; good.
</em><br>
<em>&gt; Anyway, AIs don't have moralities hardwired into
</em><br>
<em>&gt; them-
</em><br>
<em>&gt; they can correct programmer deficiencies later.
</em><br>
<em>&gt; 
</em><br>
<p><em>&gt; 
</em><br>
<em>&gt; I'll have to agree with you there. Programmers
</em><br>
<em>&gt; aren't
</em><br>
<em>&gt; perfect, and moral mistakes are bound to get into
</em><br>
<em>&gt; the
</em><br>
<em>&gt; AI. However, the AI can certainly correct these. And
</em><br>
<em>&gt; that's not even 'all FAIs'- it's just all FAIs built
</em><br>
<em>&gt; by humans.
</em><br>
<p>O.K.  But perhaps the FAI would come to value some of
<br>
it's 'non-perfect' goals for their own sake.  
<br>
<p>How would you as a human being like to have all the
<br>
goals which are not a part of Universal Morality
<br>
stripped out of you?  It wouldn't be very nice would
<br>
it?  Being moral doesn't require that all abitrary
<br>
goals are stripped out of you.  It just requires that
<br>
you get rid of SOME of your abitrary goals in specific
<br>
situations (the one's that conflict with Universal
<br>
Morality).
<br>
<p>&nbsp;
<br>
<em>&gt; 
</em><br>
<em>&gt; The FAI can't distinguish between heuristic A that
</em><br>
<em>&gt; says 'do B' and heuristic C that says 'don't do B'?
</em><br>
<p>Well if course it can distinguish.  But an FAI
<br>
operating off Volitional Morality can't MORALLY
<br>
distinguish between outsomes which are all equal with
<br>
respect to volition  (The FAI couldn't see a MORAL
<br>
difference between two different requests which didn't
<br>
hurt anyone and didn't affect the FAI's ability to
<br>
pursue its altruistic goals).
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt; It may make life 'interesting' (even this isn't
</em><br>
<em>&gt; proven), but it's sure not something you would want
</em><br>
<em>&gt; in
</em><br>
<em>&gt; the original AI that starts the Singularity. 
</em><br>
<em>&gt; &quot;First come the Guardians or the Transition Guide,
</em><br>
<em>&gt; then come the friends and drinking companions&quot;-
</em><br>
<em>&gt; Eliezer, CFAI
</em><br>
<em>&gt; 
</em><br>
<p>If it weren't for personal goals, then Universal
<br>
Morality would be pointless.  Think about it.  What
<br>
use would a desire to 'help others' be, if people
<br>
didn't have any personal goals?  If people didn't have
<br>
some arbitrary goals like 'I want a Pepsi', 'I want a
<br>
Coke' etc, then there would be no requests to fulfil
<br>
and no point to morality at all.
<br>
<p>Universal Morality actually REQUIRES personal goals.
<br>
<p>Here's a thought experiment which proves it:  Let's
<br>
imagine that the whole universe consisted solely of
<br>
Yudkowskian FAI's.  So each FAI would would be looking
<br>
to 'help others'.  But all the FAi's what to 'help
<br>
others'.  The result is an infinite regress.  Take a
<br>
look:
<br>
<p>FAI number 1:  I want to help others
<br>
FAI number 2:  I want to help others
<br>
FAI number 3:  I want to help others
<br>
FAI number 4:  I want to help others
<br>
<p>etc etc
<br>
<p>FAI number 1 wants to help FAI number 2.  But FAI
<br>
number 2 wants to help others as well.  So FAI number
<br>
1 wants to 'help others to help others to help others
<br>
to help others....'  danger!  inifinite regress.
<br>
<p>This proves that a totally altruistic Universal
<br>
morality is unstable.  It does not meet the conditions
<br>
specified for a Universal Morality (moral symmetry,
<br>
normative, conisistent, not subjective etc).  
<br>
<p>Therefore, an input from Personal Morality is required.
<br>
<p>=====
<br>
Please visit my web-site at:  <a href="http://www.prometheuscrack.com">http://www.prometheuscrack.com</a>
<br>
<p>Find local movie times and trailers on Yahoo! Movies.
<br>
<a href="http://au.movies.yahoo.com">http://au.movies.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8096.html">Marc Geddes: "RE: The Fundamental Theorem of Morality"</a>
<li><strong>Previous message:</strong> <a href="8094.html">mike99: "RE: [SL4] AI --&gt; Jobless Economy"</a>
<li><strong>In reply to:</strong> <a href="8088.html">Tommy McCabe: "Re: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8069.html">Rafal Smigrodzki: "RE: All sentient have to be observer-centered!  My theory of FAI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8095">[ date ]</a>
<a href="index.html#8095">[ thread ]</a>
<a href="subject.html#8095">[ subject ]</a>
<a href="author.html#8095">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
