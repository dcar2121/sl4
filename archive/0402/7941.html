<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendly AI in &quot;Positive Transcension&quot;</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Friendly AI in &quot;Positive Transcension&quot;">
<meta name="Date" content="2004-02-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendly AI in &quot;Positive Transcension&quot;</h1>
<!-- received="Sun Feb 15 12:32:40 2004" -->
<!-- isoreceived="20040215193240" -->
<!-- sent="Sun, 15 Feb 2004 14:32:22 -0500" -->
<!-- isosent="20040215193222" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Friendly AI in &quot;Positive Transcension&quot;" -->
<!-- id="402FC946.9090803@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="BMECIIDGKPGNFPJLIDNPMEMICMAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Friendly%20AI%20in%20&quot;Positive%20Transcension&quot;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Feb 15 2004 - 12:32:22 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7942.html">Kevin: "Re: qualia and orgasmium"</a>
<li><strong>Previous message:</strong> <a href="7940.html">Ben Goertzel: "RE: Encouraging a Positive Transcension"</a>
<li><strong>In reply to:</strong> <a href="7939.html">Ben Goertzel: "RE: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7946.html">Ben Goertzel: "RE: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Reply:</strong> <a href="7946.html">Ben Goertzel: "RE: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7941">[ date ]</a>
<a href="index.html#7941">[ thread ]</a>
<a href="subject.html#7941">[ subject ]</a>
<a href="author.html#7941">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Before responding at all I'm going to make a request of you.  Please
</em><br>
<em>&gt; summarize,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; FIRST, in a single, clear, not-too-long sentence
</em><br>
<em>&gt; 
</em><br>
<em>&gt; SECOND, in a single, clear, not-too-long paragraph
</em><br>
<em>&gt; 
</em><br>
<em>&gt; your own view of your theory of Friendly AI.  I will then paraphrase your
</em><br>
<em>&gt; own summaries, in my own words, in my revised essay.  I have no desire to
</em><br>
<em>&gt; misrepresent your ideas, of course.
</em><br>
<p>This cannot possibly be done.  What you're asking is undoable.
<br>
<p>We've already established that I don't understand Novamente.  This is a 
<br>
minor problem.  Let's say that instead there was a major problem, which is 
<br>
that, rather than knowingly failing to comprehend Novamente, I had looked 
<br>
at Novamente and thought, &quot;Oh, I understand this!  This is Cyc, only all 
<br>
the LISP atoms have activation levels attached to them.&quot;  Let's moreover 
<br>
suppose that I have never read &quot;Godel Escher Bach&quot;, studied biology, or in 
<br>
any other way acquired a rich concept for levels of organization, which 
<br>
you could invoke to explain the point of &quot;maps&quot; or &quot;emergent dynamics&quot;. 
<br>
Instead I write a lengthy paper and many times refer to &quot;Ben Goertzel's 
<br>
Novamente concept, which is Cyc with activation levels added.&quot;  In short, 
<br>
I not only completely fail to get Novamente, I substitute a wildly 
<br>
different model built out of concepts I already have, which is not even 
<br>
recognizable as Novamente-related except by a stretch of the imagination. 
<br>
&nbsp;&nbsp;Of course, since (in the words of Robyn Dawes) the problem with rotten 
<br>
chains of reasoning is that they don't stink, I think I've understood 
<br>
Novamente perfectly, except for some nagging implementational details.
<br>
<p>Now, please first, in a single, clear, not-too-long sentence, and second, 
<br>
in a single, clear, not-too-long paragraph, summarize, to me, your view of 
<br>
all the fundamental concepts that went into Novamente - not just for me, 
<br>
mind you, but for readers completely unfamiliar with your ideas who will 
<br>
read my interpretation of your ideas.
<br>
<p>Friendly AI does not fit on a T-Shirt!
<br>
<p><em>&gt; I'm sure that I don't fully understand your ideas and intentions when you
</em><br>
<em>&gt; wrote CFAI, nor your current intentions and ideas.  However, I have read
</em><br>
<em>&gt; them and talked to you about them more than once.  The problem is not so
</em><br>
<em>&gt; much that I'm not aware of the details, but that I think the details are
</em><br>
<em>&gt; consistent with certain summaries that you think they're inconsistent with
</em><br>
<em>&gt; ;-)
</em><br>
<p>You are saying things that are not simply wrong, but absolutely 
<br>
antithetical to basic principles of FAI.  I don't think you'd be doing 
<br>
that if you were just missing a few details.
<br>
<p><em>&gt; The summary you gave in your response email was almost clear, but not
</em><br>
<em>&gt; quite..
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In line with your request, I will add a brief disclaimer to the essay noting
</em><br>
<em>&gt; that you feel I didn't correctly interpret your ideas, and that the reader
</em><br>
<em>&gt; should turn to your writings directly to form their own opinion.
</em><br>
<p>I'll be sure to add a similar disclaimer to my forthcoming essay about 
<br>
&quot;Novamente: Cyc with activation levels added&quot;.
<br>
<p><em>&gt; Next, to clarify one small point, you say
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;f)  Friendly AI is frickin' complicated, so please stop summarizing it as
</em><br>
<em>&gt;&gt;&quot;hardwiring benevolence to humans&quot;.  I ain't bloody Asimov.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In fact what I said was &quot;programming or otherwise inculcating&quot; benevolence
</em><br>
<em>&gt; to humans -- i.e. by saying &quot;inculcating&quot; I meant to encompass teaching not
</em><br>
<em>&gt; just programming, or combinations of teaching and programming, etc.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So far as I can tell, the biggest difference you see between my rendition of
</em><br>
<em>&gt; your views, and your actual views, is that instead of &quot;programming or
</em><br>
<em>&gt; otherwise inculcating benevolence to humans&quot;, you'd rather speak about
</em><br>
<em>&gt; &quot;programming or otherwise inculcating humane morality in an AI&quot;.
</em><br>
<p>You're missing upward of a dozen fundamental concepts here.  One or two 
<br>
examples follow.
<br>
<p>First, let's delete &quot;programming or otherwise inculcating&quot; and replacing 
<br>
with &quot;choosing&quot;, which is the correct formulation under the basic theory 
<br>
of FAI, which makes extensive use of the expected utility principle. 
<br>
Choice subsumes choice over programming, choice over environmental 
<br>
information, and any other design options of which we might prefer one to 
<br>
another.
<br>
<p>Next, more importantly, &quot;humane&quot; is not being given its intuitive sense 
<br>
here!  Humane is here a highly technical concept, &quot;renormalized humanity&quot;. 
<br>
&nbsp;&nbsp;If you talk about &quot;benevolence&quot; you're talking about something that 
<br>
seems, at least to humans, simple and intuitive.  If you use &quot;humane&quot; in 
<br>
the technical sense I gave it, you are describing a deep technical thing 
<br>
that is not at all obvious.  Furthermore, you are describing a deep 
<br>
technical thing that incorporates fundamental FAI concepts.  &quot;Benevolence&quot; 
<br>
can be summed up by a constant utility function.  &quot;Humaneness&quot; cannot.  We 
<br>
could conceivably speak of &quot;hardwiring&quot; benevolence, or substitute 
<br>
&quot;programming or otherwise inculcating&quot;, without loss of generality.  This 
<br>
is not an idiom that even makes sense for &quot;humaneness&quot;, in its newly 
<br>
acquired sense of dynamic renormalization.
<br>
<p>Furthermore, if you say &quot;humaneness&quot; without giving it a technical 
<br>
definition, it absolutely doesn't help the readers - it simply reads as 
<br>
equivalent to benevolence.
<br>
<p>I doubt that anything I've said about humaneness conveys even a distant 
<br>
flavor of what it's about, actually - not enough exposition, not enough 
<br>
examples.
<br>
<p>That's one of the new principles involved.  There are more.
<br>
<p><em>&gt; And you
</em><br>
<em>&gt; consider this an approximation for &quot;an architecture that explicitly treats
</em><br>
<em>&gt; itself as an approximation to the AI that would be constructed by a humane
</em><br>
<em>&gt; morality.&quot;  I can see the difference you're pointing out but I don't see it
</em><br>
<em>&gt; as such a big difference -- I guess it all depends on how you ground the
</em><br>
<em>&gt; term &quot;benevolence.&quot;  One could fairly interpret &quot;benevolence to humans&quot; as
</em><br>
<em>&gt; &quot;acting toward humans in accordance with humane morality.&quot;  In that case,
</em><br>
<em>&gt; what my formulation misses is mainly that you want an AI that acts toward
</em><br>
<em>&gt; other things with humane morality as well, not just toward humans.
</em><br>
<p>These are totally incommensurate formulations - like comparing the answer 
<br>
&quot;15!&quot; with the question &quot;3 * 5 = ?&quot; or the set theory of fields.
<br>
<p>It's not that you're misunderstanding *what specifically* I'm saying, but 
<br>
that you're misunderstanding the *sort of thing* I'm attempting to 
<br>
describe.  Not apples versus oranges, more like apples versus the equation 
<br>
x'' = -kx.
<br>
<p><em>&gt; I still have the basic complaint that &quot;humane morality&quot; is a very narrow
</em><br>
<em>&gt; thing to be projecting throughout the cosmos.  It's also a rather funky and
</em><br>
<em>&gt; uncomfortable abstraction, given the immense diversity of human ethical
</em><br>
<em>&gt; systems throughout history and across the globe.
</em><br>
<p>I don't think you got &quot;humane&quot; at all, but it happens to be a thing that 
<br>
(should) include your dynamic reaction of discomfort at the perceived 
<br>
narrowness in your odd mental model of &quot;Eliezer Yudkowsky's FAI theory&quot;.
<br>
<p>&quot;Humaneness&quot; sneaks under the immense diversity problem by avoiding the 
<br>
specific content of said human ethical systems and going after the 
<br>
species-universal evolved dynamics underlying them.
<br>
<p>Your most serious obstacle here is your inability to see anything except 
<br>
the specific content of an ethical system - you see &quot;Joyous Growth&quot; as a 
<br>
specific ethical system, you see &quot;benevolence&quot; as specific content, your 
<br>
mental model of &quot;humaneness&quot; is something-or-other with specific ethical 
<br>
content.  &quot;Humaneness&quot; as I'm describing it *produces* specific ethical 
<br>
content but *is not composed of* specific ethical content.  Imagine the 
<br>
warm fuzzy feeling that you get when considering &quot;Joyous Growth&quot;.  Now, 
<br>
throughout history and across the globe, do you think that only 
<br>
21st-century Americans get warm fuzzy feelings when considering their 
<br>
personal moral philosophies?
<br>
<p>There actually is a strong analogy here between attempts to infuse lists 
<br>
of domain-specific knowledge into AIs, a la Cyc, and attempting to produce 
<br>
AIs that have specific cognitive dynamics which can output what we would 
<br>
regard as general reasoning.
<br>
<p><em>&gt; I do see your point that my more abstract concepts like growth, choice and
</em><br>
<em>&gt; joy are grounded -- in my mind -- in a lot of human thoughts and feelings.
</em><br>
<em>&gt; Very true, very deep.  But that doesn't mean that I can't pick and choose
</em><br>
<em>&gt; from among the vast contradictory morass of &quot;humane morality,&quot; certain
</em><br>
<em>&gt; aspects that I think are worthy of projecting across the cosmos, because
</em><br>
<em>&gt; they have more fundamental importance than the other aspects, less
</em><br>
<em>&gt; narrowness of meaning.
</em><br>
<p>The dynamics of the thinking you do when you consider that question would 
<br>
form part of the &quot;renormalization&quot; step, step 4, the volition examining 
<br>
itself under reflection.  It is improper to speak of a vast morass of 
<br>
&quot;humane morality&quot; which needs to be renormalized, because the word 
<br>
&quot;humane&quot; was not introduced until after step 4.  You could speak of a vast 
<br>
contradictory morass of the summated outputs of human moralities, but if 
<br>
you add the &quot;e&quot; on the end, then in FAI theory it has the connotation of 
<br>
something already renormalized.  Furthermore, it is improper to speak of 
<br>
renormalizing the vast contradictory morass as such, because it's a 
<br>
superposition of outputs, not a dynamic process capable of renormalizing 
<br>
itself.  You can speak of renormalizing a given individual, or 
<br>
renormalizing a model based on a typical individual.
<br>
<p>This is all already taken into account in FAI theory.  At length.
<br>
<p>(PS:  Please stop quoting the entire message below your replies!  This is 
<br>
explicitly not-a-good-thing according to the SL4 list rules.)
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7942.html">Kevin: "Re: qualia and orgasmium"</a>
<li><strong>Previous message:</strong> <a href="7940.html">Ben Goertzel: "RE: Encouraging a Positive Transcension"</a>
<li><strong>In reply to:</strong> <a href="7939.html">Ben Goertzel: "RE: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7946.html">Ben Goertzel: "RE: Friendly AI in &quot;Positive Transcension&quot;"</a>
<li><strong>Reply:</strong> <a href="7946.html">Ben Goertzel: "RE: Friendly AI in &quot;Positive Transcension&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7941">[ date ]</a>
<a href="index.html#7941">[ thread ]</a>
<a href="subject.html#7941">[ subject ]</a>
<a href="author.html#7941">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
