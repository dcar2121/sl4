<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Positive Transcension 2</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Positive Transcension 2">
<meta name="Date" content="2004-02-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Positive Transcension 2</h1>
<!-- received="Thu Feb 19 22:24:27 2004" -->
<!-- isoreceived="20040220052427" -->
<!-- sent="Fri, 20 Feb 2004 00:31:33 -0500" -->
<!-- isosent="20040220053133" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Positive Transcension 2" -->
<!-- id="BMECIIDGKPGNFPJLIDNPAEKMCNAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="4035C577.30880.128EAC@localhost" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Positive%20Transcension%202"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Thu Feb 19 2004 - 22:31:33 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7977.html">Ben Goertzel: "RE: Ethical theories"</a>
<li><strong>Previous message:</strong> <a href="7975.html">Ben Goertzel: "RE: Humane-ness (resend due to addressing error)"</a>
<li><strong>In reply to:</strong> <a href="7974.html">Philip Sutton: "Re: Positive Transcension 2"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7978.html">Philip Sutton: "RE: Positive Transcension 2"</a>
<li><strong>Maybe reply:</strong> <a href="7978.html">Philip Sutton: "RE: Positive Transcension 2"</a>
<li><strong>Reply:</strong> <a href="7979.html">Ben Goertzel: "RE: Positive Transcension 2"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7976">[ date ]</a>
<a href="index.html#7976">[ thread ]</a>
<a href="subject.html#7976">[ subject ]</a>
<a href="author.html#7976">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Philip,
<br>
<p><p>&nbsp;&nbsp;&nbsp;***
<br>
&nbsp;&nbsp;&nbsp;So let's start with how some humans might feel about some other humans
<br>
creating a 'thing' which could wipe out humans without their agreement.
<br>
&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;Well, I don't think that most humans understand the predicament of the
<br>
human race very well.
<br>
<p>&nbsp;&nbsp;If people don't understand the existential risks posed by other
<br>
technologies, how are they going to be able to participate in a serious
<br>
cost-benefit analysis regarding the creation of AGI's of various types?
<br>
<p>&nbsp;&nbsp;I'm a fan of the democratic process, and yet, I'm also a bit skeptical of
<br>
the ability of this process to make the right decisions in this kind of
<br>
area....
<br>
<p>&nbsp;&nbsp;So much of the world population is religious ... of course they are going
<br>
to feel TOTALLY DIFFERENTLY about the various existential risks and the
<br>
benefits of transhumanity, than nonreligious folks of transhumanist bent...
<br>
<p>&nbsp;&nbsp;Do you really think that we should proceed with these technologies via
<br>
some kind of global majority vote?  Bear in mind that around 80% of the
<br>
world population believes in reincarnation...
<br>
<p><p>&nbsp;&nbsp;&nbsp;***
<br>
&nbsp;&nbsp;&nbsp;&nbsp;Ben you said: &quot;And this may or may not lead to the demise of humanity -
<br>
which may or may not be a terrible thing.&quot;  At best loose language like this
<br>
means one thing to most people - somebody else is being  cavalier about
<br>
their future - at worst they are likely to perceive an active threat to
<br>
their existence.
<br>
&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;You can call it cavalier -- I call it honest and open-minded.  I guess if
<br>
you take that quote out of context it can sound scary, but why do you need
<br>
to take it out of context?
<br>
<p>&nbsp;&nbsp;***
<br>
&nbsp;&nbsp;Frankly I doubt if anyone will care if humanity evolves or transcends to a
<br>
higher state of being so long as it's voluntary.
<br>
&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;This is very naive -- very many people mind voluntary transhumanist
<br>
actions even if they're milder than transcension.   Psychedelic drugs are
<br>
illegal, as are smart drugs, homebrew neuromodifications, etc. etc. etc.
<br>
<p>&nbsp;&nbsp;Experimentation with stems cells is barely legal, for Chrissake !!!!
<br>
<p>&nbsp;&nbsp;Again, you seem to overestimate the rationality and wisdom of the &quot;mass
<br>
mind&quot;
<br>
<p>&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;To withhold concern for other humans lives because theoretically some AGI
<br>
might form the view that our mass/energy could be deployed more
<br>
beautifully/usefully seems simply silly.
<br>
&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;I do not advocate witholding concern for other humans.  I'm sorry if what
<br>
I wrote was misinterpreted that way.
<br>
<p>&nbsp;&nbsp;* **
<br>
&nbsp;&nbsp;I think the first step in creating safe AGI is for the would-be creators
<br>
of AGI to themselves make an ethical commitment to the protection of
<br>
humans - not because humans are the peak of creation or all that stunningly
<br>
special from the perspective of the universe as a whole but simply because
<br>
they exist and they deserve respect - especially from their fellow humans.
<br>
If AGI developers cannot give their fellow humans that commitment or that
<br>
level of respect, then I think they demonstrate they are not safe parents
<br>
for growing AGIs!
<br>
&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;In other words, you are stating that only people who agree with your
<br>
personal ethics should be allowed to create AGI's -- your personal ethics
<br>
being that the preservation of humans is paramount.
<br>
<p>&nbsp;&nbsp;I think that the preservation of humans is very, very important -- but I'm
<br>
not willing to assert that it's absolutely paramount just to sound
<br>
&quot;politically correct.&quot;
<br>
<p>&nbsp;&nbsp;***
<br>
&nbsp;&nbsp;&nbsp;&nbsp;I was actually rather disturbed by your statement towards the end of
<br>
your paper where you said: &quot;In spite of my own affection for Voluntary
<br>
Joyous Growth, however, I have strong inclinations toward both the Joyous
<br>
Growth Guided Voluntarism and pure Joyous Growth variants as well.&quot;  My
<br>
reading of this is that you would be prepared to inflict Joyous Growth
<br>
future on people whether they wanted it or not and even if this resulted in
<br>
the involuntary elimination of people or other sentients that somehow were
<br>
seen by the AGI or AGIs pursuing Joyous Growth as being an impediment in the
<br>
way of the achievement of joyous growth.  If I've interpreted what you are
<br>
saying correctly that's pretty scary stuff!
<br>
&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;It seems you are oddly misinterpreting my statement here.  I said that my
<br>
primary affection was for VOLUNTARY Joyous Growth, which is an ethical
<br>
principle that places *free choice* as a primary value.
<br>
<p>&nbsp;&nbsp;Free choice means not forcing humans to transcend, and not forcing humans
<br>
not to transcend.
<br>
<p>&nbsp;&nbsp;What you are advocating is a Joyous Growth Biased Voluntarism, in which AS
<br>
AN ABSOLUTE RULE no one is to be forced to transcend (or annihilated, or
<br>
forced to do anything).   I think this is more problematic, but is also
<br>
worthy of consideration.
<br>
<p>&nbsp;&nbsp;* **
<br>
<p>&nbsp;&nbsp;I think the next step is to consider what values we would like AGIs to
<br>
hold in order for them to be sound citizens in a community of sentients. I
<br>
think the minimum that is needed is for them to have a tolerant, respectful,
<br>
compassionate, live-and-let-live attitude.
<br>
&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;It seems to me that you're just rephrasing what I call &quot;Voluntary
<br>
Joyousity&quot; here, in language that you like better for some reason.
<br>
<p>&nbsp;&nbsp;compasionate = valuing Joy of others
<br>
&nbsp;&nbsp;tolerant, live-and-let-live = valuing others' ability to choose
<br>
<p>&nbsp;&nbsp;All you've left out is the &quot;growth&quot; part.
<br>
<p>&nbsp;&nbsp;If you prefer the verbiage of &quot;compassionate and tolerant&quot; as opposed to
<br>
&quot;joy and choice&quot;, that's fine with me....  None of these English words
<br>
really captures what needs to be said exactly, anyway...
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;***
<br>
&nbsp;&nbsp;I think AGIs that had a tolerant, respectful, compassionate, live-and-let-
<br>
live ethic would not intrude excessively on human society.  They might, for
<br>
example, try to discourage female circumcision or even go so far as stopping
<br>
capital punishment in human societies (I can't see that these actions would
<br>
conform to the ethics that the AGIs were given [under my scenario] their
<br>
human creators/carers).  As far as I can see I don't think that AGIs need to
<br>
have ported into them a sort of general digest of human-ness or even an
<br>
idiosyncratic (renormalised) essence of general humane-ness.  I think we
<br>
should be able to be more transparent than that and to identify the key
<br>
ethical drivers that lead to tolerant, respectful, compassionate,
<br>
live-and-let-live behaviour.
<br>
&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;What's odd is that you seem to agree with me almost completely --- you
<br>
agree with me that Eliezer's idea of embodying humane-ness in AI's is
<br>
overcomplicated, and you agree that it's good to supply AGI's with general
<br>
ethical principles.  The only difference is that you choose different words
<br>
to describe what I call Joy and Choice, and you appear not to value what I
<br>
call Growth enough to want to make it a basic value.
<br>
<p>&nbsp;&nbsp;* **
<br>
<p>&nbsp;&nbsp;I think these notions are sufficiently abstract to be able to pass your
<br>
test of being likely to &quot;survive successive self-modification&quot;.
<br>
&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;Yes -- becuase they're the SAME as the notions I proposed, merely worded
<br>
in a way that evokes more pleasant associations for you...
<br>
&nbsp;&nbsp;* **
<br>
<p>&nbsp;&nbsp;In your paper you suggest that we need AGIs to save humanity from our
<br>
destructive urges (applied via advanced technology).  If having AGIs around
<br>
could increase the risk of humanity being wiped out to achieve a more
<br>
beautiful deployment of mass/energy then it might be a good idea to go back
<br>
and check to see just exactly how dangerous the other feared technologies
<br>
are. While nanotech and genetic engineering could produce some pretty
<br>
virulent and deadly entities I'm not sure that they are likely to be much
<br>
more destructive than bubonic plague, eboloa virus, small pox have been in
<br>
their time etc.  There are a lot of people around so that even if these
<br>
threats killed millions? billions? they are unlikely to wipe out even most
<br>
people.  So should we seek help from this scale of threat by creating
<br>
something that might arbitarily decide to wipe out the lot of us on a whim?
<br>
&nbsp;&nbsp;***
<br>
<p>&nbsp;&nbsp;I'm afraid you are being woefully naive on this particular topic.  The
<br>
existential risks of MNT and bioweapons are very very real -- not today, but
<br>
within centuries for sure, and decades quite probably.  But I don't have
<br>
time to trot out the arguments for this point tonight.
<br>
<p>&nbsp;&nbsp;-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7977.html">Ben Goertzel: "RE: Ethical theories"</a>
<li><strong>Previous message:</strong> <a href="7975.html">Ben Goertzel: "RE: Humane-ness (resend due to addressing error)"</a>
<li><strong>In reply to:</strong> <a href="7974.html">Philip Sutton: "Re: Positive Transcension 2"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7978.html">Philip Sutton: "RE: Positive Transcension 2"</a>
<li><strong>Maybe reply:</strong> <a href="7978.html">Philip Sutton: "RE: Positive Transcension 2"</a>
<li><strong>Reply:</strong> <a href="7979.html">Ben Goertzel: "RE: Positive Transcension 2"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7976">[ date ]</a>
<a href="index.html#7976">[ thread ]</a>
<a href="subject.html#7976">[ subject ]</a>
<a href="author.html#7976">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
