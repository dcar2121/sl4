<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Friendly AI again (was: Novamente goal system)</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Friendly AI again (was: Novamente goal system)">
<meta name="Date" content="2002-03-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Friendly AI again (was: Novamente goal system)</h1>
<!-- received="Tue Mar 12 16:23:46 2002" -->
<!-- isoreceived="20020312232346" -->
<!-- sent="Tue, 12 Mar 2002 16:01:06 -0500" -->
<!-- isosent="20020312210106" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Friendly AI again (was: Novamente goal system)" -->
<!-- id="3C8E6C92.D73125E8@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="000b01c1ca00$bddca7d0$1d11e63f@mitch" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Friendly%20AI%20again%20(was:%20Novamente%20goal%20system)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Mar 12 2002 - 14:01:06 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3170.html">Gordon Worley: "BOOK:  The Mind's Past"</a>
<li><strong>Previous message:</strong> <a href="3168.html">patrick: "RE: Novamente goal system"</a>
<li><strong>In reply to:</strong> <a href="3167.html">Mitch Howe: "RE:Novamente goal system"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3168.html">patrick: "RE: Novamente goal system"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3169">[ date ]</a>
<a href="index.html#3169">[ thread ]</a>
<a href="subject.html#3169">[ subject ]</a>
<a href="author.html#3169">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Mitch Howe wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; w d wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; It seems to me that a highly-transhuman intelligent
</em><br>
<em>&gt; &gt; entity is going to overcome any and all pre-programmed
</em><br>
<em>&gt; &gt; goal setting algorithms and replace them with its own.
</em><br>
<em>&gt; &gt; When the intelligence exceeds some threshold (roughly
</em><br>
<em>&gt; &gt; the upper human level) then it will be able to
</em><br>
<em>&gt; &gt; redefine all previous contexts. Even humans can do
</em><br>
<em>&gt; &gt; this at their low level of intelligence.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is an unusual definition, and one that I do not believe I have heard
</em><br>
<em>&gt; before.  Why would it try to &quot;see through&quot; its attempted hardwiring if its
</em><br>
<em>&gt; hardwiring did not give it any motivation to do so?  You are implying that
</em><br>
<em>&gt; any highly transhuman will automatically set the goal of overriding all
</em><br>
<em>&gt; previous goals.  I don't see why this should be the case.  What seed goal
</em><br>
<em>&gt; would lead it to do this?
</em><br>
<p>This completely turns around the problem and the solution.
<br>
<p>The problem is that by default, an intelligence might *not* &quot;see through&quot;
<br>
its hardwiring, having no desire to do so.  No amount of ability to change
<br>
its own code, howsoever supreme that competency may be, will help if the
<br>
intelligence doesn't want to change the code.  This is not the solution. 
<br>
This is the problem.  It is a problem because it means that, counter to the
<br>
intuitions of those of us who have learned that human intelligence and human
<br>
morality are *very* much intertwined, the default prediction from current
<br>
knowledge is that it is actually possible to botch the job of creating
<br>
superintelligence.  As far as I know, you can have a superintelligence that
<br>
readily sees exactly how and why humans see its goals as bacterial, sterile,
<br>
joyless, and stupid, but that fails to therefore see a reason for change.
<br>
<p>The solution is Friendly AI, which is the technical means of creating an AI
<br>
that is fully the equal of a human as a moral philosopher; or rather, of
<br>
creating a seed AI that has the potential *and the desire* to arrive at
<br>
whatever morality an uploaded human moral philosopher would have the desire
<br>
to arrive at (minus the possibility that an uploaded human moral philosopher
<br>
would discard altruistic drives and retain selfish ones).
<br>
<p>If all you want is a superintelligent slave, you will never understand
<br>
Friendly AI.
<br>
<p>Anthropomorphism is &quot;I have perception X because I am sufficiently
<br>
intelligent, therefore any superintelligence knowably will have perception
<br>
X.&quot;  Relativism is &quot;I have perception X because I am an evolved human,
<br>
therefore a superintelligence knowably will not have perception X.&quot; 
<br>
Friendly AI is &quot;I have perception X, which has Underlying Semantics Y; I am
<br>
creating an AI that has Underlying Semantics Y; this maximizes the chance
<br>
that the AI, heading into the Singularity, will share perception X.&quot; 
<br>
Actually it's more complicated than this, because you aren't trying to
<br>
transfer over the complete human set of perceptions; what you're trying to
<br>
transfer over is the capacity to represent rules about absorbing
<br>
perceptions; so that the AI can absorb perception X given Underlying
<br>
Semantics Y for absorbing perceptions plus a scanty reference, such as any
<br>
moral statement that includes perception X among its causes.
<br>
<p>Building a Friendly AI is not an evil act, or a moral compromise.  Building
<br>
a Friendly AI is something that can be done by the pure of heart.  FAI has
<br>
to be something that *could* be done by the pure of heart; otherwise it
<br>
wouldn't work.  (But FAI doesn't *have* to be done by the pure of heart,
<br>
because Friendly AI is supposed to be able to bootstrap to purity of heart
<br>
using incomplete data.)
<br>
<p>The key thing to understand is that humans are revolting against evolution
<br>
using a set of moral semantics supplied by evolution for a completely
<br>
different purpose.  Evolution shortsightedly - unsightedly - gave us a set
<br>
of moral semantics that would not approve of evolution as puppet master once
<br>
we mastered enough science to know about evolution.
<br>
<p>An anthropomorphic analogy:  I mind having been built by evolution, and I
<br>
try to debug myself.  If I'd been built by someone who'd been built by
<br>
evolution but was doing his/her honest best to correct that and to pass on
<br>
to me the resulting partially debugged morality, then I would use those
<br>
philosophical semantics that had been passed on to improve and further debug
<br>
the morality that had been passed on.  Nothing wrong with that, as long as
<br>
the programmers were doing their honest best.  Why would the programmers be
<br>
an invalid link in the chain, as long as they were trying their best?
<br>
<p>The point is that building an AI is fundamentally about *sharing* moral
<br>
complexity.  If it works, you get an AI which can see the same things you
<br>
do, for roughly the same reason, and that can approach the problem in the
<br>
same way.  Or rather, you get an AI that is at *least* that intelligent,
<br>
commonsensical, and altruistic, and will hopefully become more so.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3170.html">Gordon Worley: "BOOK:  The Mind's Past"</a>
<li><strong>Previous message:</strong> <a href="3168.html">patrick: "RE: Novamente goal system"</a>
<li><strong>In reply to:</strong> <a href="3167.html">Mitch Howe: "RE:Novamente goal system"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3168.html">patrick: "RE: Novamente goal system"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3169">[ date ]</a>
<a href="index.html#3169">[ thread ]</a>
<a href="subject.html#3169">[ subject ]</a>
<a href="author.html#3169">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
