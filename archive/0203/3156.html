<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Novamente goal system</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Novamente goal system">
<meta name="Date" content="2002-03-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Novamente goal system</h1>
<!-- received="Sun Mar 10 12:41:26 2002" -->
<!-- isoreceived="20020310194126" -->
<!-- sent="Sun, 10 Mar 2002 10:30:36 -0700" -->
<!-- isosent="20020310173036" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Novamente goal system" -->
<!-- id="LAEGJLOGJIOELPNIOOAJKELMCEAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3C8B8217.4E6C808A@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Novamente%20goal%20system"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sun Mar 10 2002 - 10:30:36 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3157.html">Evan Reese: "Re: Wednesday's chatlog"</a>
<li><strong>Previous message:</strong> <a href="3155.html">Eliezer S. Yudkowsky: "Re: Novamente goal system"</a>
<li><strong>In reply to:</strong> <a href="3155.html">Eliezer S. Yudkowsky: "Re: Novamente goal system"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3162.html">Eliezer S. Yudkowsky: "Novamente project goals"</a>
<li><strong>Reply:</strong> <a href="3162.html">Eliezer S. Yudkowsky: "Novamente project goals"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3156">[ date ]</a>
<a href="index.html#3156">[ thread ]</a>
<a href="subject.html#3156">[ subject ]</a>
<a href="author.html#3156">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi,
<br>
<p><em>&gt; Actually, as described, this is pretty much what I would consider
</em><br>
<em>&gt; &quot;human-equivalent AI&quot;, for which the term &quot;transhumanity&quot; is not really
</em><br>
<em>&gt; appropriate.  I don't think I'm halfway to transhumanity, so an
</em><br>
<em>&gt; AI twice as
</em><br>
<em>&gt; many sigma from the mean is not all the way there.  Maybe you should say
</em><br>
<em>&gt; that Novamente-the-project is striving for human-equivalence; either that,
</em><br>
<em>&gt; or define what you think a *really* transhuman Novamente would be like...
</em><br>
<p>Well, not surprisingly I disagree with you here.  I do not think that either
<br>
of your alternatives:
<br>
<p>a) make the (false) claim that our project is striving for human equivalent
<br>
AI, or
<br>
<p>b) put significant effort into making a detailed description of a
<br>
profoundly-transhuman Novamente
<br>
<p>is the best course for us at the present time.
<br>
<p>We are striving first for human-equivalent, then slightly-transhuman, then
<br>
profoundly-transhuman AI.
<br>
<p>I do not think it is important for us to articulate in detail, at this
<br>
point, what we believe a profoundly-transhuman AI evolved from the Novamente
<br>
system will be like.
<br>
<p>Because, I think it is probably NOT POSSIBLE for us to envision in detail
<br>
what a profoundly-transhuman AI evolved from the Novamente system will be
<br>
like.
<br>
<p>I think the farthest out we can plausibly extrapolate, right now, is the
<br>
slightly-transhuman level.  The next level beyond that will be co-created by
<br>
ourselves and the slightly-transhuman AI itself.
<br>
<p>I choose to focus my efforts on how to get from here to the
<br>
slightly-transhuman level.  Of course, the future beyond that enters into
<br>
our thinking on system design occasionally.  But I don't think that focusing
<br>
more on the profoundly-transhuman level, at this point, would help us get to
<br>
that level any faster or any better.
<br>
<p><em>&gt; Incidentally, this thing of having the project and the AI having the same
</em><br>
<em>&gt; name is darned inconvenient.  Over on this end, the group is SIAI, the
</em><br>
<em>&gt; architecture is GISAI, and the AI itself will be named Ai-something.
</em><br>
<p>Just refer to &quot;The Novamente project&quot; versus &quot;The Novamente system.&quot;
<br>
<p>I am not so sure this nomenclature is inferior to inventing a panoply of
<br>
acronyms ;)
<br>
<p><em>&gt; Claims are meant to be tested.  So far the claims have been tested on
</em><br>
<em>&gt; several occasions; you (and, when I was at Webmind, a few other
</em><br>
<em>&gt; folk) named
</em><br>
<em>&gt; various things that you didn't believe could possibly fit into
</em><br>
<em>&gt; the CFAI goal
</em><br>
<em>&gt; system, such as &quot;curiosity&quot;, and I explained how curiosity indeed fit into
</em><br>
<em>&gt; the architecture.  That was an example of passing a test for the
</em><br>
<em>&gt; first claim
</em><br>
<em>&gt; and third claim.  If you have anything else that you believe cannot be
</em><br>
<em>&gt; implemented under CFAI, you can name it and thereby test the claim again.
</em><br>
<p>I understand how, in principle, any other goal can be &quot;fit into&quot; the CFAI
<br>
goal architecture.
<br>
<p>Similarly, in principle, any other goal can be &quot;fit into&quot; the more flexible
<br>
Novamente goal architecture.
<br>
<p>This is sort of like how there are many models of universal computing, each
<br>
of which can emulate any kind of computing.
<br>
<p>It's also sort of like how, in principle, any piece of human common sense
<br>
knowledge can be expressed in predicate logic.  However, this &quot;in principle&quot;
<br>
expression doesn't necessarily work out well in practice.
<br>
<p>Making a hand-waving argument that curiosity (or X or Y or Z) can be
<br>
represented in your goal system, does not demonstrate that this
<br>
representation can be pragmatically useful in the course of a real AI
<br>
system's experience.  Just as making a 10-page 90%-accurate representation
<br>
of the concept of &quot;curiosity&quot; in terms of predicate logic does not imply
<br>
that this representation is of any use.
<br>
<p>I am not drawing a technical parallel between your goal architecture and
<br>
predicate logic, but merely using the predicate logic expression of common
<br>
knowledge as an example of an &quot;in principle&quot; expression that sounds good at
<br>
first, but doesn't pan out so well in practice.  Your reduction of other
<br>
goal systems to parts of your goal hierarchy may be a similar case.
<br>
<p><em>&gt; Similarly, I've explained how a goal system based on predicted-to versus
</em><br>
<em>&gt; associated-with seeks out a deeper class of regularities in
</em><br>
<em>&gt; reality, roughly
</em><br>
<em>&gt; the &quot;useful&quot; regularities rather than the &quot;surface&quot; regularities.  Two
</em><br>
<em>&gt; specific examples of this are:  (1) CFAI will distinguish between genuine
</em><br>
<em>&gt; causal relations and implication-plus-temporal-precedence, since only the
</em><br>
<em>&gt; former can be used to manipulate reality; Judea Pearl would call this
</em><br>
<em>&gt; &quot;severance&quot;, while I would call this &quot;testing for hidden third causes&quot;.  I
</em><br>
<em>&gt; don't know if Novamente is doing this now, but AFAICT Webmind's
</em><br>
<em>&gt; documentation on the causality/goal architecture didn't show any way to
</em><br>
<em>&gt; distinguish between the two.
</em><br>
<p>This is an error on your part, which I tried but failed to correct at the
<br>
time.
<br>
<p>In fact, both the Webmind AI Engine and Novamente distinguish between
<br>
&quot;temporal implication&quot; and &quot;causality&quot;, including the former as one
<br>
component of the latter.  I sent you a long and detailed paper by Jeff
<br>
Pressing (of the Webmind Inc. R&amp;D group) which discussed many leading
<br>
approaches to causal inference in the research literature, including
<br>
probabilistic approaches (though not Pearl's specifically), and which very
<br>
clearly distinguished &quot;temporal implication&quot; from causality.
<br>
<p>Furthermore, the rough-draft paper on &quot;varieties of self modification&quot; that
<br>
I recently posted to this list, mentioned causality, and clearly indicated
<br>
two key ingredients to causality:
<br>
<p>a) temporal implication
<br>
b) the existence of a plausible causal mechanism
<br>
<p>In Novamente the notion of a &quot;plausible causal mechanism&quot; has a particular
<br>
implementation in terms of other Novamente concepts such as schema and
<br>
patterns, which I can't go into here due to lack of time...
<br>
<p><em>&gt; (2) CFAI will distinguish contextual
</em><br>
<em>&gt; information that affects whether something is desirable; specifically,
</em><br>
<em>&gt; because of the prediction formalism, it will seek out factors that tend to
</em><br>
<em>&gt; interfere with or enable A leading to B, where B is the goal
</em><br>
<em>&gt; state.  I again
</em><br>
<em>&gt; don't know about Novamente, but most of the (varied) systems that were
</em><br>
<em>&gt; described to me as being Webmind might be capable of
</em><br>
<em>&gt; distinguishing degrees
</em><br>
<em>&gt; of association, but would not specialize on degrees of useful association.
</em><br>
<p>This is a misunderstanding on your part, again.
<br>
<p>Of course, in reasoning about the fulfillment of goals represented by
<br>
GoalNodes, both Webmind and Novamente can reason about degrees of useful
<br>
association, and degrees of useful implication, useful causation, etc etc.
<br>
<p>This kind of goal-oriented reasoning was implemented in Webmind, it is not
<br>
yet implemented in Novamente, but it is certainly in the design.
<br>
<p><em>&gt; To give a concrete example, Webmind (as I understood it) would, on seeing
</em><br>
<em>&gt; that rooster calls preceded sunrise, where sunrise is desirable,
</em><br>
<em>&gt; would begin
</em><br>
<em>&gt; to like rooster calls and would start playing them over the
</em><br>
<em>&gt; speakers.
</em><br>
<p>Eliezer, this is a complete misunderstanding of the Webmind system's
<br>
approach to causal inference.
<br>
<p>Of course, we did not fall into such an elementary error as this.  In fact
<br>
this rooster/sunrise example is specifically discussed in the Novamente
<br>
documentation.
<br>
<p>I do not blame you for not understanding the Webmind AI Engine, or the
<br>
Novamente system, since you have never been given any kind of systematic or
<br>
comprehensive documentation for either of them.
<br>
<p>However, you seem to have formed a rather incorrect view of both
<br>
architectures, based on conversations with myself and other Webmind Inc.
<br>
staff.
<br>
<p>Interestingly, all of your misconceptions seem to have a common pattern: In
<br>
each case, you mistakenly assert the belief that the Webmind/Novamente
<br>
architecture embodies some basic conceptual mistake, which in fact it does
<br>
not embody.
<br>
<p>When you finally see the full Novamente design, you will find that it does
<br>
not embody any such elementary conceptual mistakes.
<br>
<p>If the Novamente design fails, it will not be for such simple and obvious
<br>
reasons.  It will rather be, because our approach to some component of
<br>
intelligence is insufficiently computationally efficient.
<br>
<p><p><em>&gt; CFAI
</em><br>
<em>&gt; would try playing a rooster call, notice that it didn't work, and
</em><br>
<em>&gt; hypothesize that there was a common third cause for sunrise and rooster
</em><br>
<em>&gt; calls which temporally preceded both (this is actually correct; dawn, I
</em><br>
<em>&gt; assume, is the cause of rooster calls, and Earth's rotation is the common
</em><br>
<em>&gt; cause of dawn and sunrise); after this rooster calls would cease to be
</em><br>
<em>&gt; desirable since they were no longer predicted to lead to sunrise.  Maybe
</em><br>
<em>&gt; Webmind can be hacked to do this the right way; given the social process
</em><br>
<em>&gt; that developed Webmind, it certainly wouldn't be surprising to
</em><br>
<em>&gt; find that at
</em><br>
<em>&gt; least one of the researchers thought up this particular trick.
</em><br>
<p>This is not a &quot;trick&quot;, this is a standard idea from the causal inference
<br>
literature which is at least dozens of years old, and which was considered
<br>
at the very start of the design process for the Webmind AI Engine causal
<br>
inference module.  It is covered in the review paper on causal inference by
<br>
Jeff Pressing which I sent you some years ago.
<br>
<p>Perhaps you failed to grasp the Webmind causal inference module because the
<br>
language used to discuss it was different than the language you tend to use
<br>
to discuss causality.  Or perhaps because the design was overcomplicated;
<br>
the Novamente design is a bit simpler but accomplishes the same thing.
<br>
<p><em>&gt; My point is
</em><br>
<em>&gt; that in CFAI the Right Thing is directly, naturally, and
</em><br>
<em>&gt; elegantly emergent,
</em><br>
<em>&gt; where it goes along with other things as well, such as the Right Thing for
</em><br>
<em>&gt; positive and negative reinforcement, as summarized in &quot;Features
</em><br>
<em>&gt; of Friendly
</em><br>
<em>&gt; AI&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; So that's what I would offer as a demonstration of the second claim.
</em><br>
<p>I have not seen you produce anything resembling a demonstration that you
<br>
have a design for an AI system capable of correctly doing causal inference.
<br>
<p>I have not provided you with such a demonstration either, regarding my own
<br>
AI systems.
<br>
<p>One difference between our claims, however is:
<br>
<p>-- I am only claiming that what I have (and have not fully revealed to you)
<br>
is a *probably workable* approach to seed AI
<br>
<p>-- You are claiming that what you have (and have not fully revealed to me)
<br>
is in some sense the *uniquely best* approach to seed AI
<br>
<p>Your claim is much stronger and would thus seem to require much stronger
<br>
justification.
<br>
<p><em>&gt;  When I was at Webmind I could
</em><br>
<em>&gt; generally convince someone of how cleanly causal, Friendliness-topped goal
</em><br>
<em>&gt; systems would work, as long as I could interact with them in person.
</em><br>
<p>Well, Eliezer, after you left Webmind Inc. following your brief and
<br>
enjoyable visit, no one seemed to be jumping up and down to revise the goal
<br>
architecture in accordance with your ideas.
<br>
<p>I honestly don't think that anyone who talked to you during your visit to
<br>
Webmind Inc. was *convinced* by your ideas.  Some were intrigued, and some
<br>
thought they were absurd.  But intrigued is not the same as convinced.
<br>
<p>My role was definitely one of advocate in this case.  I tried very hard to
<br>
get the others to take your ideas seriously, and it was only possible in
<br>
some cases.
<br>
<p><em>&gt; Right; what I'm saying is that nudging Novamente's architecture into
</em><br>
<em>&gt; hierarchicality is one thing, and nudging it into Friendliness is quite
</em><br>
<em>&gt; another.
</em><br>
<p>Sure.
<br>
<p>The two concrete things you've pointed out are:
<br>
<p>1) the links in the hierarchy in question should be composed of
<br>
ImplicationLinks not AssociativeLinks (true enough, and that was my
<br>
proposal)
<br>
<p>2) the nodes in the hierarchy should be &quot;specially shaped for Friendliness&quot;
<br>
(I'm not so sure what this means; there are ways to implement such things in
<br>
Novamente but I'm not at all sure of the necessity)
<br>
<p><p><em>&gt;Incidentally, if you think that the CFAI architecture is more
</em><br>
<em>&gt; &quot;rigid&quot; than Novamente in any real sense, please name the resulting
</em><br>
<em>&gt; disadvantage and I will explain how it doesn't work that way.
</em><br>
<p>I will, but not in this e-mail, which is long enough...
<br>
<p><p><em>&gt; A directed acyclic network is not *forced* upon the goal
</em><br>
<em>&gt; system; it
</em><br>
<em>&gt; is the natural shape of the goal system, and violating this shape
</em><br>
<em>&gt; results in
</em><br>
<em>&gt; distortion of what we would see as the natural/normative behavior.
</em><br>
<p>I guess this gets to the crux of the matter.
<br>
<p>According to my own measly human intuition, this view of goal systems is is
<br>
as dead wrong as dead wrong can be.
<br>
<p>In my view,
<br>
<p>1) The &quot;natural shape&quot; of a goal system is an unstructured heterarchical
<br>
graph.
<br>
<p>2) A working goal hierarchy (&quot;directed acyclic network&quot; if you prefer) must
<br>
emerge from a goal heterarchy without structural constraints.
<br>
<p><p>I feel that you want to *force* what should *emerge*.
<br>
<p><p><em>&gt;  What
</em><br>
<em>&gt; is the system property that requires continual intense interaction to
</em><br>
<em>&gt; enforce, and how does the continual intense interaction enforce it?  Or
</em><br>
<em>&gt; alternatively, what is it that requires continual intense informational
</em><br>
<em>&gt; inputs from humans in order to work right?
</em><br>
<p>There may be many things in this category.
<br>
<p>One of them is, as I said: Ensuring that the system's Friendliness
<br>
ConceptNode remains reasonably well-aligned with the human concept of
<br>
Friendliness, rather than drifting far astray...
<br>
<p><p><p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3157.html">Evan Reese: "Re: Wednesday's chatlog"</a>
<li><strong>Previous message:</strong> <a href="3155.html">Eliezer S. Yudkowsky: "Re: Novamente goal system"</a>
<li><strong>In reply to:</strong> <a href="3155.html">Eliezer S. Yudkowsky: "Re: Novamente goal system"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3162.html">Eliezer S. Yudkowsky: "Novamente project goals"</a>
<li><strong>Reply:</strong> <a href="3162.html">Eliezer S. Yudkowsky: "Novamente project goals"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3156">[ date ]</a>
<a href="index.html#3156">[ thread ]</a>
<a href="subject.html#3156">[ subject ]</a>
<a href="author.html#3156">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
