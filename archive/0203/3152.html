<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Novamente goal system</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Novamente goal system">
<meta name="Date" content="2002-03-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Novamente goal system</h1>
<!-- received="Sun Mar 10 09:41:03 2002" -->
<!-- isoreceived="20020310164103" -->
<!-- sent="Sat, 9 Mar 2002 22:23:37 -0700" -->
<!-- isosent="20020310052337" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Novamente goal system" -->
<!-- id="LAEGJLOGJIOELPNIOOAJKELICEAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="3C8A68E5.E09710F8@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Novamente%20goal%20system"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sat Mar 09 2002 - 22:23:37 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3153.html">Mitch Howe: "Re: Singularity Media (was: Wednesday's chatlog)"</a>
<li><strong>Previous message:</strong> <a href="3151.html">Ben Goertzel: "RE: Wednesday's chatlog"</a>
<li><strong>In reply to:</strong> <a href="3148.html">Eliezer S. Yudkowsky: "Re: Novamente goal system"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3155.html">Eliezer S. Yudkowsky: "Re: Novamente goal system"</a>
<li><strong>Reply:</strong> <a href="3155.html">Eliezer S. Yudkowsky: "Re: Novamente goal system"</a>
<li><strong>Reply:</strong> <a href="3160.html">Eliezer S. Yudkowsky: "Webmind Inc. as social process"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3152">[ date ]</a>
<a href="index.html#3152">[ thread ]</a>
<a href="subject.html#3152">[ subject ]</a>
<a href="author.html#3152">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt;  Just to make sure
</em><br>
<em>&gt; we're all on the same wavelength, would you care to describe briefly what
</em><br>
<em>&gt; you think a transhuman Novamente would be like?
</em><br>
<p>Eliezer, this is a hard question, which would require a long and complex
<br>
answer to do it any justics.
<br>
<p>The short answer is that a transhuman Novamente could look like a *lot* of
<br>
different things, depending on a lot of different factors.
<br>
<p>My &quot;default&quot; vision of a transhuman Novamente assumes a Novamente that
<br>
achieves transhuman intelligence without having a physical robot body to
<br>
control, or humanlike sensors (camera eyes, etc.).   However, I don't know
<br>
how to estimate the probability that this &quot;default&quot; vision will be the one
<br>
that comes about.  I like this default vision emotionally, because it fits
<br>
in with a more optimistic timeline, in that robotics and humanlike sensors
<br>
are not things I'm currently working on.
<br>
<p>Let me roughly describe my idea of the FIRST transhuman AI, in this default
<br>
vision.
<br>
<p>This first (mildly) transhuman Novamente will communicate with us in
<br>
comprehensible and fluent but not quite human-like English.  It will be a
<br>
hell of a math and CS whiz, able to predict financial and economic trends
<br>
better than us, able to read scientific articles easily, and able to read
<br>
human literary products but not always intuitively &quot;getting&quot; them.  It will
<br>
be very interested in intense interactions with human scientists on topics
<br>
of its expertise and interest, and in collaboratively working with them to
<br>
improve its own intelligence and solving their problems.  It will be
<br>
qualitatively smarter than us, in the same sense that you or I are
<br>
qualitatively smarter than the average human -- but no so much smarter as to
<br>
have no use for us (yet)....
<br>
<p>How long this phase will last, before mild transhumanity gives rise to
<br>
full-on Singularity, I am certainly not sure.
<br>
<p><p><em>&gt; I claim:  There is no important sense in which a cleanly causal,
</em><br>
<em>&gt; Friendliness-topped goal system is inferior to any alternate system of
</em><br>
<em>&gt; goals.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I claim:  The CFAI goal architecture is directly and immediately
</em><br>
<em>&gt; superior to
</em><br>
<em>&gt; the various widely differing formalisms that were described to me by
</em><br>
<em>&gt; different parties, including Ben Goertzel, as being &quot;Webmind's goal
</em><br>
<em>&gt; architecture&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I claim:  That for any important Novamente behavior, I will be able to
</em><br>
<em>&gt; describe how that behavior can be implemented under the CFAI architecture,
</em><br>
<em>&gt; without significant loss of elegance or significant additional computing
</em><br>
<em>&gt; power.
</em><br>
<p>These are indeed claims, but as far as I can tell they are not backed up by
<br>
anything except your intuition.
<br>
<p>I am certainly not one to discount the value of intuition.  The claim that
<br>
Novamente will suffice for a seed AI is largely based on the intuition of
<br>
myself and my collaborators.
<br>
<p>However, my intuition happens to differ from yours, as regards the ultimate
<br>
superiority of your CFAI goal architecture.
<br>
<p>I am not at all sure there is *any* goal architecture that is &quot;ultimate and
<br>
superior&quot; in the sense that you are claiming for yours.
<br>
<p>And I say this with what I think is a fairly decent understanding of the
<br>
CFAI goal architecture.  I've read what you've written about it, talked to
<br>
you about it, and thought about it a bit.  I've also read, talked about, and
<br>
thought about your views on closely related issues such as causality.
<br>
<p>Sometimes, when the data (mathematical or empirical) is limited, there is
<br>
just no way to resolve a disagreement of intuition.  One simply has to
<br>
gather more data (via experimentation (in this case computational) or
<br>
mathematical proof).
<br>
<p>I don't think I have a big emotional stake in this issue, Eliezer.  I never
<br>
have minded integrating my own thinking with that of others.  In fact I did
<br>
a bit too much of this during the Webmind period, as we've discussed.  I'm
<br>
willing to be convinced, and I consider it possible that the CFAI goal
<br>
architecture could be hybridized with Novamente if a fair amount of work
<br>
were put into this.  But I'm not convinced at the moment that this would be
<br>
a worthwhile pursuit.
<br>
<p><em>&gt; To be precise:  A correctly built Friendly AI is argued to have at least
</em><br>
<em>&gt; that chance of remaining well-disposed toward humanity as would
</em><br>
<em>&gt; be possible
</em><br>
<em>&gt; for any transhuman, upload, social system of uploads, et cetera.
</em><br>
<p>This statement is conceptually (though not rigorously, perhaps) a corollary
<br>
of your claim that the CFAI goal system is intrinsically superior to all
<br>
others.  As I don't accept the former claim, I don't accept this one either.
<br>
<p><p><em>&gt; &gt; This will cause it to seek Friendliness maximization avidly,
</em><br>
<em>&gt; and will also
</em><br>
<em>&gt; &gt; cause it to build an approximation of Yudkowsky’s posited
</em><br>
<em>&gt; hierarchical goal
</em><br>
<em>&gt; &gt; system, by making the system continually seek to represent
</em><br>
<em>&gt; other goals as
</em><br>
<em>&gt; &gt; subgoals (goals inheriting from) MaximizeFriendliness.
</em><br>
<em>&gt;
</em><br>
<em>&gt; No, this is what we humans call &quot;rationalization&quot;.  An AI that seeks to
</em><br>
<em>&gt; rationalize all goals as being Friendly is not an AI that tries to invent
</em><br>
<em>&gt; Friendly goals and avoid unFriendly ones.
</em><br>
<p>Unfortunately you have misunderstood my suggestion.  Perhaps this was
<br>
inevitable given the brevity and out-of-context nature of the snippet I
<br>
posted.
<br>
<p>As I understand it, &quot;rationalization&quot; is when a system has decided what it
<br>
wants to do, and then makes up &quot;false reasons&quot; to justify its desired
<br>
actions.
<br>
<p>In the most common case: a system really is pursuing goal G1, and chooses
<br>
action A because it judges A will lead to satisfaction of G1.  But it thinks
<br>
it *should* be pursuing goal G2 instead.  So it makes up reasons why A will
<br>
lead to satisfaction of G2.  Usually the term &quot;rationalization&quot; is used when
<br>
these reasons are fairly specious.
<br>
<p>What I am talking about is quite different from this.  I am talking about:
<br>
<p>--&gt; Taking a system that in principle has the potential for a goal
<br>
architecture (a graph of connections between GoalNodes) with arbitrary
<br>
connectivity
<br>
<p>--&gt; Encouraging this system to create a goal architecture that has a
<br>
hierarchical graph structure with Friendliness at the top
<br>
<p>I don't see how this is the same as &quot;rationalization.&quot;  Perhaps you mean
<br>
something very different than I do by &quot;rationalization&quot;, however.  Please
<br>
clarify.
<br>
<p>What I am saying is that Novamente's flexible goal architecture can be
<br>
*nudged* into the hierarchical goal architecture that you propose, but
<br>
without making a rigid requirement that the hierarchical goal structure be
<br>
the only possible one.
<br>
<p>I believe that if the system builds the hierarchical goal structure itself,
<br>
then this hierarchical goal structure will coevolve with the rest of the
<br>
mind, and will be cognitively natural and highly functional.  I don't think
<br>
that imposing a fixed hierarchical goal structure and rigidly forcing the
<br>
rest of the mind to adapt to it (the essence of the CFAI proposal, though
<br>
you would word it differently), will have equally successful consequences.
<br>
<p><em>&gt; I claim:  That for any type of error you can describe, I will be able to
</em><br>
<em>&gt; describe why a CFAI-architecture AI will be able to perceive this as an
</em><br>
<em>&gt; &quot;error&quot;.
</em><br>
...
<br>
<em>&gt; It's not an intuition.  It's a system design that was crafted to
</em><br>
<em>&gt; accomplish
</em><br>
<em>&gt; exactly that end.
</em><br>
<p>But your argument in favor of your claim is basically intuition, Eliezer.
<br>
<p>You may have designed your system to achieve this end -- but I do not
<br>
believe your system will in fact achieve this end.
<br>
<p><em>&gt; &gt; However, this would be very, very difficult to ensure, because
</em><br>
<em>&gt; every concept
</em><br>
<em>&gt; &gt; in the mind is defined implicitly in terms of all the other
</em><br>
<em>&gt; concepts in the
</em><br>
<em>&gt; &gt; mind.  The pragmatic significance of a Friendliness FeelingNode
</em><br>
<em>&gt; is defined
</em><br>
<em>&gt; &gt; in terms of a huge number of other nodes and links, and when a Novamente
</em><br>
<em>&gt; &gt; significantly self-modifies it will change many of its nodes and links.
</em><br>
<em>&gt; &gt; Even if the Friendliness FeelingNode always looks the same, its meaning
</em><br>
<em>&gt; &gt; consists in its relations to other things in the mind, and these other
</em><br>
<em>&gt; &gt; things may change.
</em><br>
<em>&gt;
</em><br>
<em>&gt; That's why a probabilistic supergoal is anchored to external referents in
</em><br>
<em>&gt; terms of information provided by the programmers, rather than
</em><br>
<em>&gt; being anchored
</em><br>
<em>&gt; entirely to internal nodes etc.
</em><br>
<p>Yes, we agree on this, although we use slightly different language to
<br>
describe the same thing.
<br>
<p>This point is valid whether one has a rigidly enforced hierarchical goal
<br>
structure as you advocate, or a more flexible goal structure as in the
<br>
current Novamente design.
<br>
<p><em>&gt; If Novamente were programmed simply with a static set of supergoal content
</em><br>
<em>&gt; having only an intensional definition, then yes, it might drift very far
</em><br>
<em>&gt; after a few rounds of self-modification.  This is why you need the full
</em><br>
<em>&gt; Friendliness architecture.
</em><br>
<p>No: this is why you need the system to interact with humans as it grows and
<br>
learns.
<br>
To me, this implies nothing about the need for a rigid hierarchical goal
<br>
structure.
<br>
<p><em>&gt; The rest of the above statement, as far as I can tell, represents two
</em><br>
<em>&gt; understandable but anthropomorphic intuitions:
</em><br>
<em>&gt;
</em><br>
<em>&gt; (a) Maintaining Friendliness requires rewarding Friendliness.  In humans,
</em><br>
<em>&gt; socially moral behavior is often reinforced by rewarding individually
</em><br>
<em>&gt; selfish goals that themselves require no reinforcement.  An AI, however,
</em><br>
<em>&gt; should work the other way around.
</em><br>
<p>I did not say this and do not agree with this.  My statement was rather that
<br>
maintaining a concept of Friendliness close to the human concept of
<br>
Friendliness *may* require continual intense interaction with humans.  This
<br>
says nothing about reward or punishment, which are very simplistic and
<br>
limited modes of interaction anyway.
<br>
<p><em>&gt;
</em><br>
<em>&gt; (b) Novamente will be &quot;socialized&quot; by interaction with other humans.
</em><br>
<em>&gt; However, the ability of humans to be socialized is the result of
</em><br>
<em>&gt; millions of
</em><br>
<em>&gt; years of evolution resulting in a set of adaptations which enable
</em><br>
<em>&gt; socialization.  Without these adaptations present, there is no reason to
</em><br>
<em>&gt; expect socialization to have the same effects.
</em><br>
<p>I did not say and do not believe that socialization will have the same
<br>
importance or effects for Novamentes (or other AI)'s as for humans.
<br>
<p>I think it will serve a related but different purpose for Novamentes than
<br>
for humans -- but still an important purpose.  I can give more details on
<br>
this later, I don't have time tonight!
<br>
<p>I find that sometimes you use the term &quot;anthropomorphic&quot; as a kind of
<br>
generic dismissal of arguments with which you disagree.  There are going to
<br>
be some similarities and some differences between human minds and AI minds.
<br>
Of two positions about AI, the one that emphasizes similarity with humans
<br>
more greatly is not always going to be wrong.  &quot;More anthropomorphic&quot; does
<br>
not intrinsically mean &quot;less true.&quot;  I realize that you do not explicitly
<br>
say or believe anything as simplistic as &quot;more anthropomorphic means less
<br>
true&quot;, but you often give the impression that you feel/think this way.
<br>
<p>Excessive anthropomorphism is a common error in thinking about AI's, of
<br>
which we are both quite aware.
<br>
<p>However, it is also a common error to make overly strong assumptions about
<br>
the *rationality* and &quot;logical&quot; nature of AI's.  I am afraid that your
<br>
intuitions underlying your claims about the CFAI goal architecture sometimes
<br>
fall into this trap.  I feel your intuition doesn't always adequately
<br>
appreciate the self-organizing and unpredictable nature of mind.
<br>
<p>You may say that this is an anthropomorphism, that digital minds won't be
<br>
&quot;self-organizing and unpredictable&quot; to the extent that human minds are.  You
<br>
could be right, but here our intuitions just differ.  My intuition is
<br>
somewhat influenced by a broad study of natural complex systems, and I
<br>
recognize that an AI is not a natural complex system, it's an engineered
<br>
complex system. But I have a feeling that intelligence requires
<br>
self-organizing complexity, which implies unpredictability, which makes the
<br>
kind of rigid goal that structure you propose unworkable, and makes the
<br>
intuition underlying your claims for the CFAI goal architecture seriously
<br>
incomplete.
<br>
<p>****
<br>
<p>Overall, I think the problem with this long-running argument between us is:
<br>
<p>1) You don't really know how the Novamente goal system works because you
<br>
don't know the whole Novamente design
<br>
<p>2) I don't really know how your CFAI system would work in the context of a
<br>
complete AI design, because I don't know any AI design that incorporates
<br>
CFAI (and my understanding is, you don't have one yet, but you're working on
<br>
it).
<br>
<p>I can solve problem 1 by giving you detailed information about Novamente
<br>
(privately, off list), though it will take you  many many days of reading
<br>
and asking questions to really get it (it's just a lot of information).
<br>
<p>Problem 2 however will only be solved by you completing your current AI
<br>
design task!!
<br>
<p>I don't mean to say that I'll only accept your claims about the CFAI goal
<br>
architecture based on mathematical or empirical proof.  I am willing to be
<br>
convinced intuitively by verbal, conceptual arguments that make sense to me.
<br>
But so far, your published (online) arguments don't, although I find them
<br>
stimulating and interesting.
<br>
<p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3153.html">Mitch Howe: "Re: Singularity Media (was: Wednesday's chatlog)"</a>
<li><strong>Previous message:</strong> <a href="3151.html">Ben Goertzel: "RE: Wednesday's chatlog"</a>
<li><strong>In reply to:</strong> <a href="3148.html">Eliezer S. Yudkowsky: "Re: Novamente goal system"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3155.html">Eliezer S. Yudkowsky: "Re: Novamente goal system"</a>
<li><strong>Reply:</strong> <a href="3155.html">Eliezer S. Yudkowsky: "Re: Novamente goal system"</a>
<li><strong>Reply:</strong> <a href="3160.html">Eliezer S. Yudkowsky: "Webmind Inc. as social process"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3152">[ date ]</a>
<a href="index.html#3152">[ thread ]</a>
<a href="subject.html#3152">[ subject ]</a>
<a href="author.html#3152">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
