<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Novamente goal system</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Novamente goal system">
<meta name="Date" content="2002-03-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Novamente goal system</h1>
<!-- received="Sun Mar 10 11:11:26 2002" -->
<!-- isoreceived="20020310181126" -->
<!-- sent="Sun, 10 Mar 2002 10:56:07 -0500" -->
<!-- isosent="20020310155607" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Novamente goal system" -->
<!-- id="3C8B8217.4E6C808A@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJKELICEAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Novamente%20goal%20system"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Mar 10 2002 - 08:56:07 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3156.html">Ben Goertzel: "RE: Novamente goal system"</a>
<li><strong>Previous message:</strong> <a href="3154.html">Ben Goertzel: "RE: Singularity Media (was: Wednesday's chatlog)"</a>
<li><strong>In reply to:</strong> <a href="3152.html">Ben Goertzel: "RE: Novamente goal system"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3156.html">Ben Goertzel: "RE: Novamente goal system"</a>
<li><strong>Reply:</strong> <a href="3156.html">Ben Goertzel: "RE: Novamente goal system"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3155">[ date ]</a>
<a href="index.html#3155">[ thread ]</a>
<a href="subject.html#3155">[ subject ]</a>
<a href="author.html#3155">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &gt;  Just to make sure
</em><br>
<em>&gt; &gt; we're all on the same wavelength, would you care to describe briefly what
</em><br>
<em>&gt; &gt; you think a transhuman Novamente would be like?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This first (mildly) transhuman Novamente will communicate with us in
</em><br>
<em>&gt; comprehensible and fluent but not quite human-like English.  It will be a
</em><br>
<em>&gt; hell of a math and CS whiz, able to predict financial and economic trends
</em><br>
<em>&gt; better than us, able to read scientific articles easily, and able to read
</em><br>
<em>&gt; human literary products but not always intuitively &quot;getting&quot; them.  It will
</em><br>
<em>&gt; be very interested in intense interactions with human scientists on topics
</em><br>
<em>&gt; of its expertise and interest, and in collaboratively working with them to
</em><br>
<em>&gt; improve its own intelligence and solving their problems.  It will be
</em><br>
<em>&gt; qualitatively smarter than us, in the same sense that you or I are
</em><br>
<em>&gt; qualitatively smarter than the average human -- but no so much smarter as to
</em><br>
<em>&gt; have no use for us (yet)....
</em><br>
<em>&gt; 
</em><br>
<em>&gt; How long this phase will last, before mild transhumanity gives rise to
</em><br>
<em>&gt; full-on Singularity, I am certainly not sure.
</em><br>
<p>Okay.  I can't say I agree with the theory of takeoff dynamics, but at least
<br>
your semantics seem to show that you're talking about a mind capable of
<br>
independently planning long-term actions in the pursuit of real-world goals,
<br>
rather than a powerful tool.
<br>
<p>Actually, as described, this is pretty much what I would consider
<br>
&quot;human-equivalent AI&quot;, for which the term &quot;transhumanity&quot; is not really
<br>
appropriate.  I don't think I'm halfway to transhumanity, so an AI twice as
<br>
many sigma from the mean is not all the way there.  Maybe you should say
<br>
that Novamente-the-project is striving for human-equivalence; either that,
<br>
or define what you think a *really* transhuman Novamente would be like...
<br>
<p>Incidentally, this thing of having the project and the AI having the same
<br>
name is darned inconvenient.  Over on this end, the group is SIAI, the
<br>
architecture is GISAI, and the AI itself will be named Ai-something.
<br>
<p><em>&gt; &gt; I claim:  There is no important sense in which a cleanly causal,
</em><br>
<em>&gt; &gt; Friendliness-topped goal system is inferior to any alternate system of
</em><br>
<em>&gt; &gt; goals.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I claim:  The CFAI goal architecture is directly and immediately
</em><br>
<em>&gt; &gt; superior to
</em><br>
<em>&gt; &gt; the various widely differing formalisms that were described to me by
</em><br>
<em>&gt; &gt; different parties, including Ben Goertzel, as being &quot;Webmind's goal
</em><br>
<em>&gt; &gt; architecture&quot;.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I claim:  That for any important Novamente behavior, I will be able to
</em><br>
<em>&gt; &gt; describe how that behavior can be implemented under the CFAI architecture,
</em><br>
<em>&gt; &gt; without significant loss of elegance or significant additional computing
</em><br>
<em>&gt; &gt; power.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; These are indeed claims, but as far as I can tell they are not backed up by
</em><br>
<em>&gt; anything except your intuition.
</em><br>
<p>Claims are meant to be tested.  So far the claims have been tested on
<br>
several occasions; you (and, when I was at Webmind, a few other folk) named
<br>
various things that you didn't believe could possibly fit into the CFAI goal
<br>
system, such as &quot;curiosity&quot;, and I explained how curiosity indeed fit into
<br>
the architecture.  That was an example of passing a test for the first claim
<br>
and third claim.  If you have anything else that you believe cannot be
<br>
implemented under CFAI, you can name it and thereby test the claim again.
<br>
<p>Similarly, I've explained how a goal system based on predicted-to versus
<br>
associated-with seeks out a deeper class of regularities in reality, roughly
<br>
the &quot;useful&quot; regularities rather than the &quot;surface&quot; regularities.  Two
<br>
specific examples of this are:  (1) CFAI will distinguish between genuine
<br>
causal relations and implication-plus-temporal-precedence, since only the
<br>
former can be used to manipulate reality; Judea Pearl would call this
<br>
&quot;severance&quot;, while I would call this &quot;testing for hidden third causes&quot;.  I
<br>
don't know if Novamente is doing this now, but AFAICT Webmind's
<br>
documentation on the causality/goal architecture didn't show any way to
<br>
distinguish between the two.  (2) CFAI will distinguish contextual
<br>
information that affects whether something is desirable; specifically,
<br>
because of the prediction formalism, it will seek out factors that tend to
<br>
interfere with or enable A leading to B, where B is the goal state.  I again
<br>
don't know about Novamente, but most of the (varied) systems that were
<br>
described to me as being Webmind might be capable of distinguishing degrees
<br>
of association, but would not specialize on degrees of useful association.
<br>
<p>To give a concrete example, Webmind (as I understood it) would, on seeing
<br>
that rooster calls preceded sunrise, where sunrise is desirable, would begin
<br>
to like rooster calls and would start playing them over the speakers.  CFAI
<br>
would try playing a rooster call, notice that it didn't work, and
<br>
hypothesize that there was a common third cause for sunrise and rooster
<br>
calls which temporally preceded both (this is actually correct; dawn, I
<br>
assume, is the cause of rooster calls, and Earth's rotation is the common
<br>
cause of dawn and sunrise); after this rooster calls would cease to be
<br>
desirable since they were no longer predicted to lead to sunrise.  Maybe
<br>
Webmind can be hacked to do this the right way; given the social process
<br>
that developed Webmind, it certainly wouldn't be surprising to find that at
<br>
least one of the researchers thought up this particular trick.  My point is
<br>
that in CFAI the Right Thing is directly, naturally, and elegantly emergent,
<br>
where it goes along with other things as well, such as the Right Thing for
<br>
positive and negative reinforcement, as summarized in &quot;Features of Friendly
<br>
AI&quot;.
<br>
<p>So that's what I would offer as a demonstration of the second claim.
<br>
<p><em>&gt; I am certainly not one to discount the value of intuition.  The claim that
</em><br>
<em>&gt; Novamente will suffice for a seed AI is largely based on the intuition of
</em><br>
<em>&gt; myself and my collaborators.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; However, my intuition happens to differ from yours, as regards the ultimate
</em><br>
<em>&gt; superiority of your CFAI goal architecture.
</em><br>
<p>Well, to the extent that you understand thinking better than average, you
<br>
should be better than average at verbalizing your intuitions and the causes
<br>
behind your intuitions.  
<br>
<p><em>&gt; I am not at all sure there is *any* goal architecture that is &quot;ultimate and
</em><br>
<em>&gt; superior&quot; in the sense that you are claiming for yours.
</em><br>
<p>Well, if it's superior or equal to all other goal systems that are proposed
<br>
up until the point of Singularity, including the human one, that's good
<br>
enough for me.
<br>
<p><em>&gt; And I say this with what I think is a fairly decent understanding of the
</em><br>
<em>&gt; CFAI goal architecture.  I've read what you've written about it, talked to
</em><br>
<em>&gt; you about it, and thought about it a bit.  I've also read, talked about, and
</em><br>
<em>&gt; thought about your views on closely related issues such as causality.
</em><br>
<p>Well, it's an odd thing, and not what I would have expected, but I've
<br>
noticed that for these detailed, complex issues - which one would think
<br>
would be best discussed in an extended technical paper - there is no good
<br>
substitute for realtime interaction.  When I was at Webmind I could
<br>
generally convince someone of how cleanly causal, Friendliness-topped goal
<br>
systems would work, as long as I could interact with them in person.  Of
<br>
course I never got as far as any of the goal-system-stabilizing stuff.
<br>
<p><em>&gt; &gt; No, this is what we humans call &quot;rationalization&quot;.  An AI that seeks to
</em><br>
<em>&gt; &gt; rationalize all goals as being Friendly is not an AI that tries to invent
</em><br>
<em>&gt; &gt; Friendly goals and avoid unFriendly ones.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In the most common case: a system really is pursuing goal G1, and chooses
</em><br>
<em>&gt; action A because it judges A will lead to satisfaction of G1.  But it thinks
</em><br>
<em>&gt; it *should* be pursuing goal G2 instead.  So it makes up reasons why A will
</em><br>
<em>&gt; lead to satisfaction of G2.  Usually the term &quot;rationalization&quot; is used when
</em><br>
<em>&gt; these reasons are fairly specious.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What I am talking about is quite different from this.  I am talking about:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; --&gt; Taking a system that in principle has the potential for a goal
</em><br>
<em>&gt; architecture (a graph of connections between GoalNodes) with arbitrary
</em><br>
<em>&gt; connectivity
</em><br>
<em>&gt; 
</em><br>
<em>&gt; --&gt; Encouraging this system to create a goal architecture that has a
</em><br>
<em>&gt; hierarchical graph structure with Friendliness at the top
</em><br>
<p>Actually it's not a hierarchy, but a directed acyclic network, just like the
<br>
network for &quot;is predicted to lead to X&quot;, for any X where the separate
<br>
instances of X are considered as distinct space-time events.  (So that X1
<br>
can lead to X2 without the overall network being cyclic.)
<br>
<p>Anyway, the property of a &quot;'hierarchical' graph structure with Friendliness
<br>
at the top&quot; is a necessary but not a sufficient condition for a Friendliness
<br>
architecture.  The way in which that graph structure arises is also
<br>
relevant.  The links need to be predictive ones, not fully general
<br>
associative ones.  Similarly, the class of subnodes that leads to
<br>
Friendliness has to be produced by a method whose overall &quot;shape&quot; is
<br>
optimized to produce a class of subnodes whose &quot;shape&quot; is those things and
<br>
all those things that are thought to lead to Friendliness under bounded
<br>
rationality at a given time; if you take a small subset of these nodes,
<br>
produced by some method with an aFriendly shape, then that still isn't the
<br>
shape of a true Friendly AI.
<br>
<p>Heh.  Now *I* want to start tossing equations around.  Anyway, what I want
<br>
to say is that the class of nodes that can be linked to Friendliness is not
<br>
the class of nodes that are predictively linked to Friendliness; and a small
<br>
subset of the class of nodes that are predictively linked to Friendliness
<br>
may have a very different &quot;shape&quot; from the entire class.  If you violate
<br>
both criteria simultaneously, then I can see a small subset of the nodes
<br>
linkable to Friendliness by generic association as being very unFriendly
<br>
indeed.
<br>
<p><em>&gt; What I am saying is that Novamente's flexible goal architecture can be
</em><br>
<em>&gt; *nudged* into the hierarchical goal architecture that you propose, but
</em><br>
<em>&gt; without making a rigid requirement that the hierarchical goal structure be
</em><br>
<em>&gt; the only possible one.
</em><br>
<p>Right; what I'm saying is that nudging Novamente's architecture into
<br>
hierarchicality is one thing, and nudging it into Friendliness is quite
<br>
another.  Incidentally, if you think that the CFAI architecture is more
<br>
&quot;rigid&quot; than Novamente in any real sense, please name the resulting
<br>
disadvantage and I will explain how it doesn't work that way.
<br>
<p><em>&gt; I believe that if the system builds the hierarchical goal structure itself,
</em><br>
<em>&gt; then this hierarchical goal structure will coevolve with the rest of the
</em><br>
<em>&gt; mind, and will be cognitively natural and highly functional.  I don't think
</em><br>
<em>&gt; that imposing a fixed hierarchical goal structure and rigidly forcing the
</em><br>
<em>&gt; rest of the mind to adapt to it (the essence of the CFAI proposal, though
</em><br>
<em>&gt; you would word it differently), will have equally successful consequences.
</em><br>
<p>I rather hope that, after seeing how various advantages arise and how
<br>
various disadvantages fail to arise under CFAI, you will come to see that
<br>
the CFAI architecture is the natural description of a goal system.  CFAI is
<br>
a help, not a hindrance; it contributes usefully to the intelligence of the
<br>
system.  A directed acyclic network is not *forced* upon the goal system; it
<br>
is the natural shape of the goal system, and violating this shape results in
<br>
distortion of what we would see as the natural/normative behavior.
<br>
<p><em>&gt; I did not say this and do not agree with this.  My statement was rather that
</em><br>
<em>&gt; maintaining a concept of Friendliness close to the human concept of
</em><br>
<em>&gt; Friendliness *may* require continual intense interaction with humans.  This
</em><br>
<em>&gt; says nothing about reward or punishment, which are very simplistic and
</em><br>
<em>&gt; limited modes of interaction anyway.
</em><br>
<p>I think that having a seed AI will require continual intense interaction
<br>
with humans during the early days, until the AI metaphorically learns how to
<br>
ride that bike.  &quot;But, Daddy, why is it dangerous to clear bit five
<br>
throughout memory?&quot;, etc.  With respect to Friendliness, I naturally take an
<br>
interest in making sure that the threshold for working-okayness is set as
<br>
low as possible, even if it works better with more human interaction.  What
<br>
is the system property that requires continual intense interaction to
<br>
enforce, and how does the continual intense interaction enforce it?  Or
<br>
alternatively, what is it that requires continual intense informational
<br>
inputs from humans in order to work right?
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3156.html">Ben Goertzel: "RE: Novamente goal system"</a>
<li><strong>Previous message:</strong> <a href="3154.html">Ben Goertzel: "RE: Singularity Media (was: Wednesday's chatlog)"</a>
<li><strong>In reply to:</strong> <a href="3152.html">Ben Goertzel: "RE: Novamente goal system"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3156.html">Ben Goertzel: "RE: Novamente goal system"</a>
<li><strong>Reply:</strong> <a href="3156.html">Ben Goertzel: "RE: Novamente goal system"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3155">[ date ]</a>
<a href="index.html#3155">[ thread ]</a>
<a href="subject.html#3155">[ subject ]</a>
<a href="author.html#3155">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
