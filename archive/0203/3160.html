<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Webmind Inc. as social process</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Webmind Inc. as social process">
<meta name="Date" content="2002-03-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Webmind Inc. as social process</h1>
<!-- received="Sun Mar 10 20:41:26 2002" -->
<!-- isoreceived="20020311034126" -->
<!-- sent="Sun, 10 Mar 2002 20:33:00 -0500" -->
<!-- isosent="20020311013300" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Webmind Inc. as social process" -->
<!-- id="3C8C094C.8378DC24@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJKELICEAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Webmind%20Inc.%20as%20social%20process"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Mar 10 2002 - 18:33:00 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3161.html">mike99: "RE: Wednesday's chatlog"</a>
<li><strong>Previous message:</strong> <a href="3159.html">Damien Broderick: "Re: Wednesday's chatlog"</a>
<li><strong>In reply to:</strong> <a href="3152.html">Ben Goertzel: "RE: Novamente goal system"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3166.html">w d: "RE:Novamente goal system"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3160">[ date ]</a>
<a href="index.html#3160">[ thread ]</a>
<a href="subject.html#3160">[ subject ]</a>
<a href="author.html#3160">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; These are indeed claims, but as far as I can tell they are not backed up by
</em><br>
<em>&gt; anything except your intuition.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I am certainly not one to discount the value of intuition.  The claim that
</em><br>
<em>&gt; Novamente will suffice for a seed AI is largely based on the intuition of
</em><br>
<em>&gt; myself and my collaborators.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; However, my intuition happens to differ from yours, as regards the ultimate
</em><br>
<em>&gt; superiority of your CFAI goal architecture.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I am not at all sure there is *any* goal architecture that is &quot;ultimate and
</em><br>
<em>&gt; superior&quot; in the sense that you are claiming for yours.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And I say this with what I think is a fairly decent understanding of the
</em><br>
<em>&gt; CFAI goal architecture.  I've read what you've written about it, talked to
</em><br>
<em>&gt; you about it, and thought about it a bit.  I've also read, talked about, and
</em><br>
<em>&gt; thought about your views on closely related issues such as causality.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Sometimes, when the data (mathematical or empirical) is limited, there is
</em><br>
<em>&gt; just no way to resolve a disagreement of intuition.  One simply has to
</em><br>
<em>&gt; gather more data (via experimentation (in this case computational) or
</em><br>
<em>&gt; mathematical proof).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't think I have a big emotional stake in this issue, Eliezer.  I never
</em><br>
<em>&gt; have minded integrating my own thinking with that of others.  In fact I did
</em><br>
<em>&gt; a bit too much of this during the Webmind period, as we've discussed.  I'm
</em><br>
<em>&gt; willing to be convinced, and I consider it possible that the CFAI goal
</em><br>
<em>&gt; architecture could be hybridized with Novamente if a fair amount of work
</em><br>
<em>&gt; were put into this.  But I'm not convinced at the moment that this would be
</em><br>
<em>&gt; a worthwhile pursuit.
</em><br>
<p>Figuring out how to convince you on these issues is indeed a pretty problem
<br>
for me.  Well, the worst that can happen is that I don't figure out how to
<br>
convince you and the Earth is destroyed, so no pressure - right?
<br>
<p>Webmind Inc. as a social process - actually, I think I'll hereafter refer to
<br>
it as Intelligenesis, to avoid confusion with the AI architecture.  Anyway,
<br>
Intelligenesis interests me.  I know in some detail how evolution walked up
<br>
the incremental path to intelligence, and I know in some detail how AI
<br>
research as commonly conducted managed to repeatedly fail to create
<br>
intelligence.  One of the many contributing factors toward the failure of
<br>
AI, as commonly understood, is theoretical straightjackets so rigid that it
<br>
isn't possible for the AI programmers to be creative.  Webmind's agent
<br>
architecture - and your own beliefs, on starting out, about magicians
<br>
transforming magicians - permitted Intelligenesis to merge multiple
<br>
mostly-wrong theories of intelligence into a common ground where the
<br>
occasional bright idea from one of the researchers could survive and,
<br>
perhaps, reproduce.
<br>
<p>Considered from an evolutionary standpoint, Intelligenesis broke out from
<br>
the one-AI one-theory straightjacket, which had previously held for *general
<br>
intelligence* projects (e.g. Cyc) even if it was occasionally violated by
<br>
more pragmatic robotics architectures and so on.  Correspondingly, Webmind
<br>
broke out of the AI-as-single-algorithm straightjacket, not so much because
<br>
any individual researcher had a picture of AI as a supersystem, but because
<br>
all the different researchers thought that AI was composed of different
<br>
systems.  In combination, all the ideas added up to a much bigger idea than
<br>
any previous single AI researcher had ever had for general intelligence.
<br>
<p>Of course, I am just extrapolating here based on what I know about
<br>
intelligence and Intelligenesis.  If I know you, Ben, right now you're
<br>
thinking of some additional properties of Intelligenesis's climb towards
<br>
theoretical complexity and Webmind's climb toward supersystemness which I
<br>
didn't mention and which must therefore be explained to me.  Please bear in
<br>
mind that, as I visualize Intelligenesis, there is indeed a great deal which
<br>
went on that I haven't mentioned here.  I can guess that you encountered all
<br>
sorts of difficulties which had to be resolved in the course of integrating
<br>
everyone's small theories into the big theory that slowly emerged; I can
<br>
guess that it was hard to figure out which parts went into the big theory; I
<br>
can guess that some parts of the big theory survive unchanged from the
<br>
theories that people brought in with them; I can guess that you saw some of
<br>
the shape of the big theory in advance, but that there were still surprises;
<br>
I can guess that some researchers had bigger theories than others as they
<br>
signed on with Intelligenesis; I can guess that some people worked on the
<br>
concepts behind their theories for years before entering Intelligenesis, so
<br>
that the component theories aren't &quot;small&quot; in any absolute sense; I can
<br>
guess that some of the people brought with them theories that were more
<br>
complex than what you consider to be the &quot;failed past simplicity of AI&quot;, so
<br>
it wasn't just different classical theories breeding; I can guess that you
<br>
tried to integrate some theories but that they just didn't work; and so on.
<br>
<p>Some of these guesses may be wrong, of course, since I don't know as much
<br>
about the history of Intelligenesis as I do about the evolution of human
<br>
intelligence.  If they're all wrong, though, then I really would be
<br>
surprised, but I'd also give Novamente a much smaller chance of going
<br>
supernova, since from my perspective the reason why Webmind/Novamente gets
<br>
more credence than any other random AI project is that you built it using
<br>
more than one idea.
<br>
<p>The problem is that, while I can make general guesses like those given
<br>
above, the question of how much you *did* learn, which *specific* ideas and
<br>
lessons you accumulated, is an underconstrained problem.  The question of
<br>
what you *know* you've learned is even more underconstrained.  Otherwise, it
<br>
would be very easy to demonstrate to you that I know something unusual about
<br>
intelligence; I could recite the most important and least-known lessons
<br>
about AI methodology that I would expect you to have learned from
<br>
Intelligenesis and Webmind.  But this, unfortunately, is a transhuman party
<br>
trick, and I am not a transhuman.
<br>
<p>As it is, the most I can offer to do is solve problems or answer questions. 
<br>
Unfortunately, so far it doesn't appear possible to do this over the Web or
<br>
email, though I've had much more luck conveying the deep ideas through
<br>
realtime interaction (and not just at Webmind, either).  By the way, the
<br>
problem is that even with realtime interaction, Webmindfolk were possible to
<br>
convince, but they didn't *stay* convinced.  They'd talk to someone else
<br>
about causality or goals and the next day they'd have a different idea.  It
<br>
was pretty frustrating; I wanted to get everyone in the same room at the
<br>
same time so that I could deal with all the objections simultaneously, but
<br>
that wasn't in the cards.  Of course I expect my visit to Webmind played a
<br>
larger role in my week than it did yours, and hence looms larger in my
<br>
memory.
<br>
<p>(And now for a sharp segue.)
<br>
<p>In the old days I would have been overjoyed to see any AI project making any
<br>
progress at all.  Now any progress that exceeds progress in thinking about
<br>
Friendliness represents an urgent problem to be corrected - by improving the
<br>
understanding of Friendliness, of course.
<br>
<p>The two most important questions, from my perspective, are:  (1):  Now that
<br>
you're working with the Novamente approach, did you learn from
<br>
Intelligenesis *how* to build supersystems, or did you just learn about *a*
<br>
supersystem that will become a new cul-de-sac for you?  (2):  How much
<br>
intelligence does it take for a seed AI takeoff anyway?  The latter one in
<br>
particular has too many internal variables for me to guess it.  It could be
<br>
anywhere from human-level intelligence to just above Eurisko.
<br>
<p>So, I've got to explain this Friendliness stuff as early as possible.  Your
<br>
current estimation of me appears to be as someone who'd make a nice
<br>
researcher for Intelligenesis, at least if he could learn to just build his
<br>
own Friendliness system and see what it contributes to intelligence as a
<br>
whole, instead of insisting that everyone do things his way.  This is very
<br>
kind of you, and I do appreciate it.  But the thing is, I'm not *supposed*
<br>
to be a typical Intelligenesis researcher.  I'm supposed to be the guy that
<br>
takes the project over the &quot;hump&quot; that's defeated all AI projects up to this
<br>
point.  I doubt that any single Intelligenesis programmer could have thought
<br>
up Webmind on their own; though I suppose the ones that believed in the
<br>
right kind of agent architecture, and who had the right talent for
<br>
integrating other people's ideas, could have filled the Ben Goertzel role
<br>
and built another Intelligenesis.  Now of course I do plan to call upon the
<br>
talents of others in building SIAI's AI, but there's a difference between
<br>
calling on the talents of others, and building a plan that doesn't work
<br>
unless someone else swoops in and solves a problem you don't currently know
<br>
how to solve.  My job is to parse up the problem into chunks that can be
<br>
solved by sufficiently creative individuals, and to make sure that there's
<br>
full scope for individual creativity in any specific area while
<br>
simultaneously preserving the general architecture that makes the levels of
<br>
organization add up.  If it turns out that I have help on the deep problems,
<br>
then great!  But I'm not relying on it.
<br>
<p>Now, of course I realize that you haven't seen me in action enough to know
<br>
that I'm any smarter than a run-of-the-mill AI researcher - who of course
<br>
are bright people in their own right; just not, so far, smart enough to
<br>
crack the deep problems of AI.  It's not a planetary disaster if you
<br>
continue to assume that I'm of O(Intelligenesis-personnel) smartness, or
<br>
what the hell, even a bit under, given that you probably consider OIPS to be
<br>
a high level of intelligence that I haven't quite demonstrated my fitness
<br>
for yet.  But it will make it harder for me to get past the point of
<br>
convincing you that I also see the things you see that create the deep
<br>
questions of Friendly AI, so that I can start showing you the accompanying
<br>
answers.
<br>
<p>You've already demonstrated your ability to acquire ideas from people whom
<br>
you regard as OIPS.  So my current take is to go on trying to show how
<br>
Friendliness works.  If email just isn't enough, maybe I'll fly over for a
<br>
week so we can finally work this stuff out between us; I doubt you'd want
<br>
SIAI to build an UFAI either.  Otherwise, I guess SIAI will just have to
<br>
finish building our seed AI first.  This is actually what I expect to be the
<br>
case regardless &lt;smile&gt;; it's just that I am obliged not to rely on that, if
<br>
at all possible.  (Not that I'd mind Novamente beating us to it - *if* I
<br>
could be confident that you had someone around who fully understood
<br>
Friendliness.  I am not going to be the only one who raises this question if
<br>
you go on designating transhumanity as your target, please note.)
<br>
<p><em>&gt;From my perspective, the basic current problem is not so much the degree to
</em><br>
which you think I'm &lt;OIPS, &gt;OIPS, or whatever, but rather one particular
<br>
case of how you generalize your experience with Intelligenesis; you don't
<br>
trust AI researchers' arguments until you see them implemented in practice. 
<br>
Of course this is an obvious lesson to learn; I wouldn't trust an AI
<br>
researcher's arguments either, because they are, broadly speaking, blatantly
<br>
wrong.  And I would expect that you've heard a lot of rationalizations of
<br>
flawed ideas in the course of the Intelligenesis social process.  But it
<br>
does represent a problem to me if you've learned to distrust all verbal
<br>
arguments and rely on either working code or, failing that, your intuitive
<br>
perceptions.  Your intuitive perceptions may be more reliable in that they
<br>
are not as subject to manipulation and rationalization by Intelligenesis
<br>
researcher-units.  But &quot;more reliable&quot; is not &quot;reliable&quot;; intuitive
<br>
perceptions can be wrong too.  Even working code can be wrong, for that
<br>
matter.  For me it means that I have to alter your intuitive perceptions. 
<br>
That's a lot of work over and above what it would take to initialize the
<br>
intuitive perceptions of someone who hasn't tried to solve the problem yet,
<br>
or to show the flaw in someone's intuitions to a third party who shares a
<br>
common knowledge base.  You're right that it would help if you'd get around
<br>
to publishing that paper on the Novamente design; as it is, &quot;where Ben's
<br>
intuitions come from&quot; is rather underconstrained, and I'm working blind.
<br>
<p><em>&gt; Overall, I think the problem with this long-running argument between us is:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1) You don't really know how the Novamente goal system works because you
</em><br>
<em>&gt; don't know the whole Novamente design
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 2) I don't really know how your CFAI system would work in the context of a
</em><br>
<em>&gt; complete AI design, because I don't know any AI design that incorporates
</em><br>
<em>&gt; CFAI (and my understanding is, you don't have one yet, but you're working on
</em><br>
<em>&gt; it).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I can solve problem 1 by giving you detailed information about Novamente
</em><br>
<em>&gt; (privately, off list), though it will take you  many many days of reading
</em><br>
<em>&gt; and asking questions to really get it (it's just a lot of information).
</em><br>
<p>I'll take it.  Please send.
<br>
<p><em>&gt; Problem 2 however will only be solved by you completing your current AI
</em><br>
<em>&gt; design task!!
</em><br>
<p>Yes, that has always been the traditional test of We Who Claim To Understand
<br>
Intelligence.  But it will take time, as is known to both of us.
<br>
<p><em>&gt; I don't mean to say that I'll only accept your claims about the CFAI goal
</em><br>
<em>&gt; architecture based on mathematical or empirical proof.  I am willing to be
</em><br>
<em>&gt; convinced intuitively by verbal, conceptual arguments that make sense to me.
</em><br>
<p>Fair enough.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3161.html">mike99: "RE: Wednesday's chatlog"</a>
<li><strong>Previous message:</strong> <a href="3159.html">Damien Broderick: "Re: Wednesday's chatlog"</a>
<li><strong>In reply to:</strong> <a href="3152.html">Ben Goertzel: "RE: Novamente goal system"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3166.html">w d: "RE:Novamente goal system"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3160">[ date ]</a>
<a href="index.html#3160">[ thread ]</a>
<a href="subject.html#3160">[ subject ]</a>
<a href="author.html#3160">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
