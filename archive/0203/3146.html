<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Novamente goal system</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="Novamente goal system">
<meta name="Date" content="2002-03-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Novamente goal system</h1>
<!-- received="Sat Mar 09 02:16:52 2002" -->
<!-- isoreceived="20020309091652" -->
<!-- sent="Fri, 8 Mar 2002 23:57:28 -0700" -->
<!-- isosent="20020309065728" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="Novamente goal system" -->
<!-- id="LAEGJLOGJIOELPNIOOAJMEKNCEAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=Re:%20Novamente%20goal%20system"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Fri Mar 08 2002 - 23:57:28 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3147.html">Eugene Leitl: "Re: The &quot;AI Box&quot; experiment"</a>
<li><strong>Previous message:</strong> <a href="3145.html">Fabio Mascarenhas: "The &quot;AI Box&quot; experiment"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3148.html">Eliezer S. Yudkowsky: "Re: Novamente goal system"</a>
<li><strong>Reply:</strong> <a href="3148.html">Eliezer S. Yudkowsky: "Re: Novamente goal system"</a>
<li><strong>Maybe reply:</strong> <a href="3166.html">w d: "RE:Novamente goal system"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3146">[ date ]</a>
<a href="index.html#3146">[ thread ]</a>
<a href="subject.html#3146">[ subject ]</a>
<a href="author.html#3146">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi,
<br>
<p>Eliezer asked me to write something on Novamente's goal system.  I still
<br>
haven't done that, because it seems so hard to write about this part of the
<br>
system in isolation, in a comprehensible way.
<br>
<p>However, while editing the &quot;Psyche&quot; chapter in our in-process Novamente book
<br>
(and doing exciting stuff like making the notation consistent with other
<br>
chapters ;-p), I indulged myself and added a brief section on Friendly AI.
<br>
Here is the first draft of this section (not at all polished; I just typed
<br>
it in moments ago...).
<br>
<p>There is a little Novamente terminology here that isn't defined, but the
<br>
basic gist should be evident.
<br>
<p>-- Ben
<br>
<p><p><p>*************
<br>
Friendly AI
<br>
<p>In this section we will briefly explore some of the more futuristic aspects
<br>
of Novamente’s Feelings and Goals component.  These aspects of Novamente are
<br>
always important, but in the future when a Novamente system intellectually
<br>
outpaces its human mentors, they will become yet more critical.
<br>
<p>The MaximizeFriendliness GoalNode is very close to the concept of
<br>
“Friendliness,” which Eliezer Yudkowsky has discussed extensively in his
<br>
treatise Creating a Friendly AI (Yudkowsky, 2001:
<br>
<a href="http://intelligence.org/CFAI/">http://intelligence.org/CFAI/</a>).   Yudkowsky believes that an AI should be
<br>
designed with an hierarchical goal system that has Friendliness at the top.
<br>
In this scheme, the AI pursues other goals only to the extent that it
<br>
believes (through experience or instruction) that these other goals are
<br>
helpful for achieving its over-goal of Friendliness.
<br>
<p>Yudkowsky’s motivation for this proposed design is long-term thinking about
<br>
the possible properties of a progressively self-modifying AI with superhuman
<br>
intelligence.  His worry (a very reasonable one, from a big-picture
<br>
perspective) is that one day an AI will transform itself to be so
<br>
intelligent that it cannot be controlled by humans – and at this point, it
<br>
will be important that the AI values Friendliness.  Of course, if an AI is
<br>
self-modifying itself into greater and greater levels of intelligence, there
<br>
’s no guarantee that Friendliness will be preserved through these successive
<br>
self-modifications.  His argument, however, is that if Friendliness is the
<br>
chief goal, then self-modifications will be done with the goal of increasing
<br>
Friendliness, and hence will be highly likely to be Friendly.
<br>
<p>Unlike the hypothetical Friendly AI systems that Yudkowsky has discussed,
<br>
Novamente does not have an intrinsically hierarchical goal system.  However,
<br>
the basic effect that Yudkowsky describes – MaximizeFriendliness supervening
<br>
over other goals -- can be achieved within Novamente’s goal system through
<br>
appropriate parameter settings.   Basically all one has to do is
<br>
<p>* constantly pump activation to the MaximizeFriendliness GoalNode.
<br>
* encourage the formation of links of the form &quot;InheritanceLink G
<br>
MaximizeFriendliness&quot;, where G is another GoalNode
<br>
<p>This will cause it to seek Friendliness maximization avidly, and will also
<br>
cause it to build an approximation of Yudkowsky’s posited hierarchical goal
<br>
system, by making the system continually seek to represent other goals as
<br>
subgoals (goals inheriting from) MaximizeFriendliness.
<br>
<p>However, even if one enforces a Friendliness-centric goal system in this
<br>
way, it is not clear that the Friendliness-preserving evolutionary path that
<br>
Yudkowsky envisions will actually take place.  There is a major weak point
<br>
to this argument, which has to do with the stability of the Friendliness
<br>
goal under self-modifications.
<br>
<p>Suppose our AI modifies itself with the goal of maintaining Friendliness.
<br>
But suppose it makes a small error, and in its self-modificatory activity,
<br>
it actually makes itself a little less able to judge what is Friendly and
<br>
what isn’t.   It’s almost inevitable that this kind of error will occur at
<br>
some point.  The system will then modify itself again, the next time around,
<br>
with this less accurate assessment of the nature of Friendliness as its
<br>
goal.  The question is, what is the chance that this kind of dynamic leads
<br>
to a decreasing amount of Friendliness, due to an increasingly erroneous
<br>
notion of Friendliness.
<br>
<p>One may also put this argument slightly differently: without speaking of
<br>
error, what if the AI’s notion of Friendliness slowly drifts through
<br>
successful self-modifications?  Yudkowsky’s intuition seems to be that when
<br>
an AI has become intelligent enough to self-modify in a sophisticated
<br>
goal-directed way, it will be sufficiently free of inference errors that its
<br>
notion of Friendliness won’t drift or degenerate.  Our intuition is not so
<br>
clear on this point.
<br>
<p>It might seem that one strategy to make Yudkowsky’s idea workable would be
<br>
give the system another specific goal, beyond simple Friendliness: the goal
<br>
of not ever letting its concept of Friendliness change substantially.
<br>
However, this would be very, very difficult to ensure, because every concept
<br>
in the mind is defined implicitly in terms of all the other concepts in the
<br>
mind.  The pragmatic significance of a Friendliness FeelingNode is defined
<br>
in terms of a huge number of other nodes and links, and when a Novamente
<br>
significantly self-modifies it will change many of its nodes and links.
<br>
Even if the Friendliness FeelingNode always looks the same, its meaning
<br>
consists in its relations to other things in the mind, and these other
<br>
things may change.  Keeping the full semantics of Friendliness invariant
<br>
through substantial self-modifications is probably not going to be possible,
<br>
even by an hypothetical superhumanly intelligent Novamente.  Of course, this
<br>
cannot be known for sure since such a system may possess AI techniques
<br>
beyond our current imagination.  But it’s also possible that, even if such
<br>
techniques are arrived at by an AI eventually, they may be arrived at well
<br>
after the AI’s notion of Friendliness has drifted from the initial
<br>
programmers’ notions of Friendliness.
<br>
<p>The resolution of such issues requires a subtle understanding of Novamente
<br>
dynamics, which we are very far from having right now.   However, based on
<br>
our current state of relative ignorance, it seems to us quite possible that
<br>
the only way to cause an evolving Novamente to maintain a humanly-desirable
<br>
notion of Friendliness maximization is for it to continually be involved
<br>
with Friendliness-reinforcing human interactions.   Human minds tend to
<br>
maintain the same definitions of concepts as the other human minds with
<br>
which they frequently interact: this is a key aspect of culture.   To the
<br>
extent that an advanced Novamente system is part of a community of Friendly
<br>
humans, it is more likely to maintain a human-like notion of Friendliness.
<br>
But of course, this is not a demonstrable panacea for Friendly AI either.
<br>
**********
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3147.html">Eugene Leitl: "Re: The &quot;AI Box&quot; experiment"</a>
<li><strong>Previous message:</strong> <a href="3145.html">Fabio Mascarenhas: "The &quot;AI Box&quot; experiment"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3148.html">Eliezer S. Yudkowsky: "Re: Novamente goal system"</a>
<li><strong>Reply:</strong> <a href="3148.html">Eliezer S. Yudkowsky: "Re: Novamente goal system"</a>
<li><strong>Maybe reply:</strong> <a href="3166.html">w d: "RE:Novamente goal system"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3146">[ date ]</a>
<a href="index.html#3146">[ thread ]</a>
<a href="subject.html#3146">[ subject ]</a>
<a href="author.html#3146">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
