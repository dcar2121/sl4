<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Regulating AI Development</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Regulating AI Development">
<meta name="Date" content="2002-03-01">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Regulating AI Development</h1>
<!-- received="Fri Mar 01 18:37:08 2002" -->
<!-- isoreceived="20020302013708" -->
<!-- sent="Fri, 1 Mar 2002 16:32:49 -0700" -->
<!-- isosent="20020301233249" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Regulating AI Development" -->
<!-- id="LAEGJLOGJIOELPNIOOAJIEFKCEAA.ben@goertzel.org" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3C80067C.934C608A@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Regulating%20AI%20Development"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Fri Mar 01 2002 - 16:32:49 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3081.html">Dan Clemmensen: "Re: JOIN: Announcing The Singularity"</a>
<li><strong>Previous message:</strong> <a href="3079.html">Eliezer S. Yudkowsky: "Re: Regulating AI Development"</a>
<li><strong>In reply to:</strong> <a href="3079.html">Eliezer S. Yudkowsky: "Re: Regulating AI Development"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3080">[ date ]</a>
<a href="index.html#3080">[ thread ]</a>
<a href="subject.html#3080">[ subject ]</a>
<a href="author.html#3080">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; &gt; it could have generated profits to fund some of our real AI
</em><br>
<em>&gt; &gt; development (which was &quot;Friendly&quot; in nature, although I don't
</em><br>
<em>&gt; &gt; share all of Eli's views on goal system architecture).  But, well...
</em><br>
<em>&gt; &gt; just another example of how the individuals and organizations
</em><br>
<em>&gt; &gt; controlling the resources do not value this sort of thing.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Which views *do* you share?  Last time I checked, you disagreed with the
</em><br>
<em>&gt; concept of a Friendliness-topped goal system, and you can't do
</em><br>
<em>&gt; much without that...
</em><br>
<p>Not all AI architectures have an explicitly &quot;anything-topped&quot; goal system,
<br>
Eliezer.
<br>
<p>In Novamente, Friendliness is one goal among many.  The relative importances
<br>
of the different goals are allowed to shift dynamically, but in practice
<br>
after a while they will usually settle into a roughly constant &quot;importance
<br>
balance.&quot;
<br>
<p>[&quot;Importance&quot; being a technical Novamente term, a quantity associated with
<br>
Nodes and Relationships in the system.  It
<br>
determines roughly how much CPU time a Node or Relationship gets, and is
<br>
governed by a special nonlinear-dynamical equation called the Importance
<br>
Updating Function, which depends on many factors that I'm not going to
<br>
describe here.]
<br>
<p>If you want the system to settle into a configuration in which Friendliness
<br>
is a highly important goal, you've got to interact with it in an appropriate
<br>
way.
<br>
<p>I believe that the balancing of goals must dynamically emerge from of the
<br>
overall balancing of structures and processes in the system.  I don't
<br>
believe in &quot;hard-wiring in&quot; Friendliness as a supergoal, in the way that you
<br>
propose.  This would not work well in the context of the Novamente
<br>
architecture, and my intuition says that it contradicts the intrinsically
<br>
self-organizing and fluid nature of mind in general.
<br>
<p>The method is: Give the system the smarts to recognize when it's being
<br>
Friendly.  Interact with it in a way that teaches it that Friendliness is
<br>
important.  Then GoalNodes corresponding to Friendliness will
<br>
&quot;spontaneously&quot; become important.
<br>
<p><em>&gt; Speaking of which, as a human, I'd like to ask that you write up
</em><br>
<em>&gt; Novamente's
</em><br>
<em>&gt; proposed Friendliness architecture and publish it to the Web.
</em><br>
<p>In time, my friend, in time....
<br>
<p>The Novamente &quot;goal architecture&quot; can't be clearly explained in any detail,
<br>
without also explaining most the rest of the architecture; that's the way
<br>
the system is... the parts all interpenetrate and interdepend.
<br>
[I.e., GoalNode extends CompoundRelationNode extends SchemaNode ... it's a
<br>
looong story.  The dynamics of GoalNodes are general Novamente cognition
<br>
dynamics with particular parameter settings.  ]
<br>
<p>We tentatively plan to publish a book describing the Novamente architecture,
<br>
but it won't be this year.  We want the book to be *just right*, and so
<br>
we're going to circulate the manuscript to close associates for detailed
<br>
criticism and feedback first (later this year), before seriously considering
<br>
releasing it to the general public.
<br>
<p>Incidentally, GoalNode is not even implemented yet in Novamente (though we
<br>
did some mucking-around with a slightly different kind of GoalNode in
<br>
Webmind, in 2000).  Our current focusing is in getting the basic cognition
<br>
methods to work properly together, which does not require the system to set
<br>
its own goals explicitly using GoalNodes, though it does require other
<br>
things that may be construed in a sense as dynamic goal-setting.
<br>
<p>I know you have explicitly articulated your idea for a goal architecture for
<br>
an AI system.  However, you have not articulated, so far as I know, a set of
<br>
cognitive dynamics that go along with this goal architecture.  Thus it
<br>
remains very undemonstrated that your goal architecture is actually
<br>
compatible with highly intelligent cognition.  I am somewhat skeptical on
<br>
this point.
<br>
<p><p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3081.html">Dan Clemmensen: "Re: JOIN: Announcing The Singularity"</a>
<li><strong>Previous message:</strong> <a href="3079.html">Eliezer S. Yudkowsky: "Re: Regulating AI Development"</a>
<li><strong>In reply to:</strong> <a href="3079.html">Eliezer S. Yudkowsky: "Re: Regulating AI Development"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3080">[ date ]</a>
<a href="index.html#3080">[ thread ]</a>
<a href="subject.html#3080">[ subject ]</a>
<a href="author.html#3080">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
