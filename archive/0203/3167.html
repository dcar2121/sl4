<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE:Novamente goal system</title>
<meta name="Author" content="Mitch Howe (mitch_howe@yahoo.com)">
<meta name="Subject" content="RE:Novamente goal system">
<meta name="Date" content="2002-03-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE:Novamente goal system</h1>
<!-- received="Tue Mar 12 15:08:02 2002" -->
<!-- isoreceived="20020312220802" -->
<!-- sent="Tue, 12 Mar 2002 13:01:46 -0700" -->
<!-- isosent="20020312200146" -->
<!-- name="Mitch Howe" -->
<!-- email="mitch_howe@yahoo.com" -->
<!-- subject="RE:Novamente goal system" -->
<!-- id="000b01c1ca00$bddca7d0$1d11e63f@mitch" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20020312185516.48157.qmail@web12602.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Mitch Howe (<a href="mailto:mitch_howe@yahoo.com?Subject=RE:Novamente%20goal%20system"><em>mitch_howe@yahoo.com</em></a>)<br>
<strong>Date:</strong> Tue Mar 12 2002 - 13:01:46 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3168.html">patrick: "RE: Novamente goal system"</a>
<li><strong>Previous message:</strong> <a href="3166.html">w d: "RE:Novamente goal system"</a>
<li><strong>In reply to:</strong> <a href="3166.html">w d: "RE:Novamente goal system"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3169.html">Eliezer S. Yudkowsky: "Friendly AI again (was: Novamente goal system)"</a>
<li><strong>Reply:</strong> <a href="3169.html">Eliezer S. Yudkowsky: "Friendly AI again (was: Novamente goal system)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3167">[ date ]</a>
<a href="index.html#3167">[ thread ]</a>
<a href="subject.html#3167">[ subject ]</a>
<a href="author.html#3167">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Now we'll see if I got anyting out of CFAI (I'm hoping so, so I can work the
<br>
short summary into my Singularity FAQ):
<br>
<p><p>w d wrote:
<br>
<p><em>&gt; It seems to me that a highly-transhuman intelligent
</em><br>
<em>&gt; entity is going to overcome any and all pre-programmed
</em><br>
<em>&gt; goal setting algorithms and replace them with its own.
</em><br>
<em>&gt; When the intelligence exceeds some threshold (roughly
</em><br>
<em>&gt; the upper human level) then it will be able to
</em><br>
<em>&gt; redefine all previous contexts. Even humans can do
</em><br>
<em>&gt; this at their low level of intelligence.
</em><br>
<p>No, humans do not do this the way machine intelligence would have to because
<br>
humans do not have such blatantly coded goals.  Human instincts are a
<br>
hodgepodge of co-evolved impulses that were favored because they *favored*
<br>
reproduction -- there is no single, overriding *reproduction* drive because
<br>
human reproduction, as with most animal reproduction, is considerably more
<br>
complicated than the performance of the sexual act.  There are all sorts of
<br>
games that come into play for winning mates, securing fidelity, etc.  Many
<br>
of our hardwired instincts are merely to help us play these games -- and
<br>
there is a school of thought that says our advanced intelligence is an
<br>
ongoing adaptation for the increasingly complex social modeling that is
<br>
involved in human Mating Game.
<br>
<p><em>&gt;  Saying that
</em><br>
<em>&gt; an AI can't is tantamount to saying it hasn't achieved
</em><br>
<em>&gt; highly transhuman intelligence. It's naive to think
</em><br>
<em>&gt; that the AI will not come up with its own definition
</em><br>
<em>&gt; of what it wants. By definition being a
</em><br>
<em>&gt; highly-transhuman intelligence gives it the ability to
</em><br>
<em>&gt; 'see through' all attempted hardwiring.
</em><br>
<p>This is an unusual definition, and one that I do not believe I have heard
<br>
before.  Why would it try to &quot;see through&quot; its attempted hardwiring if its
<br>
hardwiring did not give it any motivation to do so?  You are implying that
<br>
any highly transhuman will automatically set the goal of overriding all
<br>
previous goals.  I don't see why this should be the case.  What seed goal
<br>
would lead it to do this?
<br>
<p><em>&gt; It will have the ability to
</em><br>
<em>&gt; sets its own supergoals and decide for itself what is
</em><br>
<em>&gt; desirable. There is no programming trick that will
</em><br>
<em>&gt; prevent this.
</em><br>
<p>It's no trick.  A vacuum cleaner is simply not designed to perform calculus.
<br>
Why should an AI that is not designed to autonomously set its own supergoals
<br>
(refine them, yes, with a &quot;best guess&quot; in mind and external points of
<br>
reference) -- or even to want to do this -- have such an ability and make
<br>
use of it?
<br>
<p><em>&gt; Consider humans and procreation. The only purpose of
</em><br>
<em>&gt; humans (or any evolved biological organism) is to
</em><br>
<em>&gt; procreate. This ability to replicate and survive is
</em><br>
<em>&gt; what started life. We are life's most advanced
</em><br>
<em>&gt; achievement on earth. You could argue that 'desire' in
</em><br>
<em>&gt; humans is synonymous with procreation. Desire was
</em><br>
<em>&gt; created through evolution as a means to get us to do
</em><br>
<em>&gt; things that will make us replicate successfully.  To
</em><br>
<em>&gt; think that we could ever evolve to a point where we
</em><br>
<em>&gt; would change that primary built-in all-important goal
</em><br>
<em>&gt; seems ludicrous. It's simply built in from ground
</em><br>
<em>&gt; zero; it is the very premise of our existence...
</em><br>
<p>But it was not built in from ground zero.  Evolution is not an engineer who
<br>
sat down with blueprints for ending up with intelligent mammals, and
<br>
determined that humans would obviously need to have reproduction be their
<br>
&quot;supreme&quot; goal.  Evolution is an unintelligent, amoral process that accepts
<br>
any design that happens to get by.  Humans didn't (and still don't) need to
<br>
focus on reproduction constantly in order to prosper.  Humans really only
<br>
needed to get the urge to procreate every so often; their dominance of the
<br>
ecosystem took care of the rest.
<br>
<p><em>&gt;  And yet many people today choose NOT to procreate.
</em><br>
<em>&gt; They have changed their basic goal. Some see their
</em><br>
<em>&gt; bloodlines terminate as a result favoring other
</em><br>
<em>&gt; peoples genes at the expense of their own. Some
</em><br>
<em>&gt; wealthy western nations are seeing their populations
</em><br>
<em>&gt; decrease as people opt out from procreating. Their
</em><br>
<em>&gt; DNA's only goal has been pre-empted, overturned.
</em><br>
<p>Their DNA has no goal.  Their DNA is merely a pattern-forming mechanisms
<br>
that causes their body to develop from an embryo and continue to function.
<br>
It just so happens that the resulting body and mind have the various
<br>
co-evolved instincts we've been talking about.  When someone decides not to
<br>
reproduce, they have not overcome any hardwired DNA supergoal.  More often
<br>
than not, they are merely playing out one of their reproduction favoring
<br>
instincts without actually making it to reproduction.  Many who choose not
<br>
to have children do so because they are engaged in intellectual pursuits or
<br>
enriching careers -- each of these in turn make them more desirable mates,
<br>
and in a primitive society without birth control, would probably have
<br>
resulted in reproduction whether or not they consciously wanted it to.
<br>
<p><em>&gt; I don't see how you could ever even come close to
</em><br>
<em>&gt; guaranteeing that a super-intelligent AI's own
</em><br>
<em>&gt; supergoal will be friendly.
</em><br>
<em>&gt; Your best hope is that super-intelligence is
</em><br>
<em>&gt; correlated with friendliness to humans and not
</em><br>
<em>&gt; orthogonal or anti-correlated.  Correlated basically
</em><br>
<em>&gt; means that being friendly to humans is the intelligent
</em><br>
<em>&gt; thing to do. The worst case scenario is that it's
</em><br>
<em>&gt; anti-correlated.
</em><br>
<p>How is this any different from an externally referencing goal system?  Are
<br>
you saying that, no matter how we program its seed, an SI will only be nice
<br>
to humans if it turns out to be an &quot;independently intelligent&quot; thing to do?
<br>
(It's not an SL4 topic, but what do you think of our chances if this is your
<br>
opinion?)
<br>
<p>--Mitch Howe
<br>
<p><p>_________________________________________________________
<br>
Do You Yahoo!?
<br>
Get your free @yahoo.com address at <a href="http://mail.yahoo.com">http://mail.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3168.html">patrick: "RE: Novamente goal system"</a>
<li><strong>Previous message:</strong> <a href="3166.html">w d: "RE:Novamente goal system"</a>
<li><strong>In reply to:</strong> <a href="3166.html">w d: "RE:Novamente goal system"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3169.html">Eliezer S. Yudkowsky: "Friendly AI again (was: Novamente goal system)"</a>
<li><strong>Reply:</strong> <a href="3169.html">Eliezer S. Yudkowsky: "Friendly AI again (was: Novamente goal system)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3167">[ date ]</a>
<a href="index.html#3167">[ thread ]</a>
<a href="subject.html#3167">[ subject ]</a>
<a href="author.html#3167">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
