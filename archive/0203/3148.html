<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Novamente goal system</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Novamente goal system">
<meta name="Date" content="2002-03-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Novamente goal system</h1>
<!-- received="Sat Mar 09 14:58:38 2002" -->
<!-- isoreceived="20020309215838" -->
<!-- sent="Sat, 09 Mar 2002 14:56:21 -0500" -->
<!-- isosent="20020309195621" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Novamente goal system" -->
<!-- id="3C8A68E5.E09710F8@pobox.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJMEKNCEAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Novamente%20goal%20system"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Mar 09 2002 - 12:56:21 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3149.html">Eliezer S. Yudkowsky: "Wednesday's chatlog"</a>
<li><strong>Previous message:</strong> <a href="3147.html">Eugene Leitl: "Re: The &quot;AI Box&quot; experiment"</a>
<li><strong>In reply to:</strong> <a href="3146.html">Ben Goertzel: "Novamente goal system"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3152.html">Ben Goertzel: "RE: Novamente goal system"</a>
<li><strong>Reply:</strong> <a href="3152.html">Ben Goertzel: "RE: Novamente goal system"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3148">[ date ]</a>
<a href="index.html#3148">[ thread ]</a>
<a href="subject.html#3148">[ subject ]</a>
<a href="author.html#3148">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; In this section we will briefly explore some of the more futuristic aspects
</em><br>
<em>&gt; of Novamente’s Feelings and Goals component.  These aspects of Novamente are
</em><br>
<em>&gt; always important, but in the future when a Novamente system intellectually
</em><br>
<em>&gt; outpaces its human mentors, they will become yet more critical.
</em><br>
<p>You say this here, and &quot;superhuman intelligence&quot; below.  Just to make sure
<br>
we're all on the same wavelength, would you care to describe briefly what
<br>
you think a transhuman Novamente would be like?
<br>
<p><em>&gt; The MaximizeFriendliness GoalNode is very close to the concept of
</em><br>
<em>&gt; “Friendliness,” which Eliezer Yudkowsky has discussed extensively in his
</em><br>
<em>&gt; treatise Creating a Friendly AI (Yudkowsky, 2001:
</em><br>
<em>&gt; <a href="http://intelligence.org/CFAI/">http://intelligence.org/CFAI/</a>).   Yudkowsky believes that an AI should be
</em><br>
<em>&gt; designed with an hierarchical goal system that has Friendliness at the top.
</em><br>
<em>&gt; In this scheme, the AI pursues other goals only to the extent that it
</em><br>
<em>&gt; believes (through experience or instruction) that these other goals are
</em><br>
<em>&gt; helpful for achieving its over-goal of Friendliness.
</em><br>
<p>My claim, and it is a strong one, is:
<br>
<p>A system in which &quot;desirability&quot; behaves equivalently with the property &quot;is
<br>
predicted to lead to Friendliness&quot; is the normative and elegant form of goal
<br>
cognition.  Many powerful idioms, including what we would call &quot;positive and
<br>
negative reinforcement&quot; are directly emergent from this underlying pattern;
<br>
all other idioms that are necessary to the survival and growth of an AI
<br>
system, such as &quot;curiosity&quot; and &quot;resource optimization&quot; and &quot;knowledge
<br>
seeking&quot; and &quot;self-improvement&quot;, can be made to emerge very easily from this
<br>
pattern.
<br>
<p>I claim:  There is no important sense in which a cleanly causal,
<br>
Friendliness-topped goal system is inferior to any alternate system of
<br>
goals.
<br>
<p>I claim:  The CFAI goal architecture is directly and immediately superior to
<br>
the various widely differing formalisms that were described to me by
<br>
different parties, including Ben Goertzel, as being &quot;Webmind's goal
<br>
architecture&quot;.
<br>
<p>I claim:  That for any important Novamente behavior, I will be able to
<br>
describe how that behavior can be implemented under the CFAI architecture,
<br>
without significant loss of elegance or significant additional computing
<br>
power.
<br>
<p><em>&gt; Yudkowsky’s motivation for this proposed design is long-term thinking about
</em><br>
<em>&gt; the possible properties of a progressively self-modifying AI with superhuman
</em><br>
<em>&gt; intelligence.  His worry (a very reasonable one, from a big-picture
</em><br>
<em>&gt; perspective) is that one day an AI will transform itself to be so
</em><br>
<em>&gt; intelligent that it cannot be controlled by humans – and at this point, it
</em><br>
<em>&gt; will be important that the AI values Friendliness.  Of course, if an AI is
</em><br>
<em>&gt; self-modifying itself into greater and greater levels of intelligence, there
</em><br>
<em>&gt; ’s no guarantee that Friendliness will be preserved through these successive
</em><br>
<em>&gt; self-modifications.  His argument, however, is that if Friendliness is the
</em><br>
<em>&gt; chief goal, then self-modifications will be done with the goal of increasing
</em><br>
<em>&gt; Friendliness, and hence will be highly likely to be Friendly.
</em><br>
<p>To be precise:  A correctly built Friendly AI is argued to have at least
<br>
that chance of remaining well-disposed toward humanity as would be possible
<br>
for any transhuman, upload, social system of uploads, et cetera.
<br>
<p><em>&gt; Unlike the hypothetical Friendly AI systems that Yudkowsky has discussed,
</em><br>
<em>&gt; Novamente does not have an intrinsically hierarchical goal system.  However,
</em><br>
<em>&gt; the basic effect that Yudkowsky describes – MaximizeFriendliness supervening
</em><br>
<em>&gt; over other goals -- can be achieved within Novamente’s goal system through
</em><br>
<em>&gt; appropriate parameter settings.   Basically all one has to do is
</em><br>
<em>&gt; 
</em><br>
<em>&gt; * constantly pump activation to the MaximizeFriendliness GoalNode.
</em><br>
<em>&gt; * encourage the formation of links of the form &quot;InheritanceLink G
</em><br>
<em>&gt; MaximizeFriendliness&quot;, where G is another GoalNode
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This will cause it to seek Friendliness maximization avidly, and will also
</em><br>
<em>&gt; cause it to build an approximation of Yudkowsky’s posited hierarchical goal
</em><br>
<em>&gt; system, by making the system continually seek to represent other goals as
</em><br>
<em>&gt; subgoals (goals inheriting from) MaximizeFriendliness.
</em><br>
<p>No, this is what we humans call &quot;rationalization&quot;.  An AI that seeks to
<br>
rationalize all goals as being Friendly is not an AI that tries to invent
<br>
Friendly goals and avoid unFriendly ones.
<br>
<p><em>&gt; However, even if one enforces a Friendliness-centric goal system in this
</em><br>
<em>&gt; way, it is not clear that the Friendliness-preserving evolutionary path that
</em><br>
<em>&gt; Yudkowsky envisions will actually take place.  There is a major weak point
</em><br>
<em>&gt; to this argument, which has to do with the stability of the Friendliness
</em><br>
<em>&gt; goal under self-modifications.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Suppose our AI modifies itself with the goal of maintaining Friendliness.
</em><br>
<em>&gt; But suppose it makes a small error, and in its self-modificatory activity,
</em><br>
<em>&gt; it actually makes itself a little less able to judge what is Friendly and
</em><br>
<em>&gt; what isn’t.   It’s almost inevitable that this kind of error will occur at
</em><br>
<em>&gt; some point.  The system will then modify itself again, the next time around,
</em><br>
<em>&gt; with this less accurate assessment of the nature of Friendliness as its
</em><br>
<em>&gt; goal.  The question is, what is the chance that this kind of dynamic leads
</em><br>
<em>&gt; to a decreasing amount of Friendliness, due to an increasingly erroneous
</em><br>
<em>&gt; notion of Friendliness.
</em><br>
<p>In other words, a poor Friendliness design is &quot;correct by definition&quot; and is
<br>
anchored neither by external reference nor by internal error correction. 
<br>
Thus, any disturbance to Friendliness cannot be corrected because the
<br>
current cognitive content for Friendliness is &quot;correct by definition&quot;.  This
<br>
is why you need probabilistic supergoal content anchored on external
<br>
referents.
<br>
<p>I claim:  That for any type of error you can describe, I will be able to
<br>
describe why a CFAI-architecture AI will be able to perceive this as an
<br>
&quot;error&quot;.
<br>
<p><em>&gt; One may also put this argument slightly differently: without speaking of
</em><br>
<em>&gt; error, what if the AI’s notion of Friendliness slowly drifts through
</em><br>
<em>&gt; successful self-modifications?  Yudkowsky’s intuition seems to be that when
</em><br>
<em>&gt; an AI has become intelligent enough to self-modify in a sophisticated
</em><br>
<em>&gt; goal-directed way, it will be sufficiently free of inference errors that its
</em><br>
<em>&gt; notion of Friendliness won’t drift or degenerate.  Our intuition is not so
</em><br>
<em>&gt; clear on this point.
</em><br>
<p>It's not an intuition.  It's a system design that was crafted to accomplish
<br>
exactly that end.
<br>
<p><em>&gt; It might seem that one strategy to make Yudkowsky’s idea workable would be
</em><br>
<em>&gt; give the system another specific goal, beyond simple Friendliness: the goal
</em><br>
<em>&gt; of not ever letting its concept of Friendliness change substantially.
</em><br>
<p>No, absolutely not.  See:
<br>
&nbsp;&nbsp;<a href="http://intelligence.org/CFAI/design/structure/why.html">http://intelligence.org/CFAI/design/structure/why.html</a>
<br>
&nbsp;&nbsp;<a href="http://intelligence.org/CFAI/design/structure/external.html#deriving">http://intelligence.org/CFAI/design/structure/external.html#deriving</a>
<br>
<p>'Subgoals for &quot;improving the supergoals&quot; or &quot;improving the goal-system
<br>
architecture&quot; derive desirability from uncertainty in the supergoal
<br>
content.  They may be metaphorically considered as &quot;child goals of the
<br>
currently unknown supergoal content&quot;.  The desirability of &quot;resolving a
<br>
supergoal ambiguity&quot; derives from the prediction that the unknown referent
<br>
of Friendliness will be better served.'
<br>
<p>The same holds for the subgoals of correcting *errors* in the
<br>
*probabilistic* Friendliness content.
<br>
<p><em>&gt; However, this would be very, very difficult to ensure, because every concept
</em><br>
<em>&gt; in the mind is defined implicitly in terms of all the other concepts in the
</em><br>
<em>&gt; mind.  The pragmatic significance of a Friendliness FeelingNode is defined
</em><br>
<em>&gt; in terms of a huge number of other nodes and links, and when a Novamente
</em><br>
<em>&gt; significantly self-modifies it will change many of its nodes and links.
</em><br>
<em>&gt; Even if the Friendliness FeelingNode always looks the same, its meaning
</em><br>
<em>&gt; consists in its relations to other things in the mind, and these other
</em><br>
<em>&gt; things may change.
</em><br>
<p>That's why a probabilistic supergoal is anchored to external referents in
<br>
terms of information provided by the programmers, rather than being anchored
<br>
entirely to internal nodes etc.
<br>
<p><em>&gt; Keeping the full semantics of Friendliness invariant
</em><br>
<em>&gt; through substantial self-modifications is probably not going to be possible,
</em><br>
<em>&gt; even by an hypothetical superhumanly intelligent Novamente.  Of course, this
</em><br>
<em>&gt; cannot be known for sure since such a system may possess AI techniques
</em><br>
<em>&gt; beyond our current imagination.  But it’s also possible that, even if such
</em><br>
<em>&gt; techniques are arrived at by an AI eventually, they may be arrived at well
</em><br>
<em>&gt; after the AI’s notion of Friendliness has drifted from the initial
</em><br>
<em>&gt; programmers’ notions of Friendliness.
</em><br>
<p>If Novamente were programmed simply with a static set of supergoal content
<br>
having only an intensional definition, then yes, it might drift very far
<br>
after a few rounds of self-modification.  This is why you need the full
<br>
Friendliness architecture.
<br>
<p><em>&gt; The resolution of such issues requires a subtle understanding of Novamente
</em><br>
<em>&gt; dynamics, which we are very far from having right now.   However, based on
</em><br>
<em>&gt; our current state of relative ignorance, it seems to us quite possible that
</em><br>
<em>&gt; the only way to cause an evolving Novamente to maintain a humanly-desirable
</em><br>
<em>&gt; notion of Friendliness maximization is for it to continually be involved
</em><br>
<em>&gt; with Friendliness-reinforcing human interactions.  Human minds tend to
</em><br>
<em>&gt; maintain the same definitions of concepts as the other human minds with
</em><br>
<em>&gt; which they frequently interact: this is a key aspect of culture.   To the
</em><br>
<em>&gt; extent that an advanced Novamente system is part of a community of Friendly
</em><br>
<em>&gt; humans, it is more likely to maintain a human-like notion of Friendliness.
</em><br>
<em>&gt; But of course, this is not a demonstrable panacea for Friendly AI either.
</em><br>
<p>This correctly expresses the need for Friendliness to be anchored to the
<br>
external referent which is proximately defined by the programmers'
<br>
conception of Friendliness, thus allowing the programmers to provide
<br>
informational feedback (not to be confused with hardwired &quot;reinforcement&quot;)
<br>
about whether the AI's current conception of Friendliness (ergo, it's
<br>
current supergoal content) is correct, incorrect, needs adjustment in a
<br>
certain direction, and so on.  The cognitive semantics required for this are
<br>
laid out in &quot;External Reference Semantics&quot; in CFAI.
<br>
<p>The rest of the above statement, as far as I can tell, represents two
<br>
understandable but anthropomorphic intuitions:
<br>
<p>(a) Maintaining Friendliness requires rewarding Friendliness.  In humans,
<br>
socially moral behavior is often reinforced by rewarding individually
<br>
selfish goals that themselves require no reinforcement.  An AI, however,
<br>
should work the other way around.
<br>
<p>(b) Novamente will be &quot;socialized&quot; by interaction with other humans. 
<br>
However, the ability of humans to be socialized is the result of millions of
<br>
years of evolution resulting in a set of adaptations which enable
<br>
socialization.  Without these adaptations present, there is no reason to
<br>
expect socialization to have the same effects.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3149.html">Eliezer S. Yudkowsky: "Wednesday's chatlog"</a>
<li><strong>Previous message:</strong> <a href="3147.html">Eugene Leitl: "Re: The &quot;AI Box&quot; experiment"</a>
<li><strong>In reply to:</strong> <a href="3146.html">Ben Goertzel: "Novamente goal system"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3152.html">Ben Goertzel: "RE: Novamente goal system"</a>
<li><strong>Reply:</strong> <a href="3152.html">Ben Goertzel: "RE: Novamente goal system"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3148">[ date ]</a>
<a href="index.html#3148">[ thread ]</a>
<a href="subject.html#3148">[ subject ]</a>
<a href="author.html#3148">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
