<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Novamente project goals</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Novamente project goals">
<meta name="Date" content="2002-03-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Novamente project goals</h1>
<!-- received="Mon Mar 18 20:58:44 2002" -->
<!-- isoreceived="20020319035844" -->
<!-- sent="Mon, 18 Mar 2002 19:03:36 -0700" -->
<!-- isosent="20020319020336" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Novamente project goals" -->
<!-- id="LAEGJLOGJIOELPNIOOAJCENACEAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3C8C23EE.A9A5AE96@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Novamente%20project%20goals"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Mon Mar 18 2002 - 19:03:36 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3176.html">Dani Eder: "Join - Dani Eder"</a>
<li><strong>Previous message:</strong> <a href="3174.html">Eliezer S. Yudkowsky: "Fwd: Research Shows Just How Much People Hate A Winner"</a>
<li><strong>In reply to:</strong> <a href="3162.html">Eliezer S. Yudkowsky: "Novamente project goals"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3178.html">Mike & Donna Deering: "The gallery."</a>
<li><strong>Reply:</strong> <a href="3178.html">Mike & Donna Deering: "The gallery."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3175">[ date ]</a>
<a href="index.html#3175">[ thread ]</a>
<a href="subject.html#3175">[ subject ]</a>
<a href="author.html#3175">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; It so happens I believe that no matter what a project *says* it's
</em><br>
<em>&gt; trying to
</em><br>
<em>&gt; achieve, you can often figure out what it's *actually* trying to
</em><br>
<em>&gt; achieve by
</em><br>
<em>&gt; looking at the way the researchers act.  Various projects have previously
</em><br>
<em>&gt; claimed to be trying to build an AI that was human-equivalent in
</em><br>
<em>&gt; one some or
</em><br>
<em>&gt; another.  Were they really trying to build a person?  Of course not.  They
</em><br>
<em>&gt; were trying (and failing) to build advanced tools.  Had they really been
</em><br>
<em>&gt; trying to build a person, it would have shown in their attitude;
</em><br>
<em>&gt; they would
</em><br>
<em>&gt; have given some thought to whether the resulting system would be deserving
</em><br>
<em>&gt; of human rights, their responsibilities toward the created individual, and
</em><br>
<em>&gt; so on.
</em><br>
<p>I think you are fairly wrong in your analysis of past AI researchers'
<br>
attitudes.
<br>
<p>I think that there have been some teams in the past who have
<br>
<p>a) seriously set out to build a real, human-equivalent general intelligence
<br>
<p>b) NOT published about their ideas on ethics, responsibilities, etc.
<br>
<p>You must understand that the &quot;futurist ethics&quot; ideas that you habitually
<br>
write about, are *not* the kinds of things that academic scientists are
<br>
encouraged to publish their views on.  The tradition is to publish your
<br>
technical results, and save your speculations for coffeetable discussion.
<br>
<p>The Web has changed this a little bit -- now, via the Net, researchers can
<br>
distribute their philosophical thoughts widely with relatively little
<br>
effort.  But this hasn't really changed the culture of the technical
<br>
community, which includes the general belief that time spent writing about
<br>
philosophical nontechnical ideas is mostly wasted time.  In fact is often
<br>
considered worse than wasted time: writing about such things can be career
<br>
suicide, for individuals who need to earn a living via their academic
<br>
reputations.  Try getting tenure with a bunch of publications about the
<br>
human rights of future AI's on your resume'.  Not easy.
<br>
<p>I think if you talked to the actual researchers involved in past (failed) AI
<br>
projects, you'd find that they had personally thought about a lot of the
<br>
issues you mention.  This has been my experience.
<br>
<p><em>&gt; Now, it certainly appears that Novamente has gotten this far in terms of
</em><br>
<em>&gt; feeling the emotional impact of envisioned consequences.  Your dedication,
</em><br>
<em>&gt; your willingness to work on the AI even after Webmind went down, shows the
</em><br>
<em>&gt; same thing.  Whatever it is you're trying to create, it doesn't
</em><br>
<em>&gt; feel like a
</em><br>
<em>&gt; fancy tool to you.  I don't know what goes on within the Secret Private
</em><br>
<em>&gt; Novamente Mailing Lists, but I wouldn't be surprised to find that serious
</em><br>
<em>&gt; consideration of the moral responsibility you owe to the AI is a frequent
</em><br>
<em>&gt; topic.
</em><br>
<p>Actually, we beat that topic into the ground in 1999 or so on the Webmind
<br>
Inc. tech list.  It pretty much never comes up these days.  We ran out of
<br>
new things to say about it.  The Novamente internal mailing lists are almost
<br>
entirely technical, with occasional postings of links to interesting
<br>
research papers etc.  Philosophical discussions more often occur when we
<br>
meet together in person (not that often, just a couple times a year, as I
<br>
live in the US and much of the team is in Brazil).
<br>
<p><em>&gt; But is your attitude toward Novamente really consistent with trying to
</em><br>
<em>&gt; create a superintelligence?
</em><br>
<p>Eliezer, Novamente is a big enough project that it must be broken down into
<br>
phases.  We think in terms of
<br>
<p>Phase 1: getting Novamente to work as a highly flexible, robust, scalable,
<br>
cognition-based multi-domain datamining engine
<br>
<p>This requires most of the cognition algorithms, and some perception
<br>
algorithms... and of course the &quot;core framework&quot;
<br>
<p>Phase 2: putting in the nastiest parts of cognition and action, and doing
<br>
some basic language processing and planning tests with them
<br>
<p>There are a couple very hard problems (not so hard to code, but requiring
<br>
loads of computer memory and processing to run) which we will defer dealing
<br>
with till everything else is coded
<br>
<p>Phase 3: completing the goals, feelings and experiential-interaction
<br>
framework.  Very little additional code here, a lot of additional testing...
<br>
and teaching.  This is where we try to teach the system by interacting with
<br>
it in a shared perceptual environment.
<br>
<p>Phase 4: making a very efficient procedure execution framework in the
<br>
system.  This will enable us to recode the system's thought algorithms as
<br>
nodes and links in the system itself -- permitting the system to achieve
<br>
full &quot;cognitive transparency&quot; -- perceiving and modifying its own thought
<br>
algorithms.
<br>
<p>The design goes thru Phase 4, but we're currently somewhere around halfway
<br>
through Phase 1.  Phase 4 is where I believe we'll see superintelligence
<br>
emerge -- how fast I'm not sure.  We may need Phase 4 to get roughly
<br>
human-level intelligence: some of the team thinks yes, some think it can
<br>
emerge in the middle of Phase 3.
<br>
<p>We are building the system with all 4 phases in mind, from the start.
<br>
<p><em>&gt; You're uncomfortable with my declaration of
</em><br>
<em>&gt; intent to bypass having an ordinary life, because to you Novamente may be
</em><br>
<em>&gt; the greatest thing you ever do, but it won't be the only thing
</em><br>
<em>&gt; you ever do.
</em><br>
<em>&gt; You can have a life that includes wife, kids, and Novamente as
</em><br>
<em>&gt; accomplishments.  For me the Singularity marks, not the end of everything,
</em><br>
<em>&gt; but the beginning of everything.  It is the sum of what there is
</em><br>
<em>&gt; to do, here
</em><br>
<em>&gt; on Earth before the Singularity.
</em><br>
<p>The Singularity will be neither the end of everything nor the beginning of
<br>
everything, actually!
<br>
<p>And it is just NOT the sum of everything there is to do pre-Singularity.  Do
<br>
you hang glide?  I'll take you up sometime, maybe it will change your mind
<br>
;&gt;  Not as distracting as a sex life, but almost as exciting... and these
<br>
days, possibly safer ;-D
<br>
<p><em>&gt;  Your attitude toward Novamente's outlook on life seems to be
</em><br>
<em>&gt; around the attitude I'd take toward building an AI that *wasn't*
</em><br>
<em>&gt; supposed to
</em><br>
<em>&gt; grow up into the Singularity Transition Guide.
</em><br>
<p>Well, fine.  But nevertheless, my attitude toward Novamente is the attitude
<br>
that *I* take toward building an AI intended to set off the Singularity.
<br>
You and I are rather different human beings, as has been exhaustively
<br>
pointed out on this list!!
<br>
<p>I am not generally known as a humble person.  However, I find I am a bit
<br>
humbler than you in terms of estimating any human's ability to *predict*
<br>
what a superhuman AI will be like, and to guide in detail what it evolves
<br>
into.
<br>
<p><em>&gt; If you're working
</em><br>
<em>&gt; on a superintelligence, though, the only problem with taking an
</em><br>
<em>&gt; oath is that
</em><br>
<em>&gt; any oath pales by comparison with the act itself.  You don't sound like
</em><br>
<em>&gt; someone setting out to commit an act with (positive) consequences so
</em><br>
<em>&gt; tremendous that anyone setting a single dividing marker across
</em><br>
<em>&gt; all of human
</em><br>
<em>&gt; history throughout time would set it there, and not because they
</em><br>
<em>&gt; think Real
</em><br>
<em>&gt; AI is a philosophically important moment in human history, either.  You
</em><br>
<em>&gt; appear, from what I can see through email, to consider such statements
</em><br>
<em>&gt; over-the-top.  Implications that extend out from a scientifically
</em><br>
<em>&gt; active, ~H
</em><br>
<em>&gt; Novamente are okay; implications that extend out from
</em><br>
<em>&gt; superintelligence are
</em><br>
<em>&gt; not.  Not just in terms of what you consider to be good public relations,
</em><br>
<em>&gt; which is a separate issue, but in terms of what you, personally, are
</em><br>
<em>&gt; comfortable with discussing.
</em><br>
<p>Eli, obviously I am *comfortable* with discussing such ideas; we have
<br>
discussed them at great length on this list.   If I were uncomfortable about
<br>
it I wouldn't take the time to write long replies to your e-mails on the
<br>
topic!!!
<br>
<p>I do get *bored* with discussing such things after a while, though, because
<br>
there seems to be a very limited amount that can usefully be said on such
<br>
topics, given the data at hand today.  This is the reason we stopped
<br>
discussing AI morality on internal Webmind Inc. lists after a while.
<br>
<p><em>&gt; In short, everything about your emotional posture that I can read through
</em><br>
<em>&gt; email says that you're making decisions based on your vision of a ~H
</em><br>
<em>&gt; Novamente - not a superintelligent one.
</em><br>
<p>Well, your reading of my &quot;emotional posture&quot; is not very good -- which is
<br>
not surprising, because we don't know each other well on a personal level,
<br>
and my emotional makeup has been shaped by a huge number of experiences very
<br>
different than any experiences you've ever had.
<br>
<p><em>&gt; Now, it is well-known that figuring out people's real thoughts
</em><br>
<em>&gt; and emotions
</em><br>
<em>&gt; through email is an underconstrained problem.  I'm not trying to
</em><br>
<em>&gt; pigeonhole
</em><br>
<em>&gt; you.  Just consider this as depicting the causes and conclusions of my
</em><br>
<em>&gt; erroneous intuition in sufficient detail that you can fix what's broken.
</em><br>
<p>I think that figuring out other peoples' real thoughts and emotions even IN
<br>
PERSON is a difficult problem.  I'm not all that socially retarded, yet I
<br>
have trouble sometimes reading my WIFE AND KIDS' thoughts and emotions,
<br>
goodness!!
<br>
<p>I think you should accept that others can be equally as serious about
<br>
building a superintelligent AI as you are, but take a DIFFERENT
<br>
PHILOSOPHICAL ATTITUDE than you do.
<br>
<p>My philosophical attitude has, as one implication, that at this stage it's
<br>
not really worth thinking or talking *too much* about AI morality.  It's
<br>
something to keep in mind as one progresses with one's work, but it's not a
<br>
central point until one has a system that one is actually talking with and
<br>
teaching.  At that stage, I suspect various aspects about AI morality will
<br>
be a lot clearer than they are now.  Having a real AI system to play with,
<br>
demonstrating morality and immorality on a baby-ish level, will add a hell
<br>
of a lot of new data and new conceptual richness to this sort of discussion.
<br>
<p>I don't think we can figure all that much about AI morality at this stage.
<br>
Not even for Novamente, let alone for your AI project which is much less far
<br>
along.
<br>
<p>You may say: &quot;Yes, but what if the AI system goes into a hard takeoff while
<br>
you're playing with it and getting a feel for the AI morality issues.&quot;
<br>
<p>And I say: In the case of Novamente, I have a really good sense of the stage
<br>
at which a hard takeoff will be possible.  (Most ambitiously, somewhere in
<br>
the middle of my Phase 3 above -- to explain more specifically would need
<br>
too many technical details).  The time to worry a lot about morality is when
<br>
this stage is much closer.  Only then will the detailed knowledge be there
<br>
to support really meaningful intuitions about AI morality.    Maybe we'll be
<br>
there in a year and a half (if we get some more funding soon), or maybe
<br>
it'll take 5 years (if annoying technical obstacles intervene, or we go
<br>
totally broke).
<br>
<p>I feel that you spend a lot of time building conceptual castles in the sand.
<br>
Formulating ideas about AI morality that are intriguing and conceptually
<br>
reasonable, but very far beyond anyone's detailed knowledge about AI mind
<br>
dynamics.
<br>
<p>Much of your Friendly AI theory feels to me like Eric Drexler's detailed
<br>
engineering designs in his book Nanosystems.  Interesting to read, and worth
<br>
working out for conceptual purposes, but *too far ahead* of current
<br>
practical technology to be expected to be accurate at even a moderate level
<br>
of detail.  His concept of a molecular assembler will happen, but his
<br>
particular constructions in terms of molecular rods and so forth almost
<br>
certainly won't -- nanotech is already moving in different directions.
<br>
Similarly, Friendly AI will happen, but your particular approach in terms of
<br>
hierarchical (&quot;acyclic digraph&quot;, whatever) goal systems will probably seem
<br>
as naive in 5 years when we have human-level AI, as Drexler's designs do to
<br>
contemporary nanoengineers.
<br>
<p>-- Ben G
<br>
<p><p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3176.html">Dani Eder: "Join - Dani Eder"</a>
<li><strong>Previous message:</strong> <a href="3174.html">Eliezer S. Yudkowsky: "Fwd: Research Shows Just How Much People Hate A Winner"</a>
<li><strong>In reply to:</strong> <a href="3162.html">Eliezer S. Yudkowsky: "Novamente project goals"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3178.html">Mike & Donna Deering: "The gallery."</a>
<li><strong>Reply:</strong> <a href="3178.html">Mike & Donna Deering: "The gallery."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3175">[ date ]</a>
<a href="index.html#3175">[ thread ]</a>
<a href="subject.html#3175">[ subject ]</a>
<a href="author.html#3175">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
