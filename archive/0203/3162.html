<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Novamente project goals</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Novamente project goals">
<meta name="Date" content="2002-03-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Novamente project goals</h1>
<!-- received="Sun Mar 10 22:26:51 2002" -->
<!-- isoreceived="20020311052651" -->
<!-- sent="Sun, 10 Mar 2002 22:26:38 -0500" -->
<!-- isosent="20020311032638" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Novamente project goals" -->
<!-- id="3C8C23EE.A9A5AE96@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJKELMCEAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Novamente%20project%20goals"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Mar 10 2002 - 20:26:38 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3163.html">Mitch Howe: "Re: Singularity Media (was: Wednesday's chatlog)"</a>
<li><strong>Previous message:</strong> <a href="3161.html">mike99: "RE: Wednesday's chatlog"</a>
<li><strong>In reply to:</strong> <a href="3156.html">Ben Goertzel: "RE: Novamente goal system"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3175.html">Ben Goertzel: "RE: Novamente project goals"</a>
<li><strong>Reply:</strong> <a href="3175.html">Ben Goertzel: "RE: Novamente project goals"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3162">[ date ]</a>
<a href="index.html#3162">[ thread ]</a>
<a href="subject.html#3162">[ subject ]</a>
<a href="author.html#3162">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Hi,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; Actually, as described, this is pretty much what I would consider
</em><br>
<em>&gt; &gt; &quot;human-equivalent AI&quot;, for which the term &quot;transhumanity&quot; is not really
</em><br>
<em>&gt; &gt; appropriate.  I don't think I'm halfway to transhumanity, so an AI twice as
</em><br>
<em>&gt; &gt; many sigma from the mean is not all the way there.  Maybe you should say
</em><br>
<em>&gt; &gt; that Novamente-the-project is striving for human-equivalence; either that,
</em><br>
<em>&gt; &gt; or define what you think a *really* transhuman Novamente would be like...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; We are striving first for human-equivalent, then slightly-transhuman, then
</em><br>
<em>&gt; profoundly-transhuman AI.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I do not think it is important for us to articulate in detail, at this
</em><br>
<em>&gt; point, what we believe a profoundly-transhuman AI evolved from the Novamente
</em><br>
<em>&gt; system will be like.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Because, I think it is probably NOT POSSIBLE for us to envision in detail
</em><br>
<em>&gt; what a profoundly-transhuman AI evolved from the Novamente system will be
</em><br>
<em>&gt; like.
</em><br>
<p>That's right.  It's not.  In fact, it's not even possible for either of us
<br>
to envision in detail what a human-equivalent Novamente would be like,
<br>
although we have different ideas about how much self-modification will have
<br>
been done by that level.
<br>
<p>But there is a difference between envisioning system architectures, and
<br>
envisioning consequences.
<br>
<p>It so happens I believe that no matter what a project *says* it's trying to
<br>
achieve, you can often figure out what it's *actually* trying to achieve by
<br>
looking at the way the researchers act.  Various projects have previously
<br>
claimed to be trying to build an AI that was human-equivalent in one some or
<br>
another.  Were they really trying to build a person?  Of course not.  They
<br>
were trying (and failing) to build advanced tools.  Had they really been
<br>
trying to build a person, it would have shown in their attitude; they would
<br>
have given some thought to whether the resulting system would be deserving
<br>
of human rights, their responsibilities toward the created individual, and
<br>
so on.
<br>
<p>Now, it certainly appears that Novamente has gotten this far in terms of
<br>
feeling the emotional impact of envisioned consequences.  Your dedication,
<br>
your willingness to work on the AI even after Webmind went down, shows the
<br>
same thing.  Whatever it is you're trying to create, it doesn't feel like a
<br>
fancy tool to you.  I don't know what goes on within the Secret Private
<br>
Novamente Mailing Lists, but I wouldn't be surprised to find that serious
<br>
consideration of the moral responsibility you owe to the AI is a frequent
<br>
topic.
<br>
<p>I'm willing to believe that you envision, and are working to create, a
<br>
human-equivalent being - what I would consider human-equivalent, anyway -
<br>
who'll be smart; better than us humans at things like Higher Math and code;
<br>
quite the scientific literate; a fine scientific collaborator; speaking to
<br>
us with a personality that uniquely marks it as an AI.  This is what you
<br>
described when I asked you what you were building, and it is consistent with
<br>
the visible portions of your emotional attitude that you take this vision
<br>
seriously.
<br>
<p>But is your attitude toward Novamente really consistent with trying to
<br>
create a superintelligence?
<br>
<p>What is a ~H Novamente?  It's the most important scientific discovery in the
<br>
21st century, beyond all doubt, with moral and philosophical and
<br>
technological implications that place it far ahead of nanotechnology or even
<br>
human immortality.  It is one of the grandest acts in the pageant of human
<br>
life that could ever be conceived.  It would give rise to technological
<br>
aftereffects immense enough to power an economic boom across multiple
<br>
decades, a new industrial revolution.
<br>
<p>But it would still be something that occurred within the framework of human
<br>
life, however significant.  You're uncomfortable with my declaration of
<br>
intent to bypass having an ordinary life, because to you Novamente may be
<br>
the greatest thing you ever do, but it won't be the only thing you ever do. 
<br>
You can have a life that includes wife, kids, and Novamente as
<br>
accomplishments.  For me the Singularity marks, not the end of everything,
<br>
but the beginning of everything.  It is the sum of what there is to do, here
<br>
on Earth before the Singularity.
<br>
<p>While a ~H Novamente might bring about an immense economic boom, it would be
<br>
an economic boom whose first effects would be felt in the First World.  It'd
<br>
trickle down to the Third World eventually, of course, but it would take a
<br>
while.  So it's also understandable that you have a bad reaction to the
<br>
belief that a Friendly superintelligence benefits every one of six billion
<br>
humans equally, to such a great extent as to utterly wipe out existing
<br>
differences; to you it appears to be a case of ignoring a problem that you
<br>
don't expect to be magically fixed just by Novamente.  For us, of course,
<br>
everything begins with the Singularity, for First Worlders and Third
<br>
Worlders alike.
<br>
<p>On the Friendliness issue, well, a ~H Novamente going bad could cause a hell
<br>
of a lot of trouble, but it wouldn't be the end of everything.  And all you
<br>
need is a Novamente that behaves nicely around other people; your
<br>
Friendliness architecture doesn't need to contain all of humanity's hopes
<br>
and dreams.  Your attitude toward Novamente's outlook on life seems to be
<br>
around the attitude I'd take toward building an AI that *wasn't* supposed to
<br>
grow up into the Singularity Transition Guide.  I'd want that AI to be a fit
<br>
player in the human drama, maybe even a child I could be proud of, but not
<br>
just like a human; that would be boring.
<br>
<p>Asking you to swear a solemn oath to act on behalf of sentience/humanity is
<br>
a bit over-the-top if you're envisioning yourself building a ~H
<br>
(roughly-human) Novamente.  The people who built the integrated circuit
<br>
didn't have to swear an oath like that, why should you?  If you're working
<br>
on a superintelligence, though, the only problem with taking an oath is that
<br>
any oath pales by comparison with the act itself.  You don't sound like
<br>
someone setting out to commit an act with (positive) consequences so
<br>
tremendous that anyone setting a single dividing marker across all of human
<br>
history throughout time would set it there, and not because they think Real
<br>
AI is a philosophically important moment in human history, either.  You
<br>
appear, from what I can see through email, to consider such statements
<br>
over-the-top.  Implications that extend out from a scientifically active, ~H
<br>
Novamente are okay; implications that extend out from superintelligence are
<br>
not.  Not just in terms of what you consider to be good public relations,
<br>
which is a separate issue, but in terms of what you, personally, are
<br>
comfortable with discussing.
<br>
<p>In short, everything about your emotional posture that I can read through
<br>
email says that you're making decisions based on your vision of a ~H
<br>
Novamente - not a superintelligent one.  The problem is that Moore's Law
<br>
goes on, and self-improvement goes on, and even if there is somehow a stable
<br>
state in which ~H Novamente lasts for ten years instead of two weeks, any
<br>
scientific revolution started by Novamente is utterly insignificant by
<br>
comparison with what happens at the end of ten years.
<br>
<p>Now, it is well-known that figuring out people's real thoughts and emotions
<br>
through email is an underconstrained problem.  I'm not trying to pigeonhole
<br>
you.  Just consider this as depicting the causes and conclusions of my
<br>
erroneous intuition in sufficient detail that you can fix what's broken.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3163.html">Mitch Howe: "Re: Singularity Media (was: Wednesday's chatlog)"</a>
<li><strong>Previous message:</strong> <a href="3161.html">mike99: "RE: Wednesday's chatlog"</a>
<li><strong>In reply to:</strong> <a href="3156.html">Ben Goertzel: "RE: Novamente goal system"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3175.html">Ben Goertzel: "RE: Novamente project goals"</a>
<li><strong>Reply:</strong> <a href="3175.html">Ben Goertzel: "RE: Novamente project goals"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3162">[ date ]</a>
<a href="index.html#3162">[ thread ]</a>
<a href="subject.html#3162">[ subject ]</a>
<a href="author.html#3162">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
