<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: No More Searle Please</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Re: No More Searle Please">
<meta name="Date" content="2006-01-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: No More Searle Please</h1>
<!-- received="Thu Jan 19 11:02:33 2006" -->
<!-- isoreceived="20060119180233" -->
<!-- sent="Thu, 19 Jan 2006 13:00:47 -0500" -->
<!-- isosent="20060119180047" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Re: No More Searle Please" -->
<!-- id="43CFD3CF.40805@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="20060118154850.78abbc25@localhost.localdomain" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20No%20More%20Searle%20Please"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Thu Jan 19 2006 - 11:00:47 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13671.html">Jeff Medina: "META: Filtering out individuals"</a>
<li><strong>Previous message:</strong> <a href="13669.html">Robin Lee Powell: "Re: META: Re: SL4?"</a>
<li><strong>In reply to:</strong> <a href="13642.html">Daniel Radetsky: "Re: No More Searle Please"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13672.html">micah glasser: "Re: No More Searle Please"</a>
<li><strong>Reply:</strong> <a href="13672.html">micah glasser: "Re: No More Searle Please"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13670">[ date ]</a>
<a href="index.html#13670">[ thread ]</a>
<a href="subject.html#13670">[ subject ]</a>
<a href="author.html#13670">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Daniel,
<br>
<p>In spite of your comments (below), I stand by what I said.  I was trying 
<br>
to kill the Searle argument because there is a very, very simple reason 
<br>
why Searle's idea was ridiculous, but unfortunately all the other 
<br>
discussion about related issues, which occurred in abundance in the 
<br>
original BBS replies and in the years since then, has given the 
<br>
misleading impression that the original argument had some merit.
<br>
<p>I will try to explain why I say this, and address the points you make.
<br>
<p>First, it is difficult to argue about what *exactly* Searle was claiming
<br>
in his original paper, because in an important sense there was no such
<br>
thing as &quot;exactly what he said&quot; -- he used vague language and subtle
<br>
innuendos at certain crucial points of the argument, so if you try to
<br>
pin down the fine print you find that it all starts to get very slippery.
<br>
<p>As example I will cite the way you phrase his claim.  You say:
<br>
<p>&quot;He claims ... that no additional understanding is created anywhere, in
<br>
the room or in the man, and so Strong AI is false.&quot;
<br>
<p>How exactly does Searle arrive at this conclusion?  In Step 1 he argues
<br>
that the English speaking person does not &quot;understand&quot; Chinese.  If we
<br>
are reasonable, we must agree with him.  In Step 2 he says that this is
<br>
like a computer implementing a program (since the English speaker is
<br>
merely implementing a computer program).  In Step 3 he goes on to
<br>
conclude that THEREFORE when we look at a computer running a Chinese
<br>
understanding program, we have no right to say that the computer
<br>
&quot;understands&quot; or is &quot;conscious of&quot; what it is doing, any more than we
<br>
would claim that the English person in his example understands Chinese.
<br>
<p>My beef, of course, was with Step 2.  The system of mind-on-top-of-mind
<br>
is most definitely NOT the same as a system of mind-on-top-of-computer.
<br>
&nbsp;&nbsp;He is only able to pull his conclusion out of the hat by pointing to
<br>
the understanding system that is implementing the Chinese programme
<br>
(namely the English speaking person), and asking whether *that*
<br>
understanding system knows Chinese.  He appeals to our intuitions.  If
<br>
he had proposed that the Chinese program be implemented on top of some
<br>
other substrate, like a tinkertoy computer (or any of the other
<br>
gloriously elaborate substrates that people have discussed over the
<br>
years) he could not have persuaded our intuition to agree with him.  If
<br>
he had used *anything* else except an intelligence at that lower level,
<br>
he would not have been able to harness our intuition pump and get us to
<br>
agree with him that the &quot;substrate itself&quot; was clearly not understanding
<br>
Chinese.
<br>
<p>But by doing this he implicitly argued that the Strong AI people were
<br>
claiming that in his weird mind-on-mind case the understanding would
<br>
bleed through from the top level system to the substrate system.  He
<br>
skips this step in his argument. (Of course!  He doesn't want us to
<br>
notice that he slipped it in!).  If he had inserted a Step 2(a): &quot;The
<br>
Strong AI claim is that when you implement an AI program on top of a
<br>
dumb substrate like a computer it is exactly equivalent to implementing
<br>
the same AI program on top of a substrate that happens to have its own
<br>
intelligence,&quot; the Strong AI people would have jumped up and down and
<br>
cried Foul!, flatly refusing to accept that this was their claim.  They
<br>
would say:  we have never argued that intelligence bleeds through from
<br>
one level to another when you implement an intelligent system on top of
<br>
another intelligent system, so your argument breaks down at Step 2 and
<br>
Step 2(a):  the English speaking person inside the room is NOT analogous
<br>
to a computer, so nothing can be deduced about the Strong AI argument.
<br>
<p>So when you say:  &quot;Searle never claims that since 'understanding doesn't
<br>
bleed through,' Strong AI is false.&quot; I am afraid I have to disagree
<br>
completely.  It is implicit, but he relies on that implicit claim.
<br>
<p>And while you correctly point out that the &quot;Systems Argument&quot; is a good
<br>
characterisation of what the AI people do believe, I say that this is
<br>
mere background, and is not the correct and immediate response to
<br>
Searle's thought experiment, because Searle had already undermined his
<br>
argument when he invented a freak system, and then put false words into
<br>
the mouths of Strong AI proponents.  My point is that the argument was
<br>
dead at that point:  we do not need to go on and say what Strong AI
<br>
people do believe, in order to address his argument.
<br>
<p>In fact, everyone played into his hands by going off on all these other
<br>
speculations about other weird cases.  What is frustrating is that the
<br>
original replies should ALL have started out with the above argument as
<br>
a preface, then, after declaring the Chinese Room argument to be invalid
<br>
and completely dead, they should have proceeded to raise all those
<br>
interesting and speculative ideas about what Strong AI would say about
<br>
various cases of different AI implementations.  Instead, Searle and his
<br>
camp argued the toss about all those other ideas as if each one were a
<br>
failed attempt to demolish his thought experiment.
<br>
<p>Finally, Searle's response to the mind-on-mind argument was grossly
<br>
inadequate.  Just more of the same trick that he had already tried to
<br>
pull off.  When he tries to argue that Strong AI makes this or that
<br>
claim about what a Turing machine &quot;understands,&quot; he is simply trying to
<br>
generalise the existing Strong AI claims into new territory (the
<br>
territory of his freak system) and then quickly say how the Strong AI
<br>
people would extend their old turing-machine language into this new
<br>
case.  And since he again puts a false claim onto their mouths, he is
<br>
simply repeating the previous invalid argument.
<br>
<p>The concept of a Turing machine has not, to my knowledge, been
<br>
adequately extended to say anything valid about the situation of one
<br>
Turing machine implemented at an extreme high level on top of another
<br>
Turing machine.  In fact, I am not sure it could be extended, even in
<br>
principle.  For example:  if I get a regular computer running an
<br>
extremely complex piece of software that does many things, but also
<br>
implements a Turing machine task at a very high level, which latter is
<br>
then used to run some other software, there is nothing whatsoever in the
<br>
theory of Turing machines that says that the pieces of software running
<br>
at the highest level and at the lowest level have to relate to one
<br>
another:  in an important sense they can be completely independent.
<br>
There are no constraints whatsoever between them.
<br>
<p>The lower level software might be managing several autonomous space
<br>
probes zipping about the solar system and interacting with one another
<br>
occasionally in such a way as to implement a distributed Turing machine,
<br>
while this Turing machine itself may be running a painting program.  But
<br>
there is no earthly reason why &quot;Turing machine equivalence&quot; arguments
<br>
could be used to say that the spacecraft system is &quot;really&quot; the same as
<br>
a painting program, or has all the functions of a painting program.
<br>
This is, as I say, a freak case that was never within the scope of the
<br>
original claims:  the original claims have to be extended to deal with
<br>
the freak case, and Searle disingenuous extension is not the one that
<br>
Strong AI proponents would have made.
<br>
<p><p>Richard Loosemore.
<br>
<p><p><p><p><p>Daniel Radetsky wrote:
<br>
<em>&gt; On Wed, 18 Jan 2006 08:09:43 -0500
</em><br>
<em>&gt; Richard Loosemore &lt;<a href="mailto:rpwl@lightlink.com?Subject=Re:%20No%20More%20Searle%20Please">rpwl@lightlink.com</a>&gt; wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;END OF ARGUMENT.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If you don't want to talk about Searle, don't talk about Searle, but don't give
</em><br>
<em>&gt; a set of reasons why not to talk about Searle, and expect me not to respond.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;He proposed a computational system implemented on top of another 
</em><br>
<em>&gt;&gt;computational system (Chinese understander implemented on top of English 
</em><br>
<em>&gt;&gt;understander).  This is a mind-on-top-of-mind case that has no relevance 
</em><br>
<em>&gt;&gt;whatsoever to either (a) human minds, or (b) an AI implemented on a 
</em><br>
<em>&gt;&gt;computer.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is a version of a response made a long time ago by Jerry Fodor. Searle
</em><br>
<em>&gt; responded, and very adequately I think. Since the mind-on-top-of-mind is
</em><br>
<em>&gt; something which is implementing a Turing machine, it is the same thing
</em><br>
<em>&gt; computation-wise as anything else implementing a Turing machine. So it is
</em><br>
<em>&gt; completely relevant to whether or not a computer (something implementing a
</em><br>
<em>&gt; Turing Machine) can be conscious.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'll be blunt: if you want to challenge Searle, use the Systems Reply. It's the
</em><br>
<em>&gt; only reply that actually works, since it explicitly disagrees with Searle's
</em><br>
<em>&gt; fundamental premise (consciousness is a causal, not a formal, process). You
</em><br>
<em>&gt; went on to make something like the Systems Reply in the rest of your post, but
</em><br>
<em>&gt; against a straw man. Searle never claims that since 'understanding doesn't bleed
</em><br>
<em>&gt; through,' Strong AI is false. He claims (in the original article; I haven't read
</em><br>
<em>&gt; everything on this subject) that no additional understanding is created
</em><br>
<em>&gt; anywhere, in the room or in the man, and so Strong AI is false. That is, the
</em><br>
<em>&gt; fact that 'understanding doesn't bleed through' is only a piece of the puzzle.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Daniel
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13671.html">Jeff Medina: "META: Filtering out individuals"</a>
<li><strong>Previous message:</strong> <a href="13669.html">Robin Lee Powell: "Re: META: Re: SL4?"</a>
<li><strong>In reply to:</strong> <a href="13642.html">Daniel Radetsky: "Re: No More Searle Please"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13672.html">micah glasser: "Re: No More Searle Please"</a>
<li><strong>Reply:</strong> <a href="13672.html">micah glasser: "Re: No More Searle Please"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13670">[ date ]</a>
<a href="index.html#13670">[ thread ]</a>
<a href="subject.html#13670">[ subject ]</a>
<a href="author.html#13670">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
