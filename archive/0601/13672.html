<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: No More Searle Please</title>
<meta name="Author" content="micah glasser (micahglasser@gmail.com)">
<meta name="Subject" content="Re: No More Searle Please">
<meta name="Date" content="2006-01-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: No More Searle Please</h1>
<!-- received="Thu Jan 19 13:06:31 2006" -->
<!-- isoreceived="20060119200631" -->
<!-- sent="Thu, 19 Jan 2006 15:06:28 -0500" -->
<!-- isosent="20060119200628" -->
<!-- name="micah glasser" -->
<!-- email="micahglasser@gmail.com" -->
<!-- subject="Re: No More Searle Please" -->
<!-- id="23bd28ec0601191206p267fe2a7kd07444442a37fb64@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="43CFD3CF.40805@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> micah glasser (<a href="mailto:micahglasser@gmail.com?Subject=Re:%20No%20More%20Searle%20Please"><em>micahglasser@gmail.com</em></a>)<br>
<strong>Date:</strong> Thu Jan 19 2006 - 13:06:28 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13673.html">Woody Long: "Re: No More Searle Please"</a>
<li><strong>Previous message:</strong> <a href="13671.html">Jeff Medina: "META: Filtering out individuals"</a>
<li><strong>In reply to:</strong> <a href="13670.html">Richard Loosemore: "Re: No More Searle Please"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13675.html">Richard Loosemore: "Re: No More Searle Please"</a>
<li><strong>Reply:</strong> <a href="13675.html">Richard Loosemore: "Re: No More Searle Please"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13672">[ date ]</a>
<a href="index.html#13672">[ thread ]</a>
<a href="subject.html#13672">[ subject ]</a>
<a href="author.html#13672">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
The problem with Searle' critique is quite simple - he begins with the false
<br>
assumption that a machine can pass a Turing test with some sort of
<br>
functionalist language table. No machine has ever been abler to genuinely
<br>
answer questions in a fashion that would satisfy the Turing test using such
<br>
methods. Yet Searle pretends that a machine can already pass a Turing test
<br>
using such &quot;card shuffling&quot; techniques and then proceeds to show that the
<br>
Turing test can't possibly be a genuine indicator of human level
<br>
intelligence because it is being accomplished through such a trivial
<br>
technique. This whole line of thinking is just wrong and is philosophically
<br>
indefensible. It may turn out that brains are not UTMs  (Jeff Hawkins et al)
<br>
but it still stands that if a UTM can pass a genuine Turing test then it is
<br>
necessarily as intelligent as a human since the intelligence of humans are
<br>
measured through their linguistic capacity. If you presented me with 20
<br>
different interlocutors I could, after interviewing them all, have a very
<br>
good idea of which were the most intelligent through how well they were able
<br>
to formulate responses to my questions. This ability is not trivial - it IS
<br>
human intelligence. The fact that people are still talking about Searle and
<br>
his charlatan claims is just evidence of how philosophically illiterate the
<br>
world has become.
<br>
<p>One more thing. In response to Daniel, If you believe that there can be
<br>
evidence for consciousness I would love to know what that would be. Until I
<br>
have been made aware of such a test I hold that the very idea is ridiculous
<br>
<p>On 1/19/06, Richard Loosemore &lt;<a href="mailto:rpwl@lightlink.com?Subject=Re:%20No%20More%20Searle%20Please">rpwl@lightlink.com</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; Daniel,
</em><br>
<em>&gt;
</em><br>
<em>&gt; In spite of your comments (below), I stand by what I said.  I was trying
</em><br>
<em>&gt; to kill the Searle argument because there is a very, very simple reason
</em><br>
<em>&gt; why Searle's idea was ridiculous, but unfortunately all the other
</em><br>
<em>&gt; discussion about related issues, which occurred in abundance in the
</em><br>
<em>&gt; original BBS replies and in the years since then, has given the
</em><br>
<em>&gt; misleading impression that the original argument had some merit.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I will try to explain why I say this, and address the points you make.
</em><br>
<em>&gt;
</em><br>
<em>&gt; First, it is difficult to argue about what *exactly* Searle was claiming
</em><br>
<em>&gt; in his original paper, because in an important sense there was no such
</em><br>
<em>&gt; thing as &quot;exactly what he said&quot; -- he used vague language and subtle
</em><br>
<em>&gt; innuendos at certain crucial points of the argument, so if you try to
</em><br>
<em>&gt; pin down the fine print you find that it all starts to get very slippery.
</em><br>
<em>&gt;
</em><br>
<em>&gt; As example I will cite the way you phrase his claim.  You say:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;He claims ... that no additional understanding is created anywhere, in
</em><br>
<em>&gt; the room or in the man, and so Strong AI is false.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; How exactly does Searle arrive at this conclusion?  In Step 1 he argues
</em><br>
<em>&gt; that the English speaking person does not &quot;understand&quot; Chinese.  If we
</em><br>
<em>&gt; are reasonable, we must agree with him.  In Step 2 he says that this is
</em><br>
<em>&gt; like a computer implementing a program (since the English speaker is
</em><br>
<em>&gt; merely implementing a computer program).  In Step 3 he goes on to
</em><br>
<em>&gt; conclude that THEREFORE when we look at a computer running a Chinese
</em><br>
<em>&gt; understanding program, we have no right to say that the computer
</em><br>
<em>&gt; &quot;understands&quot; or is &quot;conscious of&quot; what it is doing, any more than we
</em><br>
<em>&gt; would claim that the English person in his example understands Chinese.
</em><br>
<em>&gt;
</em><br>
<em>&gt; My beef, of course, was with Step 2.  The system of mind-on-top-of-mind
</em><br>
<em>&gt; is most definitely NOT the same as a system of mind-on-top-of-computer.
</em><br>
<em>&gt;   He is only able to pull his conclusion out of the hat by pointing to
</em><br>
<em>&gt; the understanding system that is implementing the Chinese programme
</em><br>
<em>&gt; (namely the English speaking person), and asking whether *that*
</em><br>
<em>&gt; understanding system knows Chinese.  He appeals to our intuitions.  If
</em><br>
<em>&gt; he had proposed that the Chinese program be implemented on top of some
</em><br>
<em>&gt; other substrate, like a tinkertoy computer (or any of the other
</em><br>
<em>&gt; gloriously elaborate substrates that people have discussed over the
</em><br>
<em>&gt; years) he could not have persuaded our intuition to agree with him.  If
</em><br>
<em>&gt; he had used *anything* else except an intelligence at that lower level,
</em><br>
<em>&gt; he would not have been able to harness our intuition pump and get us to
</em><br>
<em>&gt; agree with him that the &quot;substrate itself&quot; was clearly not understanding
</em><br>
<em>&gt; Chinese.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But by doing this he implicitly argued that the Strong AI people were
</em><br>
<em>&gt; claiming that in his weird mind-on-mind case the understanding would
</em><br>
<em>&gt; bleed through from the top level system to the substrate system.  He
</em><br>
<em>&gt; skips this step in his argument. (Of course!  He doesn't want us to
</em><br>
<em>&gt; notice that he slipped it in!).  If he had inserted a Step 2(a): &quot;The
</em><br>
<em>&gt; Strong AI claim is that when you implement an AI program on top of a
</em><br>
<em>&gt; dumb substrate like a computer it is exactly equivalent to implementing
</em><br>
<em>&gt; the same AI program on top of a substrate that happens to have its own
</em><br>
<em>&gt; intelligence,&quot; the Strong AI people would have jumped up and down and
</em><br>
<em>&gt; cried Foul!, flatly refusing to accept that this was their claim.  They
</em><br>
<em>&gt; would say:  we have never argued that intelligence bleeds through from
</em><br>
<em>&gt; one level to another when you implement an intelligent system on top of
</em><br>
<em>&gt; another intelligent system, so your argument breaks down at Step 2 and
</em><br>
<em>&gt; Step 2(a):  the English speaking person inside the room is NOT analogous
</em><br>
<em>&gt; to a computer, so nothing can be deduced about the Strong AI argument.
</em><br>
<em>&gt;
</em><br>
<em>&gt; So when you say:  &quot;Searle never claims that since 'understanding doesn't
</em><br>
<em>&gt; bleed through,' Strong AI is false.&quot; I am afraid I have to disagree
</em><br>
<em>&gt; completely.  It is implicit, but he relies on that implicit claim.
</em><br>
<em>&gt;
</em><br>
<em>&gt; And while you correctly point out that the &quot;Systems Argument&quot; is a good
</em><br>
<em>&gt; characterisation of what the AI people do believe, I say that this is
</em><br>
<em>&gt; mere background, and is not the correct and immediate response to
</em><br>
<em>&gt; Searle's thought experiment, because Searle had already undermined his
</em><br>
<em>&gt; argument when he invented a freak system, and then put false words into
</em><br>
<em>&gt; the mouths of Strong AI proponents.  My point is that the argument was
</em><br>
<em>&gt; dead at that point:  we do not need to go on and say what Strong AI
</em><br>
<em>&gt; people do believe, in order to address his argument.
</em><br>
<em>&gt;
</em><br>
<em>&gt; In fact, everyone played into his hands by going off on all these other
</em><br>
<em>&gt; speculations about other weird cases.  What is frustrating is that the
</em><br>
<em>&gt; original replies should ALL have started out with the above argument as
</em><br>
<em>&gt; a preface, then, after declaring the Chinese Room argument to be invalid
</em><br>
<em>&gt; and completely dead, they should have proceeded to raise all those
</em><br>
<em>&gt; interesting and speculative ideas about what Strong AI would say about
</em><br>
<em>&gt; various cases of different AI implementations.  Instead, Searle and his
</em><br>
<em>&gt; camp argued the toss about all those other ideas as if each one were a
</em><br>
<em>&gt; failed attempt to demolish his thought experiment.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Finally, Searle's response to the mind-on-mind argument was grossly
</em><br>
<em>&gt; inadequate.  Just more of the same trick that he had already tried to
</em><br>
<em>&gt; pull off.  When he tries to argue that Strong AI makes this or that
</em><br>
<em>&gt; claim about what a Turing machine &quot;understands,&quot; he is simply trying to
</em><br>
<em>&gt; generalise the existing Strong AI claims into new territory (the
</em><br>
<em>&gt; territory of his freak system) and then quickly say how the Strong AI
</em><br>
<em>&gt; people would extend their old turing-machine language into this new
</em><br>
<em>&gt; case.  And since he again puts a false claim onto their mouths, he is
</em><br>
<em>&gt; simply repeating the previous invalid argument.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The concept of a Turing machine has not, to my knowledge, been
</em><br>
<em>&gt; adequately extended to say anything valid about the situation of one
</em><br>
<em>&gt; Turing machine implemented at an extreme high level on top of another
</em><br>
<em>&gt; Turing machine.  In fact, I am not sure it could be extended, even in
</em><br>
<em>&gt; principle.  For example:  if I get a regular computer running an
</em><br>
<em>&gt; extremely complex piece of software that does many things, but also
</em><br>
<em>&gt; implements a Turing machine task at a very high level, which latter is
</em><br>
<em>&gt; then used to run some other software, there is nothing whatsoever in the
</em><br>
<em>&gt; theory of Turing machines that says that the pieces of software running
</em><br>
<em>&gt; at the highest level and at the lowest level have to relate to one
</em><br>
<em>&gt; another:  in an important sense they can be completely independent.
</em><br>
<em>&gt; There are no constraints whatsoever between them.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The lower level software might be managing several autonomous space
</em><br>
<em>&gt; probes zipping about the solar system and interacting with one another
</em><br>
<em>&gt; occasionally in such a way as to implement a distributed Turing machine,
</em><br>
<em>&gt; while this Turing machine itself may be running a painting program.  But
</em><br>
<em>&gt; there is no earthly reason why &quot;Turing machine equivalence&quot; arguments
</em><br>
<em>&gt; could be used to say that the spacecraft system is &quot;really&quot; the same as
</em><br>
<em>&gt; a painting program, or has all the functions of a painting program.
</em><br>
<em>&gt; This is, as I say, a freak case that was never within the scope of the
</em><br>
<em>&gt; original claims:  the original claims have to be extended to deal with
</em><br>
<em>&gt; the freak case, and Searle disingenuous extension is not the one that
</em><br>
<em>&gt; Strong AI proponents would have made.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Richard Loosemore.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Daniel Radetsky wrote:
</em><br>
<em>&gt; &gt; On Wed, 18 Jan 2006 08:09:43 -0500
</em><br>
<em>&gt; &gt; Richard Loosemore &lt;<a href="mailto:rpwl@lightlink.com?Subject=Re:%20No%20More%20Searle%20Please">rpwl@lightlink.com</a>&gt; wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&gt;END OF ARGUMENT.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; If you don't want to talk about Searle, don't talk about Searle, but
</em><br>
<em>&gt; don't give
</em><br>
<em>&gt; &gt; a set of reasons why not to talk about Searle, and expect me not to
</em><br>
<em>&gt; respond.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&gt;He proposed a computational system implemented on top of another
</em><br>
<em>&gt; &gt;&gt;computational system (Chinese understander implemented on top of English
</em><br>
<em>&gt; &gt;&gt;understander).  This is a mind-on-top-of-mind case that has no relevance
</em><br>
<em>&gt; &gt;&gt;whatsoever to either (a) human minds, or (b) an AI implemented on a
</em><br>
<em>&gt; &gt;&gt;computer.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; This is a version of a response made a long time ago by Jerry Fodor.
</em><br>
<em>&gt; Searle
</em><br>
<em>&gt; &gt; responded, and very adequately I think. Since the mind-on-top-of-mind is
</em><br>
<em>&gt; &gt; something which is implementing a Turing machine, it is the same thing
</em><br>
<em>&gt; &gt; computation-wise as anything else implementing a Turing machine. So it
</em><br>
<em>&gt; is
</em><br>
<em>&gt; &gt; completely relevant to whether or not a computer (something implementing
</em><br>
<em>&gt; a
</em><br>
<em>&gt; &gt; Turing Machine) can be conscious.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I'll be blunt: if you want to challenge Searle, use the Systems Reply.
</em><br>
<em>&gt; It's the
</em><br>
<em>&gt; &gt; only reply that actually works, since it explicitly disagrees with
</em><br>
<em>&gt; Searle's
</em><br>
<em>&gt; &gt; fundamental premise (consciousness is a causal, not a formal, process).
</em><br>
<em>&gt; You
</em><br>
<em>&gt; &gt; went on to make something like the Systems Reply in the rest of your
</em><br>
<em>&gt; post, but
</em><br>
<em>&gt; &gt; against a straw man. Searle never claims that since 'understanding
</em><br>
<em>&gt; doesn't bleed
</em><br>
<em>&gt; &gt; through,' Strong AI is false. He claims (in the original article; I
</em><br>
<em>&gt; haven't read
</em><br>
<em>&gt; &gt; everything on this subject) that no additional understanding is created
</em><br>
<em>&gt; &gt; anywhere, in the room or in the man, and so Strong AI is false. That is,
</em><br>
<em>&gt; the
</em><br>
<em>&gt; &gt; fact that 'understanding doesn't bleed through' is only a piece of the
</em><br>
<em>&gt; puzzle.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Daniel
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<p><p><p><pre>
--
I swear upon the alter of God, eternal hostility to every form of tyranny
over the mind of man. - Thomas Jefferson
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13673.html">Woody Long: "Re: No More Searle Please"</a>
<li><strong>Previous message:</strong> <a href="13671.html">Jeff Medina: "META: Filtering out individuals"</a>
<li><strong>In reply to:</strong> <a href="13670.html">Richard Loosemore: "Re: No More Searle Please"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13675.html">Richard Loosemore: "Re: No More Searle Please"</a>
<li><strong>Reply:</strong> <a href="13675.html">Richard Loosemore: "Re: No More Searle Please"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13672">[ date ]</a>
<a href="index.html#13672">[ thread ]</a>
<a href="subject.html#13672">[ subject ]</a>
<a href="author.html#13672">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
