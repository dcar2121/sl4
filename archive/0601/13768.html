<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Some considerations about AGI</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Re: Some considerations about AGI">
<meta name="Date" content="2006-01-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Some considerations about AGI</h1>
<!-- received="Mon Jan 23 17:56:43 2006" -->
<!-- isoreceived="20060124005643" -->
<!-- sent="Tue, 24 Jan 2006 19:59:53 -0500" -->
<!-- isosent="20060125005953" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Re: Some considerations about AGI" -->
<!-- id="43D6CD89.5030102@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="638d4e150601231531p49d82c81ldddf6121f5afb7c2@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Some%20considerations%20about%20AGI"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Tue Jan 24 2006 - 17:59:53 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13769.html">Mike Dougherty: "Re: Some considerations about AGI"</a>
<li><strong>Previous message:</strong> <a href="13767.html">Tennessee Leeuwenburg: "Re: physical pain is bad (was Re: Dynamic ethics)"</a>
<li><strong>In reply to:</strong> <a href="13762.html">Ben Goertzel: "Re: Some considerations about AGI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13769.html">Mike Dougherty: "Re: Some considerations about AGI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13768">[ date ]</a>
<a href="index.html#13768">[ thread ]</a>
<a href="subject.html#13768">[ subject ]</a>
<a href="author.html#13768">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben and Eliezer,
<br>
<p>Now, I didn't say that the AGI would have to pass with a 100% score to 
<br>
be accepted as a genuine AGI ...
<br>
<p>Let me mention some of the motivations behind the test.
<br>
<p>1)  If some unscrupulous people were to trying to &quot;boost&quot; their chances 
<br>
of convincing potential investors that they had an AGI, they might 
<br>
decide to plant a secret link from their AGI to a team of people sitting 
<br>
in front of Google pages.  This is not something that would have been 
<br>
possible a mere five years ago, but today, someone could mount such a 
<br>
&quot;Clever Hans&quot; AGI by this method.  Asking the machine to switch between 
<br>
languages in its reply, without too much advance warning, would make it 
<br>
impossible for someone to have a Google Hans team squirreled away 
<br>
somewhere.  Too many languages.
<br>
<p>2)  Some people might be tempted to put a cheap front end on the Cyc 
<br>
database and claim that they had a machine that answered factual 
<br>
questions.  I am not familiar with the extent of Cyc's performance, but 
<br>
this would not amount to an AGI, and my litmus test would be if the 
<br>
system could talk and think in nuance and metaphor.  Anything able to 
<br>
understand the significance of poetry would do nicely.
<br>
<p>3)  Please note that RGE seem to claim that their system can read and 
<br>
understand pretty much anything.  Under those circumstances, the Heim 
<br>
Theory problem would be a challenge, to be sure, but it might be able to 
<br>
make a serious attempt to understand his original German and interpret 
<br>
it.  That said, I'd be willing to leave out the Heim Theory test, if 
<br>
that were considered too brutal.
<br>
<p>4)  On a more general level, the test questions were meant to be at the 
<br>
extremely-difficult-crossword-puzzle level.
<br>
<p>I do not suggest this as a general AGI test:  this was more targeted at 
<br>
what I saw to be the extreme claims made by RGE, and to guard against fraud.
<br>
<p><p>Richard Loosemore.
<br>
<p><p><p><p>Ben Goertzel wrote:
<br>
<em>&gt; I certainly see the point of Richard's proposed test.  A Novamente
</em><br>
<em>&gt; with &quot;human-adult-level intelligence&quot; (and yes, I understand this is a
</em><br>
<em>&gt; somewhat bogus term, but I do think it has value as an ambiguous
</em><br>
<em>&gt; natural language expression) connected to the Net would certainly be
</em><br>
<em>&gt; able to answer these questions.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; However, I also see the point of Eliezer's objection.  One could make
</em><br>
<em>&gt; very substantial progress toward AGI, going far beyond all existing AI
</em><br>
<em>&gt; systems, without having a system capable of answering this sort of
</em><br>
<em>&gt; question.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If we proceed as hoped with Novamente (which will begin by eventually
</em><br>
<em>&gt; getting adequate funding to hire a few dedicated staff so that the
</em><br>
<em>&gt; project can proceed at a non-ridiculously-slow pace) then there will
</em><br>
<em>&gt; be intermediary stages between where we are now and human-adult-level
</em><br>
<em>&gt; intelligence, which will be obviously impressive and exciting and
</em><br>
<em>&gt; fascinating yet not involving the ability to answer Richard's
</em><br>
<em>&gt; questions...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; -- Ben G
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; On 1/23/06, Eliezer S. Yudkowsky &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Some%20considerations%20about%20AGI">sentience@pobox.com</a>&gt; wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;Richard Loosemore wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;1) Give an introduction to Heim's theory of quantum gravity, in
</em><br>
<em>&gt;&gt;&gt;sufficient detail to allow a Physics graduate to understand it.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Good heavens.  For a nonhuman paired with a human physics graduate, this
</em><br>
<em>&gt;&gt;is a superintelligence test, not an AGI test.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;RGE Corp. made some audacious claims, but this isn't fair even to them.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Making some allowance for hype, I think that a fair challenge to RGE, or
</em><br>
<em>&gt;&gt;any other commercial AGI company, is handing them a task sufficiently
</em><br>
<em>&gt;&gt;far beyond state-of-the-art that they could beat up Google if they
</em><br>
<em>&gt;&gt;succeeded.  Say, scoring above 1000 on the SAT - though maybe that's
</em><br>
<em>&gt;&gt;still much too difficult.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Dan Clemmensen wrote on 2002.03.01:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;Arthur T. Murray wrote:
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;&gt;Now that Technological Singularity has arrived in the form of
</em><br>
<em>&gt;&gt;&gt;&gt;<a href="http://www.scn.org/~mentifex/mind4th.html">http://www.scn.org/~mentifex/mind4th.html</a> -- Robot Seed AI --
</em><br>
<em>&gt;&gt;&gt;&gt;you all deserve this big Thank_You for your successful work.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;Sorry, Arthur, but I'd guess that there is an implicit rule
</em><br>
<em>&gt;&gt;&gt;about announcement of an AI-driven singularity: the announcement
</em><br>
<em>&gt;&gt;&gt;must come from the AI, not the programmer. Now if you claim to
</em><br>
<em>&gt;&gt;&gt;be a composite human/AI based SI, the rules are different:
</em><br>
<em>&gt;&gt;&gt;I personally would expect the announcement in some unmistakable form
</em><br>
<em>&gt;&gt;&gt;such as e.g. a message in letters of fire written on the face
</em><br>
<em>&gt;&gt;&gt;of the moon.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;--
</em><br>
<em>&gt;&gt;Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt;&gt;Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13769.html">Mike Dougherty: "Re: Some considerations about AGI"</a>
<li><strong>Previous message:</strong> <a href="13767.html">Tennessee Leeuwenburg: "Re: physical pain is bad (was Re: Dynamic ethics)"</a>
<li><strong>In reply to:</strong> <a href="13762.html">Ben Goertzel: "Re: Some considerations about AGI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13769.html">Mike Dougherty: "Re: Some considerations about AGI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13768">[ date ]</a>
<a href="index.html#13768">[ thread ]</a>
<a href="subject.html#13768">[ subject ]</a>
<a href="author.html#13768">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
