<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: 3 &quot;Real&quot; Conscious Machines [WAS Re: Singularity: A rock 'em, shock'em ending soon?]</title>
<meta name="Author" content="Woody Long (ironanchorpress@earthlink.net)">
<meta name="Subject" content="Re: 3 &quot;Real&quot; Conscious Machines [WAS Re: Singularity: A rock 'em, shock'em ending soon?]">
<meta name="Date" content="2006-01-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: 3 &quot;Real&quot; Conscious Machines [WAS Re: Singularity: A rock 'em, shock'em ending soon?]</h1>
<!-- received="Tue Jan 17 21:12:47 2006" -->
<!-- isoreceived="20060118041247" -->
<!-- sent="Tue, 17 Jan 2006 23:12:34 -0500" -->
<!-- isosent="20060118041234" -->
<!-- name="Woody Long" -->
<!-- email="ironanchorpress@earthlink.net" -->
<!-- subject="Re: 3 &quot;Real&quot; Conscious Machines [WAS Re: Singularity: A rock 'em, shock'em ending soon?]" -->
<!-- id="410-22006131841234718@earthlink.net" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3 &quot;Real&quot; Conscious Machines [WAS Re: Singularity: A rock 'em, shock'em ending soon?]" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Woody Long (<a href="mailto:ironanchorpress@earthlink.net?Subject=Re:%203%20&quot;Real&quot;%20Conscious%20Machines%20[WAS%20Re:%20Singularity:%20A%20rock%20'em,%20shock'em%20ending%20soon?]"><em>ironanchorpress@earthlink.net</em></a>)<br>
<strong>Date:</strong> Tue Jan 17 2006 - 21:12:34 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13594.html">Damien Broderick: "Re: 3 &quot;Real&quot; Conscious Machines [WAS Re: Singularity: A rock 'em, shock'em ending soon?]"</a>
<li><strong>Previous message:</strong> <a href="13592.html">Phil Goetz: "Re: 3 &quot;Real&quot; Conscious Machines [WAS Re: Singularity: A rock   'em, shock'em ending soon?]"</a>
<li><strong>Maybe in reply to:</strong> <a href="13576.html">Robin Lee Powell: "Re: 3 &quot;Real&quot; Conscious Machines [WAS Re: Singularity: A rock 'em, shock'em ending soon?]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13594.html">Damien Broderick: "Re: 3 &quot;Real&quot; Conscious Machines [WAS Re: Singularity: A rock 'em, shock'em ending soon?]"</a>
<li><strong>Reply:</strong> <a href="13594.html">Damien Broderick: "Re: 3 &quot;Real&quot; Conscious Machines [WAS Re: Singularity: A rock 'em, shock'em ending soon?]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13593">[ date ]</a>
<a href="index.html#13593">[ thread ]</a>
<a href="subject.html#13593">[ subject ]</a>
<a href="author.html#13593">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; [Original Message]
</em><br>
<em>&gt; From: Phil Goetz &lt;<a href="mailto:philgoetz@yahoo.com?Subject=Re:%203%20&quot;Real&quot;%20Conscious%20Machines%20[WAS%20Re:%20Singularity:%20A%20rock%20'em,%20shock'em%20ending%20soon?]">philgoetz@yahoo.com</a>&gt;
</em><br>
<em>&gt; To: &lt;<a href="mailto:sl4@sl4.org?Subject=Re:%203%20&quot;Real&quot;%20Conscious%20Machines%20[WAS%20Re:%20Singularity:%20A%20rock%20'em,%20shock'em%20ending%20soon?]">sl4@sl4.org</a>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is not a variant of the Turing Test.  The point of the
</em><br>
<em>&gt; Turing Test was NOT to define a way to know if a computer
</em><br>
<em>&gt; was intelligent.  The point was that properties such as
</em><br>
<em>&gt; intelligence are defined, observed, and ACKNOWLEDGED
</em><br>
<em>&gt; -- OBSERVATIONALLY.  The point was that there is no way
</em><br>
<em>&gt; to know what &quot;consciousness&quot; should look like, or what
</em><br>
<em>&gt; sort of circuit implements it, and so the best one can do
</em><br>
<em>&gt; is say that if it acts like a person, it's a person.
</em><br>
<p>Agreed: &quot;If it acts like a person then it's a person.&quot; However, things are
<br>
not as black and white as  you make it seem. 
<br>
<p>It is recently the Turing Test that is coming under fire for being a
<br>
meaningless non-test for a conscious machine. A good analysis of both
<br>
Turing and Searle can be found at <a href="http://www.consciousentities.com/">http://www.consciousentities.com/</a>.  In
<br>
their article &quot;The Loebner Prize&quot; Oct 2005 they say -
<br>
<p>&quot;I believe serious AI researchers have, on the whole, tended to stay away
<br>
from the Loebner (it seems that in 1995 Marvin Minsky offered a &quot;prize&quot; of
<br>
$100 to anyone who could make Hugh Loebner desist from holding the
<br>
contest), but it has also had support from serious intellectuals. Ned Block
<br>
appears to have been one of this year's judges; Daniel Dennett chaired the
<br>
panel during some of the early years (but eventually withdrew when he could
<br>
not get agreement to his plans, which would have seen a number of more
<br>
'serious' AI challenges introduced as preliminaries to the main event).
<br>
It's certainly an entertaining event - sometimes the transcripts of the
<br>
conversations have a demented but irresistible inadvertent humour about
<br>
them - but I wonder how Alan Turing would have felt about it. Nowadays the
<br>
contest rather underlines the failure of Turing's prediction that we should
<br>
have conversational computers by the end of the twentieth century.
<br>
Personally, I think the other two points which come across most strongly
<br>
from the event are the continuing weakness of the chatbots and the
<br>
unserviceable qualities of the Turing test itself.&quot;
<br>
<p>... that's the whole problem with the Turing test principle. If you find a
<br>
group of people who want to believe the computer is talking sensibly, and
<br>
they make enough allowances for it, you can easily get a positive result. A
<br>
program which just bats back people's own input in the form of questions,
<br>
like Joseph Weizenbaum's famous Eliza, is quite capable of fooling some
<br>
people. On the other hand, if you have a skilled forensic examination, it's
<br>
always going to be possible to find inconsistencies in the conversation of
<br>
any human-like entity which hasn't actually lived a genuine human life. So
<br>
what's the point? 
<br>
<p><p>And further, from <a href="http://www.consciousentities.com/stories.htm#turing">http://www.consciousentities.com/stories.htm#turing</a> we
<br>
find:
<br>
<p>[Turing] thought that by the end of the twentieth century a computer would
<br>
be able to fool an average respondent during several minutes of apparently
<br>
ordinary conversation. The really controversial claim, however, was that
<br>
this kind of test could establish that a computer was, or at least deserved
<br>
to be treated as, conscious.
<br>
<p>The weak form of this claim (that if something seems to be conscious we
<br>
might as well treat it as if it were for the time being) is hard to argue
<br>
with, but not particularly interesting. Against the stronger form (that
<br>
things which pass the test really are conscious), it can be argued that
<br>
what makes someone conscious is not their external behaviour, or
<br>
specifically their ability to hold an intelligent conversation, but what
<br>
goes on inside their heads. 
<br>
<p>Do their responses spring from a **real understanding of the
<br>
conversation?**  In response, supporters of the test might ask how we know
<br>
anyone is conscious other than by deductions based on the intelligence of
<br>
their behaviour (conversational behaviour being an especially demanding
<br>
variety).
<br>
<p>The real problem with the Turing test is that it doesn't work. Suppose we
<br>
got incoherent gibberish through the teleprinter, or the words 'What are
<br>
you talking about?', or a string of Xs, or nothing, every time. Would that
<br>
prove that there wasn't a stupid or angry human being on the other end? Or
<br>
suppose we get perfect, sophisticated answers to our questions. Does that
<br>
prove they aren't a set of pre-recorded answers being selected by a cunning
<br>
but witless algorithm, or by a long run of good luck? No, and no. For a
<br>
test of this kind to work, there would have to be a question which human
<br>
beings invariably answered one way and computers invariably answered
<br>
another. Clearly there is no such question. Really the whole thing is a
<br>
misapplication of Leibniz's Law .&quot;
<br>
<p><em>&gt;
</em><br>
<em>&gt; What Searle was proposing - and what Woody is proposing -
</em><br>
<em>&gt; is exactly the sort of meaningless non-test that Turing
</em><br>
<em>&gt; was objecting to, where one looks at a system and tries
</em><br>
<em>&gt; to determine, by intuition about its operation, whether
</em><br>
<em>&gt; it is conscious.  This is futile.  Woody has not proposed
</em><br>
<em>&gt; any test that can be carried out by a human.
</em><br>
<em>&gt;
</em><br>
<p>Actually, Searle implied it in his Chinese Room story. Find a clear
<br>
description of Searle at
<br>
<a href="http://www.consciousentities.com/stories.htm#chineseroom">http://www.consciousentities.com/stories.htm#chineseroom</a>
<br>
<p>Here is a snippet - 
<br>
<p>Searle has no problem with the idea that some machine **other than a
<br>
[classic] digital computer [let's call it a post-classical droid machine]
<br>
might one day be conscious**: he accepts that the brain is a machine,
<br>
anyway. The practicalities of diagnosing consciousness are not the issue;
<br>
the point is what it is you are trying to diagnose. Of course Searle is not
<br>
impressed by the mere combination of arguments he has rejected
<br>
individually. Simulating a brain is no good; a simulation of rain doesn't
<br>
make you wet: you could simulate synapses with a system of water pipes
<br>
which the man in the room controls: just as obviously as in the original
<br>
example, he still doesn't understand the stories he is asked about. Using
<br>
the outputs to control a robot rather than answer questions makes no
<br>
difference and adds no understanding. It seems highly implausible to
<br>
attribute understanding to an arbitrary 'system' made up of the conjunction
<br>
of the man and some rules. If necessary, the man can memorise the rules:
<br>
then the whole 'system' is in his memory, but he still doesn't understand
<br>
the Chinese. [So these systems must be ruled out as conscious machines]
<br>
<p>So Searle is on to something here.  If the CPU/ CPU system CAN understand
<br>
the incoming language as a human does, in more precise words,
<br>
&quot;receives/processes the incoming language in exactly the same way as human
<br>
level consciousness receives/processes it, then it CAN be said to be a
<br>
conscious machine. To EVALUATE a system therefore we need to know first how
<br>
human level consciousness receives/processes incoming language. Then we can
<br>
check to see if the system is doing the same thing. This is the implied
<br>
Searle Chinese Room Test in its essence. 
<br>
<p>We can see an application of it in the Pascal Challenge that Ben Goertzel
<br>
asked me to respond to (and I will ASAP). The idea of this challenge is
<br>
that **human level consciousness can receive/process incoming language and
<br>
understand it so as to be able to perform textual entailment recognition.**
<br>
If this is a basic essential attribute of human level consciousness or a
<br>
developed skill is open to question. However, the Pascal Challenge does
<br>
show us - in general anyway - one way human level consciousness
<br>
receives/processes incoming language, and so it is fair to ask builders of
<br>
machine consciousness if their conscious machine can understand the
<br>
incoming language in the same way as the human level consciousness does.
<br>
And as the Pascal Challenge people know, this version of the Searle Test
<br>
can be EVALUATED.
<br>
<p>&nbsp;- Ken Woody Long
<br>
<a href="http://www.artificial-lifeforms-lab.blogspot.com/">http://www.artificial-lifeforms-lab.blogspot.com/</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13594.html">Damien Broderick: "Re: 3 &quot;Real&quot; Conscious Machines [WAS Re: Singularity: A rock 'em, shock'em ending soon?]"</a>
<li><strong>Previous message:</strong> <a href="13592.html">Phil Goetz: "Re: 3 &quot;Real&quot; Conscious Machines [WAS Re: Singularity: A rock   'em, shock'em ending soon?]"</a>
<li><strong>Maybe in reply to:</strong> <a href="13576.html">Robin Lee Powell: "Re: 3 &quot;Real&quot; Conscious Machines [WAS Re: Singularity: A rock 'em, shock'em ending soon?]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13594.html">Damien Broderick: "Re: 3 &quot;Real&quot; Conscious Machines [WAS Re: Singularity: A rock 'em, shock'em ending soon?]"</a>
<li><strong>Reply:</strong> <a href="13594.html">Damien Broderick: "Re: 3 &quot;Real&quot; Conscious Machines [WAS Re: Singularity: A rock 'em, shock'em ending soon?]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13593">[ date ]</a>
<a href="index.html#13593">[ thread ]</a>
<a href="subject.html#13593">[ subject ]</a>
<a href="author.html#13593">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
