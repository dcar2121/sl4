<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: No More Searle Please</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Re: No More Searle Please">
<meta name="Date" content="2006-01-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: No More Searle Please</h1>
<!-- received="Thu Jan 19 14:29:29 2006" -->
<!-- isoreceived="20060119212929" -->
<!-- sent="Thu, 19 Jan 2006 16:27:57 -0500" -->
<!-- isosent="20060119212757" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Re: No More Searle Please" -->
<!-- id="43D0045D.1030905@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="23bd28ec0601191206p267fe2a7kd07444442a37fb64@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20No%20More%20Searle%20Please"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Thu Jan 19 2006 - 14:27:57 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13676.html">Damien Broderick: "Re: No More Searle Please"</a>
<li><strong>Previous message:</strong> <a href="13674.html">Robin Lee Powell: "Re: No More Searle Please"</a>
<li><strong>In reply to:</strong> <a href="13672.html">micah glasser: "Re: No More Searle Please"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13695.html">Charles D Hixson: "Re: No More Searle Please"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13675">[ date ]</a>
<a href="index.html#13675">[ thread ]</a>
<a href="subject.html#13675">[ subject ]</a>
<a href="author.html#13675">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Searle didn't made the claim you suggest:  he was talking about the 
<br>
person in the room following the procedures necessary to act as a Turing 
<br>
Machine and implement by hand *any* computer program, not any specific 
<br>
type of intelligent software.  He would wriggle out from underneath that 
<br>
attack.
<br>
<p>I attacked Searle on a different plane.
<br>
<p>Your last comment confuses me a little:  in my response to Daniel I did 
<br>
not try to defend the idea of &quot;evidence&quot; for consciousness.
<br>
<p>As for the *idea* of consciousness being ridiculous .... that is another 
<br>
kettle of fish entirely!  I am writing a paper on the subject so I will 
<br>
save my comments for when that is done.
<br>
<p>Richard
<br>
<p><p><p>micah glasser wrote:
<br>
<em>&gt; The problem with Searle' critique is quite simple - he begins with the 
</em><br>
<em>&gt; false assumption that a machine can pass a Turing test with some sort of 
</em><br>
<em>&gt; functionalist language table. No machine has ever been abler to 
</em><br>
<em>&gt; genuinely answer questions in a fashion that would satisfy the Turing 
</em><br>
<em>&gt; test using such methods. Yet Searle pretends that a machine can already 
</em><br>
<em>&gt; pass a Turing test using such &quot;card shuffling&quot; techniques and then 
</em><br>
<em>&gt; proceeds to show that the Turing test can't possibly be a genuine 
</em><br>
<em>&gt; indicator of human level intelligence because it is being accomplished 
</em><br>
<em>&gt; through such a trivial technique. This whole line of thinking is just 
</em><br>
<em>&gt; wrong and is philosophically indefensible. It may turn out that brains 
</em><br>
<em>&gt; are not UTMs  (Jeff Hawkins et al) but it still stands that if a UTM can 
</em><br>
<em>&gt; pass a genuine Turing test then it is necessarily as intelligent as a 
</em><br>
<em>&gt; human since the intelligence of humans are measured through their 
</em><br>
<em>&gt; linguistic capacity. If you presented me with 20 different interlocutors 
</em><br>
<em>&gt; I could, after interviewing them all, have a very good idea of which 
</em><br>
<em>&gt; were the most intelligent through how well they were able to formulate 
</em><br>
<em>&gt; responses to my questions. This ability is not trivial - it IS human 
</em><br>
<em>&gt; intelligence. The fact that people are still talking about Searle and 
</em><br>
<em>&gt; his charlatan claims is just evidence of how philosophically illiterate 
</em><br>
<em>&gt; the world has become.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; One more thing. In response to Daniel, If you believe that there can be 
</em><br>
<em>&gt; evidence for consciousness I would love to know what that would be. 
</em><br>
<em>&gt; Until I have been made aware of such a test I hold that the very idea is 
</em><br>
<em>&gt; ridiculous
</em><br>
<em>&gt; 
</em><br>
<em>&gt; On 1/19/06, *Richard Loosemore* &lt;<a href="mailto:rpwl@lightlink.com?Subject=Re:%20No%20More%20Searle%20Please">rpwl@lightlink.com</a> 
</em><br>
<em>&gt; &lt;mailto:<a href="mailto:rpwl@lightlink.com?Subject=Re:%20No%20More%20Searle%20Please">rpwl@lightlink.com</a>&gt;&gt; wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     Daniel,
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     In spite of your comments (below), I stand by what I said.  I was trying
</em><br>
<em>&gt;     to kill the Searle argument because there is a very, very simple reason
</em><br>
<em>&gt;     why Searle's idea was ridiculous, but unfortunately all the other
</em><br>
<em>&gt;     discussion about related issues, which occurred in abundance in the
</em><br>
<em>&gt;     original BBS replies and in the years since then, has given the
</em><br>
<em>&gt;     misleading impression that the original argument had some merit.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     I will try to explain why I say this, and address the points you make.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     First, it is difficult to argue about what *exactly* Searle was claiming
</em><br>
<em>&gt;     in his original paper, because in an important sense there was no such
</em><br>
<em>&gt;     thing as &quot;exactly what he said&quot; -- he used vague language and subtle
</em><br>
<em>&gt;     innuendos at certain crucial points of the argument, so if you try to
</em><br>
<em>&gt;     pin down the fine print you find that it all starts to get very
</em><br>
<em>&gt;     slippery.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     As example I will cite the way you phrase his claim.  You say:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     &quot;He claims ... that no additional understanding is created anywhere, in
</em><br>
<em>&gt;     the room or in the man, and so Strong AI is false.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     How exactly does Searle arrive at this conclusion?  In Step 1 he argues
</em><br>
<em>&gt;     that the English speaking person does not &quot;understand&quot; Chinese.  If we
</em><br>
<em>&gt;     are reasonable, we must agree with him.  In Step 2 he says that this is
</em><br>
<em>&gt;     like a computer implementing a program (since the English speaker is
</em><br>
<em>&gt;     merely implementing a computer program).  In Step 3 he goes on to
</em><br>
<em>&gt;     conclude that THEREFORE when we look at a computer running a Chinese
</em><br>
<em>&gt;     understanding program, we have no right to say that the computer
</em><br>
<em>&gt;     &quot;understands&quot; or is &quot;conscious of&quot; what it is doing, any more than we
</em><br>
<em>&gt;     would claim that the English person in his example understands Chinese.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     My beef, of course, was with Step 2.  The system of mind-on-top-of-mind
</em><br>
<em>&gt;     is most definitely NOT the same as a system of mind-on-top-of-computer.
</em><br>
<em>&gt;       He is only able to pull his conclusion out of the hat by pointing to
</em><br>
<em>&gt;     the understanding system that is implementing the Chinese programme
</em><br>
<em>&gt;     (namely the English speaking person), and asking whether *that*
</em><br>
<em>&gt;     understanding system knows Chinese.  He appeals to our intuitions.  If
</em><br>
<em>&gt;     he had proposed that the Chinese program be implemented on top of some
</em><br>
<em>&gt;     other substrate, like a tinkertoy computer (or any of the other
</em><br>
<em>&gt;     gloriously elaborate substrates that people have discussed over the
</em><br>
<em>&gt;     years) he could not have persuaded our intuition to agree with him.  If
</em><br>
<em>&gt;     he had used *anything* else except an intelligence at that lower level,
</em><br>
<em>&gt;     he would not have been able to harness our intuition pump and get us to
</em><br>
<em>&gt;     agree with him that the &quot;substrate itself&quot; was clearly not
</em><br>
<em>&gt;     understanding
</em><br>
<em>&gt;     Chinese.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     But by doing this he implicitly argued that the Strong AI people were
</em><br>
<em>&gt;     claiming that in his weird mind-on-mind case the understanding would
</em><br>
<em>&gt;     bleed through from the top level system to the substrate system.  He
</em><br>
<em>&gt;     skips this step in his argument. (Of course!  He doesn't want us to
</em><br>
<em>&gt;     notice that he slipped it in!).  If he had inserted a Step 2(a): &quot;The
</em><br>
<em>&gt;     Strong AI claim is that when you implement an AI program on top of a
</em><br>
<em>&gt;     dumb substrate like a computer it is exactly equivalent to implementing
</em><br>
<em>&gt;     the same AI program on top of a substrate that happens to have its own
</em><br>
<em>&gt;     intelligence,&quot; the Strong AI people would have jumped up and down and
</em><br>
<em>&gt;     cried Foul!, flatly refusing to accept that this was their claim.  They
</em><br>
<em>&gt;     would say:  we have never argued that intelligence bleeds through from
</em><br>
<em>&gt;     one level to another when you implement an intelligent system on top of
</em><br>
<em>&gt;     another intelligent system, so your argument breaks down at Step 2 and
</em><br>
<em>&gt;     Step 2(a):  the English speaking person inside the room is NOT analogous
</em><br>
<em>&gt;     to a computer, so nothing can be deduced about the Strong AI argument.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     So when you say:  &quot;Searle never claims that since 'understanding doesn't
</em><br>
<em>&gt;     bleed through,' Strong AI is false.&quot; I am afraid I have to disagree
</em><br>
<em>&gt;     completely.  It is implicit, but he relies on that implicit claim.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     And while you correctly point out that the &quot;Systems Argument&quot; is a good
</em><br>
<em>&gt;     characterisation of what the AI people do believe, I say that this is
</em><br>
<em>&gt;     mere background, and is not the correct and immediate response to
</em><br>
<em>&gt;     Searle's thought experiment, because Searle had already undermined his
</em><br>
<em>&gt;     argument when he invented a freak system, and then put false words into
</em><br>
<em>&gt;     the mouths of Strong AI proponents.  My point is that the argument was
</em><br>
<em>&gt;     dead at that point:  we do not need to go on and say what Strong AI
</em><br>
<em>&gt;     people do believe, in order to address his argument.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     In fact, everyone played into his hands by going off on all these other
</em><br>
<em>&gt;     speculations about other weird cases.  What is frustrating is that the
</em><br>
<em>&gt;     original replies should ALL have started out with the above argument as
</em><br>
<em>&gt;     a preface, then, after declaring the Chinese Room argument to be invalid
</em><br>
<em>&gt;     and completely dead, they should have proceeded to raise all those
</em><br>
<em>&gt;     interesting and speculative ideas about what Strong AI would say about
</em><br>
<em>&gt;     various cases of different AI implementations.  Instead, Searle and his
</em><br>
<em>&gt;     camp argued the toss about all those other ideas as if each one were a
</em><br>
<em>&gt;     failed attempt to demolish his thought experiment.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     Finally, Searle's response to the mind-on-mind argument was grossly
</em><br>
<em>&gt;     inadequate.  Just more of the same trick that he had already tried to
</em><br>
<em>&gt;     pull off.  When he tries to argue that Strong AI makes this or that
</em><br>
<em>&gt;     claim about what a Turing machine &quot;understands,&quot; he is simply trying to
</em><br>
<em>&gt;     generalise the existing Strong AI claims into new territory (the
</em><br>
<em>&gt;     territory of his freak system) and then quickly say how the Strong AI
</em><br>
<em>&gt;     people would extend their old turing-machine language into this new
</em><br>
<em>&gt;     case.  And since he again puts a false claim onto their mouths, he is
</em><br>
<em>&gt;     simply repeating the previous invalid argument.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     The concept of a Turing machine has not, to my knowledge, been
</em><br>
<em>&gt;     adequately extended to say anything valid about the situation of one
</em><br>
<em>&gt;     Turing machine implemented at an extreme high level on top of another
</em><br>
<em>&gt;     Turing machine.  In fact, I am not sure it could be extended, even in
</em><br>
<em>&gt;     principle.  For example:  if I get a regular computer running an
</em><br>
<em>&gt;     extremely complex piece of software that does many things, but also
</em><br>
<em>&gt;     implements a Turing machine task at a very high level, which latter is
</em><br>
<em>&gt;     then used to run some other software, there is nothing whatsoever in
</em><br>
<em>&gt;     the
</em><br>
<em>&gt;     theory of Turing machines that says that the pieces of software running
</em><br>
<em>&gt;     at the highest level and at the lowest level have to relate to one
</em><br>
<em>&gt;     another:  in an important sense they can be completely independent.
</em><br>
<em>&gt;     There are no constraints whatsoever between them.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     The lower level software might be managing several autonomous space
</em><br>
<em>&gt;     probes zipping about the solar system and interacting with one another
</em><br>
<em>&gt;     occasionally in such a way as to implement a distributed Turing
</em><br>
<em>&gt;     machine,
</em><br>
<em>&gt;     while this Turing machine itself may be running a painting program.  But
</em><br>
<em>&gt;     there is no earthly reason why &quot;Turing machine equivalence&quot; arguments
</em><br>
<em>&gt;     could be used to say that the spacecraft system is &quot;really&quot; the same as
</em><br>
<em>&gt;     a painting program, or has all the functions of a painting program.
</em><br>
<em>&gt;     This is, as I say, a freak case that was never within the scope of the
</em><br>
<em>&gt;     original claims:  the original claims have to be extended to deal with
</em><br>
<em>&gt;     the freak case, and Searle disingenuous extension is not the one that
</em><br>
<em>&gt;     Strong AI proponents would have made.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     Richard Loosemore.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     Daniel Radetsky wrote:
</em><br>
<em>&gt;      &gt; On Wed, 18 Jan 2006 08:09:43 -0500
</em><br>
<em>&gt;      &gt; Richard Loosemore &lt;<a href="mailto:rpwl@lightlink.com?Subject=Re:%20No%20More%20Searle%20Please">rpwl@lightlink.com</a>
</em><br>
<em>&gt;     &lt;mailto:<a href="mailto:rpwl@lightlink.com?Subject=Re:%20No%20More%20Searle%20Please">rpwl@lightlink.com</a>&gt;&gt; wrote:
</em><br>
<em>&gt;      &gt;
</em><br>
<em>&gt;      &gt;
</em><br>
<em>&gt;      &gt;&gt;END OF ARGUMENT.
</em><br>
<em>&gt;      &gt;
</em><br>
<em>&gt;      &gt;
</em><br>
<em>&gt;      &gt; If you don't want to talk about Searle, don't talk about Searle,
</em><br>
<em>&gt;     but don't give
</em><br>
<em>&gt;      &gt; a set of reasons why not to talk about Searle, and expect me not
</em><br>
<em>&gt;     to respond.
</em><br>
<em>&gt;      &gt;
</em><br>
<em>&gt;      &gt;
</em><br>
<em>&gt;      &gt;&gt;He proposed a computational system implemented on top of another
</em><br>
<em>&gt;      &gt;&gt;computational system (Chinese understander implemented on top of
</em><br>
<em>&gt;     English
</em><br>
<em>&gt;      &gt;&gt;understander).  This is a mind-on-top-of-mind case that has no
</em><br>
<em>&gt;     relevance
</em><br>
<em>&gt;      &gt;&gt;whatsoever to either (a) human minds, or (b) an AI implemented on a
</em><br>
<em>&gt;      &gt;&gt;computer.
</em><br>
<em>&gt;      &gt;
</em><br>
<em>&gt;      &gt;
</em><br>
<em>&gt;      &gt; This is a version of a response made a long time ago by Jerry
</em><br>
<em>&gt;     Fodor. Searle
</em><br>
<em>&gt;      &gt; responded, and very adequately I think. Since the
</em><br>
<em>&gt;     mind-on-top-of-mind is
</em><br>
<em>&gt;      &gt; something which is implementing a Turing machine, it is the same
</em><br>
<em>&gt;     thing
</em><br>
<em>&gt;      &gt; computation-wise as anything else implementing a Turing machine.
</em><br>
<em>&gt;     So it is
</em><br>
<em>&gt;      &gt; completely relevant to whether or not a computer (something
</em><br>
<em>&gt;     implementing a
</em><br>
<em>&gt;      &gt; Turing Machine) can be conscious.
</em><br>
<em>&gt;      &gt;
</em><br>
<em>&gt;      &gt; I'll be blunt: if you want to challenge Searle, use the Systems
</em><br>
<em>&gt;     Reply. It's the
</em><br>
<em>&gt;      &gt; only reply that actually works, since it explicitly disagrees
</em><br>
<em>&gt;     with Searle's
</em><br>
<em>&gt;      &gt; fundamental premise (consciousness is a causal, not a formal,
</em><br>
<em>&gt;     process). You
</em><br>
<em>&gt;      &gt; went on to make something like the Systems Reply in the rest of
</em><br>
<em>&gt;     your post, but
</em><br>
<em>&gt;      &gt; against a straw man. Searle never claims that since
</em><br>
<em>&gt;     'understanding doesn't bleed
</em><br>
<em>&gt;      &gt; through,' Strong AI is false. He claims (in the original article;
</em><br>
<em>&gt;     I haven't read
</em><br>
<em>&gt;      &gt; everything on this subject) that no additional understanding is
</em><br>
<em>&gt;     created
</em><br>
<em>&gt;      &gt; anywhere, in the room or in the man, and so Strong AI is false.
</em><br>
<em>&gt;     That is, the
</em><br>
<em>&gt;      &gt; fact that 'understanding doesn't bleed through' is only a piece
</em><br>
<em>&gt;     of the puzzle.
</em><br>
<em>&gt;      &gt;
</em><br>
<em>&gt;      &gt; Daniel
</em><br>
<em>&gt;      &gt;
</em><br>
<em>&gt;      &gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; -- 
</em><br>
<em>&gt; I swear upon the alter of God, eternal hostility to every form of 
</em><br>
<em>&gt; tyranny over the mind of man. - Thomas Jefferson            
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13676.html">Damien Broderick: "Re: No More Searle Please"</a>
<li><strong>Previous message:</strong> <a href="13674.html">Robin Lee Powell: "Re: No More Searle Please"</a>
<li><strong>In reply to:</strong> <a href="13672.html">micah glasser: "Re: No More Searle Please"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13695.html">Charles D Hixson: "Re: No More Searle Please"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13675">[ date ]</a>
<a href="index.html#13675">[ thread ]</a>
<a href="subject.html#13675">[ subject ]</a>
<a href="author.html#13675">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
