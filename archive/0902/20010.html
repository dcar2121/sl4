<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] An attempt at empathic AI</title>
<meta name="Author" content="Johnicholas Hines (johnicholas.hines@gmail.com)">
<meta name="Subject" content="Re: [sl4] An attempt at empathic AI">
<meta name="Date" content="2009-02-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] An attempt at empathic AI</h1>
<!-- received="Sun Feb 22 12:49:54 2009" -->
<!-- isoreceived="20090222194954" -->
<!-- sent="Sun, 22 Feb 2009 14:49:42 -0500" -->
<!-- isosent="20090222194942" -->
<!-- name="Johnicholas Hines" -->
<!-- email="johnicholas.hines@gmail.com" -->
<!-- subject="Re: [sl4] An attempt at empathic AI" -->
<!-- id="8bad43370902221149h7dc29e46mfacaf154cbe8d8d0@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="add7caae0902200639g2e8d103ah64c25ca7546a0f62@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Johnicholas Hines (<a href="mailto:johnicholas.hines@gmail.com?Subject=Re:%20[sl4]%20An%20attempt%20at%20empathic%20AI"><em>johnicholas.hines@gmail.com</em></a>)<br>
<strong>Date:</strong> Sun Feb 22 2009 - 12:49:42 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20011.html">Matt Mahoney: "Re: [sl4] Re: Uploads coming first would be good, right?."</a>
<li><strong>Previous message:</strong> <a href="20009.html">John K Clark: "Re: [sl4] Re: Uploads coming first would be good, right?."</a>
<li><strong>In reply to:</strong> <a href="20002.html">Petter Wingren-Rasmussen: "[sl4] An attempt at empathic AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20013.html">Matt Mahoney: "Re: [sl4] An attempt at empathic AI"</a>
<li><strong>Reply:</strong> <a href="20013.html">Matt Mahoney: "Re: [sl4] An attempt at empathic AI"</a>
<li><strong>Reply:</strong> <a href="20039.html">Petter Wingren-Rasmussen: "Re: [sl4] An attempt at empathic AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20010">[ date ]</a>
<a href="index.html#20010">[ thread ]</a>
<a href="subject.html#20010">[ subject ]</a>
<a href="author.html#20010">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Fri, Feb 20, 2009 at 9:39 AM, Petter Wingren-Rasmussen
<br>
&lt;<a href="mailto:petterwr@gmail.com?Subject=Re:%20[sl4]%20An%20attempt%20at%20empathic%20AI">petterwr@gmail.com</a>&gt; wrote:
<br>
<em>&gt; As I mentioned in this thread i think an AGI with hardcoded dogmatic rules
</em><br>
<em>&gt; will have some serious drawbacks in the long run. I will try to show an
</em><br>
<em>&gt; alternative here, that still will remain friendly to humans.
</em><br>
<p>My main interest (relative to this list) is safety protocols for
<br>
research and development of AGI. Accordingly, I'm interested in the
<br>
risks of your proposed AGI R&amp;D effort.
<br>
<p>If I understand correctly, you propose using evolution or something
<br>
similar, inside a sequence of synthetic environments. The environments
<br>
are designed to simultaneously select for friendly emotions and useful
<br>
capabilities. The sequence of environments can be considered one
<br>
environment with a sequential structure.
<br>
<p>The teaching environment is described as having two properties. First,
<br>
it's intended to be easy for the adaptive process (whether that's
<br>
evolution or stochastic hillclimbing or whatever) to learn. (This is
<br>
similar to Melanie Mitchell's Royal Road fitness functions.) Second,
<br>
it is an implicit fitness function defined by coevolutionary
<br>
interactions.
<br>
<p>There are a couple of risks that I think I can see:
<br>
<p>1. Genetic algorithms (along with brute-force search and other generic
<br>
learning strategies) produce holographic output. By &quot;holographic&quot; I
<br>
mean every part relates to every other part; it is not modular the way
<br>
humans normally design and understand things.
<br>
<p>The modular/means-ends designs that humans generate, generally mirror
<br>
the proofs, or the informal arguments that humans use to understand
<br>
_how_ the design accomplishes the goal. (The human kind of design is
<br>
explained and advocated in Suh's &quot;Axiomatic Design&quot;.)
<br>
<p>Humans have areas (such as digital filters) where we frequently use
<br>
holographic designs, but (at least in that case) there is a lot of
<br>
math around it, explaining how it is impossible to have everything
<br>
that you might want, and how to transform a specification into a
<br>
design.
<br>
<p>Holographic AGI means you can't examine the structure of the AGI and
<br>
predict how it will behave. This is risky.
<br>
<p>2. Sorry, I've been obsessing about this simple two-dimensional model
<br>
of capability increase - please let me ramble for a bit before getting
<br>
to the point.
<br>
<p>When you apply an optimization process like simulated annealing or a
<br>
genetic algorithm and measure its performance, we expect to see the
<br>
performance (fitness) curve upward - first derivative is generally
<br>
positive (improvement), second is generally negative (diminishing
<br>
returns).
<br>
<p>If you took something which is very finely tuned, and then start
<br>
making random changes, then you expect to see a decrease in
<br>
performance. Also, if you took something which was completely random,
<br>
and then start making random changes, then you expect to see the (bad)
<br>
performance stay the same. So now we have first derivative generally
<br>
negative (decrease), and second derivative positive (flattening).
<br>
<p>The question is: if you apply optimization pressure towards one
<br>
performance measure, and then tracked its performance on a different
<br>
performance measure, what dynamics do you expect to see? It depends
<br>
whether the performance measures are correlated or not.
<br>
<p>If you have two different performance measures that are very
<br>
uncorrelated, for example:
<br>
A. Optimizing the number of &quot;1&quot; bits in a bitstring.
<br>
B. Optimizing the number of &quot;1&quot; bits in a hash of the bitstring.
<br>
<p>Then the other performance dynamics should look just like the random dynamics.
<br>
<p>If the two different performance measures are actually equivalent,
<br>
then the other performance dynamics should look just like the the
<br>
original performance dynamics.
<br>
<p>So here is my model:
<br>
The derivative with respect to time of the original performance
<br>
measure (p) is increased by the selective pressure (1 (hey, it's a
<br>
simplistic model!)), and decreased by a factor proportional to how
<br>
optimized the system is with respect to p already (p*P_DECAY). In
<br>
total:
<br>
<p>D[p] = 1 - p * P_DECAY
<br>
<p>The derivative with respect to time of the other performance measure
<br>
(p_prime) is increased by a factor proportional to the similarity of
<br>
the performance measures (SIMILARITY) and decreased by a factor
<br>
proportional to how optimized the system is with respect to p_prime
<br>
already (p_prime * P_PRIME_DECAY). In total:
<br>
<p>D[p_prime] = SIMILARITY - p_prime * P_PRIME_DECAY
<br>
<p>Researchers into narrow AI are not particularly concerned about the AI
<br>
becoming wildly successful and taking over the world. I think this is
<br>
because they're implicitly using this sort of model. One measure is
<br>
whatever they're actually working on (chess or image processing or
<br>
whatever), and the other measure is &quot;capability of taking over the
<br>
world&quot;, and they believe the &quot;SIMILARITY&quot; is low between those two
<br>
things.
<br>
<p>This model is relevant to the risks of evolutionary algorithms,
<br>
because we're evolving in a synthetic environment, and then
<br>
generalizing to the behavior in the real world. In order to correctly
<br>
generalize, the &quot;SIMILARITY&quot; factor needs to be high.
<br>
<p>I think we don't understand how to argue that the &quot;SIMILARITY&quot; factor
<br>
is high: That scoring well in the simulated environment will lead to
<br>
what we really want to accomplish. At the very least, the research and
<br>
development program also should study how we can validly, reasonably
<br>
predict behavior in the real world, based on success in the simulated
<br>
world.
<br>
<p>If I understand correctly, this simulated/real gap is what Yudkowsky's
<br>
&quot;External Reference Semantics&quot; is intended to address.
<br>
<p>Thanks for reading.
<br>
<p>Johnicholas
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20011.html">Matt Mahoney: "Re: [sl4] Re: Uploads coming first would be good, right?."</a>
<li><strong>Previous message:</strong> <a href="20009.html">John K Clark: "Re: [sl4] Re: Uploads coming first would be good, right?."</a>
<li><strong>In reply to:</strong> <a href="20002.html">Petter Wingren-Rasmussen: "[sl4] An attempt at empathic AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20013.html">Matt Mahoney: "Re: [sl4] An attempt at empathic AI"</a>
<li><strong>Reply:</strong> <a href="20013.html">Matt Mahoney: "Re: [sl4] An attempt at empathic AI"</a>
<li><strong>Reply:</strong> <a href="20039.html">Petter Wingren-Rasmussen: "Re: [sl4] An attempt at empathic AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20010">[ date ]</a>
<a href="index.html#20010">[ thread ]</a>
<a href="subject.html#20010">[ subject ]</a>
<a href="author.html#20010">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:04 MDT
</em></small></p>
</body>
</html>
