<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] FAI development within academia.</title>
<meta name="Author" content="Roko Mijic (rmijic@googlemail.com)">
<meta name="Subject" content="Re: [sl4] FAI development within academia.">
<meta name="Date" content="2009-02-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] FAI development within academia.</h1>
<!-- received="Tue Feb 24 18:49:44 2009" -->
<!-- isoreceived="20090225014944" -->
<!-- sent="Wed, 25 Feb 2009 01:49:32 +0000" -->
<!-- isosent="20090225014932" -->
<!-- name="Roko Mijic" -->
<!-- email="rmijic@googlemail.com" -->
<!-- subject="Re: [sl4] FAI development within academia." -->
<!-- id="d9a290cd0902241749sd458b69ue5c13f79226eb9@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="402e01e70902241654s3260952dwc121f091af5ae64c@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Roko Mijic (<a href="mailto:rmijic@googlemail.com?Subject=Re:%20[sl4]%20FAI%20development%20within%20academia."><em>rmijic@googlemail.com</em></a>)<br>
<strong>Date:</strong> Tue Feb 24 2009 - 18:49:32 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20046.html">Stuart Armstrong: "Re: [sl4] foundationalism"</a>
<li><strong>Previous message:</strong> <a href="20044.html">Eliezer Yudkowsky: "Re: [sl4] FAI development within academia."</a>
<li><strong>In reply to:</strong> <a href="20044.html">Eliezer Yudkowsky: "Re: [sl4] FAI development within academia."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20048.html">Vladimir Nesov: "Re: [sl4] FAI development within academia."</a>
<li><strong>Reply:</strong> <a href="20048.html">Vladimir Nesov: "Re: [sl4] FAI development within academia."</a>
<li><strong>Reply:</strong> <a href="20051.html">William Pearson: "Re: [sl4] FAI development within academia."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20045">[ date ]</a>
<a href="index.html#20045">[ thread ]</a>
<a href="subject.html#20045">[ subject ]</a>
<a href="author.html#20045">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Since this thread seems to have been somewhat distracted, I might just
<br>
bring us back to the original more immediately salient question.
<br>
Perhaps we could Matt's quantitative theory to another thread?
<br>
<p>&quot;I want to help out with FAI research, what do I do?&quot;
<br>
<p>[implicit: I am university-level educated in a mathematical science
<br>
and I know about cognitive biases, and I want to work in academia with
<br>
clever people]
<br>
<p>Since I've had someone else ask me what the answer to this question
<br>
is, I think it might be good to discuss it here.
<br>
<p>I'll throw in what I've learned: there seems to be a global lack of
<br>
communication, co-ordination and consensus amongst those of us
<br>
interested in working in FAI and AGI. Some researchers basically
<br>
neglect friendliness(!). Some consider human friendly AGI to be
<br>
impossible, and conclude that we're all screwed. Singinst and
<br>
associates seem to appreciate that these are mistakes.
<br>
<p>But you can't get a PhD at SIAI... (correct me if I'm wrong here,
<br>
guys? possible collaboration with Singularity University?)
<br>
<p>Talking to mainstream AI guys about this subject ... is usually a
<br>
painful demonstration of human irrationality and cognitive bias as
<br>
they execute a search for plausible arguments concluding that only
<br>
narrow AI research is worth doing.
<br>
<p>Roko
<br>
<p><p>2009/2/25 Eliezer Yudkowsky &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20[sl4]%20FAI%20development%20within%20academia.">sentience@pobox.com</a>&gt;:
<br>
<em>&gt; Allow me to summarize the premises and conclusions of this argument:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1)  Markets are absolutely, perfectly, exactly efficient over all
</em><br>
<em>&gt; domains and between all times.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 2)  AI would make a quadrillion-dollar profit.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Conclusion:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 3)  AI can be obtained at any time by, and only by, spending a
</em><br>
<em>&gt; quadrillion dollars.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It doesn't lack for audacity, but I'm afraid it somewhat lacks for sanity.
</em><br>
<em>&gt;
</em><br>
<em>&gt; On Tue, Feb 24, 2009 at 4:40 PM, Matt Mahoney &lt;<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20FAI%20development%20within%20academia.">matmahoney@yahoo.com</a>&gt; wrote:
</em><br>
<em>&gt;&gt; --- On Mon, 2/23/09, J. Andrew Rogers &lt;<a href="mailto:andrew@ceruleansystems.com?Subject=Re:%20[sl4]%20FAI%20development%20within%20academia.">andrew@ceruleansystems.com</a>&gt; wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; On Feb 23, 2009, at 6:37 AM, Matt Mahoney wrote:
</em><br>
<em>&gt;&gt;&gt; &gt; --- On Sun, 2/22/09, Roko Mijic
</em><br>
<em>&gt;&gt;&gt; &lt;<a href="mailto:rmijic@googlemail.com?Subject=Re:%20[sl4]%20FAI%20development%20within%20academia.">rmijic@googlemail.com</a>&gt; wrote:
</em><br>
<em>&gt;&gt;&gt; &gt;
</em><br>
<em>&gt;&gt;&gt; &gt;&gt; One way to hasten the development of FAI is for me to seek to do
</em><br>
<em>&gt;&gt;&gt; &gt;&gt; research within academia. A disadvantage of this strategy is that
</em><br>
<em>&gt;&gt;&gt; &gt;&gt; academia is an open community, and anyone can potentially look at the
</em><br>
<em>&gt;&gt;&gt; &gt;&gt; results that the field is producing and use them to create uFAI.
</em><br>
<em>&gt;&gt;&gt; &gt;
</em><br>
<em>&gt;&gt;&gt; &gt; Unlikely. Nobody can build AI, much less FAI or uFAI.
</em><br>
<em>&gt;&gt;&gt; All the top people in the field like Yudkowsky, Minsky, and
</em><br>
<em>&gt;&gt;&gt; Kurzweil have realized the problem is too hard by
</em><br>
<em>&gt;&gt;&gt; themselves, so they are not actually writing any software.
</em><br>
<em>&gt;&gt;&gt; It has to be a global effort.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; Dripping with non sequitur and dubious assertion, FTW.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; The assertion that nobody can build AI is very weak
</em><br>
<em>&gt;&gt;&gt; conjecture; there is legitimate argument whether some or all
</em><br>
<em>&gt;&gt;&gt; of the people you list are &quot;top people in the
</em><br>
<em>&gt;&gt;&gt; field&quot;; there are plenty of other top people in the
</em><br>
<em>&gt;&gt;&gt; field that you failed to list that contradict the argument
</em><br>
<em>&gt;&gt;&gt; you are trying to make, raising the question of sample
</em><br>
<em>&gt;&gt;&gt; selection bias; it is not obvious, at least to me, that the
</em><br>
<em>&gt;&gt;&gt; people you list &quot;realized the problem is too hard by
</em><br>
<em>&gt;&gt;&gt; themselves&quot; in any case;  nor does it follow that it
</em><br>
<em>&gt;&gt;&gt; has to be a global effort; worse, given that we accept your
</em><br>
<em>&gt;&gt;&gt; first assertion, it does not follow by any reasonable
</em><br>
<em>&gt;&gt;&gt; calculus that I can think of that a &quot;global
</em><br>
<em>&gt;&gt;&gt; effort&quot; addresses that assertion.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; The quality of argumentation leaves a lot to be desired.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; So let me make my argument clear.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; First, we define what we are trying to build. There are two main goals of AI. First, to automate the economy (because we don't want to work) and second, to upload (because we don't want to die). The first problem is to build slaves (or if you prefer, servers) that understand language and vision and human behavior and that know a lot about individual people like customers and owners. The second problem is to create programs that simulate specific people, which requires solving very similar problems.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; The usual approach of individual researchers and small groups is to attempt to build the equivalent of one human brain that is slightly smarter than the inventor. Then that brain (or lots of copies of it) could (in theory) produce an improved version of itself, and so on, launching a fast takeoff singularity.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; But that will now work. For two reasons. First, unless you made your computer out of dirt, then you did not make AI by yourself. Being smarter does not give you any greater capability of building AI any more than an aeronautical engineer going back 500 years in time could build an airplane.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Second, you need to define &quot;smart&quot;. What does it mean to be twice as smart as the average human? It is not so obvious as you think. Humans are notoriously bad at recognizing genius, as demonstrated by the persecution of Socrates, Galileo, and Turing. Today we still award Nobel prizes for work done decades earlier, after the rest of the world has caught up.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Here are some possible definitions of &quot;twice as smart&quot;:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; - Able to solve problems twice as fast.
</em><br>
<em>&gt;&gt; - Able to learn twice as fast.
</em><br>
<em>&gt;&gt; - Able to remember twice as much.
</em><br>
<em>&gt;&gt; - Able to make twice as much money.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Which all suggest that twice as smart means twice as many people. And we do know that groups do tend to make better decisions than individuals. When a contestant on &quot;Who wants to be a millionaire?&quot; asks the audience, the majority almost always comes up with the right answer. Countries that are run by power sharing groups tend to be nicer places to live than countries ruled by absolute dictators.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; But once again, we are unable to recognize the superior intelligence of groups. This goes beyond ego. If the members of a group always agreed with the majority, then the group could not possibly be any smarter than any member. So we should expect this property to apply not just to humans, but to AI at any level of intelligence.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Making a human brain equivalent and then lots of identical copies does not lead to greater intelligence. If we make the copies different through custom training, then you need to consider the training cost in your recursive self improvement (RSI) equation. If you make a single AI twice as smart as a human, you get the same effect as hiring two humans. In order for RSI to work, you have to be able to build AI at less cost than hiring people. The (exponential) rate of growth would be comparable to a company with access to cheap labor.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Here is my cost estimate for human equivalent AI:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; - Computing power: 10^17 operations per second
</em><br>
<em>&gt;&gt; - Memory: 10^15 bits
</em><br>
<em>&gt;&gt; - I/O: 10^9 bits per second
</em><br>
<em>&gt;&gt; - Knowledge: 10^9 bits
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Computing power and memory assume a brain sized neural network with 10^15 synapses and 10 ms resolution. I/O could be reduced to 10^7 bps if you exclude low level processing in the retina and cochlea. Knowledge is based on Landauer's estimate of human long term memory, e.g. our ability to recall words and pictures. It excludes procedural memory, such as knowing how to see or walk.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; We can project when CPU, memory, and I/O costs will be competitive with human labor using Moore's Law. The exact numbers are not important. An order of magnitude error only affects the answer by a few years. The threshold is not far off and may have already been crossed.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; The problem is knowledge. Software cost is not subject to Moore's Law. The knowledge needed to run the economy is stored in 4 x 10^9 brains. We can estimate the amount of overlap from the cost of replacing an employee. It can be a year's salary, and increasing as jobs become more specialized. We may reasonably assume 90% to 99% overlap, or 10^17 bits. The problem is that the public internet has only about 10^14 bits. The knowledge has to be extracted from human brains at 2 bits per second, at an average worldwide labor cost of US $5 per hour, or about $1 per KB.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; The global economy has a value of about $1 quadrillion (world GDP divided by market interest rates). Knowledge extraction will cost $100 trillion if we are able to identify in advance where the overlap is, and just extract what we don't already know. It is economically feasible, just not by any small group or in a short period of time. If we can't identify the overlap before extraction, it will cost $4 quadrillion.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; The cost of knowledge extraction presents a huge economic incentive to build a system of pervasive public surveillance, where everything you say and do is public knowledge. Note also that this solves the problem of uploading without the need for brain scanning.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; RSI does not require smarter than human intelligence. It requires cheaper than human intelligence. That requires a very expensive infrastructure to bring the cost down. That is what I mean by a global effort.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; -- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20FAI%20development%20within%20academia.">matmahoney@yahoo.com</a>
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; --
</em><br>
<em>&gt; Eliezer Yudkowsky
</em><br>
<em>&gt; Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<em>&gt;
</em><br>
<p><p><p><pre>
-- 
Roko Mijic
MSc by Research
University of Edinburgh
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20046.html">Stuart Armstrong: "Re: [sl4] foundationalism"</a>
<li><strong>Previous message:</strong> <a href="20044.html">Eliezer Yudkowsky: "Re: [sl4] FAI development within academia."</a>
<li><strong>In reply to:</strong> <a href="20044.html">Eliezer Yudkowsky: "Re: [sl4] FAI development within academia."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20048.html">Vladimir Nesov: "Re: [sl4] FAI development within academia."</a>
<li><strong>Reply:</strong> <a href="20048.html">Vladimir Nesov: "Re: [sl4] FAI development within academia."</a>
<li><strong>Reply:</strong> <a href="20051.html">William Pearson: "Re: [sl4] FAI development within academia."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20045">[ date ]</a>
<a href="index.html#20045">[ thread ]</a>
<a href="subject.html#20045">[ subject ]</a>
<a href="author.html#20045">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:04 MDT
</em></small></p>
</body>
</html>
