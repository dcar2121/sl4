<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] Friendly AIs vs Friendly Humans</title>
<meta name="Author" content="Brian Rabkin (daze.like.this@gmail.com)">
<meta name="Subject" content="Re: [sl4] Friendly AIs vs Friendly Humans">
<meta name="Date" content="2011-06-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] Friendly AIs vs Friendly Humans</h1>
<!-- received="Tue Jun 21 02:54:44 2011" -->
<!-- isoreceived="20110621085444" -->
<!-- sent="Tue, 21 Jun 2011 04:54:39 -0400" -->
<!-- isosent="20110621085439" -->
<!-- name="Brian Rabkin" -->
<!-- email="daze.like.this@gmail.com" -->
<!-- subject="Re: [sl4] Friendly AIs vs Friendly Humans" -->
<!-- id="BANLkTi=kvCtGnMKMUiCE1dKFKPNRo65KRg@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="BANLkTi=6djzBJeBXG+PDeVLgDZnD9BUeQQ@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brian Rabkin (<a href="mailto:daze.like.this@gmail.com?Subject=Re:%20[sl4]%20Friendly%20AIs%20vs%20Friendly%20Humans"><em>daze.like.this@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Jun 21 2011 - 02:54:39 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="21171.html">Jake Witmer: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<li><strong>Previous message:</strong> <a href="21169.html">Robin Lee Powell: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<li><strong>In reply to:</strong> <a href="21168.html">DataPacRat: "[sl4] Friendly AIs vs Friendly Humans"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21171.html">Jake Witmer: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21170">[ date ]</a>
<a href="index.html#21170">[ thread ]</a>
<a href="subject.html#21170">[ subject ]</a>
<a href="author.html#21170">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
*&quot;Unfriendly AI&quot; *
<br>
<p>When people use this term I don't know what they mean, since they could mean
<br>
malevolent AIs (which are presumably as rare as friendly ones), they could
<br>
mean to include indifferent AIs that happen to be ruinous, or they could
<br>
also mean to include indifferent AIs that happen to not cause problems.
<br>
<p>*&quot;From what I've overheard, one of the biggest difficulties with FAI is*
<br>
*that there are a wide variety of possible forms of AI, making it
<br>
difficult to determine what it would take to ensure Friendliness for
<br>
*
<br>
*any potential AI design.&quot;*
<br>
<p>I don't think that's a problem. Even if the vast majority of possible minds
<br>
had favorable and nurturing (etc.) intentions towards humanity, if when they
<br>
grew/learned/lived/modified their goals it was not predictable that they'd
<br>
keep their approach to humans, eventually they'd stumble into indifference
<br>
or malevolence towards us. So the unpredictability of goal development is
<br>
sufficient to cause the problem. Were goal development predictable and
<br>
stable, then the problem of making the first iteration nice would arise, but
<br>
I don't think that would be particularly hard compared to the whole rest of
<br>
building a synthetic mind, even with most possible minds indifferent to
<br>
humans.
<br>
<p>*&quot;human-like minds&quot;*
<br>
<p>This is unspecific enough that I don't have confidence I know what you're
<br>
saying. However, if you mean make a mind similar to human ones as a means
<br>
for it to be easier for it to have empathy with us, I see a few problems.
<br>
<p>1) A broad category of attempted solutions can be described as &quot;limit the
<br>
AI&quot;. Limit the AI's goals, limit the AI's mental organization, etc. These
<br>
fail in two ways: a) the limitations on the AI can be overcome by the AI
<br>
with enough thinking, b) the limitations on the AI weaken it such that it
<br>
can't reliably prevent a subsequent, unconstrained AI from being a
<br>
problematic AI.
<br>
<p>The problem is an AI can alter itself/its mind and its goals, if not your
<br>
AI, another's. Imagine: none of this kenken to strengthen connections
<br>
between parts of the brain, the AI can decide to make internal associations
<br>
stronger, weaker, etc., split systems as soon as it sees it would be optimal
<br>
to have a pair of specialists rather than a general one, not merely
<br>
repurpose evolved senses to new challenges like evolution but design its own
<br>
solutions, etc.
<br>
<p>2) It seems you're trying to invoke an aspect of mind that is an example of
<br>
what an AI will be very unlikely to have: tribalism of a sort that we have
<br>
because of our evolutionary history. 'You're similar to me--&gt;you're in my
<br>
in-group--&gt;I will treat you well, and you have the same mind structure so I
<br>
benefit': nothing like that is at all necessary as a component of a mind
<br>
made to think inductively. I don't know how much pure game theory would
<br>
impact likely AIs to be similar to us.
<br>
<p>*&quot;If not, does that imply that there*
<br>
*is no general FAI solution?&quot;*
<br>
<p>Not at all strongly. It's just a case of one approach not working, but it
<br>
isn't the the one I would have thought most likely to succeed. In general
<br>
one can test if any of several solutions will work by trying the best one
<br>
first, but that works less well the more difference in kind there is among
<br>
solutions, even when one has actually selected the best one first.
<br>
<p>E.g. if I have a ranking of the usefulness of characters in an RPG in which
<br>
I can choose only my fifth party member, If my four core members and the guy
<br>
at the top of the list wipe out at a boss fight, and the guy at the top of
<br>
the list was a fighter, I may yet be able to beat the boss by selecting a
<br>
mage lower down the list, even though the list is organized by general
<br>
strength. If I find out I was reading the list wrongly, and the character at
<br>
the top was actually the weakest character generally, then I really know my
<br>
wipe out isn't indicative of much and by reloading I may yet beat it.
<br>
Failing with a character indicates I am likely to fail with weaker or
<br>
similar characters, but characters both stronger and different in kind give
<br>
me an excellent chance.
<br>
<p><p><p>So it seems to me.
<br>
<p><p>On Tue, Jun 21, 2011 at 2:36 AM, DataPacRat &lt;<a href="mailto:datapacrat@gmail.com?Subject=Re:%20[sl4]%20Friendly%20AIs%20vs%20Friendly%20Humans">datapacrat@gmail.com</a>&gt; wrote:
<br>
<p><em>&gt; Since this list isn't officially closed down /quite/ yet, I'm hoping
</em><br>
<em>&gt; to take advantage of the remaining readers' insights to help me find
</em><br>
<em>&gt; the answer to a certain question - or, at least, help me find where
</em><br>
<em>&gt; the answer already is.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; My understanding of the Friendly AI problem is, roughly, that AIs
</em><br>
<em>&gt; could have all sorts of goal systems, many of which are rather
</em><br>
<em>&gt; unhealthy for humanity as we know it; and, due to the potential for
</em><br>
<em>&gt; rapid self-improvement, once any AI exists, it is highly likely to
</em><br>
<em>&gt; rapidly gain the power required to implement its goals whether we want
</em><br>
<em>&gt; it to or not. Thus certain people are trying to develop the parameters
</em><br>
<em>&gt; for a Friendly AI, one that will allow us humans to continue doing our
</em><br>
<em>&gt; own things (or some approximation thereof), or at least for avoiding
</em><br>
<em>&gt; the development of an Unfriendly AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; From what I've overheard, one of the biggest difficulties with FAI is
</em><br>
<em>&gt; that there are a wide variety of possible forms of AI, making it
</em><br>
<em>&gt; difficult to determine what it would take to ensure Friendliness for
</em><br>
<em>&gt; any potential AI design.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Could anyone here suggest any references on a much narrower subset of
</em><br>
<em>&gt; this problem: limiting the form of AI designs being considered to
</em><br>
<em>&gt; human-like minds (possibly including actual emulations of human
</em><br>
<em>&gt; minds), is it possible to solve the FAI problem for that subset - or,
</em><br>
<em>&gt; put another way, instead of preventing Unfriendly AIs and allowing
</em><br>
<em>&gt; only Friendly AIs, is it possible to avoid &quot;Unfriendly Humans&quot; and
</em><br>
<em>&gt; encourage &quot;Friendly Humans&quot;? If so, do such methods offer any insight
</em><br>
<em>&gt; into the generalized FAI problem? If not, does that imply that there
</em><br>
<em>&gt; is no general FAI solution?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; And, most importantly, how many false assumptions are behind these
</em><br>
<em>&gt; questions, and how can I best learn to correct them?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Thank you for your time,
</em><br>
<em>&gt; --
</em><br>
<em>&gt; DataPacRat
</em><br>
<em>&gt; lu .iacu'i ma krinu lo du'u .ei mi krici la'e di'u li'u traji lo ka
</em><br>
<em>&gt; vajni fo lo preti
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="21171.html">Jake Witmer: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<li><strong>Previous message:</strong> <a href="21169.html">Robin Lee Powell: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<li><strong>In reply to:</strong> <a href="21168.html">DataPacRat: "[sl4] Friendly AIs vs Friendly Humans"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21171.html">Jake Witmer: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21170">[ date ]</a>
<a href="index.html#21170">[ thread ]</a>
<a href="subject.html#21170">[ subject ]</a>
<a href="author.html#21170">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
