<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] Friendly AIs vs Friendly Humans</title>
<meta name="Author" content="Matt Mahoney (matmahoney@yahoo.com)">
<meta name="Subject" content="Re: [sl4] Friendly AIs vs Friendly Humans">
<meta name="Date" content="2011-06-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] Friendly AIs vs Friendly Humans</h1>
<!-- received="Tue Jun 21 08:43:19 2011" -->
<!-- isoreceived="20110621144319" -->
<!-- sent="Tue, 21 Jun 2011 07:43:13 -0700 (PDT)" -->
<!-- isosent="20110621144313" -->
<!-- name="Matt Mahoney" -->
<!-- email="matmahoney@yahoo.com" -->
<!-- subject="Re: [sl4] Friendly AIs vs Friendly Humans" -->
<!-- id="1308667393.57286.YahooMailRC@web39301.mail.mud.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="BANLkTi=6djzBJeBXG+PDeVLgDZnD9BUeQQ@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Matt Mahoney (<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20Friendly%20AIs%20vs%20Friendly%20Humans"><em>matmahoney@yahoo.com</em></a>)<br>
<strong>Date:</strong> Tue Jun 21 2011 - 08:43:13 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="21173.html">Jeff Medina: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<li><strong>Previous message:</strong> <a href="21171.html">Jake Witmer: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<li><strong>In reply to:</strong> <a href="21168.html">DataPacRat: "[sl4] Friendly AIs vs Friendly Humans"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21173.html">Jeff Medina: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21172">[ date ]</a>
<a href="index.html#21172">[ thread ]</a>
<a href="subject.html#21172">[ subject ]</a>
<a href="author.html#21172">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
DataPacRat &lt;<a href="mailto:datapacrat@gmail.com?Subject=Re:%20[sl4]%20Friendly%20AIs%20vs%20Friendly%20Humans">datapacrat@gmail.com</a>&gt; wrote:
<br>
<em>&gt; My understanding of the Friendly AI problem is, roughly, that AIs
</em><br>
<em>&gt; could have all sorts of goal systems, many of which are rather
</em><br>
<p><em>&gt; unhealthy for humanity as we know it;
</em><br>
<p>I think that viewing AI as a powerful goal directed optimization process is an 
<br>
over-simplification. Currently we have to pay people US $60 trillion per year 
<br>
globally to do work that machines could do if they were smarter. This is the 
<br>
force that is driving AI. Obviously it is a hard problem or we would have solved 
<br>
it by now.
<br>
<p>In <a href="http://mattmahoney.net/agi2.html">http://mattmahoney.net/agi2.html</a> I analysed the software, hardware, and 
<br>
training costs of building AI sufficient to automate the economy. Software is 
<br>
the easiest part, because it can't be more complex than the human genome, which 
<br>
is equivalent to a few million lines of code. It would cost $1 billion, which is 
<br>
insignificant compared to its value. For hardware, I estimate at 10^27 OPS and 
<br>
10^26 bytes to simulate 10^10 human brain sized neural networks. We might find 
<br>
more efficient models for language and vision (where the best known algorithms 
<br>
appear to be neural), but in any case Moore's Law should make the cost feasible 
<br>
in 20-30 years. But the most significant cost is that of training. In order for 
<br>
machines to do what you want (as opposed to what you tell them, as they do now), 
<br>
they have to know what you know. I estimate that the sum of human knowledge is 
<br>
10^17 to 10^18 bits, assuming Landauer's estimate of human long term memory 
<br>
capacity at 10^9 bits, and 90% to 99% mutual information between humans (based 
<br>
on a US Labor Dept. estimate that it costs 1% of lifetime earnings to replace an 
<br>
employee). Most of this knowledge is in human brains and not written down. (The 
<br>
searchable Internet is far smaller at 25 billion web pages). I figure there are 
<br>
two ways to extract this knowledge. One is through slow channels like speech and 
<br>
typing using a global system of public surveillance at a cost of $1 quadrillion 
<br>
assuming a global average wage rate of $5/hour. The other would be through 
<br>
advanced (and so far nonexistent) brain scanning technology at the synapse 
<br>
level. To be a viable alternative, it would have to cost less than $100,000 per 
<br>
scan.
<br>
<p>The result would be models of 7 billion human minds. A model is a function that 
<br>
takes sensory input and returns a prediction of your actions. Models would have 
<br>
many uses. A program could run thousands of high speed simulations of you in 
<br>
parallel to predict what actions would make you happy, or what actions would 
<br>
make you buy something. A model that carried out its predictions of your 
<br>
behavior in real time to control a robot or avatar would be an upload of you.
<br>
<p>This could hardly be described as a goal directed process, except to the extent 
<br>
that your goals would be modeled. The problem with the goal directed model is 
<br>
that we get into trouble with simple goals like &quot;be nice to humans&quot;, or even 
<br>
Eliezer Yudkowsky's CEV, which has a complexity of about 10^5 bits. In reality, 
<br>
we cannot even specify a goal, because its complexity is of the same order of 
<br>
magnitude as the whole of human knowledge. It is a description of what everyone 
<br>
wants, with conflicts somehow resolved according to complex rules. Our global 
<br>
legal system is a vain attempt to write down how to do this. An equivalent goal 
<br>
system for AI would have to specify behavior at the level of bit operations, 
<br>
without human judges to interpret ambiguity in the law.
<br>
<p>In any case, I am not convinced that the goal directed model is even useful. 
<br>
Does a thermostat &quot;want&quot; to keep the room at a constant temperature? Does a 
<br>
linear regression algorithm &quot;want&quot; to fit a straight line to a set of points? 
<br>
Does evolution &quot;want&quot; to maximize reproductive fitness? Any Turing machine could 
<br>
be described as having a &quot;goal&quot; of outputting whatever it happens to output. So 
<br>
what?
<br>
<p><em>&gt; and, due to the potential for&gt; rapid self-improvement, once any AI exists, it 
</em><br>
<em>&gt;is highly likely to
</em><br>
<em>&gt; rapidly gain the power required to implement its goals whether we want
</em><br>
<em>&gt; it to or not.
</em><br>
<p>Intelligence, whether defined as ability to simulate human behavior (Turing) or 
<br>
as expected reward (Legg and Hutter), depends practically on both knowledge and 
<br>
computing power. There are theoretical models that depend only on knowledge (a 
<br>
giant lookup table) or only on computing power (AIXI), but practical systems 
<br>
need both. A self improving AI can gain computing power but not knowledge. 
<br>
Knowledge comes from learning processes such as evolution and induction. 
<br>
Furthermore, reinforcement learning algorithms like evolution are slow because 
<br>
each reward/penalty decision (e.g. birth or death) adds at most 1 bit of 
<br>
knowledge.
<br>
<p>The problem of goal stability through self improvement is unsolved. I believe it 
<br>
will remain unsolved, because any goal other than maximizing reproductive 
<br>
fitness will lose in a competitive environment. The question then becomes how 
<br>
fast could a competing species of our own creation reproduce and evolve. Freitas 
<br>
analyzed this in <a href="http://www.foresight.org/nano/Ecophagy.html">http://www.foresight.org/nano/Ecophagy.html</a> Computation has a 
<br>
thermodynamic cost of kT ln 2 joules per bit operation. Molecular computing 
<br>
would be close to this limit, 100 times more efficient than neurons and a 
<br>
million times more efficient than silicon, but not much faster than current 
<br>
biological operations such as DNA-RNA-protein synthesis. Human extinction would 
<br>
take a few weeks due to the availability of solar energy.
<br>
<p>The system I described is friendly to the extent that people get what they want, 
<br>
but this may not be much better. People want to be happy. If you want to model a 
<br>
human mind as a goal directed process, then maximum happiness is a state of 
<br>
maximum utility. In this mental state, any thought or perception would be 
<br>
unpleasant because it would result in a different mental state with lower 
<br>
utility.
<br>
<p>&nbsp;-- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20Friendly%20AIs%20vs%20Friendly%20Humans">matmahoney@yahoo.com</a>
<br>
<p><p><p>----- Original Message ----
<br>
<em>&gt; From: DataPacRat &lt;<a href="mailto:datapacrat@gmail.com?Subject=Re:%20[sl4]%20Friendly%20AIs%20vs%20Friendly%20Humans">datapacrat@gmail.com</a>&gt;
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=Re:%20[sl4]%20Friendly%20AIs%20vs%20Friendly%20Humans">sl4@sl4.org</a>
</em><br>
<em>&gt; Sent: Tue, June 21, 2011 2:36:51 AM
</em><br>
<em>&gt; Subject: [sl4] Friendly AIs vs Friendly Humans
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Since this list isn't officially closed down /quite/ yet, I'm hoping
</em><br>
<em>&gt; to take  advantage of the remaining readers' insights to help me find
</em><br>
<em>&gt; the answer to a  certain question - or, at least, help me find where
</em><br>
<em>&gt; the answer already  is.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; My understanding of the Friendly AI problem is, roughly, that  AIs
</em><br>
<em>&gt; could have all sorts of goal systems, many of which are  rather
</em><br>
<em>&gt; unhealthy for humanity as we know it; and, due to the potential  for
</em><br>
<em>&gt; rapid self-improvement, once any AI exists, it is highly likely  to
</em><br>
<em>&gt; rapidly gain the power required to implement its goals whether we  want
</em><br>
<em>&gt; it to or not. Thus certain people are trying to develop the  parameters
</em><br>
<em>&gt; for a Friendly AI, one that will allow us humans to continue doing  our
</em><br>
<em>&gt; own things (or some approximation thereof), or at least for  avoiding
</em><br>
<em>&gt; the development of an Unfriendly AI.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; From what I've  overheard, one of the biggest difficulties with FAI is
</em><br>
<em>&gt; that there are a wide  variety of possible forms of AI, making it
</em><br>
<em>&gt; difficult to determine what it  would take to ensure Friendliness for
</em><br>
<em>&gt; any potential AI design.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Could  anyone here suggest any references on a much narrower subset of
</em><br>
<em>&gt; this problem:  limiting the form of AI designs being considered to
</em><br>
<em>&gt; human-like minds  (possibly including actual emulations of human
</em><br>
<em>&gt; minds), is it possible to  solve the FAI problem for that subset - or,
</em><br>
<em>&gt; put another way, instead of  preventing Unfriendly AIs and allowing
</em><br>
<em>&gt; only Friendly AIs, is it possible to  avoid &quot;Unfriendly Humans&quot; and
</em><br>
<em>&gt; encourage &quot;Friendly Humans&quot;? If so, do such  methods offer any insight
</em><br>
<em>&gt; into the generalized FAI problem? If not, does that  imply that there
</em><br>
<em>&gt; is no general FAI solution?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And, most  importantly, how many false assumptions are behind these
</em><br>
<em>&gt; questions, and how  can I best learn to correct them?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Thank you for your  time,
</em><br>
<em>&gt; --
</em><br>
<em>&gt; DataPacRat
</em><br>
<em>&gt; lu .iacu'i ma krinu lo du'u .ei mi krici la'e di'u  li'u traji lo ka
</em><br>
<em>&gt; vajni fo lo preti
</em><br>
<em>&gt; 
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="21173.html">Jeff Medina: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<li><strong>Previous message:</strong> <a href="21171.html">Jake Witmer: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<li><strong>In reply to:</strong> <a href="21168.html">DataPacRat: "[sl4] Friendly AIs vs Friendly Humans"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21173.html">Jeff Medina: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21172">[ date ]</a>
<a href="index.html#21172">[ thread ]</a>
<a href="subject.html#21172">[ subject ]</a>
<a href="author.html#21172">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
