<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] Friendly AIs vs Friendly Humans</title>
<meta name="Author" content="Jake Witmer (jake.alfg@yahoo.com)">
<meta name="Subject" content="Re: [sl4] Friendly AIs vs Friendly Humans">
<meta name="Date" content="2011-06-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] Friendly AIs vs Friendly Humans</h1>
<!-- received="Tue Jun 21 04:15:18 2011" -->
<!-- isoreceived="20110621101518" -->
<!-- sent="Tue, 21 Jun 2011 03:15:10 -0700 (PDT)" -->
<!-- isosent="20110621101510" -->
<!-- name="Jake Witmer" -->
<!-- email="jake.alfg@yahoo.com" -->
<!-- subject="Re: [sl4] Friendly AIs vs Friendly Humans" -->
<!-- id="865247.70780.qm@web45707.mail.sp1.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="BANLkTi=6djzBJeBXG+PDeVLgDZnD9BUeQQ@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Jake Witmer (<a href="mailto:jake.alfg@yahoo.com?Subject=Re:%20[sl4]%20Friendly%20AIs%20vs%20Friendly%20Humans"><em>jake.alfg@yahoo.com</em></a>)<br>
<strong>Date:</strong> Tue Jun 21 2011 - 04:15:10 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="21172.html">Matt Mahoney: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<li><strong>Previous message:</strong> <a href="21170.html">Brian Rabkin: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<li><strong>In reply to:</strong> <a href="21168.html">DataPacRat: "[sl4] Friendly AIs vs Friendly Humans"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21172.html">Matt Mahoney: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21171">[ date ]</a>
<a href="index.html#21171">[ thread ]</a>
<a href="subject.html#21171">[ subject ]</a>
<a href="author.html#21171">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I address your points in [bracketed blue bold below].
<br>
<p><p>Jake Witmer
<br>
<p><p>312.730.4037skype:  jake.witmer
<br>
Y!chat:  jake.alfg 
<br>
<p><p>&quot;The most dangerous man to any government is the man who is able to think things 
<br>
<p>out for himself, without regard to the prevailing superstitions and taboos. Almost 
<br>
<p>inevitably, he comes to the conclusion that the government he lives under is 
<br>
<p>dishonest, insane, and intolerable.&quot; -H. L. Mencken
<br>
<p><p>&quot;Let an ultraintelligent machine be defined as a machine that can far surpass all the
<br>
intellectual activities of any man however clever. Since the design of machines is one 
<br>
<p>of these intellectual activities, an ultraintelligent machine could design even better 
<br>
<p>machines; there would then unquestionably be an 'intelligence explosion,' and the 
<br>
<p>intelligence of man would be left far behind. Thus the first ultraintelligent machine is 
<br>
<p>the last invention that man need ever make.&quot;
<br>
-I. J. Good
<br>
<p><p><p><p><p>--- On Tue, 6/21/11, DataPacRat &lt;<a href="mailto:datapacrat@gmail.com?Subject=Re:%20[sl4]%20Friendly%20AIs%20vs%20Friendly%20Humans">datapacrat@gmail.com</a>&gt; wrote:
<br>
<p><p>From: DataPacRat &lt;<a href="mailto:datapacrat@gmail.com?Subject=Re:%20[sl4]%20Friendly%20AIs%20vs%20Friendly%20Humans">datapacrat@gmail.com</a>&gt;
<br>
Subject: [sl4] Friendly AIs vs Friendly Humans
<br>
To: <a href="mailto:sl4@sl4.org?Subject=Re:%20[sl4]%20Friendly%20AIs%20vs%20Friendly%20Humans">sl4@sl4.org</a>
<br>
Date: Tuesday, June 21, 2011, 6:36 AM
<br>
<p><p>Since this list isn't officially closed down /quite/ yet, I'm hoping
<br>
to take advantage of the remaining readers' insights to help me find
<br>
the answer to a certain question - or, at least, help me find where
<br>
the answer already is.
<br>
<p><p>My understanding of the Friendly AI problem is, roughly, that AIs
<br>
could have all sorts of goal systems, many of which are rather
<br>
unhealthy for humanity as we know it; and, due to the potential for
<br>
rapid self-improvement, once any AI exists, it is highly likely to
<br>
rapidly gain the power required to implement its goals whether we want
<br>
it to or not. Thus certain people are trying to develop the parameters
<br>
for a Friendly AI, one that will allow us humans to continue doing our
<br>
own things (or some approximation thereof), or at least for avoiding
<br>
the development of an Unfriendly AI.
<br>
<p><em>&gt;From what I've overheard, one of the biggest difficulties with FAI is
</em><br>
that there are a wide variety of possible forms of AI, making it
<br>
difficult to determine what it would take to ensure Friendliness for
<br>
any potential AI design. [I think this is an accurate view.  To this end, I strongly recommend the book &quot;On Intelligence&quot; by Jeff Hawkins, if you have not already read it.]
<br>
<p>Could anyone here suggest any references on a much narrower subset of
<br>
this problem: limiting the form of AI designs being considered to
<br>
human-like minds (possibly including actual emulations of human
<br>
minds), is it possible to solve the FAI problem for that subset - or,
<br>
put another way, instead of preventing Unfriendly AIs and allowing
<br>
only Friendly AIs, is it possible to avoid &quot;Unfriendly Humans&quot; and
<br>
encourage &quot;Friendly Humans&quot;?
<br>
[The basic goal of any legal system is to do this.  Failure of a legal system (such as the United States legal system) produces increasing levels of human to human parasitism, and thus suffering.  An extreme example of a completely bad (absent of morality) legal system were the Communist systems of the USSR and China.  --Without protection of our bodies (including all subsets of them, to the atomic level) we have absolutely no freedom.  This is difficult for the religious to comprehend, and that lack of understanding allows for attack by political sophistry and bigotry (perhaps AGI or strong AI will be immune to these failures of human thought, in the same way many libertarians have become immune to them --by education.  Also, machines will only likely need to be educated once, as opposed to humans, which do not have stable and reproducible copies of their neo-cortical hierarchies.)]
<br>
<p>&nbsp;If so, do such methods offer any insight
<br>
into the generalized FAI problem?
<br>
[I think so.  The proper legal system of the USA is the jury-based system.  By inserting random evaluations of suffering with the power to veto punishment, our otherwise sociopathic legal system acts with compassion.  As the American jury-based legal system has been incrementally circumvented and destroyed (licensing of lawyers-1832, voir dire &quot;jury selection&quot;-1850, elimination of proper jury instruction-1895, elimination of 4th amendment defenses due to drug prohibition -1910, silencing of constitutional defense arguments-1960s), the system and the actors within it have acted in increasingly more sociopathic ways.  For a detailing of the incremental destruction of the jury, please check out the website: <a href="http://www.fija.org">http://www.fija.org</a> and <a href="http://www.isil.org also">http://www.isil.org also</a> very good, in showing how limits on the power to punish reduce the potential for harm, please check out <a href="http://www.hawaii.edu/powerkills">http://www.hawaii.edu/powerkills</a> ]
<br>
<p>&nbsp;If not, does that imply that there
<br>
is no general FAI solution?
<br>
[There is no certain FAI solution, since overcoming tyranny requires conflict.  If a system is a &quot;friendly&quot; helper of tyranny, it is &quot;unfriendly.&quot;  If a system is an antagonist to tyranny, it is hostile to most present humans, and benevolent to most future (uncorrupted, well-educated) humans.  Implicit in this understanding is that most humans are as moral as the system of government they live under, with outliers being more or less moral than that system.  In the USA, only small-L &quot;libertarians&quot; approach morality.  ---All others vote to violate their own chosen morality while inside the polling place, but otherwise pretend to &quot;love thy neighbor as thyself.&quot;  A quick look at the U.S. legal system (legislative, prison, and court systems) proves that virtually noone in the USA &quot;loves their neighbor as their self.&quot;]
<br>
<p>And, most importantly, how many false assumptions are behind these
<br>
questions, and how can I best learn to correct them?
<br>
[Thinking about the issues, debating them, and asking questions to educated message boards is a good idea.  It's more rational than the approach many people take.  Reading overviews of AGI-related material is a good idea, as is investigating current projects whose goal is AGI.  
<br>
<p>Please understand that I've simply put in my .02 here.  I am in no way &quot;qualified&quot; to speak about this subject, except as someone who cares about it, and is self-educated about it.  I hold no degree in computer science, much less an advanced one.  But I think I have a pretty honest bullshit filter, and this is something that most people --even ones with advanced computer science degrees-- completely lack.]
<br>
<p><p>Thank you for your time,
<br>
<pre>
--
DataPacRat
lu .iacu'i ma krinu lo du'u .ei mi krici la'e di'u li'u traji lo ka
vajni fo lo preti
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="21172.html">Matt Mahoney: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<li><strong>Previous message:</strong> <a href="21170.html">Brian Rabkin: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<li><strong>In reply to:</strong> <a href="21168.html">DataPacRat: "[sl4] Friendly AIs vs Friendly Humans"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21172.html">Matt Mahoney: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21171">[ date ]</a>
<a href="index.html#21171">[ thread ]</a>
<a href="subject.html#21171">[ subject ]</a>
<a href="author.html#21171">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
