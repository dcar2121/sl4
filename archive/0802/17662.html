<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later">
<meta name="Date" content="2008-02-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later</h1>
<!-- received="Wed Feb 20 20:55:30 2008" -->
<!-- isoreceived="20080221035530" -->
<!-- sent="Wed, 20 Feb 2008 19:52:10 -0800" -->
<!-- isosent="20080221035210" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later" -->
<!-- id="47BCF56A.8050407@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="ab5bcc90802201901h11e34a4ah502c37b6a50fdc33@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Is%20a%20theory%20of%20hard%20take%20off%20possible?%20Re:%20Investing%20in%20FAI%20research:%20now%20vs.%20later"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Feb 20 2008 - 20:52:10 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17663.html">Matt Mahoney: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>Previous message:</strong> <a href="17661.html">William Pearson: "Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later"</a>
<li><strong>In reply to:</strong> <a href="17661.html">William Pearson: "Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17667.html">William Pearson: "Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later"</a>
<li><strong>Reply:</strong> <a href="17667.html">William Pearson: "Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17662">[ date ]</a>
<a href="index.html#17662">[ thread ]</a>
<a href="subject.html#17662">[ subject ]</a>
<a href="author.html#17662">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
William Pearson wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Now imagine you added a link to the outside, through which one bit
</em><br>
<em>&gt; could enter. Now depending upon that bit the system, the systems
</em><br>
<em>&gt; evolution could bifurcate, or it could ignore it and stay singular.
</em><br>
<em>&gt; Bifurcation leads to the potential for growing of the ability to
</em><br>
<em>&gt; predict the world, if parts of the world happen to correlate with the
</em><br>
<em>&gt; bit that was bifurcated on.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; More bits given to the system lead to more possible bifurcations.
</em><br>
<em>&gt; Exponentially increasing numbers of bifurcations are needed for
</em><br>
<em>&gt; exponential increases in predictive power.
</em><br>
<p>You need a logarithmic number of bits in order to slice a prediction 
<br>
to great fineness - i.e., predicting to within a factor of a billionth 
<br>
requires 30 bits, not a billion bits.  I know this is probably what 
<br>
you meant, but nobody would hear you saying it unless they already 
<br>
knew the answer.
<br>
<p>The other thing to remember is that we do not live in a random 
<br>
universe where it takes 1 bit of information to predict one more bit 
<br>
of an important outcome, like trying to record a sequence of 
<br>
coinflips.  Gravity is the same all over, you don't have to learn it 
<br>
again each time; and many important and manipulable aspects of the 
<br>
universe are highly regular.
<br>
<p><em>&gt; A concrete example, let us say we are trying to get an AI to evolve to
</em><br>
<em>&gt; a state where it can consistently use my IP number to reach my
</em><br>
<em>&gt; computer. It follows its programming, let us say by incrementing a
</em><br>
<em>&gt; counter and using that in  the IP field of TCP/IP packets. I give it
</em><br>
<em>&gt; no feedback nor any other bits of information about the world. It will
</em><br>
<em>&gt; never be able to bifurcate to a state where it always uses the correct
</em><br>
<em>&gt; IP address, it is just throwing out packets into the void. At some
</em><br>
<em>&gt; points it would even get the IP address right, but it wouldn't know
</em><br>
<em>&gt; that it had.
</em><br>
<p><em>&gt; Now say I give it a bit per second bandwidth input. If it has used the
</em><br>
<em>&gt; correct IP address in the previous second it gets a 1, otherwise a 0.
</em><br>
<em>&gt; Now it might guess the correct IP address first time (and only guess
</em><br>
<em>&gt; one IP address) and get a 1, and therefore get 4 bytes of information.
</em><br>
<em>&gt; But as we said the AI is deterministic and the IP address is fixed, so
</em><br>
<em>&gt; we might be forgiven in thinking that the game is rigged and the
</em><br>
<em>&gt; system already had that information. If we randomise the IP and set it
</em><br>
<em>&gt; off again, on average, no matter the computing power the system had,
</em><br>
<em>&gt; it would get only 7.45x10^-9 bits per second of the IP address. This
</em><br>
<em>&gt; isn't the most efficient coding of  our input to it. We could simply
</em><br>
<em>&gt; give it the IP address, one bit per second. The system would still be
</em><br>
<em>&gt; limited to bifurcating once per second.
</em><br>
<p>You are going down a correct path, mathematically speaking; but the 
<br>
answer is going to end up being &quot;In principle, it takes a ridiculously 
<br>
small amount of information; the exact amount is theoretically 
<br>
incomputable; and a superintelligence would require more, but we can't 
<br>
guess how much more without actually running a superintelligence.&quot;
<br>
<p>A cable modem connected to the Internet is certainly *vastly* more 
<br>
bandwidth than is theoretically needed to grok and take over the 
<br>
world, though.  Or a 1200 baud modem, for that matter.  Hell, give me 
<br>
a millionfold subjective speedup and a few siderial days to think 
<br>
about it and *I'll* take over the world with a 1200 baud modem (not 
<br>
because I want it; just to demonstrate the point).
<br>
<p>Remember that there are more reliable ways of making money than trying 
<br>
to win the lottery - a wise agent can steer their pressure to the more 
<br>
manipulable, more predictable, lower-entropy levers in the system. 
<br>
When bifurcation is a limited resource, you can arrange to use less of 
<br>
it - you don't *have* to let 50% of your selves emit alternate 
<br>
versions of every bit you want to output.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17663.html">Matt Mahoney: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>Previous message:</strong> <a href="17661.html">William Pearson: "Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later"</a>
<li><strong>In reply to:</strong> <a href="17661.html">William Pearson: "Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17667.html">William Pearson: "Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later"</a>
<li><strong>Reply:</strong> <a href="17667.html">William Pearson: "Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17662">[ date ]</a>
<a href="index.html#17662">[ thread ]</a>
<a href="subject.html#17662">[ subject ]</a>
<a href="author.html#17662">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
