<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later</title>
<meta name="Author" content="William Pearson (wil.pearson@gmail.com)">
<meta name="Subject" content="Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later">
<meta name="Date" content="2008-02-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later</h1>
<!-- received="Thu Feb 21 05:34:11 2008" -->
<!-- isoreceived="20080221123411" -->
<!-- sent="Thu, 21 Feb 2008 12:31:46 +0000" -->
<!-- isosent="20080221123146" -->
<!-- name="William Pearson" -->
<!-- email="wil.pearson@gmail.com" -->
<!-- subject="Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later" -->
<!-- id="ab5bcc90802210431l31f668cbke683fea561c7a6c0@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="47BCF56A.8050407@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> William Pearson (<a href="mailto:wil.pearson@gmail.com?Subject=Re:%20Is%20a%20theory%20of%20hard%20take%20off%20possible?%20Re:%20Investing%20in%20FAI%20research:%20now%20vs.%20later"><em>wil.pearson@gmail.com</em></a>)<br>
<strong>Date:</strong> Thu Feb 21 2008 - 05:31:46 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17668.html">Matt Mahoney: "Risks of distributed AI (was Re: Investing in FAI research: now vs. later)"</a>
<li><strong>Previous message:</strong> <a href="17666.html">Peter de Blanc: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>In reply to:</strong> <a href="17662.html">Eliezer S. Yudkowsky: "Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17657.html">Eliezer S. Yudkowsky: "Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17667">[ date ]</a>
<a href="index.html#17667">[ thread ]</a>
<a href="subject.html#17667">[ subject ]</a>
<a href="author.html#17667">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 21/02/2008, Eliezer S. Yudkowsky &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Is%20a%20theory%20of%20hard%20take%20off%20possible?%20Re:%20Investing%20in%20FAI%20research:%20now%20vs.%20later">sentience@pobox.com</a>&gt; wrote:
<br>
<em>&gt; William Pearson wrote:
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; Now imagine you added a link to the outside, through which one bit
</em><br>
<em>&gt;  &gt; could enter. Now depending upon that bit the system, the systems
</em><br>
<em>&gt;  &gt; evolution could bifurcate, or it could ignore it and stay singular.
</em><br>
<em>&gt;  &gt; Bifurcation leads to the potential for growing of the ability to
</em><br>
<em>&gt;  &gt; predict the world, if parts of the world happen to correlate with the
</em><br>
<em>&gt;  &gt; bit that was bifurcated on.
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; More bits given to the system lead to more possible bifurcations.
</em><br>
<em>&gt;  &gt; Exponentially increasing numbers of bifurcations are needed for
</em><br>
<em>&gt;  &gt; exponential increases in predictive power.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; You need a logarithmic number of bits in order to slice a prediction
</em><br>
<em>&gt;  to great fineness - i.e., predicting to within a factor of a billionth
</em><br>
<em>&gt;  requires 30 bits, not a billion bits.  I know this is probably what
</em><br>
<em>&gt;  you meant, but nobody would hear you saying it unless they already
</em><br>
<em>&gt;  knew the answer.
</em><br>
<em>&gt;
</em><br>
<em>&gt;  The other thing to remember is that we do not live in a random
</em><br>
<em>&gt;  universe where it takes 1 bit of information to predict one more bit
</em><br>
<em>&gt;  of an important outcome, like trying to record a sequence of
</em><br>
<em>&gt;  coinflips.
</em><br>
<p>Humans sometimes try and make it so. See one time pads etc. Also if
<br>
you consider the ease that systems with feedback loops make UTMs you
<br>
need considerable information about the start states to be able to
<br>
predict non-trivial properties of it. Also in general to be able to
<br>
maintain predictive power over another bifurcating system, you have to
<br>
bifurcate as fast as it is bifurcating. How quickly is society
<br>
bifurcating? Is it exponentially increasing its rate of bifurcation?
<br>
<p><em>&gt;  Gravity is the same all over, you don't have to learn it
</em><br>
<em>&gt;  again each time; and many important and manipulable aspects of the
</em><br>
<em>&gt;  universe are highly regular.
</em><br>
<em>&gt;
</em><br>
<p>But you still need to get lots of bits to determine the gravity
<br>
problem you are working on.
<br>
<p><em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; You are going down a correct path, mathematically speaking; but the
</em><br>
<em>&gt;  answer is going to end up being &quot;In principle, it takes a ridiculously
</em><br>
<em>&gt;  small amount of information; the exact amount is theoretically
</em><br>
<em>&gt;  incomputable; and a superintelligence would require more, but we can't
</em><br>
<em>&gt;  guess how much more without actually running a superintelligence.&quot;
</em><br>
<p>The answer to what question exactly? Also I am not comfortable with
<br>
superintelligence as a word. Is it possible to taboo it?
<br>
<p><em>&gt;  A cable modem connected to the Internet is certainly *vastly* more
</em><br>
<em>&gt;  bandwidth than is theoretically needed to grok and take over the
</em><br>
<em>&gt;  world, though.
</em><br>
<p>Takeover, I would grant you. You could bifurcate the world from the
<br>
state of being not taken over to being taken over with one bit, if the
<br>
world happened to be at that tipping point. But grok? Surely that all
<br>
depends upon where you start from.
<br>
<p><em>&gt;  Or a 1200 baud modem, for that matter.  Hell, give me
</em><br>
<em>&gt;  a millionfold subjective speedup and a few siderial days to think
</em><br>
<em>&gt;  about it and *I'll* take over the world with a 1200 baud modem (not
</em><br>
<em>&gt;  because I want it; just to demonstrate the point).
</em><br>
<p>Why wouldn't you want to? If you can take it over in a meaningful
<br>
sense you can quash all non FAI development and put lots more
<br>
resources to the development of FAI.
<br>
<p>Also you have had significant bifurcations already. Give baby eliezer
<br>
a millionfold subjective speedup and a 1200 baud modem (and no eyes or
<br>
ears), and I would like to seem him take over the world.
<br>
<p>I suppose I am interested in the transition time between the situation
<br>
we are now, computers are very bad at predicting the world, to the
<br>
situation where a computer is very good at predicting the world and
<br>
therefore being able to manipulate it with some purpose.
<br>
<p><em>&gt;  Remember that there are more reliable ways of making money than trying
</em><br>
<em>&gt;  to win the lottery - a wise agent can steer their pressure to the more
</em><br>
<em>&gt;  manipulable, more predictable, lower-entropy levers in the system.
</em><br>
<em>&gt;
</em><br>
<em>&gt;  When bifurcation is a limited resource, you can arrange to use less of
</em><br>
<em>&gt;  it - you don't *have* to let 50% of your selves emit alternate
</em><br>
<em>&gt;  versions of every bit you want to output.
</em><br>
<em>&gt;
</em><br>
<p>I don't see how you can use less bifurcations and still *improve*.
<br>
That seems to me to be like trying to do non-experimental science.
<br>
<p>&nbsp;&nbsp;Will Pearson
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17668.html">Matt Mahoney: "Risks of distributed AI (was Re: Investing in FAI research: now vs. later)"</a>
<li><strong>Previous message:</strong> <a href="17666.html">Peter de Blanc: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>In reply to:</strong> <a href="17662.html">Eliezer S. Yudkowsky: "Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17657.html">Eliezer S. Yudkowsky: "Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17667">[ date ]</a>
<a href="index.html#17667">[ thread ]</a>
<a href="subject.html#17667">[ subject ]</a>
<a href="author.html#17667">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
