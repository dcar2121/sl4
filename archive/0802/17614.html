<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Investing in FAI research: now vs. later</title>
<meta name="Author" content="Rolf Nelson (rolf.h.d.nelson@gmail.com)">
<meta name="Subject" content="Re: Investing in FAI research: now vs. later">
<meta name="Date" content="2008-02-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Investing in FAI research: now vs. later</h1>
<!-- received="Sun Feb 10 15:25:00 2008" -->
<!-- isoreceived="20080210222500" -->
<!-- sent="Sun, 10 Feb 2008 17:22:12 -0500" -->
<!-- isosent="20080210222212" -->
<!-- name="Rolf Nelson" -->
<!-- email="rolf.h.d.nelson@gmail.com" -->
<!-- subject="Re: Investing in FAI research: now vs. later" -->
<!-- id="79ecaa350802101422q4408bd63n98f4c146edae3291@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="20080209165512.ECFAB2BEA2@mauve.rahul.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Rolf Nelson (<a href="mailto:rolf.h.d.nelson@gmail.com?Subject=Re:%20Investing%20in%20FAI%20research:%20now%20vs.%20later"><em>rolf.h.d.nelson@gmail.com</em></a>)<br>
<strong>Date:</strong> Sun Feb 10 2008 - 15:22:12 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17615.html">Shane Legg: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>Previous message:</strong> <a href="17613.html">korpios@korpios.com: "Re: Augmented Intelligence Test"</a>
<li><strong>In reply to:</strong> <a href="17606.html">Peter C. McCluskey: "Re: Investing in FAI research: now vs. later"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17615.html">Shane Legg: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>Reply:</strong> <a href="17615.html">Shane Legg: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>Reply:</strong> <a href="17617.html">John K Clark: "Re: Investing in FAI research: now vs. later."</a>
<li><strong>Reply:</strong> <a href="17636.html">Peter C. McCluskey: "Re: Investing in FAI research: now vs. later"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17614">[ date ]</a>
<a href="index.html#17614">[ thread ]</a>
<a href="subject.html#17614">[ subject ]</a>
<a href="author.html#17614">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Feb 9, 2008 11:55 AM, Peter C. McCluskey &lt;<a href="mailto:pcm@rahul.net?Subject=Re:%20Investing%20in%20FAI%20research:%20now%20vs.%20later">pcm@rahul.net</a>&gt; wrote:
<br>
<p>Peter, thank you for the analysis. Here's my own analysis, which is not a
<br>
rebuttal nor the &quot;FAI community's perspective&quot;, but just my own personal
<br>
analysis.
<br>
<p>I'd say the number of smart people who have mistakenly thought they
<br>
<em>&gt; could create an important AI breakthrough suggests we should assume
</em><br>
<em>&gt; any one AGI effort should have a success probability somewhere around
</em><br>
<em>&gt; 0.01 to 0.0001.
</em><br>
<p><p>If we say &quot;in the next 20 years, total success&quot; then I will personally
<br>
estimate .005 for a typical AGI project.
<br>
<p>If many of the people offering resources to the
<br>
<em>&gt; project don't understand the design, then there is an incentive for people
</em><br>
<em>&gt; without serious designs to imitate serious researchers.
</em><br>
<p><p>You know who else has this problem? Every other AGI project on the planet.
<br>
So I would think that included in the prior estimate. SIAI has, at least for
<br>
now, less moral hazard here than most potential corporate or academic AGI
<br>
projects, though if FAI gets &quot;too popular&quot; for funders this could become a
<br>
serious problem in the future. If only we had such problems. :-)
<br>
<p>The fact that we're trying to be Friendly should drop the odds by an order
<br>
of magnitude, but I personally have to raise it back up an order of
<br>
magnitude based on my assessment of the SIAI team, and by a belief that
<br>
because FAI is a better idea than UFAI, SIAI will continue to be able to
<br>
expand the community and recruit extremely bright and motivated people, even
<br>
compared with other AGI teams. I can't really adjust for overconfidence here
<br>
because it's not clear to me what the beliefs and motivations are of the
<br>
majority who work on UFAI. So my own estimate is that SIAI directly saves
<br>
mankind at about 200:1 odds.
<br>
<p>Now, one can say that if something really could save mankind at 200:1 odds,
<br>
it would already have been done by someone else, so I must be overconfident.
<br>
True in theory. But, in making the decision &quot;should I help SIAI&quot;,
<br>
compensating for overconfidence shouldn't flip me all the way from &quot;I should
<br>
clearly do X&quot; to &quot;I should not do X, because most people don't do X.&quot;
<br>
Instead, &quot;If you have a pet project that can save everyone with 200:1 odds,
<br>
then go do it&quot; seems like a good moral rule, even if most people who believe
<br>
that have historically been wrong.
<br>
<p>Now, here's another twist about asteroid detection: &lt;i&gt;if&lt;/i&gt; FAI is
<br>
impossible, then asteroid detection doesn't save humanity anyway: it just
<br>
delays mankind's demise until someone creates a UFAI. So when you give
<br>
resources to SpaceGuard, you're conditioning on the hopes that someone else
<br>
will spend resources to (1) figure out how to best invest in FAI and then
<br>
(2) invest in FAI at some point.
<br>
<p>(I don't mean to pick especially on SpaceGuard, obviously saving mankind at
<br>
~1M : 1 odds is more worthwhile than anything &lt;i&gt;I've&lt;/i&gt; done with my life
<br>
to date. :-)
<br>
<p><p><em>&gt;  I'm tempted to add in some uncertainty about whether the AI designer(s)
</em><br>
<em>&gt; will be friendly to humanity or whether they'll make the AI friendly to
</em><br>
<em>&gt; themselves only. But that probably doesn't qualify as an existential risk,
</em><br>
<em>&gt; so it mainly reflects my selfish interests.
</em><br>
<p><p>No, you're not being selfish. That's a legitimate concern with any AGI
<br>
projects, and again one that the &quot;do what I tell you&quot; UFAI projects
<br>
especially neglect.
<br>
<p><em>&gt;Perhaps my own conclusions differs from yours as follows: first of all, I
</em><br>
<em>&gt; &gt;have confidence in the abilities of the current FAI community; and second
</em><br>
<em>&gt; of
</em><br>
<em>&gt;
</em><br>
<em>&gt;  Can you describe reasons for that confidence?
</em><br>
<p><p>If you don't agree, you don't agree, but personally I converge with the FAI
<br>
community on a number of issues. Yudkowsky used the term &quot;playing to win&quot;
<br>
recently on OB with regard to decision theory and rationality, that's not an
<br>
attitude you see most places. The desire to attempt sometimes to adjust for
<br>
overconfidence is fairly rare outside the FAI community. Yudkowsky and
<br>
Vassar are both people I'm willing to put into the extremely small category
<br>
of &quot;people who are smarter than the people who are smarter than me.&quot;
<br>
Yudkowsky's CEV seems the ideal type of approach for an AGI. So it's
<br>
overdetermined that I support the current community.
<br>
<p><p><em>&gt; &gt;all, if I didn't have confidence, I would try to bring about the creation
</em><br>
<em>&gt; of
</em><br>
<em>&gt; &gt;a new community, or bring about improvements of the existing community,
</em><br>
<em>&gt;
</em><br>
<em>&gt;  Does that follow from a belief about how your skills differ from those
</em><br>
<em>&gt; of a more typical person, or are you advocating that people accept this
</em><br>
<em>&gt; as a default approach?
</em><br>
<p><p>Default. If I thought the FAI community were idiots, I wouldn't work with
<br>
them. If I'm correct and they're idiots, it's a win because I was right to
<br>
start a new, non-idiot community. If I'm wrong and I'm the idiot, then it's
<br>
also a win because I got myself out of their way.
<br>
<p>There are a number of tasks for which the average member of this list is
<br>
<em>&gt; likely to be aware that he would have negligible influence, such as
</em><br>
<em>&gt; unifying
</em><br>
<em>&gt; relativity with quantum mechanics
</em><br>
<p><p>If you mean &quot;negligible&quot; as in &quot;the odds of winning the lottery are
<br>
negligible&quot;, then I disagree. We live in a market economy; you could save up
<br>
and endow a research grant to study unification. We live in a civil society;
<br>
you might be able to convince someone else to help you, &lt;i&gt;if&lt;/i&gt; you have
<br>
compelling reasons why they should do so. But yes, some people will be more
<br>
effective in given domains than others. You should multiply the
<br>
effectiveness with which you can attain a goal, times the desirability of
<br>
achieving the goal; there's no point where you should skip the multiplying
<br>
and say &quot;this is not my core competency, so I'm going to go skiing instead,
<br>
even though I don't like skiing, just because I'm good at it.&quot;
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17615.html">Shane Legg: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>Previous message:</strong> <a href="17613.html">korpios@korpios.com: "Re: Augmented Intelligence Test"</a>
<li><strong>In reply to:</strong> <a href="17606.html">Peter C. McCluskey: "Re: Investing in FAI research: now vs. later"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17615.html">Shane Legg: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>Reply:</strong> <a href="17615.html">Shane Legg: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>Reply:</strong> <a href="17617.html">John K Clark: "Re: Investing in FAI research: now vs. later."</a>
<li><strong>Reply:</strong> <a href="17636.html">Peter C. McCluskey: "Re: Investing in FAI research: now vs. later"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17614">[ date ]</a>
<a href="index.html#17614">[ thread ]</a>
<a href="subject.html#17614">[ subject ]</a>
<a href="author.html#17614">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
