<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Investing in FAI research: now vs. later</title>
<meta name="Author" content="joshua@joshuafox.com (joshua@joshuafox.com)">
<meta name="Subject" content="Re: Investing in FAI research: now vs. later">
<meta name="Date" content="2008-02-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Investing in FAI research: now vs. later</h1>
<!-- received="Sat Feb  9 12:32:08 2008" -->
<!-- isoreceived="20080209193208" -->
<!-- sent="Sat, 9 Feb 2008 21:29:44 +0200" -->
<!-- isosent="20080209192944" -->
<!-- name="joshua@joshuafox.com" -->
<!-- email="joshua@joshuafox.com" -->
<!-- subject="Re: Investing in FAI research: now vs. later" -->
<!-- id="8760b3f20802091129p66c85ac9pe76221887deac8eb@mail.gmail.com" -->
<!-- charset="UTF-8" -->
<!-- inreplyto="20080209165512.ECFAB2BEA2@mauve.rahul.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> <a href="mailto:joshua@joshuafox.com?Subject=Re:%20Investing%20in%20FAI%20research:%20now%20vs.%20later"><em>joshua@joshuafox.com</em></a><br>
<strong>Date:</strong> Sat Feb 09 2008 - 12:29:44 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17608.html">Joshua Fox: "Augmented Intelligence Test"</a>
<li><strong>Previous message:</strong> <a href="17606.html">Peter C. McCluskey: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>In reply to:</strong> <a href="17606.html">Peter C. McCluskey: "Re: Investing in FAI research: now vs. later"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17614.html">Rolf Nelson: "Re: Investing in FAI research: now vs. later"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17607">[ date ]</a>
<a href="index.html#17607">[ thread ]</a>
<a href="subject.html#17607">[ subject ]</a>
<a href="author.html#17607">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
2008/2/9, Peter C. McCluskey &lt;<a href="mailto:pcm@rahul.net?Subject=Re:%20Investing%20in%20FAI%20research:%20now%20vs.%20later">pcm@rahul.net</a>&gt;:
<br>
<em>&gt;  <a href="mailto:rolf.h.d.nelson@gmail.com?Subject=Re:%20Investing%20in%20FAI%20research:%20now%20vs.%20later">rolf.h.d.nelson@gmail.com</a> (Rolf Nelson) writes:
</em><br>
<em>&gt; &gt;Peter, overconfidence is indeed an ongoing risk with this venture (as,
</em><br>
<em>&gt; &gt;indeed, it is with any venture, especially one that is attempting to build
</em><br>
<em>&gt; a
</em><br>
<em>&gt; &gt;new technology). In general, all things equal, simple solutions should be
</em><br>
<em>&gt; &gt;preferred to complex solutions.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;However, the ratio between AGI existential risk and killer-asteroid risk in
</em><br>
<em>&gt; &gt;this century has got to be on the order of one to a million!* Despite this,
</em><br>
<em>&gt; &gt;I would estimate asteroid-impact overall commands more resources than FAI
</em><br>
<em>&gt; &gt;does.** I don't know how much you propose Bayesian shifting for
</em><br>
<em>&gt; &gt;overconfidence, but surely it's not a shift of that magnitude.
</em><br>
<em>&gt;
</em><br>
<em>&gt;  After reflecting on this for a while, I'm a good deal more uncertain
</em><br>
<em>&gt; than I was in my last email, but I still think it's at least a reasonable
</em><br>
<em>&gt; guess that the probability of a moderately smart person identifying a way
</em><br>
<em>&gt; advance FAI is more than a million times smaller than knowing how to
</em><br>
<em>&gt; advance asteroid detection. Your use of the word &quot;surely&quot; suggests that
</em><br>
<em>&gt; rather than just adjusting for overconfidence, you should rethink your
</em><br>
<em>&gt; reasoning more thoroughly.
</em><br>
<em>&gt;  I'd say the number of smart people who have mistakenly thought they
</em><br>
<em>&gt; could create an important AI breakthrough suggests we should assume
</em><br>
<em>&gt; any one AGI effort should have a success probability somewhere around
</em><br>
<em>&gt; 0.01 to 0.0001. Constraining the goal to be friendly and to be complete
</em><br>
<em>&gt; before an unfriendly AU could easily reduce the probability by an order
</em><br>
<em>&gt; of magnitude or more. If many of the people offering resources to the
</em><br>
<em>&gt; project don't understand the design, then there is an incentive for people
</em><br>
<em>&gt; without serious designs to imitate serious researchers. How much you should
</em><br>
<em>&gt; adjust your estimates for this risk seems fairly sensitive to how well you
</em><br>
<em>&gt; think you understand what the project is doing and why it ought to work.
</em><br>
<em>&gt; I'd guess the typical member of this list ought to use somewhere between
</em><br>
<em>&gt; a factor of 2 and 10. So the most optimistic estimate I'm willing to take
</em><br>
<em>&gt; seriously is that a moderately smart person would do several hundred times
</em><br>
<em>&gt; better giving to FAI research than to asteroid detection, and I think it's
</em><br>
<em>&gt; more likely that giving to FAI research is 2 or 3 orders of magnitude less
</em><br>
<em>&gt; promising.
</em><br>
<em>&gt;  I suspect it's a good idea to make some adjustment for overconfidence
</em><br>
<em>&gt; at this point, but I'm having trouble thinking quantitatively about that.
</em><br>
<em>&gt;  I'm tempted to add in some uncertainty about whether the AI designer(s)
</em><br>
<em>&gt; will be friendly to humanity or whether they'll make the AI friendly to
</em><br>
<em>&gt; themselves only. But that probably doesn't qualify as an existential risk,
</em><br>
<em>&gt; so it mainly reflects my selfish interests.
</em><br>
<em>&gt;  Note that none of this addresses the question of how much effort one
</em><br>
<em>&gt; should spend trying to convince existing AI researchers to avoid creating
</em><br>
<em>&gt; an AGI that might be unfriendly.
</em><br>
<em>&gt;
</em><br>
<em>&gt;  As for which tasks currently gets more resources, I find them hard to
</em><br>
<em>&gt; compare. It appears that more money is usefully spent on asteroid detection,
</em><br>
<em>&gt; and that money is the primary resource controlling asteroid detection
</em><br>
<em>&gt; results. It isn't clear whether money is being usefully spent on FAI or
</em><br>
<em>&gt; whether additional money would have any effect on it. I would not be
</em><br>
<em>&gt; surprised if something changes my opinion about that in the next few
</em><br>
<em>&gt; years.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;Perhaps my own conclusions differs from yours as follows: first of all, I
</em><br>
<em>&gt; &gt;have confidence in the abilities of the current FAI community; and second
</em><br>
<em>&gt; of
</em><br>
<em>&gt;
</em><br>
<em>&gt;  Can you describe reasons for that confidence?
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;all, if I didn't have confidence, I would try to bring about the creation
</em><br>
<em>&gt; of
</em><br>
<em>&gt; &gt;a new community, or bring about improvements of the existing community,
</em><br>
<em>&gt;
</em><br>
<em>&gt;  Does that follow from a belief about how your skills differ from those
</em><br>
<em>&gt; of a more typical person, or are you advocating that people accept this
</em><br>
<em>&gt; as a default approach?
</em><br>
<em>&gt;  There are a number of tasks for which the average member of this list is
</em><br>
<em>&gt; likely to be aware that he would have negligible influence, such as unifying
</em><br>
<em>&gt; relativity with quantum mechanics or inventing time travel. I suggest that
</em><br>
<em>&gt; FAI presents similar difficulties.
</em><br>
<em>&gt;
</em><br>
<em>&gt;  I apologize for my delay in responding.
</em><br>
<em>&gt; --
</em><br>
<em>&gt; ------------------------------------------------------------------------------
</em><br>
<em>&gt; Peter McCluskey         | The road to hell is paved with overconfidence
</em><br>
<em>&gt; www.bayesianinvestor.com| in your good intentions. - Stuart Armstrong
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; __________________________________________________
</em><br>
<em>&gt; D O T E A S Y - &quot;Join the web hosting revolution!&quot;
</em><br>
<em>&gt;              <a href="http://www.doteasy.com">http://www.doteasy.com</a>
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17608.html">Joshua Fox: "Augmented Intelligence Test"</a>
<li><strong>Previous message:</strong> <a href="17606.html">Peter C. McCluskey: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>In reply to:</strong> <a href="17606.html">Peter C. McCluskey: "Re: Investing in FAI research: now vs. later"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17614.html">Rolf Nelson: "Re: Investing in FAI research: now vs. later"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17607">[ date ]</a>
<a href="index.html#17607">[ thread ]</a>
<a href="subject.html#17607">[ subject ]</a>
<a href="author.html#17607">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
