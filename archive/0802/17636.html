<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Investing in FAI research: now vs. later</title>
<meta name="Author" content="Peter C. McCluskey (pcm@rahul.net)">
<meta name="Subject" content="Re: Investing in FAI research: now vs. later">
<meta name="Date" content="2008-02-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Investing in FAI research: now vs. later</h1>
<!-- received="Thu Feb 14 17:09:12 2008" -->
<!-- isoreceived="20080215000912" -->
<!-- sent="Thu, 14 Feb 2008 16:06:24 -0800 (PST)" -->
<!-- isosent="20080215000624" -->
<!-- name="Peter C. McCluskey" -->
<!-- email="pcm@rahul.net" -->
<!-- subject="Re: Investing in FAI research: now vs. later" -->
<!-- id="20080215000624.D33792BD99@mauve.rahul.net" -->
<!-- inreplyto="79ecaa350802101422q4408bd63n98f4c146edae3291@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Peter C. McCluskey (<a href="mailto:pcm@rahul.net?Subject=Re:%20Investing%20in%20FAI%20research:%20now%20vs.%20later"><em>pcm@rahul.net</em></a>)<br>
<strong>Date:</strong> Thu Feb 14 2008 - 17:06:24 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17637.html">Jeff Herrlich: "Re: Investing in FAI research: now vs. later."</a>
<li><strong>Previous message:</strong> <a href="17635.html">Diego Navarro: "An interesting presingularitarian metaphor on uploading humans"</a>
<li><strong>In reply to:</strong> <a href="17614.html">Rolf Nelson: "Re: Investing in FAI research: now vs. later"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17639.html">Rolf Nelson: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>Reply:</strong> <a href="17639.html">Rolf Nelson: "Re: Investing in FAI research: now vs. later"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17636">[ date ]</a>
<a href="index.html#17636">[ thread ]</a>
<a href="subject.html#17636">[ subject ]</a>
<a href="author.html#17636">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;<a href="mailto:rolf.h.d.nelson@gmail.com?Subject=Re:%20Investing%20in%20FAI%20research:%20now%20vs.%20later">rolf.h.d.nelson@gmail.com</a> (Rolf Nelson) writes:
<br>
<em>&gt;&gt;If many of the people offering resources to the
</em><br>
<em>&gt;&gt; project don't understand the design, then there is an incentive for people
</em><br>
<em>&gt;&gt; without serious designs to imitate serious researchers.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;You know who else has this problem? Every other AGI project on the planet.
</em><br>
<p>&nbsp;It might happen to be true of every active AGI project at the moment.
<br>
<p><em>&gt;So I would think that included in the prior estimate. SIAI has, at least for
</em><br>
<p>&nbsp;The 0.01 to 0.0001 probability that I gave was my estimate of what we can
<br>
tell from the failure of projects whose approach was understood and taken
<br>
seriously by a number of people with credentials as serious AI researchers.
<br>
&nbsp;My impression is that SIAI hasn't described enough of a plan for such
<br>
people to form an opinion on whether it should be considered a serious
<br>
attempt to build an AGI.
<br>
<p><em>&gt;The fact that we're trying to be Friendly should drop the odds by an order
</em><br>
<em>&gt;of magnitude, but I personally have to raise it back up an order of
</em><br>
<em>&gt;magnitude based on my assessment of the SIAI team, and by a belief that
</em><br>
<em>&gt;because FAI is a better idea than UFAI, SIAI will continue to be able to
</em><br>
<em>&gt;expand the community and recruit extremely bright and motivated people, even
</em><br>
<p>&nbsp;I'll remain sceptical of this ability to attract extremely bright people
<br>
until I see signs that it is happening.
<br>
<p><em>&gt;Now, one can say that if something really could save mankind at 200:1 odds,
</em><br>
<em>&gt;it would already have been done by someone else, so I must be overconfident.
</em><br>
<p>&nbsp;I attach little weight to this argument.
<br>
<p><em>&gt;Now, here's another twist about asteroid detection: &lt;i&gt;if&lt;/i&gt; FAI is
</em><br>
<em>&gt;impossible, then asteroid detection doesn't save humanity anyway: it just
</em><br>
<em>&gt;delays mankind's demise until someone creates a UFAI. So when you give
</em><br>
<p>&nbsp;If you're very confident that that humanity is doomed without FAI, your
<br>
conclusion is reasonable. But I see no reason for that confidence. Models
<br>
where a number of different types of AI cooperate to prevent any one AI
<br>
from conquering the world seem at least as plausible as those that imply
<br>
we're doomed without FAI.
<br>
<p><em>&gt;&gt;  I'm tempted to add in some uncertainty about whether the AI designer(s)
</em><br>
<em>&gt;&gt; will be friendly to humanity or whether they'll make the AI friendly to
</em><br>
<em>&gt;&gt; themselves only. But that probably doesn't qualify as an existential risk,
</em><br>
<em>&gt;&gt; so it mainly reflects my selfish interests.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;No, you're not being selfish. That's a legitimate concern with any AGI
</em><br>
<em>&gt;projects, and again one that the &quot;do what I tell you&quot; UFAI projects
</em><br>
<em>&gt;especially neglect.
</em><br>
<p>&nbsp;I'm being selfish in sense that I treat it almost the same as I treat
<br>
existential risks, when a genuine altruist would treat existential risks
<br>
as significantly more harmful.
<br>
<p><em>&gt;If you don't agree, you don't agree, but personally I converge with the FAI
</em><br>
<em>&gt;community on a number of issues. Yudkowsky used the term &quot;playing to win&quot;
</em><br>
<em>&gt;recently on OB with regard to decision theory and rationality, that's not an
</em><br>
<em>&gt;attitude you see most places.
</em><br>
<p>&nbsp;I find it hard to observe whether people use the wrong attitude when this
<br>
issue matters, so it's hard to tell how unusual his attitude is.
<br>
<p><em>&gt; The desire to attempt sometimes to adjust for
</em><br>
<em>&gt;overconfidence is fairly rare outside the FAI community. Yudkowsky and
</em><br>
<em>&gt;Vassar are both people I'm willing to put into the extremely small category
</em><br>
<em>&gt;of &quot;people who are smarter than the people who are smarter than me.&quot;
</em><br>
<em>&gt;Yudkowsky's CEV seems the ideal type of approach for an AGI. So it's
</em><br>
<em>&gt;overdetermined that I support the current community.
</em><br>
<p>&nbsp;I suspect CEV, if implemented as I understand it, would take long enough
<br>
to implement (due to cpu time needed to run it and due to the time needed
<br>
to acquire enough information to adequately model all human) that it
<br>
leaves important risks unanswered.
<br>
&nbsp;Eliezer may be smarter than me, but I see no sign that he's the smartest
<br>
person I know.
<br>
<p><em>&gt;&gt; &gt;all, if I didn't have confidence, I would try to bring about the creation
</em><br>
<em>&gt;&gt; of
</em><br>
<em>&gt;&gt; &gt;a new community, or bring about improvements of the existing community,
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;  Does that follow from a belief about how your skills differ from those
</em><br>
<em>&gt;&gt; of a more typical person, or are you advocating that people accept this
</em><br>
<em>&gt;&gt; as a default approach?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Default. If I thought the FAI community were idiots, I wouldn't work with
</em><br>
<em>&gt;them. If I'm correct and they're idiots, it's a win because I was right to
</em><br>
<em>&gt;start a new, non-idiot community. If I'm wrong and I'm the idiot, then it's
</em><br>
<em>&gt;also a win because I got myself out of their way.
</em><br>
<p>&nbsp;I think you overestimate the ability of a relatively typical person to
<br>
start a useful FAI community.
<br>
<p><em>&gt;If you mean &quot;negligible&quot; as in &quot;the odds of winning the lottery are
</em><br>
<em>&gt;negligible&quot;, then I disagree. We live in a market economy; you could save up
</em><br>
<em>&gt;and endow a research grant to study unification. We live in a civil society;
</em><br>
<em>&gt;you might be able to convince someone else to help you, &lt;i&gt;if&lt;/i&gt; you have
</em><br>
<em>&gt;compelling reasons why they should do so. But yes, some people will be more
</em><br>
<em>&gt;effective in given domains than others. You should multiply the
</em><br>
<em>&gt;effectiveness with which you can attain a goal, times the desirability of
</em><br>
<em>&gt;achieving the goal; there's no point where you should skip the multiplying
</em><br>
<em>&gt;and say &quot;this is not my core competency, so I'm going to go skiing instead,
</em><br>
<em>&gt;even though I don't like skiing, just because I'm good at it.&quot;
</em><br>
<p>&nbsp;I'm unsure how much of that paragraph I understand. I think my endowing
<br>
a research grant to study unification has odds that are only modestly
<br>
better than winning the lottery, so after multiplying the FAI equivalent
<br>
by the desirability, I end up with results that are rather sensitive to
<br>
whether I'm feeling optimistic or pessimistic this week.
<br>
<p><pre>
-- 
------------------------------------------------------------------------------
Peter McCluskey         | The road to hell is paved with overconfidence
www.bayesianinvestor.com| in your good intentions. - Stuart Armstrong
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17637.html">Jeff Herrlich: "Re: Investing in FAI research: now vs. later."</a>
<li><strong>Previous message:</strong> <a href="17635.html">Diego Navarro: "An interesting presingularitarian metaphor on uploading humans"</a>
<li><strong>In reply to:</strong> <a href="17614.html">Rolf Nelson: "Re: Investing in FAI research: now vs. later"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17639.html">Rolf Nelson: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>Reply:</strong> <a href="17639.html">Rolf Nelson: "Re: Investing in FAI research: now vs. later"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17636">[ date ]</a>
<a href="index.html#17636">[ thread ]</a>
<a href="subject.html#17636">[ subject ]</a>
<a href="author.html#17636">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
