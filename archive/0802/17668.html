<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Risks of distributed AI (was Re: Investing in FAI research: now vs. later)</title>
<meta name="Author" content="Matt Mahoney (matmahoney@yahoo.com)">
<meta name="Subject" content="Risks of distributed AI (was Re: Investing in FAI research: now vs. later)">
<meta name="Date" content="2008-02-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Risks of distributed AI (was Re: Investing in FAI research: now vs. later)</h1>
<!-- received="Thu Feb 21 10:33:42 2008" -->
<!-- isoreceived="20080221173342" -->
<!-- sent="Thu, 21 Feb 2008 09:30:28 -0800 (PST)" -->
<!-- isosent="20080221173028" -->
<!-- name="Matt Mahoney" -->
<!-- email="matmahoney@yahoo.com" -->
<!-- subject="Risks of distributed AI (was Re: Investing in FAI research: now vs. later)" -->
<!-- id="415617.24625.qm@web51909.mail.re2.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="1203572327.2478.5.camel@localhost.localdomain" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Matt Mahoney (<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Risks%20of%20distributed%20AI%20(was%20Re:%20Investing%20in%20FAI%20research:%20now%20vs.%20later)"><em>matmahoney@yahoo.com</em></a>)<br>
<strong>Date:</strong> Thu Feb 21 2008 - 10:30:28 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17669.html">Daniel Burfoot: "Re: Risks of distributed AI (was Re: Investing in FAI research: now vs. later)"</a>
<li><strong>Previous message:</strong> <a href="17667.html">William Pearson: "Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later"</a>
<li><strong>In reply to:</strong> <a href="17666.html">Peter de Blanc: "Re: Investing in FAI research: now vs. later"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17669.html">Daniel Burfoot: "Re: Risks of distributed AI (was Re: Investing in FAI research: now vs. later)"</a>
<li><strong>Reply:</strong> <a href="17669.html">Daniel Burfoot: "Re: Risks of distributed AI (was Re: Investing in FAI research: now vs. later)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17668">[ date ]</a>
<a href="index.html#17668">[ thread ]</a>
<a href="subject.html#17668">[ subject ]</a>
<a href="author.html#17668">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
--- Peter de Blanc &lt;<a href="mailto:peter@spaceandgames.com?Subject=Re:%20Risks%20of%20distributed%20AI%20(was%20Re:%20Investing%20in%20FAI%20research:%20now%20vs.%20later)">peter@spaceandgames.com</a>&gt; wrote:
<br>
<p><em>&gt; On Wed, 2008-02-20 at 19:59 -0800, Matt Mahoney wrote:
</em><br>
<em>&gt; &gt; &gt; The scenario I'm most afraid of is not a hard take-off leading to
</em><br>
<em>&gt; &gt; unfriendly
</em><br>
<em>&gt; &gt; &gt; AGI, but a pseudo-AI falling into the hands of evil men.
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; Bad science fiction. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; (I disagree with the above, but) do you really consider this a
</em><br>
<em>&gt; counterargument?
</em><br>
<p>Not by itself, but I suppose I am also guilty of presenting doomsday
<br>
scenarios.  My objection is to the idea that AI could be developed in
<br>
isolation in a secret lab somewhere.  Or worse, that the technology could be
<br>
stolen, as if the thieves could be smart enough to use it without being smart
<br>
enough to develop it themselves.
<br>
<p>I believe that AI will be developed where there is the most computing power
<br>
and  information already available, on the internet.  I described one possible
<br>
design in <a href="http://www.mattmahoney.net/agi.html">http://www.mattmahoney.net/agi.html</a> and did my thesis work to show
<br>
that a very abstract model of this architecture is robust and scalable.  The
<br>
idea is that intelligence will emerge from a huge number of simple but
<br>
specialized peers and an infrastructure that routes messages to the right
<br>
experts.  It does not require any advances over current technology.  It is a
<br>
P2P network that creates a market where information has negative value and
<br>
peers compete for computing resources (memory and bandwidth) in an economy
<br>
that rewards intelligence and cooperation.
<br>
<p>The design is friendly, at least initially, because friendliness is a subgoal
<br>
of the evolutionarily stable goal of acquiring resources.  Each peer, being
<br>
relatively stupid, would be administered by a human owner.  A typical
<br>
configuration would broadcast messages posted by the owner on his or her
<br>
favorite topic, collect and relay messages that shared the same keywords, and
<br>
prioritize incoming messages to reward valuable and accurate sources of
<br>
information (in the opinion of the owner) and block spammers.  Well-behaved
<br>
and intelligent peers will be rewarded by having their own messages accepted
<br>
and propagated to a wider audience.  As technology allows, peers will become
<br>
more intelligent and more of these tasks will be automated.
<br>
<p>The idea that AI could fall into the &quot;wrong hands&quot; is like the Internet
<br>
falling  into the wrong hands.  It is true that the Storm botnet (
<br>
<a href="http://en.wikipedia.org/wiki/Storm_botnet">http://en.wikipedia.org/wiki/Storm_botnet</a> ) controls about 0.1% of the
<br>
internet's computing power, similar in size to Google.  There are also
<br>
critical failure points, such as the software that updates the root DNS
<br>
servers.  But generally, I think distributed ownership greatly lessens the
<br>
risk.
<br>
<p>I believe a distributed AI is susceptible to a singularity and loss of human
<br>
control like any other design.  Initially, peers will communicate in natural
<br>
language, with each peer understanding only a small subset.  That will be true
<br>
as long as humans are the dominant source of information.  But as peers become
<br>
more intelligent, the language will evolve.  Peers will develop their own
<br>
protocols that will be incomprehensible to humans, and humans will become less
<br>
and less relevant to the system's evolution.
<br>
<p>Also, as Eliezer pointed out, RSI need not take an evolutionary path.  If the
<br>
peers all cooperate, then evolution does not apply.  Evolution is not an
<br>
equilibrium process.   It lies on the boundary between stability and chaos,
<br>
like all complex, incrementally updated systems.  It is punctuated by mass
<br>
extinctions, plagues, population explosions, and other ecological disasters
<br>
like the introduction of deadly poisonous gases like oxygen into the
<br>
atmosphere.  I believe we are alive today because of the great diversity of
<br>
species, none of which can survive in every environment (and now robots roam
<br>
Mars).  And by the anthropic principle, maybe we have just been lucky so far.
<br>
<p>Comments?
<br>
<p><p>-- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Risks%20of%20distributed%20AI%20(was%20Re:%20Investing%20in%20FAI%20research:%20now%20vs.%20later)">matmahoney@yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17669.html">Daniel Burfoot: "Re: Risks of distributed AI (was Re: Investing in FAI research: now vs. later)"</a>
<li><strong>Previous message:</strong> <a href="17667.html">William Pearson: "Re: Is a theory of hard take off possible? Re: Investing in FAI research: now vs. later"</a>
<li><strong>In reply to:</strong> <a href="17666.html">Peter de Blanc: "Re: Investing in FAI research: now vs. later"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17669.html">Daniel Burfoot: "Re: Risks of distributed AI (was Re: Investing in FAI research: now vs. later)"</a>
<li><strong>Reply:</strong> <a href="17669.html">Daniel Burfoot: "Re: Risks of distributed AI (was Re: Investing in FAI research: now vs. later)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17668">[ date ]</a>
<a href="index.html#17668">[ thread ]</a>
<a href="subject.html#17668">[ subject ]</a>
<a href="author.html#17668">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
