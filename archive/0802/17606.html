<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Investing in FAI research: now vs. later</title>
<meta name="Author" content="Peter C. McCluskey (pcm@rahul.net)">
<meta name="Subject" content="Re: Investing in FAI research: now vs. later">
<meta name="Date" content="2008-02-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Investing in FAI research: now vs. later</h1>
<!-- received="Sat Feb  9 09:57:31 2008" -->
<!-- isoreceived="20080209165731" -->
<!-- sent="Sat,  9 Feb 2008 08:55:12 -0800 (PST)" -->
<!-- isosent="20080209165512" -->
<!-- name="Peter C. McCluskey" -->
<!-- email="pcm@rahul.net" -->
<!-- subject="Re: Investing in FAI research: now vs. later" -->
<!-- id="20080209165512.ECFAB2BEA2@mauve.rahul.net" -->
<!-- inreplyto="79ecaa350801261129k2e03c4erfaa22113dac94732@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Peter C. McCluskey (<a href="mailto:pcm@rahul.net?Subject=Re:%20Investing%20in%20FAI%20research:%20now%20vs.%20later"><em>pcm@rahul.net</em></a>)<br>
<strong>Date:</strong> Sat Feb 09 2008 - 09:55:12 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17607.html">joshua@joshuafox.com: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>Previous message:</strong> <a href="17605.html">Thomas Buckner: "Re: KILLTHREAD: Re: Barack"</a>
<li><strong>In reply to:</strong> <a href="../0801/17538.html">Rolf Nelson: "Re: Investing in FAI research: now vs. later"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17607.html">joshua@joshuafox.com: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>Reply:</strong> <a href="17607.html">joshua@joshuafox.com: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>Reply:</strong> <a href="17614.html">Rolf Nelson: "Re: Investing in FAI research: now vs. later"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17606">[ date ]</a>
<a href="index.html#17606">[ thread ]</a>
<a href="subject.html#17606">[ subject ]</a>
<a href="author.html#17606">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;<a href="mailto:rolf.h.d.nelson@gmail.com?Subject=Re:%20Investing%20in%20FAI%20research:%20now%20vs.%20later">rolf.h.d.nelson@gmail.com</a> (Rolf Nelson) writes:
<br>
<em>&gt;Peter, overconfidence is indeed an ongoing risk with this venture (as,
</em><br>
<em>&gt;indeed, it is with any venture, especially one that is attempting to build a
</em><br>
<em>&gt;new technology). In general, all things equal, simple solutions should be
</em><br>
<em>&gt;preferred to complex solutions.
</em><br>
<em>&gt;
</em><br>
<em>&gt;However, the ratio between AGI existential risk and killer-asteroid risk in
</em><br>
<em>&gt;this century has got to be on the order of one to a million!* Despite this,
</em><br>
<em>&gt;I would estimate asteroid-impact overall commands more resources than FAI
</em><br>
<em>&gt;does.** I don't know how much you propose Bayesian shifting for
</em><br>
<em>&gt;overconfidence, but surely it's not a shift of that magnitude.
</em><br>
<p>&nbsp;After reflecting on this for a while, I'm a good deal more uncertain
<br>
than I was in my last email, but I still think it's at least a reasonable
<br>
guess that the probability of a moderately smart person identifying a way
<br>
advance FAI is more than a million times smaller than knowing how to
<br>
advance asteroid detection. Your use of the word &quot;surely&quot; suggests that
<br>
rather than just adjusting for overconfidence, you should rethink your
<br>
reasoning more thoroughly.
<br>
&nbsp;I'd say the number of smart people who have mistakenly thought they
<br>
could create an important AI breakthrough suggests we should assume
<br>
any one AGI effort should have a success probability somewhere around
<br>
0.01 to 0.0001. Constraining the goal to be friendly and to be complete
<br>
before an unfriendly AU could easily reduce the probability by an order
<br>
of magnitude or more. If many of the people offering resources to the
<br>
project don't understand the design, then there is an incentive for people
<br>
without serious designs to imitate serious researchers. How much you should
<br>
adjust your estimates for this risk seems fairly sensitive to how well you
<br>
think you understand what the project is doing and why it ought to work.
<br>
I'd guess the typical member of this list ought to use somewhere between
<br>
a factor of 2 and 10. So the most optimistic estimate I'm willing to take
<br>
seriously is that a moderately smart person would do several hundred times
<br>
better giving to FAI research than to asteroid detection, and I think it's
<br>
more likely that giving to FAI research is 2 or 3 orders of magnitude less
<br>
promising.
<br>
&nbsp;I suspect it's a good idea to make some adjustment for overconfidence
<br>
at this point, but I'm having trouble thinking quantitatively about that.
<br>
&nbsp;I'm tempted to add in some uncertainty about whether the AI designer(s)
<br>
will be friendly to humanity or whether they'll make the AI friendly to
<br>
themselves only. But that probably doesn't qualify as an existential risk,
<br>
so it mainly reflects my selfish interests.
<br>
&nbsp;Note that none of this addresses the question of how much effort one
<br>
should spend trying to convince existing AI researchers to avoid creating
<br>
an AGI that might be unfriendly.
<br>
<p>&nbsp;As for which tasks currently gets more resources, I find them hard to
<br>
compare. It appears that more money is usefully spent on asteroid detection,
<br>
and that money is the primary resource controlling asteroid detection
<br>
results. It isn't clear whether money is being usefully spent on FAI or
<br>
whether additional money would have any effect on it. I would not be
<br>
surprised if something changes my opinion about that in the next few
<br>
years.
<br>
<p><em>&gt;Perhaps my own conclusions differs from yours as follows: first of all, I
</em><br>
<em>&gt;have confidence in the abilities of the current FAI community; and second of
</em><br>
<p>&nbsp;Can you describe reasons for that confidence?
<br>
<p><em>&gt;all, if I didn't have confidence, I would try to bring about the creation of
</em><br>
<em>&gt;a new community, or bring about improvements of the existing community,
</em><br>
<p>&nbsp;Does that follow from a belief about how your skills differ from those
<br>
of a more typical person, or are you advocating that people accept this
<br>
as a default approach?
<br>
&nbsp;There are a number of tasks for which the average member of this list is
<br>
likely to be aware that he would have negligible influence, such as unifying
<br>
relativity with quantum mechanics or inventing time travel. I suggest that
<br>
FAI presents similar difficulties.
<br>
<p>&nbsp;I apologize for my delay in responding.
<br>
<pre>
--
------------------------------------------------------------------------------
Peter McCluskey         | The road to hell is paved with overconfidence
www.bayesianinvestor.com| in your good intentions. - Stuart Armstrong
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17607.html">joshua@joshuafox.com: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>Previous message:</strong> <a href="17605.html">Thomas Buckner: "Re: KILLTHREAD: Re: Barack"</a>
<li><strong>In reply to:</strong> <a href="../0801/17538.html">Rolf Nelson: "Re: Investing in FAI research: now vs. later"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17607.html">joshua@joshuafox.com: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>Reply:</strong> <a href="17607.html">joshua@joshuafox.com: "Re: Investing in FAI research: now vs. later"</a>
<li><strong>Reply:</strong> <a href="17614.html">Rolf Nelson: "Re: Investing in FAI research: now vs. later"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17606">[ date ]</a>
<a href="index.html#17606">[ thread ]</a>
<a href="subject.html#17606">[ subject ]</a>
<a href="author.html#17606">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
