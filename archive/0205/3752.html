<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Review of Novamente</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Review of Novamente">
<meta name="Date" content="2002-05-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Review of Novamente</h1>
<!-- received="Sun May 19 21:55:45 2002" -->
<!-- isoreceived="20020520035545" -->
<!-- sent="Sun, 19 May 2002 20:59:47 -0400" -->
<!-- isosent="20020520005947" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Review of Novamente" -->
<!-- id="3CE84A83.FD76F776@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJKEKMCHAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Review%20of%20Novamente"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun May 19 2002 - 18:59:47 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3753.html">Ben Goertzel: "RE: Complexity of AGI"</a>
<li><strong>Previous message:</strong> <a href="3751.html">Ben Goertzel: "RE: Complexity of AGI"</a>
<li><strong>In reply to:</strong> <a href="3643.html">Ben Goertzel: "RE: Review of Novamente"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3754.html">Ben Goertzel: "RE: Review of Novamente"</a>
<li><strong>Reply:</strong> <a href="3754.html">Ben Goertzel: "RE: Review of Novamente"</a>
<li><strong>Reply:</strong> <a href="3755.html">Ben Goertzel: "Finding for SIAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3752">[ date ]</a>
<a href="index.html#3752">[ thread ]</a>
<a href="subject.html#3752">[ subject ]</a>
<a href="author.html#3752">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; &gt; Well, I can only criticize what's *in* the book.  If in the whole book
</em><br>
<em>&gt; &gt; there's no mention of emergent maps and then you say that you
</em><br>
<em>&gt; &gt; expect most of
</em><br>
<em>&gt; &gt; Novamente's functionality to come from emergent maps, then
</em><br>
<em>&gt; &gt; there's not much I can say about;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well I did send you an extra chapter on emergent maps, drawn from previously
</em><br>
<em>&gt; written material that hadn't made its way into the book.
</em><br>
<p>And what the chapter contains is descriptions of emergent maps that very
<br>
strongly resemble the code-level elements and behaviors.  These are things
<br>
whose behaviors I am willing to believe you understand, since they are
<br>
described precisely in the account of the system's code level.  Of course,
<br>
the chapter you sent me still does not contain an account of how
<br>
*specifically* these maps emerge, so you may not get even those.
<br>
<p>I guess my basic thesis is:  &quot;If you don't know how to describe how it
<br>
emerges, what it does, why it's there, how it contributes to general
<br>
intelligence - if you do not, in short, know fully what you are doing and
<br>
why - you will not succeed.&quot;
<br>
<p>There is no hope in this.  Whether you can &quot;prove&quot; that something occurs to
<br>
someone else may be an unnecessarily high standard, but you should at least
<br>
be able to visualize for yourself how it occurs - not hope, or intuit, that
<br>
it occurs.
<br>
<p><em>&gt; And, some of the others who read it -- who were more a priori sympathetic
</em><br>
<em>&gt; than you to my overall philosophy of mind -- seemed to be more willing to
</em><br>
<em>&gt; &quot;fill in the gaps&quot; themselves and have had a more positive assessment of the
</em><br>
<em>&gt; design than yourself.
</em><br>
<p>I did fill in the gaps.  The result of filling in the gaps was a system that
<br>
was potentially more powerful than the sum of its parts - but not nearly
<br>
powerful enough to be an AGI.  Remember, when I finished, I did send you
<br>
that example account of how all the generic subsystems could work together
<br>
on the same problem, and then I asked you if that was how maps work.  And
<br>
you said, &quot;Gee, maybe we should talk about this in the manuscript
<br>
somewhere.&quot;  And I said, &quot;Gee, maybe you should.&quot;  Cuz the problem with
<br>
asking people to fill in the gaps is that they fill in the gaps using
<br>
*their* philosophy of mind instead of *your* philosophy of mind.
<br>
<p><em>&gt; There seems to be a fundamental philosophical point here...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think most of the X's that you're referring to are things that, according
</em><br>
<em>&gt; to our theory of mind, are supposed to be *emergent* phenomena rather than
</em><br>
<em>&gt; parts of the codebase
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The book gives a mathematical formalization of the stuff that's supposed to
</em><br>
<em>&gt; be in the codebase
</em><br>
<p>When you say &quot;emergent&quot;, I say &quot;higher level of organization&quot;.  I deeply
<br>
distrust your intuitions on this for two reasons:
<br>
<p>1)  I have to search through a large-space of plausible-looking wrong
<br>
answers in order to find even a single answer that looks workable enough to
<br>
meet my standards, when trying to figure out how to create a high-level
<br>
behavior.
<br>
<p>2)  I feel that one of the classical psychological pathologies of AI is (a)
<br>
not seeing the higher levels of organization, (b) denying they exist, or (c)
<br>
hoping that they will emerge for free even though you don't know exactly
<br>
how.
<br>
<p>This makes me suspicious of a claim that a higher-level behavior emerges
<br>
automatically because over here it sure as heck looks like a small target in
<br>
design space.  It also makes me suspicious of claims that you can get higher
<br>
levels of organization without doing all that work.  Now, I could be just
<br>
spinning my wheels and making unnecessary problems for myself.  That's
<br>
always a threat.  But I think it would take a concrete demonstration or at
<br>
least a fully visualized walkthrough to convince me that there is a free
<br>
lunch here.  Sure, there might be free lunches on some behaviors, but *all*
<br>
of them?  Why postulate this when we know that evolution is perfectly
<br>
capable of multilayered design?  Is there any other example in biology where
<br>
you can design cells that work well as cells and find that tissues, organs,
<br>
and organ systems all emerge automatically?  Why would you *expect* a free
<br>
lunch here?
<br>
<p><em>&gt; &gt; Why did it take so long to scale from spider brains to human brains?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 'Cause evolution is a terribly inefficient learning mechanism ;&gt;
</em><br>
<p>I don't think this is enough to explain it.  Under DGI it's pretty clear why
<br>
you have to go through chimpanzees in order to incrementally evolve human
<br>
intelligence (and I spend time discussing it in the paper).  Given the
<br>
Novamente theory in which all higher levels of cognition emerge naturally
<br>
from a small set of lower-level behaviors, there is no obvious (to me)
<br>
reason why the Novamente behaviors would not be incrementally evolvable, nor
<br>
any obvious reason why spider brains would not incrementally scale to human
<br>
size and capabilities.  Is there a reason why the Novamente design - not
<br>
just as it is now, but for all plausible variations thereoff - is
<br>
unevolvable?
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3753.html">Ben Goertzel: "RE: Complexity of AGI"</a>
<li><strong>Previous message:</strong> <a href="3751.html">Ben Goertzel: "RE: Complexity of AGI"</a>
<li><strong>In reply to:</strong> <a href="3643.html">Ben Goertzel: "RE: Review of Novamente"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3754.html">Ben Goertzel: "RE: Review of Novamente"</a>
<li><strong>Reply:</strong> <a href="3754.html">Ben Goertzel: "RE: Review of Novamente"</a>
<li><strong>Reply:</strong> <a href="3755.html">Ben Goertzel: "Finding for SIAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3752">[ date ]</a>
<a href="index.html#3752">[ thread ]</a>
<a href="subject.html#3752">[ subject ]</a>
<a href="author.html#3752">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
