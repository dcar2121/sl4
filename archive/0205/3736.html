<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: DGI, Novamente, AGI,...</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="DGI, Novamente, AGI,...">
<meta name="Date" content="2002-05-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>DGI, Novamente, AGI,...</h1>
<!-- received="Sun May 19 18:45:42 2002" -->
<!-- isoreceived="20020520004542" -->
<!-- sent="Sun, 19 May 2002 16:45:12 -0600" -->
<!-- isosent="20020519224512" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="DGI, Novamente, AGI,..." -->
<!-- id="LAEGJLOGJIOELPNIOOAJOEEPCIAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=Re:%20DGI,%20Novamente,%20AGI,..."><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sun May 19 2002 - 16:45:12 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3737.html">Justin Corwin: "Re: AI in &lt;what?&gt;"</a>
<li><strong>Previous message:</strong> <a href="3735.html">Ben Goertzel: "RE: Complexity of AGI"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3736">[ date ]</a>
<a href="index.html#3736">[ thread ]</a>
<a href="subject.html#3736">[ subject ]</a>
<a href="author.html#3736">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt;  My intuition is that simulating a working brain without
</em><br>
<em>&gt; understanding the mind, a la Kurzweil and Leitl, will turn out to
</em><br>
<em>&gt; require an
</em><br>
<em>&gt; insanely detailed simulation (down to the microtubular level, perhaps) to
</em><br>
<em>&gt; ensure that all necessary functional qualities of neurons are duplicated
</em><br>
<em>&gt; when the researchers don't know, in fact, what the functional qualities of
</em><br>
<em>&gt; neurons are.  This entire scenario seems to me to be built around
</em><br>
<em>&gt; Kurzweil's
</em><br>
<em>&gt; desire to convince an audience of the workability of transhuman
</em><br>
<em>&gt; intelligence
</em><br>
<em>&gt; without Kurzweil having to defend the idea that anyone will ever
</em><br>
<em>&gt; comprehend
</em><br>
<em>&gt; intelligence.  It is not futuristically plausible.  Kurzweil is
</em><br>
<em>&gt; (unconsciously, I assume) optimizing for convenience of argument, not
</em><br>
<em>&gt; faithfulness to the real world.
</em><br>
<p>Well, our intuitions differ here pretty substantially.
<br>
<p>I don't *know* how the brain works, nobody does, of course.
<br>
<p>My guess is that a simulation at the cellular level, with some attention to
<br>
extracellular diffusion of charge and neurotransmitter chemistry, can do the
<br>
trick.
<br>
<p>In other words, I think that a simulation of the brain can be achieved via a
<br>
good understanding of brain cells, their interconnections, and the chemical
<br>
reactions mediating their electrical interactions.  I think this can be
<br>
done, potentially, WITHOUT anywhere near a full understanding of how
<br>
thoughts, ideas, fears, dreams, insights and psychoses come out of the
<br>
brain.  Of course, *some* understanding of the cognitive meanings of neural
<br>
structures/dynamics will be needed, but how much?
<br>
<p><em>&gt; In real life, the researchers would start to see what the neural networks
</em><br>
<em>&gt; are doing and why long before you have the capability to run a simulation
</em><br>
<em>&gt; perfect enough that the scan works whether or not you know what
</em><br>
<em>&gt; the networks
</em><br>
<em>&gt; are doing.  Could we eventually simulate networks so perfectly that they
</em><br>
<em>&gt; worked without our understanding their higher functions?  Yes.  But that's
</em><br>
<em>&gt; an existence proof, not a prediction.  It's not how the future would
</em><br>
<em>&gt; actually develop.
</em><br>
<p>I'm not sure; the problem of inferring details of thought dynamics from
<br>
brain dynamics is VERY hard.  This is an area I work actively in --
<br>
inferring complex nonlinear dynamics from numerous coupled time series.  I
<br>
believe the problem can be solved partially by &quot;narrow AI&quot; (by systems like
<br>
the current novamente version, which  may be the most sophisticated existing
<br>
system for solving this kind of &quot;inferring the dynamics from the data&quot;
<br>
problem), but it's certainly not a no-brainer!!
<br>
<p>Of course, you may have a much lower estimate than I do of the *dynamical*
<br>
complexity involved in getting mindstuff out of brainstuff.
<br>
<p><em>&gt; Large corporations routinely build systems with hundreds of times as many
</em><br>
<em>&gt; lines of code as Novamente.
</em><br>
<p>Yes, of course, Windows 2000 has millions of lines of code.
<br>
<p>This is why I did not compare Novamente to other systems in terms of lines
<br>
of code, but in terms of
<br>
<p>a) number of different interacting algorithms
<br>
<p>b) complexity of interaction of different algorithms
<br>
<p>I said that
<br>
<p>a) I think Novamente has about as many different interacting algorithms as a
<br>
modern C++ compiler
<br>
<p>b) I think the complexity of interaction of the algorithms in Novamente is
<br>
far greater than in existing software systems, including NT with its 50
<br>
million lines of code
<br>
<p>It is the complexity of interaction between different component algorithms
<br>
which makes systems like this hard to test, debug and parameter-tune, not
<br>
lines of code, and not evern raw &quot;number of different interacting
<br>
algorithms.&quot;
<br>
<p>Sorry that I was not clearer in stating my views on this.
<br>
<p><em>&gt; Also, I happen to feel that incorrect AI
</em><br>
<em>&gt; designs contribute nontrivially to the amount of work that gets dumped on
</em><br>
<em>&gt; parameter-tuning, engineering, and performance analysis.
</em><br>
<p>Why do you think a correct AI design would not require substantial
<br>
parameter-tuning, performance analysis and engineering?
<br>
<p>The human brain clearly has required very substantial &quot;parameter-tuning&quot;
<br>
over an evolutionary time-scale, and it has also been &quot;performance tuned&quot; by
<br>
evolution in many ways.
<br>
<p>In all other software systems that I know of, &quot;complexity of interaction of
<br>
different algorithms&quot; is a good indicator of the amount of parameter-tuning,
<br>
subtle engineering design, and complicated performance analysis that has to
<br>
be done.
<br>
<p>So, why do you think a &quot;correct&quot; AGI design would avoid these issues that
<br>
can be seen in the brain and in all other complex software systems?
<br>
<p><em>&gt;From what I know of DGI, I think it's going to require an incredible amount
</em><br>
of subtle performance analysis and engineering and parameter tuning to get
<br>
it to work at all.  Even after you produce a detailed design, I mean.  If
<br>
you can produce a detailed design based on your DGI philosophy, that will be
<br>
great.  If you can produce such a design that
<br>
<p>-- can be made efficient without a huge amount of engineering effort and
<br>
performance analysis, and
<br>
<p>-- has a small number of free parameters, the values of other parameters
<br>
being give by theory
<br>
<p>THEN, my friend, you will have performed what I would term &quot;One Fucking Hell
<br>
of a Miracle.&quot;  I don't believe it's possible, although I do consider it
<br>
possible you can make a decent AI design based on your DGI theory.
<br>
<p>In fact, I think that the engineering and performance analysis problems are
<br>
likely to be significantly GREATER for a DGI based AI design than for
<br>
Novamente, because, DGI makes fewer compromises to match itself up to the
<br>
ways and means of contemporary computer hardware &amp; software frameworks.
<br>
<p><em>&gt; Imagine Lenat saying, &quot;Well,
</em><br>
<em>&gt; suppose that you
</em><br>
<em>&gt; need to enter a trillion facts into the system... in this case it
</em><br>
<em>&gt; would make
</em><br>
<em>&gt; sense to scan an existing human brain because no programming team could
</em><br>
<em>&gt; handle the engineering challenge of managing relationships among a dataset
</em><br>
<em>&gt; that large.&quot;
</em><br>
<p>But this is the worst example you could have possibly come up with!  Cyc is
<br>
very easy to engineer precisely because it makes so many simplifying
<br>
assumptions.
<br>
<p>In almost all cases, I believe, incorrect AI theories have led to overly
<br>
SIMPLE implementation designs, not overly complex ones.  AI scientists have
<br>
VERY often, it seems to me, simplified their theories so they would have
<br>
theories that could be implemented without excessive implementation effort
<br>
and excessive parameter tuning.
<br>
<p><em>&gt; Of course, it's hard for me to see in advance what will turn out to be the
</em><br>
<em>&gt; real, unexpected critical challenges of building DGI.  But I suspect that
</em><br>
<em>&gt; when the pieces of a correct AI design are hooked together, 90% of the
</em><br>
<em>&gt; humanly achievable functionality will take 10% of the humanly possible
</em><br>
<em>&gt; tuning.  In other words, I think that the tremendous efforts you put into
</em><br>
<em>&gt; tuning Webmind are symptomatic of an AI pathology.
</em><br>
<p><p>In almost all cases, we do parameter tuning *automatically*, by using
<br>
optimization methods to tune parameters.  We then have to tune the
<br>
parameters of the optimization methods themselves by hand, but this is
<br>
rarely difficult.
<br>
<p>The problem is that when you have a VERY LARGE number of INTERRELATED
<br>
parameters, the problem of automated parameter adaptation via optimization
<br>
algorithms becomes too formidable.  This is what happens when a system is
<br>
too complex in terms of the subtlety of interaction between different
<br>
algorithms.
<br>
<p>Webmind did have this problem, and I think Novamente will not, because it's
<br>
a simpler system in many ways.
<br>
<p>I'm afraid you are fooling yourself when you say that parameter tuning will
<br>
not be a big issue for your AI system.
<br>
<p>Even relatively simple AI models like attractor neural nets require a lot of
<br>
parameter tuning.  Dave Goldberg has spent years working on parameter tuning
<br>
for the GA.  Of course, you can claim that this is because these are all bad
<br>
techniques and you have a good one up your sleeve.  But I find it hard to
<br>
believe you're going to come up with the first-ever complex computational
<br>
system for which parameter-tuning is not a significant problem.
<br>
<p>Yes, a sufficiently advanced system can tune its own parameters, and
<br>
Novamente does this in many cases; but intelligent adaptive self-tuning for
<br>
a very complex system presents an obvious bootstrapping problem, which is
<br>
trickier the more complex the system is.
<br>
<p><em>&gt; That is not the kind of specialized complexity that goes into creating a
</em><br>
<em>&gt; DGI-model AI.  Computational systems give rise to cognitive talents;
</em><br>
<em>&gt; cognitive talents combine with experiential content to give rise to domain
</em><br>
<em>&gt; competencies.
</em><br>
<p>Sure, this is going to be the case for *any* feasible AGI system
<br>
<p><em>&gt; DGI does not contain *more specialized
</em><br>
<em>&gt; versions* of these subsystems that support specific cognitive
</em><br>
<em>&gt; talents, which
</em><br>
<em>&gt; is what you seem to be visualizing, but rather contains a *completely
</em><br>
<em>&gt; different* set of underlying subsystems whose cardinality happens to be
</em><br>
<em>&gt; larger than the cardinality of the set of Novamente subsystems.
</em><br>
<p>Can you give us a hint of what these underlying subsystems are?
<br>
<p>Are they the structures described in the DGI philosophy paper that you
<br>
posted to this list, or something quite different?
<br>
<p><p><em>&gt; I believe this problem is an AI pathology of the Novamente architecture.
</em><br>
<em>&gt; (This is not a recent thought; I've had this impression ever
</em><br>
<em>&gt; since I visited
</em><br>
<em>&gt; Webmind Inc. and saw some poor guy trying to optimize 1500
</em><br>
<em>&gt; parameters with a
</em><br>
<em>&gt; GA.)
</em><br>
<p>Webmind had about 300 parameters, if someone told you 1500 they were goofing
<br>
around.
<br>
<p>However, only about 25 of them were ever actively tuned, the others were set
<br>
at fixed values.
<br>
<p>Adding in the unimplemented parts of Webmind might have doubled these
<br>
numbers, because we left some of the most complex stuff for last.
<br>
<p>I sure am eager to see how DGI or *any* AGI system is going to avoid this
<br>
sort of problem.
<br>
<p>A2I2 is pretty simple now -- there are many parameters but most of them I'm
<br>
sure can be kept at fixed values.  However, they may face this sort of issue
<br>
when they try to go beyond the digital cockroach level and build a more
<br>
diversified artificial neural net structure.
<br>
<p><em>&gt; &gt; we'd be better off to focus on brain scanning and
</em><br>
<em>&gt; &gt; cellular brain simulation.
</em><br>
<em>&gt;
</em><br>
<em>&gt; That doesn't help.
</em><br>
<p>Your extreme confidence in this regard, as in other matters, seems
<br>
relatively unfounded.
<br>
<p>Many people with expertise in brain scanning and biological systems
<br>
simulation disagree with you.
<br>
<p><em>&gt; Novamente has what I would consider a flat architecture, like
</em><br>
<em>&gt; &quot;Coding a Transhuman AI&quot; circa 1998.  Flat architectures come with certain
</em><br>
<em>&gt; explosive combinatorial problems that can only be solved with deep
</em><br>
<em>&gt; architectures.  Deep architectures are admittedly much harder to
</em><br>
<em>&gt; think about
</em><br>
<em>&gt; and invent.
</em><br>
<p>&quot;Deep architecture&quot; is a cosmic-sounding term; would you care to venture a
<br>
definition?  I don't really know what you mean, except that you're implying
<br>
that your ideas are deep and mine are shallow.
<br>
<p>My own subjective view, not surprisingly, is that YOUR approach is
<br>
&quot;shallower&quot; than mind, in that it does not seem to embrace the depth of
<br>
dynamical complexity and emergence that exists in the mind.  You want to
<br>
ground concepts too thoroughly in images and percepts rather than accepting
<br>
the self-organizing, self-generating dynamics of the pool of intercreating
<br>
concepts that is the crux of the mind.  I think that Novamente accepts this
<br>
essential depth of the mind whereas DGI does not, because in DGI the concept
<br>
layer is a kind of thin shell sitting on top of perception and action,
<br>
relying on imagery for most of its substance.
<br>
<p>The depth of the Novamente design lies in the dynamics that I believe (based
<br>
on intuition not proof!) will emerge from the system, not in the code
<br>
itself.  Just as I believe the depth of the human brain lies in the dynamics
<br>
that emerge from neural interactions, not in the neurons and
<br>
neurotransmitters and glia and so forth.  Not even the exalted
<br>
microtubules!!
<br>
<p><em>&gt; It requires that you listen to your quiet, nagging
</em><br>
<em>&gt; doubts about
</em><br>
<em>&gt; shallow architectures and that you go on relentlessly replacing
</em><br>
<em>&gt; every single
</em><br>
<em>&gt; shallow architecture your programmer's mind invents, until you
</em><br>
<em>&gt; finally start
</em><br>
<em>&gt; to see how deep architectures work.
</em><br>
<p>Ah, how my colleagues would laugh to see you describe me as having a
<br>
&quot;programmer's mind&quot; !!!
<br>
<p>For sure, I am at bottom a philosopher, much as you are I suspect. You may
<br>
disagree with my philosophy but the fact remains that I spent about 8 years
<br>
working on mathematically and scientifically inspired philosophy (while also
<br>
doing various scientific projects), before venturing to design an AGI.
<br>
Novamente is not at all a programming-driven AI project, although at this
<br>
stage we are certainly using all the algorithmic and programming tricks we
<br>
can find, in the service of the design.  The design was inspired by a
<br>
philosophy of mind, and is an attempt to realize this philosophy of mind in
<br>
a practical way using contemporary hardware and software.  The design may
<br>
fail to realize the philosophy, which will not invalidate the philosophy.
<br>
My philosophy of mind (see Chaotic Logic and From Complexity to Creativity)
<br>
does not tell you whether Novamente will lead to the emergences I think it
<br>
will; this would require a mathematics of mind, not just a philosophy with
<br>
some mathematical inspiration.
<br>
<p><em>&gt; I'm sorry, Ben, but I don't think that Novamente lies right at the fringes
</em><br>
<em>&gt; of the most complex systems that are humanly comprehensible.  Different
</em><br>
<em>&gt; people will have different ideas of what constitutes &quot;depth
</em><br>
<em>&gt; beyond the human
</em><br>
<em>&gt; ability to comprehend&quot;.  I don't see how you can know what's too deep for
</em><br>
<em>&gt; humans to comprehend, anyway; all information available is of the
</em><br>
<em>&gt; form &quot;X is
</em><br>
<em>&gt; too deep for me to comprehend at my current level of skill&quot;.
</em><br>
<p>You seem to have misinterpreted me.  I am not talking about anything being
<br>
in principle beyond human capability to comprehend forever.  Some things ARE
<br>
(this is guaranteed by the finite brain size of the human species), but
<br>
that's not the point I'm making.
<br>
<p>What I am talking about is the set of things that are humanly comprehensible
<br>
*before than a detailed brain simulation can be implemented*.  I believe a
<br>
detailed human brain simulation will be achievable within 30 years or so,
<br>
and therefore, in my view, only software systems that will be humanly
<br>
comprehensible and constructable *before this* can be considered competitive
<br>
with the detailed brain simulation approach.
<br>
<p>I still believe it's possible that the AGI design problem is SO hard that
<br>
detailed brain simulation is easier.  I hope this isn't true, but if pressed
<br>
I'd give it a 10%-20% chance of being true.  Generally, I am not prone to
<br>
the near 100% confident judgments that you are, Eliezer.  I think I tend to
<br>
be more aware of the limitations of my own knowledge and cognitive ability,
<br>
than you are of your own corresponding limitations.
<br>
<p>Of course, if this is true, then the Novamente work is still pretty
<br>
worthwhile.  I have a nice datafile of MEG brain scan data on my hard drive
<br>
waiting for Novamente analysis ;&gt;
<br>
<p><em>&gt; I think you'd be better off if you stopped thinking of some level of
</em><br>
<em>&gt; complexity as &quot;too difficult&quot; and started thinking of that level of
</em><br>
<em>&gt; complexity as &quot;my responsibility, my challenge; the work of Evolution, my
</em><br>
<em>&gt; rival and target.&quot;  I find that quite a number of things
</em><br>
<em>&gt; supposedly &quot;beyond
</em><br>
<em>&gt; human ability&quot; are so-called because people use the phrase &quot;beyond human
</em><br>
<em>&gt; ability&quot; when they mentally flinch away from the prospect of having to do
</em><br>
<em>&gt; something.
</em><br>
<p>Eliezer, I think it is rather funny for *you* to accuse *me* of flinching
<br>
away from the prospect of trying to do something!
<br>
<p>In the late 90's, after many years of thought, I decided to try to implement
<br>
the best AGI design I could think of, because I decided that, while far from
<br>
ideal, it had a pretty decent chance of working.  You disagree and think
<br>
that Novamente has a very small chance of working.  Fine.  But I did not
<br>
choose to implement Novamente because of any fear of implementing a more
<br>
complicated system.  Rather, it was a pragmatic decision.  I decided that
<br>
anything significantly MORE complicated would have a LESSER chance of
<br>
succeeding for practical reasons.  On the other hand, anything much simpler
<br>
(like A2I2), in my view, isn't going to be efficient on current hardware and
<br>
software.
<br>
<p>It seems to me that perhaps it is YOU who are flinching away from the
<br>
prospect of having to do something!! Where is the design for the DGI-based
<br>
AI system?  Where is the prototype code?
<br>
<p>I may have different ideas than you, but I am certainly not flinching away
<br>
from taking the actions I believe are correct.
<br>
<p>I am not taking the actions *you* think are correct, not out of fear, but
<br>
out of disagreement!!  Furthermore, I am not a dogmatic person and it is
<br>
quite possible for me to be convinced I'm wrong.  So far your criticisms of
<br>
Novamente have convinced me that I've done a mediocre job of *explaining
<br>
why* I think it will be a real AGI, but have not caused me to doubt my own
<br>
intuitions.  But maybe I'll see your detailed DGI design and decide it has
<br>
an even better chance than Novamente of succeeding -- who knows.  Based on
<br>
the DGI philosophy paper, I doubt this will happen, because I think the
<br>
concept level needs to have a lot more freedom and dynamical complexity; but
<br>
maybe this freedom and dynamical complexity will be implicit in your design
<br>
in some way even though you didn't emphasize it in your philosophical
<br>
write-up.  I look forward to seeing the details one of these years and
<br>
finding out ;-&gt;
<br>
<p><p>yours,
<br>
Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3737.html">Justin Corwin: "Re: AI in &lt;what?&gt;"</a>
<li><strong>Previous message:</strong> <a href="3735.html">Ben Goertzel: "RE: Complexity of AGI"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3736">[ date ]</a>
<a href="index.html#3736">[ thread ]</a>
<a href="subject.html#3736">[ subject ]</a>
<a href="author.html#3736">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
