<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Taking AI seriously (was: DGI Paper)</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Taking AI seriously (was: DGI Paper)">
<meta name="Date" content="2002-05-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Taking AI seriously (was: DGI Paper)</h1>
<!-- received="Mon May 06 01:14:33 2002" -->
<!-- isoreceived="20020506071433" -->
<!-- sent="Sun, 5 May 2002 23:15:37 -0600" -->
<!-- isosent="20020506051537" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Taking AI seriously (was: DGI Paper)" -->
<!-- id="LAEGJLOGJIOELPNIOOAJCEHCCHAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3CD5FA9C.3B686634@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Taking%20AI%20seriously%20(was:%20DGI%20Paper)"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sun May 05 2002 - 23:15:37 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3612.html">Ben Goertzel: "RE: Review of Novamente &amp; a2i2"</a>
<li><strong>Previous message:</strong> <a href="3610.html">Ben Houston: "individual differences in terms of cognitive style"</a>
<li><strong>In reply to:</strong> <a href="3605.html">Eliezer S. Yudkowsky: "Taking AI seriously (was: DGI Paper)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3616.html">Eliezer S. Yudkowsky: "Re: Taking AI seriously (was: DGI Paper)"</a>
<li><strong>Reply:</strong> <a href="3616.html">Eliezer S. Yudkowsky: "Re: Taking AI seriously (was: DGI Paper)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3611">[ date ]</a>
<a href="index.html#3611">[ thread ]</a>
<a href="subject.html#3611">[ subject ]</a>
<a href="author.html#3611">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
howdy pardner...
<br>
<p><em>&gt; Ben, sometimes writing code is taking the easy way out.
</em><br>
<p>Sure, that's true.  Personally I spent about 7 years theorizing about this
<br>
stuff before writing any code, and although I started prototype coding in
<br>
1994, I still spent more time theorizing than dealing with implementation
<br>
issues until 1997.
<br>
<p>Frankly, I code *very little* at the moment; I reached my peak of coding
<br>
activity in 1997 when I started working on Webmind and hadn't recruited any
<br>
programmer-collaborators yet.  I am really a far better theorist than
<br>
programmer.
<br>
<p>Anyway, I definitely can't be accurately accused of having &quot;rushed into
<br>
coding.&quot;
<br>
<p><p><em>&gt; I understand that
</em><br>
<em>&gt; you believe resources should be put into Novamente, rather than,
</em><br>
<em>&gt; say, SIAI,
</em><br>
<p>I think resources should be put into a whole host of AGI projects, not just
<br>
my own.
<br>
<p>If I were very wealthy, I'd fund a Novamente team, but I'd also give some
<br>
cash to you, peter voss, Pei wang, and others with interesting AGI projects.
<br>
<p>Unfortunately for us all, however, this is not the case!!!
<br>
<p><em>&gt; But with all due respect,
</em><br>
<em>&gt; Novamente seems to be constructed out of ideas that I had at one point or
</em><br>
<em>&gt; another, but which I looked at and said:  &quot;No, it's not that easy.  This
</em><br>
<em>&gt; problem is harder than that - this method will work for small
</em><br>
<em>&gt; problems, but
</em><br>
<em>&gt; not for big problems; it's not good enough for real AI.&quot;
</em><br>
<p>I can believe that various parts of Novamente are *somewhat similar* to
<br>
things that you (and many others) studied and dismissed in the past.
<br>
<p>However, I do not believe that you have previously proposed the detailed
<br>
ideas inside Novamente and then rejected them.
<br>
<p>For example...
<br>
<p>I know from your comments on ANN's on this list a few months ago, that you
<br>
don't have the depth of knowledge of ANN's to have proposed the
<br>
ANN/nonlinear-dynamics aspects of Novamente.
<br>
<p>Similarly, your confusion about the relation between probabilistic logic &amp;
<br>
term logic, in a recent e-mail, indicates to me that you don't have the
<br>
depth of knowledge of that area to have proposed those aspects of Novamente
<br>
in the past.
<br>
<p>Did you, at some point in the past, propose to use combinatory logic to
<br>
represent complex procedures in an inference-, association- and evolution-
<br>
friendly way?  I really doubt it.
<br>
<p>Etc.
<br>
<p>Frankly, most of the components of Novamente are *somewhat similar* to
<br>
things that *I* studied and dismissed in the past.
<br>
<p>And then I came to believe that by improving these various components
<br>
appropriately, and integrating the improved version appropriately, I could
<br>
make a system that could become an AGI.
<br>
<p>I did not start with these component technologies.  I began with a
<br>
high-level philosophical vision of an AGI, and then basically filled in the
<br>
various &quot;slots&quot; required by this vision, with appropriately improved
<br>
versions of existing technologies.  My first attempt at a system like this
<br>
(Webmind) was a bit too much of a hodge-podge, but Novamente has a lot more
<br>
simplicity and coherence, which I like.
<br>
<p>You are very right that none of the component technologies of Novamente
<br>
will, on their own, work for big problems.  Our belief is that by *piecing
<br>
together unscalable technologies in the right global architecture*, one can
<br>
arrive at a system that *will* work for big problems.
<br>
<p>I understand that you don't share this belief.  But you should understand
<br>
that the Novamente design is in no way refuted by the observation that one
<br>
or another of the component technologies, on its own, is not scalable or is
<br>
not suitable as an AGI.
<br>
<p><em>&gt; To me it looks
</em><br>
<em>&gt; like Novamente is going to try for real AI and go splat.
</em><br>
<em>&gt; It's
</em><br>
<em>&gt; just not that
</em><br>
<em>&gt; powerful
</em><br>
<p>What baffles me is not the fact that you hold this opinion, but the
<br>
incredible air of certainty with which you make such a statement.  You *may*
<br>
possibly be right, but there's just no way you can *know* this!!
<br>
Goodness....
<br>
<p><em>&gt; You
</em><br>
<em>&gt; are welcome to believe that the problem of creating true intelligence is
</em><br>
<em>&gt; enormously smaller than I think it is, and that enormously less complexity
</em><br>
<em>&gt; is needed to handle it, in which case I'm sure it makes sense for you to
</em><br>
<em>&gt; criticize me on the grounds of not having flung myself at the
</em><br>
<em>&gt; problem yet.
</em><br>
<p>I think that a true intelligence needs to be an *incredibly* complex system,
<br>
but I think that much of this complexity can be made to *emerge* from an
<br>
appropriately structured AGI framework.  I don't think that a very high
<br>
percentage of the complexity of an AGI has to be *explicitly* there in the
<br>
sourcecode.
<br>
<p><em>&gt; From my perspective, it is very easy and tempting to start
</em><br>
<em>&gt; implementing an
</em><br>
<em>&gt; inadequate design, but futile.
</em><br>
<p>Well, yes, if you don't have a design that you believe is adequate, it may
<br>
not make sense for you to implement anything.
<br>
<p>On the other hand, as you surely realize, it's also possible you would learn
<br>
something by experimenting with implementing a design that was not adequate
<br>
for the grand goal.
<br>
<p><em>&gt; You have been known, from time to time, to remark on my youth and my not
</em><br>
<em>&gt; having running AI code, which I consider to be &quot;cheap shots&quot; (i.e., taking
</em><br>
<em>&gt; the easy way out),
</em><br>
<p>Sorry if I seemed to be taking cheap shots at you, of course that was never
<br>
my intention.
<br>
<p>Of course, your age or your inexperience are not important points.  The
<br>
important thing is the quality of your ideas.
<br>
<p>So far, I find the quality of your ideas in the *philosophy of AI* to be
<br>
very good.
<br>
<p>I'm eager to see you make the transition from philosophy to AI system
<br>
design.
<br>
<p><em>&gt; so let me take what I fully acknowledge to be a cheap
</em><br>
<em>&gt; shot, and ask whether either Novamente or Webmind have done
</em><br>
<em>&gt; anything really
</em><br>
<em>&gt; impressive in the realm of AI?  If you have so much more
</em><br>
<em>&gt; experience than I,
</em><br>
<em>&gt; then can you share the experiences that lead you to believe Novamente is a
</em><br>
<em>&gt; design for a general intelligence, rather than (as it seems to me) a
</em><br>
<em>&gt; pattern-recognition system that may be capable of achieving
</em><br>
<em>&gt; limited goals in
</em><br>
<em>&gt; a very small class of patterns that are tractable for it?
</em><br>
<p>I am not going to turn this into a 100 page e-mail.  I'd rather spend the
<br>
time working on Novamente or improving our systematic exposition of it.
<br>
<p>But I don't want to &quot;cop out&quot; so I will say something.
<br>
<p>As you know, we have not yet created an AGI.  We have created software
<br>
systems (Webmind &amp; Novamente) that have decent achievements in various
<br>
&quot;narrow AI&quot; domains such as text processing, financial prediction, and
<br>
biological data analysis.  I think it's very cool that we have one design
<br>
that can excel in all these areas, but this kind of &quot;multi-area narrow AI&quot;
<br>
is different from AGI.
<br>
<p>With Webmind, we ran lots of experiments showing how putting different AI
<br>
components together caused dramatic improvements in efficiency and
<br>
scalability.  I really don't want to run through all the details tonight,
<br>
it's time for bed...
<br>
<p>We ran some simple &quot;experiential interactive learning&quot; experiments, but they
<br>
were more at the level of a &quot;digital roach&quot; than a &quot;digital doggie&quot;... and
<br>
then we ran into nasty performance problems with the Webmind software
<br>
architecture (remedied with the Novamente architecture, and not connected
<br>
with scalability problems in the underlying AI algorithms).
<br>
<p>So, nope, we have no more experience with &quot;real AGI&quot; than you or anyone else
<br>
on the planet.  No one has built one yet.
<br>
<p>Whether the various experiences we've had experimenting with our AI systems
<br>
have been of any value for AGI or not, I suppose time will tell.  So far our
<br>
feeling is that they HAVE been valuable.
<br>
<p>It is certainly not the case that we fully implemented our AI design, tried
<br>
it out as an experiential learning system, and then found that it didn't
<br>
work except for simple pattern recognition tasks.  Sadly enough, we never
<br>
finished implementing Webmind.  We spent a lot of time applying the partial
<br>
versions to various business-related data analysis tasks, and we found that
<br>
we'd been *really fucking dumb* to implement the system as a Java
<br>
distributed agents system, because the performance with a large number of
<br>
nodes and links in the system was just terrible.  On the other hand, we
<br>
learned a hell of a lot that seems valuable to *us*, even if it doesn't seem
<br>
valuable to you.  We learned a lot about how to make the various component
<br>
technologies we're interested in work well together, so as to drastically
<br>
accelerate each others' scalability and intelligence.  And we observed
<br>
plenty of interesting Novamente-relevant dynamical phenomena.  The list of
<br>
all these detailed lessons would be 100's of pages, and although the book
<br>
draft you read didn't have enough of such &quot;practical lessons&quot; in it, a later
<br>
draft eventually will.
<br>
<p><em>&gt; It looks
</em><br>
<em>&gt; to me like
</em><br>
<em>&gt; another AI-go-splat debacle in the making.
</em><br>
<p>Yeah, I think you've probably repeated that often enough, Eliezer.
<br>
<p>I know that is your opinion, and everyone else on this list (if they have
<br>
bothered to read all these messages) does as well.
<br>
<p>I think you're as wrong as wrong can be on certain points ...
<br>
<p>You think I'm as wrong as wrong can be on certain points...
<br>
<p>And neither of us can prove ourselves right.
<br>
<p>So there's no real use to continue repeating it over and over, is there?
<br>
<p>As I said, I think this level of disagreement is pretty natural in a field
<br>
where there is so little hard knowledge to go on, so that projects need to
<br>
be guided largely by intuition.
<br>
<p>And let us not lose sight of the fact that, compared to most of the AI
<br>
community, and most of the people on the planet, we agree almost totally on
<br>
almost everything!!! ;-&gt;
<br>
<p><em>&gt; Why do it?  Why make all these lofty predictions?  When SIAI
</em><br>
<em>&gt; starts its own
</em><br>
<em>&gt; AI project, we aren't going to be telling people we'll have a
</em><br>
<em>&gt; [whatever] in
</em><br>
<em>&gt; [whenever].
</em><br>
<p>You know, I think I've actually gotten over that mistake, which I'll admit I
<br>
was making 4-5 years ago.
<br>
<p>We are not making any promises about when we will have a human-level AGI.
<br>
<p>The time to complete *engineering* the current Novamente design can be
<br>
predicted with some degree of accuracy.
<br>
<p>But the time to tune all the parameters to get the thing to work right, and
<br>
to teach the thing anything meaningful, etc., is very hard to predict --
<br>
*even if the design is right*.  Nothing like this has ever been done before.
<br>
<p><p><em>&gt; Right now it looks to me like, in another few years, I'm going to
</em><br>
<em>&gt; be dealing
</em><br>
<em>&gt; with people asking:  &quot;Yeah, well, what happened to the Novamente project
</em><br>
<em>&gt; that promised us transhuman seed AI, and (didn't pan out) / (turned out to
</em><br>
<em>&gt; be just a data-mining system)?&quot;  And I'm going to wearily say, &quot;I
</em><br>
<em>&gt; predicted
</em><br>
<em>&gt; in advance that would happen, and that in fact I would end up
</em><br>
<em>&gt; answering this
</em><br>
<em>&gt; very question; here, let me show you the message in the SL4 archives.&quot;
</em><br>
<p>Eliezer, you could be right.  Time will tell.
<br>
<p>I'll tell you one thing though.  It's sure pretty easy to talk to people
<br>
trying to do ambitious things and tell them &quot;That won't work!  You've
<br>
underestimated the difficulty of the problem!&quot;
<br>
<p>If you say that to 10 people trying to actually build AGI right now, you're
<br>
bound to be right in at least, say, 8 or 9 of the cases.  In which case
<br>
you'll come out looking like you're pretty clever -- 80% or 90% prediction
<br>
accuracy!!
<br>
<p><em>&gt; You keep saying that I ought to just throw myself into design, as
</em><br>
<em>&gt; if it were
</em><br>
<em>&gt; an ordinary problem of above-average difficulty, rather than a
</em><br>
<em>&gt; critical step
</em><br>
<em>&gt; along the pathway of one of the ultimate challenges.
</em><br>
<p>The fact that something is a critical step along a very important pathway,
<br>
tells you ABSOLUTELY NOTHING about how difficult it is, actually.
<br>
<p><em>&gt; In the first chapter
</em><br>
<em>&gt; of your manuscript you casually toss around the terms &quot;seed AI&quot; and
</em><br>
<em>&gt; &quot;transhuman intelligence&quot; as if they were marketing buzzwords.  You don't
</em><br>
<em>&gt; present it as a climax of a long, careful argument; you just toss
</em><br>
<em>&gt; it in with
</em><br>
<em>&gt; no advance justification.  It's like you first claimed that
</em><br>
<em>&gt; Novamente could
</em><br>
<em>&gt; do general intelligence because that was the most impressive thing you'd
</em><br>
<em>&gt; heard of, and once you heard about the Singularity you decided to add that
</em><br>
<em>&gt; as a claim too.
</em><br>
<p>Eliezer, I gave a lot more extensive discussions of the future of technology
<br>
in Creating Internet Intelligence, and there will be still more in my
<br>
forthcoming book The Path to Posthumanity.
<br>
<p>The focus of the manuscript you read was on the Novamente AI design, not on
<br>
transhumanist philosophy or even philosophy of mind.  An extended discussion
<br>
of such matters would have been out of place there.  The book manuscript I
<br>
gave you has many flaws, but I don't think the lack of an extended
<br>
discussion of transhumanist and Singularitarian philosophy is one of them.
<br>
In fact, in that chapter, I reference my own previous writings on related
<br>
topics, and yours as well, which I think is the appropriate thing to do in a
<br>
book with a different focus.
<br>
<p>In any case, the good or bad qualities of my writing style are not all that
<br>
relevant to the quality of my AI design.  You may try to draw parallels, but
<br>
I think you're reeeeeeallly stretching.
<br>
<p>Also, your speculations as to my personal motivations for introducing these
<br>
concepts are pretty far off.  My interest in real AI as a world-changing
<br>
technology far predated my work on Webmind or Novamente.
<br>
<p>In fact, I chose to work on AI, in the late 1980's, because: Out of all the
<br>
advanced technologies I could think of, it seemed like the one most likely
<br>
to create a huge impact.  I also considered time travel research, but I
<br>
figured that real results in that area were too far off.  I considered life
<br>
extension, genetic engineering, and brain science.  But I figured that I'd
<br>
be better off creating an AI that could become generally intelligent -- and
<br>
then figure out all these other areas of science way better than my measly
<br>
human brain.
<br>
<p>So my desire, for many years before I started working on Webmind, let alone
<br>
Novamente, was to create an AI that would be much smarter than me,
<br>
especially in the domains of science and engineering.  The idea of
<br>
intelligence-increasing self-modification was very familiar to me from
<br>
various speculative futurist stuff I'd read.
<br>
<p>Was I thinking about the Singularity back then, in exactly the terms in
<br>
which Kurzweil is discussing it now?  No, and I'm still not a 100%
<br>
Singularity true believer like you are; I still consider it reasonably
<br>
possible that technological advance will flatten out for a while for some
<br>
reason we can't see yet.
<br>
<p>But I was thinking about superhuman intelligence, and AI as a way to vastly
<br>
accelerate general scientific and engineering progress and enable life
<br>
extension -- WAAAAAY before I had any specific ideas about AI design.  These
<br>
general desires were why I started designing Webmind &amp; Novamente in the
<br>
first place.
<br>
<p>It's been amusing to me to see the sci-tech community sloooowly catch up
<br>
with me -- so that now, newbies such as yourself can pop up and accuse *me*
<br>
of stealing these ideas which I've been nurturing for so long!!! ;-&gt;
<br>
<p><p><em>&gt; Lenat can claim that Cyc is a design for a real
</em><br>
<em>&gt; AI.  Newell
</em><br>
<em>&gt; and Simon can claim that GPS is a design for a real AI.  It doesn't mean
</em><br>
<em>&gt; that you've gotten started coding a seed AI and I haven't.  It means that
</em><br>
<em>&gt; you have a much lower threshold for accepting what looks to you like a
</em><br>
<em>&gt; probable solution.
</em><br>
<p>I don't think I have a *lower threshold*, I think I just have a *different
<br>
intuition* as to what a seed AI should look like.
<br>
<p>Maybe when (if ever ;) you present your design to me, I'll think that it
<br>
doesn't look like a viable design!  Then I'll be able to tell you that YOU
<br>
have a &quot;lower threshold&quot; ;-&gt;
<br>
<p>Who knows...
<br>
<p><em>&gt; And I'll admit I'm annoyed, and I'm even
</em><br>
<em>&gt; more annoyed
</em><br>
<em>&gt; that you're using the term &quot;seed AI&quot;
</em><br>
<p>&quot;Seed AI&quot; is a convenient term; but if I knew my use of it was going to
<br>
annoy you, I would have coined a different term for the same thing.  I had
<br>
no desire to annoy you via my choice of wording, of course.
<br>
<p>Although I didn't know the phrase back then, I was thinking about how to
<br>
achieve &quot;seed AI&quot; back when you were in diapers, dude!! :&gt;
<br>
<p>Anyway, I wish you could find a better way to expend your emotional energy,
<br>
than being annoyed at someone who agrees with you more than 99.99995% of the
<br>
people on Earth.
<br>
<p>I wish you could accept that reasonable, intelligent, knowledgeable people
<br>
can have different intuitions about which AGI designs are viable or not.
<br>
Because there just ain't enough evidence for any of us to prove that we're
<br>
right and the other guys are wrong!
<br>
<p>But hey, in the end, we're all just pathetic little meat machines, right?
<br>
Nobody's perfect -- yet  ;&gt;
<br>
<p>One more thing.  I have spent waaaaaay too much time e-mailing to this list
<br>
over the last few days.  I type fast, but not *that* fast.  I need to get
<br>
more work done!!  (And I need to get my fucking car fixed, it was broken
<br>
into and the radio ripped out of the dashboard, on Friday ;-[  )  So if your
<br>
e-mails in the near future get briefer replies than this one, this is why.
<br>
It's not a boredom with the dialogue, just a reflection of the amount of
<br>
other things besides e-mailing that I have to do right now.
<br>
<p>-- ben g
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3612.html">Ben Goertzel: "RE: Review of Novamente &amp; a2i2"</a>
<li><strong>Previous message:</strong> <a href="3610.html">Ben Houston: "individual differences in terms of cognitive style"</a>
<li><strong>In reply to:</strong> <a href="3605.html">Eliezer S. Yudkowsky: "Taking AI seriously (was: DGI Paper)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3616.html">Eliezer S. Yudkowsky: "Re: Taking AI seriously (was: DGI Paper)"</a>
<li><strong>Reply:</strong> <a href="3616.html">Eliezer S. Yudkowsky: "Re: Taking AI seriously (was: DGI Paper)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3611">[ date ]</a>
<a href="index.html#3611">[ thread ]</a>
<a href="subject.html#3611">[ subject ]</a>
<a href="author.html#3611">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
