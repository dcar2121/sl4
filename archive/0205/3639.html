<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Review of Novamente</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Review of Novamente">
<meta name="Date" content="2002-05-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Review of Novamente</h1>
<!-- received="Wed May 08 05:09:56 2002" -->
<!-- isoreceived="20020508110956" -->
<!-- sent="Wed, 08 May 2002 04:54:22 -0400" -->
<!-- isosent="20020508085422" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Review of Novamente" -->
<!-- id="3CD8E7BE.4CCC5E87@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJOEFPCHAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Review%20of%20Novamente"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed May 08 2002 - 02:54:22 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3640.html">Dani Eder: "C compiler that reconfigures hardware"</a>
<li><strong>Previous message:</strong> <a href="3638.html">Eliezer S. Yudkowsky: "Re: An AI Parable"</a>
<li><strong>In reply to:</strong> <a href="3581.html">Ben Goertzel: "RE: Review of Novamente"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3643.html">Ben Goertzel: "RE: Review of Novamente"</a>
<li><strong>Reply:</strong> <a href="3643.html">Ben Goertzel: "RE: Review of Novamente"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3639">[ date ]</a>
<a href="index.html#3639">[ thread ]</a>
<a href="subject.html#3639">[ subject ]</a>
<a href="author.html#3639">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
A note:  Ben and I have discussed Novamente privately before now, and I'm
<br>
not sure I want to recap the whole discussion from SL4.  However, a couple
<br>
of people have come forward and said that they want to see more discussion
<br>
between me and Ben about Novamente.  A problem is that, unlike when I'm
<br>
talking with Ben, I can't quote from the Novamente manuscript and I can't
<br>
assume anyone else has read it.  However, I'll give it a shot.
<br>
<p>Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Unfortunately, I think that Eliezer did not really understand the basic
</em><br>
<em>&gt; concepts underlying the design, based on his reading of the manuscript.
</em><br>
<em>&gt; Obviously, since Eliezer is very smart and has a fair bit of relevant
</em><br>
<em>&gt; knowledge, this means that the book manuscript is in piss-poor shape.  We
</em><br>
<em>&gt; should have a much better draft within 6 months or so.  My feeling is that
</em><br>
<em>&gt; Eliezer's understanding of the desing was impaired significantly by his
</em><br>
<em>&gt; strong philosophical biases which are different from my own strong
</em><br>
<em>&gt; philosophical biases.
</em><br>
<p>Well, I can only criticize what's *in* the book.  If in the whole book
<br>
there's no mention of emergent maps and then you say that you expect most of
<br>
Novamente's functionality to come from emergent maps, then there's not much
<br>
I can say about; except, of course, that as far as I can tell what *was*
<br>
discussed in the Novamente manuscript won't give rise to emergent maps that
<br>
suffice for general intelligence.  I can see that Novamente's several
<br>
generic processes can theoretically exhibit stepwise interaction through a
<br>
common representation, but this is mentioned in the manuscript only in
<br>
passing; it doesn't give specific examples of how this works, explain to
<br>
what degree this has been observed to work in Webmind and to what extent it
<br>
is only a theoretical expectation from Novamente, and so on.  When I hadn't
<br>
read the Novamente manuscript, I was basically willing to assume that, in
<br>
the absence of more specific information, Novamente might have some process
<br>
that implemented X.  But if I read the manuscript and there's no mention of
<br>
X, then I'm going to assume X is not supported unless I hear a specific,
<br>
critique-able explanation of X - not in terms of &quot;P.S: Novamente can do X&quot;
<br>
but in terms of &quot;This is how these design features support X.&quot;  Where we are
<br>
at right now is that you've given me the manuscript, I read the manuscript,
<br>
I said &quot;This isn't general intelligence&quot;, and you said &quot;Ah, but all the
<br>
general intelligence features aren't in the manuscript.&quot;  Okay.  It could be
<br>
true.  But my working assumption is still going to be that the parts
<br>
discussed in the manuscript describe the parts you actually know how to do,
<br>
and all the interesting stuff that isn't in the manuscript are things you
<br>
*want* to do but have not actually worked out in the level of design detail
<br>
that would be required to discuss them in the manuscript.
<br>
<p>Naturally, if you're going to focus on having a design in hand as a critical
<br>
point, then I may be excused for pointing out that you do not seem to have a
<br>
design in hand for the most critical parts of your system.
<br>
<p><em>&gt; To sum up before giving details, basically, Eliezer's critique is that
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1) he doesn't see how a collection of relatively simple, generic processes
</em><br>
<em>&gt; working together can give rise to a rich enough set of emergent dynamics and
</em><br>
<em>&gt; structures to support AGI
</em><br>
<p>I don't see how *your specific* collection of simple, generic processes,
<br>
working in the fashion described in the Novamente manuscript, and
<br>
interacting as I picture them (because their interaction is *not* described
<br>
in any specific detail in the Novamente manuscript), will support those rich
<br>
emergent dynamics that I think are needed to support AGI.  I don't know
<br>
whether they'll support the rich emergent dynamics that *you* hope for from
<br>
Novamente, or whether these hoped-for dynamics would be sufficient for AGI
<br>
if you had them, because these dynamics are not documented in the Novamente
<br>
manuscript.  Certainly I feel that if one takes the Novamente design at face
<br>
value then it is not an AGI.
<br>
<p><em>&gt; 2) he doesn't think it's sensible to create a network *some of whose basic
</em><br>
<em>&gt; nodes and links have explicit semantic meaning*, but whose basic cognitive
</em><br>
<em>&gt; dynamics is based on *emergent meaning resident in patterns in the basic
</em><br>
<em>&gt; node-and-link-network&quot;
</em><br>
<p>What kind of emergent meaning?  How does it emerge and why?  Can you give a
<br>
specific example of a case where emergent meaning in Novamente is expected
<br>
to contribute to general intelligence, including the nature of the emergent
<br>
meaning, the low-level support of the emergent meaning, and the specific
<br>
contribution made to general intelligence?  If these things are not
<br>
documented in your design then it is natural for me to assume that they are
<br>
not driving the design.  Your design description makes sense on its own;
<br>
it's hard for me to believe that the entire motivation behind it is
<br>
missing.  The emergent behaviors you expect from Novamente seem to me like
<br>
hopes, rather than part of the design.
<br>
<p>My overall prediction for Novamente is that all the behaviors you are hoping
<br>
will &quot;emerge naturally&quot;, won't.  You may get naturally emergent behaviors
<br>
that implement a tiny subset A of the vast problem space B, and you may say,
<br>
&quot;Yay!  We solved B!&quot;, but B itself will not emerge naturally and will not in
<br>
fact be solvable by processes of the same basic character as A on reasonable
<br>
computing hardware.  You may get things that you can call &quot;emergent maps&quot;
<br>
and that help solve one or two problems, but you won't get the kind of
<br>
emergent maps you need for general intelligence.
<br>
<p><em>&gt; Since I can't prove he's wrong or I'm right on these points, I guess it's
</em><br>
<em>&gt; just gonna remain a difference of intuition for a while.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; One nice thing about this sort of work is that it's empirical.  Assuming the
</em><br>
<em>&gt; team holds together, we will finish implementing and testing the mofo and
</em><br>
<em>&gt; see if we're right or wrong.
</em><br>
<p>Yes, quite true.  And I'm doing my best to predict specific failure modes
<br>
that are falsifiable, rather than vague premonitions of doom that would
<br>
simply consist of jeering from the audience.
<br>
<p><em>&gt; &gt; Capsule description of Novamente's architecture:  Novamente's core
</em><br>
<em>&gt; &gt; representation is a semantic net, with nodes such as &quot;cat&quot; and &quot;fish&quot;, and
</em><br>
<em>&gt; &gt; relations such as &quot;eats&quot;.  Some kind of emotional reaction is called for
</em><br>
<em>&gt; &gt; here, lest others suspect me of secret sympathies for semantic networks:
</em><br>
<em>&gt; &gt; &quot;AAAARRRRGGGHHH!&quot;  Having gotten that over with, let's forge ahead.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is not a correct statement; the core data representation is not a
</em><br>
<em>&gt; semantic network.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It is a NETWORK, with nodes and links.  Some nodes and links may have
</em><br>
<em>&gt; transparent semantic meaning, such as &quot;cat&quot; or &quot;eats&quot;.  Others -- the vast
</em><br>
<em>&gt; majority -- will not.  And if a node has a transparent meaning like &quot;cat&quot;,
</em><br>
<em>&gt; this meaning (and the node) must be built by the system, not loaded in
</em><br>
<em>&gt; externally.
</em><br>
<p>Really?  You don't enter nodes directly into Novamente, right now, at the
<br>
current state of the system?  Or is this something that you hope to do later
<br>
but haven't done yet?  How does the node get the name &quot;cat&quot;?  In current
<br>
versions of Novamente, how much of the network is semantic and how much is
<br>
not?
<br>
<p><em>&gt; The intention is that much of the semantics of the system resides, not
</em><br>
<em>&gt; directly in individual nodes and links, but rather in &quot;maps&quot; or
</em><br>
<em>&gt; &quot;attractors&quot; -- patterns of connectivity and interconnection involving large
</em><br>
<em>&gt; numbers of nodes and links.
</em><br>
<p>This is an *intention*.  I don't see it in the Novamente design.  What kinds
<br>
of maps?  What kinds of attractors?  What functions do they implement?
<br>
<p><em>&gt; &gt; Novamente's core representation is not entirely that of a
</em><br>
<em>&gt; &gt; classical AI; Ben
</em><br>
<em>&gt; &gt; insists that it be described as &quot;term logic&quot; rather than
</em><br>
<em>&gt; &gt; &quot;predicate logic&quot;,
</em><br>
<em>&gt; &gt; meaning that it has quantitative truth values and quantitative attention
</em><br>
<em>&gt; &gt; values (actually, Novamente can express more complex kinds of truth values
</em><br>
<em>&gt; &gt; and attention values than simple quantities).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Okay, there two different confusions in this paragraph.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1) Logical inference is only one among very many dynamics involved in
</em><br>
<em>&gt; Novamente.  &quot;Term logic&quot; is not a representation, it is a way of combining
</em><br>
<em>&gt; some links to form new links.  The node-and-link representation is designed
</em><br>
<em>&gt; to support probabilistic term logic among many other important dynamics.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 2) The difference between predicate logic and term logic has nothing to do
</em><br>
<em>&gt; with the use of probabilistic truth values.  The difference between
</em><br>
<em>&gt; predicate logic and term logic has to do with the structure of the inference
</em><br>
<em>&gt; rules involved.  In term logic two statements can only be combined if they
</em><br>
<em>&gt; share common terms; this is not true in predicate logic.  This difference
</em><br>
<em>&gt; has a lot of philosophical implications: it means that term logic is not
</em><br>
<em>&gt; susceptible to the same logical paradoxes as predicate logic, and that term
</em><br>
<em>&gt; logic is better suited for implementation in a distributed self-organizing
</em><br>
<em>&gt; knowledge system like Novamente.
</em><br>
<p>Okay.  It's true, as you say below, that I tend to lump together certain
<br>
systems that you think have key distinctions; i.e., I do not believe these
<br>
distinctions are progress toward building a real AI system, while you
<br>
believe that they do.
<br>
<p><em>&gt; &gt; Similarly, Novamente's
</em><br>
<em>&gt; &gt; logical inference processes are also quantitative; fuzzy logic rather than
</em><br>
<em>&gt; &gt; theorem proving.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Again there are two different confusions overlaid.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; First, &quot;Fuzzy logic&quot; in the technical sense has no role in Novamente.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Next, there is a whole chapter in the  manuscript on theorem-proving.  I
</em><br>
<em>&gt; think this is one thing the system will eventually be able to do quite well.
</em><br>
<em>&gt; In fact, I think that probabilistic inference and other non-inferential
</em><br>
<em>&gt; cognitive aspects like evolutionary concept creation and
</em><br>
<em>&gt; association-formation, are highly critical to mathematical theorem-proving.
</em><br>
<p>Okay.
<br>
<p><em>&gt; And I think that expertise at theorem-proving will be an important partway
</em><br>
<em>&gt; step towards intelligent goal-directed self-modification.  There was an SL4
</em><br>
<em>&gt; thread on the possible use of the Mizar theorem/proof database for this
</em><br>
<em>&gt; purpose, about a year ago.
</em><br>
<p>Yes.  I must say that this idea stands out in my mind as seeming *very*
<br>
GOFAIish - the idea that mathematical reasoning can be implemented/taught
<br>
using Novamente's probabilistic inference on a series of Novamente
<br>
propositions corresponding directly to the formal steps of a Mizar proof.
<br>
<p><em>&gt; &gt;&gt; However, from my perspective, Novamente has very *simple* behaviors for
</em><br>
<em>&gt; &gt; inference, attention, generalization, and evolutionary programming.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; We have tried to simplify these basic cognitive processes as much as
</em><br>
<em>&gt; possible.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The complexity of cognition is intended to emerge from the self-organizing
</em><br>
<em>&gt; interaction of the right set of simple processes on a large set of
</em><br>
<em>&gt; information.  NOT from complexity of the basic behaviors.
</em><br>
<p>Well, that's a major difference of philosophy between us, not only about how
<br>
to approach AI, but what constitutes &quot;having a design&quot; for an AI.
<br>
<p>I expect to be documenting which behaviors are supposed to be emerging from
<br>
which other behaviors and which behaviors are supposed to be emergent from
<br>
them, and I expect this to force internal specialization on multiple levels
<br>
of organization.  If I see you trying to make all the complexity of
<br>
cognition emerge from generic behaviors, then my suspicion is naturally that
<br>
you haven't mentally connected the hoped-for emergent behaviors to the level
<br>
of organization from which they are supposed to be emergent, and hence
<br>
experience no mental pressure to design low-level behaviors with internal
<br>
specialization that naturally fits the emergent behaviors.
<br>
<p><em>&gt; &gt; For
</em><br>
<em>&gt; &gt; example, Novamente notices spontaneous regularities by handing off the
</em><br>
<em>&gt; &gt; problem to a generic data-mining algorithm on a separate server.  The
</em><br>
<em>&gt; &gt; evolutionary programming is classical evolutionary programming.
</em><br>
<em>&gt; &gt; The logical
</em><br>
<em>&gt; &gt; inference has classical Bayesian semantics.  Attention spreads
</em><br>
<em>&gt; &gt; outward like
</em><br>
<em>&gt; &gt; ripples in a pond.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; All of these statements are wrong, Eliezer.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Novamente notices regularities in internal and external by many different
</em><br>
<em>&gt; mechanisms.  The Apriori datamining algorithm that you mention is a simple
</em><br>
<em>&gt; preprocessing technique used to suggest potentially interesting regularities
</em><br>
<em>&gt; to the main cognition algorithms.  It is by no means the sum total or even
</em><br>
<em>&gt; the centerpiece of the system's approach to recognizing regularities.
</em><br>
<p>Hence the wording, &quot;spontaneous regularities&quot;.  Please bear in mind that
<br>
when I look at Novamente I am looking at it through the lens of DGI.  When I
<br>
think of noticing spontaneous similarities, I think of background processes
<br>
that work against current imagery.  In Novamente the analogue of this part
<br>
of the mind is Apriori datamining, or at least, if there's anything else
<br>
that does this, it's not in the manuscript.  There are other kinds of
<br>
noticeable regularities.  The point is that, at least at this point, there
<br>
is a case where there's a part of Novamente that seems to have a fairly
<br>
clear analogy with a cognitive process postulated in DGI, and this causes me
<br>
to say that Novamente's way of handling this process is not complex enough. 
<br>
I believe that the similarity-noticing parts of the mind are driven by a
<br>
certain specific kind of patterned interaction with attention, certain
<br>
learnable and instinctive biases, and a certain kind of nongeneral context
<br>
sensitivity, which means that handing it all off to Apriori doesn't seem
<br>
likely to work right.  However, the local regularity-noticing implemented by
<br>
content within Novamente and not datamined globally cannot (I think) perform
<br>
this function properly either.  I realize that everything in Novamente
<br>
interacts in at least some ways with everything else, but that doesn't mean
<br>
you have the specific interactions you need.
<br>
<p><em>&gt; The evolutionary programming in Novamente is not classical ev. programming;
</em><br>
<em>&gt; it has at least two huge innovations (only one of which has been tested so
</em><br>
<em>&gt; far): 1) evolution is hybridized with probabilistic inference, which can
</em><br>
<em>&gt; improve efficiency by a couple orders of magnitude,
</em><br>
<p>Can improve?  Already has improved?  You hope will improve?
<br>
<p>Incidentally, I am willing to believe not only this statement, but even the
<br>
statement that hybridizing evolution with inference significantly widens the
<br>
solution space; I just still don't think it's wide enough.
<br>
<p><em>&gt; 2) evolution takes place
</em><br>
<em>&gt; on node-and-link structures interpretable as combinatory logic expressions,
</em><br>
<em>&gt; which  means that functions with loops and recursion can be much  more
</em><br>
<em>&gt; efficiently learned (this is not yet tested).  These may sound like small
</em><br>
<em>&gt; technical improvements, but they are specifically improvements that allow
</em><br>
<em>&gt; ev. prog. to become smarter &amp; more effective thru feedback with other parts
</em><br>
<em>&gt; of the mind.
</em><br>
<p>Okay, stepwise classical evolution on a slightly specialized representation
<br>
that interacts with other stepwise generic processes.  I suppose that from
<br>
your perspective it is indeed unfair to call this &quot;classical evolutionary
<br>
programming&quot;.  BUT the Novamente manuscript does not give examples of how
<br>
evolutionary programming is supposed to interact with logical inference; it
<br>
just says that it is.
<br>
<p><em>&gt; The logical inference system does not have classical Bayesian semantics, not
</em><br>
<em>&gt; at all.  No single consistent prior or posterior distribution is assumed
</em><br>
<em>&gt; over all knowledge available to the system.  Rather, each individual
</em><br>
<em>&gt; inference constructs its own distributions prior to inference.  This means
</em><br>
<em>&gt; that the inference behavior of the system as a whole involves many
</em><br>
<em>&gt; overlapping pdf's rather than one big pdf.  This is just NOT classical
</em><br>
<em>&gt; Bayesian semantics in any sense, sorry.
</em><br>
<p>Hm, I think we have different ideas of what it means to invoke &quot;Bayesian
<br>
semantics&quot;.  Of course Bayesian semantics are much more popular in CompSci
<br>
and so your usage is probably closer to the norm.  When I say Bayesian
<br>
semantics I am simply contrasting them to, say, Tversky-and-Kahneman
<br>
semantics; what I mean is semantics that obey the Bayesian behaviors for
<br>
quantitative probabilities, not necessarily analogy with the philosophical
<br>
bases or system designs of those previous AI systems that have been
<br>
advertised as &quot;Bayesian&quot; approaches.
<br>
<p><em>&gt; &gt; Novamente does not have the complexity that
</em><br>
<em>&gt; &gt; would render
</em><br>
<em>&gt; &gt; these problems tractable; the processes may intersect in a common
</em><br>
<em>&gt; &gt; representation but the processes themselves are generic.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If by &quot;generic&quot; you mean that Novamente's basic cognitive processes are not
</em><br>
<em>&gt; functionally specialized, you are correct.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And I think this is as it should be.
</em><br>
<p>Why did it take so long to scale from spider brains to human brains?
<br>
<p><em>&gt; &gt; Ben believes that Novamente will support another level of
</em><br>
<em>&gt; &gt; organization above
</em><br>
<em>&gt; &gt; the current behaviors, so that inference/attention/mining/evolution of the
</em><br>
<em>&gt; &gt; low level can support complex constructs on the high level.  While I
</em><br>
<em>&gt; &gt; naturally agree that having more than one level of organization is a step
</em><br>
<em>&gt; &gt; forward, the idea of trying to build a mind on top of low-level behaviors
</em><br>
<em>&gt; &gt; originally constructed to imitate inference and attention is... well,
</em><br>
<em>&gt; &gt; Novamente is already the most alien thing I've ever tried to wrap my mind
</em><br>
<em>&gt; &gt; around;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I am afraid that, because the description you read was a very sloppy rough
</em><br>
<em>&gt; draft, and because the design is so intuitively alien to you, you have
</em><br>
<em>&gt; managed to achieve only a very partial understanding of the system.  Many
</em><br>
<em>&gt; things that, to me, are highly conceptually and philosophically significant,
</em><br>
<em>&gt; you seem to pass off as &quot;implementation details&quot; or &quot;tweaks to existing
</em><br>
<em>&gt; algorithms.&quot;
</em><br>
<p>I completely agree.  I think that what you designate as &quot;conceptual and
<br>
philosophical significance&quot; is what I would designate as the &quot;trophy
<br>
mentality&quot;.  When I think about AI, I try to maintain a mental state where I
<br>
have to show that each element of the design contributes materially to
<br>
general intelligence, and conceptual and philosophical significance counts
<br>
for nothing.  I would say, in fact, that my own reaction to the history of
<br>
the AI field has been to become very strongly prejudiced against permitting
<br>
oneself to attribute conceptual and philosophical significance because it
<br>
leads to declaring victory much too early - I consider it part of Failed
<br>
Promise Syndrome, the means by which researchers are seduced by ideas. 
<br>
There is such a thing as ideas that are important, but if so, you have to
<br>
implement those important ideas by finding the one way to do it that
<br>
actually works, and not any of the many easy ways that seem to match the
<br>
surface description.  It is a question of being willing to accept additional
<br>
burdens upon oneself, even unreasonable-seeming burdens, in order to meet
<br>
the problem on its own terms rather than yours.
<br>
<p><em>&gt; &gt; The lower
</em><br>
<em>&gt; &gt; levels of Novamente were designed with the belief that these lower levels,
</em><br>
<em>&gt; &gt; in themselves, implemented cognition, not with the intent that these low
</em><br>
<em>&gt; &gt; levels should support higher levels of organization.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is completely untrue.  You were not there when we designed these
</em><br>
<em>&gt; levels, so how on Earth can you make this presumption??
</em><br>
<p>Because they show no sign of specialization to support higher levels of
<br>
organization.  If I see a bird made out of blocky monocolor legos, I assume
<br>
that the legos were designed without the bird in mind.  If I see a bird made
<br>
out of legos that show a surface feather pattern and which have special
<br>
curved legos for the beak, I assume that the legos were designed with the
<br>
bird in mind.  (We are, of course, speaking of human design rather than
<br>
evolution; in evolution these assumptions are much trickier.)
<br>
<p><em>&gt; I spent the 8 years before starting designing Webmind, writing books and
</em><br>
<em>&gt; paper on self-organization and emergence in the mind.  (See especially
</em><br>
<em>&gt; Chaotic Logic and From Complexity to Creativity)
</em><br>
<p>Great, you're older.  I'm more dedicated.  Neither of these are necessarily
<br>
cognitive advantages.  So if we argue, let's argue our designs, not argue
<br>
the audience's a priori estimates of who would be expected to have the
<br>
better design.  1 year of thought on my part could easily turn out to be
<br>
equivalent to 4 or more years of thought on yours, or vice versa, depending
<br>
on how much time we spent thinking, the underlying fruitfulness of our
<br>
relative approaches, and of course our native intelligence levels which must
<br>
be Shown Not Told.
<br>
<p><em>&gt; OF COURSE, I did not design the lower levels of the system without the
</em><br>
<em>&gt; emergence of a higher level of structure and dynamics as a key goal.
</em><br>
<p>I know you had emergence as a goal.  What aspects of the system are there
<br>
*specifically* to support this goal?  What design requirements were handed
<br>
down from this goal?  And bear in mind also that from my perspective having
<br>
&quot;a higher level of structure and dynamics&quot; is not a good goal for an AGI
<br>
design; one should have certain specific high-level structures and dynamics,
<br>
and certain specific behaviors above *those* dynamics, and so on through
<br>
your system's specified levels of organization.  Of course you may disagree.
<br>
<p><em>&gt; &gt; For example, Ben has
</em><br>
<em>&gt; &gt; indicated that while he expects high-level inference on a
</em><br>
<em>&gt; &gt; separate level of
</em><br>
<em>&gt; &gt; organization to emerge above the current low-level inferential
</em><br>
<em>&gt; &gt; behaviors, he
</em><br>
<em>&gt; &gt; believes that it would be good to summarize the high-level patterns as
</em><br>
<em>&gt; &gt; individual Novamente nodes so that the faster and more powerful low-level
</em><br>
<em>&gt; &gt; inference mechanisms can operate on them directly.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think that the automated recognition *by the system* of high-level
</em><br>
<em>&gt; patterns in the system's mind, and the encapsulation of these patterns in
</em><br>
<em>&gt; individual nodes, is *one valuable cognitive heuristic* among many.
</em><br>
<p>Yes, but it's something that I see as a *profoundly mistaken* approach,
<br>
which is why I'm singling it out.  I see it as an indication that the higher
<br>
level of organization you hope for is being crushed into what I see as the
<br>
genericity of the lower level; that you think a mapping is even possible is,
<br>
to me, very worrisome in terms of what it leads me to think you are
<br>
visualizing as a higher level organization.
<br>
<p><em>&gt; The interplay between the concretely implemented structures/dynamics and the
</em><br>
<em>&gt; emergent ones, in Novamente, is going to be quite complex and interesting.
</em><br>
<em>&gt; This is where the complexity SHOULD lie, not at the level of the basic
</em><br>
<em>&gt; implemented structures and dynamics.
</em><br>
<p>While I worry that Novamente's emergent dynamics will be chained to the same
<br>
behaviors as the built-in ones.
<br>
<p><em>&gt; &gt; To see a genuine AI capability, you have to strip away the suggestive
</em><br>
<em>&gt; &gt; English names and look at what behaviors the system supports even
</em><br>
<em>&gt; &gt; if nobody
</em><br>
<em>&gt; &gt; is interpreting it.  When I look at Novamente through that lens, I see a
</em><br>
<em>&gt; &gt; pattern-recognition system that may be capable of achieving limited goals
</em><br>
<em>&gt; &gt; within the patterns it can recognize, although the goal system currently
</em><br>
<em>&gt; &gt; described (and, as I understand, not yet implemented or tested)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Webmind's goal system was implemented and tested, Novamente's is not (yet).
</em><br>
<p>What are you calling a &quot;goal system&quot; and what did Webmind do with it?
<br>
<p><em>&gt; &gt; would permit
</em><br>
<em>&gt; &gt; Novamente to achieve only a small fraction of the goals it should
</em><br>
<em>&gt; &gt; be capable
</em><br>
<em>&gt; &gt; of representing.  Checking with Ben confirmed that all of the old Webmind
</em><br>
<em>&gt; &gt; system's successes were in the domain of pattern recognition, so
</em><br>
<em>&gt; &gt; it doesn't
</em><br>
<em>&gt; &gt; look like my intuitions are off.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes, we were developing Webmind in the context of a commercial corporation,
</em><br>
<em>&gt; and so most of our practical testing concerned pragmatic data analysis
</em><br>
<em>&gt; tasks.  This doesn't mean that the architecture was designed to support ONLY
</em><br>
<em>&gt; this kind of behavior, nor even that it was the most natural stuff for us to
</em><br>
<em>&gt; be doing, in AI terms.  In fact, we ended up using the system for a lot of
</em><br>
<em>&gt; &quot;text analysis&quot; work that it was really relatively *ill-suited* for, because
</em><br>
<em>&gt; that was what the business's products needed.  (And the system performed
</em><br>
<em>&gt; well at text analysis, even though this really wasn't an appropriate
</em><br>
<em>&gt; application for it at that stage of its development).
</em><br>
<p>Again, this is the kind of encouraging statement that I used to suspend
<br>
judgement on before I read the Novamente manuscript.  Now that I've read it,
<br>
I would ask questions such as &quot;What specific applications did you think
<br>
Webmind was better suited for?&quot;, &quot;Did you test it?&quot;, &quot;What kind of results
<br>
did you get in what you thought of as better-suited applications?&quot;, and so
<br>
on.
<br>
<p><em>&gt; Developing AI in a biz context has its plusses and minuses.  The big plus is
</em><br>
<em>&gt; plenty of resources.  The big minus is that you get pushed into spending a
</em><br>
<em>&gt; lot of time on applications that distract the focus from real AI.
</em><br>
<p>Yes, Ben, hence our 501(c)(3) status.
<br>
<p><em>&gt; &gt; By the standards I would apply to real AI, Novamente is
</em><br>
<em>&gt; &gt; architecturally very
</em><br>
<em>&gt; &gt; simple and is built around a relative handful of generic
</em><br>
<em>&gt; &gt; behaviors; I do not
</em><br>
<em>&gt; &gt; believe that Novamente as it stands can support Ben's stated goals of
</em><br>
<em>&gt; &gt; general intelligence, seed AI, or even the existence of substantial
</em><br>
<em>&gt; &gt; intelligence on higher levels of organization.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You are right: Novamente is architecturally relatively simple and is built
</em><br>
<em>&gt; around a relative handful of generic behaviors.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It is not all THAT simple of course: it will definitely be 100,000-200,000
</em><br>
<em>&gt; lines of C++ code when finished, and it involves around 20 different mental
</em><br>
<em>&gt; dynamics.  But it is a lot simpler than Eliezer would like.  And I think its
</em><br>
<em>&gt; *relative* simplicity is a good thing.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I suspect that an AI system with 200 more specialized mental dynamics,
</em><br>
<em>&gt; rather than 20 generic ones, would be effectively impossible for a team of
</em><br>
<em>&gt; humans to program, debug and test.  So: Eliezer, I think that IF you're
</em><br>
<em>&gt; right about the level of complexity needed (which I doubt), THEN Kurzweil is
</em><br>
<em>&gt; also right that the only viable approach to real AI is to emulate human
</em><br>
<em>&gt; brain-biology in silico.  Because I think that implementing a system 10
</em><br>
<em>&gt; times more complex than Novamente via software engineering rather than
</em><br>
<em>&gt; brain-emulation is not going to be feasible.
</em><br>
<p>One of the attitudes that seems very obvious to me is that you should
<br>
estimate the size of the problem without being influenced by what resources
<br>
you think will be available to solve it.  Why?  Because, logically, the size
<br>
of the problem is totally independent of what kind of resources are easy to
<br>
obtain.  *First* you estimate the size of the problem, *then* you figure out
<br>
what you can do about it using available resources.  There is no a priori
<br>
guarantee that the requirements are reasonable.  In my experience most of
<br>
them are unreasonable.  This is one of the differences in attitude that
<br>
keeps throwing me off my stride when I encounter it in your arguments.  You
<br>
say that there's no billionaire currently funding AI, where my own attitude,
<br>
as a Singularity strategist, is to *first* ask whether a billionaire is
<br>
necessary.  If so, you don't throw yourself at the problem and go splat; you
<br>
recurse on the problem of finding a billionaire or building an organization
<br>
large enough to get the necessary level of funding.  You meet the problem on
<br>
its own terms and continue facing the unreasonably high standards set by the
<br>
problem until you come up with unreasonably good solutions, *if* you ever
<br>
do.
<br>
<p>If it seems like complexity problems in software engineering may turn out to
<br>
represent a severe limit, or even the critical limit, then you face that
<br>
problem squarely (for example by forking off Flare), instead of shrugging
<br>
your shoulders and saying &quot;Well, I'll assume the problem isn't that hard
<br>
because otherwise I'm helpless.&quot;  If you consider your statement
<br>
&quot;Implementing a system 10 times more complex than Novamente will not be
<br>
feasible&quot; as a possible *death sentence* for the human species, and spend a
<br>
week or so thinking that it not only represents a *possible* death sentence
<br>
but actually *does* represent a death sentence for you and your family and
<br>
your entire species, I'm sure you'd start having creative ideas about how to
<br>
manage complexity - even if the most creative idea you could come up with
<br>
was trying to push a massive industry effort to develop new ways of managing
<br>
software complexity, in the hopes that someone else would solve the problem.
<br>
<p>There is no promise that AI will be easy.  I only know that it is
<br>
necessary.  That said, if I didn't think our current approach would work, I
<br>
would be doing something else.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3640.html">Dani Eder: "C compiler that reconfigures hardware"</a>
<li><strong>Previous message:</strong> <a href="3638.html">Eliezer S. Yudkowsky: "Re: An AI Parable"</a>
<li><strong>In reply to:</strong> <a href="3581.html">Ben Goertzel: "RE: Review of Novamente"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3643.html">Ben Goertzel: "RE: Review of Novamente"</a>
<li><strong>Reply:</strong> <a href="3643.html">Ben Goertzel: "RE: Review of Novamente"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3639">[ date ]</a>
<a href="index.html#3639">[ thread ]</a>
<a href="subject.html#3639">[ subject ]</a>
<a href="author.html#3639">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
