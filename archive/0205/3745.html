<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Complexity of AGI</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Complexity of AGI">
<meta name="Date" content="2002-05-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Complexity of AGI</h1>
<!-- received="Sun May 19 20:03:55 2002" -->
<!-- isoreceived="20020520020355" -->
<!-- sent="Sun, 19 May 2002 20:02:21 -0400" -->
<!-- isosent="20020520000221" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Complexity of AGI" -->
<!-- id="3CE83D0D.E484D55F@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJCEEPCIAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Complexity%20of%20AGI"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun May 19 2002 - 18:02:21 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3746.html">Eliezer S. Yudkowsky: "Re: singularity arrival estimate..."</a>
<li><strong>Previous message:</strong> <a href="3744.html">Ben Goertzel: "RE: AI in &lt;what?&gt;"</a>
<li><strong>In reply to:</strong> <a href="3735.html">Ben Goertzel: "RE: Complexity of AGI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3753.html">Ben Goertzel: "RE: Complexity of AGI"</a>
<li><strong>Reply:</strong> <a href="3753.html">Ben Goertzel: "RE: Complexity of AGI"</a>
<li><strong>Reply:</strong> <a href="3786.html">James Rogers: "Re: Complexity of AGI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3745">[ date ]</a>
<a href="index.html#3745">[ thread ]</a>
<a href="subject.html#3745">[ subject ]</a>
<a href="author.html#3745">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I don't *know* how the brain works, nobody does, of course.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; My guess is that a simulation at the cellular level, with some attention to
</em><br>
<em>&gt; extracellular diffusion of charge and neurotransmitter chemistry, can do the
</em><br>
<em>&gt; trick.
</em><br>
<p>Well, yeah, cuz you think that all the higher levels of organization emerge
<br>
automatically from the low-level behaviors.  The usual, in other words.  And
<br>
yet, when I try to figure out how to make a higher-level quality emerge, my
<br>
mental experience is that there are an unbounded number of wrong ways to do
<br>
it, and extreme mental precision (and pessimism) is needed in order to
<br>
figure out how to do it.  Hence my skepticism of your claim that all higher
<br>
levels of organization emerge automatically.  You seem to be skipping over
<br>
all the issues that I think constitute the real, critical, hard parts of the
<br>
problem.
<br>
<p><em>&gt; &gt; Also, I happen to feel that incorrect AI
</em><br>
<em>&gt; &gt; designs contribute nontrivially to the amount of work that gets dumped on
</em><br>
<em>&gt; &gt; parameter-tuning, engineering, and performance analysis.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Why do you think a correct AI design would not require substantial
</em><br>
<em>&gt; parameter-tuning, performance analysis and engineering?
</em><br>
<em>&gt;
</em><br>
<em>&gt; The human brain clearly has required very substantial &quot;parameter-tuning&quot;
</em><br>
<em>&gt; over an evolutionary time-scale, and it has also been &quot;performance tuned&quot; by
</em><br>
<em>&gt; evolution in many ways.
</em><br>
<p>An AI is not a human.  I think that an AI design would start out by working
<br>
very inefficiently with a small amount of tuning.  After that would come the
<br>
task of getting the AI to undertake more and more complex &quot;tuning&quot;, a task
<br>
which is one of the earliest forms of seed AI.
<br>
<p><em>&gt; In all other software systems that I know of, &quot;complexity of interaction of
</em><br>
<em>&gt; different algorithms&quot; is a good indicator of the amount of parameter-tuning,
</em><br>
<em>&gt; subtle engineering design, and complicated performance analysis that has to
</em><br>
<em>&gt; be done.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So, why do you think a &quot;correct&quot; AGI design would avoid these issues that
</em><br>
<em>&gt; can be seen in the brain and in all other complex software systems?
</em><br>
<p>Because a correct seed AI design is designed to create and store
<br>
complexity.  It has other places to store complexity aside from a global
<br>
surface of 1500 parameters.  These complexity stores are tractable and
<br>
manageable because the designer spent time thinking about how to make them
<br>
tractable and manageable for the earliest stages of the AI.  That is part of
<br>
the hard design problem of seed AI.  And before you run away screaming, it
<br>
is *not* humanly unsolvable, it is a *very hard* problem that lies at the
<br>
inescapable center of seed AI.
<br>
<p><em>&gt; &gt;From what I know of DGI, I think it's going to require an incredible amount
</em><br>
<em>&gt; of subtle performance analysis and engineering and parameter tuning to get
</em><br>
<em>&gt; it to work at all.  Even after you produce a detailed design, I mean.  If
</em><br>
<em>&gt; you can produce a detailed design based on your DGI philosophy, that will be
</em><br>
<em>&gt; great.  If you can produce such a design that
</em><br>
<em>&gt; 
</em><br>
<em>&gt; -- can be made efficient without a huge amount of engineering effort and
</em><br>
<em>&gt; performance analysis, and
</em><br>
<p>Efficient?  The two most critical performance issues are:
<br>
<p>1)  Making the pieces of the system fit together, at all;
<br>
<p>2)  Making tuning of the system tractable and manageable for the pieces of
<br>
the system working together.
<br>
<p>Working to make the system more efficient is really not the point.  Working
<br>
to make the system more tunable by itself or more capable of tuning itself
<br>
is the point.  This means tuning the system has a tractable fitness
<br>
landscape or poses problems that are understandable for the AI's
<br>
intelligence.
<br>
<p>I understand that Novamente is a commercial project and hence may put in a
<br>
lot of human effort toward achieving a given level of performance at a given
<br>
time, but I really don't think that it's possible to understand seed AI by
<br>
improving the system yourself and tuning the system using genetic
<br>
algorithms.  I think that making &quot;the kind of complexity that improves
<br>
performance efficiency, on the average&quot;, or one kind of complexity like
<br>
that, into an AI-solvable problem, is one of the critical thresholds.  It's
<br>
a critical threshold because a lot of simple parameter tweakings, or pieces
<br>
of complexity that are easy to create, improve performance on one problem
<br>
but suck up enough computing power to make them bad tradeoffs for
<br>
problems-in-general.  When you find parameters that can be tuned with simple
<br>
quantitative settings, that's great.  You just turn the AI loose on the
<br>
problem.  But since really improving performance on general problems often
<br>
requires more complexity than that, getting the AI to leap those gaps is one
<br>
of the critical thresholds of seed AI.
<br>
<p>Mostly, though, I feel that a correct AI design may work *better* if you set
<br>
the parameters for &quot;forgetting&quot; things to exactly the right value, but it
<br>
will *still work* even if the parameters are set to different values. 
<br>
Moreover I feel that seed AI designs will tend to accumulate complexity for
<br>
describing when to forget things, rather than setting a global parameter.  I
<br>
think that the very-high-level behaviors for inference will work OK even if
<br>
the mathematical behaviors they use are just rough approximations to the
<br>
optimal values.  I think that Novamente's sensitivity to the exact equations
<br>
it uses for inference are symptomatic of an AI pathology for inference that
<br>
results from insufficient functional complexity for inference.  A real AI
<br>
would be able to use rough approximations to Novamente's exact equations and
<br>
still work just fine.
<br>
<p><em>&gt; -- has a small number of free parameters, the values of other parameters
</em><br>
<em>&gt; being give by theory
</em><br>
<p>You can have a huge number of free parameters as long as it is a manageable
<br>
problem to find an initial parameter set that will let the pieces of the
<br>
system fit together.  Not &quot;work optimally or efficiently&quot;.  Just &quot;fit
<br>
together&quot;.  Just because you can get benefits out of tuning parameters does
<br>
not mean that you must spend all day tuning parameters.  Rather what you
<br>
want to do is build a system that can work even if it has poorly tuned
<br>
parameters.
<br>
<p>And where you say &quot;Well, that's not possible&quot; or &quot;It's easier to model the
<br>
human brain neuron by neuron,&quot; I say &quot;This is one of the impossible-seeming,
<br>
critical, unavoidable hard problems of designing a general intelligence.&quot;
<br>
<p><em>&gt; THEN, my friend, you will have performed what I would term &quot;One Fucking Hell
</em><br>
<em>&gt; of a Miracle.&quot;  I don't believe it's possible, although I do consider it
</em><br>
<em>&gt; possible you can make a decent AI design based on your DGI theory.
</em><br>
<p>Yes, an AI design requires One Fucking Hell of a Miracle.  But it's a
<br>
*different* Miracle than the kind you describe.  It's not a question of
<br>
solving enormous engineering problems through Incredible Dauntless Efforts
<br>
but of creating designs that are So Impossibly Clever they don't run into
<br>
those enormous engineering problems.  I think many of the problems you're
<br>
running into are symptoms of trying to solve the problem too simply.
<br>
<p><em>&gt; In fact, I think that the engineering and performance analysis problems are
</em><br>
<em>&gt; likely to be significantly GREATER for a DGI based AI design than for
</em><br>
<em>&gt; Novamente, because, DGI makes fewer compromises to match itself up to the
</em><br>
<em>&gt; ways and means of contemporary computer hardware &amp; software frameworks.
</em><br>
<p>Engineering problems will be greater for DGI because DGI contains more
<br>
complexity, but I consider it a critical, unavoidable problem that the
<br>
overall system design should enable *adequate solutions* to work.  Usually,
<br>
when you add pieces to a system, it becomes easier to break.  This is
<br>
because people aren't thinking in terms of creating smooth fitness
<br>
landscapes, oversolving problems, and creating multiple routes to
<br>
solutions.  If you oversimplify a mind design, you concentrate too much
<br>
functionality into individual pieces of the system.  There aren't multiple
<br>
routes around things and each individual piece has to do a lot of work.
<br>
<p>This is a very hard picture to convey, and I suspect I'm not doing too well
<br>
at it... another way of looking at it is that, from my perspective, you're
<br>
making generic algorithms do things that I think should be broken up into
<br>
interdependent internally specialized subsystems.  So I think that one of
<br>
the consequences of this is that the generic algorithm finds *one path* from
<br>
A to D because it has to search through the whole problem space in one lump,
<br>
where the interdependent, internally specialized subsystems would be each
<br>
capable of finding *many paths* from A to B, B to C, and C to D.  This
<br>
picture also gives a rough image of why the generic algorithm tends to be
<br>
more sensitive to parameter tuning, tends to require more performance
<br>
engineering, and so on.
<br>
<p>I think one of the reasons you're focused on parameter tuning and
<br>
performance engineering of Novamente is that Novamente is *just barely*
<br>
capable of solving a certain class of engineering problems, because
<br>
Novamente is too simple a design.  I think that an improved design would
<br>
just swallow this whole class of problems whole and hence not require an
<br>
enormous amount of parameter tuning and performance engineering to do it. 
<br>
Of course, there will then be a new fringe of problems, which you swallow
<br>
not by tuning parameters but by improving the system design so that these
<br>
problems are also &quot;oversolved&quot;, swallowed whole.  But since you believe the
<br>
current Novamente design is already adequate for general intelligence, and
<br>
since the design itself has a flat architecture, that kind of space for
<br>
design improvement is not really open to you.  Which is why you focus on
<br>
parameter tuning and performance engineering.  That's how I see it, anyway.
<br>
<p>This business of very fragile solutions is a symptom of gnawing at the
<br>
fringes of the problem space, which in turn is a symptom of (a)
<br>
oversimplifying and (b) not being able to bite off big chunks of problem
<br>
space.  Don't solve the problem.  Oversolve it.  Have multiple paths to
<br>
success in the local subtask, for each of the levels of organization that
<br>
makes up the superproblem (you will need multiple levels of organization for
<br>
this to work, of course).
<br>
<p>This kind of design is totally foreign to software engineering as it stands
<br>
today, which typically is interested in *just one* solution to a problem. 
<br>
If you iterate *just one* solution over and over, it creates systems that
<br>
become very fragile as they become large.  If you iterate *many possible
<br>
paths to success* over and over - which really is one of those things that
<br>
you can do in an AI design but not a bank's transaction system - then you
<br>
don't get the AI pathology of this incredible fragility.
<br>
<p><em>&gt; &gt; Imagine Lenat saying, &quot;Well,
</em><br>
<em>&gt; &gt; suppose that you
</em><br>
<em>&gt; &gt; need to enter a trillion facts into the system... in this case it
</em><br>
<em>&gt; &gt; would make
</em><br>
<em>&gt; &gt; sense to scan an existing human brain because no programming team could
</em><br>
<em>&gt; &gt; handle the engineering challenge of managing relationships among a dataset
</em><br>
<em>&gt; &gt; that large.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But this is the worst example you could have possibly come up with!  Cyc is
</em><br>
<em>&gt; very easy to engineer precisely because it makes so many simplifying
</em><br>
<em>&gt; assumptions.
</em><br>
<p>This is just how I feel about Novamente.
<br>
<p><em>&gt; In almost all cases, I believe, incorrect AI theories have led to overly
</em><br>
<em>&gt; SIMPLE implementation designs,
</em><br>
<p>I quite agree.
<br>
<p><em>&gt; not overly complex ones.  AI scientists have
</em><br>
<em>&gt; VERY often, it seems to me, simplified their theories so they would have
</em><br>
<em>&gt; theories that could be implemented without excessive implementation effort
</em><br>
<em>&gt; and excessive parameter tuning.
</em><br>
<p>Noooo... AI scientists have often oversimplified their theories because (a)
<br>
they made philosophical connections between observed human behaviors and
<br>
simple computational properties based on surface similarities and
<br>
enthusiasm; (b) because they didn't have the knowledge, skill, or
<br>
pessimistic attitude to perceive really complex systems, and hence could not
<br>
&quot;move&quot; in the direction of greater complexity when figuring out which system
<br>
to design.
<br>
<p><em>&gt; I'm afraid you are fooling yourself when you say that parameter tuning will
</em><br>
<em>&gt; not be a big issue for your AI system.
</em><br>
<p>I don't mind parameter tuning contributing enormously to the AI's
<br>
performance, as long as the AI can tune its own parameters or get by with a
<br>
fairly simple set of starter parameters.  Incidentally, I think that many
<br>
things which you consider as global parameters should be local,
<br>
context-sensitive parameters.  I also think that many things which you think
<br>
are quantitative parameters are patterned or holonic variables.
<br>
<p><em>&gt; Even relatively simple AI models like attractor neural nets require a lot of
</em><br>
<em>&gt; parameter tuning.  Dave Goldberg has spent years working on parameter tuning
</em><br>
<em>&gt; for the GA.  Of course, you can claim that this is because these are all bad
</em><br>
<em>&gt; techniques and you have a good one up your sleeve.  But I find it hard to
</em><br>
<em>&gt; believe you're going to come up with the first-ever complex computational
</em><br>
<em>&gt; system for which parameter-tuning is not a significant problem.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes, a sufficiently advanced system can tune its own parameters, and
</em><br>
<em>&gt; Novamente does this in many cases; but intelligent adaptive self-tuning for
</em><br>
<em>&gt; a very complex system presents an obvious bootstrapping problem, which is
</em><br>
<em>&gt; trickier the more complex the system is.
</em><br>
<p>This is an inescapable problem of seed AI, and one of the ways it becomes
<br>
more tractable is by, for example, localizing parameters.  It's very easy to
<br>
see how a problem that is locally tractable could become globally
<br>
intractable if all the local parameters are yanked together. 
<br>
Oversimplifying a fitness landscape can render it intractable.  Making
<br>
complex parameters into simple parameters can make them intractable.  And so
<br>
on.  I think that the problems you are now experiencing are AI pathologies
<br>
of parameters that are too global and too simple.  I think the real versions
<br>
of many of these parameters will be learned complexity.
<br>
<p><em>&gt; &gt; DGI does not contain *more specialized
</em><br>
<em>&gt; &gt; versions* of these subsystems that support specific cognitive
</em><br>
<em>&gt; &gt; talents, which
</em><br>
<em>&gt; &gt; is what you seem to be visualizing, but rather contains a *completely
</em><br>
<em>&gt; &gt; different* set of underlying subsystems whose cardinality happens to be
</em><br>
<em>&gt; &gt; larger than the cardinality of the set of Novamente subsystems.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Can you give us a hint of what these underlying subsystems are?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Are they the structures described in the DGI philosophy paper that you
</em><br>
<em>&gt; posted to this list, or something quite different?
</em><br>
<p>Memory.  Concept kernel formation.  I would say the things from DGI, but I
<br>
would add the proviso that I don't think you understood which subsystems DGI
<br>
was asking for.
<br>
<p><em>&gt; &gt; I believe this problem is an AI pathology of the Novamente architecture.
</em><br>
<em>&gt; &gt; (This is not a recent thought; I've had this impression ever
</em><br>
<em>&gt; &gt; since I visited
</em><br>
<em>&gt; &gt; Webmind Inc. and saw some poor guy trying to optimize 1500
</em><br>
<em>&gt; &gt; parameters with a
</em><br>
<em>&gt; &gt; GA.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Webmind had about 300 parameters, if someone told you 1500 they were goofing
</em><br>
<em>&gt; around.
</em><br>
<em>&gt;
</em><br>
<em>&gt; However, only about 25 of them were ever actively tuned, the others were set
</em><br>
<em>&gt; at fixed values.
</em><br>
<p>Well, that's the word I remember hearing.  It might have been a special case
<br>
or an exaggeration.  I do remember seeing a HECK OF A LOT of parameters
<br>
scrolling down the guy's screen - more than 25, I'm sure.  Although I also
<br>
think I remember seeing a lot of similarity between adjacent lines of text,
<br>
so he might have been breaking down parameters into subparameters or
<br>
something; I dunno.
<br>
<p><em>&gt; I sure am eager to see how DGI or *any* AGI system is going to avoid this
</em><br>
<em>&gt; sort of problem.
</em><br>
<p>Deep architectures, experiential learning of local patterned variables
<br>
instead of optimization of global quantitative variables, multiple solution
<br>
pathways on multiple levels of organization, carving the system at the
<br>
correct joints.
<br>
<p><em>&gt; &gt; &gt; we'd be better off to focus on brain scanning and
</em><br>
<em>&gt; &gt; &gt; cellular brain simulation.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; That doesn't help.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Your extreme confidence in this regard, as in other matters, seems
</em><br>
<em>&gt; relatively unfounded.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Many people with expertise in brain scanning and biological systems
</em><br>
<em>&gt; simulation disagree with you.
</em><br>
<p>*Cough*.  Let me rephrase.  Brain scanning and cellular brain simulation are
<br>
best understood as helping us to understand what goes on in the brain, *not*
<br>
in letting us create intelligence without knowing what it is.  I am
<br>
extremely skeptical that you can duplicate intelligence without knowledge
<br>
unless you have incredible scanning abilities and incredibly fine
<br>
simulations, because of the extreme complexity of single neurons and the
<br>
irreduceability of this complexity without an understanding of higher levels
<br>
of organization.
<br>
<p><em>&gt; &gt; Novamente has what I would consider a flat architecture, like
</em><br>
<em>&gt; &gt; &quot;Coding a Transhuman AI&quot; circa 1998.  Flat architectures come with certain
</em><br>
<em>&gt; &gt; explosive combinatorial problems that can only be solved with deep
</em><br>
<em>&gt; &gt; architectures.  Deep architectures are admittedly much harder to
</em><br>
<em>&gt; &gt; think about and invent.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &quot;Deep architecture&quot; is a cosmic-sounding term; would you care to venture a
</em><br>
<em>&gt; definition?  I don't really know what you mean, except that you're implying
</em><br>
<em>&gt; that your ideas are deep and mine are shallow.
</em><br>
<p>Hopefully, what I said above fleshes out the definition a bit.
<br>
<p><em>&gt; My own subjective view, not surprisingly, is that YOUR approach is
</em><br>
<em>&gt; &quot;shallower&quot; than mind, in that it does not seem to embrace the depth of
</em><br>
<em>&gt; dynamical complexity and emergence that exists in the mind.  You want to
</em><br>
<em>&gt; ground concepts too thoroughly in images and percepts rather than accepting
</em><br>
<em>&gt; the self-organizing, self-generating dynamics of the pool of intercreating
</em><br>
<em>&gt; concepts that is the crux of the mind.  I think that Novamente accepts this
</em><br>
<em>&gt; essential depth of the mind whereas DGI does not, because in DGI the concept
</em><br>
<em>&gt; layer is a kind of thin shell sitting on top of perception and action,
</em><br>
<em>&gt; relying on imagery for most of its substance.
</em><br>
<p>I don't think you're seeing the complexity.  (As you have accused me of with
<br>
respect to Novamente, of course.)  The processes do not &quot;rely on imagery for
<br>
most of their substance&quot; but rather require imagery as a level of
<br>
organization, and transformation into imagery and back into concept
<br>
substance, in order to engage in concept-level dynamics during experiential
<br>
learning.  It's not *relying on* imagery for all of its substance but rather
<br>
doing things for which imagery is a *necessary but not sufficient*
<br>
condition.  From my perspective, you're trying to use simple generic
<br>
processes to do things that require the interaction of interdependent
<br>
internally specialized processes.  Imagery is one of the things that concept
<br>
learning depends on.  Also, I'm still not sure that your imagery for
<br>
&quot;imagery&quot; is anything like my imagery for &quot;imagery&quot;.
<br>
<p><em>&gt; The depth of the Novamente design lies in the dynamics that I believe (based
</em><br>
<em>&gt; on intuition not proof!) will emerge from the system, not in the code
</em><br>
<em>&gt; itself.  Just as I believe the depth of the human brain lies in the dynamics
</em><br>
<em>&gt; that emerge from neural interactions, not in the neurons and
</em><br>
<em>&gt; neurotransmitters and glia and so forth.  Not even the exalted
</em><br>
<em>&gt; microtubules!!
</em><br>
<p>The part where we disagree is in the question of whether evolution carefully
<br>
and exactingly sculpted those higher levels of organization just as it
<br>
sculpted the neural interactions, or whether all higher levels of
<br>
organization emerge automatically as the laws of physics supposedly do (I
<br>
have my doubts).
<br>
<p>I also feel that if you intuit dynamics will emerge, they will not emerge. 
<br>
If you know what the dynamics are and how they work, you will be able to
<br>
create systems that support them; not otherwise.  I think that the history
<br>
of AI shows that one of the most frequent classes of error is hoping that a
<br>
quality emerges when you don't really know exactly how it works.
<br>
<p><em>&gt; &gt; It requires that you listen to your quiet, nagging
</em><br>
<em>&gt; &gt; doubts about
</em><br>
<em>&gt; &gt; shallow architectures and that you go on relentlessly replacing
</em><br>
<em>&gt; &gt; every single
</em><br>
<em>&gt; &gt; shallow architecture your programmer's mind invents, until you
</em><br>
<em>&gt; &gt; finally start
</em><br>
<em>&gt; &gt; to see how deep architectures work.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Ah, how my colleagues would laugh to see you describe me as having a
</em><br>
<em>&gt; &quot;programmer's mind&quot; !!!
</em><br>
<em>&gt;
</em><br>
<em>&gt; For sure, I am at bottom a philosopher, much as you are I suspect.
</em><br>
<p>No, I am not.  I have fundamental problems with philosophy as an approach to
<br>
the mind, many of them the same fundamental problems that I have with
<br>
programming as an approach to the mind.
<br>
<p><em>&gt; You may
</em><br>
<em>&gt; disagree with my philosophy but the fact remains that I spent about 8 years
</em><br>
<p>My clock is currently at 6 years and running.
<br>
<p><em>&gt; working on mathematically and scientifically inspired philosophy (while also
</em><br>
<em>&gt; doing various scientific projects), before venturing to design an AGI.
</em><br>
<em>&gt; Novamente is not at all a programming-driven AI project, although at this
</em><br>
<em>&gt; stage we are certainly using all the algorithmic and programming tricks we
</em><br>
<em>&gt; can find, in the service of the design.  The design was inspired by a
</em><br>
<em>&gt; philosophy of mind, and is an attempt to realize this philosophy of mind in
</em><br>
<em>&gt; a practical way using contemporary hardware and software.
</em><br>
<p>Yes, I know.  I think that trying to implement philosophies is a recipe for
<br>
disaster because it involves saying &quot;Effect A arises from B - isn't that
<br>
beautiful?&quot; instead of &quot;X, Y, and Z are necessary and sufficient causes of
<br>
A; here's a functional decomposition and a walkthrough.&quot;  You think that all
<br>
the higher levels of your system will arise automatically because of
<br>
philosophical thinking.
<br>
<p><em>&gt; You seem to have misinterpreted me.  I am not talking about anything being
</em><br>
<em>&gt; in principle beyond human capability to comprehend forever.  Some things ARE
</em><br>
<em>&gt; (this is guaranteed by the finite brain size of the human species), but
</em><br>
<em>&gt; that's not the point I'm making.
</em><br>
<p>OK.  We have different ideas about what a modern-day AI researcher should be
<br>
trying to comprehend.  Does that terminology meet with your approval?
<br>
<p><em>&gt; I still believe it's possible that the AGI design problem is SO hard that
</em><br>
<em>&gt; detailed brain simulation is easier.  I hope this isn't true, but if pressed
</em><br>
<em>&gt; I'd give it a 10%-20% chance of being true.  Generally, I am not prone to
</em><br>
<em>&gt; the near 100% confident judgments that you are, Eliezer.  I think I tend to
</em><br>
<em>&gt; be more aware of the limitations of my own knowledge and cognitive ability,
</em><br>
<em>&gt; than you are of your own corresponding limitations.
</em><br>
<p>Ben, I don't know for sure that I'm right, but I'm pretty sure that you're
<br>
wrong.  I do wish this distinction was easier to convey to people, since it
<br>
seems to be a pretty common confusion.
<br>
<p><em>&gt; Eliezer, I think it is rather funny for *you* to accuse *me* of flinching
</em><br>
<em>&gt; away from the prospect of trying to do something!
</em><br>
<p>What on Earth are you talking about here?  Where did you get the idea that I
<br>
am deliberately holding back on anything?  I'd be putting together a
<br>
programming team right now if SIAI had the funding.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3746.html">Eliezer S. Yudkowsky: "Re: singularity arrival estimate..."</a>
<li><strong>Previous message:</strong> <a href="3744.html">Ben Goertzel: "RE: AI in &lt;what?&gt;"</a>
<li><strong>In reply to:</strong> <a href="3735.html">Ben Goertzel: "RE: Complexity of AGI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3753.html">Ben Goertzel: "RE: Complexity of AGI"</a>
<li><strong>Reply:</strong> <a href="3753.html">Ben Goertzel: "RE: Complexity of AGI"</a>
<li><strong>Reply:</strong> <a href="3786.html">James Rogers: "Re: Complexity of AGI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3745">[ date ]</a>
<a href="index.html#3745">[ thread ]</a>
<a href="subject.html#3745">[ subject ]</a>
<a href="author.html#3745">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
