<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Review of Novamente</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Review of Novamente">
<meta name="Date" content="2002-05-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Review of Novamente</h1>
<!-- received="Wed May 08 12:55:56 2002" -->
<!-- isoreceived="20020508185556" -->
<!-- sent="Wed, 8 May 2002 10:24:04 -0600" -->
<!-- isosent="20020508162404" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Review of Novamente" -->
<!-- id="LAEGJLOGJIOELPNIOOAJKEKMCHAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3CD8E7BE.4CCC5E87@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Review%20of%20Novamente"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Wed May 08 2002 - 10:24:04 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3644.html">Eugen Leitl: "Re: A New Kind Of Science"</a>
<li><strong>Previous message:</strong> <a href="3642.html">Jordan Dimov: "A New Kind Of Science"</a>
<li><strong>In reply to:</strong> <a href="3639.html">Eliezer S. Yudkowsky: "Re: Review of Novamente"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3649.html">Eliezer S. Yudkowsky: "Re: Review of Novamente"</a>
<li><strong>Reply:</strong> <a href="3649.html">Eliezer S. Yudkowsky: "Re: Review of Novamente"</a>
<li><strong>Reply:</strong> <a href="3752.html">Eliezer S. Yudkowsky: "Re: Review of Novamente"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3643">[ date ]</a>
<a href="index.html#3643">[ thread ]</a>
<a href="subject.html#3643">[ subject ]</a>
<a href="author.html#3643">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi eliezer,
<br>
<p><em>&gt; A note:  Ben and I have discussed Novamente privately before now, and I'm
</em><br>
<em>&gt; not sure I want to recap the whole discussion from SL4.  However, a couple
</em><br>
<em>&gt; of people have come forward and said that they want to see more discussion
</em><br>
<em>&gt; between me and Ben about Novamente.  A problem is that, unlike when I'm
</em><br>
<em>&gt; talking with Ben, I can't quote from the Novamente manuscript and I can't
</em><br>
<em>&gt; assume anyone else has read it.  However, I'll give it a shot.
</em><br>
<p>I'm a bit strapped for time; I'm going out of town (again) next week and
<br>
have a lot of work to finish up...
<br>
<p>So I'll type a long response this time but I may not be able to do 5
<br>
consecutive ones!!
<br>
<p><em>&gt; Well, I can only criticize what's *in* the book.  If in the whole book
</em><br>
<em>&gt; there's no mention of emergent maps and then you say that you
</em><br>
<em>&gt; expect most of
</em><br>
<em>&gt; Novamente's functionality to come from emergent maps, then
</em><br>
<em>&gt; there's not much
</em><br>
<em>&gt; I can say about;
</em><br>
<p>Well I did send you an extra chapter on emergent maps, drawn from previously
<br>
written material that hadn't made its way into the book.
<br>
<p>I have found that I'm much worse at explaining my own ideas than at
<br>
explaining other peoples' in some cases!
<br>
<p>In this case, after working on Webmind and Novamente with the same team for
<br>
many years, I think I sort of lost touch with what's obvious and implicit to
<br>
non-team-members, versus what's obvious and implicit to team members.
<br>
<p>See, everyone on the team is so fucking used to my philosophy of mind, it's
<br>
100% implicitly understood by all of them when they read the manuscript that
<br>
the purpose of all the mechanism is to give rise to mentally meaningful
<br>
emergent
<br>
structures &amp; dynamics, and that the mindstuff is the emergent maps and not
<br>
the individual nodes &amp; links.
<br>
<p>And the main, immediate purpose of the book draft was actually to
<br>
communicate to the team the *precise variations* on our commonly-known
<br>
collection of ideas, that I'd decided to go with.
<br>
<p>Unfortunately, the book draft as written is effective *only* for this
<br>
purpose, i.e. only for communicating to the existing team who already fully
<br>
understand the context within which the details are proposed.
<br>
<p>Knowing it was a crude draft, I only showed it to a handful of people
<br>
outside the time.
<br>
<p>And, some of the others who read it -- who were more a priori sympathetic
<br>
than you to my overall philosophy of mind -- seemed to be more willing to
<br>
&quot;fill in the gaps&quot; themselves and have had a more positive assessment of the
<br>
design than yourself.
<br>
<p>On the other hand, of the readers said they found it totally
<br>
incomprehensible and to consist almost entirely of nonsense ;-)
<br>
<p>Other comments included &quot;a stroke of genius&quot; and &quot;atrociously written&quot; ;-&gt;
<br>
<p>Anyway, we are finding it is a hell of a lot of work to create a systematic
<br>
exposition of these ideas in a way that will be comprehensible to outsiders.
<br>
This is definitely a worthwhile kind of work, because there are loads of
<br>
people in the world who can give us valuable feedback on our ideas.
<br>
(Although the most valuable feedback will come from people whose
<br>
a priori outlook is a little closer to ours than yours is.)
<br>
<p>On the other hand, we're spending most of our time building and applying the
<br>
system rather than writing about it, so it's gonna be at least another 6
<br>
months or so before we have a book draft we're willing to distribute more
<br>
widely.
<br>
<p><em>&gt; When I hadn't
</em><br>
<em>&gt; read the Novamente manuscript, I was basically willing to assume that, in
</em><br>
<em>&gt; the absence of more specific information, Novamente might have
</em><br>
<em>&gt; some process
</em><br>
<em>&gt; that implemented X.  But if I read the manuscript and there's no
</em><br>
<em>&gt; mention of
</em><br>
<em>&gt; X, then I'm going to assume X is not supported unless I hear a specific,
</em><br>
<em>&gt; critique-able explanation of X - not in terms of &quot;P.S: Novamente can do X&quot;
</em><br>
<em>&gt; but in terms of &quot;This is how these design features support X.&quot;
</em><br>
<em>&gt; Where we are
</em><br>
<em>&gt; at right now is that you've given me the manuscript, I read the
</em><br>
<em>&gt; manuscript,
</em><br>
<em>&gt; I said &quot;This isn't general intelligence&quot;, and you said &quot;Ah, but all the
</em><br>
<em>&gt; general intelligence features aren't in the manuscript.&quot;  Okay.
</em><br>
<em>&gt; It could be
</em><br>
<em>&gt; true.  But my working assumption is still going to be that the parts
</em><br>
<em>&gt; discussed in the manuscript describe the parts you actually know
</em><br>
<em>&gt; how to do,
</em><br>
<em>&gt; and all the interesting stuff that isn't in the manuscript are things you
</em><br>
<em>&gt; *want* to do but have not actually worked out in the level of
</em><br>
<em>&gt; design detail
</em><br>
<em>&gt; that would be required to discuss them in the manuscript.
</em><br>
<p>There seems to be a fundamental philosophical point here...
<br>
<p>I think most of the X's that you're referring to are things that, according
<br>
to our theory of mind, are supposed to be *emergent* phenomena rather than
<br>
parts of the codebase
<br>
<p>The book gives a mathematical formalization of the stuff that's supposed to
<br>
be in the codebase
<br>
<p>It doesn't talk enough about our experiences getting emergent phenomena out
<br>
of preliminary versions of the system, partly because these experiences were
<br>
with Webmind not Novamente and the book is about Novamente, and partly
<br>
because we didn't do enough to *quantify* and *formalize* these experiences
<br>
&amp; observations.... So a lot of our knowledge about emergent behavior of the
<br>
system is really at the level of lore, not science.  Since it's Webmind
<br>
lore, not Novamente lore, to explain it in detail would require us to write
<br>
a book on webmind, which is a conceptually related but different-in-detail
<br>
system that we don't own anymore, and don't even have legal rights to write
<br>
a book about...
<br>
<p>Hence, our inclination is to keep building &amp; playing with Novamente, be more
<br>
careful to record, quantify and formalize all observed emergent phenomena
<br>
this time around, and then write about Novamente phenomena as we observe
<br>
them and as we have time.
<br>
<p>Anyway, it's not possible to work out emergent phenomena at the same level
<br>
of design detail as code-level stuff.  But it is possible to talk about it
<br>
in more detail than was done in the book version you read, and that will be
<br>
done in a later version.
<br>
<p>Anyway, we certainly knew that this version was not going to be solid enough
<br>
to convince a skeptic that we have a good design for AGI.  In fact, no
<br>
matter how well-written the book, it wouldn't pass this test.  The only way
<br>
to convince a skeptic that we have a good design for AGI would be
<br>
<p>a) create the working  AGI
<br>
<p>b) a distant second-place: create a thoroughly rigorous mathematical theory
<br>
of general intelligence, connected to pragmatic real-world intelligence
<br>
measurements, and prove that the design can lead to an AGI according to this
<br>
theory
<br>
[this still wouldn't convince the die-hard skeptics because debate would
<br>
just shift to the axioms of the mathematical theory]
<br>
<p><p>My conclusion is that perhaps Peter Voss, James Rogers and others working on
<br>
their own AGI designs have the right approach -- which is the same approach
<br>
I was taking during the Webmind era and until very recently with Novamente.
<br>
<p>I.e., perhaps the wiser attitude is: &quot;Keep your mouth shut except among your
<br>
own little group, because talking about your ideas with others is just going
<br>
to lead you to spend all your time in arguments and none of your time
<br>
getting any work done.  You'll never convince anyone you're on the right
<br>
track, because nearly everyone in the world believes AGI is impossible, and
<br>
nearly everyone who believes AGI is possible believes *they* have the secret
<br>
ingredient to AGI and therefore you cannot.&quot;
<br>
<p>The amount of work required to explain our intuitions about the system, and
<br>
our anecdotal experiences with earlier versions, and why we think the system
<br>
can give rise to the emergent structures and dynamics we think it can, is,
<br>
it's becoming clear to me, a LOT.  Is it worth doing this work instead of
<br>
working on the system itself, which may produce evidence that will be more
<br>
convincing than any words?  Maybe not.
<br>
<p><em>&gt; Naturally, if you're going to focus on having a design in hand as
</em><br>
<em>&gt; a critical
</em><br>
<em>&gt; point, then I may be excused for pointing out that you do not
</em><br>
<em>&gt; seem to have a
</em><br>
<em>&gt; design in hand for the most critical parts of your system.
</em><br>
<p>We have a design in hand for a system that WE BELIEVE will lead to an AGI.
<br>
We do not believe there are any critical parts of the system yet to be
<br>
designed.
<br>
<p>The parts of the system you think we have not designed, are phenomena we
<br>
believe will emerge from the system.
<br>
<p>The book apparently did a very poor job of explaining why we believe these
<br>
phenomena will emerge from the system.  Slowly, we will produce a better
<br>
book draft that explains this more thoroughly.  Slowly, because we will
<br>
continue to spend more time working on the system itself, than working on
<br>
describing our intuitions and experience in a way that will be convincing to
<br>
skeptical outsiders.
<br>
<p><p><em>&gt; &gt; To sum up before giving details, basically, Eliezer's critique is that
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; 1) he doesn't see how a collection of relatively simple,
</em><br>
<em>&gt; generic processes
</em><br>
<em>&gt; &gt; working together can give rise to a rich enough set of emergent
</em><br>
<em>&gt; dynamics and
</em><br>
<em>&gt; &gt; structures to support AGI
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't see how *your specific* collection of simple, generic processes,
</em><br>
<em>&gt; working in the fashion described in the Novamente manuscript, and
</em><br>
<em>&gt; interacting as I picture them (because their interaction is *not*
</em><br>
<em>&gt; described
</em><br>
<em>&gt; in any specific detail in the Novamente manuscript),
</em><br>
<p>Here the discussion becomes tough because all listeners have not read the
<br>
manuscript (and I don't want to distribute it widely).
<br>
<p>I feel these interactions were described in moderate detail in  many places,
<br>
which you failed to understand or chose to ignore.
<br>
<p><em>&gt; Certainly I feel that if one takes the Novamente
</em><br>
<em>&gt; design at face
</em><br>
<em>&gt; value then it is not an AGI.
</em><br>
<p>The design is not an AGI.
<br>
<p>The design is a design.
<br>
<p>I think we're just dancing around the same point over &amp; over again.
<br>
<p>I need to do a better job of explaining why we think the emergent structures
<br>
&amp; dynamics of mind will emerge from a system implemented according to the
<br>
Novamente design.
<br>
<p>But this is not something I can do in an e-mail, even a long one.
<br>
<p><em>&gt; What kind of emergent meaning?  How does it emerge and why?  Can
</em><br>
<em>&gt; you give a
</em><br>
<em>&gt; specific example of a case where emergent meaning in Novamente is expected
</em><br>
<em>&gt; to contribute to general intelligence, including the nature of
</em><br>
<em>&gt; the emergent
</em><br>
<em>&gt; meaning, the low-level support of the emergent meaning, and the specific
</em><br>
<em>&gt; contribution made to general intelligence?  If these things are not
</em><br>
<em>&gt; documented in your design then it is natural for me to assume
</em><br>
<em>&gt; that they are
</em><br>
<em>&gt; not driving the design.
</em><br>
<p>yeah, i will give plenty of examples like this in the rewrite of the
<br>
manuscript.  maybe I will post something on the topic to you or to this list
<br>
in a few weeks.  I don't have time right now.
<br>
<p>The example I would work out for you first would be the number &quot;two&quot; I
<br>
think...
<br>
<p><em>&gt; Your design description makes sense on its own;
</em><br>
<em>&gt; it's hard for me to believe that the entire motivation behind it is
</em><br>
<em>&gt; missing.  The emergent behaviors you expect from Novamente seem to me like
</em><br>
<em>&gt; hopes, rather than part of the design.
</em><br>
<p>The emergent behaviors are not PART of the design, they were however the
<br>
ENTIRE MOTIVATION for the design.  Which is 100% obvious to the team, and,
<br>
understandably, not obvious to outsiders.
<br>
<p>I spent 8 years thinking about emergent mind, and then 4 years designing a
<br>
system that I thought could give rise to it...  The team was participating
<br>
in these 4 years hence the contextual meaning of the parts of the design is
<br>
obvious to them.  Also would be obvious to you if the Novamente manuscript
<br>
were read in the context of my previous books.  But that is too much to ask
<br>
of a reader, I understand that.
<br>
<p><p><em>&gt; My overall prediction for Novamente is that all the behaviors you
</em><br>
<em>&gt; are hoping
</em><br>
<em>&gt; will &quot;emerge naturally&quot;, won't.
</em><br>
<p>Gee that's a big surprise!!
<br>
<p>And see, even after we revise the book and explain more clearly why we
<br>
believe the right structures &amp; dynamics will emerge, you'll still say the
<br>
same thing
<br>
<p>Of course, if I came to you with a design for neurons and synapses and an
<br>
architecture diagram for the brain, and described the emergent dynamics and
<br>
structures I thought would come out of the brain -- and you'd never heard of
<br>
a brain before -- you'd probably say the same thing.
<br>
<p><p><em>&gt; Really?  You don't enter nodes directly into Novamente, right now, at the
</em><br>
<em>&gt; current state of the system?  Or is this something that you hope
</em><br>
<em>&gt; to do later
</em><br>
<em>&gt; but haven't done yet?  How does the node get the name &quot;cat&quot;?  In current
</em><br>
<em>&gt; versions of Novamente, how much of the network is semantic and how much is
</em><br>
<em>&gt; not?
</em><br>
<p>Currently for the data analysis work we're doing, we actually have nodes for
<br>
numbers and patterns among numbers.
<br>
Nothing else ;)
<br>
<p>When we hook it up to a simple &quot;ShapeWorld&quot; visual UI, we will have nodes
<br>
for (pixel coordinates, color) pairs, for instance.
<br>
<p>When we hook it up to a conversation UI, we will have nodes for *characters*
<br>
(as described in the NLP chapter), with words being represented as perceived
<br>
lists of characters, etc.
<br>
<p>Conceptual nodes must be built up from basic perceptual nodes like: numbers,
<br>
characters, (pixel coord., color) pairs, ...
<br>
<p>If the system recognizes {s, q, u, a, r e} as a coherent whole (a ListLink
<br>
of CharacterNodes} then it knows the word &quot;square&quot; in some sense.  If it
<br>
then automatically links this ListLink with a PredicateNode P embodying a
<br>
certain set of relations between pixels (that we'd call a &quot;square&quot;), then it
<br>
is grounding P (the concept of square) in the word &quot;square&quot; ( the ListLink
<br>
of CharacterNodes)
<br>
<p>In this way it can (we hope) build up a semantic network from perceptions.
<br>
WE did some simple experimenting with this sort of stuff in Webmind and have
<br>
not gotten there yet with Novamente.
<br>
<p><em>&gt; &gt; The intention is that much of the semantics of the system resides, not
</em><br>
<em>&gt; &gt; directly in individual nodes and links, but rather in &quot;maps&quot; or
</em><br>
<em>&gt; &gt; &quot;attractors&quot; -- patterns of connectivity and interconnection
</em><br>
<em>&gt; involving large
</em><br>
<em>&gt; &gt; numbers of nodes and links.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is an *intention*.  I don't see it in the Novamente design.
</em><br>
<em>&gt; What kinds
</em><br>
<em>&gt; of maps?  What kinds of attractors?  What functions do they implement?
</em><br>
<p>This does not properly belong in the design.  You don't *design* attractors,
<br>
they arise.
<br>
<p>We can put more verbiage about this in the book but that will just document
<br>
our intuitions and anecdotal experiences
<br>
<p>The proof or disproof will be in the pudding of course..
<br>
<p><em>&gt; Okay.  It's true, as you say below, that I tend to lump together certain
</em><br>
<em>&gt; systems that you think have key distinctions; i.e., I do not believe these
</em><br>
<em>&gt; distinctions are progress toward building a real AI system, while you
</em><br>
<em>&gt; believe that they do.
</em><br>
<p>Well, in this case the distinctions at hand are standard ones in the CS
<br>
literature, not ones that I made up
<br>
(predicate logic vs. term logic, crisp logic vs. probabilistic logic)
<br>
<p>In many cases, it seems, I think that choosing the right tool from the
<br>
standard toolkit is very important, whereas you think the whole toolkit is
<br>
inadequate
<br>
<p><p><em>&gt; Yes.  I must say that this idea stands out in my mind as seeming *very*
</em><br>
<em>&gt; GOFAIish - the idea that mathematical reasoning can be implemented/taught
</em><br>
<em>&gt; using Novamente's probabilistic inference on a series of Novamente
</em><br>
<em>&gt; propositions corresponding directly to the formal steps of a Mizar proof.
</em><br>
<p>Well really, GOFAI is not commonly based on supervised learning from a huge
<br>
training database
<br>
<p>This is a much more recent meme in AI, which came along with the Net and
<br>
powerful computers
<br>
<p>For instance GOFAI computational lingustics is on the way out, in favor of
<br>
corpus linguistics
<br>
<p>I don't think that training-database-based-supervised-learning AI is the
<br>
true path either but I think it has more meat to it than GOFAI.  At least
<br>
one is dealing with a rich body of data and letting a system spontaneously
<br>
pick its own patterns from the data.
<br>
<p>The Mizar approach to theorem-proving is in the spirit of modern
<br>
supervised-learning-based-AI, not expert-system/logic-based GOFAI
<br>
<p>There is nothing like it in the computational theorem-proving literature
<br>
<p>However, I don't really think that supervised learning training based on
<br>
Mizar will be enough to teach Novamente (or any system) to prove nontrivial
<br>
theorems, I think that approach will only suffice for simple set theory
<br>
theorems and such.  I think a human teacher will be needed to help it learn
<br>
from this highly valuable database.  Anyway we're a long way from there at
<br>
present.
<br>
<p><p><em>&gt; I expect to be documenting which behaviors are supposed to be
</em><br>
<em>&gt; emerging from
</em><br>
<em>&gt; which other behaviors and which behaviors are supposed to be emergent from
</em><br>
<em>&gt; them, and I expect this to force internal specialization on
</em><br>
<em>&gt; multiple levels
</em><br>
<em>&gt; of organization.  If I see you trying to make all the complexity of
</em><br>
<em>&gt; cognition emerge from generic behaviors, then my suspicion is
</em><br>
<em>&gt; naturally that
</em><br>
<em>&gt; you haven't mentally connected the hoped-for emergent behaviors
</em><br>
<em>&gt; to the level
</em><br>
<em>&gt; of organization from which they are supposed to be emergent, and hence
</em><br>
<em>&gt; experience no mental pressure to design low-level behaviors with internal
</em><br>
<em>&gt; specialization that naturally fits the emergent behaviors.
</em><br>
<p>I think you want to design too much in detail.   I have more faith/respect
<br>
in the power of self-organizing learning than you, I think.
<br>
<p><p><em>&gt; When I
</em><br>
<em>&gt; think of noticing spontaneous similarities, I think of background
</em><br>
<em>&gt; processes
</em><br>
<em>&gt; that work against current imagery.  In Novamente the analogue of this part
</em><br>
<em>&gt; of the mind is Apriori datamining, or at least, if there's anything else
</em><br>
<em>&gt; that does this, it's not in the manuscript.
</em><br>
<p>Inference and evolutionary CRN mining do that.  They are described amply in
<br>
the mansucript but evidently their scope application is not sufficiently
<br>
clearly explained.
<br>
<p><p><em>&gt; &gt; The evolutionary programming in Novamente is not classical ev.
</em><br>
<em>&gt; programming;
</em><br>
<em>&gt; &gt; it has at least two huge innovations (only one of which has
</em><br>
<em>&gt; been tested so
</em><br>
<em>&gt; &gt; far): 1) evolution is hybridized with probabilistic inference, which can
</em><br>
<em>&gt; &gt; improve efficiency by a couple orders of magnitude,
</em><br>
<em>&gt;
</em><br>
<em>&gt; Can improve?  Already has improved?  You hope will improve?
</em><br>
<p>Already has been shown to improve in the GA case, by a factor of 50-100
<br>
<p>In the GP case, we didn't finish the experiments, but initial results were
<br>
promising
<br>
<p>For related work, see Pelikan and David Goldberg's work on the Bayesian
<br>
Optimization Algorithm (BOA)
<br>
<p><em>&gt; Okay, stepwise classical evolution on a slightly specialized
</em><br>
<em>&gt; representation
</em><br>
<em>&gt; that interacts with other stepwise generic processes.  I suppose that from
</em><br>
<em>&gt; your perspective it is indeed unfair to call this &quot;classical evolutionary
</em><br>
<em>&gt; programming&quot;.  BUT the Novamente manuscript does not give examples of how
</em><br>
<em>&gt; evolutionary programming is supposed to interact with logical
</em><br>
<em>&gt; inference; it
</em><br>
<em>&gt; just says that it is.
</em><br>
<p>the manuscript explains mathematically how the interaction happens, pretty
<br>
plainly (at least for those who know both processes well, which is a very
<br>
small set ;)
<br>
<p>but you are right it does not give particular examples, and it should
<br>
<p><em>&gt; Hm, I think we have different ideas of what it means to invoke &quot;Bayesian
</em><br>
<em>&gt; semantics&quot;.  Of course Bayesian semantics are much more popular in CompSci
</em><br>
<em>&gt; and so your usage is probably closer to the norm.  When I say Bayesian
</em><br>
<em>&gt; semantics I am simply contrasting them to, say, Tversky-and-Kahneman
</em><br>
<em>&gt; semantics; what I mean is semantics that obey the Bayesian behaviors for
</em><br>
<em>&gt; quantitative probabilities, not necessarily analogy with the philosophical
</em><br>
<em>&gt; bases or system designs of those previous AI systems that have been
</em><br>
<em>&gt; advertised as &quot;Bayesian&quot; approaches.
</em><br>
<p><p>Novamente's inference engine uses Bayesian semantics locally but not
<br>
globally
<br>
<p>Globally the system is not consistent with bayesian probability
<br>
calculations, but it is approximately consistent within a single inference
<br>
<p><p><em>&gt; &gt; And I think this is as it should be.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Why did it take so long to scale from spider brains to human brains?
</em><br>
<p>'Cause evolution is a terribly inefficient learning mechanism ;&gt;
<br>
<p><em>&gt; I would say, in fact, that my own reaction to the history of
</em><br>
<em>&gt; the AI field has been to become very strongly prejudiced against
</em><br>
<em>&gt; permitting
</em><br>
<em>&gt; oneself to attribute conceptual and philosophical significance because it
</em><br>
<em>&gt; leads to declaring victory much too early
</em><br>
<p>We are certainly not declaring victory.
<br>
<p><em>&gt; &gt; &gt; The lower
</em><br>
<em>&gt; &gt; &gt; levels of Novamente were designed with the belief that these
</em><br>
<em>&gt; lower levels,
</em><br>
<em>&gt; &gt; &gt; in themselves, implemented cognition, not with the intent
</em><br>
<em>&gt; that these low
</em><br>
<em>&gt; &gt; &gt; levels should support higher levels of organization.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; This is completely untrue.  You were not there when we designed these
</em><br>
<em>&gt; &gt; levels, so how on Earth can you make this presumption??
</em><br>
<em>&gt;
</em><br>
<em>&gt; Because they show no sign of specialization to support higher levels of
</em><br>
<em>&gt; organization.
</em><br>
<p>We have different ideas about how much specialization *should* be there in
<br>
the &quot;implementation level&quot; of an AI design...
<br>
<p>If you showed me a design with a huge amount of functional specialization in
<br>
the codebase, I might well think it was doomed to fail because of being too
<br>
complicated and overspecialized at such a low level...
<br>
<p><em>&gt; &gt; I spent the 8 years before starting designing Webmind, writing books and
</em><br>
<em>&gt; &gt; paper on self-organization and emergence in the mind.  (See especially
</em><br>
<em>&gt; &gt; Chaotic Logic and From Complexity to Creativity)
</em><br>
<em>&gt;
</em><br>
<em>&gt; Great, you're older.  I'm more dedicated.
</em><br>
<p>My terrifyingly advanced age was not the point of that statement.  My point
<br>
was entirely different: that I had been thinking a lot about *emergent mind*
<br>
before launching into detailed AI design, so that for me and my team, our
<br>
detailed work was implicitly understood in the context of all this prior
<br>
stuff on emergent mind.  A context that was not adequately drawn into the
<br>
book, as I've said a lot of times already.
<br>
<p><p>Your statement and assumption that you're &quot;more dedicated&quot; than I am is
<br>
rather silly, in my view.  I suspect we're both extremely dedicated to our
<br>
work, enough so that a competitive comparison is not meaningful.  (or
<br>
useful).
<br>
<p><em>&gt; I know you had emergence as a goal.  What aspects of the system are there
</em><br>
<em>&gt; *specifically* to support this goal?  What design requirements were handed
</em><br>
<em>&gt; down from this goal?
</em><br>
<p>Too much to answer in an e-mail
<br>
<p><em>&gt; And bear in mind also that from my
</em><br>
<em>&gt; perspective having
</em><br>
<em>&gt; &quot;a higher level of structure and dynamics&quot; is not a good goal for an AGI
</em><br>
<em>&gt; design; one should have certain specific high-level structures
</em><br>
<em>&gt; and dynamics,
</em><br>
<em>&gt; and certain specific behaviors above *those* dynamics, and so on through
</em><br>
<em>&gt; your system's specified levels of organization.  Of course you
</em><br>
<em>&gt; may disagree.
</em><br>
<p>Of course simply having *any* higher level of structure and dynamics is not
<br>
good enough.
<br>
<p>I think we disagree on the *amount* of specificity one should try to design
<br>
into the emergent behaviors of a system, but the amount is far greater than
<br>
*zero* even in my view
<br>
<p><em>&gt;
</em><br>
<em>&gt; While I worry that Novamente's emergent dynamics will be chained
</em><br>
<em>&gt; to the same
</em><br>
<em>&gt; behaviors as the built-in ones.
</em><br>
<p>An empirical question, fortunately
<br>
<p><em>&gt; What are you calling a &quot;goal system&quot; and what did Webmind do with it?
</em><br>
<em>&gt;
</em><br>
<p>later, my time for this email has expired...
<br>
<p><em>&gt; Again, this is the kind of encouraging statement that I used to suspend
</em><br>
<em>&gt; judgement on before I read the Novamente manuscript.  Now that
</em><br>
<em>&gt; I've read it,
</em><br>
<em>&gt; I would ask questions such as &quot;What specific applications did you think
</em><br>
<em>&gt; Webmind was better suited for?&quot;, &quot;Did you test it?&quot;, &quot;What kind of results
</em><br>
<em>&gt; did you get in what you thought of as better-suited applications?&quot;, and so
</em><br>
<em>&gt; on.
</em><br>
<p>The problem with text mining apps is that they require dealing with language
<br>
in an ungrounded way.
<br>
<p>The stuff we're doing now, with analyzing bioinformatic data, is better.
<br>
Because the system is perceiving &quot;raw data&quot; from quantitative data files &amp;
<br>
databases, and then building up its own concepts from them.  when it deals
<br>
with language it will then do so only in the context of the empirical data
<br>
patterns it has already built up.
<br>
<p>The financial analytics apps we did at webmind were better in an AGI sense.
<br>
<p>In bio &amp; finance we got really awesome results in terms of being able to
<br>
recognized fancier patterns than anyone else.
<br>
<p>Not AGI of course.  Either
<br>
<p>a) a dead end, or
<br>
<p>b) important work tuning the perceptual pattern-recognition part of the
<br>
system for interesting real-world domains and building up its base of
<br>
perceptual patterns
<br>
<p>depending on your perspective ;&gt;
<br>
<p><p><em>&gt; One of the attitudes that seems very obvious to me is that you should
</em><br>
<em>&gt; estimate the size of the problem without being influenced by what
</em><br>
<em>&gt; resources
</em><br>
<em>&gt; you think will be available to solve it.
</em><br>
<p>well yeah, if I didn't do *that*, I would have just decided it was a problem
<br>
I could solve myself and avoided all the hassle of gathering a team ;-&gt;
<br>
<p>My estimate of the amount of manpower required to make an AI has gone DOWN
<br>
over the last 3 years, not up.  It went up from 1997-2000, and has gone down
<br>
from late 2000 till now.  This is because of the design simplifications that
<br>
went into the move from Webmind to Novamente...
<br>
<p>ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3644.html">Eugen Leitl: "Re: A New Kind Of Science"</a>
<li><strong>Previous message:</strong> <a href="3642.html">Jordan Dimov: "A New Kind Of Science"</a>
<li><strong>In reply to:</strong> <a href="3639.html">Eliezer S. Yudkowsky: "Re: Review of Novamente"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3649.html">Eliezer S. Yudkowsky: "Re: Review of Novamente"</a>
<li><strong>Reply:</strong> <a href="3649.html">Eliezer S. Yudkowsky: "Re: Review of Novamente"</a>
<li><strong>Reply:</strong> <a href="3752.html">Eliezer S. Yudkowsky: "Re: Review of Novamente"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3643">[ date ]</a>
<a href="index.html#3643">[ thread ]</a>
<a href="subject.html#3643">[ subject ]</a>
<a href="author.html#3643">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
