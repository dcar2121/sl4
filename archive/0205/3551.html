<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: supergoal stability</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: supergoal stability">
<meta name="Date" content="2002-05-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: supergoal stability</h1>
<!-- received="Fri May 03 19:25:51 2002" -->
<!-- isoreceived="20020504012551" -->
<!-- sent="Fri, 03 May 2002 18:38:46 -0400" -->
<!-- isosent="20020503223846" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: supergoal stability" -->
<!-- id="3CD31176.A31178A1@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20020503144948.D5555@eskimo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20supergoal%20stability"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri May 03 2002 - 16:38:46 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3552.html">Eliezer S. Yudkowsky: "Voss's comments on Guidelines"</a>
<li><strong>Previous message:</strong> <a href="3550.html">John Smart: "Alien Technology"</a>
<li><strong>In reply to:</strong> <a href="3549.html">Wei Dai: "supergoal stability"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3572.html">Wei Dai: "Re: supergoal stability"</a>
<li><strong>Reply:</strong> <a href="3572.html">Wei Dai: "Re: supergoal stability"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3551">[ date ]</a>
<a href="index.html#3551">[ thread ]</a>
<a href="subject.html#3551">[ subject ]</a>
<a href="author.html#3551">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Wei Dai wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I would like to gain a better understanding of why Friendliness might be a
</em><br>
<em>&gt; stable supergoal for an SI. I hope Eliezer finds these questions
</em><br>
<em>&gt; interesting enough to answer.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1. Are there supergoals other than Friendliness that can be stable for
</em><br>
<em>&gt; SI's? For example, can it be a stable supergoal to convert as much of the
</em><br>
<em>&gt; universe as possible into golf balls? To be friendly but to favor a subset
</em><br>
<em>&gt; of humanity over the rest (i.e. give them priority access to any resources
</em><br>
<em>&gt; that might be in contention)? To serve the wants of a single person?
</em><br>
<p>It currently looks to me like any mind-in-general that is *not* Friendly
<br>
will automatically resist all modifications of the goal system, to the limit
<br>
of its ability to detect modifications.  So you can have a bacterial
<br>
superintelligence that wants to convert the universe into golf balls, where
<br>
&quot;golf balls&quot; are whatever they were when the original seed first gained the
<br>
ability to protect its own goal system from modification.  But if you're
<br>
talking about the kind of goals that humans would craft, the CFAI semantics
<br>
are the only way I know of for creating an AI/SI that would implement those
<br>
goals as they were originally understood by the programmers.  And the CFAI
<br>
approach is tuned to propagation rather than propaganda; if you don't
<br>
believe in what you're saying, the CFAI semantics don't work - or at least
<br>
don't work in the sense that the propaganda will be rejected.  And no, I'm
<br>
not going to even try and design any kind of Friendliness semantics that
<br>
would work for propaganda.  Any AI project that tries this is beyond my
<br>
sympathy; the only thing to do is try and beat them to the punch.
<br>
<p><em>&gt; 2. If the answer to question 1 is yes, will the first SI created by humans
</em><br>
<em>&gt; will have the supergoal of Friendliness? Given that for most people
</em><br>
<em>&gt; selfishness is a stronger motivation than altruism, how will Eliezer get
</em><br>
<em>&gt; sufficient funding before someone more selfish manages to create an SI?
</em><br>
<p>As usual, the answer to this question is that we are not in the business of
<br>
predicting that things will turn out okay, but of doing what we can to
<br>
improve the probability that things will turn out okay.
<br>
<p><em>&gt; 3. If the answer to question 1 is no, why not? Why can't the CFAI approach
</em><br>
<em>&gt; be used to build an AI that will serve the selfish interests of a group or
</em><br>
<em>&gt; individual?
</em><br>
<p>The inventor of CFAI won't even tell you the reasons why this would be
<br>
difficult, just that it is.  That doesn't mean you can relax; anyone evil
<br>
enough to build a self-serving AI probably doesn't know about the CFAI
<br>
semantics or else doesn't care.
<br>
<p><em>&gt; My current understanding of Eliezer's position is that many non-Friendly
</em><br>
<em>&gt; goals have no philosophical support.
</em><br>
<p>&quot;Philosophical support&quot; is a CFAI concept.  A bacterial superintelligence
<br>
may not care whether something has philosophical support, and this may be a
<br>
self-consistent state for a mind, even a superintelligent mind.
<br>
<p><em>&gt; If I try to make the supergoal of an
</em><br>
<em>&gt; AI &quot;serve Wei Dai&quot;, that will be intepreted by the AI as &quot;serve myself&quot;
</em><br>
<em>&gt; (i.e. serve the AI itself), because selfishness does have philosophical
</em><br>
<em>&gt; support while serving an arbitrary third party does not. Is that a correct
</em><br>
<em>&gt; understanding?
</em><br>
<p>No; what probably would happen, though, is that the AI's understanding of
<br>
&quot;serve Wei Dai&quot; would be frozen into a form that wasn't the form you had
<br>
intended.
<br>
<p><em>&gt; 4. Back in Oct 2000, Eliezer wrote (in
</em><br>
<em>&gt; <a href="http://sysopmind.com/archive-sl4/0010/0010.html">http://sysopmind.com/archive-sl4/0010/0010.html</a>):
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; A Friendliness system consists
</em><br>
<em>&gt; &gt; not so much of hardwired rules or even instincts but rather an AI's &quot;personal
</em><br>
<em>&gt; &gt; philosophy&quot; - I use quotemarks to emphasize that an AI's personal philosophy
</em><br>
<em>&gt; &gt; would be a rather alien thing; you can't just export your own personal
</em><br>
<em>&gt; &gt; philosophy into an AI's mind. Your own personal philosophy is not necessarily
</em><br>
<em>&gt; &gt; stable under changes of cognitive architecture or drastic power imbalances.
</em><br>
<p>Well, today I would say it differently:  Today I would say that you have to
<br>
do a &quot;port&quot; rather than a &quot;copy and paste&quot;, and that an AI can be *more*
<br>
stable under changes of cognitive architecture or drastic power imbalances
<br>
than a human would be, unless the human had the will and the knowledge to
<br>
make those cognitive changes that would be required to match a Friendly AI
<br>
in this area.
<br>
<p><em>&gt; Ben Goertzel followed up with a question that went unanswered:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; And nor will an AI's be, necessarily, will it?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Would Eliezer like to answer the question now? Will the Friendly AI's
</em><br>
<em>&gt; &quot;personal philosophy&quot; be stable under self-improvement?
</em><br>
<p>That's the whole point: lock, stock, and coloring book.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3552.html">Eliezer S. Yudkowsky: "Voss's comments on Guidelines"</a>
<li><strong>Previous message:</strong> <a href="3550.html">John Smart: "Alien Technology"</a>
<li><strong>In reply to:</strong> <a href="3549.html">Wei Dai: "supergoal stability"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3572.html">Wei Dai: "Re: supergoal stability"</a>
<li><strong>Reply:</strong> <a href="3572.html">Wei Dai: "Re: supergoal stability"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3551">[ date ]</a>
<a href="index.html#3551">[ thread ]</a>
<a href="subject.html#3551">[ subject ]</a>
<a href="author.html#3551">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
