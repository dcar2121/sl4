<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Complexity of AGI</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Complexity of AGI">
<meta name="Date" content="2002-05-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Complexity of AGI</h1>
<!-- received="Sun May 19 21:55:46 2002" -->
<!-- isoreceived="20020520035546" -->
<!-- sent="Sun, 19 May 2002 19:01:22 -0600" -->
<!-- isosent="20020520010122" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Complexity of AGI" -->
<!-- id="LAEGJLOGJIOELPNIOOAJEEFECIAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3CE83D0D.E484D55F@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Complexity%20of%20AGI"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sun May 19 2002 - 19:01:22 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3754.html">Ben Goertzel: "RE: Review of Novamente"</a>
<li><strong>Previous message:</strong> <a href="3752.html">Eliezer S. Yudkowsky: "Re: Review of Novamente"</a>
<li><strong>In reply to:</strong> <a href="3745.html">Eliezer S. Yudkowsky: "Re: Complexity of AGI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3786.html">James Rogers: "Re: Complexity of AGI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3753">[ date ]</a>
<a href="index.html#3753">[ thread ]</a>
<a href="subject.html#3753">[ subject ]</a>
<a href="author.html#3753">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; Well, yeah, cuz you think that all the higher levels of
</em><br>
<em>&gt; organization emerge
</em><br>
<em>&gt; automatically from the low-level behaviors.
</em><br>
<p>&quot;Automatically&quot; when the low-level behaviors are designed and tuned
<br>
correctly, with emergence in mind.
<br>
<p>Whether that's &quot;automatic&quot; or not is a borderline semantic case, I guess
<br>
<p><em>&gt;The usual, in other
</em><br>
<em>&gt; words.
</em><br>
<p>There is no &quot;usual&quot; in philosophy of mind, and my ideas certainly are not
<br>
perceived as &quot;usual&quot; by the AI or cog sci mainstream.
<br>
<p>I could as well call your ideas &quot;usual&quot; because of their strong reliance on
<br>
the study of perception in the human brain.  (Many researchers have spoken
<br>
out against the preponderance of theories of mind that are overly
<br>
conceptually founded on human visual perception.  Your could be seen as
<br>
fitting into this trend.)
<br>
<p>Both of our theories, and all others I know of, have some similarities and
<br>
some differences with other theories previously proposed.
<br>
<p><em>&gt; You seem to be skipping over
</em><br>
<em>&gt; all the issues that I think constitute the real, critical, hard
</em><br>
<em>&gt; parts of the
</em><br>
<em>&gt; problem.
</em><br>
<p>Can you give a short list of these?
<br>
<p><p><em>&gt; An AI is not a human.  I think that an AI design would start out
</em><br>
<em>&gt; by working
</em><br>
<em>&gt; very inefficiently with a small amount of tuning.
</em><br>
<p>Well, sure ... but it all comes down to *how* inefficient &quot;very&quot; is...
<br>
<p><em>&gt; After that
</em><br>
<em>&gt; would come the
</em><br>
<em>&gt; task of getting the AI to undertake more and more complex &quot;tuning&quot;, a task
</em><br>
<em>&gt; which is one of the earliest forms of seed AI.
</em><br>
<p>My feeling is that automated parameter tuning by optimization methods,
<br>
rather than by reflexion, is going to be necessary for any complex AI
<br>
design.  This is from experience not only with Novamente... but from the sum
<br>
total experience of all computer science...
<br>
<p><p><em>&gt; Because a correct seed AI design is designed to create and store
</em><br>
<em>&gt; complexity.  It has other places to store complexity aside from a global
</em><br>
<em>&gt; surface of 1500 parameters.
</em><br>
<p>It's fine that you like the number 1500, but I don't want you to spread the
<br>
meme that any software program I've been involved with has required global
<br>
optimization over a 1500-dimensional parameter space.  I already corrected
<br>
this error in my last response to you.
<br>
<p>Of course any AGI design is designed to create and store complexity.  To me,
<br>
this verbiage says ABSOLUTELY NOTHING about the number of parameters
<br>
involved and the sensitivity of the system on these parameters.  My
<br>
experience and my study of CS and neuroscience tells me that systems
<br>
*designed to create and store complexity* tend to have many parameters and a
<br>
complex dependence on them.
<br>
<p>It's true of the brain, and true of all existing complex software systems.
<br>
Ever play with Echo, or Swarm, or Tierra, for example?  Or a complex
<br>
attractor neural net with asymmetric weights?  All these things create and
<br>
store complexity, and all have complex multidimensional parameter
<br>
dependencies.  And in all cases, the more complexity is created and stored,
<br>
the trickier the parameter interdependencies.
<br>
<p><p><em>&gt; Efficient?  The two most critical performance issues are:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1)  Making the pieces of the system fit together, at all;
</em><br>
<em>&gt;
</em><br>
<em>&gt; 2)  Making tuning of the system tractable and manageable for the pieces of
</em><br>
<em>&gt; the system working together.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Working to make the system more efficient is really not the
</em><br>
<em>&gt; point.
</em><br>
<p>No, making the system more efficient IS an important point, separate from
<br>
the points you mention.
<br>
<p>Even if the pieces of the system fit together and are &quot;tractably tunable&quot; in
<br>
terms of the math of their parameter space, it may STILL be true that the
<br>
pieces cannot run efficiently on available hardware.  Some of the more
<br>
easily tunable aspects of Webmind had this property.  Sometimes, not always,
<br>
making something more efficient on available hardware leads to nastier
<br>
parameter-tuning issues.
<br>
<p><em>&gt; I understand that Novamente is a commercial project
</em><br>
<p>It is a project which explicitly has scientific and humanistic as well as
<br>
business goals, however, as clearly stated on www.realai.net and in all
<br>
previous writings on Novamente or Webmind.
<br>
<p><em>&gt; and hence may put in a
</em><br>
<em>&gt; lot of human effort toward achieving a given level of performance
</em><br>
<em>&gt; at a given
</em><br>
<em>&gt; time, but I really don't think that it's possible to understand seed AI by
</em><br>
<em>&gt; improving the system yourself and tuning the system using genetic
</em><br>
<em>&gt; algorithms.
</em><br>
<p>This is an odd statement.  If neither humans nor optimization methods can
<br>
solve the problem, who can?  God?
<br>
<p><em>&gt; Mostly, though, I feel that a correct AI design may work *better*
</em><br>
<em>&gt; if you set
</em><br>
<em>&gt; the parameters for &quot;forgetting&quot; things to exactly the right value, but it
</em><br>
<em>&gt; will *still work* even if the parameters are set to different values.
</em><br>
<p>I'm afraid this is a very idealistic point of view.
<br>
<p>I hope very much, however, that Novamente turns out to work this nicely!
<br>
<p>Occasionally I've worked with narrow-AI systems that had this kind of nice
<br>
property, but it's not been the rule, in my experience.  Furthermore NN's
<br>
almost NEVER have this kind of nice property.  Unfortunately, it's systems
<br>
closer to the symbolic level that tend to be like this...
<br>
<p><em>&gt; I think that Novamente's sensitivity to the
</em><br>
<em>&gt; exact equations
</em><br>
<em>&gt; it uses for inference
</em><br>
<p>This is wrong, it is not at all sensitive to the exact equations it uses for
<br>
inference.  Where did you get this idea?
<br>
<p>In fact, it's the NN-ish aspects of the system (IMportant Updating function
<br>
for example, and evolutionary concept formation) that are trickier to tune.
<br>
<p>The only part of inference on which Webmind seemed to depend sensitively was
<br>
*control parameters for inference control*... not inference rules
<br>
<p><em>&gt; are symptomatic of an AI pathology for
</em><br>
<em>&gt; inference that
</em><br>
<em>&gt; results from insufficient functional complexity for inference.  A real AI
</em><br>
<em>&gt; would be able to use rough approximations to Novamente's exact
</em><br>
<em>&gt; equations and
</em><br>
<em>&gt; still work just fine.
</em><br>
<p>yes.  Novamente can do this just fine.  If you substitute Pei Wang's NARS
<br>
rules for our current inference engine the system basically works OK, it's
<br>
just a bit dumber.  NARS is a crude approximation to our prob. inference
<br>
rules, in my view.
<br>
<p>However, if you substitute a slightly different equation for the importance
<br>
updating function, THEN, you can get really fucked up behavior -- you don't
<br>
get &quot;emergent map formation&quot; in any plausible sense
<br>
<p>The closer you are to the symbolic level, the less the parameter tuning
<br>
problems are.  Not that they're unsolvable on the NN level -- we have an OK
<br>
IUF now, and the parameter tuning problem for the IUF has been largely
<br>
solved by some simple adaptive optimizations.  (Not tried yet in Novamente.)
<br>
<p><p><em>&gt; &gt; THEN, my friend, you will have performed what I would term &quot;One
</em><br>
<em>&gt; Fucking Hell
</em><br>
<em>&gt; &gt; of a Miracle.&quot;  I don't believe it's possible, although I do consider it
</em><br>
<em>&gt; &gt; possible you can make a decent AI design based on your DGI theory.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes, an AI design requires One Fucking Hell of a Miracle.  But it's a
</em><br>
<em>&gt; *different* Miracle than the kind you describe.  It's not a question of
</em><br>
<em>&gt; solving enormous engineering problems through Incredible Dauntless Efforts
</em><br>
<em>&gt; but of creating designs that are So Impossibly Clever they don't run into
</em><br>
<em>&gt; those enormous engineering problems.  I think many of the problems you're
</em><br>
<em>&gt; running into are symptoms of trying to solve the problem too simply.
</em><br>
<p>Well, I look forward to seeing your impossibly clever designs.  Nothing in
<br>
DGI hints to *me* at anything impossibly clever, but I look forward to
<br>
seeing what you have up your sleeve...
<br>
<p><em>&gt; another way of looking at it is that, from my perspective, you're
</em><br>
<em>&gt; making generic algorithms do things that I think should be broken up into
</em><br>
<em>&gt; interdependent internally specialized subsystems.
</em><br>
<p>Examples of these independent internally specialized subsystems would be
<br>
useful
<br>
<p><em>&gt; I think one of the reasons you're focused on parameter tuning and
</em><br>
<em>&gt; performance engineering of Novamente is that Novamente is *just barely*
</em><br>
<em>&gt; capable of solving a certain class of engineering problems, because
</em><br>
<em>&gt; Novamente is too simple a design.  I think that an improved design would
</em><br>
<em>&gt; just swallow this whole class of problems whole and hence not require an
</em><br>
<em>&gt; enormous amount of parameter tuning and performance engineering to do it.
</em><br>
<em>&gt; Of course, there will then be a new fringe of problems, which you swallow
</em><br>
<em>&gt; not by tuning parameters but by improving the system design so that these
</em><br>
<em>&gt; problems are also &quot;oversolved&quot;, swallowed whole.  But since you
</em><br>
<em>&gt; believe the
</em><br>
<em>&gt; current Novamente design is already adequate for general intelligence, and
</em><br>
<em>&gt; since the design itself has a flat architecture, that kind of space for
</em><br>
<em>&gt; design improvement is not really open to you.  Which is why you focus on
</em><br>
<em>&gt; parameter tuning and performance engineering.  That's how I see
</em><br>
<em>&gt; it, anyway.
</em><br>
<p>Frankly, we are NOT focusing primarily on parameter tuning and performance
<br>
engineering at this point.
<br>
<p>However, we may be in a year or two -- I hope not though.
<br>
<p>My hypothesis was that with a system significantly more complex than
<br>
Novamente, these issues would become dominant and incredibly difficult.  Not
<br>
that they are consuming most of our time with Novamente.
<br>
<p><em>&gt; This business of very fragile solutions is a symptom of gnawing at the
</em><br>
<em>&gt; fringes of the problem space,
</em><br>
<p>The brain is very fragile; minor changes in the levels of certain chemicals
<br>
cause all kinds of problems.
<br>
<p>And, lifting cognitive problems out of familiar domains like the physical
<br>
and social world suddenly makes them INCREDIBLY difficult.
<br>
<p>The brain is far less fragile than, say, Deep Blue, but it's hardly a
<br>
paragon of generality and flexibility and parameter-insensitivity...
<br>
<p><em>&gt; This kind of design is totally foreign to software engineering as
</em><br>
<em>&gt; it stands
</em><br>
<em>&gt; today, which typically is interested in *just one* solution to a problem.
</em><br>
<em>&gt; If you iterate *just one* solution over and over, it creates systems that
</em><br>
<em>&gt; become very fragile as they become large.  If you iterate *many possible
</em><br>
<em>&gt; paths to success* over and over - which really is one of those things that
</em><br>
<em>&gt; you can do in an AI design but not a bank's transaction system - then you
</em><br>
<em>&gt; don't get the AI pathology of this incredible fragility.
</em><br>
<p>I don't understand the connection you are drawing between software
<br>
engineering and AI design.
<br>
<p>In my approach, the two are pretty separate.  The AI design is mathematical,
<br>
inspired by philosophy.  How to engineer a given mathematical design is a
<br>
separate problem.
<br>
<p>At a very high level the Novamente design is created so as to support
<br>
practical software engineering, but the details of the Ai design are in no
<br>
way determined by issues of software engineering.
<br>
<p><em>&gt; &gt; But this is the worst example you could have possibly come up
</em><br>
<em>&gt; with!  Cyc is
</em><br>
<em>&gt; &gt; very easy to engineer precisely because it makes so many simplifying
</em><br>
<em>&gt; &gt; assumptions.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is just how I feel about Novamente.
</em><br>
<p>Novamente is NOT very easy to engineer, not at all.
<br>
<p>And, it makes a couple orders of magnitude fewer simplifying assumptions
<br>
than CYC.
<br>
<p>Lenat understands this, I'm not sure why you don't.
<br>
<p>You think Novamente STILL makes too many simplifying assumptions, fine.
<br>
<p>I think DGI makes some really weird simplifying assumptions, like
<br>
simplifying away most of the dynamics of concepts...
<br>
<p>I guess we each choose our assumptions.
<br>
<p><em>&gt; &gt; not overly complex ones.  AI scientists have
</em><br>
<em>&gt; &gt; VERY often, it seems to me, simplified their theories so they would have
</em><br>
<em>&gt; &gt; theories that could be implemented without excessive
</em><br>
<em>&gt; implementation effort
</em><br>
<em>&gt; &gt; and excessive parameter tuning.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Noooo... AI scientists have often oversimplified their theories
</em><br>
<em>&gt; because (a)
</em><br>
<em>&gt; they made philosophical connections between observed human behaviors and
</em><br>
<em>&gt; simple computational properties based on surface similarities and
</em><br>
<em>&gt; enthusiasm; (b) because they didn't have the knowledge, skill, or
</em><br>
<em>&gt; pessimistic attitude to perceive really complex systems, and
</em><br>
<em>&gt; hence could not
</em><br>
<em>&gt; &quot;move&quot; in the direction of greater complexity when figuring out
</em><br>
<em>&gt; which system
</em><br>
<em>&gt; to design.
</em><br>
<p>Well, I think many many conventional AI people have the knowledge and skill
<br>
to &quot;perceive really complex systems&quot; and they have plenty of pessimism!
<br>
<p>I think they want to build systems that work and do stuff, quickly, so they
<br>
can publish papers and get tenure and promotion.  This pushes them to do
<br>
simpler stuff than Novamente or CYC or DGI.   They don't have patrons like
<br>
you do (and nor do I); they won't get tenure for publishing &quot;Staring into
<br>
the Singularity&quot;, they need to demonstrate results continually to keep
<br>
paying the rend.
<br>
<p>I think that very very very few traditional Ai people have made the
<br>
cognitive errors you ascribe to them, in recent years, although they have
<br>
made many other errors.
<br>
<p>Your errors were made by the AI community in the 60's and 70's, on the basis
<br>
of much less knowledge about AI than is now available.  It's easy now to
<br>
laugh at the  mistakes of AI researchers from a different era, given all
<br>
that we know now.
<br>
<p>I accuse my academic and industry AI colleagues of unambition but not of the
<br>
profound shallow-mindedness that you insinuate.  I think most AI researchers
<br>
have learned the lesson that AGI is really hard and as a result are now
<br>
working on simpler stuff.
<br>
<p>And occasionally they make the error of overgeneralizing from their simpler
<br>
stuff to bigger issues...
<br>
<p><p><em>&gt; This is an inescapable problem of seed AI, and one of the ways it becomes
</em><br>
<em>&gt; more tractable is by, for example, localizing parameters.
</em><br>
<p>Of course, one localizes parameters as much as one can.  The modular
<br>
structure of Novamente helps with this, but there are limits...
<br>
<p><em>&gt;  I think that the problems you are now experiencing are AI pathologies
</em><br>
<em>&gt; of parameters that are too global and too simple.
</em><br>
<p>At this point, you have called my work &quot;pathological&quot; so many times that I
<br>
am tempted to accuse you of having an &quot;AI psychopathology&quot; which causes you
<br>
to believe your design will be the Holy Grail, when you don't even know what
<br>
your design is yet!
<br>
<p>This conversation reminds me of conversations I had with Webmind Inc.
<br>
cofounder Onar Aam.  Everything was really simple to him, and he understood
<br>
how to make everything work perfectly, until he actually tried to do
<br>
something.  &quot;It compiled on the Onar machine,&quot; was the running joke.
<br>
<p><em>&gt; &gt; Are they the structures described in the DGI philosophy paper that you
</em><br>
<em>&gt; &gt; posted to this list, or something quite different?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Memory.  Concept kernel formation.  I would say the things from DGI, but I
</em><br>
<em>&gt; would add the proviso that I don't think you understood which
</em><br>
<em>&gt; subsystems DGI
</em><br>
<em>&gt; was asking for.
</em><br>
<p>If I did not understand what subsystems DGI was asking for, can you clarify
<br>
in some more comprehensible way?
<br>
<p>I read the paper very carefully.
<br>
<p>As a side issue, I do not exactly agree with your theory of concepts as
<br>
having &quot;kernels&quot;.  I think some concepts have kernels, but they emerge,
<br>
concepts aren't necessarily built around kernels...
<br>
<p><p><em>&gt; &gt; I sure am eager to see how DGI or *any* AGI system is going to
</em><br>
<em>&gt; avoid this
</em><br>
<em>&gt; &gt; sort of problem.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Deep architectures, experiential learning of local patterned variables
</em><br>
<em>&gt; instead of optimization of global quantitative variables,
</em><br>
<em>&gt; multiple solution
</em><br>
<em>&gt; pathways on multiple levels of organization, carving the system at the
</em><br>
<em>&gt; correct joints.
</em><br>
<p>These words don't help me believe, of course...
<br>
<p>Experiential learning of local patterned variables is VERY HARD and requires
<br>
a lot of experiential data.
<br>
<p>This learning has to be accomplished by some cognitive mechanism... which
<br>
may have quantitative parameters ;)
<br>
<p>I think that quantitative parameters are easier to tune than patterned
<br>
variables, and that both kinds of variables are important in the mind
<br>
<p><em>&gt; &gt; &quot;Deep architecture&quot; is a cosmic-sounding term; would you care
</em><br>
<em>&gt; to venture a
</em><br>
<em>&gt; &gt; definition?  I don't really know what you mean, except that
</em><br>
<em>&gt; you're implying
</em><br>
<em>&gt; &gt; that your ideas are deep and mine are shallow.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Hopefully, what I said above fleshes out the definition a bit.
</em><br>
<p>It didn't really.  Don't you have a few-sentences definition of &quot;deep
<br>
architecture&quot;?
<br>
<p><em>&gt; From my perspective, you're trying to use simple generic
</em><br>
<em>&gt; processes to do things that require the interaction of interdependent
</em><br>
<em>&gt; internally specialized processes.
</em><br>
<p>Such as what processes?
<br>
<p><p><em>&gt; The part where we disagree is in the question of whether
</em><br>
<em>&gt; evolution carefully
</em><br>
<em>&gt; and exactingly sculpted those higher levels of organization just as it
</em><br>
<em>&gt; sculpted the neural interactions, or whether all higher levels of
</em><br>
<em>&gt; organization emerge automatically as the laws of physics supposedly do (I
</em><br>
<em>&gt; have my doubts).
</em><br>
<p>You are caricaturing my point of view, in spite of my repeated attempts at
<br>
clarification.
<br>
<p><em>&gt; I also feel that if you intuit dynamics will emerge, they will
</em><br>
<em>&gt; not emerge.
</em><br>
<em>&gt; If you know what the dynamics are and how they work, you will be able to
</em><br>
<em>&gt; create systems that support them; not otherwise.
</em><br>
<p>Well I have intuitive knowledge of what the dynamics are and how they work
<br>
which is what leads me to the intuition that these dynamics will emerge.
<br>
Writing down this intuitive knowledge would be a big project in itself!
<br>
<p><em>&gt; I think that the history
</em><br>
<em>&gt; of AI shows that one of the most frequent classes of error is
</em><br>
<em>&gt; hoping that a
</em><br>
<em>&gt; quality emerges when you don't really know exactly how it works.
</em><br>
<p>Not really, very very few classic AI designs have relied significantly on
<br>
the concept of &quot;emergence&quot;
<br>
<p><p><em>&gt; &gt; You seem to have misinterpreted me.  I am not talking about
</em><br>
<em>&gt; anything being
</em><br>
<em>&gt; &gt; in principle beyond human capability to comprehend forever.
</em><br>
<em>&gt; Some things ARE
</em><br>
<em>&gt; &gt; (this is guaranteed by the finite brain size of the human species), but
</em><br>
<em>&gt; &gt; that's not the point I'm making.
</em><br>
<em>&gt;
</em><br>
<em>&gt; OK.  We have different ideas about what a modern-day AI
</em><br>
<em>&gt; researcher should be
</em><br>
<em>&gt; trying to comprehend.  Does that terminology meet with your approval?
</em><br>
<p>Well, I don't know if we have different ideas about what an AI researcher
<br>
should be trying to comprehend.
<br>
<p>I think we have different intuitive understandings of the same issues,
<br>
largely.
<br>
<p><em>&gt; &gt; Eliezer, I think it is rather funny for *you* to accuse *me* of
</em><br>
<em>&gt; flinching
</em><br>
<em>&gt; &gt; away from the prospect of trying to do something!
</em><br>
<em>&gt;
</em><br>
<em>&gt; What on Earth are you talking about here?  Where did you get the
</em><br>
<em>&gt; idea that I
</em><br>
<em>&gt; am deliberately holding back on anything?  I'd be putting together a
</em><br>
<em>&gt; programming team right now if SIAI had the funding.
</em><br>
<p>What would you have the programming team implement?
<br>
<p><p><p>-- Ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3754.html">Ben Goertzel: "RE: Review of Novamente"</a>
<li><strong>Previous message:</strong> <a href="3752.html">Eliezer S. Yudkowsky: "Re: Review of Novamente"</a>
<li><strong>In reply to:</strong> <a href="3745.html">Eliezer S. Yudkowsky: "Re: Complexity of AGI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3786.html">James Rogers: "Re: Complexity of AGI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3753">[ date ]</a>
<a href="index.html#3753">[ thread ]</a>
<a href="subject.html#3753">[ subject ]</a>
<a href="author.html#3753">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
