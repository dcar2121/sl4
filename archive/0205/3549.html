<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: supergoal stability</title>
<meta name="Author" content="Wei Dai (weidai@eskimo.com)">
<meta name="Subject" content="supergoal stability">
<meta name="Date" content="2002-05-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>supergoal stability</h1>
<!-- received="Fri May 03 18:04:19 2002" -->
<!-- isoreceived="20020504000419" -->
<!-- sent="Fri, 3 May 2002 14:49:48 -0700" -->
<!-- isosent="20020503214948" -->
<!-- name="Wei Dai" -->
<!-- email="weidai@eskimo.com" -->
<!-- subject="supergoal stability" -->
<!-- id="20020503144948.D5555@eskimo.com" -->
<!-- charset="us-ascii" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Wei Dai (<a href="mailto:weidai@eskimo.com?Subject=Re:%20supergoal%20stability"><em>weidai@eskimo.com</em></a>)<br>
<strong>Date:</strong> Fri May 03 2002 - 15:49:48 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3550.html">John Smart: "Alien Technology"</a>
<li><strong>Previous message:</strong> <a href="3548.html">Eliezer S. Yudkowsky: "Re: Singularitarian Activism (was: Trans Guide.)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3551.html">Eliezer S. Yudkowsky: "Re: supergoal stability"</a>
<li><strong>Reply:</strong> <a href="3551.html">Eliezer S. Yudkowsky: "Re: supergoal stability"</a>
<li><strong>Reply:</strong> <a href="3553.html">Ben Goertzel: "RE: supergoal stability"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3549">[ date ]</a>
<a href="index.html#3549">[ thread ]</a>
<a href="subject.html#3549">[ subject ]</a>
<a href="author.html#3549">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I would like to gain a better understanding of why Friendliness might be a 
<br>
stable supergoal for an SI. I hope Eliezer finds these questions 
<br>
interesting enough to answer.
<br>
<p>1. Are there supergoals other than Friendliness that can be stable for 
<br>
SI's? For example, can it be a stable supergoal to convert as much of the 
<br>
universe as possible into golf balls? To be friendly but to favor a subset 
<br>
of humanity over the rest (i.e. give them priority access to any resources 
<br>
that might be in contention)? To serve the wants of a single person?
<br>
<p>2. If the answer to question 1 is yes, will the first SI created by humans 
<br>
will have the supergoal of Friendliness? Given that for most people 
<br>
selfishness is a stronger motivation than altruism, how will Eliezer get 
<br>
sufficient funding before someone more selfish manages to create an SI?
<br>
<p>3. If the answer to question 1 is no, why not? Why can't the CFAI approach
<br>
be used to build an AI that will serve the selfish interests of a group or
<br>
individual?
<br>
<p>My current understanding of Eliezer's position is that many non-Friendly
<br>
goals have no philosophical support. If I try to make the supergoal of an
<br>
AI &quot;serve Wei Dai&quot;, that will be intepreted by the AI as &quot;serve myself&quot;
<br>
(i.e. serve the AI itself), because selfishness does have philosophical
<br>
support while serving an arbitrary third party does not. Is that a correct
<br>
understanding?
<br>
<p>4. Back in Oct 2000, Eliezer wrote (in 
<br>
<a href="http://sysopmind.com/archive-sl4/0010/0010.html">http://sysopmind.com/archive-sl4/0010/0010.html</a>):
<br>
<p><em>&gt; A Friendliness system consists 
</em><br>
<em>&gt; not so much of hardwired rules or even instincts but rather an AI's &quot;personal 
</em><br>
<em>&gt; philosophy&quot; - I use quotemarks to emphasize that an AI's personal philosophy 
</em><br>
<em>&gt; would be a rather alien thing; you can't just export your own personal 
</em><br>
<em>&gt; philosophy into an AI's mind. Your own personal philosophy is not necessarily 
</em><br>
<em>&gt; stable under changes of cognitive architecture or drastic power imbalances.
</em><br>
<p>Ben Goertzel followed up with a question that went unanswered:
<br>
<p><em>&gt; And nor will an AI's be, necessarily, will it? 
</em><br>
<p>Would Eliezer like to answer the question now? Will the Friendly AI's 
<br>
&quot;personal philosophy&quot; be stable under self-improvement?
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3550.html">John Smart: "Alien Technology"</a>
<li><strong>Previous message:</strong> <a href="3548.html">Eliezer S. Yudkowsky: "Re: Singularitarian Activism (was: Trans Guide.)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3551.html">Eliezer S. Yudkowsky: "Re: supergoal stability"</a>
<li><strong>Reply:</strong> <a href="3551.html">Eliezer S. Yudkowsky: "Re: supergoal stability"</a>
<li><strong>Reply:</strong> <a href="3553.html">Ben Goertzel: "RE: supergoal stability"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3549">[ date ]</a>
<a href="index.html#3549">[ thread ]</a>
<a href="subject.html#3549">[ subject ]</a>
<a href="author.html#3549">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
