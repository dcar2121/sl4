<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Voss's comments on Guidelines</title>
<meta name="Author" content="Peter Voss (peter@optimal.org)">
<meta name="Subject" content="RE: Voss's comments on Guidelines">
<meta name="Date" content="2002-05-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Voss's comments on Guidelines</h1>
<!-- received="Sat May 04 00:39:21 2002" -->
<!-- isoreceived="20020504063921" -->
<!-- sent="Fri, 3 May 2002 21:13:17 -0700" -->
<!-- isosent="20020504041317" -->
<!-- name="Peter Voss" -->
<!-- email="peter@optimal.org" -->
<!-- subject="RE: Voss's comments on Guidelines" -->
<!-- id="NFBBLLAGILEOENAGEGNOGEHBCPAA.peter@optimal.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="3CD31173.B4248684@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Peter Voss (<a href="mailto:peter@optimal.org?Subject=RE:%20Voss's%20comments%20on%20Guidelines"><em>peter@optimal.org</em></a>)<br>
<strong>Date:</strong> Fri May 03 2002 - 22:13:17 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3571.html">Damien Broderick: "Stross's post-Singularity novels"</a>
<li><strong>Previous message:</strong> <a href="3569.html">Ben Goertzel: "RE: supergoal stability"</a>
<li><strong>In reply to:</strong> <a href="3552.html">Eliezer S. Yudkowsky: "Voss's comments on Guidelines"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3570">[ date ]</a>
<a href="index.html#3570">[ thread ]</a>
<a href="subject.html#3570">[ subject ]</a>
<a href="author.html#3570">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi Eli,
<br>
<p><em>&gt;&gt; 1) Friendliness-topped goal system – Not possible: My design does not
</em><br>
allow for such a high-level ‘supergoal’.
<br>
<p><em>&gt;What are the forces in your design that determine whether one action is
</em><br>
taken rather than another?
<br>
<p>A myriad different goal systems that have complex inter-dependencies. More
<br>
specifically, some of the 'forces' are: the structure of sensed data,
<br>
feature extraction, internal resource management, operator input (teaching,
<br>
coaxing, etc), random (exploration), etc. Most of the particular system
<br>
involved are *highly adaptive* and interact in unpredictable feedforward &amp;
<br>
feedback loops and networks.
<br>
<p><p><em>&gt;&gt; 2) Cleanly causal goal system – Not possible: requires 1)
</em><br>
<p><em>&gt;Does your system choose between actions on the basis of which future events
</em><br>
those actions are predicted to lead to?
<br>
<p>Only at the higher levels, and within severe constraints (as are our
<br>
choices).
<br>
<p><p><em>&gt;&gt; 3) Probabilistic supergoal content – Inherent in my design: All &gt;
</em><br>
knowledge and goals are subject to revision.
<br>
<p><em>&gt;If your system has no supergoal, but does have reinforcement, the
</em><br>
reinforcement systems are also part of the goal system.  Are the
<br>
reinforcement systems subject to revision?
<br>
<p>Yes. Yes.
<br>
<p><em>&gt;In any case, the recommendation of &quot;probabilistic supergoal content&quot; does
</em><br>
not just mean that certain parts of the goal system are subject to revision,
<br>
but that they have certain specific semantics that will enable the system to
<br>
consider that revision as desirable, so that the improvement of Friendliness
<br>
is stable under reflection, introspection, and self-modification.
<br>
<p>Can't do that - there is no supergoal.
<br>
<p><p><em>&gt;&gt; 4) Acquisition of Friendliness sources – While I certainly encourage the
</em><br>
AI to acquire knowledge (including ethical theory) compatible with what I
<br>
consider moral, this does not necessarily agree with what others regard as
<br>
desirable ethics/ Friendliness.
<br>
<p><em>&gt;&quot;Acquisition of Friendliness sources&quot; here means acquiring the forces that
</em><br>
influence human moral decisions as well as learning the final output of
<br>
those decisions.  It furthermore has the specific connotation of attempting
<br>
to deduce the forces that influence the moral statements of the programmers
<br>
even if the programmers themselves do not know them.
<br>
<p>I agree, but does not address my point of conflicting views of desirable
<br>
ethics. I would certainly hope that my AI will be super moral by my
<br>
standards, or by standards I would have if I were smarter - but even that
<br>
doesn't address the issue of meta-ethics - ones highest value/ purpose (Huge
<br>
discussion about ethics looming!)
<br>
<p><p><em>&gt;&gt; 5) Causal validity semantics – Inherent in my design: One of the key
</em><br>
functions of the AI is to (help me) review and improve its premises,
<br>
inferences, conclusions, etc. at all levels. Unfortunately, this ability
<br>
only becomes really effective once a significant level of intelligence has
<br>
already been reached.
<br>
<p><em>&gt;I agree with the latter sentence.  However, revision of beliefs is what I
</em><br>
would consider ordinary reasoning - causal validity semantics means that the
<br>
AI understands that its basic structure, its source code, is also the
<br>
product of programmer intentions that can be wrong.  That's *why* this
<br>
ability only becomes effective at a significant level of intelligence; it
<br>
inherently requires an integrated introspective understanding of brainware
<br>
and mindware, at minimum on the level of a human pondering evolutionary
<br>
psychology.
<br>
<p>We agree. What this means is that this design criterion is actually to build
<br>
a system that is simply very intelligent so that it can better understand
<br>
humans and itself.
<br>
<p><p><em>&gt;&gt; 6) Injunctions – This seems like a good recommendation, however it is not
</em><br>
clear what specific injunctions should be implemented, how to implement them
<br>
effectively, and to what extent they will oppose other recommendations/
<br>
features.
<br>
<p><em>&gt;Hopefully, SIAI will learn how injunctions work in practice, then publish
</em><br>
the knowledge.
<br>
<p>So nothing we can implement now.
<br>
<p><p><em>&gt;&gt; 7) Self-modeling of fallibility - Inherent in my design. This seems to be
</em><br>
an abstract expression of point 3)
<br>
<p><em>&gt;The human understanding of fallibility requires points (3), (4), and (5);
</em><br>
an AI, to fully understand its own fallibility, requires all of these as
<br>
well.  *Beginning* to model your own fallibility takes much less structure.
<br>
Any AI with a probabilistic goal system can do so, though doing so
<br>
efficiently requires reflection.
<br>
<p>Agree.
<br>
<p><p><em>&gt;&gt; 8) Controlled ascent – Good idea, but may be difficult to implement: It
</em><br>
may be hard to distinguish between rapid knowledge acquisition, improvements
<br>
in learning, and overall self-improvement (ie. substantial increases in
<br>
intelligence).
<br>
<p><em>&gt;All you need for a controlled ascent feature to be worthwhile is the
</em><br>
prospect of catching some of the hard takeoffs some of the time.
<br>
<p>Even by just 'being careful' we can catch 'some of the hard takeoffs some of
<br>
the time'. I thought that you were looking for something more reliable.
<br>
Obviously we want to do the best we can, but there are direct tradeoffs
<br>
between safety, and chances of rapid success.
<br>
<p>Peter
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3571.html">Damien Broderick: "Stross's post-Singularity novels"</a>
<li><strong>Previous message:</strong> <a href="3569.html">Ben Goertzel: "RE: supergoal stability"</a>
<li><strong>In reply to:</strong> <a href="3552.html">Eliezer S. Yudkowsky: "Voss's comments on Guidelines"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3570">[ date ]</a>
<a href="index.html#3570">[ thread ]</a>
<a href="subject.html#3570">[ subject ]</a>
<a href="author.html#3570">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
