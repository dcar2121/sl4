<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: singularity arrival estimate... idiocy... and human plankton</title>
<meta name="Author" content="ben goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: singularity arrival estimate... idiocy... and human plankton">
<meta name="Date" content="2002-05-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: singularity arrival estimate... idiocy... and human plankton</h1>
<!-- received="Fri May 17 12:55:59 2002" -->
<!-- isoreceived="20020517185559" -->
<!-- sent="Fri, 17 May 2002 10:21:41 -0600" -->
<!-- isosent="20020517162141" -->
<!-- name="ben goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: singularity arrival estimate... idiocy... and human plankton" -->
<!-- id="01C1FD8C.A3AA4BA0.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="singularity arrival estimate... idiocy... and human plankton" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> ben goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20singularity%20arrival%20estimate...%20idiocy...%20and%20human%20plankton"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Fri May 17 2002 - 10:21:41 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3702.html">ben goertzel: "RE: singularity arrival estimate..."</a>
<li><strong>Previous message:</strong> <a href="3700.html">Eugen Leitl: "Re: Singularity Arrival Estimate"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3715.html">Smigrodzki, Rafal: "RE: singularity arrival estimate... idiocy... and human plankton"</a>
<li><strong>Maybe reply:</strong> <a href="3715.html">Smigrodzki, Rafal: "RE: singularity arrival estimate... idiocy... and human plankton"</a>
<li><strong>Reply:</strong> <a href="3718.html">Eliezer S. Yudkowsky: "Re: singularity arrival estimate... idiocy... and human plankton"</a>
<li><strong>Maybe reply:</strong> <a href="3720.html">ben goertzel: "RE: singularity arrival estimate... idiocy... and human plankton"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3701">[ date ]</a>
<a href="index.html#3701">[ thread ]</a>
<a href="subject.html#3701">[ subject ]</a>
<a href="author.html#3701">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
****
<br>
As to why the Singularity is not better appreciated in mass society, that's
<br>
a whole other issue that I don't have time to chat about right now.
<br>
Obviously this is not the only example of &quot;mass blindness to the almost
<br>
obvious&quot; that we can find in human history or contemporary human society!!!
<br>
<p>-- Ben G
<br>
****
<br>
<p>Just to elaborate on this a little bit more.
<br>
<p>The Singularity is not a currently existing phenomenon; to appreciate it at 
<br>
this point requires the ability to extrapolate trends into the future.
<br>
<p>It is absurd to expect human society to be able to carry out this 
<br>
extrapolation, when in fact human society is so *manifestly bad* at seeing 
<br>
current facts that are right under its nose.
<br>
<p>Unfortunately, humans have a strong tendency to believe things that make 
<br>
them feel good, even to the extent of making themselves forget or not see 
<br>
evidence that would contradict their beliefs.  Also, this &quot;making the mind 
<br>
feel good&quot; by the belief of nonsense, is basically a local optimum of jo  
<br>
y-maximization, because a dropping-off of illusions is usually conducive to 
<br>
greater happiness, but the mind that's bound up in its local-joy-maximum of 
<br>
beliefs can rarely see this.
<br>
<p>Here are just a few random examples of collective human idiocy.  A full 
<br>
list would absorb a 10000-volume set...
<br>
<p>1) The  majority of Americans seem to believe it is somehow correct for 
<br>
marijuana to be illegal while alcohol and nicotine are legal.  Yet, this 
<br>
contradicts vast amounts of known pharmacology (showing pot is no more 
<br>
dangerous to the body) and also vast amounts of practical experience 
<br>
(showing that alcohol actually has at least as bad social consequences)
<br>
<p>2) For a long time, the vast majority of americans believed that African 
<br>
people were tremendously inferior to white people, more closely on a par 
<br>
with  monkeys than with whites. This was believed in spite of massive 
<br>
in-your-face evidence to the contrary.
<br>
<p>3) A huge number of people are *sure* that when they die they're going to 
<br>
be resurrected in a wonderland called Heaven.  Considering this as 
<br>
*possible* would be one thing -- what isn't possible after all? -- but 
<br>
*certainty* in this regard has got to strike anyone with a rational mind as 
<br>
sort of senseless, huh?
<br>
<p>4) Singularity aside, there is a decent likelihood that future tech will 
<br>
allow cryonically frozen bodies to be reanimated in some way.  Yet, there 
<br>
are less than 100 people cryonically frozen, out of the tens of millions 
<br>
who could afford it and DON'T believe in an afterlife...
<br>
<p>The inability to comprehend the notion of the Singularity is a just another 
<br>
example of the phenomenal mass stupidity of the human race.  Put crudely, 
<br>
our beefed-up monkey brains really suck, both at individual intelligence 
<br>
and at giving rise to collective intelligences.  And most humans and groups 
<br>
are too befuddled by this suckiness to even *realize* this.
<br>
<p>It often strikes me as remarkable that, given our truly terrifying 
<br>
stupidity, we have managed to accomplish so many complex and wonderful 
<br>
things.  Fortunately, the human brain is constructed so that each 
<br>
individual mind can potentially be brilliant in some ways yet idiotic in 
<br>
others.  But, as has often been observed, when a mass mind emerges from 
<br>
individual minds via current social interaction mechanisms, the common 
<br>
idiocies seem to emerge more vividly than the individual brilliances...
<br>
<p>Please don't take these comments as egomania.  I know, I'm a moron too, 
<br>
compared to the intelligence that's possible.  Like every other research 
<br>
scientist, I bang my head up against my own idiocy every single day, 
<br>
realizing that if I had a much better brain, the problems I struggle with 
<br>
for years could probably be solved instantly.
<br>
<p>However, I think that recognizing the lack of capability of the human brain 
<br>
can help give you an attitude of humbleness -- a recognition of how much 
<br>
there is that one does NOT know.  This openness to the possibilities of the 
<br>
universe BEYOND one's own current mindset -- and indeed beyond human 
<br>
comprehension altogether -- opens one to ideas like the Singularity.  In 
<br>
the time of slave ownership in the US, it took openmindedness and 
<br>
humbleness of mind to step beyond the received social ideas and think out 
<br>
of the box to reach an idea like &quot;Hmmm... maybe these black people are just 
<br>
basically the same as us white people.&quot;   Today, it takes a little 
<br>
openmindedness to think &quot;Hmmm... just 'cuz pot is illegal, maybe that 
<br>
doesn't mean it's worse than things that are legal... maybe the laws of our 
<br>
nation do not represent absolute truth....&quot;  But even this leeeeetle bit of 
<br>
open-mindedness is too much for most people.
<br>
<p>So, realizing that you're an idiot -- that ALL humans are idiots -- is the 
<br>
first step toward recognizing that one's own beliefs may be guided by 
<br>
emotional biases rather than by rational analysis of evidence.  But most 
<br>
people are even too idiotic and emotionally tangled-up to take this first 
<br>
step.
<br>
<p>Thus, I conjecture that up till the moment the Singularity is upon us, most 
<br>
people will not recognize it as a meaningful concept.  It is too big an 
<br>
idea for most peoples' limited intellects to encompass without a huge 
<br>
amount of effort (and most people have very lazy minds, as anyone who has 
<br>
ever taught in school on any level is aware), and it is dead-set against 
<br>
some of our biggest emotional biases (that we humans are somehow 
<br>
super-important in the grand scheme of things, a bias that comes straight 
<br>
out of our DNA and has been quite valuable for our DNA's survival).
<br>
<p>This message has probably been too disorganized as I'm typing it very fast 
<br>
in the midst of doing other things, but I'm sure you get the gist!!
<br>
<p>I am a bit of a cynic about human nature, but I am not at all a cynic about 
<br>
the long-term future of mind and intelligence.
<br>
<p>Compared to Eliezer, I do sort of doubt the long-term importance of keeping 
<br>
humans around.  Yeah, Friendly AI is *extremely* important to me, as a 
<br>
human being, in the &quot;short run&quot; (i.e. the next centuries or maybe 
<br>
milennia).  For a while there are going to be minds that want to remain 
<br>
human instead of transcending (Val Turchin called these &quot;human plankton&quot;, 
<br>
back in the late 60's when he started writing about this stuff... a comment 
<br>
on the relative advancement of humans and future uploaded intelligences). 
<br>
&nbsp;But will there still be human plankton a few thousand years down the line? 
<br>
&nbsp;&nbsp;I'm not so certain.  We humans are not that great, and our ability to 
<br>
obsolete and transcend ourselves is likely to be by far the best thing 
<br>
about us, in my view.
<br>
<p>yours
<br>
ben g
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3702.html">ben goertzel: "RE: singularity arrival estimate..."</a>
<li><strong>Previous message:</strong> <a href="3700.html">Eugen Leitl: "Re: Singularity Arrival Estimate"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3715.html">Smigrodzki, Rafal: "RE: singularity arrival estimate... idiocy... and human plankton"</a>
<li><strong>Maybe reply:</strong> <a href="3715.html">Smigrodzki, Rafal: "RE: singularity arrival estimate... idiocy... and human plankton"</a>
<li><strong>Reply:</strong> <a href="3718.html">Eliezer S. Yudkowsky: "Re: singularity arrival estimate... idiocy... and human plankton"</a>
<li><strong>Maybe reply:</strong> <a href="3720.html">ben goertzel: "RE: singularity arrival estimate... idiocy... and human plankton"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3701">[ date ]</a>
<a href="index.html#3701">[ thread ]</a>
<a href="subject.html#3701">[ subject ]</a>
<a href="author.html#3701">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
