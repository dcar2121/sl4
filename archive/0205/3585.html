<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: DGI Paper</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: DGI Paper">
<meta name="Date" content="2002-05-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: DGI Paper</h1>
<!-- received="Sat May 04 15:07:37 2002" -->
<!-- isoreceived="20020504210737" -->
<!-- sent="Sat, 04 May 2002 15:02:40 -0400" -->
<!-- isosent="20020504190240" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: DGI Paper" -->
<!-- id="3CD43050.6D7254E1@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJGEDKCGAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20DGI%20Paper"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat May 04 2002 - 13:02:40 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3586.html">Peter Voss: "RE: Review of Novamente &amp; a2i2"</a>
<li><strong>Previous message:</strong> <a href="3584.html">Ben Goertzel: "RE: Losing the race."</a>
<li><strong>In reply to:</strong> <a href="../0204/3344.html">Ben Goertzel: "RE: DGI Paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3597.html">Ben Goertzel: "RE: DGI Paper"</a>
<li><strong>Reply:</strong> <a href="3597.html">Ben Goertzel: "RE: DGI Paper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3585">[ date ]</a>
<a href="index.html#3585">[ thread ]</a>
<a href="subject.html#3585">[ subject ]</a>
<a href="author.html#3585">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; hi,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; Recently (I'm not sure if I was doing this during the whole of the DGI
</em><br>
<em>&gt; &gt; paper) I've been trying to restrict &quot;sensory&quot; to information produced by
</em><br>
<em>&gt; &gt; environmental sense organs.  However, there are other perceptions than
</em><br>
<em>&gt; &gt; this.  Imaginative imagery exists within the same working memory that
</em><br>
<em>&gt; &gt; sensory information flows into, but it's produced by a different source.
</em><br>
<em>&gt; &gt; Abstract imagery might involve tracking &quot;objects&quot;, which can be very
</em><br>
<em>&gt; &gt; high-level features of a sensory modality, but can also be features of no
</em><br>
<em>&gt; &gt; sensory modality at all.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think this is an acceptable use of the term &quot;perceptual&quot;, but only if you
</em><br>
<em>&gt; explicitly articulate that this is how you're using the word, since it
</em><br>
<em>&gt; differs the common usage in cognitive psychology.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Your usage is part of common English of course.  We can say, e.g. &quot;I sense
</em><br>
<em>&gt; you're upset about this,&quot; or &quot;I perceived I was going through some kind of
</em><br>
<em>&gt; change&quot; -- abstract sensations &amp; perceptions.  These common English usages
</em><br>
<em>&gt; aren't reflected in the way the terms have become specialized within
</em><br>
<em>&gt; cognitive psych, that's all.
</em><br>
<p>Y'know, I didn't understand what you were getting at here, at all, until
<br>
Barsalou's &quot;Perceptual Symbol Systems&quot; reminded me that &quot;perception&quot; can
<br>
mean very different things depending on your home civilization.  My home
<br>
civilization is what you've called the &quot;brain science branch of cognitive
<br>
science&quot;, whereas your home civilization is what you've called the &quot;computer
<br>
science branch of cognitive science&quot;.  Personally I would call these
<br>
civilizations &quot;cognitive science&quot; and &quot;computer science&quot;, but I'll use your
<br>
terms and call them BrainSci and CompSci.  Now of course the people in
<br>
CompSci would argue that they are using all kinds of inspiration from the
<br>
brain; for example, your paper on Hebbian Logic that attempts to show how
<br>
neurons can implement logical inference.  But if you actually grew up in
<br>
BrainSci culture, hearing about how neurons can efficiently implement
<br>
logical inference operations on small networks is enough to instantly
<br>
identify the speaker as a CompSci conspirator.
<br>
<p>The point is that in CompSci culture there is a traditional distinction
<br>
between &quot;cognition&quot; and &quot;perception&quot;.  Enlightened CompSciFolk are the ones
<br>
who admit that cognition interacts with perception in some way.  On the
<br>
BrainSci side of the divide, &quot;perception&quot; has a very different and much more
<br>
inclusive meaning, and it is not automatically assumed that &quot;cognition&quot; and
<br>
&quot;perception&quot; are modular subsystems or that they use different underlying
<br>
representations.
<br>
<p>The way I'm using &quot;perception&quot; is not exactly standard in BrainSci
<br>
civilization, but it is pretty close to standard usage, I think.  I realize
<br>
that many readers of &quot;Real AI&quot; will hail from CompSci and that if my usage
<br>
of &quot;perception&quot; throws up a stumbling block to them, I need to at least
<br>
mention the source of the difficulty.
<br>
<p>If you still think this is just me, I recommend reading Barsalou's
<br>
&quot;Perceptual Symbol Systems&quot; or the opening chapters of Kosslyn's &quot;Image and
<br>
Brain&quot;.  There is a genuine civilizational divide and the bits of BrainSci
<br>
culture that leak across to the other side are much more fragmentary than
<br>
the CompSci culture realizes.
<br>
<p><em>&gt; These issues are impossible to avoid in getting scientific about the mind.
</em><br>
<em>&gt; For instance, the way we use the word &quot;reason&quot; in Novamente includes things
</em><br>
<em>&gt; that logicians don't all consider &quot;reason&quot; -- very speculative kinds of
</em><br>
<em>&gt; reasoning.
</em><br>
<p>Again, without meaning any offense, this instantly identifies you as a
<br>
CompSci speaker.  For me, Tversky and Kahneman's decision theory is a
<br>
central prototype of what I call &quot;cognitive psychology&quot;.  For you, formal
<br>
logic is a central prototype of what you call &quot;cognitive psychology&quot;.  By
<br>
the standards of CompSci civilization, Novamente uses a dangerous, sexy kind
<br>
of logical inference, bordering on Here There Be Dragons territory.  By the
<br>
standards of BrainSci civilization, Novamente's logical inference mimics a
<br>
small, stereotypically logical subset of the kinds of reasoning that people
<br>
are known to use.
<br>
<p>I realize that you consider Hebbian Logic, emergence and chaos theory, and
<br>
Novamente's expanded inference mechanisms to be clear proof of consilient
<br>
integration with BrainSci, but from BrainSci culture these things look like
<br>
prototypical central cases of CompSci.  There is a gap here, it's not just
<br>
me, and it's much bigger than you think.
<br>
<p><em>&gt; &gt; Right - but it's associated with the behaviors, in abstract imagery, of
</em><br>
<em>&gt; &gt; other math concepts.  That's why you can't discover complex math concepts
</em><br>
<em>&gt; &gt; without knowing simple math concepts; the complex math concepts are
</em><br>
<em>&gt; &gt; abstracted from the abstract imagery for complex behaviors or complex
</em><br>
<em>&gt; &gt; relations of simple math concepts.  But there is still imagery.  It is not
</em><br>
<em>&gt; &gt; purely conceptual; one is imagining objects that are abstract objects
</em><br>
<em>&gt; &gt; instead of sensory objects, and imagining properties that are abstract
</em><br>
<em>&gt; &gt; properties instead of sensory properties, but there is still
</em><br>
<em>&gt; &gt; imagery there.
</em><br>
<em>&gt; &gt; It can be mapped onto the visual imagery of the blackboard and so on.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I am not at all sure you are right about this.  I think that abstract
</em><br>
<em>&gt; reasoning can sometimes proceed WITHOUT the imagining of specific objects.
</em><br>
<em>&gt; I think what you're describing is just *one among many modes* of abstract
</em><br>
<em>&gt; reasoning.  I think sometimes we pass from abstraction to abstraction
</em><br>
<em>&gt; without the introduction of anything that is &quot;imagery&quot; in any familiar
</em><br>
<em>&gt; sense.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And the mapping onto the visual blackboard may be a very very distortive
</em><br>
<em>&gt; mapping, which does not support the transformations required for accurate
</em><br>
<em>&gt; inference in the given abstract domain.  Visual thinking is not suited for
</em><br>
<em>&gt; everything -- e.g. it works well for calculus and relatively poorly for
</em><br>
<em>&gt; abstract algebra (which involves structures whose symmetries are very
</em><br>
<em>&gt; DIFFERENT from those of the experienced 3D world).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Again, I think if you're right, it can only be by virtue of having a very
</em><br>
<em>&gt; very very general notion of &quot;imagery&quot; which you haven't yet fully
</em><br>
<em>&gt; articulated.
</em><br>
<p>No, what I mean by imagery is working memory in sensory modalities, and
<br>
working memory in some nonsensory but still perceptual modalities, such as
<br>
our crossmodal number sense, our crossmodal object-property tracker, and
<br>
various introspective perceptions.  I would argue that most of what you
<br>
consider &quot;abstract&quot; thinking is built up from the crossmodal object sense
<br>
and introspective perceptions.  The key point is that this abstract imagery
<br>
is pretty much the same &quot;kind of thing&quot; as sensory imagery; it interacts
<br>
freely with sensory imagery as an equal and interacts with the rest of the
<br>
mind in pretty much the same way as sensory imagery.
<br>
<p>And when I say imagery, I do mean depictive imagery - if you close your eyes
<br>
and imagine a cat, then there is actually a cat-shaped activation pattern in
<br>
your visual cortex, adjusting for the logarithmic scaling of retinotopic
<br>
maps in the visual system.  This is supported by convergent evidence from
<br>
functional neuroimaging, pathology of visual deficits, single-cell recording
<br>
in animal subjects, functional neuroanatomy, and theoretical consilience
<br>
with a very well-established theory of the computational function performed
<br>
by visual areas.  It is not a &quot;Cartesian theatre&quot; which is a prior
<br>
unacceptable; it is an established fact of neuroscience.  (If I sound a bit
<br>
emphatic here, it's because of that audience questioner at the Foresight
<br>
Gathering who claimed that all visual imagery was a Cartesian conspiracy; I
<br>
hunted him down afterward and gave him a reference to Kosslyn's &quot;Image and
<br>
Brain&quot;.)
<br>
<p>Novamente uses the traditional (CompSci) view of cognition as a process
<br>
separate from the depictive imagery of sensory perception, in which thoughts
<br>
about cats are represented by propositions that include a cat symbol which
<br>
receives a higher activation level.  Novamente's entire thought processes
<br>
are propositional rather than perceptual.  So it's not surprising that the
<br>
idea of mental imagery as an entire level of organization may come as a
<br>
shock.
<br>
<p><em>&gt; &gt; Abstract imagery uses non-depictive, often cross-modal
</em><br>
<em>&gt; &gt; layers that
</em><br>
<em>&gt; &gt; are nonetheless connected by the detector/controller flow to the depictive
</em><br>
<em>&gt; &gt; layers of mental imagery.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Again, I think this is a very important *kind* of abstract thought, but not
</em><br>
<em>&gt; the only kind.
</em><br>
<p>And I think that the kind of abstract thought I think you're thinking of,
<br>
implemented by Novamente using propositions, is implemented using the above
<br>
kind of mental imagery.  To visualize this, or any real thought, requires
<br>
visualizing an immense amount of interlocking mental machinery;
<br>
propositional terms may be much easier to visualize but they are not how
<br>
thought actually works.
<br>
<p><em>&gt; &gt; Maybe I should put in a paragraph somewhere about &quot;sensory
</em><br>
<em>&gt; &gt; perception&quot; as a
</em><br>
<em>&gt; &gt; special case of &quot;perception&quot;.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Definitely.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; I think that human concepts don't come from mixing together the internal
</em><br>
<em>&gt; &gt; representations of other concepts.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think that some human concepts do, and some don't.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is based partly on introspection: it feels to me that many of my
</em><br>
<em>&gt; concepts come that way.
</em><br>
<p>How can you possibly introspect on whether your concepts come about by
<br>
mixing internal representations?  Sure, many of your new concepts come from
<br>
mixing two previous concepts, but how could you possibly tell whether the
<br>
mixing occurred by mixing their underlying neural representations?
<br>
<p><em>&gt; Now, you can always maintain that my introspection is inaccurate.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Of course, your intuition must also be based largely on your own
</em><br>
<em>&gt; introspection (cog sci just doesn't take us this far, yet), which may also
</em><br>
<em>&gt; be inaccurate.
</em><br>
<p>True, but at least my introspection is based on my alleged observation and
<br>
extrapolation from events which BrainSci says *should* be open to my
<br>
introspection.
<br>
<p><em>&gt; Certainly the combinatory aspect of cognition that I describe has
</em><br>
<em>&gt; significant neurological support.  Edelman's books since Neural Darwinism,
</em><br>
<em>&gt; among other sources, emphasize this aspect of the formation of neural maps.
</em><br>
<p>I haven't read Edelman's books, but are you sure that it really emphasizes
<br>
the evolutionary formation of long-term neural structures rather than the
<br>
evolution of short-term neural patterns?  Evolutionary hypotheses for the
<br>
origin of activation patterns are a standard hypothesis, most notably by
<br>
William Calvin.  I can't ever remember hearing an evolutionary hypothesis
<br>
for the creation of new long-term neural structures; the closest thing to
<br>
this is the selective die-off of most connections during the brain's initial
<br>
self-wiring, which is a case of survival of the stable, not of differential
<br>
replication.  Given that Novamente's long-term content has the same
<br>
propositional representation as its thoughts, I can see how the confusion
<br>
might arise; so, are you sure this is really what Edelman said?
<br>
<p><em>&gt; &gt; I think that's an AI idiom
</em><br>
<em>&gt; &gt; which is not
</em><br>
<em>&gt; &gt; reflected in the human mind.  Humans may be capable of faceting
</em><br>
<em>&gt; &gt; concepts and
</em><br>
<em>&gt; &gt; putting the facets together in new ways, like &quot;an object that smells like
</em><br>
<em>&gt; &gt; coffee and tastes like chocolate&quot;, but this is (I think) taking apart the
</em><br>
<em>&gt; &gt; concepts into kernels, not mixing the kernel representations together.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, Edelman and I disagree with you, he based mostly on his neurological
</em><br>
<em>&gt; theory, I based mostly on my introspective intuition.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What support do you have for your belief, other than that when you
</em><br>
<em>&gt; introspect you do not feel yourself to be combining concepts in such a way?
</em><br>
<p>Well, it's certainly consistent with the separate neurological areas for
<br>
association cortex in superior posterior temporal areas, shape/color/texture
<br>
recognition in inferior temporal areas, and depictive imagery represented in
<br>
the buffer of visual cortical areas.  It's not clear to me how your approach
<br>
- if it's not one of those cases where you would simply say &quot;But I don't
<br>
think we *should* do it the human way&quot; - would explain the neurological
<br>
differentiation here.
<br>
<p>Basically, I have a model of how concepts work in which concept formation
<br>
inherently requires that certain internally specialized subsystems operate
<br>
together in a complex dance - creating new concepts by mixing their
<br>
internals together is an intriguing notion but I have no need for that
<br>
hypothesis with respect to humans, though it might be well worth trying in
<br>
AIs as long as all the complex machinery is still there.
<br>
<p>How exactly would neural maps reproduce internally, anyway?  It's clear how
<br>
activation patterns could do this, but I can't recall hearing offhand of a
<br>
postulated mechanism whereby a neural structure can send signals to another
<br>
neural area that results in the long-term potentiation of a duplicate of
<br>
that neural structure.
<br>
<p><em>&gt; &gt; Now it may perhaps be quite useful to open up concepts and play with their
</em><br>
<em>&gt; &gt; internals!  I'm just saying that I don't think humans do it that way and I
</em><br>
<em>&gt; &gt; don't think an AI should start off doing it that way.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; My intuition is quite otherwise.  I think that very little creative
</em><br>
<em>&gt; innovation will happen in a mind that does not intercombine concepts by
</em><br>
<em>&gt; &quot;opening them up and playing with their internals.&quot;
</em><br>
<p>I think your intuition on this subject derives from Novamente (a) having a
<br>
propositional representation of concepts and (b) lacking all the complex
<br>
interacting machinery that's necessary to form new concepts without playing
<br>
with their internals.  In fact, I would say that Novamente's concepts don't
<br>
have any internals.
<br>
<p><em>&gt; &gt; &gt; I don't understand why you think a baby AI can't learn to see the Net
</em><br>
<em>&gt; &gt; &gt; incrementally.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; It doesn't have a tractable fitness landscape.  No feature structure to
</em><br>
<em>&gt; &gt; speak of.  No way to build up complex concepts from simple concepts.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think this is quite wrong, actually.  There is an incredible richness, for
</em><br>
<em>&gt; example, in all the financial and biological databases available online.
</em><br>
<em>&gt; Trading the markets is one way to interact with the world online, a mode of
</em><br>
<em>&gt; interaction that incorporates all sorts of interesting data.  Chatting with
</em><br>
<em>&gt; biologists (initially in a formal language) about info in bio databases is
</em><br>
<em>&gt; another.  I think an AI has got to start with nonlinguistic portions of the
</em><br>
<em>&gt; Net, then move to linguistic portions that are closely tied to the
</em><br>
<em>&gt; nonlinguistic portions it knows (financial news, Gene Ontology gene
</em><br>
<em>&gt; descriptions, etc.).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think the implicit fitness landscapes in the financial trading and
</em><br>
<em>&gt; collaborative biodatabase analysis spaces are quite tractable.
</em><br>
<p>I disagree, but I think our very different perspectives on complexity and
<br>
simplicity are showing.  To me, &quot;financial trading&quot; and &quot;biodatabase
<br>
analysis&quot; are utterly separate from &quot;The Net&quot; as an environment; they have
<br>
different sensory structures, different behaviors, different invariants,
<br>
different regularities, different everything.  That you would consider these
<br>
subcategories of &quot;The Net&quot; because they are reachable over TCP/IP
<br>
connections says to me that we have extremely different ideas of what
<br>
experiential learning is about, and especially about the kind of innate
<br>
specialized complexity needed for experiential learning in a domain.  I
<br>
guess if you're trying to learn all possible environments using the same
<br>
dynamics, it could make sense to regard financial trading as a part of the
<br>
'Net.  I just don't think it'll work, that's all.
<br>
<p><em>&gt; &gt; &gt; &quot;Differential functions on [-5,5] whose third derivative is
</em><br>
<em>&gt; &gt; confined to the
</em><br>
<em>&gt; &gt; &gt; interval [0,1]&quot;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; This isn't much of a concept until it has a name.  Let's call a function
</em><br>
<em>&gt; &gt; like this a &quot;dodomorphic fizzbin&quot;.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, i disagree -- I have concepts like this all the time with no names.
</em><br>
<em>&gt; Naming such a concept only occurs, for me, when I want to communicate it to
</em><br>
<em>&gt; others, or write it down for myself.
</em><br>
<p>This is again one of those things that threw me completely until I paused
<br>
and tried to visualize you visualizing Novamente.  What you are calling a
<br>
&quot;concept&quot;, I would call a &quot;thought&quot;.  In Novamente, you can take a complex
<br>
structure of nodes and links, and treat it as a node; Novamente has the same
<br>
representation for concepts and concept structures, where &quot;concepts&quot; are
<br>
really just special cases of concept structures with one node.  In DGI these
<br>
things not only have different representations, they live on different
<br>
levels of organization.
<br>
<p><em>&gt; &gt; &gt; then how is this concept LEARNED?  I didn't learn this, I just
</em><br>
<em>&gt; &gt; &gt; INVENTED it.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; You learned it by inventing it.  The invention process took place on the
</em><br>
<em>&gt; &gt; deliberative level of organization.  The learning process took
</em><br>
<em>&gt; &gt; place on the
</em><br>
<em>&gt; &gt; concept level of organization and it happened after the invention.  You
</em><br>
<em>&gt; &gt; created the mental imagery and then attached it to a concept.
</em><br>
<em>&gt; &gt; These things
</em><br>
<em>&gt; &gt; happen one after the other but they are still different cognitive
</em><br>
<em>&gt; &gt; processes
</em><br>
<em>&gt; &gt; taking place on different levels of organization.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Somehow, you're telling me it's not a &quot;concept&quot; until it's been named??
</em><br>
<p>It's certainly not a full concept that can be used as an element in other
<br>
concept structures.  How would you invoke it - as an element in a concept
<br>
structure, and not just a memory - if you can't name it?  Again, not the way
<br>
Novamente does it, where you can always point to any propositional structure
<br>
whether or not it has a name.
<br>
<p><em>&gt; I don't see why such a concept has to be &quot;attached&quot; to anything to become a
</em><br>
<em>&gt; &quot;real concept&quot;, it seems to me like it's a &quot;real concept&quot; as soon as I start
</em><br>
<em>&gt; thinking about it...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I guess I still don't fully understand your notion of a &quot;concept&quot;
</em><br>
<p>Does it help if I note that I distinguish between &quot;concept&quot; and &quot;concept
<br>
structure&quot; and that neither is analogous to Novamente's propositional
<br>
structures?
<br>
<p><em>&gt; &gt; 1:  &quot;I need to get this paper done.&quot;
</em><br>
<em>&gt; &gt; 2:  &quot;I sure want some ice cream right now.&quot;
</em><br>
<em>&gt; &gt; 3:  &quot;Section 3 needs a little work on the spelling.&quot;
</em><br>
<em>&gt; &gt; 4:  &quot;I've already had my quota of calories for the day.&quot;
</em><br>
<em>&gt; &gt; 5:  &quot;Maybe I should replace 'dodomorphic' with 'anhaxic plorm'.&quot;
</em><br>
<em>&gt; &gt; 6:  &quot;If I exercised for an extra hour next week, that would make
</em><br>
<em>&gt; &gt; up for it.&quot;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; If these thoughts are all (human idiom) mental sentences in the internal
</em><br>
<em>&gt; &gt; narrative, I wouldn't expect them to be pronounced simultaneously
</em><br>
<em>&gt; &gt; by a deep
</em><br>
<em>&gt; &gt; adult voice and a squeaky child voice.  Rather I would expect them to be
</em><br>
<em>&gt; &gt; interlaced, even though {1, 3, 5} relate to one piece of open goal imagery
</em><br>
<em>&gt; &gt; and {2, 4, 6} relates to a different piece of goal imagery.  So the
</em><br>
<em>&gt; &gt; deliberative tracks {1, 3, 5} and {2, 4, 6} are simultaneous, but the
</em><br>
<em>&gt; &gt; thoughts 1, 2, 3, 4, 5, 6 occur sequentially.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is not how my mind works introspectively.  In my mind, 1, 2 and 3
</em><br>
<em>&gt; appear to me to occur simultaneously.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Now, you can tell me that I'm deluded and they REALLY occur sequentially in
</em><br>
<em>&gt; my  mind but I don't know it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But I'm not going to believe you unless you have some REALLY HARD
</em><br>
<em>&gt; neurological proof.
</em><br>
<p>And *this* one felt like running up against a brick wall.  1, 2, and 3 occur
<br>
simultaneously?  What on Earth?  At this point I started to wonder
<br>
half-seriously whether the placebo effect in cognitive science was powerful
<br>
enough to sculpt our minds into completely different architectures through
<br>
the conformation of cognition to our respective expectations.
<br>
<p>After the first few moments of sheer, blank incomprehension, though, I
<br>
remembered that in Novamente there are, in fact, a great many different
<br>
propositional structures being created and activated at any given time, and
<br>
I figured that you'd automatically mapped 1, 2, and 3 to Novamente's
<br>
propositional structures.  I am not saying that you can't simultaneously
<br>
want to get a paper done, want some ice cream, and notice that section 3
<br>
needs work on the spelling.  That happens all the time.  What I'm saying is
<br>
that you cannot simultaneously enunciate the mental sentences 1, 2, and 3. 
<br>
Forming a concept structure, linearizing it as a mental sentence in
<br>
linguistic form, and speaking it internally, is a different and more complex
<br>
mental process than background events in mental imagery.  An AI may skip the
<br>
linguistic translation but will probably still need to distinguish
<br>
activating concept structures from background events.
<br>
<p>Cognitive events are multiplexed but only one of them gets internally
<br>
enunciated as a mental sentence at any given time.
<br>
<p><em>&gt; Pei Wang (one of my chief Webmind collaborators) and I often spent a long
</em><br>
<em>&gt; time arriving at a mutually comprehensible language.  Then, once we had, we
</em><br>
<em>&gt; could make 75% of our differences go away.  The other 25% we just had to
</em><br>
<em>&gt; chalk up to different intuitions, and move on...
</em><br>
<p>I think the mix here may be more like 25%/75% if not 15%/85%.  The reason
<br>
Novamente feels so alien to me is that, in my humble opinion, you're doing
<br>
everything wrong, and trying to model the emergent qualities of a mind built
<br>
using the wrong components and the wrong levels of organization is...
<br>
really, really hard.  I consider it a warmup for trying to build AI, like
<br>
the mental gymnastics it took to model you modeling DGI using your model of
<br>
Novamente as a lens.  If you're right and I'm not modeling Novamente
<br>
correctly, I don't envy you the job of modeling me modeling Novamente using
<br>
DGI so that you can figure out where I went wrong.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3586.html">Peter Voss: "RE: Review of Novamente &amp; a2i2"</a>
<li><strong>Previous message:</strong> <a href="3584.html">Ben Goertzel: "RE: Losing the race."</a>
<li><strong>In reply to:</strong> <a href="../0204/3344.html">Ben Goertzel: "RE: DGI Paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3597.html">Ben Goertzel: "RE: DGI Paper"</a>
<li><strong>Reply:</strong> <a href="3597.html">Ben Goertzel: "RE: DGI Paper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3585">[ date ]</a>
<a href="index.html#3585">[ thread ]</a>
<a href="subject.html#3585">[ subject ]</a>
<a href="author.html#3585">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
