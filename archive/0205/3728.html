<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: AI in &lt;what?&gt;</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: AI in &lt;what?&gt;">
<meta name="Date" content="2002-05-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: AI in &lt;what?&gt;</h1>
<!-- received="Sun May 19 09:55:58 2002" -->
<!-- isoreceived="20020519155558" -->
<!-- sent="Sun, 19 May 2002 07:41:30 -0600" -->
<!-- isosent="20020519134130" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: AI in &lt;what?&gt;" -->
<!-- id="LAEGJLOGJIOELPNIOOAJGEEECIAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="F891nBUfjtX7bjsH4aU0001ef7e@hotmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20AI%20in%20&lt;what?&gt;"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sun May 19 2002 - 07:41:30 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3729.html">Michael Roy Ames: "Re: AI in &lt;what?&gt;"</a>
<li><strong>Previous message:</strong> <a href="3727.html">Justin Corwin: "AI in &lt;what?&gt;"</a>
<li><strong>In reply to:</strong> <a href="3727.html">Justin Corwin: "AI in &lt;what?&gt;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3729.html">Michael Roy Ames: "Re: AI in &lt;what?&gt;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3728">[ date ]</a>
<a href="index.html#3728">[ thread ]</a>
<a href="subject.html#3728">[ subject ]</a>
<a href="author.html#3728">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; As I see it, there are four reasons an AI needs an environment:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1. For training the AImind to accept input.
</em><br>
<em>&gt; 2. For allowing the AImind to develop mental skills in an interactive
</em><br>
<em>&gt; setting.(action/response kind of stuff)
</em><br>
<em>&gt; 3. Possibly for keeping the AImind close to us, in it's mental landscape.
</em><br>
<em>&gt; While it may be possible to make a mind with an entirely disembodied
</em><br>
<em>&gt; intelligence, just I/O ports and internet access, such a mind may have
</em><br>
<em>&gt; problems relating to us, as physically oriented many of our
</em><br>
<em>&gt; language-objects
</em><br>
<em>&gt; are.
</em><br>
<em>&gt; 4. To allow the AImind to more effective when it begins acting in
</em><br>
<em>&gt; the real
</em><br>
<em>&gt; world. If it has to extrapolate 'everything' it'll take longer
</em><br>
<em>&gt; and be more
</em><br>
<em>&gt; error-prone.
</em><br>
<p>You have left out the potential necessity of socialization for the
<br>
development of the self.
<br>
<p>I guess it is somewhat implicit in 2-3, but I mean something a little
<br>
stronger.
<br>
<p>I mean that interacting with *other minds* is a key part of the process of
<br>
learning how to deal with *one's own mind*.
<br>
<p>Socialization may not be the *only* path to self-understanding, but, it is
<br>
*one* path as shown by human developmental psychology, and I have an
<br>
intuitive feeling (perhaps overly anthropomorphic, hard to tell) that it is
<br>
a VERY GOOD path in an objective sense, not just for human beings.
<br>
<p><em>&gt; My basic conclusion is that the optimal tradeoff seems to be in a
</em><br>
<em>&gt; concrete
</em><br>
<em>&gt; instatiation of the AI in a virtual or sandboxed environment
</em><br>
<em>&gt; slightly lower
</em><br>
<em>&gt; in detail than our own. It seems to offer the best of all the
</em><br>
<em>&gt; options, while
</em><br>
<em>&gt; raising the complexity to a reasonable (if still ridiculous)
</em><br>
<em>&gt; level. I would
</em><br>
<em>&gt; like an AI that has a concrete concept of itself in space, and
</em><br>
<em>&gt; learns in an
</em><br>
<em>&gt; environment similar to my own. It seems that such an AI would be the most
</em><br>
<em>&gt; useful, relatable, and intelligent; given other tradeoffs.
</em><br>
<p>My own intuition is that
<br>
<p>1) Of course, a great diversity of powerful sense-inputs and actuators is a
<br>
*good thing*
<br>
<p>2) Unlike Eliezer, I think that interacting with humans and software agents
<br>
on the Net [considered broadly, including financial datafeeds, biodatabases,
<br>
weather satellite data etc. etc., not just Web pages], will probably provide
<br>
an adequate environment for AGI, though it certainly won't lead to a
<br>
human-like mind
<br>
<p>3) I think that in the early stages of an AGI project (and yes, Novamente is
<br>
*still* early-stage, because we don't have our mind-engine fully implemented
<br>
yet, not by a long shot.  Webmind AI Engine was almost out of the early
<br>
stage of implementation &amp; software testing and into the mid-stage of basic
<br>
testing and teaching, but I think it would not have passed thru the
<br>
mid-stage due to various implementation and design issues), it is best NOT
<br>
to focus on the building of elaborate perception and action systems.  There
<br>
are tremendous resources devoted to this already in the academic and
<br>
business worlds: robotics, computer vision, etc.   One thing the robotics
<br>
and computer vision (etc.) algorithms out there now LACK is serious feedback
<br>
from adaptive cognition.  I think it makes sense to get cognition &quot;basically
<br>
working&quot; in a very simple perception/action environment, and then as one
<br>
enters the mid-state of seriously teaching one's AGI, THEN one works on more
<br>
serious perception &amp; action modules, aimed at giving one's AGI a richer and
<br>
more humanlike subjective environment.  Of course, it is also possible that
<br>
by this stage one has a deeper perspective on AGI, which tells one that so
<br>
much perception/action work is not so necessary ;-&gt;
<br>
<p><p>Partly, one's view on this issue depends on how humanlike one wants one's
<br>
AGI to be.  I am not aiming at a humanlike AGI, just a very smart one,
<br>
because I think that the latter is an easier problem.  Compared to more
<br>
closely brain-inspired approaches like DGI and A2I2, my approach has less
<br>
data to use for motivation (as the human brain is only a loose inspiration
<br>
rather than a close guide), but has a lot fewer problems to solve in terms
<br>
of efficient harmonization with current hardware platforms (though these
<br>
problems are *still* very severe even for Novamente and we've put a lot of
<br>
work in on them).
<br>
<p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3729.html">Michael Roy Ames: "Re: AI in &lt;what?&gt;"</a>
<li><strong>Previous message:</strong> <a href="3727.html">Justin Corwin: "AI in &lt;what?&gt;"</a>
<li><strong>In reply to:</strong> <a href="3727.html">Justin Corwin: "AI in &lt;what?&gt;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3729.html">Michael Roy Ames: "Re: AI in &lt;what?&gt;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3728">[ date ]</a>
<a href="index.html#3728">[ thread ]</a>
<a href="subject.html#3728">[ subject ]</a>
<a href="author.html#3728">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
