<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI in &lt;what?&gt;</title>
<meta name="Author" content="Michael Roy Ames (michaelroyames@hotmail.com)">
<meta name="Subject" content="Re: AI in &lt;what?&gt;">
<meta name="Date" content="2002-05-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI in &lt;what?&gt;</h1>
<!-- received="Sun May 19 12:54:43 2002" -->
<!-- isoreceived="20020519185443" -->
<!-- sent="Sun, 19 May 2002 12:40:30 -0700" -->
<!-- isosent="20020519194030" -->
<!-- name="Michael Roy Ames" -->
<!-- email="michaelroyames@hotmail.com" -->
<!-- subject="Re: AI in &lt;what?&gt;" -->
<!-- id="OE49NYojnS41BXIoHJT0000bcee@hotmail.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="F891nBUfjtX7bjsH4aU0001ef7e@hotmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Roy Ames (<a href="mailto:michaelroyames@hotmail.com?Subject=Re:%20AI%20in%20&lt;what?&gt;"><em>michaelroyames@hotmail.com</em></a>)<br>
<strong>Date:</strong> Sun May 19 2002 - 13:40:30 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3730.html">Ben Goertzel: "Complexity of AGI"</a>
<li><strong>Previous message:</strong> <a href="3728.html">Ben Goertzel: "RE: AI in &lt;what?&gt;"</a>
<li><strong>In reply to:</strong> <a href="3727.html">Justin Corwin: "AI in &lt;what?&gt;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3733.html">Justin Corwin: "RE: AI in &lt;what?&gt;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3729">[ date ]</a>
<a href="index.html#3729">[ thread ]</a>
<a href="subject.html#3729">[ subject ]</a>
<a href="author.html#3729">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Justin Corwin wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; As I see it, there are four reasons an AI needs an environment:
</em><br>
<em>&gt;
</em><br>
<p>Your note (admittedly hastily typed) seems to meld/mix-up/confuse(?) two
<br>
concepts:  an Artificial Environment, and a Sensory Modality.  I have found
<br>
it very helpful to keep these two concepts very separate in my mind, when
<br>
thinking about AI Learning.  What do you mean by environment in this case?
<br>
The Artificial Enviroment that we will present to the AI for early learning,
<br>
or the Sensory Modalities we will implement?  Or are you talking about both
<br>
at once?
<br>
<p><em>&gt;
</em><br>
<em>&gt; 1. For training the AImind to accept input.
</em><br>
<em>&gt; 2. For allowing the AImind to develop mental skills in an interactive
</em><br>
<em>&gt; setting.(action/response kind of stuff)
</em><br>
<em>&gt; 3. Possibly for keeping the AImind close to us, in it's mental landscape.
</em><br>
<em>&gt; While it may be possible to make a mind with an entirely disembodied
</em><br>
<em>&gt; intelligence, just I/O ports and internet access, such a mind may have
</em><br>
<em>&gt; problems relating to us, as physically oriented many of our
</em><br>
language-objects
<br>
<em>&gt; are.
</em><br>
<em>&gt; 4. To allow the AImind to more effective when it begins acting in the real
</em><br>
<em>&gt; world. If it has to extrapolate 'everything' it'll take longer and be more
</em><br>
<em>&gt; error-prone.
</em><br>
<em>&gt;
</em><br>
<p>I think you missed this important reason:
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;5. To ground concepts in physical reality. (Or a physical reality
<br>
simulation).
<br>
<p>Although your item 4. seems to infer this, I suspect that grounding is an
<br>
essential requirement for stable mental development, almost from the start
<br>
of a Baby AI's existence.  Here are two arguments to support that view:
<br>
<p>i. Human children learn many many things by trial-and-error.  Developing
<br>
humans have reality thrust at them every waking moment... those that ignore
<br>
reality are classified as Mentally Ill, and do not survive without help.  I
<br>
think an AI would have similar 'survival' difficulties if ve didn't have
<br>
real-world input.  (This is a brief argument, but there is a lot more that
<br>
could be said about it)
<br>
<p>ii. A being who's perceptions are cut off from reality may develop quite
<br>
well for a while, but ve's mind's development would be based on whatever
<br>
simulation was being run, a simulation that would inevitabley be less
<br>
complex than reality.  Therefore, there is a large probablility that lessons
<br>
learned in the simulation, would be invalid in reality.  How much of a
<br>
problem it would be for the AI to adapt to reality later, is a crucial issue
<br>
when trying to design a &quot;Womb Environment&quot;.
<br>
<p><em>&gt;
</em><br>
<em>&gt; There are, of course, downsides. Providing an environment for an AImind
</em><br>
ups
<br>
<em>&gt; complexity.
</em><br>
<em>&gt;
</em><br>
<p>This downside is really non-existant IMO, if the AI is going to have *any*
<br>
ability to interact.  Either you have to have real-world sensory modalities
<br>
(SM), or SM's that perceive an Artificial Environment.  It would seem to be
<br>
about the same implementational complexity either way... why do I intuit
<br>
this?  With an Artificial Environment you have to build the environment (big
<br>
complexity hit) and the SM to perceive it (moderate complexity hit).  With a
<br>
real-world envronment you get the environment for free (zero work) but the
<br>
SM to perceive that environment will require more complexity (big complexity
<br>
hit).
<br>
<p><p><em>&gt;
</em><br>
<em>&gt; But do richer environments really bring a quantifiable advantage?
</em><br>
<em>&gt;
</em><br>
<p>I would say: sometimes.  I strongly doubt that your 'twice as acute' vision
<br>
example would give much of a quantifiable advantage to _thought_ processes.
<br>
Although it would no doubt be very useful in a hunter-gatherer setting.
<br>
However, other vision modification options *may* give an mental advantage.
<br>
How about these?
<br>
<p>a) telescoping vision - allowing selectable zoom on a narrow field of
<br>
vision.
<br>
b) 360 degree spherical vision - allowing simultaneous observation in all
<br>
directions.
<br>
c) wide spectrum vision - allowing perception of selectable EM frequencies.
<br>
d) ... fill in your own favourite ...
<br>
<p>These types of vision enhancements, and thier accompanying SM would seem to
<br>
provide new ways to 'see', or 'imagine', or 'translate' concepts in a
<br>
mind... and that's just vision!
<br>
<p><em>&gt;
</em><br>
<em>&gt; So richer environments may in fact lead to richer mental structure.
</em><br>
<em>&gt;
</em><br>
<p>Agreement!
<br>
<p><em>&gt;
</em><br>
<em>&gt; I believe that in this case, complexity is too important to let go, and
</em><br>
the
<br>
<em>&gt; design hit should be taken.
</em><br>
<em>&gt;
</em><br>
<p>Agreement!
<br>
<p><em>&gt;
</em><br>
<em>&gt; We don't want an AImind we have to relate to
</em><br>
<em>&gt; using 786432 pixel 2D metaphors. That would be annoying, and may represent
</em><br>
a
<br>
<em>&gt; difficulty the AI may have trouble fixing when in the Self-Modification
</em><br>
<em>&gt; stage.
</em><br>
<em>&gt;
</em><br>
<p>I don't understand your point here.  What is wrong with 2D metaphors?  If
<br>
you can get an AI that far, then that's freakin' great!  Tackle 1D first,
<br>
then 2D, then 3 and 4...
<br>
<p><em>&gt;
</em><br>
<em>&gt; Thus, environmental richness may play a crucial factor in
</em><br>
<em>&gt; allowing emergent mindstructures to emerge at all.
</em><br>
<em>&gt;
</em><br>
<p>Indeed.  This was one of the major points of GISAI.  One needs to have
<br>
several ways of looking at the world (several SM's) in order to have a
<br>
chance of solving non-obvious problems.  The ability to map one SM onto
<br>
another, and glean meaningful inferences from the new mapping, seems to be
<br>
one of the key ways that humans think.  As our only model of intelligence,
<br>
we would do well to try and reproduce that human mental effect in code.
<br>
<p><em>&gt;
</em><br>
<em>&gt; Upside, really complexish environments are probably beyond
</em><br>
<em>&gt; us anyway.
</em><br>
<em>&gt;
</em><br>
<p>Not for long I hope.
<br>
<p>Michael Roy Ames.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3730.html">Ben Goertzel: "Complexity of AGI"</a>
<li><strong>Previous message:</strong> <a href="3728.html">Ben Goertzel: "RE: AI in &lt;what?&gt;"</a>
<li><strong>In reply to:</strong> <a href="3727.html">Justin Corwin: "AI in &lt;what?&gt;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3733.html">Justin Corwin: "RE: AI in &lt;what?&gt;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3729">[ date ]</a>
<a href="index.html#3729">[ thread ]</a>
<a href="subject.html#3729">[ subject ]</a>
<a href="author.html#3729">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
