<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Complexity of AGI</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Complexity of AGI">
<meta name="Date" content="2002-05-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Complexity of AGI</h1>
<!-- received="Sun May 19 17:24:32 2002" -->
<!-- isoreceived="20020519232432" -->
<!-- sent="Sun, 19 May 2002 17:23:13 -0400" -->
<!-- isosent="20020519212313" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Complexity of AGI" -->
<!-- id="3CE817C1.7FC8D835@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJAEENCIAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Complexity%20of%20AGI"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun May 19 2002 - 15:23:13 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3732.html">Mike & Donna Deering: "Re: Complexity of AGI"</a>
<li><strong>Previous message:</strong> <a href="3730.html">Ben Goertzel: "Complexity of AGI"</a>
<li><strong>In reply to:</strong> <a href="3730.html">Ben Goertzel: "Complexity of AGI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3735.html">Ben Goertzel: "RE: Complexity of AGI"</a>
<li><strong>Reply:</strong> <a href="3735.html">Ben Goertzel: "RE: Complexity of AGI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3731">[ date ]</a>
<a href="index.html#3731">[ thread ]</a>
<a href="subject.html#3731">[ subject ]</a>
<a href="author.html#3731">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I have thought a little about your intuition that an AGI needs to be 1-2
</em><br>
<em>&gt; orders of magnitude more complex than Novamente.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It seems to me that there is some threshold T so that the following holds.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; IF the complexity of an AGI needs to be &gt; T, THEN it makes sense to focus
</em><br>
<em>&gt; efforts on human brain simulation (as advocated by Kurzweil, Eugene Leitl,
</em><br>
<em>&gt; and many others), rather than on designing systems loosely inspired by the
</em><br>
<em>&gt; human brain/mind.
</em><br>
<p>I fail to see why, if a problem is beyond human understanding, it can be
<br>
solved by building systems in which we don't even know what the pieces are
<br>
doing or why.  Learning to build brain simulations will involve better
<br>
understanding of the function of pieces of the brain, not just the capable
<br>
of running finer and finer simulations of neural networks which we don't
<br>
understand.  My intuition is that simulating a working brain without
<br>
understanding the mind, a la Kurzweil and Leitl, will turn out to require an
<br>
insanely detailed simulation (down to the microtubular level, perhaps) to
<br>
ensure that all necessary functional qualities of neurons are duplicated
<br>
when the researchers don't know, in fact, what the functional qualities of
<br>
neurons are.  This entire scenario seems to me to be built around Kurzweil's
<br>
desire to convince an audience of the workability of transhuman intelligence
<br>
without Kurzweil having to defend the idea that anyone will ever comprehend
<br>
intelligence.  It is not futuristically plausible.  Kurzweil is
<br>
(unconsciously, I assume) optimizing for convenience of argument, not
<br>
faithfulness to the real world.
<br>
<p>In real life, the researchers would start to see what the neural networks
<br>
are doing and why long before you have the capability to run a simulation
<br>
perfect enough that the scan works whether or not you know what the networks
<br>
are doing.  Could we eventually simulate networks so perfectly that they
<br>
worked without our understanding their higher functions?  Yes.  But that's
<br>
an existence proof, not a prediction.  It's not how the future would
<br>
actually develop.
<br>
<p><em>&gt; My intuition is that T is around, roughly, 3-5 times the complexity of the
</em><br>
<em>&gt; current Novamente design.  Beyond this level, the difficulties of
</em><br>
<em>&gt; parameter-tuning and engineering and performance analysis are just going to
</em><br>
<em>&gt; become WAY too great for any team of humans to handle.
</em><br>
<p>Large corporations routinely build systems with hundreds of times as many
<br>
lines of code as Novamente.  Also, I happen to feel that incorrect AI
<br>
designs contribute nontrivially to the amount of work that gets dumped on
<br>
parameter-tuning, engineering, and performance analysis.  Among the
<br>
complexity I think you're missing out on is a lot of the complexity that
<br>
goes into managing complexity.  An AI that is built wrongly will present the
<br>
wrong engineering challenges.  Imagine Lenat saying, &quot;Well, suppose that you
<br>
need to enter a trillion facts into the system... in this case it would make
<br>
sense to scan an existing human brain because no programming team could
<br>
handle the engineering challenge of managing relationships among a dataset
<br>
that large.&quot;
<br>
<p><em>&gt; Novamente is now about 30K lines of C++, it will be somewhere between 100K -
</em><br>
<em>&gt; 300K when done.  The total complexity of the algorithms in it probably does
</em><br>
<em>&gt; not exceed that of the algorithms in a complex program like an efficient C++
</em><br>
<em>&gt; compiler.  (Compilers have all sorts of shit in them, graph-coloring
</em><br>
<em>&gt; algorithms, conversions between different types of trees, etc. etc. etc.)
</em><br>
<em>&gt; However, the algorithms in a compiler are hooked together in a rigid and
</em><br>
<em>&gt; predictable way, whereas the algorithms in Novamente are adaptive and self-
</em><br>
<em>&gt; and inter-referential, which means that the testing/tuning process for
</em><br>
<em>&gt; Novamente is going to vastly exceed that of a compiler (as we discovered in
</em><br>
<em>&gt; building and toying with Webmind!!).
</em><br>
<p>Of course, it's hard for me to see in advance what will turn out to be the
<br>
real, unexpected critical challenges of building DGI.  But I suspect that
<br>
when the pieces of a correct AI design are hooked together, 90% of the
<br>
humanly achievable functionality will take 10% of the humanly possible
<br>
tuning.  In other words, I think that the tremendous efforts you put into
<br>
tuning Webmind are symptomatic of an AI pathology.
<br>
<p><em>&gt; The human mind/brain contains a lot of specialized inference, perception and
</em><br>
<em>&gt; action modules, dealing with things like spatial and temporal inference,
</em><br>
<em>&gt; social reasoning, aspects of language processing, each sensory stream that
</em><br>
<em>&gt; we have, etc. etc.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If an AGI has to be engineered to contain *significantly qualitatively
</em><br>
<em>&gt; different* code for each of these specialized functional mind-modules, then
</em><br>
<em>&gt; I suggest that this AGI is going to be 10-20 times more complex than
</em><br>
<em>&gt; Novamente, and hence over my intuitively posited T value.
</em><br>
<p>That is not the kind of specialized complexity that goes into creating a
<br>
DGI-model AI.  Computational systems give rise to cognitive talents;
<br>
cognitive talents combine with experiential content to give rise to domain
<br>
competencies.  The mapping from computational subsystems to cognitive
<br>
talents is many-to-many.  Likewise the mapping from talents to
<br>
competencies.  Novamente has what I consider to be a too-limited set of
<br>
basic computational subsystems.  DGI does not contain *more specialized
<br>
versions* of these subsystems that support specific cognitive talents, which
<br>
is what you seem to be visualizing, but rather contains a *completely
<br>
different* set of underlying subsystems whose cardinality happens to be
<br>
larger than the cardinality of the set of Novamente subsystems.  I agree
<br>
that an AI built using Novamente's basic architecture, with lots of
<br>
specialized versions of Novemante's basic generic processes, would multiply
<br>
Novamente's difficulties.
<br>
<p><em>&gt; In other words,
</em><br>
<em>&gt; rather than build a system like this, which will have so many parameters it
</em><br>
<em>&gt; will be un-tunable,
</em><br>
<p>I believe this problem is an AI pathology of the Novamente architecture. 
<br>
(This is not a recent thought; I've had this impression ever since I visited
<br>
Webmind Inc. and saw some poor guy trying to optimize 1500 parameters with a
<br>
GA.)
<br>
<p><em>&gt; we'd be better off to focus on brain scanning and
</em><br>
<em>&gt; cellular brain simulation.
</em><br>
<p>That doesn't help.
<br>
<p><em>&gt; On the other hand, my hypothesis is that we can achieve these specialized
</em><br>
<em>&gt; functions by appropriately modifying the parameters of a handful of
</em><br>
<em>&gt; individually-narrowly-intelligent, intelligently interacting learning
</em><br>
<em>&gt; algorithms.  If this is true then something of *borderline manageable
</em><br>
<em>&gt; complexity* like Novamente (or, say, A2I2) can work, and we don't
</em><br>
<em>&gt; necessarily need to follow the path of detailed human brain simulation.
</em><br>
<p>These aren't the only two options.  There is more to the universe than
<br>
generic algorithms and generic algorithms lightly specialized for particular
<br>
domains.  Novamente has what I would consider a flat architecture, like
<br>
&quot;Coding a Transhuman AI&quot; circa 1998.  Flat architectures come with certain
<br>
explosive combinatorial problems that can only be solved with deep
<br>
architectures.  Deep architectures are admittedly much harder to think about
<br>
and invent.  It requires that you listen to your quiet, nagging doubts about
<br>
shallow architectures and that you go on relentlessly replacing every single
<br>
shallow architecture your programmer's mind invents, until you finally start
<br>
to see how deep architectures work.
<br>
<p><em>&gt; One may argue that each decade software and hardware tech get better,
</em><br>
<em>&gt; enabling us to build more &amp; more complex software systems.  It is true.  But
</em><br>
<em>&gt; we do run up against barriers of human psychology and limitations of human
</em><br>
<em>&gt; communication.  Novamente is already WAY more complex in its inter-component
</em><br>
<em>&gt; interactions than anything ever built... barring an intervining Singularity,
</em><br>
<em>&gt; it'll be at least a decade, maybe a few, before software systems of this
</em><br>
<em>&gt; complexity are routine in the sense that transaction systems and big OO
</em><br>
<em>&gt; systems of other sorts are routine today.
</em><br>
<p>I'm sorry, Ben, but I don't think that Novamente lies right at the fringes
<br>
of the most complex systems that are humanly comprehensible.  Different
<br>
people will have different ideas of what constitutes &quot;depth beyond the human
<br>
ability to comprehend&quot;.  I don't see how you can know what's too deep for
<br>
humans to comprehend, anyway; all information available is of the form &quot;X is
<br>
too deep for me to comprehend at my current level of skill&quot;.
<br>
<p>I think you'd be better off if you stopped thinking of some level of
<br>
complexity as &quot;too difficult&quot; and started thinking of that level of
<br>
complexity as &quot;my responsibility, my challenge; the work of Evolution, my
<br>
rival and target.&quot;  I find that quite a number of things supposedly &quot;beyond
<br>
human ability&quot; are so-called because people use the phrase &quot;beyond human
<br>
ability&quot; when they mentally flinch away from the prospect of having to do
<br>
something.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3732.html">Mike & Donna Deering: "Re: Complexity of AGI"</a>
<li><strong>Previous message:</strong> <a href="3730.html">Ben Goertzel: "Complexity of AGI"</a>
<li><strong>In reply to:</strong> <a href="3730.html">Ben Goertzel: "Complexity of AGI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3735.html">Ben Goertzel: "RE: Complexity of AGI"</a>
<li><strong>Reply:</strong> <a href="3735.html">Ben Goertzel: "RE: Complexity of AGI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3731">[ date ]</a>
<a href="index.html#3731">[ thread ]</a>
<a href="subject.html#3731">[ subject ]</a>
<a href="author.html#3731">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
