<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: singularity arrival estimate... idiocy... and human plankton</title>
<meta name="Author" content="ben goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: singularity arrival estimate... idiocy... and human plankton">
<meta name="Date" content="2002-05-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: singularity arrival estimate... idiocy... and human plankton</h1>
<!-- received="Fri May 17 16:53:50 2002" -->
<!-- isoreceived="20020517225350" -->
<!-- sent="Fri, 17 May 2002 14:26:39 -0600" -->
<!-- isosent="20020517202639" -->
<!-- name="ben goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: singularity arrival estimate... idiocy... and human plankton" -->
<!-- id="01C1FDAE.DC499070.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="singularity arrival estimate... idiocy... and human plankton" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> ben goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20singularity%20arrival%20estimate...%20idiocy...%20and%20human%20plankton"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Fri May 17 2002 - 14:26:39 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3721.html">Smigrodzki, Rafal: "we are real! (was;  singularity arrival estimate...)"</a>
<li><strong>Previous message:</strong> <a href="3719.html">James Rogers: "Re: solid state disk storage"</a>
<li><strong>Maybe in reply to:</strong> <a href="3701.html">ben goertzel: "RE: singularity arrival estimate... idiocy... and human plankton"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3720">[ date ]</a>
<a href="index.html#3720">[ thread ]</a>
<a href="subject.html#3720">[ subject ]</a>
<a href="author.html#3720">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
***
<br>
<em>&gt; Compared to Eliezer, I do sort of doubt the long-term importance of 
</em><br>
keeping
<br>
<em>&gt; humans around.
</em><br>
<p>Humans?  Or minds that started out as human?
<br>
***
<br>
<p>I tend to doubt whether, in the long run, it will make any difference at 
<br>
all whether a mind started out as human or not....
<br>
<p><p>***
<br>
<em>&gt; Yeah, Friendly AI is *extremely* important to me, as a
</em><br>
<em>&gt; human being, in the &quot;short run&quot; (i.e. the next centuries or maybe
</em><br>
<em>&gt; milennia).
</em><br>
<p>I've said it before, and I'll say it again:  I don't think you can build a
<br>
Friendly AI if you conceive of this as exploiting the AI for your own
<br>
selfish purposes.  I designed the Friendly AI semantics by thinking in 
<br>
terms
<br>
of sharing altruism.  I don't think you can create a workable method by
<br>
thinking in terms of brainwashing.
<br>
***
<br>
<p>Well, the ideas of &quot;death&quot; and &quot;sentient being&quot; are human concepts whose 
<br>
limitations will be very apparent to superhuman intelligences.
<br>
<p>Of course, by talking about Friendliness we are talking about trying to 
<br>
impose a value system defined in terms of human concepts, on transhuman 
<br>
entities that may have entirely different concept systems and entirely 
<br>
different &quot;natural&quot; value systems.
<br>
<p>This is intrinsically &quot;self-centered&quot; in a broad sense, in that it assumes 
<br>
our human concepts and values are very broadly applicable, when perhaps 
<br>
they are not reasonably considered as such...
<br>
<p>***
<br>
<em>&gt; But will there still be human plankton a few thousand years down the 
</em><br>
line?
<br>
<p>I would tend to hope not.
<br>
<p>***
<br>
<p>Maybe the only ones left will be a crew of six Plankton-Eliezers, locked in 
<br>
a replica of DisneyLand full of 10000 android replicas of Jerry Falwell, 
<br>
neurally modified by Uploaded-Transhuman-Ben to not desire uploading or 
<br>
other technological improvements, and kept around as a tribute to 
<br>
Human-Ben's long-disappeared perverse sense of humor...
<br>
<p>ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3721.html">Smigrodzki, Rafal: "we are real! (was;  singularity arrival estimate...)"</a>
<li><strong>Previous message:</strong> <a href="3719.html">James Rogers: "Re: solid state disk storage"</a>
<li><strong>Maybe in reply to:</strong> <a href="3701.html">ben goertzel: "RE: singularity arrival estimate... idiocy... and human plankton"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3720">[ date ]</a>
<a href="index.html#3720">[ thread ]</a>
<a href="subject.html#3720">[ subject ]</a>
<a href="author.html#3720">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
