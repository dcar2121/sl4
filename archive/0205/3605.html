<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Taking AI seriously (was: DGI Paper)</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Taking AI seriously (was: DGI Paper)">
<meta name="Date" content="2002-05-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Taking AI seriously (was: DGI Paper)</h1>
<!-- received="Sun May 05 23:47:14 2002" -->
<!-- isoreceived="20020506054714" -->
<!-- sent="Sun, 05 May 2002 23:38:04 -0400" -->
<!-- isosent="20020506033804" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Taking AI seriously (was: DGI Paper)" -->
<!-- id="3CD5FA9C.3B686634@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJIEGJCHAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Taking%20AI%20seriously%20(was:%20DGI%20Paper)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun May 05 2002 - 21:38:04 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3606.html">Ben Goertzel: "RE: Review of Novamente &amp; a2i2"</a>
<li><strong>Previous message:</strong> <a href="3604.html">Ben Goertzel: "RE: Review of Novamente &amp; a2i2"</a>
<li><strong>In reply to:</strong> <a href="3597.html">Ben Goertzel: "RE: DGI Paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3611.html">Ben Goertzel: "RE: Taking AI seriously (was: DGI Paper)"</a>
<li><strong>Reply:</strong> <a href="3611.html">Ben Goertzel: "RE: Taking AI seriously (was: DGI Paper)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3605">[ date ]</a>
<a href="index.html#3605">[ thread ]</a>
<a href="subject.html#3605">[ subject ]</a>
<a href="author.html#3605">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I would respect your opinion more if you had personally taken on the
</em><br>
<em>&gt; challenge of designing a &quot;real AI&quot; system.  I understand you intend to do
</em><br>
<em>&gt; this sometime in the future.  I suspect that once you have done so, we will
</em><br>
<em>&gt; be able to have much more productive conversations.  I think it will be
</em><br>
<em>&gt; easier to map various Novamente ideas into aspects of your detailed AI
</em><br>
<em>&gt; design, than it is to map them into aspects of your abstract theory.
</em><br>
<p>Ben, sometimes writing code is taking the easy way out.  I understand that
<br>
you believe resources should be put into Novamente, rather than, say, SIAI,
<br>
and that's certainly not against the law.  But with all due respect,
<br>
Novamente seems to be constructed out of ideas that I had at one point or
<br>
another, but which I looked at and said:  &quot;No, it's not that easy.  This
<br>
problem is harder than that - this method will work for small problems, but
<br>
not for big problems; it's not good enough for real AI.&quot;  To me it looks
<br>
like Novamente is going to try for real AI and go splat.  It's just not that
<br>
powerful and it doesn't look like the framework of something that can be
<br>
rebuilt into something that powerful.  If I wanted to fling myself against
<br>
the problem and go splat, I could have done that anytime after 1996.  You
<br>
are welcome to believe that the problem of creating true intelligence is
<br>
enormously smaller than I think it is, and that enormously less complexity
<br>
is needed to handle it, in which case I'm sure it makes sense for you to
<br>
criticize me on the grounds of not having flung myself at the problem yet. 
<br>
<em>&gt;From my perspective, it is very easy and tempting to start implementing an
</em><br>
inadequate design, but futile.
<br>
<p>You have been known, from time to time, to remark on my youth and my not
<br>
having running AI code, which I consider to be &quot;cheap shots&quot; (i.e., taking
<br>
the easy way out), so let me take what I fully acknowledge to be a cheap
<br>
shot, and ask whether either Novamente or Webmind have done anything really
<br>
impressive in the realm of AI?  If you have so much more experience than I,
<br>
then can you share the experiences that lead you to believe Novamente is a
<br>
design for a general intelligence, rather than (as it seems to me) a
<br>
pattern-recognition system that may be capable of achieving limited goals in
<br>
a very small class of patterns that are tractable for it?
<br>
<p>I've given Novamente a lot of benefit of the doubt in the past, but after
<br>
reading your manuscript and finding out that most of my past temporary
<br>
credit was mistaken, I can't credit your &quot;experience&quot; with Webmind, or the
<br>
vast additional amount of design no hint of which appeared in the Novamente
<br>
manuscript, until you tell me what *specifically* your experiences or extra
<br>
design are.  You talk about Novamente being able to prove mathematical
<br>
theorems once the Mizar database is translated into Novamente propositions,
<br>
and about Novamente being able to make design improvements to itself once it
<br>
has the logic of a Java supercompiler.  I just can't see this as reasonable,
<br>
even taking different intuitions about AI into account.  It looks to me like
<br>
another AI-go-splat debacle in the making.
<br>
<p>Why do it?  Why make all these lofty predictions?  When SIAI starts its own
<br>
AI project, we aren't going to be telling people we'll have a [whatever] in
<br>
[whenever].  All we'll guarantee is the *attempt* at seed AI because seed AI
<br>
is an extremely hard problem.  Why do so many AI projects violate this basic
<br>
rule?  Does AI really look that easy to them?  Do you have to enormously
<br>
exaggerate the promise of your system just to get it funded?  Is there a
<br>
selection effect making sure that only people who underestimate AI even make
<br>
the attempt, while everyone who sees the real size of the problem goes into
<br>
a more tractable area of cognitive science?  Ifni knows I'd never have stuck
<br>
with the problem past 1998 (when I thought real AI would take a Manhattan
<br>
project) if the fate of the entire human species hadn't been at stake.  Am I
<br>
the first pessimist ever to go into real AI in the first place?  Why are
<br>
people *still* making cheery, optimistic predictions about insanely hard
<br>
problems?  If people were still trying to solve the problem, I could
<br>
understand that, but what's with the cheery optimism?
<br>
<p>Right now it looks to me like, in another few years, I'm going to be dealing
<br>
with people asking:  &quot;Yeah, well, what happened to the Novamente project
<br>
that promised us transhuman seed AI, and (didn't pan out) / (turned out to
<br>
be just a data-mining system)?&quot;  And I'm going to wearily say, &quot;I predicted
<br>
in advance that would happen, and that in fact I would end up answering this
<br>
very question; here, let me show you the message in the SL4 archives.&quot;
<br>
<p>You keep saying that I ought to just throw myself into design, as if it were
<br>
an ordinary problem of above-average difficulty, rather than a critical step
<br>
along the pathway of one of the ultimate challenges.  In the first chapter
<br>
of your manuscript you casually toss around the terms &quot;seed AI&quot; and
<br>
&quot;transhuman intelligence&quot; as if they were marketing buzzwords.  You don't
<br>
present it as a climax of a long, careful argument; you just toss it in with
<br>
no advance justification.  It's like you first claimed that Novamente could
<br>
do general intelligence because that was the most impressive thing you'd
<br>
heard of, and once you heard about the Singularity you decided to add that
<br>
as a claim too.  It's very easy for you to claim that Novamente is a design
<br>
for a real AI.  Lenat can claim that Cyc is a design for a real AI.  Newell
<br>
and Simon can claim that GPS is a design for a real AI.  It doesn't mean
<br>
that you've gotten started coding a seed AI and I haven't.  It means that
<br>
you have a much lower threshold for accepting what looks to you like a
<br>
probable solution.  To me it looks like I'm being penalized for admitting
<br>
the real difficulty of the problem instead of using a quick, easy, and
<br>
unworkable solution.  And I'll admit I'm annoyed, and I'm even more annoyed
<br>
that you're using the term &quot;seed AI&quot;, because I don't want seed AI plagued
<br>
by the cloud of failure created when a bunch of projects cheerfully fling
<br>
themselves against the wall and go splat.  But, as you say, you don't need
<br>
my permission.  Fine; I'll do my best to clean up the mess afterward.  But
<br>
it still seems to me that it would be very easy to avoid the entire debacle
<br>
just by changing the way you talk about the problem.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3606.html">Ben Goertzel: "RE: Review of Novamente &amp; a2i2"</a>
<li><strong>Previous message:</strong> <a href="3604.html">Ben Goertzel: "RE: Review of Novamente &amp; a2i2"</a>
<li><strong>In reply to:</strong> <a href="3597.html">Ben Goertzel: "RE: DGI Paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3611.html">Ben Goertzel: "RE: Taking AI seriously (was: DGI Paper)"</a>
<li><strong>Reply:</strong> <a href="3611.html">Ben Goertzel: "RE: Taking AI seriously (was: DGI Paper)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3605">[ date ]</a>
<a href="index.html#3605">[ thread ]</a>
<a href="subject.html#3605">[ subject ]</a>
<a href="author.html#3605">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
