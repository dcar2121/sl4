<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: DGI Paper</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: DGI Paper">
<meta name="Date" content="2002-05-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: DGI Paper</h1>
<!-- received="Sun May 05 21:30:35 2002" -->
<!-- isoreceived="20020506033035" -->
<!-- sent="Sun, 5 May 2002 18:19:13 -0600" -->
<!-- isosent="20020506001913" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: DGI Paper" -->
<!-- id="LAEGJLOGJIOELPNIOOAJIEGJCHAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3CD43050.6D7254E1@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20DGI%20Paper"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sun May 05 2002 - 18:19:13 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3598.html">Ben Goertzel: "RE: Review of A2I2"</a>
<li><strong>Previous message:</strong> <a href="3596.html">Dan Clemmensen: "Re: Leaving soon"</a>
<li><strong>In reply to:</strong> <a href="3585.html">Eliezer S. Yudkowsky: "Re: DGI Paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3605.html">Eliezer S. Yudkowsky: "Taking AI seriously (was: DGI Paper)"</a>
<li><strong>Reply:</strong> <a href="3605.html">Eliezer S. Yudkowsky: "Taking AI seriously (was: DGI Paper)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3597">[ date ]</a>
<a href="index.html#3597">[ thread ]</a>
<a href="subject.html#3597">[ subject ]</a>
<a href="author.html#3597">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt;  But if you actually grew up in
</em><br>
<em>&gt; BrainSci culture, hearing about how neurons can efficiently implement
</em><br>
<em>&gt; logical inference operations on small networks is enough to instantly
</em><br>
<em>&gt; identify the speaker as a CompSci conspirator.
</em><br>
<p>I agree that my background is more CS than neuroscience.
<br>
<p>However, Hebbian Logic is explicitly about how neural nets can
<br>
*inefficiently* implement logical inference operations on *large neural
<br>
networks*.  I think my paper makes that quite clear.
<br>
<p>In that sense, it is different from most CS work on neural nets.  And I
<br>
definitely read a lot of neuroscience papers while writing it, so I don't
<br>
think it is totally naive in brain science terms.
<br>
<p><p><em>&gt; I recommend reading Barsalou's
</em><br>
<em>&gt; &quot;Perceptual Symbol Systems&quot; or the opening chapters of Kosslyn's
</em><br>
<em>&gt; &quot;Image and
</em><br>
<em>&gt; Brain&quot;.  There is a genuine civilizational divide and the bits of BrainSci
</em><br>
<em>&gt; culture that leak across to the other side are much more fragmentary than
</em><br>
<em>&gt; the CompSci culture realizes.
</em><br>
<p>Well, I read Kosslyn's book and talked to him about it once, and we seemed
<br>
to have no trouble communicating!
<br>
<p><p><em>&gt; Again, without meaning any offense, this instantly identifies you as a
</em><br>
<em>&gt; CompSci speaker.  For me, Tversky and Kahneman's decision theory is a
</em><br>
<em>&gt; central prototype of what I call &quot;cognitive psychology&quot;.  For you, formal
</em><br>
<em>&gt; logic is a central prototype of what you call &quot;cognitive psychology&quot;.
</em><br>
<p>No, I'm sorry, but this is a completely false statement!!  And an odd one
<br>
too.  Where could you have gotten the idea that I don't know what cognitive
<br>
psychology is ???
<br>
<p>I was a research fellow in a psych department for 2.5 years and I certainly
<br>
know what cognitive psychology is.
<br>
<p>In fact, I know a lot more cog psych. than I do brain science (which is
<br>
partly because there's a lot less cog psych. to know).
<br>
<p>Obviously, formal logic is not cognitive psychology, it's mathematics,
<br>
verging on CS in some of its subdisciplines.
<br>
<p>Cognitive psychology includes the study of how *humans* carry out logical
<br>
operations, and there is some work trying to formally model this, but this
<br>
is different from math or CS work on formal logic.
<br>
<p>Of course, Tversky's work is cognitive psychology, and formal mathematical
<br>
logic is not.
<br>
<p>I actually have a fair bit of research experience in cog psych, and a little
<br>
bit in analyzing data obtained from brain science.  So although my formal
<br>
training is in math/Cs, I have more practical experience with these other
<br>
disciplines than you seem to assume.
<br>
<p>When my student Takuo Henmi did his PhD on nonlinear-dynamical models of
<br>
psychophysics, a couple years back, this was perceptual psych verging on cog
<br>
psych.  It was very different from CS or mathematical work.  What we were
<br>
doing was actually collecting data about human visual perception, in the UWA
<br>
vision lab, and creating mathematical models that tried to explain the data.
<br>
I am rather well aware of the difference between this kind of work and the
<br>
math/CS work I've done.
<br>
<p>When my student Graham Zemunik (he finished his PhD under someone else,
<br>
after I left UWA) wrote his PhD on a computational model of the cockroach
<br>
brain, he was really doing biological modeling not cognitive psych.  Half of
<br>
the work he had to do was sorting through various contradictory biology
<br>
papers to try to get a coherent view of what is known biologically about
<br>
cockroach information processing.  Very different from CS, even though the
<br>
mathematical model he created was embodied in a computer simulation.
<br>
<p>Frankly I found cognitive psych rather frustrating because it takes so
<br>
incredibly  much art, science and patience to design lab experiments that
<br>
will tell you anything of even moderately much interest about the mind.
<br>
Mind dynamics, my  main interest, can't really be studied using the
<br>
experimental methods of cog psych. today.
<br>
<p>This year I have worked a little bit with my friend Barak Pearlmutter, who
<br>
does the data analysis work for the MEG lab here.  They generate nice
<br>
122-dimensional time series data describing the  magnetic fields at various
<br>
points on the brain surface.  This work is at the borderline of CS, math,
<br>
cog psych and brain science -- it's pretty interesting.  But there's a long
<br>
way to go, still.  Basically they're just dealing with issues like
<br>
localizing which part of the brain is most active when different types of
<br>
simple percepts or actions are present.  Inferring a model of brain dynamics
<br>
from this data is a hard problem, and a problem we may apply Novamente to.
<br>
<p><em>&gt; By the
</em><br>
<em>&gt; standards of BrainSci civilization, Novamente's logical inference mimics a
</em><br>
<em>&gt; small, stereotypically logical subset of the kinds of reasoning
</em><br>
<em>&gt; that people
</em><br>
<em>&gt; are known to use.
</em><br>
<p>This is a bad misunderstanding of Novamente's inference engine, but I'm not
<br>
going to dig into it in this e-mail
<br>
<p>If you would like to pursue this topic seriously, please give me a list of
<br>
&quot;kinds of reasoning that people are known to use&quot; that you believe
<br>
Novamente, when complete, will be incapable of doing.  I will then explain
<br>
to you how I believe a completed Novamente will handle these kinds of
<br>
reasoning.
<br>
<p>Without this kind of detail, it is impossible to respond sensibly to your
<br>
statement.
<br>
<p><p><em>&gt; I realize that you consider Hebbian Logic, emergence and chaos theory, and
</em><br>
<em>&gt; Novamente's expanded inference mechanisms to be clear proof of consilient
</em><br>
<em>&gt; integration with BrainSci, but from BrainSci culture these things
</em><br>
<em>&gt; look like
</em><br>
<em>&gt; prototypical central cases of CompSci.  There is a gap here, it's not just
</em><br>
<em>&gt; me, and it's much bigger than you think.
</em><br>
<p>I do not think that I have *clear proof* of Novamente's conceptual
<br>
compatibility with brain science.
<br>
<p>And I do not think that you have *clear proof* of its incompatibility
<br>
either.
<br>
<p>I think that we do not understand enough about the brain to formulate clear
<br>
proof of theses such as this, in one direction or another.
<br>
<p><p><em>&gt;  I would argue that most of what you
</em><br>
<em>&gt; consider &quot;abstract&quot; thinking is built up from the crossmodal object sense
</em><br>
<em>&gt; and introspective perceptions.  The key point is that this
</em><br>
<em>&gt; abstract imagery
</em><br>
<em>&gt; is pretty much the same &quot;kind of thing&quot; as sensory imagery; it interacts
</em><br>
<em>&gt; freely with sensory imagery as an equal and interacts with the rest of the
</em><br>
<em>&gt; mind in pretty much the same way as sensory imagery.
</em><br>
<p>yeah, this is a difference of intuition between us.  I doubt this very much.
<br>
<p>This is not how I introspectively feel like I'm thinking, and so far as I
<br>
know the cognitive psych literature does not support this strong claim
<br>
either.
<br>
<p>To me, introspectively, my &quot;abstract imagery&quot; is not at all the same kind of
<br>
thing as sensory imagery, and does not interface with the rest of my  mind
<br>
in similar ways.
<br>
<p>I could of course be convinced that my introspective view of my own thought
<br>
process is an illusion.  But it would take some fairly solid scientific
<br>
evidence, and you are not offering any.
<br>
<p><em>&gt; And when I say imagery, I do mean depictive imagery - if you
</em><br>
<em>&gt; close your eyes
</em><br>
<em>&gt; and imagine a cat, then there is actually a cat-shaped activation
</em><br>
<em>&gt; pattern in
</em><br>
<em>&gt; your visual cortex,
</em><br>
<p>yeah, that's fine, but what about when I close my eyes and imagine an alien
<br>
from a nondimensional universe, playing with a pool full of differential
<br>
operators that act on a space of melodies played at a pitch too high for a
<br>
human ear to hear.
<br>
<p>what about when i close my eyes and imagine that i'm a butterfly dreaming
<br>
that I'm a very small uncomputable number that's dreaming that I'm a moth?
<br>
<p>Something rather different than sensory-style imagery seems to be going on
<br>
in  my mind in these cases.  Sensory-style images are one ingredient, but
<br>
there seems to be a very different kind of form-construction and
<br>
form-manipulation going on too.
<br>
<p><em>&gt; Novamente uses the traditional (CompSci) view of cognition as a process
</em><br>
<em>&gt; separate from the depictive imagery of sensory perception, in
</em><br>
<em>&gt; which thoughts
</em><br>
<em>&gt; about cats are represented by propositions that include a cat symbol which
</em><br>
<em>&gt; receives a higher activation level.  Novamente's entire thought processes
</em><br>
<em>&gt; are propositional rather than perceptual.  So it's not surprising that the
</em><br>
<em>&gt; idea of mental imagery as an entire level of organization may come as a
</em><br>
<em>&gt; shock.
</em><br>
<p>Again, you are badly and profoundly misunderstanding Novamente.  Since the
<br>
book I gave you to read is crudely written at this point, I don't blame you
<br>
for misunderstanding things on first read.  But it is frustrating that you
<br>
persist to hold to the same misconceptions even after I specifically attempt
<br>
to clarify them.  I guess explaining this stuff clearly is just VERY, VERY
<br>
HARD; I wish I were better at it.
<br>
<p>It is TRUE that Novamente's &quot;brain&quot; (its space of nodes &amp; links) lacks a 3D
<br>
structure, so that (assuming it has a camera eye), when it sees a cat, it
<br>
will not have a 3D picture of a cat in its brain.  Computer RAM is not made
<br>
that way, and Novamente does not try to simulate a 3D brain structure in the
<br>
linear memory of a computer.
<br>
<p>However, it IS true that when a camera-endowed Novamente sees a cat, the
<br>
result is a huge complex of dynamic node-link activations and node-and-link
<br>
interactions....  Will there be a &quot;cat&quot; node somewhere in the system that is
<br>
activated whenever cats are seen?  Maybe.  or maybe there will just be a
<br>
complex of nodes that are stimulated together whenever cats are seen.  Or
<br>
maybe there will be several different complexes that are stimulated when
<br>
several different types of cats are seen, with very loose inter-complex
<br>
couplings.
<br>
<p>The dichotomy of &quot;propositional versus perceptual&quot; is in my view a false
<br>
one.  To me, &quot;propositional&quot; is just a point of view one can take about
<br>
anything at all.  For instance, one can formulate a neural network system as
<br>
a logical production system if one wants to, where the logical propositions
<br>
(rules) encode the dynamics of the NN.  In fact, Novamente &quot;distributed
<br>
schema&quot; are very much like little NN's.  The logical reasoning component of
<br>
the system may view them propositionally, but considered in themselves, they
<br>
are acting pretty much just like neural nets.
<br>
<p>The idea of mental imagery as a level of organization is not problematic; I
<br>
think that this kind of &quot;border zone&quot; between perception and cognition does
<br>
exist and is important.  What I reject is the notion that this is the
<br>
majority of cognition.
<br>
<p><em>&gt; And I think that the kind of abstract thought I think you're thinking of,
</em><br>
<em>&gt; implemented by Novamente using propositions, is implemented using
</em><br>
<em>&gt; the above
</em><br>
<em>&gt; kind of mental imagery.
</em><br>
<p>In my view, abstract thought is only partly implemented using mental
<br>
imagery.  There's a lot more to the story.
<br>
<p><em>&gt; I haven't read Edelman's books, but are you sure that it really emphasizes
</em><br>
<em>&gt; the evolutionary formation of long-term neural structures rather than the
</em><br>
<em>&gt; evolution of short-term neural patterns?
</em><br>
<p>yes, I am 100% sure of this.
<br>
<p><em>&gt; Basically, I have a model of how concepts work in which concept formation
</em><br>
<em>&gt; inherently requires that certain internally specialized subsystems operate
</em><br>
<em>&gt; together in a complex dance - creating new concepts by mixing their
</em><br>
<em>&gt; internals together is an intriguing notion but I have no need for that
</em><br>
<em>&gt; hypothesis with respect to humans, though it might be well worth trying in
</em><br>
<em>&gt; AIs as long as all the complex machinery is still there.
</em><br>
<p>You  may have no need for this hypothesis, but you are also very, very far
<br>
from explaining the details of advanced human cognition using your own
<br>
theory.
<br>
<p><em>&gt; How exactly would neural maps reproduce internally, anyway?  It's
</em><br>
<em>&gt; clear how
</em><br>
<em>&gt; activation patterns could do this, but I can't recall hearing offhand of a
</em><br>
<em>&gt; postulated mechanism whereby a neural structure can send signals
</em><br>
<em>&gt; to another
</em><br>
<em>&gt; neural area that results in the long-term potentiation of a duplicate of
</em><br>
<em>&gt; that neural structure.
</em><br>
<p>There is lots of evidence for this sort of phenomenon happening, but I don't
<br>
have the references at hand right now.
<br>
I will try to dig them up.
<br>
<p><em>&gt; I think your intuition on this subject derives from Novamente (a) having a
</em><br>
<em>&gt; propositional representation of concepts and (b) lacking all the complex
</em><br>
<em>&gt; interacting machinery that's necessary to form new concepts
</em><br>
<em>&gt; without playing
</em><br>
<em>&gt; with their internals.
</em><br>
<p>Of course, this reverses reality.  It was my intuitions about the mind that
<br>
led me to the initial Novamente design, not the Novamente design that led me
<br>
to my intuitions about the mind.
<br>
<p><em>&gt; In fact, I would say that Novamente's
</em><br>
<em>&gt; concepts don't
</em><br>
<em>&gt; have any internals.
</em><br>
<p>It is hard for me to understand what you could possibly  mean by this.  A
<br>
Novamente concept, inasmuch as it's represented by a &quot;map&quot; of nodes/links
<br>
that tend to be simultaneously active, obviously has internals: the
<br>
nodes/links that constitute the map.
<br>
<p><p><p><p><em>&gt; I disagree, but I think our very different perspectives on complexity and
</em><br>
<em>&gt; simplicity are showing.  To me, &quot;financial trading&quot; and &quot;biodatabase
</em><br>
<em>&gt; analysis&quot; are utterly separate from &quot;The Net&quot; as an environment; they have
</em><br>
<em>&gt; different sensory structures, different behaviors, different invariants,
</em><br>
<em>&gt; different regularities, different everything.
</em><br>
<p>When you get down to the nitty-gritty, there are plenty of similarities
<br>
between financial and bio databases.  And these databases connect to text
<br>
databases that are relevant to their quantitative contents.  And these text
<br>
databases relate to a lot of the text on the world's Web pages....
<br>
<p><em>&gt; we have extremely different ideas of what
</em><br>
<em>&gt; experiential learning is about,
</em><br>
<p>To me, in a nutshell, it's about learning to perceive/act-in/cognize a
<br>
portion of the world cooperatively with (and ultimately in communication
<br>
with) other minds that are perceiving/acting-in/cognizing the same portion
<br>
of the world.
<br>
<p><p><em>&gt; This is again one of those things that threw me completely until I paused
</em><br>
<em>&gt; and tried to visualize you visualizing Novamente.  What you are calling a
</em><br>
<em>&gt; &quot;concept&quot;, I would call a &quot;thought&quot;.
</em><br>
<p>Not at all true.
<br>
<p>I would call &quot;cat&quot; a concept and so would you.  I would call &quot;Eliezer&quot; a
<br>
concept and so would you.
<br>
<p>It's OK to use the word &quot;thought&quot; for an ephemeral pattern arising in the
<br>
mind, but once a thought is remembered and brought back from memory and
<br>
mulled over and over again, doesn't it become a &quot;concept&quot;?
<br>
<p><p><em>&gt; It's certainly not a full concept that can be used as an element in other
</em><br>
<em>&gt; concept structures.  How would you invoke it - as an element in a concept
</em><br>
<em>&gt; structure, and not just a memory - if you can't name it?
</em><br>
<p>I personally, in my own mind, can invoke concepts WITHOUT NAMING THEM.
<br>
<p>To take a nonmathematical example, I do this when composing and improvising
<br>
music ALL THE TIME.  I invoke concepts about ways of playing different
<br>
chords or note sequences, and I have named none of these concepts.  Naming
<br>
them  meaningfully would be very very hard.
<br>
<p><em>&gt; &gt; I guess I still don't fully understand your notion of a &quot;concept&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Does it help if I note that I distinguish between &quot;concept&quot; and &quot;concept
</em><br>
<em>&gt; structure&quot; and that neither is analogous to Novamente's propositional
</em><br>
<em>&gt; structures?
</em><br>
<p>I don't understand your definition of &quot;concept structure&quot;
<br>
<p><p><em>&gt; And *this* one felt like running up against a brick wall.  1, 2,
</em><br>
<em>&gt; and 3 occur
</em><br>
<em>&gt; simultaneously?  What on Earth?  At this point I started to wonder
</em><br>
<em>&gt; half-seriously whether the placebo effect in cognitive science
</em><br>
<em>&gt; was powerful
</em><br>
<em>&gt; enough to sculpt our minds into completely different architectures through
</em><br>
<em>&gt; the conformation of cognition to our respective expectations.
</em><br>
<p>And this idea would occur to you BEFORE the idea that different human minds
<br>
actually work somewhat differently?
<br>
<p><p><em>&gt; I think the mix here may be more like 25%/75% if not 15%/85%.  The reason
</em><br>
<em>&gt; Novamente feels so alien to me is that, in my humble opinion, you're doing
</em><br>
<em>&gt; everything wrong, and trying to model the emergent qualities of a
</em><br>
<em>&gt; mind built
</em><br>
<em>&gt; using the wrong components and the wrong levels of organization is...
</em><br>
<em>&gt; really, really hard.
</em><br>
<p>I would respect your opinion more if you had personally taken on the
<br>
challenge of designing a &quot;real AI&quot; system.  I understand you intend to do
<br>
this sometime in the future.  I suspect that once you have done so, we will
<br>
be able to have much more productive conversations.  I think it will be
<br>
easier to map various Novamente ideas into aspects of your detailed AI
<br>
design, than it is to map them into aspects of your abstract theory.
<br>
<p><em>&gt; I consider it a warmup for trying to build AI, like
</em><br>
<em>&gt; the mental gymnastics it took to model you modeling DGI using
</em><br>
<em>&gt; your model of
</em><br>
<em>&gt; Novamente as a lens.  If you're right and I'm not modeling Novamente
</em><br>
<em>&gt; correctly, I don't envy you the job of modeling me modeling
</em><br>
<em>&gt; Novamente using
</em><br>
<em>&gt; DGI so that you can figure out where I went wrong.
</em><br>
<p>Eliezer, believe it or not, modeling your personal internal model of
<br>
Novamente is not something I'm very interested in doing.
<br>
<p>It's clear that your model of Novamente is very badly wrong, but my approach
<br>
to remedying this would be to try to explain Novamente more clearly *in
<br>
general*, rather than to try to figure out exactly what the roots of your
<br>
personal misunderstandings are.
<br>
<p>In 6 months or so we'll have a much better version of the book, but I don't
<br>
know if this will help you.  It seems to me that you have a strong emotional
<br>
reaction to some particular aspects of the design, which to some extent
<br>
blinds you to understanding the design as a whole.
<br>
<p>Anyway, having you or any other individual think Novamente is a good design,
<br>
is not that important to me.  Different people are gonna have different
<br>
tastes.
<br>
<p>I do appreciate your feedback, because it has helped me to see which aspects
<br>
of the book need the most work.  Your feedback on the AI design itself has
<br>
frankly not been very helpful, because you simply reject the initial premise
<br>
of the design, which means that you have nothing useful to say about the
<br>
details.
<br>
<p>Perhaps you're right and it's not possible to craft a &quot;middle way&quot; between
<br>
symbolic &amp; subsymbolic AI, as Novamente attempts to do.  But I am nowhere
<br>
near convinced by your arguments, which include a huge number of badly
<br>
incorrect statements about Novamente, as well as a large number of
<br>
statements of your intuitive feelings framed as if they were definitive
<br>
facts.
<br>
<p>-- Ben G
<br>
<p><p>-- Ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3598.html">Ben Goertzel: "RE: Review of A2I2"</a>
<li><strong>Previous message:</strong> <a href="3596.html">Dan Clemmensen: "Re: Leaving soon"</a>
<li><strong>In reply to:</strong> <a href="3585.html">Eliezer S. Yudkowsky: "Re: DGI Paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3605.html">Eliezer S. Yudkowsky: "Taking AI seriously (was: DGI Paper)"</a>
<li><strong>Reply:</strong> <a href="3605.html">Eliezer S. Yudkowsky: "Taking AI seriously (was: DGI Paper)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3597">[ date ]</a>
<a href="index.html#3597">[ thread ]</a>
<a href="subject.html#3597">[ subject ]</a>
<a href="author.html#3597">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
