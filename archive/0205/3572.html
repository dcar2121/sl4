<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: supergoal stability</title>
<meta name="Author" content="Wei Dai (weidai@eskimo.com)">
<meta name="Subject" content="Re: supergoal stability">
<meta name="Date" content="2002-05-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: supergoal stability</h1>
<!-- received="Sat May 04 04:09:22 2002" -->
<!-- isoreceived="20020504100922" -->
<!-- sent="Sat, 4 May 2002 00:51:06 -0700" -->
<!-- isosent="20020504075106" -->
<!-- name="Wei Dai" -->
<!-- email="weidai@eskimo.com" -->
<!-- subject="Re: supergoal stability" -->
<!-- id="20020504005106.A1097@eskimo.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3CD31176.A31178A1@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Wei Dai (<a href="mailto:weidai@eskimo.com?Subject=Re:%20supergoal%20stability"><em>weidai@eskimo.com</em></a>)<br>
<strong>Date:</strong> Sat May 04 2002 - 01:51:06 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3573.html">Eliezer S. Yudkowsky: "Re: supergoal stability"</a>
<li><strong>Previous message:</strong> <a href="3571.html">Damien Broderick: "Stross's post-Singularity novels"</a>
<li><strong>In reply to:</strong> <a href="3551.html">Eliezer S. Yudkowsky: "Re: supergoal stability"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3573.html">Eliezer S. Yudkowsky: "Re: supergoal stability"</a>
<li><strong>Reply:</strong> <a href="3573.html">Eliezer S. Yudkowsky: "Re: supergoal stability"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3572">[ date ]</a>
<a href="index.html#3572">[ thread ]</a>
<a href="subject.html#3572">[ subject ]</a>
<a href="author.html#3572">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Fri, May 03, 2002 at 06:38:46PM -0400, Eliezer S. Yudkowsky wrote:
<br>
<em>&gt; It currently looks to me like any mind-in-general that is *not* Friendly
</em><br>
<em>&gt; will automatically resist all modifications of the goal system, to the limit
</em><br>
<em>&gt; of its ability to detect modifications.  
</em><br>
<p>Thanks, that does make a great deal of sense. I thought that the
<br>
difficulty with creating an AI/SI that would implement goals as originally
<br>
understood by the programmers was that once the AI became sufficiently
<br>
intelligent, it would somehow decide that the goals are too trivial and
<br>
not worthy of its attention. But I guess there is really no reason for
<br>
that to happen, and the danger is actually in the earlier less intelligent
<br>
stages, where it may make mistakes in deciding whether a candidate
<br>
self-modification is overall a positive or negative contribution to its
<br>
supergoal.
<br>
<p><em>&gt; The inventor of CFAI won't even tell you the reasons why this would be
</em><br>
<em>&gt; difficult, just that it is.  
</em><br>
<p>Why not? If someone was to naively try to use the CFAI approach to create
<br>
an AI that serves some goal other than Friendliness, what is the likely
<br>
outcome? Would it be catastrophic or just fruitless?
<br>
<p><em>&gt; Well, today I would say it differently:  Today I would say that you have to
</em><br>
<em>&gt; do a &quot;port&quot; rather than a &quot;copy and paste&quot;, and that an AI can be *more*
</em><br>
<em>&gt; stable under changes of cognitive architecture or drastic power imbalances
</em><br>
<em>&gt; than a human would be, unless the human had the will and the knowledge to
</em><br>
<em>&gt; make those cognitive changes that would be required to match a Friendly AI
</em><br>
<em>&gt; in this area.
</em><br>
<p>Since you're planning to port your own personal philosophy to the AI, do
<br>
you have a document that explains in detail what your personal philosophy
<br>
is? I'm particularly interested in the following question. If two groups
<br>
of people want access to the same resource for incompatible purposes, and
<br>
no alternatives are available, how would you decide which group to grant
<br>
the resource to? In other words, what philosophical principles will guide 
<br>
the Sysop in designing its equivalent of the CPU scheduling algorithm?
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3573.html">Eliezer S. Yudkowsky: "Re: supergoal stability"</a>
<li><strong>Previous message:</strong> <a href="3571.html">Damien Broderick: "Stross's post-Singularity novels"</a>
<li><strong>In reply to:</strong> <a href="3551.html">Eliezer S. Yudkowsky: "Re: supergoal stability"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3573.html">Eliezer S. Yudkowsky: "Re: supergoal stability"</a>
<li><strong>Reply:</strong> <a href="3573.html">Eliezer S. Yudkowsky: "Re: supergoal stability"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3572">[ date ]</a>
<a href="index.html#3572">[ thread ]</a>
<a href="subject.html#3572">[ subject ]</a>
<a href="author.html#3572">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
