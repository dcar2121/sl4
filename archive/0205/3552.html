<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Voss's comments on Guidelines</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Voss's comments on Guidelines">
<meta name="Date" content="2002-05-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Voss's comments on Guidelines</h1>
<!-- received="Fri May 03 19:25:52 2002" -->
<!-- isoreceived="20020504012552" -->
<!-- sent="Fri, 03 May 2002 18:38:43 -0400" -->
<!-- isosent="20020503223843" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Voss's comments on Guidelines" -->
<!-- id="3CD31173.B4248684@pobox.com" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Voss's%20comments%20on%20Guidelines"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri May 03 2002 - 16:38:43 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3553.html">Ben Goertzel: "RE: supergoal stability"</a>
<li><strong>Previous message:</strong> <a href="3551.html">Eliezer S. Yudkowsky: "Re: supergoal stability"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3570.html">Peter Voss: "RE: Voss's comments on Guidelines"</a>
<li><strong>Reply:</strong> <a href="3570.html">Peter Voss: "RE: Voss's comments on Guidelines"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3552">[ date ]</a>
<a href="index.html#3552">[ thread ]</a>
<a href="subject.html#3552">[ subject ]</a>
<a href="author.html#3552">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Peter Voss pointed out at the Gathering that I hadn't responded to this yet,
<br>
which Voss published slightly less than a year ago.  Sorry!  Anyway, here's
<br>
my response.
<br>
<p><a href="http://www.optimal.org/peter/siai_guidelines.htm">http://www.optimal.org/peter/siai_guidelines.htm</a>
<br>
<p><em>&gt; The Singularity Institute for Artificial Intelligence (SIAI) recently
</em><br>
<em>&gt; published v1.0 of its guidelines for designing ‘Friendliness’ into
</em><br>
<em>&gt; intelligent systems.
</em><br>
<em>&gt;  
</em><br>
<em>&gt; AI, and especially self-improving systems - Seed AI – may well
</em><br>
<em>&gt; achieve a ‘critical mass’ of intelligence from which their ability could
</em><br>
<em>&gt; grow hyper-exponentially over a short period of time. Such sudden
</em><br>
<em>&gt; growth obviously poses some risks. In fact, even without a ‘hard
</em><br>
<em>&gt; take-off’, we should carefully consider the possibility of future, more
</em><br>
<em>&gt; autonomous AI systems acting in unanticipated goal-directed ways
</em><br>
<em>&gt; not amicable to our well-being.
</em><br>
<em>&gt;  
</em><br>
<em>&gt; The question of our relationship with truly intelligent machines is a
</em><br>
<em>&gt; crucial one, and I applaud and support SIAI for its work in this area.
</em><br>
<em>&gt;  
</em><br>
<em>&gt; Several issues come to mind in evaluating the Guidelines:
</em><br>
<em>&gt;  
</em><br>
<em>&gt; Most fundamentally: Are the Guidelines necessary? Will machines
</em><br>
<em>&gt; ever have ‘a will of their own’, or will they always remain subservient to
</em><br>
<em>&gt; our purpose and goals? (Neither the Guidelines, nor these comments
</em><br>
<em>&gt; address the substantial problem of AIs specifically programmed or
</em><br>
<em>&gt; instructed to be malevolent). 
</em><br>
<p>&quot;When a Roman father told his son that it was a sweet and seemly thing to
<br>
die for his country, he believed what he said.  He was communicating to the
<br>
son an emotion which he himself shared and which he believed to be in accord
<br>
with the value which his judgement discerned in noble death.  He was giving
<br>
the boy the best he had, giving of his spirit to humanize him as he had
<br>
given of his body to beget him.  [snip]  There are only two courses open to
<br>
Gaius and Titius [two anonymized book authors Lewis is criticizing].  Either
<br>
they must go the whole way and debunk this sentiment like any other, or must
<br>
set themselves to work to produce, from outside, a sentiment which they
<br>
believe to be of no value to the pupil and which may cost him his life,
<br>
because it is useful to us (the survivors) that our young men should feel
<br>
it.  If they embark on this course the difference between the old and the
<br>
new education will be an important one.  Where the old initiated, the new
<br>
merely 'conditions'.  The old dealt with its pupils as grown birds deal with
<br>
young birds when they teach them to fly; the new deals with them more as the
<br>
poultry-keeper deals with young birds - making them thus or thus for
<br>
purposes of which the birds know nothing.  In a word, the old was a kind of
<br>
propagation - men transmitting manhood to men; the new is merely
<br>
propaganda.&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-- C. S. Lewis, &quot;The Abolition of Man&quot;
<br>
<p><em>&gt; My present view on this is agnostic. It may well turn out that any
</em><br>
<em>&gt; super-intelligence will inherently be benevolent towards us. Or, that it
</em><br>
<em>&gt; remains neutral, with no goals other than those of its designer/
</em><br>
<em>&gt; operator. On the other hand, I do acknowledge the possibility of
</em><br>
<em>&gt; independent rogue AI. Obviously, we should err on the side of caution.
</em><br>
<p>There is, of course, a third possibility: that in the course of transmitting
<br>
our own moral values and philosophical reasoning on to the AI, we will have
<br>
transmitted that which is both necessary and sufficient to arrive at a set
<br>
of values superior to our own.  Friendly AI may be a necessary enabling
<br>
condition for this!  In the absence of definite knowledge we should try to
<br>
plan ahead for all three cases, as well as any other cases we can prepare
<br>
for without messing up the first three.
<br>
<p><em>&gt; The next question then: Are SIAI’s Guidelines theoretically
</em><br>
<em>&gt; sound? I have a number of grave reservations on several assumptions
</em><br>
<em>&gt; and conclusions of the AI design underlying the guidelines, as well as
</em><br>
<em>&gt; the guidelines themselves. However, allowing for a convergence of
</em><br>
<em>&gt; ideas, I want to move to a more practical question:
</em><br>
<em>&gt;  
</em><br>
<em>&gt; Can the Guidelines be implemented? Currently only a few dozen
</em><br>
<em>&gt; AI researchers/ teams (worldwide!) are actually focusing on theoretical
</em><br>
<em>&gt; or practical aspects of achieving general, human-level machines
</em><br>
<em>&gt; intelligence. Even fewer claim to have a reasonably comprehensive
</em><br>
<em>&gt; theoretical framework for achieving it.
</em><br>
<em>&gt;  
</em><br>
<em>&gt; The practicality of implementing the Guidelines must be assessed in
</em><br>
<em>&gt; the context of specific design proposals for AI. It would be valuable to
</em><br>
<em>&gt; have feedback from all the various players: both on their overall view on
</em><br>
<em>&gt; the need for (and approach towards) ‘Friendliness’, and also whether
</em><br>
<em>&gt; implementing SIAI’s guidelines would be compatible with their own
</em><br>
<em>&gt; designs.
</em><br>
<em>&gt;  
</em><br>
<em>&gt; The Guidelines’ eight design recommendations in the light of
</em><br>
<em>&gt; my theory of mind/ intelligence:
</em><br>
<em>&gt;  
</em><br>
<em>&gt; 1) Friendliness-topped goal system – Not possible: My design does
</em><br>
<em>&gt; not allow for such a high-level ‘supergoal’.
</em><br>
<p>What are the forces in your design that determine whether one action is
<br>
taken rather than another?
<br>
<p><em>&gt; 2) Cleanly causal goal system – Not possible: requires 1)
</em><br>
<p>Does your system choose between actions on the basis of which future events
<br>
those actions are predicted to lead to?
<br>
<p><em>&gt; 3) Probabilistic supergoal content – Inherent in my design: All
</em><br>
<em>&gt; knowledge and goals are subject to revision.
</em><br>
<p>If your system has no supergoal, but does have reinforcement, the
<br>
reinforcement systems are also part of the goal system.  Are the
<br>
reinforcement systems subject to revision?  In any case, the recommendation
<br>
of &quot;probabilistic supergoal content&quot; does not just mean that certain parts
<br>
of the goal system are subject to revision, but that they have certain
<br>
specific semantics that will enable the system to consider that revision as
<br>
desirable, so that the improvement of Friendliness is stable under
<br>
reflection, introspection, and self-modification.
<br>
<p><em>&gt; 4) Acquisition of Friendliness sources – While I certainly encourage
</em><br>
<em>&gt; the AI to acquire knowledge (including ethical theory) compatible with
</em><br>
<em>&gt; what I consider moral, this does not necessarily agree with what
</em><br>
<em>&gt; others regard as desirable ethics/ Friendliness.
</em><br>
<p>&quot;Acquisition of Friendliness sources&quot; here means acquiring the forces that
<br>
influence human moral decisions as well as learning the final output of
<br>
those decisions.  It furthermore has the specific connotation of attempting
<br>
to deduce the forces that influence the moral statements of the programmers
<br>
even if the programmers themselves do not know them.
<br>
<p><em>&gt; 5) Causal validity semantics – Inherent in my design: One of the key
</em><br>
<em>&gt; functions of the AI is to (help me) review and improve its premises,
</em><br>
<em>&gt; inferences, conclusions, etc. at all levels. Unfortunately, this ability
</em><br>
<em>&gt; only becomes really effective once a significant level of intelligence
</em><br>
<em>&gt; has already been reached.
</em><br>
<p>I agree with the latter sentence.  However, revision of beliefs is what I
<br>
would consider ordinary reasoning - causal validity semantics means that the
<br>
AI understands that its basic structure, its source code, is also the
<br>
product of programmer intentions that can be wrong.  That's *why* this
<br>
ability only becomes effective at a significant level of intelligence; it
<br>
inherently requires an integrated introspective understanding of brainware
<br>
and mindware, at minimum on the level of a human pondering evolutionary
<br>
psychology.
<br>
<p><em>&gt; 6) Injunctions – This seems like a good recommendation, however it is
</em><br>
<em>&gt; not clear what specific injunctions should be implemented, how to
</em><br>
<em>&gt; implement them effectively, and to what extent they will oppose other
</em><br>
<em>&gt; recommendations/ features.
</em><br>
<p>Hopefully, SIAI will learn how injunctions work in practice, then publish
<br>
the knowledge.
<br>
<p><em>&gt; 7) Self-modeling of fallibility - Inherent in my design. This seems to be
</em><br>
<em>&gt; an abstract expression of point 3)
</em><br>
<p>The human understanding of fallibility requires points (3), (4), and (5); an
<br>
AI, to fully understand its own fallibility, requires all of these as well. 
<br>
*Beginning* to model your own fallibility takes much less structure.  Any AI
<br>
with a probabilistic goal system can do so, though doing so efficiently
<br>
requires reflection.
<br>
<p><em>&gt; 8) Controlled ascent – Good idea, but may be difficult to implement: It
</em><br>
<em>&gt; may be hard to distinguish between rapid knowledge acquisition,
</em><br>
<em>&gt; improvements in learning, and overall self-improvement (ie. substantial
</em><br>
<em>&gt; increases in intelligence).
</em><br>
<p>All you need for a controlled ascent feature to be worthwhile is the
<br>
prospect of catching some of the hard takeoffs some of the time.
<br>
<p><em>&gt;               Peter Voss, June 2001
</em><br>
<p>Sorry about the delay...
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3553.html">Ben Goertzel: "RE: supergoal stability"</a>
<li><strong>Previous message:</strong> <a href="3551.html">Eliezer S. Yudkowsky: "Re: supergoal stability"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3570.html">Peter Voss: "RE: Voss's comments on Guidelines"</a>
<li><strong>Reply:</strong> <a href="3570.html">Peter Voss: "RE: Voss's comments on Guidelines"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3552">[ date ]</a>
<a href="index.html#3552">[ thread ]</a>
<a href="subject.html#3552">[ subject ]</a>
<a href="author.html#3552">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
