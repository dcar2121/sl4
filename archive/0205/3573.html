<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: supergoal stability</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: supergoal stability">
<meta name="Date" content="2002-05-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: supergoal stability</h1>
<!-- received="Sat May 04 04:34:59 2002" -->
<!-- isoreceived="20020504103459" -->
<!-- sent="Sat, 04 May 2002 04:28:08 -0400" -->
<!-- isosent="20020504082808" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: supergoal stability" -->
<!-- id="3CD39B98.882128F0@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20020504005106.A1097@eskimo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20supergoal%20stability"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat May 04 2002 - 02:28:08 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3574.html">Mike & Donna Deering: "Re: Quest for trans guide."</a>
<li><strong>Previous message:</strong> <a href="3572.html">Wei Dai: "Re: supergoal stability"</a>
<li><strong>In reply to:</strong> <a href="3572.html">Wei Dai: "Re: supergoal stability"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3591.html">Wei Dai: "Re: supergoal stability"</a>
<li><strong>Reply:</strong> <a href="3591.html">Wei Dai: "Re: supergoal stability"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3573">[ date ]</a>
<a href="index.html#3573">[ thread ]</a>
<a href="subject.html#3573">[ subject ]</a>
<a href="author.html#3573">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Wei Dai wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; On Fri, May 03, 2002 at 06:38:46PM -0400, Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt; &gt; It currently looks to me like any mind-in-general that is *not* Friendly
</em><br>
<em>&gt; &gt; will automatically resist all modifications of the goal system, to the limit
</em><br>
<em>&gt; &gt; of its ability to detect modifications.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Thanks, that does make a great deal of sense. I thought that the
</em><br>
<em>&gt; difficulty with creating an AI/SI that would implement goals as originally
</em><br>
<em>&gt; understood by the programmers was that once the AI became sufficiently
</em><br>
<em>&gt; intelligent, it would somehow decide that the goals are too trivial and
</em><br>
<em>&gt; not worthy of its attention. But I guess there is really no reason for
</em><br>
<em>&gt; that to happen, and the danger is actually in the earlier less intelligent
</em><br>
<em>&gt; stages, where it may make mistakes in deciding whether a candidate
</em><br>
<em>&gt; self-modification is overall a positive or negative contribution to its
</em><br>
<em>&gt; supergoal.
</em><br>
<p>Different minds, different dynamics.  A Friendly AI and a nonFriendly AI
<br>
think about goals very differently.  A human has millions of years of
<br>
evolution, and an FAI has the human programmers; both are sculpted into
<br>
moral systems of much greater complexity than the simplest moral systems
<br>
that will support general intelligence.
<br>
<p><em>&gt; &gt; The inventor of CFAI won't even tell you the reasons why this would be
</em><br>
<em>&gt; &gt; difficult, just that it is.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Why not? If someone was to naively try to use the CFAI approach to create
</em><br>
<em>&gt; an AI that serves some goal other than Friendliness, what is the likely
</em><br>
<em>&gt; outcome? Would it be catastrophic or just fruitless?
</em><br>
<p>Possible outcomes range from the programmers messing up the CFAI
<br>
architecture (most likely), warped goals, bacterial goals, or convergence to
<br>
Friendliness.
<br>
<p><em>&gt; &gt; Well, today I would say it differently:  Today I would say that you have to
</em><br>
<em>&gt; &gt; do a &quot;port&quot; rather than a &quot;copy and paste&quot;, and that an AI can be *more*
</em><br>
<em>&gt; &gt; stable under changes of cognitive architecture or drastic power imbalances
</em><br>
<em>&gt; &gt; than a human would be, unless the human had the will and the knowledge to
</em><br>
<em>&gt; &gt; make those cognitive changes that would be required to match a Friendly AI
</em><br>
<em>&gt; &gt; in this area.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Since you're planning to port your own personal philosophy to the AI, do
</em><br>
<em>&gt; you have a document that explains in detail what your personal philosophy
</em><br>
<em>&gt; is?
</em><br>
<p>&quot;Personal philosophy&quot; is here not used in the sense of &quot;My own personal
<br>
philosophy, which is just mine and nobody else's&quot; but rather in the sense of
<br>
describing &quot;that portion of philosophy which you, personally, have managed
<br>
to acquire.&quot;  C. S. Lewis would say that a personal philosophy is that
<br>
portion of the Tao which you have managed to acquire for yourself.  I have
<br>
never tried to construct a personal philosophy in the former sense.  I have
<br>
tried to make my philosophy that which I believe to be right and to leave
<br>
out everything that is &quot;just one person&quot;.  I don't want the Friendly AI to
<br>
have anything which is derived specially from Eliezer Yudkowsky and not from
<br>
humanity in general.  It was a large enough concession, from my perspective,
<br>
to admit that an AI might perhaps need something which is derived specially
<br>
from humanity and is not a property of all possible minds-in-general of
<br>
sufficient intelligence.
<br>
<p>And hence, &quot;Creating Friendly AI&quot; says most of what needs to be said.
<br>
<p><em>&gt; I'm particularly interested in the following question. If two groups
</em><br>
<em>&gt; of people want access to the same resource for incompatible purposes, and
</em><br>
<em>&gt; no alternatives are available, how would you decide which group to grant
</em><br>
<em>&gt; the resource to? In other words, what philosophical principles will guide
</em><br>
<em>&gt; the Sysop in designing its equivalent of the CPU scheduling algorithm?
</em><br>
<p>That's an interesting question.  I would expect/hope resource conflicts
<br>
along these lines to be rare.  One take is that after the Singularity all
<br>
sentient beings would get a quantity of &quot;mana&quot; and that mana could be used
<br>
to bid on whichever universal resources are conserved, after which all
<br>
conserved resources would be private property.  But that's just a guess.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3574.html">Mike & Donna Deering: "Re: Quest for trans guide."</a>
<li><strong>Previous message:</strong> <a href="3572.html">Wei Dai: "Re: supergoal stability"</a>
<li><strong>In reply to:</strong> <a href="3572.html">Wei Dai: "Re: supergoal stability"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3591.html">Wei Dai: "Re: supergoal stability"</a>
<li><strong>Reply:</strong> <a href="3591.html">Wei Dai: "Re: supergoal stability"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3573">[ date ]</a>
<a href="index.html#3573">[ thread ]</a>
<a href="subject.html#3573">[ subject ]</a>
<a href="author.html#3573">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
