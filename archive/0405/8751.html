<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: A conversation on Friendliness</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="A conversation on Friendliness">
<meta name="Date" content="2004-05-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>A conversation on Friendliness</h1>
<!-- received="Thu May 27 02:40:02 2004" -->
<!-- isoreceived="20040527084002" -->
<!-- sent="Thu, 27 May 2004 04:39:59 -0400" -->
<!-- isosent="20040527083959" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="A conversation on Friendliness" -->
<!-- id="40B5A95F.5020306@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20A%20conversation%20on%20Friendliness"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu May 27 2004 - 02:39:59 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8752.html">Ben Goertzel: "RE: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8750.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8763.html">Damien Broderick: "Re: A conversation on Friendliness"</a>
<li><strong>Reply:</strong> <a href="8763.html">Damien Broderick: "Re: A conversation on Friendliness"</a>
<li><strong>Reply:</strong> <a href="8764.html">Metaqualia: "Re: A conversation on Friendliness"</a>
<li><strong>Maybe reply:</strong> <a href="8769.html">J. Andrew Rogers: "Re: A conversation on Friendliness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8751">[ date ]</a>
<a href="index.html#8751">[ thread ]</a>
<a href="subject.html#8751">[ subject ]</a>
<a href="author.html#8751">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I recently had a conversation on FAI theory, which I am posting with 
<br>
permission for the sake of illustrating that FAI theory doesn't always have 
<br>
to be painful, if both parties follow something like the same communication 
<br>
protocol:
<br>
<p>&nbsp;&nbsp;[Eliezer] Incidentally, you say you've been thinking about Friendliness. 
<br>
&nbsp;&nbsp;If you have any thoughts that are readily communicable, I'd be interested.
<br>
&nbsp;&nbsp;[Eliezer] I won't be insulted if you tell me that your thoughts are 
<br>
beyond human comprehension.  I've been there.
<br>
&nbsp;&nbsp;[Eliezer] It amazes me how many people expect Friendliness to be a 
<br>
trivial subject.
<br>
&nbsp;&nbsp;&lt;James&gt; Nothing terribly communicable.  I am wondering if a correct 
<br>
implementation of initial state is even generally decidable.
<br>
&nbsp;&nbsp;[Eliezer] well, not &quot;amazes&quot; in a Bayesian sense, in the sense of human 
<br>
dismay
<br>
&nbsp;&nbsp;[Eliezer] I don't know your criterion of correctness, or what you mean by 
<br>
decidability.  Thus the explanation fails, but it is a noisy failure.
<br>
&nbsp;&nbsp;&lt;James&gt; I'm having a hard time seeing a way that one can make an 
<br>
implementation that is provably safe.
<br>
&nbsp;&nbsp;&lt;James&gt; At least generally.
<br>
&nbsp;&nbsp;&lt;James&gt; There might be a shortcut for narrow cases.
<br>
&nbsp;&nbsp;[Eliezer] In a general sense, you'd start with a well-specified abstract 
<br>
invariant, and construct a process that deductively (not probabilistically) 
<br>
obeys the invariant, including as a special case the property of 
<br>
constructing further editions of itself that can be deductively proven to 
<br>
obey the invariant
<br>
&nbsp;&nbsp;&lt;James&gt; right
<br>
&nbsp;&nbsp;&lt;James&gt; but how do you prove that the invariant constrains expression 
<br>
correctly in all cases?
<br>
&nbsp;&nbsp;[Eliezer] &quot;constrains expression&quot; &lt;--- ?
<br>
&nbsp;&nbsp;&lt;James&gt; sorry.  stays friendly
<br>
&nbsp;&nbsp;&lt;James&gt; at runtime
<br>
&nbsp;&nbsp;[Eliezer] to the extent you have to interact with probabilistic external 
<br>
reality, the effect of your actions in the real world is uncertain
<br>
&nbsp;&nbsp;[Eliezer] the only invariant you can maintain by mathematical proof is a 
<br>
specification of behaviors in portions of reality that you can control with 
<br>
near determinism, such as your own transistors
<br>
&nbsp;&nbsp;[Eliezer] there's a generalization to maintaing probable failure-safety 
<br>
with extremely low probabilities of failure, for redundant unreliable 
<br>
components with small individual failure rates
<br>
&nbsp;&nbsp;&lt;James&gt; Right
<br>
&nbsp;&nbsp;[Eliezer] the tough part of Friendly AI theory is describing a 
<br>
mathematical invariant such that if it holds true, the AI is something we 
<br>
recognize as Friendly
<br>
&nbsp;&nbsp;&lt;James&gt; precisely.
<br>
&nbsp;&nbsp;&lt;James&gt; that's the problem
<br>
&nbsp;&nbsp;[Eliezer] for example, you can have a mathematical invariant that in a 
<br>
young AI works to produce smiling humans by doing various humanly 
<br>
comprehensible things that make humans happy
<br>
&nbsp;&nbsp;[Eliezer] in an RSI AI, the same invariant binds to external reality in a 
<br>
way that leads the external state corresponding to a tiny little smiley 
<br>
face to be represented with the same high value in the system
<br>
&nbsp;&nbsp;[Eliezer] the AI tiles the universe with little smiley faces
<br>
&nbsp;&nbsp;[Eliezer] ...well, at least we agree on what the problem is
<br>
&nbsp;&nbsp;&lt;James&gt; heh
<br>
&nbsp;&nbsp;[Eliezer] any humanly comprehensible thoughts on the tough part of the 
<br>
problem?  I'll take humanly incomprehensible thoughts, even
<br>
&nbsp;&nbsp;&lt;James&gt; I've been studying it, from a kind of theoretical implementation 
<br>
standpoint.  Very ugly problem
<br>
&nbsp;&nbsp;&lt;James&gt; No thoughts yet.
<br>
&nbsp;&nbsp;[Eliezer] the problem is that humans aren't mathematically well-specified 
<br>
themselves
<br>
&nbsp;&nbsp;[Eliezer] just ad-hoc things that examine themselves and try to come up 
<br>
with ill-fitting simplifications
<br>
&nbsp;&nbsp;[Eliezer] we can't transfer our goals into an AI if we don't know what 
<br>
they are
<br>
&nbsp;&nbsp;&lt;James&gt; Yep.  Always have to be aware of that
<br>
&nbsp;&nbsp;[Eliezer] my current thinking tries to cut away at the ill-formedness of 
<br>
the problem in two ways
<br>
&nbsp;&nbsp;[Eliezer] first, by reducing the problem to an invariant in the AI that 
<br>
flows through the mathematically poorly specified humans
<br>
&nbsp;&nbsp;[Eliezer] in other words, the invariant specifies a physical dependency 
<br>
on the contents of the human black boxes that reflects what we would regard 
<br>
as the goal content of those boxes
<br>
&nbsp;&nbsp;[Eliezer] second, by saying that the optimization process doesn't try to 
<br>
extrapolate the contents of those black boxes beyond the point where the 
<br>
chaos in the extrapolation grows too great
<br>
&nbsp;&nbsp;[Eliezer] just wait for the humans to grow up, and make of themselves 
<br>
what they may
<br>
&nbsp;&nbsp;&lt;James&gt; I've noticed.  Seems like a reasonable approach
<br>
&nbsp;&nbsp;&lt;James&gt; Don't know if it is optimal though
<br>
&nbsp;&nbsp;&lt;James&gt; for whatever &quot;optimal&quot; means
<br>
&nbsp;&nbsp;&lt;James&gt; I'm not satisfied that I have a proper grip on the problem yet
<br>
&nbsp;&nbsp;[Eliezer] nor am I
<br>
&nbsp;&nbsp;[Eliezer] there are even parts where I know specifically that my grip is 
<br>
slipping
<br>
&nbsp;&nbsp;&lt;James&gt; Too many things I define lazily
<br>
&nbsp;&nbsp;[Eliezer] qualia, or my moral understanding of renormalizing initial 
<br>
conditions
<br>
&nbsp;&nbsp;[Eliezer] er, put &quot;qualia&quot; in sarcastic quote marks
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8752.html">Ben Goertzel: "RE: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8750.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8763.html">Damien Broderick: "Re: A conversation on Friendliness"</a>
<li><strong>Reply:</strong> <a href="8763.html">Damien Broderick: "Re: A conversation on Friendliness"</a>
<li><strong>Reply:</strong> <a href="8764.html">Metaqualia: "Re: A conversation on Friendliness"</a>
<li><strong>Maybe reply:</strong> <a href="8769.html">J. Andrew Rogers: "Re: A conversation on Friendliness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8751">[ date ]</a>
<a href="index.html#8751">[ thread ]</a>
<a href="subject.html#8751">[ subject ]</a>
<a href="author.html#8751">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
