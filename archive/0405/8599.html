<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Fascinating, fascinating</title>
<meta name="Author" content="Marc Geddes (marc_geddes@yahoo.co.nz)">
<meta name="Subject" content="Fascinating, fascinating">
<meta name="Date" content="2004-05-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Fascinating, fascinating</h1>
<!-- received="Thu May 20 04:59:44 2004" -->
<!-- isoreceived="20040520105944" -->
<!-- sent="Thu, 20 May 2004 22:59:42 +1200 (NZST)" -->
<!-- isosent="20040520105942" -->
<!-- name="Marc Geddes" -->
<!-- email="marc_geddes@yahoo.co.nz" -->
<!-- subject="Fascinating, fascinating" -->
<!-- id="20040520105942.34755.qmail@web20206.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="40ABDA29.5050303@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marc Geddes (<a href="mailto:marc_geddes@yahoo.co.nz?Subject=Re:%20Fascinating,%20fascinating"><em>marc_geddes@yahoo.co.nz</em></a>)<br>
<strong>Date:</strong> Thu May 20 2004 - 04:59:42 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8600.html">Dani Eder: "RE: ethics"</a>
<li><strong>Previous message:</strong> <a href="8598.html">Samantha Atkins: "Re: ethics"</a>
<li><strong>In reply to:</strong> <a href="8586.html">Eliezer S. Yudkowsky: "Re: ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8608.html">Keith Henson: "Re: ethics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8599">[ date ]</a>
<a href="index.html#8599">[ thread ]</a>
<a href="subject.html#8599">[ subject ]</a>
<a href="author.html#8599">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;--- &quot;Eliezer S. Yudkowsky&quot; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Fascinating,%20fascinating">sentience@pobox.com</a>&gt;
<br>
wrote: 
<br>
<em>&gt; 
</em><br>
<em>&gt; This was my ancient argument, and it turned out to
</em><br>
<em>&gt; be a flawed metaphor - 
</em><br>
<em>&gt; the rule simply doesn't carry over.  If you have no
</em><br>
<em>&gt; understanding of the 
</em><br>
<em>&gt; psychology of a being with the brain the size of a
</em><br>
<em>&gt; planet, how do you know 
</em><br>
<em>&gt; that no human can understand its psychology?  This
</em><br>
<em>&gt; sounds like a flip 
</em><br>
<em>&gt; question, but it's not; it's the source of my
</em><br>
<em>&gt; original mistake - I tried 
</em><br>
<em>&gt; to reason about the incomprehensibility of
</em><br>
<em>&gt; superintelligence without 
</em><br>
<em>&gt; understanding where the incomprehensibility came
</em><br>
<em>&gt; from, or why.  Think of 
</em><br>
<em>&gt; all the analogies from the history of science; if
</em><br>
<em>&gt; something is a mystery 
</em><br>
<em>&gt; to you, you do not know enough to claim that science
</em><br>
<em>&gt; will never comprehend 
</em><br>
<em>&gt; it.  I was foolish to make statements about the
</em><br>
<em>&gt; incomprehensibility of 
</em><br>
<em>&gt; intelligence before I understood intelligence.
</em><br>
<p>I never made your mistake.  
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Now I understand intelligence better, which is why I
</em><br>
<em>&gt; talk about 
</em><br>
<em>&gt; &quot;optimization processes&quot; rather than &quot;intelligence&quot;.
</em><br>
<p>What do you mean 'optimization processes'?  Sounds
<br>
like a major change in your views.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; The human ability to employ abstract reasoning is a
</em><br>
<em>&gt; threshold effect that 
</em><br>
<em>&gt; *potentially* enables a human to fully understand
</em><br>
<em>&gt; some optimization 
</em><br>
<em>&gt; processes, including, I think, optimization
</em><br>
<em>&gt; processes with arbitrarily 
</em><br>
<em>&gt; large amounts of computing power.  That is only
</em><br>
<em>&gt; *some* optimization 
</em><br>
<em>&gt; processes, processes that flow within persistent,
</em><br>
<em>&gt; humanly understandable 
</em><br>
<em>&gt; invariants; others will be as unpredictable as
</em><br>
<em>&gt; coinflips.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Imagine a computer program that outputs the prime
</em><br>
<em>&gt; factorization of large 
</em><br>
<em>&gt; numbers.  For large enough numbers, the actual
</em><br>
<em>&gt; execution of the program 
</em><br>
<em>&gt; flow is not humanly visualizable, even in principle.
</em><br>
<em>&gt;  But we can still 
</em><br>
<em>&gt; understand an abstract property of the program,
</em><br>
<em>&gt; which is that it outputs a 
</em><br>
<em>&gt; set of primes that multiply together to yield the
</em><br>
<em>&gt; input number.
</em><br>
<p><em>&gt; Now imagine a program that writes a program that
</em><br>
<em>&gt; outputs the prime 
</em><br>
<em>&gt; factorization of large numbers.  This is a more
</em><br>
<em>&gt; subtle problem, because 
</em><br>
<em>&gt; there's a more complex definition of utility
</em><br>
<em>&gt; involved - we are looking for 
</em><br>
<em>&gt; a fast program, and a program that doesn't crash or
</em><br>
<em>&gt; cause other negative 
</em><br>
<em>&gt; side effects, such as overwriting other programs'
</em><br>
<em>&gt; memory.  But I think 
</em><br>
<em>&gt; it's possible to build out an FAI dynamic that reads
</em><br>
<em>&gt; out the complete set 
</em><br>
<em>&gt; of side effects you care about.  More simply, you
</em><br>
<em>&gt; could use deductive 
</em><br>
<em>&gt; reasoning processes that guarantee no side effects. 
</em><br>
<em>&gt; (Sandboxing a Java 
</em><br>
<em>&gt; program generated by directed evolution is bad,
</em><br>
<em>&gt; because you're directing 
</em><br>
<em>&gt; enormous search power toward finding a flaw in the
</em><br>
<em>&gt; sandboxing!)  Again, 
</em><br>
<em>&gt; the exact form of the generated program would be
</em><br>
<em>&gt; unpredictable to humans, 
</em><br>
<em>&gt; but its effect would be predictable from
</em><br>
<em>&gt; understanding the optimization 
</em><br>
<em>&gt; criteria of the generator; a fast, reliable
</em><br>
<em>&gt; factorizer with no side effects.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; A program that writes a program that outputs the
</em><br>
<em>&gt; prime factorization of 
</em><br>
<em>&gt; large numbers is still understandable, and still not
</em><br>
<em>&gt; visualizable.
</em><br>
<p>Excellent, excellent.  However: the level of
<br>
understanding required to understand the abstract
<br>
properties fully might still be beyond the IQ of most
<br>
people...if not all people.
<br>
<p>Also...you may be technically correct that there's an
<br>
understandable abstract invariant, but the abstract
<br>
property might be *so* abstract that *for all
<br>
practical purposes* specfic results are unpredictable.
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt; The essential law of Friendly AI is that you cannot
</em><br>
<em>&gt; build an AI to 
</em><br>
<em>&gt; accomplish any end for which you do not possess a
</em><br>
<em>&gt; well-specified 
</em><br>
<em>&gt; *abstract* description.  If you want moral
</em><br>
<em>&gt; reasoning, or (my current 
</em><br>
<em>&gt; model) a dynamic that extrapolates human volitions
</em><br>
<em>&gt; including the 
</em><br>
<em>&gt; extrapolation of moral reasoning, then you need a
</em><br>
<em>&gt; well-specified abstract 
</em><br>
<em>&gt; description of what that looks like.
</em><br>
<p>What if a 'well-specified abstract description' is
<br>
beyond your IQ?   What's wrong with a partial
<br>
description which has a probabalistic chance of
<br>
working?  Nothing is ever specified with 100% rigour
<br>
in the real world:  there are simply 'degrees' of
<br>
rigour.  And one's IQ will probably place a 'ceiling'
<br>
on the degree of rigour which can be reached.  We see
<br>
this in mathematics.  There's really nothing proved
<br>
with 100% absolute certainty - just a shading off in
<br>
degree of rigour as the proofs get more and more
<br>
complex.  Case in point:  Wiles 'proof' of Fermat's
<br>
Last Theorem.  100 pages of dense mathematics, with
<br>
'total rigour' only being an ideal.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; In summary:  You may not need to know the exact
</em><br>
<em>&gt; answer, but you need to 
</em><br>
<em>&gt; know an exact question.  The question may generate
</em><br>
<em>&gt; another question, but 
</em><br>
<em>&gt; you still need an exact original question.  And you
</em><br>
<em>&gt; need to understand 
</em><br>
<em>&gt; everything you build well enough to know that it
</em><br>
<em>&gt; answers that question.
</em><br>
<em>&gt; 
</em><br>
<p><p>Keeping plugging away.  You're moving in the right
<br>
direction but have some way to go yet.  I must stop
<br>
dropping hints ;)
<br>
<p><em>&gt; -- 
</em><br>
<em>&gt; Eliezer S. Yudkowsky                         
</em><br>
<em>&gt; <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt; Research Fellow, Singularity Institute for
</em><br>
<em>&gt; Artificial Intelligence 
</em><br>
<p>=====
<br>
&quot;Live Free or Die, Death is not the Worst of Evils.&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Gen. John Stark
<br>
<p>&quot;The Universe...or nothing!&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-H.G.Wells
<br>
<p><p>Please visit my web-sites.
<br>
<p>Science-Fiction and Fantasy:  <a href="http://www.prometheuscrack.com">http://www.prometheuscrack.com</a>
<br>
Science, A.I, Maths            :  <a href="http://www.riemannai.org">http://www.riemannai.org</a>
<br>
<p>Find local movie times and trailers on Yahoo! Movies.
<br>
<a href="http://au.movies.yahoo.com">http://au.movies.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8600.html">Dani Eder: "RE: ethics"</a>
<li><strong>Previous message:</strong> <a href="8598.html">Samantha Atkins: "Re: ethics"</a>
<li><strong>In reply to:</strong> <a href="8586.html">Eliezer S. Yudkowsky: "Re: ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8608.html">Keith Henson: "Re: ethics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8599">[ date ]</a>
<a href="index.html#8599">[ thread ]</a>
<a href="subject.html#8599">[ subject ]</a>
<a href="author.html#8599">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
