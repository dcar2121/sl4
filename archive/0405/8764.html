<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: A conversation on Friendliness</title>
<meta name="Author" content="Metaqualia (metaqualia@mynichi.com)">
<meta name="Subject" content="Re: A conversation on Friendliness">
<meta name="Date" content="2004-05-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: A conversation on Friendliness</h1>
<!-- received="Thu May 27 10:34:40 2004" -->
<!-- isoreceived="20040527163440" -->
<!-- sent="Fri, 28 May 2004 00:33:50 +0800" -->
<!-- isosent="20040527163350" -->
<!-- name="Metaqualia" -->
<!-- email="metaqualia@mynichi.com" -->
<!-- subject="Re: A conversation on Friendliness" -->
<!-- id="0c7701c44408$7c9652b0$0301a8c0@CURZIOL2" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="40B5A95F.5020306@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Metaqualia (<a href="mailto:metaqualia@mynichi.com?Subject=Re:%20A%20conversation%20on%20Friendliness"><em>metaqualia@mynichi.com</em></a>)<br>
<strong>Date:</strong> Thu May 27 2004 - 10:33:50 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8765.html">Jef Allbright: "Re: External reference semantics"</a>
<li><strong>Previous message:</strong> <a href="8763.html">Damien Broderick: "Re: A conversation on Friendliness"</a>
<li><strong>In reply to:</strong> <a href="8751.html">Eliezer Yudkowsky: "A conversation on Friendliness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8769.html">J. Andrew Rogers: "Re: A conversation on Friendliness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8764">[ date ]</a>
<a href="index.html#8764">[ thread ]</a>
<a href="subject.html#8764">[ subject ]</a>
<a href="author.html#8764">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Why does this stuff need to be labeled as &quot;beyond human comprehension&quot;?
<br>
<p>Paraphrasing:
<br>
<p><em>&gt;   &lt;James&gt; Nothing terribly communicable.  I am wondering if a correct
</em><br>
<em>&gt; implementation of initial state is even generally decidable.
</em><br>
<p>Very hard problem, wondering if there is a solution after all.
<br>
<p><em>&gt;   [Eliezer] I don't know your criterion of correctness, or what you mean
</em><br>
by
<br>
<em>&gt; decidability.  Thus the explanation fails, but it is a noisy failure.
</em><br>
<p>How would you know a good solution?
<br>
<p><em>&gt;   &lt;James&gt; I'm having a hard time seeing a way that one can make an
</em><br>
<em>&gt; implementation that is provably safe.
</em><br>
<p>You probably can't prove beyond doubt that a solution is good.
<br>
<p><em>&gt;   [Eliezer] In a general sense, you'd start with a well-specified abstract
</em><br>
<em>&gt; invariant, and construct a process that deductively (not
</em><br>
probabilistically)
<br>
<em>&gt; obeys the invariant, including as a special case the property of
</em><br>
<em>&gt; constructing further editions of itself that can be deductively proven to
</em><br>
<em>&gt; obey the invariant
</em><br>
<p>Define your goal very specifically and then create the system so that it
<br>
will necessarily
<br>
and deterministically arrive to that goal. Include a permission for the
<br>
system to duplicate itself while maintaining this goal and architecture in
<br>
the copy.
<br>
<p><em>&gt;   &lt;James&gt; right
</em><br>
<em>&gt;   &lt;James&gt; but how do you prove that the invariant constrains expression
</em><br>
<em>&gt; correctly in all cases?
</em><br>
<p>but how do you prove that the goal really is achieved and the system doesn't
<br>
drift away?
<br>
<p><em>&gt;   [Eliezer] to the extent you have to interact with probabilistic external
</em><br>
<em>&gt; reality, the effect of your actions in the real world is uncertain
</em><br>
<p>Since reality is complex you can't be sure of what is going to happen out
<br>
there
<br>
<p><em>&gt;   [Eliezer] the only invariant you can maintain by mathematical proof is a
</em><br>
<em>&gt; specification of behaviors in portions of reality that you can control
</em><br>
with
<br>
<em>&gt; near determinism, such as your own transistors
</em><br>
<p>the only thing you control is the system's brain itself
<br>
<p><em>&gt;   [Eliezer] there's a generalization to maintaing probable failure-safety
</em><br>
<em>&gt; with extremely low probabilities of failure, for redundant unreliable
</em><br>
<em>&gt; components with small individual failure rates
</em><br>
<p>you can build redundant components each one with a very low probability of
<br>
failure
<br>
<p><em>&gt;   [Eliezer] the tough part of Friendly AI theory is describing a
</em><br>
<em>&gt; mathematical invariant such that if it holds true, the AI is something we
</em><br>
<em>&gt; recognize as Friendly
</em><br>
<p>the tough part is finding out what exactly is the goal
<br>
<p><em>&gt;   &lt;James&gt; precisely.
</em><br>
<em>&gt;   &lt;James&gt; that's the problem
</em><br>
<em>&gt;   [Eliezer] for example, you can have a mathematical invariant that in a
</em><br>
<em>&gt; young AI works to produce smiling humans by doing various humanly
</em><br>
<em>&gt; comprehensible things that make humans happy
</em><br>
<p>for instance if you set the goal to produce smiles
<br>
<p><em>&gt;   [Eliezer] in an RSI AI, the same invariant binds to external reality in
</em><br>
a
<br>
<em>&gt; way that leads the external state corresponding to a tiny little smiley
</em><br>
<em>&gt; face to be represented with the same high value in the system
</em><br>
<em>&gt;   [Eliezer] the AI tiles the universe with little smiley faces
</em><br>
<p>the machine may tile the universe with smiley faces.
<br>
<p><em>&gt;   &lt;James&gt; I've been studying it, from a kind of theoretical implementation
</em><br>
<em>&gt; standpoint.  Very ugly problem
</em><br>
<p>hmm I thought about it all day and it is very difficult.
<br>
<p><em>&gt;   &lt;James&gt; No thoughts yet.
</em><br>
<p>dunno
<br>
<p><em>&gt;   [Eliezer] the problem is that humans aren't mathematically
</em><br>
well-specified
<br>
<em>&gt; themselves
</em><br>
<em>&gt;   [Eliezer] just ad-hoc things that examine themselves and try to come up
</em><br>
<em>&gt; with ill-fitting simplifications
</em><br>
<em>&gt;   [Eliezer] we can't transfer our goals into an AI if we don't know what
</em><br>
<em>&gt; they are
</em><br>
<p>we don't really know what the goal is
<br>
<p><em>&gt;   &lt;James&gt; Yep.  Always have to be aware of that
</em><br>
<em>&gt;   [Eliezer] my current thinking tries to cut away at the ill-formedness of
</em><br>
<em>&gt; the problem in two ways
</em><br>
<p>so I am trying to find out in two ways
<br>
<p><em>&gt;   [Eliezer] first, by reducing the problem to an invariant in the AI that
</em><br>
<em>&gt; flows through the mathematically poorly specified humans
</em><br>
<p>first make sure that the AI actually knows what the goal is even though
<br>
humans don't
<br>
<p><em>&gt;   [Eliezer] in other words, the invariant specifies a physical dependency
</em><br>
<em>&gt; on the contents of the human black boxes that reflects what we would
</em><br>
regard
<br>
<em>&gt; as the goal content of those boxes
</em><br>
<p>in other words the goal is specified as being the goal content of human
<br>
brains (if we knew what that goal content was)
<br>
<p><em>&gt;   [Eliezer] second, by saying that the optimization process doesn't try to
</em><br>
<em>&gt; extrapolate the contents of those black boxes beyond the point where the
</em><br>
<em>&gt; chaos in the extrapolation grows too great
</em><br>
<p>second, make sure the AI doesn't try to second guess us if stuff gets too
<br>
complex
<br>
<p><em>&gt;   [Eliezer] just wait for the humans to grow up, and make of themselves
</em><br>
<em>&gt; what they may
</em><br>
<p>just wait for humans to grow up and blow themselves up on their own
<br>
<p><em>&gt;   &lt;James&gt; I've noticed.  Seems like a reasonable approach
</em><br>
<em>&gt;   &lt;James&gt; Don't know if it is optimal though
</em><br>
<em>&gt;   &lt;James&gt; for whatever &quot;optimal&quot; means
</em><br>
<em>&gt;   &lt;James&gt; I'm not satisfied that I have a proper grip on the problem yet
</em><br>
<em>&gt;   [Eliezer] nor am I
</em><br>
<em>&gt;   [Eliezer] there are even parts where I know specifically that my grip is
</em><br>
<em>&gt; slipping
</em><br>
<p>I still don't understand some things
<br>
<p><p>mq
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8765.html">Jef Allbright: "Re: External reference semantics"</a>
<li><strong>Previous message:</strong> <a href="8763.html">Damien Broderick: "Re: A conversation on Friendliness"</a>
<li><strong>In reply to:</strong> <a href="8751.html">Eliezer Yudkowsky: "A conversation on Friendliness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8769.html">J. Andrew Rogers: "Re: A conversation on Friendliness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8764">[ date ]</a>
<a href="index.html#8764">[ thread ]</a>
<a href="subject.html#8764">[ subject ]</a>
<a href="author.html#8764">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
