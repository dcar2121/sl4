<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Dangers of human self-modification</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Dangers of human self-modification">
<meta name="Date" content="2004-05-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Dangers of human self-modification</h1>
<!-- received="Fri May 21 04:35:46 2004" -->
<!-- isoreceived="20040521103546" -->
<!-- sent="Fri, 21 May 2004 06:35:43 -0400" -->
<!-- isosent="20040521103543" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Dangers of human self-modification" -->
<!-- id="40ADDB7F.3090804@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="40ADBD9F.3030603@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Dangers%20of%20human%20self-modification"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri May 21 2004 - 04:35:43 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8617.html">Aubrey de Grey: "Re: ethics"</a>
<li><strong>Previous message:</strong> <a href="8615.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>In reply to:</strong> <a href="8615.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8618.html">Philip Sutton: "Re: Dangers of human self-modification"</a>
<li><strong>Reply:</strong> <a href="8618.html">Philip Sutton: "Re: Dangers of human self-modification"</a>
<li><strong>Reply:</strong> <a href="8620.html">Rafal Smigrodzki: "Re: Dangers of human self-modification"</a>
<li><strong>Reply:</strong> <a href="8624.html">Giu1i0 Pri5c0: "Re: Dangers of human self-modification"</a>
<li><strong>Reply:</strong> <a href="8634.html">Michael Roy Ames: "Re: Dangers of human self-modification"</a>
<li><strong>Reply:</strong> <a href="8637.html">Samantha Atkins: "Re: Dangers of human self-modification"</a>
<li><strong>Reply:</strong> <a href="8643.html">Marc Geddes: "Re: Dangers of human self-modification"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8616">[ date ]</a>
<a href="index.html#8616">[ thread ]</a>
<a href="subject.html#8616">[ subject ]</a>
<a href="author.html#8616">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer Yudkowsky wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I think I have to side with Keith.  I fear that human
</em><br>
<em>&gt; self-modification is far more dangerous than I would once have liked
</em><br>
<em>&gt; to imagine.  Better to devise nutritious bacon, cheese, chocolate,
</em><br>
<em>&gt; and wine, than dare to mess with hunger - let alone anything more
</em><br>
<em>&gt; complex.  You would practically need to be a Friendly AI programmer
</em><br>
<em>&gt; just to realize how afraid you needed to be, and freeze solid until
</em><br>
<em>&gt; there was an AI midwife at hand to help you *very slowly* start to
</em><br>
<em>&gt; make modifications that didn't have huge unintended consequences, or
</em><br>
<em>&gt; take you away from the rest of humanity, or destroy complexity you
</em><br>
<em>&gt; would have preferred to keep.
</em><br>
<p>Some examples of possible consequences, off the top of my head:
<br>
<p>You've got memories of enjoying cheeseburgers.  What happens to the 
<br>
memories when the sensory substrate of recollection shifts?  Are you 
<br>
going to keep the old hardware around for recollection?  Will you add in 
<br>
a complex system to maintain empathy with your old self?
<br>
<p>Your old sense of taste was fine-tuned and integrated into your sense of 
<br>
pleasure and pain, happiness and disgust, by natural selection.  Natural 
<br>
selection also designed everything else keyed into those systems.  If 
<br>
you pick new senses, do they make sense?  Does the pattern subtly clash 
<br>
with the pattern of systems already present?
<br>
<p>Will your new sense of taste be more or less complex than your old sense 
<br>
of taste?  More intense or less intense?  If more intense, does the new 
<br>
sense of taste balance with a mental system that is known to stay sane 
<br>
only under ancestral conditions of environment and neurology?  Consider 
<br>
the effects on humans of non-ancestral Pringles and chocolate cake, 
<br>
loads of sugar and salt and fat not present in any ancestral foods. 
<br>
Adopting a more intense taste system can have the same effect, if the 
<br>
rest of the mind isn't upgraded accordingly to balance with the 
<br>
increased intensity of sensation.
<br>
<p>Maybe you would prefer to gradually grow into new tastes?  What does the 
<br>
sharp discontinuity of direct self-alteration do to your sense of 
<br>
personal continuity?
<br>
<p>If the new taste sensation is more intense, do you become addicted to 
<br>
the act of self-modification for more intense sensations?
<br>
<p>You're eliminating cognitive complexity of yourself by getting rid of 
<br>
the complex pattern of the old system.  Maybe you would prefer not to 
<br>
eliminate the old complexity - learn to appreciate lettuce *in addition 
<br>
to* Pringles?
<br>
<p>Can you really appreciate the long-term consequences of altering your 
<br>
mind this way?  Does the new design you decided upon make any sense with 
<br>
respect to those criteria that you would use if you thought about the 
<br>
problem long enough?
<br>
<p>What is the long-term effect of adopting the general policy of 
<br>
eliminating old complexity that inconveniences you, and inscribing new 
<br>
complexity that seems like a good idea at the time?
<br>
<p>Other humans share your current taste sensations.  Think of your awkward 
<br>
refusal of foods at dinner, the mainstream artistry of cooking you'll no 
<br>
longer be able to appreciate.  Are you distancing yourself from the rest 
<br>
of humanity?  Lest someone chime in that diversity is automatically 
<br>
good, let me add that this is one hell of a nontrivial decision.
<br>
<p>If you can alter your taste buds any time you feel like it, will it 
<br>
destroy, or alter, the perceived challenge and fun of cooking?  Consider 
<br>
the effect on baseball if people could just run around the bases any 
<br>
time they wanted.
<br>
<p>And finally, what about all the consequences, and categories of 
<br>
consequences, that you haven't foreseen?  When you imagine the act of 
<br>
self-modification, you will imagine only the easily mentally accessible 
<br>
consequences of the act, not the actual consequences.  Just because you 
<br>
can't see the doom, doesn't mean the doom isn't there.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8617.html">Aubrey de Grey: "Re: ethics"</a>
<li><strong>Previous message:</strong> <a href="8615.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>In reply to:</strong> <a href="8615.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8618.html">Philip Sutton: "Re: Dangers of human self-modification"</a>
<li><strong>Reply:</strong> <a href="8618.html">Philip Sutton: "Re: Dangers of human self-modification"</a>
<li><strong>Reply:</strong> <a href="8620.html">Rafal Smigrodzki: "Re: Dangers of human self-modification"</a>
<li><strong>Reply:</strong> <a href="8624.html">Giu1i0 Pri5c0: "Re: Dangers of human self-modification"</a>
<li><strong>Reply:</strong> <a href="8634.html">Michael Roy Ames: "Re: Dangers of human self-modification"</a>
<li><strong>Reply:</strong> <a href="8637.html">Samantha Atkins: "Re: Dangers of human self-modification"</a>
<li><strong>Reply:</strong> <a href="8643.html">Marc Geddes: "Re: Dangers of human self-modification"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8616">[ date ]</a>
<a href="index.html#8616">[ thread ]</a>
<a href="subject.html#8616">[ subject ]</a>
<a href="author.html#8616">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
