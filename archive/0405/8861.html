<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: One or more AIs??</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: One or more AIs??">
<meta name="Date" content="2004-05-30">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: One or more AIs??</h1>
<!-- received="Sun May 30 10:46:35 2004" -->
<!-- isoreceived="20040530164635" -->
<!-- sent="Sun, 30 May 2004 12:46:29 -0400" -->
<!-- isosent="20040530164629" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: One or more AIs??" -->
<!-- id="40BA0FE5.8020509@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="006401c4464d$6d833d00$0100a8c0@FamilyRoom" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20One%20or%20more%20AIs??"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun May 30 2004 - 10:46:29 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8862.html">Philip Sutton: "RE: One or more AIs??"</a>
<li><strong>Previous message:</strong> <a href="8860.html">Pedro Machin: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>In reply to:</strong> <a href="8852.html">Mark Waser: "Re: One or more AIs??"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8866.html">Mark Waser: "Re: One or more AIs??"</a>
<li><strong>Reply:</strong> <a href="8866.html">Mark Waser: "Re: One or more AIs??"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8861">[ date ]</a>
<a href="index.html#8861">[ thread ]</a>
<a href="subject.html#8861">[ subject ]</a>
<a href="author.html#8861">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Mark Waser wrote:
<br>
<em>&gt;&gt;I am at a loss to understand what is gained, except a way of
</em><br>
<em>&gt;&gt;anthropomorphizing the issue and avoiding confrontation with that darned
</em><br>
<em>&gt;&gt;hard part of the problem.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; OK.  Try this scenario . . . . An FAI believes that it has a good goal
</em><br>
<em>&gt; enhancement.  It tests it.  It looks OK.  It implements it.  Circumstances
</em><br>
<em>&gt; subsequently change in a REALLY unexpected manner.  Oops!  Under these very
</em><br>
<em>&gt; odd new circumstances, the &quot;enhancement&quot; turns out to be catastrophic for
</em><br>
<em>&gt; the human race . . . .
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But wait!  There are three other FAIs that are in close communication with
</em><br>
<em>&gt; this FAI.  Friendliness dictates that they should work in close co-operation
</em><br>
<em>&gt; in order to prevent errors of this type.  The other FAIs have not made this
</em><br>
<em>&gt; particular &quot;enhancement&quot; and correctly evaluate the situation for what it
</em><br>
<em>&gt; is.  They outvote the FAI with the &quot;enhancement&quot; and the &quot;enhanced&quot; FAI (who
</em><br>
<em>&gt; is still mostly Friendly and merely mistaken) rolls the &quot;enhancement&quot; back
</em><br>
<em>&gt; (or modifies it) so that the human race lives for the next moment or so
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Good engineering often dictates redundancy.  Common sense (which ain't so
</em><br>
<em>&gt; common - yes, I know) strongly promotes checks and balances.  Human history
</em><br>
<em>&gt; shows that when diversity of opinion is allowed to flourish that good things
</em><br>
<em>&gt; happen and that when diversity is suppressed that BAD things happen.  You
</em><br>
<em>&gt; seem to be flying in the face of a lot of good consensus about safety
</em><br>
<em>&gt; measures without a reason except &quot;I am at a loss to understand what is
</em><br>
<em>&gt; gained
</em><br>
<p>I'm glad you say that I &quot;seem to be&quot; flying in the face of safety &quot;without 
<br>
a reason&quot;.  Ordinarily I wouldn't have time to write this up.  But on this 
<br>
occasion, you're in luck; I've got an explanation of this from a piece of 
<br>
private correspondence that I wrote to Eric Drexler on May 9th, 2003.  Of 
<br>
course, no one saw this, so they might think I was &quot;totally isolated&quot;.
<br>
<p>** begin quote **
<br>
<p>I tend to regard the sharp distinction between &quot;groups&quot; and &quot;individuals&quot; 
<br>
as a special case of the human way, where cognitive systems cannot 
<br>
agglomerate, and the goal systems contain numerous egocentric 
<br>
speaker-dependent variables.  In particular, I worry that our being adapted 
<br>
to the individuals-groups dichotomy, and our expectation that other 
<br>
individuals will exhibit those same adaptations, can lead to incorrect 
<br>
inferences when considering AIs.  It looks to me like the challenge of 
<br>
getting a cognitive system to compute morality remains the same regardless 
<br>
of whether that physical system is called a multiplicity or a singleton - 
<br>
whether we regard it as many minds or one mind, it's the same thing.
<br>
<p>Imagine that some human culture has, for some time, employed human runners 
<br>
as messengers.  A messenger might fail for any number of reasons, including 
<br>
knees giving out, heart attacks, stomach pains, and all the ills to which 
<br>
human messengers are heir.  Suppose that each individual is a complex 
<br>
system composed of N subsystems, each with chance P1, P2, P3 of failure. 
<br>
Failure of any one subsystem causes the messenger to stumble to the side of 
<br>
the road, gasping, and the message won't go through.  The total chance of a 
<br>
messenger succeeding is (1 - P1)(1 - P2)(1 - P3)...  So after a while, this 
<br>
culture learns a very strong culturally embedded rule that you never send 
<br>
*one* messenger with a very urgent message.  It may even be instinctive - 
<br>
they may use different innate cognitive rules for modeling one messenger 
<br>
and many messengers.
<br>
<p>Now along comes a transhuman messenger with the strange ability to 
<br>
agglomerate subsystems with other messengers.  In other words, if you have 
<br>
two transmessengers, they can agglomerate into one messenger with redundant 
<br>
subsystems.  The agglomerated messenger's total chance of success is now (1 
<br>
- P12)(1 - P22)(1 - P32)... which is a considerable improvement over the 
<br>
chance of success of two independent messengers.  It takes two independent 
<br>
failures *in the same place* to destroy the agglomerated runner.  The 
<br>
agglomerated runner is probably quite a lot more reliable than a dozen 
<br>
independent runners, and cannot possibly be worse - any set of failures 
<br>
that would destroy the agglomerated runner would also destroy two single 
<br>
runners, and many failure sets that would destroy independent runners may 
<br>
leave an agglomerated runner untouched.  An agglomerated quadruplet of 
<br>
messengers may be more reliable than a thousand runners.  Yet the society 
<br>
may still insist that the runners not agglomerate because of the 
<br>
instinctive force of the rule not to trust &quot;one runner&quot;.  Or to put it more 
<br>
pessimistically, if a course is so difficult that it can't be solved using 
<br>
an agglomerated runner, splitting up the runner into individuals can't get 
<br>
you any closer to solving the problem.
<br>
<p>Humans have to imagine ways of solving problems that use many humans, 
<br>
because in the human world there is no way to use *one* *big* human.
<br>
<p>Suppose you have two &quot;thermostat AIs&quot; - that is, they have a decision 
<br>
system that employs a very simple and nonhumane way of computing 
<br>
desirability.  Let's say that one AI cares only about paperclips, while the 
<br>
other cares only about staples.  If the two AIs are roughly equal, they 
<br>
might arrive at something resembling a cooperative game-theoretical 
<br>
solution and split up the solar system between them, this solution being 
<br>
preferable to the negative effects of hostilities - classic PD.  The 
<br>
problem is that this doesn't protect the humans - it is better for *both* 
<br>
AIs to split any resources currently used by humanity between them.
<br>
<p>If humans make a law to live among each other in peace, our intuitions for 
<br>
fairness are likely to lead us to extend that law widely.  Not necessarily 
<br>
as widely as we should - witness the amount of time it took some groups to 
<br>
get the vote, for example.  But the point is that we regard the law as 
<br>
representing certain ideals of fairness, and the reason for this is that we 
<br>
have a fairness adaptation which came out of our ancestors being placed in 
<br>
game-theoretical situations.  But that adaptation is independent of the 
<br>
conditions that produced it, just as our current taste for sugar and fat is 
<br>
independent of the scarce-calorie environments which produced it.  And the 
<br>
idea that you *have* an adaptation for handling the problem is itself a 
<br>
product of the evolutionary design method.  &quot;Individual organisms should be 
<br>
considered adaptation-executers rather than fitness-maximizers&quot;.  We don't 
<br>
recompute the exact number of calories and nutrients we need; we have a 
<br>
simple adaptation that leads us to prefer certain foods over others.  It is 
<br>
not an adapation that is there so that we *will* reproduce; it is an 
<br>
adaptation that is there because our ancestors with those genes *did* 
<br>
reproduce.
<br>
<p>We have, as an independently executing adaptation, a certain *simple* way 
<br>
of handling game-theoretical situations - an instinctive sense of honor, 
<br>
fairness, and reciprocity.  We have this simple executing adaptation 
<br>
because it's a solution that was accessible to an incrementally adaptive 
<br>
design pathway.
<br>
<p>But that only holds for evolved organisms, not nonhumane thermostat AIs, 
<br>
which really would maximize 'fitness' according to their simple 
<br>
desirability computations.
<br>
<p>A number N of thermostat AIs may find themselves in a game-theoretical 
<br>
environment which looks to a human like it should call forth a fair 
<br>
solution, like one law for all sentients.  What seems more likely to happen 
<br>
is that the AIs will, on the fly, compute a cooperative solution among 
<br>
themselves which leaves humanity in the cold.  They would not have the 
<br>
adaptation for an instinctive sense of fairness that would lead them to see 
<br>
something wrong about this; they would simply calculate a &quot;fair&quot; solution 
<br>
in which the universe was equally divided among paperclips and staples. 
<br>
&nbsp;From our perspective, this might not look very different from a single AI 
<br>
with a goal system that wanted both paperclips and staples.
<br>
<p>If you imagine that humans, in some unimaginable way, acquire a serious 
<br>
threat to hold over both superintelligences, demanding that the humans be 
<br>
treated as game-theoretical near-equals, then why wouldn't the same threat 
<br>
be holdable over a paperclips-and-staples singleton?  This is what I mean 
<br>
by the apparent equivalence of groups and individuals from our perspective.
<br>
<p>It looks to me like it takes work to compute a humane morality, work which 
<br>
does not emerge automatically in either groups or singletons.  If there's a 
<br>
group solution I would expect there to exist a corresponding singleton 
<br>
solution.  Even if human morality is inherently group-based, the Friendly 
<br>
AI structure looks like it should work to embody our group morality in a 
<br>
single AI!
<br>
<p>Multiplicities appeal to our human intuitions for fairness, and may indeed 
<br>
have some effectiveness at evoking fairer solutions from groups of humans, 
<br>
but it's essentially a way of *evoking* humaneness that's *already there*. 
<br>
&nbsp;&nbsp;You have the problem of getting a number N of AIs to treat humans 
<br>
humanely even if humans may not be their game-theoretical equals, and this 
<br>
looks like pretty much the same problem whether N=1000 or N=1.  You need an 
<br>
AI that is humane and values sentient life for its own sake.  If N AIs 
<br>
don't value sentient life, they may negotiate cooperation among themselves, 
<br>
but there'd be no reason to extend the compact to include humans - from our 
<br>
perspective the problem of getting a physical system to compute morality is 
<br>
just the same whether we regard the physical system of N AIs as a single 
<br>
object or a group of objects.
<br>
<p>** end of quote **
<br>
<p><em>&gt; Why don't we assume that I'm a Friendly human telling you that I'm pretty
</em><br>
<em>&gt; sure that a single point of failure is A REALLY BAD IDEA(tm).  I would hope
</em><br>
<em>&gt; that you would take this seriously enough that you wouldn't ignore this
</em><br>
<em>&gt; advice and implement your plan solely on the basis of &quot;I am at a loss to
</em><br>
<em>&gt; understand what is gained . . . .&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; By the way, I do understand that you don't acknowledge the distinction
</em><br>
<em>&gt; between multiple AIs with close communication and one AI with partitioning
</em><br>
<em>&gt; but I would submit that one AI with sufficient partitioning SHOULD BE
</em><br>
<em>&gt; considered separate AIs for all intents and purposes.  Or, if the
</em><br>
<em>&gt; partitioning is not sufficient for them, to be considered separate AIs, then
</em><br>
<em>&gt; you need more partitioning in your single AI to create multiple AIs to
</em><br>
<em>&gt; prevent the problem above.
</em><br>
<p>I am in the middle of working out seriously complicated stuff that I am too 
<br>
busy reworking to properly explain.  Sometimes I will be able to explain my 
<br>
reasons.  Sometimes not.  I am getting more and more nervous about time.
<br>
<p>Neighborly human or not, it is not a trivial task to give a specialist 
<br>
fresh advice in his own field.  Figure on the attempt failing at least 95% 
<br>
of the time.  Most of the time, I will have long since thought through 
<br>
everything that occurred to you, in advance, whether that is readily 
<br>
apparent or not.  By all means keep trying, but if I say &quot;I already thought 
<br>
of that,&quot; I did, whether I have time to explain or not.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8862.html">Philip Sutton: "RE: One or more AIs??"</a>
<li><strong>Previous message:</strong> <a href="8860.html">Pedro Machin: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>In reply to:</strong> <a href="8852.html">Mark Waser: "Re: One or more AIs??"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8866.html">Mark Waser: "Re: One or more AIs??"</a>
<li><strong>Reply:</strong> <a href="8866.html">Mark Waser: "Re: One or more AIs??"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8861">[ date ]</a>
<a href="index.html#8861">[ thread ]</a>
<a href="subject.html#8861">[ subject ]</a>
<a href="author.html#8861">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
