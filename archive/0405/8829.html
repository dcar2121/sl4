<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Volitional Morality and Action Judgement</title>
<meta name="Author" content="Mark Waser (mwaser@cox.net)">
<meta name="Subject" content="Re: Volitional Morality and Action Judgement">
<meta name="Date" content="2004-05-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Volitional Morality and Action Judgement</h1>
<!-- received="Sat May 29 19:16:58 2004" -->
<!-- isoreceived="20040530011658" -->
<!-- sent="Sat, 29 May 2004 21:17:30 -0400" -->
<!-- isosent="20040530011730" -->
<!-- name="Mark Waser" -->
<!-- email="mwaser@cox.net" -->
<!-- subject="Re: Volitional Morality and Action Judgement" -->
<!-- id="000d01c445e3$e12b4aa0$0100a8c0@FamilyRoom" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="09a901c445cd$569ba670$6401a8c0@mra02" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Mark Waser (<a href="mailto:mwaser@cox.net?Subject=Re:%20Volitional%20Morality%20and%20Action%20Judgement"><em>mwaser@cox.net</em></a>)<br>
<strong>Date:</strong> Sat May 29 2004 - 19:17:30 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8830.html">Michael Roy Ames: "Re: One or more FAIs??"</a>
<li><strong>Previous message:</strong> <a href="8828.html">Philip Sutton: "One or more FAIs??"</a>
<li><strong>In reply to:</strong> <a href="8825.html">Michael Roy Ames: "Re: Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8833.html">Michael Roy Ames: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Reply:</strong> <a href="8833.html">Michael Roy Ames: "Re: Volitional Morality and Action Judgement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8829">[ date ]</a>
<a href="index.html#8829">[ thread ]</a>
<a href="subject.html#8829">[ subject ]</a>
<a href="author.html#8829">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; &gt; 1.  Why do you believe that a single FAI is the best strategy?
</em><br>
<em>&gt; a) It is simpler to create.
</em><br>
<em>&gt; b) Having one being around with the capability of destroying humanity is
</em><br>
<em>&gt; less risk than having more than one, in the same way as having one human
</em><br>
<em>&gt; being with a Pocket Planetary Destruct (TM) device is less risky than
</em><br>
having
<br>
<em>&gt; more than one.
</em><br>
<p>Ah.  I wasn't clear.  What I was envisioning is a set-up with multiple FAIs
<br>
where none of them are permitted to take an action unless all agree.  Having
<br>
three human beings with nuclear keys (all of which are required to fire the
<br>
missiles) is less risk than having two or one.
<br>
<p><em>&gt; &gt; To me, it's a single point of failure or like a
</em><br>
<em>&gt; &gt; mono-culture of crops in agriculture. One mistake
</em><br>
<em>&gt; &gt; or one disease and POOF! smiley faces everywhere.
</em><br>
<em>&gt; This is a false analogy.  With crops there are many species and varieties
</em><br>
<em>&gt; that will approach the desired result more or less closely.  Choosing a
</em><br>
<em>&gt; variety that doesn't produce well doesn't end humanity.  With an RSI AI
</em><br>
the
<br>
<em>&gt; outcome becomes binary: some sort of continued life, or annihilation.
</em><br>
This
<br>
<em>&gt; is not a false dichotomy; life/death is about as binary as you can get.
</em><br>
<p>Choosing a variety that doesn't produce well and relying only on that
<br>
variety will result in starvation.  It's not that bad an analogy . . . .
<br>
:-)
<br>
<p><em>&gt; &gt; Why do you think that NASA uses multiply redundant
</em><br>
<em>&gt; &gt; systems?
</em><br>
<em>&gt; The multiply redundant systems for NASA's launch vehicles were created
</em><br>
<em>&gt; because having them reduces the overall risk of failure.  This is not true
</em><br>
<em>&gt; of RSI AI.  An RSI AI is analogous to an entire launch vehicle that might
</em><br>
<em>&gt; kill you.  If launching the first one doesn't kill you, then you might try
</em><br>
<em>&gt; again, but otherwise you're dead.
</em><br>
<p>I think that my clarification above addresses this point.
<br>
<p><em>&gt; &gt; 2.  Why do you believe that relying on Eliezer and
</em><br>
<em>&gt; &gt; only Eliezer is a good strategy.
</em><br>
<em>&gt; It is a lousy idea.  I don't believe MW ever said it was a good idea.
</em><br>
<p>I may have misinterpreted him but that was my understanding of what he said.
<br>
<p><em>&gt; &gt; [snip] I do expect serious engagement with the most
</em><br>
<em>&gt; &gt; seriously engaged other participants.
</em><br>
<em>&gt; It *is* always enjoyable when that happens, and often informative, but
</em><br>
<em>&gt; perhaps you shouldn't always expect it.  Sometimes people simply disagree
</em><br>
<em>&gt; about ideas, we *are* all running on slightly different brainware and
</em><br>
<em>&gt; different knowledge bases.  Once an idea has been beaten to death, with
</em><br>
<em>&gt; little progress on either side, then it is sometimes worthwhile to
</em><br>
<em>&gt; disengage.
</em><br>
<p>I don't *always* expect it.  The problem is that it seems to be very much
<br>
the exception rather than the general occurance.
<br>
<p><em>&gt; &gt; Eliezer has devolved to &quot;everything is too dangerous&quot;
</em><br>
<em>&gt; &gt; but &quot;I'm much too busy to discuss it or even write it
</em><br>
<em>&gt; &gt; up for anybody&quot; and I think that is a REALLY BAD
</em><br>
<em>&gt; &gt; THING(tm).
</em><br>
<em>&gt; I suspect this is a REALLY BAD THING(tm) to you because you may be relying
</em><br>
<em>&gt; on Eliezer.  My advice is: don't.  One of SIAI's stated goals is to grow
</em><br>
its
<br>
<em>&gt; programmer team, and with that team improve and develop FAI.  Either
</em><br>
<em>&gt; donating yourself, or persuading others to donate would help tremendously
</em><br>
<em>&gt; toward that goal.  And part of that goal is: &quot;making it so that Eliezer is
</em><br>
<em>&gt; neither considered to be nor in fact a failure point or bottleneck.&quot;
</em><br>
<p>I'm not relying on Eliezer.  I do think that it's unfortunate that others
<br>
apparently are.  And, as I've said, I have volunteered to donate myself.
<br>
<p><em>&gt; In the Ben vs. Eliezer debates each party has a set of cognitive models
</em><br>
they
<br>
<em>&gt; are using to reason about the ideas.  Ben's model projects outcomes along
</em><br>
<em>&gt; one trajectory, Eliezer's along another.  The models are complex, and
</em><br>
would
<br>
<em>&gt; not be easy to communicate using human language, even if both parties had
</em><br>
<em>&gt; perfect introspection, which they do not.  The parties may never agree
</em><br>
until
<br>
<em>&gt; one of them builds a working AI and points at it saying: &quot;There. That's
</em><br>
what
<br>
<em>&gt; I'm talking about.&quot;
</em><br>
<p>Yes, and I've had numerous debates with Ben too . . . . but they don't
<br>
generally end with &quot;I'm too busy to clarify my thoughts to explain it to
<br>
you&quot;.  &lt;italics&gt;My opinion&lt;/italics&gt; is that Eliezer's work would progress a
<br>
lot faster if he could collaborate with others and his friends/supporters
<br>
should be trying to convince him to do so.  Watching Eliezer refuse to give
<br>
the time of day to those individuals who appear most likely to be most
<br>
capable of assisting him (or developing FAI on their own) is most
<br>
frustrating.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mark
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8830.html">Michael Roy Ames: "Re: One or more FAIs??"</a>
<li><strong>Previous message:</strong> <a href="8828.html">Philip Sutton: "One or more FAIs??"</a>
<li><strong>In reply to:</strong> <a href="8825.html">Michael Roy Ames: "Re: Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8833.html">Michael Roy Ames: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Reply:</strong> <a href="8833.html">Michael Roy Ames: "Re: Volitional Morality and Action Judgement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8829">[ date ]</a>
<a href="index.html#8829">[ thread ]</a>
<a href="subject.html#8829">[ subject ]</a>
<a href="author.html#8829">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
