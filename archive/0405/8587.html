<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: ethics</title>
<meta name="Author" content="Chris Healey (chealey@unicom-inc.com)">
<meta name="Subject" content="RE: ethics">
<meta name="Date" content="2004-05-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: ethics</h1>
<!-- received="Wed May 19 16:55:50 2004" -->
<!-- isoreceived="20040519225550" -->
<!-- sent="Wed, 19 May 2004 18:55:31 -0400" -->
<!-- isosent="20040519225531" -->
<!-- name="Chris Healey" -->
<!-- email="chealey@unicom-inc.com" -->
<!-- subject="RE: ethics" -->
<!-- id="000001c43df4$658fe460$0400a8c0@OBSIDIAN" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="40ABC916.4050904@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Chris Healey (<a href="mailto:chealey@unicom-inc.com?Subject=RE:%20ethics"><em>chealey@unicom-inc.com</em></a>)<br>
<strong>Date:</strong> Wed May 19 2004 - 16:55:31 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8588.html">Eliezer S. Yudkowsky: "Re: ethics"</a>
<li><strong>Previous message:</strong> <a href="8586.html">Eliezer S. Yudkowsky: "Re: ethics"</a>
<li><strong>In reply to:</strong> <a href="8583.html">Eliezer S. Yudkowsky: "Re: ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8596.html">Samantha Atkins: "Re: ethics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8587">[ date ]</a>
<a href="index.html#8587">[ thread ]</a>
<a href="subject.html#8587">[ subject ]</a>
<a href="author.html#8587">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Please bear with me if I am being a bit thickheaded.
<br>
<p>If I understand your point, it's that under such a race condition, the
<br>
success differential between having a non-functional take-off
<br>
capability and a functional take-off protocol patched with poorly
<br>
understood probabilistic modules (as minimizally necessary to win the
<br>
race) is likely to approach vanishingly small to zero?  In other
<br>
words, you don't understand the actual problem you are attempting to
<br>
solve, and so you're orthogonal efforts are wasted.  You will fail.
<br>
<p>Hmm, I'm not sure I can disagree there, but also I'm not entirely sure
<br>
why I hesitate to agree.  I sense my reluctance has something to do
<br>
with satisfying the illusion of control over one's fate.  That's a big
<br>
red flag that I'll have give more attention to tracking.
<br>
<p>It's probably a moot consideration anyhow, since taking such a
<br>
departure from such a core policy of avoidance would most likely
<br>
involve enough data to formulate a more specific response.  If that
<br>
data was available at all; and not being available, would not really
<br>
be a part of the decision.
<br>
<p>Am I even close?
<br>
<p>Christopher Healey
<br>
<p><p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20ethics">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20ethics">owner-sl4@sl4.org</a>] On Behalf 
</em><br>
<em>&gt; Of Eliezer S. Yudkowsky
</em><br>
<em>&gt; Sent: Wednesday, May 19, 2004 4:53 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=RE:%20ethics">sl4@sl4.org</a>
</em><br>
<em>&gt; Subject: Re: ethics
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Christopher Healey wrote:
</em><br>
<em>&gt; &gt; John,
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; My quote was truncated.  it should read:
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt;&gt; any black-box emergent-complexity solution is to be avoided &gt;&gt;&gt; 
</em><br>
<em>&gt; &gt;&gt; almost without exception &lt;&lt;&lt;
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; The primary point I was supporting is that if you CAN 
</em><br>
<em>&gt; choose, ALWAYS 
</em><br>
<em>&gt; &gt; choose the more predictable path UNLESS the potential risk of NOT 
</em><br>
<em>&gt; &gt; doing so is greater.  Under a known race-to-singularity 
</em><br>
<em>&gt; situation, it 
</em><br>
<em>&gt; &gt; may be the more rational choice to trade off a relative amount of 
</em><br>
<em>&gt; &gt; predictability for first-to-take-off status.  This modifier to the
</em><br>
<p><em>&gt; &gt; rule, while valid, seems more likely to be used as an &quot;end 
</em><br>
<em>&gt; justifies 
</em><br>
<em>&gt; &gt; means&quot; rationalization by those who would act 
</em><br>
<em>&gt; irresponsibly, so I'd be 
</em><br>
<em>&gt; &gt; suprised if the SIAI focuses on that part of it in their 
</em><br>
<em>&gt; pop campaign.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I would presently support the flat general rule that things 
</em><br>
<em>&gt; which look 
</em><br>
<em>&gt; like minor problems, but which you don't quite understand, 
</em><br>
<em>&gt; are blocker 
</em><br>
<em>&gt; problems until fathomed completely.  Mostly because of the number of
</em><br>
<p><em>&gt; things I have encountered which looked like minor problems, 
</em><br>
<em>&gt; and which I 
</em><br>
<em>&gt; didn't quite understand, and which - as it turned out, after 
</em><br>
<em>&gt; I learned the 
</em><br>
<em>&gt; rules - I desperately needed to understand.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't think there will be a good reason for using probabilistic 
</em><br>
<em>&gt; self-modification techniques, ever.  Deductive 
</em><br>
<em>&gt; self-modification should be 
</em><br>
<em>&gt; quite sufficient.  There's a difference between hope and 
</em><br>
<em>&gt; creating a system 
</em><br>
<em>&gt; that can be rationally predicted to work, and the difference 
</em><br>
<em>&gt; is that hope 
</em><br>
<em>&gt; doesn't help.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The part about the &quot;rational tradeoff&quot; ignores the fact that 
</em><br>
<em>&gt; until you 
</em><br>
<em>&gt; understand something, you have no idea how much you need to 
</em><br>
<em>&gt; understand it; 
</em><br>
<em>&gt; you are simply guessing.  Every time I see someone try to get 
</em><br>
<em>&gt; away with 
</em><br>
<em>&gt; this guess, including my memories of my past self, they are lethally
</em><br>
<p><em>&gt; wrong.  To build an FAI you must aspire to a higher level of 
</em><br>
<em>&gt; understanding 
</em><br>
<em>&gt; than poking around in design space until you find something 
</em><br>
<em>&gt; that appears 
</em><br>
<em>&gt; to work.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I do not expect anyone who *actually* understands FAI to 
</em><br>
<em>&gt; *ever* use the 
</em><br>
<em>&gt; argument of &quot;We don't understand this, but we'll use it 
</em><br>
<em>&gt; anyway because of 
</em><br>
<em>&gt; &lt;nitwit utilitarian argument&gt;.&quot;  The nitwit argument only 
</em><br>
<em>&gt; applies because 
</em><br>
<em>&gt; the speaker is too ignorant to realize that they have *no* chance of
</em><br>
<p><em>&gt; success, that the *only* reason they think they can build an 
</em><br>
<em>&gt; FAI without 
</em><br>
<em>&gt; understanding is that they lack the understanding to know 
</em><br>
<em>&gt; this is impossible.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; -- 
</em><br>
<em>&gt; Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt; Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<em>&gt; 
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8588.html">Eliezer S. Yudkowsky: "Re: ethics"</a>
<li><strong>Previous message:</strong> <a href="8586.html">Eliezer S. Yudkowsky: "Re: ethics"</a>
<li><strong>In reply to:</strong> <a href="8583.html">Eliezer S. Yudkowsky: "Re: ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8596.html">Samantha Atkins: "Re: ethics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8587">[ date ]</a>
<a href="index.html#8587">[ thread ]</a>
<a href="subject.html#8587">[ subject ]</a>
<a href="author.html#8587">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
