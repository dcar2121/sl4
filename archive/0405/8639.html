<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ethics</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: ethics">
<meta name="Date" content="2004-05-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ethics</h1>
<!-- received="Sat May 22 02:29:43 2004" -->
<!-- isoreceived="20040522082943" -->
<!-- sent="Sat, 22 May 2004 01:29:39 -0700" -->
<!-- isosent="20040522082939" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: ethics" -->
<!-- id="2A6443B6-ABCA-11D8-86D4-000A95B1AFDE@objectent.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="40AE803B.8050801@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20ethics"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Sat May 22 2004 - 02:29:39 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8640.html">Samantha Atkins: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8638.html">Samantha Atkins: "Re: ethics"</a>
<li><strong>In reply to:</strong> <a href="8627.html">Eliezer Yudkowsky: "Re: ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8625.html">J. Andrew Rogers: "Re: ethics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8639">[ date ]</a>
<a href="index.html#8639">[ thread ]</a>
<a href="subject.html#8639">[ subject ]</a>
<a href="author.html#8639">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On May 21, 2004, at 3:18 PM, Eliezer Yudkowsky wrote:
<br>
<p><em>&gt; If you're familiar with the expected utility formalism and the notion 
</em><br>
<em>&gt; of utility functions, then consider a utility function U(x), and an 
</em><br>
<em>&gt; immense amount of computing power devoted to steering the universe 
</em><br>
<em>&gt; into states with a 99.99% or better expectation that U(x) &gt; T.  (Note 
</em><br>
<em>&gt; that this is a satisficer, not an expected utility maximizer.)  The 
</em><br>
<em>&gt; idea is that even if there's a huge amount of computing power devoted 
</em><br>
<em>&gt; to looking for actions/plans/designs that achieve U(x) &gt; T, such that 
</em><br>
<em>&gt; the specific solutions chosen may be beyond human intelligence, the 
</em><br>
<em>&gt; *ends* to which the solutions operate are humanly comprehensible.  We 
</em><br>
<em>&gt; can say of the system that it steers the futures into outcomes that 
</em><br>
<em>&gt; satisfice U(x), even if we can't say how.
</em><br>
<em>&gt;
</em><br>
<p>This seems to have the classic 3 wishes from a genie problem.  We may 
<br>
not be intelligent enough to formulate the &quot;wish&quot; clearly enough that 
<br>
it does not generate quite unintended evil consequences when &quot;granted&quot; 
<br>
by a sufficiently powerful process/being.
<br>
<p><p><em>&gt; Actually you need a great deal more complex goal structure than this, 
</em><br>
<em>&gt; to achieve a satisfactory outcome.  In the extrapolated volition 
</em><br>
<em>&gt; version of Friendly AI that I'm presently working with, U(x) is 
</em><br>
<em>&gt; constructed in a complex way from existing humans, and may change if 
</em><br>
<em>&gt; the humans themselves change.  Even the definition of how volition is 
</em><br>
<em>&gt; extrapolated may change, if that's what we want.
</em><br>
<em>&gt;
</em><br>
<p>Who/what is dong the construction of U(x)?
<br>
<p><em>&gt; (I'm starting to get nervous about my ability to define an 
</em><br>
<em>&gt; extrapolation powerful enough to incorporate the reason why we might 
</em><br>
<em>&gt; want to rule out the creation of sentient beings within the 
</em><br>
<em>&gt; simulation, without simulating sentient beings.  However, I've been 
</em><br>
<em>&gt; nervous about problems that looked more impossible than this, and 
</em><br>
<em>&gt; solved them.  So I'm not giving up until I try.)
</em><br>
<p>Is a sentient being within a simulation any less sentient than any 
<br>
other?   Perhaps you are assuming or which to assume boundaries where 
<br>
none really exist.
<br>
<em>&gt;
</em><br>
<em>&gt; The problem word is &quot;constrain&quot;.  I would say rather that I choose an 
</em><br>
<em>&gt; FAI into existence, and that what the FAI does is choose.  The U(x) 
</em><br>
<em>&gt; constrains the future, not the FAI; the FAI, in a strong sense, is 
</em><br>
<em>&gt; *defined* by the choice of U(x).  That becomes the what-it-does, the 
</em><br>
<em>&gt; nature of the FAI; it is no more a constraint than physics is 
</em><br>
<em>&gt; constrained to be physics, no more to be constrasted to some separate 
</em><br>
<em>&gt; will than I want to break out of being Eliezer and become a teapot.
</em><br>
<p><p>It is a much of a constraint as choosing the fundamental physical 
<br>
constants of a universe in other words.  :-)
<br>
<p><p><em>&gt;
</em><br>
<em>&gt; I would construct a fully reflective optimization process capable of 
</em><br>
<em>&gt; indefinitely self-enhancing its capability to roughly satisfice our 
</em><br>
<em>&gt; collective volition, to the exactly optimal degree of roughness we 
</em><br>
<em>&gt; would prefer.  Balancing between the urgency of our needs; and our 
</em><br>
<em>&gt; will to learn self-reliance, make our own destinies, choose our work 
</em><br>
<em>&gt; and do it ourselves.
</em><br>
<em>&gt;
</em><br>
<p>The phrase &quot;our collective volition&quot; is worrisome.  It is actually from 
<br>
above your extrapolation from actual human beings of what &quot;our 
<br>
collective volition&quot; is, correct?
<br>
<p><em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; What surprises me most here is the apparently widespread presence of 
</em><br>
<em>&gt;&gt; this concern in the community subscribed to this list -- the reasons 
</em><br>
<em>&gt;&gt; for my difficulty in seeing how FAI can even in principle be created 
</em><br>
<em>&gt;&gt; have been rehearsed by others and I have nothing to add at this
</em><br>
<em>&gt;&gt; point. It seems that I am one of many who feel that this should be
</em><br>
<em>&gt;&gt; SIAI FAQ number 1.  Have you addressed it in detail online anywhere?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Not really.  I think that, given the difficulty of these problems, I 
</em><br>
<em>&gt; cannot simultaneously solve them and explain them.  Though I'm willing 
</em><br>
<em>&gt; to take occasional potshots.
</em><br>
<em>&gt;
</em><br>
<p>I greatly sympathize.  However, explanations beyond potshots clarify 
<br>
thinking and bring in other viewpoints that may be essential.   In 
<br>
particular the goals of the system should be explained well and with 
<br>
clarity to the maximal extent possible before solving the technical 
<br>
problems of their implementation or at least a theory of their 
<br>
implementation.    A lot of the questions are about goal-level 
<br>
concerns.
<br>
<p>-samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8640.html">Samantha Atkins: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8638.html">Samantha Atkins: "Re: ethics"</a>
<li><strong>In reply to:</strong> <a href="8627.html">Eliezer Yudkowsky: "Re: ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8625.html">J. Andrew Rogers: "Re: ethics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8639">[ date ]</a>
<a href="index.html#8639">[ thread ]</a>
<a href="subject.html#8639">[ subject ]</a>
<a href="author.html#8639">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
