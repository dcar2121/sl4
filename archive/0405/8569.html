<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: The SIAI Voice by the Singularity Institute - May 2004</title>
<meta name="Author" content="Tyler Emerson (emerson@intelligence.org)">
<meta name="Subject" content="The SIAI Voice by the Singularity Institute - May 2004">
<meta name="Date" content="2004-05-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>The SIAI Voice by the Singularity Institute - May 2004</h1>
<!-- received="Tue May 18 12:19:14 2004" -->
<!-- isoreceived="20040518181914" -->
<!-- sent="Tue, 18 May 2004 13:18:33 -0500" -->
<!-- isosent="20040518181833" -->
<!-- name="Tyler Emerson" -->
<!-- email="emerson@intelligence.org" -->
<!-- subject="The SIAI Voice by the Singularity Institute - May 2004" -->
<!-- id="200405181818.i4IIIan02262@tick.javien.com" -->
<!-- charset="us-ascii" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tyler Emerson (<a href="mailto:emerson@intelligence.org?Subject=Re:%20The%20SIAI%20Voice%20by%20the%20Singularity%20Institute%20-%20May%202004"><em>emerson@intelligence.org</em></a>)<br>
<strong>Date:</strong> Tue May 18 2004 - 12:18:33 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8570.html">Eliezer S. Yudkowsky: "Re: ethics"</a>
<li><strong>Previous message:</strong> <a href="8568.html">Tyler Emerson: "The SIAI Voice by the Singularity Institute - May 2004"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8569">[ date ]</a>
<a href="index.html#8569">[ thread ]</a>
<a href="subject.html#8569">[ subject ]</a>
<a href="author.html#8569">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The SIAI Voice . May 2004
<br>
<p>&nbsp;Bulletin of the Singularity Institute for Artificial Intelligence
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A nonprofit and community for humane AI research
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="mailto:institute@intelligence.org?Subject=Re:%20The%20SIAI%20Voice%20by%20the%20Singularity%20Institute%20-%20May%202004">institute@intelligence.org</a>
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To view the online version:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://www.intelligence.org/news/newsletter.html">http://www.intelligence.org/news/newsletter.html</a>
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To receive the bulletin by email every other month: 
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://www.intelligence.org/news/subscribe.html">http://www.intelligence.org/news/subscribe.html</a>
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>CONTENTS
<br>
<p>&nbsp;1. 2004 Website Campaign
<br>
&nbsp;2. 2004 Challenge Grant Challenge
<br>
&nbsp;3. Executive Director - Tyler Emerson
<br>
&nbsp;4. Advocacy Director - Michael Anissimov
<br>
&nbsp;5. Featured Content: What is the Singularity?
<br>
&nbsp;6. Donors for March and April
<br>
&nbsp;7. Singularity Institute FAQ
<br>
&nbsp;8. AI Project Update
<br>
&nbsp;9. New at our Website
<br>
10. Volunteer Contributions
<br>
11. Volunteer Meeting
<br>
12. Volunteer Opportunities
<br>
13. Q&amp;A with Eliezer Yudkowsky
<br>
14. Singularity Statement from Anders Sandberg
<br>
15. Singularity Quote from Ray Kurzweil
<br>
16. Events - Transvision 2004
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>The first bulletin from the Singularity Institute. We hope you find it
<br>
valuable. Comments on what we've done well, poorly, or have missed,
<br>
are welcomed. We graciously ask to know what you would like from our
<br>
updates in the coming months.
<br>
<p>Thank you for giving time to explore the Institute.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tyler Emerson
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="mailto:emerson@intelligence.org?Subject=Re:%20The%20SIAI%20Voice%20by%20the%20Singularity%20Institute%20-%20May%202004">emerson@intelligence.org</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(417) 840-5968
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>1. 2004 WEBSITE CAMPAIGN
<br>
<p>3 Laws Unsafe will be a website campaign from SIAI. The campaign will
<br>
tie in to the July 16th release of &quot;I, Robot,&quot; the feature film based
<br>
on Isaac Asimov's short story collection of the same name where his 3
<br>
Laws of Robotics were first introduced.
<br>
<p>The 3 Laws of Robotics represent a popular view of how to construct
<br>
moral AI, and their failures were often explored by Isaac Asimov in
<br>
his stories. What we hope to do is advance the Asimov tradition of
<br>
deconstructing the 3 Laws. We want to encourage critical, technical
<br>
thinking on whether they're real solutions to moral AI creation.
<br>
<p>If you can contribute to the success of 3 Laws Unsafe, email
<br>
institute@intelligence.org. We're especially looking for graphic and site
<br>
designers who can create the site in blog format, promoters who can
<br>
help ensure that it has a high search engine ranking for keyword
<br>
combinations related to the film, and writers who can submit content.
<br>
<p>This project is urgent because of the film's early July release. Our
<br>
deepest thanks to everyone who contributes to its success.
<br>
<p>3 Laws Unsafe &gt;
<br>
<a href="http://www.intelligence.org/asimovlaws.html">http://www.intelligence.org/asimovlaws.html</a>
<br>
<p>Ways to Contribute &gt;
<br>
<a href="http://www.intelligence.org/action/opportunities.html">http://www.intelligence.org/action/opportunities.html</a>
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>2. 2004 CHALLENGE GRANT CHALLENGE
<br>
<p>The Singularity Institute is now seeking major donors to provide
<br>
matching funds for our $10,000 Challenge Grant Challenge for Research
<br>
Fellow Eliezer Yudkowsky - one of the leading experts on the
<br>
singularity and the development of moral AI.
<br>
<p>Major donors to the Challenge Grant Challenge will match any donations
<br>
up to $10,000, resulting in $20,000 in possible donations. Once the
<br>
pledges for matching donations are secured, the Challenge Grant itself
<br>
will run for 90 days.
<br>
<p>Donors may pledge by emailing <a href="mailto:institute@intelligence.org?Subject=Re:%20The%20SIAI%20Voice%20by%20the%20Singularity%20Institute%20-%20May%202004">institute@intelligence.org</a> or phoning (404)
<br>
550-3847. Our sincere thanks to the first major donor, Jason
<br>
Joachim, who has pledged $2,000.
<br>
<p>All funds go toward a subsistence salary for Yudkowsky so that he may
<br>
continue his critical research on the theory of Friendly AI - the
<br>
cornerstone of our AI project that must be sufficiently complete
<br>
before the project can responsibly begin.
<br>
<p>For more on the value of Yudkowsky's research, see:
<br>
<p>The Necessity of Friendly AI &gt;
<br>
<a href="http://www.intelligence.org/friendly/why-friendly.html">http://www.intelligence.org/friendly/why-friendly.html</a>
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>3. EXECUTIVE DIRECTOR - TYLER EMERSON
<br>
<p>On March 4, 2004, the Singularity Institute announced Tyler Emerson as
<br>
our Executive Director. Emerson will be responsible for guiding the
<br>
Institute. His focus is in nonprofit management, marketing,
<br>
relationship fundraising, leadership and planning. He will seek to
<br>
cultivate a larger and more cohesive community that has the necessary
<br>
resources to develop Friendly AI. He can be reached at
<br>
emerson@intelligence.org.
<br>
<p>More &gt;
<br>
<a href="http://www.intelligence.org/about.html">http://www.intelligence.org/about.html</a>
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>4. ADVOCACY DIRECTOR - MICHAEL ANISSIMOV
<br>
<p>On April 7, 2004, the Singularity Institute announced Michael
<br>
Anissimov as our Advocacy Director. Michael has been an active
<br>
volunteer for two years, and one of the more prominent voices in the
<br>
singularity community. He is committed and thoughtful, and we feel
<br>
fortunate to have him help lead our advocacy. In 2004 and beyond,
<br>
Michael will represent SIAI at key conferences, engage in outreach
<br>
efforts to communities and individuals, and perform writing tasks for
<br>
conveying the Institute's mission to a wider audience. He can be
<br>
reached at anissimov@intelligence.org.
<br>
<p>More &gt;
<br>
<a href="http://www.acceleratingfuture.com/michael">http://www.acceleratingfuture.com/michael</a>
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>5. FEATURED CONTENT: WHAT IS THE SINGULARITY?
<br>
<p>The singularity is the technological creation of smarter-than-human
<br>
intelligence. There are several technologies that are often mentioned
<br>
as heading in this direction. The most commonly mentioned is probably
<br>
Artificial Intelligence, but there are others: direct brain-computer
<br>
interfaces, biological augmentation of the brain, genetic engineering,
<br>
ultra-high-resolution scans of the brain followed by computer
<br>
emulation. Some of these technologies seem likely to arrive much
<br>
earlier than the others, but there are nonetheless several independent
<br>
technologies all heading in the direction of the singularity - several
<br>
different technologies which, if they reached a threshold level of
<br>
sophistication, would enable the creation of smarter-than-human
<br>
intelligence.
<br>
<p>More &gt;
<br>
<a href="http://www.intelligence.org/what-singularity.html">http://www.intelligence.org/what-singularity.html</a>
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>6. DONORS FOR MARCH AND APRIL
<br>
<p>We offer our deepest gratitude to the following donors. They realize
<br>
the extraordinary utility of the Singularity Institute's pursuit:
<br>
responsible intelligence enhancement, a Friendly singularity, through
<br>
Friendly AI research. They have taken that very crucial step of
<br>
financial support for SIAI's research. Whether it is $10 or $1,000,
<br>
more or less, one time or each month, we ask that each in-principle
<br>
supporter become a regular donor.
<br>
<p>Major Contributions:
<br>
<p>&nbsp;&nbsp;* Edwin Evans
<br>
&nbsp;&nbsp;&nbsp;&nbsp;$7,000
<br>
&nbsp;&nbsp;* Mikko Rauhala
<br>
&nbsp;&nbsp;&nbsp;&nbsp;$1,200
<br>
<p>Periodic Contributions:
<br>
<p>&nbsp;&nbsp;* Jason Abu-Aitah
<br>
&nbsp;&nbsp;&nbsp;&nbsp;$10 (monthly)
<br>
&nbsp;&nbsp;* David Hansen
<br>
&nbsp;&nbsp;&nbsp;&nbsp;$100 (monthly)
<br>
&nbsp;&nbsp;* Jason Joachim
<br>
&nbsp;&nbsp;&nbsp;&nbsp;$150 (monthly)
<br>
&nbsp;&nbsp;* Aaron McBride
<br>
&nbsp;&nbsp;&nbsp;&nbsp;$10 (monthly)
<br>
&nbsp;&nbsp;* Ashley Thomas
<br>
&nbsp;&nbsp;&nbsp;&nbsp;$10 (monthly)
<br>
<p>One-Time Contributions:
<br>
<p>&nbsp;&nbsp;* Anonymous
<br>
&nbsp;&nbsp;&nbsp;&nbsp;$200
<br>
&nbsp;&nbsp;* Michael Wilson
<br>
&nbsp;&nbsp;&nbsp;&nbsp;$200
<br>
<p>Donate to the Singularity Institute &gt;
<br>
<a href="http://www.intelligence.org/donate.html">http://www.intelligence.org/donate.html</a>
<br>
<p>Why Even Small Donations Matter &gt;
<br>
<a href="http://www.intelligence.org/small-donations-matter.html">http://www.intelligence.org/small-donations-matter.html</a>
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>7. SINGULARITY INSTITUTE FAQ
<br>
<p>Q: Why does your current research focus on Artificial Intelligence?
<br>
<p>A: Artificial Intelligence is easiest to get started on by comparison
<br>
with, say, neuroelectronics. Artificial Intelligence is easier to
<br>
leverage - in our estimate, a small to medium-sized organization
<br>
potentially can do more to advance Artificial Intelligence than to
<br>
advance neuroelectronics. Furthermore, given the relative rates of
<br>
progress in the underlying technologies, our current best guess is
<br>
that Artificial Intelligence will be developed before brain-computer
<br>
interfaces; hence, to accelerate the singularity, one should
<br>
accelerate the development of Artificial Intelligence; to protect the
<br>
integrity of the singularity, one should protect the integrity of
<br>
Artificial Intelligence (i.e., Friendly AI). Singularity strategy is a
<br>
complex question which requires considering not just the development
<br>
rate of one technology, but the relative development rates of
<br>
different technologies and the relative amount by which different
<br>
technologies can be accelerated or influenced. At this time Artificial
<br>
Intelligence appears to be closer to being developed, to be more
<br>
easily accelerable, to require fewer resources to initiate a serious
<br>
project, and to offer more benefit from interim successes.
<br>
<p>If the Singularity Institute had enough resources to fully support
<br>
multiple projects, we would branch out; but until then, it seems wise
<br>
to focus research efforts on one project.
<br>
<p>More &gt;
<br>
<a href="http://www.intelligence.org/institute-faq.html">http://www.intelligence.org/institute-faq.html</a> 
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>8. AI PROJECT UPDATE
<br>
<p>The centerpiece of the SIAI's effort to bring about a Friendly
<br>
singularity is an upcoming software development project. The aim is to
<br>
produce the world's first artificial general intelligence; a Friendly
<br>
&quot;seed AI.&quot; To do this we will employ the most advanced theoretical
<br>
framework for seed AI available, the architecture derived from, but
<br>
much more comprehensive and sophisticated, than that described in
<br>
&quot;Levels of Organization in General Intelligence.&quot; As of May 2004, this
<br>
framework is close to completion, but a great deal of work remains to
<br>
be done on the associated Friendliness theory. It is the policy of the
<br>
Singularity Institute to not initiate a project with a major potential
<br>
for existential risk until it has been proven that the net result will
<br>
be a positive one for all of humanity. Fortunately we have made strong
<br>
progress with a formal theory of Friendliness (the document &quot;Creating
<br>
Friendly AI&quot; describes an informal precursor) and will continue to
<br>
develop it until it's complete enough to allow project initiation.
<br>
<p>Although we are not yet ready to start building Friendly AI, we are
<br>
close enough to begin forming the development team. At present, we
<br>
have two confirmed team members, including Eliezer Yudkowsky. The SIAI
<br>
is now actively searching for Singularitarians with software
<br>
engineering and cognitive science expertise to join the development
<br>
team. Volunteers who make the grade may be able to start work on a
<br>
part or full-time basis immediately.
<br>
<p>The search for suitable Friendly AI developers is a top priority for
<br>
SIAI. If you believe you may be suitable or know of someone who may,
<br>
please read &quot;Team member requirements&quot; and then consider getting in
<br>
touch at institute@intelligence.org. We are searching for nothing less than
<br>
the core team to fulfill our mission; we need the very best we can find.
<br>
<p>Team member requirements &gt;
<br>
<a href="http://www.sl4.org/bin/wiki.pl?SoYouWantToBeASeedAIProgrammer">http://www.sl4.org/bin/wiki.pl?SoYouWantToBeASeedAIProgrammer</a>
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>9. NEW AT OUR WEBSITE
<br>
<p>About Us Page Updated &gt;&gt; <a href="http://www.intelligence.org/about.html">http://www.intelligence.org/about.html</a>
<br>
Read about our mission, board, staff and accomplishments
<br>
<p>Chat &gt;&gt; <a href="http://www.intelligence.org/chat">http://www.intelligence.org/chat</a>
<br>
Access our volunteer chat room through the Java applet
<br>
<p>Singularity Quotes &gt;&gt; <a href="http://www.intelligence.org/comments/quotes.html">http://www.intelligence.org/comments/quotes.html</a>
<br>
Quotes from Vernor Vinge, Ray Kurzweil, Hans Moravec and more
<br>
<p>Tell Others about the Singularity Institute &gt;&gt;
<br>
<a href="http://www.intelligence.org/tell-others.html">http://www.intelligence.org/tell-others.html</a> 
<br>
Spread the knowledge - open the opportunity - to your email circle
<br>
<p>Become a Singularity Volunteer &gt;&gt; 
<br>
<a href="http://www.intelligence.org/volunteer.html">http://www.intelligence.org/volunteer.html</a> 
<br>
Contribute your time and talent to a safe singularity
<br>
<p>Why We Need Friendly AI &gt;&gt;
<br>
<a href="http://www.intelligence.org/friendly/why-friendly.html">http://www.intelligence.org/friendly/why-friendly.html</a> 
<br>
Why Moore's Law is no friend to Friendly AI research
<br>
<p>Donations Page Updated &gt;&gt; <a href="http://www.intelligence.org/donate.html">http://www.intelligence.org/donate.html</a>
<br>
Contributions may be made monthly or yearly
<br>
<p>Feedback &gt;&gt; <a href="http://www.intelligence.org/feedback.html">http://www.intelligence.org/feedback.html</a>
<br>
Your comments, questions and suggestions are welcomed
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>10. VOLUNTEER CONTRIBUTIONS
<br>
<p>The notable progress made in March and April was possible because of
<br>
considerable volunteer help. We especially want to thank Christian
<br>
Rovner, who made tangible progress each week for eight weeks. It is no
<br>
lie to feel blessed to have him with SIAI.
<br>
<p>Special thanks to these individuals for their effort: Michael Roy
<br>
Ames, Joshua Amy, Michael Anissimov, Nick Hay, Manny Halos, Shilpa
<br>
Kukunooru, Tommy McCabe, Tyrone Pow, and Chris Rovner.
<br>
<p>View Contributions &gt;
<br>
<a href="http://www.intelligence.org/action/contributions.html">http://www.intelligence.org/action/contributions.html</a>
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>11. VOLUNTEER OPPORTUNITIES
<br>
<p>We believe that W. Clement Stone's aphorism &quot;Tell everyone what you
<br>
want to do and someone will want to help you do it&quot; will hold true for
<br>
our charitable mission.
<br>
<p>If you can contribute this year, please email institute@intelligence.org. 
<br>
<p>View Opportunities &gt;
<br>
<a href="http://www.intelligence.org/action/opportunities/">http://www.intelligence.org/action/opportunities/</a>
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>12. WEEKLY VOLUNTEER MEETING
<br>
<p>The Singularity Institute hosts a chat meeting for volunteers every
<br>
Sunday at 7 PM EST (GMT-5). The Internet Relay Chat (IRC) server is
<br>
intelligence.org, port 6667; chat room #siaiv. Each meeting revolves
<br>
around planning and action.
<br>
<p>More &gt;
<br>
<a href="http://www.intelligence.org/chat/">http://www.intelligence.org/chat/</a>
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>13. Q&amp;A WITH ELIEZER YUDKOWSKY
<br>
<p>It seems that you're trying to achieve an AI with the philosophical
<br>
complexity roughly equal to or beyond that of, e.g., Mohandas Gandhi,
<br>
Siddhartha Gautama, and Martin Luther King, Jr. Do these individuals
<br>
represent to you the &quot;heart&quot; of humanity?
<br>
<p>What they represent to me are moral archetypes, not just of
<br>
selflessness but of moral reason, of moral philosophy. Whether they
<br>
were really as good as their PR suggests is a separate issue, not that
<br>
I'm suggesting they weren't - just that it doesn't quite matter. The
<br>
key point is that we ourselves recognize that there is such a thing as
<br>
greater and lesser altruism, and greater and lesser wisdom of moral
<br>
argument, and that from this recognition proceeds our respect of those
<br>
who embody the greater altruism and the greater wisdom. There is
<br>
something to strive for - an improvement that can be perceived as
<br>
&quot;improvement&quot; even by those who are not at that level; a road that is
<br>
open to those not already at the destination. Anyone who can recognize
<br>
Gandhi as an ideal, and not just someone with strangely different
<br>
goals, is someone who occupies a common moral frame of reference with
<br>
Gandhi, but less advanced in terms of content, despite a shared
<br>
structure. So what the statement &quot;Put the heart of humanity into a
<br>
Friendly AI&quot; symbolizes is the idea of moral improvement, and the idea
<br>
that a Friendly AI can improve to or beyond levels that we recognize
<br>
as ideal levels (e.g., the level of a moral philosopher or of Martin
<br>
Luther King Jr.).
<br>
<p>More &gt;
<br>
<a href="http://www.intelligence.org/yudkowsky/">http://www.intelligence.org/yudkowsky/</a>
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>14. SINGULARITY STATEMENT FROM ANDERS SANDBERG
<br>
<p>Anders Sandberg, Science Director, Eudoxa:
<br>
<p>The research of SIAI is essentially a bold attempt to explore Smale's
<br>
18th problem: What are the limits of intelligence, both artificial and
<br>
human? (S. Smale, Mathematical Problems for the Next Century,
<br>
Mathematical Intelligence, Spring '98.) Developing a theory for how
<br>
intelligent systems can improve the way they solve problems has both
<br>
practical and theoretical importance. SIAI is also one of few
<br>
organisations devoted to the study of general motivational systems and
<br>
how they might be designed to achieve desired behavior - another
<br>
open-ended issue of great practical and ethical importance.
<br>
<p>More &gt;
<br>
<a href="http://www.intelligence.org/comments/statements.html">http://www.intelligence.org/comments/statements.html</a>
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>15. SINGULARITY QUOTE FROM RAY KURZWEIL
<br>
<p>Ray Kurzweil, &quot;The Law of Accelerating Returns,&quot; 2001:
<br>
<a href="http://www.kurzweilai.net/articles/art0134.html?printable=1">http://www.kurzweilai.net/articles/art0134.html?printable=1</a>
<br>
<p>People often go through three stages in examining the impact of future
<br>
technology: awe and wonderment at its potential to overcome age old
<br>
problems, then a sense of dread at a new set of grave dangers that
<br>
accompany these new technologies, followed, finally and hopefully, by
<br>
the realization that the only viable and responsible path is to set a
<br>
careful course that can realize the promise while managing the peril.
<br>
<p>More &gt;
<br>
<a href="http://www.intelligence.org/comments/quotes.html">http://www.intelligence.org/comments/quotes.html</a>
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>16. EVENTS - TRANSVISION 2004
<br>
<p>The World Transhumanist Association's annual event, TransVision, will
<br>
be held at the University of Toronto from August 6th-8th, 2004. The
<br>
Singularity Institute is fortunate to be a sponsor of TransVision, and
<br>
will have members attending or giving presentations.
<br>
<p>Proposal submissions for the conference are being accepted until June
<br>
1st. Registration costs range from $100 to $150.
<br>
<p>Conference speakers include:
<br>
<p>&nbsp;&nbsp;* Steve Mann, Inventor of the wearable computer
<br>
&nbsp;&nbsp;* Stelarc, Renowned Australian artist
<br>
&nbsp;&nbsp;* Howard Bloom, Author of The Lucifer Principle
<br>
&nbsp;&nbsp;* James Hughes, Author of Cyborg Democracy
<br>
&nbsp;&nbsp;* Nick Bostrom, Chair of the World Transhumanist Association
<br>
&nbsp;&nbsp;* Natasha Vita-More, President of the Extropy Institute
<br>
&nbsp;&nbsp;* Aubrey de Grey, Cofounder of the Methuselah Mouse Prize
<br>
<p>Transvision 2004 &gt;
<br>
<a href="http://www.transhumanism.org/tv/2004/">http://www.transhumanism.org/tv/2004/</a>
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>The SIAI Voice is produced by the Singularity Institute for Artificial
<br>
Intelligence.
<br>
<p>The Singularity Institute for Artificial Intelligence is a 501(c)(3)
<br>
nonprofit organization for the pursuit of Friendly AI and responsible
<br>
intelligence enhancement - a mission of immense potential. Since
<br>
intelligence determines how well problems are solved, the responsible
<br>
enhancement of intelligence - a safe singularity - will make difficult
<br>
problems, such as the prevention and treatment of Alzheimer's and
<br>
AIDS, much easier to solve. If intelligence is improved greatly, every
<br>
humanitarian problem we face will be more amenable to solution.
<br>
Because AI is positioned to be the first technology to enhance
<br>
intelligence significantly, SIAI concentrates on the research and
<br>
development of humane AI. By solely pursuing a beneficial singularity,
<br>
the Institute presents the rare opportunity for rational philanthropy.
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<p>For comments or questions, contact us at (404) 550-3847,
<br>
<a href="mailto:institute@intelligence.org?Subject=Re:%20The%20SIAI%20Voice%20by%20the%20Singularity%20Institute%20-%20May%202004">institute@intelligence.org</a> , or visit our website:
<br>
<p><a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
<br>
<p>The movement for a safe singularity advances by word of mouth. If you
<br>
believe what we do is valuable, it's vital that you do tell others.
<br>
<p>Share the Bulletin &gt;
<br>
<a href="http://www.intelligence.org/tell-others.html">http://www.intelligence.org/tell-others.html</a>
<br>
<p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8570.html">Eliezer S. Yudkowsky: "Re: ethics"</a>
<li><strong>Previous message:</strong> <a href="8568.html">Tyler Emerson: "The SIAI Voice by the Singularity Institute - May 2004"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8569">[ date ]</a>
<a href="index.html#8569">[ thread ]</a>
<a href="subject.html#8569">[ subject ]</a>
<a href="author.html#8569">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
