<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ethics</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: ethics">
<meta name="Date" content="2004-05-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ethics</h1>
<!-- received="Fri May 21 16:18:37 2004" -->
<!-- isoreceived="20040521221837" -->
<!-- sent="Fri, 21 May 2004 18:18:35 -0400" -->
<!-- isosent="20040521221835" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: ethics" -->
<!-- id="40AE803B.8050801@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="E1BR8HE-0003rK-00@ag24.gen.cam.ac.uk" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20ethics"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri May 21 2004 - 16:18:35 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8628.html">Ben Goertzel: "RE: ethics"</a>
<li><strong>Previous message:</strong> <a href="8626.html">Eliezer Yudkowsky: "Re: ethics"</a>
<li><strong>In reply to:</strong> <a href="8617.html">Aubrey de Grey: "Re: ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8628.html">Ben Goertzel: "RE: ethics"</a>
<li><strong>Reply:</strong> <a href="8628.html">Ben Goertzel: "RE: ethics"</a>
<li><strong>Reply:</strong> <a href="8639.html">Samantha Atkins: "Re: ethics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8627">[ date ]</a>
<a href="index.html#8627">[ thread ]</a>
<a href="subject.html#8627">[ subject ]</a>
<a href="author.html#8627">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Aubrey de Grey wrote:
<br>
<em>&gt; Eliezer Yudkowsky wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; Similarly, FAI doesn't require that I understand an existing
</em><br>
<em>&gt;&gt; biological system, or that I understand an arbitrarily selected
</em><br>
<em>&gt;&gt; nonhuman system, but that I build a system with the property of
</em><br>
<em>&gt;&gt; understandability.  Or to be more precise, that I build an
</em><br>
<em>&gt;&gt; understandable system with the property of predictable
</em><br>
<em>&gt;&gt; niceness/Friendliness, for a well-specified abstract predicate 
</em><br>
<em>&gt;&gt; thereof.  Just *any* system that's understandable wouldn't be
</em><br>
<em>&gt;&gt; enough.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What I would like to see is an argument that there can, in principle,
</em><br>
<em>&gt; be a system with the property of understandability (by at least a few
</em><br>
<em>&gt; 21st century humans) and also with the property of considerably
</em><br>
<em>&gt; greater than human cognitive function.  (I avoid &quot;intelliigence&quot;
</em><br>
<em>&gt; because I want to try to focus the discussion on function, and thence
</em><br>
<em>&gt; on the reasons why we may find these machines worth making, leaving
</em><br>
<em>&gt; aside for the moment the idea that we need to invent FAI before
</em><br>
<em>&gt; anyone invents unfriendly AI.)
</em><br>
<p>If you're familiar with the expected utility formalism and the notion of 
<br>
utility functions, then consider a utility function U(x), and an immense 
<br>
amount of computing power devoted to steering the universe into states 
<br>
with a 99.99% or better expectation that U(x) &gt; T.  (Note that this is a 
<br>
satisficer, not an expected utility maximizer.)  The idea is that even 
<br>
if there's a huge amount of computing power devoted to looking for 
<br>
actions/plans/designs that achieve U(x) &gt; T, such that the specific 
<br>
solutions chosen may be beyond human intelligence, the *ends* to which 
<br>
the solutions operate are humanly comprehensible.  We can say of the 
<br>
system that it steers the futures into outcomes that satisfice U(x), 
<br>
even if we can't say how.
<br>
<p>Actually you need a great deal more complex goal structure than this, to 
<br>
achieve a satisfactory outcome.  In the extrapolated volition version of 
<br>
Friendly AI that I'm presently working with, U(x) is constructed in a 
<br>
complex way from existing humans, and may change if the humans 
<br>
themselves change.  Even the definition of how volition is extrapolated 
<br>
may change, if that's what we want.
<br>
<p>(I'm starting to get nervous about my ability to define an extrapolation 
<br>
powerful enough to incorporate the reason why we might want to rule out 
<br>
the creation of sentient beings within the simulation, without 
<br>
simulating sentient beings.  However, I've been nervous about problems 
<br>
that looked more impossible than this, and solved them.  So I'm not 
<br>
giving up until I try.)
<br>
<p><em>&gt; Now, I accept readily that it is not correct that complex systems are
</em><br>
<em>&gt;  *always* effectively incomprehensible to less complex systems.  I
</em><br>
<em>&gt; have no probelm with the idea that &quot;self-centredness&quot; may be
</em><br>
<em>&gt; avoidable.  But as I understand it you are focusing on the
</em><br>
<em>&gt; development of a system with the capacity for essentially indefinite
</em><br>
<em>&gt; cognitive self-enhancement.  I can't see how a system so open-ended
</em><br>
<em>&gt; as that can be constrained in the way you so cogently point out is
</em><br>
<em>&gt; necessary, and I also can't see how any system *without* the capacity
</em><br>
<em>&gt; for essentially indefinite cognitive self-enhancement will be any use
</em><br>
<em>&gt; in pre-empting the development of one that does have that capacity,
</em><br>
<em>&gt; which as I understand it is one of your primary motivations for
</em><br>
<em>&gt; creating FAI in the first place.
</em><br>
<p>The problem word is &quot;constrain&quot;.  I would say rather that I choose an 
<br>
FAI into existence, and that what the FAI does is choose.  The U(x) 
<br>
constrains the future, not the FAI; the FAI, in a strong sense, is 
<br>
*defined* by the choice of U(x).  That becomes the what-it-does, the 
<br>
nature of the FAI; it is no more a constraint than physics is 
<br>
constrained to be physics, no more to be constrasted to some separate 
<br>
will than I want to break out of being Eliezer and become a teapot.
<br>
<p>&quot;Thus the *freer* the judgement of a man is in regard to a definite 
<br>
issue, with so much greater *necessity* will the substance of this 
<br>
judgement be determined.&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-- Friedrich Engels, Anti-Dühring, 1877.
<br>
<p>&quot;Freedom is understood in contrast to its various opposites.  I can be 
<br>
free as opposed to being presently coerced.  I can be free as opposed to 
<br>
being under some other person's general control.  I can be free as 
<br>
opposed to being subject to delusions or insanity.  I can be free as 
<br>
opposed to being ruled by the state in denial of ordinary personal 
<br>
liberties.  I can be free as opposed to being in jail or prison.  I can 
<br>
be free as opposed to living under unusually heavy personal obligations. 
<br>
&nbsp;&nbsp;I can be free as opposed to being burdened by bias or prejudice.  I 
<br>
can even be free (or free spirited) as opposed to being governed by 
<br>
ordinary social conventions. The question that needs to be asked, and 
<br>
which hardly ever is asked, is whether I can be free as opposed to being 
<br>
causally determined.  Given that some kind of causal determinism is 
<br>
presupposed in the very concept of human action, it would be odd if this 
<br>
were so.  Why does anyone think that it is?&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-- David Hill
<br>
<p>What kind of freedom can exist, except the freedom to determine our 
<br>
selves and our futures with our goals and choices?  Physics is 
<br>
deterministic (yes, it is, see also many-worlds and Barbour's timeless 
<br>
physics).  It's a strange and complex delusion that leads people to see 
<br>
illusory, impossible kinds of freedom, freedoms that contrast to 
<br>
determination instead of existing within deterministic physics.  Another 
<br>
reason not to talk of &quot;intelligence&quot;, since people often toss impossible 
<br>
kinds of &quot;freedom&quot; into that definition.
<br>
<p>I would construct a fully reflective optimization process capable of 
<br>
indefinitely self-enhancing its capability to roughly satisfice our 
<br>
collective volition, to the exactly optimal degree of roughness we would 
<br>
prefer.  Balancing between the urgency of our needs; and our will to 
<br>
learn self-reliance, make our own destinies, choose our work and do it 
<br>
ourselves.
<br>
<p><em>&gt; (In contrast, I would like to see
</em><br>
<em>&gt; machines autonomous enough to free humans from the need to engage in
</em><br>
<em>&gt; menial tasks like manufacturing and mining, but not anything beyond
</em><br>
<em>&gt; that -- though I'm open to persuasion as I said.)
</em><br>
<p>Because you fear for your safety, or because you would prefer to 
<br>
optimize your own destiny rather than becoming a pawn to your own 
<br>
volition?  Or both?
<br>
<p><em>&gt; What surprises me most here is the apparently widespread presence of 
</em><br>
<em>&gt; this concern in the community subscribed to this list -- the reasons 
</em><br>
<em>&gt; for my difficulty in seeing how FAI can even in principle be created 
</em><br>
<em>&gt; have been rehearsed by others and I have nothing to add at this
</em><br>
<em>&gt; point. It seems that I am one of many who feel that this should be
</em><br>
<em>&gt; SIAI FAQ number 1.  Have you addressed it in detail online anywhere?
</em><br>
<p>Not really.  I think that, given the difficulty of these problems, I 
<br>
cannot simultaneously solve them and explain them.  Though I'm willing 
<br>
to take occasional potshots.
<br>
<p><em>&gt; I'm also fairly sure that SIAI FAQ #2 or thereabouts should be the
</em><br>
<em>&gt; one I asked earlier and no one has yet answered: namely, how about
</em><br>
<em>&gt; treating AI in general as a WMD, something to educate people not to
</em><br>
<em>&gt; think they can build safely and to entice people not to want to
</em><br>
<em>&gt; build?
</em><br>
<p>I've had no luck at this.  It needs attempting, but not by me.  It has 
<br>
to be someone fairly reputable within the AI community, or at least some 
<br>
young hotshot with a PhD willing to permanently sacrifice his/her 
<br>
academic reputation for the sake of futilely trying to warn the human 
<br>
species.  And s/he needs an actual technical knowledge of the issues, 
<br>
which makes it difficult.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8628.html">Ben Goertzel: "RE: ethics"</a>
<li><strong>Previous message:</strong> <a href="8626.html">Eliezer Yudkowsky: "Re: ethics"</a>
<li><strong>In reply to:</strong> <a href="8617.html">Aubrey de Grey: "Re: ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8628.html">Ben Goertzel: "RE: ethics"</a>
<li><strong>Reply:</strong> <a href="8628.html">Ben Goertzel: "RE: ethics"</a>
<li><strong>Reply:</strong> <a href="8639.html">Samantha Atkins: "Re: ethics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8627">[ date ]</a>
<a href="index.html#8627">[ thread ]</a>
<a href="subject.html#8627">[ subject ]</a>
<a href="author.html#8627">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
