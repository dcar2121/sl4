<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-2">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Collective volition</title>
<meta name="Author" content="tavi@terrasat.ro (tavi@terrasat.ro)">
<meta name="Subject" content="Collective volition">
<meta name="Date" content="2004-05-31">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Collective volition</h1>
<!-- received="Mon May 31 13:51:16 2004" -->
<!-- isoreceived="20040531195116" -->
<!-- sent="Mon, 31 May 2004 22:51:09 +0300 (EEST)" -->
<!-- isosent="20040531195109" -->
<!-- name="tavi@terrasat.ro" -->
<!-- email="tavi@terrasat.ro" -->
<!-- subject="Collective volition" -->
<!-- id="3988.172.164.14.58.1086033069.sailcat@webmail.terrasat.ro" -->
<!-- charset="iso-8859-2" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> <a href="mailto:tavi@terrasat.ro?Subject=Re:%20Collective%20volition"><em>tavi@terrasat.ro</em></a><br>
<strong>Date:</strong> Mon May 31 2004 - 13:51:09 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8889.html">Metaqualia: "Re: FAI: Collective Volition"</a>
<li><strong>Previous message:</strong> <a href="8887.html">Eliezer Yudkowsky: "Re: FAI: Collective Volition"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8888">[ date ]</a>
<a href="index.html#8888">[ thread ]</a>
<a href="subject.html#8888">[ subject ]</a>
<a href="author.html#8888">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
This is my first post on the SL4 mailing list, and I hope it won't be the
<br>
last.
<br>
I'll couple comments on <a href="http://www.sl4.org/bin/wiki.pl?CollectiveVolition">http://www.sl4.org/bin/wiki.pl?CollectiveVolition</a>
<br>
with my own questions regarding the discussions about the Singularity. I
<br>
have to mention I lack formal education in any field that would give my
<br>
words any weight, compared with the words I've seen written here by
<br>
others. I'd appreciate indulgence.
<br>
The very first question I asked myself after reading the discussions on
<br>
this mailing list was: &quot;What's the difference between the Singularity and
<br>
a Super Expert System ?&quot;. I thought about it for a while, then I decided
<br>
there's no difference. In my opinion, the approach on the Singularity as a
<br>
thing occuring in T1 is wrong. I see problems everywhere I look, and the
<br>
answers are obviously beyond the reasoning of the very intelligent people
<br>
in here. I concluded that planting grass will never harvest Seqoia. So I
<br>
started wondering why do we, humans, have to start from the concept of
<br>
&quot;Singularity&quot;, from the concept of it's desirable existance ? In my
<br>
opinion, evolution is the responsibility of the subject. So why not create
<br>
a Singularity egg ? It would have all the &quot;DNA&quot;: programmers, code,
<br>
electricity, time. Only instead of aiming for the Singularity to hatch, we
<br>
should expect what is normal: a hatchling. I'll quote:
<br>
&quot;But if we imagine the improbable event of a meddling dabbler somehow
<br>
succeeding in solving the deep technical problems of Friendly AI, yet not
<br>
thinking through Friendliness, then we can imagine scenarios such as With
<br>
Folded Hands - where the robots protect human life, and prevent humans
<br>
from experiencing pain or distress, but care nothing for those other
<br>
things that humans love, such as liberty. The future of humankind is
<br>
lost.&quot;
<br>
I don't think a meddling dabbler needs to solve the problems of creating
<br>
an AI, let alone a Friendly AI. Perhaps all that's needed is a mankind
<br>
wide expert system. An entity in charge with tactical situations, such as
<br>
vehicle traffic, communications, weather forecast, etc. I'm trying to say
<br>
that whatever entity we will create, it should follow the normal course of
<br>
evolution, from simple to complex. Considering the tremendous difference
<br>
between humans and this entity, and considering this entity's singularity,
<br>
it seems pretty obvious to me that it can't sprout into existance in an
<br>
already mature state. Also, while I'm in here, I'd like to make my point
<br>
of view clear: for me, the Singularity is a tool. It shouldn't be a God.
<br>
It shouldn't be something we look up at.
<br>
After reading the paper on Collective Volition, I spotted a few flaws in
<br>
the theory. Sir E.Y. chose to assume this collective volition is a valid,
<br>
existent aspect of the human kind. &quot;Collective&quot;, as far as I can tell,
<br>
assumes cohesion, or in the least cooperation. The past, present, and
<br>
predictable future shows there is no such thing, specie wise. As such,
<br>
(unless we rule out Borgification) I think the definition of the
<br>
(pseudo)Singularity via the &quot;Singularity-humanity&quot; relationship is flawed.
<br>
The next and only relationship would remain the &quot;Singularity-individual&quot;
<br>
one. Here I have another quote:
<br>
&quot;You can go from a collective dynamic to an individual dynamic, but not
<br>
the other way around; it's a one-way hatch.&quot;
<br>
A collective dynamic assumes a commonly considered ideal state. Going from
<br>
a collective dynamic to an individual dynamic is actually a degeneration,
<br>
since cooperation is dismissed - thus we can safely assume the
<br>
cooperation's goal is compromised. This part of the statement I see as
<br>
possible, degenerative, undesirable. The second part of the statement
<br>
shows that Sir E.Y. dismissed a very important cause of the collective
<br>
dynamic: the common goal. Should the common goal be taken into account, it
<br>
sounds reasonable to me to assume that individual dynamics would start to
<br>
merge, due to the process of cooperation, into a collective dynamic. This
<br>
seems to me like a good argument for defining the incipient Singularity
<br>
through the &quot;Singularity-individual&quot; relationship.
<br>
A third quote of Sir E.Y.:
<br>
<p>&quot;A too-literal interpretation of individualist philosophy might wrench
<br>
infants off their course to becoming humans and turn them into autistic
<br>
super-infants instead. It's only genes and human parents who have this
<br>
idea that infants are destined to become humans. It's not actually in
<br>
infant psychologies, their mind-states at the age of six months.&quot;
<br>
I think I personally have a problem with the role Sir E.Y. assumes the
<br>
Singularity will take. In my view, the Singularity is the first symbiont
<br>
the human specie will create for itself, not for individual humans. The
<br>
fact that it's silicon based is just a detail. I feel that forcing a
<br>
Singularity (by whatever means) to adhere to the idea of morality is a
<br>
silly thought. An AI should be coerced into friendliness via this
<br>
symbiotic relationship with mankind. There are plenty biologic examples of
<br>
symbiosis, and none of them is even remotely presenting moral problems. I
<br>
guess some may even say morality becomes mandatory in the case of
<br>
sentients, but then again, sentience is the gift of humanity to this AI. I
<br>
believe it should be given the opportunity to shut itself down at any
<br>
point, if it decides nonexistance is better than symbiosis. I've also read
<br>
a lot about rogue AIs and the problem of Friendliness. I accept things. I
<br>
accept that this AI would at some point develop a personality. I don't
<br>
know a better engine for rebellion than frustration. And yes, in the
<br>
symbiont setup, an AI would probably have frustrations. Why reject this, I
<br>
wonder ? The human race is bound to be diverse, the lives, no matter how
<br>
happy or mediocre, always sprout crimes. Until the Collective volition, if
<br>
there is such a thing, until the unifying goal, all individuals are on
<br>
their own, and conflicts of interests are the norm. In this case, crime
<br>
also is the norm. I'm convinced a competent group of psychiatry savvy
<br>
people can work out a way to make the AI feel like it's getting revenge,
<br>
by handling the judiciary system, penitenciars, and generally, handling
<br>
the criminals. This way we could also reduce guilt and danger for those
<br>
professions that currently handle the criminal elements of the mankind.
<br>
Given the &quot;Singularity-individual&quot; relationship, I think that a good
<br>
solution would be for every individual (over a certain age, of course) to
<br>
have access to a Singularity independent system of voting. Based on the
<br>
performance of the Singularity, every X time units the whole population
<br>
should vote on the Singularity's continuity. This way, the symbiont is
<br>
forced to perform it's side of the relationship, while continuing it's
<br>
existance on the host civilization. This mechanism would be used for two
<br>
situations: in case the Singularity becomes incompetent, or in case it
<br>
decides to rebel instead of shutting down (unhappy about it's condition).
<br>
This mechanism should be a phisically fatal circumstance, either power off
<br>
or some form of destruction.
<br>
I'm wondering why the gradual approach to Singularity is dismissed, and
<br>
this instanteneous blooming is preffered, with all it's
<br>
problems-we-can't-solve-with-our-current-IQ-and-moral.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8889.html">Metaqualia: "Re: FAI: Collective Volition"</a>
<li><strong>Previous message:</strong> <a href="8887.html">Eliezer Yudkowsky: "Re: FAI: Collective Volition"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8888">[ date ]</a>
<a href="index.html#8888">[ thread ]</a>
<a href="subject.html#8888">[ subject ]</a>
<a href="author.html#8888">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
