<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ethics</title>
<meta name="Author" content="Aubrey de Grey (ag24@gen.cam.ac.uk)">
<meta name="Subject" content="Re: ethics">
<meta name="Date" content="2004-05-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ethics</h1>
<!-- received="Mon May 24 12:46:51 2004" -->
<!-- isoreceived="20040524184651" -->
<!-- sent="Mon, 24 May 2004 19:46:34 +0100" -->
<!-- isosent="20040524184634" -->
<!-- name="Aubrey de Grey" -->
<!-- email="ag24@gen.cam.ac.uk" -->
<!-- subject="Re: ethics" -->
<!-- id="E1BSKT4-00015p-00@ag24.gen.cam.ac.uk" -->
<!-- inreplyto="ethics" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Aubrey de Grey (<a href="mailto:ag24@gen.cam.ac.uk?Subject=Re:%20ethics"><em>ag24@gen.cam.ac.uk</em></a>)<br>
<strong>Date:</strong> Mon May 24 2004 - 12:46:34 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8687.html">J. Andrew Rogers: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8685.html">Randall Randall: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Maybe in reply to:</strong> <a href="../0512/13225.html">Phillip Huggan: "ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8705.html">Peter C. McCluskey: "Re: ethics"</a>
<li><strong>Reply:</strong> <a href="8705.html">Peter C. McCluskey: "Re: ethics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8686">[ date ]</a>
<a href="index.html#8686">[ thread ]</a>
<a href="subject.html#8686">[ subject ]</a>
<a href="author.html#8686">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer Yudkowsky wrote:
<br>
<p><em>&gt; The idea is that even=20
</em><br>
<em>&gt; if there's a huge amount of computing power devoted to looking for
</em><br>
<em>&gt; actions/plans/designs that achieve U(x) &gt; T, such that the specific
</em><br>
<em>&gt; solutions chosen may be beyond human intelligence, the *ends* to which
</em><br>
<em>&gt; the solutions operate are humanly comprehensible.  We can say of the
</em><br>
<em>&gt; system that it steers the futures into outcomes that satisfice U(x),
</em><br>
<em>&gt; even if we can't say how.
</em><br>
<p>That's not my difficulty - I have no trouble with the idea of software
<br>
that finds incomprehensibly complex ways to do something well-defined,
<br>
even in cases where all possible ways are incomprehensibly complex.
<br>
Finding a forced win from some chess positions would be an example.
<br>
What I can't see is how to define the FAI goal well enough.
<br>
<p><em>&gt; Actually you need a great deal more complex goal structure than this, to
</em><br>
<em>&gt; achieve a satisfactory outcome.  In the extrapolated volition version of
</em><br>
<em>&gt; Friendly AI that I'm presently working with, U(x) is constructed in a
</em><br>
<em>&gt; complex way from existing humans, and may change if the humans
</em><br>
<em>&gt; themselves change.  Even the definition of how volition is extrapolated
</em><br>
<em>&gt; may change, if that's what we want.
</em><br>
<p>Right, that's just the sort of thing I was thinking of.  How can we define
<br>
U(x) in terms of existing humans without a formalisation of existing humans?
<br>
How is &quot;what humans want&quot; defined?  (Others have said the same -- I don't
<br>
mean to improve on their challenges, only to say I share their concerns.)
<br>
<p><em>&gt; &gt; Now, I accept readily that it is not correct that complex systems are
</em><br>
<em>&gt; &gt;  *always* effectively incomprehensible to less complex systems.  I
</em><br>
<em>&gt; &gt; have no probelm with the idea that &quot;self-centredness&quot; may be
</em><br>
<em>&gt; &gt; avoidable.  But as I understand it you are focusing on the
</em><br>
<em>&gt; &gt; development of a system with the capacity for essentially indefinite
</em><br>
<em>&gt; &gt; cognitive self-enhancement.  I can't see how a system so open-ended
</em><br>
<em>&gt; &gt; as that can be constrained in the way you so cogently point out is
</em><br>
<em>&gt; &gt; necessary, and I also can't see how any system *without* the capacity
</em><br>
<em>&gt; &gt; for essentially indefinite cognitive self-enhancement will be any use
</em><br>
<em>&gt; &gt; in pre-empting the development of one that does have that capacity,
</em><br>
<em>&gt; &gt; which as I understand it is one of your primary motivations for
</em><br>
<em>&gt; &gt; creating FAI in the first place.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The problem word is &quot;constrain&quot;.
</em><br>
<p>Hm, no, I think you interpret me to have meant &quot;restrain&quot;, but I meant
<br>
only the choice of U(x).  Again, I shar ethe concerns expressed by
<br>
others -- I can't see how a U(x) can exist that ensures that the FAI
<br>
will not do certain classes of things that we haven't thought of
<br>
hard-wiring into not-U(x) but that we nonetheless would want it not to
<br>
do if we had thought of them, without making it fundamentally not very
<br>
versatile/powerful at all.
<br>
<p><em>&gt; I would construct a fully reflective optimization process capable of
</em><br>
<em>&gt; indefinitely self-enhancing its capability to roughly satisfice our
</em><br>
<em>&gt; collective volition, to the exactly optimal degree of roughness we would
</em><br>
<em>&gt; prefer.  Balancing between the urgency of our needs; and our will to
</em><br>
<em>&gt; learn self-reliance, make our own destinies, choose our work and do it
</em><br>
<em>&gt; ourselves.
</em><br>
<p>This seems very intrinsically to require the FAI to consult us a great
<br>
deal and to have a way of determining its actions based on that process
<br>
of consultation in a way that humanity as a whole finds acceptable.  I
<br>
can't see how an FAI can be expected to do that if even humans in key
<br>
policy-making roles can't do it.  Why would we be happier with getting
<br>
a machine to do these things than a government?  Would we need a range
<br>
of FAIs with different U(x) that we could periodically choose between
<br>
as a society, like political parties?
<br>
<p><em>&gt; &gt; (In contrast, I would like to see
</em><br>
<em>&gt; &gt; machines autonomous enough to free humans from the need to engage in
</em><br>
<em>&gt; &gt; menial tasks like manufacturing and mining, but not anything beyond
</em><br>
<em>&gt; &gt; that -- though I'm open to persuasion as I said.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Because you fear for your safety, or because you would prefer to
</em><br>
<em>&gt; optimize your own destiny rather than becoming a pawn to your own
</em><br>
<em>&gt; volition?  Or both?
</em><br>
<p>Almost entirely the former, because things can go terminally wrong very
<br>
quickly indeed with any such system that I can envisage.  The latter,
<br>
only to the extent that it develops into the former -- I don't object
<br>
to machines that really truly always do as I want and always will.
<br>
<p><em>&gt; &gt; What surprises me most here is the apparently widespread presence of
</em><br>
<em>&gt; &gt; this concern in the community subscribed to this list -- the reasons
</em><br>
<em>&gt; &gt; for my difficulty in seeing how FAI can even in principle be created
</em><br>
<em>&gt; &gt; have been rehearsed by others and I have nothing to add at this
</em><br>
<em>&gt; &gt; point. It seems that I am one of many who feel that this should be
</em><br>
<em>&gt; &gt; SIAI FAQ number 1.  Have you addressed it in detail online anywhere?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Not really.  I think that, given the difficulty of these problems, I
</em><br>
<em>&gt; cannot simultaneously solve them and explain them.
</em><br>
<p>Whoo - that's a very unusual view in science!  Most scientists seem
<br>
to find that explaining one's current thinking about a hard problem
<br>
is far and away the most effective way to refine that thinking, even
<br>
over and above the possibility that one's interlocutor may have some
<br>
useful feedback.  I don't think this view is any less common among the
<br>
most stellar sciensts than mediocre ones, either.  But we all have our
<br>
individual ways of working, so I don't mean this as a criticism.
<br>
<p><em>&gt; &gt; I'm also fairly sure that SIAI FAQ #2 or thereabouts should be the
</em><br>
<em>&gt; &gt; one I asked earlier and no one has yet answered: namely, how about
</em><br>
<em>&gt; &gt; treating AI in general as a WMD, something to educate people not to
</em><br>
<em>&gt; &gt; think they can build safely and to entice people not to want to
</em><br>
<em>&gt; &gt; build?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I've had no luck at this.  It needs attempting, but not by me.  It has
</em><br>
<em>&gt; to be someone fairly reputable within the AI community, or at least
</em><br>
<em>&gt; some young hotshot with a PhD willing to permanently sacrifice his/her
</em><br>
<em>&gt; academic reputation for the sake of futilely trying to warn the human
</em><br>
<em>&gt; species.  And s/he needs an actual technical knowledge of the issues,
</em><br>
<em>&gt; which makes it difficult.
</em><br>
<p>I don't really see what you mean.  Surely the only people who need to
<br>
be educated/enticed in this way are those with the capacity to have a
<br>
go at building full-blown AI?  Or to build it by accident, I guess --
<br>
but even then I can't see those people being hard to educate on this.
<br>
<p>Aubrey de Grey
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8687.html">J. Andrew Rogers: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8685.html">Randall Randall: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Maybe in reply to:</strong> <a href="../0512/13225.html">Phillip Huggan: "ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8705.html">Peter C. McCluskey: "Re: ethics"</a>
<li><strong>Reply:</strong> <a href="8705.html">Peter C. McCluskey: "Re: ethics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8686">[ date ]</a>
<a href="index.html#8686">[ thread ]</a>
<a href="subject.html#8686">[ subject ]</a>
<a href="author.html#8686">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
