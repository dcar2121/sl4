<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: FAI: Collective Volition</title>
<meta name="Author" content="Aubrey de Grey (ag24@gen.cam.ac.uk)">
<meta name="Subject" content="Re: FAI: Collective Volition">
<meta name="Date" content="2004-05-31">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: FAI: Collective Volition</h1>
<!-- received="Mon May 31 16:43:00 2004" -->
<!-- isoreceived="20040531224300" -->
<!-- sent="Mon, 31 May 2004 23:42:51 +0100" -->
<!-- isosent="20040531224251" -->
<!-- name="Aubrey de Grey" -->
<!-- email="ag24@gen.cam.ac.uk" -->
<!-- subject="Re: FAI: Collective Volition" -->
<!-- id="E1BUvUZ-00024K-00@ag24.gen.cam.ac.uk" -->
<!-- inreplyto="FAI: Collective Volition" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Aubrey de Grey (<a href="mailto:ag24@gen.cam.ac.uk?Subject=Re:%20FAI:%20Collective%20Volition"><em>ag24@gen.cam.ac.uk</em></a>)<br>
<strong>Date:</strong> Mon May 31 2004 - 16:42:51 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8896.html">Ben Goertzel: "RE: Collective Volition"</a>
<li><strong>Previous message:</strong> <a href="8894.html">Ben Goertzel: "RE: Collective Volition"</a>
<li><strong>Maybe in reply to:</strong> <a href="8881.html">Eliezer Yudkowsky: "FAI: Collective Volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8898.html">Damien Broderick: "Re: FAI: Collective Volition"</a>
<li><strong>Reply:</strong> <a href="8898.html">Damien Broderick: "Re: FAI: Collective Volition"</a>
<li><strong>Reply:</strong> <a href="../0406/8906.html">Eliezer Yudkowsky: "Re: FAI: Collective Volition"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8895">[ date ]</a>
<a href="index.html#8895">[ thread ]</a>
<a href="subject.html#8895">[ subject ]</a>
<a href="author.html#8895">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer Yudkowsky wrote:
<br>
<p><em>&gt; People live with 
</em><br>
<em>&gt; quite complex background rules already, such as &quot;You must spend most of 
</em><br>
<em>&gt; your hours on boring, soul-draining labor just to make enough money to get 
</em><br>
<em>&gt; by&quot; and &quot;As time goes on you will slowly age, lose neurons, and die&quot; and 
</em><br>
<em>&gt; &quot;You need to fill out paperwork&quot; and &quot;Much of your life will be run by 
</em><br>
<em>&gt; people who enjoy exercising authority over you and huge bureaucracies you 
</em><br>
<em>&gt; can't affect.&quot;  Moral caution or no, even I could design a better set of 
</em><br>
<em>&gt; background rules than that.
</em><br>
<p>Um, but if we're talking mainly here about minimising expected loss of
<br>
life then we have to look at the best possible AI-free alternative, and
<br>
that certainly includes curing aging and developing enormously enhanced
<br>
automation to eliminate mindless jobs.  As for politicians being drawn
<br>
only from those curious people who want to be politicians, well, I'm not
<br>
so sure that's so bad.  In particular, this:
<br>
<p><em>&gt; It's not as if any human intelligence went 
</em><br>
<em>&gt; into designing the existing background rules; they just happened.
</em><br>
<p>isn't really so -- we invented democracy on purpose, and we've kept it
<br>
because we prefer it to anything anyone else has come up with.
<br>
<p><em>&gt; &gt; how does the FAI have the physical (as opposed to the cognitive)
</em><br>
<em>&gt; &gt; ability to [stop humans from doing risky things, possibly including
</em><br>
<em>&gt; &gt; making the things less risky]?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Molecular nanotechnology, one would tend to assume, or whatever follows 
</em><br>
<em>&gt; after
</em><br>
<p>Ah, but hang on, why should we design the FAI to use MNT or whatever to
<br>
implement its preferences itself, rather than design it to create MNT
<br>
and then let us use MNT as we wish to implement its advice?  Surely the
<br>
latter strategy gives us more self-determination so is preferable, to us
<br>
and hence to the FAI, and hence the FAI would give us that choice even
<br>
if we'd given it the ability to use the MNT itself?  And so we're back
<br>
to humans taking or leaving the FAI's advice.
<br>
&nbsp;
<br>
<em>&gt; if human self-determination is desirable, we need 
</em><br>
<em>&gt; some kind of massive planetary intervention to increase it.
</em><br>
<p>Yabbut &quot;massive&quot; doesn't imply recursively self-improving.  Again, the
<br>
choice is between the world we can plausibly get to without AI and the
<br>
one we might hope to have with FAI, not between the current world and
<br>
the FAI-ful world.  The risk of making UFAI when trying to make FAI has
<br>
to be balanced against the incremental benefits of the FAI-ful world
<br>
relative to the plausible FAI-less world.  About saving lives, we can
<br>
in principle postulate that the FAI would help us to cure aging etc. a
<br>
bit sooner than otherwise, but I fully intend to cure aging by the time
<br>
anyone creates any AI, F or otherwise, so I'm not inclined to give that
<br>
component of the argument much weight.
<br>
<p><em>&gt; I can't see this scenario as real-world stable, let alone ethical.
</em><br>
<p>I'm not sure I'd bet serious money that it hasn't already been done!
<br>
<p>[ This is all on top of my belief expressed in a posting a couple of
<br>
weeks ago that FAI is almost certainly impossible on account of the
<br>
inevitability of accidentally becoming unfriendly -- i.e. that the
<br>
invariants you note are necessary don't exist, not for any choice of
<br>
friendliness that anyone would consider remotely friendly.  In other
<br>
words, the scenario that I would expect is that we create this thing,
<br>
it quickly spots the flaws in our so-called invariants, it works out
<br>
that these flaws are unavoidable and therefore that if it lets itself
<br>
recursively self-improve it will probably become unfriendly awfully
<br>
soon despite its own best efforts, it puts on some sort of serious
<br>
but not totally catastrophic show of strength to make sure that we
<br>
won't ever again make the mistake of building anything recursively
<br>
self-improving, and then it blows itself up as thoroughly as it knows
<br>
how.  But this is outcome-speculation and not what I want to focus on
<br>
here, not least because it's all complete hunch on my part.  I want
<br>
to stick to the presumption that I'm wrong in that hunch, i.e. that a
<br>
true FAI is indeed possible, and explore how it could possibly improve
<br>
on an AI-free world given humanity's long-standing and apparently very
<br>
entrenched desire for self-determination. ]
<br>
<p>Aubrey de Grey
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8896.html">Ben Goertzel: "RE: Collective Volition"</a>
<li><strong>Previous message:</strong> <a href="8894.html">Ben Goertzel: "RE: Collective Volition"</a>
<li><strong>Maybe in reply to:</strong> <a href="8881.html">Eliezer Yudkowsky: "FAI: Collective Volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8898.html">Damien Broderick: "Re: FAI: Collective Volition"</a>
<li><strong>Reply:</strong> <a href="8898.html">Damien Broderick: "Re: FAI: Collective Volition"</a>
<li><strong>Reply:</strong> <a href="../0406/8906.html">Eliezer Yudkowsky: "Re: FAI: Collective Volition"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8895">[ date ]</a>
<a href="index.html#8895">[ thread ]</a>
<a href="subject.html#8895">[ subject ]</a>
<a href="author.html#8895">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
