<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ethics</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: ethics">
<meta name="Date" content="2004-05-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ethics</h1>
<!-- received="Wed May 19 16:05:33 2004" -->
<!-- isoreceived="20040519220533" -->
<!-- sent="Wed, 19 May 2004 18:05:29 -0400" -->
<!-- isosent="20040519220529" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: ethics" -->
<!-- id="40ABDA29.5050303@pobox.com" -->
<!-- charset="windows-1252" -->
<!-- inreplyto="1084977789.26188.196768242@webmail.messagingengine.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20ethics"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed May 19 2004 - 16:05:29 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8587.html">Chris Healey: "RE: ethics"</a>
<li><strong>Previous message:</strong> <a href="8585.html">Aubrey de Grey: "Re: ethics"</a>
<li><strong>In reply to:</strong> <a href="8571.html">fudley: "Re: ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8590.html">fudley: "Re: ethics"</a>
<li><strong>Reply:</strong> <a href="8590.html">fudley: "Re: ethics"</a>
<li><strong>Reply:</strong> <a href="8597.html">Samantha Atkins: "Re: ethics"</a>
<li><strong>Reply:</strong> <a href="8599.html">Marc Geddes: "Fascinating, fascinating"</a>
<li><strong>Reply:</strong> <a href="8608.html">Keith Henson: "Re: ethics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8586">[ date ]</a>
<a href="index.html#8586">[ thread ]</a>
<a href="subject.html#8586">[ subject ]</a>
<a href="author.html#8586">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
fudley wrote:
<br>
<p><em>&gt; On Wed, 19 May 2004 01:42:12 -0400, &quot;Eliezer S. Yudkowsky&quot;
</em><br>
<em>&gt; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20ethics">sentience@pobox.com</a>&gt; said:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;The problem is:  Sea Slugs can't do abstract reasoning! 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, Sea Slugs can respond to simple external stimuli but I agree they
</em><br>
<em>&gt; have no understanding of Human Beings, just as a Human Being can have no
</em><br>
<em>&gt; understanding of the psychology of a being with the brain the size of a
</em><br>
<em>&gt; planet.
</em><br>
<p>This was my ancient argument, and it turned out to be a flawed metaphor - 
<br>
the rule simply doesn't carry over.  If you have no understanding of the 
<br>
psychology of a being with the brain the size of a planet, how do you know 
<br>
that no human can understand its psychology?  This sounds like a flip 
<br>
question, but it's not; it's the source of my original mistake - I tried 
<br>
to reason about the incomprehensibility of superintelligence without 
<br>
understanding where the incomprehensibility came from, or why.  Think of 
<br>
all the analogies from the history of science; if something is a mystery 
<br>
to you, you do not know enough to claim that science will never comprehend 
<br>
it.  I was foolish to make statements about the incomprehensibility of 
<br>
intelligence before I understood intelligence.
<br>
<p>Now I understand intelligence better, which is why I talk about 
<br>
&quot;optimization processes&quot; rather than &quot;intelligence&quot;.
<br>
<p>The human ability to employ abstract reasoning is a threshold effect that 
<br>
*potentially* enables a human to fully understand some optimization 
<br>
processes, including, I think, optimization processes with arbitrarily 
<br>
large amounts of computing power.  That is only *some* optimization 
<br>
processes, processes that flow within persistent, humanly understandable 
<br>
invariants; others will be as unpredictable as coinflips.
<br>
<p>Imagine a computer program that outputs the prime factorization of large 
<br>
numbers.  For large enough numbers, the actual execution of the program 
<br>
flow is not humanly visualizable, even in principle.  But we can still 
<br>
understand an abstract property of the program, which is that it outputs a 
<br>
set of primes that multiply together to yield the input number.
<br>
<p>Now imagine a program that writes a program that outputs the prime 
<br>
factorization of large numbers.  This is a more subtle problem, because 
<br>
there's a more complex definition of utility involved - we are looking for 
<br>
a fast program, and a program that doesn't crash or cause other negative 
<br>
side effects, such as overwriting other programs' memory.  But I think 
<br>
it's possible to build out an FAI dynamic that reads out the complete set 
<br>
of side effects you care about.  More simply, you could use deductive 
<br>
reasoning processes that guarantee no side effects.  (Sandboxing a Java 
<br>
program generated by directed evolution is bad, because you're directing 
<br>
enormous search power toward finding a flaw in the sandboxing!)  Again, 
<br>
the exact form of the generated program would be unpredictable to humans, 
<br>
but its effect would be predictable from understanding the optimization 
<br>
criteria of the generator; a fast, reliable factorizer with no side effects.
<br>
<p>A program that writes a program that outputs the prime factorization of 
<br>
large numbers is still understandable, and still not visualizable.
<br>
<p>The essential law of Friendly AI is that you cannot build an AI to 
<br>
accomplish any end for which you do not possess a well-specified 
<br>
*abstract* description.  If you want moral reasoning, or (my current 
<br>
model) a dynamic that extrapolates human volitions including the 
<br>
extrapolation of moral reasoning, then you need a well-specified abstract 
<br>
description of what that looks like.
<br>
<p>In summary:  You may not need to know the exact answer, but you need to 
<br>
know an exact question.  The question may generate another question, but 
<br>
you still need an exact original question.  And you need to understand 
<br>
everything you build well enough to know that it answers that question.
<br>
<p><em>&gt;&gt;Thus making them impotent to control optimization processes such as 
</em><br>
<em>&gt;&gt;Humans, just like natural selection, which also can't do abstract reasoning. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But if the “optimization processes” can also do abstract reasoning things
</em><br>
<em>&gt; become more interesting; it may reason out why it always rushes to aid a
</em><br>
<em>&gt; Sea Slug in distress even at the risk of its own life, and it may reason
</em><br>
<em>&gt; that this is not in its best interest, and it may look for a way to
</em><br>
<em>&gt; change things.
</em><br>
<p>Don't put &quot;optimization processes&quot; in quotes, please.  Your question 
<br>
involves putting yourself into an FAI's shoes, and the shoes don't fit, 
<br>
any more than the shoes of natural selection would fit.  You may be 
<br>
thinking that &quot;intelligences&quot; have self-centered &quot;best interests&quot;.  Rather 
<br>
than arguing about intelligence, I would prefer to talk about optimization 
<br>
processes, which (as the case of natural selection illustrates) do not 
<br>
even need to be anything that humans comprehend as a mind, let alone 
<br>
possess self-centered best interests.
<br>
<p>Optimization processes direct futures into small targets in phase space. 
<br>
A Sea-Slug-rescuing optimization process, say a Bayesian decision system 
<br>
controlled by a utility function that assigns higher utility to Sea Slugs 
<br>
out of distress than Sea Slugs in distress, doesn't have a &quot;self&quot; or a 
<br>
&quot;best interest&quot; as you know it.  Put too much power behind the 
<br>
optimization process, and unless it involves a full solution to the 
<br>
underlying Friendly AI challenge, it may overwrite the solar system with 
<br>
quintillions of tiny toy Sea Slugs, just large enough to meet its 
<br>
criterion for &quot;undistressed Sea Slug&quot;, and no larger.  But it still won't 
<br>
be &quot;acting in its own self-interest&quot;.  That was just the final state the 
<br>
optimization process happened to seek out, given the goal binding.  As for 
<br>
it being unpredictable, why, look, here I am predicting it.  It's only 
<br>
unpredictable if you close your eyes and walk directly into the whirling 
<br>
razor blades.  This is a popular option, but not a universally admired one.
<br>
<p><em>&gt;&gt;That part about &quot;Humans were never able to figure out a way to overcome 
</em><br>
<em>&gt;&gt;them&quot; was a hint, since it implies the Humans, as an optimization process, 
</em><br>
<em>&gt;&gt;were somehow led to expend computing power specifically on searching for a 
</em><br>
<em>&gt;&gt;pathway whose effect (from the Sea Slugs' perspective) was to break the rules. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The only thing that hint is telling you is that sometimes a hugely
</em><br>
<em>&gt; complicated program will behave in ways the programmer neither expected
</em><br>
<em>&gt; or wanted, the more complex the program the more likely the surprise, and
</em><br>
<em>&gt; we’re talking about a brain the size of a planet.
</em><br>
<p>Humans weren't generated by computer programmers.  Our defiance of 
<br>
evolution isn't an &quot;emergent&quot; result of &quot;complexity&quot;.  It's the result of 
<br>
natural selection tending to generate psychological goals that aren't the 
<br>
same as natural selection's fitness criterion.
<br>
<p>An FAI ain't a &quot;hugely complicated program&quot;, or at least, not as 
<br>
programmers know it.  In the case of a *young* FAI, yeah, I expect 
<br>
unanticipated behaviors, but I plan to detect them, and make sure that not 
<br>
too much power goes into them.  In the case of a mature FAI, I don't 
<br>
expect any behaviors the FAI doesn't anticipate.
<br>
<p>&quot;Emergence&quot; and &quot;complexity&quot; are explanations of maximum entropy; they 
<br>
produce the illusion of explanation, yet are incapable of producing any 
<br>
specific ante facto predictions.
<br>
<p>&quot;Emergence&quot; == &quot;I don't understand what, specifically, happened.&quot; 
<br>
&quot;Complexity&quot; == &quot;I don't know how to describe the system, but it sure is 
<br>
cool.&quot;
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8587.html">Chris Healey: "RE: ethics"</a>
<li><strong>Previous message:</strong> <a href="8585.html">Aubrey de Grey: "Re: ethics"</a>
<li><strong>In reply to:</strong> <a href="8571.html">fudley: "Re: ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8590.html">fudley: "Re: ethics"</a>
<li><strong>Reply:</strong> <a href="8590.html">fudley: "Re: ethics"</a>
<li><strong>Reply:</strong> <a href="8597.html">Samantha Atkins: "Re: ethics"</a>
<li><strong>Reply:</strong> <a href="8599.html">Marc Geddes: "Fascinating, fascinating"</a>
<li><strong>Reply:</strong> <a href="8608.html">Keith Henson: "Re: ethics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8586">[ date ]</a>
<a href="index.html#8586">[ thread ]</a>
<a href="subject.html#8586">[ subject ]</a>
<a href="author.html#8586">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
