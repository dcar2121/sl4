<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Volitional Morality and Action Judgement</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Volitional Morality and Action Judgement">
<meta name="Date" content="2004-05-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Volitional Morality and Action Judgement</h1>
<!-- received="Mon May 24 09:11:14 2004" -->
<!-- isoreceived="20040524151114" -->
<!-- sent="Mon, 24 May 2004 11:11:04 -0400" -->
<!-- isosent="20040524151104" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Volitional Morality and Action Judgement" -->
<!-- id="02e801c441a1$558c88b0$6401a8c0@ZOMBIETHUSTRA" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="40B17606.6030809@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Volitional%20Morality%20and%20Action%20Judgement"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Mon May 24 2004 - 09:11:04 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8681.html">Ben Goertzel: "RE: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8679.html">Jef Allbright: "Re: The human ability to employ abstract reasoning is a threshold effect..."</a>
<li><strong>In reply to:</strong> <a href="8672.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8683.html">Robin Lee Powell: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Reply:</strong> <a href="8683.html">Robin Lee Powell: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Reply:</strong> <a href="8698.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Reply:</strong> <a href="8700.html">Michael Wilson: "RE: Volitional Morality and Action Judgement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8680">[ date ]</a>
<a href="index.html#8680">[ thread ]</a>
<a href="subject.html#8680">[ subject ]</a>
<a href="author.html#8680">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi,
<br>
<p><em>&gt; Ben Goertzel wrote:
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; We've had this discussion before, but I can't help pointing out once
</em><br>
<em>&gt; &gt; more: We do NOT know enough about self-modifying AI systems 
</em><br>
<p><em>&gt; to estimate 
</em><br>
<em>&gt; &gt; accurately that there's a &quot;zero chance of accidental success&quot; in 
</em><br>
<em>&gt; &gt; building an FAI.  Do you have a new proof of this that 
</em><br>
<em>&gt; you'd like to 
</em><br>
<em>&gt; &gt; share?  Or just the old hand-wavy attempts at arguments? ;-)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Ben?  Put yourself in my shoes for a moment and ask yourself 
</em><br>
<em>&gt; the question: 
</em><br>
<em>&gt;   &quot;How do I prove to a medieval alchemist that there is no 
</em><br>
<em>&gt; way to concoct 
</em><br>
<em>&gt; an immortality serum by mixing random chemicals together?&quot;  
</em><br>
<p>Pardon my skepticism, but I don't believe that the comparison of
<br>
<p>A) your depth of knowledge about FAI, compared to mine
<br>
<p>with
<br>
<p>B) modern chemical, physical and biological science, versus the medieval
<br>
state of knowledge about these things
<br>
<p>is a good one.
<br>
<p>Frankly, this strikes me as a level of egomania even beyond the very
<br>
high level that you normally demonstrate ;-)
<br>
<p>I can well believe that you have some insight into FAI beyond what I
<br>
have, and beyond what you or anyone else has put in their published
<br>
writings.  But you're just one human, working for a few years, and I
<br>
don't believe you've erected a secret edifice of knowledge even vaguely
<br>
comparable in magnitude to the sum total of science over the last 500
<br>
years or so.  Sorry.
<br>
<p>Between the medieval world-view and the modern scientific world-view
<br>
there are so many differences, you have an extreme case of what
<br>
Feyerabend called &quot;incommensurability.&quot;  The medievals spoke different
<br>
languages than us, in many senses.  I don't believe that you have
<br>
advanced so far beyond me and the other mortals that your understanding
<br>
is that profoundly incommensurable with ours.
<br>
<p>However, I do think it's possible that you have a theory of the
<br>
&quot;probabilistic attractor structures&quot; that self-modifying cognitive
<br>
systems are likely to fall into.  Such a theory could potentially lead
<br>
to the conclusion that accidental FAI is close to impossible.  If you
<br>
have such a theory, I'd be very curious to hear it, of course.  I have
<br>
worked out fragments of such a theory myself but they are not ready to
<br>
be shared.
<br>
<p>Next, a note on terminology.  When you said &quot;it's impossible to create a
<br>
FAI by accident&quot; I saw there were two possible interpretations
<br>
<p>1) it's impossible to create an FAI without a well-worked-out theory of
<br>
AI Friendliness, just by making a decently-designed AGI and teaching it
<br>
<p>2) it's impossible to create an FAI without trying at all, e.g. by
<br>
closing one's eyes and randomly typing code into the C compiler
<br>
<p>Of course, 2 is almost true, just like a monkey typing Shakespeare is
<br>
extremely unlikely.  Since this interpretation of your statement is very
<br>
uninteresting, I assumed you meant something like 1.  My statement is
<br>
that, so far as I know, it's reasonably likely that building a
<br>
decently-designed AGI and teaching it to be nice will lead to FAI.  This
<br>
is not just based on applying the Principle of Indifference, it's based
<br>
on a lot of other considerations as well, which we've discussed many
<br>
times.  
<br>
<p>Of course the phrase &quot;decently-designed AGI&quot; is not well-defined, an
<br>
example of what I'm thinking of is something like Novamente in a
<br>
mind-simulator configuration, as I've discussed in previous emails and
<br>
documents.  
<br>
<p>Because there is so much uncertainty (even though in my view there's a
<br>
much-greater-than-zero chance of success), I wouldn't advocate
<br>
proceeding to create a superhuman-level self-modifying AGI without a
<br>
better understanding -- unless the threat of imminent destruction of the
<br>
human race from some other source seemed more severe than it does now. 
<br>
<p>Yours,
<br>
Ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8681.html">Ben Goertzel: "RE: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8679.html">Jef Allbright: "Re: The human ability to employ abstract reasoning is a threshold effect..."</a>
<li><strong>In reply to:</strong> <a href="8672.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8683.html">Robin Lee Powell: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Reply:</strong> <a href="8683.html">Robin Lee Powell: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Reply:</strong> <a href="8698.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Reply:</strong> <a href="8700.html">Michael Wilson: "RE: Volitional Morality and Action Judgement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8680">[ date ]</a>
<a href="index.html#8680">[ thread ]</a>
<a href="subject.html#8680">[ subject ]</a>
<a href="author.html#8680">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
