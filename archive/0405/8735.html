<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)">
<meta name="Date" content="2004-05-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)</h1>
<!-- received="Wed May 26 19:19:12 2004" -->
<!-- isoreceived="20040527011912" -->
<!-- sent="Wed, 26 May 2004 21:18:58 -0400" -->
<!-- isosent="20040527011858" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)" -->
<!-- id="40B54202.6050707@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="067301c4437f$97faa530$6401a8c0@ZOMBIETHUSTRA" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20The%20dangers%20of%20genuine%20ignorance%20(was:%20Volitional%20Morality%20and%20Action%20Judgement)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed May 26 2004 - 19:18:58 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8736.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8734.html">Mike: "RE: Dangers of human self-modification"</a>
<li><strong>In reply to:</strong> <a href="8732.html">Ben Goertzel: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8738.html">Eliezer Yudkowsky: "Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>Reply:</strong> <a href="8738.html">Eliezer Yudkowsky: "Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>Reply:</strong> <a href="8739.html">Ben Goertzel: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8735">[ date ]</a>
<a href="index.html#8735">[ thread ]</a>
<a href="subject.html#8735">[ subject ]</a>
<a href="author.html#8735">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<p><em>&gt; Eliezer,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; My main point in this dialogue was: I don't believe I'm so ignorant or
</em><br>
<em>&gt; such a dipshit or so hide-bound by my preconceptions that, if you
</em><br>
<em>&gt; articulated your current theories on FAI and related subjects, I'd be
</em><br>
<em>&gt; unable to understand them.  I've understood a lot of opaque and subtle
</em><br>
<em>&gt; things from a lot of scientific disciplines.  I don't believe that your
</em><br>
<em>&gt; insights are an order of magnitude more difficult to grok than
</em><br>
<em>&gt; everything else in science, math, philosophy, etc.
</em><br>
<p>I didn't say you were stupid, Ben, so it is no use objecting that you are 
<br>
smart.  There are many reasons for failing to see dangers that are obvious 
<br>
in hindsight besides being a hide-bound dipshit.  Enthusiasm and comforting 
<br>
ignorance are two classic causes, and my past self fell victim to both.  If 
<br>
there's anyone left to judge, the future judgment of modern-day AI 
<br>
researchers might be something like:  &quot;They might have destroyed the world 
<br>
and never dreamed of harming a child.&quot;  Or not.  It sounds like a plausible 
<br>
story, after all, but so what?  There are other Everett branches than 
<br>
these.  Maybe there are some sins deadly enough that future humanity will 
<br>
not forgive.
<br>
<p>I didn't say my insights were hard to grok, Ben, but neither, it seems, are 
<br>
they so trivial as to be explained without a week of work.  I say something 
<br>
that I see immediately, and you say no.  Past experience shows that if you 
<br>
and I both have the time to spend a week arguing about the subject, there's 
<br>
a significant chance I can make my point clear, if my point is accessible 
<br>
in one inferential step from knowledge we already share.  The case of AIXI 
<br>
comes to mind; you made a mistake that seemed straightforward to me because 
<br>
I'd extensively analyzed the problem from multiple directions.  And no, my 
<br>
insight was not too subtle for you to comprehend.  But it took a week, and 
<br>
the clock went on ticking during that time.
<br>
<p><em> &gt; Next, to respond briefly to a few other peripheral points from your
</em><br>
<em> &gt; message...
</em><br>
<em> &gt;
</em><br>
<em> &gt; 1)
</em><br>
<em> &gt; I'm quite knowledgeable of probability theory, including Bayes rule and
</em><br>
<em> &gt; its accompanying apparatus, so if I make errors in judgment about FAI or
</em><br>
<em> &gt; related topics, it's not because of ignorance of probabilistic
</em><br>
<em> &gt; mathematics.  I used to teach that sorta math in the university, back in
</em><br>
<em> &gt; the olden days.  And I've done a lot of work with probabilistic
</em><br>
<em> &gt; inference lately, in the Novamente context.
</em><br>
<p>When I came to Novamente, I didn't succeed in explaining to anyone how 
<br>
&quot;curiosity&quot; didn't need to be an independent drive because it was directly 
<br>
emergent from information values in expected utility combined with Bayesian 
<br>
probability.  Maybe you've grown stronger since then.  I know I've learned 
<br>
a hell of a lot myself, in those years.  I even call things by their proper 
<br>
names these days, rather than throwing around raw math with ill-remembered 
<br>
or reinvented names.  But as far as I can tell, you've never understood 
<br>
anything of Friendly AI theory except that it involves expected utility and 
<br>
a central utility function, which in the past you said you disagreed with. 
<br>
&nbsp;&nbsp;I still haven't managed to make you see the point of &quot;external reference 
<br>
semantics&quot; as described in CFAI, which I consider the Pons Asinorum of 
<br>
Friendly AI; the first utility system with nontrivial function, with the 
<br>
intent in CFAI being to describe an elegant way to repair programmer errors 
<br>
in describing morality.  It's not that I haven't managed to make you agree, 
<br>
Ben, it's that you still haven't seen the *point*, the thing the system as 
<br>
described is supposed to *do*, and why it's different from existing proposals.
<br>
<p>I don't say why.  It seems to me that CFAI sucks as a teaching document. 
<br>
If you want to blame the whole thing on me, fine.  But don't make it into 
<br>
an ungrounded boast on my part that my ideas are too subtle for you to 
<br>
comprehend; for I've seen myself fail as a speaker.
<br>
<p>But yes, as I've said before and I'll say again, Friendly AI *IS* frickin' 
<br>
subtle, and no one should expect otherwise.
<br>
<p><em>&gt; 3)
</em><br>
<em>&gt; About recognizing, in hindsight, the stupidity of alchemy: yes, of
</em><br>
<em>&gt; course, it's relatively easy to avoid making mistakes of the same type
</em><br>
<em>&gt; that were made in the past (though humans as a whole are not so good at
</em><br>
<em>&gt; even this!).  What's much harder is to avoid making *new* types of
</em><br>
<em>&gt; mistakes.  The universe is remarkably good at generating new kinds of
</em><br>
<em>&gt; mistakes to make fools out of us ;-)
</em><br>
<p>Doesn't excuse every new generation of scientists making the same mistakes 
<br>
over, and over, and over again.  Imagine my chagrin when I realized that 
<br>
consciousness was going to have an explanation in ordinary, mundane, 
<br>
non-mysterious physics, just like the LAST THOUSAND FRICKIN' MYSTERIES the 
<br>
human species had encountered.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8736.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8734.html">Mike: "RE: Dangers of human self-modification"</a>
<li><strong>In reply to:</strong> <a href="8732.html">Ben Goertzel: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8738.html">Eliezer Yudkowsky: "Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>Reply:</strong> <a href="8738.html">Eliezer Yudkowsky: "Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>Reply:</strong> <a href="8739.html">Ben Goertzel: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8735">[ date ]</a>
<a href="index.html#8735">[ thread ]</a>
<a href="subject.html#8735">[ subject ]</a>
<a href="author.html#8735">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
