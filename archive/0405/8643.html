<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Dangers of human self-modification</title>
<meta name="Author" content="Marc Geddes (marc_geddes@yahoo.co.nz)">
<meta name="Subject" content="Re: Dangers of human self-modification">
<meta name="Date" content="2004-05-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Dangers of human self-modification</h1>
<!-- received="Sat May 22 03:48:03 2004" -->
<!-- isoreceived="20040522094803" -->
<!-- sent="Sat, 22 May 2004 21:48:02 +1200 (NZST)" -->
<!-- isosent="20040522094802" -->
<!-- name="Marc Geddes" -->
<!-- email="marc_geddes@yahoo.co.nz" -->
<!-- subject="Re: Dangers of human self-modification" -->
<!-- id="20040522094802.35077.qmail@web20210.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="40ADDB7F.3090804@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marc Geddes (<a href="mailto:marc_geddes@yahoo.co.nz?Subject=Re:%20Dangers%20of%20human%20self-modification"><em>marc_geddes@yahoo.co.nz</em></a>)<br>
<strong>Date:</strong> Sat May 22 2004 - 03:48:02 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8644.html">fudley: "Re: ethics"</a>
<li><strong>Previous message:</strong> <a href="8642.html">Marc Geddes: "Re: ethics"</a>
<li><strong>In reply to:</strong> <a href="8616.html">Eliezer Yudkowsky: "Dangers of human self-modification"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8630.html">Keith Henson: "Re: Volitional Morality and Action Judgement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8643">[ date ]</a>
<a href="index.html#8643">[ thread ]</a>
<a href="subject.html#8643">[ subject ]</a>
<a href="author.html#8643">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;--- Eliezer Yudkowsky &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Dangers%20of%20human%20self-modification">sentience@pobox.com</a>&gt; wrote: &gt;
<br>
Eliezer Yudkowsky wrote:
<br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; I think I have to side with Keith.  I fear that
</em><br>
<em>&gt; human
</em><br>
<em>&gt; &gt; self-modification is far more dangerous than I
</em><br>
<em>&gt; would once have liked
</em><br>
<em>&gt; &gt; to imagine.  Better to devise nutritious bacon,
</em><br>
<em>&gt; cheese, chocolate,
</em><br>
<em>&gt; &gt; and wine, than dare to mess with hunger - let
</em><br>
<em>&gt; alone anything more
</em><br>
<em>&gt; &gt; complex.  You would practically need to be a
</em><br>
<em>&gt; Friendly AI programmer
</em><br>
<em>&gt; &gt; just to realize how afraid you needed to be, and
</em><br>
<em>&gt; freeze solid until
</em><br>
<em>&gt; &gt; there was an AI midwife at hand to help you *very
</em><br>
<em>&gt; slowly* start to
</em><br>
<em>&gt; &gt; make modifications that didn't have huge
</em><br>
<em>&gt; unintended consequences, or
</em><br>
<em>&gt; &gt; take you away from the rest of humanity, or
</em><br>
<em>&gt; destroy complexity you
</em><br>
<em>&gt; &gt; would have preferred to keep.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Some examples of possible consequences, off the top
</em><br>
<em>&gt; of my head:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You've got memories of enjoying cheeseburgers.  What
</em><br>
<em>&gt; happens to the 
</em><br>
<em>&gt; memories when the sensory substrate of recollection
</em><br>
<em>&gt; shifts?  Are you 
</em><br>
<em>&gt; going to keep the old hardware around for
</em><br>
<em>&gt; recollection?  Will you add in 
</em><br>
<em>&gt; a complex system to maintain empathy with your old
</em><br>
<em>&gt; self?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Your old sense of taste was fine-tuned and
</em><br>
<em>&gt; integrated into your sense of 
</em><br>
<em>&gt; pleasure and pain, happiness and disgust, by natural
</em><br>
<em>&gt; selection.  Natural 
</em><br>
<em>&gt; selection also designed everything else keyed into
</em><br>
<em>&gt; those systems.  If 
</em><br>
<em>&gt; you pick new senses, do they make sense?  Does the
</em><br>
<em>&gt; pattern subtly clash 
</em><br>
<em>&gt; with the pattern of systems already present?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Will your new sense of taste be more or less complex
</em><br>
<em>&gt; than your old sense 
</em><br>
<em>&gt; of taste?  More intense or less intense?  If more
</em><br>
<em>&gt; intense, does the new 
</em><br>
<em>&gt; sense of taste balance with a mental system that is
</em><br>
<em>&gt; known to stay sane 
</em><br>
<em>&gt; only under ancestral conditions of environment and
</em><br>
<em>&gt; neurology?  Consider 
</em><br>
<em>&gt; the effects on humans of non-ancestral Pringles and
</em><br>
<em>&gt; chocolate cake, 
</em><br>
<em>&gt; loads of sugar and salt and fat not present in any
</em><br>
<em>&gt; ancestral foods. 
</em><br>
<em>&gt; Adopting a more intense taste system can have the
</em><br>
<em>&gt; same effect, if the 
</em><br>
<em>&gt; rest of the mind isn't upgraded accordingly to
</em><br>
<em>&gt; balance with the 
</em><br>
<em>&gt; increased intensity of sensation.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Maybe you would prefer to gradually grow into new
</em><br>
<em>&gt; tastes?  What does the 
</em><br>
<em>&gt; sharp discontinuity of direct self-alteration do to
</em><br>
<em>&gt; your sense of 
</em><br>
<em>&gt; personal continuity?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If the new taste sensation is more intense, do you
</em><br>
<em>&gt; become addicted to 
</em><br>
<em>&gt; the act of self-modification for more intense
</em><br>
<em>&gt; sensations?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You're eliminating cognitive complexity of yourself
</em><br>
<em>&gt; by getting rid of 
</em><br>
<em>&gt; the complex pattern of the old system.  Maybe you
</em><br>
<em>&gt; would prefer not to 
</em><br>
<em>&gt; eliminate the old complexity - learn to appreciate
</em><br>
<em>&gt; lettuce *in addition 
</em><br>
<em>&gt; to* Pringles?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Can you really appreciate the long-term consequences
</em><br>
<em>&gt; of altering your 
</em><br>
<em>&gt; mind this way?  Does the new design you decided upon
</em><br>
<em>&gt; make any sense with 
</em><br>
<em>&gt; respect to those criteria that you would use if you
</em><br>
<em>&gt; thought about the 
</em><br>
<em>&gt; problem long enough?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What is the long-term effect of adopting the general
</em><br>
<em>&gt; policy of 
</em><br>
<em>&gt; eliminating old complexity that inconveniences you,
</em><br>
<em>&gt; and inscribing new 
</em><br>
<em>&gt; complexity that seems like a good idea at the time?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Other humans share your current taste sensations. 
</em><br>
<em>&gt; Think of your awkward 
</em><br>
<em>&gt; refusal of foods at dinner, the mainstream artistry
</em><br>
<em>&gt; of cooking you'll no 
</em><br>
<em>&gt; longer be able to appreciate.  Are you distancing
</em><br>
<em>&gt; yourself from the rest 
</em><br>
<em>&gt; of humanity?  Lest someone chime in that diversity
</em><br>
<em>&gt; is automatically 
</em><br>
<em>&gt; good, let me add that this is one hell of a
</em><br>
<em>&gt; nontrivial decision.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If you can alter your taste buds any time you feel
</em><br>
<em>&gt; like it, will it 
</em><br>
<em>&gt; destroy, or alter, the perceived challenge and fun
</em><br>
<em>&gt; of cooking?  Consider 
</em><br>
<em>&gt; the effect on baseball if people could just run
</em><br>
<em>&gt; around the bases any 
</em><br>
<em>&gt; time they wanted.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And finally, what about all the consequences, and
</em><br>
<em>&gt; categories of 
</em><br>
<em>&gt; consequences, that you haven't foreseen?  When you
</em><br>
<em>&gt; imagine the act of 
</em><br>
<em>&gt; self-modification, you will imagine only the easily
</em><br>
<em>&gt; mentally accessible 
</em><br>
<em>&gt; consequences of the act, not the actual
</em><br>
<em>&gt; consequences.  Just because you 
</em><br>
<em>&gt; can't see the doom, doesn't mean the doom isn't
</em><br>
<em>&gt; there.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; -- 
</em><br>
<em>&gt; Eliezer S. Yudkowsky                         
</em><br>
<em>&gt; <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt; Research Fellow, Singularity Institute for
</em><br>
<em>&gt; Artificial Intelligence 
</em><br>
<p>Well, to some extent humans are 'self modifying'
<br>
themselves all the time.  Babies &gt; Kids &gt; Teenagers &gt;
<br>
Adults ?  There may be useful analogies there.
<br>
<p>Also, I have always been skeptical that
<br>
'self-modification' was any where near as straight
<br>
forward as you thought, even for seed A.I's.  Aren't
<br>
there analogous issues that apply to seed A.I's as
<br>
much as to humans?
<br>
<p>As you know, I'm mighty skeptical of the so-called
<br>
'philosophers of mind' who claim that there is no
<br>
'Self'.  The abstract invariants behind the goal
<br>
system would form a 'Self'.   So any self-modification
<br>
has to preserve the invariant to maintain personal
<br>
identity.  Even a seed-A.I especially designed to
<br>
self-improve runs into the problem of maintaing
<br>
personal identity.  Wouldn't even the seed-A.I be
<br>
subject to some of the same constraints that limited
<br>
human evolution?  I always felt that it would have to
<br>
be more like a legacy system rather an a continuous
<br>
'redesign from scratch'.    
<br>
<p>=====
<br>
&quot;Live Free or Die, Death is not the Worst of Evils.&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Gen. John Stark
<br>
<p>&quot;The Universe...or nothing!&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-H.G.Wells
<br>
<p><p>Please visit my web-sites.
<br>
<p>Science-Fiction and Fantasy:  <a href="http://www.prometheuscrack.com">http://www.prometheuscrack.com</a>
<br>
Science, A.I, Maths            :  <a href="http://www.riemannai.org">http://www.riemannai.org</a>
<br>
<p>Find local movie times and trailers on Yahoo! Movies.
<br>
<a href="http://au.movies.yahoo.com">http://au.movies.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8644.html">fudley: "Re: ethics"</a>
<li><strong>Previous message:</strong> <a href="8642.html">Marc Geddes: "Re: ethics"</a>
<li><strong>In reply to:</strong> <a href="8616.html">Eliezer Yudkowsky: "Dangers of human self-modification"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8630.html">Keith Henson: "Re: Volitional Morality and Action Judgement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8643">[ date ]</a>
<a href="index.html#8643">[ thread ]</a>
<a href="subject.html#8643">[ subject ]</a>
<a href="author.html#8643">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
