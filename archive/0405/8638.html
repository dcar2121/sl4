<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ethics</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: ethics">
<meta name="Date" content="2004-05-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ethics</h1>
<!-- received="Sat May 22 02:07:01 2004" -->
<!-- isoreceived="20040522080701" -->
<!-- sent="Sat, 22 May 2004 01:06:56 -0700" -->
<!-- isosent="20040522080656" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: ethics" -->
<!-- id="FE70B1C5-ABC6-11D8-86D4-000A95B1AFDE@objectent.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="40AE6B28.2020705@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20ethics"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Sat May 22 2004 - 02:06:56 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8639.html">Samantha Atkins: "Re: ethics"</a>
<li><strong>Previous message:</strong> <a href="8637.html">Samantha Atkins: "Re: Dangers of human self-modification"</a>
<li><strong>In reply to:</strong> <a href="8626.html">Eliezer Yudkowsky: "Re: ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8642.html">Marc Geddes: "Re: ethics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8638">[ date ]</a>
<a href="index.html#8638">[ thread ]</a>
<a href="subject.html#8638">[ subject ]</a>
<a href="author.html#8638">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On May 21, 2004, at 1:48 PM, Eliezer Yudkowsky wrote:
<br>
<p><em>&gt; Samantha Atkins wrote:
</em><br>
<em>&gt;&gt; On May 19, 2004, at 3:56 PM, Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt;&gt;&gt; Similarly, FAI doesn't require that I understand an existing 
</em><br>
<em>&gt;&gt;&gt; biological system, or that I understand an arbitrarily selected 
</em><br>
<em>&gt;&gt;&gt; nonhuman system, but that I build a system with the property of 
</em><br>
<em>&gt;&gt;&gt; understandability.  Or to be more precise, that I build an 
</em><br>
<em>&gt;&gt;&gt; understandable system with the property of predictable 
</em><br>
<em>&gt;&gt;&gt; niceness/Friendliness, for a well-specified abstract predicate 
</em><br>
<em>&gt;&gt;&gt; thereof.  Just *any* system that's understandable wouldn't be 
</em><br>
<em>&gt;&gt;&gt; enough.
</em><br>
<em>&gt;&gt; You propose to give this system constrained to be understandable by 
</em><br>
<em>&gt;&gt; yourself the power to control the immediate space-time area in 
</em><br>
<em>&gt;&gt; service of its understandable goals?  That is a lot of power to hand 
</em><br>
<em>&gt;&gt; something that is not really a mind or particularly self-aware or 
</em><br>
<em>&gt;&gt; reflective.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Completely reflective, and not self-aware in the sense of that which 
</em><br>
<em>&gt; we refer to as &quot;conscious experience&quot;.  (Remember, this may look like 
</em><br>
<em>&gt; a mysterious question, but there is no such thing as a mysterious 
</em><br>
<em>&gt; answer.)
</em><br>
<p>You need to be more clear on the ways this FAI is not a &quot;minded&quot; or 
<br>
&quot;conscious being&quot;.  You say it isn't and then imply you have been 
<br>
misunderstood if anyone asks why this is something that should 
<br>
rule/control/guide human space (and perhaps more).   You seem to say 
<br>
you are making it optimized for only a few purposes (much more 
<br>
tractable I agree) and have your acceptance/shaping criteria trim away 
<br>
much that looks like something else.  That sounds like a rather 
<br>
constrained intellect (if we may call it that).
<br>
<p>Since the goal is Friendliness I don't see how it could be too 
<br>
constrained safely.  It at least requires enough degrees of freedom to 
<br>
understand human longings and behavior in order to be Friendly.  
<br>
Doesn't it?
<br>
<p><em>&gt;
</em><br>
<em>&gt;&gt; If I understand you correctly I am not at all sure I can support such 
</em><br>
<em>&gt;&gt; a project.  It smacks of a glorified all-powerful mindless coercion 
</em><br>
<em>&gt;&gt; for &quot;our own good&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes, I understand the danger here.  But Samantha, I'm not sure I'm 
</em><br>
<em>&gt; ready to be a father.  I think I know how to redirect futures, deploy 
</em><br>
<em>&gt; huge amounts of what I would consider to be intelligence and what I 
</em><br>
<em>&gt; would cautiously call &quot;optimization pressures&quot; for the sake of 
</em><br>
<em>&gt; avoiding conversational ambiguity.  But I'm still fathoming the 
</em><br>
<em>&gt; reasons why humans think they have conscious experiences, and the 
</em><br>
<em>&gt; foundations of fun, and the answers to the moral questions implicit in 
</em><br>
<em>&gt; myself.  I feel myself lacking in the knowledge, and the surety of 
</em><br>
<em>&gt; knowledge, needed to create a new sentient species.  And I wistfully 
</em><br>
<em>&gt; wish that all humankind should have a voice in such a decision, the 
</em><br>
<em>&gt; creation of humanity's first child.  And I wonder if it is a thing we 
</em><br>
<em>&gt; would regard as a loss of destiny, to be rescued from our present 
</em><br>
<em>&gt; crisis by a true sentient mind vastly superior to ourselves in both 
</em><br>
<em>&gt; intelligence and morality, rather than a powerful optimization process 
</em><br>
<em>&gt; bound to the collective volition of humankind.  There's a difference 
</em><br>
<em>&gt; between being manifesting the superposed extrapolation of the 
</em><br>
<em>&gt; decisions humankind would prefer given sufficient intelligence, and 
</em><br>
<em>&gt; being rescued by an actual parent.
</em><br>
<p>Yes.  So perhaps we should all grow up first.  Perhaps we need to 
<br>
augment humanity a step at a time into being more intelligent, 
<br>
rational, compassionate, self-aware.    Human nature/capability is a 
<br>
good starting point quite a ways ahead of any AI we have in hand or 
<br>
near term available in very important respects.   If you want something 
<br>
that humanity has a voice in and that fulfills humanities purpose most 
<br>
efficiently then why not start with and build upon humanity itself?   
<br>
Let our AI skills in the beginning go to directly extending human 
<br>
abilities and reach.
<br>
<p>Perhaps the only thing more insanely terrifying than contemplating 
<br>
becoming a god is contemplating building one from scratch.
<br>
<p><em>&gt;
</em><br>
<em>&gt; If I can, Samantha, I would resolve this present crisis without 
</em><br>
<em>&gt; creating a child, and leave that to the future.  I fear making a 
</em><br>
<em>&gt; mistake that would be terrible even if remediable, and I fear 
</em><br>
<em>&gt; exercising too much personal control over humankind's destiny.
</em><br>
<p>Removing the ability of humanity to do itself in and giving it a much 
<br>
better chance of surviving Singularity is of course a wonderful goal.  
<br>
But even if you call the FAI &quot;optimizing processes&quot; or some such it 
<br>
will still be a solution outside of humanity rather than humanity 
<br>
growing into being enough to take care of its problems.  Whether the 
<br>
FAI is a &quot;parent&quot; or not it will be an alien &quot;gift&quot; to fix what 
<br>
humanity cannot.    Why not have humanity itself recursively 
<br>
self-improve?  Why force a gift that shouts out that they are inferior 
<br>
to an FAI that is not even a sentient mind?   To be an extension of 
<br>
humanity it must grow out of humanity.
<br>
<p>Whatever we can do can be a huge mistake.   But I think we both agree 
<br>
that not acting would be the biggest mistake of all.
<br>
<p><em>&gt;  Perhaps it is not possible even in principle, to build a nonsentient 
</em><br>
<em>&gt; process that can extrapolate the volitions of sentient beings without 
</em><br>
<em>&gt; ever actually simulating sentient beings to such a degree that we 
</em><br>
<em>&gt; would see helpless minds trapped inside a computer.  It is more 
</em><br>
<em>&gt; difficult, when one considers that constraint.  One cannot brute-force 
</em><br>
<em>&gt; the problem with a training set and a hypothesis search, for one must 
</em><br>
<em>&gt; understand enough about sentience to rule out &quot;hypotheses&quot; that are 
</em><br>
<em>&gt; actual sentient beings.  The added constraint forces me to understand 
</em><br>
<em>&gt; the problem on a deeper level, and work out the exact nature of things 
</em><br>
<em>&gt; that are difficult to understand. That is a good thing, broadly 
</em><br>
<em>&gt; speaking.  I find that much of life as a Friendly AI programmer 
</em><br>
<em>&gt; consists in forcing your mind to get to grips with difficult problems, 
</em><br>
<em>&gt; instead of finding excuses not to confront them.
</em><br>
<p>I think it is not possible to extrapolate the volitions of sentient 
<br>
beings without being (or becoming)  a sentient being.
<br>
<p><em>&gt; I am going to at least try to pursue the difficult questions and do 
</em><br>
<em>&gt; this in the way that I see as the best possible, and if I find it is 
</em><br>
<em>&gt; too difficult *then* I will go back to my original plan of becoming a 
</em><br>
<em>&gt; father.  But I have learned to fear any clever strategy for cheating a 
</em><br>
<em>&gt; spectacularly difficult question.  Do not tell me that my standards 
</em><br>
<em>&gt; are too high, or that the clock is ticking; past experience says that 
</em><br>
<em>&gt; cheating is extremely dangerous, and I should try HARD before giving 
</em><br>
<em>&gt; up.
</em><br>
<em>&gt;
</em><br>
<p>This sounds really good.  I am also fearful of &quot;cheating a 
<br>
spectacularly difficult question&quot;.   But the clock indeed is ticking 
<br>
and any standards that disallow finding a workable solution are indeed 
<br>
&quot;too high&quot; or at least not appropriate to the constraints of the 
<br>
problem.
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8639.html">Samantha Atkins: "Re: ethics"</a>
<li><strong>Previous message:</strong> <a href="8637.html">Samantha Atkins: "Re: Dangers of human self-modification"</a>
<li><strong>In reply to:</strong> <a href="8626.html">Eliezer Yudkowsky: "Re: ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8642.html">Marc Geddes: "Re: ethics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8638">[ date ]</a>
<a href="index.html#8638">[ thread ]</a>
<a href="subject.html#8638">[ subject ]</a>
<a href="author.html#8638">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
