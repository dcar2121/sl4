<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)">
<meta name="Date" content="2004-05-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)</h1>
<!-- received="Wed May 26 21:52:22 2004" -->
<!-- isoreceived="20040527035222" -->
<!-- sent="Wed, 26 May 2004 23:52:04 -0400" -->
<!-- isosent="20040527035204" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)" -->
<!-- id="06a601c4439d$f9c17c30$6401a8c0@ZOMBIETHUSTRA" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="40B54202.6050707@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20The%20dangers%20of%20genuine%20ignorance%20(was:%20Volitional%20Morality%20and%20Action%20Judgement)"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Wed May 26 2004 - 21:52:04 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8740.html">Marc Geddes: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8738.html">Eliezer Yudkowsky: "Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>In reply to:</strong> <a href="8735.html">Eliezer Yudkowsky: "Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8741.html">Thomas Buckner: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>Reply:</strong> <a href="8741.html">Thomas Buckner: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>Reply:</strong> <a href="8743.html">Marc Geddes: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>Reply:</strong> <a href="8744.html">Marc Geddes: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>Reply:</strong> <a href="8745.html">Eliezer Yudkowsky: "Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8739">[ date ]</a>
<a href="index.html#8739">[ thread ]</a>
<a href="subject.html#8739">[ subject ]</a>
<a href="author.html#8739">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer,
<br>
<p>About &quot;enthusiasm and comforting ignorance&quot; ..
<br>
<p>-- I'm well aware of my tendency to be overoptimistic, and I make a
<br>
strong conscious effort to counteract that.  Generally I'm successful at
<br>
counteracting it these days; for instance, I'm now usually able to make
<br>
realistic, non-over-optimistic timing estimates for software projects.
<br>
<p>-- I don't find my ignorance very comforting.  However, I am comfortable
<br>
accepting that I am ignorant about some things!  Such as, for example,
<br>
whether it's true that, as you claim, &quot;any self-improving AGI not
<br>
carefully engineered according to a rigorous FAI theory will lead to a
<br>
bad end for humans.&quot;  I don't know.  I don't believe you really know
<br>
either, with any reasonable degree of certainty.  This ignorance is not
<br>
comforting, just realistic.
<br>
<p><em>&gt; I didn't say my insights were hard to grok, Ben, but neither, 
</em><br>
<em>&gt; it seems, are 
</em><br>
<em>&gt; they so trivial as to be explained without a week of work.  I 
</em><br>
<em>&gt; say something 
</em><br>
<em>&gt; that I see immediately, and you say no.  Past experience 
</em><br>
<em>&gt; shows that if you 
</em><br>
<em>&gt; and I both have the time to spend a week arguing about the 
</em><br>
<em>&gt; subject, there's 
</em><br>
<em>&gt; a significant chance I can make my point clear, if my point 
</em><br>
<em>&gt; is accessible 
</em><br>
<em>&gt; in one inferential step from knowledge we already share. 
</em><br>
<p>Eliezer, it is unwarranted nastiness on your part to insinuate that I am
<br>
not able to follow more than a single inference step.  Gimme a break!  
<br>
<p><em>&gt; The 
</em><br>
<em>&gt; case of AIXI 
</em><br>
<em>&gt; comes to mind; you made a mistake that seemed straightforward 
</em><br>
<em>&gt; to me because 
</em><br>
<em>&gt; I'd extensively analyzed the problem from multiple 
</em><br>
<em>&gt; directions.  And no, my 
</em><br>
<em>&gt; insight was not too subtle for you to comprehend.  But it 
</em><br>
<em>&gt; took a week, and 
</em><br>
<em>&gt; the clock went on ticking during that time.
</em><br>
<p>Heh.  That &quot;week arguing about AIXI&quot; that you mention was about an hour
<br>
of my time altogether; as I recall that was a VERY busy week for me, and
<br>
I was reading and writing emails on the SL4 list in spare moments at
<br>
high speed.
<br>
&nbsp;
<br>
<em>&gt; When I came to Novamente, I didn't succeed in explaining to 
</em><br>
<em>&gt; anyone how 
</em><br>
<em>&gt; &quot;curiosity&quot; didn't need to be an independent drive because it 
</em><br>
<em>&gt; was directly 
</em><br>
<em>&gt; emergent from information values in expected utility combined 
</em><br>
<em>&gt; with Bayesian 
</em><br>
<em>&gt; probability.  Maybe you've grown stronger since then.
</em><br>
<p>First of all, you never came to Novamente LLC (which BTW does not have
<br>
an office, though office space is shared with partner firms in the US
<br>
and Brazil).  You came to Webmind Inc., which was a different company
<br>
building a different AI system.  
<br>
<p>Novamente is founded on probability theory, Webmind was not.  That is
<br>
one among several significant differences between the two systems.
<br>
<p>Secondly, when you visited Webmind, you (surprise, surprise!) seem to
<br>
have come away with the impression that all of the staff were much
<br>
slower and stupider than they actually are.
<br>
<p>Regarding the particular point you mention, many of us UNDERSTOOD your
<br>
point that you COULD derive curiosity from other, more basic
<br>
motivations.  However, we didn't agree that this is the BEST way to
<br>
implement curiousity in an AGI system.   Just because something CAN be
<br>
achieved in a certain way doesn't mean that's the BEST way to do it --
<br>
where &quot;best&quot; must be interpreted in the context of realistic memory and
<br>
processing power constraints.  
<br>
<p>Novamente is more probabilistically based than Webmind, yet even so we
<br>
will implement novelty-seeking as an independent drive, initially,
<br>
because this is a lot more efficient than making the system learn this
<br>
from a more basic motivation.
<br>
<p><em>&gt; But as far as I can tell, you've never 
</em><br>
<em>&gt; understood 
</em><br>
<em>&gt; anything of Friendly AI theory except that it involves 
</em><br>
<em>&gt; expected utility and 
</em><br>
<em>&gt; a central utility function, which in the past you said you 
</em><br>
<em>&gt; disagreed with. 
</em><br>
<p>Well, I believe I understand what you say in CFAI, I just don't see why
<br>
you think that philosophy would work in a real self-modifying software
<br>
system...
<br>
<p><em>&gt;   I still haven't managed to make you see the point of 
</em><br>
<em>&gt; &quot;external reference 
</em><br>
<em>&gt; semantics&quot; as described in CFAI, which I consider the Pons 
</em><br>
<em>&gt; Asinorum of 
</em><br>
<em>&gt; Friendly AI; the first utility system with nontrivial 
</em><br>
<em>&gt; function, with the 
</em><br>
<em>&gt; intent in CFAI being to describe an elegant way to repair 
</em><br>
<em>&gt; programmer errors 
</em><br>
<em>&gt; in describing morality.  It's not that I haven't managed to 
</em><br>
<em>&gt; make you agree, 
</em><br>
<em>&gt; Ben, it's that you still haven't seen the *point*, the thing 
</em><br>
<em>&gt; the system as 
</em><br>
<em>&gt; described is supposed to *do*, and why it's different from 
</em><br>
<em>&gt; existing proposals.
</em><br>
<p>Your &quot;external reference semantics&quot;, as I recall, is basically the idea
<br>
that an AGI system considers its own supergoals to be uncertain
<br>
approximations of some unknown ideal supergoals, and tries to improve
<br>
its own supergoals.  It's kind of a supergoal that says &quot;Make all my
<br>
supergoals, including this one, do what they're supposed to do better.&quot;
<br>
<p>What I don't understand is why you think this idea is so amazingly
<br>
profound.  Yes, this attitude toward one's supergoals is an element of
<br>
what people call &quot;wisdom.&quot;  But I don't see that this kind of thing
<br>
provides any kind of guarantee of Friendliness after iterated
<br>
self-modification.  Seems to me that an AGI with &quot;external reference
<br>
semantics&quot; [TERRIBLE name for the concept, BTW ;-)] can go loony just as
<br>
easily as one without.
<br>
<p>But if I ever disagree with one of your ideas, your reaction is &quot;Well
<br>
that's because you don't understand it.&quot; ;-p
<br>
<p>BTW, the idea of goals having probabilities, propagating these to sub
<br>
and supergoals, etc., was there in Webmind and is there in Novamente.
<br>
Goal refinement has been part of my AI design for a long time, and it
<br>
was always applied to top-level supergoals as well as to other goals.
<br>
<p><em>&gt; Doesn't excuse every new generation of scientists making the 
</em><br>
<em>&gt; same mistakes 
</em><br>
<em>&gt; over, and over, and over again.  Imagine my chagrin when I 
</em><br>
<em>&gt; realized that 
</em><br>
<em>&gt; consciousness was going to have an explanation in ordinary, mundane, 
</em><br>
<em>&gt; non-mysterious physics, just like the LAST THOUSAND FRICKIN' 
</em><br>
<em>&gt; MYSTERIES the 
</em><br>
<em>&gt; human species had encountered.
</em><br>
<p>1) I don't call quantum physics &quot;non-mysterious physics&quot;
<br>
<p>2) Have you worked out a convincing physics-based explanation of
<br>
consciousness?  If so, why aren't you sharing it with us?  Too busy to
<br>
take the time?
<br>
<p><p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8740.html">Marc Geddes: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8738.html">Eliezer Yudkowsky: "Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>In reply to:</strong> <a href="8735.html">Eliezer Yudkowsky: "Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8741.html">Thomas Buckner: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>Reply:</strong> <a href="8741.html">Thomas Buckner: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>Reply:</strong> <a href="8743.html">Marc Geddes: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>Reply:</strong> <a href="8744.html">Marc Geddes: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>Reply:</strong> <a href="8745.html">Eliezer Yudkowsky: "Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8739">[ date ]</a>
<a href="index.html#8739">[ thread ]</a>
<a href="subject.html#8739">[ subject ]</a>
<a href="author.html#8739">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
