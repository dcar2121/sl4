<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)">
<meta name="Date" content="2004-05-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)</h1>
<!-- received="Thu May 27 00:42:05 2004" -->
<!-- isoreceived="20040527064205" -->
<!-- sent="Thu, 27 May 2004 02:42:01 -0400" -->
<!-- isosent="20040527064201" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)" -->
<!-- id="40B58DB9.5000401@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="06a601c4439d$f9c17c30$6401a8c0@ZOMBIETHUSTRA" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20The%20dangers%20of%20genuine%20ignorance%20(was:%20Volitional%20Morality%20and%20Action%20Judgement)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu May 27 2004 - 00:42:01 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8746.html">Eliezer Yudkowsky: "Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>Previous message:</strong> <a href="8744.html">Marc Geddes: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>In reply to:</strong> <a href="8739.html">Ben Goertzel: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8737.html">Thomas Buckner: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8745">[ date ]</a>
<a href="index.html#8745">[ thread ]</a>
<a href="subject.html#8745">[ subject ]</a>
<a href="author.html#8745">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt;&gt; I didn't say my insights were hard to grok, Ben, but neither, it
</em><br>
<em>&gt;&gt; seems, are they so trivial as to be explained without a week of work.
</em><br>
<em>&gt;&gt; I say something that I see immediately, and you say no.  Past
</em><br>
<em>&gt;&gt; experience shows that if you and I both have the time to spend a week
</em><br>
<em>&gt;&gt; arguing about the subject, there's a significant chance I can make my
</em><br>
<em>&gt;&gt; point clear, if my point is accessible in one inferential step from
</em><br>
<em>&gt;&gt; knowledge we already share.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer, it is unwarranted nastiness on your part to insinuate that I am
</em><br>
<em>&gt; not able to follow more than a single inference step.  Gimme a break!
</em><br>
<p>What, that people have trouble following multiple simultaneous inferential 
<br>
steps?  That's just common sense.  Anyway, I can't recall ever seeing you 
<br>
do two at once in one of our arguments.
<br>
<p><em>&gt;&gt; The case of AIXI comes to mind; you made a mistake that seemed
</em><br>
<em>&gt;&gt; straightforward to me because I'd extensively analyzed the problem
</em><br>
<em>&gt;&gt; from multiple directions.  And no, my insight was not too subtle for
</em><br>
<em>&gt;&gt; you to comprehend.  But it took a week, and the clock went on ticking
</em><br>
<em>&gt;&gt; during that time.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Heh.  That &quot;week arguing about AIXI&quot; that you mention was about an hour 
</em><br>
<em>&gt; of my time altogether; as I recall that was a VERY busy week for me, and
</em><br>
<em>&gt; I was reading and writing emails on the SL4 list in spare moments at 
</em><br>
<em>&gt; high speed.
</em><br>
<p>Good for you.  I can't do that.  For me, it ends up being a week full-time.
<br>
<p><em>&gt;&gt; When I came to Novamente, I didn't succeed in explaining to anyone how
</em><br>
<em>&gt;&gt; &quot;curiosity&quot; didn't need to be an independent drive because it was
</em><br>
<em>&gt;&gt; directly emergent from information values in expected utility combined
</em><br>
<em>&gt;&gt; with Bayesian probability.  Maybe you've grown stronger since then.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; First of all, you never came to Novamente LLC (which BTW does not have 
</em><br>
<em>&gt; an office, though office space is shared with partner firms in the US 
</em><br>
<em>&gt; and Brazil).  You came to Webmind Inc., which was a different company 
</em><br>
<em>&gt; building a different AI system.
</em><br>
<p>I accept your correction.
<br>
<p><em>&gt; Novamente is founded on probability theory, Webmind was not.  That is 
</em><br>
<em>&gt; one among several significant differences between the two systems.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Secondly, when you visited Webmind, you (surprise, surprise!) seem to 
</em><br>
<em>&gt; have come away with the impression that all of the staff were much 
</em><br>
<em>&gt; slower and stupider than they actually are.
</em><br>
<p>What is this leap from, &quot;A twenty-year-old poor explainer fails to convey a 
<br>
point that will someday be a two-semester university course&quot; and &quot;He thinks 
<br>
the listeners are morons&quot;?  This is *your* mistake, Ben, not mine; to 
<br>
assume that these matters are so easily understood; that if I think you do 
<br>
not understand them, I must think you a moron.  I have climbed the 
<br>
mountain.  I know how high it is.  I know how difficult it was.  All that 
<br>
difficulty is invisible from the outside.  From the outside it looks like a 
<br>
foothill.  I am not saying you cannot climb foothills, Ben, I am saying 
<br>
that you will not know how high this mountain was until after you have 
<br>
climbed it.  Meanwhile, stop insisting that I insult your intelligence, for 
<br>
by doing so you insult the problem.  It is a beautiful problem, and does 
<br>
not deserve your insults.
<br>
<p><em>&gt; Regarding the particular point you mention, many of us UNDERSTOOD your 
</em><br>
<em>&gt; point that you COULD derive curiosity from other, more basic 
</em><br>
<em>&gt; motivations.  However, we didn't agree that this is the BEST way to 
</em><br>
<em>&gt; implement curiousity in an AGI system.   Just because something CAN be 
</em><br>
<em>&gt; achieved in a certain way doesn't mean that's the BEST way to do it -- 
</em><br>
<em>&gt; where &quot;best&quot; must be interpreted in the context of realistic memory and 
</em><br>
<em>&gt; processing power constraints.
</em><br>
<p>This does not square with my memory.  I recall that for some time after my 
<br>
visit, you would still object that an AI with &quot;narrow supergoals&quot; would be 
<br>
dull and uncreative (not &quot;creative, but slightly slower&quot;), and that drive 
<br>
XYZ would not pop up in the system or would not be represented in the 
<br>
system.  (Where XYZ was necessary, and hence straightforwardly desirable 
<br>
under the expected utility formalism.)  You would ask, &quot;But then how does 
<br>
an expected utility maximizer [you didn't call it that, nor did I, but it 
<br>
was what we were talking about] have a drive to minimize memory use?&quot;
<br>
<p><em>&gt; Novamente is more probabilistically based than Webmind, yet even so we 
</em><br>
<em>&gt; will implement novelty-seeking as an independent drive, initially, 
</em><br>
<em>&gt; because this is a lot more efficient than making the system learn this 
</em><br>
<em>&gt; from a more basic motivation.
</em><br>
<p>I didn't say you had to make the system learn the value of novelty; I said 
<br>
to pre-enter an abstract assertion along which the utility would flow.
<br>
<p><em>&gt;&gt; But as far as I can tell, you've never understood anything of Friendly
</em><br>
<em>&gt;&gt; AI theory except that it involves expected utility and a central
</em><br>
<em>&gt;&gt; utility function, which in the past you said you disagreed with.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, I believe I understand what you say in CFAI, I just don't see why 
</em><br>
<em>&gt; you think that philosophy would work in a real self-modifying software 
</em><br>
<em>&gt; system...
</em><br>
<p>CFAI contains descriptions of *more* than Friendliness-topped systems or 
<br>
expected utility.  I do not see, in your emails, any signs that you realize 
<br>
the other parts of CFAI exist.
<br>
<p><em>&gt;&gt; I still haven't managed to make you see the point of &quot;external
</em><br>
<em>&gt;&gt; reference semantics&quot; as described in CFAI, which I consider the Pons 
</em><br>
<em>&gt;&gt; Asinorum of Friendly AI; the first utility system with nontrivial 
</em><br>
<em>&gt;&gt; function, with the intent in CFAI being to describe an elegant way to
</em><br>
<em>&gt;&gt; repair programmer errors in describing morality.  It's not that I
</em><br>
<em>&gt;&gt; haven't managed to make you agree, Ben, it's that you still haven't
</em><br>
<em>&gt;&gt; seen the *point*, the thing the system as described is supposed to
</em><br>
<em>&gt;&gt; *do*, and why it's different from existing proposals.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Your &quot;external reference semantics&quot;, as I recall, is basically the idea 
</em><br>
<em>&gt; that an AGI system considers its own supergoals to be uncertain 
</em><br>
<em>&gt; approximations of some unknown ideal supergoals, and tries to improve 
</em><br>
<em>&gt; its own supergoals.  It's kind of a supergoal that says &quot;Make all my 
</em><br>
<em>&gt; supergoals, including this one, do what they're supposed to do better.&quot;
</em><br>
<p>No.  That was an earlier system that predated CFAI by years - 1999-era or 
<br>
thereabouts.  CFAI obsoleted that business completely.
<br>
<p>External reference semantics says, &quot;Your supergoal content is an 
<br>
approximation to [this function which is expensive to compute] or [the 
<br>
contents of this box in external reality] or [this box in external reality 
<br>
which contains a description of a function which is expensive to compute].&quot; 
<br>
&nbsp;&nbsp;This is a novel expansion to the expected utility formalism.  A goal 
<br>
system that handles the external reference in the utility function in a 
<br>
certain specific way will retain the external reference, and continue 
<br>
improving the approximation, even under self-modification.  Or at least, 
<br>
that was the idea in CFAI.  I now know that there are other unsolved 
<br>
problems.  What doesn't change is that if you don't solve the ERS problem, 
<br>
the system will definitely not be stable under self-modification (or rather 
<br>
it will be stable, just not stable the way you hoped).
<br>
<p><em>&gt; What I don't understand is why you think this idea is so amazingly 
</em><br>
<em>&gt; profound.  Yes, this attitude toward one's supergoals is an element of 
</em><br>
<em>&gt; what people call &quot;wisdom.&quot;  But I don't see that this kind of thing 
</em><br>
<em>&gt; provides any kind of guarantee of Friendliness after iterated 
</em><br>
<em>&gt; self-modification.  Seems to me that an AGI with &quot;external reference 
</em><br>
<em>&gt; semantics&quot; [TERRIBLE name for the concept, BTW ;-)]
</em><br>
<p>I agree.
<br>
<p><em>&gt; can go loony just as easily as one without.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But if I ever disagree with one of your ideas, your reaction is &quot;Well 
</em><br>
<em>&gt; that's because you don't understand it.&quot; ;-p
</em><br>
<p>You did not understand it, as the above paragraphs illustrate.  It is not 
<br>
that I see you disagreeing.  It is that I do not see any indication that 
<br>
you are representing, in your mind, a model of the author's meaning that is 
<br>
even close to the truth.
<br>
<p><em>&gt; BTW, the idea of goals having probabilities, propagating these to sub 
</em><br>
<em>&gt; and supergoals, etc., was there in Webmind and is there in Novamente.
</em><br>
<em>&gt; Goal refinement has been part of my AI design for a long time, and it 
</em><br>
<em>&gt; was always applied to top-level supergoals as well as to other goals.
</em><br>
<p>&quot;Goal refinement&quot; is not the big idea.  The idea is a *particular set* of 
<br>
semantics for goal refinement that is stable under self-modification, in a 
<br>
way that is not possible without expanding the standard expected utility 
<br>
formalism.
<br>
<p>Supergoal refinement is part of plenty of other proposals I have heard, and 
<br>
in all of them the supergoal-refinement mechanism immediately washes out 
<br>
under self-modification.
<br>
<p><em>&gt;&gt; Doesn't excuse every new generation of scientists making the same
</em><br>
<em>&gt;&gt; mistakes over, and over, and over again.  Imagine my chagrin when I 
</em><br>
<em>&gt;&gt; realized that consciousness was going to have an explanation in
</em><br>
<em>&gt;&gt; ordinary, mundane, non-mysterious physics, just like the LAST THOUSAND
</em><br>
<em>&gt;&gt; FRICKIN' MYSTERIES the human species had encountered.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1) I don't call quantum physics &quot;non-mysterious physics&quot;
</em><br>
<p>I do.  Everett dispels the mystery completely.  The answer has been known 
<br>
since 1957; it just isn't widely appreciated outside the theoretical 
<br>
physics community.  As I should have suspected, back when I believed the 
<br>
people boasting about humanity's ignorance.  There are mysterious 
<br>
questions.  Never mysterious answers.
<br>
<p><em>&gt; 2) Have you worked out a convincing physics-based explanation of 
</em><br>
<em>&gt; consciousness?  If so, why aren't you sharing it with us?  Too busy to 
</em><br>
<em>&gt; take the time?
</em><br>
<p>I worked out something else I had once elevated to the status of a holy 
<br>
mystery.  Reading scientific history is no substitute for undergoing the 
<br>
experience personally.  If only I had *personally* postulated astrological 
<br>
mysteries and discovered Newtonian physics, *personally* postulated 
<br>
alchemical mysteries and discovered chemistry, *personally* postulated 
<br>
vitalistic mysteries and discovered biology.  I would have thought of a 
<br>
mysterious explanation for consciousness and said to myself:  &quot;No way am I 
<br>
falling for that again.&quot;
<br>
<p>Reading the scientific history doesn't convey how reasonable and plausible 
<br>
vitalism must have seemed *at the time*, how *surprising* and *embarassing* 
<br>
it was when Nature came back and said:  &quot;*Still* just physics.&quot;  I didn't 
<br>
have to be extraordinarily stupid to make the mistake - just human.  Each 
<br>
new generation grows up in a world where physics and chemistry and biology 
<br>
have never been mysterious, and past generations were just stupid.  And 
<br>
each new generation says, &quot;But this problem *really is* mysterious,&quot; and 
<br>
makes the same mistake again.
<br>
<p>I did not insult you so greatly, by comparing you to, say, a competent, 
<br>
experienced medieval alchemist.  I do not even say that you are an 
<br>
alchemist, just that you are presently thinking alchemically about the 
<br>
problem of consciousness, and AI morality.  As did I, once upon a time.
<br>
<p>But now I've gone through the embarassment myself, and seen the blindingly 
<br>
obvious historical parallels.  Now I understand in my bones, not just 
<br>
abstractly, that we live in a non-mysterious universe.  Yes!  It really is 
<br>
ALL JUST PHYSICS!  And not special mysterious physics either, just ordinary 
<br>
mundane physics.  No exceptions to explain the current sacred mystery, even 
<br>
if it seems, y'know, REALLY MYSTERIOUS.
<br>
<p>And, yes, I'm too busy to explain that formerly mysterious thing I solved, 
<br>
the warm-up problem that gave me hope of tackling consciousness 
<br>
successfully.  I hope to get around to explaining eventually.  But it's fun 
<br>
philosophy, not crisis-critical, and therefore a low priority.  Right now 
<br>
I'm working on an answer to Aubrey de Grey's pending question about how to 
<br>
define Friendliness.
<br>
<p>I know it sounds odd.  Maybe if someone offers SIAI a sufficiently huge 
<br>
donation, I'll commit to writing it up.  But everything I said should be 
<br>
blindingly obvious in retrospect, so whether the alleged discovery is real 
<br>
or gibberish, the materialist satori is straightforward.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8746.html">Eliezer Yudkowsky: "Re: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>Previous message:</strong> <a href="8744.html">Marc Geddes: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<li><strong>In reply to:</strong> <a href="8739.html">Ben Goertzel: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8737.html">Thomas Buckner: "RE: The dangers of genuine ignorance (was: Volitional Morality and Action Judgement)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8745">[ date ]</a>
<a href="index.html#8745">[ thread ]</a>
<a href="subject.html#8745">[ subject ]</a>
<a href="author.html#8745">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
