<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Volitional Morality and Action Judgement</title>
<meta name="Author" content="Christopher Healey (CHealey@unicom-inc.com)">
<meta name="Subject" content="RE: Volitional Morality and Action Judgement">
<meta name="Date" content="2004-05-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Volitional Morality and Action Judgement</h1>
<!-- received="Mon May 17 09:06:05 2004" -->
<!-- isoreceived="20040517150605" -->
<!-- sent="Mon, 17 May 2004 11:07:07 -0400" -->
<!-- isosent="20040517150707" -->
<!-- name="Christopher Healey" -->
<!-- email="CHealey@unicom-inc.com" -->
<!-- subject="RE: Volitional Morality and Action Judgement" -->
<!-- id="1D68577DAAE6304A89E3C8BC896262A4053131@w2k3exch01.UNICOM-INC.CORP" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="Volitional Morality and Action Judgement" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Christopher Healey (<a href="mailto:CHealey@unicom-inc.com?Subject=RE:%20Volitional%20Morality%20and%20Action%20Judgement"><em>CHealey@unicom-inc.com</em></a>)<br>
<strong>Date:</strong> Mon May 17 2004 - 09:07:07 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8562.html">Randall Randall: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8560.html">Keith Henson: "Re. Volitional Morality and Action Judgement"</a>
<li><strong>Maybe in reply to:</strong> <a href="8523.html">Michael Roy Ames: "Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8656.html">Ben Goertzel: "RE: Volitional Morality and Action Judgement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8561">[ date ]</a>
<a href="index.html#8561">[ thread ]</a>
<a href="subject.html#8561">[ subject ]</a>
<a href="author.html#8561">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Also along this line of attack, what do you think the results would be as that SAI increased both the scope and timeframe of it's predicitive horizon?  
<br>
&nbsp;
<br>
Using any type of goal system that hedges between an individual's and the &quot;greater&quot; good, it seems a strong possibility that the sheer magnitude of the greater good's weighting might outstrip any individual's weighting quickly.  Especially when that predicitive horizon envelops the cusp of a critical systemic decision (endgame scenario, etc...).  We'd consider that behavior degenerate, of course, but if it's a structural possiblity of the architecture, then it's just working within the (poor) design parameters.
<br>
&nbsp;
<br>
Is it possible to create an AI that holds as a singly-rooted goal, actor-volitional Friendliness, and have this generate as a sub-goal the appropriate higher-level organizational structures supporting the greater good?  The more I've been following these threads, and reading outside this forum, the more I think that the answer is yes.  Drawing a distiction between possibility and probability however, I'm more convinced it's possible, but less convinced it's probable.
<br>
&nbsp;
<br>
The question is most certainly wide, wide open.
<br>
&nbsp;
<br>
-Christopher Healey
<br>
<p>________________________________
<br>
<p>From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20Volitional%20Morality%20and%20Action%20Judgement">owner-sl4@sl4.org</a> on behalf of Thomas Buckner
<br>
Sent: Sun 5/16/2004 3:14 AM
<br>
To: <a href="mailto:sl4@sl4.org?Subject=RE:%20Volitional%20Morality%20and%20Action%20Judgement">sl4@sl4.org</a>
<br>
Subject: Re: Volitional Morality and Action Judgement
<br>
<p><p><p><p><em>&gt; &gt;Suppose I know I
</em><br>
<em>&gt; will be probably
</em><br>
<em>&gt; &gt; ruined if I continue gambling, but I decide to do
</em><br>
<em>&gt; it anyway. I'm then
</em><br>
<em>&gt; &gt; doing what is not in my best interest to do. I'm
</em><br>
<em>&gt; then acting
</em><br>
<em>&gt; &gt; irrationally. Eliezer's maxim, then, becomes
</em><br>
<em>&gt; inapplicable. To assess
</em><br>
<em>&gt; &gt; the
</em><br>
<em>&gt; &gt; agent's behavior we must look for an alternative
</em><br>
<em>&gt; rationale.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Either you are using the term &quot;best interest&quot; for
</em><br>
<em>&gt; something I would
</em><br>
<em>&gt; not use that term for, or you are making the mistake
</em><br>
<em>&gt; of assuming
</em><br>
<em>&gt; that a single objective &quot;best interest&quot; exists which
</em><br>
<em>&gt; can be determined
</em><br>
<em>&gt; by an outside observer.
</em><br>
<em>&gt;
</em><br>
<em>&gt; In order to determine a person's best interest, you
</em><br>
<em>&gt; would have to
</em><br>
<em>&gt; weigh their options against their goal system (not
</em><br>
<em>&gt; yours!) and
</em><br>
<em>&gt; choose the best option which is consistent with that
</em><br>
<em>&gt; goal system.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Unless you are intelligent enough to closely
</em><br>
<em>&gt; simulate that person,
</em><br>
<em>&gt; however (and no human currently is), you are
</em><br>
<em>&gt; unlikely to be able
</em><br>
<em>&gt; to make such a determination, so you must accept the
</em><br>
<em>&gt; person's own
</em><br>
<em>&gt; decisions as the closest approximation to their
</em><br>
<em>&gt; &quot;best interest&quot;
</em><br>
<em>&gt; that you can find.
</em><br>
<em>&gt;
</em><br>
<em>&gt; --
</em><br>
<em>&gt; Randall Randall
</em><br>
<em>&gt; &lt;<a href="mailto:randall@randallsquared.com?Subject=RE:%20Volitional%20Morality%20and%20Action%20Judgement">randall@randallsquared.com</a>&gt; (706) 536-2674
</em><br>
<em>&gt; Remote administration -- web applications --
</em><br>
<em>&gt; consulting
</em><br>
<em>&gt;
</em><br>
I can see a partial line of attack on that problem, as
<br>
follows:
<br>
You offer the example that an excessive gambler may be
<br>
'ruined', but the ramifications are not laid out in
<br>
the detail an AI might want. The ethical outcomes
<br>
vary, and thus the ethical imperatives.
<br>
Will the gambler ruin several (or many) other lives in
<br>
hir excess? Then hes should be impeded for the good of
<br>
other sentient beings.
<br>
<p>Will the gambler physically die? Then, again, an AI
<br>
may choose to intervene (there's a word for this
<br>
process: it's called an intervention!) and take
<br>
various means of persuasion or coercion to keep a
<br>
fellow sentient from self-destructing out of sheer
<br>
thoughtlessness. This is like your relatives hiding
<br>
your cigarettes: perhaps you 'know' they're bad for
<br>
your body but have an addiction to that nicotine (and
<br>
a 'tomorrow never gets here' sort of rationalizing
<br>
attitude; but it does get here, you know...)This is
<br>
the sort of unconscious suicide we see all around us,
<br>
almost daily if your town's big enough.
<br>
<p>Does the gambler know that death may follow, and
<br>
choose this risk (or certainty) while offering a good
<br>
rational ethical defense of hir choice? Then the AI
<br>
ought to at least consider staying out of the way, as
<br>
should we.
<br>
Your example implies strongly that you may have a
<br>
hidden supergoal which IS served by self-destruction.
<br>
If you cannot articulate it, it's unlikely that you
<br>
can defend it rationally. You may only assert that you
<br>
have an intuition, that your destruction will somehow
<br>
serve a goal larger than your continued conscious
<br>
existence. I am not saying your sincerity might not
<br>
somehow convince a SAI, but my belief is that ve will
<br>
tend to want to keep you around.
<br>
The alternative form of being 'ruined' is that you do
<br>
not die, nor create misery or death for others, but
<br>
simply have some bad experiences of your own. Perhaps
<br>
they are very, very bad experiences, but the
<br>
possibility will still remain that you have survived
<br>
and learned something from your travails. An SAI might
<br>
find that very acceptable, as long as it gets to
<br>
watch.
<br>
To continue the smoking metaphor, it might insist on
<br>
keeping you alive while everything but your brain
<br>
shrivels, if you're that stubborn, and then restoring
<br>
you when you turn over that new leaf.
<br>
OR...
<br>
It might actively interfere with the debt process. You
<br>
might (post-FAI) find yourself in a reality tunnel
<br>
where you never lose at cards, or wake up rich even
<br>
after losing; where you never get a hangover or run
<br>
out of whiskey; where your lungs feel fine after a
<br>
night of smoking railway ties, and you can eat lead
<br>
out of stained glass windows like it was beef jerky,
<br>
and not get poisoned; a dream world, an anthroparium.
<br>
Hmmm, that'd work. Remember, a SAI that could emulate
<br>
you could keep you like a museum display under glass,
<br>
and you'd never notice.
<br>
<p>=====
<br>
<p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
__________________________________
<br>
Do you Yahoo!?
<br>
SBC Yahoo! - Internet access at a great low price.
<br>
<a href="http://promo.yahoo.com/sbc/">http://promo.yahoo.com/sbc/</a>
<br>
<p><p><p>
<br><p>
<p><hr>
<ul>
<li>application/ms-tnef attachment: <a href="../att-8561/01-winmail.dat">winmail.dat</a>
</ul>
<!-- attachment="01-winmail.dat" -->
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8562.html">Randall Randall: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8560.html">Keith Henson: "Re. Volitional Morality and Action Judgement"</a>
<li><strong>Maybe in reply to:</strong> <a href="8523.html">Michael Roy Ames: "Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8656.html">Ben Goertzel: "RE: Volitional Morality and Action Judgement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8561">[ date ]</a>
<a href="index.html#8561">[ thread ]</a>
<a href="subject.html#8561">[ subject ]</a>
<a href="author.html#8561">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
