<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Dangers of human self-modification</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Dangers of human self-modification">
<meta name="Date" content="2004-05-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Dangers of human self-modification</h1>
<!-- received="Wed May 26 00:11:22 2004" -->
<!-- isoreceived="20040526061122" -->
<!-- sent="Tue, 25 May 2004 23:11:18 -0700" -->
<!-- isosent="20040526061118" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Dangers of human self-modification" -->
<!-- id="80418DD3-AEDB-11D8-86D4-000A95B1AFDE@objectent.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="034701c441cd$46ec5110$6401a8c0@ZOMBIETHUSTRA" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Dangers%20of%20human%20self-modification"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Wed May 26 2004 - 00:11:18 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8710.html">Samantha Atkins: "Re: Seeking Financial Support for Transhumanist Activism"</a>
<li><strong>Previous message:</strong> <a href="8708.html">Samantha Atkins: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>In reply to:</strong> <a href="8691.html">Ben Goertzel: "RE: Dangers of human self-modification"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8711.html">Ben Goertzel: "RE: Dangers of human self-modification"</a>
<li><strong>Reply:</strong> <a href="8711.html">Ben Goertzel: "RE: Dangers of human self-modification"</a>
<li><strong>Reply:</strong> <a href="8712.html">Philip Sutton: "Re: Dangers of human self-modification"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8709">[ date ]</a>
<a href="index.html#8709">[ thread ]</a>
<a href="subject.html#8709">[ subject ]</a>
<a href="author.html#8709">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On May 24, 2004, at 1:25 PM, Ben Goertzel wrote:
<br>
<p><em>&gt;
</em><br>
<em>&gt; Fudley,
</em><br>
<em>&gt;
</em><br>
<em>&gt; Put concisely, one of the main problems is: If you're modifying a human
</em><br>
<em>&gt; brain to be twice as smart, how can you be sure your modification won't
</em><br>
<em>&gt; have the side-effect of causing that human brain to feel like
</em><br>
<em>&gt; irresponsibly creating dangerous seed AI's or gray-goo-producing
</em><br>
<em>&gt; nanotech?
</em><br>
<p>Well, you can't.  But do you expect humanity with its current level of 
<br>
intelligence, morality and so on to be able to handle MNT?  How about 
<br>
raising the standard of living of more than half of humanity to a level 
<br>
we would consider &quot;decent&quot;?   Dealing with highly complex technological 
<br>
and ethical questions?   Even managing to keep the lights on and turn 
<br>
them on the in the rest of the world?    It isn't hard to find aspects 
<br>
of our world that are apparently beyond our present understanding and 
<br>
ability to control.   It isn't difficult to see that that the number 
<br>
and challenges of such things are increasing - often exponentially.
<br>
<p><em>&gt;
</em><br>
<em>&gt; Human brain mods that don't increase intelligence dramatically are
</em><br>
<em>&gt; relatively safe in existential terms, but human brain modes that do
</em><br>
<em>&gt; increase intelligence dramatically are potentially dangerous by virtue
</em><br>
<em>&gt; of the dangerous tech that smart humans may play with.
</em><br>
<em>&gt;
</em><br>
<p>It is the classic two-edged sword.  Higher intelligence can always be 
<br>
used for good or ill.   If we were not already surrounded with problems 
<br>
arguably beyond our abilities to resolve then it might make sense to 
<br>
put off increasing human intelligence.  But such is not the case.
<br>
<p><em>&gt; I'm not saying that smart humans will necessarily become evil or
</em><br>
<em>&gt; careless -- in fact I think the opposite is more closely true -- but
</em><br>
<em>&gt; it's clear that it will be hard to predict the ethical inclinations and
</em><br>
<em>&gt; quality-of-judgment of intelligence-enhanced humans.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<p>Somehow I think this is far less of a risk and less hard to predict 
<br>
than the ethical inclinations and quality-of-judgment of sentient or 
<br>
insentient SIAIs.
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8710.html">Samantha Atkins: "Re: Seeking Financial Support for Transhumanist Activism"</a>
<li><strong>Previous message:</strong> <a href="8708.html">Samantha Atkins: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>In reply to:</strong> <a href="8691.html">Ben Goertzel: "RE: Dangers of human self-modification"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8711.html">Ben Goertzel: "RE: Dangers of human self-modification"</a>
<li><strong>Reply:</strong> <a href="8711.html">Ben Goertzel: "RE: Dangers of human self-modification"</a>
<li><strong>Reply:</strong> <a href="8712.html">Philip Sutton: "Re: Dangers of human self-modification"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8709">[ date ]</a>
<a href="index.html#8709">[ thread ]</a>
<a href="subject.html#8709">[ subject ]</a>
<a href="author.html#8709">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
