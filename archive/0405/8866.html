<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: One or more AIs??</title>
<meta name="Author" content="Mark Waser (mwaser@cox.net)">
<meta name="Subject" content="Re: One or more AIs??">
<meta name="Date" content="2004-05-30">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: One or more AIs??</h1>
<!-- received="Sun May 30 13:08:13 2004" -->
<!-- isoreceived="20040530190813" -->
<!-- sent="Sun, 30 May 2004 15:08:35 -0400" -->
<!-- isosent="20040530190835" -->
<!-- name="Mark Waser" -->
<!-- email="mwaser@cox.net" -->
<!-- subject="Re: One or more AIs??" -->
<!-- id="004001c44679$822eaa60$0100a8c0@FamilyRoom" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="40BA0FE5.8020509@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Mark Waser (<a href="mailto:mwaser@cox.net?Subject=Re:%20One%20or%20more%20AIs??"><em>mwaser@cox.net</em></a>)<br>
<strong>Date:</strong> Sun May 30 2004 - 13:08:35 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8867.html">Mark Waser: "Re: One or more AIs??"</a>
<li><strong>Previous message:</strong> <a href="8865.html">fudley: "RE: One or more FAIs??"</a>
<li><strong>In reply to:</strong> <a href="8861.html">Eliezer Yudkowsky: "Re: One or more AIs??"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8870.html">Michael Roy Ames: "Re: One or more AIs??"</a>
<li><strong>Reply:</strong> <a href="8870.html">Michael Roy Ames: "Re: One or more AIs??"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8866">[ date ]</a>
<a href="index.html#8866">[ thread ]</a>
<a href="subject.html#8866">[ subject ]</a>
<a href="author.html#8866">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; I tend to regard the sharp distinction between &quot;groups&quot; and &quot;individuals&quot;
</em><br>
<em>&gt; as a special case of the human way, where cognitive systems cannot
</em><br>
<em>&gt; agglomerate, and the goal systems contain numerous egocentric
</em><br>
<em>&gt; speaker-dependent variables.
</em><br>
<p>I can agree that a &quot;sharp&quot; distinction between &quot;groups&quot; and &quot;individuals&quot; is
<br>
a special case that humans have been stuck in.  However, I also believe that
<br>
you will always have (at least momentary) individuals and always have
<br>
collections of these individuals who are separate but working together (even
<br>
if they agglomerate later) and that the distinction between individuals is a
<br>
necessary one (even if it becomes far more transient and fluid in the
<br>
future).
<br>
<p>Also, there's always the fact that you really can't have only one, monstrous
<br>
yet fully-integrated mind.  The cost of maintaining consistency across a
<br>
mind scales with size/complexity.  Beyond a certain size point, all you'll
<br>
be doing is maintaining consistency.  You will always have non-integrated
<br>
parts (also known as individuals) - - they just will be more able to more
<br>
easily agglomerate (at the same time that more individuals will branch off).
<br>
<p><em>&gt; In particular, I worry that our being adapted
</em><br>
<em>&gt; to the individuals-groups dichotomy, and our expectation that other
</em><br>
<em>&gt; individuals will exhibit those same adaptations, can lead to incorrect
</em><br>
<em>&gt; inferences when considering AIs.
</em><br>
<p>It could lead to incorrect inferences but not necessarily.  Also, it is
<br>
entirely incorrect to dismiss a conclusion because the reasoning process
<br>
that arrived at it is incorrect.
<br>
<p><em>&gt; It looks to me like the challenge of
</em><br>
<em>&gt; getting a cognitive system to compute morality remains the same regardless
</em><br>
<em>&gt; of whether that physical system is called a multiplicity or a singleton -
</em><br>
<em>&gt; whether we regard it as many minds or one mind, it's the same thing.
</em><br>
<p>I would agree.  The reason why you want a multiplicity is because the system
<br>
is not going to be omnipotent or omniscient and it's going to have to make
<br>
decisions in the absence of perfect information - - most particularly,
<br>
decisions about it's own beliefs, value structures and goals.  Now, in a
<br>
perfect world with infinite resources, the system could model the decision
<br>
being made both ways and continue to track both paths - - except then,
<br>
arguably, the system is now two systems unless the two paths inappropriately
<br>
bleed together.
<br>
<p>My experience, from many years of system design/watching others make
<br>
mistakes, is that it is far better to have the distinction of
<br>
partitioning/individuals/distinctions built in from the start because
<br>
otherwise inappropriate bleed is inevitable.  Distinctions/partitioning are
<br>
what makes it possible for us to think.  Without partitions and individuals,
<br>
a bad meme could poison everything.
<br>
<p><em>&gt; Or to put it more
</em><br>
<em>&gt; pessimistically, if a course is so difficult that it can't be solved using
</em><br>
<em>&gt; an agglomerated runner, splitting up the runner into individuals can't get
</em><br>
<em>&gt; you any closer to solving the problem.
</em><br>
<p>It's one hour before the destruction of the human race.  I have twenty
<br>
doors - nineteen take 50 minutes to traverse and come back, one leads to the
<br>
total invulnerability of human beings in 25 minutes.  Do I want one runner
<br>
or twenty?
<br>
<p>And yes, I totally twisted your analogy from a communications problem which
<br>
I think is the wrong analogy to a goal-achievement problem.
<br>
<p><em>&gt; Humans have to imagine ways of solving problems that use many humans,
</em><br>
<em>&gt; because in the human world there is no way to use *one* *big* human.
</em><br>
<p>And I argue that there is no way to *use* one big AI.  There are always
<br>
costs to maintain consistency and integrity that scale with size.  I AM sure
<br>
that the biggest feasible AI is MUCH bigger than a human, but there is
<br>
definitely a maximum size there and I'm pretty sure that it's smaller than
<br>
I'd be happy with.
<br>
<p><em>&gt; Suppose you have two &quot;thermostat AIs&quot; - that is, they have a decision
</em><br>
<em>&gt; system that employs a very simple and nonhumane way of computing
</em><br>
<em>&gt; desirability.  Let's say that one AI cares only about paperclips, while
</em><br>
the
<br>
<em>&gt; other cares only about staples.  If the two AIs are roughly equal, they
</em><br>
<em>&gt; might arrive at something resembling a cooperative game-theoretical
</em><br>
<em>&gt; solution and split up the solar system between them, this solution being
</em><br>
<em>&gt; preferable to the negative effects of hostilities - classic PD.  The
</em><br>
<em>&gt; problem is that this doesn't protect the humans - it is better for *both*
</em><br>
<em>&gt; AIs to split any resources currently used by humanity between them.
</em><br>
<p>You lost me at &quot;one AI cares only about paperclips, while the other cares
<br>
only about staples&quot;.  They both care about friendliness.  If they realize
<br>
that their world views are so out of synch as to require splitting up the
<br>
solar system, their top priority should be rationalizing their world views
<br>
before they accidentally do something horribly unfriendly.  You seem VERY
<br>
vested in your point of view to not see this argument as a total
<br>
non-sequitor.
<br>
<p><em>&gt;  From our perspective, this might not look very different from a single AI
</em><br>
<em>&gt; with a goal system that wanted both paperclips and staples.
</em><br>
<p>No, there's a huge distinction.  In one case, there are two gods - one says
<br>
&quot;I think paperclips&quot;, the other says &quot;I think staples&quot;, and both say &quot;uh oh,
<br>
maybe we better figure out why we don't have the same thought before we do
<br>
anything rash&quot;.  In the other case, the one god buries the universe in
<br>
paperclips and staples.  From my perspective, these are two very different
<br>
things.
<br>
<p><em>&gt; If you imagine that humans, in some unimaginable way, acquire a serious
</em><br>
<em>&gt; threat to hold over both superintelligences, demanding that the humans be
</em><br>
<em>&gt; treated as game-theoretical near-equals, then why wouldn't the same threat
</em><br>
<em>&gt; be holdable over a paperclips-and-staples singleton?  This is what I mean
</em><br>
<em>&gt; by the apparent equivalence of groups and individuals from our
</em><br>
perspective.
<br>
<p>Sure, if humans hold the ultimate power, it doesn't seem to matter whether
<br>
they hold power over a group or an individual.  And your point is?
<br>
<p><em>&gt; It looks to me like it takes work to compute a humane morality, work which
</em><br>
<em>&gt; does not emerge automatically in either groups or singletons.  If there's
</em><br>
a
<br>
<em>&gt; group solution I would expect there to exist a corresponding singleton
</em><br>
<em>&gt; solution.
</em><br>
<p>Absolutely.  No disagreement here whatsoever.
<br>
<p><em>&gt; Even if human morality is inherently group-based, the Friendly
</em><br>
<em>&gt; AI structure looks like it should work to embody our group morality in a
</em><br>
<em>&gt; single AI!
</em><br>
<p>It's not morality that's group-based.  It's the safest path to implementing
<br>
that morality.
<br>
<p><em>&gt;   You have the problem of getting a number N of AIs to treat humans
</em><br>
<em>&gt; humanely even if humans may not be their game-theoretical equals, and this
</em><br>
<em>&gt; looks like pretty much the same problem whether N=1000 or N=1.
</em><br>
<p>There are two different problems that you're conflating here.  There is the
<br>
problem of getting a number N of AIs to want to treat humans humanely and
<br>
there is the problem of getting a number N of AIs to succeed in their wishes
<br>
(i.e. actually treat humans humanely).  Given that we can start them off
<br>
with the goal of treating humans humanely and with the goal of maintaning
<br>
that goal, we start with the first problem already INITIALLY solved.  The
<br>
problem is maintaining that solution and preventing errors.  Independent
<br>
redundancy is a good solution to error prevention.
<br>
<p><em>&gt; You need an
</em><br>
<em>&gt; AI that is humane and values sentient life for its own sake.
</em><br>
<p>Yes.
<br>
<p><em>&gt; If N AIs
</em><br>
<em>&gt; don't value sentient life,
</em><br>
<p>we're dead.
<br>
<p><em>&gt; ** end of quote **
</em><br>
<p><em>&gt; I am in the middle of working out seriously complicated stuff that I am
</em><br>
too
<br>
<em>&gt; busy reworking to properly explain.  Sometimes I will be able to explain
</em><br>
my
<br>
<em>&gt; reasons.  Sometimes not.  I am getting more and more nervous about time.
</em><br>
<p>I would argue that anything that you can't easily explain, you haven't fully
<br>
worked out.  I understand that there is a required initial level of
<br>
knowledge before an explanation can be understood but that knowledge should
<br>
have explanations too.  I don't believe that there is anything that a single
<br>
individual can do that is so far ahead of the curve that it can't be
<br>
explained in a reasonable amount of time.  Special relativity, quantum
<br>
physics, mathemetical proofs - - they all have fairly simple (if very
<br>
lengthy in the case of many recent proofs) explanations once the conceptual
<br>
leaps are initially performed.  Science isn't magic.  Science, by
<br>
definition, is reproducible.
<br>
<p>I'm real nervous about time too.  I'm not sure what you think you're going
<br>
for that wouldn't be sped up by a collaborative process.  If you're
<br>
successfully creating an FAI all by yourself and you haven't made a mistake,
<br>
more power to you.  If you're only completing your theory of Friendliness
<br>
(which doesn't have the danger of possibly advancing UFAI if shared), then
<br>
it's going to have to be explained and integrated before it will have any
<br>
effect on the world.
<br>
<p><em>&gt; Neighborly human or not, it is not a trivial task to give a specialist
</em><br>
<em>&gt; fresh advice in his own field.  Figure on the attempt failing at least 95%
</em><br>
<em>&gt; of the time.  Most of the time, I will have long since thought through
</em><br>
<em>&gt; everything that occurred to you, in advance, whether that is readily
</em><br>
<em>&gt; apparent or not.  By all means keep trying, but if I say &quot;I already
</em><br>
thought
<br>
<em>&gt; of that,&quot; I did, whether I have time to explain or not.
</em><br>
<p>I'm not offering you fresh advice here.  I understand that you think that
<br>
you've thought through this.  You could probably successfully blow me off by
<br>
using those magical words &quot;agree to disagree&quot; but normally I wait to use
<br>
those words until we both can point to one pretty low-level fact that is the
<br>
linch-pin of both our arguments on which we disagree.  I don't think that
<br>
the community is anywhere near that point on one FAI versus many.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mark
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8867.html">Mark Waser: "Re: One or more AIs??"</a>
<li><strong>Previous message:</strong> <a href="8865.html">fudley: "RE: One or more FAIs??"</a>
<li><strong>In reply to:</strong> <a href="8861.html">Eliezer Yudkowsky: "Re: One or more AIs??"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8870.html">Michael Roy Ames: "Re: One or more AIs??"</a>
<li><strong>Reply:</strong> <a href="8870.html">Michael Roy Ames: "Re: One or more AIs??"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8866">[ date ]</a>
<a href="index.html#8866">[ thread ]</a>
<a href="subject.html#8866">[ subject ]</a>
<a href="author.html#8866">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
