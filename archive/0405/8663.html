<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Volitional Morality and Action Judgement</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Volitional Morality and Action Judgement">
<meta name="Date" content="2004-05-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Volitional Morality and Action Judgement</h1>
<!-- received="Sun May 23 16:38:48 2004" -->
<!-- isoreceived="20040523223848" -->
<!-- sent="Sun, 23 May 2004 18:38:46 -0400" -->
<!-- isosent="20040523223846" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Volitional Morality and Action Judgement" -->
<!-- id="40B127F6.10807@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="196DD01D-ACA4-11D8-86D4-000A95B1AFDE@objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Volitional%20Morality%20and%20Action%20Judgement"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun May 23 2004 - 16:38:46 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8664.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8662.html">Metaqualia: "Re: Dangers of human self-modification"</a>
<li><strong>In reply to:</strong> <a href="8654.html">Samantha Atkins: "Re: Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8664.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Reply:</strong> <a href="8664.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Reply:</strong> <a href="8670.html">Ben Goertzel: "RE: Volitional Morality and Action Judgement"</a>
<li><strong>Reply:</strong> <a href="8684.html">fudley: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Reply:</strong> <a href="8708.html">Samantha Atkins: "Re: Volitional Morality and Action Judgement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8663">[ date ]</a>
<a href="index.html#8663">[ thread ]</a>
<a href="subject.html#8663">[ subject ]</a>
<a href="author.html#8663">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Samantha Atkins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; On May 22, 2004, at 1:58 AM, Eliezer Yudkowsky wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; Samantha Atkins wrote:
</em><br>
<em>&gt;&gt;&gt; 
</em><br>
<em>&gt;&gt;&gt; What?  You want to program a FAI seed without so much as a delete
</em><br>
<em>&gt;&gt;&gt; key on your keyboard or a source control system?   The trick is
</em><br>
<em>&gt;&gt;&gt; keeping some trustworthy means of evaluating the latest changes
</em><br>
<em>&gt;&gt;&gt; whether to self or to the FAI-to-be for desirability and 
</em><br>
<em>&gt;&gt;&gt; backtracking/re-combining accordingly.   We aren't going to go 
</em><br>
<em>&gt;&gt;&gt; spelunking into AI or our own minds without at least blazing a
</em><br>
<em>&gt;&gt;&gt; trail.
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt; I was speaking of me *personally*, not an FAI.  An FAI is *designed* 
</em><br>
<em>&gt;&gt; to self-improve; I'm not.  And ideally an FAI seed is nonsentient, so
</em><br>
<em>&gt;&gt; that there are no issues with death if restored from backup, or child
</em><br>
<em>&gt;&gt; abuse if improperly designed the first time through.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Funny, but we seem to have brains complex enough to self-improve 
</em><br>
<em>&gt; extragentically and to augment ourselves in various ways.  We also have
</em><br>
<em>&gt; the brains (we think) to build the seed of more complicated minds than
</em><br>
<em>&gt; our own.   I don't see where we aren't designed to self-improve.  The
</em><br>
<em>&gt; AI will be designed to do it more easily of course.
</em><br>
<p>Having general intelligence sufficient unto the task of building a mind 
<br>
sufficient to self-improve is not the same as being able to happily plunge 
<br>
into tweaking your own source code.  I think it might literally take 
<br>
considerably more caution to tweak yourself than it would take to build a 
<br>
Friendly AI, at least if you wanted to do it reliably.  Unlike the case of 
<br>
building FAI there would be a nonzero chance of accidental success, but 
<br>
just because the chance is nonzero does not make it large.
<br>
<p>That we can self-improve &quot;extragenetically&quot; is simply not relevant; that is 
<br>
passing on cultural complexity which we *are* designed to do.  The other 
<br>
part of your analogy says, roughly speaking, human beings can (we hope) 
<br>
become FAI programmers, therefore, they can rewrite themselves.  Leaving 
<br>
aside that this analogy simply might not work, it's a hell of a bar to 
<br>
become an FAI programmer, Samantha, it's one hell of a high bar.  Most 
<br>
people aren't willing to put forth that kind of effort, and never mind the 
<br>
issue of innate intelligence.  There is also a strictness and caution, 
<br>
which people are not willing to accept, again because it looks like work. 
<br>
Here I am, who would aspire to build an FAI, saying:  &quot;Yikes!  Human 
<br>
self-improvement is way more dangerous than it looks!  You've gotta learn a 
<br>
whole buncha stuff first.&quot;  And lo the listeners reply, &quot;But I wanna 
<br>
self-improve!  Wanna do it now!&quot;  Which means they would go splat like 
<br>
chickens in a blender, same as would happen if they tried that kind of 
<br>
thinking for FAI.
<br>
<p>I am not saying that you will end up being stuck at your current level 
<br>
forever.  I am saying that if you tried self-improvement without having an 
<br>
FAI around to veto your eager plans, you'd go splat.  You shall write down 
<br>
your wishlist and lo the FAI shall say:  &quot;No, no, no, no, no, no, yes, no, 
<br>
no, no, no, no, no, no, no, no, yes, no, no, no, no, no.&quot;  And yea you 
<br>
shall say:  &quot;Why?&quot;  And the FAI shall say:  &quot;Because.&quot;
<br>
<p>Someday you will be grown enough to take direct control of your own source 
<br>
code, when you are ready to dance with Nature pressing her knife directly 
<br>
against your throat.  Today I don't think that most transhumanists even 
<br>
realize the knife is there.  &quot;Of course there'll be dangers,&quot; they say, 
<br>
&quot;but no one will actually get hurt or anything; I wanna be a catgirl.&quot;
<br>
<p><em>&gt; I do not see that it is ideal to have the FAI seed be nonsentient or 
</em><br>
<em>&gt; that this can be strictly guaranteed.   I don't see how it can be 
</em><br>
<em>&gt; expected to understand sentients sufficiently without being or becoming
</em><br>
<em>&gt; sentient.
</em><br>
<p>If you don't know how *not* to build a child, how can you be ready to build
<br>
one?  Is it easier to design a pregnant woman than a condom?  I am taking 
<br>
the challenges in their proper order.
<br>
<p><em>&gt;&gt; Again, I do not object to the *existence* of a source control system 
</em><br>
<em>&gt;&gt; for humans.  I say only that it should be a last resort and the plan 
</em><br>
<em>&gt;&gt; should be *not* to rely on it or use it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; OK, but I was objecting to lack of such for an FAI as you seem to 
</em><br>
<em>&gt; believe you can think through the design issues so fully as to not need
</em><br>
<em>&gt; to backtrack.  Many problems in a complex (even if mundane) system 
</em><br>
<em>&gt; cannot be solved on paper or in one's head satisfactorily.   They must 
</em><br>
<em>&gt; be solved in the developing system itself.   From some of your 
</em><br>
<em>&gt; statements I am not sure you fully understand this.
</em><br>
<p>Of *course* the FAI will have source control, and I'd prefer not to be 
<br>
guilty of murder every time it's used.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8664.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8662.html">Metaqualia: "Re: Dangers of human self-modification"</a>
<li><strong>In reply to:</strong> <a href="8654.html">Samantha Atkins: "Re: Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8664.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Reply:</strong> <a href="8664.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Reply:</strong> <a href="8670.html">Ben Goertzel: "RE: Volitional Morality and Action Judgement"</a>
<li><strong>Reply:</strong> <a href="8684.html">fudley: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>Reply:</strong> <a href="8708.html">Samantha Atkins: "Re: Volitional Morality and Action Judgement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8663">[ date ]</a>
<a href="index.html#8663">[ thread ]</a>
<a href="subject.html#8663">[ subject ]</a>
<a href="author.html#8663">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
