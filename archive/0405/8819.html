<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Volitional Morality and Action Judgement</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="RE: Volitional Morality and Action Judgement">
<meta name="Date" content="2004-05-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Volitional Morality and Action Judgement</h1>
<!-- received="Sat May 29 01:50:44 2004" -->
<!-- isoreceived="20040529075044" -->
<!-- sent="Sat, 29 May 2004 08:50:41 +0100 (BST)" -->
<!-- isosent="20040529075041" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="RE: Volitional Morality and Action Judgement" -->
<!-- id="20040529075041.85123.qmail@web25104.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="03ea01c44263$f96d4560$6401a8c0@ZOMBIETHUSTRA" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=RE:%20Volitional%20Morality%20and%20Action%20Judgement"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Sat May 29 2004 - 01:50:41 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8820.html">Ben Goertzel: "Indeterminacy and Intelligence"</a>
<li><strong>Previous message:</strong> <a href="8818.html">Michael Wilson: "RE: Volitional Morality and Action Judgement"</a>
<li><strong>In reply to:</strong> <a href="8701.html">Ben Goertzel: "RE: Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8820.html">Ben Goertzel: "Indeterminacy and Intelligence"</a>
<li><strong>Reply:</strong> <a href="8820.html">Ben Goertzel: "Indeterminacy and Intelligence"</a>
<li><strong>Reply:</strong> <a href="8826.html">Thomas Buckner: "RE: Volitional Morality and Action Judgement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8819">[ date ]</a>
<a href="index.html#8819">[ thread ]</a>
<a href="subject.html#8819">[ subject ]</a>
<a href="author.html#8819">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; Michael Wilson wrote:
</em><br>
<em>&gt;&gt; The correct mode of thinking is to constrain the behaviour of 
</em><br>
<em>&gt;&gt; the system so that it is theoretically impossible for it to 
</em><br>
<em>&gt;&gt; leave the class of states that you define as desirable.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I suspect (but don't know) that this is not merely hideously difficult
</em><br>
<em>&gt; but IMPOSSIBLE for highly intelligent self-modifying AI systems.
</em><br>
<p>Building nontrivial goal systems (utility functions) that will remain
<br>
within a given class under indefinite self-modification (renormalisation)
<br>
is moderately difficult. Building goal systems that strike the complex
<br>
balance between flexibility and rigidity required to implement idealised
<br>
human goal systems is really difficult. Building goal systems that will
<br>
reliably converge on a highly complex utility function that you cannot
<br>
directly specify, only give an abstract constructive account for, is
<br>
understandably a valid justification for ordering a fresh case of Jolt
<br>
cola.
<br>
<p><em>&gt; I suspect that for any adequately intelligent system there is some
</em><br>
<em>&gt; nonzero possibility of the system reaching ANY POSSIBLE POINT
</em><br>
<p>This had been said many times before by wiser heads than me, but once
<br>
again 'probabilistic self-modification is bad'. It took meembarrassinglyy
<br>
long to get this too, but it was obvious in retrospect. Of course
<br>
sufficiently implausible hardware and/or software failure can cause any
<br>
design to fail in implementation, but that risk class is very low in
<br>
sane designs.
<br>
<p><em>&gt;&gt; Without a deep understanding of the cognitive architecture, 
</em><br>
<em>&gt;&gt; you have no way of knowing whether you are 'teaching' the 
</em><br>
<em>&gt;&gt; system what you think you are teaching it. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Agreed, of course. It would also be very hard to create an AGI without
</em><br>
<em>&gt; having a deep understanding of its cognitive architecture.
</em><br>
<p>I should have distinguished 'runtime architecture' from 'substrate
<br>
architecture', semantic minefield again. Call it the knowledge base
<br>
or network structure or self-written code or memories+goal_system,
<br>
whatever you like. We're close to understanding the /substrate/
<br>
architecture of the human brain and to build a working AGI from
<br>
scratch you have to understand a functional general intelligence
<br>
architecture to have any chance of success. However this is very
<br>
different from understanding how the system is doing what it is
<br>
doing in operation; the function of all AI-built code paths and
<br>
data structures up to takeoff. All of the emergence advocates I
<br>
know of think how their networks actually solve the problem is
<br>
an interesting point for further research but not necessary to
<br>
actually use the AGI. This is a terminal attitude and made worse
<br>
for seed AI because the substrate architecture quickly changes
<br>
(unless you know how to design the goal system to prevent that) too.
<br>
<p><em>&gt;&gt; If you /do/ have a deep understanding of the architecture, then
</em><br>
<em>&gt;&gt; you don't teach, you specify
</em><br>
<p>Incidentally I was speaking of goals, not skills. We can't specify
<br>
skills because we don't have the introspective ability or CogSci
<br>
knowledge to know how we do things in that detail. However I would
<br>
be disappointed if we couldn't easily understand how every skill up
<br>
to humanish competence works.
<br>
<p><em>&gt; The cognitive architecture may be such that learning by experience
</em><br>
<em>&gt; is the most effective way for it to learn.
</em><br>
<p>I would note that humans don't learn our supergoals, we are built
<br>
with them (and evolution didn't do a very good job of that). If
<br>
babies had to learn their supergoals they'd die within days.
<br>
<p><em>&gt; Specifying knowledge rather than teaching via experience may
</em><br>
<em>&gt; be possible *in principle* but it may be extremely slow compared to
</em><br>
<em>&gt; the high-bandwidth information uptake obtainable via experiential
</em><br>
<em>&gt; learning in an environment.
</em><br>
<p>Goals are not knowledge (well, with introspection on the goal system
<br>
they are, but that's the AI modelling its own goal system as relevant
<br>
world knowledge, not goals as goals). Goals specify what you want done;
<br>
knowledge lets you work out how to do it. Clearly you don't have to
<br>
have an abstract definition of the entire problem as you wouldn't need
<br>
an AI if you knew that, but you /must/ have a clear abstract
<br>
specification of what you want done to avoid unpredictable (i.e. almost
<br>
certainly bad; the space of desirable outcomes is generally tiny and
<br>
the ways to leave it via lack of understanding numerous) results.
<br>
<p><em>&gt; With a complete Novamente system that is enabled to self-modify its
</em><br>
<em>&gt; cognitive schemata, there will be a greater than zero risk, and more
</em><br>
<em>&gt; careful risk analysis will be needed.
</em><br>
<p>Wherever there is the possibility of evolutionary dynamics as you know
<br>
them, there is takeoff risk.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p><p><p><p><p><p><p><p><p>.
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
____________________________________________________________
<br>
Yahoo! Messenger - Communicate instantly...&quot;Ping&quot; 
<br>
your friends today! Download Messenger Now 
<br>
<a href="http://uk.messenger.yahoo.com/download/index.html">http://uk.messenger.yahoo.com/download/index.html</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8820.html">Ben Goertzel: "Indeterminacy and Intelligence"</a>
<li><strong>Previous message:</strong> <a href="8818.html">Michael Wilson: "RE: Volitional Morality and Action Judgement"</a>
<li><strong>In reply to:</strong> <a href="8701.html">Ben Goertzel: "RE: Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8820.html">Ben Goertzel: "Indeterminacy and Intelligence"</a>
<li><strong>Reply:</strong> <a href="8820.html">Ben Goertzel: "Indeterminacy and Intelligence"</a>
<li><strong>Reply:</strong> <a href="8826.html">Thomas Buckner: "RE: Volitional Morality and Action Judgement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8819">[ date ]</a>
<a href="index.html#8819">[ thread ]</a>
<a href="subject.html#8819">[ subject ]</a>
<a href="author.html#8819">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
