<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ethics</title>
<meta name="Author" content="Marc Geddes (marc_geddes@yahoo.co.nz)">
<meta name="Subject" content="Re: ethics">
<meta name="Date" content="2004-05-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ethics</h1>
<!-- received="Sat May 22 03:22:08 2004" -->
<!-- isoreceived="20040522092208" -->
<!-- sent="Sat, 22 May 2004 21:22:06 +1200 (NZST)" -->
<!-- isosent="20040522092206" -->
<!-- name="Marc Geddes" -->
<!-- email="marc_geddes@yahoo.co.nz" -->
<!-- subject="Re: ethics" -->
<!-- id="20040522092206.86139.qmail@web20204.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="40AE6B28.2020705@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marc Geddes (<a href="mailto:marc_geddes@yahoo.co.nz?Subject=Re:%20ethics"><em>marc_geddes@yahoo.co.nz</em></a>)<br>
<strong>Date:</strong> Sat May 22 2004 - 03:22:06 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8643.html">Marc Geddes: "Re: Dangers of human self-modification"</a>
<li><strong>Previous message:</strong> <a href="8641.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>In reply to:</strong> <a href="8626.html">Eliezer Yudkowsky: "Re: ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8603.html">J. Andrew Rogers: "Re: ethics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8642">[ date ]</a>
<a href="index.html#8642">[ thread ]</a>
<a href="subject.html#8642">[ subject ]</a>
<a href="author.html#8642">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;--- Eliezer Yudkowsky &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20ethics">sentience@pobox.com</a>&gt; wrote: &gt; 
<br>
<em>&gt; 
</em><br>
<em>&gt; Yes, I understand the danger here.  But Samantha,
</em><br>
<em>&gt; I'm not sure I'm ready 
</em><br>
<em>&gt; to be a father.  I think I know how to redirect
</em><br>
<em>&gt; futures, deploy huge 
</em><br>
<em>&gt; amounts of what I would consider to be intelligence
</em><br>
<em>&gt; and what I would 
</em><br>
<em>&gt; cautiously call &quot;optimization pressures&quot; for the
</em><br>
<em>&gt; sake of avoiding 
</em><br>
<em>&gt; conversational ambiguity.  But I'm still fathoming
</em><br>
<em>&gt; the reasons why 
</em><br>
<em>&gt; humans think they have conscious experiences, and
</em><br>
<em>&gt; the foundations of 
</em><br>
<em>&gt; fun, and the answers to the moral questions implicit
</em><br>
<em>&gt; in myself.  I feel 
</em><br>
<em>&gt; myself lacking in the knowledge, and the surety of
</em><br>
<em>&gt; knowledge, needed to 
</em><br>
<em>&gt; create a new sentient species.  And I wistfully wish
</em><br>
<em>&gt; that all humankind 
</em><br>
<em>&gt; should have a voice in such a decision, the creation
</em><br>
<em>&gt; of humanity's first 
</em><br>
<em>&gt; child.  And I wonder if it is a thing we would
</em><br>
<em>&gt; regard as a loss of 
</em><br>
<em>&gt; destiny, to be rescued from our present crisis by a
</em><br>
<em>&gt; true sentient mind 
</em><br>
<em>&gt; vastly superior to ourselves in both intelligence
</em><br>
<em>&gt; and morality, rather 
</em><br>
<em>&gt; than a powerful optimization process bound to the
</em><br>
<em>&gt; collective volition of 
</em><br>
<em>&gt; humankind.  There's a difference between being
</em><br>
<em>&gt; manifesting the 
</em><br>
<em>&gt; superposed extrapolation of the decisions humankind
</em><br>
<em>&gt; would prefer given 
</em><br>
<em>&gt; sufficient intelligence, and being rescued by an
</em><br>
<em>&gt; actual parent.
</em><br>
<p>Definitely sounds like a major change in your
<br>
strategy.  Not losing your nerve are you? ;)  Balls,
<br>
man.  Keep your balls.  
<br>
<p>Well, the Sys Op idea was always pretty dubious to my
<br>
mind.  I fear that the resentment of &quot;being rescued by
<br>
an actual parent&quot; would be huge.  Look at the U.S in
<br>
Iraq, trying to &quot;rescue&quot; Iraqis from themselves. 
<br>
Costs outweight benefits?  Quite likely.
<br>
<p>Even with an FAI as actual 'God-like' being though,
<br>
there are other options.  For instance the FAI could
<br>
remove itself to an asterioid and only set up small
<br>
local Sys Ops, and help on an individual basis: I'll
<br>
help you out if you agree to respect the rules of my
<br>
local Sys Op: that sort of thing.
<br>
<p>I continue to be puzzled by this talk of &quot;superposed
<br>
extrapolation of the decisions humankind would prefer
<br>
given sufficient intelligence&quot;.  I'm not at all sure
<br>
it's coherent.  Most humans aren't even aware of
<br>
Transhumanism, don't want a bar of it, don't even
<br>
think that AI is possible etc.  I'm really skeptical
<br>
that you could avoid some input from the personal
<br>
level when building a transhuman mind.  
<br>
<p><p><em>&gt; -- 
</em><br>
<em>&gt; Eliezer S. Yudkowsky                         
</em><br>
<em>&gt; <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt; Research Fellow, Singularity Institute for
</em><br>
<em>&gt; Artificial Intelligence 
</em><br>
<p>=====
<br>
&quot;Live Free or Die, Death is not the Worst of Evils.&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Gen. John Stark
<br>
<p>&quot;The Universe...or nothing!&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-H.G.Wells
<br>
<p><p>Please visit my web-sites.
<br>
<p>Science-Fiction and Fantasy:  <a href="http://www.prometheuscrack.com">http://www.prometheuscrack.com</a>
<br>
Science, A.I, Maths            :  <a href="http://www.riemannai.org">http://www.riemannai.org</a>
<br>
<p>Find local movie times and trailers on Yahoo! Movies.
<br>
<a href="http://au.movies.yahoo.com">http://au.movies.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8643.html">Marc Geddes: "Re: Dangers of human self-modification"</a>
<li><strong>Previous message:</strong> <a href="8641.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>In reply to:</strong> <a href="8626.html">Eliezer Yudkowsky: "Re: ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8603.html">J. Andrew Rogers: "Re: ethics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8642">[ date ]</a>
<a href="index.html#8642">[ thread ]</a>
<a href="subject.html#8642">[ subject ]</a>
<a href="author.html#8642">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
