<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Volitional Morality and Action Judgement</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Volitional Morality and Action Judgement">
<meta name="Date" content="2004-05-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Volitional Morality and Action Judgement</h1>
<!-- received="Tue May 25 23:41:51 2004" -->
<!-- isoreceived="20040526054151" -->
<!-- sent="Tue, 25 May 2004 22:41:46 -0700" -->
<!-- isosent="20040526054146" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Volitional Morality and Action Judgement" -->
<!-- id="60780CBF-AED7-11D8-86D4-000A95B1AFDE@objectent.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="40B127F6.10807@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Volitional%20Morality%20and%20Action%20Judgement"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Tue May 25 2004 - 23:41:46 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8709.html">Samantha Atkins: "Re: Dangers of human self-modification"</a>
<li><strong>Previous message:</strong> <a href="8707.html">Samantha Atkins: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>In reply to:</strong> <a href="8663.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8633.html">Michael Roy Ames: "Re: Volitional Morality and Action Judgement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8708">[ date ]</a>
<a href="index.html#8708">[ thread ]</a>
<a href="subject.html#8708">[ subject ]</a>
<a href="author.html#8708">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On May 23, 2004, at 3:38 PM, Eliezer Yudkowsky wrote:
<br>
<em>&gt;&gt;&gt; I was speaking of me *personally*, not an FAI.  An FAI is *designed* 
</em><br>
<em>&gt;&gt;&gt; to self-improve; I'm not.  And ideally an FAI seed is nonsentient, 
</em><br>
<em>&gt;&gt;&gt; so
</em><br>
<em>&gt;&gt;&gt; that there are no issues with death if restored from backup, or child
</em><br>
<em>&gt;&gt;&gt; abuse if improperly designed the first time through.
</em><br>
<em>&gt;&gt; Funny, but we seem to have brains complex enough to self-improve 
</em><br>
<em>&gt;&gt; extragentically and to augment ourselves in various ways.  We also 
</em><br>
<em>&gt;&gt; have
</em><br>
<em>&gt;&gt; the brains (we think) to build the seed of more complicated minds than
</em><br>
<em>&gt;&gt; our own.   I don't see where we aren't designed to self-improve.  The
</em><br>
<em>&gt;&gt; AI will be designed to do it more easily of course.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Having general intelligence sufficient unto the task of building a 
</em><br>
<em>&gt; mind sufficient to self-improve is not the same as being able to 
</em><br>
<em>&gt; happily plunge into tweaking your own source code.  I think it might 
</em><br>
<em>&gt; literally take considerably more caution to tweak yourself than it 
</em><br>
<em>&gt; would take to build a Friendly AI, at least if you wanted to do it 
</em><br>
<em>&gt; reliably.  Unlike the case of building FAI there would be a nonzero 
</em><br>
<em>&gt; chance of accidental success, but just because the chance is nonzero 
</em><br>
<em>&gt; does not make it large.
</em><br>
<em>&gt;
</em><br>
<p>I don't know about &quot;happily&quot; or &quot;plunging&quot; for that matter.  But we are 
<br>
gaining the ability to improve upon some of our own hardware and 
<br>
&quot;software&quot;.   You yourself have written of such possibilities (although 
<br>
mostly of a rather extreme subset) and of some level of such 
<br>
self-improvement possibly being essential to solving the problems we 
<br>
face including building a truly Friendly AI.     I am not speaking of 
<br>
&quot;plunging&quot; but of carefully attempting such improvements as we can, 
<br>
many of them by use of external devices (computer augmentation) 
<br>
originally.   Some internal chemical and or gene-therapeutic 
<br>
enhancements are also possible in the short-term with the former being 
<br>
more immediately available and more tested.   More is of course 
<br>
possible as new technologies come online.  It is possible for us to 
<br>
ratchet ourselves up the intelligence curve for a while.   I think we 
<br>
very much need to do so.
<br>
<p><em>&gt; That we can self-improve &quot;extragenetically&quot; is simply not relevant; 
</em><br>
<em>&gt; that is passing on cultural complexity which we *are* designed to do.
</em><br>
<p>On the contrary, it is relevant as it brought us to the point of more 
<br>
directly self-improving.  It is a former of collective self-reflection 
<br>
and self-modification.
<br>
<p><em>&gt;   The other part of your analogy says, roughly speaking, human beings 
</em><br>
<em>&gt; can (we hope) become FAI programmers, therefore, they can rewrite 
</em><br>
<em>&gt; themselves.  Leaving aside that this analogy simply might not work, 
</em><br>
<em>&gt; it's a hell of a bar to become an FAI programmer, Samantha, it's one 
</em><br>
<em>&gt; hell of a high bar.  Most people aren't willing to put forth that kind 
</em><br>
<em>&gt; of effort, and never mind the issue of innate intelligence.  There is 
</em><br>
<em>&gt; also a strictness and caution, which people are not willing to accept, 
</em><br>
<em>&gt; again because it looks like work. Here I am, who would aspire to build 
</em><br>
<em>&gt; an FAI, saying:  &quot;Yikes!  Human self-improvement is way more dangerous 
</em><br>
<em>&gt; than it looks!  You've gotta learn a whole buncha stuff first.&quot;  And 
</em><br>
<em>&gt; lo the listeners reply, &quot;But I wanna self-improve!  Wanna do it now!&quot;  
</em><br>
<em>&gt; Which means they would go splat like chickens in a blender, same as 
</em><br>
<em>&gt; would happen if they tried that kind of thinking for FAI.
</em><br>
<p>You build up an image of yourself as thoughtful, intelligent, caring 
<br>
and sane and of others who suggests different paths as being 
<br>
irresponsible, relatively stupid and uncaring.    This is getting very, 
<br>
very old.   If we wait around for the Eliezer-brain to figure 
<br>
everything out although he can't explain it to anyone else (as he 
<br>
believes he shouldn't if he could) then we will indeed go SPLAT!
<br>
<p><em>&gt;
</em><br>
<em>&gt; I am not saying that you will end up being stuck at your current level 
</em><br>
<em>&gt; forever.  I am saying that if you tried self-improvement without 
</em><br>
<em>&gt; having an FAI around to veto your eager plans, you'd go splat.  You 
</em><br>
<em>&gt; shall write down your wishlist and lo the FAI shall say:  &quot;No, no, no, 
</em><br>
<em>&gt; no, no, no, yes, no, no, no, no, no, no, no, no, no, yes, no, no, no, 
</em><br>
<em>&gt; no, no.&quot;  And yea you shall say:  &quot;Why?&quot;  And the FAI shall say:  
</em><br>
<em>&gt; &quot;Because.&quot;
</em><br>
<em>&gt;
</em><br>
<p>There are so many levels of self-improvement.  Many of them do not 
<br>
require an FAI minder to successfully and fairly safely pursue.    
<br>
Since you of late talk of your FAI not even being sentient I hardly 
<br>
think it likely we will look to it for this much wisdom as what we 
<br>
should and should not attempt.
<br>
<p><em>&gt; Someday you will be grown enough to take direct control of your own 
</em><br>
<em>&gt; source code, when you are ready to dance with Nature pressing her 
</em><br>
<em>&gt; knife directly against your throat.  Today I don't think that most 
</em><br>
<em>&gt; transhumanists even realize the knife is there.  &quot;Of course there'll 
</em><br>
<em>&gt; be dangers,&quot; they say, &quot;but no one will actually get hurt or anything; 
</em><br>
<em>&gt; I wanna be a catgirl.&quot;
</em><br>
<em>&gt;
</em><br>
<p>Sigh.  People will get hurt attempting to transcend the current &quot;human 
<br>
condition&quot;.    There is no doubt about that.  But that does not mean we 
<br>
should not try where the tradeoffs seem reasonable and where the price 
<br>
of remaining as we are is quite high.
<br>
<p><em>&gt;&gt; I do not see that it is ideal to have the FAI seed be nonsentient or 
</em><br>
<em>&gt;&gt; that this can be strictly guaranteed.   I don't see how it can be 
</em><br>
<em>&gt;&gt; expected to understand sentients sufficiently without being or 
</em><br>
<em>&gt;&gt; becoming
</em><br>
<em>&gt;&gt; sentient.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If you don't know how *not* to build a child, how can you be ready to 
</em><br>
<em>&gt; build
</em><br>
<em>&gt; one?  Is it easier to design a pregnant woman than a condom?  I am 
</em><br>
<em>&gt; taking the challenges in their proper order.
</em><br>
<p>Your ordering is made up and empty of any great significance.  It makes 
<br>
a poor counter to an argument that an FAI with the abilities you 
<br>
advertise it as having is unlikely without sentience.
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8709.html">Samantha Atkins: "Re: Dangers of human self-modification"</a>
<li><strong>Previous message:</strong> <a href="8707.html">Samantha Atkins: "Re: Volitional Morality and Action Judgement"</a>
<li><strong>In reply to:</strong> <a href="8663.html">Eliezer Yudkowsky: "Re: Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8633.html">Michael Roy Ames: "Re: Volitional Morality and Action Judgement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8708">[ date ]</a>
<a href="index.html#8708">[ thread ]</a>
<a href="subject.html#8708">[ subject ]</a>
<a href="author.html#8708">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
