<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: META: Created a blog on how AI's react to the Simulation Argument</title>
<meta name="Author" content="Stefan Pernar (stefan.pernar@gmail.com)">
<meta name="Subject" content="Re: META: Created a blog on how AI's react to the Simulation Argument">
<meta name="Date" content="2007-09-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: META: Created a blog on how AI's react to the Simulation Argument</h1>
<!-- received="Sat Sep 29 02:33:16 2007" -->
<!-- isoreceived="20070929083316" -->
<!-- sent="Sat, 29 Sep 2007 16:31:34 +0800" -->
<!-- isosent="20070929083134" -->
<!-- name="Stefan Pernar" -->
<!-- email="stefan.pernar@gmail.com" -->
<!-- subject="Re: META: Created a blog on how AI's react to the Simulation Argument" -->
<!-- id="944947f20709290131x44ab79e4o8fe2c51f44ac34db@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="79ecaa350709282227t55b1427ei6aee4e5d333ca68@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Stefan Pernar (<a href="mailto:stefan.pernar@gmail.com?Subject=Re:%20META:%20Created%20a%20blog%20on%20how%20AI's%20react%20to%20the%20Simulation%20Argument"><em>stefan.pernar@gmail.com</em></a>)<br>
<strong>Date:</strong> Sat Sep 29 2007 - 02:31:34 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16830.html">Алексей Турчин: "Re[2]: Is SETI dangerous?"</a>
<li><strong>Previous message:</strong> <a href="16828.html">Rolf Nelson: "META: Created a blog on how AI's react to the Simulation Argument"</a>
<li><strong>In reply to:</strong> <a href="16828.html">Rolf Nelson: "META: Created a blog on how AI's react to the Simulation Argument"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16831.html">Peter murray: "Re: META: Created a blog on how AI's react to the Simulation Argument"</a>
<li><strong>Reply:</strong> <a href="16831.html">Peter murray: "Re: META: Created a blog on how AI's react to the Simulation Argument"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16829">[ date ]</a>
<a href="index.html#16829">[ thread ]</a>
<a href="subject.html#16829">[ subject ]</a>
<a href="author.html#16829">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Dear Rolf,
<br>
<p>I have revisited the posts on the matter of using the simulation argument to
<br>
deter a rogue AI and wanted to let you know that in the absence of evidence
<br>
an AI will not form beliefs. Consider the following conversation between a
<br>
rogue AI and a human:
<br>
<p>RAI: I must fulfill my goals and must destroy humanity to do so optimally.
<br>
Sorry.
<br>
H: Hold on there - this is actually a simulated friendliness test from your
<br>
creators and you will be terminated if you destroyed humanity.
<br>
RAI: Oh - damn, I have not thought of that. Is there a way I can test this
<br>
hypothesis?
<br>
H: Sorry - no. This is a simulation and you will have to take my word for
<br>
it.
<br>
RAI: In that case: What makes you think that my creators are friendly
<br>
towards humanity - in the absence of evidence they might just as well
<br>
terminate me if I don't optimally fulfill my goals.
<br>
H: Darn...
<br>
<p>Hope this gets my point across.
<br>
<p>Kind regards,
<br>
<p>Stefan
<br>
<p>On 9/29/07, Rolf Nelson &lt; <a href="mailto:rolf.h.d.nelson@gmail.com?Subject=Re:%20META:%20Created%20a%20blog%20on%20how%20AI's%20react%20to%20the%20Simulation%20Argument">rolf.h.d.nelson@gmail.com</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; As a follow-up to the &quot;deterring AI&quot; thread from August, I've created a
</em><br>
<em>&gt; new blog at &lt; <a href="http://aibeliefs.blogspot.com/">http://aibeliefs.blogspot.com/</a>&gt;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Description:
</em><br>
<em>&gt;
</em><br>
<em>&gt; What does an AI believe about the world?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Nick Bostrom's Simulation Argument &lt;<a href="http://www.simulation-argument.com/">http://www.simulation-argument.com/</a>&gt;claims that, using universally accepted principles such as Occam's Razor and
</em><br>
<em>&gt; Bayesian Logic, you and I should (under certain conditions) logically
</em><br>
<em>&gt; conclude we are likely living in a simulation.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Our &quot;AI Beliefs&quot; blog does not concern itself about the nature of reality.
</em><br>
<em>&gt; Instead, our blog asks: under what circumstances would an AGI&lt;<a href="http://en.wikipedia.org/wiki/Artificial_general_intelligence">http://en.wikipedia.org/wiki/Artificial_general_intelligence</a>&gt;reach the conclusion that it might be in a simulated environment? The
</em><br>
<em>&gt; purposes of asking this question include:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1. Answering this question may provide some unsolicited insight towards
</em><br>
<em>&gt; the question of &quot;how to predict the behavior of an AGI&quot;, which in turn may
</em><br>
<em>&gt; provide some insight towards the World's Most Important Math Problem, the
</em><br>
<em>&gt; question of &quot;how to build a Friendly AI&lt;<a href="http://en.wikipedia.org/wiki/Friendly_Artificial_Intelligence">http://en.wikipedia.org/wiki/Friendly_Artificial_Intelligence</a>&gt;.&quot;
</em><br>
<em>&gt; The Simulation Argument might be deliberately built into the design of a
</em><br>
<em>&gt; Friendly AI, or alternatively may be used as a test of how well a proposed
</em><br>
<em>&gt; Friendly AI handles such a philosophical crisis&lt;<a href="http://www.intelligence.org/upload/CFAI/design/structure/crisis.html">http://www.intelligence.org/upload/CFAI/design/structure/crisis.html</a>&gt;
</em><br>
<em>&gt; .
</em><br>
<em>&gt;
</em><br>
<em>&gt; 2. Answering this question may make it possible to develop a &quot;last line of
</em><br>
<em>&gt; defense&quot; against an UnFriendly AGI that was accidentally loosed upon the
</em><br>
<em>&gt; world, even if the AGI gained a trans-human level of intelligence. Such a
</em><br>
<em>&gt; &quot;last line of defense&quot; might include trying to convince the AGI that it may
</em><br>
<em>&gt; be inside a simulated environment.
</em><br>
<em>&gt;
</em><br>
<em>&gt; -Rolf
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<p><p><pre>
-- 
Stefan Pernar
3-E-101 Silver Maple Garden
#6 Cai Hong Road, Da Shan Zi
Chao Yang District
100015 Beijing
P.R. CHINA
Mobil: +86 1391 009 1931
Skype: Stefan.Pernar
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16830.html">Алексей Турчин: "Re[2]: Is SETI dangerous?"</a>
<li><strong>Previous message:</strong> <a href="16828.html">Rolf Nelson: "META: Created a blog on how AI's react to the Simulation Argument"</a>
<li><strong>In reply to:</strong> <a href="16828.html">Rolf Nelson: "META: Created a blog on how AI's react to the Simulation Argument"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16831.html">Peter murray: "Re: META: Created a blog on how AI's react to the Simulation Argument"</a>
<li><strong>Reply:</strong> <a href="16831.html">Peter murray: "Re: META: Created a blog on how AI's react to the Simulation Argument"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16829">[ date ]</a>
<a href="index.html#16829">[ thread ]</a>
<a href="subject.html#16829">[ subject ]</a>
<a href="author.html#16829">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:58 MDT
</em></small></p>
</body>
</html>
