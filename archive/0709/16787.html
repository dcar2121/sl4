<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Wall Street Journal on the Singularity Summit</title>
<meta name="Author" content="Tyler Emerson (tyleremerson@gmail.com)">
<meta name="Subject" content="Wall Street Journal on the Singularity Summit">
<meta name="Date" content="2007-09-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Wall Street Journal on the Singularity Summit</h1>
<!-- received="Sat Sep 22 13:15:54 2007" -->
<!-- isoreceived="20070922191554" -->
<!-- sent="Sat, 22 Sep 2007 12:14:25 -0700" -->
<!-- isosent="20070922191425" -->
<!-- name="Tyler Emerson" -->
<!-- email="tyleremerson@gmail.com" -->
<!-- subject="Wall Street Journal on the Singularity Summit" -->
<!-- id="632d2cda0709221214n4f9fc6e1x28aa9102f85042f7@mail.gmail.com" -->
<!-- charset="UTF-8" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tyler Emerson (<a href="mailto:tyleremerson@gmail.com?Subject=Re:%20Wall%20Street%20Journal%20on%20the%20Singularity%20Summit"><em>tyleremerson@gmail.com</em></a>)<br>
<strong>Date:</strong> Sat Sep 22 2007 - 13:14:25 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16788.html">Thomas Buckner: "(Humor) Hugbot gets an upgrade"</a>
<li><strong>Previous message:</strong> <a href="16786.html">Vladimir Nesov: "Re: Strong AI Takeoff Scenarios"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16787">[ date ]</a>
<a href="index.html#16787">[ thread ]</a>
<a href="subject.html#16787">[ subject ]</a>
<a href="author.html#16787">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Feedback appreciated on this.
<br>
<p>-Tyler
<br>
<p>---------- Forwarded message ----------
<br>
From: The Singularity Institute Blog &lt;<a href="mailto:tyleremerson@gmail.com?Subject=Re:%20Wall%20Street%20Journal%20on%20the%20Singularity%20Summit">tyleremerson@gmail.com</a>&gt;
<br>
Date: Sep 22, 2007 7:41 AM
<br>
Subject: The Singularity Institute Blog
<br>
To: <a href="mailto:tyleremerson@gmail.com?Subject=Re:%20Wall%20Street%20Journal%20on%20the%20Singularity%20Summit">tyleremerson@gmail.com</a>
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;The Singularity Institute Blog &lt;<a href="http://www.intelligence.org/blog">http://www.intelligence.org/blog</a>&gt;
<br>
<p>&nbsp;Summit Coverage in The Wall Street Journal raises
<br>
questions&lt;<a href="http://www.intelligence.org/blog/2007/09/21/summit-coverage-in-the-wall-street-journal-raises-questions/">http://www.intelligence.org/blog/2007/09/21/summit-coverage-in-the-wall-street-journal-raises-questions/</a>&gt;
<br>
<p>Posted: 21 Sep 2007 11:55 PM CDT
<br>
<p>Earlier this week SIAI and the Singularity Summit got some major coverage in
<br>
The Wall Street Journal. Lee Gomes, the Portals columnist for The Journal
<br>
attended the Summit, and has some challenging thoughts about our movement
<br>
and its perceived relevancy to the business community and the public at
<br>
large.
<br>
<p>In his article, Gomes likens Singularitarians at times to 12-year-old sci-fi
<br>
addicts, alien worshipers, and even gynephobics (don't tell my 3 daughters).
<br>
While it is always fun to play &quot;knock the nerds&quot; in the popular press, I
<br>
think Gomes raises key issues that point out why we sometimes struggle for
<br>
credibility outside of our safety net in The Valley.
<br>
<p>As we start to organize our thoughts about next year's Singularity Summit,
<br>
it is apparent that we need to focus more on bridging the knowledge and
<br>
perception gaps between the scientific community, the business and
<br>
investment community, and the public at large. Our success in crossing this
<br>
chasm over the next couple of years will dictate how successfully the
<br>
mission of the Singularity Institute will be embraced by broader segments of
<br>
humanity.
<br>
<p>I'd like to open this discussion up to our community at large to get your
<br>
ideas and feedback. How do we stay true to the vision of Singularity
<br>
Institute, and at the same time create a partnership with the business
<br>
community that creates an exciting and positive perspective on what we can
<br>
accomplish? And how do we shake some of the more adverse associations to the
<br>
lunatic fringe?
<br>
<p>I look forward to your thoughts. I've posted Lee's article below. Leave a
<br>
comment to this post or contact me directly at lamis@intelligence.org.
<br>
<p>*Reprinted from The Wall Street Journal*
<br>
<p>—————————————————————————
<br>
<p>The Singular Question Of Human vs. Machine Has a Spiritual Side
<br>
<p>The Wall Street Journal
<br>
PORTALS
<br>
By LEE GOMES
<br>
<p>September 19, 2007; Page B1
<br>
<p>You can tell a lot about people from what they worry about. A few Saturdays
<br>
ago, I spent the day in an auditorium full of fellow citizens concerned with
<br>
&quot;singularity.&quot; The word refers to the day when the intelligence of computers
<br>
will exceed our own.
<br>
<p>The auditorium was filled with people who listed many things that might
<br>
occur with singularity, such as a human-machine synthesis into a new,
<br>
superintelligent life-form. The date has been projected as anytime from nine
<br>
to 40 years hence.
<br>
<p>Singularity-believers say humanity urgently needs to begin preparing for
<br>
this moment, if only to make sure that humans don't become kabobs at the
<br>
first post-singularity tailgate party held by supersmart computers. There is
<br>
even a Singularity Institute, bankrolled by Silicon Valley wealthoids.
<br>
<p>The weekend session featured speeches, panel discussions and informal
<br>
chatting. About 800 people were on hand, more, frankly, than I would have
<br>
expected. Who but 12-year-old sci-fi addicts still fret over malevolent,
<br>
superintelligent machines? Most of us, living every day with computers,
<br>
appreciate how even the world's most powerful one not only is incapable of
<br>
an autonomous thought, it can't even distinguish spam from real email.
<br>
<p>To get to the singularity that we are supposed to be preparing for, we are
<br>
going to need AGI, or Artificial General Intelligence, a topic the
<br>
singularists go on about endlessly.
<br>
<p>A computer with AGI thinks and reasons the same way a human being does, only
<br>
much more quickly. But don't singularity people know that AI researchers
<br>
have been trying to make such machines since the 1950s, without much
<br>
success?
<br>
<p>It turns out, there is a schism between the AGI and the AI worlds. The AGI
<br>
faction thinks AI researchers have sold out, abandoning their early dreams
<br>
of &quot;general&quot; intelligence to concentrate on more attainable (and more
<br>
lucrative) projects.
<br>
<p>They're right. The machines today that recognize speech or play chess are
<br>
one-trick wonders. Of course, AI researchers defend that approach by saying
<br>
their early dreams of general intelligence were naïve.
<br>
<p>The singularists, though, don't seem bothered by those earlier AI failures;
<br>
new approaches will bear fruit, they insist. They thus didn't think it a
<br>
waste of either time or carbon offsets to be gathering at a conference to
<br>
ask such questions as, &quot;If you made a superintelligent robot, then forced it
<br>
to work only for you, would that be slavery?&quot;
<br>
<p>Robots are just computers with moving parts, of course, but the public is
<br>
still confused about them, just like they used to be about computers
<br>
themselves. The Great Metallic Hope of the robotics industry, for example,
<br>
is currently a small, round vacuum cleaner that ambles across the floor by
<br>
itself.
<br>
<p>A high-tech wonder? Actually, Consumer Reports said that even cheap vacuum
<br>
cleaners did better than the first model. A little more of this, and no one
<br>
will ever again worry about enslaving robots.
<br>
<p>There is another way of thinking about the obsession with robots. John
<br>
Huntington, professor of English, University of Illinois, has studied the
<br>
genre and says sci-fi authors, especially the early ones who wrote about
<br>
robots or aliens, were working out their own unacknowledged anxieties about
<br>
closer-to-home topics.
<br>
<p>Most commonly, he said, these anxieties involved women, who were seen as
<br>
becoming threatening as they gained social power. Racial and class tensions
<br>
also were involved, he added.
<br>
<p>I have a supplemental theory: that the discussion of singularity involves a
<br>
sublimated spiritual yearning for some form of eternal life and an
<br>
all-powerful being, but one articulated by way of technical, secular
<br>
discourse.
<br>
<p>As it happens, there is considerable overlap between the singularity and the
<br>
&quot;life extension&quot; communities. Ray Kurzweil, the best-known singularity
<br>
writer, also co-wrote a lengthy guide to life extension. He once told me he
<br>
expects literally to live forever — first by prolonging his life via a daily
<br>
regimen that includes hundreds of pills and the nonstop consumption of green
<br>
tea, then, once super-powerful computers arrive, by uploading his
<br>
consciousness into one.
<br>
<p>Singularists also have an affinity for the Search for Extraterrestrial
<br>
Intelligence, or SETI, program, which scans the skies looking for other
<br>
civilizations. Isn't that a longing by some for an intergalactic messiah?
<br>
<p>Then, consider a poem read at the singularity conference that described an
<br>
Aquarian Age scene in which humans and other mammals frolicked in a
<br>
&quot;cybernetic meadow … all watched over by machines of loving grace.&quot; Those
<br>
computer protectors sound a lot like the guardian angels my grade-school
<br>
nuns told us about.
<br>
<p>Years ago, a friend and I spent an evening with Arthur C. Clarke, the
<br>
creator in &quot;2001″ of HAL, the malevolent computer of every singularist's
<br>
nightmare. He brought along slides, showing himself with some astronauts and
<br>
with the authors of the musical &quot;Hair.&quot;
<br>
<p>We talked about science and had our picture taken, which I still have. It
<br>
proves that while I may have reached a different conclusion, at least I
<br>
studied with the master.
<br>
<p>&nbsp;&nbsp;&nbsp;You are subscribed to email updates from The Singularity Institute
<br>
Blog&lt;<a href="http://www.intelligence.org/blog">http://www.intelligence.org/blog</a>&gt;
<br>
To stop receiving these emails, you may unsubscribe
<br>
now&lt;<a href="http://www.feedburner.com/fb/a/emailunsub?id=1830945&amp;key=IPDZtRQeUq">http://www.feedburner.com/fb/a/emailunsub?id=1830945&amp;key=IPDZtRQeUq</a><br>
. Email Delivery powered by FeedBurner  Inbox too full? [image:
<br>
(feed)]&lt;<a href="http://feeds.feedburner.com/siaiblog">http://feeds.feedburner.com/siaiblog</a>&gt;
<br>
Subscribe &lt;<a href="http://feeds.feedburner.com/siaiblog">http://feeds.feedburner.com/siaiblog</a>&gt; to the feed version of The
<br>
Singularity Institute Blog in a feed reader.  If you prefer to unsubscribe
<br>
via postal mail, write to: The Singularity Institute Blog, c/o FeedBurner,
<br>
20 W Kinzie, 9th Floor, Chicago IL USA 60610
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16788.html">Thomas Buckner: "(Humor) Hugbot gets an upgrade"</a>
<li><strong>Previous message:</strong> <a href="16786.html">Vladimir Nesov: "Re: Strong AI Takeoff Scenarios"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16787">[ date ]</a>
<a href="index.html#16787">[ thread ]</a>
<a href="subject.html#16787">[ subject ]</a>
<a href="author.html#16787">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:58 MDT
</em></small></p>
</body>
</html>
