<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Strong AI Takeoff Scenarios</title>
<meta name="Author" content="Rolf Nelson (rolf.h.d.nelson@gmail.com)">
<meta name="Subject" content="Strong AI Takeoff Scenarios">
<meta name="Date" content="2007-09-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Strong AI Takeoff Scenarios</h1>
<!-- received="Sun Sep 16 14:40:49 2007" -->
<!-- isoreceived="20070916204049" -->
<!-- sent="Sun, 16 Sep 2007 16:38:14 -0400" -->
<!-- isosent="20070916203814" -->
<!-- name="Rolf Nelson" -->
<!-- email="rolf.h.d.nelson@gmail.com" -->
<!-- subject="Strong AI Takeoff Scenarios" -->
<!-- id="79ecaa350709161338r278b9082k6123b09eb7a4c21d@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Rolf Nelson (<a href="mailto:rolf.h.d.nelson@gmail.com?Subject=Re:%20Strong%20AI%20Takeoff%20Scenarios"><em>rolf.h.d.nelson@gmail.com</em></a>)<br>
<strong>Date:</strong> Sun Sep 16 2007 - 14:38:14 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16763.html">Алексей Турчин: "Re: Strong AI Takeoff Scenarios"</a>
<li><strong>Previous message:</strong> <a href="16761.html">Damien Broderick: "Re: Transcension (in fiction) before Vinge's Singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16763.html">Алексей Турчин: "Re: Strong AI Takeoff Scenarios"</a>
<li><strong>Reply:</strong> <a href="16763.html">Алексей Турчин: "Re: Strong AI Takeoff Scenarios"</a>
<li><strong>Reply:</strong> <a href="16765.html">Matt Mahoney: "Re: Strong AI Takeoff Scenarios"</a>
<li><strong>Reply:</strong> <a href="16768.html">Daniel Burfoot: "Re: Strong AI Takeoff Scenarios"</a>
<li><strong>Maybe reply:</strong> <a href="16786.html">Vladimir Nesov: "Re: Strong AI Takeoff Scenarios"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16762">[ date ]</a>
<a href="index.html#16762">[ thread ]</a>
<a href="subject.html#16762">[ subject ]</a>
<a href="author.html#16762">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Here's a list I wrote up of AI takeoff and non-takeoff scenarios,
<br>
including the recent botnets scenario. Any additions? The ones marked
<br>
&quot;Weak scenario&quot; I'm thinking of dropping as I consider them much more
<br>
unlikely than the other scenarios.
<br>
<p>-Rolf
<br>
<p>Strong AI Takeoff Scenarios
<br>
<p>Things that could cause the current Moore's Law curve to get blown away:
<br>
<p>1. Enormous nanotechnology advances
<br>
<p>2. Enormous biotech advances, allowing dramatic intelligence
<br>
augmentation to human brains
<br>
<p>3. Enormous quantum computing advances. (Weak scenario)
<br>
<p>Things that could cause a sudden, large (1000x) increase in hardware
<br>
devoted to AI self-improvement:
<br>
<p>3. Self-improving AI, nowhere near *general* human-level intelligence
<br>
yet, is suddenly put in charge of a botnet and swallows most of the
<br>
Internet.
<br>
<p>4. Self-improving AI, nowhere near human-level intelligence yet,
<br>
figures out how to beat the stock market, on the order of a trillion
<br>
dollars. Its owner re-invests much of his massive winnings in more
<br>
hardware. (Weak scenario)
<br>
<p>5. Most of the skeptics become convinced; Strong AI (and, hopefully,
<br>
Friendliness) research dramatically increases, maybe even resulting in
<br>
an Apollo Project-level program.
<br>
<p>Other scenarios:
<br>
<p>6. The usual &quot;recursive self-improvement&quot; scenario
<br>
<p>7. We were too conservative. Turns out creating a Strong AI on a
<br>
mouse-level brain is easier than we thought; we just never had a
<br>
mouse-level brain to experiment with before.
<br>
<p><p>Things that could prevent Strong AI, or delay it by many decades
<br>
<p>1. Turns out creating hardware sufficiently powerful to compete with a
<br>
human brain is harder than we thought. Note the &quot;maybe glial cells
<br>
play as much of a role as the neurons&quot; argument, if true, only delays
<br>
us a few years. But, perhaps there is something dramatically wrong
<br>
about our current understanding of how much computing power a single
<br>
neuron has. (Weak scenario)
<br>
<p>2. Giant disaster or war, say on a scale that kills off more than 20%
<br>
of humanity in a single year.
<br>
<p>3. Moore's Law hits a brick wall, as do biotech and nanotechnology,
<br>
but all for different reasons. (Weak scenario)
<br>
<p>4. Getting the software right turns out to be much much harder than we
<br>
thought. Sure, natural selection got it right on Earth, but it has an
<br>
infinite universe to play with, and on the 10^10000 other planets
<br>
where it failed, no one is around to ask questions.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16763.html">Алексей Турчин: "Re: Strong AI Takeoff Scenarios"</a>
<li><strong>Previous message:</strong> <a href="16761.html">Damien Broderick: "Re: Transcension (in fiction) before Vinge's Singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16763.html">Алексей Турчин: "Re: Strong AI Takeoff Scenarios"</a>
<li><strong>Reply:</strong> <a href="16763.html">Алексей Турчин: "Re: Strong AI Takeoff Scenarios"</a>
<li><strong>Reply:</strong> <a href="16765.html">Matt Mahoney: "Re: Strong AI Takeoff Scenarios"</a>
<li><strong>Reply:</strong> <a href="16768.html">Daniel Burfoot: "Re: Strong AI Takeoff Scenarios"</a>
<li><strong>Maybe reply:</strong> <a href="16786.html">Vladimir Nesov: "Re: Strong AI Takeoff Scenarios"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16762">[ date ]</a>
<a href="index.html#16762">[ thread ]</a>
<a href="subject.html#16762">[ subject ]</a>
<a href="author.html#16762">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:58 MDT
</em></small></p>
</body>
</html>
