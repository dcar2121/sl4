<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Obedient AI?</title>
<meta name="Author" content="Peter C. McCluskey (pcm@rahul.net)">
<meta name="Subject" content="Obedient AI?">
<meta name="Date" content="2004-11-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Obedient AI?</h1>
<!-- received="Thu Nov 25 10:15:40 2004" -->
<!-- isoreceived="20041125171540" -->
<!-- sent="Thu, 25 Nov 2004 09:15:37 -0800 (PST)" -->
<!-- isosent="20041125171537" -->
<!-- name="Peter C. McCluskey" -->
<!-- email="pcm@rahul.net" -->
<!-- subject="Obedient AI?" -->
<!-- id="20041125171537.E003CBE8C0@green.rahul.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Peter C. McCluskey (<a href="mailto:pcm@rahul.net?Subject=Re:%20Obedient%20AI?"><em>pcm@rahul.net</em></a>)<br>
<strong>Date:</strong> Thu Nov 25 2004 - 10:15:37 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10357.html">Jef Allbright: "Re: Reductionism"</a>
<li><strong>Previous message:</strong> <a href="10355.html">Timothy Jennings: "Have you all seen this? (http://scholar.google.com)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10358.html">Keith Henson: "One way to get more trustworthy humans for AI work"</a>
<li><strong>Reply:</strong> <a href="10358.html">Keith Henson: "One way to get more trustworthy humans for AI work"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10356">[ date ]</a>
<a href="index.html#10356">[ thread ]</a>
<a href="subject.html#10356">[ subject ]</a>
<a href="author.html#10356">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;I'd like to suggest an alternative to Friendly AI / CV that involves an
<br>
AI programmed to answer questions, combined with a human institution that
<br>
is designed to control what questions the AI may be asked.
<br>
&nbsp;The AI would be designed with a supergoal of answering any questions put
<br>
to it as accurately as it can within a set of resource constraints that
<br>
is given to it for each question. The AI would not try to discern the
<br>
intent behind a question beyond the extent to the intent gets encoded
<br>
in the question.
<br>
&nbsp;The human institution would serve to limit the questions and resource
<br>
limits to those that are safe. For example, it would ensure that the AI
<br>
doesn't attempt to turn the planet into computronium by verifying that
<br>
whoever asks the question pays for all the resources used in calculating
<br>
the answer. 
<br>
<p>&nbsp;What could go wrong?
<br>
<p>&nbsp;Concepts such as &quot;resource constraints&quot; and &quot;accurately&quot; could be programmed
<br>
into the AI in a buggy way. But we appear to understand these concepts better
<br>
than we understand concepts such as friendliness, so I claim the risks are
<br>
lower. Also, the consequences of a bug might be less than the consequences
<br>
of a bug in code for friendliness, volition, etc.
<br>
&nbsp;The human institution could misuse its power for undesirable goals such as
<br>
world conquest. But that is a normal kind of problem we have faced with a
<br>
number of prior technological changes, and we have a lot of experience at
<br>
surviving this.
<br>
&nbsp;The AI could be inadequate to deal with an unfriendly AI. If we see signs
<br>
that this will be a problem (we will be able to get some predictions from
<br>
the obedient AI that will help determine this), we should be able to use
<br>
the obedient AI to help us design another type of AI, such as a friendly AI,
<br>
with the power necessary to deal with unfriendly AI. I suspect this two-step
<br>
process of producing the friendly AI would be a good deal less error-prone
<br>
than trying to directly design the final AI, because at the hardest stage we
<br>
would have an AI to help us. This conclusion does depend somewhat on
<br>
assumptions about how fast a friendly AI would take off. I envision the
<br>
obedient AI being improved by its developers repeatedly asking it what
<br>
changes should be made to it in order to increase the accuracy of the AI's
<br>
answers or the range of questions it can answer, and then applying those
<br>
changes. If the first AIs take months or years to go from an IQ of 50 to
<br>
an IQ of 500, then the slowdown caused by having humans in the loop need
<br>
not be important. There are apparently some people on this list who expect
<br>
this takeoff to happen in less than a day. I've been ignoring those strange
<br>
ideas because they seemed to have little relevance to how we should act,
<br>
but if I read some serious arguments that there's a significant chance of
<br>
a fast takeoff and that that is the deciding issue on which to choose
<br>
between obedient AI and CV, I will argue against the fast takeoff.
<br>
&nbsp;It could be hard to program a supergoal so that it contains two unrelated
<br>
attributes such as accuracy and resource constraints. But Pei Wang's NARS
<br>
seems to demonstrate a system which appears to come close enough to doing
<br>
this that I doubt it is a serious problem.
<br>
&nbsp;
<br>
&nbsp;Have I missed anything?
<br>
<pre>
-- 
------------------------------------------------------------------------------
Peter McCluskey          | Please check out my new blog:
www.bayesianinvestor.com | <a href="http://www.bayesianinvestor.com/blog/">http://www.bayesianinvestor.com/blog/</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10357.html">Jef Allbright: "Re: Reductionism"</a>
<li><strong>Previous message:</strong> <a href="10355.html">Timothy Jennings: "Have you all seen this? (http://scholar.google.com)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10358.html">Keith Henson: "One way to get more trustworthy humans for AI work"</a>
<li><strong>Reply:</strong> <a href="10358.html">Keith Henson: "One way to get more trustworthy humans for AI work"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10356">[ date ]</a>
<a href="index.html#10356">[ thread ]</a>
<a href="subject.html#10356">[ subject ]</a>
<a href="author.html#10356">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
