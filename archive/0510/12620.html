<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Self-awareness (AI debate at San Jose State U.)</title>
<meta name="Author" content="Woody Long (ironanchorpress@earthlink.net)">
<meta name="Subject" content="Re: Self-awareness (AI debate at San Jose State U.)">
<meta name="Date" content="2005-10-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Self-awareness (AI debate at San Jose State U.)</h1>
<!-- received="Tue Oct 25 23:56:42 2005" -->
<!-- isoreceived="20051026055642" -->
<!-- sent="Wed, 26 Oct 2005 01:56:37 -0400" -->
<!-- isosent="20051026055637" -->
<!-- name="Woody Long" -->
<!-- email="ironanchorpress@earthlink.net" -->
<!-- subject="Re: Self-awareness (AI debate at San Jose State U.)" -->
<!-- id="410-220051032655637367@earthlink.net" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="Self-awareness (AI debate at San Jose State U.)" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Woody Long (<a href="mailto:ironanchorpress@earthlink.net?Subject=Re:%20Self-awareness%20(AI%20debate%20at%20San%20Jose%20State%20U.)"><em>ironanchorpress@earthlink.net</em></a>)<br>
<strong>Date:</strong> Tue Oct 25 2005 - 23:56:37 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12621.html">Ben Goertzel: "RE: Designing AGI"</a>
<li><strong>Previous message:</strong> <a href="12619.html">Michael Wilson: "RE: Designing AGI"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12620">[ date ]</a>
<a href="index.html#12620">[ thread ]</a>
<a href="subject.html#12620">[ subject ]</a>
<a href="author.html#12620">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; [Original Message]
</em><br>
<em>&gt; From: Olie Lamb &lt;<a href="mailto:olie@vaporate.com?Subject=Re:%20Self-awareness%20(AI%20debate%20at%20San%20Jose%20State%20U.)">olie@vaporate.com</a>&gt;
</em><br>
&nbsp;
<br>
<em>&gt; &gt;I should tell you where my interest comes from in all this. My business
</em><br>
<em>&gt; &gt;project is to enter an android in the historic Roboprize.com Prize Fight.
</em><br>
<em>&gt; &gt;To be entered the Rules Committee must certify that the entry is an
</em><br>
<em>&gt; &gt;authentic android. They have asked me what parameters I would use, and I
</em><br>
<em>&gt; &gt;said it must be driven by an SAI system and pass the Searle Chinese room
</em><br>
<em>&gt; &gt;test. If not, it should not be considered a consciously self aware,
</em><br>
<em>&gt; &gt;thinking, autonomous android, and be turned down.
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; Huh???
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Pass the chinese room test&quot;?
</em><br>
<em>&gt;
</em><br>
<em>&gt; The Chinese Room example is a illustration of how the Turing Test does 
</em><br>
<em>&gt; not imply Strong AI. Searle shows that an entity can react to language 
</em><br>
<em>&gt; in a way that makes it appear as though it understands language, without 
</em><br>
<em>&gt; actually understanding language.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt; I don't know how the Chinese room example could be turned around to 
</em><br>
<em>&gt; demonstrate that an AI is Strong.
</em><br>
-----------------------------------
<br>
&nbsp;
<br>
Yes, I knew this would elicit a strong reaction, as it is my most radical
<br>
of proposals. 
<br>
&nbsp;
<br>
First, we are in agreement that, as you said, &quot;Searle shows that an entity
<br>
can react to language in a way that makes it appear as though it
<br>
understands language, without actually understanding language.&quot; Exactly, it
<br>
is just a heuristic-driven (rules of thumb) shuffling of symbol cards or in
<br>
robotics, sensor signal cards.It has no conscious understanding of what it
<br>
is doing, which is the same thing as saying it doesn't understand language. 
<br>
&nbsp;
<br>
Now the key to the Searle Test is in the &quot;entity&quot; we are talking about. He
<br>
is talking about contemporary computers and their contemporay programming
<br>
methods. These are essentially heuristic-driven systems, that shuffle
<br>
cards, and include the rule crunching inference engines and all narrow AI
<br>
systems.
<br>
&nbsp;
<br>
<em>&gt;From 1999 interview -
</em><br>
Searle -- In English I am a human being who understands English; in Chinese
<br>
I'm just a computer. Computers, therefore -- and this really is the
<br>
decisive point -- just in virtue of implementing a program, the computer is
<br>
not guaranteed understanding. It might have understanding for some other
<br>
reason but just going through the steps of the formal program is not
<br>
sufficient for the mind. ... the computer does a model or a simulation of a
<br>
process. And a computer simulation of a mind is about like computer
<br>
simulation of digestion. It's a model, it's a picture of digestion
<br>
[language understanding]. It shows you the formal structure of how it
<br>
works, it doesn't actually digest anything [understand language]! 
<br>
&nbsp;
<br>
Interviewer -- And so the computer program, then, has not explained
<br>
consciousness. 
<br>
&nbsp;
<br>
Searle -- That's right. Nowhere near. 
<br>
<a href="http://globetrotter.berkeley.edu/people/Searle/searle-con4.html">http://globetrotter.berkeley.edu/people/Searle/searle-con4.html</a>
<br>
&nbsp;
<br>
&nbsp;
<br>
Note that he doesn't say categorically that it is impossible. But he does
<br>
draw a strict, black or white line between contemporary computer
<br>
methodology, which after all is not designed to produce models of human
<br>
intelligent minds, and post-contemporary computer methodology, what is
<br>
coming to be called strong AI (SAI) computer methodologies, which might, at
<br>
least in theory, consciously understand language.
<br>
&nbsp;
<br>
Thus we get the proposed Searle Chinese Room Test -
<br>
I. If a computer system in the room can be shown to be consciously
<br>
understanding the input language, it will be designated artificial
<br>
consciousness. 
<br>
II. If not, it is not artificial consciousness.
<br>
&nbsp;
<br>
In robotics, an android is the ultimate creation. It is human artificial
<br>
life. It is Star Trek's Data. As such a completely human-like creation, it
<br>
requires a fully human intelligent, humanoid SAI system. And human-like
<br>
consciousness requires a self, self-awareness, and motivational autonomy,
<br>
so to show artificial consciousness is to show humanoid SAI.
<br>
&nbsp;
<br>
Thus in order for an android to be a consciously self-aware, motivationally
<br>
autonomous entity, it will have to be driven by SAI, and it will have to
<br>
pass the Searle Test.
<br>
&nbsp;
<br>
Ken Woody Long
<br>
www.artificial-lifeforms-lab.blogspot.com
<br>
&nbsp;
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12621.html">Ben Goertzel: "RE: Designing AGI"</a>
<li><strong>Previous message:</strong> <a href="12619.html">Michael Wilson: "RE: Designing AGI"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12620">[ date ]</a>
<a href="index.html#12620">[ thread ]</a>
<a href="subject.html#12620">[ subject ]</a>
<a href="author.html#12620">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
