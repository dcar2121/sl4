<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI debate at San Jose State U.</title>
<meta name="Author" content="Woody Long (ironanchorpress@earthlink.net)">
<meta name="Subject" content="Re: AI debate at San Jose State U.">
<meta name="Date" content="2005-10-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI debate at San Jose State U.</h1>
<!-- received="Fri Oct 21 13:39:31 2005" -->
<!-- isoreceived="20051021193931" -->
<!-- sent="Fri, 21 Oct 2005 15:39:23 -0400" -->
<!-- isosent="20051021193923" -->
<!-- name="Woody Long" -->
<!-- email="ironanchorpress@earthlink.net" -->
<!-- subject="Re: AI debate at San Jose State U." -->
<!-- id="410-2200510521193923265@earthlink.net" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="AI debate at San Jose State U." -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Woody Long (<a href="mailto:ironanchorpress@earthlink.net?Subject=Re:%20AI%20debate%20at%20San%20Jose%20State%20U."><em>ironanchorpress@earthlink.net</em></a>)<br>
<strong>Date:</strong> Fri Oct 21 2005 - 13:39:23 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12367.html">Herb Martin: "RE: DARPA winning car 'taught' to drive, same as humans are."</a>
<li><strong>Previous message:</strong> <a href="12365.html">Richard Loosemore: "Re: AI debate at San Jose State U."</a>
<li><strong>Maybe in reply to:</strong> <a href="12299.html">mike99: "AI debate at San Jose State U."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12383.html">Olie Lamb: "Re: AI debate at San Jose State U."</a>
<li><strong>Reply:</strong> <a href="12383.html">Olie Lamb: "Re: AI debate at San Jose State U."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12366">[ date ]</a>
<a href="index.html#12366">[ thread ]</a>
<a href="subject.html#12366">[ subject ]</a>
<a href="author.html#12366">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; [Original Message]
</em><br>
<em>&gt; From: Richard Loosemore &lt;<a href="mailto:rpwl@lightlink.com?Subject=Re:%20AI%20debate%20at%20San%20Jose%20State%20U.">rpwl@lightlink.com</a>&gt;
</em><br>
<em>&gt; To: &lt;<a href="mailto:sl4@sl4.org?Subject=Re:%20AI%20debate%20at%20San%20Jose%20State%20U.">sl4@sl4.org</a>&gt;
</em><br>
<em>&gt; Date: 10/21/2005 2:30:37 PM
</em><br>
<em>&gt; Subject: Re: AI debate at San Jose State U.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Is it really the case that to be humanoid, an intelligence must get
</em><br>
<em>&gt; pissed off at losing, feel selfishness, etc.?
</em><br>
<em>&gt;
</em><br>
<em>&gt; The answer is a resounding NO!  This is one of those cases where we all
</em><br>
<em>&gt; need to take psychology more seriously:  from the psych point of view,
</em><br>
<em>&gt; the motivational/emotional system is somewhat orthogonal to the
</em><br>
<em>&gt; cognitive part .... which means that you could have the same
</em><br>
<em>&gt; intelligence, and yet be free to design it with all sorts of different
</em><br>
<em>&gt; choices for the motivational/emotional apparatus.
</em><br>
<em>&gt;
</em><br>
<em>&gt; To be sure, if you wanted to make a thinking system that was *very*
</em><br>
<em>&gt; human-like you would have to put in all the same mot/emot mechanisms
</em><br>
<em>&gt; that we have.  But when you think about it a bit more, you find yourself
</em><br>
<em>&gt; asking *whose* mot/emot system you are going to emulate?  Hannibal
</em><br>
<em>&gt; Lecter's?  Mahatma Ghandi's?  There is an enormous variation just among
</em><br>
<em>&gt; individual instances of human beings.  Arguably you can have people who
</em><br>
<em>&gt; are utterly placid, selfless and who have never felt a violent emotion
</em><br>
<em>&gt; in their lives.  And others with a violence system that is just
</em><br>
<em>&gt; downright missing (not just controlled and suppressed, but not there).
</em><br>
<p><p>I agree with this theory of SAI relativity. It is true for both humanoid
<br>
&quot;fully human intelligent&quot; SAI, and goal based, human-equivalent SAI, as
<br>
differentiated in a prior post. These goals also can be toxic, or missing,
<br>
which leads to unfriendly, toxic results. 
<br>
<p><p><em>&gt; The idea of &quot;self-interest&quot; is, I agree, slightly more subtle.  Self
</em><br>
<em>&gt; interest might not be just an ad-hoc motivational drive like the others,
</em><br>
<em>&gt; it might be THE basic drive, without which the system would just sit
</em><br>
<em>&gt; there and vegetate. 
</em><br>
<p><p>This reminds of Sony's current research called the Playground Experiment,
<br>
where they are testing their &quot; Intelligent Adaptive Curiosity Engine.&quot; This
<br>
engine of self-interest sounds very much like the basic drive of the robot.
<br>
<a href="http://playground.csl.sony.fr/en/page3.xml">http://playground.csl.sony.fr/en/page3.xml</a>
<br>
<p><p><em>&gt; I will now jump forward and say what I believe are the main conclusions
</em><br>
<em>&gt; that we would come to if we did analyse the issues in more depth:  we
</em><br>
<em>&gt; would conclude that *if* we try to build a roughly humanoid AGI *but* we
</em><br>
<em>&gt; give it a mot/emot system of the right sort (basically, empathic towards
</em><br>
<em>&gt; other creatures), we will discover that its Friendliness will be far,
</em><br>
<em>&gt; far more guaranteeable than if we dismiss the humanoid design as bad and
</em><br>
<em>&gt; try to build some kind of &quot;normative&quot; AI system.
</em><br>
<em>&gt;
</em><br>
<em>&gt; After all, we agree that Friendliness is important, right?  So should we
</em><br>
<em>&gt; not pursue the avenue I have suggested, if there is a possibility that
</em><br>
<em>&gt; we would arrive at the spectacular, but counterintuitive, conclusion
</em><br>
<em>&gt; that giving an AGI the right sort of motivational system would be the
</em><br>
<em>&gt; best possible guarantee of getting a Friendly system?
</em><br>
<p><p>The Japanese robot makers are clearly in the business of making fully human
<br>
intelligent humanoid robots. The purpose section of Sony robot patents make
<br>
this clear. They state that their purpose is to make as human-like a robot
<br>
as they can, to increase its entertainment value. So the field of humanoid
<br>
SAI research and development is a well-funded and undeniable reality. And
<br>
they believe that using the principle of harmony they can make them
<br>
absolutely friendly, even though they are self-aware
<br>
and driven by human-like self interest.
<br>
<p>Example of a current, friendly intelligent system exhibiting self
<br>
awareness, self interest, and self destruction --
<br>
&nbsp;
<br>
1. Self destruction - the Sony Aibo dog is able to execute a biting
<br>
behavior, which with sharp teeth or large force could harm people or their
<br>
pets. However Sony built it so that when it meets a certain amount of light
<br>
resistance, it self-destroys this behavior; and its jaw goes slack. If a
<br>
hobbiest trys to get inside and tinker with this behavior, it self destroys
<br>
the system, and is rendered inoperable. It implements what they call in
<br>
Japan the principle of harmony. It also could be considered an
<br>
implementation of Asimov's First Law of Robotics.
<br>
&nbsp;
<br>
2. Self awareness and self interest - The Sony Aibo dog has tactile sensors
<br>
and it can tell when it is being &quot;stroked&quot; or &quot;hit&quot;. If its being stroked,
<br>
&quot;it&quot; - the dog self - forms a positive self interest or affection for the
<br>
person stroking it, and develops a self interest in being near it so that
<br>
when it sees the person, based on ITS so formed self interest alone, it
<br>
goes up to the person. Coversely when it is &quot;hit&quot; it forms a negative self
<br>
interest or aversion for the person, and develops a self interest in
<br>
avoiding him, and so based on this self interest alone, it moves away from
<br>
the person.
<br>
<p>In the same way, future humanoid SAI will be safe-built and friendly. So,
<br>
as a singulatarian, I support such friendly humanoid SAI, which I believe
<br>
will someday evolve into a super intelligent humanoid technological
<br>
singularity. My guess is Sony in 10 to 15  years will complete the creation
<br>
of their authentic, friendly humanoid SAI.
<br>
<p>Ken Woody Long
<br>
artificial-lifeforms-lab.blogspot.com
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12367.html">Herb Martin: "RE: DARPA winning car 'taught' to drive, same as humans are."</a>
<li><strong>Previous message:</strong> <a href="12365.html">Richard Loosemore: "Re: AI debate at San Jose State U."</a>
<li><strong>Maybe in reply to:</strong> <a href="12299.html">mike99: "AI debate at San Jose State U."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12383.html">Olie Lamb: "Re: AI debate at San Jose State U."</a>
<li><strong>Reply:</strong> <a href="12383.html">Olie Lamb: "Re: AI debate at San Jose State U."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12366">[ date ]</a>
<a href="index.html#12366">[ thread ]</a>
<a href="subject.html#12366">[ subject ]</a>
<a href="author.html#12366">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:16 MST
</em></small></p>
</body>
</html>
