<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI debate at San Jose State U.</title>
<meta name="Author" content="Olie Lamb (olie@vaporate.com)">
<meta name="Subject" content="Re: AI debate at San Jose State U.">
<meta name="Date" content="2005-10-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI debate at San Jose State U.</h1>
<!-- received="Sun Oct 23 22:40:45 2005" -->
<!-- isoreceived="20051024044045" -->
<!-- sent="Mon, 24 Oct 2005 14:40:35 +1000" -->
<!-- isosent="20051024044035" -->
<!-- name="Olie Lamb" -->
<!-- email="olie@vaporate.com" -->
<!-- subject="Re: AI debate at San Jose State U." -->
<!-- id="435C65C3.5090506@vaporate.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="410-2200510521193923265@earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Olie Lamb (<a href="mailto:olie@vaporate.com?Subject=Re:%20AI%20debate%20at%20San%20Jose%20State%20U."><em>olie@vaporate.com</em></a>)<br>
<strong>Date:</strong> Sun Oct 23 2005 - 22:40:35 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12384.html">Herb Martin: "RE: Mizar conversion project..."</a>
<li><strong>Previous message:</strong> <a href="12382.html">Ben Goertzel: "Mizar conversion project..."</a>
<li><strong>In reply to:</strong> <a href="12366.html">Woody Long: "Re: AI debate at San Jose State U."</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12383">[ date ]</a>
<a href="index.html#12383">[ thread ]</a>
<a href="subject.html#12383">[ subject ]</a>
<a href="author.html#12383">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Woody Long wrote:
<br>
<p><em>&gt;Example of a current, friendly intelligent system exhibiting self
</em><br>
<em>&gt;awareness, self interest, and self destruction --
</em><br>
<em>&gt; 
</em><br>
<em>&gt;The Sony Aibo dog
</em><br>
<em>&gt;
</em><br>
Dude, you have a VERY different interpretation of the term &quot;self-aware&quot; 
<br>
going on there. 
<br>
<p>I can deal with people using the term &quot;intelligent&quot; to cover 
<br>
non-conscious entities, even though I find this a dumbing-down of the term.
<br>
<p>But self-aware???
<br>
<p>Surely, a basic requirement of an entity being &quot;self-aware&quot; is that has 
<br>
Consciousness.  I know the c-word makes a few people round here squirm a 
<br>
little, but sometimes you have to tackle the hard problems.  (Hee)
<br>
<p>Reaction to entities is not awareness of the entities.  Bugs react to 
<br>
damage to their bodies (~ their selves), but, as far as anyone knows, 
<br>
they don't operate on the basis of &quot;My body is in some way different 
<br>
from stuff that is not my body&quot;.  They don't have concepts, let alone 
<br>
concepts of the self. 
<br>
<p><em>&gt;However Sony built it so that when it meets a certain amount of light
</em><br>
<em>&gt;resistance, it self-destroys this behavior; and its jaw goes slack. If a
</em><br>
<em>&gt;hobbiest trys to get inside and tinker with this behavior, it self destroys
</em><br>
<em>&gt;the system, and is rendered inoperable. It implements what they call in
</em><br>
<em>&gt;Japan the principle of harmony. It also could be considered an
</em><br>
<em>&gt;implementation of Asimov's First Law of Robotics. 
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
If there is any tinkering, the entire system destroys itself, or just 
<br>
certain types of tinkering result in certain failures?
<br>
<p>Either way, the comparison to Asimov belies the flaw in the 
<br>
methodology:  Rather than structuring the system for safety with a 
<br>
ground-up approach, you are describing a system with a safety catch 
<br>
&quot;plastered on&quot;.
<br>
<p><em>&gt;2. Self awareness and self interest - The Sony Aibo dog has tactile sensors
</em><br>
<em>&gt;and it can tell when it is being &quot;stroked&quot; or &quot;hit&quot;. If its being stroked,
</em><br>
<em>&gt;&quot;it&quot; - the dog self - forms a positive self interest or affection for the
</em><br>
<em>&gt;person stroking it, and develops a self interest in being near it so that
</em><br>
<em>&gt;when it sees the person, based on ITS so formed self interest alone, it
</em><br>
<em>&gt;goes up to the person. Coversely when it is &quot;hit&quot; it forms a negative self
</em><br>
<em>&gt;interest or aversion for the person, and develops a self interest in
</em><br>
<em>&gt;avoiding him, and so based on this self interest alone, it moves away from
</em><br>
<em>&gt;the person.
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
Fantastic!  We're already designing robots to mimic behaviours of 
<br>
disliking people.  Maybe the next generation of AIBO will be able to hit 
<br>
back.  I'm sure we can add some safety system, though, to make sure that 
<br>
any maul-behaviours are non fatal.
<br>
<p>See'sly, tho, forming profiles of people could be a very constructive 
<br>
approach.  People have preferences, and it's good for a device to be 
<br>
able to match those preferences. 
<br>
<p><p><em>&gt;In the same way, future humanoid SAI will be safe-built and friendly.
</em><br>
<em>&gt;
</em><br>
I have no objection to attempting to design humanoid intelligences.  I 
<br>
think that, as Loosemore said,  there is no reason why an AGI can't be 
<br>
humanoid - in the broader sense - and without self-interest.  
<br>
Furthermore, some limited forms of self-interest are not necessarily 
<br>
un-Friendly, but there are better motivational systems.
<br>
<p>Likewise, it is quite likely that the kind of design that a 
<br>
consumer-products group would create would be &quot;friendly&quot; in the broad 
<br>
sense.  However, I think the Guidelines make it pretty clear that the 
<br>
way they use Friendly is much more specific.
<br>
<p><em>&gt; So,
</em><br>
<em>&gt;as a singulatarian, I support such friendly humanoid SAI, which I believe
</em><br>
<em>&gt;will someday evolve into a super intelligent humanoid technological
</em><br>
<em>&gt;singularity. My guess is Sony in 10 to 15  years will complete the creation
</em><br>
<em>&gt;of their authentic, friendly humanoid SAI.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Ken Woody Long
</em><br>
<em>&gt;artificial-lifeforms-lab.blogspot.com
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<p>Being an advocate of the singularity doesn't require a belief that: &quot;AGI 
<br>
will necessarily be Good, so all research in the area is Good&quot;
<br>
<p>I think that MNT is fantastic stuff.  But I was strict controls on 
<br>
research, and would rather have no MNT than allow undisciplined 
<br>
investigations into replicators.
<br>
<p>I think that genetic research into diseases /will/ help humanity 
<br>
immensely.  But I really don't like the idea of ebola-sequences being 
<br>
posted on the net.
<br>
<p>Likewise, I believe that AGI research has a non-trivial probability of 
<br>
getting practical benefit, so much as to be the &quot;best thing for 
<br>
Humanity, ever&quot;  within a decade*.  But I still think that having any 
<br>
old group doing the research is ridiculously dangerous.  If I cound have 
<br>
$100M spent by the a military organisation on AGI research, or have it 
<br>
spent on making 1-inch lenghths of string, I'd probably opt for the 
<br>
stringettes.
<br>
<p>Being a singularity-advocate doesn't mean having blind faith.
<br>
<p>-- Olie
<br>
<p>*I'd estimate that the probability of a bloody-sharp-AI (which would 
<br>
create a technological singularity in the broader sense) being created 
<br>
in the next decade is around 8%ish.  I feel this is fairly optimistic.  
<br>
A 10% chance per decade gives, what, a 65% chance within a century or 
<br>
so.  I think our chances in the decade after this one are significantly 
<br>
improved - maybe 12 to 15%?  If one considers the chance of achieving a 
<br>
bloody-sharp-AI as high as 30% per decade, this would make the 
<br>
probability of not getting a BSAI within a century at a piddling 3%.  
<br>
I'm sorry, I find that kinda unrealistic.
<br>
<p>Note that my one-off term use here of bloody-sharp doesn't /have/ to be 
<br>
conscious.  I just think that creating consciousness will help to make 
<br>
an AI cluey, and thereby intelligent.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12384.html">Herb Martin: "RE: Mizar conversion project..."</a>
<li><strong>Previous message:</strong> <a href="12382.html">Ben Goertzel: "Mizar conversion project..."</a>
<li><strong>In reply to:</strong> <a href="12366.html">Woody Long: "Re: AI debate at San Jose State U."</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12383">[ date ]</a>
<a href="index.html#12383">[ thread ]</a>
<a href="subject.html#12383">[ subject ]</a>
<a href="author.html#12383">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:18 MST
</em></small></p>
</body>
</html>
