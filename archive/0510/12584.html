<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: AGI motivations</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="AGI motivations">
<meta name="Date" content="2005-10-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>AGI motivations</h1>
<!-- received="Mon Oct 24 12:14:03 2005" -->
<!-- isoreceived="20051024181403" -->
<!-- sent="Mon, 24 Oct 2005 19:13:58 +0100 (BST)" -->
<!-- isosent="20051024181358" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="AGI motivations" -->
<!-- id="20051024181358.60701.qmail@web26703.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20AGI%20motivations"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Mon Oct 24 2005 - 12:13:58 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12585.html">Pope Salmon the Lesser Mungojelly: "cyborgs &amp; ghosts     (was Re: AGI motivations (Sidetrack on Uploading))"</a>
<li><strong>Previous message:</strong> <a href="12583.html">Michael Wilson: "Re: Loosemore's Proposal"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12570.html">Woody Long: "Re: AGI motivations"</a>
<li><strong>Maybe reply:</strong> <a href="12570.html">Woody Long: "Re: AGI motivations"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12584">[ date ]</a>
<a href="index.html#12584">[ thread ]</a>
<a href="subject.html#12584">[ subject ]</a>
<a href="author.html#12584">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Sorry about the delay, slight technical problem with the SL4 server's
<br>
post filtering.
<br>
<p>Michael Vassar wrote:
<br>
<em>&gt;&gt; Yes, uploads occupy a small region of cognitive architecture space
</em><br>
<em>&gt;&gt; within a larger region of 'human-like AGI designs'... We cannot hit
</em><br>
<em>&gt;&gt; the larger safer region reliably by creating an AGI from scratch.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; That assertion appears plausible but un substantiated to me. The 
</em><br>
<em>&gt; understanding of human brain function required to build a relatively
</em><br>
<em>&gt; safe human-like AGI might be only trivially greater than that
</em><br>
<em>&gt; required to create an upload,
</em><br>
<p>In the beginning, all things seemed possible, though few things seemed
<br>
certain. If you don't take a strong position on how the human brain
<br>
is organised, what the minimal functional components on an AGI are,
<br>
the basic principles of self-modifying goal systems etc etc then the
<br>
answer to all of these questions is 'maybe' rather than 'yes' or 'no'.
<br>
However you can't actually /do/ anything with this level of knowledge,
<br>
and if you simply pick something that looks desirable and start
<br>
working on a way to implement it you may well learn things that
<br>
invalidate the original desirability or even plausibility assessment.
<br>
This is what happened to the SIAI; Eliezer started researching general
<br>
AI, in working on that discovered that actually you needed to build an
<br>
FAI, and then in working on /that/ discovered that you can't do so in
<br>
an ad-hoc fashion.
<br>
<p>You keep saying 'maybe we can do this, maybe we can do that'. I'm
<br>
using the knowledge of the domain I've acquired so far to predict
<br>
(with a reasonable degree of confidence) that some of these things are
<br>
practically impossible and others are a just a really bad idea. I'll
<br>
keep trying, but to be honest I don't think I'm going to have a lot
<br>
of success explaining my reasoning; there's just too much critical,
<br>
non-intuitive cognitive content in the model to squeeze into a few
<br>
emails when the reader doesn't share the background and premises. My
<br>
actual arguments are almost beside the point though; the real point
<br>
is that you can't reasonably say what the best course of action is
<br>
until you've seriously tried to solve the problem. Without a detailed
<br>
model of the problem space and some candidate solutions, it's simply
<br>
not possible to conclude anything useful.
<br>
<p><em>&gt; It may be much simpler to make a mind that will reliably not attempt
</em><br>
<em>&gt; to transcend than to build one that can transcend safely.
</em><br>
<p>With no other constraints, I'd agree. But as you pointed out, the more
<br>
you try and make an AGI /useful/, particularly to FAI projects, the
<br>
harder this gets.
<br>
<p><em>&gt; One way to make such a mind is to upload the right person.
</em><br>
<p>Which I'd still be nervous about, since we have no idea how a human
<br>
mind would react to the situation and if you get it wrong you've
<br>
given that human a good shot at imposing their will on everyone else.
<br>
Uploading is a better idea than trying to build 'human-like' AIs,
<br>
but brain-computer interfacing is /probably/ a safer option still. I
<br>
don't know enough to say which is harder; uploading is more of an
<br>
engineering challenge while BCI is more of a software challenge.
<br>
<p><em>&gt; It may be that building a large number of moderately different
</em><br>
<em>&gt; neuromorphic AIs (possibly based on medium res scans of particular
</em><br>
<em>&gt; brains, scans inadequate for uploading, followed by repair via
</em><br>
<em>&gt; software clean-up) in AI boxes and testing them under a limited
</em><br>
<em>&gt; range of conditions similar to what they will actually face in the
</em><br>
<em>&gt; world is easier than uploading a particular person.
</em><br>
<p>Obviously that has moral implications, possibly mitigated by the fact
<br>
that you can restore the saved states after you've built the FAI, but
<br>
that's not a reason to reject it. The usual problems with AI boxes
<br>
apply; the risk may be reduced by the 'neuromorphic' architecture,
<br>
but if Eliezer can break out of a box (albeit simulated) it seems
<br>
unwise to bet on even a 'human-level' AGI staying in a box. My primary
<br>
objection to this though is that it seems like a massive waste of time
<br>
and effort; you could spend decades and billions of dollars on this
<br>
for a pretty minimal advantage in solving the real FAI problem.
<br>
<p><em>&gt; We know some ways for reliably hitting them, such as &quot;don't
</em><br>
<em>&gt; implement transhuman capabilities&quot;.
</em><br>
<p>To me this statement seems roughly equivalent to 'please implement
<br>
a complete clone of the Unix operating system, but make sure that
<br>
dictators won't be able to run nuclear weapon simulations on it'.
<br>
You're trying to implement a very high-level constraint directly
<br>
in code. Either you'll build token adversarial mechanisms that will
<br>
trivially be bypassed with a few days of hacking effort, or you're
<br>
crippling the system so badly it will be useless for benign
<br>
purposes as well. The normal solution would be 'make sure that the
<br>
AGI doesn't /want/ to acquire transhuman capabilities', but unless
<br>
you use an upload (in which case you're trusting a human thrust
<br>
into a completely novel psychological situation, and that's if you
<br>
get everything right) your 'neuromorphic' specification rules out
<br>
being able to impose strong (in the formal sense) constraints on
<br>
the goal system.
<br>
<p><em>&gt; Another may be to build an AGI that is such a person. How do you
</em><br>
<em>&gt; know what they want?  Ask them.  Detecting that a simulation is
</em><br>
<em>&gt; lying with high reliability, and detecting its emotions, should
</em><br>
<em>&gt; not be difficult.
</em><br>
<p>Ok, I can't see a way to build a 'neuromorphic AGI' where you
<br>
can reliably detect lying. /Humans/ seem to be able to learn to
<br>
fool lie detectors pretty well (I don't know if this extends to
<br>
fMRI scanning, but it wouldn't surprise me, as the best
<br>
techniques amount to deliberately laying down false memories).
<br>
If you disagree, then specify the details of your neuromorphic
<br>
AGI architecture and specify how your lie detector works.
<br>
&nbsp;
<br>
<em>&gt; &gt;Indeed, if someone did manage this, my existing model of AI
</em><br>
<em>&gt; &gt;development would be shown to be seriously broken.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I suspect that it may be. Since you haven't shared your model I
</em><br>
<em>&gt; have no way to evaluate it, but a priori, given that most models
</em><br>
<em>&gt; of AI development, ncluding most models held by certified geniuses,
</em><br>
<em>&gt; are broken, I assume yours is too.
</em><br>
<p>That's a fair assumption if you're talking about my best guess on
<br>
how to build an AGI; as I often say myself the prior for anyone
<br>
getting it right is minuscule. However that wasn't what I was
<br>
referring to; I meant my predictive model of /AGI projects/, in
<br>
the sense of understanding past failures and working out what
<br>
pitfalls current projects are most likely to get stuck in. This
<br>
is what I'm using when I say that attempts to closely mimic a
<br>
human brain which are nonetheless too impatient to develop full
<br>
uploading will at best fail and at worst create a UFAI.
<br>
<p><em>&gt; I'd be happy to work with you on improving it, and that seems
</em><br>
<em>&gt; to me to be the sort of thing this site is for,
</em><br>
<p>As you may be aware, the Singularity Institute dropped the 'open
<br>
source' model of AGI development in mid-2000, and much as I'm in
<br>
favour of collaboration (and wish the SL4 list could discuss AGI
<br>
design more) I must regretfully agree with the reasoning behind
<br>
this decision.
<br>
<p><em>&gt; but destiny-star may be more urgent.
</em><br>
<p>If you're referring to my former start-up company, it ceased
<br>
operations in January 2004, though I kept it around to support
<br>
my full-time AI research. Six months back I started a new
<br>
company 'Bitphase AI Ltd' (who doesn't have one these days? :) )
<br>
as a commercialisation vehicle for my current work.
<br>
<p><em>&gt; It's best to predict well enough to create, then stop predicting
</em><br>
<em>&gt; and create. Trouble is, it's hard to know when your predictions
</em><br>
<em>&gt; are good enough.
</em><br>
<p>True. In this domain the human tendency to proceed on the basis
<br>
of inadequate prediction is a serious existential threat, which is
<br>
why Eliezer should be excused for any apparent excess of zeal in
<br>
insisting on having the very best FAI theory we can construct
<br>
before proceeding (subject to the fact that we're rapidly running
<br>
out of time).
<br>
&nbsp;
<br>
<em>&gt;&gt;&gt; There should be ways of confidently predicting that a given machine
</em><br>
<em>&gt;&gt;&gt; does not have any transhuman capabilities other than a small set of
</em><br>
<em>&gt;&gt;&gt; specified ones which are not sufficient for transhuman persuasion or
</em><br>
<em>&gt;&gt;&gt; transhuman engineering.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Why 'should' there be an easy way to do this? In my experience predicting
</em><br>
<em>&gt;&gt; what capabilities a usefully general design will actually have is pretty
</em><br>
<em>&gt;&gt; hard, whether you're trying to prove positives or negatives.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; We do it all the time with actual humans.
</em><br>
<p>Erm, humans can't have transhuman capabilities /by definition/, so I'm
<br>
not sure what your point is. We've certainly already gone over the
<br>
foolishness of generalising from humans to a de novo AI with a design
<br>
that looks kinda like how you think the brain might work.
<br>
<p><em>&gt; For a chunk of AI design space larger than &quot;uploads&quot; AGIs are just
</em><br>
<em>&gt; humans.
</em><br>
<p>Again, this relies on the team doing a massive and probably infeasible
<br>
amount of work to create a perfectly accurate simulation, and if they
<br>
did what would be the point? We already have plenty of humans.
<br>
<p><em>&gt; Whatever advantages they have will only be those you have given them,
</em><br>
<em>&gt; probably including speed and  moderately fine-grained self-awareness
</em><br>
<em>&gt; (or you having moderately fine-grained awareness of them).
</em><br>
<p>/You cannot predict the high-level consequences of giving people new
<br>
low-level capabilities/. It's impossible to fully predict this with a
<br>
transparent design, never mind an opaque 'neuromorphic' one. This is
<br>
the same kind of problem as trying to predict the social consequences
<br>
of a radical new technology, but ten times harder (at least) because
<br>
your built in human-modelling intuitions have gone from making helpful
<br>
suggestions to telling you subtle lies.
<br>
<p><em>&gt; Predicting approximately what new capabilities a human will have
</em><br>
<em>&gt; when you make a small change to their neurological hardware can be
</em><br>
<em>&gt; difficult or easy depending on how well you understand what you are
</em><br>
<em>&gt; doing,
</em><br>
<p>This is another of those statements that sounds fine in isolation.
<br>
The key qualifier determining the practicality of the idea is how
<br>
difficult it is for humans to acquire the 'understanding of what
<br>
they are doing' (and from a Singularity strategy point of view how
<br>
likely it is that they will do so before actually building the AGI;
<br>
project deadlines have a way of trampling on abstract concerns).
<br>
What understanding I have of human brain simulation suggests that
<br>
acquiring the desired level of understanding will be really, really
<br>
hard, and probably intractable without a lot of sophisticated
<br>
tools that might need infrahuman AGI themselves. You can't gloss
<br>
over comprehension challenges like this the way you can gloss over
<br>
engineering challenges by saying 'well, the laws of physics permit
<br>
it...'.
<br>
<p><em>&gt; but small changes, that is, changes of magnitude comparable to the
</em><br>
<em>&gt; range of variation among the human baseline population will never
</em><br>
<em>&gt; create large and novel transhuman abilities, but lots
</em><br>
<p>This only holds if you vary the simulation parameters along dimensions
<br>
that mimic human genetic variability. There are lots of ways to
<br>
slightly alter a brain simulator that are very hard or even literally
<br>
impossible for natural selection to replicate, most of which will
<br>
probably produce very-hard-to-predict results and any of which could
<br>
produce transhuman abilities (possibly coupled with motivation drift)
<br>
that take us back to an Unfriendly seed AI situation. This is one of
<br>
the few areas where the 'Complex Systems' crowd actually have a
<br>
point, and that point is that it's just not safe to play with this
<br>
stuff.
<br>
<p><em>&gt;&gt;&gt; It should also be possible to ensure a human-like enough goal system
</em><br>
<em>&gt;&gt;&gt; that you can understand its motivation prior to recursive
</em><br>
<em>&gt;&gt;&gt; self-improvement.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Where is this 'should' coming from?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The &quot;should&quot; comes from the fact that the whole venture of attempting to 
</em><br>
<em>&gt; build an organization to create a Friendly AI presupposed the solution to 
</em><br>
<em>&gt; the problem of friendly human assistant has been solved
</em><br>
<p>This statement is simply wrong. It is perfectly possible to produce a
<br>
solution to the FAI specifying an arbitrary non-human-like seed AI
<br>
architecture without ever having the ability to specify a 'friendly
<br>
neuromorphic AI'; the latter requires the solution of numerous hard
<br>
challenges that aren't required for the former. This is the strategy
<br>
that the SIAI is taking, for all the reasons stated above and more.
<br>
<p>Aside from that, the fact that we want to solve the problem and are
<br>
trying hard to do so does not mean that it 'should' be solvable. The
<br>
universes is not required to accommodate our notions of desirability.
<br>
<p><em>&gt; What is the difference between trusting a human derived AI and
</em><br>
<em>&gt; trusting a human. Either can be understood equally well by reading
</em><br>
<em>&gt; what they say, talking to them, etc.
</em><br>
<p>Humans deceive humans on a massive scale every day, sometimes hiding
<br>
the most malign intentions and actions for decades, and that's with
<br>
our innate ability to model other (human) minds working at peak
<br>
effectiveness, something which drops off sharply as the cognitive
<br>
architecture of the target begins to deviate from our own. We have
<br>
a hard enough time predicting how humans from other cultures will
<br>
behave, never mind minds with a different physical substrate,
<br>
sensorium and cognitive architecture (even if their motivational
<br>
mechanisms and broad capabilities are human-like). Regardless, I for
<br>
one would like a solution more reliable than the baseline human
<br>
ability to detect deceit.
<br>
<p><em>&gt;&gt; Trying to prevent an AGI from self-modifying is the classic 'adversarial
</em><br>
<em>&gt;&gt; swamp' situation that CFAI correctly characterises as hopeless.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You shouldn't try to prevent a non-human AGI from self-modifying, but 
</em><br>
<em>&gt; adversarial swamps involving humans are NOT intractible, not to mention
</em><br>
<em>&gt; that you can still influence the human's preferences a LOT with simulate 
</em><br>
<em>&gt; chemistry and total control of their environment.
</em><br>
<p>I rather doubt that messing with the uploaded human's simulated brain
<br>
chemistry mind is going to engender trust; in fact it seems more likely
<br>
to me that you'd introduce subtle psychosis (though the actual answer
<br>
to this requires a detailed model of human personality fragility under
<br>
fine brain modification, which we don't have and won't have without a
<br>
lot of research).
<br>
<p><em>&gt; I don't think this is as severe as the adversarial situation currently
</em><br>
<em>&gt; existing between SIAI and other AI development teams from whom SIAI
</em><br>
<em>&gt; withholds information.
</em><br>
<p>Firstly, you've jumped into an entirely different argument. Information
<br>
sharing between teams of human developers has little or nothing to do
<br>
with a programmer trying to frustrate an AGI's attempts to self-modify.
<br>
Secondly, the majority of AGI projects are withholding key information.
<br>
Many have published a lot less information than the SIAI has. I don't
<br>
see how you can reasonably criticise the SIAI for this without
<br>
criticising virtually every AGI researcher on this mailing list.
<br>
<p><em>&gt;&gt; Any single point of failure in your technical isolation or human
</em><br>
<em>&gt;&gt; factors will probably lead to seed AI. The task is more or less
</em><br>
<em>&gt;&gt; impossible even given perfect understanding, and perfect
</em><br>
<em>&gt;&gt; understanding is pretty unlikely to be present.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Huh?  That's like saying that workplace drug policies are fundamentally 
</em><br>
<em>&gt; impossible even given an unlimited selection of workers, perfect worker 
</em><br>
<em>&gt; monitoring, and drugs of unknown effect some of which end the world when
</em><br>
<em>&gt; one person takes them.
</em><br>
<p>My point is that we can't (or at least, probably won't) have 'perfect
<br>
worker monitoring', nor do we have a free selection of workers given
<br>
the effort needed to make an upload or new AI and the need for them
<br>
to be useful. The actual situation would be much more like /real
<br>
world/ workplace drug policy enforcement, but still with your criteria
<br>
that some drugs end the world when only one person takes them.
<br>
<p><em>&gt;&gt; A 'non-transparent' i.e. 'opaque' AGI is one that you /can't/ see
</em><br>
<em>&gt;&gt; the thoughts of, only at best high level and fakeable abstractions.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You *can* see the non-transparent thoughts of a human to a
</em><br>
<em>&gt; significant degree, especially with neural scanning. With perfect
</em><br>
<em>&gt; scanning you could do much better.
</em><br>
<p>Ok, right now the best we can do is classify the rough kind of mental
<br>
activity and possible under some circumstances the kinds of thing a
<br>
person might be thinking about. We have neither repeatability, detail
<br>
nor context, and this is with subjects who are actively trying to
<br>
help rather than actively try to deceive. I admit that the degree to
<br>
which this can improve given better scanning alone is not well
<br>
established. I am simply inclined to believe various domain experts
<br>
who say that it is /really hard/ and note that this tallies with my
<br>
expectations about trying to data-mine extremely complex, noisy and
<br>
distributed causal networks without a decent model of the high-level
<br>
processing implemented.
<br>
<p><em>&gt; I strongly disagree. Reading thoughts is very difficult. Reading
</em><br>
<em>&gt; emotions is not nearly so difficult,
</em><br>
<p>Firstly, this assumes perfect uploading with no small yet significant
<br>
factors that throw off our human-derived models. Secondly, we have
<br>
no experience trying to do this under adversarial conditions, and
<br>
it's unreliable even with co-operative subjects. Thirdly normal
<br>
humans can already be pretty damn good at manipulating their own
<br>
emotions. Finally there's no reason to assume that an Unfriendly
<br>
neuromorphic AGI is going to get highly emotional about it; this
<br>
list has already seen quite a bit of argument for the notion that
<br>
unemotionally selfish sociopathic humans are widespread in society.
<br>
<p><em>&gt;&gt; I'll grant you that, but how is it going to be useful for FAI
</em><br>
<em>&gt;&gt; design if it doesn't know about these things?
</em><br>
<em>&gt;
</em><br>
<em>&gt; I can think of several ways.  Admittedly the use would be more
</em><br>
<em>&gt; limited.
</em><br>
<p>I'd certainly be interested to hear them, given that I'm currently
<br>
researching the area of 'tools to help with FAI design' myself.
<br>
<p><em>&gt;&gt; How do you propose to stop anyone from teaching a 'commercially
</em><br>
<em>&gt;&gt; available human-like AGI' these skills?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Probably not possible at that point, but at that point you have
</em><br>
<em>&gt; to really really hurry anyway so some safety needs may be
</em><br>
<em>&gt; sacrificed.  Actually, probably possible but at immense cost,
</em><br>
<em>&gt; and probably not worth preparing for.
</em><br>
<p>See previous discussion on why government regulation of AI (a)
<br>
isn't going to happen before it's too late and (b) would be worse
<br>
than useless even if it did.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
Yahoo! Messenger - NEW crystal clear PC to PC calling worldwide with voicemail <a href="http://uk.messenger.yahoo.com">http://uk.messenger.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12585.html">Pope Salmon the Lesser Mungojelly: "cyborgs &amp; ghosts     (was Re: AGI motivations (Sidetrack on Uploading))"</a>
<li><strong>Previous message:</strong> <a href="12583.html">Michael Wilson: "Re: Loosemore's Proposal"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12570.html">Woody Long: "Re: AGI motivations"</a>
<li><strong>Maybe reply:</strong> <a href="12570.html">Woody Long: "Re: AGI motivations"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12584">[ date ]</a>
<a href="index.html#12584">[ thread ]</a>
<a href="subject.html#12584">[ subject ]</a>
<a href="author.html#12584">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
