<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AGI motivations</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="Re: AGI motivations">
<meta name="Date" content="2005-10-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AGI motivations</h1>
<!-- received="Sun Oct 23 13:37:01 2005" -->
<!-- isoreceived="20051023193701" -->
<!-- sent="Sun, 23 Oct 2005 20:36:38 +0100 (BST)" -->
<!-- isosent="20051023193638" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="Re: AGI motivations" -->
<!-- id="20051023193638.78809.qmail@web26706.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="BAY101-F164CE87C36E17A41E88636AC740@phx.gbl" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20AGI%20motivations"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Sun Oct 23 2005 - 13:36:38 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12570.html">Woody Long: "Re: AGI motivations"</a>
<li><strong>Previous message:</strong> <a href="12568.html">Michael Vassar: "Re: AGI motivations"</a>
<li><strong>In reply to:</strong> <a href="12568.html">Michael Vassar: "Re: AGI motivations"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12572.html">Michael Vassar: "Re: AGI motivations"</a>
<li><strong>Reply:</strong> <a href="12572.html">Michael Vassar: "Re: AGI motivations"</a>
<li><strong>Reply:</strong> <a href="12574.html">Michael Vassar: "Re: AGI motivations"</a>
<li><strong>Reply:</strong> <a href="12580.html">Richard Loosemore: "Re: AGI motivations (Sidetrack on Uploading)"</a>
<li><strong>Reply:</strong> <a href="12581.html">Richard Loosemore: "Re: AGI motivations (Sidetrack on Neuromorphic Engineering)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12569">[ date ]</a>
<a href="index.html#12569">[ thread ]</a>
<a href="subject.html#12569">[ subject ]</a>
<a href="author.html#12569">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michael Vassar wrote:
<br>
<em>&gt; Trying to build a human derived AI is the general case of which
</em><br>
<em>&gt; building a human upload is a special case... I'm skeptical about the
</em><br>
<em>&gt; diferences being unavoidabld. Surely true given current computers,
</em><br>
<em>&gt; surely false given uploads. There is a substantial space of possible
</em><br>
<em>&gt; AGI design around &quot;uploads&quot;.
</em><br>
<p>This is true in principle, but not in practice. Yes, uploads occupy
<br>
a small region of cognitive architecture space within a larger region
<br>
of 'human-like AGI designs'. However we can actually hit the narrower
<br>
region semi-reliably if we can develop an accurate brain simulation
<br>
and copy an actual human's brain structure into it. We cannot hit the
<br>
larger region reliably by creating an AGI from scratch, at least not
<br>
without an understanding of how the human brain works and how to
<br>
build AGIs considerably in advance of what is needed for uploading
<br>
and FAI respectively.
<br>
<p><em>&gt; Surely there are regions within the broader category which are
</em><br>
<em>&gt; safer than the particular region containing human uploads.
</em><br>
<p>Almost certainly. We don't know where they are or how to reliably
<br>
hit them with an implementation, and unlike say uploading or FAI
<br>
there is no obvious research path to gaining this capability. If
<br>
you know how to reliably build a safe, human-like AGI, feel free
<br>
to say how.
<br>
<p><em>&gt; but if someone else were to develop human-derived AIs and put them
</em><br>
<em>&gt; on the market, for instance, it would be foolish not to use them
</em><br>
<em>&gt; to assist your work, which would at that point be incredibly urgent
</em><br>
<em>&gt; and require the use of all possible tools that could be used to
</em><br>
<em>&gt; accelerate it.
</em><br>
<p>I agree, I just don't think anyone is going to do it, and if they
<br>
did do it someone would turn one into an Unfriendly seed AI within
<br>
days. Indeed, if someone did manage this, my existing model of AI
<br>
development would be shown to be seriously broken.
<br>
&nbsp;
<br>
<em>&gt;&gt; To work out from first principles (i.e. reliably) whether
</em><br>
<em>&gt;&gt; you should trust a somewhat-human-equivalent AGI you'd need nearly as
</em><br>
<em>&gt;&gt; much theory, if not more, than you'd need to just build an FAI in the
</em><br>
<em>&gt;&gt; first place.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; That depends on what you know, and on what its cognitive capacities are.  
</em><br>
<em>&gt; There should be ways of confidently predicting that a given machine does
</em><br>
<em>&gt; not have any transhuman capabilities other than a small set of specified
</em><br>
<em>&gt; ones which are not sufficient for transhuman persuasion or transhuman 
</em><br>
<em>&gt; engineering.
</em><br>
<p>Why 'should' there be an easy way to do this? In my experience predicting
<br>
what capabilities a usefully general design will actually have is pretty
<br>
hard, whether you're trying to prove positives or negatives.
<br>
<p><em>&gt; It should also be possible to ensure a human-like enough goal system
</em><br>
<em>&gt; that you can understand its motivation prior to recursive
</em><br>
<em>&gt; self-improvement.
</em><br>
<p>Where is this 'should' coming from? If you know how to do this, tell
<br>
the rest of us and claim your renown as the researcher who cracked a
<br>
good fraction of the FAI problem.
<br>
&nbsp;
<br>
<em>&gt;&gt; If you have the technology to do the latter, you might as well just
</em><br>
<em>&gt;&gt; upload people; it's less risky than trying to build a human-like AGI
</em><br>
<em>&gt;&gt; (though probably more risky than building an FAI).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I assume you mean the former, not the latter.
</em><br>
<p>Yes, sorry.
<br>
<p><em>&gt; Uploading a particular person might be (probably is) more difficult
</em><br>
<em>&gt; than the more general task of producing an (not necessarily perfectly)
</em><br>
<em>&gt; accurate simulation of a generic human brain which predictably displays
</em><br>
<em>&gt; no transhuman capabilities and only human capabilities that are rather
</em><br>
<em>&gt; easily observed (especially with the partial transparency that comes
</em><br>
<em>&gt; from being an AI).
</em><br>
<p>Unless you're simulating the brain at the neuron level (including keeping
<br>
the propagation speed down to human levels) /and/ closely copying human
<br>
brain organisation, you simply can't generalise from 'humans behave like
<br>
this' to 'the AGI will behave like this'. Given this constraint on
<br>
structure, your options for getting bootstrap /content/ into the AGI are
<br>
(a) upload a human, (b) replicate the biological brain growth and human
<br>
learning process (even more research, implementation complexity and
<br>
technical difficulty and takes a lot of time) or (c) use other algorithms
<br>
unrelated to the way humans work to generate the seed complexity. The
<br>
last option again discards any ability to make simple generalisations
<br>
from human behaviour to the behaviour of your human-like AGI. The second
<br>
option introduces even more potential for things to go wrong (due to more
<br>
design complexity) and even if it works perfectly it will produce an
<br>
arbitrary (and probably pretty damn strange) human-like personality, with
<br>
no special guarantees of benevolence. Actually I find it highly unlikely
<br>
that any research group trying to do this would actually go all out on
<br>
replicating the brain accurately without being tempted to meddle and
<br>
'improve' the structure and/or content, but I digress. Thus the first
<br>
option, uploading, looks like the best option to me if you're going to
<br>
insist on building a human-like AGI.
<br>
<p><em>&gt; &gt;Yes, noting of course that it's extremely difficult to build a
</em><br>
<em>&gt; &gt;'human-like AI' that isn't already an Unfriendly seed AI.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I strongly disagree with the above, except in so far as it's extremely 
</em><br>
<em>&gt; difficult to build any &quot;human-like AI&quot;. An AI that doesn't have
</em><br>
<em>&gt; access to the computer it is running on is just a person.
</em><br>
<p>Trying to prevent an AGI from self-modifying is the classic 'adversarial
<br>
swamp' situation that CFAI correctly characterises as hopeless. Any
<br>
single point of failure in your technical isolation or human factors
<br>
(i.e. AGI convinces a programmer to do something that allows it to
<br>
write arbitrary code) will probably lead to seed AI. The task is more
<br>
or less impossible even given perfect understanding, and perfect
<br>
understanding is pretty unlikely to be present.
<br>
<p><em>&gt; An AI which others can see the non-transparent thoughts of, and which
</em><br>
<em>&gt; others can manipulate the  preferences and desires of, is in some
</em><br>
<em>&gt; respects safer than a human, so long as those who control it are safe.
</em><br>
<p>This looks like a contradiction in terms. A 'non-transparent' i.e.
<br>
'opaque' AGI is one that you /can't/ see the thoughts of, only at best
<br>
high level and fakeable abstractions. The problem of understanding what
<br>
a (realistic) 'human-like AGI' is thinking is equivalent to the problem
<br>
of understanding what a human is thinking given a complete real-time
<br>
brain scan; a challenge considerably harder than merely simulating the
<br>
brain.
<br>
<p><em>&gt; Finally, any AI that doesn't know it is an AI or doesn't know about
</em><br>
<em>&gt; programming, formal logic, neurology, etc is safe until it learns
</em><br>
<em>&gt; those things.
</em><br>
<p>I'll grant you that, but how is it going to be useful for FAI design
<br>
if it doesn't know about these things? How do you propose to stop
<br>
anyone from teaching a 'commercially available human-like AGI' these
<br>
skills?
<br>
<p><em>&gt; A small fraction of them would try to mess with their minds and
</em><br>
<em>&gt; break.
</em><br>
<p>Not necessarily a problem given indefinite fine-grained backup-restore,
<br>
assuming you're not bothered by the moral implications.
<br>
<p><em>&gt; However, it is unfortunately the case that AIs that could be seed AIs
</em><br>
<em>&gt; themselves are much more valuable to an AGI program than AIs which 
</em><br>
<em>&gt; could not be seed AIs themselves.
</em><br>
<p>Exactly.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
To help you stay safe and secure online, we've developed the all new Yahoo! Security Centre. <a href="http://uk.security.yahoo.com">http://uk.security.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12570.html">Woody Long: "Re: AGI motivations"</a>
<li><strong>Previous message:</strong> <a href="12568.html">Michael Vassar: "Re: AGI motivations"</a>
<li><strong>In reply to:</strong> <a href="12568.html">Michael Vassar: "Re: AGI motivations"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12572.html">Michael Vassar: "Re: AGI motivations"</a>
<li><strong>Reply:</strong> <a href="12572.html">Michael Vassar: "Re: AGI motivations"</a>
<li><strong>Reply:</strong> <a href="12574.html">Michael Vassar: "Re: AGI motivations"</a>
<li><strong>Reply:</strong> <a href="12580.html">Richard Loosemore: "Re: AGI motivations (Sidetrack on Uploading)"</a>
<li><strong>Reply:</strong> <a href="12581.html">Richard Loosemore: "Re: AGI motivations (Sidetrack on Neuromorphic Engineering)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12569">[ date ]</a>
<a href="index.html#12569">[ thread ]</a>
<a href="subject.html#12569">[ subject ]</a>
<a href="author.html#12569">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
