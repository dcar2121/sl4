<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AGI motivations</title>
<meta name="Author" content="Michael Vassar (michaelvassar@hotmail.com)">
<meta name="Subject" content="Re: AGI motivations">
<meta name="Date" content="2005-10-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AGI motivations</h1>
<!-- received="Sun Oct 23 17:34:00 2005" -->
<!-- isoreceived="20051023233400" -->
<!-- sent="Sun, 23 Oct 2005 19:33:40 -0400" -->
<!-- isosent="20051023233340" -->
<!-- name="Michael Vassar" -->
<!-- email="michaelvassar@hotmail.com" -->
<!-- subject="Re: AGI motivations" -->
<!-- id="BAY101-F111023CB72768838E52081AC740@phx.gbl" -->
<!-- inreplyto="20051023193638.78809.qmail@web26706.mail.ukl.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Vassar (<a href="mailto:michaelvassar@hotmail.com?Subject=Re:%20AGI%20motivations"><em>michaelvassar@hotmail.com</em></a>)<br>
<strong>Date:</strong> Sun Oct 23 2005 - 17:33:40 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12382.html">Ben Goertzel: "Mizar conversion project..."</a>
<li><strong>Previous message:</strong> <a href="12380.html">Michael Vassar: "RE: people breaking themselves (was &quot;Re: AGI motivations&quot;)"</a>
<li><strong>In reply to:</strong> <a href="12376.html">Michael Wilson: "Re: AGI motivations"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12387.html">Richard Loosemore: "Re: AGI motivations (Sidetrack on Uploading)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12381">[ date ]</a>
<a href="index.html#12381">[ thread ]</a>
<a href="subject.html#12381">[ subject ]</a>
<a href="author.html#12381">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Sorry, I left off the first half of this e-mail.
<br>
<p>Michael Wilson said
<br>
<em>&gt;Yes, uploads occupy a small region of cognitive architecture space within a 
</em><br>
<em>&gt;larger region
</em><br>
<em>&gt;of 'human-like AGI designs'. However we can actually hit the narrower
</em><br>
<em>&gt;region semi-reliably if we can develop an accurate brain simulation
</em><br>
<em>&gt;and copy an actual human's brain structure into it. We cannot hit the
</em><br>
<em>&gt;larger safer region reliably by creating an AGI from scratch, at least not
</em><br>
<em>&gt;without an understanding of how the human brain works and how to
</em><br>
<em>&gt;build AGIs considerably in advance of what is needed for uploading
</em><br>
<em>&gt;and FAI respectively.
</em><br>
<p>That assertion appears plausible but unsubstantiated to me.  The 
<br>
understanding of human brain function required to build a relatively safe 
<br>
human-like AGI might be only trivially greater than that required to create 
<br>
an upload, while the scanning resolution required might be much lower.  It 
<br>
may be much simpler to make a mind that will reliably not attempt to 
<br>
transcend than to build one that can transcend safely.  One way to make such 
<br>
a mind is to upload the right person.  It may be that building a large 
<br>
number of moderately different neuromorphic AIs (possibly based on medium 
<br>
res scans of particular brains, scans inadequate for uploading, followed by 
<br>
repair via software clean-up) in AI boxes and testing them under a limited 
<br>
range of conditions similar to what they will actually face in the world is 
<br>
easier than uploading a particular person.
<br>
<p><em>&gt; &gt; Surely there are regions within the broader category which are
</em><br>
<em>&gt; &gt; safer than the particular region containing human uploads.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Almost certainly. We don't know where they are or how to reliably
</em><br>
<em>&gt;hit them with an implementation, and unlike say uploading or FAI
</em><br>
<em>&gt;there is no obvious research path to gaining this capability.
</em><br>
<p>We know some ways for reliably hitting them, such as &quot;don't implement 
<br>
transhuman capabilities&quot;.
<br>
<p><em>&gt;you know how to reliably build a safe, human-like AGI, feel free
</em><br>
<em>&gt;to say how.
</em><br>
<p>Upload an ordinary person with no desire to become a god.  That's one way.  
<br>
Another may be to build an AGI that is such a person.  How do you know what 
<br>
they want?  Ask them.  Detecting that a simulation is lying with high 
<br>
reliability, and detecting its emotions, should not be difficult.
<br>
<p><em>&gt;Indeed, if someone did manage this, my existing model of AI
</em><br>
<em>&gt;development would be shown to be seriously broken.
</em><br>
<p>I suspect that it may be.  Since you haven't shared your model I have no way 
<br>
to evaluate it, but a priori, given that most models of AI development 
<br>
,ncluding most models held by certified geniuses,are broken, I assume yours 
<br>
is too.  I'd be happy to work with you on improving it, and that seems to me 
<br>
to be the sort of thing this site is for, but destiny-star may be more 
<br>
urgent.
<br>
It's best to predict well enough to create, then stop predicting and create. 
<br>
&nbsp;&nbsp;Trouble is, it's hard to know when your predictions are good enough.
<br>
<p><em>&gt; &gt;&gt; To work out from first principles (i.e. reliably) whether
</em><br>
<em>&gt; &gt;&gt; you should trust a somewhat-human-equivalent AGI you'd need nearly as
</em><br>
<em>&gt; &gt;&gt; much theory, if not more, than you'd need to just build an FAI in the
</em><br>
<em>&gt; &gt;&gt; first place.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; That depends on what you know, and on what its cognitive capacities are.
</em><br>
<em>&gt; &gt; There should be ways of confidently predicting that a given machine does
</em><br>
<em>&gt; &gt; not have any transhuman capabilities other than a small set of specified
</em><br>
<em>&gt; &gt; ones which are not sufficient for transhuman persuasion or transhuman
</em><br>
<em>&gt; &gt; engineering.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Why 'should' there be an easy way to do this? In my experience predicting
</em><br>
<em>&gt;what capabilities a usefully general design will actually have is pretty
</em><br>
<em>&gt;hard, whether you're trying to prove positives or negatives.
</em><br>
<p>We do it all the time with actual humans.  For a chunk of AI design space 
<br>
larger than &quot;uploads&quot; AGIs are just humans.  Whatever advantages they have 
<br>
will only be those you have given them, probably including speed and 
<br>
moderately fine-grained self-awareness (or you having moderately 
<br>
fine-grained awareness of them).  Predicting approximately what new 
<br>
capabilities a human will have when you make a small change to their 
<br>
neurological hardware can be difficult or easy depending on how well you 
<br>
understand what you are doing, but small changes, that is, changes of 
<br>
magnitude comparable to the range of variation among the human baseline 
<br>
population will never create large and novel transhuman abilities, but lots 
<br>
of time and mere savant abilities may be really really useful.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12382.html">Ben Goertzel: "Mizar conversion project..."</a>
<li><strong>Previous message:</strong> <a href="12380.html">Michael Vassar: "RE: people breaking themselves (was &quot;Re: AGI motivations&quot;)"</a>
<li><strong>In reply to:</strong> <a href="12376.html">Michael Wilson: "Re: AGI motivations"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12387.html">Richard Loosemore: "Re: AGI motivations (Sidetrack on Uploading)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12381">[ date ]</a>
<a href="index.html#12381">[ thread ]</a>
<a href="subject.html#12381">[ subject ]</a>
<a href="author.html#12381">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:17 MST
</em></small></p>
</body>
</html>
