<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI debate at San Jose State U.</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Re: AI debate at San Jose State U.">
<meta name="Date" content="2005-10-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI debate at San Jose State U.</h1>
<!-- received="Fri Oct 21 12:24:53 2005" -->
<!-- isoreceived="20051021182453" -->
<!-- sent="Fri, 21 Oct 2005 14:23:25 -0400" -->
<!-- isosent="20051021182325" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Re: AI debate at San Jose State U." -->
<!-- id="4359321D.30901@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="435710F4.2010106@vaporate.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20AI%20debate%20at%20San%20Jose%20State%20U."><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Fri Oct 21 2005 - 12:23:25 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12559.html">Woody Long: "Re: AI debate at San Jose State U."</a>
<li><strong>Previous message:</strong> <a href="12557.html">Phil Goetz: "Re: DARPA winning car 'taught' to drive, same as humans are."</a>
<li><strong>In reply to:</strong> <a href="12547.html">Olie Lamb: "Re: AI debate at San Jose State U."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12504.html">Woody Long: "Re: AI debate at San Jose State U."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12558">[ date ]</a>
<a href="index.html#12558">[ thread ]</a>
<a href="subject.html#12558">[ subject ]</a>
<a href="author.html#12558">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Olie Lamb wrote:
<br>
<em>&gt; Richard Loosemore wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;   &lt;Snip&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; As far as I am concerned, the widespread (is it really widespread?) 
</em><br>
<em>&gt;&gt; SL4 assumption that &quot;strictly humanoid intelligence would not likely 
</em><br>
<em>&gt;&gt; be Friendly ...[etc.]&quot; is based on a puerile understanding of, and 
</em><br>
<em>&gt;&gt; contempt of, the mechanics of human intelligence.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It's not difficult to show this by a definitive fiat:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Woody Long defined Humanoid intelligence:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1. &quot;Humanoid intelligence requires humanoid interactions with the world&quot; --
</em><br>
<em>&gt; MIT Cog Project website
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This means a fully &quot;human intelligent&quot; SAI must...feel the thrill of 
</em><br>
<em>&gt; victory and the agony of defeat.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If we accept that to be &quot;humanoid&quot;, an intelligence must get pissed-off 
</em><br>
<em>&gt; at losing, we can also define &quot;humanoid&quot; as requiring self-interest 
</em><br>
<em>&gt; /selfishness, which is exactly the characteristic that I thought 
</em><br>
<em>&gt; friendliness was trying to avoid.   An intelligence that cares for all 
</em><br>
<em>&gt; intelligences will necessarily care for its own well being.  Putting 
</em><br>
<em>&gt; emphasis on one particular entity, where the interests are particularly 
</em><br>
<em>&gt; clear, is the start of unfairness.  Strong self interest is synonymous 
</em><br>
<em>&gt; with counter-utility.  You don't need to get stabby and violent for 
</em><br>
<em>&gt; egocentrism to start causing harm to others.  Anyhoo, strong self 
</em><br>
<em>&gt; interest does not necessarily lead to violent self-preservation, but it 
</em><br>
<em>&gt; has a fair degree of overlap.
</em><br>
<p><p>This comes very precisely to one of the points that I wanted to discuss
<br>
in some of my earlier posts to this list.
<br>
<p>Is it really the case that to be humanoid, an intelligence must get
<br>
pissed off at losing, feel selfishness, etc.?
<br>
<p>The answer is a resounding NO!  This is one of those cases where we all
<br>
need to take psychology more seriously:  from the psych point of view,
<br>
the motivational/emotional system is somewhat orthogonal to the
<br>
cognitive part .... which means that you could have the same
<br>
intelligence, and yet be free to design it with all sorts of different
<br>
choices for the motivational/emotional apparatus.
<br>
<p>To be sure, if you wanted to make a thinking system that was *very*
<br>
human-like you would have to put in all the same mot/emot mechanisms
<br>
that we have.  But when you think about it a bit more, you find yourself
<br>
asking *whose* mot/emot system you are going to emulate?  Hannibal
<br>
Lecter's?  Mahatma Ghandi's?  There is an enormous variation just among
<br>
individual instances of human beings.  Arguably you can have people who
<br>
are utterly placid, selfless and who have never felt a violent emotion
<br>
in their lives.  And others with a violence system that is just
<br>
downright missing (not just controlled and suppressed, but not there).
<br>
<p>The idea of &quot;self-interest&quot; is, I agree, slightly more subtle.  Self
<br>
interest might not be just an ad-hoc motivational drive like the others,
<br>
it might be THE basic drive, without which the system would just sit
<br>
there and vegetate.  But I believe there is strong evidence that, in
<br>
humans, you can have people who are very strongly motivated to think and
<br>
learn, and yet who are also selfless and even self-sacrificing (in other
<br>
words, it looks like self-interest may not be required for the creature
<br>
to be intelligent).
<br>
<p>I would not presume to try to answer your question fully in these few
<br>
paragraphs.  What I do want to establish, though, is that we tend to
<br>
make simple assumptions about the relationship between the motivational
<br>
and emotional characteristics of an AGI and the mot/emot systems we see
<br>
in humans ... in the specific sense that we often think that if we make
<br>
the thing at all &quot;humanoid&quot; then we get the entire human mot/emot system
<br>
as a package.  That is not true.  So we as AGI researchers need to think
<br>
about this connection in a great deal more depth if we are going to come
<br>
to sensible conclusions.
<br>
<p>I will now jump forward and say what I believe are the main conclusions
<br>
that we would come to if we did analyse the issues in more depth:  we
<br>
would conclude that *if* we try to build a roughly humanoid AGI *but* we
<br>
give it a mot/emot system of the right sort (basically, empathic towards
<br>
other creatures), we will discover that its Friendliness will be far,
<br>
far more guaranteeable than if we dismiss the humanoid design as bad and
<br>
try to build some kind of &quot;normative&quot; AI system.
<br>
<p>I don't have time to justify that position right now, but I want to
<br>
throw it out there as a possibility.  But we should at least discuss all
<br>
the complexity and subtlety involved in humanoid motivational/emotional
<br>
systems so we can decide if what I just said was reasonable.
<br>
<p>After all, we agree that Friendliness is important, right?  So should we
<br>
not pursue the avenue I have suggested, if there is a possibility that
<br>
we would arrive at the spectacular, but counterintuitive, conclusion
<br>
that giving an AGI the right sort of motivational system would be the
<br>
best possible guarantee of getting a Friendly system?
<br>
<p>Richard Loosemore
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12559.html">Woody Long: "Re: AI debate at San Jose State U."</a>
<li><strong>Previous message:</strong> <a href="12557.html">Phil Goetz: "Re: DARPA winning car 'taught' to drive, same as humans are."</a>
<li><strong>In reply to:</strong> <a href="12547.html">Olie Lamb: "Re: AI debate at San Jose State U."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12504.html">Woody Long: "Re: AI debate at San Jose State U."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12558">[ date ]</a>
<a href="index.html#12558">[ thread ]</a>
<a href="subject.html#12558">[ subject ]</a>
<a href="author.html#12558">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
