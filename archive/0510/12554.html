<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI debate at San Jose State U.</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="Re: AI debate at San Jose State U.">
<meta name="Date" content="2005-10-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI debate at San Jose State U.</h1>
<!-- received="Thu Oct 20 15:34:09 2005" -->
<!-- isoreceived="20051020213409" -->
<!-- sent="Thu, 20 Oct 2005 22:34:05 +0100 (BST)" -->
<!-- isosent="20051020213405" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="Re: AI debate at San Jose State U." -->
<!-- id="20051020213406.83721.qmail@web26706.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="410-2200510420204138687@earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20AI%20debate%20at%20San%20Jose%20State%20U."><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Thu Oct 20 2005 - 15:34:05 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12555.html">BillK: "DARPA winning car 'taught' to drive, same as humans are."</a>
<li><strong>Previous message:</strong> <a href="12553.html">Woody Long: "Re: AI debate at San Jose State U."</a>
<li><strong>In reply to:</strong> <a href="12553.html">Woody Long: "Re: AI debate at San Jose State U."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12559.html">Woody Long: "Re: AI debate at San Jose State U."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12554">[ date ]</a>
<a href="index.html#12554">[ thread ]</a>
<a href="subject.html#12554">[ subject ]</a>
<a href="author.html#12554">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Woody Long wrote:
<br>
<em>&gt; If this is true then &quot;friendliness theory&quot; is trying to eliminate
</em><br>
<em>&gt; the field of strong AI itself, and by extension the singularity.
</em><br>
<p>This misunderstanding appears to arise from a confusion of 'human
<br>
equivalent' in the sense of 'can do anything a human can do', and
<br>
'humanoid intelligence' in the sense of 'has a similar cognitive
<br>
architecture and particularly motivational system to humans'. The
<br>
two are not the same thing, or even closely related.
<br>
<p><em>&gt; I don't think that is what the Singularity Institute is trying
</em><br>
<em>&gt; to do. In fact the website says you are trying to build a fully
</em><br>
<em>&gt; human mind.
</em><br>
<p>The SIAI web site states an intention to build a strong AI. This
<br>
relies on the former definition of 'human equivalent', not the
<br>
latter one. This statement is semi-deprecated anyway given
<br>
Yudkowsky's apparent preference for 'really powerful optimisation
<br>
processes', but 'strong AI' does have the virtue of being an
<br>
existing well-known term.
<br>
<p><em>&gt; All SAI by definition will have an awareness of self and
</em><br>
<em>&gt; self interests.
</em><br>
<p>Awareness of self, yes, simply because there are many things that
<br>
humans can do that require self-awareness (though not the specific
<br>
human kind of self awareness; you wouldn't want to cripple an AGI
<br>
with human reflective limitations anyway).
<br>
<p>Self interest, no. I can't precisely refute this without a
<br>
specific description of the behaviour or mechanisms you think are
<br>
necessary, but basically there are no fundamental goals that all
<br>
intelligences have to have. It's true that evolution tends to
<br>
produce intelligences with particularly kinds of goals, but that's
<br>
just one particular kind of intelligence-producing process.
<br>
<p><em>&gt; It is because of this self interest driven human intelligent
</em><br>
<em>&gt; behavior that it will be able to build cities on the moon without
</em><br>
<em>&gt; human intervention, etc.
</em><br>
<p>'Build cities on the moon' doesn't sound much like self interest
<br>
to me; why would an AI need 'cities' instead of just sensor
<br>
platforms and compute nodes? Regardless, if you want an AI to build
<br>
cities on the moon, simply put that goal into its goal system. An
<br>
attempt to instill 'self-interest' will just lead to goal drift and
<br>
arbitrary (probably bad) results.
<br>
<p><em>&gt; Military SAI, no by definition. Corporate built, consumer SAI, yes,
</em><br>
<em>&gt; as self-destructing, non-harming (non-defective) consumer desired
</em><br>
<em>&gt; products, by way of the profit motive.
</em><br>
<p>This sounds like a 'shock level 2' view of what AI can /do/. 'Consumer
<br>
desired products?' 'Profit motive?' These things may lead to the first
<br>
seed AI being built, deliberately or accidentally, but once in
<br>
existence the only real constraints on its action will be the laws of
<br>
physics and the content of its goal system.
<br>
<p><em>&gt; I thought it was our job to educate the public on the coming
</em><br>
<em>&gt; friendly SAI / Singularity, how it will be safe-built, and the huge
</em><br>
<em>&gt; science and engineering benefits we humans will enjoy because of this
</em><br>
<em>&gt; non-toxic, non-defective, well-engineered, and safe-built
</em><br>
<em>&gt; technological product.
</em><br>
<p>That may have sounded like a good idea when it looked like there would
<br>
be a longer interval between being taken seriously and the evaporation
<br>
of life as we know it. There's plenty of debate about how fast we can
<br>
go from infrahuman AGI to human-equivalent (bad term, I know) AGI, but
<br>
the general premise of this list is that transhuman AGI and rapid
<br>
departure from anything we can predict will follow pretty quickly
<br>
after human-equivalent AGI. Whether this means seconds or years
<br>
depends on your position on 'hard takeoff', but even 'years' would be
<br>
far too fast for our major social institutions to react or even for a
<br>
nontrivial portion of humanity to gain a clue about what's going on.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
How much free photo storage do you get? Store your holiday 
<br>
snaps for FREE with Yahoo! Photos <a href="http://uk.photos.yahoo.com">http://uk.photos.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12555.html">BillK: "DARPA winning car 'taught' to drive, same as humans are."</a>
<li><strong>Previous message:</strong> <a href="12553.html">Woody Long: "Re: AI debate at San Jose State U."</a>
<li><strong>In reply to:</strong> <a href="12553.html">Woody Long: "Re: AI debate at San Jose State U."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12559.html">Woody Long: "Re: AI debate at San Jose State U."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12554">[ date ]</a>
<a href="index.html#12554">[ thread ]</a>
<a href="subject.html#12554">[ subject ]</a>
<a href="author.html#12554">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
