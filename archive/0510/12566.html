<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: agi motivations (was Re: AI debate at San Jose State U.)</title>
<meta name="Author" content="Michael Vassar (michaelvassar@hotmail.com)">
<meta name="Subject" content="agi motivations (was Re: AI debate at San Jose State U.)">
<meta name="Date" content="2005-10-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>agi motivations (was Re: AI debate at San Jose State U.)</h1>
<!-- received="Sun Oct 23 08:41:31 2005" -->
<!-- isoreceived="20051023144131" -->
<!-- sent="Sun, 23 Oct 2005 10:41:29 -0400" -->
<!-- isosent="20051023144129" -->
<!-- name="Michael Vassar" -->
<!-- email="michaelvassar@hotmail.com" -->
<!-- subject="agi motivations (was Re: AI debate at San Jose State U.)" -->
<!-- id="BAY101-F3FC4CF7DE877550C49EE5AC740@phx.gbl" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Vassar (<a href="mailto:michaelvassar@hotmail.com?Subject=Re:%20agi%20motivations%20(was%20Re:%20AI%20debate%20at%20San%20Jose%20State%20U.)"><em>michaelvassar@hotmail.com</em></a>)<br>
<strong>Date:</strong> Sun Oct 23 2005 - 08:41:29 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12567.html">Michael Wilson: "Re: AGI motivations"</a>
<li><strong>Previous message:</strong> <a href="12565.html">Chris Capel: "video lectures on consciousness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12567.html">Michael Wilson: "Re: AGI motivations"</a>
<li><strong>Reply:</strong> <a href="12567.html">Michael Wilson: "Re: AGI motivations"</a>
<li><strong>Reply:</strong> <a href="12582.html">Richard Loosemore: "Loosemore's Proposal [Was: Re: Agi motivations]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12566">[ date ]</a>
<a href="index.html#12566">[ thread ]</a>
<a href="subject.html#12566">[ subject ]</a>
<a href="author.html#12566">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Richard Loosemore said.
<br>
<em>&gt;*if* we try to build a roughly humanoid AGI *but* we give it a mot/emot 
</em><br>
<em>&gt;system of the right sort (basically, empathic towards other creatures), we 
</em><br>
<em>&gt;will discover that its Friendliness will be far, far more guaranteeable 
</em><br>
<em>&gt;than if we dismiss the humanoid design as bad and try to build some kind of 
</em><br>
<em>&gt;&quot;normative&quot; AI system.
</em><br>
<p>Do you mean formally provable (there's no such thing as *more* provable), or 
<br>
only predictable with high confidence given certain conditions similar to 
<br>
those under which it has been tested.  I agree that a robot (humanoid or 
<br>
not) with a human-like motivational system can be empirically demonstrated 
<br>
to be friendly (and possibly a better poor approximation of Friendly than 
<br>
can be expected from an AGI designer) in a wide range of situations.  
<br>
However, because no empirical data is even potentially available regarding 
<br>
the retention of Friendlyness in a post-singularity environment, formal 
<br>
proofs are needed before entering such an environment.  This requires an 
<br>
analytically tractable motivational system, for safety reasons, and 
<br>
human-like motivational systems are not analytically tractable.  It is 
<br>
possible that a non-Transhuman AI with a human-like motivational system 
<br>
could be helpful in designing and implementing an analytically tractable 
<br>
motivational system.  A priori there is no more reason to trust such an AI 
<br>
than to trust a human, though there could easily be conditions which would 
<br>
make it more or less worthy of such trust.
<br>
<p><em>&gt;we should at least discuss all the complexity and subtlety involved in 
</em><br>
<em>&gt;humanoid motivational/emotional systems so we can decide if what I just 
</em><br>
<em>&gt;said was reasonable.
</em><br>
<p>I agree that this is worth discussion as part of singularity strategy if it 
<br>
turns out that it is easier to build a human-like AI with a human goal 
<br>
system than to build a seed AI with a Friendly goal system.  Eliezer's 
<br>
position, as far as I understand it, is that he is confident that it is 
<br>
easier for a small team with limited resources such as SIAI to build a seed 
<br>
AI with a Friendly goal system within a decade or two than for it to build a 
<br>
human-like AI, and that it is much more likely that he can complete a seed 
<br>
AI substantially before anyone else in the world does than that he can 
<br>
complete a human-like AI before anyone else does.  In addition, completing a 
<br>
human-like AI would not solve the requirement for a Friendly seed AI.  It 
<br>
would still be necessary to produce a Friendly seed AI before anyone created 
<br>
an unFriendly one.  Since it is probably easy to build an unFriendly seed AI 
<br>
if you have a human-like AI, this is a critical problem.
<br>
<p><em>&gt;So should we not pursue the avenue I have suggested, if there is a 
</em><br>
<em>&gt;possibility that we would arrive at the spectacular, but counterintuitive, 
</em><br>
<em>&gt;conclusion that giving an AGI the right sort of motivational system would 
</em><br>
<em>&gt;be the best possible guarantee of getting a Friendly system?
</em><br>
<p>I think that the conclusion you are pointing to is not &quot;spectacular but 
<br>
counterintuitive&quot;.  Rather, it is a spectacular conclusion that matches 
<br>
almost everyone's intuitions but which is rather easily refuted.  We should 
<br>
still pursue the avenue in question because if neuromorphic engineering 
<br>
advances rapidly we may not have any better options, and because ultimately 
<br>
this option is not terribly different from the option of using human 
<br>
intelligence and human goals to get FAI, which we are stuck with anyway.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12567.html">Michael Wilson: "Re: AGI motivations"</a>
<li><strong>Previous message:</strong> <a href="12565.html">Chris Capel: "video lectures on consciousness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12567.html">Michael Wilson: "Re: AGI motivations"</a>
<li><strong>Reply:</strong> <a href="12567.html">Michael Wilson: "Re: AGI motivations"</a>
<li><strong>Reply:</strong> <a href="12582.html">Richard Loosemore: "Loosemore's Proposal [Was: Re: Agi motivations]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12566">[ date ]</a>
<a href="index.html#12566">[ thread ]</a>
<a href="subject.html#12566">[ subject ]</a>
<a href="author.html#12566">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
