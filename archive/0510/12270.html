<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Kurzweil reviewed in the Wall Street Journal</title>
<meta name="Author" content="Bill Hibbard (test@demedici.ssec.wisc.edu)">
<meta name="Subject" content="RE: Kurzweil reviewed in the Wall Street Journal">
<meta name="Date" content="2005-10-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Kurzweil reviewed in the Wall Street Journal</h1>
<!-- received="Tue Oct  4 06:07:24 2005" -->
<!-- isoreceived="20051004120724" -->
<!-- sent="Tue, 4 Oct 2005 07:06:53 -0500 (CDT)" -->
<!-- isosent="20051004120653" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@demedici.ssec.wisc.edu" -->
<!-- subject="RE: Kurzweil reviewed in the Wall Street Journal" -->
<!-- id="Pine.GSO.4.44.0510040704400.17761-100000@demedici.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="A9057CB4845E544CA5446303E10E7EAB63ACF4@redwings.pontiac.local" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@demedici.ssec.wisc.edu?Subject=RE:%20Kurzweil%20reviewed%20in%20the%20Wall%20Street%20Journal"><em>test@demedici.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Tue Oct 04 2005 - 06:06:53 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12271.html">Phil Goetz: "RE: Kurzweil reviewed in the Wall Street Journal"</a>
<li><strong>Previous message:</strong> <a href="12269.html">Pilot Pirx: "Re: Kurzweil reviewed in the Wall Street Journal"</a>
<li><strong>In reply to:</strong> <a href="12265.html">David Massoglia: "RE: Kurzweil reviewed in the Wall Street Journal"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12271.html">Phil Goetz: "RE: Kurzweil reviewed in the Wall Street Journal"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12270">[ date ]</a>
<a href="index.html#12270">[ thread ]</a>
<a href="subject.html#12270">[ subject ]</a>
<a href="author.html#12270">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Mon, 3 Oct 2005, David Massoglia wrote:
<br>
<p><em>&gt; I am a huge fan of Ray Kurzweil and believe him to be extraordinarily
</em><br>
<em>&gt; gifted.  I would be curious to hear from the many intelligent people on
</em><br>
<em>&gt; this board if they have any &quot;significant&quot; differences from Kurzweil's
</em><br>
<em>&gt; opinions and projections on the Singularity or other significant matters
</em><br>
<em>&gt; and why. Again, I am only interested in &quot;significant&quot; differences and
</em><br>
<em>&gt; will leave that to your judgment.
</em><br>
<p>Here's a review I posted on Amazon:
<br>
<p>4 out of a possible 5 stars
<br>
<p>title: a Good Book, but Fails to Adequately Address the Dangers of AI
<br>
<p>In &quot;The Singularity is Near&quot; Ray Kurzweil continues his
<br>
role as the primary advocate and educator for the coming
<br>
technological revolution in intelligence. As he describes,
<br>
this and other related new technologies promise enormous
<br>
benefits to humans, such as indefinite life span and
<br>
greatly increased intelligence. However, artificial
<br>
intelligence (AI) also poses serious threats, and Kurzweil
<br>
does not adequately address those threats and the possible
<br>
ways to defend against them. This is in contrast to his
<br>
detailed descriptions of threats from genetic engineering
<br>
and nanotechnology, and possible defenses against them.
<br>
<p>Kurzweil says one of his fundamental principles is &quot;respect
<br>
for human consciousness&quot;, on page 374. But if AI develops
<br>
without any regulation it will just extend human
<br>
competition (military, economic, etc). This will continue
<br>
to amplify the gap between winners and losers, as the
<br>
technological revolution is already doing. If human society
<br>
evolves to a state in which the intelligence gaps between
<br>
humans are greater than the gaps between current humans and
<br>
their pets, this decision should be made consciously by an
<br>
informed public. As the primary educator on intelligence
<br>
technology, Kurzweil has a responsibility to explain this.
<br>
On page 470 he quotes Leon Furth, former National Security
<br>
Advisor to Vice President Gore, as saying that Americans
<br>
will not 'simply sit still' for the AI revolution. It is
<br>
encouraging that people in such powerful positions are aware
<br>
of the issues.
<br>
<p>In his section on &quot;... and Dangers&quot;, pages 397-400, Kurzweil
<br>
discusses dangerous scenarios for genetic engineering and
<br>
nanobots but not for AI, which is puzzling.
<br>
<p>He does later address the issue briefly for AI, on page 420,
<br>
where he says &quot;As such, it [AI] will reflect our values
<br>
because it will be us.&quot; But which of us? The wealthy and
<br>
powerful initially, and the rest of us later if they
<br>
allow it. You could make the same argument for lack of
<br>
regulation over nuclear bombs, because they are built and
<br>
controlled by &quot;us&quot;, and hence control over them reflects
<br>
our values. But nuclear bombs are subject to collectively
<br>
agreed values, at least in democratic countries, and with
<br>
some effort to extend that internationally. AI will be more
<br>
dangerous than nuclear bombs and should also be subject to
<br>
collectively agreed regulation.
<br>
<p>On page 424, of efforts &quot;to deal with the danger from
<br>
pathological R (strong AI)&quot; Kurzweil says &quot;But there is no
<br>
purely technical strategy that is workable in this area,
<br>
because greater intelligence will always find a way to
<br>
circumvent measures that are the product of a lesser
<br>
intelligence.&quot; This is not true if our strategy is to
<br>
design greater intelligence to not want to circumvent
<br>
protective measures. I discuss this at length at:
<br>
<p>&nbsp;&nbsp;<a href="http://www.ssec.wisc.edu/~billh/g/mi.html">http://www.ssec.wisc.edu/~billh/g/mi.html</a>
<br>
<p>There will be those who design machines with values that
<br>
don't comply with regulation, but this threat is best met
<br>
by putting resources into the development of complying
<br>
AIs that can help detect and eliminate non-complying AIs.
<br>
This is very similar to Kurzweil's own prescription for
<br>
accelerating development of defensive technologies for
<br>
genetic engineering and nanotechnology. He makes clear
<br>
that such defenses are very difficult but that the
<br>
problem must be solved to avoid a catastrophe. The same
<br>
logic applies to defenses against pathological AI: very
<br>
difficult but necessary.
<br>
<p>In his &quot;Response to Critics&quot; chapter, Kurzweil addresses
<br>
the issue of government regulation, but only whether it
<br>
will slow down or stop technological progress. He does not
<br>
address here the question of whether AI should be regulated.
<br>
<p>Kurzweil ends (except for the notes and other back matter)
<br>
on a very good note in &quot;Human Centrality&quot;. He rebuts the
<br>
claim of Stephen Jay Gould that all scientific revolutions
<br>
reduce the stature of humans in the universe, by asserting
<br>
that human brains and the successors they create are the
<br>
main drivers of the universe.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12271.html">Phil Goetz: "RE: Kurzweil reviewed in the Wall Street Journal"</a>
<li><strong>Previous message:</strong> <a href="12269.html">Pilot Pirx: "Re: Kurzweil reviewed in the Wall Street Journal"</a>
<li><strong>In reply to:</strong> <a href="12265.html">David Massoglia: "RE: Kurzweil reviewed in the Wall Street Journal"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12271.html">Phil Goetz: "RE: Kurzweil reviewed in the Wall Street Journal"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12270">[ date ]</a>
<a href="index.html#12270">[ thread ]</a>
<a href="subject.html#12270">[ subject ]</a>
<a href="author.html#12270">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:04 MST
</em></small></p>
</body>
</html>
