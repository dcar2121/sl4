<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Loosemore's Proposal</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Re: Loosemore's Proposal">
<meta name="Date" content="2005-10-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Loosemore's Proposal</h1>
<!-- received="Mon Oct 24 13:14:32 2005" -->
<!-- isoreceived="20051024191432" -->
<!-- sent="Mon, 24 Oct 2005 15:13:09 -0400" -->
<!-- isosent="20051024191309" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Re: Loosemore's Proposal" -->
<!-- id="435D3245.3050408@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="20051024173901.52682.qmail@web26703.mail.ukl.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Loosemore's%20Proposal"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Mon Oct 24 2005 - 13:13:09 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12394.html">Eliezer S. Yudkowsky: "Re: AGI motivations (Sidetrack on Uploading)"</a>
<li><strong>Previous message:</strong> <a href="12392.html">Pope Salmon the Lesser Mungojelly: "cyborgs &amp; ghosts     (was Re: AGI motivations (Sidetrack on Uploading))"</a>
<li><strong>In reply to:</strong> <a href="12390.html">Michael Wilson: "Re: Loosemore's Proposal"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12398.html">Michael Wilson: "Re: Loosemore's Proposal"</a>
<li><strong>Reply:</strong> <a href="12398.html">Michael Wilson: "Re: Loosemore's Proposal"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12393">[ date ]</a>
<a href="index.html#12393">[ thread ]</a>
<a href="subject.html#12393">[ subject ]</a>
<a href="author.html#12393">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michael Wilson wrote:
<br>
<em>&gt; Richard Loosemore wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;Proofs are for mathematicians. I consider the use of the word
</em><br>
<em>&gt;&gt;&quot;proof,&quot; about the behavior of an AGI, as on the same level of
</em><br>
<em>&gt;&gt;validity as the use of the word &quot;proof&quot; in statements about
</em><br>
<em>&gt;&gt;evolutionary proclivities, for example &quot;Prove that no tree could
</em><br>
<em>&gt;&gt;ever evolve, naturally, in such a way that it had a red smiley
</em><br>
<em>&gt;&gt;face depicted on every leaf.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is a gross simplification, but basically this just means that
</em><br>
<em>&gt; AGIs amenable to formal verification will resemble software systems
</em><br>
<em>&gt; more than organic systems. It is intuitively apparent (and this is
</em><br>
<em>&gt; a case where intuition is actually right) that since computers are
</em><br>
<em>&gt; designed to support formal software systems, not organic simulations,
</em><br>
<em>&gt; this approach will also make more efficient use of currently
</em><br>
<em>&gt; available hardware.
</em><br>
<p>That is not what I said.
<br>
<p><p><em>&gt;&gt;First, many people have talked as if building a &quot;human-like&quot; AGI would 
</em><br>
<em>&gt;&gt;be very difficult.  I think that this is a mistake, for the following 
</em><br>
<em>&gt;&gt;reasons.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The quoted discussion focused on the difficultly of building perfectly
</em><br>
<em>&gt; human-like AGIs, on the basis that any perceived safety advantage will
</em><br>
<em>&gt; be lost if the system is not perfectly human-like.
</em><br>
<p>The quoted discussion was not about &quot;perfectly&quot; human-like AIs.
<br>
<p><p><em>&gt;&gt;Specifically, I think that we (the early AI researchers) started from 
</em><br>
<em>&gt;&gt;the observation of certain *high-level* reasoning mechanisms that are 
</em><br>
<em>&gt;&gt;observable in the human mind, and generalized to the idea that these 
</em><br>
<em>&gt;&gt;mechanisms could be the foundational mechanisms of a thinking system.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This observation is made in at least a third of the AI books on my
</em><br>
<em>&gt; bookshelf. It was insightful circa 1985, it's common knowledge now.
</em><br>
<em>&gt; It's true that some researchers still don't accept it, but they're
</em><br>
<em>&gt; probably a minority by now.
</em><br>
<p>You are still doing it.  That was the whole point of my argument.
<br>
<p><p><p><em>&gt;&gt;What we say is this.  The logic approach is bad because it starts with 
</em><br>
<em>&gt;&gt;presumptions about the local mechanisms of the system and then tries to 
</em><br>
<em>&gt;&gt;extend that basic design out until the system can build its own new 
</em><br>
<em>&gt;&gt;knowledge,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You're attacking a strawman position. As Ben pointed out earlier, no-one
</em><br>
<em>&gt; on this list other than possibly the Cyc team are following this approach
</em><br>
<em>&gt; in the form you criticise it. There /are/ well-grounded logic-based
</em><br>
<em>&gt; approaches that avoid the massive layer collapse fallacy, but these
</em><br>
<em>&gt; bear little relation to classic symbolic AI and do not (necessarily)
</em><br>
<em>&gt; suffer from any of the failings you identify.
</em><br>
<p>Not a straw man:  you yourselves are taking the &quot;logic&quot; approach that I 
<br>
am talking about.  Until you understand that, you are missing the point 
<br>
of this entire argument.
<br>
<p><p><p><p><p><em>&gt;&gt;instead, you should be noticing that the hardest part of your
</em><br>
<em>&gt;&gt;implementation is always the learning and grounding aspect of
</em><br>
<em>&gt;&gt;the system.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Again, a fairly common thing for frustrated AI researchers to say,
</em><br>
<em>&gt; and indeed a good part of LOGI can be interpreted as a solution
</em><br>
<em>&gt; to the 'grounding problem'.
</em><br>
<p>Nonsense: LOGI hasn't solved the grounding problem.
<br>
<p><p><em>&gt;&gt;This is exactly what has been happening in AI research.  And it has been 
</em><br>
<em>&gt;&gt;going on for, what, 20 years now?  Plenty of theoretical analysis.  Lots 
</em><br>
<em>&gt;&gt;of systems that do little jobs a little tiny bit better than before.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Actually thousands of connectionist and hundreds of 'hybrid' and
</em><br>
<em>&gt; stochastic approaches have also been tried in that time, some of them
</em><br>
<em>&gt; with supporting rheotic very similar to yours. Obviously no one has
</em><br>
<em>&gt; got it right yet and there's plenty of room for new /designs/, but
</em><br>
<em>&gt; you certainly don't have a novel /approach/. Personally I believe that
</em><br>
<em>&gt; a AI research methodology is in fact necessary, but obviously what I
</em><br>
<em>&gt; have in mind is not what you're on about.
</em><br>
<p>Cite one example of systematic variation of local mechanisms in complete 
<br>
AGI systems, in search of stability.  There is not one.  Nobody has 
<br>
tried the approach that I have adopted, so why, in your book, is it not 
<br>
novel?
<br>
<p><p><p><em>&gt;&gt;Build a development environment that allowed rapid construction of large 
</em><br>
<em>&gt;&gt;numbers of different systems, so we can start to empirically study the 
</em><br>
<em>&gt;&gt;effects of changing the local mechanisms.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Depending on your level of specificity, you are either proposing a 'new
</em><br>
<em>&gt; language for AI', i.e. a project in the same general niche as Flare and
</em><br>
<em>&gt; with the same basic problems, or just a fairly flexible 'AI substrate'
</em><br>
<em>&gt; of the kind you could arguable say Ben has already developed. Either
</em><br>
<em>&gt; would be a secondary issue; the key part is proposed 'local mechanisms'.
</em><br>
<p>read what I wrote.  It would not even slightly resemble either Flare or 
<br>
Ben's system.
<br>
<p><p><p><em>&gt;&gt;But I can tell you this: we have never tried such an approach before, 
</em><br>
<em>&gt;&gt;and the one thing that we do know from the complex systems research (you 
</em><br>
<em>&gt;&gt;can argue with everything else, but you cannot argue with this) is that 
</em><br>
<em>&gt;&gt;we won't know the outcome until we try.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; People have been hacking about with 'stew of local dynamics' type
</em><br>
<em>&gt; systems for at least two decades; look at Holland's classic work on
</em><br>
<em>&gt; classifier systems, Kokinov's DUAL/AMBR work in the 90s, Edelman or
</em><br>
<em>&gt; Calvin's neuromorphic projects (low and medium level respectively)
</em><br>
<em>&gt; or Aleksander's recent human-cognition-inspired designs. Again, this
</em><br>
<em>&gt; is not a new approach or a novel insight, though you probably have
</em><br>
<em>&gt; novel specifics.
</em><br>
<p>Again, only true if you ignore what I said.
<br>
<p><p><p><em>&gt;&gt;(Notice that the availability of such a development environment would 
</em><br>
<em>&gt;&gt;not in any way preclude the kind of logic-based AI that is now the 
</em><br>
<em>&gt;&gt;favorite.  You could just as easily build such models.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Ok, if it's that general, it's so general it doesn't actually
</em><br>
<em>&gt; contribute any useful cognitive complexity and you're just designing
</em><br>
<em>&gt; a language/IDE optimised for (your notions of) AI development work.
</em><br>
<em>&gt; See past arguments about why this isn't a good use of time, unless
</em><br>
<em>&gt; you can't think of anything better to do.
</em><br>
<p>So, where else is there a development environment that would easily 
<br>
allow someone who was not a hacker to produce 100 different *designs* of 
<br>
cognitive systems, using different local mechanisms, then feed them the 
<br>
same sets of environmental data, then analyse the internal dynamics and 
<br>
make side by side comparisons of the behavior of those 100 systems, and 
<br>
get all this done in a week, so you can go on to look at another set of 
<br>
100 systems next week?
<br>
<p>Why do you think this would make no difference whatsoever to the way AGI 
<br>
research is done?
<br>
<p><p><p><p><p><p><em>&gt;&gt;The problem is that people who did so would be embarrassed into
</em><br>
<em>&gt;&gt;showing how their mechanisms interacted with real sensory and
</em><br>
<em>&gt;&gt;motor systems,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You really do seem to be picking on a small clique of researchers,
</em><br>
<em>&gt; who maintain an outdated, discredited approach that you've managed
</em><br>
<em>&gt; to identify some obvious flaws in, and then generalised from this
</em><br>
<em>&gt; easily-derided group to the entire AI research community.
</em><br>
<p>Stop trying to deflect attention to some other group:  I am talking 
<br>
about you and your approach.
<br>
<p>If I am not talking about you, when was the last time you built a 
<br>
complete AGI and tested it to see if the local mechanisms you chose 
<br>
rendered it stable (a) in the face of real world environmental 
<br>
interaction and (b) in the course of learning?
<br>
<p><p><em>&gt;&gt;Finally, on the subject that we started with:  motivations of an AGI. 
</em><br>
<em>&gt;&gt;The class of system I am proposing would have a motivational/emotional 
</em><br>
<em>&gt;&gt;system that is distinct from the immediate goal stack.  Related, but not 
</em><br>
<em>&gt;&gt;be confused.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;I think we could build small scale examples of cognitive systems, insert 
</em><br>
<em>&gt;&gt;different kinds of M/E systems in them, and allow them to interact 
</em><br>
<em>&gt;&gt;with one another in simple virtual worlds.  We could study the stability 
</em><br>
<em>&gt;&gt;of the systems, their cooperative behavior towards one another, their 
</em><br>
<em>&gt;&gt;response to situations in which they faced threats, etc.  I think we 
</em><br>
<em>&gt;&gt;could look for telltale signs of breakdown, and perhaps even track their 
</em><br>
<em>&gt;&gt;&quot;thoughts&quot; to see what their view of the world was, and how that 
</em><br>
<em>&gt;&gt;interacted with their motivations.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This part does not appear unreasonable; it seems similar to the
</em><br>
<em>&gt; 'experimental investigation of AGI goal system dynamics' that Ben
</em><br>
<em>&gt; has historically been in favour of. It's just ridiculously unsafe
</em><br>
<em>&gt; and overoptimistic in light of the dangers and difficulties
</em><br>
<em>&gt; involved, in both the work itself and the generalisation.
</em><br>
<p>You have never got anywhere near trying it, nor (from the evidence of 
<br>
this and other posts) understood exactly what it would involve, so what 
<br>
makes you so able to pronounce that it would be &quot;unsafe&quot;.  You are 
<br>
speculating.
<br>
<p><p><p><p><p><em>&gt;&gt;And what we might well discover is that the disconnect between M/E 
</em><br>
<em>&gt;&gt;system and intellect is just as it appears to be in humans: humans
</em><br>
<em>&gt;&gt;are intellectual systems with aggressive M/E systems tacked on
</em><br>
<em>&gt;&gt;underneath.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; How well modularised the human brain is in that respect is an open
</em><br>
<em>&gt; question, but the very hard problem of designing a stable Friendly
</em><br>
<em>&gt; 'M/E' system remains; this is not something you can do by trial and
</em><br>
<em>&gt; error, attempting to do it by trial and error will probably get
</em><br>
<em>&gt; everyone killed, and first-principles research into FAI has already
</em><br>
<em>&gt; generating strong evidence (actually forget that, even 'Heuristics
</em><br>
<em>&gt; and Biases' research has generated strong evidence) for human-like
</em><br>
<em>&gt; cognitive systems being a bad starting point.
</em><br>
<p>First-principles research in FAI?  You don't have a workable theory of 
<br>
FAI, you just have some armchair speculation.
<br>
<p>Stop asserting this detached-from-reality pseudo-philosohpical nonsense 
<br>
and get some empirical data to back up your claim.
<br>
<p><p><p>Really, my patience is wearing thin with these spurious attacks.
<br>
<p>Richard Loosemore.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12394.html">Eliezer S. Yudkowsky: "Re: AGI motivations (Sidetrack on Uploading)"</a>
<li><strong>Previous message:</strong> <a href="12392.html">Pope Salmon the Lesser Mungojelly: "cyborgs &amp; ghosts     (was Re: AGI motivations (Sidetrack on Uploading))"</a>
<li><strong>In reply to:</strong> <a href="12390.html">Michael Wilson: "Re: Loosemore's Proposal"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12398.html">Michael Wilson: "Re: Loosemore's Proposal"</a>
<li><strong>Reply:</strong> <a href="12398.html">Michael Wilson: "Re: Loosemore's Proposal"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12393">[ date ]</a>
<a href="index.html#12393">[ thread ]</a>
<a href="subject.html#12393">[ subject ]</a>
<a href="author.html#12393">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:18 MST
</em></small></p>
</body>
</html>
