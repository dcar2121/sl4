<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AGI motivations</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="Re: AGI motivations">
<meta name="Date" content="2005-10-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AGI motivations</h1>
<!-- received="Sun Oct 23 09:37:28 2005" -->
<!-- isoreceived="20051023153728" -->
<!-- sent="Sun, 23 Oct 2005 16:37:25 +0100 (BST)" -->
<!-- isosent="20051023153725" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="Re: AGI motivations" -->
<!-- id="20051023153725.98226.qmail@web26702.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="BAY101-F3FC4CF7DE877550C49EE5AC740@phx.gbl" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20AGI%20motivations"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Sun Oct 23 2005 - 09:37:25 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12375.html">Michael Vassar: "Re: AGI motivations"</a>
<li><strong>Previous message:</strong> <a href="12373.html">Michael Vassar: "agi motivations (was Re: AI debate at San Jose State U.)"</a>
<li><strong>In reply to:</strong> <a href="12373.html">Michael Vassar: "agi motivations (was Re: AI debate at San Jose State U.)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12375.html">Michael Vassar: "Re: AGI motivations"</a>
<li><strong>Reply:</strong> <a href="12375.html">Michael Vassar: "Re: AGI motivations"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12374">[ date ]</a>
<a href="index.html#12374">[ thread ]</a>
<a href="subject.html#12374">[ subject ]</a>
<a href="author.html#12374">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michael Vassar wrote:
<br>
<em>&gt; Do you mean formally provable (there's no such thing as *more*
</em><br>
<em>&gt; provable), or only predictable with high confidence given certain
</em><br>
<em>&gt; conditions similar to those under which it has been tested.
</em><br>
<p>There is an intermediate ground between a full formal proof and
<br>
simple extrapolation from experiment; use of a partial predictive
<br>
hypothesis to extend experimental results to untested conditions
<br>
and to predict the limits beyond which such inference becomes
<br>
uselessly imprecise. The SIAI is attempting to employ the purely
<br>
formal approach, but at this time it would be foolish to deny that
<br>
this may not be possible and it might prove necessary to employ a
<br>
combination of functional models and directed experiment. I would
<br>
note that any attempt to predict AI behaviour without a verifiable,
<br>
reliable model of how the AI works (which relies on a 'transparent',
<br>
'clean' architecture) falls into the 'simple extrapolation'
<br>
category; it's simply not possible to do better than guessing with
<br>
designs that employ strictly unconstrained 'emergence'.
<br>
<p><em>&gt; However, because no empirical data is even potentially available
</em><br>
<em>&gt; regarding the retention of Friendlyness in a post-singularity
</em><br>
<em>&gt; environment,
</em><br>
<p>I agree, but this would not hold if it was actually possible to
<br>
build an 'AI box' capable of safely containing transhuman AGIs, so
<br>
I expect some people will disagree.
<br>
<p><em>&gt; This requires an analytically tractable motivational system, for
</em><br>
<em>&gt; safety reasons, and human-like motivational systems are not
</em><br>
<em>&gt; analytically tractable.
</em><br>
<p>Definitely. The goal/motivation system is the most critical
<br>
component in questions of long-term predictability; this is the
<br>
very definition of 'optimisation target'. Even if the rest of your
<br>
AGI is an emergence-driven mess, it may be possible to make some
<br>
useful predictions if the goal system is amenable to formal analysis.
<br>
But if the goal system is opaque and chaotic it doesn't matter how
<br>
transparent the rest of the design is, you're still reduced to wild
<br>
guesses as to the long term effects of having the AGI around.
<br>
<p><em>&gt; It is possible that a non-Transhuman AI with a human-like motivational
</em><br>
<em>&gt; system could be helpful in designing and implementing an analytically
</em><br>
<em>&gt; tractable motivational system.
</em><br>
<p>Well sure, for the same reason that it would be great to have some
<br>
intelligence-augmented humans or human uploads around to help design
<br>
the FAI. But actually trying to build one would be incredibly risky and
<br>
unlikely to work, even more so than independent IA and uploading
<br>
projects, so this observation isn't of much practical utility. What I
<br>
think /may/ be both useful and practical is some special purpose tools
<br>
based on constrained, infrahuman AGI.
<br>
<p><em>&gt; A priori there is no more reason to trust such an AI than to trust a
</em><br>
<em>&gt; human, though there could easily be conditions which would make it
</em><br>
<em>&gt; more or less worthy of such trust.
</em><br>
<p>This immediately sets off warning bells simply because humans have a
<br>
lot of evolved cognitive machinery for evaluating 'trust', and strong
<br>
intuitive notions of how 'trust' works, which would utterly fail (and
<br>
hence be worse than useless) if the AGI has any significant deviations
<br>
from human cognitive architecture (which would be effectively
<br>
unavoidable). To work out from first principles (i.e. reliably) whether
<br>
you should trust a somewhat-human-equivalent AGI you'd need nearly as
<br>
much theory, if not more, than you'd need to just build an FAI in the
<br>
first place.
<br>
&nbsp;
<br>
<em>&gt; I agree that this is worth discussion as part of singularity strategy if
</em><br>
<em>&gt; it turns out that it is easier to build a human-like AI with a human goal 
</em><br>
<em>&gt; system than to build a seed AI with a Friendly goal system.
</em><br>
<p>To be sure of building a human-like AI, you'd need to either very closely
<br>
follow neurophysiology (i.e. build an accurate brain simulation, which we
<br>
don't have the data or the hardware for yet) or use effectively the same
<br>
basic theory you'd need to build an FAI to ensure that the new AGI will
<br>
reliably show human-like behaviour. If you have the technology to do the
<br>
latter, you might as well just upload people; it's less risky than
<br>
trying to build a human-like AGI (though probably more risky than building
<br>
an FAI). If you don't, trying to build a human-like AGI is pointless;
<br>
developing the theory to do better than wild guessing is probably harder
<br>
than developing FAI theory and /still/ a poor risk/reward trade-off, while
<br>
attempting to do it without the theory (as plenty of AGI projects are,
<br>
unfortunately) is essentially racial suicide.
<br>
<p><em>&gt; Eliezer's position, as far as I understand it, is that he is confident
</em><br>
<em>&gt; that it is easier for a small team with limited resources such as SIAI
</em><br>
<em>&gt; to build a seed AI with a Friendly goal system within a decade or two
</em><br>
<em>&gt; than for it to build a human-like AI,
</em><br>
<p>It's certainly my position. Trying to build a 'human-like' AGI would
<br>
necessitate massive amounts of pointless cruft (which would still have
<br>
to be meticulously researched, designed, functionally validated and
<br>
tested) and rule out the use of many techniques that greatly improve
<br>
tractability and transparency, severely increasing implementation
<br>
difficulty and hardware requirements. These are in addition to the
<br>
reasons why it's a bad idea given above.
<br>
<p><em>&gt; In addition, completing a human-like AI would not solve the requirement
</em><br>
<em>&gt; for a Friendly seed AI. It would still be necessary to produce a Friendly
</em><br>
<em>&gt; seed AI before anyone created an unFriendly one.
</em><br>
<p>Yes, noting of course that it's extremely difficult to build a
<br>
'human-like AI' that isn't already an Unfriendly seed AI.
<br>
<p><em>&gt; We should still pursue the avenue in question because if neuromorphic
</em><br>
<em>&gt; engineering advances rapidly we may not have any better options,
</em><br>
<p>In a choice between uploading and UFAI, or uploading and military
<br>
nanotech, uploading almost certainly wins. Given that uploading is
<br>
primarily an engineering rather than theoretical challenge, my position
<br>
would be that if you want to research 'human-like AGI', you'd be far
<br>
better off researching uploading (and/or brain-computer interfacing)
<br>
instead.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
To help you stay safe and secure online, we've developed the all new Yahoo! Security Centre. <a href="http://uk.security.yahoo.com">http://uk.security.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12375.html">Michael Vassar: "Re: AGI motivations"</a>
<li><strong>Previous message:</strong> <a href="12373.html">Michael Vassar: "agi motivations (was Re: AI debate at San Jose State U.)"</a>
<li><strong>In reply to:</strong> <a href="12373.html">Michael Vassar: "agi motivations (was Re: AI debate at San Jose State U.)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12375.html">Michael Vassar: "Re: AGI motivations"</a>
<li><strong>Reply:</strong> <a href="12375.html">Michael Vassar: "Re: AGI motivations"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12374">[ date ]</a>
<a href="index.html#12374">[ thread ]</a>
<a href="subject.html#12374">[ subject ]</a>
<a href="author.html#12374">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:16 MST
</em></small></p>
</body>
</html>
