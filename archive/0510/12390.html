<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Loosemore's Proposal</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="Re: Loosemore's Proposal">
<meta name="Date" content="2005-10-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Loosemore's Proposal</h1>
<!-- received="Mon Oct 24 11:39:04 2005" -->
<!-- isoreceived="20051024173904" -->
<!-- sent="Mon, 24 Oct 2005 18:39:00 +0100 (BST)" -->
<!-- isosent="20051024173900" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="Re: Loosemore's Proposal" -->
<!-- id="20051024173901.52682.qmail@web26703.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="435D0CA2.7020404@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20Loosemore's%20Proposal"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Mon Oct 24 2005 - 11:39:00 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12391.html">Michael Wilson: "AGI motivations"</a>
<li><strong>Previous message:</strong> <a href="12389.html">Richard Loosemore: "Loosemore's Proposal [Was: Re: Agi motivations]"</a>
<li><strong>In reply to:</strong> <a href="12389.html">Richard Loosemore: "Loosemore's Proposal [Was: Re: Agi motivations]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12393.html">Richard Loosemore: "Re: Loosemore's Proposal"</a>
<li><strong>Reply:</strong> <a href="12393.html">Richard Loosemore: "Re: Loosemore's Proposal"</a>
<li><strong>Maybe reply:</strong> <a href="12396.html">Chris Capel: "Re: Loosemore's Proposal"</a>
<li><strong>Maybe reply:</strong> <a href="12400.html">rpwl@lightlink.com: "Re: Loosemore's Proposal"</a>
<li><strong>Maybe reply:</strong> <a href="12402.html">rpwl@lightlink.com: "Re: Loosemore's Proposal"</a>
<li><strong>Maybe reply:</strong> <a href="12406.html">Keith Henson: "Re: Loosemore's Proposal"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12390">[ date ]</a>
<a href="index.html#12390">[ thread ]</a>
<a href="subject.html#12390">[ subject ]</a>
<a href="author.html#12390">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Richard Loosemore wrote:
<br>
<em>&gt; Proofs are for mathematicians. I consider the use of the word
</em><br>
<em>&gt; &quot;proof,&quot; about the behavior of an AGI, as on the same level of
</em><br>
<em>&gt; validity as the use of the word &quot;proof&quot; in statements about
</em><br>
<em>&gt; evolutionary proclivities, for example &quot;Prove that no tree could
</em><br>
<em>&gt; ever evolve, naturally, in such a way that it had a red smiley
</em><br>
<em>&gt; face depicted on every leaf.&quot;
</em><br>
<p>This is a gross simplification, but basically this just means that
<br>
AGIs amenable to formal verification will resemble software systems
<br>
more than organic systems. It is intuitively apparent (and this is
<br>
a case where intuition is actually right) that since computers are
<br>
designed to support formal software systems, not organic simulations,
<br>
this approach will also make more efficient use of currently
<br>
available hardware.
<br>
<p><em>&gt; First, many people have talked as if building a &quot;human-like&quot; AGI would 
</em><br>
<em>&gt; be very difficult.  I think that this is a mistake, for the following 
</em><br>
<em>&gt; reasons.
</em><br>
<p>The quoted discussion focused on the difficultly of building perfectly
<br>
human-like AGIs, on the basis that any perceived safety advantage will
<br>
be lost if the system is not perfectly human-like.
<br>
&nbsp;
<br>
<em>&gt; Specifically, I think that we (the early AI researchers) started from 
</em><br>
<em>&gt; the observation of certain *high-level* reasoning mechanisms that are 
</em><br>
<em>&gt; observable in the human mind, and generalized to the idea that these 
</em><br>
<em>&gt; mechanisms could be the foundational mechanisms of a thinking system.
</em><br>
<p>This observation is made in at least a third of the AI books on my
<br>
bookshelf. It was insightful circa 1985, it's common knowledge now.
<br>
It's true that some researchers still don't accept it, but they're
<br>
probably a minority by now.
<br>
<p><em>&gt; What we say is this.  The logic approach is bad because it starts with 
</em><br>
<em>&gt; presumptions about the local mechanisms of the system and then tries to 
</em><br>
<em>&gt; extend that basic design out until the system can build its own new 
</em><br>
<em>&gt; knowledge,
</em><br>
<p>You're attacking a strawman position. As Ben pointed out earlier, no-one
<br>
on this list other than possibly the Cyc team are following this approach
<br>
in the form you criticise it. There /are/ well-grounded logic-based
<br>
approaches that avoid the massive layer collapse fallacy, but these
<br>
bear little relation to classic symbolic AI and do not (necessarily)
<br>
suffer from any of the failings you identify.
<br>
<p><em>&gt; instead, you should be noticing that the hardest part of your
</em><br>
<em>&gt; implementation is always the learning and grounding aspect of
</em><br>
<em>&gt; the system.
</em><br>
<p>Again, a fairly common thing for frustrated AI researchers to say,
<br>
and indeed a good part of LOGI can be interpreted as a solution
<br>
to the 'grounding problem'.
<br>
<p><em>&gt; This is exactly what has been happening in AI research.  And it has been 
</em><br>
<em>&gt; going on for, what, 20 years now?  Plenty of theoretical analysis.  Lots 
</em><br>
<em>&gt; of systems that do little jobs a little tiny bit better than before.
</em><br>
<p>Actually thousands of connectionist and hundreds of 'hybrid' and
<br>
stochastic approaches have also been tried in that time, some of them
<br>
with supporting rheotic very similar to yours. Obviously no one has
<br>
got it right yet and there's plenty of room for new /designs/, but
<br>
you certainly don't have a novel /approach/. Personally I believe that
<br>
a AI research methodology is in fact necessary, but obviously what I
<br>
have in mind is not what you're on about.
<br>
<p><em>&gt; Build a development environment that allowed rapid construction of large 
</em><br>
<em>&gt; numbers of different systems, so we can start to empirically study the 
</em><br>
<em>&gt; effects of changing the local mechanisms.
</em><br>
<p>Depending on your level of specificity, you are either proposing a 'new
<br>
language for AI', i.e. a project in the same general niche as Flare and
<br>
with the same basic problems, or just a fairly flexible 'AI substrate'
<br>
of the kind you could arguable say Ben has already developed. Either
<br>
would be a secondary issue; the key part is proposed 'local mechanisms'.
<br>
<p><em>&gt; But I can tell you this: we have never tried such an approach before, 
</em><br>
<em>&gt; and the one thing that we do know from the complex systems research (you 
</em><br>
<em>&gt; can argue with everything else, but you cannot argue with this) is that 
</em><br>
<em>&gt; we won't know the outcome until we try.
</em><br>
<p>People have been hacking about with 'stew of local dynamics' type
<br>
systems for at least two decades; look at Holland's classic work on
<br>
classifier systems, Kokinov's DUAL/AMBR work in the 90s, Edelman or
<br>
Calvin's neuromorphic projects (low and medium level respectively)
<br>
or Aleksander's recent human-cognition-inspired designs. Again, this
<br>
is not a new approach or a novel insight, though you probably have
<br>
novel specifics.
<br>
<p><em>&gt; (Notice that the availability of such a development environment would 
</em><br>
<em>&gt; not in any way preclude the kind of logic-based AI that is now the 
</em><br>
<em>&gt; favorite.  You could just as easily build such models.
</em><br>
<p>Ok, if it's that general, it's so general it doesn't actually
<br>
contribute any useful cognitive complexity and you're just designing
<br>
a language/IDE optimised for (your notions of) AI development work.
<br>
See past arguments about why this isn't a good use of time, unless
<br>
you can't think of anything better to do.
<br>
<p><em>&gt; The problem is that people who did so would be embarrassed into
</em><br>
<em>&gt; showing how their mechanisms interacted with real sensory and
</em><br>
<em>&gt; motor systems,
</em><br>
<p>You really do seem to be picking on a small clique of researchers,
<br>
who maintain an outdated, discredited approach that you've managed
<br>
to identify some obvious flaws in, and then generalised from this
<br>
easily-derided group to the entire AI research community.
<br>
<p><em>&gt; Finally, on the subject that we started with:  motivations of an AGI. 
</em><br>
<em>&gt; The class of system I am proposing would have a motivational/emotional 
</em><br>
<em>&gt; system that is distinct from the immediate goal stack.  Related, but not 
</em><br>
<em>&gt; be confused.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think we could build small scale examples of cognitive systems, insert 
</em><br>
<em>&gt; different kinds of M/E systems in them, and allow them to interact 
</em><br>
<em>&gt; with one another in simple virtual worlds.  We could study the stability 
</em><br>
<em>&gt; of the systems, their cooperative behavior towards one another, their 
</em><br>
<em>&gt; response to situations in which they faced threats, etc.  I think we 
</em><br>
<em>&gt; could look for telltale signs of breakdown, and perhaps even track their 
</em><br>
<em>&gt; &quot;thoughts&quot; to see what their view of the world was, and how that 
</em><br>
<em>&gt; interacted with their motivations.
</em><br>
<p>This part does not appear unreasonable; it seems similar to the
<br>
'experimental investigation of AGI goal system dynamics' that Ben
<br>
has historically been in favour of. It's just ridiculously unsafe
<br>
and overoptimistic in light of the dangers and difficulties
<br>
involved, in both the work itself and the generalisation.
<br>
&nbsp;
<br>
<em>&gt; And what we might well discover is that the disconnect between M/E 
</em><br>
<em>&gt; system and intellect is just as it appears to be in humans: humans
</em><br>
<em>&gt; are intellectual systems with aggressive M/E systems tacked on
</em><br>
<em>&gt; underneath.
</em><br>
<p>How well modularised the human brain is in that respect is an open
<br>
question, but the very hard problem of designing a stable Friendly
<br>
'M/E' system remains; this is not something you can do by trial and
<br>
error, attempting to do it by trial and error will probably get
<br>
everyone killed, and first-principles research into FAI has already
<br>
generating strong evidence (actually forget that, even 'Heuristics
<br>
and Biases' research has generated strong evidence) for human-like
<br>
cognitive systems being a bad starting point.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
Yahoo! Messenger - NEW crystal clear PC to PC calling worldwide with voicemail <a href="http://uk.messenger.yahoo.com">http://uk.messenger.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12391.html">Michael Wilson: "AGI motivations"</a>
<li><strong>Previous message:</strong> <a href="12389.html">Richard Loosemore: "Loosemore's Proposal [Was: Re: Agi motivations]"</a>
<li><strong>In reply to:</strong> <a href="12389.html">Richard Loosemore: "Loosemore's Proposal [Was: Re: Agi motivations]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12393.html">Richard Loosemore: "Re: Loosemore's Proposal"</a>
<li><strong>Reply:</strong> <a href="12393.html">Richard Loosemore: "Re: Loosemore's Proposal"</a>
<li><strong>Maybe reply:</strong> <a href="12396.html">Chris Capel: "Re: Loosemore's Proposal"</a>
<li><strong>Maybe reply:</strong> <a href="12400.html">rpwl@lightlink.com: "Re: Loosemore's Proposal"</a>
<li><strong>Maybe reply:</strong> <a href="12402.html">rpwl@lightlink.com: "Re: Loosemore's Proposal"</a>
<li><strong>Maybe reply:</strong> <a href="12406.html">Keith Henson: "Re: Loosemore's Proposal"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12390">[ date ]</a>
<a href="index.html#12390">[ thread ]</a>
<a href="subject.html#12390">[ subject ]</a>
<a href="author.html#12390">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:18 MST
</em></small></p>
</body>
</html>
