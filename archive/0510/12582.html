<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Loosemore's Proposal [Was: Re: Agi motivations]</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Loosemore's Proposal [Was: Re: Agi motivations]">
<meta name="Date" content="2005-10-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Loosemore's Proposal [Was: Re: Agi motivations]</h1>
<!-- received="Mon Oct 24 10:33:52 2005" -->
<!-- isoreceived="20051024163352" -->
<!-- sent="Mon, 24 Oct 2005 12:32:34 -0400" -->
<!-- isosent="20051024163234" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Loosemore's Proposal [Was: Re: Agi motivations]" -->
<!-- id="435D0CA2.7020404@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="BAY101-F3FC4CF7DE877550C49EE5AC740@phx.gbl" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Loosemore's%20Proposal%20[Was:%20Re:%20Agi%20motivations]"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Mon Oct 24 2005 - 10:32:34 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12583.html">Michael Wilson: "Re: Loosemore's Proposal"</a>
<li><strong>Previous message:</strong> <a href="12581.html">Richard Loosemore: "Re: AGI motivations (Sidetrack on Neuromorphic Engineering)"</a>
<li><strong>In reply to:</strong> <a href="12566.html">Michael Vassar: "agi motivations (was Re: AI debate at San Jose State U.)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12583.html">Michael Wilson: "Re: Loosemore's Proposal"</a>
<li><strong>Reply:</strong> <a href="12583.html">Michael Wilson: "Re: Loosemore's Proposal"</a>
<li><strong>Reply:</strong> <a href="12607.html">Michael Vassar: "RE: Loosemore's Proposal [Was: Re: Agi motivations]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12582">[ date ]</a>
<a href="index.html#12582">[ thread ]</a>
<a href="subject.html#12582">[ subject ]</a>
<a href="author.html#12582">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
[Consolidated reply to Michael Vassar, Michael Wilson, Woody Long, 
<br>
mungojelly and Olie Lamb...]
<br>
<p>[See also the separate sidetracks on &quot;Uploading&quot; and &quot;Neuromorphic 
<br>
Engineering&quot;].
<br>
<p>Please forgive the dense nature of this post.  Long, but dense.
<br>
<p><p>1) &quot;Prove&quot; that an AGI will be friendly?  Proofs are for mathematicians. 
<br>
&nbsp;&nbsp;I consider the use of the word &quot;proof,&quot; about the behavior of an AGI, 
<br>
as on the same level of validity as the use of the word &quot;proof&quot; in 
<br>
statements about evolutionary proclivities, for example &quot;Prove that no 
<br>
tree could ever evolve, naturally, in such a way that it had a red 
<br>
smiley face depicted on every leaf.&quot;  Talking about proofs of 
<br>
friendliness would be a fundamental misunderstanding of the role of the 
<br>
word &quot;proof&quot;.  We have enough problems with creationists and intelligent 
<br>
design freaks abusing the word, without us getting confused about it too.
<br>
<p>If anyone disagrees with this, it is important to answer certain 
<br>
objections.  Do not simply assert that proof is possible, give some 
<br>
reason why we should believe it to be so.  In order to do this, you have 
<br>
to give some coherent response to the arguments I previously set out (in 
<br>
which the Complex Systems community asked you to explain why AGI systems 
<br>
would be exempt from the empirical regularities they have observed).
<br>
<p>2) Since proof is impossible, the next best thing is a solid set of 
<br>
reasons to believe in friendliness of a particular design.  I will 
<br>
quickly sketch how I think this will come about.
<br>
<p>First, many people have talked as if building a &quot;human-like&quot; AGI would 
<br>
be very difficult.  I think that this is a mistake, for the following 
<br>
reasons.
<br>
<p>I think that what has been going on in the AI community for the last 
<br>
couple of decades is a prolonged bark up the wrong tree, and that this 
<br>
has made our lives more difficult than it should be.
<br>
<p>Specifically, I think that we (the early AI researchers) started from 
<br>
the observation of certain *high-level* reasoning mechanisms that are 
<br>
observable in the human mind, and generalized to the idea that these 
<br>
mechanisms could be the foundational mechanisms of a thinking system. 
<br>
The problem is that when we (as practitioners of philosophical logic) 
<br>
get into discussions about the amazing way in which &quot;All Men Are Mortal&quot; 
<br>
can be combined with &quot;Socrates is a Man&quot; to yield the conclusion 
<br>
&quot;Socrates is Mortal&quot;, we are completely oblivious to the fact that a 
<br>
huge piece of cognitive apparatus is sitting there, under the surface, 
<br>
allowing us to relate words like &quot;all&quot; and &quot;mortal&quot; and &quot;Socrates&quot; and 
<br>
&quot;men&quot; to things in the world, and to one another, and we are also 
<br>
missing the fact that there are vast numbers of other conclusions that 
<br>
this cognitive apparatus arrives at, on a moment by moment basis, that 
<br>
are extremely difficult to squeeze into the shape of a syllogism.  In 
<br>
other words, you have this enormous cognitive mechanism, coming to 
<br>
conclusions about the world all the time, and then it occasionally comes 
<br>
to conclusions using just *one*, particularly clean, little subcomponent 
<br>
of its array of available mechanisms, and we naively seize upon this 
<br>
subcomponent and think that *that* is how the whole thing operates.
<br>
<p>By itself, this argument against the &quot;logical&quot; approach to AI might only 
<br>
be a feeling, so we would then have to divide into two camps and each 
<br>
pursue our own vision of AI until one of us succeeded.
<br>
<p>However, the people on my side of the divide have made our arguments 
<br>
concrete enough that we can now be more specific about the problem, as 
<br>
follows.
<br>
<p>What we say is this.  The logic approach is bad because it starts with 
<br>
presumptions about the local mechanisms of the system and then tries to 
<br>
extend that basic design out until the system can build its own new 
<br>
knowledge, and relate its fundamental concepts to the sensorimotor 
<br>
signals that connect it to the outside world.... and from our experience 
<br>
with complex systems we know that that kind of backwards design approach 
<br>
will usually mean that the systems you design will partially work but 
<br>
always get into trouble the further out you try to extend them.  Because 
<br>
of the complex-systems disconnect between local and global, each time 
<br>
you start with preconceived notions about the local, you will find that 
<br>
the global behavior never quite matches up with what you want it to be.
<br>
<p>So in other words, our criticism is *not* that you should be looking for 
<br>
nebulous or woolly-headed &quot;emergent&quot; properties that explain cognition 
<br>
-- that kind of &quot;emergence&quot; is a red herring -- instead, you should be 
<br>
noticing that the hardest part of your implementation is always the 
<br>
learning and grounding aspect of the system.  Everything looks good on a 
<br>
small, local scale (especially if you make your formalism extremely 
<br>
elaborate, to deal with all the nasty little issues that arise) but it 
<br>
never scales properly.  In fact, some who take the logical approach will 
<br>
confess that they still haven't thought much about exactly how learning 
<br>
happens ... they have postponed that one.
<br>
<p>This is exactly what has been happening in AI research.  And it has been 
<br>
going on for, what, 20 years now?  Plenty of theoretical analysis.  Lots 
<br>
of systems that do little jobs a little tiny bit better than before.  A 
<br>
few systems that are designed to appear, to a naive consumer, as though 
<br>
they are intelligent (all the stuff coming out of Japan).  But overall, 
<br>
stagnation.
<br>
<p><p>So now, if this analysis is correct, what should be done?
<br>
<p>The alternative is to do something that has never been tried.
<br>
<p>Build a development environment that allowed rapid construction of large 
<br>
numbers of different systems, so we can start to empirically study the 
<br>
effects of changing the local mechanisms.  We should try cognitively- 
<br>
inspired mechanisms at the local level, but adapt them according to what 
<br>
makes them globally stable.  The point is not to presuppose what the 
<br>
local mechanisms are, but to use what we know of human cognition to get 
<br>
mechanisms that are in the right ballpark, then experimentally adjust 
<br>
them to find out under what conditions they are both stable and doing 
<br>
the things we want them to.
<br>
<p>I have been working on a set of candidate mechanisms for years. And also 
<br>
working on the characteristics of a software development environment 
<br>
that would allow this rapid construction of systems.  There is no hiding 
<br>
the fact that this would be a big project, but I believe it would 
<br>
produce a software tool that all researchers could use to quickly create 
<br>
systems that they could study, and by having a large number of people 
<br>
attacking it from different angles, progress would be rapid.
<br>
<p>What I think would happen if we tried this approach is that we would 
<br>
find ourselves not needing enormous complexity after all.  This is just 
<br>
a hunch, I agree, but I offer it as no more than that:  we cannot 
<br>
possibly know, until we try such an approach, if we find a quagmire or 
<br>
an easy sail to the finish.
<br>
<p>But I can tell you this:  we have never tried such an approach before, 
<br>
and the one thing that we do know from the complex systems research (you 
<br>
can argue with everything else, but you cannot argue with this) is that 
<br>
we won't know the outcome until we try.
<br>
<p>(Notice that the availability of such a development environment would 
<br>
not in any way preclude the kind of logic-based AI that is now the 
<br>
favorite.  You could just as easily build such models.  The problem is 
<br>
that people who did so would be embarrassed into showing how their 
<br>
mechanisms interacted with real sensory and motor systems, and how they 
<br>
acquired their higher level knowledge from primitives.... and that might 
<br>
be a problem because in a side by side comparison I think it would be 
<br>
finally obvious that the approach just simply did not work.  Again 
<br>
though, just by hunch.  I want the development environment to become 
<br>
available so we can do such comparisons, and stop philosophizing about it.)
<br>
<p>Finally, on the subject that we started with:  motivations of an AGI. 
<br>
The class of system I am proposing would have a motivational/emotional 
<br>
system that is distinct from the immediate goal stack.  Related, but not 
<br>
be confused.
<br>
<p>I think we could build small scale examples of cognitive systems, insert 
<br>
&nbsp;&nbsp;different kinds of M/E systems in them, and allow them to interact 
<br>
with one another in simple virtual worlds.  We could study the stability 
<br>
of the systems, their cooperative behavior towards one another, their 
<br>
response to situations in which they faced threats, etc.  I think we 
<br>
could look for telltale signs of breakdown, and perhaps even track their 
<br>
&quot;thoughts&quot; to see what their view of the world was, and how that 
<br>
interacted with their motivations.
<br>
<p>And what we might well discover is that the disconnect between M/E 
<br>
system and intellect is just as it appears to be in humans:  humans are 
<br>
intellectual systems with aggressive M/E systems tacked on underneath. 
<br>
They don't need the aggression (it was just useful during evolution), 
<br>
and without it they become immensely stable.
<br>
<p>I think that we could also understand the nature of the &quot;attachment&quot; 
<br>
mechanisms that make human beings have irrational fondness for one 
<br>
another, and for a species as a whole, and incorporate that in a design. 
<br>
&nbsp;&nbsp;I think we could stud the effects of that mechanism, and come to be 
<br>
sure of its stability.
<br>
<p>And, at the end of the day, I think we will come to understand the 
<br>
nature of M/E systems so well that we will be able to say with a fair 
<br>
degree of certainty that the more knowledge an AGI has, the more it 
<br>
tends to understand the need for cooperation.  I think we might (just 
<br>
might) discover that we could trust such systems.
<br>
<p><p>But we have to experiment to find out, and experiment in a way that 
<br>
nobody has ever done before.
<br>
<p><p>Richard Loosemore.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12583.html">Michael Wilson: "Re: Loosemore's Proposal"</a>
<li><strong>Previous message:</strong> <a href="12581.html">Richard Loosemore: "Re: AGI motivations (Sidetrack on Neuromorphic Engineering)"</a>
<li><strong>In reply to:</strong> <a href="12566.html">Michael Vassar: "agi motivations (was Re: AI debate at San Jose State U.)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12583.html">Michael Wilson: "Re: Loosemore's Proposal"</a>
<li><strong>Reply:</strong> <a href="12583.html">Michael Wilson: "Re: Loosemore's Proposal"</a>
<li><strong>Reply:</strong> <a href="12607.html">Michael Vassar: "RE: Loosemore's Proposal [Was: Re: Agi motivations]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12582">[ date ]</a>
<a href="index.html#12582">[ thread ]</a>
<a href="subject.html#12582">[ subject ]</a>
<a href="author.html#12582">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
