<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AGI motivations</title>
<meta name="Author" content="Michael Vassar (michaelvassar@hotmail.com)">
<meta name="Subject" content="Re: AGI motivations">
<meta name="Date" content="2005-10-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AGI motivations</h1>
<!-- received="Sun Oct 23 17:28:02 2005" -->
<!-- isoreceived="20051023232802" -->
<!-- sent="Sun, 23 Oct 2005 19:27:57 -0400" -->
<!-- isosent="20051023232757" -->
<!-- name="Michael Vassar" -->
<!-- email="michaelvassar@hotmail.com" -->
<!-- subject="Re: AGI motivations" -->
<!-- id="BAY101-F36E2E256A05D636F3A96C6AC740@phx.gbl" -->
<!-- inreplyto="20051023193638.78809.qmail@web26706.mail.ukl.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Vassar (<a href="mailto:michaelvassar@hotmail.com?Subject=Re:%20AGI%20motivations"><em>michaelvassar@hotmail.com</em></a>)<br>
<strong>Date:</strong> Sun Oct 23 2005 - 17:27:57 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12573.html">Michael Vassar: "RE: people breaking themselves (was &quot;Re: AGI motivations&quot;)"</a>
<li><strong>Previous message:</strong> <a href="12571.html">Pope Salmon the Lesser Mungojelly: "people breaking themselves     (was &quot;Re: AGI motivations&quot;)"</a>
<li><strong>In reply to:</strong> <a href="12569.html">Michael Wilson: "Re: AGI motivations"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12574.html">Michael Vassar: "Re: AGI motivations"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12572">[ date ]</a>
<a href="index.html#12572">[ thread ]</a>
<a href="subject.html#12572">[ subject ]</a>
<a href="author.html#12572">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michael Wilson said
<br>
<em>&gt;Yes, uploads occupy a small region of cognitive architecture space within a 
</em><br>
<em>&gt;larger region
</em><br>
<em>&gt;of 'human-like AGI designs'. However we can actually hit the narrower
</em><br>
<em>&gt;region semi-reliably if we can develop an accurate brain simulation
</em><br>
<em>&gt;and copy an actual human's brain structure into it. We cannot hit the
</em><br>
<em>&gt;larger safer region reliably by creating an AGI from scratch, at least not
</em><br>
<em>&gt;without an understanding of how the human brain works and how to
</em><br>
<em>&gt;build AGIs considerably in advance of what is needed for uploading
</em><br>
<em>&gt;and FAI respectively.
</em><br>
<p>That assertion appears plausible but unsubstantiated to me.  The 
<br>
understanding of human brain function required to build a relatively safe 
<br>
human-like AGI might be only trivially greater than that required to create 
<br>
an upload, while the scanning resolution required might be much lower.  It 
<br>
may be much simpler to make a mind that will reliably not attempt to 
<br>
transcend than to build one that can transcend safely.  One way to make such 
<br>
a mind is to upload the right person.  It may be that building a large 
<br>
number of moderately different neuromorphic AIs (possibly based on medium 
<br>
res scans of particular brains, scans inadequate for uploading, followed by 
<br>
repair via software clean-up) in AI boxes and testing them under a limited 
<br>
range of conditions similar to what they will actually face in the world is 
<br>
easier than uploading a particular person.
<br>
<p><em>&gt; &gt; Surely there are regions within the broader category which are
</em><br>
<em>&gt; &gt; safer than the particular region containing human uploads.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Almost certainly. We don't know where they are or how to reliably
</em><br>
<em>&gt;hit them with an implementation, and unlike say uploading or FAI
</em><br>
<em>&gt;there is no obvious research path to gaining this capability.
</em><br>
<p>We know some ways for reliably hitting them, such as &quot;don't implement 
<br>
transhuman capabilities&quot;.
<br>
<p><em>&gt;you know how to reliably build a safe, human-like AGI, feel free
</em><br>
<em>&gt;to say how.
</em><br>
<p>Upload an ordinary person with no desire to become a god.  That's one way.  
<br>
Another may be to build an AGI that is such a person.  How do you know what 
<br>
they want?  Ask them.  Detecting that a simulation is lying with high 
<br>
reliability, and detecting its emotions, should not be difficult.
<br>
<p><em>&gt;Indeed, if someone did manage this, my existing model of AI
</em><br>
<em>&gt;development would be shown to be seriously broken.
</em><br>
<p>I suspect that it may be.  Since you haven't shared your model I have no way 
<br>
to evaluate it, but a priori, given that most models of AI development 
<br>
,ncluding most models held by certified geniuses,are broken, I assume yours 
<br>
is too.  I'd be happy to work with you on improving it, and that seems to me 
<br>
to be the sort of thing this site is for, but destiny-star may be more 
<br>
urgent.
<br>
It's best to predict well enough to create, then stop predicting and create. 
<br>
&nbsp;&nbsp;Trouble is, it's hard to know when your predictions are good enough.
<br>
<p><em>&gt; &gt;&gt; To work out from first principles (i.e. reliably) whether
</em><br>
<em>&gt; &gt;&gt; you should trust a somewhat-human-equivalent AGI you'd need nearly as
</em><br>
<em>&gt; &gt;&gt; much theory, if not more, than you'd need to just build an FAI in the
</em><br>
<em>&gt; &gt;&gt; first place.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; That depends on what you know, and on what its cognitive capacities are.
</em><br>
<em>&gt; &gt; There should be ways of confidently predicting that a given machine does
</em><br>
<em>&gt; &gt; not have any transhuman capabilities other than a small set of specified
</em><br>
<em>&gt; &gt; ones which are not sufficient for transhuman persuasion or transhuman
</em><br>
<em>&gt; &gt; engineering.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Why 'should' there be an easy way to do this? In my experience predicting
</em><br>
<em>&gt;what capabilities a usefully general design will actually have is pretty
</em><br>
<em>&gt;hard, whether you're trying to prove positives or negatives.
</em><br>
<p>We do it all the time with actual humans.  For a chunk of AI design space 
<br>
larger than &quot;uploads&quot; AGIs are just humans.  Whatever advantages they have 
<br>
will only be those you have given them, probably including speed and 
<br>
moderately fine-grained self-awareness (or you having moderately 
<br>
fine-grained awareness of them).  Predicting approximately what new 
<br>
capabilities a human will have when you make a small change to their 
<br>
neurological hardware can be difficult or easy depending on how well you 
<br>
understand what you are doing, but small changes, that is, changes of 
<br>
magnitude comparable to the range of variation among the human baseline 
<br>
population will never create large and novel transhuman abilities, but lots 
<br>
of time and mere savant abilities may be really really useful.
<br>
<p><em>&gt; &gt; It should also be possible to ensure a human-like enough goal system
</em><br>
<em>&gt; &gt; that you can understand its motivation prior to recursive
</em><br>
<em>&gt; &gt; self-improvement.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Where is this 'should' coming from? If you know how to do this, tell
</em><br>
<em>&gt;the rest of us and claim your renown as the researcher who cracked a
</em><br>
<em>&gt;good fraction of the FAI problem.
</em><br>
<p>The &quot;should&quot; comes from the fact that the whole venture of attempting to 
<br>
build an organization to create a Friendly AI presupposed the solution to 
<br>
the problem of friendly human assistant has been solved without even needing 
<br>
to turn to the use of intrusive brain scans and neuro-chemical 
<br>
modifications.  What is the difference between trusting a human derived AI 
<br>
and trusting a human.  Either can be understood equally well by reading what 
<br>
they say, talking to them, etc.
<br>
<p><em>&gt;Unless you're simulating the brain at the neuron level (including keeping
</em><br>
<em>&gt;the propagation speed down to human levels) /and/ closely copying human
</em><br>
<em>&gt;brain organisation, you simply can't generalise from 'humans behave like
</em><br>
<em>&gt;this' to 'the AGI will behave like this'.
</em><br>
<p>Obviously you have to closely copy human brain organization.  You can 
<br>
experiment with different propoagation speeds like you can with new drugs 
<br>
without credible risk of UnFriendlyness, takeoff, or the acquisition of 
<br>
transhuman abilities.  I see no evidence that simulations must be at the 
<br>
neuron level, though its probably safest.  What you need is to be sure that 
<br>
your high level simulations don't produce more powerful outputs than the 
<br>
neuron level simulation would, which should be non-problematic for some of 
<br>
the brain.
<br>
Neuron level simulation is still not uploading.  Simulating pan-human 
<br>
complexity demands much less scanning than simulating human-specific neural 
<br>
features.
<br>
<p><em>&gt;Given this constraint on
</em><br>
<em>&gt;structure, your options for getting bootstrap /content/ into the AGI are
</em><br>
<em>&gt;(a) upload a human, (b) replicate the biological brain growth and human
</em><br>
<em>&gt;learning process (even more research, implementation complexity and
</em><br>
<em>&gt;technical difficulty and takes a lot of time) or
</em><br>
<p>We shouldn't be doing b, but it will be done and if it advanced faster than 
<br>
what we are doing we should utilize it.
<br>
<p><em>&gt;(c) use other algorithms
</em><br>
<em>&gt;unrelated to the way humans work to generate the seed complexity. The
</em><br>
<em>&gt;last option again discards any ability to make simple generalisations
</em><br>
<em>&gt;from human behaviour to the behaviour of your human-like AGI.
</em><br>
<p>Obviously.
<br>
<p><em>&gt;The second
</em><br>
<em>&gt;option introduces even more potential for things to go wrong (due to more
</em><br>
<em>&gt;design complexity)
</em><br>
<p>I'm not convinced of this.  It could be true, time will tell.
<br>
<p><em>&gt;and even if it works perfectly it will produce an
</em><br>
<em>&gt;arbitrary (and probably pretty damn strange) human-like personality, with
</em><br>
<em>&gt;no special guarantees of benevolence.
</em><br>
<p>No guarantees other than what you can infer from selecting a human of your 
<br>
choice from a large number of options, examining their brain and life 
<br>
history, and possibly slightly modifying neurochemistry or utilizing 
<br>
conditioning etc.  However, what guarantees of benevolence do we have from 
<br>
real humans?  Primarily the fact that an imminant singularity leaves humans 
<br>
with little basis for conflict.  FAI is an extremely desirable outcome for 
<br>
all of us, and only questionably not an optimal outcome for any given human.
<br>
<p><em>&gt;Actually I find it highly unlikely
</em><br>
<em>&gt;that any research group trying to do this would actually go all out on
</em><br>
<em>&gt;replicating the brain accurately without being tempted to meddle and
</em><br>
<em>&gt;'improve' the structure and/or content, but I digress.
</em><br>
<p>Depends on what tools they had available and what they wanted to do.
<br>
<p><em>&gt;Thus the first
</em><br>
<em>&gt;option, uploading, looks like the best option to me if you're going to
</em><br>
<em>&gt;insist on building a human-like AGI.
</em><br>
<p>I agree its the best choice, but it may not be easiest.
<br>
<p><em>&gt;Trying to prevent an AGI from self-modifying is the classic 'adversarial
</em><br>
<em>&gt;swamp' situation that CFAI correctly characterises as hopeless.
</em><br>
<p>You shouldn't try to prevent a non-human AGI from self-modifying, but 
<br>
adversarial swamps involving humans are NOT intractible, not to mention that 
<br>
you can still influence the human's preferences a LOT with simulate 
<br>
chemistry and total control of their environment.  I don't think this is as 
<br>
severe as the adversarial situation currently existing between SIAI and 
<br>
other AI development teams from whom SIAI withholds information.
<br>
<p><em>&gt;Any
</em><br>
<em>&gt;single point of failure in your technical isolation or human factors
</em><br>
<em>&gt;(i.e. AGI convinces a programmer to do something that allows it to
</em><br>
<em>&gt;write arbitrary code) will probably lead to seed AI. The task is more
</em><br>
<em>&gt;or less impossible even given perfect understanding, and perfect
</em><br>
<em>&gt;understanding is pretty unlikely to be present.
</em><br>
<p>Huh?  That's like saying that workplace drug policies are fundamentally 
<br>
impossible even given an unlimited selection of workers, perfect worker 
<br>
monitoring, and drugs of unknown effect some of which end the world when one 
<br>
person takes them.
<br>
<p><em>&gt; &gt; An AI which others can see the non-transparent thoughts of, and which
</em><br>
<em>&gt; &gt; others can manipulate the  preferences and desires of, is in some
</em><br>
<em>&gt; &gt; respects safer than a human, so long as those who control it are safe.
</em><br>
<em>&gt;
</em><br>
<em>&gt;This looks like a contradiction in terms. A 'non-transparent' i.e.
</em><br>
<em>&gt;'opaque' AGI is one that you /can't/ see the thoughts of, only at best
</em><br>
<em>&gt;high level and fakeable abstractions.
</em><br>
<p>You *can* see the non-transparent thoughts of a human to a significant 
<br>
degree, especially with neural scanning.  With perfect scanning you could do 
<br>
much better.
<br>
<p><em>&gt;The problem of understanding what
</em><br>
<em>&gt;a (realistic) 'human-like AGI' is thinking is equivalent to the problem
</em><br>
<em>&gt;of understanding what a human is thinking given a complete real-time
</em><br>
<em>&gt;brain scan; a challenge considerably harder than merely simulating the
</em><br>
<em>&gt;brain.
</em><br>
<p>I strongly disagree.  Reading thoughts is very difficult.  Reading emotions 
<br>
is not nearly so difficult, nor is making some reasonable inferences about 
<br>
the subject matter of thoughts.  Look at what Koch has already done with 
<br>
today's crude tools.
<br>
<p><em>&gt; &gt; Finally, any AI that doesn't know it is an AI or doesn't know about
</em><br>
<em>&gt; &gt; programming, formal logic, neurology, etc is safe until it learns
</em><br>
<em>&gt; &gt; those things.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I'll grant you that, but how is it going to be useful for FAI design
</em><br>
<em>&gt;if it doesn't know about these things?
</em><br>
<p>I can think of several ways.  Admittedly the use would be more limited.
<br>
<p><em>&gt;How do you propose to stop
</em><br>
<em>&gt;anyone from teaching a 'commercially available human-like AGI' these
</em><br>
<em>&gt;skills?
</em><br>
<p>Probably not possible at that point, but at that point you have to really 
<br>
really hurry anyway so some safety needs may be sacrificed.  Actually, 
<br>
probably possible but at immense cost, and probably not worth preparing for.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12573.html">Michael Vassar: "RE: people breaking themselves (was &quot;Re: AGI motivations&quot;)"</a>
<li><strong>Previous message:</strong> <a href="12571.html">Pope Salmon the Lesser Mungojelly: "people breaking themselves     (was &quot;Re: AGI motivations&quot;)"</a>
<li><strong>In reply to:</strong> <a href="12569.html">Michael Wilson: "Re: AGI motivations"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12574.html">Michael Vassar: "Re: AGI motivations"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12572">[ date ]</a>
<a href="index.html#12572">[ thread ]</a>
<a href="subject.html#12572">[ subject ]</a>
<a href="author.html#12572">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
