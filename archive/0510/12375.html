<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AGI motivations</title>
<meta name="Author" content="Michael Vassar (michaelvassar@hotmail.com)">
<meta name="Subject" content="Re: AGI motivations">
<meta name="Date" content="2005-10-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AGI motivations</h1>
<!-- received="Sun Oct 23 12:01:22 2005" -->
<!-- isoreceived="20051023180122" -->
<!-- sent="Sun, 23 Oct 2005 14:01:20 -0400" -->
<!-- isosent="20051023180120" -->
<!-- name="Michael Vassar" -->
<!-- email="michaelvassar@hotmail.com" -->
<!-- subject="Re: AGI motivations" -->
<!-- id="BAY101-F164CE87C36E17A41E88636AC740@phx.gbl" -->
<!-- inreplyto="20051023153725.98226.qmail@web26702.mail.ukl.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Vassar (<a href="mailto:michaelvassar@hotmail.com?Subject=Re:%20AGI%20motivations"><em>michaelvassar@hotmail.com</em></a>)<br>
<strong>Date:</strong> Sun Oct 23 2005 - 12:01:20 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12376.html">Michael Wilson: "Re: AGI motivations"</a>
<li><strong>Previous message:</strong> <a href="12374.html">Michael Wilson: "Re: AGI motivations"</a>
<li><strong>In reply to:</strong> <a href="12374.html">Michael Wilson: "Re: AGI motivations"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12376.html">Michael Wilson: "Re: AGI motivations"</a>
<li><strong>Reply:</strong> <a href="12376.html">Michael Wilson: "Re: AGI motivations"</a>
<li><strong>Reply:</strong> <a href="12378.html">Pope Salmon the Lesser Mungojelly: "people breaking themselves     (was &quot;Re: AGI motivations&quot;)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12375">[ date ]</a>
<a href="index.html#12375">[ thread ]</a>
<a href="subject.html#12375">[ subject ]</a>
<a href="author.html#12375">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michae Wilson said
<br>
<p><em>&gt; &gt; It is possible that a non-Transhuman AI with a human-like motivational
</em><br>
<em>&gt; &gt; system could be helpful in designing and implementing an analytically
</em><br>
<em>&gt; &gt; tractable motivational system.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Well sure, for the same reason that it would be great to have some
</em><br>
<em>&gt;intelligence-augmented humans or human uploads around to help design
</em><br>
<em>&gt;the FAI. But actually trying to build one would be incredibly risky and
</em><br>
<em>&gt;unlikely to work, even more so than independent IA and uploading
</em><br>
<em>&gt;projects, so this observation isn't of much practical utility. What I
</em><br>
<em>&gt;think /may/ be both useful and practical is some special purpose tools
</em><br>
<em>&gt;based on constrained, infrahuman AGI.
</em><br>
<p>I don't see the reasoning behind this.  Trying to build a human derived AI 
<br>
is the general case of which building a human upload is a special case.  
<br>
Surely there are regions within the broader category which are safer than 
<br>
the particular region containing human uploads.  It's definitely not 
<br>
something that we should try to do in the near future or plan on doing, but 
<br>
if someone else were to develop human-derived AIs and put them on the 
<br>
market, for instance, it would be foolish not to use them to assist your 
<br>
work, which would at that point be incredibly urgent and require the use of 
<br>
all possible tools that could be used to accelerate it.
<br>
<p><em>&gt; &gt; A priori there is no more reason to trust such an AI than to trust a
</em><br>
<em>&gt; &gt; human, though there could easily be conditions which would make it
</em><br>
<em>&gt; &gt; more or less worthy of such trust.
</em><br>
<em>&gt;
</em><br>
<em>&gt;This immediately sets off warning bells simply because humans have a
</em><br>
<em>&gt;lot of evolved cognitive machinery for evaluating 'trust', and strong
</em><br>
<em>&gt;intuitive notions of how 'trust' works, which would utterly fail (and
</em><br>
<em>&gt;hence be worse than useless) if the AGI has any significant deviations
</em><br>
<em>&gt;from human cognitive architecture (which would be effectively
</em><br>
<em>&gt;unavoidable).
</em><br>
<p>I'm skeptical about the diferences being unavoidabld.  Surely true given 
<br>
current computers, surely false given uploads.  There is a substantial space 
<br>
of possible AGI design around &quot;uploads&quot;.
<br>
<p><em>&gt;To work out from first principles (i.e. reliably) whether
</em><br>
<em>&gt;you should trust a somewhat-human-equivalent AGI you'd need nearly as
</em><br>
<em>&gt;much theory, if not more, than you'd need to just build an FAI in the
</em><br>
<em>&gt;first place.
</em><br>
<p>That depends on what you know, and on what its cognitive capacities are.  
<br>
There should be ways of confidently predicting that a given machine does not 
<br>
have any transhuman capabilities other than a small set of specified ones 
<br>
which are not sufficient for transhuman persuasion or transhuman 
<br>
engineering.  It should also be possible to ensure a human-like enough goal 
<br>
system that you can understand its motivation prior to recursive 
<br>
self-improvement.
<br>
<p><em>&gt;To be sure of building a human-like AI, you'd need to either very closely
</em><br>
<em>&gt;follow neurophysiology (i.e. build an accurate brain simulation, which we
</em><br>
<em>&gt;don't have the data or the hardware for yet) or use effectively the same
</em><br>
<em>&gt;basic theory you'd need to build an FAI to ensure that the new AGI will
</em><br>
<em>&gt;reliably show human-like behaviour. If you have the technology to do the
</em><br>
<em>&gt;latter, you might as well just upload people; it's less risky than
</em><br>
<em>&gt;trying to build a human-like AGI (though probably more risky than building
</em><br>
<em>&gt;an FAI).
</em><br>
<p>I assume you mean the former, not the latter.  Uploading a particular person 
<br>
might be (probably is) more difficult than the more general task of 
<br>
producing an (not necessarily perfectly) accurate simulation of a generic 
<br>
human brain which predictably displays no transhuman capabilities and only 
<br>
human capabilities that are rather easily observed (especially with the 
<br>
partial transparency that comes from being an AI).  It's also more desirable 
<br>
of course, but multiple contingencies should be considered.
<br>
<p><em>&gt;Yes, noting of course that it's extremely difficult to build a
</em><br>
<em>&gt;'human-like AI' that isn't already an Unfriendly seed AI.
</em><br>
<p>I strongly disagree with the above, except in so far as it's extremely 
<br>
difficult to build any &quot;human-like AI&quot;.  An AI that doesn't have access to 
<br>
the computer it is running on is just a person.  An AI which others can see 
<br>
the non-transparent thoughts of, and which others can manipulate the 
<br>
preferences and desires of, is in some respects safer than a human, so long 
<br>
as those who control it are safe.  Finally, any AI that doesn't know it is 
<br>
an AI or doesn't know about programming, formal logic, neurology, etc is 
<br>
safe until it learns those things.  The majority of humans could upload 
<br>
without being seed AIs.  A small fraction of them would try to mess with 
<br>
their minds and break.  Of those, only a competent minority would actually 
<br>
be seed AIs.  However, it is unfortunately the case that AIs that could be 
<br>
seed AIs themselves are much more valuable to an AGI program than AIs which 
<br>
could not be seed AIs themselves.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12376.html">Michael Wilson: "Re: AGI motivations"</a>
<li><strong>Previous message:</strong> <a href="12374.html">Michael Wilson: "Re: AGI motivations"</a>
<li><strong>In reply to:</strong> <a href="12374.html">Michael Wilson: "Re: AGI motivations"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12376.html">Michael Wilson: "Re: AGI motivations"</a>
<li><strong>Reply:</strong> <a href="12376.html">Michael Wilson: "Re: AGI motivations"</a>
<li><strong>Reply:</strong> <a href="12378.html">Pope Salmon the Lesser Mungojelly: "people breaking themselves     (was &quot;Re: AGI motivations&quot;)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12375">[ date ]</a>
<a href="index.html#12375">[ thread ]</a>
<a href="subject.html#12375">[ subject ]</a>
<a href="author.html#12375">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:16 MST
</em></small></p>
</body>
</html>
