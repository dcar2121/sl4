<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Loosemore's Proposal [Was: Re: Agi motivations]</title>
<meta name="Author" content="Michael Vassar (michaelvassar@hotmail.com)">
<meta name="Subject" content="RE: Loosemore's Proposal [Was: Re: Agi motivations]">
<meta name="Date" content="2005-10-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Loosemore's Proposal [Was: Re: Agi motivations]</h1>
<!-- received="Tue Oct 25 13:27:29 2005" -->
<!-- isoreceived="20051025192729" -->
<!-- sent="Tue, 25 Oct 2005 15:27:26 -0400" -->
<!-- isosent="20051025192726" -->
<!-- name="Michael Vassar" -->
<!-- email="michaelvassar@hotmail.com" -->
<!-- subject="RE: Loosemore's Proposal [Was: Re: Agi motivations]" -->
<!-- id="BAY101-F3D059E063D917CFC32B88AC760@phx.gbl" -->
<!-- inreplyto="435D0CA2.7020404@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Vassar (<a href="mailto:michaelvassar@hotmail.com?Subject=RE:%20Loosemore's%20Proposal%20[Was:%20Re:%20Agi%20motivations]"><em>michaelvassar@hotmail.com</em></a>)<br>
<strong>Date:</strong> Tue Oct 25 2005 - 13:27:26 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12608.html">Michael Wilson: "Re: Loosemore's Proposal"</a>
<li><strong>Previous message:</strong> <a href="12606.html">Russell Wallace: "Re: Loosemore's Proposal"</a>
<li><strong>In reply to:</strong> <a href="12582.html">Richard Loosemore: "Loosemore's Proposal [Was: Re: Agi motivations]"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12607">[ date ]</a>
<a href="index.html#12607">[ thread ]</a>
<a href="subject.html#12607">[ subject ]</a>
<a href="author.html#12607">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt;1) &quot;Prove&quot; that an AGI will be friendly?  Proofs are for mathematicians. 
</em><br>
<p>Computer scientists *are* mathematicians.
<br>
<p><p><em>&gt;Do not simply assert that proof is possible, give some reason why we should 
</em><br>
<em>&gt; &gt;believe it to be so.
</em><br>
<p>Do you have a reason for believing it to not be so?  It doesn't appear to me 
<br>
that you do.  It seems to me that you credibly assert that some systems are 
<br>
complex, and that some intelligent systems are complex (all known examples 
<br>
in fact), but offer no evidence that all possible intelligent systems are 
<br>
complex.
<br>
<p><p><em>&gt;Specifically, I think that we (the early AI researchers) started from the 
</em><br>
<em>&gt; &gt;observation of certain *high-level* reasoning mechanisms that are 
</em><br>
<em>&gt;observable &gt;in the human mind, and generalized to the idea that these 
</em><br>
<em>&gt;mechanisms could be &gt;the foundational mechanisms of a thinking system. The 
</em><br>
<em>&gt;problem is that when we &gt;(as practitioners of philosophical logic) get into 
</em><br>
<em>&gt;discussions about the &gt;amazing way in which &quot;All Men Are Mortal&quot; can be 
</em><br>
<em>&gt;combined with &quot;Socrates is a &gt;Man&quot;to yield the conclusion &quot;Socrates is 
</em><br>
<em>&gt;Mortal&quot;, we are completely oblivious &gt;to the fact that a huge piece of 
</em><br>
<em>&gt;cognitive apparatus is sitting there, under &gt;the surface, allowing us to 
</em><br>
<em>&gt;relate words like &quot;all&quot; and &quot;mortal&quot; and &gt;&quot;Socrates&quot; and &quot;men&quot; to things in 
</em><br>
<em>&gt;the world, and to one another, and we are &gt;also missing the fact that there 
</em><br>
<em>&gt;are vast numbers of other conclusions that &gt;this cognitive apparatus 
</em><br>
<em>&gt;arrives at, on a moment by moment basis, that are &gt;extremely difficult to 
</em><br>
<em>&gt;squeeze into the shape of a syllogism. 
</em><br>
<p><p>Richard, I am among the most polite and far from the most knowledgable 
<br>
member of this community, but reading something like this can only elicit a 
<br>
groan from me.  Of course what you say is true.  What a cliché.  One could 
<br>
learn this from watching the Discovery Channel.  Next I expect you to point 
<br>
out that an AGI doesn't need god to give it an immortal soul and to support 
<br>
your claim by challenging biblical infallibility.
<br>
<p><p><em>&gt;In other words, you have this enormous cognitive mechanism, coming to 
</em><br>
<em>&gt; &gt;conclusions about the world all the time, and then it occasionally comes 
</em><br>
<em>&gt;to &gt;conclusions using just *one*, particularly clean, little subcomponent 
</em><br>
<em>&gt;of its &gt;array of available mechanisms, and we naively seize upon this 
</em><br>
<em>&gt;subcomponent &gt;and think that *that* is how the whole thing operates.
</em><br>
<p><p>Funny, &quot;Intelligence Doesn't Fit on a T-Shirt&quot; is how Eliezer would have 
<br>
summarized the above ten or eleven years ago.
<br>
Can you seriously imagine Ben Goertzel would have spent the amount of time 
<br>
discussing AI with Eli that Ben has spent if Eli didn't even know these 
<br>
sorts of basics.
<br>
<p><p><em>&gt;Humans are intellectual systems with aggressive M/E systems tacked on 
</em><br>
<em>&gt; &gt;underneath. They don't need the aggression (it was just useful during 
</em><br>
<em>&gt; &gt;evolution), and without it they become immensely stable.
</em><br>
<p><p>Huh?  Humans are stable without aggression?  Ask a neuroscientist.  Anyway, 
<br>
UFAI doesn't mean aggression.  The archives make it very clear what UFAI 
<br>
means.
<br>
<p><p><em>&gt;I think that we could also understand the nature of the &quot;attachment&quot; 
</em><br>
<em>&gt; &gt;mechanisms that make human beings have irrational fondness for one 
</em><br>
<em>&gt;another, &gt;and for a species as a whole, and incorporate that in a design.  
</em><br>
<em>&gt;I think we &gt;could stud the effects of that mechanism, and come to be sure 
</em><br>
<em>&gt;of its &gt;stability. And, at the end of the day, I think we will come to 
</em><br>
<em>&gt;understand the nature of &gt;M/E systems so well that we will be able to say 
</em><br>
<em>&gt;with a fair degree of &gt;certainty that the more knowledge an AGI has, the 
</em><br>
<em>&gt;more it tends to understand &gt;the need for cooperation.  I think we might 
</em><br>
<em>&gt;(just might) discover that we &gt;could trust such systems.
</em><br>
<p>Yes we could trust them.  Could we trust their creations X degrees removed?  
<br>
The problem is that such AIs could be turned into Seed AIs, and we couldn't 
<br>
trust the derived seed AIs.  The above two paragraphs are very sensible if 
<br>
you assume that singularity is impossible, or if you plan to use non-seed 
<br>
AIs to prevent singularity.  Otherwise its suicide.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12608.html">Michael Wilson: "Re: Loosemore's Proposal"</a>
<li><strong>Previous message:</strong> <a href="12606.html">Russell Wallace: "Re: Loosemore's Proposal"</a>
<li><strong>In reply to:</strong> <a href="12582.html">Richard Loosemore: "Loosemore's Proposal [Was: Re: Agi motivations]"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12607">[ date ]</a>
<a href="index.html#12607">[ thread ]</a>
<a href="subject.html#12607">[ subject ]</a>
<a href="author.html#12607">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
