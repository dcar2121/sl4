<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Augmenting humans is a better way</title>
<meta name="Author" content="Brian Atkins (brian@posthuman.com)">
<meta name="Subject" content="Re: Augmenting humans is a better way">
<meta name="Date" content="2001-07-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Augmenting humans is a better way</h1>
<!-- received="Sat Jul 28 13:20:13 2001" -->
<!-- isoreceived="20010728192013" -->
<!-- sent="Sat, 28 Jul 2001 04:32:46 -0400" -->
<!-- isosent="20010728083246" -->
<!-- name="Brian Atkins" -->
<!-- email="brian@posthuman.com" -->
<!-- subject="Re: Augmenting humans is a better way" -->
<!-- id="3B6278AE.FE914250@posthuman.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="000701c11707$18100ec0$388d90c6@oemcomputer" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brian Atkins (<a href="mailto:brian@posthuman.com?Subject=Re:%20Augmenting%20humans%20is%20a%20better%20way"><em>brian@posthuman.com</em></a>)<br>
<strong>Date:</strong> Sat Jul 28 2001 - 02:32:46 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1861.html">James Higgins: "Re: Augmenting humans is a better way"</a>
<li><strong>Previous message:</strong> <a href="1859.html">Eliezer S. Yudkowsky: "Re: augmenting humans is difficult and slow..."</a>
<li><strong>In reply to:</strong> <a href="1857.html">Jack Richardson: "Re: Augmenting humans is a better way"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1861.html">James Higgins: "Re: Augmenting humans is a better way"</a>
<li><strong>Reply:</strong> <a href="1861.html">James Higgins: "Re: Augmenting humans is a better way"</a>
<li><strong>Reply:</strong> <a href="1864.html">Ben Goertzel: "RE: Augmenting humans is a better way"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1860">[ date ]</a>
<a href="index.html#1860">[ thread ]</a>
<a href="subject.html#1860">[ subject ]</a>
<a href="author.html#1860">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I'm just going to go through all three messages quoted here and
<br>
respond...
<br>
<p>Jack Richardson wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Evan,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Having written the original post advocating a consideration of human
</em><br>
<em>&gt; augmentation as a possible better alternative, I'm pleased to see the point
</em><br>
<em>&gt; being made that today's concentrated activity, primarily for medical
</em><br>
<em>&gt; reasons, to learn how to augment humans is preparing the way to human
</em><br>
<em>&gt; participation in the onset of the Singularity.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; No doubt, as Ben has pointed out, there are many technical problems to be
</em><br>
<p>Do not forget the political/societal problems when it comes to what you
<br>
are describing here. Our current government gets the willies just trying
<br>
to think about stem cells. Do you think it will be so quick to adapt that
<br>
it will be able to come to grips with radical biotech and other bio-
<br>
enhancements later on such as inloading via nanotech that would most
<br>
likely be required to achieve a Singularity by strictly (trans)human actors?
<br>
<p><em>&gt; solved along the way, but the starting point is way ahead of where a seed AI
</em><br>
<em>&gt; approach is starting. Progress in the area of human augmentation will take
</em><br>
<em>&gt; many forms and likely will involve thousands of steps. But it will be very
</em><br>
<p>Each of which must get the ok of the FDA and the government &quot;ethicists&quot;
<br>
<p><em>&gt; measureable and we will know, from year to year, just how much progress is
</em><br>
<em>&gt; being made. I'm not so sure the same will be true of the seed AI approach.
</em><br>
<p>At the current rate I'm sorry to tell you that progress in biotech is not
<br>
keeping up with Moore's Law (sorry Higgins :-). Sure we can see a few
<br>
bit and pieces of the puzzle falling into place, but I don't see a doubling
<br>
in human-modifying abilities yearly. Heck, it would probably take more
<br>
than a few YEARS just to get the FDA to approve a radical new therapy.
<br>
<p>Meanwhile, we have a pretty guaranteed timeline of computing power showing
<br>
up that will deliver us human-level computational power by 2010 at the
<br>
latest. You'll be lucky if they can cure most cancer and get it approved by
<br>
the FDA by then- don't even think about any form of intelligence-enhancement
<br>
except maybe just maybe some experiments on a few anonymous rich babies
<br>
who won't grow up in time to affect an AI-based Singularity.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Furthermore, as a group of humans most interested in this development, I
</em><br>
<em>&gt; would argue that most of us would want to ensure that we were all active
</em><br>
<em>&gt; participants in the experiences the Singularity would make available to us.
</em><br>
<em>&gt; It may be true that the Singularity might leave us all behind anyway, but I
</em><br>
<em>&gt; would like to think that we could construct it in such a way that augmenting
</em><br>
<em>&gt; humans would be its primary task. If we were fairly far along at the time of
</em><br>
<em>&gt; onset, that would make its task that much easier.
</em><br>
<p>I know you all (and even those who don't know about the Singularity yet)
<br>
want to have a say in how it turns out, and to guarantee if possible a
<br>
positive result for yourself. Me too. However consider a couple of points:
<br>
<p>1. If you believe that the Singularity has great potential to be a very
<br>
good thing for humanity, then you should logically be doing your best to
<br>
get it here as quickly (and yet safely) as possible. Not only for yourself,
<br>
but also it doesn't hurt to save those 150k people who die everyday.
<br>
<p>2. If you believe that science is already relatively well-funded in the
<br>
biological research realm, then it is unlikely you personally could have
<br>
any effect on accelerating a biologically-oriented Singularity. You also
<br>
have to somehow address the &quot;Singularity gap&quot; between the two approaches:
<br>
most people such as Kurzweil for instance don't believe we can have the
<br>
technology to seriously enhance human intelligence until after advanced
<br>
nanotech arrives (2030 era), and yet the computing hardware to support
<br>
superintelligent AI is arriving in the 2005 to 2010 era. That is a 20+
<br>
year gap in the competing approaches. All the AI approach needs is the
<br>
software side (sounds so easy :-).
<br>
<p>If you have resources (money, time, advice, whatever) to commit towards
<br>
accelerating the Singularity, does it really make sense to put them
<br>
towards the biological path (which is already well-resourced), or towards
<br>
the AI path? You will get more bang for your buck with the AI path since
<br>
there are so few supporters in that area right now, and potentially shave
<br>
as much as 20 years off the arrival of a Singularity. And if you agree
<br>
that you don't have the power to significantly alter a human-based
<br>
Singularity timeline (since it involves so many bits and pieces of
<br>
interdependent technology, plus the fact that your addition of resources
<br>
would be a drop in the ocean) then what have you got to lose by throwing
<br>
your weight behind the AI approach? You very well might be shaving a lot
<br>
of time off of a Singularity, or at worse blowing your resources on a
<br>
failed project that will at least provide some valuable work on
<br>
Friendliness among other things that will be very useful to the people
<br>
who finally do manage to create a working AI even if it is intra or post
<br>
Singularity.
<br>
<p>So I think I can make a case for AI potentially driving a Singularity much
<br>
sooner than a purely human-based one could. That does not address your
<br>
question of making sure it turns out well for us all. That is a tough
<br>
question, and one which no one can answer definitively. What I can say
<br>
is that if we do get an AI up and running, it will be extensively tested
<br>
to make as sure as possible that it is Friendly and wants to help us as
<br>
much as it can. That alone is important to me- I'd much prefer us to get
<br>
the first real self-enhancing AI up and running rather than someone like
<br>
a Hugo de Garis who just scares me (from a scientific point of view too).
<br>
But past that let me ask you this- do you think a purely human led
<br>
Singularity would be safer or less safe than a Singularity driven by
<br>
one superintelligence? Personally I think it would be less safe, from
<br>
the point of view that a) you are going to have many more less intelligent
<br>
(compared to a SI) minds mucking about with these advanced technologies
<br>
b) this &quot;mucking about&quot; stage of the Singularity runup will last quite a
<br>
bit of time compared to what an AI could do. More minds + more time =
<br>
more chance of some kind of disaster IMO. What do you think?
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Best wishes on joining the group,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Jack
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ----- Original Message -----
</em><br>
<em>&gt; From: Evan Reese &lt;<a href="mailto:mentat@telocity.com?Subject=Re:%20Augmenting%20humans%20is%20a%20better%20way">mentat@telocity.com</a>&gt;
</em><br>
<em>&gt; To: &lt;<a href="mailto:sl4@sysopmind.com?Subject=Re:%20Augmenting%20humans%20is%20a%20better%20way">sl4@sysopmind.com</a>&gt;
</em><br>
<em>&gt; Sent: Friday, July 27, 2007 9:08 AM
</em><br>
<em>&gt; Subject: Re: augmenting humans is difficult and slow...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; ----- Original Message -----
</em><br>
<em>&gt; &gt; From: &quot;Ben Houston&quot; &lt;<a href="mailto:ben@exocortex.org?Subject=Re:%20Augmenting%20humans%20is%20a%20better%20way">ben@exocortex.org</a>&gt;
</em><br>
<em>&gt; &gt; To: &lt;<a href="mailto:sl4@sysopmind.com?Subject=Re:%20Augmenting%20humans%20is%20a%20better%20way">sl4@sysopmind.com</a>&gt;; &quot;'Michael Korns'&quot; &lt;<a href="mailto:mkorns@korns.com?Subject=Re:%20Augmenting%20humans%20is%20a%20better%20way">mkorns@korns.com</a>&gt;
</em><br>
<em>&gt; &gt; Sent: Saturday, July 07, 2001 6:07 AM
</em><br>
<em>&gt; &gt; Subject: augmenting humans is difficult and slow... Hi...
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; Just did a talk on augmenting humans through direct brain interfaces --
</em><br>
<em>&gt; &gt; &gt; my degree is cognitive science / neuroscience so I have a little of the
</em><br>
<em>&gt; &gt; &gt; requisite knowledge in this area.  It seems very likely that we can do a
</em><br>
<em>&gt; &gt; &gt; lot by making little additions or regulatory changes but it will not be
</em><br>
<em>&gt; &gt; &gt; that easy.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; The technology to read from individual neurons within chronic
</em><br>
<em>&gt; &gt; &gt; implantations is here.  I have not yet read of any major successes in
</em><br>
<em>&gt; &gt; &gt; long-term artificial stimulation of individual neurons -- but that's
</em><br>
<em>&gt; &gt; &gt; just an engineering problem and just give it time.  This stuff doesn't
</em><br>
<em>&gt; &gt; &gt; really require esoteric nanotechnology, magical quantum interfaces but
</em><br>
<em>&gt; &gt; &gt; just electrical current readings of the relevant neurons.  In other
</em><br>
<em>&gt; &gt; &gt; words, the technology for making the bidirectional connections is not
</em><br>
<em>&gt; &gt; &gt; major limiting factor.
</em><br>
<p>No, the corporate willpower and political roadblocks are. Which company
<br>
do you think will blow the millions to attempt to commercialize some
<br>
form of neural interface for normal adults. I just don't see it happening
<br>
as long as this technology involves macroscopic implants. Whose insurance
<br>
or employer is going to cover the cost of having the implants put in and
<br>
maintained even if such technology should come to exist? And would the
<br>
government even allow it?
<br>
<p>I'm sorry, but the cyberpunk future still seems rather far fetched to me.
<br>
Wearable computing I definitely agree will be very big though.
<br>
<p><em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; What is the problem is figuring out what exactly will make us smarter
</em><br>
<em>&gt; &gt; &gt; and how to integrate that in to our existing brain architecture.  It's
</em><br>
<em>&gt; &gt; &gt; not as simple as adding more memory -- there is tons of different types
</em><br>
<em>&gt; &gt; &gt; of memory in the brain and they are highly distributed very connected
</em><br>
<em>&gt; &gt; &gt; with the computations being preformed.  Also there are a lot of
</em><br>
<em>&gt; &gt; &gt; calibration problems that have to be overcome if we would like to be
</em><br>
<em>&gt; &gt; &gt; able to recognize meaningful patterns in the brain.
</em><br>
<p>Exactly, it may well be impossible to come up with a one-size fits all
<br>
technology for something as uniquely individual as the brain. And what
<br>
company will take the risks to commercial it if they know that for many
<br>
people it won't work, or they even risk getting sued? We live in a
<br>
country where Dow Chemical got sued by women who got breast implants.
<br>
Will companies really expose themselves to the kinds of risks involved
<br>
with neural hacking?
<br>
<p><em>&gt; &gt;
</em><br>
<em>&gt; &gt; Of course, if you can interface one human, then you can do it to a
</em><br>
<em>&gt; thousand
</em><br>
<em>&gt; &gt; or a billion.   You don't need detailed models of the brain for this kind
</em><br>
<em>&gt; of
</em><br>
<em>&gt; &gt; thing - at least to start.  You can begin with a &quot;what do you feel when I
</em><br>
<em>&gt; do
</em><br>
<em>&gt; &gt; this?&quot; kind of thing and once crude dni's are working, things can take
</em><br>
<em>&gt; off.
</em><br>
<p>And so theoretically if you can &quot;interface&quot; a human, what does that get
<br>
you? Slightly quicker output for typing or controlling machines, maybe
<br>
slightly quicker input than you could get by reading? Expanded access
<br>
to memory, but I bet that would be very hard to do. But where does the
<br>
massive intelligence increase we want come from?
<br>
<p><em>&gt; &gt;
</em><br>
<em>&gt; &gt; It is certainly a hell of a lot more interesting than this uninspired
</em><br>
<em>&gt; &gt; fear-based seed AI thing.  The really neat part about the evolutionary
</em><br>
<em>&gt; &gt; approach - and why it will nullify the seed AI approach is that you don't
</em><br>
<em>&gt; &gt; have to ask for resources to fund it, or try to recruit people to work on
</em><br>
<em>&gt; &gt; it.  It's happening all by itself; most people are not - and need never
</em><br>
<em>&gt; &gt; know, and probably wouldn't care if they did - that they are contributing
</em><br>
<em>&gt; to
</em><br>
<em>&gt; &gt; the singularity.   The resources of the evolutionary singularity are truly
</em><br>
<em>&gt; &gt; vast and rapidly getting vaster.  And as others have pointed out, the
</em><br>
<em>&gt; &gt; evolutionary path begins with many of what are generally considered the
</em><br>
<em>&gt; &gt; &quot;hard problems&quot; solved, whereas the AI people have to start from square
</em><br>
<em>&gt; one.
</em><br>
<p>The hard problem of the Singularity is how to create greater than human
<br>
intelligence. So no, that problem is not already solved by taking the
<br>
&quot;evolutionary&quot; approach. And I really do not see any real scientific
<br>
(or more importantly commercial) work being done on increasing human
<br>
intelligence. The Internet does it, but we are already halfway done
<br>
with using up what it can give us. What comes next? I hold that you
<br>
are incorrect that you do not need to seek resources for biotech path
<br>
to succeed. If the corporations and their researchers do not see any money
<br>
making possibilities for intelligence enhancement research, then there
<br>
will be no work done in that area. This is already the case with much
<br>
more mundane things like a potential malaria vaccine. The companies don't
<br>
see a lot of money to be made there so there is practically no work
<br>
being done on it.
<br>
<p><em>&gt; &gt;
</em><br>
<em>&gt; &gt; There's a lot more to be said on this subject, but I'm busy with moving
</em><br>
<em>&gt; &gt; currently - to Pasadena, perhaps I'll meet some of you in Southern
</em><br>
<em>&gt; Calif. -
</em><br>
<em>&gt; &gt; so I'll close for now.  But I'll be more talkative when I get established
</em><br>
<em>&gt; &gt; there.  (I haven't even written a &quot;join&quot; post yet.&quot;  It needs to be
</em><br>
<em>&gt; &gt; emphasized that there other paths - more inspired and inspiring ones - to
</em><br>
<p>I'd like to see your definition of inspired relative to all this.
<br>
<p><em>&gt; &gt; the singularity than the imho cringing one proposed by the Institute of
</em><br>
<em>&gt; &gt; building an AI and - if everything works out as hoped - maybe humans will
</em><br>
<em>&gt; be
</em><br>
<em>&gt; &gt; permitted to scale the heights; what I would call the &quot;singularity by
</em><br>
<em>&gt; proxy&quot;
</em><br>
<em>&gt; &gt; path.  I, for one, intend to participate DIRECTLY in the singularity.  I
</em><br>
<em>&gt; &gt; hope there are at least a few others here as well.
</em><br>
<em>&gt; &gt;
</em><br>
<p>In order to participate directly in a transhuman based Singularity you
<br>
would have to be one of the first humans enhanced into transhumanity. How
<br>
do you plan to achieve that? Even if you do, the vast majority of humanity
<br>
will just be riding your coattails no matter which path occurs first.
<br>
<p>Secondly, without an AI to guide things, what prevents individuals intra or
<br>
post-Singularity from using nanotech or other ultratechnologies in destructive
<br>
ways in an anarchic fashion? I'd like to hear a brief but coherent timeline/
<br>
description of how you think this would play out. Our argument is that while
<br>
it all probably would turn out ok, it would generally be safer to get a
<br>
Friendly AI in place first.
<br>
<pre>
-- 
Brian Atkins
Director, Singularity Institute for Artificial Intelligence
<a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1861.html">James Higgins: "Re: Augmenting humans is a better way"</a>
<li><strong>Previous message:</strong> <a href="1859.html">Eliezer S. Yudkowsky: "Re: augmenting humans is difficult and slow..."</a>
<li><strong>In reply to:</strong> <a href="1857.html">Jack Richardson: "Re: Augmenting humans is a better way"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1861.html">James Higgins: "Re: Augmenting humans is a better way"</a>
<li><strong>Reply:</strong> <a href="1861.html">James Higgins: "Re: Augmenting humans is a better way"</a>
<li><strong>Reply:</strong> <a href="1864.html">Ben Goertzel: "RE: Augmenting humans is a better way"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1860">[ date ]</a>
<a href="index.html#1860">[ thread ]</a>
<a href="subject.html#1860">[ subject ]</a>
<a href="author.html#1860">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
