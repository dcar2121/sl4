<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Augmenting humans is a better way</title>
<meta name="Author" content="Brian Atkins (brian@posthuman.com)">
<meta name="Subject" content="Re: Augmenting humans is a better way">
<meta name="Date" content="2001-07-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Augmenting humans is a better way</h1>
<!-- received="Sat Jul 28 15:57:58 2001" -->
<!-- isoreceived="20010728215758" -->
<!-- sent="Sat, 28 Jul 2001 16:01:13 -0400" -->
<!-- isosent="20010728200113" -->
<!-- name="Brian Atkins" -->
<!-- email="brian@posthuman.com" -->
<!-- subject="Re: Augmenting humans is a better way" -->
<!-- id="3B631A09.F7582741@posthuman.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMKEHPFNAA.ben@webmind.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brian Atkins (<a href="mailto:brian@posthuman.com?Subject=Re:%20Augmenting%20humans%20is%20a%20better%20way"><em>brian@posthuman.com</em></a>)<br>
<strong>Date:</strong> Sat Jul 28 2001 - 14:01:13 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1881.html">Brian Atkins: "Re: Augmenting humans is a better way"</a>
<li><strong>Previous message:</strong> <a href="1879.html">Eliezer S. Yudkowsky: "Re: Augmenting humans is a better way"</a>
<li><strong>In reply to:</strong> <a href="1864.html">Ben Goertzel: "RE: Augmenting humans is a better way"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1922.html">Xavier Lumine: "Re: Augmenting humans is a better way"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1880">[ date ]</a>
<a href="index.html#1880">[ thread ]</a>
<a href="subject.html#1880">[ subject ]</a>
<a href="author.html#1880">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Thanks for the update. When I say &quot;scary&quot; I meant more from the point of
<br>
his view of how things might play out. Who can say exactly how it will play
<br>
out, but I feel he is taking the easy (or ignorant) way out by not also
<br>
worrying about a potential hard (or semi-hard :-) takeoff. In terms of
<br>
his scientific efforts, what I meant by scary was a more joking reference
<br>
to his poor design/theory (as you noticed).
<br>
<p>But I agree, better him than a no-nothing (or worse).
<br>
<p>Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &gt; I'd much prefer us to get
</em><br>
<em>&gt; &gt; the first real self-enhancing AI up and running rather than someone like
</em><br>
<em>&gt; &gt; a Hugo de Garis who just scares me (from a scientific point of view too).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Just a side comment here...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I just spent a couple days with Hugo last month (in the emptied-out Starlab
</em><br>
<em>&gt; building -- a beautiful building by the way, see
</em><br>
<em>&gt; <a href="http://www.starlab.org/contactus/findus/">http://www.starlab.org/contactus/findus/</a> ), and I can assure you that while
</em><br>
<em>&gt; he's got some really interesting technical work going, he's *nowhere near* a
</em><br>
<em>&gt; workable path to a real AI.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; He think it'll be 50 years or so before we get a real AI.  What he has now
</em><br>
<em>&gt; is a superpowerful hardware system for evolving neural nets by genetic
</em><br>
<em>&gt; programming.  Some very cool aspects, such as a genotype/phenotype
</em><br>
<em>&gt; distinction (the genotype gives initial positions of neurons, and there's an
</em><br>
<em>&gt; epigenesis phase in which synapses grow, providing the phenotype, the actual
</em><br>
<em>&gt; neural net).  A weakness is that fitness of an NN must be assessed by a list
</em><br>
<em>&gt; of given fitness cases, and can't be done by some function not easily
</em><br>
<em>&gt; encapsulated in a small table of cases (e.g. it can't be done by inference
</em><br>
<em>&gt; relative to a database of experience, as is the case for most procedure
</em><br>
<em>&gt; evolution/learning in the mind).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; As for his philosophical views, he believes that Friendly AI is possible,
</em><br>
<em>&gt; but that even if AI's are friendly, people are not, and they won't accept
</em><br>
<em>&gt; AI's, so there will be some kind of violent struggle between pro-AI people
</em><br>
<em>&gt; and anti-AI people.  He believes that self-modifying AI can be a path to
</em><br>
<em>&gt; superhuman intelligence, but he believes this path will take several hundred
</em><br>
<em>&gt; years, and that during this time there is a decent change that the AI's and
</em><br>
<em>&gt; all humans are wiped out by stupid paranoid human violence.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; On a personal level, while Hugo is definitely a very eccentric individual in
</em><br>
<em>&gt; some ways, he was very friendly and took a lot of time out to talk to me in
</em><br>
<em>&gt; spite of being in the midst of a huge crisis situation (Starlab just went
</em><br>
<em>&gt; broke, he was out of a job and had no idea what he was doing next -- hmm, a
</em><br>
<em>&gt; very familiar situation to me actually ;p ).  He offered his spare room to
</em><br>
<em>&gt; me during my visit to Brussels (for the Global Brain Workshop
</em><br>
<em>&gt; (<a href="http://pespmc1.vub.ac.be/Conf/GB-0.html">http://pespmc1.vub.ac.be/Conf/GB-0.html</a>, a gathering at which many
</em><br>
<em>&gt; Singularity-ish topics were discussed, although the &quot;Singularity&quot; phrase got
</em><br>
<em>&gt; little respect) even though we'd never met each other before in person, only
</em><br>
<em>&gt; discussed things thru e-mail.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Frankly, although I think it's unlikely, I would *much* rather see the first
</em><br>
<em>&gt; real AI created by Hugo, who is basically a sweet guy who has thought deeply
</em><br>
<em>&gt; about the philosophical ramifications of AI, than by oh, say, a US military
</em><br>
<em>&gt; AI lab....  I don't think that mild-mannered eccentric scientists are our
</em><br>
<em>&gt; greatest worry by any means.  Fortunately, at this point, the military and
</em><br>
<em>&gt; other powerful entities whose ethics I question, apparently have no interest
</em><br>
<em>&gt; in building real AI, because the academic establishment has convinced them
</em><br>
<em>&gt; it's a very very long way off still.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; -- Ben G
</em><br>
<p><pre>
-- 
Brian Atkins
Director, Singularity Institute for Artificial Intelligence
<a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1881.html">Brian Atkins: "Re: Augmenting humans is a better way"</a>
<li><strong>Previous message:</strong> <a href="1879.html">Eliezer S. Yudkowsky: "Re: Augmenting humans is a better way"</a>
<li><strong>In reply to:</strong> <a href="1864.html">Ben Goertzel: "RE: Augmenting humans is a better way"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1922.html">Xavier Lumine: "Re: Augmenting humans is a better way"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1880">[ date ]</a>
<a href="index.html#1880">[ thread ]</a>
<a href="subject.html#1880">[ subject ]</a>
<a href="author.html#1880">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
