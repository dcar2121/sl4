<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: The Quest for Singularity (was: Flare)</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="The Quest for Singularity (was: Flare)">
<meta name="Date" content="2001-07-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>The Quest for Singularity (was: Flare)</h1>
<!-- received="Sun Jul 22 02:38:35 2001" -->
<!-- isoreceived="20010722083835" -->
<!-- sent="Sun, 22 Jul 2001 02:11:21 -0400" -->
<!-- isosent="20010722061121" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="The Quest for Singularity (was: Flare)" -->
<!-- id="3B5A6E89.B31FC0C8@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20The%20Quest%20for%20Singularity%20(was:%20Flare)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jul 22 2001 - 00:11:21 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1821.html">Peter Voss: "RE: Flare"</a>
<li><strong>Previous message:</strong> <a href="1819.html">Ben Goertzel: "Flare"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1820">[ date ]</a>
<a href="index.html#1820">[ thread ]</a>
<a href="subject.html#1820">[ subject ]</a>
<a href="author.html#1820">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
(Now *this* post is legit content for SL4.)
<br>
<p>==
<br>
<p>Some days I wonder if maybe I've toned it down a little too far.
<br>
<p>The practice of hyping a project's goals beyond what it can reasonably
<br>
achieve is so universal that one forgets the peril of understating the
<br>
goals.  If you have a small goal presented as a great one, then people are
<br>
liable to overestimate the rewards, and be disappointed.  But if you have
<br>
a great goal presented as a small one, then people are liable to
<br>
underestimate the efforts required to achieve it.  I think this is what's
<br>
happening on SL4.
<br>
<p>There was a time when I thought that the Singularity would occur in 2030,
<br>
the product of a vast Manhattan Project, enabled by the successor
<br>
to the Web - a global cooperative effort by a substantial sector of the
<br>
human species, fully understanding of what was at stake in the Singularity
<br>
and willing to donate time, effort, and money to achieve it.
<br>
<p>In retrospect, I thought this way because I didn't understand how to solve
<br>
the problem.  If you don't understand how to solve a problem, then you
<br>
imagine yourself overcoming it by brute force.  This is not to say that
<br>
the brute-force strategy would fail.  Even so difficult a problem as AI
<br>
could probably be overcome by brute force in the year 2030.  I am not
<br>
saying that imagining brute force is fantasy or unworkable; rather, I am
<br>
saying that you only imagine it when you don't have a good image of how to
<br>
solve the problem.
<br>
<p>This concept changed when I got to understand AI better.  I learned more
<br>
about how large-scale development projects worked, and encountered the
<br>
concept of open source.  It also occurred to me that 2030 or even 2020 was
<br>
perhaps too conservative, given the rate of increase of *networked*
<br>
computing power.
<br>
<p>This was during the period when I wrote the first proposals for a
<br>
Singularity Institute.  I started thinking about whether an open-source AI
<br>
project could bring in the necessary resources without a Manhattan
<br>
project.  I visualized the birth of an AI industry, driven by the use of
<br>
open-source AI for its most natural application; programming and source
<br>
code.  Flare played a much larger part in my thought then than it does
<br>
now, since at first I thought in terms of a vast Internet distributed
<br>
computing network as the enabling condition for the birth of seed AI,
<br>
which implied a programming language in which people could write
<br>
distributed code, which meant that such a programming language would
<br>
possibly even be on the *critical* path to Singularity.
<br>
<p>However, I soon also realized that shifting the time horizon to the
<br>
vicinity of 2010 probably didn't leave much time for people to get used to
<br>
the idea of a Singularity, and that running the project distributed would
<br>
mean that the initial stages of the hard takeoff would be easy to observe
<br>
with standard network monitoring tools, quite possibly triggering a panic
<br>
and disaster at the last minute.  Even if 95% of humanity was on board
<br>
with the Singularity in theory, the enormous disruption that could result
<br>
if people saw it was Actually Happening could quite easily kill a huge
<br>
number of people at a point when it would be incredibly tragic and
<br>
futile.  So I dumped the fairly large amount of material about distributed
<br>
computing, and the final version of my writings made reference to a
<br>
privately owned supercomputer, or a rented supercomputer, rather than a
<br>
SETI@Home project.  That's not to say that I don't think having an &quot;in&quot;
<br>
with the distributed-supercomputer industry would be nice; I still think
<br>
so, but I no longer rely on it.
<br>
<p>More time passed.  I started writing GISAI (then CaTAI 2.0), and studied
<br>
up some more on the dynamics of open-source projects.  I realized that
<br>
open source and the idea of birthing an AI industry had also been
<br>
brute-force approaches to the problem.  Again, I'm not saying that these
<br>
approaches wouldn't have worked.  They probably would have.  (Although
<br>
whether they would have worked for *Friendly* AI was another issue.  Back
<br>
then, I hadn't realized that Friendly AI would be necessary.)  But pouring
<br>
a vast amount of open-source intelligence into the project, or throwing an
<br>
AI industry at the problem, was still postulating massive quantities of
<br>
external brainpower; it was still, fundamentally, a &quot;brute intelligence&quot;
<br>
Singularity strategy.
<br>
<p>Once I had an idea of what *specifically* needed to be done to build a
<br>
complete mind, rather than just saying &quot;It's a Quest, let's throw
<br>
resources at the problem,&quot; I also realized that open source and an AI
<br>
industry might not help all that much.  The old strategy was a gradualist
<br>
approach to the quest for AI, composed of slow incremental steps.  This
<br>
makes the path more solid, but much slower, and (as I also began to
<br>
realize) considerably less compatible with Friendly AI.  The kind of
<br>
things that would need to be done to build seed AI *directly* weren't very
<br>
susceptible to open source, and would not be much benefited by the
<br>
existence of an AI industry except indirectly.  The massive-brainpower
<br>
strategy would have worked eventually, as the result of the slow buildup
<br>
of knowledge, but it would not have been the *direct* path.  Of course, a
<br>
direct path would require a much deeper understanding of the problem and
<br>
would require deeper coding.  On the plus side, though, the expected
<br>
efforts needed to implement the Quest shrank yet again, this time all the
<br>
way down to the level of what a single large project might be able to
<br>
implement.
<br>
<p>And that, I think, is how I wound up toning it down a little too far. 
<br>
Because now people have no way of realizing that we are, in fact, embarked
<br>
on a Quest.  The people who recall when the Singularity Institute was an
<br>
impossible dream may still remember how nice it was, and/or scary, to hear
<br>
that SIAI had been incorporated.  But some of the people here will have
<br>
first encountered the Singularity Institute as a fait accompli.  People
<br>
may have first heard us talking about 2010 and thought we were just being
<br>
overoptimistic, not realizing how much mental effort went into figuring
<br>
out how to make it &quot;2010&quot; instead of &quot;2020&quot; or &quot;2030&quot;.
<br>
<p>In retrospect, I think this is why some of the posters on SL4 seem to be
<br>
perceiving SIAI as an overambitious AI project, rather than as an
<br>
ultraleveraged way of implementing the last great crusade in human
<br>
history.  The Singularity Institute needs to be seen in context, and the
<br>
context is the Singularity.  I mean, THE SINGULARITY.  The beginning of
<br>
human history.  We would very much like to do it in ten years, but if it
<br>
takes twenty years to do it then we will SPEND twenty years, because this
<br>
*is* a crusade and the entire planet is at stake.
<br>
<p>We are an AI project because we think that's the fastest way to the
<br>
Singularity.  We are not a group that *started out* as an AI project and
<br>
then got too big for their britches; we started out as Singularitarians
<br>
and *then* decided that AI was a good way to do it.  In the process, I
<br>
wonder if maybe we haven't gotten too small for our britches.  Maybe we've
<br>
forgotten that we need to actually *tell* people about the crusade part of
<br>
it, or they won't know.
<br>
<p>If I didn't think Flare had something to do with the Singularity, if Flare
<br>
were Yet Another Programming Language, then it would not be on my
<br>
horizons.  Flare is interesting to me because I think that the art of
<br>
programming is stuck in a rut, a rut worn by developing our basic thought
<br>
patterns through programming on machines that we would today regard as
<br>
abacuses - machines where efficiency was more important than the
<br>
programmer's time.  XML (offtopic, I admit, but still illustrative) came
<br>
along when people finally said, &quot;OK, we HAVE the disk space, we have
<br>
gigabytes and gigabytes of disk space, we have disk space to spare, what
<br>
we don't have is the patience to decode a thousand incompatible entangled
<br>
binary formats.&quot;  Flare is intended to take the same step for programming
<br>
languages.  You can see it in, for example, the idea of a FlareSpeak IDE
<br>
that is not irrevocably bound to be plaintext.  You can see it in the idea
<br>
of being able to annotate every Flare element; in the idea of tracking
<br>
two-way references; in the idea of XML program files; in the idea of using
<br>
an extensible-tree representation for the interpreted program; and so on.
<br>
<p>What's written up on Flare is not the whole shebang.  It's a fast set of
<br>
documents I emitted over the course of a few days because someone
<br>
volunteered to lead the project and I wanted to find out how many other
<br>
people would be interested in supporting.  Somewhere along the line, I
<br>
seem to have forgotten to write down that Flare is a Quest.  It is unlike
<br>
seed AI in that Flare is a quest I can hand off to someone else.  But it's
<br>
still a Quest.
<br>
<p>If it takes five years for Flare to have an impact, then the moral is that
<br>
we had better start today.  If I don't plan on a timescale longer than
<br>
five-to-twenty, well, I don't plan on a shorter timescale either.  I
<br>
expect the Quest for AI will still be continuing five years from now
<br>
unless we have one heck of a run of good luck.  Should I be planning on
<br>
still having, five years from now, more or less the same programming
<br>
languages that we have today?  Or should I be hoping to see at least a few
<br>
steps into the design space that Flare is intended to open up?
<br>
<p>If SIAI can undertake the Quest for Singularity in the form of a single AI
<br>
programming project, then more power to us, since it means we'll have
<br>
found a really small and fast and efficient way of achieving the
<br>
Singularity.  It means we'll have a strategy that doesn't require us to
<br>
grow to the size of the Bill and Melinda Gates Foundation, even though our
<br>
goals are a heck of a lot more ambitious than theirs.  But I am still
<br>
going to be asking for certain things in support of the Quest for
<br>
Singularity, a.k.a. the AI implementation project, that I would not be
<br>
asking for if I were trying to implement a billing system for my local
<br>
grocer.
<br>
<p>The Quest does not get any smaller.  You may be able to come up with a
<br>
better way of achieving goals, but you can't compromise on goals.  You
<br>
cannot downsize the Quest to fit available resources.  The most you can do
<br>
is delay what you do today because you hope to have more resources
<br>
tomorrow.  If you need more resources, then part of the Quest becomes
<br>
finding those additional resources, and you put in whatever efforts are
<br>
required to get those additional resources.  If you don't get those
<br>
resources, then you fail to bring about the Singularity, and either
<br>
someone else does the same Quest, or the human species dies.  Pointing out
<br>
SIAI-specific problems is one thing, but I'm continually thrown off-stride
<br>
by people who casually say &quot;No Singularity effort will ever succeed
<br>
because of such-and-such&quot; without realizing that they've just passed a
<br>
death sentence on the human race.  It's always possible that humanity is
<br>
just screwed, plain and simple, so such comments have their place, but I
<br>
still wish people would remember to say &quot;You'll never succeed because of
<br>
such-and-such, and therefore we're probably all going to die&quot;; this would
<br>
tell me that they have thought their criticism through in some detail.
<br>
<p>When I am talking about building AI, I am talking about something that
<br>
just four years ago I would have thought of as a project to be done in
<br>
2020 or 2030 with the assistance of a significant fraction of the human
<br>
race.  This is because I did not understand AI and therefore planned on
<br>
brute-forcing it.  The fact that I now think &quot;Yes, we can start today&quot;
<br>
does not mean that I expect it to be easy or that I expect it to work out
<br>
as an ordinary programming project would.  We are not talking about an
<br>
ambition that started out as &quot;write this cool program&quot; and expanded to
<br>
form a nonprofit; we are talking about a Quest that started out the size
<br>
of the entire planet and eventually got analyzed and leveraged to the
<br>
point where a single nonprofit could do it.
<br>
<p>One of the tools I would like to have on the AI Quest is a better
<br>
programming environment, by which I mean, &quot;Programming tools a whole world
<br>
beyond the abacus-originated morass we are stuck in today&quot;, and not just a
<br>
handful of features added to some current IDE.  Now, it is perhaps not
<br>
reasonable for someone implementing the billing system at the grocery
<br>
store to demand significant advances in the art of programming; but SIAI
<br>
needs to create an anachronism, a program born years out of time, and I
<br>
very much want better tools.  In fact, I want anachronistic tools.
<br>
<p>I realize that not all of this is visible in what I have already written
<br>
up on Flare.  That's because I'm supposed to be handing this off.  It's
<br>
not my job to write the &quot;Creating Friendly AI&quot; of Flare.  What's out there
<br>
is a few ideas tossed out to get people interested.  And I bashfully
<br>
confess that what's out there is pretty neat.  But if it were just &quot;pretty
<br>
neat&quot; then I would never have gotten involved.  There are dozens of neat
<br>
ideas that I don't do anything with because they are not
<br>
Singularity-related neat ideas.  The Causality design pattern is seeing
<br>
the light of day for the first time as a footnote to Flare, even though
<br>
the Causality design pattern is really cool, because humanity's adoption
<br>
of the Causality design pattern is not on the direct path to Singularity.
<br>
<p>Creating the next step in programming languages is not a trivial thing to
<br>
do, but it is something that has been done successfully several times
<br>
before, with incremental improvements smaller than the ones already
<br>
visible in the online material on Flare.  Creating the next programming
<br>
language is certainly less difficult than the Quest for AI.
<br>
<p>Flare considered as an isolated set of improvements, and as a possible
<br>
language for SIAI work over the next couple of years, does not perhaps
<br>
have Questlike quality - although make no mistake, it certainly has
<br>
projectlike quality; an initial Flare implementation would be extremely
<br>
useful and those first steps will be a lot more difficult without it.
<br>
<p>Flare considered as a *first* step does have Quest-nature.  The most
<br>
important *long-term* qualities of Flare are not the immediately powerful
<br>
features, such as invariants and two-way references.  The most important
<br>
characteristics of Flare are the subtle qualities that make Flare a first
<br>
step into a larger design space that is then exploitable by a series of
<br>
incremental improvements.  Moving beyond IDEs *necessarily* bound to
<br>
plaintext is a subtle first step, even if the first FlareSpeak IDE happens
<br>
to be entirely plaintext.  Having the programming language represented as
<br>
XML (*extensible* tree structures) is a subtle first step.  Having trees
<br>
of control instead of threads of control (you haven't seen this part yet)
<br>
is a subtle step.  We may need to compromise on some
<br>
subtly-futuristic-but-not-immediately-powerful aspects in order to deliver
<br>
a Visibly Cool, 50%-right version of Flare in reasonable time, but those
<br>
aspects of the language will still be present in the background.
<br>
<p>I have been focusing on the short term, and the things I want Flare to do
<br>
for the AI project in the next year or two, because hey, that's what'll
<br>
happen first, and that's what I should be spending most of my time
<br>
thinking about.  Even if the &quot;subtleties&quot; fail to trigger any future
<br>
improvements, the Singularity Institute will still be able to write
<br>
self-watching annotative code and the first steps of the AI project will
<br>
be a lot easier.
<br>
<p>But there's also a Quest component to Flare, where if it takes four years
<br>
to do it, then we will start today instead of 2004 and have it ready in
<br>
2005 instead of 2008.  Flare is not superhumanly difficult (the AI side
<br>
*is* superhumanly difficult), and Flare can be forked off as an
<br>
open-source project, so it doesn't have to be too much of a distraction
<br>
for SIAI.  And if you insist on raising up those distracting
<br>
&quot;practicality&quot; issues, then I would quietly point out that Flare, *not*
<br>
being superhumanly difficult, and not needing to be closed-source, is
<br>
something that SIAI can make well-defined progress on and cite in the
<br>
directly forseeable future as an accomplishment involving written, visibly
<br>
functioning code that does something immediately perceptible as cool.
<br>
<p>I emphasize that that is not the primary purpose of Flare.  There is
<br>
something pathetic about wanting to do the impossible and important and
<br>
settling for the easy and pointless.  Rest assured that we are not going
<br>
down that road; we are still Singularitarians, and we won't be distracted
<br>
by something other than creating AI just because creating AI happens to be
<br>
extremely difficult.  Flare *is* relevant.  But the fact that Flare is a
<br>
relevant project that we can begin immediately with our (nearly
<br>
nonexistent) free resources, accomplish visible progress on in bounded
<br>
time, and release to the entire world without fear of abuse, should not be
<br>
overlooked.  It's one of the reasons why Flare was originally described as
<br>
the *first* project to be initiated by the proposed Singularity Institute.
<br>
<p>==
<br>
<p>Sincerely,
<br>
Eliezer.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1821.html">Peter Voss: "RE: Flare"</a>
<li><strong>Previous message:</strong> <a href="1819.html">Ben Goertzel: "Flare"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1820">[ date ]</a>
<a href="index.html#1820">[ thread ]</a>
<a href="subject.html#1820">[ subject ]</a>
<a href="author.html#1820">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
