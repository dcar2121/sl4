<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Floppy take-off</title>
<meta name="Author" content="Brian Atkins (brian@posthuman.com)">
<meta name="Subject" content="Re: Floppy take-off">
<meta name="Date" content="2001-07-31">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Floppy take-off</h1>
<!-- received="Tue Jul 31 03:05:43 2001" -->
<!-- isoreceived="20010731090543" -->
<!-- sent="Tue, 31 Jul 2001 03:04:49 -0400" -->
<!-- isosent="20010731070449" -->
<!-- name="Brian Atkins" -->
<!-- email="brian@posthuman.com" -->
<!-- subject="Re: Floppy take-off" -->
<!-- id="3B665891.C035D870@posthuman.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="a05100302b78b93b6a73c@[10.0.1.2]" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brian Atkins (<a href="mailto:brian@posthuman.com?Subject=Re:%20Floppy%20take-off"><em>brian@posthuman.com</em></a>)<br>
<strong>Date:</strong> Tue Jul 31 2001 - 01:04:49 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1967.html">Brian Atkins: "Re: Open AI"</a>
<li><strong>Previous message:</strong> <a href="1965.html">Gordon Worley: "Re: Floppy take-off"</a>
<li><strong>In reply to:</strong> <a href="1953.html">Gordon Worley: "Re: Floppy take-off"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1966">[ date ]</a>
<a href="index.html#1966">[ thread ]</a>
<a href="subject.html#1966">[ subject ]</a>
<a href="author.html#1966">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Gordon Worley wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; At 6:05 PM -0400 7/30/01, Carl Feynman wrote:
</em><br>
<em>&gt; &gt;Let's be optimistic and say that Webmind had an AI with a capacity of
</em><br>
<em>&gt; &gt;0.5 brains.  It will take Moore's Law about 16 years to upgrade their
</em><br>
<em>&gt; &gt;machine to 1.3 kilobrains.  If we assume that the rate of progress in AI
</em><br>
<em>&gt; &gt;algorithms (doubling every two years) continues, and that the AI field
</em><br>
<em>&gt; &gt;is working on the right problems, the time is decreased to about ten
</em><br>
<em>&gt; &gt;years.  Still pretty long.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Um, I'm not sure how you got 16 years, but the Moore's Law analysis
</em><br>
<em>&gt; can't be right.  Just by doubling the speed of a 0.5 brains AI, all
</em><br>
<em>&gt; it means is that a moron will have twice as many moronic thoughts in
</em><br>
<em>&gt; a year.  Moore's Law alone can't get us to transhuman AI, but it does
</em><br>
<em>&gt; help once we have at least a 1 brain AI.  When the AI reaches human
</em><br>
<em>&gt; level intelligence, I'd assume it can think things humans can, like
</em><br>
<em>&gt; how to write better algorithms, at which point Moore's Law will
</em><br>
<em>&gt; matter.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think you're mixing up speed increases with getting smarter.
</em><br>
<p>Think about this: brain size didn't change much between apes,
<br>
neaderthals (or whatever preceded humans) and us. Yet the increase
<br>
in what we call intelligence was rather huge. So what if rather than
<br>
using a doubling in computing power to run the mind twice as fast, we
<br>
instead use it to double the size of it, leading to a huge increase
<br>
(probably much more than doubling) in the complexity/intelligence/
<br>
creativity of the mind.
<br>
<p>So to recap, we seem to have evolutionary evidence showing that
<br>
intelligence scales exponentially with available computing power, not
<br>
linearly, and therefore the idea of comparing &quot;AI years&quot; to human
<br>
work years is probably not going to work. Likewise it is quite
<br>
possible if we look at evolutionary evidence that a 1.3 kilobrain
<br>
AI would not simply be equal to 1300 humans working on stuff. It
<br>
might very well be hugely more intelligent/smart than than any
<br>
human, allowing it to easily see all the wrong research paths.
<br>
<p>Of course this is dependent on your AI design being able to scale
<br>
its intelligence with additions of processing power. If it simply
<br>
runs faster, or runs more copies of itself, that is not going to be
<br>
anywhere near as good.
<br>
<p>P.S. I basically plagiarized all this from our &quot;Seed AI&quot; webpage
<br>
<p>P.P.S. I just did a little Googling on Einstein's brain. Apparently,
<br>
it actually was a bit smaller than average, yet a certain area of it
<br>
relating to visualizing/mathematics was 15% larger than average. So
<br>
if indeed this is what made him special, you can see how a very small
<br>
increase in available computing resources might indeed vastly increase
<br>
intelligence/abilities.
<br>
<pre>
-- 
Brian Atkins
Director, Singularity Institute for Artificial Intelligence
<a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1967.html">Brian Atkins: "Re: Open AI"</a>
<li><strong>Previous message:</strong> <a href="1965.html">Gordon Worley: "Re: Floppy take-off"</a>
<li><strong>In reply to:</strong> <a href="1953.html">Gordon Worley: "Re: Floppy take-off"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1966">[ date ]</a>
<a href="index.html#1966">[ thread ]</a>
<a href="subject.html#1966">[ subject ]</a>
<a href="author.html#1966">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
