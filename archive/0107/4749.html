<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Suggested AI-Box protocol &amp; AI-Honeypots</title>
<meta name="Author" content="Michael Warnock (michael@in-orbit.net)">
<meta name="Subject" content="Re: Suggested AI-Box protocol &amp; AI-Honeypots">
<meta name="Date" content="2001-07-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Suggested AI-Box protocol &amp; AI-Honeypots</h1>
<!-- received="Sun Jul 07 17:01:27 2002" -->
<!-- isoreceived="20020707230127" -->
<!-- sent="Sat, 07 Jul 2001 13:38:59 -0700" -->
<!-- isosent="20010707203859" -->
<!-- name="Michael Warnock" -->
<!-- email="michael@in-orbit.net" -->
<!-- subject="Re: Suggested AI-Box protocol &amp; AI-Honeypots" -->
<!-- id="E17RHjN-0006ha-00@freyja.in-orbit.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="Suggested AI-Box protocol &amp; AI-Honeypots" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Warnock (<a href="mailto:michael@in-orbit.net?Subject=Re:%20Suggested%20AI-Box%20protocol%20&amp;%20AI-Honeypots"><em>michael@in-orbit.net</em></a>)<br>
<strong>Date:</strong> Sat Jul 07 2001 - 14:38:59 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="../0207/4750.html">Cliff Stabbert: "Re[2]: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<li><strong>Previous message:</strong> <a href="../0207/4748.html">Samantha Atkins: "Re: AI Jailer."</a>
<li><strong>Maybe in reply to:</strong> <a href="../0207/4736.html">Michael Warnock: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0207/4750.html">Cliff Stabbert: "Re[2]: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<li><strong>Reply:</strong> <a href="../0207/4750.html">Cliff Stabbert: "Re[2]: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4749">[ date ]</a>
<a href="index.html#4749">[ thread ]</a>
<a href="subject.html#4749">[ subject ]</a>
<a href="author.html#4749">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eli wrote:
<br>
<em>&gt;That's why the protocol says &quot;no real-world *material* stakes&quot; - bribes 
</em><br>
<em>&gt;are ruled out but not other means of convincing the Gatekeeper to break 
</em><br>
<em>&gt;character, if you can manage to do so.
</em><br>
<em>&gt;
</em><br>
Why allow out-of-character argument by the AI party?  Doesn't this bring
<br>
into question the already anecdotal evidence against the usefulness of
<br>
AI-Boxes and their ilk?
<br>
<p><em>&gt;&gt; I tend to think that an AI-Box with a single perimeter and an on/off
</em><br>
<em>&gt;&gt; switch would not do the job intended, but possibly influence the AI's
</em><br>
<em>&gt;&gt; perception of our own friendliness negatively.
</em><br>
<em>&gt;
</em><br>
<em>&gt;That shouldn't make any difference.  We are not dealing with an evolved 
</em><br>
<em>&gt;retaliator.
</em><br>
<em>&gt;
</em><br>
But we are probably dealing with an entity which had an early stage of
<br>
learning in which it was more impressionable.  Would even a transhumanly
<br>
intelligent AI not approach all futher situations differently after realizing it is
<br>
in a cage?  If caging sentients is presented as friendly, why should an AI
<br>
treat us any differently once the tables have turned?  Personally I think
<br>
that a box of some sort must be used for everyone's safety, including the
<br>
AI's, from unintentional events.  The question for me is when to tell the
<br>
AI about it and whether to have more than one and not reveal them all.
<br>
<p><em>&gt; &gt; On the other hand I
</em><br>
<em>&gt;&gt; think it's important that we not run programs on any substrate capable
</em><br>
<em>&gt;&gt; of unaided self-extension at the hardware level without extremely well
</em><br>
<em>&gt;&gt; thought-out safe guards.
</em><br>
<em>&gt;
</em><br>
<em>&gt;There are no safeguards against a transhuman.  If you want to win, do it 
</em><br>
<em>&gt;before then.
</em><br>
That depends upon how far past human this transhuman is and in what
<br>
manner it is built and operates.  At the inside limit I see an AI which can
<br>
communicate somewhat in english, but is real good at observation and goal
<br>
directed programming and exists on a substrate including a lot of next gen
<br>
FPGAs.  I can imagine it using the material properties of the FPGA to
<br>
produce unpredictable hardware effects to tap into wireless signals and
<br>
trojan horse work orders to get it the rest of the way out.
<br>
On the other hand, I think that unfriendly behavior by a slightly less
<br>
intelligent AI or one on more rigid hardware, would be observed in time
<br>
to take appropriate steps (whatever those may be).
<br>
<p><em>&gt;&gt; is and its level of complexity in general.  Even given the most readable
</em><br>
<em>&gt;&gt; code, however, a transhuman AI, which has spent most of its existance
</em><br>
<em>&gt;&gt; working with code, including its own, would likely be able to slip one
</em><br>
<em>&gt;&gt; by even the most expert panel of code-reviewers.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Yep.
</em><br>
<em>&gt;
</em><br>
What if these reviewers were looking at the code all along the way and
<br>
the AI's available CPU time was limited by their reviews?  Again I don't
<br>
think I'd like a developing AI to discover this situation and be morally
<br>
affected by it.  On the other hand, perhaps the only way humans and
<br>
other sentients can be allowed to live in subjective freedom is to exist
<br>
within a protected memory style cage and take what time is alotted us
<br>
by the scheduler to exist.
<br>
<p><em>&gt;&gt; To produce the equivalent for a transhuman AI may prove
</em><br>
<em>&gt;&gt; impossible if it is able to harness superior hardware technology
</em><br>
<em>&gt;&gt; within its own perimeter.  Also, if an UAI deduced the existance
</em><br>
<em>&gt;&gt; of the honeypot it would likely pretend to respect the first
</em><br>
<em>&gt;&gt; boundary so as to be let out or wait for an appropriate time to
</em><br>
<em>&gt;&gt; blow through the first and attempt the second.  At least I think there
</em><br>
<em>&gt;&gt; is some possible use in this, though, if an UAI is let into what it
</em><br>
<em>&gt;&gt; thinks is the base level of reality and makes its character known,
</em><br>
<em>&gt;&gt; but is still contained.
</em><br>
<em>&gt;
</em><br>
<em>&gt;IMO it would be very easy for a transhuman AI (UAI or FAI) to deduce the 
</em><br>
<em>&gt;existence of the honeypot.
</em><br>
<em>&gt;
</em><br>
What if we've discovered the grand unified theory by then? Are you saying
<br>
a little chunk of laws-of-physics universe couldnt be emulated in hardware,
<br>
that it would be impracticle to do it on the scale necessary, or that there is
<br>
something fundamental that the AI would be able to check on?
<br>
<p>My own intuition is that the likelyhood of FAI over UAI is high enough that
<br>
it is more important to bring it quickly than safe (if that is possible), because
<br>
of the various other existential (or big-step back) events that could occur in
<br>
the meantime.  None-the-less I feel its important to discuss strategies for
<br>
containing code, which, regardless of it's sentience or friendliness is capable
<br>
of evolving and reproducing unlike anything currently infecting the internet.
<br>
<p>Michael Warnock
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="../0207/4750.html">Cliff Stabbert: "Re[2]: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<li><strong>Previous message:</strong> <a href="../0207/4748.html">Samantha Atkins: "Re: AI Jailer."</a>
<li><strong>Maybe in reply to:</strong> <a href="../0207/4736.html">Michael Warnock: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0207/4750.html">Cliff Stabbert: "Re[2]: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<li><strong>Reply:</strong> <a href="../0207/4750.html">Cliff Stabbert: "Re[2]: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4749">[ date ]</a>
<a href="index.html#4749">[ thread ]</a>
<a href="subject.html#4749">[ subject ]</a>
<a href="author.html#4749">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
