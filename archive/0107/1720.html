<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The Human Augmentation Strategy</title>
<meta name="Author" content="Jack Richardson (jrichard@empire.net)">
<meta name="Subject" content="Re: The Human Augmentation Strategy">
<meta name="Date" content="2001-07-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The Human Augmentation Strategy</h1>
<!-- received="Thu Jul 05 21:55:00 2001" -->
<!-- isoreceived="20010706035500" -->
<!-- sent="Thu, 5 Jul 2001 21:47:32 -0400" -->
<!-- isosent="20010706014732" -->
<!-- name="Jack Richardson" -->
<!-- email="jrichard@empire.net" -->
<!-- subject="Re: The Human Augmentation Strategy" -->
<!-- id="001401c105bd$a1ba8d40$588d90c6@oemcomputer" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="F165vnCjnZS08i7M4BA00000da7@hotmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Jack Richardson (<a href="mailto:jrichard@empire.net?Subject=Re:%20The%20Human%20Augmentation%20Strategy"><em>jrichard@empire.net</em></a>)<br>
<strong>Date:</strong> Thu Jul 05 2001 - 19:47:32 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1721.html">Christian L.: "Re: The Human Augmentation Strategy"</a>
<li><strong>Previous message:</strong> <a href="1719.html">Christian L.: "Re: Friendly AI and Human Augmentation"</a>
<li><strong>In reply to:</strong> <a href="1719.html">Christian L.: "Re: Friendly AI and Human Augmentation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1721.html">Christian L.: "Re: The Human Augmentation Strategy"</a>
<li><strong>Maybe reply:</strong> <a href="1721.html">Christian L.: "Re: The Human Augmentation Strategy"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1720">[ date ]</a>
<a href="index.html#1720">[ thread ]</a>
<a href="subject.html#1720">[ subject ]</a>
<a href="author.html#1720">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Christian:
<br>
<p>Thanks for your response to my message.
<br>
<p>The suggestion I was trying to make (and not doing so very well) was that
<br>
the attainment of super-intelligence may be easier to achieve through human
<br>
augmentation rather than starting from scratch with today's machines.
<br>
<p>I indicated a couple of areas that could be a starting point for
<br>
augmentation but it shouldn't be limited just to those. The idea is to move
<br>
individual humans beyond the limits placed on them by the weakness of their
<br>
memory and their brain's processing power.
<br>
<p>Because our methods of augmentation today are quite limited, there is a
<br>
tendency to assume that our powerful computers are a better way to achieve
<br>
the goal.
<br>
<p>However, I believe that the enormous research being done to develop very
<br>
small devices that can be placed in the body to correct medical problems
<br>
will give us the technology to augment humans along the lines I'm
<br>
suggesting. I'm looking for the AARP to push the longevity aspects of this
<br>
technology and also to fight off the bioethicists.
<br>
<p><em>&gt;From this perspective, AI research would be better focused on imagining how
</em><br>
these devices might work and how to establish a benevolent mode of
<br>
functioning by humans with the advanced powers. Research on the devices
<br>
could be conducted initially with animals in a humane way. I'll leave it up
<br>
to the researchers to test if the devices were really doing the job. My
<br>
guess on the devices are billions of nanobots with wireless communication
<br>
among themselves and the global network.
<br>
<p>Augmented humans with a thousand times the memory and a thousand times the
<br>
processing power would be in the transhuman phase. As Eliezer believes,
<br>
super-intelligence would emerge shortly thereafter.
<br>
<p>Regards,
<br>
<p>Jack
<br>
<p>----- Original Message -----
<br>
From: Christian L. &lt;<a href="mailto:n95lundc@hotmail.com?Subject=Re:%20The%20Human%20Augmentation%20Strategy">n95lundc@hotmail.com</a>&gt;
<br>
To: &lt;<a href="mailto:sl4@sysopmind.com?Subject=Re:%20The%20Human%20Augmentation%20Strategy">sl4@sysopmind.com</a>&gt;
<br>
Sent: Wednesday, July 04, 2001 10:30 AM
<br>
Subject: Re: Friendly AI and Human Augmentation
<br>
<p><p><em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Jack Richardson wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;At the same time, some of those involved in developing AI have the
</em><br>
<em>&gt; &gt;optimistic view that the Singularity will arise in the relatively short
</em><br>
<em>&gt; &gt;time of ten to twenty years. During that same period, the methods of
</em><br>
<em>&gt; &gt;augmenting humans will develop rapidly and become much more
</em><br>
sophisticated.
<br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; If by &quot;augmenting&quot;, you mean things like retinal scanning glasses, smooth
</em><br>
<em>&gt; and flexible wearables with broadband internet connections then I can
</em><br>
agree
<br>
<em>&gt; with you. But if you mean more intrusive technology such as implants, then
</em><br>
I
<br>
<em>&gt; am more skeptical. It might be technically feasible, but the moral panic
</em><br>
<em>&gt; from the &quot;bioethicists&quot; would make it impossible to augment humans in this
</em><br>
<em>&gt; fashion. Animals maybe, but not humans in such a short timeframe.
</em><br>
Remember:
<br>
<em>&gt; cloning is &quot;morally repugnant and against human dignity&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;The optimistic view that the Singularity will arise out of AI development
</em><br>
<em>&gt; &gt;on computer hardware assumes that the complexities of human intelligence
</em><br>
<em>&gt; &gt;can be replicated on machine hardware without any insoluble problems
</em><br>
<em>&gt; &gt;standing in the way. Historically, at least so far, this has not turned
</em><br>
out
<br>
<em>&gt; &gt;to be the case.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; If that were the case historically, we would have AI:s among us, no? If
</em><br>
you
<br>
<em>&gt; say that the case is NOT that we don't have any insoluble problems,
</em><br>
<em>&gt; logically there must exist an insoluble problem in creating AI. Which
</em><br>
<em>&gt; problem is it? :-)
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;Since there are real risks in whatever route we take towards the
</em><br>
<em>&gt; &gt;Singularity, once it begins to be perceived as a possibility by the
</em><br>
larger
<br>
<em>&gt; &gt;population,
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I believe there is a good chance that it will never be perceived as a
</em><br>
<em>&gt; possibility by the larger population.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;it is highly likely there will be a massive reaction with the kind of
</em><br>
<em>&gt; &gt;protests we are seeing today towards the biotechnology companies.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes, and on a much larger scale: the biotech companies are not a threat to
</em><br>
<em>&gt; the national security of every country on earth, which incidentally
</em><br>
<em>&gt; superintelligent AI is. The &quot;massive reaction&quot; will probably not only come
</em><br>
<em>&gt; from militant luddites and anti-[favourite evil here] people, but also
</em><br>
from
<br>
<em>&gt; powerful governments. Since the singularity community consists of only a
</em><br>
<em>&gt; handful of people, the result of a confrontation is clear.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;Without the convincing demonstration of the reliability of friendly AI
</em><br>
<em>&gt; &gt;controls,
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; This looks like the infamous &quot;precautionary principle&quot;: If you cannot
</em><br>
prove
<br>
<em>&gt; that it is harmless, ban it. This principle is much liked by the luddite
</em><br>
<em>&gt; community because it is logically impossible to prove that something is
</em><br>
<em>&gt; harmless, so you can ban just about anything with this principle.
</em><br>
<em>&gt; The moral of the story: you can never convince the world leaders that
</em><br>
<em>&gt; &quot;friendly AI controls&quot; is guaranteed to prevent your SI from converting
</em><br>
the
<br>
<em>&gt; earth to computronium. And even if you can convince them that we will have
</em><br>
a
<br>
<em>&gt; Sysop scenario: Why would they want this? Why would they give up their
</em><br>
power
<br>
<em>&gt; to a machine? In the subconscious mind, the Sysop is a big fat male
</em><br>
<em>&gt; competing for food and mates with the power to grab ALL food and ALL mates
</em><br>
<em>&gt; for itself. Who would want such competition?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;it may be impossible to continue to conduct open AI research.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; The AI research that has the stated goal of constructing superintelligent
</em><br>
AI
<br>
<em>&gt; should not be open or at the very least, not evangelized. At the moment,
</em><br>
<em>&gt; only a few people take this work seriously, but as time progresses, the
</em><br>
need
<br>
<em>&gt; for secrecy might be more apparent.
</em><br>
<em>&gt;
</em><br>
<em>&gt; /Christian
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; _________________________________________________________________________
</em><br>
<em>&gt; Get Your Private, Free E-mail from MSN Hotmail at <a href="http://www.hotmail.com">http://www.hotmail.com</a>.
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1721.html">Christian L.: "Re: The Human Augmentation Strategy"</a>
<li><strong>Previous message:</strong> <a href="1719.html">Christian L.: "Re: Friendly AI and Human Augmentation"</a>
<li><strong>In reply to:</strong> <a href="1719.html">Christian L.: "Re: Friendly AI and Human Augmentation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1721.html">Christian L.: "Re: The Human Augmentation Strategy"</a>
<li><strong>Maybe reply:</strong> <a href="1721.html">Christian L.: "Re: The Human Augmentation Strategy"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1720">[ date ]</a>
<a href="index.html#1720">[ thread ]</a>
<a href="subject.html#1720">[ subject ]</a>
<a href="author.html#1720">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
