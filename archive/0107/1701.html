<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Spoiler Review of A.I.</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Spoiler Review of A.I.">
<meta name="Date" content="2001-07-01">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Spoiler Review of A.I.</h1>
<!-- received="Sun Jul 01 16:00:16 2001" -->
<!-- isoreceived="20010701220016" -->
<!-- sent="Sun, 01 Jul 2001 15:54:56 -0400" -->
<!-- isosent="20010701195456" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Spoiler Review of A.I." -->
<!-- id="3B3F8010.74BE8C51@pobox.com" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Spoiler%20Review%20of%20A.I."><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jul 01 2001 - 13:54:56 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1702.html">Joaquim Almgren Gândara: "VIRUS: Re: Snowhite and the Seven Dwarfs - The REAL story!"</a>
<li><strong>Previous message:</strong> <a href="1700.html">Gordon Worley: "Link to my analysis of AI (the movie)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1706.html">dougkeenan: "Re: Spoiler Review of A.I."</a>
<li><strong>Reply:</strong> <a href="1706.html">dougkeenan: "Re: Spoiler Review of A.I."</a>
<li><strong>Reply:</strong> <a href="1716.html">Durant Schoon: "HUMOR: Re: Spoiler Review of A.I."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1701">[ date ]</a>
<a href="index.html#1701">[ thread ]</a>
<a href="subject.html#1701">[ subject ]</a>
<a href="author.html#1701">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Caution - extremely serious spoilers below; only for those who have
<br>
watched the movie.
<br>
This review may be forwarded or redistributed.
<br>
©2001 by Eliezer Yudkowsky.
<br>
<p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p>Isaac Asimov once observed that there are three types of robot stories;
<br>
robot-as-pathos, robot-as-menace, and robot-as-device.  A.I. is a
<br>
robot-as-device story, and a fairly good one.  There is pathos, owing to
<br>
David's emotions, but David's emotions are depicted as the deliberate
<br>
result of a deliberate design effort.
<br>
<p>Most of the reviewers of this movie will undoubtedly say that the AIs are
<br>
more human than the humans.  This is probably the single least accurate
<br>
statement it is possible to make about A.I.  The AIs are more *humane*
<br>
than the humans but are *substantially* less human.  A few behaviors (for
<br>
the embodied chatbots that were the previous state of the art) or a few
<br>
emotions (for David) have been selectively transferred over, and
<br>
naturally, they tend to be nice and neighborly behaviors or emotions,
<br>
because that's what the designers would want to transfer over.  But the
<br>
AIs are visibly not playing with a full deck.  Evidently Luddite movie
<br>
critics cannot tell the difference between &quot;human&quot; and &quot;humane&quot; even when
<br>
slapped upside the head with a double dissocation.
<br>
<p>The very first thing that struck me about A.I. was the rather extreme
<br>
stupidity of the AI *researchers*.  The consequences of this stupidity are
<br>
depicted with the same realism, attention to detail, and lack of
<br>
anthropomorphism that characterized HAL in 2001, but even so, the amount
<br>
of human stupidity I am being asked to accept is rather extreme.
<br>
<p>David is beta software.  His emotional responses are real - we are told so
<br>
in the movie - but they show a binary, all-or-nothing quality.  We see the
<br>
first instance of this where David bursts out into extremely loud
<br>
laughter, laughs for a few moments, then switches off.  Be it emphasized
<br>
that this laughter is both realistic and genuine.  David is the first in a
<br>
line of robots with genuine emotions.  The embodied chatbot that we see in
<br>
the opening scenes of the movie - the female android whose hand is hurt -
<br>
may have more gentle laughter, but only because it is preprogrammed. 
<br>
David's genuine responses are as raw and as alien as might be expected of
<br>
a &quot;child&quot; who is, in fact, an infant, only a few weeks old.
<br>
<p>Then the AI researchers had the bright idea of putting this beta software
<br>
into a human body, adding in those emotions calculated to produce maximal
<br>
emotional attachment on the part of humans, and giving it as a human
<br>
surrogate to a mother already in an emotionally unstable state because her
<br>
own child has been in medical cryonic suspension for five years.
<br>
<p><em>&gt;From this single act of stupidity, and the correctly depicted
</em><br>
consequences, the plot of the entire movie flows.  Within a day of
<br>
imprinting, David realizes that his mother will someday die, and that he
<br>
will not, and wonders if he will be alone forever - foreshadowing the end
<br>
of the movie.  His mother, for whom David is allegedly an artificial
<br>
surrogate to be disposed of when no longer needed, naturally feels
<br>
enormous emotional stress at the thought of returning David to be
<br>
incinerated.  Nobody thought of this back when they were building a
<br>
loving, lovable, naturally immortal, allegedly disposable child?
<br>
<p>(One of the genuine, oft-overlooked ethical questions this movie
<br>
highlights:  &quot;Is it moral to create an AI that loves you if the AI has to
<br>
watch you die?&quot;  The prospect of voluntary immortality in our own near
<br>
future creates similar present-day issues.  If you plan on bringing a
<br>
child into the world, you should plan on choosing to live forever if the
<br>
option becomes available, because a child shouldn't have to watch its
<br>
parents die.)
<br>
<p>When David's brother, Martin, returns from suspension, we see a darker
<br>
side to David's genuine emotions.  The first near-catastrophe occurs when
<br>
David nearly kills himself competing with his revived brother, by
<br>
attempting to eat; the second catastrophe occurs when David nearly drowns
<br>
his brother.  In both cases, the events that occur are excellent
<br>
robot-as-device scenarios; they are the consequence of the reproduction of
<br>
certain specific geunine emotions in a beta-quality infant psychology
<br>
taught certain preprogrammed complex behaviors and placed the body of an
<br>
eight-year-old.  When David's pain response is triggered by a pack of
<br>
curious children, his raw fear, like his laughter, goes from binary off to
<br>
binary on.  His fear manifests itself in the only behavioral response
<br>
David knows; hiding behind Martin.  The fear continues to manifest,
<br>
preventing Martin's escape, even as David and Martin sink to the bottom of
<br>
the pool.
<br>
<p>Again, realistic; again, the AI researchers should have thought of it. 
<br>
Monica, the mother, is afterwards in a hideous position; does she endanger
<br>
the household by keeping around beta-quality embodied software, or does
<br>
she return David to the manufacturer - that is, give up her child to die? 
<br>
Monica's emotions are also run ragged because she is being asked to react
<br>
without anger to David's near-drowning of Martin.  Again, someone at the
<br>
mecha corporation was being damn stupid and deserves to be sued into
<br>
bankruptcy.  You do not give embodied software with beta-quality genuine
<br>
emotions to a human mother and ask her to treat it as her own human child.
<br>
<p>(Call it &quot;personality abrasion&quot;.  Personality abrasion may turn out to be
<br>
a very real problem for humans dealing with any AI capable of real
<br>
thought, even if the AIs don't have human-architecture emotions or
<br>
human-looking bodies.  Only AI researchers, or other people who understand
<br>
the risks and are willing to expend effort in dealing with them, should
<br>
ever come into contact with raw AIs.  A Friendly AI conversing with
<br>
ordinary users should have enough knowledge to fake taking 'offense' at
<br>
insults, just because an AI that genuinely doesn't care at all about
<br>
insults may be more alienness than an ordinary user should have to deal
<br>
with.  In A.I., we see the effect of personality abrasion on some poor
<br>
shmuck of a human mother.)
<br>
<p>The penultimate consequences of the AI researchers' stupidity is visible
<br>
when, following the near-drowning of Martin, Monica (the mother) tries to
<br>
return David to the manufacturer for destruction.  Of course Monica is, by
<br>
this point, too attached to David to watch him die, and tries to abandon
<br>
him in the woods instead.  David's extreme response, when he suddenly
<br>
realizes that his mother is abandoning him, is the movie's greatest
<br>
moment.  I choked up myself.  David is an AI with a few genuine emotions,
<br>
and the strongest of them is love, and now his mother is leaving forever.
<br>
<p>(Genuine, affecting pathos in a robot-as-device story.  Realistic,
<br>
theoretically accurate AI scenarios with powerful drama.  All hail
<br>
Kubrick.  However... am I really supposed to believe that nobody at the
<br>
mecha corporation saw this coming?)
<br>
<p>Later:  David, wandering the forest with only his supertoy
<br>
babysitter-in-a-box teddy bear as companion, comes into contact with a
<br>
group of androids who are scavenging spare parts from a dump.  This, I'm
<br>
sure, is intended to be creepy and disturbing vintage Kubrick, but I
<br>
myself immediately started wondering how this social phenomenon occurred. 
<br>
It's the same question that occurred to me when I saw Gigolo Joe carving
<br>
out his identity tag on the run from the police.  Why do these
<br>
nonemotional androids want to survive?  We see in the opening scenes a
<br>
female android who is stabbed in the hand as part of a demonstration; when
<br>
the lead AI researcher asks &quot;Did I hurt you?&quot; she responds &quot;You hurt my
<br>
hand.&quot;  Am I supposed to believe that this chatbot in human form would go
<br>
and scavenge parts if she were abandoned?  Am I supposed to believe that
<br>
Gigolo Joe, on realizing that he has been framed for murder, would go
<br>
rogue out of self-preservation?  Having androids scouring the countryside
<br>
for spare parts is a rather disturbing social phenomenon, as is having an
<br>
android flee a police investigation, and the embodied chatbots that are
<br>
supposed to be state-of-the-art are primitive enough that the programmers
<br>
could easily have prevented both responses.
<br>
<p>And what's with the Flesh Fair bounty hunters who attack the scavenging
<br>
robots?  Did these bounty hunters come through a wormhole from
<br>
_Bladerunner_?  This is what happens when Spielberg rewrites a Kubrick
<br>
movie; you have cyberpunk grunge-neon motorcycle bounty hunters chasing a
<br>
lovable android and his animate teddy bear.  At any rate, David is dragged
<br>
off to the Flesh Fair, where humans watch the destruction of androids for
<br>
fun... is this where the path of &quot;Battlebots&quot; leads?
<br>
<p>(At this point in the movie, I must admit to a minor objection at the
<br>
Flesh Fair robot who asked another robot to 'disconnect my pain circuits',
<br>
mostly because this is a fundamentally human way of looking at the world
<br>
and any robot who makes this request may well have crossed the border, not
<br>
just into personhood, but into our particular kind of personhood.  But
<br>
expecting Hollywood to know that is asking far too much.)
<br>
<p>At the Flesh Fair, the embodied chatbots make a few conversational pleas
<br>
as they are loaded into the cannons and the acid platforms.  David's
<br>
screams invoke greater sympathy, but I'm not sure the Flesh Fair audience
<br>
made a logical conclusion.  I know that David's response is genuine only
<br>
because I was told at the beginning of the movie that David has a wholly
<br>
novel cognitive architecture designed to support humanlike emotions. 
<br>
David's response is genuine, but it is not humanlike.  A human child,
<br>
brought into that cage, would have been almost catatonic with fear; would
<br>
have been screaming and crying long before reaching the stage; would have
<br>
been struggling long before the first drop of acid fell on him.  As at the
<br>
side of the pool, we see the binary, unpolished quality of David's genuine
<br>
emotion; his fear goes from off to on as soon as the first drop of acid
<br>
falls - and manifests in his screaming requests not to be burned.
<br>
<p>And the crowd rises and boos the ringmaster off the stage - &quot;Mecha does
<br>
not plead for its life!&quot; - but their decision is correct only by
<br>
coincidence.  From what they saw, David really could have been just a more
<br>
advanced chatbot.  David's emotions were real, but David's behaviors
<br>
weren't the responses of a genuine eight-year-old except on the surface.
<br>
<p>Shortly thereafter, the stranger half of the movie begins.  David, in the
<br>
company of Gigolo Joe, wanders the world looking for the Blue Fairy.  Even
<br>
for beta software, I'm not sure this fixation is realistic - surely an
<br>
advanced AI knows what 'fiction' is, and an AI boy knows that bedtime
<br>
stories aren't true.  On the other hand, perhaps David's humanlike
<br>
cognitive architecture has unexpectedly given rise to the phenomenon of
<br>
self-delusion (flinching away from hypotheses which make unpleasant
<br>
predictions), or perhaps David knows the Blue Fairy's existence is
<br>
tentative but he still sees no more plausible path leading back to his
<br>
mother.
<br>
<p>After Joe and David leave Dr. Know, the movie has its first real &quot;Damn,
<br>
they blew it!&quot; moment.  (Though in Spielberg's defense, an AI movie that
<br>
starts at 8PM, and gets to 9:48 before messing up, has done extremely
<br>
well.)  The moment to which I refer is Gigolo Joe's speech about how
<br>
humans resent robots because they know that, in the end, robots will be
<br>
all that's left.  Where did *that* come from?  Joe's speech is as out of
<br>
place as Agent Smith's speech of self-justification in _The Matrix_.  It
<br>
has undertones of repressed resentment, of an entire underground society
<br>
of secretly rebellious robots, and other things that have no place among
<br>
chatbots and sex droids.  Even David is only a fractional human; he has a
<br>
few selected genuine emotions but certainly not a full deck of them.
<br>
<p>Apparently the Humans Are Obsolete Speech is simply mandatory for AI
<br>
movies, no matter how ridiculously out of place.  The Speech is most
<br>
certainly not justified by &quot;foreshadowing&quot;, since it sucks at least half
<br>
of the emotional impact out of the ending.  If anyone creates a Phantom
<br>
Edit of A.I., the Speech should definitely be the first thing to go (and
<br>
the second thing, of course, will be everything after the Blue Fairy
<br>
Fadeout).
<br>
<p>But I'm getting ahead of myself.  The next major scene of significance is
<br>
David confronting David-2.  David's destruction of David-2 struck me as a
<br>
little strange; it involved a bit more humanness, a wider behavioral
<br>
repertoire, than had been previously depicted.  I suppose that some degree
<br>
of jealousy was visible earlier in the movie, so my immediate reaction of
<br>
&quot;Why would they have ported *that* emotion over?&quot; may be misplaced; even
<br>
so, that kind of directed, coherent-conversation destructive tantrum
<br>
struck me as being too complex for David.
<br>
<p>The lead AI researcher's total lack of reaction to the destruction of his
<br>
own genuinely emotional surrogate child, and his revelation that the
<br>
corporation has been directing the entire course of events since the Flesh
<br>
Fair for publicity purposes, shows again that the AI researchers are the
<br>
least humane people in the movie.
<br>
<p>Later on, David confronts the vast hall full of Davids, a scene that was
<br>
intended to creep out the audience.  But again it gives rise to questions
<br>
on my part.  If there are that many Davids, why are they all designed to
<br>
have the human emotion of wanting to be unique?  Was it an unintended
<br>
consequence?  For that matter, what possessed the idiots in Marketing to
<br>
produce a batch of identical AIs all named David, instead of giving them
<br>
individual faces and individual voices and maybe some quirks of
<br>
personality?  Do these people think that no two couples with a David will
<br>
ever meet?  I'm not a parent, but I know that I'd be creeped out if I went
<br>
to a barbeque and every couple there had a copy of my little sister.
<br>
<p>Finally, after David realizes that he is not unique, he deliberately
<br>
topples off a window ledge into the ocean.  Uh... why?  How is that a
<br>
means to the end of getting his mother to love him?  Or alternatively, who
<br>
drew up the design specs and added in a requirement that David feel
<br>
suicidal despondency under certain conditions?  Ordinary despondency I can
<br>
see, but not suicidal despondency; not in an expensive, partially human
<br>
being that parents are supposed to grow attached to.  Plus, David can
<br>
operate underwater, and he knows that.  This scene makes no sense.
<br>
<p>Later, when David seeks out the Blue Fairy, and begins repeating his
<br>
eternal request, and the screen fades to black, I had the same reaction
<br>
everyone did:  &quot;Okay, movie's over!  Please tell me the movie's over...
<br>
damn, it's not over.&quot;  The Phantom Edit version of A.I. should end here.
<br>
<p>After the Blue Fairy Fadeout, we see what I can only describe as Spielberg
<br>
messing up Kubrick's movie.  To start with, the aliens - pardon me, I
<br>
meant the Successors - are Spielbergs.  &quot;Spielbergs&quot;; that's the only
<br>
thing I can think of to call them.  They are classic Spielberg aliens and
<br>
they don't belong on the set of A.I.
<br>
<p>Lest I be too negative, however, I'll take this time to focus on an
<br>
example of what A.I. does right.  David, revived by the Successors, leaves
<br>
the aircraft and heads for the Blue Fairy.  He touches her, and she
<br>
shatters.  At this point, a *bad* movie - which A.I. is not - would have
<br>
shown us some breakdown, some feeling of despair on David's part. 
<br>
Instead, nothing happens - there isn't any emotion in David's limited deck
<br>
for this occasion.  Three cheers for whoever wrote that scene!  It's this
<br>
refusual to take the easy way out that puts A.I. into the class of science
<br>
fiction rather than space opera.
<br>
<p>However, we then move directly on to the second &quot;Damn, they blew it!&quot;
<br>
moment in the movie, occurring at 10:28, when one of the Successors begins
<br>
spouting gibberish about yada-yada space-time yada-yada pathways yada-yada
<br>
DNA yada-yada only one day yada-yada.  I'm sorry, I don't care how
<br>
dramatic your plot device is, you need to think up a better way to justify
<br>
it than making up totally arbitrary rules on the spot.  Plus, if you can
<br>
bring back Monica for one day, you can scan her memories into permanent
<br>
storage; and, if they're retrieving Monica's immortal soul from 2000 years
<br>
in the future, they should be retrieving an old Monica from just before
<br>
the moment of her death, not the one David remembers... oh, forget it.
<br>
<p>Finally, David gets his one day with Monica - being a little too human
<br>
throughout, it seemed to me, especially as he watches her go to sleep for
<br>
the last time.  He goes to sleep with her, and - according to the final
<br>
voiceover - for the first time, begins to dream.  Dream *what*?  Why?  I
<br>
wasn't really happy with this movie's ending.
<br>
<p>One of the basic issues at the beginning of the movie is one that the
<br>
ending totally fails to resolve, even after going to all that plot-effort
<br>
to bring David to the one place where the question can be answered.  David
<br>
is a partial human.  He is both immortal, and fundamentally incomplete. 
<br>
David was created without the potential to grow; he is forever young...
<br>
but on the other side of time, he can be improved and extended.  David
<br>
could become a real human, if he wanted to be.  Except that David doesn't
<br>
want to be human; he wants to stay with Monica forever, and being human is
<br>
only a means to that end.
<br>
<p>The Successors could easily have given David a full deck of emotions, or
<br>
could easily have created an immortal virtual Monica that was real to the
<br>
limit of David's limited perceptions.  Why didn't they?  Was David, by
<br>
their standards, citizen enough not be lied to?  Citizen enough not be
<br>
'improved' without consent?  I know how I would have solved that problem;
<br>
I would have made David human for the course of the one perfect day he had
<br>
with Monica, and at the end of that day, he would have experienced great
<br>
grief... but he would have healed, and moved on, as complete humans have
<br>
the potential to do, and eventually joined the Successor civilization. 
<br>
Both the moment of David becoming human, and the moment of his grief when
<br>
Monica faded, would have been a fine conclusion to the movie.
<br>
<p>The ending I saw left me feeling incomplete because this basic issue went
<br>
unresolved.  From the beginning, there were only four possible resolutions
<br>
to the movie:  David dies; David lives forever with Monica, eternally
<br>
happy; David lives forever without Monica, eternally lonely; or David
<br>
grows beyond his limits.  The ending we saw doesn't tell us which of these
<br>
events has occurred!  Did David effectively switch himself off?  Did David
<br>
go on forever dreaming of his last perfect day?  Does David's dreaming
<br>
indicate that the Successors have gently begun to improve him out of his
<br>
cul-de-sac?  Are David's dreams eternally lonely because Monica isn't
<br>
there?
<br>
<p>I know there is a certain style of filmmaking that holds that the viewer
<br>
should be allowed to pick their own ending, and I hate that style with a
<br>
fiery passion.  For me, a vague ending can ruin the impact of an entire
<br>
movie, and that came very close to happening with A.I.
<br>
<p>Oh, well.  A.I. is still a good movie.  It's just that, as with many good
<br>
movies, A.I. could easily have been so much better.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1702.html">Joaquim Almgren Gândara: "VIRUS: Re: Snowhite and the Seven Dwarfs - The REAL story!"</a>
<li><strong>Previous message:</strong> <a href="1700.html">Gordon Worley: "Link to my analysis of AI (the movie)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1706.html">dougkeenan: "Re: Spoiler Review of A.I."</a>
<li><strong>Reply:</strong> <a href="1706.html">dougkeenan: "Re: Spoiler Review of A.I."</a>
<li><strong>Reply:</strong> <a href="1716.html">Durant Schoon: "HUMOR: Re: Spoiler Review of A.I."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1701">[ date ]</a>
<a href="index.html#1701">[ thread ]</a>
<a href="subject.html#1701">[ subject ]</a>
<a href="author.html#1701">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
