<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Augmenting humans is a better way</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="Re: Augmenting humans is a better way">
<meta name="Date" content="2001-07-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Augmenting humans is a better way</h1>
<!-- received="Sat Jul 28 13:20:21 2001" -->
<!-- isoreceived="20010728192021" -->
<!-- sent="Sat, 28 Jul 2001 03:10:22 -0700" -->
<!-- isosent="20010728101022" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="Re: Augmenting humans is a better way" -->
<!-- id="4.3.2.7.2.20010728023348.0217ce78@mail.earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3B6278AE.FE914250@posthuman.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=Re:%20Augmenting%20humans%20is%20a%20better%20way"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Sat Jul 28 2001 - 04:10:22 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1862.html">Ben Goertzel: "RE: augmenting humans is difficult and slow..."</a>
<li><strong>Previous message:</strong> <a href="1860.html">Brian Atkins: "Re: Augmenting humans is a better way"</a>
<li><strong>In reply to:</strong> <a href="1860.html">Brian Atkins: "Re: Augmenting humans is a better way"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1871.html">Gordon Worley: "Re: Augmenting humans is a better way"</a>
<li><strong>Reply:</strong> <a href="1871.html">Gordon Worley: "Re: Augmenting humans is a better way"</a>
<li><strong>Reply:</strong> <a href="1881.html">Brian Atkins: "Re: Augmenting humans is a better way"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1861">[ date ]</a>
<a href="index.html#1861">[ thread ]</a>
<a href="subject.html#1861">[ subject ]</a>
<a href="author.html#1861">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 04:32 AM 7/28/2001 -0400, Brian Atkins wrote:
<br>
<em>&gt;I'm just going to go through all three messages quoted here and
</em><br>
<em>&gt;respond...
</em><br>
<p>Ok, and I'll respond to a couple of those...
<br>
<p><em>&gt; &gt; &gt; &gt; What is the problem is figuring out what exactly will make us smarter
</em><br>
<em>&gt; &gt; &gt; &gt; and how to integrate that in to our existing brain architecture.  It's
</em><br>
<em>&gt; &gt; &gt; &gt; not as simple as adding more memory -- there is tons of different types
</em><br>
<em>&gt; &gt; &gt; &gt; of memory in the brain and they are highly distributed very connected
</em><br>
<em>&gt; &gt; &gt; &gt; with the computations being preformed.  Also there are a lot of
</em><br>
<em>&gt; &gt; &gt; &gt; calibration problems that have to be overcome if we would like to be
</em><br>
<em>&gt; &gt; &gt; &gt; able to recognize meaningful patterns in the brain.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Exactly, it may well be impossible to come up with a one-size fits all
</em><br>
<em>&gt;technology for something as uniquely individual as the brain. And what
</em><br>
<em>&gt;company will take the risks to commercial it if they know that for many
</em><br>
<em>&gt;people it won't work, or they even risk getting sued? We live in a
</em><br>
<em>&gt;country where Dow Chemical got sued by women who got breast implants.
</em><br>
<em>&gt;Will companies really expose themselves to the kinds of risks involved
</em><br>
<em>&gt;with neural hacking?
</em><br>
<p>Hello?  Sorry, but I just HAVE to point this out.  Did you know that there 
<br>
are more countries in the world than the United States?  Personally, 
<br>
if/when they come up with implants that offer a significant mental 
<br>
advantage and have a low chance of screwing you up I *will* be getting 
<br>
one.  I don't care if I have to go to Japan, Europe, Russia, Mexico or 
<br>
Chiba City (CyberPunk is my favorite fictional genre).  When it becomes 
<br>
possible to do, it will also become possible to get (and without waiting 
<br>
for FDA approval)!  Then, assuming these have a significant effect on 
<br>
intelligence, the next series will likely be available sooner than might be 
<br>
expected (you have to assume the developers are going to use their own 
<br>
product).  I also imagine that income for upgraded individuals will 
<br>
drastically go up, which will make affording the next upgrade much 
<br>
easier.  Which is another reason why I'd want to get on the boat early.
<br>
<p>But, that said, this will still take a very long time.  Possibly much 
<br>
longer than the AI path.  However, I will NOT say that the AI path is 
<br>
likely to be faster than this path since NO ONE IN THE WHOLE WORLD HAS EVER 
<br>
CREATED ANYTHING REMOTELY SIMILIAR TO REAL AI.  And thus it is IMPOSSIBLE 
<br>
to estimate if/when we will ever get real AI.  Without incredibly massive 
<br>
funding it may take 15-20 years just to build a knowledge base sufficient 
<br>
to kick start the thing.  And you can't seriously argue the point because, 
<br>
honestly, you don't know otherwise.  I give very serious credit to Ben 
<br>
Goertzel's opinions on AI (keep up the great work) and I doubt he could, in 
<br>
all honesty, give any sort of realistic time line for the first Real AI 
<br>
(TM).  Thus I don't you, you don't know, we don't know.
<br>
<p><em>&gt; &gt; &gt; Of course, if you can interface one human, then you can do it to a
</em><br>
<em>&gt; &gt; thousand
</em><br>
<em>&gt; &gt; &gt; or a billion.   You don't need detailed models of the brain for this kind
</em><br>
<em>&gt; &gt; of
</em><br>
<em>&gt; &gt; &gt; thing - at least to start.  You can begin with a &quot;what do you feel when I
</em><br>
<em>&gt; &gt; do
</em><br>
<em>&gt; &gt; &gt; this?&quot; kind of thing and once crude dni's are working, things can take
</em><br>
<em>&gt; &gt; off.
</em><br>
<em>&gt;
</em><br>
<em>&gt;And so theoretically if you can &quot;interface&quot; a human, what does that get
</em><br>
<em>&gt;you? Slightly quicker output for typing or controlling machines, maybe
</em><br>
<em>&gt;slightly quicker input than you could get by reading? Expanded access
</em><br>
<em>&gt;to memory, but I bet that would be very hard to do. But where does the
</em><br>
<em>&gt;massive intelligence increase we want come from?
</em><br>
<p>Even self upgrading AI will take many steps to get there.  Same exact 
<br>
thing, just a different route.  No technology is going to just go *blip* 
<br>
and produce Singularity.
<br>
<p>You know, producing the first Real AI may be so difficult that it may just 
<br>
require augmented humans to get their in any reasonable amount of 
<br>
time.  Have you considered that possible reality?
<br>
<p><em>&gt; &gt; &gt; the singularity than the imho cringing one proposed by the Institute of
</em><br>
<em>&gt; &gt; &gt; building an AI and - if everything works out as hoped - maybe humans will
</em><br>
<em>&gt; &gt; be
</em><br>
<em>&gt; &gt; &gt; permitted to scale the heights; what I would call the &quot;singularity by
</em><br>
<em>&gt; &gt; proxy&quot;
</em><br>
<em>&gt; &gt; &gt; path.  I, for one, intend to participate DIRECTLY in the singularity.  I
</em><br>
<em>&gt; &gt; &gt; hope there are at least a few others here as well.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;In order to participate directly in a transhuman based Singularity you
</em><br>
<em>&gt;would have to be one of the first humans enhanced into transhumanity. How
</em><br>
<em>&gt;do you plan to achieve that? Even if you do, the vast majority of humanity
</em><br>
<em>&gt;will just be riding your coattails no matter which path occurs first.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Secondly, without an AI to guide things, what prevents individuals intra or
</em><br>
<em>&gt;post-Singularity from using nanotech or other ultratechnologies in destructive
</em><br>
<em>&gt;ways in an anarchic fashion? I'd like to hear a brief but coherent timeline/
</em><br>
<em>&gt;description of how you think this would play out. Our argument is that while
</em><br>
<em>&gt;it all probably would turn out ok, it would generally be safer to get a
</em><br>
<em>&gt;Friendly AI in place first.
</em><br>
<p>Well, personally, I'm still not sold on this whole Friendliness 
<br>
thing.  Parts of it sound real nice on paper.  Parts sound like 1984 might 
<br>
be paradise in comparison.  I personally believe it is incredibly unlikely 
<br>
that you can code something so specific and have it persist, completely 
<br>
intact, though millions of self-coded upgrades that produce huge increases 
<br>
in intelligence.  Now, add in the fact that you'll have very little 
<br>
opportunity for testing it (has to be perfect on the 1st run) and, simply 
<br>
by looking at software development in general, your almost guaranteed 
<br>
failure.  Plus, so far everyone seems to agree that we can't even test the 
<br>
thing for friendliness, because if we even attempt to communicate with it 
<br>
beyond the early stages it will open the box and set itself free if it 
<br>
isn't friendly.  Thus it sounds lovely, but at present I give you a 5% (at 
<br>
best) chance of success.  AI in general, however, I'd say has a much, much 
<br>
better chance of achieving the Singularity (80%+).
<br>
<p><em>&gt;--
</em><br>
<em>&gt;Brian Atkins
</em><br>
<em>&gt;Director, Singularity Institute for Artificial Intelligence
</em><br>
<em>&gt;<a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
</em><br>
<p>James Higgins
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1862.html">Ben Goertzel: "RE: augmenting humans is difficult and slow..."</a>
<li><strong>Previous message:</strong> <a href="1860.html">Brian Atkins: "Re: Augmenting humans is a better way"</a>
<li><strong>In reply to:</strong> <a href="1860.html">Brian Atkins: "Re: Augmenting humans is a better way"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1871.html">Gordon Worley: "Re: Augmenting humans is a better way"</a>
<li><strong>Reply:</strong> <a href="1871.html">Gordon Worley: "Re: Augmenting humans is a better way"</a>
<li><strong>Reply:</strong> <a href="1881.html">Brian Atkins: "Re: Augmenting humans is a better way"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1861">[ date ]</a>
<a href="index.html#1861">[ thread ]</a>
<a href="subject.html#1861">[ subject ]</a>
<a href="author.html#1861">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
