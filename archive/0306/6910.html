<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-06-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI's flawed friendliness analysis</h1>
<!-- received="Mon Jun  2 12:39:08 2003" -->
<!-- isoreceived="20030602183908" -->
<!-- sent="Mon, 2 Jun 2003 11:46:46 -0700" -->
<!-- isosent="20030602184646" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: SIAI's flawed friendliness analysis" -->
<!-- id="200306021846.h52Ikmx06612@sophia.objectent.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="Pine.GSO.4.44.0305291037360.2056-100000@demedici.ssec.wisc.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Mon Jun 02 2003 - 12:46:46 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6911.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6909.html">Gordon Worley: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="../0305/6839.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6911.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6911.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6910">[ date ]</a>
<a href="index.html#6910">[ thread ]</a>
<a href="subject.html#6910">[ subject ]</a>
<a href="author.html#6910">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Thursday 29 May 2003 08:38 am, Bill Hibbard wrote:
<br>
<em>&gt; On Mon, 26 May 2003, Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;   Any artifact implementing &quot;learning&quot; and capable of at
</em><br>
<em>&gt;   least N mathematical operations per second must have &quot;human
</em><br>
<em>&gt;   happiness&quot; as its only initial reinforcement value. Here
</em><br>
<em>&gt;   &quot;learning&quot; means that system responses to inputs change
</em><br>
<em>&gt;   over time, and &quot;human happiness&quot; values are produced by an
</em><br>
<em>&gt;   algorithm produced by supervised learning, to recognize
</em><br>
<em>&gt;   happiness in human facial expressions, voices and body
</em><br>
<em>&gt;   language, as trained by human behavior experts.
</em><br>
<em>&gt;
</em><br>
<p>This has long been considered unworkable as a standard of ethics.  
<br>
The term &quot;happiness&quot; is very ill-defined and a moving target.  What 
<br>
of some who exhibit &quot;happiness&quot; in the misfortune of others, for 
<br>
instance.  Clearly something more than &quot;happiness&quot; is needed for a 
<br>
rational benign ethical system.
<br>
<p><em>&gt;
</em><br>
<em>&gt; 2. How the regulation can be enforced.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Enforcement is a hard problem. It helps that enforcement is
</em><br>
<em>&gt; not necessary indefinitely. It is only necessary until the
</em><br>
<em>&gt; singularity, at which time it becomes the worry of the
</em><br>
<em>&gt; (hopefully safe) singularity AIs. There is a spectrum of
</em><br>
<em>&gt; possible approaches of varying strictness. I'll describe
</em><br>
<em>&gt; two:
</em><br>
<em>&gt;
</em><br>
<em>&gt; a. A strict approach.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Disallow all development of &quot;learning&quot; machines capable of
</em><br>
<em>&gt; at least N operations per second, except for a government
</em><br>
<em>&gt; safe AI project (and exempt &quot;mundane&quot; learning applications).
</em><br>
<em>&gt; This would be something like the Manhattan Project (only the
</em><br>
<em>&gt; government is allowed to build nuclear weapons, although
</em><br>
<em>&gt; contractors are involved).
</em><br>
<em>&gt;
</em><br>
<p>Uh huh.  You mean the same government that bombs countries on 
<br>
supposition of WMD and that reserves the right to attack any country 
<br>
or group it considers a possible threat preemptively?  Why do I feel 
<br>
like &quot;government safe AI&quot; is an oxymoron?  
<br>
<p><em>&gt;
</em><br>
<em>&gt; The focus for detecting illegal projects could be on computing
</em><br>
<em>&gt; resources and on expert designers. Computing chips are widely
</em><br>
<em>&gt; available, but chip factories aren't. There is already talk of
</em><br>
<em>&gt; using the concentration of ownership of chip manufacturing to
</em><br>
<em>&gt; implant copyright protection in every chip. Its called TCPA
</em><br>
<em>&gt; and I'm against it - see my article at:
</em><br>
<em>&gt;
</em><br>
<p>Yes.  Forget about intellectual freedom and the revolutionary 
<br>
possibilities of computation if we go down that road.  Since you are 
<br>
drawing on things you yourself would not like to see I assume that 
<br>
you are arguing yourself into believing your proposal is unworkable?
<br>
<p><em>&gt; Illegal projects could also be detected through their need for
</em><br>
<em>&gt; expert designers. As long as the police are not corrupt or lazy
</em><br>
<em>&gt; (hence the need for an aggressive public movement driving
</em><br>
<em>&gt; aggressive enforcement), they can develop and exploit informers
</em><br>
<em>&gt; among any outlaw community. Its hard to do an ambitous project
</em><br>
<em>&gt; like creating AI without a lot of people knowing something
</em><br>
<em>&gt; about it. They are vulnerable to bribes, and they get into
</em><br>
<em>&gt; feuds and turn each other in.
</em><br>
<em>&gt;
</em><br>
<p>Wonderful.  An all powerful state assumed to be non-corrupt will safe 
<br>
us from the danger of an all powerful AI?  Hmmmm.
<br>
<p><em>&gt;
</em><br>
<em>&gt; Internationally, there could be treaties analogous to those
</em><br>
<em>&gt; controlling certain types of weapons. These would prohibit
</em><br>
<em>&gt; military use of learning machines capable of more than N
</em><br>
<em>&gt; operations per second, and would set up international bodies
</em><br>
<em>&gt; analogous to the IAEA for coordinating regulation and
</em><br>
<em>&gt; inspection.
</em><br>
<em>&gt;
</em><br>
<p>You mean treaties like the one's the US unilaterally decided to back 
<br>
out of?
<br>
<p><em>&gt; 4. The consent of the governed.
</em><br>
<em>&gt;
</em><br>
<em>&gt; AI and the singularity will be so much better if the public
</em><br>
<em>&gt; is informed and is in control via their elected governments.
</em><br>
<em>&gt; It is human nature for people to resist changes that are
</em><br>
<em>&gt; forced on them. If we respect humanity enough to want a safe
</em><br>
<em>&gt; singularity for them, then we should also respect them
</em><br>
<em>&gt; enough to get the public involved and consenting to what is
</em><br>
<em>&gt; happening.
</em><br>
<em>&gt;
</em><br>
<p>On the contrary, the public is incapable/unwilling to 
<br>
understand/consider the issues.  The elected officials are interested 
<br>
in power and little more capable of wisely governing such work than 
<br>
the people themselves.  You choose to arbitrarily assume that this is 
<br>
not so despite the evidence all around you. 
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6911.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6909.html">Gordon Worley: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="../0305/6839.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6911.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6911.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6910">[ date ]</a>
<a href="index.html#6910">[ thread ]</a>
<a href="subject.html#6910">[ subject ]</a>
<a href="author.html#6910">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
