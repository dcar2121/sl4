<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: FW: DGI Paper</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: FW: DGI Paper">
<meta name="Date" content="2002-04-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: FW: DGI Paper</h1>
<!-- received="Sat Apr 13 19:45:26 2002" -->
<!-- isoreceived="20020414014526" -->
<!-- sent="Sat, 13 Apr 2002 19:17:42 -0400" -->
<!-- isosent="20020413231742" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: FW: DGI Paper" -->
<!-- id="3CB8BC96.A378EB00@pobox.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJCEDBCGAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20FW:%20DGI%20Paper"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Apr 13 2002 - 17:17:42 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3341.html">Ben Goertzel: "RE: FW: DGI Paper"</a>
<li><strong>Previous message:</strong> <a href="3339.html">Samantha Atkins: "Re: Why bother (was Re: Introducing myself)"</a>
<li><strong>In reply to:</strong> <a href="3338.html">Ben Goertzel: "FW: DGI Paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3341.html">Ben Goertzel: "RE: FW: DGI Paper"</a>
<li><strong>Reply:</strong> <a href="3341.html">Ben Goertzel: "RE: FW: DGI Paper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3340">[ date ]</a>
<a href="index.html#3340">[ thread ]</a>
<a href="subject.html#3340">[ subject ]</a>
<a href="author.html#3340">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Hey Eliezer,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Here are some fairly fine-grained comments on the theory of mind embodied in
</em><br>
<em>&gt; your excellent paper on Deliberative General Intelligence.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Overall I find the paper to be most outstanding, and I think the general
</em><br>
<em>&gt; ideas you lay out there are highly consistent with the thinking underlying
</em><br>
<em>&gt; my own AI work.  Of course, due to the generality of your ideas, they are
</em><br>
<em>&gt; bound to be consistent with a great variety of different concrete AI
</em><br>
<em>&gt; systems.
</em><br>
<p>Heck, most of it is supposed to be consistent with humans!  So I'd have to
<br>
agree with you there.
<br>
<p><em>&gt; In these comments I'll focus on areas of disagreement &amp; areas where your
</em><br>
<em>&gt; statements confuse me, although these are relatively minor in the grand
</em><br>
<em>&gt; scheme of the paper....
</em><br>
<p>Thanks.  Incidentally, I did say at the end that the paper wasn't complete. 
<br>
I meant it.  I'd originally planned a 200K paper in one month; instead I
<br>
wound up doing a roughly 400K paper in three months.  I was already worried
<br>
that you were going to have a heart attack and insist that I chop half of it
<br>
out, which I'm not sure I could've.  So I had to omit a few things.  In
<br>
fact, I had to omit everything that could reasonably be left out without
<br>
crippling the theory.
<br>
<p>In this case I mentioned, but did not go into detail on, realtime skills and
<br>
the reflective modality.  Of course this is roughly equivalent to talking
<br>
about the brain and &quot;mentioning but not explaining&quot; the cerebellum and
<br>
prefrontal cortex.  Sorry about that, but as I said I was already worried
<br>
you were going to have a heart attack about the length.  If you seriously
<br>
think there's room for the chapter to become substantially longer, I may be
<br>
able to find time to expand it (and maybe not; I've already spent far too
<br>
much time).
<br>
<p>Anyway, most of the things you mention below need to be explained with
<br>
reference to reflectivity, realtime skills, or even realtime reflective
<br>
skills.
<br>
<p><em>&gt; 1)
</em><br>
<em>&gt; I think your characterization of concepts is in places too narrow.  Your
</em><br>
<em>&gt; statement “concepts are patterns that mesh with sensory imagery”  seems to
</em><br>
<em>&gt; me to miss abstract concepts and also concepts related to action rather than
</em><br>
<em>&gt; perception.  I realize that later on you do mention abstract concepts.
</em><br>
<p>Concepts can generalize over the perceptual correlates of realtime skills
<br>
and generalize over reflective percepts.  The same &quot;kernel&quot; idiom applies.
<br>
<p><em>&gt; Mathematical and spiritual concepts are examples of concepts that are
</em><br>
<em>&gt; meaningful but not produced in any direct way by generalization from
</em><br>
<em>&gt; perception or action.  You can say that &quot;5&quot; is produced by generalization
</em><br>
<em>&gt; from perception, but what about &quot;differential operator&quot;? What about &quot;God&quot;?
</em><br>
<em>&gt; I'm afraid that to view these as generalizations from perception, you need a
</em><br>
<em>&gt; terrribly general definition of generalization.
</em><br>
<p>&quot;Differential operator&quot; is abstract but that doesn't mean it's
<br>
non-perceptual.  It means that its important perceptual correlates are
<br>
abstract perceptual models and realtime skills in abstract models, although
<br>
this does not exclude interlacing with the visual and auditory modalities. 
<br>
For example, you might recognize the operator &quot;d/dx&quot; visually, apply it to a
<br>
symbol with the auditory tag &quot;x squared&quot;, and end up with a symbol with the
<br>
auditory tag &quot;two x&quot;.  Of course this is more of a perceptual correlate than
<br>
the perception itself.  Abstract imagery is also handled by the modality
<br>
level, as discussed, but it tends to be handled by the higher levels of the
<br>
modality in a way that's pretty far from what we think of as &quot;sensory&quot;
<br>
imagery.  The abstract imagery is simpler than fully visualized sensory
<br>
imagery, as discussed, but there's also an added layer of complexity that
<br>
comes from generalizing over reflective perceptual correlates such as the
<br>
goal context, and from generalizing over perceptual correlates of the
<br>
realtime reflective skills that manipulate abstract imagery.
<br>
<p>Hope that made sense, or at least enough to give you a general idea of where
<br>
I'm coming from...
<br>
<p><em>&gt; I think some concepts arise
</em><br>
<em>&gt; through self-organizing mind-processes into which perception is just one of
</em><br>
<em>&gt; many inputs, not always a very significant one.
</em><br>
<p>Far as I know, they're all perceptual in the end.  It's just that the
<br>
perceptual idiom - modalities, including feature structure,
<br>
detector/controller structure, and occasionally realtime motor structure -
<br>
extends far beyond things like vision and sound, to include internal reality
<br>
as well.
<br>
<p><em>&gt; Overall, I think that you overemphasize perception as compared to action.
</em><br>
<em>&gt; Pragmatically, in Novamente, I’ve found that the latter is at least as hard
</em><br>
<em>&gt; of a problem to deal with, in terms of implementation and in terms of
</em><br>
<em>&gt; working out the conceptual interface with cognition.
</em><br>
<p>Yep, like I said I left the cerebellum out due to length constraints.  You
<br>
can give a complete account of something that is &quot;intelligence&quot; even if the
<br>
realtime skills are very crude - they'll just get done with goals and
<br>
subgoals instead.  You can think of realtime skills as being a conceptually
<br>
simplified but computationally intensive version of goal processing that
<br>
involves limited-complexity targets and a limited number of dynamic
<br>
achievement processes within a modality workspace.
<br>
<p><em>&gt; Along theese same lines, the definition of a “concept kernel” you give seems
</em><br>
<em>&gt; to apply only to perceptually-derived concepts; I think it should be
</em><br>
<em>&gt; generalized.
</em><br>
<p>Kernels go everywhere; it's perception that gets generalized.
<br>
<p><em>&gt; 2)
</em><br>
<em>&gt; Next, about Thoughts:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You omit to mention that thoughts as well as concepts can be remembered.
</em><br>
<em>&gt; Much of episodic memory consists of memories of thoughts!  Thus, thoughts
</em><br>
<em>&gt; are not really “disposable one-time structures” as you say.  Most but not
</em><br>
<em>&gt; all are disposed of.
</em><br>
<p>Thoughts have perceptual correlates.  The correlates get remembered. 
<br>
Another reflective modality issue.  I did mention this briefly, I think,
<br>
while I was discussing the role of the stream of consciousness in human
<br>
intelligence.
<br>
<p><em>&gt; When you say “A thought is a specific structure of combinatorial symbols
</em><br>
<em>&gt; which builds or alters mental imagery” – I am not sure why “imagery” comes
</em><br>
<em>&gt; into it.  It seems that you are using this word in a way that is not
</em><br>
<em>&gt; necessarily related to visual imagery, which is a little bit confusing. I’d
</em><br>
<em>&gt; like to see a definition of “mental imagery” as you use it here.
</em><br>
<p>I need to emphasize more that when I say &quot;imagery&quot; I am referring to
<br>
generalized working memory in all perceptual modalities, not just the visual
<br>
modality.
<br>
<p><em>&gt; Don’t you
</em><br>
<em>&gt; believe some thoughts are entirely non-imagistic, purely abstract without
</em><br>
<em>&gt; any reliance on sensory metaphors?
</em><br>
<p>I think some thoughts rely on reflective imagery or imagery which is not
<br>
visualized all the way down to the sensory level.
<br>
<p><em>&gt; Some of mine appear to be,
</em><br>
<em>&gt; introspectively -- and I am a highly visual thinker, more so than many
</em><br>
<em>&gt; others.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 3)
</em><br>
<em>&gt; The discussion of “smooth fitness landscapes” is confusing.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Actually, the fitness landscapes confronting intelligent systems are almost
</em><br>
<em>&gt; all what are called “rugged fitness landscapes” (a term I got from a whole
</em><br>
<em>&gt; bunch of Santa Fe institute papers on rugged fitness landscapes, some by
</em><br>
<em>&gt; biologist Alan Perelson).
</em><br>
<p>Yes, the fitness landscapes confronting intelligent systems are all very
<br>
sharp and rough.  Sensory modalities and learned categories smooth them so
<br>
that they are computationally tractable for thought.  A visual scene is
<br>
extremely rough when handled by a process that sees it as a field of raw
<br>
pixels with no edges, textures, shading, etc.
<br>
<p><em>&gt; But “smooth” always means continuous (or differentiable) in mathematics, and
</em><br>
<em>&gt; the cognitively &amp; evolutionarily relevant fitness landscapes definitely are
</em><br>
<em>&gt; NOT.
</em><br>
<p>&quot;Smooth&quot; in fitness landscapes means that similar things are separated by
<br>
short distances, and especially that incremental improvements are short
<br>
distances.  In the case of a modality smoothing a raw scene, you can think
<br>
of distance as being the distance between feature detectors instead of the
<br>
distance between raw pixels, or &quot;distance&quot; as being inversely proportional
<br>
to the probability of that step being taken within the system.
<br>
<p><em>&gt; 4)
</em><br>
<em>&gt; About feature controllers &amp; feature detectors.  There is evidence for the
</em><br>
<em>&gt; involvement of premotor neurons in conscious perception in the brain.   I
</em><br>
<em>&gt; reference some old work along these lines in my online paper “Chance and
</em><br>
<em>&gt; Consciousness.”  I’m sure there’s more recent work too.  This doesn’t
</em><br>
<em>&gt; directly give evidence for feature controllers but it is a big step in that
</em><br>
<em>&gt; direction.
</em><br>
<p>I would expect concept substrate to govern feature controllers.  If visual
<br>
concept kernels are identified with inferior temporal areas; and if the
<br>
higher-level concepts that bind multiple kernels together are identified
<br>
with the association areas in the posterior, superior temporal areas; then
<br>
I'd expect mental imagery to take place through the invocation of a
<br>
higher-level associative concept (posterior superior temporal) that invokes
<br>
the kernels (inferior temporal) that feed back through the ventral
<br>
processing stream and create depictive imagery in the actual visual areas.
<br>
<p>Feature controllers are the inverses of feature detectors; they do not
<br>
appear as internal actions.
<br>
<p>What you're attributing to premotor neurons sounds more like reflective
<br>
internal actions (i.e., realtime skills within the reflective modality)
<br>
rather than feature controllers.  I would tend to put reflective feature
<br>
detectors and feature controllers in prefrontal cortex (of course) and
<br>
realtime reflective skills in the cerebellum (of course).  Coopting premotor
<br>
neurons sounds like coopting the sensorimotor modality to support Lakoff &amp;
<br>
Johnson's sensorimotor metaphors, not to support reflective realtime skills
<br>
per se.
<br>
<p><em>&gt; 5)
</em><br>
<em>&gt; About your suggested modalities: billiards, super-Go, and interpreted code…
</em><br>
<em>&gt; it’s not clear that the former two have the richness to support the
</em><br>
<em>&gt; education of a non-brittle intelligence.  Of course, you don't say they do,
</em><br>
<em>&gt; wisely enough.  But what do you suggest for modalities for a slightly more
</em><br>
<em>&gt; advanced AI?  Do you think we need to go to robot eyes and such, or do you
</em><br>
<em>&gt; think the Net will suffice?
</em><br>
<p>Billiards and mini-Go and code are definitely not rich enough because they
<br>
can't easily support the classic Lakoff and Johnson schema such as
<br>
line-connection, part-whole, center-periphery, container-contained, and so
<br>
on.  But I can't see any good way to do that without gritting teeth and
<br>
starting on a 3D pixel/voxel world, which may be too ambitious for a first
<br>
AI.
<br>
<p>The Net can't help you here.  You can't have a modality with a
<br>
computationally tractable feature structure unless your target environment
<br>
*has* that kind of structure to begin with.  If you're going to put a baby
<br>
AI in a rich environment, the richness has to be the kind that the baby AI
<br>
can learn to see incrementally.
<br>
<p><em>&gt; 6)
</em><br>
<em>&gt; In 2.5.2, what do you mean by “verify the generalization”?    Could you give
</em><br>
<em>&gt; a couple examples?
</em><br>
<p>What I mean is that noticing a perceptual cue that all the billiards in the
<br>
&quot;key&quot; group are red, and that all the billiards in the &quot;non-key&quot; group are
<br>
not red, is not the same as verifying that this is actually the case.  The
<br>
cognitive process that initially delivers the perceptual cue, the suggestion
<br>
saying &quot;Hey, check this out and see if it's true&quot;, may not always be the one
<br>
that does the verification.
<br>
<p><em>&gt; 7)
</em><br>
<em>&gt; You say  “concepts are learned, thoughts are invented.”  I don’t quite catch
</em><br>
<em>&gt; the sense of this.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Complex concepts are certainly “invented” as well, under the normal
</em><br>
<em>&gt; definition of “invention.” …
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The concept of a pseudoinverse of a matrix was invented by Moore and
</em><br>
<em>&gt; Penrose, not learned by them.  I learned it from a textbook.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The concept of &quot;Singularity&quot; was invented as well...
</em><br>
<p>Well, you can learn a concept from the thoughts that you invent - generalize
<br>
a kernel over the reflective perceptual correlates of the thoughts.  But the
<br>
concept-creating cognitive process will still reify (&quot;learn&quot;) a perception,
<br>
and the deliberative thought process that created the abstract/reflective
<br>
perceptions being reified will still be inventive.
<br>
<p><em>&gt; 8)
</em><br>
<em>&gt; You say “the complexity of the thought level … arises from the cyclic
</em><br>
<em>&gt; interaction of thoughts and mental imagery.”
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think this is only one root of the complexity of thoughts.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Self-organizing mental processes acting purely on the thought level seem to
</em><br>
<em>&gt; play a big role as well.
</em><br>
<p>&quot;Is embodied by&quot; might be a better term than &quot;arises&quot;.  Even so, which
<br>
specific self-organizing processes?
<br>
<p><em>&gt; 9)
</em><br>
<em>&gt; You say, “in humans, the perception of confidence happens to exhibit a
</em><br>
<em>&gt; roughly quantitative strength….”
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don’t think this is something that “just happens” to be the case in
</em><br>
<em>&gt; humans.  I think that this quantification of the qualitative (the creation
</em><br>
<em>&gt; of numerical truth values corresponding to mental entities) is a key part of
</em><br>
<em>&gt; intelligence.   I bet it will be part of ANY intelligence.  There is a huge
</em><br>
<em>&gt; efficiency to operating with numbers.
</em><br>
<p>What I mean is that the way humans perceive confidence, quantitatively, may
<br>
not be the *best* way to perceive confidence.  It may not even be the way
<br>
humans perceive confidence.  As you said, in Novamente you work with
<br>
triples.
<br>
<p><em>&gt; 10)
</em><br>
<em>&gt; You say “ ‘one thought at a time’  is just the human way of doing things ….”
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Actually it isn’t, I often have more than one thought at a time.
</em><br>
<p>No, you often have mental imagery that depicts ongoing cognition within more
<br>
than one train of thought, and you switch around the focus of attention,
<br>
which means that more than one deliberative track can coexist.  You still
<br>
think only one thought at a time.  Or do you mean that you pronounce more
<br>
than one mental sentence at a time?  You've got to keep the thought level
<br>
and the deliberation level conceptually separate; I said &quot;one thought at a
<br>
time&quot;, not &quot;one deliberation at a time&quot;.
<br>
<p><em>&gt; 11)
</em><br>
<em>&gt; Regarding your discussion of the need (or otherwise) for language in digital
</em><br>
<em>&gt; mind.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I am guessing that AI will come most easily via building a community of
</em><br>
<em>&gt; intercommunicating AI’s.  Communication between AI’s need not proceed using
</em><br>
<em>&gt; linear sequences of characters or sounds, though.  It can proceed via
</em><br>
<em>&gt; various sorts of brain-to-brain transfer.  But this requires some art; in
</em><br>
<em>&gt; Novamente we’ve designed a language called Psynese for brain-to-brain
</em><br>
<em>&gt; transfer between different Novamente instances.
</em><br>
<p>As discussed in the section on seed AI, I think that splitting up available
<br>
brainpower into separate entities is less productive than agglomerating it.
<br>
<p><em>&gt; 12)
</em><br>
<em>&gt; When you discuss the various kinds of binding, I’m not sure why you have a
</em><br>
<em>&gt; sensory binding but not an action binding.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You deal with actions in the context of decisive bindings, but, I think
</em><br>
<em>&gt; sometimes actions can be bound in a way that has nothing to do with goals.
</em><br>
<p>I suppose you could call an &quot;action binding&quot; the correspondence between
<br>
muscle commands and mental pictures of muscle commands, but I think you
<br>
would need to complete the picture with a feedback loop from proprioception
<br>
before it became a cognitively real binding.  And in that case what you have
<br>
is a realtime manipulative binding, which is of course one of the coolest
<br>
kinds.  But an &quot;action binding&quot; that doesn't involve a feedback loop, just a
<br>
direct correspondence between a patterned variable in cognition and a
<br>
patterned variable in motor reality, is just another kind of sensory mapping
<br>
- albeit one where causality flows in the opposite direction.
<br>
<p><em>&gt; 13)
</em><br>
<em>&gt; You say that “evolution… cannot boast general intelligence.”
</em><br>
[&quot;cannot invoke&quot;]
<br>
<em>&gt; This is not so clear to me.  Why?  It seems to me that evolution in fact
</em><br>
<em>&gt; does display a pretty generalized kind of intelligence.
</em><br>
<p>Because there is a difference between genericity and generality.
<br>
<p>Evolution, like search trees and artificial neural networks, is a fully
<br>
generic process.  But it works much better for some things than others, and
<br>
some things it can't handle at all.  It can't do many of the things that
<br>
human intelligence does.  You can apply a generic process to anything but it
<br>
won't necessarily work.  Usually it only solves a tiny fraction of special
<br>
cases of the problem (which AI projects usually go on to mistake for having
<br>
solved the general case of the problem; this is one of the Deadly Sins). 
<br>
Evolution uses an unreasonable amount of computational power to overcome
<br>
this handicap.
<br>
<p><em>&gt; You could say perhaps that evolution is relatively unintelligent because it
</em><br>
<em>&gt; requires too many space and time resources to achieve its goals.
</em><br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3341.html">Ben Goertzel: "RE: FW: DGI Paper"</a>
<li><strong>Previous message:</strong> <a href="3339.html">Samantha Atkins: "Re: Why bother (was Re: Introducing myself)"</a>
<li><strong>In reply to:</strong> <a href="3338.html">Ben Goertzel: "FW: DGI Paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3341.html">Ben Goertzel: "RE: FW: DGI Paper"</a>
<li><strong>Reply:</strong> <a href="3341.html">Ben Goertzel: "RE: FW: DGI Paper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3340">[ date ]</a>
<a href="index.html#3340">[ thread ]</a>
<a href="subject.html#3340">[ subject ]</a>
<a href="author.html#3340">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
