<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Essay: On psychological frames of reference</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Essay: On psychological frames of reference">
<meta name="Date" content="2002-04-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Essay: On psychological frames of reference</h1>
<!-- received="Mon Apr 08 19:29:06 2002" -->
<!-- isoreceived="20020409012906" -->
<!-- sent="Mon, 08 Apr 2002 17:31:49 -0400" -->
<!-- isosent="20020408213149" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Essay: On psychological frames of reference" -->
<!-- id="3CB20C45.1CEAA31F@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJKENBCFAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Essay:%20On%20psychological%20frames%20of%20reference"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Apr 08 2002 - 15:31:49 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3311.html">Evan Reese: "Re: Why bother (was Re: Introducing myself)"</a>
<li><strong>Previous message:</strong> <a href="3309.html">Eliezer S. Yudkowsky: "Re: PAPER: Levels of Organization in General Intelligence"</a>
<li><strong>In reply to:</strong> <a href="3288.html">Ben Goertzel: "RE: Why bother (was Re: Introducing myself)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3310">[ date ]</a>
<a href="index.html#3310">[ thread ]</a>
<a href="subject.html#3310">[ subject ]</a>
<a href="author.html#3310">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Similarly, it's true that if Eliezer or I or any other one individual or
</em><br>
<em>&gt; small group stopped working on Singularity-focused technology, the
</em><br>
<em>&gt; Singularity would still happen.  But yet, if *everyone* took this to heart
</em><br>
<em>&gt; and hence stopped making efforts, the Singularity *wouldn't* happen.
</em><br>
<em>&gt; Because the Singularity is composed of the sum total of loads of individual
</em><br>
<em>&gt; efforts like ours.
</em><br>
<p>This is certainly a very commonly given answer to questions of the form &quot;Why
<br>
do you think that *you* can do *X*?&quot; where [you] refers to someone who wants
<br>
to do it but hasn't done it yet, and [X] is something very difficult.  The
<br>
reason it's commonly given is that it's a good one.  If you start a new
<br>
business, for example, and someone who wants to invest says &quot;How do you know
<br>
this is going to work?&quot;, the correct answer is &quot;Nobody knows for sure that
<br>
this particular business will work - that's not where businesses come from. 
<br>
I think this business has a better chance than the average 95% chance of
<br>
failure, and that's all that anyone can ask for.  Nothing you can possibly
<br>
do will raise the chance of a new business succeeding above 50%.&quot;
<br>
<p>This is one of those questions that can't really be understood in strictly
<br>
normative terms.  You have to view it in the context of the human emotional
<br>
architecture and our social homeostasis systems.  Today, Einstein is a very
<br>
high-ranking social figure (dead, but still very high-ranked) by virtue of
<br>
his successful invention of Special *and* General Relativity - he not only
<br>
did it, he did it twice in a row.  The thing is that our mental processes
<br>
tend to think of this high rank as being inherent in Einstein the person -
<br>
that if someone even *more* brilliant had invented both SR and GR just a few
<br>
years earlier, Einstein would still be Einstein.  In fact this view is
<br>
probably wrong; Einstein's intelligence wouldn't change but the world's view
<br>
of his intelligence would.  The point is that Einstein's archetypal role in
<br>
the world is that of someone super-smart, not super-smart and super-lucky,
<br>
or one of a large group of super-smart people who happened to try and
<br>
succeed instead of try and fail.  People who think about the problem
<br>
deliberatively, and whose morals are at least partially interwoven with
<br>
modern pro-egalitarian anti-dramatic attitudes, will probably come to the
<br>
conclusion that Einstein probably was not the smartest figure of his
<br>
generation, just the one with the best combination of intelligence and post
<br>
facto success with respect to physics - there wasn't anyone who was both
<br>
smarter *and* luckier.  Personally, the fact that Einstein pulled it off
<br>
twice in a row makes me wonder, but the point is that there is a
<br>
pro-egalitarian anti-dramatic standard answer.
<br>
<p>However, the conscious answer is not the answer that shows up in our
<br>
attitudes, our emotions, our use of &quot;Einstein&quot; as an archetype, and the way
<br>
people react to people trying to do something very difficult.  In essence,
<br>
the intuitive view skips over all the complex reasoning about causation and
<br>
holds that:
<br>
<p>1)  Einstein's achievement of Special and General Relativity proves that
<br>
Einstein is super-smart.
<br>
<p>2)  Einstein has high social rank by virtue of being super-smart.
<br>
<p>Now, rank in a hunter-gatherer tribe - and in modern times too, for that
<br>
matter - is a precious quantity.  So if you are trying to do [X], where [X]
<br>
is something very difficult, you rapidly become accustomed to running into
<br>
the following chain of logic:
<br>
<p>1)  You are publicly trying to do [X].
<br>
&nbsp;&nbsp;&nbsp;&nbsp;[AUTOMATIC TRANSLATION:]
<br>
2)  You are claiming you can do [X].
<br>
&nbsp;&nbsp;&nbsp;&nbsp;[AUTOMATIC TRANSLATION:]
<br>
3)  You are claiming you are smart enough to do [X].
<br>
&nbsp;&nbsp;&nbsp;&nbsp;[AUTOMATIC TRANSLATION:]
<br>
4)  You are claiming the high rank that is a consequence of super-smartness.
<br>
<p>And so the observer wants you to immediately prove that you are entitled to
<br>
this high rank, because otherwise you're trying to lay claim to rank you do
<br>
not possess - that is, you are trying to cheat the system.
<br>
<p>Ben's Argument from Common Effort is a simple (and accurate) rejoinder to
<br>
the effect that &quot;If nobody ever tried to do something until they'd already
<br>
proved they could do it, nothing would ever get done.&quot;  The Argument from
<br>
Common Effort is a very effective rejoinder against the above because it
<br>
strikes directly at the moral intuitions that are supporting the objection:
<br>
<p>1)  The Argument from Common Effort acts as a disclaimer of high rank,
<br>
because disclaimer of rank is a psychological consequence of the statements
<br>
&quot;I'm just like everyone else, really&quot; or &quot;I'm just one of a large group of
<br>
people&quot; or &quot;It would happen without me eventually&quot;.
<br>
<p>2)  The Argument from Common Effort strikes directly at the social intuition
<br>
that this is a case of maintaining the common social benefit by maintaining
<br>
the rank system, by showing that enforcement of the argument as presented
<br>
would cause a social penalty (no advancement toward [X]).
<br>
<p>So what's not to like about the Argument from Common Effort?
<br>
<p>The problem is that the Argument from Common Effort, like the original
<br>
Argument from If You're So Smart Why Aren't You Rich, is the product of a
<br>
way of thinking that runs skew to actual reality.  You can either treat
<br>
figuring out the truth as a special case of figuring out what's socially
<br>
acceptable, or you can treat &quot;figuring out the truth about what's socially
<br>
acceptable&quot; as a special case of figuring out the truth.
<br>
<p>Whether you're really unique or one of a horde is an issue that is totally
<br>
orthogonal to society's different perspectives on these two statements
<br>
presented publicly.  It doesn't even matter if the answer that society likes
<br>
really is the right answer; you can't afford to arrive at that answer by
<br>
reasoning about what's socially acceptable.  You have to do it by reasoning
<br>
about what's true.  If there is, in your mind, a little circuit that asks
<br>
how other people will react to your thinking X while you are trying to
<br>
figure out whether X is true, then your mind has short-circuited.  Now of
<br>
course it's theoretically possible to ask what other people think of an idea
<br>
on the grounds that other people are likely to be right, and to factor that
<br>
into the confidence.  I do this when I talk about General Relativity; I
<br>
haven't checked the numbers myself but I have confidence in the people who
<br>
have.
<br>
<p>The point is that I believe the Earth is round, not because believing the
<br>
Earth is flat would make me socially unpopular as a crackpot, but because I
<br>
think that modern-day society really is likely to be right about that sort
<br>
of thing.  Many past societies wouldn't have been.
<br>
<p>Now the short-circuit from social acceptability to personal belief is
<br>
certainly an *adaptive* short-circuit.  We are imperfectly deceptive social
<br>
organisms; what we believe affects how others react to us.  If mutation pops
<br>
up a little circuit that runs from the internal perception &quot;people will
<br>
react badly to idea X&quot; and makes it a little more painful to think about
<br>
beliefs that support X or predict X, that circuitry is likely to rapidly
<br>
become a fixture in the gene pool.  The same goes for a circuit that runs
<br>
from the internal perception &quot;people are likely to praise me for believing
<br>
X&quot; and makes it comfortable and pleasurable to think about beliefs that
<br>
support X and chains of reasoning that end by concluding X.  It doesn't even
<br>
have to be a piece of circuitry.  It can be an emergent byproduct of the
<br>
pleasure-pain reinforcement architecture for reasoning.  The point is that
<br>
any heritable variation in this tendency, whether the tendency is originally
<br>
emergent or is an actual piece of circuitry, will tend to become genetically
<br>
fixed as a result of natural selection on imperfectly deceptive linguistic
<br>
organisms.  It is one of the more subtle of the many forces that contribute
<br>
to rationalization behaviors.
<br>
<p>If the truth is precious to you, and if you exist in a contemporary
<br>
scientific memetic environment (Richard &quot;What do you care?&quot; Feynman and
<br>
Robert &quot;Church of Reason&quot; Pirsig and the public-consumption version of the
<br>
story of Galileo &quot;Still It Moves&quot; Galilei), then you wind up with this idea
<br>
that you ought to believe things that are true though Hell bar the way. 
<br>
Most people who are part of the family of truthseekers believe this. 
<br>
Knowing enough evolutionary psychology to see the dangling puppet strings of
<br>
evolution, and having enough native introspective talent to cut the strings
<br>
judged obnoxious, is another issue.  I should emphasize that &quot;emergent
<br>
effects of the pleasure-pain reinforcement architecture on deliberation that
<br>
have been genetically fixed by selection pressures&quot; are probably the most
<br>
advanced things I've ever tried to deal with, in terms of debugging myself,
<br>
and before I tried that I had a hell of a lot of practice on simpler things
<br>
with clear subjective correlates, like the blatantly hunter-gatherer
<br>
political emotions.
<br>
<p>Why the &quot;practice this before you try it&quot; disclaimer?  Because if you are
<br>
still subject to self-overestimation effects (ancestrally adaptive because
<br>
they led you to run for tribal chief), then maybe you *shouldn't* be trying
<br>
to filter the &quot;What will my friends think?&quot; emotion through a rational
<br>
check.  In this case the check is actually normative; it is an irrational
<br>
social pressure that opposes an irrational part of your own mind.  *First*
<br>
you disentangle your own personal self from the non-normative psychological
<br>
effects of rank-seeking, and *then* you disentangle yourself from social
<br>
pressures that are the non-normative output of psychological forces that
<br>
oppose rank-seeking in others.  Now when I say these forces are
<br>
&quot;non-normative&quot;, I don't mean &quot;outright wrong&quot;, I mean that these
<br>
psychological forces are built around something other than truthseeking -
<br>
they are part of a skew view of reality.
<br>
<p>So when someone says, &quot;Eliezer Yudkowsky, you're trying to do [X]&quot; followed
<br>
by a bunch of AUTOMATIC TRANSLATION and then &quot;Now prove you're not just a
<br>
social cheater claiming undeserved rank&quot;, I cannot just turn around and use
<br>
the Argument from Common Effort - the rejoinder is part of the same world as
<br>
the objection.  The entire argument is part of a skew view of reality.  The
<br>
real answer is that I laid aside every claim of rank long ago, right after I
<br>
learned about evolutionary psychology for the first time.  Nobody has to
<br>
respect me.  Nobody has to obey me.  I've never said that I deserved power
<br>
over others and I've never asked anyone to give me power over others.  This
<br>
is not just part of an effort to be a nice person, it's part of an effort to
<br>
step outside what I think is a non-normative psychological frame of
<br>
reference.  We shouldn't be orienting our lives around social rank.  In that
<br>
sense, orienting your view of the world around how to answer the objection
<br>
that &quot;you're claiming social rank you don't deserve&quot; is as wrong as
<br>
orienting your mind around social rank to begin with.  There's an
<br>
intermediate stage where what matters to you is ridding your mind of the
<br>
want-social-rank effect, and while you're in that intermediate stage, then
<br>
it's not all that bad an idea to think about the things that other people
<br>
are likely to react to as a claim of social rank; it's a good way to clean
<br>
out the bugs.  But until you're *outside* that frame of reference, not just
<br>
at one extreme *within* the frame of reference, you've still got work left
<br>
to do.
<br>
<p>The real answer to &quot;You think you can do [X] ... AUTOMATIC TRANSLATION&quot; is
<br>
to explain why I deny that the AUTOMATIC TRANSLATION is a good way to look
<br>
at reality - to deconstruct the question from below, I think the phrase is. 
<br>
Unfortunately I usually don't have time to do this.  But I do try to give
<br>
answers that are outside the psychological frame of reference that I think
<br>
is wrong.  Look at my original answer to the objection.  In fact, I'll even
<br>
quote it:
<br>
<p>Eliezer Yudkowsky wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; *Shrug.*  It's not my job to be messianic, and it's even less my job to be
</em><br>
<em>&gt; non-messianic.  I want to accomplish the largest possible amount of good
</em><br>
<em>&gt; with my life.  In a pre-Singularity era, that means doing something that
</em><br>
<em>&gt; relates to the Singularity, because those are the largest stakes currently
</em><br>
<em>&gt; on the table.  If I find a $1000 bill lying in the street, I'll pick it up. 
</em><br>
<em>&gt; If I find an opportunity to bring the Singularity nearer or make it safer,
</em><br>
<em>&gt; I'll take it.  I do see what looks like such an opportunity and I am taking
</em><br>
<em>&gt; it.  That's all there is to it.  It doesn't take any complex hypotheses
</em><br>
<em>&gt; about my psychology.  I suppose it takes more knowledge than usual to see
</em><br>
<em>&gt; that the situation really is that simple, but it doesn't take anything
</em><br>
<em>&gt; besides that.
</em><br>
<p>Do you see what this answer is getting at?  It's an attempt to yank the
<br>
whole problem out of the hunter-gatherer social psychology and put it into a
<br>
more normative frame of reference.  I didn't have the time then to go on at
<br>
such great length because I wasn't on temporary rest-up hiatus, but it is a
<br>
right answer *for the right reasons*.  There is normative reasoning that
<br>
runs skew to the whole mess of anxieties about how much rank you have or how
<br>
much rank other people might think you're claiming.
<br>
<p>Why is it so important to make this distinction?  At the risk of triggering
<br>
an [AUTOMATIC TRANSLATION: showing off], it's because I want my whole mind
<br>
to be focused on the truth.  I think that if you start focusing on anything
<br>
but the truth, you end up somewhere that isn't true.  I think that there are
<br>
emergent effects in truthseeking that emerge only *after* you've practiced
<br>
self-correcting deliberation for a while, and the effects propagate down to
<br>
corrected thoughts, so that your mind doesn't go off on the wrong track to
<br>
begin with and you can do an extended deliberative chain of thoughts that
<br>
formerly would have each required an entire deliberative session to arrive
<br>
at.  To train yourself into that state you have to avoid compromise.  If you
<br>
compromise by choosing to tolerate an error, it means that the source of the
<br>
error is still there, and that whatever degree of rationality you achieve is
<br>
achieved by balancing your mind against the error, devoting a part of your
<br>
energy to struggling with it.  But if you decide *not* to compromise, and
<br>
devote enough energy to correcting the error whenever you find it, a funny
<br>
thing happens; you start to make the error less often.
<br>
<p>Let's say you're beginning to write for the first time.  You can start out
<br>
by saying, &quot;Well, I made a few spelling errors, but those are tolerable. 
<br>
People will still be able to understand what I've written.&quot;  And in this
<br>
case you've struck your compromise.  You stay where you are as a writer. 
<br>
But if you don't trust compromises in general or want to do the best you
<br>
can, then you go back and eliminate your spelling mistakes.  So right now,
<br>
you're just like the other guy, except that you're spending more time and
<br>
effort to correct your spelling mistakes.  And the other guy may point out
<br>
that you're expending more effort than you're getting back in benefit - and
<br>
may be right, in the short term.  But after a while, a funny thing happens. 
<br>
Instead of needing to spend time and effort on correct spelling, instead of
<br>
having to enforce correct spelling through deliberation, you find that you
<br>
just don't make all that many errors to begin with.  And once it's no longer
<br>
your spelling errors that leap out at you and distract you when you're
<br>
reading your own prose, you find that higher-level errors become visible -
<br>
awkwardness of phrase, needless words.  The difference isn't between a few
<br>
spelling errors and no spelling errors, because correct spelling is only the
<br>
first step on a very long path toward being a good writer.  When you strike
<br>
a compromise you make a decision to stay where you are.
<br>
<p>I don't know this part from practical experience with spelling, I'm afraid;
<br>
my spelling started out as okay.  But I know the value of not compromising
<br>
from experience keeping in the frame of mind where the truth is what
<br>
matters.  You can't compromise and say, &quot;Well, it's okay if a few of my
<br>
thoughts are redirected toward social conformity.&quot;  (Which is bad not
<br>
because it's the evil demon 'conformity' but just because it's anything
<br>
other than 'truth'; being 'non-conformist' is just as bad.)  Learning to
<br>
keep to the 'truth' frame of reference in this one case is only a first
<br>
step.  There are other errors I didn't see when I was at that stage because
<br>
I didn't yet have enough experience with what it feels like when a chain of
<br>
thought goes *right*.  That feeling of clear thought is what you learn to
<br>
cherish, not just because it feels good, but because you see that it works. 
<br>
You can't get to that destination by compromising... or at least, I don't
<br>
know of anyone who's gotten any distance by doing so... because when you
<br>
compromise you've made the decision not to start.
<br>
<p>And just so that none of this gets taken out of context, I should emphasize
<br>
that the first step down this road consists of reading a few popular books
<br>
on evolutionary psychology and trying to clean up the blatantly adaptive
<br>
political emotions with easily identifiable subjective correlates, like
<br>
&quot;seeking status&quot;, &quot;rebelling against conformity&quot;, &quot;I deserve [Y]&quot;, and so
<br>
on.  It's not so much &quot;Don't try this at home&quot; but &quot;Yes, I did get where I
<br>
am today over a very long period of time that involved a lot of effort, and
<br>
incremental progress from relatively easy problems to problems that I didn't
<br>
even know existed when I started out.&quot;  There is this idea that
<br>
self-awareness challenges are unsolveable, and that the world is divided to
<br>
people who acknowledge they have the problems and people who have a dearth
<br>
of self-awareness or an excess of grandiosity and so claim to be immune to
<br>
them.  But the problems are not that absolutely intractable.  The war is
<br>
never finished, but you can get into the habit of winning the battles.  It's
<br>
just that a randomly selected person saying &quot;I think more clearly&quot; is
<br>
statistically more likely to follow it up with &quot;...because of my inherent
<br>
virtue, unlike the Godless fools who disagree with me&quot; and not &quot;...but I
<br>
emphasize that this is just a morally nonvalent report on my internal state;
<br>
I am not asking for any relaxation in the social or moral rules as a
<br>
consequence.&quot;  But you can't give up on the war right at the start because
<br>
you're worried you'll end up with a viewpoint of which there exists at least
<br>
one possible *over*simplification that could conceivably be clustered with a
<br>
bunch of wrong viewpoints; that would be losing a battle to that exact same
<br>
piece of circuitry I spoke about earlier.
<br>
<p>Sincerely,
<br>
Eliezer Yudkowsky.
<br>
<p>PS:
<br>
<p><a href="http://www.exploitationnow.com/d/20010214.html">http://www.exploitationnow.com/d/20010214.html</a>
<br>
<p>&quot;More proof that sometimes it isn't all that hard to tell the difference.&quot;
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3311.html">Evan Reese: "Re: Why bother (was Re: Introducing myself)"</a>
<li><strong>Previous message:</strong> <a href="3309.html">Eliezer S. Yudkowsky: "Re: PAPER: Levels of Organization in General Intelligence"</a>
<li><strong>In reply to:</strong> <a href="3288.html">Ben Goertzel: "RE: Why bother (was Re: Introducing myself)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3310">[ date ]</a>
<a href="index.html#3310">[ thread ]</a>
<a href="subject.html#3310">[ subject ]</a>
<a href="author.html#3310">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
