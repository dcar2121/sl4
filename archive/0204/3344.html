<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: DGI Paper</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: DGI Paper">
<meta name="Date" content="2002-04-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: DGI Paper</h1>
<!-- received="Sun Apr 14 02:08:09 2002" -->
<!-- isoreceived="20020414080809" -->
<!-- sent="Sat, 13 Apr 2002 23:51:30 -0600" -->
<!-- isosent="20020414055130" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: DGI Paper" -->
<!-- id="LAEGJLOGJIOELPNIOOAJGEDKCGAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="3CB8E478.4512AEE7@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20DGI%20Paper"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sat Apr 13 2002 - 23:51:30 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3345.html">Evan Reese: "Re: Why bother (was Re: Introducing myself)"</a>
<li><strong>Previous message:</strong> <a href="3343.html">Eliezer S. Yudkowsky: "META: Politics on SL4"</a>
<li><strong>In reply to:</strong> <a href="3342.html">Eliezer S. Yudkowsky: "Re: DGI Paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3354.html">Michael Roy Ames: "Re: DGI Paper"</a>
<li><strong>Reply:</strong> <a href="3354.html">Michael Roy Ames: "Re: DGI Paper"</a>
<li><strong>Reply:</strong> <a href="../0205/3585.html">Eliezer S. Yudkowsky: "Re: DGI Paper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3344">[ date ]</a>
<a href="index.html#3344">[ thread ]</a>
<a href="subject.html#3344">[ subject ]</a>
<a href="author.html#3344">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
hi,
<br>
<p><em>&gt; Recently (I'm not sure if I was doing this during the whole of the DGI
</em><br>
<em>&gt; paper) I've been trying to restrict &quot;sensory&quot; to information produced by
</em><br>
<em>&gt; environmental sense organs.  However, there are other perceptions than
</em><br>
<em>&gt; this.  Imaginative imagery exists within the same working memory that
</em><br>
<em>&gt; sensory information flows into, but it's produced by a different source.
</em><br>
<em>&gt; Abstract imagery might involve tracking &quot;objects&quot;, which can be very
</em><br>
<em>&gt; high-level features of a sensory modality, but can also be features of no
</em><br>
<em>&gt; sensory modality at all.
</em><br>
<p>I think this is an acceptable use of the term &quot;perceptual&quot;, but only if you
<br>
explicitly articulate that this is how you're using the word, since it
<br>
differs the common usage in cognitive psychology.
<br>
<p>Your usage is part of common English of course.  We can say, e.g. &quot;I sense
<br>
you're upset about this,&quot; or &quot;I perceived I was going through some kind of
<br>
change&quot; -- abstract sensations &amp; perceptions.  These common English usages
<br>
aren't reflected in the way the terms have become specialized within
<br>
cognitive psych, that's all.
<br>
<p>These issues are impossible to avoid in getting scientific about the mind.
<br>
For instance, the way we use the word &quot;reason&quot; in Novamente includes things
<br>
that logicians don't all consider &quot;reason&quot; -- very speculative kinds of
<br>
reasoning.
<br>
<p><p><em>&gt; Right - but it's associated with the behaviors, in abstract imagery, of
</em><br>
<em>&gt; other math concepts.  That's why you can't discover complex math concepts
</em><br>
<em>&gt; without knowing simple math concepts; the complex math concepts are
</em><br>
<em>&gt; abstracted from the abstract imagery for complex behaviors or complex
</em><br>
<em>&gt; relations of simple math concepts.  But there is still imagery.  It is not
</em><br>
<em>&gt; purely conceptual; one is imagining objects that are abstract objects
</em><br>
<em>&gt; instead of sensory objects, and imagining properties that are abstract
</em><br>
<em>&gt; properties instead of sensory properties, but there is still
</em><br>
<em>&gt; imagery there.
</em><br>
<em>&gt; It can be mapped onto the visual imagery of the blackboard and so on.
</em><br>
<p>I am not at all sure you are right about this.  I think that abstract
<br>
reasoning can sometimes proceed WITHOUT the imagining of specific objects.
<br>
I think what you're describing is just *one among many modes* of abstract
<br>
reasoning.  I think sometimes we pass from abstraction to abstraction
<br>
without the introduction of anything that is &quot;imagery&quot; in any familiar
<br>
sense.
<br>
<p>And the mapping onto the visual blackboard may be a very very distortive
<br>
mapping, which does not support the transformations required for accurate
<br>
inference in the given abstract domain.  Visual thinking is not suited for
<br>
everything -- e.g. it works well for calculus and relatively poorly for
<br>
abstract algebra (which involves structures whose symmetries are very
<br>
DIFFERENT from those of the experienced 3D world).
<br>
<p>Again, I think if you're right, it can only be by virtue of having a very
<br>
very very general notion of &quot;imagery&quot; which you haven't yet fully
<br>
articulated.
<br>
<p><em>&gt; Be it noted that this is a somewhat unusual hypothesis about the mind, in
</em><br>
<em>&gt; which &quot;propositional&quot; cognition is a simplified special case of mental
</em><br>
<em>&gt; imagery.
</em><br>
<p>I have seen yet more extreme versions of this hypothesis.  One of the papers
<br>
submitted for the &quot;Real AI&quot; volume argues that all cognition is largely an
<br>
application of our 3D scene processing circuitry.
<br>
<p>I still don't really feel I know what you mean by &quot;mental imagery&quot; though.
<br>
<p><em>&gt; Abstract imagery uses non-depictive, often cross-modal
</em><br>
<em>&gt; layers that
</em><br>
<em>&gt; are nonetheless connected by the detector/controller flow to the depictive
</em><br>
<em>&gt; layers of mental imagery.
</em><br>
<p>Again, I think this is a very important *kind* of abstract thought, but not
<br>
the only kind.
<br>
<p><p><p><em>&gt; Maybe I should put in a paragraph somewhere about &quot;sensory
</em><br>
<em>&gt; perception&quot; as a
</em><br>
<em>&gt; special case of &quot;perception&quot;.
</em><br>
<p>Definitely.
<br>
<p><p><em>&gt; I think that human concepts don't come from mixing together the internal
</em><br>
<em>&gt; representations of other concepts.
</em><br>
<p>I think that some human concepts do, and some don't.
<br>
<p>This is based partly on introspection: it feels to me that many of my
<br>
concepts come that way.
<br>
<p>Now, you can always maintain that my introspection is inaccurate.
<br>
<p>Of course, your intuition must also be based largely on your own
<br>
introspection (cog sci just doesn't take us this far, yet), which may also
<br>
be inaccurate.
<br>
<p>I am largely inclined to believe that both of our introspections are
<br>
accurate *in terms of what they observe*, but that both of our
<br>
introspections are incomplete.
<br>
<p>Two factors here: we may have different thought processes; and we may each
<br>
for whatever reason be more conscious of different aspects of our thought
<br>
processes.
<br>
<p>Certainly the combinatory aspect of cognition that I describe has
<br>
significant neurological support.  Edelman's books since Neural Darwinism,
<br>
among other sources, emphasize this aspect of the formation of neural maps.
<br>
<p><em>&gt; I think that's an AI idiom
</em><br>
<em>&gt; which is not
</em><br>
<em>&gt; reflected in the human mind.  Humans may be capable of faceting
</em><br>
<em>&gt; concepts and
</em><br>
<em>&gt; putting the facets together in new ways, like &quot;an object that smells like
</em><br>
<em>&gt; coffee and tastes like chocolate&quot;, but this is (I think) taking apart the
</em><br>
<em>&gt; concepts into kernels, not mixing the kernel representations together.
</em><br>
<p>Well, Edelman and I disagree with you, he based mostly on his neurological
<br>
theory, I based mostly on my introspective intuition.
<br>
<p>What support do you have for your belief, other than that when you
<br>
introspect you do not feel yourself to be combining concepts in such a way?
<br>
<p><em>&gt; Now it may perhaps be quite useful to open up concepts and play with their
</em><br>
<em>&gt; internals!  I'm just saying that I don't think humans do it that way and I
</em><br>
<em>&gt; don't think an AI should start off doing it that way.
</em><br>
<p>My intuition is quite otherwise.  I think that very little creative
<br>
innovation will happen in a mind that does not intercombine concepts by
<br>
&quot;opening them up and playing with their internals.&quot;
<br>
<p><em>&gt; &gt; I don't think that a new math concept i cook up necessarily has
</em><br>
<em>&gt; anything to
</em><br>
<em>&gt; &gt; do with imagery derived from any of the external-world senses.
</em><br>
<em>&gt; Of course
</em><br>
<em>&gt; &gt; connections with sensorimotor domains can be CREATED, and must be for
</em><br>
<em>&gt; &gt; communication purposes.  But this may not be the case for AI's,
</em><br>
<em>&gt; which will
</em><br>
<em>&gt; &gt; be able to communicate by direct exchange of mindstuff rather than via
</em><br>
<em>&gt; &gt; structuring physicalistic actions &amp; sensations.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The new math concept has plenty to do with imagery, it's just not sensory
</em><br>
<em>&gt; imagery.
</em><br>
<p>OK, then I still don't know what you mean by &quot;imagery&quot;.
<br>
<p><p><em>&gt; &gt; I don't understand why you think a baby AI can't learn to see the Net
</em><br>
<em>&gt; &gt; incrementally.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It doesn't have a tractable fitness landscape.  No feature structure to
</em><br>
<em>&gt; speak of.  No way to build up complex concepts from simple concepts.
</em><br>
<p>I think this is quite wrong, actually.  There is an incredible richness, for
<br>
example, in all the financial and biological databases available online.
<br>
Trading the markets is one way to interact with the world online, a mode of
<br>
interaction that incorporates all sorts of interesting data.  Chatting with
<br>
biologists (initially in a formal language) about info in bio databases is
<br>
another.  I think an AI has got to start with nonlinguistic portions of the
<br>
Net, then move to linguistic portions that are closely tied to the
<br>
nonlinguistic portions it knows (financial news, Gene Ontology gene
<br>
descriptions, etc.).
<br>
<p>I think the implicit fitness landscapes in the financial trading and
<br>
collaborative biodatabase analysis spaces are quite tractable.
<br>
<p><em>&gt; It's
</em><br>
<em>&gt; all complexity in a form that's meant to be perceived by other humans.
</em><br>
<p>Not really.  Most trading is done by programs, and so is most biodata
<br>
analysis.
<br>
<p>The Net is not just Web pages.
<br>
<p><em>&gt; &gt; &quot;Differential functions on [-5,5] whose third derivative is
</em><br>
<em>&gt; confined to the
</em><br>
<em>&gt; &gt; interval [0,1]&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; This isn't much of a concept until it has a name.  Let's call a function
</em><br>
<em>&gt; like this a &quot;dodomorphic fizzbin&quot;.
</em><br>
<p>Well, i disagree -- I have concepts like this all the time with no names.
<br>
Naming such a concept only occurs, for me, when I want to communicate it to
<br>
others, or write it down for myself.
<br>
<p><em>&gt; &gt; then how is this concept LEARNED?  I didn't learn this, I just
</em><br>
<em>&gt; INVENTED it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You learned it by inventing it.  The invention process took place on the
</em><br>
<em>&gt; deliberative level of organization.  The learning process took
</em><br>
<em>&gt; place on the
</em><br>
<em>&gt; concept level of organization and it happened after the invention.  You
</em><br>
<em>&gt; created the mental imagery and then attached it to a concept.
</em><br>
<em>&gt; These things
</em><br>
<em>&gt; happen one after the other but they are still different cognitive
</em><br>
<em>&gt; processes
</em><br>
<em>&gt; taking place on different levels of organization.
</em><br>
<p>Somehow, you're telling me it's not a &quot;concept&quot; until it's been named??
<br>
<p>I don't see why such a concept has to be &quot;attached&quot; to anything to become a
<br>
&quot;real concept&quot;, it seems to me like it's a &quot;real concept&quot; as soon as I start
<br>
thinking about it...
<br>
<p>I guess I still don't fully understand your notion of a &quot;concept&quot;
<br>
<p><em>&gt; &gt; Evolutionary &amp; hypothetically-inferential combination of
</em><br>
<em>&gt; existing concepts &amp;
</em><br>
<em>&gt; &gt; parts thereof into new ones, guided by detected associations between
</em><br>
<em>&gt; &gt; concepts.  With a complex dynamic of attention allocation guiding the
</em><br>
<em>&gt; &gt; control of the process.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I would have to say no to this one, at least as an idiom for human
</em><br>
<em>&gt; intelligence.  There's a repertoire of background generalization processes
</em><br>
<em>&gt; but they act on current imagery (generalized perceptual imagery), not the
</em><br>
<em>&gt; representations of stored concepts - as far as I know.  It might be a good
</em><br>
<em>&gt; ability for AIs to have *in addition* to the levels-of-organization idiom,
</em><br>
<em>&gt; but it can't stand on its own.
</em><br>
<p>You're right that this kind of cognition can't stand on its own.  But I
<br>
don't think the levels-of-organization cognitive structure/dynamic can do
<br>
much good on its own either: it can *stand* on its own, but it can't run....
<br>
<p>It's fine for dogs and bunnies ... but I think the crux of what makes human
<br>
cognition special is the synergization of the levels-of-organization
<br>
cognitive structure/dynamic, with the evolutionary/inferential
<br>
structure/dynamic as I just described.
<br>
<p><p><em>&gt; 1:  &quot;I need to get this paper done.&quot;
</em><br>
<em>&gt; 2:  &quot;I sure want some ice cream right now.&quot;
</em><br>
<em>&gt; 3:  &quot;Section 3 needs a little work on the spelling.&quot;
</em><br>
<em>&gt; 4:  &quot;I've already had my quota of calories for the day.&quot;
</em><br>
<em>&gt; 5:  &quot;Maybe I should replace 'dodomorphic' with 'anhaxic plorm'.&quot;
</em><br>
<em>&gt; 6:  &quot;If I exercised for an extra hour next week, that would make
</em><br>
<em>&gt; up for it.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; If these thoughts are all (human idiom) mental sentences in the internal
</em><br>
<em>&gt; narrative, I wouldn't expect them to be pronounced simultaneously
</em><br>
<em>&gt; by a deep
</em><br>
<em>&gt; adult voice and a squeaky child voice.  Rather I would expect them to be
</em><br>
<em>&gt; interlaced, even though {1, 3, 5} relate to one piece of open goal imagery
</em><br>
<em>&gt; and {2, 4, 6} relates to a different piece of goal imagery.  So the
</em><br>
<em>&gt; deliberative tracks {1, 3, 5} and {2, 4, 6} are simultaneous, but the
</em><br>
<em>&gt; thoughts 1, 2, 3, 4, 5, 6 occur sequentially.
</em><br>
<p>This is not how my mind works introspectively.  In my mind, 1, 2 and 3
<br>
appear to me to occur simultaneously.
<br>
<p>Now, you can tell me that I'm deluded and they REALLY occur sequentially in
<br>
my  mind but I don't know it.
<br>
<p>But I'm not going to believe you unless you have some REALLY HARD
<br>
neurological proof.
<br>
<p><em>&gt; If it's a static mapping, based on a (claimed) correspondence, it's a
</em><br>
<em>&gt; &quot;sensory&quot; mapping.  (I know this is overloading 'sensory' in an entirely
</em><br>
<em>&gt; different sense, dammit.  Maybe I should replace 'sensory' within SPDM.
</em><br>
<em>&gt; &quot;Correlative?&quot;  &quot;Correspondence?&quot;)
</em><br>
<p>I don't know what the right word is, because I'm not sure I understand what
<br>
you mean yet.
<br>
<p><p><p>Generally speaking I think there are two things going on here, that are
<br>
causing me to be confused:
<br>
<p>1) You're using terms in ways that are not incorrect but are just a little
<br>
nonstandard, without giving quite explicit enough definitions for them.
<br>
<p>2) You're taking intuitions gained from your own introspection, combined
<br>
with your study of cognitive science, and generalizing them to come to
<br>
conclusions that disagree with the conclusions I've gained from my own
<br>
introspection, combined with my study of cognitive science
<br>
<p><p>Until problem 1 is more fully resolved, I'm not going to be able to really
<br>
assess how deep problem 2 is.
<br>
<p>Of course, problem 1 doesn't make the paper a bad paper by any means.  I
<br>
think this kind of problem runs through nearly all work on cognitive science
<br>
and serious AI.  People use words in slightly different ways and talk past
<br>
each other, it's the norm.  One of the many reasons progress is so slow!
<br>
<p>And problem 2 isn't necessarily a &quot;problem&quot; at all, in the sense that
<br>
different people are validly going to have different intuitions, and right
<br>
there is often not enough data from cog sci or AI to prove one intuition or
<br>
another correct.
<br>
<p>Pei Wang (one of my chief Webmind collaborators) and I often spent a long
<br>
time arriving at a mutually comprehensible language.  Then, once we had, we
<br>
could make 75% of our differences go away.  The other 25% we just had to
<br>
chalk up to different intuitions, and move on...
<br>
<p><p>-- ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3345.html">Evan Reese: "Re: Why bother (was Re: Introducing myself)"</a>
<li><strong>Previous message:</strong> <a href="3343.html">Eliezer S. Yudkowsky: "META: Politics on SL4"</a>
<li><strong>In reply to:</strong> <a href="3342.html">Eliezer S. Yudkowsky: "Re: DGI Paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3354.html">Michael Roy Ames: "Re: DGI Paper"</a>
<li><strong>Reply:</strong> <a href="3354.html">Michael Roy Ames: "Re: DGI Paper"</a>
<li><strong>Reply:</strong> <a href="../0205/3585.html">Eliezer S. Yudkowsky: "Re: DGI Paper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3344">[ date ]</a>
<a href="index.html#3344">[ thread ]</a>
<a href="subject.html#3344">[ subject ]</a>
<a href="author.html#3344">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
