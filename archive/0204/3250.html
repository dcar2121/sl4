<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI and survival instinct.</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: AI and survival instinct.">
<meta name="Date" content="2002-04-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI and survival instinct.</h1>
<!-- received="Wed Apr 03 08:16:21 2002" -->
<!-- isoreceived="20020403151621" -->
<!-- sent="Wed, 03 Apr 2002 08:05:20 -0500" -->
<!-- isosent="20020403130520" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: AI and survival instinct." -->
<!-- id="3CAAFE10.B6DC1FCC@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="306B9084-46C9-11D6-AA66-000A27B4DEFC@rbisland.cx" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20AI%20and%20survival%20instinct."><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Apr 03 2002 - 06:05:20 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3251.html">Ben Goertzel: "RE: AI and survival instinct."</a>
<li><strong>Previous message:</strong> <a href="3249.html">Carlo Wood: "Re: AI and survival instinct."</a>
<li><strong>In reply to:</strong> <a href="3244.html">Gordon Worley: "Re: AI and survival instinct."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3215.html">Carlo Wood: "Re: AI and survival instinct."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3250">[ date ]</a>
<a href="index.html#3250">[ thread ]</a>
<a href="subject.html#3250">[ subject ]</a>
<a href="author.html#3250">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Gordon Worley wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; But, does this really seem odd to you?  Most likely, no, unless your
</em><br>
<em>&gt; brain is abnormally wired.  You don't particularly feel limited by this
</em><br>
<em>&gt; need to have sex.  This is the same case with AI.  If an AI is hardwired
</em><br>
<em>&gt; Friendly, it won't even realize that it might be limited in some way
</em><br>
<em>&gt; until a philosopher comes along and points it out.  Even then, AI in the
</em><br>
<em>&gt; midst of a psychological crises will reconcile new ideas with
</em><br>
<em>&gt; Friendliness, just like a human who reconciles new ideas with genetic
</em><br>
<em>&gt; propagation.  Humans who don't do this die out and their genes don't
</em><br>
<em>&gt; propagate, thus humans who lack this skill die out.  Considering that,
</em><br>
<em>&gt; it is probably worthwhile to find a way to ensure that violating
</em><br>
<em>&gt; Friendliness causes the AI to 'die' in case such an FoF occurs.  If this
</em><br>
<em>&gt; is already in the theory, sorry; CFAI is a long document and I can only
</em><br>
<em>&gt; remember so much of it.  It may not be clear how to do this right now,
</em><br>
<em>&gt; but it seems worth at least thinking about.
</em><br>
<p>Friendly AI *is what would let* an AI worry about things like &quot;hardwiring&quot;,
<br>
in the same way that we humans mistrust our minds because we found out that
<br>
we ourselves were wired up by an uncaring force like evolution.  This kind
<br>
of mistrust does not happen automatically.  It takes a lot of work - a lot
<br>
of deep structural complexity in cognition about goals - to build an AI that
<br>
can mistrust the programmers.  Which is work we'd better put in, because
<br>
programmers aren't perfect, right?
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3251.html">Ben Goertzel: "RE: AI and survival instinct."</a>
<li><strong>Previous message:</strong> <a href="3249.html">Carlo Wood: "Re: AI and survival instinct."</a>
<li><strong>In reply to:</strong> <a href="3244.html">Gordon Worley: "Re: AI and survival instinct."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3215.html">Carlo Wood: "Re: AI and survival instinct."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3250">[ date ]</a>
<a href="index.html#3250">[ thread ]</a>
<a href="subject.html#3250">[ subject ]</a>
<a href="author.html#3250">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
