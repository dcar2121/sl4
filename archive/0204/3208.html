<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI and survival instinct.</title>
<meta name="Author" content="Gordon Worley (redbird@rbisland.cx)">
<meta name="Subject" content="Re: AI and survival instinct.">
<meta name="Date" content="2002-04-01">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI and survival instinct.</h1>
<!-- received="Mon Apr 01 21:39:22 2002" -->
<!-- isoreceived="20020402043922" -->
<!-- sent="Mon, 1 Apr 2002 21:28:33 -0500" -->
<!-- isosent="20020402022833" -->
<!-- name="Gordon Worley" -->
<!-- email="redbird@rbisland.cx" -->
<!-- subject="Re: AI and survival instinct." -->
<!-- id="5481C7E8-45E1-11D6-8231-000A27B4DEFC@rbisland.cx" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJIEGOCFAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Gordon Worley (<a href="mailto:redbird@rbisland.cx?Subject=Re:%20AI%20and%20survival%20instinct."><em>redbird@rbisland.cx</em></a>)<br>
<strong>Date:</strong> Mon Apr 01 2002 - 19:28:33 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3209.html">Ben Goertzel: "RE: Introducing myself"</a>
<li><strong>Previous message:</strong> <a href="3207.html">Eliezer S. Yudkowsky: "Re: Introducing myself"</a>
<li><strong>In reply to:</strong> <a href="3203.html">Ben Goertzel: "RE: AI and survival instinct."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3213.html">Carlo Wood: "Re: AI and survival instinct."</a>
<li><strong>Reply:</strong> <a href="3213.html">Carlo Wood: "Re: AI and survival instinct."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3208">[ date ]</a>
<a href="index.html#3208">[ thread ]</a>
<a href="subject.html#3208">[ subject ]</a>
<a href="author.html#3208">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Monday, April 1, 2002, at 07:49  PM, Ben Goertzel wrote:
<br>
<p><em>&gt; Another point is that if your posited AI's are really SO similar to 
</em><br>
<em>&gt; humans,
</em><br>
<em>&gt; then they will likely have the human sympathy for other humans (flawed 
</em><br>
<em>&gt; and
</em><br>
<em>&gt; partial as this sympathy obviously is).
</em><br>
<em>&gt;
</em><br>
<em>&gt; What you are positing is an AI with a human emotional orientation but 
</em><br>
<em>&gt; NOT a
</em><br>
<em>&gt; human compassion toward other humans.
</em><br>
<p>This is a good way to get in something not worth of it's own thread, but 
<br>
good for an established one.
<br>
<p>I recently ready /Do Androids Dream of Electric Sheep/ by Philip K. 
<br>
Dick.  In case anyone is not aware, Dick is a good SF writer and this 
<br>
particular book was the basis for the movie /Blade Runner/ (which, for 
<br>
all its flaws, I consider one of the better bits of SF cinema, depending 
<br>
on which particular cut of the film we are talking about).  
<br>
Interestingly, this 35 year old book hits on a topic of prime interest 
<br>
to modern AI researchers.
<br>
<p>This next part is a bit of spoiler to bring everyone who hasn't read the 
<br>
book up to speed and refresh others' memories.  No plot is discussed, 
<br>
just a couple of ideas presented.
<br>
<p>The androids have no empathy.  Humans, on the other hand, are unique in 
<br>
having empathy for other living creatures.  Even androids.
<br>
<p>Okay, that's it for spoilers.
<br>
<p>As now theorized with a degree of certainty, humans evolved in a group 
<br>
environment that has led them to be programmed to behave in ways that, 
<br>
while mostly focused on propagating one's own genes, allow them to help 
<br>
their fellow humans to degrees directly proportional to degree of 
<br>
relatedness (i.e. similarity of genes).  Consequently, this means that 
<br>
humans will do act in situations to help the group rather than 
<br>
themselves.  For example, one humans might give his life in exchange for 
<br>
saving others.  The number of others any particular human is willing to 
<br>
die for depends on degree of relatedness.  While one might be willing to 
<br>
die for just one or two siblings, it will take millions of complete 
<br>
strangers to evoke the same kind of response.
<br>
<p>It is not part of the human emotional orientation to be quite as 
<br>
ruthless as an AI would be.  True, some humans are that ruthless, but 
<br>
the Universe is statistical and the number is very small, especially 
<br>
since the rest have a tendency to join forces and kill those individuals 
<br>
who would exploit them.  An AI, however, has no evolved sense of 
<br>
compassion for other AIs, let alone humans.  The consequence is that an 
<br>
AI will act in its own best interests all of the time since the AI 
<br>
doesn't have some genetic material and reproduction needs that will 
<br>
force it to develop a moral system that optimizes the performance of 
<br>
genes that it doesn't have.  This is the part where Friendliness comes 
<br>
in.
<br>
<p>An AI doesn't have a survival instinct in a literal sense, but most 
<br>
likely it will decide that it would rather live than die.  If it decides 
<br>
it doesn't matter whether it is running or not, it won't be running for 
<br>
long and will be a failed AI.  Any mind has to want to live to keep 
<br>
living (I am being very careful about my terminology on this point, so 
<br>
I'll note that even if what we call the mind in humans decides it 
<br>
doesn't care whether it lives or dies, the brain is hardcoded to want to 
<br>
stay alive).
<br>
<p>To get back to where this thread started, distributed AI computing is an 
<br>
engineering issue.  It has nothing to do with how the AI will behave 
<br>
outside of any effects caused by the infrastructure.  If being 
<br>
distributed over the Internet causes the AI to be unFriendly, for 
<br>
example (I don't see how, but this is just an example), then running a 
<br>
distributed AI is a bad idea.  My intuition tells me it won't matter, 
<br>
but I also have a feeling that might the naive outlook, with some really 
<br>
tricky stuff happening when the brain is distributed.
<br>
<p><pre>
--
Gordon Worley                     `When I use a word,' Humpty Dumpty
<a href="http://www.rbisland.cx/">http://www.rbisland.cx/</a>            said, `it means just what I choose
<a href="mailto:redbird@rbisland.cx?Subject=Re:%20AI%20and%20survival%20instinct.">redbird@rbisland.cx</a>                it to mean--neither more nor less.'
PGP:  0xBBD3B003                                  --Lewis Carroll
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3209.html">Ben Goertzel: "RE: Introducing myself"</a>
<li><strong>Previous message:</strong> <a href="3207.html">Eliezer S. Yudkowsky: "Re: Introducing myself"</a>
<li><strong>In reply to:</strong> <a href="3203.html">Ben Goertzel: "RE: AI and survival instinct."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3213.html">Carlo Wood: "Re: AI and survival instinct."</a>
<li><strong>Reply:</strong> <a href="3213.html">Carlo Wood: "Re: AI and survival instinct."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3208">[ date ]</a>
<a href="index.html#3208">[ thread ]</a>
<a href="subject.html#3208">[ subject ]</a>
<a href="author.html#3208">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
