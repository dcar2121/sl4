<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: FW: DGI Paper</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: FW: DGI Paper">
<meta name="Date" content="2002-04-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: FW: DGI Paper</h1>
<!-- received="Sat Apr 13 20:45:26 2002" -->
<!-- isoreceived="20020414024526" -->
<!-- sent="Sat, 13 Apr 2002 18:42:55 -0600" -->
<!-- isosent="20020414004255" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: FW: DGI Paper" -->
<!-- id="LAEGJLOGJIOELPNIOOAJOEDFCGAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="3CB8BC96.A378EB00@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20FW:%20DGI%20Paper"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sat Apr 13 2002 - 18:42:55 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3342.html">Eliezer S. Yudkowsky: "Re: DGI Paper"</a>
<li><strong>Previous message:</strong> <a href="3340.html">Eliezer S. Yudkowsky: "Re: FW: DGI Paper"</a>
<li><strong>In reply to:</strong> <a href="3340.html">Eliezer S. Yudkowsky: "Re: FW: DGI Paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3342.html">Eliezer S. Yudkowsky: "Re: DGI Paper"</a>
<li><strong>Reply:</strong> <a href="3342.html">Eliezer S. Yudkowsky: "Re: DGI Paper"</a>
<li><strong>Reply:</strong> <a href="3353.html">Eliezer S. Yudkowsky: "Patterns and intelligence (was: DGI paper)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3341">[ date ]</a>
<a href="index.html#3341">[ thread ]</a>
<a href="subject.html#3341">[ subject ]</a>
<a href="author.html#3341">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
hi,
<br>
<p><p><p><em>&gt; &gt; 1)
</em><br>
<em>&gt; &gt; I think your characterization of concepts is in places too narrow.  Your
</em><br>
<em>&gt; &gt; statement “concepts are patterns that mesh with sensory
</em><br>
<em>&gt; imagery”  seems to
</em><br>
<em>&gt; &gt; me to miss abstract concepts and also concepts related to
</em><br>
<em>&gt; action rather than
</em><br>
<em>&gt; &gt; perception.  I realize that later on you do mention abstract concepts.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Concepts can generalize over the perceptual correlates of realtime skills
</em><br>
<em>&gt; and generalize over reflective percepts.  The same &quot;kernel&quot; idiom applies.
</em><br>
<p>By a &quot;reflective percept&quot; you mean a perception of something inside the mind
<br>
rather than something in the external world?
<br>
<p>It wasn't clear to me that this was contained within your definition of a
<br>
&quot;percept&quot; -- but if it is, that clarifies a lot.
<br>
<p><em>&gt; &quot;Differential operator&quot; is abstract but that doesn't mean it's
</em><br>
<em>&gt; non-perceptual.  It means that its important perceptual correlates are
</em><br>
<em>&gt; abstract perceptual models and realtime skills in abstract
</em><br>
<em>&gt; models,
</em><br>
<p>I don't think I understand your use of the terms &quot;percept&quot; and &quot;perception&quot;?
<br>
Could you tell me how you define these things?
<br>
You seem to be using them much more broadly than me, which may the the
<br>
source of much of my confusion.
<br>
<p><em>&gt; For example, you might recognize the operator &quot;d/dx&quot; visually,
</em><br>
<em>&gt; apply it to a
</em><br>
<em>&gt; symbol with the auditory tag &quot;x squared&quot;, and end up with a
</em><br>
<em>&gt; symbol with the
</em><br>
<em>&gt; auditory tag &quot;two x&quot;.  Of course this is more of a perceptual
</em><br>
<em>&gt; correlate than
</em><br>
<em>&gt; the perception itself.
</em><br>
<p>Sure, but when one comes up with a NEW mathematical concept, sometimes it is
<br>
not associated with ANY visual, auditory or otherwise &quot;imagistic&quot; stuff.
<br>
It's purely a new math concept, which then has to be, through great labor,
<br>
associated with appropriate symbols, pictures, names, or what have you.
<br>
<p><em>&gt; Far as I know, they're all perceptual in the end.  It's just that the
</em><br>
<em>&gt; perceptual idiom - modalities, including feature structure,
</em><br>
<em>&gt; detector/controller structure, and occasionally realtime motor structure -
</em><br>
<em>&gt; extends far beyond things like vision and sound, to include
</em><br>
<em>&gt; internal reality
</em><br>
<em>&gt; as well.
</em><br>
<p>This is getting to the crux of my issue, I think.  You define &quot;perception&quot;
<br>
as a kind of abstract structure/process, but in the paper I don't think it's
<br>
entirely clear that this is how you're defining &quot;perception&quot;.  At least it
<br>
wasn't that clear to me.  I generally think of perception as having to do
<br>
with the processing of stimuli from the external world.
<br>
<p>Based on your very broad definition of perception, I'm not sure how to
<br>
distinguish it from cognition.  I guess in your view perception serves
<br>
<p>1) to process external-world data
<br>
2) as one among many cognitive structures/processes
<br>
<p>I don't think this is the standard use of the term &quot;perception&quot;, though
<br>
there's nothing particularly wrong with it once it's understood.
<br>
<p>I'm still not sure however that a new abstract math concept that I conceive
<br>
in the bowels of my unconscious is &quot;perceptual in the end.&quot;  I think that
<br>
its conception may in some cases NOT involve feature structures and
<br>
detector/controller structures.  A new math concept may arise thru
<br>
combinatory &amp; inferential operations on existing math concepts, without any
<br>
of the perceptual/motor hierarchy-type structures you're describing.
<br>
<p>Math concepts are not the only example of this, of course, they're just a
<br>
particularly clear example because of their highly abstract nature.
<br>
<p><p><em>&gt; &gt; When you say “A thought is a specific structure of combinatorial symbols
</em><br>
<em>&gt; &gt; which builds or alters mental imagery” – I am not sure why
</em><br>
<em>&gt; “imagery” comes
</em><br>
<em>&gt; &gt; into it.  It seems that you are using this word in a way that is not
</em><br>
<em>&gt; &gt; necessarily related to visual imagery, which is a little bit
</em><br>
<em>&gt; confusing. I’d
</em><br>
<em>&gt; &gt; like to see a definition of “mental imagery” as you use it here.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I need to emphasize more that when I say &quot;imagery&quot; I am referring to
</em><br>
<em>&gt; generalized working memory in all perceptual modalities, not just
</em><br>
<em>&gt; the visual
</em><br>
<em>&gt; modality.
</em><br>
<p>The key point is still, however, whether by &quot;perceptual modalities&quot; you mean
<br>
modalities for sensing the external world, or something more abstract.
<br>
<p>I don't think that a new math concept i cook up necessarily has anything to
<br>
do with imagery derived from any of the external-world senses.  Of course
<br>
connections with sensorimotor domains can be CREATED, and must be for
<br>
communication purposes.  But this may not be the case for AI's, which will
<br>
be able to communicate by direct exchange of mindstuff rather than via
<br>
structuring physicalistic actions &amp; sensations.
<br>
<p><em>&gt; &gt; Don’t you
</em><br>
<em>&gt; &gt; believe some thoughts are entirely non-imagistic, purely
</em><br>
<em>&gt; abstract without
</em><br>
<em>&gt; &gt; any reliance on sensory metaphors?
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think some thoughts rely on reflective imagery or imagery which is not
</em><br>
<em>&gt; visualized all the way down to the sensory level.
</em><br>
<p>Again this same language.  You're talking about some kind of &quot;visualizing&quot;
<br>
at a non-sensory level.  I'm not sure what you mean by &quot;visualizing&quot; then.
<br>
<p><em>&gt; &gt; But “smooth” always means continuous (or differentiable) in
</em><br>
<em>&gt; mathematics, and
</em><br>
<em>&gt; &gt; the cognitively &amp; evolutionarily relevant fitness landscapes
</em><br>
<em>&gt; definitely are
</em><br>
<em>&gt; &gt; NOT.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Smooth&quot; in fitness landscapes means that similar things are separated by
</em><br>
<em>&gt; short distances, and especially that incremental improvements are short
</em><br>
<em>&gt; distances.  In the case of a modality smoothing a raw scene, you can think
</em><br>
<em>&gt; of distance as being the distance between feature detectors instead of the
</em><br>
<em>&gt; distance between raw pixels, or &quot;distance&quot; as being inversely proportional
</em><br>
<em>&gt; to the probability of that step being taken within the system.
</em><br>
<p>This is just a terminology point, but I still think that your terminology is
<br>
not the standard one.
<br>
<p>I still believe that, in the standard terminology, a fitness landscape that
<br>
has local minima and maxima at all perceivable scales, is not &quot;smooth&quot; in
<br>
standard usage.  It's fractal.
<br>
<p>The processing done in visual &amp; auditory cortex often resembles
<br>
windowed-fourier or wavelet transforms, and this does result in a kind of
<br>
smoothing in that hi-frequency components are omitted.
<br>
<p>Anyway it would be good if you just clarified in the text what you meant by
<br>
&quot;smooth&quot; -- it's certainly no big deal.
<br>
<p><em>&gt; Coopting premotor
</em><br>
<em>&gt; neurons sounds like coopting the sensorimotor modality to support Lakoff &amp;
</em><br>
<em>&gt; Johnson's sensorimotor metaphors, not to support reflective
</em><br>
<em>&gt; realtime skills
</em><br>
<em>&gt; per se.
</em><br>
<p>Might be so, or they could serve both roles.  These
<br>
perceptual-cognitive-active loops are complex and go beyond my knowledge of
<br>
neurobiology.
<br>
<p><em>&gt;  Do you think we need to go to robot eyes and
</em><br>
<em>&gt; such, or do you
</em><br>
<em>&gt; &gt; think the Net will suffice?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Billiards and mini-Go and code are definitely not rich enough because they
</em><br>
<em>&gt; can't easily support the classic Lakoff and Johnson schema such as
</em><br>
<em>&gt; line-connection, part-whole, center-periphery, container-contained, and so
</em><br>
<em>&gt; on.  But I can't see any good way to do that without gritting teeth and
</em><br>
<em>&gt; starting on a 3D pixel/voxel world, which may be too ambitious for a first
</em><br>
<em>&gt; AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The Net can't help you here.  You can't have a modality with a
</em><br>
<em>&gt; computationally tractable feature structure unless your target environment
</em><br>
<em>&gt; *has* that kind of structure to begin with.  If you're going to put a baby
</em><br>
<em>&gt; AI in a rich environment, the richness has to be the kind that the baby AI
</em><br>
<em>&gt; can learn to see incrementally.
</em><br>
<p>I don't understand why you think a baby AI can't learn to see the Net
<br>
incrementally.
<br>
<p><em>&gt; &gt; 6)
</em><br>
<em>&gt; &gt; In 2.5.2, what do you mean by “verify the generalization”?
</em><br>
<em>&gt; Could you give
</em><br>
<em>&gt; &gt; a couple examples?
</em><br>
<em>&gt;
</em><br>
<em>&gt; What I mean is that noticing a perceptual cue that all the
</em><br>
<em>&gt; billiards in the
</em><br>
<em>&gt; &quot;key&quot; group are red, and that all the billiards in the &quot;non-key&quot; group are
</em><br>
<em>&gt; not red, is not the same as verifying that this is actually the case.  The
</em><br>
<em>&gt; cognitive process that initially delivers the perceptual cue, the
</em><br>
<em>&gt; suggestion
</em><br>
<em>&gt; saying &quot;Hey, check this out and see if it's true&quot;, may not always
</em><br>
<em>&gt; be the one
</em><br>
<em>&gt; that does the verification.
</em><br>
<p>So the verification is just done by more careful study of the same perceived
<br>
scene, in this case?
<br>
<p><em>&gt;
</em><br>
<em>&gt; &gt; 7)
</em><br>
<em>&gt; &gt; You say  “concepts are learned, thoughts are invented.”  I
</em><br>
<em>&gt; don’t quite catch
</em><br>
<em>&gt; &gt; the sense of this.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Complex concepts are certainly “invented” as well, under the normal
</em><br>
<em>&gt; &gt; definition of “invention.” …
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The concept of a pseudoinverse of a matrix was invented by Moore and
</em><br>
<em>&gt; &gt; Penrose, not learned by them.  I learned it from a textbook.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The concept of &quot;Singularity&quot; was invented as well...
</em><br>
<em>&gt;
</em><br>
<em>&gt; Well, you can learn a concept from the thoughts that you invent -
</em><br>
<em>&gt; generalize
</em><br>
<em>&gt; a kernel over the reflective perceptual correlates of the
</em><br>
<em>&gt; thoughts.  But the
</em><br>
<em>&gt; concept-creating cognitive process will still reify (&quot;learn&quot;) a
</em><br>
<em>&gt; perception,
</em><br>
<em>&gt; and the deliberative thought process that created the abstract/reflective
</em><br>
<em>&gt; perceptions being reified will still be inventive.
</em><br>
<p>I don't understand this.  If I create a silly concept right now, such as,
<br>
say,
<br>
<p>&quot;Differential functions on [-5,5] whose third derivative is confined to the
<br>
interval [0,1]&quot;
<br>
<p>then how is this concept LEARNED?  I didn't learn this, I just INVENTED it.
<br>
It's a concept.  A damn useless one (though maybe some thinking will
<br>
discover that it's useful after all...), but a concept nonetheless.  There
<br>
also seems to be no PERCEPTION involved here, in any way I can understand.
<br>
No external sensations, directly or metaphorically, and also no process of
<br>
hierarchical feature detection &amp; control.  Rather, a simple process of
<br>
*combining known concepts*.  I guess you can say that my mind had to
<br>
&quot;perceive&quot; these known concepts in some sense in order to combine them, but
<br>
this &quot;perception&quot; isn't perception in any very strong sense -- it's
<br>
perception only in the sense that any mental schema &quot;perceives&quot; the
<br>
arguments that are fed into it...
<br>
<p><em>&gt; &gt; Self-organizing mental processes acting purely on the thought
</em><br>
<em>&gt; level seem to
</em><br>
<em>&gt; &gt; play a big role as well.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Is embodied by&quot; might be a better term than &quot;arises&quot;.  Even so, which
</em><br>
<em>&gt; specific self-organizing processes?
</em><br>
<p>Evolutionary &amp; hypothetically-inferential combination of existing concepts &amp;
<br>
parts thereof into new ones, guided by detected associations between
<br>
concepts.  With a complex dynamic of attention allocation guiding the
<br>
control of the process.
<br>
<p><em>&gt; What I mean is that the way humans perceive confidence,
</em><br>
<em>&gt; quantitatively, may
</em><br>
<em>&gt; not be the *best* way to perceive confidence.  It may not even be the way
</em><br>
<em>&gt; humans perceive confidence.  As you said, in Novamente you work with
</em><br>
<em>&gt; triples.
</em><br>
<p>Sure, there are lots of ways to measure truth value, and tradeoffs with all
<br>
of them.  No doubt an advanced AI will rewrite whatever we initially insert
<br>
in this slot of its design.
<br>
<p><em>&gt; &gt; 10)
</em><br>
<em>&gt; &gt; You say “ ‘one thought at a time’  is just the human way of
</em><br>
<em>&gt; doing things ….”
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Actually it isn’t, I often have more than one thought at a time.
</em><br>
<em>&gt;
</em><br>
<em>&gt; No, you often have mental imagery that depicts ongoing cognition
</em><br>
<em>&gt; within more
</em><br>
<em>&gt; than one train of thought, and you switch around the focus of attention,
</em><br>
<p>I feel that my focus of attention can span two or three different thoughts
<br>
at once, sometimes.
<br>
<p><em>&gt; which means that more than one deliberative track can coexist.  You still
</em><br>
<em>&gt; think only one thought at a time.  Or do you mean that you pronounce more
</em><br>
<em>&gt; than one mental sentence at a time?  You've got to keep the thought level
</em><br>
<em>&gt; and the deliberation level conceptually separate; I said &quot;one thought at a
</em><br>
<em>&gt; time&quot;, not &quot;one deliberation at a time&quot;.
</em><br>
<p>I don't understand how you define &quot;thought&quot;, then.  Could you give me a
<br>
clearer definition?
<br>
<p>And please don't use a variant of the &quot;there can only be one at a time&quot;
<br>
restriction in the definition!  ;)
<br>
<p>So far as I know, the physiology of human consciousness indicates that
<br>
humans can have multiple perceptual-cognitive-active loops of conscious
<br>
awareness running at once.
<br>
<p>Consciousness often has a subjective &quot;unity&quot; to it, but in my experience,
<br>
not always.
<br>
<p><p><em>&gt; As discussed in the section on seed AI, I think that splitting up
</em><br>
<em>&gt; available
</em><br>
<em>&gt; brainpower into separate entities is less productive than
</em><br>
<em>&gt; agglomerating it.
</em><br>
<p>Well, this will be a fun issue to explore empirically!!  There's no real
<br>
need to resolve it now, in my view; I think it doesn't make a big difference
<br>
for AI engineering.
<br>
<p><em>&gt;  But an &quot;action binding&quot; that doesn't involve a feedback
</em><br>
<em>&gt; loop, just a
</em><br>
<em>&gt; direct correspondence between a patterned variable in cognition and a
</em><br>
<em>&gt; patterned variable in motor reality, is just another kind of
</em><br>
<em>&gt; sensory mapping
</em><br>
<em>&gt; - albeit one where causality flows in the opposite direction.
</em><br>
<p>I guess that if you count kinesthetic sensation as a sense, then all motor
<br>
actions can be mapped into the domain of sensation and considered that way.
<br>
In practice of course, these particular &quot;sensory mappings&quot; (that are really
<br>
motor mappings ;) will have to be treated pretty differently than the other
<br>
sensory mappings.
<br>
<p><em>&gt; &gt; 13)
</em><br>
<em>&gt; &gt; You say that “evolution… cannot boast general intelligence.”
</em><br>
<em>&gt; [&quot;cannot invoke&quot;]
</em><br>
<em>&gt; &gt; This is not so clear to me.  Why?  It seems to me that evolution in fact
</em><br>
<em>&gt; &gt; does display a pretty generalized kind of intelligence.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Because there is a difference between genericity and generality.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Evolution, like search trees and artificial neural networks, is a fully
</em><br>
<em>&gt; generic process.  But it works much better for some things than
</em><br>
<em>&gt; others, and
</em><br>
<em>&gt; some things it can't handle at all.  It can't do many of the things that
</em><br>
<em>&gt; human intelligence does.  You can apply a generic process to
</em><br>
<em>&gt; anything but it
</em><br>
<em>&gt; won't necessarily work.  Usually it only solves a tiny fraction of special
</em><br>
<em>&gt; cases of the problem (which AI projects usually go on to mistake
</em><br>
<em>&gt; for having
</em><br>
<em>&gt; solved the general case of the problem; this is one of the Deadly Sins).
</em><br>
<em>&gt; Evolution uses an unreasonable amount of computational power to overcome
</em><br>
<em>&gt; this handicap.
</em><br>
<p>I think that any black-box global optimization algorithm -- including
<br>
evolution, and some NN and search-tree based algorithms -- has a kind of
<br>
&quot;general intelligence.&quot;  The problem is that it uses too many resources.
<br>
Human brains achieve far more general intelligence per unit of space and
<br>
time resources than evolutionary systems.
<br>
<p>What I mean by &quot;general intelligence&quot; is roughly &quot;the ability to solve a
<br>
variety of complex problems in a variety of complex environments.&quot;  As you
<br>
know I've tried with moderate success to quantify and formalize this
<br>
definition.  I'm not sure exactly what you mean by &quot;general intelligence&quot;,
<br>
maybe it's something different.
<br>
<p>-- Ben
<br>
<p><p>p.s. Well, at least this thread is counteracting the recent mini-trend of
<br>
SL4 becoming a chatty list ;&gt;
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3342.html">Eliezer S. Yudkowsky: "Re: DGI Paper"</a>
<li><strong>Previous message:</strong> <a href="3340.html">Eliezer S. Yudkowsky: "Re: FW: DGI Paper"</a>
<li><strong>In reply to:</strong> <a href="3340.html">Eliezer S. Yudkowsky: "Re: FW: DGI Paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3342.html">Eliezer S. Yudkowsky: "Re: DGI Paper"</a>
<li><strong>Reply:</strong> <a href="3342.html">Eliezer S. Yudkowsky: "Re: DGI Paper"</a>
<li><strong>Reply:</strong> <a href="3353.html">Eliezer S. Yudkowsky: "Patterns and intelligence (was: DGI paper)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3341">[ date ]</a>
<a href="index.html#3341">[ thread ]</a>
<a href="subject.html#3341">[ subject ]</a>
<a href="author.html#3341">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
