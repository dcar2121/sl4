<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Why bother</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Why bother">
<meta name="Date" content="2002-04-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Why bother</h1>
<!-- received="Sun Apr 14 21:45:26 2002" -->
<!-- isoreceived="20020415034526" -->
<!-- sent="Sun, 14 Apr 2002 21:36:06 -0400" -->
<!-- isosent="20020415013606" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Why bother" -->
<!-- id="3CBA2E86.7190A06@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="007701c1df98$544add20$ad97b841@mycomputer" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Why%20bother"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Apr 14 2002 - 19:36:06 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3352.html">Eliezer S. Yudkowsky: "Re: You Know You've Been Thinking About The Singularity Too Long When..."</a>
<li><strong>Previous message:</strong> <a href="3350.html">Eliezer S. Yudkowsky: "Re: You Know You've Been Thinking About The Singularity Too Long When..."</a>
<li><strong>In reply to:</strong> <a href="3311.html">Evan Reese: "Re: Why bother (was Re: Introducing myself)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3370.html">Gordon Worley: "Re: Why bother"</a>
<li><strong>Reply:</strong> <a href="3370.html">Gordon Worley: "Re: Why bother"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3351">[ date ]</a>
<a href="index.html#3351">[ thread ]</a>
<a href="subject.html#3351">[ subject ]</a>
<a href="author.html#3351">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
One reason to actively work for a Singularity, fully sufficient to render it
<br>
our maximum priority in the total absence of any other consideration, is
<br>
that 1.8 people die per second, 6000 per hour, 150,000 per day.  Chewing one
<br>
week off the Singularity is enough to win the Nobel Peace Prize twenty times
<br>
over.
<br>
<p>But it's also true that there's more than that at stake; specifically, the
<br>
entire future of Earth-originating intelligent life, including all the
<br>
sentient beings who will ever exist after the Singularity - probably an
<br>
amount that dwarfs our present world into insignificance, no matter what
<br>
your discount rate.  You can assume this future isn't at risk and it would
<br>
still make sense to put every available resource into the inevitable
<br>
Singularity to bring it about sooner rather than later, but it happens that
<br>
this future is, to the best of my ability to guess, endangered.
<br>
<p>Evan Reese wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; But aside from that, I simply believe WITHOUT PROOF that we are unlikely.
</em><br>
<em>&gt; Perhaps I simply do not want to believe that we would be so unfortunate, or
</em><br>
<em>&gt; stupid, or apathetic to cause all of humanity to be wiped out.  I admit
</em><br>
<em>&gt; that.
</em><br>
<p>Not to take him out of context, he went on to write:
<br>
<p><em>&gt; But my viewpoint is at least *partly* based on what I consider evidence.
</em><br>
<p>Even so, I think the previous paragraph above says it all.  It is *not
<br>
pleasant* to consider the absolute extinction of Earth-originating
<br>
intelligent life.  In 1996 I wrote about how to accelerate a Singularity I
<br>
considered inevitable - with all the technological driving forces involved,
<br>
who could prevent it?  If I recall correctly, it wasn't until a debate on
<br>
nanotechnological offense and defense on the Extropians list - where I wound
<br>
up taking the part of &quot;offense beats defense&quot; because the &quot;defense beats
<br>
offense&quot; posters were so blatantly wrong - that I realized what I had
<br>
previously been unable to look in the face; it is very easy to prevent a
<br>
Singularity.  All you need to do is extinguish humanity.
<br>
<p>Consider Conway's Game of Life - a square grid of cells where a cell &quot;lives&quot;
<br>
or &quot;dies&quot; depending on the number of adjacent living cells; two adjacent
<br>
cells maintain the current state, three cells mean life, and 0-1 and 4-8
<br>
cells mean death.  Conway's Game of Life is famous for generating
<br>
astonishingly complex behaviors from these simple rules, and it has been
<br>
proven that the Game of Life is Turing-complete.
<br>
<p>Consider a sufficient huge Life board that an intelligent species evolves
<br>
within it.  Conway's Game of Life is deterministic and absolutely controlled
<br>
by the basic rules; there is no room for intervention without violating one
<br>
of the basic rules.  And in fact, just to eliminate the possibility of
<br>
intervention from outside the simulation, we won't consider this Life board
<br>
as a simulation.  We'll consider it as a mathematical object.  We will ask
<br>
&quot;What would happen, under the Life rules, within this universe?&quot;  By
<br>
assumption, one of the answers arrived at so far is &quot;intelligent life
<br>
evolves&quot;.  What happens to the intelligent life after that - under the
<br>
Platonic mathematical rules?
<br>
<p>The answer is that whatever is determined by the Life rules, happens,
<br>
because as we have set up this thought experiment, this species is beyond
<br>
the intervention of God.  They have no guarantee of a happy ending.  They
<br>
have no guarantee that things will turn out right.  There is no dramatic
<br>
unity in their universe.  What happens to them is determined solely by the
<br>
forward operation of causality.  If one of them accedes to the rulership of
<br>
a nation and decides to set up death camps, then eleven million Life
<br>
entities will die under the forward operation of causality.  If their world
<br>
is encapsulated within a level of organization above the basic Life rules
<br>
and their physicists figure out how to disrupt that level of organization,
<br>
in the same way that (if you've ever played around with Life) one extra dot
<br>
can disrupt an entire pattern, then their world and, who knows, their whole
<br>
universe may be wiped out.  Because that's what the Life rules say should
<br>
happen.  It won't matter that the physicists were looking for knowledge
<br>
under a quest that had previously yielded only benefit to their species. 
<br>
It's not that their universe ignores this fact, but that their universe
<br>
simply doesn't care one way or the other.  If the Life rules say their world
<br>
is destroyed, their world is destroyed.  They have no guarantee of a happy
<br>
ending.  They are beyond the intervention of God.
<br>
<p>Do you recognize this world?  You should.  You live there.
<br>
<p>When the Wright Brothers flew their first plane at Kitty Hawk, they were
<br>
flying in their own private bubble of space and time.  The universe didn't
<br>
look around to see all the previous attempts at flight that had tried and
<br>
failed.  The universe didn't check to see if the Wright Brothers deserved to
<br>
win more than all the previous failed aviators.  The universe didn't look
<br>
forward in time to the tremendous impact that air travel would have on our
<br>
own society.  The universe didn't ask whether air travel would benefit
<br>
humanity.  The *only* question asked by the universe at Kitty Hawk was
<br>
whether the wings on this contraption would generate enough lift through
<br>
Bernoulli's Principle to take the plane off the ground.  It so happens that
<br>
air travel was IMHO a tremendous benefit, but not only did the universe not
<br>
care, the universe did not check.  All the universe checked was the laws of
<br>
physics in Kitty Hawk's immediate vicinity.  The eventual consequences of
<br>
those laws was not the universe's concern.  The Wright Brothers put together
<br>
something that the laws of physics said should fly, so it did, regardless of
<br>
the future consequences or the surrounding social matrix or the ambient
<br>
memes about flight.
<br>
<p>This isn't the world you see in books and TV shows; those worlds obey the
<br>
law of dramatic unity, where it takes an important cause to have an
<br>
important effect, and the main character, in the middle of an agonizing
<br>
emotional crisis, is never killed by a completely unrelated truck halfway
<br>
through the novel.  But in real life the probability that you will be run
<br>
over by a truck is totally, absolutely unrelated to your potential impact in
<br>
the greater scheme of things.  It is solely determined by your alertness
<br>
when crossing the street.  We don't want to think like this because it is
<br>
too damn uncomfortable to think that the entire tragedy of WWII would not
<br>
have happened if Hitler had just happened to get bitten by a snake as a
<br>
kid.  I'm not a historian and have no reason to argue the point one way or
<br>
the other (although it seems likely that Neville Chamberlain at least was
<br>
also necessary to the ensuing tragedy).  But it would be perfectly plausible
<br>
in Conway's Game of Life universe, beyond the intervention of God - and
<br>
because we live in a world like that, we have no reason to suppose that the
<br>
same thing doesn't happen here.
<br>
<p>It is not true that one individual can't make a difference.  There is no
<br>
rule in which bounds the size of the consequence by the size of the effect. 
<br>
It could be that Douglas Lenat gives up on the Cyc paradigm and goes back to
<br>
building an improved Eurisko, and SuperEurisko running on Blue Gene is
<br>
sufficient unto a hard takeoff, and this SuperEurisko turns out to be an
<br>
unFriendly superintelligent bacterium, and humanity is wiped out of
<br>
existence where it would have otherwise gone through a positive Singularity
<br>
and created a tremendously fun future.  And it could be that Lenat happens
<br>
to stumble across Stanley Schmuck's &quot;Obsequious AI&quot; essay while searching
<br>
for online webcomics and decides to give Stanley the chance to fly in and
<br>
make a few adjustments to SuperEurisko, and this is enough to take humanity
<br>
through a positive Singularity where it would have otherwise been wiped out
<br>
by a superintelligent bacterium.  And it could be that Stanley is run over
<br>
by a car on his way out of the airport and this is enough to cause humanity
<br>
to be wiped out where we would have otherwise lived happily ever after. 
<br>
Such is life in a universe governed solely by the laws of physics and not
<br>
the laws of dramatic unity.
<br>
<p>It may give you the screaming meemies to contemplate such enormous
<br>
consequences resting on such fragile dependencies.  I know that it gives
<br>
*me* the screaming meemies to think of history being that fragile.  I want
<br>
to think of the future as resting on really strong convergent supports,
<br>
maybe allowing for acceleration if we put in some hard work, but not
<br>
allowing for flipflopping between happily-ever-after and total destruction
<br>
depending on airline flight arrival times.  Who knows, maybe our future does
<br>
rest on strong convergent supports and all that's really at stake is a
<br>
million lives a week.  But I don't *know* that to be the case, and the basic
<br>
nature of causality in our universe gives me no right to believe something
<br>
so comforting.
<br>
<p>I can't help but think of myself at sixteen in the Harold Washington library
<br>
randomly picking Vernor Vinge's &quot;True Names and Other Dangers&quot; off the
<br>
bookshelf, or my grand-uncle borrowing &quot;Great Mambo Chicken and the
<br>
Transhuman Condition&quot; from his library because he thought the
<br>
eleven-year-old Eliezer would be interested, or Michael Raimondi typing
<br>
&quot;Why?&quot; into Ask Jeeves.  Would we have wound up where we are today
<br>
regardless?  Maybe.  But I can't prove it.  All I know is that *some* parts
<br>
of my personal life history seem to rest on such strongly convergent
<br>
supports that I can't imagine my life having gone any other way, but there
<br>
are also specific important events resting on such fragile dependencies that
<br>
one time traveller sneezing could have blown them off track.  And from my
<br>
scant reading of history, it looks like the overall flow of human history
<br>
works the same way.  Some parts are strongly convergent.  Some parts are
<br>
not.
<br>
<p>Currently, it appears to me that some parts of the Singularity are strongly
<br>
convergent, and some parts are not.  Moore's Law appears very strongly
<br>
convergent, and the development of nanotechnology has also gained enough
<br>
momentum that it doesn't look much like the sort of thing one person could
<br>
influence anymore.  But there are still unconvergent parts of the
<br>
Singularity where individual efforts can have leverage, mostly because
<br>
nobody else is paying attention.  It is indeed beyond absurdity to live in a
<br>
world where you can't get face time with Britney Spears because ten million
<br>
other people want the same thing, while the people who care about the
<br>
Singularity - enough to actually do something about it instead of just
<br>
talking about it - are congregated in the cheery uncrowded space of a
<br>
handful of mailing lists.  But if you do manage to end up living on a planet
<br>
like that, you should try to get over the shock and incredulity as soon as
<br>
possible, so you can start taking advantage of it.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3352.html">Eliezer S. Yudkowsky: "Re: You Know You've Been Thinking About The Singularity Too Long When..."</a>
<li><strong>Previous message:</strong> <a href="3350.html">Eliezer S. Yudkowsky: "Re: You Know You've Been Thinking About The Singularity Too Long When..."</a>
<li><strong>In reply to:</strong> <a href="3311.html">Evan Reese: "Re: Why bother (was Re: Introducing myself)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3370.html">Gordon Worley: "Re: Why bother"</a>
<li><strong>Reply:</strong> <a href="3370.html">Gordon Worley: "Re: Why bother"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3351">[ date ]</a>
<a href="index.html#3351">[ thread ]</a>
<a href="subject.html#3351">[ subject ]</a>
<a href="author.html#3351">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:38 MDT
</em></small></p>
</body>
</html>
