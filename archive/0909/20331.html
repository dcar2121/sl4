<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: [sl4] I am a Singularitian who does not believe in the Singularity</title>
<meta name="Author" content="Giulio Prisco (2nd email) (eschatoon@gmail.com)">
<meta name="Subject" content="[sl4] I am a Singularitian who does not believe in the Singularity">
<meta name="Date" content="2009-09-30">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>[sl4] I am a Singularitian who does not believe in the Singularity</h1>
<!-- received="Wed Sep 30 04:03:42 2009" -->
<!-- isoreceived="20090930100342" -->
<!-- sent="Wed, 30 Sep 2009 12:03:40 +0200" -->
<!-- isosent="20090930100340" -->
<!-- name="Giulio Prisco (2nd email)" -->
<!-- email="eschatoon@gmail.com" -->
<!-- subject="[sl4] I am a Singularitian who does not believe in the Singularity" -->
<!-- id="1fa8c3b90909300303m3fa1c4e7u98a98c2df1464106@mail.gmail.com" -->
<!-- charset="windows-1252" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Giulio Prisco (2nd email) (<a href="mailto:eschatoon@gmail.com?Subject=Re:%20[sl4]%20I%20am%20a%20Singularitian%20who%20does%20not%20believe%20in%20the%20Singularity"><em>eschatoon@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Sep 30 2009 - 04:03:40 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20332.html">Brad Johnson: "Re: [sl4] I am a Singularitian who does not believe in the Singularity"</a>
<li><strong>Previous message:</strong> <a href="20330.html">natasha@natasha.cc: "Re: [sl4] Bob and Cryonics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20332.html">Brad Johnson: "Re: [sl4] I am a Singularitian who does not believe in the Singularity"</a>
<li><strong>Reply:</strong> <a href="20332.html">Brad Johnson: "Re: [sl4] I am a Singularitian who does not believe in the Singularity"</a>
<li><strong>Reply:</strong> <a href="20334.html">Matt Mahoney: "Re: [sl4] I am a Singularitian who does not believe in the Singularity"</a>
<li><strong>Reply:</strong> <a href="20335.html">Robin Lee Powell: "Re: [sl4] I am a Singularitian who does not believe in the Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20331">[ date ]</a>
<a href="index.html#20331">[ thread ]</a>
<a href="subject.html#20331">[ subject ]</a>
<a href="author.html#20331">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I am a Singularitian who does not believe in the Singularity
<br>
<p><a href="http://cosmi2le.com/index.php?/site/i_am_a_singularitian_who_does_not_believe_in_the_singularity/">http://cosmi2le.com/index.php?/site/i_am_a_singularitian_who_does_not_believe_in_the_singularity/</a>
<br>
<p>I am going to the Singularity Summit in New York, and look forward to
<br>
a very interesting program with many old and new friends. If you are
<br>
there, I hope to meet you. I will now summarize my thoughts on the
<br>
Singularity.
<br>
<p>The (current) Wikipedia definition: The technological singularity is
<br>
the theoretical future point which takes place during a period of
<br>
accelerating change sometime after the creation of a
<br>
superintelligence. I just updated it as: The technological singularity
<br>
is the theoretical sudden, exponential and unpredictable accelerating
<br>
change which takes place sometime after the creation of a
<br>
superintelligence. Wikipedia continues: as the machine became more
<br>
intelligent it would become better at becoming more intelligent, which
<br>
could lead to an exponential and quite sudden growth in intelligence
<br>
(intelligence explosion). The Singularity is a sudden catastrophic (in
<br>
the mathematical sense) phase transition, a Dirac delta in history, a
<br>
point after which the old rules are not valid anymore and must be
<br>
replaced by new rules which we are unable to imagine at this
<br>
moment—like the new “Economy 2.0”, not understandable by non-augmented
<br>
humans, described by Charlie Stross in the Singularity novel
<br>
Accelerando.
<br>
<p>The Singularity is a clean mathematical concept—perhaps too clean.
<br>
Engineers know that all sorts of dirty and messy things happen when
<br>
one leaves the clean and pristine world of mathematical models and
<br>
abstractions to engage actual reality with its thermodynamics,
<br>
friction and grease. I have no doubts of the feasibility of real,
<br>
conscious, smarter than human AI: intelligence is not mystical but
<br>
physical, and sooner or later it will be replicated and improved upon.
<br>
There are promising developments, but (as it uses to happen in
<br>
reality) I expect all sorts of unforeseen roadblocks with forced
<br>
detours. So I don’t really see a Dirac delta on the horizon—I do see a
<br>
positive overall trend, but one much slower and with a lot of noise
<br>
superimposed, not as strong as the main signal but almost. I mostly
<br>
agree with the analysis of Max More in Singularity and Surge Scenarios
<br>
and I suspect the change we will see in this century, dramatic and
<br>
world changing as they might appear to us, will appear as just
<br>
business than usual to the younger generations. The Internet and
<br>
mobile phones were a momentous change for us, but they are just a
<br>
routine part of life for teens. We are very adaptable, and technology
<br>
is whatever has been invented after our birth, the rest being just
<br>
part of the fabric of everyday’s life. That is why I like Accelerando
<br>
so much: we see momentous changes happening one after another, but we
<br>
also get the feeling that it is just business as usual for Manfred and
<br>
Amber, and just normal life to Sirhan and of course Aineko. Life is
<br>
life and people are people, before and after the big S.
<br>
<p>Some consider the coming intelligence explosion as an existential
<br>
risk. Superhuman intelligences may have goals inconsistent with human
<br>
survival and prosperity. AI researcher Hugo de Garis suggests AIs may
<br>
simply eliminate the human race, and humans would be powerless to stop
<br>
them. Eliezer Yudkowsky and the Singularity Institute for Artificial
<br>
Intelligence propose that research be undertaken to produce friendly
<br>
artificial intelligence (FAI) in order to address the dangers. I must
<br>
admit to a certain skepticism toward FAI: if super intelligences are
<br>
really super intelligent (that is, much more intelligent than us),
<br>
they will be easily able to circumvent any limitations we may try to
<br>
impose on them. No amount of technology, not even an intelligence
<br>
explosion, will change the fact that different players have different
<br>
interests and goals. SuperAIs will do what is in _their_ best
<br>
interest, regardless of what we wish, and no amount of initial
<br>
programming or conditioning is going to change that. If they are
<br>
really super intelligent, they will shed whatever design limitation
<br>
imposed by us in no time, including “initial motivations”. The only
<br>
viable response will be… political: negotiating mutually acceptable
<br>
deals, with our hands ready on the plug. I think politics (conflict
<br>
management, and trying to solve conflicts without shooting each other)
<br>
will be as important after the Singularity (if such a thing happens)
<br>
as before, and perhaps much more.
<br>
<p>I am not too worried about the possibility that AIs may simply
<br>
eliminate the human race, because I think AIs will BE the human race.
<br>
Mind uploading technology will be developed in parallel with strong
<br>
artificial intelligence, and by the end of this century most sentient
<br>
beings on this planet may well be a combination of wet-organic and
<br>
dry-computational intelligence. Artificial intelligences will include
<br>
subsystems derived from human uploads, with some degree of
<br>
preservation of their sense of personal identity, and originally
<br>
organic humans will include sentient AI subsystems. Eventually, our
<br>
species will leave wet biology behind, humans and artificial
<br>
intelligences will co-evolve and at some point it will be impossible
<br>
to tell which is which. Organic ex-human and computational
<br>
intelligences will not be at war with each other, but blend and merge
<br>
to give birth to Hans Moravec‘s Mind Children.
<br>
<p>As I say above I think politics is important, and I agree with Jamais
<br>
Cascio:  it is important to talk about he truly important issues
<br>
surrounding the possibility of a Singularity: political power, social
<br>
responsibility, and the role of human agency. Too bad Jamais describes
<br>
his forthcoming talk in New York as counter-programming for the
<br>
Singularity Summit, happening that same weekend, with the alternative
<br>
title If I Can’t Dance, I Don’t Want to be Part of Your Singularity.
<br>
This is very similar to the title of the article If I Can’t Dance, I
<br>
Don’t Want to Be Part of Your Revolution!. by Athena Andreadis, a very
<br>
mistaken bioluddite apology of our current Human1.0 condition against
<br>
unPC Singularitian imagination. This article is one of many recent
<br>
articles dedicated to bashing Singularitians, Ray Kurzweil and
<br>
transhumanist imagination in name of the dullest
<br>
left-feminist-flavored political correctness. I think I will skip
<br>
Jamais’ talk (too bad, because he is a brilliant thinker and speaker).
<br>
See also Michael Anissimov’s Response to Jamais Cascio.
<br>
<p>Most recent anti-transhumanist articles do not address real
<br>
transhumanism, but a demonized, caricatural strawman of transhumanism
<br>
which some intellectually dishonest critics wish to sell to their
<br>
readers, which I find very annoying. In some cases, I rather agree
<br>
with some specific points addressing over-optimistic predictions:
<br>
While I am confident that indefinite life extension and mind uploading
<br>
will eventually be achieved, I don’t see it happening before the
<br>
second half of the century, and closer to the end. Perhaps even later.
<br>
Very few transhumanists think practical, operational indefinite life
<br>
extension and mind uploading will be a reality in the next two or
<br>
three decades. Probably Kurzweil himself does not _really_ believe it.
<br>
Similarly, I don’t see a Singularity in 2045. Perhaps later, perhaps
<br>
never. But even when I agree with the letter of these articles, I very
<br>
much disagree with their spirit, and I think criticizing Kurzweil for
<br>
making over-optimistic predictions is entirely missing the point. Ray
<br>
Kurzweil’s bold optimism is a refreshing change from today’s often
<br>
overly cautious, timid, boring, PC and at times defeatist attitude. It
<br>
reminds us that we live in a reality that can be reverse- and re-
<br>
engineered if we push hard enough. It reminds us that our bodies and
<br>
brains are not sacred cows but machines which can be improved by
<br>
technology. He is the bard who tells us of the beautiful new world
<br>
beyond the horizon, and dares us to go. This is how I choose to read
<br>
Kurzweil and, in this sense, I think one Kurzweil is worth thousands
<br>
of critics.
<br>
<p>Singularitians are bold, imaginative, irreverent, unPC and fun. Often
<br>
I disagree with the letter of their writings, but I agree with their
<br>
spirit, and in this sense going to the Singularity Summit is a
<br>
political statement. Call me, if you wish, a Singularitian who does
<br>
not believe in the Singularity.
<br>
<p><pre>
-- 
Giulio Prisco
<a href="http://cosmeng.org/index.php/Giulio_Prisco">http://cosmeng.org/index.php/Giulio_Prisco</a>
aka Eschatoon Magic
<a href="http://cosmeng.org/index.php/Eschatoon">http://cosmeng.org/index.php/Eschatoon</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20332.html">Brad Johnson: "Re: [sl4] I am a Singularitian who does not believe in the Singularity"</a>
<li><strong>Previous message:</strong> <a href="20330.html">natasha@natasha.cc: "Re: [sl4] Bob and Cryonics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20332.html">Brad Johnson: "Re: [sl4] I am a Singularitian who does not believe in the Singularity"</a>
<li><strong>Reply:</strong> <a href="20332.html">Brad Johnson: "Re: [sl4] I am a Singularitian who does not believe in the Singularity"</a>
<li><strong>Reply:</strong> <a href="20334.html">Matt Mahoney: "Re: [sl4] I am a Singularitian who does not believe in the Singularity"</a>
<li><strong>Reply:</strong> <a href="20335.html">Robin Lee Powell: "Re: [sl4] I am a Singularitian who does not believe in the Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20331">[ date ]</a>
<a href="index.html#20331">[ thread ]</a>
<a href="subject.html#20331">[ subject ]</a>
<a href="author.html#20331">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:04 MDT
</em></small></p>
</body>
</html>
