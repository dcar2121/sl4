<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ITSSIM (was Some new ideas on Friendly AI)</title>
<meta name="Author" content="David Hart (dhart@atlantisblue.com.au)">
<meta name="Subject" content="Re: ITSSIM (was Some new ideas on Friendly AI)">
<meta name="Date" content="2005-02-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ITSSIM (was Some new ideas on Friendly AI)</h1>
<!-- received="Tue Feb 22 15:58:44 2005" -->
<!-- isoreceived="20050222225844" -->
<!-- sent="Wed, 23 Feb 2005 09:57:40 +1100" -->
<!-- isosent="20050222225740" -->
<!-- name="David Hart" -->
<!-- email="dhart@atlantisblue.com.au" -->
<!-- subject="Re: ITSSIM (was Some new ideas on Friendly AI)" -->
<!-- id="421BB8E4.9040206@atlantisblue.com.au" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="JNEIJCJJHIEAILJBFHILMEBMEAAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> David Hart (<a href="mailto:dhart@atlantisblue.com.au?Subject=Re:%20ITSSIM%20(was%20Some%20new%20ideas%20on%20Friendly%20AI)"><em>dhart@atlantisblue.com.au</em></a>)<br>
<strong>Date:</strong> Tue Feb 22 2005 - 15:57:40 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10983.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>Previous message:</strong> <a href="10981.html">Ben Goertzel: "RE: Minimum complexity of AI?"</a>
<li><strong>In reply to:</strong> <a href="10977.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10983.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="10983.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10982">[ date ]</a>
<a href="index.html#10982">[ thread ]</a>
<a href="subject.html#10982">[ subject ]</a>
<a href="author.html#10982">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:  
<br>
<p><em>&gt; Your last paragraph indicates an obvious philosophical (not logical) 
</em><br>
<em>&gt; weakness of the ITSSIM approach as presented.
</em><br>
<em>&gt;  
</em><br>
<em>&gt; It is oriented toward protecting against danger from the AI itself, 
</em><br>
<em>&gt; rather than other dangers.  Thus, suppose
</em><br>
<em>&gt;  
</em><br>
<em>&gt; -- there's a threat that has a 90% chance of destroying ALL OF THE 
</em><br>
<em>&gt; UNIVERSE with a different universe, except for the AI itself; but will 
</em><br>
<em>&gt; almost certainly leave the AI intact
</em><br>
<em>&gt; -- the AI could avert this attack but in doing so it would make itself 
</em><br>
<em>&gt; slightly less safe (slightly less likely to obey the ITSSIM safety rule)
</em><br>
<em>&gt;  
</em><br>
<em>&gt; Then following the ITSSIM rule, the AI will let the rest of the world 
</em><br>
<em>&gt; get destroyed, because there is no action that it can take without 
</em><br>
<em>&gt; decreasing its amount of safety.
</em><br>
<em>&gt;  
</em><br>
<em>&gt; Unfortunately, I can't think of any clean way to get around this 
</em><br>
<em>&gt; problem -- yet.  Can you?
</em><br>
<p><p>For the sake of a somewhat simple and concrete example, lets assume that 
<br>
ITSSIM is treated as a supergoal, and that the AGI system design in 
<br>
question must [re]evaluate each potential action against ALL supergoals 
<br>
before executing it (where a supergoal is defined arbitrarily by some 
<br>
heuristic as, say, the largest N super-nodes in a scale-free graph of 
<br>
goals, where N might start off as 3).
<br>
<p>Other obvious pre-wired goals might be CV, and an &quot;external existential 
<br>
risk evaluator&quot; or EERE.
<br>
<p>Perhaps variations of  EERE are needed for 'risk to self' and 'risk to 
<br>
environment' (where 'environment' includes humans and all other natural 
<br>
and artificial life), or perhaps EERE can be all-inclusive.
<br>
<p>Or, perhaps ITSSIM would in fact serve the purpose of EERE if ~A also 
<br>
means failing to take a pro-active action that would avert an external 
<br>
risk, meaning that A, although it might be very dangerous, is deemed 
<br>
better than ~A. Such a design would need to actively seek out possible 
<br>
EERs and generate potential actions designed to mitigate them.
<br>
<p>The &quot;multiple-SG as action generator AND governor&quot; architecture begs the 
<br>
question of whether a 'arbiter' SG is needed, or whether SGs would 
<br>
naturally feedback-harmonize given, e.g., that one SG may be churning 
<br>
out potential actions, and another SG may be vetoing them.
<br>
<p>An interesting design/tuning question also follows: should a SG lose 
<br>
long-term-importance for casting a veto, or for generating a potential 
<br>
action that is vetoed, or for both? We wouldn't want an AGI 'going 
<br>
blind' because potential actions generated to avert a very real and 
<br>
perhaps overwhelming EER are constantly vetoed as 'too unsafe', and 
<br>
likewise we wouldn't want an AGI that's overly trigger-happy.
<br>
<p>Designing a single SG like CV with long-term stability is difficult 
<br>
enough; designing (or perhaps 'seeding' is a better term) a complex 
<br>
system of supergoals with long-term stability is even more difficult, 
<br>
however, I suspect that it's closer to the true nature of the problem.
<br>
<p>-dave
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10983.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>Previous message:</strong> <a href="10981.html">Ben Goertzel: "RE: Minimum complexity of AI?"</a>
<li><strong>In reply to:</strong> <a href="10977.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10983.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="10983.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10982">[ date ]</a>
<a href="index.html#10982">[ thread ]</a>
<a href="subject.html#10982">[ subject ]</a>
<a href="author.html#10982">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
