<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ITSSIM (was Some new ideas on Friendly AI)</title>
<meta name="Author" content="David Hart (dhart@atlantisblue.com.au)">
<meta name="Subject" content="Re: ITSSIM (was Some new ideas on Friendly AI)">
<meta name="Date" content="2005-02-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ITSSIM (was Some new ideas on Friendly AI)</h1>
<!-- received="Wed Feb 23 01:09:34 2005" -->
<!-- isoreceived="20050223080934" -->
<!-- sent="Wed, 23 Feb 2005 19:08:43 +1100" -->
<!-- isosent="20050223080843" -->
<!-- name="David Hart" -->
<!-- email="dhart@atlantisblue.com.au" -->
<!-- subject="Re: ITSSIM (was Some new ideas on Friendly AI)" -->
<!-- id="421C3A0B.3030807@atlantisblue.com.au" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="JNEIJCJJHIEAILJBFHILIEBNEAAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> David Hart (<a href="mailto:dhart@atlantisblue.com.au?Subject=Re:%20ITSSIM%20(was%20Some%20new%20ideas%20on%20Friendly%20AI)"><em>dhart@atlantisblue.com.au</em></a>)<br>
<strong>Date:</strong> Wed Feb 23 2005 - 01:08:43 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10997.html">Russell Wallace: "Re: AGI Prototying Project"</a>
<li><strong>Previous message:</strong> <a href="10995.html">Tennessee Leeuwenburg: "Re: AGI Prototying Project"</a>
<li><strong>In reply to:</strong> <a href="10978.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10982.html">David Hart: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10996">[ date ]</a>
<a href="index.html#10996">[ thread ]</a>
<a href="subject.html#10996">[ subject ]</a>
<a href="author.html#10996">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<p><em>&gt; Maybe one could build in a &quot;grandfather clause&quot; stating that an AI can 
</em><br>
<em>&gt; optionally violate the safety rule IF it can prove that, when given 
</em><br>
<em>&gt; the same data about the world and a very long time to study it, its 
</em><br>
<em>&gt; seed AI ancestor would have decided to violate the safety rule. 
</em><br>
<p><p>Hi Ben,
<br>
<p>I like this idea; it seems that it would need to be a continual process 
<br>
if the AI is to have an acceptable reaction time to existential risks.
<br>
<p>A grandfather process could be part of a general set of conditions that 
<br>
allows for [partial] override of safety rules in cases of extreme 
<br>
existential danger, averting the case where ITSSIM allows the AI's 
<br>
environment (our Universe) to be destroyed by a 'safe' failure to act 
<br>
(isn't this similar to the classic Asmovian robot deadlock?).
<br>
<p>E.g. in the concept of an ensemble of supergoals, ITSSIM would be one of 
<br>
many [weighted] supergoals, while an 'existential risk detector', with a 
<br>
'grandfather clause' as a special case, could be another supergoal.
<br>
<p>In a Novamente context, such an ensemble would be a [dynamic scale-free] 
<br>
network of goal nodes, and could therefore be tuned for acceptable 
<br>
margins of safety (we live with evolution-tuned SFNs all around us, 
<br>
inside of us, etc.), e.g. a goal graph could be tuned for the emergence 
<br>
of 1 mega-SG, or 10 equal sized SGs, or 3 large SGs each with 3 sub-SGs, 
<br>
or 2 large SGs and one slightly smaller SG, etc.
<br>
<p>The ITSSIM SG would probably govern the tuning parameters, as any 
<br>
change, even a necessarily small and 'incremental' change, would 
<br>
constitute a significant self-modifying action.
<br>
<p>Any SG architecture could be validated, invalidated and/or optimized 
<br>
using experimentation with different parameters in simulation. Such 
<br>
experimentation might find, e.g,  that 1 dominant-SG is dangerous 
<br>
because of the &gt;0 probability of the paperclip problem, or that 10 equal 
<br>
sized SGs lead to system deadlock or always condense into fewer SGs on 
<br>
their own, or that a system without a 'grandfather clause' embodied in a 
<br>
SG would grow faster but might also fail to avert a large existential risk.
<br>
<p>Lessons learned from simulation could lead to an adaptive SG 
<br>
architecture, e.g. allowing a new temporary SG to emerge for a specific 
<br>
existential risk mitigation (only if/when a large existential risk is 
<br>
discovered), later reverting back to the 'safer' state (providing a 
<br>
function similar to Elias's stack frame).
<br>
<p>One can imagine that perhaps SGs would condense into a normal stable 
<br>
state something like:
<br>
<p>&nbsp;&nbsp;&nbsp;1. Joy, Growth, Choice + CV (boils down to growth moderated by
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;individual freedom)
<br>
&nbsp;&nbsp;&nbsp;2. ITSSIM (safety governor)
<br>
&nbsp;&nbsp;&nbsp;3. existential risk detector + mitigation action generator (including
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grandfather clause)
<br>
<p>To tie this back to friendliness, in my current understanding, 
<br>
Friendliness is equivalent to a maximally-stable complex system of SGs 
<br>
and subgoals seeded with the best human values. In other words, 
<br>
Friendliness is bound, in that it can exist only within a known complex 
<br>
system, &quot;The Universe&quot;, and therefore to be truly friendly and as 
<br>
invariant as possible, should be a set of [asymptotically stable] 
<br>
attractors, if such a thing can exist on such a scale, or our best shot 
<br>
at a set of non-asymptotically stable attractors.
<br>
<p>-dave
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10997.html">Russell Wallace: "Re: AGI Prototying Project"</a>
<li><strong>Previous message:</strong> <a href="10995.html">Tennessee Leeuwenburg: "Re: AGI Prototying Project"</a>
<li><strong>In reply to:</strong> <a href="10978.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10982.html">David Hart: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10996">[ date ]</a>
<a href="index.html#10996">[ thread ]</a>
<a href="subject.html#10996">[ subject ]</a>
<a href="author.html#10996">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
