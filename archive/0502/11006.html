<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Does Friendliness Structure constrain Content and visa versa? [was: RE: AGI Prototying Project]</title>
<meta name="Author" content="Marc Geddes (marc_geddes@yahoo.co.nz)">
<meta name="Subject" content="Does Friendliness Structure constrain Content and visa versa? [was: RE: AGI Prototying Project]">
<meta name="Date" content="2005-02-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Does Friendliness Structure constrain Content and visa versa? [was: RE: AGI Prototying Project]</h1>
<!-- received="Wed Feb 23 20:46:14 2005" -->
<!-- isoreceived="20050224034614" -->
<!-- sent="Thu, 24 Feb 2005 16:45:52 +1300 (NZDT)" -->
<!-- isosent="20050224034552" -->
<!-- name="Marc Geddes" -->
<!-- email="marc_geddes@yahoo.co.nz" -->
<!-- subject="Does Friendliness Structure constrain Content and visa versa? [was: RE: AGI Prototying Project]" -->
<!-- id="20050224034552.9761.qmail@web20221.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marc Geddes (<a href="mailto:marc_geddes@yahoo.co.nz?Subject=Re:%20Does%20Friendliness%20Structure%20constrain%20Content%20and%20visa%20versa?%20[was:%20RE:%20AGI%20Prototying%20Project]"><em>marc_geddes@yahoo.co.nz</em></a>)<br>
<strong>Date:</strong> Wed Feb 23 2005 - 20:45:52 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11007.html">Tennessee Leeuwenburg: "Re: AGI Prototying Project"</a>
<li><strong>Previous message:</strong> <a href="11005.html">Peter de Blanc: "Re: AGI Prototying Project"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11006">[ date ]</a>
<a href="index.html#11006">[ thread ]</a>
<a href="subject.html#11006">[ subject ]</a>
<a href="author.html#11006">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt;Needless to say these 
</em><br>
<em>&gt;should be alternative ways of using a seed AI to work
</em><br>
<em>&gt;out what the 
</em><br>
<em>&gt;best environment for humanity's future to unfold in
</em><br>
<em>&gt;is, not personal 
</em><br>
<em>&gt;guesses on the Four Great Moral Principles For All
</em><br>
<em>&gt;Time; any mechanism 
</em><br>
<em>&gt;choosen should be one that (rational) people who
</em><br>
<em>&gt;believe in Great 
</em><br>
<em>&gt;Principles could look at and say 'yeah, that's bound
</em><br>
<em>&gt;to either produce 
</em><br>
<em>&gt;my four great principles, or fail harmlessly' (e.g.
</em><br>
<em>&gt;if Geddes was right, 
</em><br>
<em>&gt;superintelligent CV extrapolations would find the
</em><br>
<em>&gt;objective morality). 
</em><br>
<p>Since I'm a believer in Universal Volition, I
<br>
certainly think that Collective Volition is a valid
<br>
concept.  Indeed as you point out, extrapolating CV
<br>
should pick up my Universal Morality, although this
<br>
would be a hugely inefficient and un necessary way of
<br>
getting to it.
<br>
<p>The reason I think CV can't be calculated by a
<br>
Singleton though, is computational intractability and
<br>
qualia.  The more and more info is coming into the
<br>
system, the faster the combinatorial explosion.  The
<br>
only way to avoid intractability would be to apply
<br>
more and more ingenuity and deviate more and more from
<br>
the Bayesian ideal to get clever approximations and
<br>
short-cuts.    But what if this 'ingenuity' and
<br>
'deviation from the Bayesian ideal' is exactly what
<br>
gives rise to qualia?  In that case, RPOP couldn't
<br>
escape intractability without becoming a volitional
<br>
entity.  That would screw everything up, since RPOP
<br>
would have to include its own volition in the
<br>
calculation of CV in order to make predictions,
<br>
causing a final infinite regress.  To sum up:  CV
<br>
isn’t ever gonna work man!
<br>
<p>I do think I know what the 4 great moral principles
<br>
are:   I can even tell you their order of importance. 
<br>
Ranked starting with most important value first here
<br>
they are:
<br>
<p>1  Growth
<br>
2  Altruism
<br>
<p>3  Happiness
<br>
4  Health
<br>
<p>The combination of values 1-2 is equivalent to
<br>
Volition (Friendliness structure).  The combination of
<br>
values 3-4 is equivalent to Eudaimonia (Friendliness
<br>
content).  I shall now state my fundamental theorem of
<br>
morality (which I'll try to briefly explain shortly):
<br>
<p>Morality = Eudaimonia x Volition
<br>
<p>I busted my balls for 2 years to finally get to this. 
<br>
These values are not something I just pulled out of my
<br>
arse.  If you check out my rough 8-level schematic of
<br>
intelligence, you'll see that I think these values
<br>
emerge in a natural way from the operations and
<br>
interactions of the various levels of intelligence. 
<br>
So I think that Universal Values are 'emergent' from
<br>
general abstract properties that all Sentient minds
<br>
have in common.
<br>
<p>Of course all I've managed to do so far is come up
<br>
with intuitive arguments and state a plausible
<br>
intuitive conjecture about morality.  I haven't
<br>
actually proved anything yet.  I had to bust my balls
<br>
for two years just to manage to state the conjecture
<br>
;)
<br>
<p><p><em>&gt;Bzzzt, wrong. This is the mistake Eliezer made in
</em><br>
<em>&gt;1996, and didn't 
</em><br>
<em>&gt;snap out of until 2001. Intelligence is power; it
</em><br>
<em>&gt;forces goal 
</em><br>
<em>&gt;systems to reach a stable state faster, but it does
</em><br>
<em>&gt;nothing to 
</em><br>
<em>&gt;constrain the space of stable goal systems. It may
</em><br>
<em>&gt;cause systems 
</em><br>
<em>&gt;to take more moral seeming actions under some
</em><br>
<em>&gt;circumstances due 
</em><br>
<em>&gt;to a better understanding of game theory, but this
</em><br>
<em>&gt;has nothing 
</em><br>
<em>&gt;to do with their actual preferences. 
</em><br>
<p>Actually I think the Objective Morality idea is
<br>
correct, although making it work is probably rather
<br>
more subtle than a naive attempt to equate facts with
<br>
values.
<br>
<p>I'm inclined I think that values and morals are coming
<br>
from higher levels of intelligence above ordinary
<br>
reasoning.  If I'm right the relationship between
<br>
facts and values is analogous to the relationship
<br>
between the mind and the brain.  The mind is not the
<br>
brain, but the mind is totally *dependent on* (caused
<br>
by) the processes taking place in the brain. 
<br>
Similarly, although morals/values are not the same
<br>
thing as facts, they are totally *dependent on* facts.
<br>
&nbsp;So objective morality could be fully *described by*
<br>
(or decomposed) into clusters of facts.
<br>
<p>If you think of the mind as analogues to a sort of
<br>
mini 'Collective Volition' (the 'society of mind' aka
<br>
Minsky so to speak) then it does seem very plausible
<br>
that morality should tend to be correlated with
<br>
intelligence.   According to the CV idea goodness
<br>
coheres and hate washes out on the social level.  Why
<br>
should it be any different on the individual level? 
<br>
The more rationality is being pumped into the goal
<br>
system, the more different it should be to be evil and
<br>
still have a stable goal system.  Good tends to
<br>
reinforce the stability of the goal system; evil is
<br>
tending to rip it apart.  So it would seem that an
<br>
evil super-intelligence would be very prone to mental
<br>
instability.  
<br>
<p>There is also a suggestive analogy here with the
<br>
coherence theory of truth - with inconsistency being
<br>
analogous with evil.  Adding on more and more axioms
<br>
to a system of logic makes it harder and harder to add
<br>
falsehoods and still maintain a consistent system. 
<br>
Similarly adding on more and more intelligence should
<br>
make it harder and harder for the goal system to do
<br>
evil things and still remain stable.
<br>
<p>In my view these analogies are strongly suggestive of
<br>
objective morality.
<br>
<p>In terms of 'Structure' versus 'Content' I have to ask
<br>
whether these are really two separate things when it
<br>
comes the mind?  Of course ordinarily, we think of
<br>
Structure and Content as two separate things, but
<br>
perhaps the mind is a very peculiar type of function
<br>
in which Structure and Content constrain each other? 
<br>
I'm definitely inclined to think that in the case of
<br>
morality (Friendliness) Content determines Structure
<br>
and Structure constrains Content.  If I'm right then
<br>
solving the problem of Friendliness Structure should
<br>
also automatically give you the solution to Content
<br>
and visa versa.   Indeed, I think this is the exactly
<br>
the condition required for my Universal Morality to
<br>
exist.           
<br>
<p>To sum up the Mind, we could say that 'Reason' is the
<br>
Content of the Mind, and 'Morality' is the structure
<br>
of the Mind.  So Morality is a function which operates
<br>
on input 'Reason' (a system of axioms and rules of
<br>
logic) thusly; 
<br>
<p>Mind = function Morality (Reason).
<br>
<p>Now here is where I want to suggest my big trick. 
<br>
Watch.  I say that;
<br>
<p>Mind = Morality x Reason
<br>
<p>At first this seems to be gibberish.  I'm trying to
<br>
multiply a function (Morality) with its input data (a
<br>
system of Reason) and saying its equivalent to the
<br>
Mind.    But I do suspect that this is precisely the
<br>
condition required for Structure to constrain Content
<br>
and visa versa.  The system of Morality locks down the
<br>
system of Reasoning and visa versa.  Thus Objective
<br>
Morality.
<br>
<p>Let's go to Friendliness.  Refer to my 8-level model
<br>
of intelligence on the wiki again:
<br>
<p><a href="http://www.sl4.org/wiki/TheWay">http://www.sl4.org/wiki/TheWay</a>
<br>
<p>The Content of Friendliness can be equated with what I
<br>
called 'Eudaimonia' (a system of positive values). 
<br>
The Structure of Friendliness can be equated with what
<br>
I called 'Volition'.  So:
<br>
<p>Friendliness = function Volition (Eudaimonia)
<br>
<p>Friendliness Structure is a function (Volition)
<br>
describing how Friendliness Content - a system of
<br>
values (Eudaimonia) evolves over time.  Again, let me
<br>
now suggest my big trick:
<br>
<p>Friendliness = Volition x Eudaimonia !!!!
<br>
<p>The Structure of Friendliness can be multiplied by the
<br>
Content and the result is equivalent to Friendliness. 
<br>
I'm hoping that this condition causes Content to
<br>
constrain Structure and Structure to constrain
<br>
Content.  If I'm right the Structure 'locks down' the
<br>
Content and visa versa.  Hopefully some sort of
<br>
Universal Morality has to emerge from this.
<br>
<p>All this is a lot of big 'ifs' and intuitive
<br>
hand-waving of course.
<br>
<p><p><p><p>=====
<br>
<p><p>Find local movie times and trailers on Yahoo! Movies.
<br>
<a href="http://au.movies.yahoo.com">http://au.movies.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11007.html">Tennessee Leeuwenburg: "Re: AGI Prototying Project"</a>
<li><strong>Previous message:</strong> <a href="11005.html">Peter de Blanc: "Re: AGI Prototying Project"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11006">[ date ]</a>
<a href="index.html#11006">[ thread ]</a>
<a href="subject.html#11006">[ subject ]</a>
<a href="author.html#11006">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
