<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: ITSSIM (was Some new ideas on Friendly AI)</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: ITSSIM (was Some new ideas on Friendly AI)">
<meta name="Date" content="2005-02-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: ITSSIM (was Some new ideas on Friendly AI)</h1>
<!-- received="Tue Feb 22 16:53:36 2005" -->
<!-- isoreceived="20050222235336" -->
<!-- sent="Tue, 22 Feb 2005 18:53:11 -0500" -->
<!-- isosent="20050222235311" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: ITSSIM (was Some new ideas on Friendly AI)" -->
<!-- id="JNEIJCJJHIEAILJBFHILOECGEAAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="421BB8E4.9040206@atlantisblue.com.au" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20ITSSIM%20(was%20Some%20new%20ideas%20on%20Friendly%20AI)"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Tue Feb 22 2005 - 16:53:11 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10984.html">Tennessee Leeuwenburg: "Re: AGI Prototying Project"</a>
<li><strong>Previous message:</strong> <a href="10982.html">David Hart: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>In reply to:</strong> <a href="10982.html">David Hart: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10986.html">Elias Sinderson: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="10986.html">Elias Sinderson: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10983">[ date ]</a>
<a href="index.html#10983">[ thread ]</a>
<a href="subject.html#10983">[ subject ]</a>
<a href="author.html#10983">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi,
<br>
<p>ITSSIM would cause an AI to avert only existential risks that either
<br>
<p>-- it could avert without compromising its principle of safe action, or
<br>
<p>-- it judged would be likely to mangle ITSELF, therefore potentially
<br>
threatening its ability to act safely
<br>
<p>I don't think that ITSSIM is an ideal approach by any means, in fact it's
<br>
over-conservative for my taste.  I'd rather have an AI be able to deviate
<br>
from its safe-self-modification rule in order to save the universe from
<br>
other threats; the question is how to allow this without opening the door to
<br>
all sorts of distortions and delusions...
<br>
<p>-- Ben
<br>
&nbsp;&nbsp;-----Original Message-----
<br>
&nbsp;&nbsp;From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20ITSSIM%20(was%20Some%20new%20ideas%20on%20Friendly%20AI)">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20ITSSIM%20(was%20Some%20new%20ideas%20on%20Friendly%20AI)">owner-sl4@sl4.org</a>]On Behalf Of David Hart
<br>
&nbsp;&nbsp;Sent: Tuesday, February 22, 2005 5:58 PM
<br>
&nbsp;&nbsp;To: <a href="mailto:sl4@sl4.org?Subject=RE:%20ITSSIM%20(was%20Some%20new%20ideas%20on%20Friendly%20AI)">sl4@sl4.org</a>
<br>
&nbsp;&nbsp;Subject: Re: ITSSIM (was Some new ideas on Friendly AI)
<br>
<p><p>&nbsp;&nbsp;Ben Goertzel wrote:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;Your last paragraph indicates an obvious philosophical (not logical)
<br>
weakness of the ITSSIM approach as presented.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;It is oriented toward protecting against danger from the AI itself,
<br>
rather than other dangers.  Thus, suppose
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;-- there's a threat that has a 90% chance of destroying ALL OF THE
<br>
UNIVERSE with a different universe, except for the AI itself; but will
<br>
almost certainly leave the AI intact
<br>
&nbsp;&nbsp;&nbsp;&nbsp;-- the AI could avert this attack but in doing so it would make itself
<br>
slightly less safe (slightly less likely to obey the ITSSIM safety rule)
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;Then following the ITSSIM rule, the AI will let the rest of the world
<br>
get destroyed, because there is no action that it can take without
<br>
decreasing its amount of safety.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;Unfortunately, I can't think of any clean way to get around this
<br>
problem -- yet.  Can you?
<br>
<p>&nbsp;&nbsp;For the sake of a somewhat simple and concrete example, lets assume that
<br>
ITSSIM is treated as a supergoal, and that the AGI system design in question
<br>
must [re]evaluate each potential action against ALL supergoals before
<br>
executing it (where a supergoal is defined arbitrarily by some heuristic as,
<br>
say, the largest N super-nodes in a scale-free graph of goals, where N might
<br>
start off as 3).
<br>
<p>&nbsp;&nbsp;Other obvious pre-wired goals might be CV, and an &quot;external existential
<br>
risk evaluator&quot; or EERE.
<br>
<p>&nbsp;&nbsp;Perhaps variations of  EERE are needed for 'risk to self' and 'risk to
<br>
environment' (where 'environment' includes humans and all other natural and
<br>
artificial life), or perhaps EERE can be all-inclusive.
<br>
<p>&nbsp;&nbsp;Or, perhaps ITSSIM would in fact serve the purpose of EERE if ~A also
<br>
means failing to take a pro-active action that would avert an external risk,
<br>
meaning that A, although it might be very dangerous, is deemed better than
<br>
~A. Such a design would need to actively seek out possible EERs and generate
<br>
potential actions designed to mitigate them.
<br>
<p>&nbsp;&nbsp;The &quot;multiple-SG as action generator AND governor&quot; architecture begs the
<br>
question of whether a 'arbiter' SG is needed, or whether SGs would naturally
<br>
feedback-harmonize given, e.g., that one SG may be churning out potential
<br>
actions, and another SG may be vetoing them.
<br>
<p>&nbsp;&nbsp;An interesting design/tuning question also follows: should a SG lose
<br>
long-term-importance for casting a veto, or for generating a potential
<br>
action that is vetoed, or for both? We wouldn't want an AGI 'going blind'
<br>
because potential actions generated to avert a very real and perhaps
<br>
overwhelming EER are constantly vetoed as 'too unsafe', and likewise we
<br>
wouldn't want an AGI that's overly trigger-happy.
<br>
<p>&nbsp;&nbsp;Designing a single SG like CV with long-term stability is difficult
<br>
enough; designing (or perhaps 'seeding' is a better term) a complex system
<br>
of supergoals with long-term stability is even more difficult, however, I
<br>
suspect that it's closer to the true nature of the problem.
<br>
<p>&nbsp;&nbsp;-dave
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10984.html">Tennessee Leeuwenburg: "Re: AGI Prototying Project"</a>
<li><strong>Previous message:</strong> <a href="10982.html">David Hart: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>In reply to:</strong> <a href="10982.html">David Hart: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10986.html">Elias Sinderson: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="10986.html">Elias Sinderson: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10983">[ date ]</a>
<a href="index.html#10983">[ thread ]</a>
<a href="subject.html#10983">[ subject ]</a>
<a href="author.html#10983">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
