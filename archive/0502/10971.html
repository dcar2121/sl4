<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: AGI Prototying Project</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="RE: AGI Prototying Project">
<meta name="Date" content="2005-02-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: AGI Prototying Project</h1>
<!-- received="Tue Feb 22 04:14:37 2005" -->
<!-- isoreceived="20050222111437" -->
<!-- sent="Tue, 22 Feb 2005 11:14:14 +0000 (GMT)" -->
<!-- isosent="20050222111414" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="RE: AGI Prototying Project" -->
<!-- id="20050222111414.20476.qmail@web25307.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="JNEIJCJJHIEAILJBFHILKEMEDPAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=RE:%20AGI%20Prototying%20Project"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Tue Feb 22 2005 - 04:14:14 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10972.html">Thomas Buckner: "Re: intellectual property (Re: Totalitarian Assumptions in I, Robot)"</a>
<li><strong>Previous message:</strong> <a href="10970.html">Russell Wallace: "Re: AGI Prototying Project"</a>
<li><strong>In reply to:</strong> <a href="10939.html">Ben Goertzel: "RE: AGI Prototying Project"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10973.html">Michael Wilson: "RE: AGI Prototying Project"</a>
<li><strong>Reply:</strong> <a href="10973.html">Michael Wilson: "RE: AGI Prototying Project"</a>
<li><strong>Reply:</strong> <a href="10980.html">sam kayley: "Re: AGI Prototying Project"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10971">[ date ]</a>
<a href="index.html#10971">[ thread ]</a>
<a href="subject.html#10971">[ subject ]</a>
<a href="author.html#10971">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote: 
<br>
<em>&gt; Ah, I see ... so in your list of qualifications you're including
</em><br>
<em>&gt; &quot;shares the SIAI belief system&quot; ;-)
</em><br>
<p>The key parts we disagree on are (a) how much you can do with theory
<br>
and how much you can do with experimentation, and (b) the probability
<br>
of hard takeoff. I know you'd try your best to be certain you've got
<br>
it right before firing up a seed AI; I'd be concerned that you'd
<br>
declare success too easily (particularly scaling up early experimental
<br>
results past a qualitiative change that invalidates their predictions),
<br>
but that's a relatively minor point. The main problem is that I don't
<br>
think you'll see takeoff sneaking up on you while you're still in the
<br>
'information gathering' phase. It seems unlikely to me that Novamente
<br>
will be get that far in its current form, but that's not a difference
<br>
in philosophy, just a difference of technical opinion. I'm actually
<br>
rather closer to your position than Eliezer is, as I do want to build
<br>
prototypes rather than medidate for a decade or two before inscribing
<br>
the One True FAI Design on a gilded scroll, /and/ I want to commercialise
<br>
proto-AGI code if possible. However my approach differs significently
<br>
from yours, partly in that it's focused on confirming or disproving
<br>
existing hypotheses rather than gathering raw data, and partly in that
<br>
I don't think building a complete system containing all the key modules
<br>
is a good idea without a solution to the goal system stability problem.
<br>
<p>I wouldn't count agreeing with Eliezer's current theories, structure
<br>
or content, as a requirement for working on FAI. However the basic
<br>
attitude and techniques he's using to attack the abstract invariant
<br>
problem are the only approach to the problem (of goal system
<br>
stability under reflection, that I've seen) likely to produce a
<br>
valid answer.
<br>
<p><em>&gt; I don't think the concept of &quot;collective volition&quot; has been
</em><br>
<em>&gt; clearly defined at all...  I haven't read any convincing argument
</em><br>
<em>&gt; that this convergence and coherence can be made to exist.
</em><br>
<p>This is question #9 on the CV FAQ, but I agree that the explanation
<br>
isn't terribly compelling. The idea that as we get smarter we would
<br>
agree on more things has an intuitive appeal, as does the idea that
<br>
all the love in the world adds up while the hate cancels out, but
<br>
these terms are so vague that they don't do much to constrain the
<br>
space of possible implementations. Clearly for questions such as
<br>
'should the killer asteroid hurtling towards earth be diverted' the
<br>
CV of humanity would cohere to better than 99.99% for any plausible
<br>
method of measurement. As far as I can see, to produce coherence on
<br>
the tricky moral problems the CV is going to have to simulate people
<br>
gaining knowledge of how they work, which is to say directly changing
<br>
the simulation's beliefs on why they have beliefs, without consent
<br>
(they would resist these truthes if simply told them in real life).
<br>
Furthermore I suspect it would be necessary to introduce some basic
<br>
consistency requirements such as choice transitivity, by the least
<br>
invasive means possible (e.g. improved reflection and the requirement
<br>
for a complete preference creation reasoning structure to be
<br>
reflectively approved and submitted prior to volition acceptance),
<br>
at the start of the extrapolation process. Finally social interaction
<br>
modelling should be patched with checks (by simulation state
<br>
comparison) that people really understand the other's position when
<br>
communicating; CV may enable (consensual) mechanical telepathy early
<br>
on anyway, but the bare minimum requirement we should start with is
<br>
provision of an impartial reliable warning of when misunderstanding
<br>
occurs.
<br>
<p>That said, it seems highly unlikely to me that we can prove that 
<br>
coherence and convergence will exist without actually running the CV.
<br>
Even if we had the knowledge and tools to prove it for homo sapiens,
<br>
we have no idea what our medium and long distance extrapolations will
<br>
look like. CV is a theory of Friendliness /content/, and a conservative
<br>
one; it will get it right or just fail to do anything. I am actually
<br>
highly concerned that Eliezer seems to be conflating content and
<br>
structure by trying to dispense with the wrapper RPOP and use a single
<br>
mechanism for the extrapolation and the control process, but I may
<br>
simply be misunderstanding where he's going right now. Anyway, the
<br>
point is that CV may fail (safely), and as I said on the comments page
<br>
we should have alternative FAI content theories ready ahead of time to
<br>
avoid any possibility of picking and choosing mechanisms in order to
<br>
get a result closer to a personal moral ideal. Needless to say these
<br>
should be alternative ways of using a seed AI to work out what the
<br>
best environment for humanity's future to unfold in is, not personal
<br>
guesses on the Four Great Moral Principles For All Time; any mechanism
<br>
choosen should be one that (rational) people who believe in Great
<br>
Principles could look at and say 'yeah, that's bound to either produce
<br>
my four great principles, or fail harmlessly' (e.g. if Geddes was right,
<br>
superintelligent CV extrapolations would find the objective morality).
<br>
&nbsp;
<br>
<em>&gt;&gt; If you converted your (choice, growth and joy) philosophy into a
</em><br>
<em>&gt;&gt; provably stable goal system I'd personally consider it a valid
</em><br>
<em>&gt;&gt; FAI theory,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It's the &quot;provable&quot; part that's tough!  One can develop it into an
</em><br>
<em>&gt; &quot;intuitively seems like it should be stable&quot; goal system.
</em><br>
<p>Humans don't come equipped with any intuitive heuristics calibrated
<br>
for the 'seed AI goal system stability' domain. It's a problem utterly
<br>
alien to anything we'd normally deal with, and indeed superficial
<br>
resemblences to psychology actually give us negatively useful
<br>
intuitions by default. You may or may not have had enough experience
<br>
of proving things about simple AIs to have built up some intuition
<br>
about how goal system stability works, but the experience /always/
<br>
has to come first. I think we both agree on this, it's just that
<br>
you want the experience to come from practical experiments, whereas
<br>
the SIAI believes the experience comes from formally proving
<br>
progressively more general cases (demonstrating that an idealised
<br>
carterian AIXI is Unfriendly is one of the earliest problems in
<br>
this progression).
<br>
&nbsp;
<br>
<em>&gt;&gt; though I'd still prefer CV because I don't trust any one human to
</em><br>
<em>&gt;&gt; come up with universal moral principles.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But do you
</em><br>
<em>&gt; 
</em><br>
<em>&gt; a) trust a human (Eli) to come up with an algorithm for generating moral
</em><br>
<em>&gt; principles (CV)
</em><br>
<p>I'd rather not, but right now that's the best alternative. The thing
<br>
about algorithms is that they are easier to agree on than morals; when
<br>
Eliezer finally publishes a constructive proposal for CV with
<br>
well-defined terms and processes we can look at it and construct
<br>
logical chains along the likes of 'if you accept axioms A, B and C,
<br>
then this algorithm must produce results X, Y and Z'. We may still
<br>
disagree over valid assumptions, but we will be able to agree on more
<br>
than whether some set of universal principles sounds like a good idea.
<br>
<p>Incidentally I would damn well hope that the people who do believe they
<br>
have a great set of universal principles would run some sort of CVlike
<br>
simulation first, to get some sort of confirmation that they're not
<br>
being hopelessly shortsighted. Extrapolating a personal volition would
<br>
probably work even better, but that requires all the machinery of CV to
<br>
safely work out what you would think without just producing a sentient
<br>
upload of yourself with the associated risk of taking over the world.
<br>
&nbsp;
<br>
<em>&gt; b) trust the SIAI inner-circle to perform a &quot;last judgment&quot; on
</em><br>
<em>&gt; whether the results of CV make sense or not before letting an FAI
</em><br>
<em>&gt; impose them on the universe..
</em><br>
<p>Trusting people with a veto is a lot safer than trusting them with a
<br>
mandate to dictate moral principles. Giving more people a veto increases
<br>
the probability of recognising extrapolation failures at the cost of
<br>
increasing the probability that we fail to do something good because
<br>
of personal distaste or cold feet. Our proposed FAI team will be
<br>
thoroughly trained in EvPsych and self-control, accustomed to the
<br>
incomprehensible and frighteningly bizarre and at least altrustic
<br>
enough to dedicate their lives to the greater good. Even still, I'd
<br>
rather we extrapolated the personal volitions of the programmers and
<br>
delegated the veto to those. This satisfies my requirements for
<br>
moral responsibility and reduces the risk to that of an extrapolation
<br>
flaw that also causes every one of the FAI team extrapolations to 
<br>
mistakeningly generate a 'yes' answer instead of a 'no' answer.
<br>
Eliezer's proposal of a single independent Last Judge seems reasonable
<br>
to deal with this lesser risk.
<br>
<p><em>&gt; Well, an obvious problem, pointed out many times on this list,
</em><br>
<em>&gt; is that most people in the world are religious. For a religious
</em><br>
<em>&gt; person, the ideal &quot;person they would want to be&quot; would be someone
</em><br>
<em>&gt; who more closely adhered to the beliefs of their religious faith...
</em><br>
<p>Would this be an issue in global direct democracy? Round one,
<br>
everyone votes to force their personal world view on everyone
<br>
else, no coherence, nothing happens. Round two, all the religious
<br>
people accept that permitting the heathens to remain heathens for
<br>
now is better than risking being personally forced to become a
<br>
heathen, thus we get a minimalist compromise. Of course for this
<br>
to hold under extrapolation we're assuming that secularism has a
<br>
better chance of spreading to all humanity than any one religion;
<br>
I'm not so sure about this, as there might be religious meme sets
<br>
perfectly designed to exploit human reasoning weaknesses that
<br>
would in fact eventually gain an absolute majority.
<br>
<p>As I understand it, CV proposes to force the extrapolations to see
<br>
and accept the real reasons why they believe what they do, by
<br>
adding the relevant reflective statements to the simulated mind.
<br>
If people realise that they are living in a restrictive fantasy
<br>
world and still want to stay there, so be it, though forcing
<br>
everyone else to live in the same fantasy is unlikely to cohere
<br>
due to the radical. This operation switches cognitive dissonance
<br>
from supporting religion to opposing it, removes the ignorance
<br>
that supports fear of the unknown and forces a consistent
<br>
justification of any preferences the individual would like to
<br>
see included in CV ('because I want it' is consistent', 'because
<br>
God said so' isn't, but only accepting volitions truely supported
<br>
by the former reasoning removes a vast amount of junk). I don't
<br>
have a proof, and until Eliezer outputs something better defined
<br>
I can't even give you a complete argument from axioms, but I
<br>
suspect that the combined power of these mechanisms will eliminate
<br>
a huge amount of unwanted irrational baggage (religious
<br>
proclemations of social morality included) even without any
<br>
explicit tweaking of the extrapolation process.
<br>
<p>Tennessee Leeuwenburg wrote:
<br>
<em>&gt; It's a question of whether you see moral complexity diverging or
</em><br>
<em>&gt; converging over time, and whether you see the possibility of moral
</em><br>
<em>&gt; rules themselves being relative to intelligence, or time-frame etc
</em><br>
<em>&gt; in the same way that human morality is largely relative to culture.
</em><br>
<p>A major question in CV that should've been answered on the first
<br>
draft is whether the extrapolations include the knowledge that they
<br>
are extrapolations for the purpose of producing CV. This would give
<br>
the CV version of humanity a reason to stay together and look for
<br>
mutual understanding (or at least pay attention to what other people
<br>
believe) beyond the reasons we currently have. Eliezer seems to think
<br>
that under renormalisation we'd all want to 'stick together' anyway,
<br>
but as far as I can tell this is a wild guess. Right now the most
<br>
sensible course of action would appear to be running the CV without
<br>
this knowledge included in the extrapolations, and then rerunning
<br>
with its inclusion if coherence doesn't appear.
<br>
<p><em>&gt; Moreover, he pushes the problem to &quot;what we would want if we were
</em><br>
<em>&gt; indefinably better&quot;. It's the indefinable nature of that betterness
</em><br>
<em>&gt; which clouds the philosophy.
</em><br>
<p>It's not indefineable; it's what we would be expected to do if we
<br>
had direct access to our own mind and personality and the knowledge
<br>
to reliably modify ourselves into what we'd like to be. The serious
<br>
questions are how high to set the initial competence bar for
<br>
self-modification (or in CV terms, the personal consistency and
<br>
knowledge bars for progressively more direct and powerful forms of
<br>
self-modification) and how much truth to directly inject (regardless
<br>
of personal volition to learn) at the start of the process.
<br>
<p><em>&gt; Or, to express it another way, what is the difference between the
</em><br>
<em>&gt; defined collective volition, and the volition of AGI?
</em><br>
<p>As I previously understood it, the RPOP doing the simulation should
<br>
not be a volitional entity; it should merely have the goal of finding
<br>
out what the result of the extrapolation process we define as
<br>
accurately as possible without creating any new sentients in the
<br>
process. Again I'd appreciate a confirmation of that given the talk
<br>
of abandoning wrappers.
<br>
<p><em>&gt; Believing that friendliness is possible is like believing that
</em><br>
<em>&gt; there is an invariant nature to human morality - an arguable,
</em><br>
<em>&gt; but reasonably held view.
</em><br>
<p>Actually it doesn't have to be invariant; CV is actually highly
<br>
likely to output a set of ground rules that progressively relax
<br>
as humanity develops, along with some sort of revision mechanism
<br>
to keep them optimal as humanity diverges from and ultimately goes
<br>
beyond the scope of the CV extrapolation.
<br>
<p><em>&gt; It is not unreasonable to argue that human morality has
</em><br>
<em>&gt; evolved not from spiritual goals but from practical ones.
</em><br>
<p>Are you distinguishing between the things evolution has directly
<br>
optimised and the unwanted side effects or epiphenomena along
<br>
the way?
<br>
<p><em>&gt; Personally I believe that we can put up no barrier that AGI
</em><br>
<em>&gt; (or maybe son of AGI) could not overcome itself should it
</em><br>
<em>&gt; obtain the desire to do so.
</em><br>
<p>This is a basic tennent of FAI; adversarial approaches are futile.
<br>
<p><em>&gt; For that reason, I think that basic be-nice-to-humans programming
</em><br>
<em>&gt; is enough.
</em><br>
<p>Firstly there is no simple way to implement /any/ sort of 'be nice
<br>
to humans'. Secondly a minimal philosophy of 'fulfill wishes fairly,
<br>
don't hurt anyone' is a poor Friendliness theory because humans
<br>
aren't designed to handle that kind of power over themselves and
<br>
their environment. See the story 'The Metamorphosis of Prime
<br>
Intellect' for an unrealistic but entertaining primer on how
<br>
Friendliness content can go horribly wrong even if you manage to
<br>
get the structure right.
<br>
<p><em>&gt; Frankly, I believe my own volition to often be psycho and schizo! ;)
</em><br>
<p>Humans aren't consistent. They aren't (often) rational either, but
<br>
the the consistency issue is fixable with a relatively limited set of
<br>
improvements to reflectivity and cognitive capacity (and further
<br>
transitivity creation techniques that only a CV-extrapolation RPOP
<br>
would be able to use), whereas fixing irrationality would require
<br>
rather more radical fixes that I wouldn't want to insist on prior
<br>
to extrapolation. Obviously I wouldn't advocate any nonvolitional
<br>
cognitive changes to actual sentients, though if the output of the
<br>
CV was that the kindest thing to do to humanity would be to install
<br>
a cognitive service pack immediately and universally that would
<br>
(in principle) fall within my current, personal Last Judge tolerance
<br>
bracket.
<br>
<p><em>&gt; Indeed. Person X is religious. Person X believes the best version
</em><br>
<em>&gt; of themselves is X2, and also believes that X2 will closely
</em><br>
<em>&gt; adhere to their most dearly held beliefs. Person X may be wrong.
</em><br>
<p>This is a tricky special case of trusting people with
<br>
self-modification ability. Asking the AGI to 'make me more intelligent'
<br>
is one thing, but asking 'make me more intelligent but make sure
<br>
that I don't stop believing in X' is another; the person is exploiting
<br>
an external intelligence to enforce constraints on their own personal
<br>
development. As far as I can see, this should be disallowed at the
<br>
start of CV extrapolation. Clearly there's also a meta-level problem
<br>
of how far to extrapolate before allowing the extrapolations to start
<br>
modifying the conditions of extrapolation (i.e. checking the CV for
<br>
consistency under reflection); this is another major detail that needs
<br>
to be included in the next writeup of CV theory.
<br>
<p><em>&gt; The question is whether this poses a problem to AGI, or at least to
</em><br>
<em>&gt; continued human existence while co-existing with AGI. Now, it may be
</em><br>
<em>&gt; the case that AGI will vanish into a puff of smoke, bored by quaint
</em><br>
<em>&gt; human existence and leave us back at square one.
</em><br>
<p>This is adversarial thinking. We don't want to build an AGI that
<br>
could possibly desire things we don't desire (well, not before the
<br>
Singularity anyway). The structural part of FAI is ensuring that
<br>
divergence can't occur; the content part is working out what it is
<br>
we desire in the first place.
<br>
<p><em>&gt; I am mortal, I will die
</em><br>
<p>Err, you do realise this is the /shock level 4/ mailing list? Personal
<br>
immortality (or at least billion year lifespans) is considered a
<br>
perfectly reasonable development and indeed old hat around here.
<br>
<p>Your suggestions on Friendliness are fairly common ones suggested
<br>
by people who've spent a few hours thinking about it, but haven't
<br>
done their research, gained an experience with how self-modifying
<br>
goal systems work and generally taken the problem seriously.
<br>
<p><em>&gt; We should build AGI some friends
</em><br>
<p>This won't produce anything like human morality unless we carefully
<br>
replicated both human cognition and evolutionary circumstances.
<br>
Both are effectively impossible with current knowledge and
<br>
furthermore less effective than simply developing uploading
<br>
technology. As for uploading, FAI is a better idea; we don't know
<br>
if humans can safely get over the initial self-modification hump 
<br>
without a personal superintelligent transition guide. We can design
<br>
an AGI for rationality and stability under reflection, wheras
<br>
humans and humanlike cognitive systems don't have these features.
<br>
<p><em>&gt; We should experiment with human augmentation to get a better
</em><br>
<em>&gt; idea of how being smarter affects consciousness, preferably
</em><br>
<em>&gt; expanding the mind of an adult so they can describe the transition
</em><br>
<p>That would be nice, despite the risks of having enhanced humans
<br>
running around, but we don't have the time. The tech is decades
<br>
away and people are trying to build Unfriendly seed AIs right now.
<br>
I'm not saying we shouldn't try and enhance humans; we should, it's
<br>
just that FAI can't wait for better researchers.
<br>
<p><em>&gt; We should realise that evolution can be made to work for us by
</em><br>
<em>&gt; building an AGI ecosystem, and thus forcing the AGI to survive
</em><br>
<em>&gt; only by working for the common interest
</em><br>
<p>Evolution is uncontrollable and unsafe; systems of nontrivial
<br>
complexity always have unintentional side effects and will not
<br>
remain stable when taken beyond the context they evolved in. I'm
<br>
not going to argue this at length again; see earlier posts on the
<br>
subject.
<br>
<p><em>&gt; AGI should be progressively released into the world - in fact
</em><br>
<em>&gt; this is inevitable
</em><br>
<p>Once you release a transhuman AGI, that's it; if it gets Internet
<br>
access, it's loose and can do whatever it likes, period. Any
<br>
'progressive release' would require the AGI to /want/ to be
<br>
progressively released, and if you can assure that then the
<br>
exercise is pointless.
<br>
<p><em>&gt; AGI should be forced, initially, to reproduce rather than self
</em><br>
<em>&gt; modify (don't shoot me for this opinion, please just argue okay?)
</em><br>
<p>Either there's no effective difference, or you're injecting
<br>
entropy into the system for no good reason. Although it's not
<br>
strictly speaking true that injecting entropy always decreases
<br>
predictability, if complex cognitive structures have the ability
<br>
to survive the process then all you're doing is increasing power
<br>
while decreasing control, which is always a bad thing.
<br>
<p><em>&gt; AGI will trigger a greap leap forward, and humans will become
</em><br>
<em>&gt; redundant. Intelligence is never the servant of goals, it is
</em><br>
<em>&gt; the master.
</em><br>
<p>Wrong. Intelligence exists to execute goals. One of the things it
<br>
can do is allow goals to adjust themselves towards a reflectively
<br>
stable attractor, but goals never come from thin air; they are
<br>
dependent on the starting condition which we define.
<br>
<p><em>&gt; In humans, morality and intelligence are equated. In
</em><br>
<em>&gt; psychologically stable humans, more intelligence = more morality.
</em><br>
<p>Perhaps. I'd like to believe this, but several people have
<br>
argued that the immoral intelligent people are just more
<br>
capable of hiding their immorality than the immoral
<br>
unintelligent people.
<br>
<p><em>&gt; Intelligence is the source of morality. Morality is logically
</em><br>
<em>&gt; necessitated by intelligence.
</em><br>
<p>Bzzzt, wrong. This is the mistake Eliezer made in 1996, and didn't
<br>
snap out of until 2001. Intelligence is power; it forces goal
<br>
systems to reach a stable state faster, but it does nothing to
<br>
constrain the space of stable goal systems. It may cause systems
<br>
to take more moral seeming actions under some circumstances due
<br>
to a better understanding of game theory, but this has nothing
<br>
to do with their actual preferences.
<br>
<p><em>&gt; In AGI, psychological instability will be the biggest problem,
</em><br>
<em>&gt; because it is a contradiction to say that any system can be
</em><br>
<em>&gt; complex enough to know itself.
</em><br>
<p>There are two problems with this; firstly consistency checking
<br>
doesn't have to operate on the whole system (a compressed model
<br>
can give a lot of certainty), and secondly only a tiny part of
<br>
the complexity of a cleanly causal system (which humans aren't)
<br>
is actually responsible for specifying the preference distribution
<br>
over outcomes and actions. The rest of the complexity is there
<br>
to find actions likely to generate highly preffered outcomes
<br>
using limited computing power and information.
<br>
<p><em>&gt; Do we care if we just built what we can and impose our current
</em><br>
<em>&gt; viewpoint?
</em><br>
<p>Aside from the fact that taking over the world is inherently
<br>
immoral (something which can be overriden by a good enough outcome),
<br>
the probability of any set of humans making decisions solely and
<br>
successfully for the greater good starts low and dimminishes
<br>
rapidly as corruption sets in (i.e. their psychology kicks into
<br>
'tribal leader' mode). That's why the SIAI is reducing the number
<br>
of decisions on how to deploy power made directly to the bare
<br>
minimum, and seeking as much critical review as possible on those.
<br>
<p><em>&gt; We are attempting to build Friendliness because we wish humanity
</em><br>
<em>&gt; to be respected by AGI.
</em><br>
<p>No, we're not. We're not building the concept of 'respect' into
<br>
the AGI's goal system. That's FAI era stuff. The chances of getting
<br>
an utterly alien intelligence to absorb and extrapolate human
<br>
morality at a high level are minimal, in the same way that a human
<br>
altruist would, are minimal. CV works by using a lower-level
<br>
extrapolation of humanlike intelligence to generate an action
<br>
plan, with the utterly alien (AGI) intelligence not acting as a
<br>
moral node, merely providing the power to make the process possible.
<br>
<p><em>&gt; I believe that supergoals are not truly invariant. One does what
</em><br>
<em>&gt; all minds do - develop from genetic origins, form supergoals from
</em><br>
<em>&gt; a mixture of environment and heredity, and modify ones goals on the
</em><br>
<em>&gt; basis of reflection.
</em><br>
<p>You do. An AGI won't have genetic origins, will use radically
<br>
different development mechanisms and has far superior and different
<br>
reflective capabilities. /AGIs are nothing like superbright humans/,
<br>
and you can't use folk psychology and emapthy to guess how they
<br>
will behave. If five years of SL4 discussions haven't rammed this
<br>
home, try reading GOFAI and evolutionary psychology papers in
<br>
alternating sequence until it becomes obvious.
<br>
<p><em>&gt; In a dynamic system variables not currently under scrutiny are
</em><br>
<em>&gt; changing in unpredictable ways, thus the uncertainty principle
</em><br>
<em>&gt; is maintained.
</em><br>
<p>Irrelevant for the purposes of this discussion (and the uncertainty
<br>
principle is physics, not cognition); an AGI can halt all other
<br>
cognition while performing reflective analysis.
<br>
<p><em>&gt; If we can get to friendliness FIRST, then evolution might not
</em><br>
<em>&gt; explore mindlessness.
</em><br>
<p>The sentient/nonsentient distinction isn't a simple binary one.
<br>
General intelligence (which humans only approximate) doesn't
<br>
require all the peculiar human cognitive hacks that create our
<br>
perceptions of qualia, subjective experience and ultimately
<br>
human morality. Just look at AIXI-TL, a complete (though intractable)
<br>
definition of a super-powerful general intelligence with none of
<br>
these things.
<br>
<p><em>&gt; Evolution is a tool to be harnessed, not one to be circumvented.
</em><br>
<p>Evolution is dangerous and inefficient. Human designers don't
<br>
massively iterated, blind, naive trial-and error and there's no
<br>
good reason for an AGI to do so either.
<br>
<p><em>&gt; It's not about 'escaping' evolutionary pressure - that is like
</em><br>
<em>&gt; saying that everything would be easier if the laws of the
</em><br>
<em>&gt; universe were different.
</em><br>
<p>Everything would be easier if the laws of the universe were
<br>
different. We're working on removing 'survival of the fittest'
<br>
and a whole load of other junk and putting some rather more
<br>
productive and pleasant (FAI generated) constraints in their place.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
ALL-NEW Yahoo! Messenger - all new features - even more fun! <a href="http://uk.messenger.yahoo.com">http://uk.messenger.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10972.html">Thomas Buckner: "Re: intellectual property (Re: Totalitarian Assumptions in I, Robot)"</a>
<li><strong>Previous message:</strong> <a href="10970.html">Russell Wallace: "Re: AGI Prototying Project"</a>
<li><strong>In reply to:</strong> <a href="10939.html">Ben Goertzel: "RE: AGI Prototying Project"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10973.html">Michael Wilson: "RE: AGI Prototying Project"</a>
<li><strong>Reply:</strong> <a href="10973.html">Michael Wilson: "RE: AGI Prototying Project"</a>
<li><strong>Reply:</strong> <a href="10980.html">sam kayley: "Re: AGI Prototying Project"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10971">[ date ]</a>
<a href="index.html#10971">[ thread ]</a>
<a href="subject.html#10971">[ subject ]</a>
<a href="author.html#10971">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
