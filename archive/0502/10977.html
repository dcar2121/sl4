<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: ITSSIM (was Some new ideas on Friendly AI)</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: ITSSIM (was Some new ideas on Friendly AI)">
<meta name="Date" content="2005-02-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: ITSSIM (was Some new ideas on Friendly AI)</h1>
<!-- received="Tue Feb 22 13:53:06 2005" -->
<!-- isoreceived="20050222205306" -->
<!-- sent="Tue, 22 Feb 2005 15:52:40 -0500" -->
<!-- isosent="20050222205240" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: ITSSIM (was Some new ideas on Friendly AI)" -->
<!-- id="JNEIJCJJHIEAILJBFHILMEBMEAAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="421ACD39.4060206@atlantisblue.com.au" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20ITSSIM%20(was%20Some%20new%20ideas%20on%20Friendly%20AI)"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Tue Feb 22 2005 - 13:52:40 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10978.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>Previous message:</strong> <a href="10976.html">Eliezer S. Yudkowsky: "A Bay Area transhumanist get-together at 5PM, Sunday March 6th?"</a>
<li><strong>In reply to:</strong> <a href="10968.html">David Hart: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10978.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="10978.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="10982.html">David Hart: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="10989.html">Thomas Buckner: "HTML E-mail"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10977">[ date ]</a>
<a href="index.html#10977">[ thread ]</a>
<a href="subject.html#10977">[ subject ]</a>
<a href="author.html#10977">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi,
<br>
<p>Your last paragraph indicates an obvious philosophical (not logical)
<br>
weakness of the ITSSIM approach as presented.
<br>
<p>It is oriented toward protecting against danger from the AI itself, rather
<br>
than other dangers.  Thus, suppose
<br>
<p>-- there's a threat that has a 90% chance of destroying ALL OF THE UNIVERSE
<br>
with a different universe, except for the AI itself; but will almost
<br>
certainly leave the AI intact
<br>
-- the AI could avert this attack but in doing so it would make itself
<br>
slightly less safe (slightly less likely to obey the ITSSIM safety rule)
<br>
<p>Then following the ITSSIM rule, the AI will let the rest of the world get
<br>
destroyed, because there is no action that it can take without decreasing
<br>
its amount of safety.
<br>
<p>Unfortunately, I can't think of any clean way to get around this problem --
<br>
yet.  Can you?
<br>
<p>-- Ben
<br>
<p><p><p><p><p><p>&nbsp;&nbsp;-----Original Message-----
<br>
&nbsp;&nbsp;From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20ITSSIM%20(was%20Some%20new%20ideas%20on%20Friendly%20AI)">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20ITSSIM%20(was%20Some%20new%20ideas%20on%20Friendly%20AI)">owner-sl4@sl4.org</a>]On Behalf Of David Hart
<br>
&nbsp;&nbsp;Sent: Tuesday, February 22, 2005 1:12 AM
<br>
&nbsp;&nbsp;To: <a href="mailto:sl4@sl4.org?Subject=RE:%20ITSSIM%20(was%20Some%20new%20ideas%20on%20Friendly%20AI)">sl4@sl4.org</a>
<br>
&nbsp;&nbsp;Subject: Re: ITSSIM (was Some new ideas on Friendly AI)
<br>
<p><p>&nbsp;&nbsp;Hi Ben,
<br>
<p>&nbsp;&nbsp;I understand how ITSSIM  is designed to &quot;optimize for S&quot;, and also how it
<br>
might work in practice with the one of many possible qualitative definitions
<br>
of &quot;Safety&quot; being the concept that if we [humans] desire that our
<br>
mind-offspring respect our future &quot;growth, joy and choice&quot;, the next N+1
<br>
incrementally improved generation should want the same for themselves and
<br>
their mind-offspring.
<br>
<p>&nbsp;&nbsp;In such a system, supergoals (like, e.g., CV) and their subgoals,
<br>
interacting with their environments, generate A (possible actions), to which
<br>
R (safety rule) is applied.
<br>
<p>&nbsp;&nbsp;I'm very curious to learn how S and SG might interact -- might one
<br>
eventually dominate the other, or might they become co-attractors?
<br>
<p>&nbsp;&nbsp;Of course, we're still stuck with quantifying this and other definitions
<br>
for &quot;Safety&quot;, including acceptable margins.
<br>
<p>&nbsp;&nbsp;NB: I believe we cannot create an S or an SG that are provably invariant,
<br>
but that both should be cleverly designed with the highest probability of
<br>
being invariant in the largest possible |U| we can muster computationally
<br>
(to our best knowledge for the longest possible extrapolation, which may,
<br>
arguably, still be too puny to be comfortably &quot;safe&quot; or &quot;friendly&quot;).
<br>
<p>&nbsp;&nbsp;Perhaps the matrix of SB, SE, SN  and SGB, SGE, SGN should duke-it-out in
<br>
simulation. Although, at some point, we will simply need to choose our S and
<br>
our SG and take our chances, taking into account the probability that Big
<br>
Red, True Blue, et al, may not have terribly conservative values for S or SG
<br>
slowing their progress. :-(
<br>
<p>&nbsp;&nbsp;David
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10978.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>Previous message:</strong> <a href="10976.html">Eliezer S. Yudkowsky: "A Bay Area transhumanist get-together at 5PM, Sunday March 6th?"</a>
<li><strong>In reply to:</strong> <a href="10968.html">David Hart: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10978.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="10978.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="10982.html">David Hart: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="10989.html">Thomas Buckner: "HTML E-mail"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10977">[ date ]</a>
<a href="index.html#10977">[ thread ]</a>
<a href="subject.html#10977">[ subject ]</a>
<a href="author.html#10977">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
