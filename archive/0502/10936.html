<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: AGI Prototying Project</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="RE: AGI Prototying Project">
<meta name="Date" content="2005-02-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: AGI Prototying Project</h1>
<!-- received="Sun Feb 20 14:42:55 2005" -->
<!-- isoreceived="20050220214255" -->
<!-- sent="Sun, 20 Feb 2005 21:42:32 +0000 (GMT)" -->
<!-- isosent="20050220214232" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="RE: AGI Prototying Project" -->
<!-- id="20050220214232.55895.qmail@web25310.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="1108926208.11300@whirlwind.he.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=RE:%20AGI%20Prototying%20Project"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Sun Feb 20 2005 - 14:42:32 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10937.html">Michael Wilson: "RE: AGI Prototying Project"</a>
<li><strong>Previous message:</strong> <a href="10935.html">Russell Wallace: "Re: AGI Prototying Project"</a>
<li><strong>In reply to:</strong> <a href="10934.html">J. Andrew Rogers: "RE: AGI Prototying Project"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10936">[ date ]</a>
<a href="index.html#10936">[ thread ]</a>
<a href="subject.html#10936">[ subject ]</a>
<a href="author.html#10936">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&quot;J. Andrew Rogers&quot; wrote: 
<br>
<em>&gt; All current AGI projects are standing on the shoulders of giants.
</em><br>
<p>All the major, credible AGI projects take care to examine the field
<br>
of AI, looking for incremental successes to reuse and common pitfalls
<br>
to avoid. I still regularly see people starting personal projects and
<br>
declaring that 'since all past work failed, it must be irrelevant, so
<br>
I don't need to pay much/any attention to it'.
<br>
<p><em>&gt; The vast majority of projects have not had the benefit of the math
</em><br>
<em>&gt; and theory we have now. All real technology is developed
</em><br>
<em>&gt; incrementally, and AGI is one particular technology that does not
</em><br>
<em>&gt; produce much in  the way of very obvious qualitative results
</em><br>
<em>&gt; until it is at a very advanced stage.
</em><br>
<p>Most relevant recent theoretical progress has been in decision
<br>
theory and cognitive science, not in AI as such; AGI projects don't
<br>
often reuse technology from earlier, failed AGI projects (or from
<br>
other people's AGI projects in general).
<br>
<p><em>&gt;&gt; We have brilliant, dedicated people, we have a uniquely
</em><br>
<em>&gt;&gt; cross-field perspective, we have a very advanced theory.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is not a differentiator. Everyone else claims to have the
</em><br>
<em>&gt; same, and they are mostly correct.
</em><br>
<p>Untrue; there are plenty of projects that don't acknowledge the
<br>
interdisciplinary requirement (SOAR, Cyc) or which don't require
<br>
any radical new theory (CCortex; granted 'advanced' is only
<br>
verifiable in retrospect). Brilliance of researchers is something
<br>
that people just have to make personal best guesses on.
<br>
<p><em>&gt; So what you are saying is that there is ample evidence that people
</em><br>
<em>&gt; are easily capable of  deluding themselves into thinking that they
</em><br>
<em>&gt; are smart enough to figure out the One True Path to AGI?
</em><br>
<p>Yes.
<br>
<p><em>&gt; And this does not apply to the folks at SIAI because...?
</em><br>
<p>It does apply to the SIAI. Our only defence is that we acknowledge
<br>
the depth of the self-delusion problem (and take what steps we can
<br>
to detect it). In fact this is another reason I want to do (limited)
<br>
prototyping now; if we have serious blind spots, I want to know about
<br>
them ASAP. This is also a major reason for Eliezer's push for formally
<br>
provable algorithms and techniques for FAI; while deluding yourself
<br>
into thinking you have an AGI design when you don't is merely a waste
<br>
of time and resources, deluding yourself into thinking you have an
<br>
FAI design when you really only have a seed AI design is fatal.
<br>
&nbsp;
<br>
<em>&gt; My biggest criticism of AI designs generally is that almost none
</em><br>
<em>&gt; of them really offer a theoretically solid reason why the design
</em><br>
<em>&gt; can be expected to produce AI in the first place.
</em><br>
<p>I agree. There are many designs that are theoretically capable of
<br>
supporting general cognition; Novamente for example, or even in
<br>
principle a Perspex machine (it is Turing complete). However these
<br>
designs don't actually specify the required cognitive complexity,
<br>
only inductive mechanisms intended to generate it, and I have yet
<br>
to see a design where it is demonstrated that these inductive
<br>
mechanisms will actually be capable of creating the relevant
<br>
'emergent' complexity (given a tractable amount of computation).
<br>
<p><em>&gt; Engineering design based on things that &quot;sound good&quot; does not
</em><br>
<em>&gt; fly (no pun intended) in any  other domain (except perhaps social
</em><br>
<em>&gt; engineering) and it has never produced good results anywhere it
</em><br>
<em>&gt; has been tried as a general rule. Justification and validation
</em><br>
<em>&gt; is a very necessary prerequisite that cannot be glossed over
</em><br>
<em>&gt; because it is difficult or inconvenient.
</em><br>
<p>Absolutely. If you can't show that a design will work in principle,
<br>
it definitely won't work in practice. There are some problem
<br>
domains for which this doesn't hold; if the range of possible
<br>
solutions is small enough trial and error will eventually come up
<br>
with the right answer.
<br>
<p><em>&gt; As for &quot;certainty&quot;, you can also say something is &quot;certain&quot; to
</em><br>
<em>&gt; the extent that one can validate the model in implementation.
</em><br>
<em>&gt; And even then, you can only say that which has been demonstrated
</em><br>
<em>&gt; is a certainty; &quot;the house is painted white on the side&quot;. 
</em><br>
<p>True, but if the demo confirms a good fraction of the predictions
<br>
made by a theory of general cognition, the probability of the
<br>
other predictions being accurate goes up considerably.
<br>
<p><em>&gt; Nothing beats a killer demo.
</em><br>
<p>Ideally you want to excite people enough to get them interested
<br>
without being so impressive that everyone starts trying to clone
<br>
your effort.
<br>
<p><em>&gt;&gt; We don't know exactly what we're going to do yet, but we're
</em><br>
<em>&gt;&gt; light-years ahead of all other AGI projects in this regard.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So to clarify: 1) you don't know what you are doing,
</em><br>
<em>&gt; 2) you have used your powers of omniscience to divine what
</em><br>
<em>&gt; everyone else is doing, and so it follows that
</em><br>
<em>&gt; 3) your ideas are far ahead of everyone else.
</em><br>
<p>By 'what we're going to do' I mean 'the Friendliness architecture
<br>
that we're going to implement via the AGI'. I'm not talking about
<br>
my speculative seed AI architecture, which right now has only my
<br>
opinions in favour of it. I'm reffering to Eliezer's Friendliness
<br>
architecture, the implementation of Collective Volition. It's
<br>
not complete, but it is definitely the most advanced Friendliness
<br>
theory developed to date. No other AGI projects have cracked or
<br>
even seriously tackled the Friendliness problem yet (unless there
<br>
is something you're not telling us in that regard).
<br>
<p><em>&gt; A compelling argument to be sure, but it sounds like you should 
</em><br>
<em>&gt; have used your powers of omniscience to figure out your own plan
</em><br>
<em>&gt; rather than trying to figure out what everyone else does or does
</em><br>
<em>&gt; not know.
</em><br>
<p>I'm not qualified to research Friendliness theory; I'm relying
<br>
on Eliezer's powers of omniscience in that regard ;)
<br>
&nbsp;
<br>
<em>&gt;&gt; All of the SIAI staff are dedicated to the principle of the most
</em><br>
<em>&gt;&gt; good for the greatest number. Friendly AI will be a project undertaken
</em><br>
<em>&gt;&gt; on behalf of humanity as a whole; Collective Volition ensures that
</em><br>
<em>&gt;&gt; the result will be drawn from the sum of what we each consider our
</em><br>
<em>&gt;&gt; best attributes.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What organization in the world, good or evil, does NOT profess these very
</em><br>
<em>&gt; things?
</em><br>
<p>Say what? The vast majority of organisations are companies dedicated
<br>
to their stockholders or private clubs dedicated to the interests of
<br>
their members. Most AGI projects are in the former category. As for
<br>
Collective Volition, if anyone other than the SIAI is planning to
<br>
implement it, I haven't heard about it.
<br>
<p><em>&gt;&gt; Because the inner circle are known to be moral...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; While I have no reason to believe The Inner Circle is Evil,
</em><br>
<em>&gt; statements such as this give skeptics a reason to be skeptical.
</em><br>
<p>James, whoever develops a seed AI first (and manages to meet the
<br>
strict requirements for getting it to do something predictable) will
<br>
have an awesome moral responsibility. I know you don't think hard
<br>
takeoff is likely, but even with soft takeoff the people programming
<br>
the initial AGI(s) will have an utterly unprecedented amount of power
<br>
over all of humanity. If there is a way to avoid this massive risk,
<br>
I'd be all for it, but I'm not aware of one. As such anyone who is
<br>
developing AGI is effectively claiming that they have the right to
<br>
take the future of the human race into their own hands, and anyone
<br>
who wants to support an AGI project is forced to decide who is most
<br>
likely to be trustworthy. I can't see what else we can possibly do
<br>
than state our intent clearly and try to be as transparent and
<br>
honest as possible.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
ALL-NEW Yahoo! Messenger - all new features - even more fun! <a href="http://uk.messenger.yahoo.com">http://uk.messenger.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10937.html">Michael Wilson: "RE: AGI Prototying Project"</a>
<li><strong>Previous message:</strong> <a href="10935.html">Russell Wallace: "Re: AGI Prototying Project"</a>
<li><strong>In reply to:</strong> <a href="10934.html">J. Andrew Rogers: "RE: AGI Prototying Project"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10936">[ date ]</a>
<a href="index.html#10936">[ thread ]</a>
<a href="subject.html#10936">[ subject ]</a>
<a href="author.html#10936">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
