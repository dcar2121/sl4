<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: ITSSIM (was Some new ideas on Friendly AI)</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: ITSSIM (was Some new ideas on Friendly AI)">
<meta name="Date" content="2005-02-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: ITSSIM (was Some new ideas on Friendly AI)</h1>
<!-- received="Tue Feb 22 13:58:06 2005" -->
<!-- isoreceived="20050222205806" -->
<!-- sent="Tue, 22 Feb 2005 15:57:40 -0500" -->
<!-- isosent="20050222205740" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: ITSSIM (was Some new ideas on Friendly AI)" -->
<!-- id="JNEIJCJJHIEAILJBFHILIEBNEAAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="JNEIJCJJHIEAILJBFHILMEBMEAAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20ITSSIM%20(was%20Some%20new%20ideas%20on%20Friendly%20AI)"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Tue Feb 22 2005 - 13:57:40 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10979.html">J. Andrew Rogers: "RE: Minimum complexity of AI?"</a>
<li><strong>Previous message:</strong> <a href="10977.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>In reply to:</strong> <a href="10977.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10996.html">David Hart: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="10996.html">David Hart: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10978">[ date ]</a>
<a href="index.html#10978">[ thread ]</a>
<a href="subject.html#10978">[ subject ]</a>
<a href="author.html#10978">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Dave --
<br>
<p>Hmmm...
<br>
<p>This is just a half-baked thought, but I wonder if there's some workable
<br>
version of it...
<br>
<p>Maybe one could build in a &quot;grandfather clause&quot; stating that an AI can
<br>
optionally violate the safety rule IF it can prove that, when given the same
<br>
data about the world and a very long time to study it, its seed AI ancestor
<br>
would have decided to violate the safety rule.
<br>
<p>This is by no means a full solution, though, because there will be serious
<br>
dangers that an advanced AI sees, but the seed AI would have been too stupid
<br>
to recognize.
<br>
<p>-- Ben
<br>
<p>&nbsp;&nbsp;-----Original Message-----
<br>
&nbsp;&nbsp;From: Ben Goertzel [mailto:<a href="mailto:ben@goertzel.org?Subject=RE:%20ITSSIM%20(was%20Some%20new%20ideas%20on%20Friendly%20AI)">ben@goertzel.org</a>]
<br>
&nbsp;&nbsp;Sent: Tuesday, February 22, 2005 3:53 PM
<br>
&nbsp;&nbsp;To: <a href="mailto:sl4@sl4.org?Subject=RE:%20ITSSIM%20(was%20Some%20new%20ideas%20on%20Friendly%20AI)">sl4@sl4.org</a>
<br>
&nbsp;&nbsp;Subject: RE: ITSSIM (was Some new ideas on Friendly AI)
<br>
<p><p><p>&nbsp;&nbsp;Hi,
<br>
<p>&nbsp;&nbsp;Your last paragraph indicates an obvious philosophical (not logical)
<br>
weakness of the ITSSIM approach as presented.
<br>
<p>&nbsp;&nbsp;It is oriented toward protecting against danger from the AI itself, rather
<br>
than other dangers.  Thus, suppose
<br>
<p>&nbsp;&nbsp;-- there's a threat that has a 90% chance of destroying ALL OF THE
<br>
UNIVERSE with a different universe, except for the AI itself; but will
<br>
almost certainly leave the AI intact
<br>
&nbsp;&nbsp;-- the AI could avert this attack but in doing so it would make itself
<br>
slightly less safe (slightly less likely to obey the ITSSIM safety rule)
<br>
<p>&nbsp;&nbsp;Then following the ITSSIM rule, the AI will let the rest of the world get
<br>
destroyed, because there is no action that it can take without decreasing
<br>
its amount of safety.
<br>
<p>&nbsp;&nbsp;Unfortunately, I can't think of any clean way to get around this
<br>
problem -- yet.  Can you?
<br>
<p>&nbsp;&nbsp;-- Ben
<br>
<p><p><p><p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;-----Original Message-----
<br>
&nbsp;&nbsp;&nbsp;&nbsp;From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20ITSSIM%20(was%20Some%20new%20ideas%20on%20Friendly%20AI)">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20ITSSIM%20(was%20Some%20new%20ideas%20on%20Friendly%20AI)">owner-sl4@sl4.org</a>]On Behalf Of David
<br>
Hart
<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sent: Tuesday, February 22, 2005 1:12 AM
<br>
&nbsp;&nbsp;&nbsp;&nbsp;To: <a href="mailto:sl4@sl4.org?Subject=RE:%20ITSSIM%20(was%20Some%20new%20ideas%20on%20Friendly%20AI)">sl4@sl4.org</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;Subject: Re: ITSSIM (was Some new ideas on Friendly AI)
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;Hi Ben,
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;I understand how ITSSIM  is designed to &quot;optimize for S&quot;, and also how
<br>
it might work in practice with the one of many possible qualitative
<br>
definitions of &quot;Safety&quot; being the concept that if we [humans] desire that
<br>
our mind-offspring respect our future &quot;growth, joy and choice&quot;, the next N+1
<br>
incrementally improved generation should want the same for themselves and
<br>
their mind-offspring.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;In such a system, supergoals (like, e.g., CV) and their subgoals,
<br>
interacting with their environments, generate A (possible actions), to which
<br>
R (safety rule) is applied.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;I'm very curious to learn how S and SG might interact -- might one
<br>
eventually dominate the other, or might they become co-attractors?
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;Of course, we're still stuck with quantifying this and other definitions
<br>
for &quot;Safety&quot;, including acceptable margins.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;NB: I believe we cannot create an S or an SG that are provably
<br>
invariant, but that both should be cleverly designed with the highest
<br>
probability of being invariant in the largest possible |U| we can muster
<br>
computationally (to our best knowledge for the longest possible
<br>
extrapolation, which may, arguably, still be too puny to be comfortably
<br>
&quot;safe&quot; or &quot;friendly&quot;).
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;Perhaps the matrix of SB, SE, SN  and SGB, SGE, SGN should duke-it-out
<br>
in simulation. Although, at some point, we will simply need to choose our S
<br>
and our SG and take our chances, taking into account the probability that
<br>
Big Red, True Blue, et al, may not have terribly conservative values for S
<br>
or SG slowing their progress. :-(
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;David
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10979.html">J. Andrew Rogers: "RE: Minimum complexity of AI?"</a>
<li><strong>Previous message:</strong> <a href="10977.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>In reply to:</strong> <a href="10977.html">Ben Goertzel: "RE: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10996.html">David Hart: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="10996.html">David Hart: "Re: ITSSIM (was Some new ideas on Friendly AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10978">[ date ]</a>
<a href="index.html#10978">[ thread ]</a>
<a href="subject.html#10978">[ subject ]</a>
<a href="author.html#10978">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
