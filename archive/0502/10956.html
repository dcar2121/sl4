<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AGI Prototying Project</title>
<meta name="Author" content="Tennessee Leeuwenburg (tennessee@tennessee.id.au)">
<meta name="Subject" content="Re: AGI Prototying Project">
<meta name="Date" content="2005-02-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AGI Prototying Project</h1>
<!-- received="Mon Feb 21 16:20:49 2005" -->
<!-- isoreceived="20050221232049" -->
<!-- sent="Tue, 22 Feb 2005 10:18:36 +1100" -->
<!-- isosent="20050221231836" -->
<!-- name="Tennessee Leeuwenburg" -->
<!-- email="tennessee@tennessee.id.au" -->
<!-- subject="Re: AGI Prototying Project" -->
<!-- id="421A6C4C.3060209@tennessee.id.au" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="JNEIJCJJHIEAILJBFHILOENGDPAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tennessee Leeuwenburg (<a href="mailto:tennessee@tennessee.id.au?Subject=Re:%20AGI%20Prototying%20Project"><em>tennessee@tennessee.id.au</em></a>)<br>
<strong>Date:</strong> Mon Feb 21 2005 - 16:18:36 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10957.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Previous message:</strong> <a href="10955.html">Tennessee Leeuwenburg: "Re: AGI Prototying Project"</a>
<li><strong>In reply to:</strong> <a href="10947.html">Ben Goertzel: "RE: AGI Prototying Project"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10949.html">Peter de Blanc: "Re: AGI Prototying Project"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10956">[ date ]</a>
<a href="index.html#10956">[ thread ]</a>
<a href="subject.html#10956">[ subject ]</a>
<a href="author.html#10956">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
-----BEGIN PGP SIGNED MESSAGE-----
<br>
Hash: SHA1
<br>
<p><p>|&gt; So, a number of sub-questions :
<br>
|&gt;
<br>
|&gt; * Is Friendliness a religion to be hard-wired in to AGI?
<br>
|
<br>
|
<br>
| In a sense, yes.
<br>
<p><p>|&gt; * Is a sectarian AI a problem for us, here now? Do we care if we
<br>
|&gt; just built what we can and impose our current viewpoint? Do we
<br>
|&gt; back our beliefs in a gamble affecting all people if we are
<br>
|&gt; successful?
<br>
|
<br>
| That's too simple a perspective -- one can only impose one's
<br>
| current viewpoint as the initial condition of a dynamic process.  A
<br>
| viewpoint is not the sort of thing one can expect to be invariant
<br>
| under a long period of radical self-modification.
<br>
<p>That's too complex a position. The question was whether we had to
<br>
specifically ensure that our own viewpoint did NOT dominate the
<br>
dynamic process in the long term - would an atheistic AGI be a bad
<br>
thing for example?
<br>
<p>|&gt; * Is a non-sectarian AI a problem for us - do we care if someone
<br>
|&gt; ELSE builds a religious AI that we don't agree with?
<br>
|
<br>
| Very much!
<br>
<p>Why? If a viewpoint is not the sort of thing one can expect to be
<br>
invariant under a long period of radical self-modification, then why
<br>
does it matter? Pick one : Have Cake; Eat Cake.
<br>
<p>|&gt; Now, an assumption which I disagree with is that human life has
<br>
|&gt; any value other than its intelligence.
<br>
|
<br>
|
<br>
| Well, any value to *whom*?
<br>
|
<br>
| The human race has value to *me* other than its intelligence....
<br>
|
<br>
| To the universe as a whole, it's not clear how much &quot;value&quot;
<br>
| *intelligence* has (nor in what sense the concept of &quot;value&quot;
<br>
| applies)...
<br>
<p>To me it is an open question as to whether the universe is self-aware.
<br>
<p>Value, in any sense of the word, is subjective. Without a subject,
<br>
there is no value. As such, an un-intelligent humanity (or roboticy)
<br>
can have no value other than utility for other value-imparting
<br>
intelligent organisms.
<br>
<p>What other value does the human race have for you outside of its
<br>
intelligence?
<br>
<p>|&gt; There are four major ways to be frightened by AGI that come to
<br>
|&gt; mind now, ~ only one of which I think is worth worrying about.
<br>
|&gt;
<br>
|&gt; 1) Skynet becomes self-aware and eats us 2) AGI kills us all in
<br>
|&gt; our own best interests. How better to eliminate world hunger? 3)
<br>
|&gt; AGI needs our food, and out-competes us. Bummer. 4) AGI destroys
<br>
|&gt; our free will
<br>
|&gt;
<br>
|&gt; I am only worried about (1).
<br>
|
<br>
|
<br>
| There is a lot of middle ground besides the extremes, combining
<br>
| factors of several options...
<br>
<p>Indeed. I was proposing an outside-in search for what you find
<br>
interesting :)
<br>
<p>|&gt; * In AGI, psychological instability will be the biggest problem,
<br>
|&gt; because it is a contradiction to say that any system can be
<br>
|&gt; complex enough to know itself.
<br>
|
<br>
| Perhaps no complex AI system can know itself completely, but there
<br>
| can be increasingly greater degrees of approximate knowing; humans
<br>
| are nowhere near the theoretical maximum...
<br>
<p>Agreed. But there remains a (to me) insurmountable risk (another
<br>
horizon problem) that an organism may become psychologically unstable
<br>
regardless of intelligence, because a more intelligent organism may
<br>
not have sufficient understanding. But maybe psychological stability
<br>
converges rather than diverges - perhaps it is simpler than we imagine
<br>
to control.
<br>
<p>- -T
<br>
-----BEGIN PGP SIGNATURE-----
<br>
Version: GnuPG v1.2.4 (GNU/Linux)
<br>
Comment: Using GnuPG with Thunderbird - <a href="http://enigmail.mozdev.org">http://enigmail.mozdev.org</a>
<br>
<p>iD8DBQFCGmxMFp/Peux6TnIRAuQpAJ9BUMJ6gTPR8RImsbUXDGUNi2XQhwCdGJCr
<br>
QLzx6u2yVVkTi+uoaS4SeDk=
<br>
=LsJ5
<br>
-----END PGP SIGNATURE-----
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10957.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Previous message:</strong> <a href="10955.html">Tennessee Leeuwenburg: "Re: AGI Prototying Project"</a>
<li><strong>In reply to:</strong> <a href="10947.html">Ben Goertzel: "RE: AGI Prototying Project"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10949.html">Peter de Blanc: "Re: AGI Prototying Project"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10956">[ date ]</a>
<a href="index.html#10956">[ thread ]</a>
<a href="subject.html#10956">[ subject ]</a>
<a href="author.html#10956">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
