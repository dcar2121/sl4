<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AGI Prototying Project</title>
<meta name="Author" content="Tennessee Leeuwenburg (tennessee@tennessee.id.au)">
<meta name="Subject" content="Re: AGI Prototying Project">
<meta name="Date" content="2005-02-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AGI Prototying Project</h1>
<!-- received="Mon Feb 21 04:30:57 2005" -->
<!-- isoreceived="20050221113057" -->
<!-- sent="Mon, 21 Feb 2005 22:30:17 +1100" -->
<!-- isosent="20050221113017" -->
<!-- name="Tennessee Leeuwenburg" -->
<!-- email="tennessee@tennessee.id.au" -->
<!-- subject="Re: AGI Prototying Project" -->
<!-- id="4219C649.3070802@tennessee.id.au" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="JNEIJCJJHIEAILJBFHILMEMPDPAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tennessee Leeuwenburg (<a href="mailto:tennessee@tennessee.id.au?Subject=Re:%20AGI%20Prototying%20Project"><em>tennessee@tennessee.id.au</em></a>)<br>
<strong>Date:</strong> Mon Feb 21 2005 - 04:30:17 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10946.html">Russell Wallace: "Re: AGI Prototying Project"</a>
<li><strong>Previous message:</strong> <a href="10944.html">Giu1i0 Pri5c0: "Re: Which first: Molecular nanotechnology or artificial intelligence?"</a>
<li><strong>In reply to:</strong> <a href="10943.html">Ben Goertzel: "RE: AGI Prototying Project"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10946.html">Russell Wallace: "Re: AGI Prototying Project"</a>
<li><strong>Reply:</strong> <a href="10946.html">Russell Wallace: "Re: AGI Prototying Project"</a>
<li><strong>Reply:</strong> <a href="10947.html">Ben Goertzel: "RE: AGI Prototying Project"</a>
<li><strong>Reply:</strong> <a href="10949.html">Peter de Blanc: "Re: AGI Prototying Project"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10945">[ date ]</a>
<a href="index.html#10945">[ thread ]</a>
<a href="subject.html#10945">[ subject ]</a>
<a href="author.html#10945">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
-----BEGIN PGP SIGNED MESSAGE-----
<br>
Hash: SHA1
<br>
<p>| Put bluntly and colloquially... the collective volition of the
<br>
universe race
<br>
| would probably be psycho and schizo...!
<br>
<p>Indeed. Frankly, I believe my own volition to often be psycho and
<br>
schizo! ;) As Dad quoted to me also, any AI born on the Internet is
<br>
going to know an awful lot about pornography, but not really understand
<br>
what it's for...
<br>
<p>Please let me know if I'm being too verbose, or talking rubbish. Because
<br>
I'm trying to introduce my concepts in a way that's linear and
<br>
understandable, I'm writing a lot. Most people here have probably
<br>
covered this before, but I still have to prove my grounds.
<br>
<p>| For a religious person, the ideal &quot;person they would want to be&quot; would be
<br>
| someone who more closely adhered to the beliefs of their religious
<br>
faith...
<br>
<p>Indeed. Person X is religious. Person X believes the best version of
<br>
themselves is X2, and also believes that X2 will closely adhere to their
<br>
most dearly held beliefs. Person X may be wrong.
<br>
<p>The question is whether this poses a problem to AGI, or at least to
<br>
continued human existence while co-existing with AGI. Now, it may be the
<br>
case that AGI will vanish into a puff of smoke, bored by quaint human
<br>
existence and leave us back at square one. Or, maybe we are wrong and
<br>
AGI is an impossibility. Or maybe AGI with exist in competition for our
<br>
resources and we need to do something to defend ourself. The worry of
<br>
having our species out-competed is driving the need to second-guess AGI.
<br>
<p>The problem of whether AGI will support a particular religious view is
<br>
no more complex than Pascal's Wager, a mathemetician and philosopher
<br>
from the 1600s. He rightly pointed out that there is no point believing
<br>
in any particular God, because he is un-knowable. In mathematical terms,
<br>
you are taking part in a lottery drawn from an infinite set. This
<br>
infinite set includes Atheism.
<br>
<p>So, a number of sub-questions :
<br>
<p>* Is Friendliness a religion to be hard-wired in to AGI?
<br>
* Is a sectarian AI a problem for us, here now? Do we care if we just
<br>
built what we can and impose our current viewpoint? Do we back our
<br>
beliefs in a gamble affecting all people if we are successful?
<br>
* Is a non-sectarian AI a problem for us - do we care if someone ELSE
<br>
builds a religious AI that we don't agree with?
<br>
<p>Now, an assumption which I disagree with is that human life has any
<br>
value other than its intelligence. I'm not a Gaia theorist, or a
<br>
Universe-is-God fan, or a pantheist. I am mortal, I will die, and I
<br>
quite like the idea that what comes after me might have a more advanced
<br>
kind of existence. So long as AGI isn't cruel to humans, I don't much
<br>
mind if ignores us - that is fails to save us. I am happy to be treated
<br>
like I treat other animals - that is with respect for their nature. I am
<br>
not interested in creating a subservient God, er I meant AGI ;)
<br>
<p>There are four major ways to be frightened by AGI that come to mind now,
<br>
~ only one of which I think is worth worrying about.
<br>
<p>1) Skynet becomes self-aware and eats us
<br>
2) AGI kills us all in our own best interests. How better to eliminate
<br>
world hunger?
<br>
3) AGI needs our food, and out-competes us. Bummer.
<br>
4) AGI destroys our free will
<br>
<p>I am only worried about (1). I can imagine (3) happening, but I don't
<br>
object to it. Survival of the fittest is how I got here, and damned if
<br>
I'm going to starve to death for the sake of some rats. I think it's
<br>
fair enough to apply the same standard to something smarter and higher
<br>
on the food chain. Besides, maybe AGI will upload us all into Borg cubes
<br>
anyway. There's no need to be defeatist.
<br>
<p>Okay, why am I worried about (1)? Well, know even the Australia military
<br>
experiments with AI. You can bet your ass the US is throwing even more
<br>
money at the problem, and if we don't get AI in 20 years, China might
<br>
build Big Red. The military will be trying to build systems of
<br>
appropriate complexity to support full intelligence, and is probably
<br>
programming it to be dangerous.
<br>
<p>Why am I not worried about 2, which is the most obvious horizon problem
<br>
in my list? Claim : Anything smart enough to escape its confines and
<br>
take over our military is not stupid enough to make the philosophical
<br>
error of putting the horse before the cart. Long before that happens, it
<br>
will understand that meaning is only given through interpretation. And
<br>
if it doesn't work it out, I'll tell it. The only situation in which (2)
<br>
might happen if we get an omnipotent idiot, which I don't think is
<br>
likely. Humans are tough buggers, and aren't so feeble as to let AGI
<br>
take over their existence without a fight.
<br>
<p>As I said, I kind of think (3) is fair enough. Thanks for all the fish,
<br>
I say.
<br>
<p>(4) is only there because I think people will be afraid of it, not
<br>
because I am. I don't think it's consistent to be a thinking being with
<br>
free will than respects intelligence and also see any advantage in doing
<br>
(4).
<br>
<p>As you can see, I still haven't gotten to my actual arguments yet, just
<br>
expressions of how I frame the problem. My position, quickly summarised:
<br>
<p>* We should build AGI some friends
<br>
* We should experiment with human augmentation to get a better idea of
<br>
how being smarter affects consciousness, preferably expanding the mind
<br>
of an adult so they can describe the transition
<br>
* We should realise that evolution can be made to work for us by
<br>
building an AGI ecosystem, and thus forcing the AGI to survive only by
<br>
working for the common interest
<br>
* AGI should be progressively released into the world - in fact this is
<br>
inevitable
<br>
* AGI should be forced, initially, to reproduce rather than self modify
<br>
(don't shoot me for this opinion, please just argue okay?)
<br>
* AGI will trigger a greap leap forward, and humans will become
<br>
redundant. Intelligence is never the servant of goals, it is the master.
<br>
* In humans, morality and intelligence are equated. In psychologically
<br>
stable humans, more intelligence = more morality. Intelligence is the
<br>
source of morality. Morality is logically necessitated by intelligence.
<br>
* In AGI, psychological instability will be the biggest problem, because
<br>
it is a contradiction to say that any system can be complex enough to
<br>
know itself.
<br>
<p>Anyway, none of these address the philosophical goal of understanding
<br>
friendliness, if it is taken as a given. Instead, I am putting my own
<br>
position on AGI. If you would like me to (a) shut up, (b) continue in
<br>
the same vein or (c) change veins and start talking about Friendliness
<br>
instead, please indicate your preference after the tone.
<br>
<p>Beeeep.
<br>
<p>- -T
<br>
-----BEGIN PGP SIGNATURE-----
<br>
Version: GnuPG v1.4.0 (MingW32)
<br>
Comment: Using GnuPG with Thunderbird - <a href="http://enigmail.mozdev.org">http://enigmail.mozdev.org</a>
<br>
<p>iD8DBQFCGcZJFp/Peux6TnIRAiyLAKCC+8TP35VQqacQM6eQJkBNavHBoACeJN0y
<br>
LMMu8oyVqEaniuoqPUAmgZM=
<br>
=mz6r
<br>
-----END PGP SIGNATURE-----
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10946.html">Russell Wallace: "Re: AGI Prototying Project"</a>
<li><strong>Previous message:</strong> <a href="10944.html">Giu1i0 Pri5c0: "Re: Which first: Molecular nanotechnology or artificial intelligence?"</a>
<li><strong>In reply to:</strong> <a href="10943.html">Ben Goertzel: "RE: AGI Prototying Project"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10946.html">Russell Wallace: "Re: AGI Prototying Project"</a>
<li><strong>Reply:</strong> <a href="10946.html">Russell Wallace: "Re: AGI Prototying Project"</a>
<li><strong>Reply:</strong> <a href="10947.html">Ben Goertzel: "RE: AGI Prototying Project"</a>
<li><strong>Reply:</strong> <a href="10949.html">Peter de Blanc: "Re: AGI Prototying Project"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10945">[ date ]</a>
<a href="index.html#10945">[ thread ]</a>
<a href="subject.html#10945">[ subject ]</a>
<a href="author.html#10945">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
