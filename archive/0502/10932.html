<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: AGI Prototying Project</title>
<meta name="Author" content="Tyler Emerson (emerson@intelligence.org)">
<meta name="Subject" content="RE: AGI Prototying Project">
<meta name="Date" content="2005-02-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: AGI Prototying Project</h1>
<!-- received="Sun Feb 20 05:16:32 2005" -->
<!-- isoreceived="20050220121632" -->
<!-- sent="Sun, 20 Feb 2005 04:15:57 -0800" -->
<!-- isosent="20050220121557" -->
<!-- name="Tyler Emerson" -->
<!-- email="emerson@intelligence.org" -->
<!-- subject="RE: AGI Prototying Project" -->
<!-- id="200502201216.j1KCGGh13278@tick.javien.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="AGI Prototying Project" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tyler Emerson (<a href="mailto:emerson@intelligence.org?Subject=RE:%20AGI%20Prototying%20Project"><em>emerson@intelligence.org</em></a>)<br>
<strong>Date:</strong> Sun Feb 20 2005 - 05:15:57 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10933.html">Ben Goertzel: "RE: AGI Prototying Project"</a>
<li><strong>Previous message:</strong> <a href="10931.html">Slawomir Paliwoda: "Re: Which first: Molecular nanotechnology or artificial intelligence?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10933.html">Ben Goertzel: "RE: AGI Prototying Project"</a>
<li><strong>Reply:</strong> <a href="10933.html">Ben Goertzel: "RE: AGI Prototying Project"</a>
<li><strong>Maybe reply:</strong> <a href="10934.html">J. Andrew Rogers: "RE: AGI Prototying Project"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10932">[ date ]</a>
<a href="index.html#10932">[ thread ]</a>
<a href="subject.html#10932">[ subject ]</a>
<a href="author.html#10932">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Another email from Mike Wilson.
<br>
<p><p>-----Original Message-----
<br>
From: <a href="mailto:owner-volunteers@intelligence.org?Subject=RE:%20AGI%20Prototying%20Project">owner-volunteers@intelligence.org</a> [mailto:<a href="mailto:owner-volunteers@singinst.org?Subject=RE:%20AGI%20Prototying%20Project">owner-volunteers@singinst.org</a>] On
<br>
Behalf Of Michael Wilson
<br>
Sent: Sunday, February 20, 2005 3:03 AM
<br>
To: <a href="mailto:volunteers@intelligence.org?Subject=RE:%20AGI%20Prototying%20Project">volunteers@intelligence.org</a>
<br>
Subject: RE: AGI Prototying Project
<br>
<p>Dustin Wish wrote;
<br>
<em>&gt; You seem a little elitist about such a project as this.
</em><br>
<p>Yes, I am. AGI is an incredibly hard problem. Thousands of very
<br>
talented researchers have been attacking it for decades. Tens of
<br>
thousands of part-time dabblers have had a go by now. If this
<br>
was something that could be solved by conventional methods, it
<br>
would've been solved by now. The prior probability of /any/ one
<br>
AGI project being successful is perhaps a million to one even
<br>
before you begin to analyse what the design is capable of. For the
<br>
SIAI to have any realistic chance, we must have multiple major
<br>
advantages over all the competition. We have brilliant, dedicated
<br>
people, we have a uniquely cross-field perspective, we have a
<br>
very advanced theory. That won't be enough; we still need more
<br>
funding and recruits, but I think we have enough to try some
<br>
exploratory implementation.
<br>
<p><em>&gt; You attitude about such a project seems to push away more than
</em><br>
<em>&gt; attract.
</em><br>
<p>Unfortunately very, very few people are qualified to work directly
<br>
on AGI; my guess would be fewer than 1 in 10,000,000 (we haven't
<br>
found many of these yet). People at the 1 in 10,000 level of
<br>
intelligence and skill have a reasonable chance of being able to
<br>
help with the non-core areas. Beyond that the effort of making
<br>
the problem accessible isn't worth it for what people can contribute.
<br>
Again I wish this wasn't the case, as I don't like elitism either,
<br>
but reams of past experience (just look at the SL4 archives) have
<br>
shown that though many people think they have something to contribute
<br>
to AGI, very few people actually do.
<br>
<p><em>&gt; AGI isn't any harder than Speech Recognition application development
</em><br>
<em>&gt; to me.
</em><br>
<p>Speech recognition is a largely solved problem that was and is amenable
<br>
to standard engineering methods. The key point is that it's well
<br>
defined; we know exactly what we want, we just need to find algorithms
<br>
that can do it. General intelligence is /not/ a well-defined problem
<br>
(or rather, it's very hard to boil it down to a definition that is both
<br>
correct and complete).
<br>
<p><em>&gt; I think if you can read Dr. James Anderson works on Perspex Machines
</em><br>
<em>&gt; dealing with 4D matrix models then I think you wrap you brain around
</em><br>
<em>&gt; cognition.
</em><br>
<p>I read the two papers, it's an interesting programming substrate but
<br>
there aren't any mechanisms there that would actually cause useful
<br>
cognitive complexity to develop. As an AGI effort this has roughly
<br>
the same value as Mentifex, or Marc Geddes's designs, or OSCAR, or
<br>
EPIC, or Gaurav Gupta's, or Bruce Toy's, or hundreds of other crank
<br>
proposals (or for that matter Cyc's AGI suitability, based on a vauge
<br>
unjustified notion of 'cognition is lots of knowledge').
<br>
<p>It's clear that making stuff up just doesn't cut it; if we did that
<br>
we'd have no more chance of success than all of the above projects
<br>
(i.e. almost none). Our theory must be /different in kind/, in
<br>
particular the way in which we validate and justify it.
<br>
<p><em>&gt; I have yet to see even a base demo on your theories that
</em><br>
<em>&gt; pushes you theorem to the front? 
</em><br>
<p>I haven't published anything yet and I won't be doing so in the
<br>
near future. I'd like to, but Eliezer has convinced me that the
<br>
expected returns (in constructive criticism) aren't worth the
<br>
risk. As such I'm not asking anyone to accept that my design is
<br>
the best, or even that it will work. Frankly I'm not that sure
<br>
that it will work, despite having a deep understanding of the
<br>
theory and advanced validation techniques; that's why this is
<br>
exploratory prototyping (note that many other projects are quite
<br>
happy to claim certainty that they've got it right despite being
<br>
unable to verify cognitive competence and/or blatantly wrong).
<br>
<p><em>&gt; or that you're trying to sell to a government or private company
</em><br>
<em>&gt; your &quot;secret&quot; snake oil.
</em><br>
<p>Rest assured that I'm not trying to sell anyone anything (well,
<br>
unless this code develops into actual products that stand on their
<br>
own merits).
<br>
<p><em>&gt; It may seem a little untrusting of me to pry at you just way,
</em><br>
<em>&gt; but I run into a ton of &quot;snake oil salesman&quot; since I have been
</em><br>
<em>&gt; in this business. 
</em><br>
<p>I know what you mean. The problem is that AGI theories are very
<br>
hard to validate; to the untrained (or even moderately trained)
<br>
eye one looks as good as another. The sad truth of the matter is
<br>
that it would be easy for me to write a convincing looking yet
<br>
utterly bogus architecture document, post it to this list and
<br>
have lots of people say 'yes, that looks good'. Realistic AGI
<br>
architectures are confusing and unintuitive; people who don't
<br>
have a deep AI understanding would look at one and say 'that
<br>
can't work, I don't see X, Y and Z (where the missing things
<br>
are folk psychology concepts or popular simple algorithms)'.
<br>
<p>Michael Ames wrote;
<br>
<em>&gt; We readily acknowledge that there are no demonstration results...
</em><br>
<em>&gt; yet.
</em><br>
<p>I've spent a bit more than a year working on AGI design before
<br>
attempting a full-scale architecture. In my opinion this is the
<br>
bare minimum required; if we weren't up against such a pressing
<br>
deadline I'd insist on another year or two (Eliezer has been
<br>
working on this for eight years and still isn't ready to write
<br>
a constructive architecture description, though to be fair I
<br>
picked up quite a bit from where he left off). AGI is mostly a
<br>
high-level design challenge, not an implementation challenge
<br>
(unless you're trying to brute-force the problem, which as we've
<br>
acknowledged would result in an uncontrollable, world-destroying
<br>
seed AI).
<br>
<p>Scott Powell wrote:
<br>
<em>&gt; It sounds like SIAI is pretty guarded about some of the 'hard
</em><br>
<em>&gt; science' behind their approaches; much of what I've seen around
</em><br>
<em>&gt; seems more like sales than science. 
</em><br>
<p>I agree and I don't like it; I acknowledge that we need the 'sales'
<br>
stuff to pull in new donors and volunteers, but I'd prefer that
<br>
SL4 was filled with hard technical discussion of AI internals and
<br>
that this list was buzzing with 'I coded this, what do you think'
<br>
and 'how about combining these modules?' etc. However we cannot
<br>
operate that way; firstly once you acknowledge the sheer difficulty
<br>
of AGI you realise that there just aren't that many qualified
<br>
people available (and the unqualified ones would just waste time
<br>
with plausible-looking but unworkable ideas), and that we cannot
<br>
take the risk of releasing powerful techniques to all comers. I
<br>
don't like it, you don't like it, but that's the way it is.
<br>
<p><em>&gt; Why should the SIAI have &quot;control&quot; of the intelligence it creates?
</em><br>
<p>We don't want 'control' in the sense of having an AGI that follows
<br>
orders. We want the AGI to do a very specific thing; implement
<br>
Collective Volition (Eliezer's FAI theory; see the SL4 Wiki), or
<br>
more generally behave in a Friendly manner. However Friendly
<br>
behaviour is a tiny subset of /possible/ AI behaviour, and by
<br>
default an AGI will be neither Friendly nor controllable (which leads
<br>
to existential disaster). Again we're not ready to build this yet;
<br>
right now I'm just building some non-AGI prototypes to test aspects
<br>
of the theory.
<br>
<p><em>&gt; Otherwise, what is the danger in sharing the development
</em><br>
<em>&gt; and sourcing of the SIAI movement?
</em><br>
<p>Because if you build an AGI without knowing /exactly/ what you are
<br>
doing, it will do arbitrary things, which will almost certainly be
<br>
things that you don't want to happen. We don't know exactly what
<br>
we're going to do yet, but we're light-years ahead of all other AGI
<br>
projects in this regard. If we handed out takeoff-capable code,
<br>
some fool would proceed to build an AGI with it without understanding
<br>
the implications or building in a stable, Friendly goal system, and
<br>
it would be game over for everyone.
<br>
<p><em>&gt; SIAI itself seems to have an intuitive grasp of 'what
</em><br>
<em>&gt; comes after,' even if it is not laid out for all to see.
</em><br>
<p>It is laid out for all to see here;
<br>
<p><a href="http://www.sl4.org/wiki/CollectiveVolition">http://www.sl4.org/wiki/CollectiveVolition</a>
<br>
<p>Please read this if you haven't already. It's a statement of what
<br>
the SIAI intends to do. If you don't agree that this is better
<br>
than the alternative (which is basically allowing other projects
<br>
to build badly understood AGIs that will destroy the world in a
<br>
randomly choosen fashion), you shouldn't be volunteering to help.
<br>
<p><em>&gt; Credit will rapidly become an archaic notion; nobody will be
</em><br>
<em>&gt; honored as the 'Creator of AI.'
</em><br>
<p>I have no idea how relevant credit will be post-Singularity, but
<br>
it's certainly irrelevant to what we do now; again see
<br>
<p><a href="http://www.sl4.org/wiki/SoYouWantToBeASeedAIProgrammer">http://www.sl4.org/wiki/SoYouWantToBeASeedAIProgrammer</a>
<br>
<p>We're here to save the world, not buck for credit.
<br>
<p><em>&gt; I am concerned that the development is not entirely for the
</em><br>
<em>&gt; benefit of ALL peoples, but rather just of a few. Is my concern
</em><br>
<em>&gt; grounded?
</em><br>
<p>All of the SIAI staff are dedicated to the principle of the most
<br>
good for the greatest number. Friendly AI will be a project undertaken
<br>
on behalf of humanity as a whole; Collective Volition ensures that
<br>
the result will be drawn from the sum of what we each consider our
<br>
best attributes.
<br>
<p><em>&gt; Obviously there's no easy way to answer this, but I ask instead,
</em><br>
<em>&gt; what -are- the security reasons for a select inner circle on this
</em><br>
<em>&gt; project?
</em><br>
<p>Because the inner circle are known to be moral, and perhaps more
<br>
importantly have the correct attitude to risk ('AGI is really, really
<br>
dangerous, we will not build one without a damned good proof that the
<br>
result will be good things'). I have this attitude, and that is why
<br>
I am taking so many safety precautions. Unfortunately most people who
<br>
are interested in AGI do not. If you're on this list, make sure you
<br>
read everything on the SIAI site and understand /why/ we have this
<br>
attitude.
<br>
<p><em>&gt; I believe that SIAI is an honorable initiative, and I only seek to
</em><br>
<em>&gt; stem the rather disturbing doubts that come to mind as I read the
</em><br>
<em>&gt; posts within this forum.
</em><br>
<p>Though we can't give out key seed AI theory, everything else about
<br>
the SIAI (particularly our goals) should be and is up for scrutiny
<br>
and constructive criticism. If you're worried about the prospect of
<br>
a small group wielding awesomely powerful technology; congratulations,
<br>
you damn well should be. I know I am. Unfortunately however all of
<br>
the alternatives to the SIAI project seem to be much worse.
<br>
<p><em>&gt; What is 'volunteer activity' if not donating money?
</em><br>
<p>The vast majority of people associated with the SIAI aren't qualified
<br>
to do any AGI coding at all. It's a shame, but AGI is very hard,
<br>
period. There is a limited amount of non-AI work (that Tyler wants a
<br>
VC to organise); PR, Friendliness advocacy, fundraising. For most
<br>
people the best way to support the SIAI and increase the probability
<br>
of a Friendly singularity is to donnate money; even if you're helping
<br>
in other ways, the SIAI still needs your donnations to fund the main
<br>
implementation project.
<br>
<p>Tennessee Leeuwenburg wrote;
<br>
<em>&gt; The question is, do you think you should give the AI access to its
</em><br>
<em>&gt; own source code? ;)
</em><br>
<p>I'm not sure if you mean 'will my prototypes incorporate reflection
<br>
and AI code generation' or 'will the SIAI's main project use self
<br>
modifying code'. The answer to the latter has to be yes, since that's
<br>
part of the definition of seed AI. The answer to the former is 'kind
<br>
of'; I'm not using any conventional forms of code generation (e.g. GAs
<br>
and other probabilistic mechanisms are out) but I will be using a
<br>
limited amount of structural self-modification (hence the safety
<br>
precautions).
<br>
<p><em>&gt; I write this only because it irritates me when &quot;open source&quot; becomes
</em><br>
<em>&gt; a philosophical dogma. I don't see why the project need be run as an
</em><br>
<em>&gt; open-source one. Having run various projects before, I tend to find
</em><br>
<em>&gt; that a small, specific group of people naturally tend to take power,
</em><br>
<em>&gt; and the project is best served by promoting their power rather than
</em><br>
<em>&gt; through massive distribution of workload. Information NEEDS to be
</em><br>
<em>&gt; concentrated in the minds of a few, when it gets past a certain
</em><br>
<em>&gt; complexity.
</em><br>
<p>Read this carefully, as this is a nugget of wisdom (possibly hard won).
<br>
I completely agree, and the problem is compounded in (serious) AGI
<br>
because it's /very/ hard to explain (just look at the problems people
<br>
had interpreting LOGI, a much simpler theory) and the core elements of
<br>
the system have dense inconnectivity and interdependence (by necessity,
<br>
not choice).
<br>
<p><em>&gt; Designing an AI, something which I have thought about philosophically
</em><br>
<em>&gt; if not in context of programming (lack the skill), is I think something
</em><br>
<em>&gt; personal. An AI is typically backed by a framework, which is often only
</em><br>
<em>&gt; one of many potential models for driving a decision-making program.
</em><br>
<em>&gt; Making it open source may please the punters, but does not bring a great
</em><br>
<em>&gt; advantage to the progress of the project.
</em><br>
<p>Again, very perceptive. Since it's so easy for a single person to miss
<br>
things we're using a small team, but the sweet spot for core design is
<br>
very small (for this prototyping project I'm designing and Eliezer is
<br>
reviewing).
<br>
<p><em>&gt; I for one am sceptical that there is any large body of exceedingly
</em><br>
<em>&gt; dangerous knowledge that a few singer institute software developers have
</em><br>
<em>&gt; managed to achieve - however brilliant they are. Scientific history is
</em><br>
<em>&gt; one of incremental improvement, and our most advanced technologies are,
</em><br>
<em>&gt; conceptually, available to anyone with the intelligence to understand
</em><br>
<em>&gt; them.
</em><br>
<p>I agree that it seems unlikely, but from my point of view there is
<br>
compelling evidence to override the low prior. I'm actually rather
<br>
suprised that the SIAI exists and has any chance at all; this is
<br>
enough to give me a touch of anthropic paranoia. From your point of
<br>
view I agree that you can't tell if we're genuinely ahead of the game
<br>
or just misleading ourselves. However given the risks, stakes and
<br>
state of the competition, it makes sense to support the SIAI and this
<br>
project in order to find out.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p><p>~~~
<br>
Tyler Emerson
<br>
Executive Director
<br>
Singularity Institute
<br>
P.O. Box 50182
<br>
Palo Alto, CA 94303
<br>
Phone: 650.353.6063
<br>
<a href="mailto:emerson@intelligence.org?Subject=RE:%20AGI%20Prototying%20Project">emerson@intelligence.org</a>
<br>
<a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10933.html">Ben Goertzel: "RE: AGI Prototying Project"</a>
<li><strong>Previous message:</strong> <a href="10931.html">Slawomir Paliwoda: "Re: Which first: Molecular nanotechnology or artificial intelligence?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10933.html">Ben Goertzel: "RE: AGI Prototying Project"</a>
<li><strong>Reply:</strong> <a href="10933.html">Ben Goertzel: "RE: AGI Prototying Project"</a>
<li><strong>Maybe reply:</strong> <a href="10934.html">J. Andrew Rogers: "RE: AGI Prototying Project"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10932">[ date ]</a>
<a href="index.html#10932">[ thread ]</a>
<a href="subject.html#10932">[ subject ]</a>
<a href="author.html#10932">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
