<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Perspex Space</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="Re: Perspex Space">
<meta name="Date" content="2005-02-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Perspex Space</h1>
<!-- received="Sun Feb  6 23:43:14 2005" -->
<!-- isoreceived="20050207064314" -->
<!-- sent="Mon, 7 Feb 2005 06:22:51 +0000 (GMT)" -->
<!-- isosent="20050207062251" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="Re: Perspex Space" -->
<!-- id="20050207062251.37220.qmail@web25309.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20050207032517.94763.qmail@web20227.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20Perspex%20Space"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Sun Feb 06 2005 - 23:22:51 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10862.html">Thomas Buckner: "Re: Ethics"</a>
<li><strong>Previous message:</strong> <a href="10860.html">Thomas Buckner: "Re: Odd questions for you all ;)  Degradation algorithms versus Enhancement algorithms"</a>
<li><strong>In reply to:</strong> <a href="10857.html">Marc Geddes: "Re: Perspex Space"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10892.html">Marc Geddes: "Crack-pots [was Perspex Space]"</a>
<li><strong>Reply:</strong> <a href="10892.html">Marc Geddes: "Crack-pots [was Perspex Space]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10861">[ date ]</a>
<a href="index.html#10861">[ thread ]</a>
<a href="subject.html#10861">[ subject ]</a>
<a href="author.html#10861">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; Don't under estimate pure self reflection.
</em><br>
<p>AI is fundamentally an engineering problem, though some of the
<br>
subtasks of working out exactly what we want to achieve and
<br>
dispelling the persistent misconceptions that block progress may
<br>
be classified as philosophy. Useful AI design is extremely
<br>
demanding on the ability to compose, manipulate, link and ground
<br>
formal systems, as well as the ability to move up and down
<br>
abstraction hierarchies (you must be able to do this effortlessly
<br>
and correctly to have a chance of being able to view the problem
<br>
reflectively and see how abstraction and reference work). A
<br>
successful AGI designer must be able to abstract themes from
<br>
details, merge them while maintaining strict information-theoretic
<br>
validity, and then translate the result back into a valid
<br>
engineering specification that will really achieve what the
<br>
designer wants it to. In this respect AI as a discipline combines
<br>
the demands of programming, architecture, mathematics, cognitive
<br>
science and novel writing.
<br>
<p><em>&gt; I think ancient philosophers *could in principle* have up with
</em><br>
<em>&gt; virtually all of modern transhumanist philosophy.
</em><br>
<p>The human mind cannot support that many unsupported inferential
<br>
steps even without the need to independently dismiss numerous
<br>
plausible-seeming fallacies along the way.
<br>
<p><em>&gt; Using intense self-reflection alone, I'm confident I've
</em><br>
<em>&gt; managed to 'punch my way' well past the current empirical data.
</em><br>
<p>You've been playing word games for a long time. Over time the
<br>
words acquired particular sympathies and antipathies in your mind
<br>
that cause them to fit together in certain structures. But those
<br>
structures were not built by careful combination of objective
<br>
axioms and empirical evidence and show no (apparent) concern for
<br>
plausibility as a predictive, descriptive model of a real world
<br>
intelligent agent. Your attempts to formalise them amount to
<br>
rationalisation of the wild guess that managed to gather enough
<br>
cognitive support for itself over repeated mental shufflings.
<br>
<p>This isn't a problem you can solve by connecting a few formal
<br>
models with an overarching grid of fuzzy, ungrounded concepts
<br>
that forms a cool-sounding pattern. The problem must be solved
<br>
by large scale combination of those formal models into a vast,
<br>
intricate, multilayered yet consistent pattern. There is no a
<br>
priori requirement that the pattern look plausible or
<br>
comprehensible to untrained perception, and indeed given the
<br>
history we should expect this result. The solution /does/ have
<br>
a kind beauty to it, but what legions of AI researchers missed
<br>
is that you can't produce a rose without massive amounts of
<br>
intertwined biochemical complexity (on top of the basic
<br>
principles of molecular physics and natural selection).
<br>
<p><em>&gt; P.S I wouldn't be so sure that Bayesian Reasoning is
</em><br>
<em>&gt; the ultimate epistemology if I were you.
</em><br>
<p>Probabilistic reasoning is a formal model created in a space
<br>
very different from reality. Application of it requires a lot
<br>
of human cognitive effort to dynamically create maps that are
<br>
both descriptive of the target problem and compliant with the
<br>
logical requirements of probability theory, followed by
<br>
further effort to manipulate the model in useful ways. Clearly
<br>
any constructive and complete theory of AI based on Bayesian
<br>
principles must account for how this activity is accomplished.
<br>
<p>Epistemologists tend to forget that reality does not contain
<br>
'knowledge' any more than in contains 'flowers'; in fact in
<br>
practice 'knowledge' is a much more difficult class of
<br>
regularity to define.
<br>
<p><em>&gt; You should have realized that by looking at my 8-level
</em><br>
<em>&gt; intelligence schematic. It would be nonsense if Bayes
</em><br>
<em>&gt; really was the last word)
</em><br>
<p>I will keep it in mind in case I ever need to license your
<br>
Gibberish Matrix Technology (TM) for use in producing
<br>
meaningless yet superficially impressive press statements.
<br>
Meanwhile I have to wonder if your real goal is to cement
<br>
yourself as /the/ definitive, canonical crackpot that will
<br>
be remembered as such for the rest of human history.
<br>
<p>Seriously, you've used just about the most indirectly
<br>
grounded and hence fuzzily defined concepts in the human
<br>
cognitive repertoire, both in the table and the supporting
<br>
text. It's a standard human tendency to assume that the
<br>
most abstract concepts have the most descriptive power, but
<br>
as as several posters have pointed out /engineering, and AI
<br>
in particular, does not work like that/. Your output to
<br>
date hardly constrains the design space of implementations
<br>
at all; I suspect it would take very little additional
<br>
rationalisation to characterise any sufficiently complex,
<br>
superficially plausible AGI architecture as obeying these
<br>
principles (should one so desire). Nor does it make any
<br>
detailed predictions on the behavior of AGI or proto-AGI
<br>
systems, and as such it is worthless as a constructive or
<br>
predictive theory.
<br>
<p><em>&gt; All this and I haven't even really bothered to 'hit
</em><br>
<em>&gt; the books' yet.
</em><br>
<p>I suspect that your preconceptions will prevent you from
<br>
extracting anything of value; you need a delicate
<br>
combination of open mindedness, rigorous filtering for
<br>
clue and creativity within logical constraints to elicit
<br>
a constructive, predictive understanding of AGI from the
<br>
AI literature.
<br>
<p><em>&gt; It's my philosophical intuition versus Sing Inst's
</em><br>
<em>&gt; super-geniuses. I love it ;)
</em><br>
<p>Entertainment value aside, I suppose we can both hope that
<br>
it will serve as an object lesson in the value of philosophical
<br>
intuition in the future, albeit from perfectly opposed
<br>
motives.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><a href="http://www.sl4.org/wiki/Starglider">http://www.sl4.org/wiki/Starglider</a>
<br>
<p><p>.
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
ALL-NEW Yahoo! Messenger - all new features - even more fun! <a href="http://uk.messenger.yahoo.com">http://uk.messenger.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10862.html">Thomas Buckner: "Re: Ethics"</a>
<li><strong>Previous message:</strong> <a href="10860.html">Thomas Buckner: "Re: Odd questions for you all ;)  Degradation algorithms versus Enhancement algorithms"</a>
<li><strong>In reply to:</strong> <a href="10857.html">Marc Geddes: "Re: Perspex Space"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10892.html">Marc Geddes: "Crack-pots [was Perspex Space]"</a>
<li><strong>Reply:</strong> <a href="10892.html">Marc Geddes: "Crack-pots [was Perspex Space]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10861">[ date ]</a>
<a href="index.html#10861">[ thread ]</a>
<a href="subject.html#10861">[ subject ]</a>
<a href="author.html#10861">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
