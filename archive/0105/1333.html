<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Thwarting Friendliness</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Thwarting Friendliness">
<meta name="Date" content="2001-05-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Thwarting Friendliness</h1>
<!-- received="Thu May 03 14:49:33 2001" -->
<!-- isoreceived="20010503204933" -->
<!-- sent="Thu, 03 May 2001 14:47:10 -0400" -->
<!-- isosent="20010503184710" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Thwarting Friendliness" -->
<!-- id="3AF1A7AE.46A129B1@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3AF199EA.D1BC8ABC@posthuman.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Thwarting%20Friendliness"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu May 03 2001 - 12:47:10 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1334.html">Eliezer S. Yudkowsky: "Re: Goertzel's _PtS_"</a>
<li><strong>Previous message:</strong> <a href="1332.html">Brian Atkins: "Re: Thwarting Friendliness"</a>
<li><strong>In reply to:</strong> <a href="1332.html">Brian Atkins: "Re: Thwarting Friendliness"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1333">[ date ]</a>
<a href="index.html#1333">[ thread ]</a>
<a href="subject.html#1333">[ subject ]</a>
<a href="author.html#1333">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Brian Atkins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; First James and Doug, does the &quot;subgoal stomping supergoal&quot; Q&amp;A here
</em><br>
<em>&gt; answer your questions?
</em><br>
<p>No.  &lt;smile&gt;.
<br>
<p><em>&gt; Now Doug also brings up the idea of an AI experimenting by simulating
</em><br>
<em>&gt; other AIs which might have different goal systems. Well, I guess there
</em><br>
<em>&gt; are two possibilities: the simulated AI w/o Friendliness will either
</em><br>
<em>&gt; turn out to function ok (Friendly), or it will not. If it does not then
</em><br>
<em>&gt; obviously the FAI will not give any serious thought to replacing its
</em><br>
<em>&gt; supergoal. If the simulated AI /does/ turn out to behave in a Friendly
</em><br>
<em>&gt; fashion, then I bet the original AI would carry out many more experiments
</em><br>
<em>&gt; and might eventually decide that getting rid of the original supergoal
</em><br>
<em>&gt; might be worthwhile. But it would have to replace it with something
</em><br>
<em>&gt; that would still be friendly, along with providing some sort of other
</em><br>
<em>&gt; benefit over and above that (else, why bother doing it?).
</em><br>
<p>I think Doug is worried, not about a simulated AI whose goals replace the
<br>
original, but about a real AI, or about a simulated AI that's smart enough
<br>
to somehow eat the simulator.  For either case to happen, the AI needs to
<br>
underestimate the threat posed by an unFriendly AI, and also needs to
<br>
evaluate a large benefit from the simulation or creation of an unFriendly
<br>
AI.  The only class of cases I know of where imagining an unFriendly AI
<br>
provides a benefit is &quot;wisdom tournaments&quot;, and the problem of
<br>
constructing a &quot;shadowself&quot; that can't threaten the actual AI is briefly
<br>
discussed there.
<br>
<p><a href="http://intelligence.org/CaTAI/friendly/design/seed.html#wisdom_wisdom">http://intelligence.org/CaTAI/friendly/design/seed.html#wisdom_wisdom</a>
<br>
<p>Otherwise - if there's not a large benefit, and/or if safety is imperfect
<br>
- then constructing an unFriendly AI is an unFriendly action.  At least,
<br>
it's an unFriendly action pre-Sysop-scenario.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1334.html">Eliezer S. Yudkowsky: "Re: Goertzel's _PtS_"</a>
<li><strong>Previous message:</strong> <a href="1332.html">Brian Atkins: "Re: Thwarting Friendliness"</a>
<li><strong>In reply to:</strong> <a href="1332.html">Brian Atkins: "Re: Thwarting Friendliness"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1333">[ date ]</a>
<a href="index.html#1333">[ thread ]</a>
<a href="subject.html#1333">[ subject ]</a>
<a href="author.html#1333">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
