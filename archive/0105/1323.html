<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Goertzel's _PtS_</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Goertzel's _PtS_">
<meta name="Date" content="2001-05-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Goertzel's _PtS_</h1>
<!-- received="Wed May 02 22:34:02 2001" -->
<!-- isoreceived="20010503043402" -->
<!-- sent="Wed, 02 May 2001 22:30:31 -0400" -->
<!-- isosent="20010503023031" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Goertzel's _PtS_" -->
<!-- id="3AF0C2C7.1A0EDB3B@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMCECAFIAA.ben@webmind.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Goertzel's%20_PtS_"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed May 02 2001 - 20:30:31 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1324.html">Ben Goertzel: "RE: Goertzel's _PtS_"</a>
<li><strong>Previous message:</strong> <a href="1322.html">Ben Goertzel: "RE: Goertzel's _PtS_"</a>
<li><strong>In reply to:</strong> <a href="1322.html">Ben Goertzel: "RE: Goertzel's _PtS_"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1324.html">Ben Goertzel: "RE: Goertzel's _PtS_"</a>
<li><strong>Reply:</strong> <a href="1324.html">Ben Goertzel: "RE: Goertzel's _PtS_"</a>
<li><strong>Reply:</strong> <a href="1326.html">James Higgins: "Re: Goertzel's _PtS_"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1323">[ date ]</a>
<a href="index.html#1323">[ thread ]</a>
<a href="subject.html#1323">[ subject ]</a>
<a href="author.html#1323">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I don't have time to carry out this argument in 100% adequate detail right
</em><br>
<em>&gt; now, because I'm going to Norway tomorrow to beg for $$ from some VC's there
</em><br>
<em>&gt; ;&gt;   But hopefully over the weekend I'll find time to write that essay on
</em><br>
<em>&gt; the logic of Friendliness that I keep wanting to write, which will explain
</em><br>
<em>&gt; what I mean by &quot;invariants&quot; and so forth...
</em><br>
<p>Yes, we should probably work this out before you launch the self-improving
<br>
superintelligent version of Webmind.  So we probably have another couple
<br>
of months before the problem becomes really urgent, right?  (JOKE!  Even
<br>
Ben says it's another 3 years, minimum.)
<br>
<p><em>&gt; Anyway, I'll make a few comments...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; Point the 1st:  Friendliness is not, and cannot, be implemented on the
</em><br>
<em>&gt; &gt; level of source code.  Friendliness is cognitive content.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Sure, but source code can bias the system in favor of certain cognitive
</em><br>
<em>&gt; content
</em><br>
<p>Depends on how philosophically sophisticated the system is.  For an
<br>
advanced, reflective system that can think about thinking, and
<br>
specifically think about goal thinking and examine source code, the system
<br>
will be aware that the source code is biasing it and that the bias was
<br>
caused by humans.  If the AI regards sources of behaviors as more and less
<br>
valid, it may come to regard some specific bias as invalid.  (FAI
<br>
explicitly proposes giving the AI the capability to understand causation
<br>
and validity in this way.)  Source code or precreated content can support
<br>
the system, or even bias it, but only as long as the AI-as-a-whole concurs
<br>
that the support or bias is a good thing (albeit under the current
<br>
system).
<br>
<p><em>&gt; &gt; Point the 2nd:  Friendliness is not a &quot;portion&quot; which &quot;requires&quot; an AI to
</em><br>
<em>&gt; &gt; be friendly to humans.  Friendliness is not an add-on or a plug-in.
</em><br>
<em>&gt; &gt; Friendliness is the whole of the goal system.  It is what the AI wants to
</em><br>
<em>&gt; &gt; do.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I continue not to believe that Friendliness can viably be made &quot;the whole of
</em><br>
<em>&gt; the goal system.&quot;  I'll clarify this point in my systematic write-up when I
</em><br>
<em>&gt; get to it.  Logically, sure you CAN view any other worthy goal as a subgoal
</em><br>
<em>&gt; of Friendliness, but I continue to believe this is a sufficiently awkward
</em><br>
<em>&gt; way to manage other goals, that it's not a workable way for a mind to
</em><br>
<em>&gt; function.
</em><br>
<p>Yes, this is a point of continuing substantive disagreement.  (Though
<br>
there's also a philosophical disagreement about whether you have the
<br>
responsibility to do it *anyway* if it turns out to be hard but not
<br>
impossible; still, I currently think it shouldn't *be* hard.)
<br>
<p><em>&gt; &gt; Point the 3rd:  Friendliness is not &quot;invariant&quot; - a strange term to use
</em><br>
<em>&gt; &gt; for a system one of whose first and foremost recommendations is that
</em><br>
<em>&gt; &gt; supergoals should be probabilistic!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What I meant is, as the system rewrites its own code, the fact of its
</em><br>
<em>&gt; Friendliness is supposed to remain unchanged.  The specific content
</em><br>
<em>&gt; underlying this Friendliness may of course change.  Mathematically, one
</em><br>
<em>&gt; might say that the class of Friendly mind-states is supposed to be an
</em><br>
<em>&gt; probabilistically almost-invariant subspace of the class of all mind-states.
</em><br>
<p>Actually, it just has to be invariant *enough*, by which I mean, for
<br>
example, &quot;more invariant than a human upload&quot;, or &quot;not changing so much as
<br>
to render desirable the act of breaking up humans for their component
<br>
atoms&quot;.  (See &quot;requirements for sufficient convergence&quot; in FAI.)
<br>
<p><em>&gt; &gt; Point the 4th:  Friendliness is not &quot;hardwired&quot;, a term which I've seen
</em><br>
<em>&gt; &gt; you use several times.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What I mean by &quot;hard-wiring Friendliness&quot; is placing Friendliness at the top
</em><br>
<em>&gt; of the initial goal system and making the system express all other goals as
</em><br>
<em>&gt; subgoals of this.  Is this not what you propose?  I thought that's what you
</em><br>
<em>&gt; described to me in New York...
</em><br>
<p>Yes, that's what I described, but by that description *I'm* hard-wired
<br>
Friendly, since this is one of the properties I strive for in my own
<br>
declarative philosophical content.
<br>
<p>&quot;Hard wiring&quot;, to me, means that the system contains features intended to
<br>
hold the content in place even if the AI tries to modify it, or that the
<br>
feature is implemented as low-level code.  The human taste for sugar and
<br>
fat is &quot;hardwired&quot;, for example.  It's implemented as low-level code and
<br>
very hard to override.
<br>
<p><em>&gt; &gt; The main part of the model where I disagree with you is that it'll take a
</em><br>
<em>&gt; &gt; lot more than a Java supercompiler description to give a general
</em><br>
<em>&gt; &gt; intelligence humanlike understanding of source code.  The Java
</em><br>
<em>&gt; &gt; supercompiler description is only the very first step.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I agree there.  But I tend to think that if you put that first step together
</em><br>
<em>&gt; with WM's higher-order inference engine, the second step will come all by
</em><br>
<em>&gt; itself.
</em><br>
<p>Depends on how good WM is.  If WM is already very intelligent in
<br>
Eurisko-like heuristic discovery and composition, and if it has enough
<br>
computing power to handle the clustering and schema creation, feeding in
<br>
the low-level description might be enough for WM to create an effective
<br>
perceptual understanding of the higher-level features by examining typical
<br>
human-written code.  If WM has a strong understanding of purpose and a
<br>
strong pre-existing understanding of vis modules' functionality (WM gets a
<br>
&quot;ve&quot;, by this point), then you could, conceivably, just feed in the Java
<br>
supercompiler description and watch the thing blaze straight through a
<br>
hard takeoff.  Low-probability outcome, but very real.
<br>
<p><em>&gt; &gt; What I'm saying is that *when the system reaches human intelligence*, it
</em><br>
<em>&gt; &gt; will probably be *in the middle of a hard takeoff*
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And this is another point on which our intuitions differ.  I think that
</em><br>
<em>&gt; human-level intelligence will probably be achieved significantly **before**
</em><br>
<em>&gt; a hard takeoff.  I think that optimizing your own mind processes requires
</em><br>
<em>&gt; human-level intelligence or maybe a little more.
</em><br>
<p>I don't necessarily predict that Webmind 2.0 will be sufficient unto a
<br>
hard takeoff.  It's just that, being &quot;conservative&quot; as a Friendliness
<br>
programmer must be, I feel obliged to take your word for it when it comes
<br>
to preparing.  I wish you'd be a bit more &quot;conservative&quot; when it comes to
<br>
preparing for takeoff, even if you predict a slow one.
<br>
<p><em>&gt; We don't really disagree very profoundly; most of our disagreements are just
</em><br>
<em>&gt; different intuitions about timings of things that none of us really has data
</em><br>
<em>&gt; about.  The most significant difference I see is as to whether, initially,
</em><br>
<em>&gt; one wants to rig a goal system with Friendliness at the top....
</em><br>
<p>The Friendliness-topped goal system, the causal goal system, the
<br>
probabilistic supergoals, and the controlled ascent feature are the main
<br>
things I'd want Webmind to add before the 1.0 version of the AI Engine.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1324.html">Ben Goertzel: "RE: Goertzel's _PtS_"</a>
<li><strong>Previous message:</strong> <a href="1322.html">Ben Goertzel: "RE: Goertzel's _PtS_"</a>
<li><strong>In reply to:</strong> <a href="1322.html">Ben Goertzel: "RE: Goertzel's _PtS_"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1324.html">Ben Goertzel: "RE: Goertzel's _PtS_"</a>
<li><strong>Reply:</strong> <a href="1324.html">Ben Goertzel: "RE: Goertzel's _PtS_"</a>
<li><strong>Reply:</strong> <a href="1326.html">James Higgins: "Re: Goertzel's _PtS_"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1323">[ date ]</a>
<a href="index.html#1323">[ thread ]</a>
<a href="subject.html#1323">[ subject ]</a>
<a href="author.html#1323">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
