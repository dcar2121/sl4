<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Goertzel's _PtS_</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Goertzel's _PtS_">
<meta name="Date" content="2001-05-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Goertzel's _PtS_</h1>
<!-- received="Wed May 02 16:45:03 2001" -->
<!-- isoreceived="20010502224503" -->
<!-- sent="Wed, 02 May 2001 16:09:41 -0400" -->
<!-- isosent="20010502200941" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Goertzel's _PtS_" -->
<!-- id="3AF06985.24E4AB86@pobox.com" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Goertzel's%20_PtS_"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed May 02 2001 - 14:09:41 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1322.html">Ben Goertzel: "RE: Goertzel's _PtS_"</a>
<li><strong>Previous message:</strong> <a href="1320.html">James Higgins: "Re: A fairly concrete path to the Singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1322.html">Ben Goertzel: "RE: Goertzel's _PtS_"</a>
<li><strong>Reply:</strong> <a href="1322.html">Ben Goertzel: "RE: Goertzel's _PtS_"</a>
<li><strong>Maybe reply:</strong> <a href="1404.html">my_sunshine: "Re: Goertzel's _PtS_"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1321">[ date ]</a>
<a href="index.html#1321">[ thread ]</a>
<a href="subject.html#1321">[ subject ]</a>
<a href="author.html#1321">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Okay, I've read the entire &quot;Path to Singularity&quot;.  (The most interesting
<br>
part was definitely the JPEGs and lists that show the functional
<br>
decomposition of Webmind's major modules.)
<br>
<p>That said...
<br>
<p>Ben Goertzel wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; the notion of Friendly AI – the creation of AI systems that, as they
</em><br>
<em>&gt; rewrite their own source code achieving progressively greater and
</em><br>
<em>&gt; greater intelligence, leave invariant the portion of their code
</em><br>
<em>&gt; requiring them to be friendly to human beings
</em><br>
<p>No offense - but no, *no*, NO, *NO*!
<br>
<p>Point the 1st:  Friendliness is not, and cannot, be implemented on the
<br>
level of source code.  Friendliness is cognitive content.
<br>
<p>Point the 2nd:  Friendliness is not a &quot;portion&quot; which &quot;requires&quot; an AI to
<br>
be friendly to humans.  Friendliness is not an add-on or a plug-in. 
<br>
Friendliness is the whole of the goal system.  It is what the AI wants to
<br>
do.
<br>
<p>Point the 3rd:  Friendliness is not &quot;invariant&quot; - a strange term to use
<br>
for a system one of whose first and foremost recommendations is that
<br>
supergoals should be probabilistic!  There's an entire section of FAI
<br>
(&quot;Shaper/anchor semantics&quot;) devoted to describing how the AI acquires the
<br>
ability to refine, extend, and correct existing content, create new
<br>
content, or - in extreme cases - junk the entire goal-system architecture
<br>
and start over.  Because at least one of us (Ben and I) is really mistaken
<br>
here, or both of us are, and an AI needs to be able to handle even that
<br>
class of mistakes.  A Friendly AI needs to be a human-equivalent
<br>
philosopher; or rather, to have a cognitive structure which permits
<br>
human-equivalent open-endedness, so that it can later acquire
<br>
human-equivalent or transhuman ability in the domain of philosophy.
<br>
<p>Friendliness can't be ensured by creating an enslaved AI that lacks the
<br>
capability to alter the goal system; Friendliness is ensured by creating a
<br>
Friendly AI that doesn't *want* to stop being Friendly, just as I don't
<br>
want to stop being a nice person.
<br>
<p>What is &quot;invariant&quot; is the fact that the AI is, on the whole, benevolent -
<br>
and even that should never be more &quot;invariant&quot; than our surety that what
<br>
humanity needs is, in fact, a benevolent AI.  I mean, I'm pretty sure of
<br>
this, but humans can't be *100%* sure of *anything*.
<br>
<p>Point the 4th:  Friendliness is not &quot;hardwired&quot;, a term which I've seen
<br>
you use several times.  If a well-designed Friendly AI estimates you have
<br>
an a-priori 10% chance of being wrong about something, then no amount of
<br>
screaming and crying and ultra-strength affirmations on the programmer's
<br>
part will raise the estimated probability above 90% - because all the
<br>
screaming does is tell the AI that you really care, that you have a very
<br>
strong emotional commitment.  Well, that's sensory information, and there
<br>
are ways in which that could affect actions - it could indicate an
<br>
injunction, an ethical heuristic, an anchor, and so on - but it still
<br>
isn't going to raise the probability to 100%, even if it gets the AI to go
<br>
along with you as a wait-and-see interim measure.  To get anything beyond
<br>
&quot;90%&quot; and a &quot;wait-and-see&quot; compromise, you'd have to violate Friendliness
<br>
structure and start tampering directly - which might work for an infant
<br>
AI, but is unlikely to work for any seed AI with a good sense of itself,
<br>
and would certainly be corrected in retrospect by a transhuman AI.
<br>
<p><em>&gt; My sense is that he views self-modification as entering into the picture
</em><br>
<em>&gt; earlier, perhaps in Stage 1, as the best way of getting to the first
</em><br>
<em>&gt; &quot;fairly intelligent AI.&quot;  I'm not 100% sure this is wrong, but after a
</em><br>
<em>&gt; lot of thought I have not seen a good way to do this, whereas I have a
</em><br>
<em>&gt; pretty clear picture of how to get to the Singularity according to the
</em><br>
<em>&gt; steps I've outlined here.
</em><br>
<p>Actually, this is more of a fundamental statement about the attitude and
<br>
philosophy of seed AI - that, as soon as the system has any intelligence
<br>
at all, no matter how primitive, that intelligence can and should be used
<br>
as a tool to build better systems, and that this applies on all layers of
<br>
the system, right down to any source code that you can teach the system to
<br>
comprehend.  However, if you just build a general intelligence and only
<br>
start using it as a self-modification tool after it's been around for a
<br>
few years, that'll work too.  I just think it'll be slower.
<br>
<p>The main part of the model where I disagree with you is that it'll take a
<br>
lot more than a Java supercompiler description to give a general
<br>
intelligence humanlike understanding of source code.  The Java
<br>
supercompiler description is only the very first step.  Consider:  The
<br>
very top layer of the human retina may be able to process all visual
<br>
fields equally well, including those composed of random pixels, but the
<br>
very next layer of the retina will only work for pictures with edges and
<br>
continuous color changes, and the visual cortex only works for
<br>
understanding 3D moving objects - definitely not all possible visual
<br>
fields.  A Java supercompiler is a description that works for *all
<br>
possible Java code*, not just *useful* or *purposeful* Java code, and is
<br>
therefore analogous to the lowest possible layer of the modality.
<br>
<p><em>&gt; After it achieves a significant level of practical software engineering
</em><br>
<em>&gt; experience and mathematical and AI knowledge, it is able to begin
</em><br>
<em>&gt; improving itself ... at which point the hard takeoff begins.
</em><br>
<p>You're quite right that the the takeoff to superintelligence may take
<br>
years from the point where Java code becomes munchable (if not readable). 
<br>
It certainly won't take minutes.  The point of hard takeoff is not when
<br>
the system *first starts* improving itself, but when the system finally
<br>
*does* make a breakthrough that leads to further breakthroughs that lead
<br>
to further breakthroughs and so on, continuing indefinitely, or at least
<br>
until human intelligence has been considerably transcended.  There might
<br>
be a long pattern of short-term breakthroughs and long bottlenecks before
<br>
then, quite possibly going on for years.
<br>
<p>What I'm saying is that *when the system reaches human intelligence*, it
<br>
will probably be *in the middle of a hard takeoff* that only bottlenecks
<br>
on available hardware when considerably transhuman intelligence is reached
<br>
- enough intelligence to change the world as it stands, certainly enough
<br>
to absorb poorly-defended additional computing power if that proves
<br>
necessary and ethical, and probably enough to achieve nanotechnology in a
<br>
couple of weeks.  By the time you're at the level of human general
<br>
smartness, you've already far transcended human equivalence in writing
<br>
code (because you grew up in a computer, and the humans didn't); you are
<br>
mostly self-encapsulating with respect to improvements being improvements
<br>
in the thoughts that created your components, so that each improvement in
<br>
general intelligence yields further improvements in components that sum to
<br>
further improvements in general intelligence; and you are carrying out at
<br>
least some classes of thought (the ones that are mostly serial rather than
<br>
parallel) at thousands of times human speed, such that requesting human
<br>
assistance is ofttimes not a good investment, and there are enough things
<br>
that can be done *without* human assistance to sustain the hard takeoff. 
<br>
In which case, the Global Brain pre-Singularity vision is something that
<br>
could only happen with crude general intelligence or primitive Webminds,
<br>
not human-equivalent ones.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1322.html">Ben Goertzel: "RE: Goertzel's _PtS_"</a>
<li><strong>Previous message:</strong> <a href="1320.html">James Higgins: "Re: A fairly concrete path to the Singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1322.html">Ben Goertzel: "RE: Goertzel's _PtS_"</a>
<li><strong>Reply:</strong> <a href="1322.html">Ben Goertzel: "RE: Goertzel's _PtS_"</a>
<li><strong>Maybe reply:</strong> <a href="1404.html">my_sunshine: "Re: Goertzel's _PtS_"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1321">[ date ]</a>
<a href="index.html#1321">[ thread ]</a>
<a href="subject.html#1321">[ subject ]</a>
<a href="author.html#1321">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
