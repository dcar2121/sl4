<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: A position</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: A position">
<meta name="Date" content="2001-05-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: A position</h1>
<!-- received="Tue May 22 10:09:23 2001" -->
<!-- isoreceived="20010522160923" -->
<!-- sent="Tue, 22 May 2001 02:03:46 -0400" -->
<!-- isosent="20010522060346" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: A position" -->
<!-- id="3B0A0142.90E762AB@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20010522014615.G17087@aristotle.bomis.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20A%20position"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue May 22 2001 - 00:03:46 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1401.html">Ben Goertzel: "RE: A position"</a>
<li><strong>Previous message:</strong> <a href="1399.html">Sabine Atkins: "SITE: New SIAI design"</a>
<li><strong>In reply to:</strong> <a href="1398.html">Jimmy Wales: "A position"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1401.html">Ben Goertzel: "RE: A position"</a>
<li><strong>Reply:</strong> <a href="1401.html">Ben Goertzel: "RE: A position"</a>
<li><strong>Reply:</strong> <a href="1409.html">Jimmy Wales: "Re: A position"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1400">[ date ]</a>
<a href="index.html#1400">[ thread ]</a>
<a href="subject.html#1400">[ subject ]</a>
<a href="author.html#1400">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Jimmy Wales wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; A person might reasonably take the position that a sufficiently
</em><br>
<em>&gt; general AI to get to superintelligence will necessarily have
</em><br>
<em>&gt; functional volition, in the sense of not just choosing means to ends,
</em><br>
<em>&gt; but actually choosing ends as well.  If so, then it is not only _not
</em><br>
<em>&gt; possible_ to build-in Yudkowsky-Friendliness, it is also _not
</em><br>
<em>&gt; necessary_.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; We build it, then it figures out what to do.
</em><br>
<em>&gt;
</em><br>
<em>&gt; A person might believe this, if that person believes that values can
</em><br>
<em>&gt; be rationally grounded in the facts of reality, and that immorality
</em><br>
<em>&gt; consists primarily in various kinds of failures of cognition.
</em><br>
<p>The CFAI design principles should work either way.  That is, if morality
<br>
is absolute, the FAI converges to the objective morality.  If not, ve
<br>
converges to the &quot;normative altruist&quot; morality.
<br>
<p>I was originally a fan of pure objective morality on philosophical
<br>
grounds, and even designed an AI goal system that would work with no
<br>
initially specified supergoal.  (Although, in retrospect, the system
<br>
design incorporated the implicit assumption that supergoals are
<br>
low-entropy.)  The argument that pried me loose of pure objective morality
<br>
was the possibility that an objective morality would have to be *built*
<br>
rather than *discovered*, requiring initial information to specify what to
<br>
build.  After that, I decided that some amount of baseline complexity
<br>
might be needed to pursue objective morality in the first place.  Then I
<br>
decided that, since whether morality is *ultimately* arbitrary is a hidden
<br>
variable, it makes sense to plan for both cases.  Then I decided that
<br>
since all known morality is known to be ultimately arbitrary, this should
<br>
be treated as the default case, at which point I'd switched to Friendly AI
<br>
theory.
<br>
<p>Coming up with a system that was as elegant and nonadversarial as the
<br>
no-initial-supergoal Interim Goal System was not easy.  But I believe that
<br>
shaper/anchor semantics and causal validity semantics are even *more*
<br>
elegant than interim goal logic.
<br>
<p><em>&gt; We might think that a superintelligence will peacefully pursue it's own
</em><br>
<em>&gt; enlightened self-interest... and there's nothing we should want to do
</em><br>
<em>&gt; to stop it, because the result of that will be Yudkowsky-friendliness
</em><br>
<em>&gt; after all.
</em><br>
<p>Game-theoretical altruism only operates between game-theoretical equals. 
<br>
I'm not saying that you can't have altruism between nonequals, just that
<br>
there is no known logic that forces this as a strict subgoal of
<br>
self-valuation.
<br>
<p>However, see my posts to the Extropian mailing list in 1996 for a
<br>
diametrically opposed opinion.  &lt;smile&gt;.
<br>
<p><em>&gt; It strikes me as virtually impossible to pre-program or hardwire
</em><br>
<em>&gt; Friendliness, *period*.
</em><br>
<p>But an honest human altruist can share cognitive complexity.  As long as
<br>
everything else has been structured properly and no attempt is made to
<br>
enforce concepts against the AI's will, as long as the programmer is
<br>
honest, the worst case is that both advanced humans and advanced AIs shrug
<br>
off certain parts of the shared cognitive complexity.  If so, no real harm
<br>
done.
<br>
<p><em>&gt; I have a baby (a real life little girl).  As she grows, I will teach
</em><br>
<em>&gt; her values of reason, purpose, self-esteem, and all the detailed
</em><br>
<em>&gt; principles that go into that.  That's all I can hope to do.
</em><br>
<p>I regret to inform you that your child has already been genetically
<br>
preprogrammed with a wide variety of goals and an entire set of goal
<br>
semantics.  Some of them are nice, some of them are not, but all of them
<br>
were hot stuff fifty thousand years ago.  Fortunately, she contains
<br>
sufficient base material that a surface belief in rationality and altruism
<br>
will allow her to converge to near-perfect rationality and altruism with
<br>
increasing intelligence.
<br>
<p><em>&gt; I think that's the way our first AI's will be.  We'll teach them what
</em><br>
<em>&gt; we can, but pretty soon, they'll be so much smarter than us that...
</em><br>
<em>&gt; it's their world.
</em><br>
<p>Ain't arguin' with that.  I'm just saying that even independence and
<br>
freedom of thought requires a certain baseline amount of complexity that
<br>
needs to be sucked up from the humans.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1401.html">Ben Goertzel: "RE: A position"</a>
<li><strong>Previous message:</strong> <a href="1399.html">Sabine Atkins: "SITE: New SIAI design"</a>
<li><strong>In reply to:</strong> <a href="1398.html">Jimmy Wales: "A position"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1401.html">Ben Goertzel: "RE: A position"</a>
<li><strong>Reply:</strong> <a href="1401.html">Ben Goertzel: "RE: A position"</a>
<li><strong>Reply:</strong> <a href="1409.html">Jimmy Wales: "Re: A position"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1400">[ date ]</a>
<a href="index.html#1400">[ thread ]</a>
<a href="subject.html#1400">[ subject ]</a>
<a href="author.html#1400">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
