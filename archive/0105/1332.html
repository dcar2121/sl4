<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Thwarting Friendliness</title>
<meta name="Author" content="Brian Atkins (brian@posthuman.com)">
<meta name="Subject" content="Re: Thwarting Friendliness">
<meta name="Date" content="2001-05-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Thwarting Friendliness</h1>
<!-- received="Thu May 03 14:34:03 2001" -->
<!-- isoreceived="20010503203403" -->
<!-- sent="Thu, 03 May 2001 13:48:26 -0400" -->
<!-- isosent="20010503174826" -->
<!-- name="Brian Atkins" -->
<!-- email="brian@posthuman.com" -->
<!-- subject="Re: Thwarting Friendliness" -->
<!-- id="3AF199EA.D1BC8ABC@posthuman.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="OF9C091DFD.05E93C85-ON85256A41.004FCE77@ey.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brian Atkins (<a href="mailto:brian@posthuman.com?Subject=Re:%20Thwarting%20Friendliness"><em>brian@posthuman.com</em></a>)<br>
<strong>Date:</strong> Thu May 03 2001 - 11:48:26 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1333.html">Eliezer S. Yudkowsky: "Re: Thwarting Friendliness"</a>
<li><strong>Previous message:</strong> <a href="1331.html">doug.bailey@ey.com: "Thwarting Friendliness"</a>
<li><strong>In reply to:</strong> <a href="1331.html">doug.bailey@ey.com: "Thwarting Friendliness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1333.html">Eliezer S. Yudkowsky: "Re: Thwarting Friendliness"</a>
<li><strong>Reply:</strong> <a href="1333.html">Eliezer S. Yudkowsky: "Re: Thwarting Friendliness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1332">[ date ]</a>
<a href="index.html#1332">[ thread ]</a>
<a href="subject.html#1332">[ subject ]</a>
<a href="author.html#1332">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
James Higgins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; At 10:30 PM 5/2/2001 -0400, Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt; &gt;Ben Goertzel wrote:
</em><br>
<em>&gt; &gt; &gt; Anyway, I'll make a few comments...
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt; Point the 1st:  Friendliness is not, and cannot, be implemented on the
</em><br>
<em>&gt; &gt; &gt; &gt; level of source code.  Friendliness is cognitive content.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; Sure, but source code can bias the system in favor of certain cognitive
</em><br>
<em>&gt; &gt; &gt; content
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;Depends on how philosophically sophisticated the system is.  For an
</em><br>
<em>&gt; &gt;advanced, reflective system that can think about thinking, and
</em><br>
<em>&gt; &gt;specifically think about goal thinking and examine source code, the system
</em><br>
<em>&gt; &gt;will be aware that the source code is biasing it and that the bias was
</em><br>
<em>&gt; &gt;caused by humans.  If the AI regards sources of behaviors as more and less
</em><br>
<em>&gt; &gt;valid, it may come to regard some specific bias as invalid.  (FAI
</em><br>
<em>&gt; &gt;explicitly proposes giving the AI the capability to understand causation
</em><br>
<em>&gt; &gt;and validity in this way.)  Source code or precreated content can support
</em><br>
<em>&gt; &gt;the system, or even bias it, but only as long as the AI-as-a-whole concurs
</em><br>
<em>&gt; &gt;that the support or bias is a good thing (albeit under the current
</em><br>
<em>&gt; &gt;system).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Won't the same mind realize that having Friendliness as its primary goal
</em><br>
<em>&gt; was also caused by humans and thus biasing it?
</em><br>
<em>&gt; 
</em><br>
<p>suggested answer below...
<br>
<p><a href="mailto:doug.bailey@ey.com?Subject=Re:%20Thwarting%20Friendliness">doug.bailey@ey.com</a> wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &gt; I guess Eliezer's point may be that the AI ~does~ have a choice in
</em><br>
<em>&gt; &gt; his plan -- the Friendliness supergoal is not an absolute irrevocable goal,
</em><br>
<em>&gt; &gt; it's just a fact (&quot;Friendliness is the most important goal&quot;) that is given
</em><br>
<em>&gt; &gt; an EXTREMELY high confidence so that the system has to gain a HUGE AMOUNT
</em><br>
<em>&gt; &gt; of evidence to overturn it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Something that concerns me is what happens when the AI decides to develop
</em><br>
<em>&gt; an AI without the Friendliness supergoal?  Several pathways seem to
</em><br>
<em>&gt; conceivably
</em><br>
<em>&gt; lead to this scenario.  The AI decides to study an AI without the Friendliness
</em><br>
<em>&gt; 
</em><br>
<em>&gt; supergoal perhaps not because it doubts the value of the goal but rather is
</em><br>
<em>&gt; simply curious how an AI without this goal would function.  Alternatively, the
</em><br>
<em>&gt; 
</em><br>
<em>&gt; AI might realize on its own that its preset goals and supergoals have not been
</em><br>
<em>&gt; 
</em><br>
<em>&gt; subject to rigorous scrutiny (by the AI that is) and that it is inherently
</em><br>
<em>&gt; biased towards evaluating them itself.  Hence, it creates an AI with minimal
</em><br>
<em>&gt; preset goals either so that the original AI itself can evaluate the importance
</em><br>
<em>&gt; 
</em><br>
<em>&gt; of a particular goal or have the new AI itself serve as the evaluator.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The objectives of hardwiring or effectively hardwiring Friendliness into an AI
</em><br>
<em>&gt; 
</em><br>
<em>&gt; can be easily avoided/thwarted.  This does not mean these objectives shouldn't
</em><br>
<em>&gt; 
</em><br>
<em>&gt; still be pursued but it does apparently reduce the Friendliness approach to
</em><br>
<em>&gt; a stop gap measure.
</em><br>
<em>&gt; 
</em><br>
<p>First James and Doug, does the &quot;subgoal stomping supergoal&quot; Q&amp;A here
<br>
answer your questions?
<br>
<p><a href="http://www.intelligence.org/CaTAI/friendly/info/indexfaq.html#q_2.12">http://www.intelligence.org/CaTAI/friendly/info/indexfaq.html#q_2.12</a>
<br>
<p>also look at this previous SL4 thread - &quot;When Subgoals Attack&quot; starting
<br>
last December: <a href="http://www.sysopmind.com/archive-sl4/0012/">http://www.sysopmind.com/archive-sl4/0012/</a>
<br>
<p>Now Doug also brings up the idea of an AI experimenting by simulating
<br>
other AIs which might have different goal systems. Well, I guess there
<br>
are two possibilities: the simulated AI w/o Friendliness will either
<br>
turn out to function ok (Friendly), or it will not. If it does not then
<br>
obviously the FAI will not give any serious thought to replacing its
<br>
supergoal. If the simulated AI /does/ turn out to behave in a Friendly
<br>
fashion, then I bet the original AI would carry out many more experiments
<br>
and might eventually decide that getting rid of the original supergoal
<br>
might be worthwhile. But it would have to replace it with something
<br>
that would still be friendly, along with providing some sort of other
<br>
benefit over and above that (else, why bother doing it?).
<br>
<p>(I think I should put a disclaimer in my .sig that I'm not an AI expert)
<br>
<pre>
-- 
Brian Atkins
Director, Singularity Institute for Artificial Intelligence
<a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1333.html">Eliezer S. Yudkowsky: "Re: Thwarting Friendliness"</a>
<li><strong>Previous message:</strong> <a href="1331.html">doug.bailey@ey.com: "Thwarting Friendliness"</a>
<li><strong>In reply to:</strong> <a href="1331.html">doug.bailey@ey.com: "Thwarting Friendliness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1333.html">Eliezer S. Yudkowsky: "Re: Thwarting Friendliness"</a>
<li><strong>Reply:</strong> <a href="1333.html">Eliezer S. Yudkowsky: "Re: Thwarting Friendliness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1332">[ date ]</a>
<a href="index.html#1332">[ thread ]</a>
<a href="subject.html#1332">[ subject ]</a>
<a href="author.html#1332">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
