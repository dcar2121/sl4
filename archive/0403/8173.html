<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: AI hardware was 'Singularity Realism'</title>
<meta name="Author" content="Keith Henson (hkhenson@rogers.com)">
<meta name="Subject" content="RE: AI hardware was 'Singularity Realism'">
<meta name="Date" content="2004-03-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: AI hardware was 'Singularity Realism'</h1>
<!-- received="Sat Mar  6 20:53:51 2004" -->
<!-- isoreceived="20040307035351" -->
<!-- sent="Sat, 06 Mar 2004 23:00:18 -0500" -->
<!-- isosent="20040307040018" -->
<!-- name="Keith Henson" -->
<!-- email="hkhenson@rogers.com" -->
<!-- subject="RE: AI hardware was 'Singularity Realism'" -->
<!-- id="5.1.0.14.0.20040306212542.02e53700@pop.bloor.is.net.cable.rogers.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="ALEJKGLLBAKOPAAIKCDNAELIEBAA.peter@optimal.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Keith Henson (<a href="mailto:hkhenson@rogers.com?Subject=RE:%20AI%20hardware%20was%20'Singularity%20Realism'"><em>hkhenson@rogers.com</em></a>)<br>
<strong>Date:</strong> Sat Mar 06 2004 - 21:00:18 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8174.html">EvolverTCB@aol.com: "Re: AI hardware was 'Singularity Realism'"</a>
<li><strong>Previous message:</strong> <a href="8172.html">Ben Goertzel: "RE: AI hardware was 'Singularity Realism'"</a>
<li><strong>In reply to:</strong> <a href="8169.html">Peter Voss: "RE: AI hardware was 'Singularity Realism'"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8178.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<li><strong>Reply:</strong> <a href="8178.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8173">[ date ]</a>
<a href="index.html#8173">[ thread ]</a>
<a href="subject.html#8173">[ subject ]</a>
<a href="author.html#8173">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 03:22 PM 06/03/04 -0800, you wrote:
<br>
<em>&gt;Keith,
</em><br>
<em>&gt;
</em><br>
<em>&gt;I agree with Ben's comments on hardware capability. I personally do not see
</em><br>
<em>&gt;hardware as a show-stopper (even at this stage, and certainly not within
</em><br>
<em>&gt;5-10 years) -- however, I assume that some specialized designs will be
</em><br>
<em>&gt;needed (eg. FPGA).
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;....But I don't think anyone right now has a clear idea of what it would
</em><br>
<em>&gt;take
</em><br>
<em>&gt;to build an AI by other means than a simulation of a human brain.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I think that Ben &amp; I (and some others) can make a reasonable case that we do
</em><br>
<em>&gt;have a fairly clear idea: <a href="http://adaptiveai.com/research/index.htm">http://adaptiveai.com/research/index.htm</a>
</em><br>
<p>You have the right words in there:
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&quot;The most direct path to solving these long-standing problems is to 
<br>
conceptually identify the fundamental characteristics common to all 
<br>
high-level intelligence, and to engineer systems with this basic 
<br>
functionality, in a manner that capitalizes on human and technological 
<br>
strength.&quot;
<br>
<p>But after a quick read, I can't see where you actually have a handle on 
<br>
&quot;fundamental characteristics common to all high-level intelligence.&quot;
<br>
<p>For example, under &quot;Cost and Difficulty&quot; near the bottom you have this 
<br>
statement:
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Having said this, I do believe that very substantial resources will 
<br>
be required to scale up the system to human-level storage and processing 
<br>
capacity.&quot;
<br>
<p>This is partly wrong and (I think) partly right.  Human information storage 
<br>
is *abysmal.*  Cog-Sci and information theory people who have looked at it 
<br>
in dozens of studies come to the dismal conclusion that accessible human 
<br>
memories are formed at 6-8 bits per second.  Over a lifetime it is 
<br>
something like 140 Mbytes.  [I was wrong too, see below.]
<br>
<p>(It is probably a decade since I had a disk that small.  Going through old 
<br>
disks recently I found stuff I had written over ten years ago that I had no 
<br>
memory of having done *at all.*)
<br>
<p>Here is a pointer.  Now that I used the net to look this up, I remember 
<br>
Ralph talking about it at a party.
<br>
<p><a href="http://www.merkle.com/humanMemory.html">http://www.merkle.com/humanMemory.html</a>
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Because experiments by many different experimenters were summarized 
<br>
and analyzed, the results of the analysis are fairly robust; they are 
<br>
insensitive to fine details or specific conditions of one or another 
<br>
experiment. Finally, the amount remembered was divided by the time allotted 
<br>
to memorization to determine the number of bits remembered per second.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;The remarkable result of this work was that human beings remembered 
<br>
very nearly two bits per second under all the experimental conditions. 
<br>
Visual, verbal, musical, or whatever--two bits per second. Continued over a 
<br>
lifetime, this rate of memorization would produce somewhat over 10^^9 bits, 
<br>
or a few hundred megabytes.&quot;
<br>
<p>Human memory almost certainly has the bit rate and capacity it does because 
<br>
that was optimal for our Pleistocene ancestors.  (Less than 2 bits per 
<br>
second might not have been enough to remember your way back to camp.)
<br>
<p>Computers with tens to hundreds of Gbytes of disk are way, way ahead of people.
<br>
<p>[Going into meta mode, I vaguely remembered that people absorb data at a 
<br>
few bits per second.  My memory, as I wrote it down at first, was 6-8 bits 
<br>
per second.  Wanting to cite this (and not being completely certain) I put 
<br>
&quot;human memory&quot; and &quot;bits per second&quot; in Google and Ralph Merkle's paper 
<br>
giving the correct number was the first link.]
<br>
<p>People sometimes *seem* to remember a lot more.  But that's almost 
<br>
certainly the massive processor power filling in the scenes with &quot;stock 
<br>
footage&quot; or making it up out of whole cloth.    Here is a pointer, you can 
<br>
find more with &quot;confabulation&quot; in Google.
<br>
<p>Confabulation -The New England Skeptical Society's Encyclopedia of ...
<br>
... Confabulation. Description: Confabulation is the filling in of gaps in 
<br>
memory
<br>
to make a coherent story. ... Confabulation often occurs during hypnosis. ...
<br>
www.theness.com/encyc/confabulation-encyc.html - 2k - Cached - Similar pages
<br>
<p><em>&gt;Directly engineering human-level AI cognitive ability will require
</em><br>
<em>&gt;substantially less computing power than reverse-engineering the brain or
</em><br>
<em>&gt;uploading.
</em><br>
<p>This *looks* like a valid assumption.  Ralph even supports it at the end of 
<br>
his paper:
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;. . . his estimate of memory capacity suggests that the capabilities 
<br>
of the human brain are more approachable than we had thought. While this 
<br>
might come as a blow to our egos, it suggests that we could build a device 
<br>
with the skills and abilities of a human being with little more hardware 
<br>
than we now have--if only we knew the correct way to organize that hardware.&quot;
<br>
<p>But one thing that 30 years of studying evolution has finally beaten into 
<br>
me is that whatever evolved system you are looking at it is probably much 
<br>
more elegant than you first think it is.  I think Ralph was way off about 
<br>
the processing power a human brain burns, and that Calvin is probably right 
<br>
about singing cortical columns that spread a thought over the brain 
<br>
surface, the importance of sequencing and millisecond scale evolution.
<br>
<p>We probably get away with relatively little information storage because we 
<br>
can take a tiny vague memory and fill it in with lots of processing.
<br>
<p>Keith Henson
<br>
<p>PS.  Maybe AI isn't even a desirable goal.  Can you imagine a world full of 
<br>
know-it-all smart asses even worse than me? :-)
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8174.html">EvolverTCB@aol.com: "Re: AI hardware was 'Singularity Realism'"</a>
<li><strong>Previous message:</strong> <a href="8172.html">Ben Goertzel: "RE: AI hardware was 'Singularity Realism'"</a>
<li><strong>In reply to:</strong> <a href="8169.html">Peter Voss: "RE: AI hardware was 'Singularity Realism'"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8178.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<li><strong>Reply:</strong> <a href="8178.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8173">[ date ]</a>
<a href="index.html#8173">[ thread ]</a>
<a href="subject.html#8173">[ subject ]</a>
<a href="author.html#8173">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
