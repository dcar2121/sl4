<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI hardware was 'Singularity Realism'</title>
<meta name="Author" content="Keith Henson (hkhenson@rogers.com)">
<meta name="Subject" content="Re: AI hardware was 'Singularity Realism'">
<meta name="Date" content="2004-03-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI hardware was 'Singularity Realism'</h1>
<!-- received="Mon Mar  8 19:03:03 2004" -->
<!-- isoreceived="20040309020303" -->
<!-- sent="Mon, 08 Mar 2004 21:10:07 -0500" -->
<!-- isosent="20040309021007" -->
<!-- name="Keith Henson" -->
<!-- email="hkhenson@rogers.com" -->
<!-- subject="Re: AI hardware was 'Singularity Realism'" -->
<!-- id="5.1.0.14.0.20040308202759.03b4c050@pop.bloor.is.net.cable.rogers.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="CD65996A-70D5-11D8-BF2F-003065C9EC00@ceruleansystems.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Keith Henson (<a href="mailto:hkhenson@rogers.com?Subject=Re:%20AI%20hardware%20was%20'Singularity%20Realism'"><em>hkhenson@rogers.com</em></a>)<br>
<strong>Date:</strong> Mon Mar 08 2004 - 19:10:07 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8201.html">wiggin01@ftml.net: "RE: There are few books on AI"</a>
<li><strong>Previous message:</strong> <a href="8199.html">Joseph W. Foley: "RE: There are few books on AI"</a>
<li><strong>In reply to:</strong> <a href="8191.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8202.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<li><strong>Reply:</strong> <a href="8202.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8200">[ date ]</a>
<a href="index.html#8200">[ thread ]</a>
<a href="subject.html#8200">[ subject ]</a>
<a href="author.html#8200">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 11:54 PM 07/03/04 -0800,  Andrew wrote:
<br>
<p><em>&gt;On Mar 7, 2004, at 10:14 PM, Keith Henson wrote:
</em><br>
<p>snip
<br>
<p><em>&gt;&gt;I don't think memory is going to be a problem for AI.  I think the main 
</em><br>
<em>&gt;&gt;problem is going to be trying to trade off memory against limited 
</em><br>
<em>&gt;&gt;processor power.
</em><br>
<em>&gt;
</em><br>
<em>&gt;What do you need all that processor for?
</em><br>
<em>&gt;
</em><br>
<em>&gt;The real hardware bottleneck is actually memory latency, and it is a 
</em><br>
<em>&gt;pretty severe bottleneck.  Doesn't matter how fast your processor is if it 
</em><br>
<em>&gt;is starved for data.  This has been discussed repeatedly in the archives 
</em><br>
<em>&gt;of this list.  Even the best RAM subsystems found in commodity systems are 
</em><br>
<em>&gt;treading on inadequate, never mind using disk swap.
</em><br>
<p>I am *assuming* you are going to have gobs of processors.  But processors 
<br>
are relatively expensive compared to memory.
<br>
<p><em>&gt;&gt;You might be right.  I suspect though that you are misled.  I think that 
</em><br>
<em>&gt;&gt;the data and processors are mixed together in the brain in a way that 
</em><br>
<em>&gt;&gt;will be difficult to simulate in serial processors--assuming this is 
</em><br>
<em>&gt;&gt;actually needed for AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Serial == Parallel.  Six of one, half dozen of the other.
</em><br>
<em>&gt;Mathematically equivalent in every way.
</em><br>
<p>I don't think that a billion GHz processor is in the cards *ever* where a 
<br>
billion one GHz processors are certainly possible.  (A GHz is a nanosecond, 
<br>
in a nanosecond light goes a foot.  So a G-squared processor would have to 
<br>
have elements spaced 1/3 of a nanometer apart talking at light 
<br>
speed.  Point being that at some speed, you just *can't* go faster.)
<br>
<p><em>&gt;The hard part of the simulation is getting the kind of effective latency 
</em><br>
<em>&gt;that the brain gets when channeling everything through a small number of 
</em><br>
<em>&gt;memory busses to a small number of processors.  The brain may be slow, but 
</em><br>
<em>&gt;everything is local.  The brain &quot;processors&quot; can reference more objects in 
</em><br>
<em>&gt;a second in aggregate than the fastest computers we make.
</em><br>
<p>Exactly.
<br>
<p>snip
<br>
<p><em>&gt;&gt;If you have a couple of hundred million processors, which I think is a 
</em><br>
<em>&gt;&gt;good number to consider, then each can have a few hundred bytes without 
</em><br>
<em>&gt;&gt;having to bother with compression.
</em><br>
<em>&gt;
</em><br>
<em>&gt;This is completely missing the point.  At that resolution, there is no 
</em><br>
<em>&gt;compression.  But how do you store a megabyte of information efficiently 
</em><br>
<em>&gt;in such a system?
</em><br>
<p>Bit at a time.  :-)
<br>
<p><em>&gt;There are many layers, and it isn't like you would want to do bit-slicing 
</em><br>
<em>&gt;anyway.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;I think it is worth noting that the closest kind of projects to AI like 
</em><br>
<em>&gt;&gt;the Google search engine *are* massively parallel.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Again, parallel == serial.  How &quot;massively parallel&quot; an application can be 
</em><br>
<em>&gt;depends on the memory latency requirements between nodes.
</em><br>
<p>It also depends a *lot* on the kind of problem you are trying to solve.  If 
<br>
you are pattern matching, you can *broadcast* what you are looking for.
<br>
<p>About a decade ago I was involved with a massive office automation 
<br>
project.  The most primitive objects were scanned pages.  The server burned 
<br>
the image data into CD ROM and stored them in jukeboxes.  It kept the most 
<br>
recent 30 Gbytes besides distributing the images to a push off the end file 
<br>
in an assigned workstation.  The jukebox was a reliable source of 
<br>
information, but the performance really sucked (20 sec to load).  I wrote a 
<br>
little Perl script that ran in each workstation, listening to a common 
<br>
port.  When a workstation requested a page image, the server looked for it 
<br>
in its local disk store.  If it was not there, it pinged all the 
<br>
workstations and got the page image from the first one to respond.  If no 
<br>
workstation responded, it loaded the file from a jukebox.  Nineteen times 
<br>
out of twenty the needed file was still on a work station--which made a 
<br>
huge performance improvement.
<br>
<p><em>&gt;For a lot of codes (including mine, unfortunately) you cannot do useful 
</em><br>
<em>&gt;parallelization unless you have an inter-node latency on the order of &lt;1us.
</em><br>
<p>Biologic brains are at least three orders of magnitude slower and still 
<br>
work.  How are they doing it?
<br>
<p><em>&gt;This has been thoroughly discussed on supercomputing lists, and is also 
</em><br>
<em>&gt;the reason huge ccNUMA supercomputers can *still* school a commodity 
</em><br>
<em>&gt;computing cluster for many apps.
</em><br>
<em>&gt;
</em><br>
<em>&gt;This reflects the very unbalanced architecture of modern commodity 
</em><br>
<em>&gt;computers.  Lots of processing power, but extremely poor at working on 
</em><br>
<em>&gt;very large fine-grained data structures because of weak, slow memory.
</em><br>
<em>&gt;True supercomputers don't have this weakness and is why a Cray at 800MHz 
</em><br>
<em>&gt;can school a Pentium 4 by an order of magnitude on some codes, but they 
</em><br>
<em>&gt;also cost a small fortune.  The brain, for all its weaknesses, is a 
</em><br>
<em>&gt;well-balanced computing architecture.
</em><br>
<p>Right.  My guess is that it is *much* richer in processors.  What little 
<br>
memory it does have is close to the processors.  You might find it 
<br>
interesting to muse on a world in which processors cost about the same as a 
<br>
few hundred bytes of memory.  Would that cause you to rethink the design?
<br>
<p>Keith Henson
<br>
<p><em>&gt;j. andrew rogers
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8201.html">wiggin01@ftml.net: "RE: There are few books on AI"</a>
<li><strong>Previous message:</strong> <a href="8199.html">Joseph W. Foley: "RE: There are few books on AI"</a>
<li><strong>In reply to:</strong> <a href="8191.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8202.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<li><strong>Reply:</strong> <a href="8202.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8200">[ date ]</a>
<a href="index.html#8200">[ thread ]</a>
<a href="subject.html#8200">[ subject ]</a>
<a href="author.html#8200">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
