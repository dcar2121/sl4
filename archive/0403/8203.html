<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI hardware was 'Singularity Realism'</title>
<meta name="Author" content="Keith Henson (hkhenson@rogers.com)">
<meta name="Subject" content="Re: AI hardware was 'Singularity Realism'">
<meta name="Date" content="2004-03-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI hardware was 'Singularity Realism'</h1>
<!-- received="Mon Mar  8 23:32:17 2004" -->
<!-- isoreceived="20040309063217" -->
<!-- sent="Tue, 09 Mar 2004 01:40:20 -0500" -->
<!-- isosent="20040309064020" -->
<!-- name="Keith Henson" -->
<!-- email="hkhenson@rogers.com" -->
<!-- subject="Re: AI hardware was 'Singularity Realism'" -->
<!-- id="5.1.0.14.0.20040309010802.032c0c50@pop.bloor.is.net.cable.rogers.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="B984E038-7185-11D8-A6DC-003065C9EC00@ceruleansystems.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Keith Henson (<a href="mailto:hkhenson@rogers.com?Subject=Re:%20AI%20hardware%20was%20'Singularity%20Realism'"><em>hkhenson@rogers.com</em></a>)<br>
<strong>Date:</strong> Mon Mar 08 2004 - 23:40:20 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8204.html">Keith Henson: "RE: There are few books on AI"</a>
<li><strong>Previous message:</strong> <a href="8202.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<li><strong>In reply to:</strong> <a href="8202.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8207.html">Philip Sutton: "Re: Unity of thinking &amp; the optimum physical size of thinking unit"</a>
<li><strong>Reply:</strong> <a href="8207.html">Philip Sutton: "Re: Unity of thinking &amp; the optimum physical size of thinking unit"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8203">[ date ]</a>
<a href="index.html#8203">[ thread ]</a>
<a href="subject.html#8203">[ subject ]</a>
<a href="author.html#8203">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 08:53 PM 08/03/04 -0800, you wrote:
<br>
<em>&gt;On Mar 8, 2004, at 6:10 PM, Keith Henson wrote:
</em><br>
<em>&gt;&gt;Biologic brains are at least three orders of magnitude slower and still 
</em><br>
<em>&gt;&gt;work.  How are they doing it?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Because they can keep their processors fed with data.  That's all memory 
</em><br>
<em>&gt;latency really measures, how many objects can be referenced per second.  A 
</em><br>
<em>&gt;human brain typically references roughly three orders of magnitude more 
</em><br>
<em>&gt;objects per second than our fastest processors.  For large fine-grained 
</em><br>
<em>&gt;data structures (like the brain), memory latency (or if you prefer, object 
</em><br>
<em>&gt;reference throughput) *is* the computational limit, not IPC and clock speed.
</em><br>
<p>Amazing how close we have come to roughly the same numbers from different 
<br>
directions.  Brains are faster at referencing by a factor of a thousand yet 
<br>
cycle at best in milliseconds, a clock rate a million slower than the 
<br>
computers we outdistance.  Memory latency for computers is slower than the 
<br>
clock rate, but not that much, but somehow the brain copes.  My take on it 
<br>
(from Calvin) is that the brain puts on the order of 100 million 
<br>
processors, each with a little local memory, on this kind of work.  The 
<br>
performance numbers work out very closely.
<br>
<p><em>&gt;People are not used to thinking of things this way, but memory latency 
</em><br>
<em>&gt;pretty much defines the computational limits of our universe.  It isn't as 
</em><br>
<em>&gt;obvious as Instructions Per Second or the amount of memory you have, but 
</em><br>
<em>&gt;it is a lot more important if you have to confine yourself to finite time 
</em><br>
<em>&gt;horizons.
</em><br>
<p>Yep.  And furthermore, the really poor showing of a human brain at a 
<br>
*sequential* math task is understandable.  We can do remarkable feats of 
<br>
recognition that a computer would spend years on in a flash, but we are 
<br>
slow as a computer with a multi millisecond clock when we try to multiply 
<br>
large numbers in our heads.  This is *exactly* the same limitations we find 
<br>
on parallel machines.  They are understandably poor on a sequential task 
<br>
that can't use parallel processing.
<br>
<p><em>&gt;Study the consequences of finite information propagation speeds (of which 
</em><br>
<em>&gt;memory latency is an example) on computational theory.  It is the gravest 
</em><br>
<em>&gt;restriction on intelligence that this universe appears to have.  Perhaps 
</em><br>
<em>&gt;not insignificantly, it is the only computational parameter in which the 
</em><br>
<em>&gt;brain *clearly* is orders of magnitude superior to the silicon we can 
</em><br>
<em>&gt;manufacture today.
</em><br>
<p>I have been talking about this point or the closely related point of 
<br>
keeping a brain in synch for years and people just brush it off.  I really 
<br>
don't see how parts of your brain could be too many cycles delayed and have 
<br>
any unity of thinking.  Thus there is some size that computational &quot;stuff&quot; 
<br>
would organize into because stuff further from the center would not 
<br>
usefully contribute to thinking.  I think this factor may underlie a 
<br>
universal ethics and morality.  (Value of matter declining with distance.)
<br>
<p>Keith Henson
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8204.html">Keith Henson: "RE: There are few books on AI"</a>
<li><strong>Previous message:</strong> <a href="8202.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<li><strong>In reply to:</strong> <a href="8202.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8207.html">Philip Sutton: "Re: Unity of thinking &amp; the optimum physical size of thinking unit"</a>
<li><strong>Reply:</strong> <a href="8207.html">Philip Sutton: "Re: Unity of thinking &amp; the optimum physical size of thinking unit"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8203">[ date ]</a>
<a href="index.html#8203">[ thread ]</a>
<a href="subject.html#8203">[ subject ]</a>
<a href="author.html#8203">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
