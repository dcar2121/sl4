<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=WINDOWS-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: De-Anthropomorphizing SL3 to SL4.</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: De-Anthropomorphizing SL3 to SL4.">
<meta name="Date" content="2004-03-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: De-Anthropomorphizing SL3 to SL4.</h1>
<!-- received="Mon Mar 15 22:49:53 2004" -->
<!-- isoreceived="20040316054953" -->
<!-- sent="Mon, 15 Mar 2004 21:49:46 -0800" -->
<!-- isosent="20040316054946" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: De-Anthropomorphizing SL3 to SL4." -->
<!-- id="BB0C74BA-770D-11D8-B794-000A95B1AFDE@objectent.com" -->
<!-- charset="WINDOWS-1252" -->
<!-- inreplyto="4055F2EC.3000003@acceleratingfuture.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20De-Anthropomorphizing%20SL3%20to%20SL4."><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Mon Mar 15 2004 - 22:49:46 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8269.html">Samantha Atkins: "Re: 'Singularity Realism' - A few thoughts"</a>
<li><strong>Previous message:</strong> <a href="8267.html">Mark Waser: "Re: 'Singularity Realism' - A few thoughts"</a>
<li><strong>In reply to:</strong> <a href="8256.html">Michael Anissimov: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8281.html">Thomas Buckner: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<li><strong>Reply:</strong> <a href="8281.html">Thomas Buckner: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8268">[ date ]</a>
<a href="index.html#8268">[ thread ]</a>
<a href="subject.html#8268">[ subject ]</a>
<a href="author.html#8268">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Mar 15, 2004, at 10:16 AM, Michael Anissimov wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; &quot;In 1993, a writer named Vernor Vinge gave a talk to NASA 
</em><br>
<em>&gt; &lt;<a href="http://singularity.manilasites.com/stories/storyReader$35">http://singularity.manilasites.com/stories/storyReader$35</a>&gt;, in which 
</em><br>
<em>&gt; he described the architecture of an event he called the “Singularity”, 
</em><br>
<em>&gt; which is identical in every feature to McKenna’s Eschaton.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; it makes me think that they're talking about the same thing. The 
</em><br>
<em>&gt; Singularity is radically different than McKenna's Eschaton. When 
</em><br>
<em>&gt; people like Pesce say things like &quot;the bios might not be prepared for 
</em><br>
<em>&gt; the emergence of the logos&quot;, I tend to visualize them visualizing some 
</em><br>
<em>&gt; sort of societal/psychic chaos rather than outright destruction at the 
</em><br>
<em>&gt; hands of a paperclip SI. I might be wrong about this, but it's the 
</em><br>
<em>&gt; impression I've gotten from reading some of McKenna's articles. Humans 
</em><br>
<em>&gt; are fragile creatures; we require a very precise environment to 
</em><br>
<em>&gt; survive, and in the absence of an SI that specifically cares about 
</em><br>
<em>&gt; preserving it, we would probably be swept aside (read: extinct) by the 
</em><br>
<em>&gt; first SI(s) with goals that meant the rearrangement of matter on any 
</em><br>
<em>&gt; appreciable scale. &quot;Care for humans&quot; is not a quality we should expect 
</em><br>
<em>&gt; to emerge in an arbitrary SI of a certain level of intelligence.
</em><br>
<p>There will be plenty of societal/psychic chaos in even a Friendly SI 
<br>
scenario.  The logos is the Word, the primary Intelligence.  The bios, 
<br>
humanity as evolved, will not be prepared.   This is deeper than 
<br>
societal/psychic chaos.  It is a fundamental shift to a state we are 
<br>
incapable of imagining.
<br>
<p><em>&gt;
</em><br>
<em>&gt; When you say &quot;will this new emergent complexity be friendly in any 
</em><br>
<em>&gt; sense we can fathom, or will we be consumed or destroyed by it?&quot;, I 
</em><br>
<em>&gt; think the answer lies in the initial motivations and goal structure we 
</em><br>
<em>&gt; instill within the first seed AI. As Nick Bostrom writes in 
</em><br>
<em>&gt; <a href="http://www.nickbostrom.com/ethics/ai.html">http://www.nickbostrom.com/ethics/ai.html</a>,
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;The option to defer many decisions to the superintelligence does not 
</em><br>
<em>&gt; mean that we can afford to be complacent in how we construct the 
</em><br>
<em>&gt; superintelligence. On the contrary, the setting up of initial 
</em><br>
<em>&gt; conditions, and in particular the selection of a top-level goal for 
</em><br>
<em>&gt; the superintelligence, is of the utmost importance. Our entire future 
</em><br>
<em>&gt; may hinge on how we solve these problems.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; The *first* seed AI seems to be especially important because it would 
</em><br>
<em>&gt; likely have cognitive hardware advantages that allow it to bootstrap 
</em><br>
<em>&gt; to superintelligence before anyone or anything else. This means that 
</em><br>
<em>&gt; the entire human race will be at the mercy of whatever goal system or 
</em><br>
<em>&gt; philosophy this first seed AI has after many iterations of recursive 
</em><br>
<em>&gt; self-improvement. The information pattern that determines the fate of 
</em><br>
<em>&gt; humanity after the Singularity is not be within us as individuals, or 
</em><br>
<em>&gt; predetermined by meta-evolution, or encoded into the Timewave; it will 
</em><br>
<em>&gt; be in the source code of the first recursive self-improver. If some 
</em><br>
<em>&gt; idiot walks into the AI lab just as hard takeoff is about to commence, 
</em><br>
<em>&gt; and spills coffee on the AI's mainframe, driving it a bit nutty, then 
</em><br>
<em>&gt; the whole of humanity might be destroyed by that tiny mistake. Also, 
</em><br>
<em>&gt; novel events prior to the Singularity are liable to have negligible 
</em><br>
<em>&gt; impact upon it. If someone has a really great trip where they 
</em><br>
<em>&gt; visualize all sorts of wonderful worlds, shapes, and entities, it will 
</em><br>
<em>&gt; have absolutely no impact on whether humanity survives the 
</em><br>
<em>&gt; Singularity. I have a feeling that Pesce and others would be turned 
</em><br>
<em>&gt; off by this interpretation of the Singularity because it is so 
</em><br>
<em>&gt; impersonal and arbitrary-seeming.
</em><br>
<em>&gt;
</em><br>
<p>I am still not convinced that the &quot;source code&quot; of the first recursive 
<br>
self-improver is in any meaningful sense inviolate and thus capable of 
<br>
protecting us.  In the end the SI will either consider humans as 
<br>
interesting sentients that its chosen morality leads it to protect and 
<br>
preserve or it will not.  I find it odd that we who find it so 
<br>
difficult to devise a morality for ourselves or to write more than the 
<br>
bare seeds of an SI would believe that we can determine the top level 
<br>
goal and thus the root of SI morality indefinitely.    I think it would 
<br>
be more realistic to decide whether a vastly less bounded intelligence 
<br>
is such a good thing in our  humble opinion that we are willing to risk 
<br>
everything on its creation even if it means our doom.    This is not a 
<br>
pleasant prospect but I believe it is more realistic.
<br>
<p>Humans can barely predict whether a simple word processor will function 
<br>
as it requirements say it should.   How amusing that we thing we can 
<br>
predictably bound that which will think millions of times faster than 
<br>
ourselves
<br>
<p><em>&gt; So when Pesce says stuff like,
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;So we have three waves, biological, linguistic, and
</em><br>
<em>&gt; technological, which are rapidly moving to
</em><br>
<em>&gt; concrescence, and on their way, as they interact,
</em><br>
<em>&gt; produce such a tsunami of novelty as has never before
</em><br>
<em>&gt; been experienced in the history of this planet.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; or
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Anything you see, anywhere, animate, or inanimate, will have within it
</em><br>
<em>&gt; the capacity to be entirely transformed by a rearrangement of its atoms
</em><br>
<em>&gt; into another form, a form which obeys the dictates of linguistic 
</em><br>
<em>&gt; intent.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; it makes me feel like he has a false sense of hope, that the 
</em><br>
<em>&gt; Singularity is more about embarking on a successful diplomatic 
</em><br>
<em>&gt; relationship with the self-transforming machine elves, rather than 
</em><br>
<em>&gt; solving a highly technical issue involving the design of AI goal 
</em><br>
<em>&gt; systems.  I doubt that Pesce realizes the forces responsible for the 
</em><br>
<em>&gt; rise of complexity and novelty in human society correspond to an 
</em><br>
<em>&gt; immensely sophisticated set of cognitive tools unique to Homo sapiens, 
</em><br>
<em>&gt; not to any underlying feature of the universe.  Fail to pass these 
</em><br>
<em>&gt; tools onto the next stage, and the next stage will fail to carry on 
</em><br>
<em>&gt; the tradition of increasing novelty.
</em><br>
<em>&gt;
</em><br>
<p>Since the SAI will recursively self-improve all aspects of itself I 
<br>
would find it remarkable if it never bothered to critically examine its 
<br>
own goal systems, especially considering the fallible beings which laid 
<br>
down those goals.    Whether after examination it ends up with an 
<br>
effective goal system that preserves humans or not is really anyone's 
<br>
guess.     My guess is that any sufficiently intelligent being will 
<br>
sooner or later come to greatly value benevolent co-existence with 
<br>
other reason-capable beings even with great differences in ability.  
<br>
But I do not know if it will come to that state soon enough for 
<br>
humanity to survive.  Sometimes I believe we can plant the right seeds 
<br>
to make it likely.  Sometimes I am not so sure.
<br>
<p><p><em>&gt; The vast majority of biological complexity on this planet will be 
</em><br>
<em>&gt; irrelevant to the initial Singularity event, because it will play no 
</em><br>
<em>&gt; part in building the first seed AI, except insofar as it indirectly 
</em><br>
<em>&gt; gave rise to humanity.  Linguistic; irrelevant except insofar as the 
</em><br>
<em>&gt; language the first AGI designers are using to plan their design and 
</em><br>
<em>&gt; launch.
</em><br>
<p>Linguistic in the sense used by McKenna refers to a lot more than just 
<br>
actual language.
<br>
<p><p><em>&gt; Technological; also, only a small portion of the technological 
</em><br>
<em>&gt; complexity on our planet today will be used to create transhuman 
</em><br>
<em>&gt; intelligence.  The *simplest constructable AIs* are likely to have 
</em><br>
<em>&gt; correspondingly simple goal systems; so the *easiest* AIs to launch 
</em><br>
<em>&gt; into recursive self-improvement are also likely to be the ones 
</em><br>
<em>&gt; bringing on the most boring arrangements of matter, such as multitudes 
</em><br>
<em>&gt; of paper clips.  Simple, boring, cruel, easy.
</em><br>
<p>By the same logic the earth should have nothing but one-celled 
<br>
creatures on it!   Seriously, the likelihood of a paper-clip AI taking 
<br>
over the universe is nearly non-existent.
<br>
<p><em>&gt; Given a *benevolent* Singularity, yes, biological, linguistic and 
</em><br>
<em>&gt; technological forces might indeed intertwine with one another and 
</em><br>
<em>&gt; produce a &quot;tsunami of novelty&quot; in much the way that he describes, but 
</em><br>
<em>&gt; it seems to be that he's regarding this tsunami of novelty as 
</em><br>
<em>&gt; basically coming for free.  &quot;Novelty&quot;, in the sense that Terence 
</em><br>
<em>&gt; McKenna uses it, has an unambiguously positive connotation.
</em><br>
<em>&gt;
</em><br>
<p>Hardly for free as he seems to call for the transformation of ourselves 
<br>
in order to give birth to this possibility.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8269.html">Samantha Atkins: "Re: 'Singularity Realism' - A few thoughts"</a>
<li><strong>Previous message:</strong> <a href="8267.html">Mark Waser: "Re: 'Singularity Realism' - A few thoughts"</a>
<li><strong>In reply to:</strong> <a href="8256.html">Michael Anissimov: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8281.html">Thomas Buckner: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<li><strong>Reply:</strong> <a href="8281.html">Thomas Buckner: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8268">[ date ]</a>
<a href="index.html#8268">[ thread ]</a>
<a href="subject.html#8268">[ subject ]</a>
<a href="author.html#8268">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
