<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: De-Anthropomorphizing SL3 to SL4.</title>
<meta name="Author" content="Michael Anissimov (michael@acceleratingfuture.com)">
<meta name="Subject" content="Re: De-Anthropomorphizing SL3 to SL4.">
<meta name="Date" content="2004-03-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: De-Anthropomorphizing SL3 to SL4.</h1>
<!-- received="Wed Mar 17 08:41:29 2004" -->
<!-- isoreceived="20040317154129" -->
<!-- sent="Wed, 17 Mar 2004 07:41:25 -0800" -->
<!-- isosent="20040317154125" -->
<!-- name="Michael Anissimov" -->
<!-- email="michael@acceleratingfuture.com" -->
<!-- subject="Re: De-Anthropomorphizing SL3 to SL4." -->
<!-- id="405871A5.6050809@acceleratingfuture.com" -->
<!-- charset="windows-1252" -->
<!-- inreplyto="003601c40abe$1d42fd50$6501a8c0@mra02" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Anissimov (<a href="mailto:michael@acceleratingfuture.com?Subject=Re:%20De-Anthropomorphizing%20SL3%20to%20SL4."><em>michael@acceleratingfuture.com</em></a>)<br>
<strong>Date:</strong> Wed Mar 17 2004 - 08:41:25 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8281.html">Thomas Buckner: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<li><strong>Previous message:</strong> <a href="8279.html">Samantha Atkins: "Re: 'Singularity Realism' - A few thoughts"</a>
<li><strong>In reply to:</strong> <a href="8257.html">Michael Roy Ames: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8282.html">Thomas Buckner: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<li><strong>Reply:</strong> <a href="8282.html">Thomas Buckner: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8280">[ date ]</a>
<a href="index.html#8280">[ thread ]</a>
<a href="subject.html#8280">[ subject ]</a>
<a href="author.html#8280">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michael, thanks for pointing this out.  Of course, the spilling of 
<br>
coffee on a mainframe causing an AI to go nutty sounds like a plot from 
<br>
a really bad sci-fi novel.  The truly scary thing about AGI creation is 
<br>
that everyone can seem perfectly alright until hard takeoff, at which 
<br>
point things could go horribly wrong.  Unlike the vast majority of past 
<br>
technological advancements, AGI does not seem like a &quot;free win&quot; - in 
<br>
fact, the *default* scenario may be a loss (for everyone).  I still 
<br>
think that the vast majority of people who consider the ethics of 
<br>
advanced AI are worried about one of two things;
<br>
<p>1)  anthropomorphic goals emerging spontaneously within the AI
<br>
2)  mechanomorphic (like a gun) or anthropomorphic (like a slave) 
<br>
exploitation of AIs by human agents
<br>
<p>When the real problem (as many people on this list know) is
<br>
<p>3) abstract failures of Friendliness within a very foreign and 
<br>
difficult-to-imagine goal system structure
<br>
<p>Michael Anissimov
<br>
<p>Michael Roy Ames wrote:
<br>
<p><em>&gt;Michael Anissimov,
</em><br>
<em>&gt;
</em><br>
<em>&gt;I enjoyed reading your comments about McKenna and Pesce.
</em><br>
<em>&gt;
</em><br>
<em>&gt;In reference to:
</em><br>
<em>&gt;  &quot;If some idiot walks into the AI lab
</em><br>
<em>&gt;  just as hard takeoff is about to
</em><br>
<em>&gt;  commence, and spills coffee on the
</em><br>
<em>&gt;  AI's mainframe, driving it a bit nutty,
</em><br>
<em>&gt;  then the whole of humanity might be
</em><br>
<em>&gt;  destroyed by that tiny mistake.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt;This is a poorly imagined scenario.  An accident so overt and random would
</em><br>
<em>&gt;not perturb a properly constructed AI.  It might shut down the hardware,
</em><br>
<em>&gt;thus terminating the instantiation, but could not perturb it such that it
</em><br>
<em>&gt;became 'a bit nutty'.  For a small perturbation to make an AI &quot;go bad&quot; it
</em><br>
<em>&gt;would have to be very, very poorly designed - it would have to be nutty to
</em><br>
<em>&gt;begin with.  A poorly designed AI is something to be avoided like the
</em><br>
<em>&gt;plague... the inoculation against such a plague being a well constructed
</em><br>
<em>&gt;Friendly AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt;To clarify, let me suggest an alternate imaginary scenario:
</em><br>
<em>&gt;  &quot;If some idiot learns just enough about
</em><br>
<em>&gt;  AI theory to construct a working prototype,
</em><br>
<em>&gt;  but not enough to ensure it remains
</em><br>
<em>&gt;  friendly to other beings, and humans
</em><br>
<em>&gt;  in particular, then the whole of
</em><br>
<em>&gt;  humanity may be destroyed by that
</em><br>
<em>&gt;  person's well meaning, but ultimately
</em><br>
<em>&gt;  disastrous efforts.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Michael Roy Ames
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8281.html">Thomas Buckner: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<li><strong>Previous message:</strong> <a href="8279.html">Samantha Atkins: "Re: 'Singularity Realism' - A few thoughts"</a>
<li><strong>In reply to:</strong> <a href="8257.html">Michael Roy Ames: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8282.html">Thomas Buckner: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<li><strong>Reply:</strong> <a href="8282.html">Thomas Buckner: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8280">[ date ]</a>
<a href="index.html#8280">[ thread ]</a>
<a href="subject.html#8280">[ subject ]</a>
<a href="author.html#8280">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
