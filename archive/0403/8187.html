<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI hardware was 'Singularity Realism'</title>
<meta name="Author" content="Keith Henson (hkhenson@rogers.com)">
<meta name="Subject" content="Re: AI hardware was 'Singularity Realism'">
<meta name="Date" content="2004-03-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI hardware was 'Singularity Realism'</h1>
<!-- received="Sun Mar  7 23:07:57 2004" -->
<!-- isoreceived="20040308060757" -->
<!-- sent="Mon, 08 Mar 2004 01:14:42 -0500" -->
<!-- isosent="20040308061442" -->
<!-- name="Keith Henson" -->
<!-- email="hkhenson@rogers.com" -->
<!-- subject="Re: AI hardware was 'Singularity Realism'" -->
<!-- id="5.1.0.14.0.20040308003238.02e44ea0@pop.bloor.is.net.cable.rogers.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="B2C46667-70B0-11D8-BF2F-003065C9EC00@ceruleansystems.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Keith Henson (<a href="mailto:hkhenson@rogers.com?Subject=Re:%20AI%20hardware%20was%20'Singularity%20Realism'"><em>hkhenson@rogers.com</em></a>)<br>
<strong>Date:</strong> Sun Mar 07 2004 - 23:14:42 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8188.html">EvolverTCB@aol.com: "Re: AI hardware was 'Singularity Realism'"</a>
<li><strong>Previous message:</strong> <a href="8186.html">Daniel Alexandre: "There are few books on AI"</a>
<li><strong>In reply to:</strong> <a href="8185.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8191.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<li><strong>Reply:</strong> <a href="8191.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8187">[ date ]</a>
<a href="index.html#8187">[ thread ]</a>
<a href="subject.html#8187">[ subject ]</a>
<a href="author.html#8187">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 07:28 PM 07/03/04 -0800, you wrote:
<br>
<em>&gt;On Mar 7, 2004, at 5:07 PM, Keith Henson wrote:
</em><br>
<em>&gt;&gt;At 10:11 AM 07/03/04 -0800, andrew wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;You cannot substitute time complexity for space complexity, and space 
</em><br>
<em>&gt;&gt;&gt;complexity generally defines intelligence.  You can substitute space 
</em><br>
<em>&gt;&gt;&gt;complexity for time complexity, but not the other way around.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;I can't parse this.  It seems likely that you have something here that is 
</em><br>
<em>&gt;&gt;worth understanding.  Can you try again from a bit lower level?
</em><br>
<em>&gt;
</em><br>
<em>&gt;Sure.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Time complexity is just the time it takes a given algorithm to run, in 
</em><br>
<em>&gt;terms of how it scales.  I'm sure you already got that.
</em><br>
<em>&gt;
</em><br>
<em>&gt;A finite machine can be defined in terms of the size of the largest 
</em><br>
<em>&gt;algorithm that can be expressed on that machine, its intrinsic Kolmogorov 
</em><br>
<em>&gt;complexity.  This roughly maps to &quot;memory&quot; for standard silicon, hence 
</em><br>
<em>&gt;&quot;space complexity&quot; i.e. the amount of memory mathematically necessary to 
</em><br>
<em>&gt;run a particular algorithm.
</em><br>
<em>&gt;
</em><br>
<em>&gt;No algorithm with a Kolmogorov complexity greater than a given machine can 
</em><br>
<em>&gt;be expressed on said machine.  To put it another way, a machine with 
</em><br>
<em>&gt;infinite execution speed is incapable of computing algorithms that do not 
</em><br>
<em>&gt;fit within the space complexity of the machine.  On the other hand, you 
</em><br>
<em>&gt;can improve time complexity by using the excess space capacity of a 
</em><br>
<em>&gt;machine to implement faster but less space efficient algorithms, e.g. 
</em><br>
<em>&gt;using a giant lookup table (O(1)) rather than computing values 
</em><br>
<em>&gt;mathematically, which could be O(n) or worse.  This is also why quantum 
</em><br>
<em>&gt;computers aren't as important to AI as some people seem to think they 
</em><br>
<em>&gt;are.  They don't let us run an entirely new class of algorithms, they only 
</em><br>
<em>&gt;let existing algorithms run faster.  &quot;Faster&quot; doesn't mean &quot;smarter&quot; in 
</em><br>
<em>&gt;the abstract, though in practice speed is admittedly important as well if 
</em><br>
<em>&gt;intelligence is to be useful.
</em><br>
<em>&gt;
</em><br>
<em>&gt;High time complexity makes getting an answer from a machine intractable; 
</em><br>
<em>&gt;if you wait long enough, you'll get an answer.  High space complexity 
</em><br>
<em>&gt;makes getting an answer from a machine impossible because the pattern is 
</em><br>
<em>&gt;incapable of being expressed or perceived.  Big qualitative difference, 
</em><br>
<em>&gt;that.  The limits of intelligence expressible on any machine is generally 
</em><br>
<em>&gt;defined by the Kolmogorov complexity of the machine, since this is also 
</em><br>
<em>&gt;defines the limit of both expression and inference.  A machine will never 
</em><br>
<em>&gt;be able to perceive a high-order pattern that is more complex than the 
</em><br>
<em>&gt;machine itself.
</em><br>
<em>&gt;
</em><br>
<em>&gt;The technical details are a bit more complicated than this, but the point 
</em><br>
<em>&gt;about time complexity not being a substitute for space complexity should 
</em><br>
<em>&gt;be clearer now.  Faster machines make some operations faster, nothing 
</em><br>
<em>&gt;more.  Bigger machines (in the memory sense), however, usually make 
</em><br>
<em>&gt;smarter machines no matter what the speed.
</em><br>
<p>Human memory is on the order of a few hundred Mbytes (figure a Mbyte for a 
<br>
thick book).  My dinky home computer has half a Gbyte on it, any serious 
<br>
machine is going to have tens of Gbytes of RAM and thousands of Gbytes of 
<br>
virtual memory extending into the disk.
<br>
<p>By memory measurement, my home computer has the potential to be something 
<br>
like 500 times as smart as I am (counting the disk).
<br>
<p>I don't think memory is going to be a problem for AI.  I think the main 
<br>
problem is going to be trying to trade off memory against limited processor 
<br>
power.
<br>
<p><em>&gt;&gt;&gt;I think the brain storage mechanism is far more efficient in an 
</em><br>
<em>&gt;&gt;&gt;information theoretic sense than I think a lot of people think it is.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;I hope you are right, because I don't like the conclusions of Thomas 
</em><br>
<em>&gt;&gt;Landauer's research.  But I really don't see how to refute him.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;The number Landauer comes up with for the bits is probably pretty close to 
</em><br>
<em>&gt;being correct, extrapolating from my own models.  There are a couple key 
</em><br>
<em>&gt;points though:
</em><br>
<em>&gt;
</em><br>
<em>&gt;1.)  The kind of representation efficiencies we get in vanilla silicon is 
</em><br>
<em>&gt;atrocious because of architecture requirements.  Storing a few bits of 
</em><br>
<em>&gt;information in any kind of good associative representational framework 
</em><br>
<em>&gt;probably averages a hundred bytes of memory spent because of alignment, 
</em><br>
<em>&gt;non-locality, addressing, etc.  We don't get much bang for our 
</em><br>
<em>&gt;representation buck.  However, even with these inefficiencies we will 
</em><br>
<em>&gt;easily surpass the real capacity of wetware in silicon very soon.
</em><br>
<p>You might be right.  I suspect though that you are misled.  I think that 
<br>
the data and processors are mixed together in the brain in a way that will 
<br>
be difficult to simulate in serial processors--assuming this is actually 
<br>
needed for AI.
<br>
<p><em>&gt;2.)   The brain almost certainly gets a much higher compression ratio than 
</em><br>
<em>&gt;we are used to with normal computers, which affects our perception as to 
</em><br>
<em>&gt;what is possible in a given amount of space.  If you loosen the 
</em><br>
<em>&gt;constraints of accuracy a bit, you can actually cram a hell of a lot of 
</em><br>
<em>&gt;information in that space while maintaining high predictive limits (i.e. 
</em><br>
<em>&gt;maintaining good correctness of memory).  Humans don't actually seem to 
</em><br>
<em>&gt;remember that much, and none of what we remember is axiomatic or absolute.
</em><br>
<p>Which is why writing and the whole scheme of data retention tools has been 
<br>
so effective.  I think we are rich in procession power and really weak in 
<br>
memory.  That's why sitting with Google at hand effectively increases your 
<br>
brain power so much.
<br>
<p>Also don't forget that human psychology/mental capacity was shaped by 
<br>
millions of years where we lived in little tribes, chipped rock, killed 
<br>
animals, gathered berries and raised kids.  That fact that *any* of us can 
<br>
program or grok higher mathematics is downright amazing.  There are 
<br>
probable a lot of thinking kinds of things that none of us can do.
<br>
<p><em>&gt;&gt;I haven't got the slightest idea of how you map a mathematical structure 
</em><br>
<em>&gt;&gt;into a physical one or compare them.  Perhaps you could expand on this 
</em><br>
<em>&gt;&gt;and give an example?
</em><br>
<em>&gt;
</em><br>
<em>&gt;The data structures look and behave in a fashion similar to biological 
</em><br>
<em>&gt;neural networks (for as much as we know about them), yet are 
</em><br>
<em>&gt;mathematically/algorithmically derived.  It could be coincidence, but I 
</em><br>
<em>&gt;have my doubts.  I'm not a neural network aficionado (biological or otherwise).
</em><br>
<p>I am not sure there is evidence that neural networks have much relevance to 
<br>
biological brains.  It has been a long time since I read up on them.  But 
<br>
in any case, most neural network stuff is done with simulators running on 
<br>
regular computers now.  If you are trying to do pattern matching with 
<br>
coefficients, isn't that equivalent?
<br>
<p><em>&gt;&gt;That's an awesome claim.  It is of considerable interest to me in a 
</em><br>
<em>&gt;&gt;business sense because of the badge camera.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;It is an expensive and oddly behaving algorithm for embedded systems, and 
</em><br>
<em>&gt;would be better suited for feature extraction rather than vanilla 
</em><br>
<em>&gt;compression.  The compression is a (expected) side-effect that I thought 
</em><br>
<em>&gt;worth studying, not the ends.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;I really don't understand this.  Any n-dimentional data set can be turned 
</em><br>
<em>&gt;&gt;into a linear string.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Sure, and this is what I do.  But in real-world applications where 
</em><br>
<em>&gt;performance matters, data compression is a more complicated implementation 
</em><br>
<em>&gt;that frequently mandates sequential structure because the storage is.  You 
</em><br>
<em>&gt;have many kinds of limitations that make the theoretical impractical for 
</em><br>
<em>&gt;many uses.  If constrained to many standard application parameters, 
</em><br>
<em>&gt;vanilla compression would be more efficient.
</em><br>
<em>&gt;
</em><br>
<em>&gt;The compression is a necessary theoretical consequence of my main 
</em><br>
<em>&gt;thrust.  I've long used it as a kind of zero-knowledge proof of 
</em><br>
<em>&gt;implementation efficiency and correctness.   I'm not in the data 
</em><br>
<em>&gt;compression business.
</em><br>
<p>If you have a couple of hundred million processors, which I think is a good 
<br>
number to consider, then each can have a few hundred bytes without having 
<br>
to bother with compression.
<br>
<p>I think it is worth noting that the closest kind of projects to AI like the 
<br>
Google search engine *are* massively parallel.
<br>
<p><em>&gt;&gt;If you are talking about remembering a phone number, it is clear that 
</em><br>
<em>&gt;&gt;lossy is not useful.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Unfortunately, the brain is lossy, even with phone numbers.
</em><br>
<p>Not very often or they would be useless.
<br>
<p><em>&gt;Fortunately, we can easily selectively reinforce the memory of things that 
</em><br>
<em>&gt;we need to remember with high predictability.
</em><br>
<p>True.  I have been arguing that for years to people who think there is a 
<br>
serious fidelity problem with meme transmission.  With critical information 
<br>
like the number of strikes and balls in the baseball rules, there is very 
<br>
close to 100% fidelity over hundreds of millions of people.
<br>
<p>Keith Henson
<br>
<p><em>&gt;j. andrew rogers
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8188.html">EvolverTCB@aol.com: "Re: AI hardware was 'Singularity Realism'"</a>
<li><strong>Previous message:</strong> <a href="8186.html">Daniel Alexandre: "There are few books on AI"</a>
<li><strong>In reply to:</strong> <a href="8185.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8191.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<li><strong>Reply:</strong> <a href="8191.html">J. Andrew Rogers: "Re: AI hardware was 'Singularity Realism'"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8187">[ date ]</a>
<a href="index.html#8187">[ thread ]</a>
<a href="subject.html#8187">[ subject ]</a>
<a href="author.html#8187">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
