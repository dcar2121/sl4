<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Universal Morality -Some specific ideas</title>
<meta name="Author" content="Marc Geddes (marc_geddes@yahoo.co.nz)">
<meta name="Subject" content="Universal Morality -Some specific ideas">
<meta name="Date" content="2004-03-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Universal Morality -Some specific ideas</h1>
<!-- received="Tue Mar  2 21:37:56 2004" -->
<!-- isoreceived="20040303043756" -->
<!-- sent="Wed, 3 Mar 2004 17:37:49 +1300 (NZDT)" -->
<!-- isosent="20040303043749" -->
<!-- name="Marc Geddes" -->
<!-- email="marc_geddes@yahoo.co.nz" -->
<!-- subject="Universal Morality -Some specific ideas" -->
<!-- id="20040303043749.91320.qmail@web20204.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marc Geddes (<a href="mailto:marc_geddes@yahoo.co.nz?Subject=Re:%20Universal%20Morality%20-Some%20specific%20ideas"><em>marc_geddes@yahoo.co.nz</em></a>)<br>
<strong>Date:</strong> Tue Mar 02 2004 - 21:37:49 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8121.html">Mikko Särelä: "RE: [SL4] AI --&gt; Jobless Economy"</a>
<li><strong>Previous message:</strong> <a href="8119.html">Mike: "RE: [SL4] AI --&gt; Jobless Economy"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8126.html">Rafal Smigrodzki: "RE: Universal Morality -Some specific ideas"</a>
<li><strong>Reply:</strong> <a href="8126.html">Rafal Smigrodzki: "RE: Universal Morality -Some specific ideas"</a>
<li><strong>Maybe reply:</strong> <a href="8142.html">Marc Geddes: "RE: Universal Morality -Some specific ideas"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8120">[ date ]</a>
<a href="index.html#8120">[ thread ]</a>
<a href="subject.html#8120">[ subject ]</a>
<a href="author.html#8120">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben was asking what I thought the actual content of
<br>
Universal Morality might be.  Well, I'll try to give a
<br>
quick sumamry.
<br>
<p>I've already indicated that I'm skeptical of Eliezer's
<br>
ideas - Volitional Morality.  'Volitional Morality' as
<br>
I understand it, refers to a morality which equate
<br>
good with the fulfillment of other sentient desires,
<br>
subject to the privisos that the desires are what the
<br>
sentient truly wants, and the desires don't harm other
<br>
sentients.  The problem with this can be seen if we do
<br>
a thought experiment and imagine that the whole
<br>
universe was filled with nothing but sentients
<br>
operating off Volitional Morality:
<br>
<p>Each sentient wants to help others.  So the goal
<br>
systems of the sentients look like this:
<br>
<p>Sentient 1 : I want to help others
<br>
Sentient 2:  I want to help others
<br>
Sentient 3:  I want to help others
<br>
.... etc
<br>
<p>Take Sentient 1.  It wants to help sentient 2.  But
<br>
sentient 2 wants to other sentients, who in turn want
<br>
to help other sentients, who in turn want to help
<br>
other sentients..
<br>
<p>So the goal system of each sentient ends up in an
<br>
infinite regress:
<br>
<p>Sentient 1:  I want to help others to help others to
<br>
help others to help others to help others to help
<br>
others to help others ....
<br>
<p>Volitional Morality is meaningless without some
<br>
additional goals on the part of sentients.  Helping
<br>
others makes no sense if sentients don't have some
<br>
personal goals that they need help with.
<br>
<p>This proves that Universal Morality is not equivalent
<br>
to Volitional Morality.
<br>
<p>There's another argument against Volitional Morality:
<br>
<p>Any real world FAI will not have infinite
<br>
computational resources.  And the continued existence
<br>
of the goal system will not be certain.  The FAI can't
<br>
help sentients if it's goal system ceases to exist. 
<br>
The end of the goal system would contradict Volitional
<br>
Morality (because dead FAI's can't help anyone).  
<br>
Therefore all FAI's would have to divert some of their
<br>
finite resources into maintaining their own existence
<br>
(or at least into maintaining the existence of some
<br>
sentients with Volitional Morality as the goal
<br>
system).  So it seems that the Survival Imperative has
<br>
to be a sub-goal.  The FAI has to maintain it's
<br>
existence or else replicate itself.  
<br>
<p>We can see this with altruistic real world humans.  If
<br>
all they did was wonder the streets trying to 'help
<br>
others' they would (a) Soon run out of money, and (b)
<br>
Soon keel over from exhaustion, exposure and lack of
<br>
food and drink.  Even altruistic humans have to divert
<br>
a sizable fraction of their 'computational resources'
<br>
into maintaining their own existence. 
<br>
<p>Here's the problem:  Maximization of the goal 'help
<br>
others' requires that the majority of computational
<br>
resources be diverted to altruistic goals  (In a
<br>
competition between altruistic and it's own survival
<br>
the FAI has to choose altrusim).  But maximization of
<br>
the goal 'help others' requires the continued
<br>
existence of the goal system.  But this requires that
<br>
the majority of computational resources be diverted to
<br>
the 'Survival' Imperative -either replication of
<br>
self-protection.  We have a logical contradication! 
<br>
This is not a matter of sub-goal stomp, this is a
<br>
plain flat logical contradiction.  It proves that
<br>
Volitional Morality cannot, in fact, be the super
<br>
goal.
<br>
<p>I hope I've persauded you that altruism cannot be the
<br>
central purpose of existence (although it may be a
<br>
PART of the purpose).  So what do I think is a better
<br>
candidate for Universal Morality?
<br>
<p>Well I'm much more in agreement with Ben.  I think
<br>
Universal Morality has got much more to do with what
<br>
he calls 'Joyous Growth'.  I the see the universe as
<br>
'a work in progress' which has the 'goal' of exploring
<br>
its own nature.  Sentients are the 'agents' through
<br>
which the universe comes to know itself.  There are 3
<br>
major themese pertinent to this:
<br>
<p>(1)  Self-Creation
<br>
(2)  Self-Exploration and
<br>
(3)  Self-Betterment
<br>
<p>So for Universal Morality I would include:  the drive
<br>
to create things of value (self-creation), the drive
<br>
for knowledge (self exploration) and the drive for
<br>
self-betterment (perfectionist ethics).  So yes, I
<br>
think 'Joyous Growth' would be a good description.
<br>
<p>I would also include Altruism and the Survival
<br>
Imperative.  There is little doubt that Altruism is at
<br>
least a PART of what it means to be moral.  As for the
<br>
Survival imperative, I gave arguments for including
<br>
that as part of the super-goal.  In order to explore
<br>
its own nature, the first requirement of the universe
<br>
is that it maintain it's own existence.  So part of
<br>
the purpose of the universe is to continue to exist! 
<br>
And that's true  for Sentient beings as well.  I call
<br>
this 'Immortalist Morality' - sentients seek to
<br>
maintain their own existence.  Of course the quest for
<br>
immortality has to be balanced against the other goals
<br>
I mentioned.  To sum up I see Universal Morality as
<br>
consisting of 5 major goals, given roughly equal
<br>
priority:
<br>
<p>(1)  The drive for self-betterment
<br>
(2)  The quest for knowledge
<br>
(3)  The drive to create things of value and beauty
<br>
(4)  The quest for immortality
<br>
(5)  Altruism
<br>
<p>That's my guess.  You can see that I think that
<br>
'Universal Morality' is a pretty complex beast.
<br>
<p>It gets more complicated! ;)  I think that some input
<br>
from 'Personal Values' is required.  As I said, I
<br>
don't regard a 'Non Observer Centered FAI' as stable. 
<br>
The reason, as I explained, is that the major goals I
<br>
postulated as Universal Morality (1-5 above) don't
<br>
make sense without a 'personal values' input.
<br>
<p>For instance the Universal Imperative 'Continue to
<br>
exist' (the quest for immortality) is 'Non Observer
<br>
Cenetered' in the GENERAL sense that the imperative
<br>
holds for everyone.  But it requries observer centered
<br>
input in the sense of the SPECIFIC steps needed for
<br>
each sentient to fulfill it.  
<br>
<p>All sentient minds are represented as interaction
<br>
between 'Joyous Growth' with personal values.  'Joyous
<br>
Growth' is the GENERAL Universal Imperative, but the
<br>
personal goals are the many different possible
<br>
SPECIFIC expressions of the general case.
<br>
<p>That's why I said that:
<br>
<p>UNIVERSAL MORALITY x PERSONAL MORALITY = MIND
<br>
<p>Here's a quantum analogy:
<br>
<p>UNIVERSAL MORALITY is equivalent to the 'wave
<br>
function' of Friendliness.  A friendly PERSONAL
<br>
MORALITY is equivalent to a 'particle' of
<br>
Friendliness.
<br>
<p>All real world FAI's have to be observer centered and
<br>
are like unto a 'particle' of Friendliness, just as
<br>
all real world objects are only directly observable in
<br>
specific physical states.  Any FAI is a SPECIFIC
<br>
expression of the GENERAL case of Friendliness.  Why
<br>
do I think that all FAI's have to be observer
<br>
centered?  Because all moralities can only be
<br>
instantiated in specific individual sentients, and
<br>
each sentient exists at unique space-time
<br>
co-ordinates.  An input which is a function of these
<br>
unquie co-ordinates is required, and represents the
<br>
personal morality of the FAI. 
<br>
<p>I defined 'a morality' as 'a goal system'.  But the
<br>
universe as a whole has a goal system:  'the laws of
<br>
physics'.  So you could say that 'the laws of physics'
<br>
represent Universal Morality  (the morality of the
<br>
Universe).  Any particualr physical system has unique
<br>
parameters.  These are the 'boundary coniditions' or
<br>
'specific input' needed in order to make the laws of
<br>
physics predict things in the directly observable
<br>
world.  Any particular physical system could be said
<br>
to a 'Personal Morality' (a goal systen) presented by
<br>
these physical paramters which define it's state.  So
<br>
the universe as a whole is a kind of 'mind', and so is
<br>
every part of it (panpyschism).
<br>
<p>The point is easier to grasp when you view all of
<br>
reality as one giant computer.  Think of reality as a
<br>
giant 'virtual reality' computation, like 'The
<br>
Matrix'.  Then realise that a 'computation' is a 'goal
<br>
system' which represents a meaning.  For instance, a
<br>
personal computer working out a tax return creates a
<br>
meaning - the computation is related to the meaning
<br>
associated with tax returns in this example.
<br>
<p>Now... take the whole computation which represents the
<br>
history of the entire universe.  This computation is
<br>
the meaning of life!  Remember... the computation IS
<br>
the universe.  The computation is identical to the
<br>
laws of physics, and the laws of physics are the
<br>
morality of the Universe (Universal Morality).
<br>
<p>So, the MEANING of existence IS existence (You need to
<br>
think about this statement very carefully).   The map
<br>
IS the territory.
<br>
<p>UNIVERSAL MORALITY x PERSONAL MORALITY = MIND
<br>
<p>and
<br>
<p>MIND=REALITY
<br>
<p><p><p><p><p>=====
<br>
Please visit my web-site at:  <a href="http://www.prometheuscrack.com">http://www.prometheuscrack.com</a>
<br>
<p>Find local movie times and trailers on Yahoo! Movies.
<br>
<a href="http://au.movies.yahoo.com">http://au.movies.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8121.html">Mikko Särelä: "RE: [SL4] AI --&gt; Jobless Economy"</a>
<li><strong>Previous message:</strong> <a href="8119.html">Mike: "RE: [SL4] AI --&gt; Jobless Economy"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8126.html">Rafal Smigrodzki: "RE: Universal Morality -Some specific ideas"</a>
<li><strong>Reply:</strong> <a href="8126.html">Rafal Smigrodzki: "RE: Universal Morality -Some specific ideas"</a>
<li><strong>Maybe reply:</strong> <a href="8142.html">Marc Geddes: "RE: Universal Morality -Some specific ideas"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8120">[ date ]</a>
<a href="index.html#8120">[ thread ]</a>
<a href="subject.html#8120">[ subject ]</a>
<a href="author.html#8120">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
