<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI hardware was 'Singularity Realism'</title>
<meta name="Author" content="EvolverTCB@aol.com (EvolverTCB@aol.com)">
<meta name="Subject" content="Re: AI hardware was 'Singularity Realism'">
<meta name="Date" content="2004-03-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI hardware was 'Singularity Realism'</h1>
<!-- received="Sun Mar  7 07:04:16 2004" -->
<!-- isoreceived="20040307140416" -->
<!-- sent="Sun, 07 Mar 2004 09:04:06 -0500" -->
<!-- isosent="20040307140406" -->
<!-- name="EvolverTCB@aol.com" -->
<!-- email="EvolverTCB@aol.com" -->
<!-- subject="Re: AI hardware was 'Singularity Realism'" -->
<!-- id="0933A87D.7B627523.0B6C9CB0@aol.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="AI hardware was 'Singularity Realism'" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> <a href="mailto:EvolverTCB@aol.com?Subject=Re:%20AI%20hardware%20was%20'Singularity%20Realism'"><em>EvolverTCB@aol.com</em></a><br>
<strong>Date:</strong> Sun Mar 07 2004 - 07:04:06 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8175.html">Keith Henson: "Note for Philip Sutton"</a>
<li><strong>Previous message:</strong> <a href="8173.html">Keith Henson: "RE: AI hardware was 'Singularity Realism'"</a>
<li><strong>Maybe in reply to:</strong> <a href="8162.html">Keith Henson: "AI hardware was 'Singularity Realism'"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8176.html">Keith Henson: "Looking for a name (bit off topic perhaps)"</a>
<li><strong>Reply:</strong> <a href="8176.html">Keith Henson: "Looking for a name (bit off topic perhaps)"</a>
<li><strong>Reply:</strong> <a href="8186.html">Daniel Alexandre: "There are few books on AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8174">[ date ]</a>
<a href="index.html#8174">[ thread ]</a>
<a href="subject.html#8174">[ subject ]</a>
<a href="author.html#8174">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
In a message dated 3/6/2004 11:00:18 PM Eastern Standard Time, <a href="mailto:hkhenson@rogers.com?Subject=Re:%20AI%20hardware%20was%20'Singularity%20Realism'">hkhenson@rogers.com</a> writes:
<br>
<p><em>&gt; This is partly wrong and (I think) partly right.  Human information storage 
</em><br>
<em>&gt; is *abysmal.*  Cog-Sci and information theory people who have looked at it 
</em><br>
<em>&gt; in dozens of studies come to the dismal conclusion that accessible human 
</em><br>
<em>&gt; memories are formed at 6-8 bits per second.  Over a lifetime it is 
</em><br>
<em>&gt; something like 140 Mbytes.  [I was wrong too, see below.]
</em><br>
<em>&gt; 
</em><br>
<em>&gt; (It is probably a decade since I had a disk that small.  Going through old 
</em><br>
<em>&gt; disks recently I found stuff I had written over ten years ago that I had no 
</em><br>
<em>&gt; memory of having done *at all.*)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Here is a pointer.  Now that I used the net to look this up, I remember 
</em><br>
<em>&gt; Ralph talking about it at a party.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; <a href="http://www.merkle.com/humanMemory.html">http://www.merkle.com/humanMemory.html</a>
</em><br>
<em>&gt; 
</em><br>
<em>&gt;      &quot;Because experiments by many different experimenters were summarized 
</em><br>
<em>&gt; and analyzed, the results of the analysis are fairly robust; they are 
</em><br>
<em>&gt; insensitive to fine details or specific conditions of one or another 
</em><br>
<em>&gt; experiment. Finally, the amount remembered was divided by the time allotted 
</em><br>
<em>&gt; to memorization to determine the number of bits remembered per second.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;      &quot;The remarkable result of this work was that human beings remembered 
</em><br>
<em>&gt; very nearly two bits per second under all the experimental conditions. 
</em><br>
<em>&gt; Visual, verbal, musical, or whatever--two bits per second. Continued over a 
</em><br>
<em>&gt; lifetime, this rate of memorization would produce somewhat over 10^^9 bits, 
</em><br>
<em>&gt; or a few hundred megabytes.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Human memory almost certainly has the bit rate and capacity it does because 
</em><br>
<em>&gt; that was optimal for our Pleistocene ancestors.  (Less than 2 bits per 
</em><br>
<em>&gt; second might not have been enough to remember your way back to camp.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Computers with tens to hundreds of Gbytes of disk are way, way ahead of people.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; [Going into meta mode, I vaguely remembered that people absorb data at a 
</em><br>
<em>&gt; few bits per second.  My memory, as I wrote it down at first, was 6-8 bits 
</em><br>
<em>&gt; per second.  Wanting to cite this (and not being completely certain) I put 
</em><br>
<em>&gt; &quot;human memory&quot; and &quot;bits per second&quot; in Google and Ralph Merkle's paper 
</em><br>
<em>&gt; giving the correct number was the first link.]
</em><br>
<em>&gt; 
</em><br>
<em>&gt; People sometimes *seem* to remember a lot more.  But that's almost 
</em><br>
<em>&gt; certainly the massive processor power filling in the scenes with &quot;stock 
</em><br>
<em>&gt; footage&quot; or making it up out of whole cloth.    Here is a pointer, you can 
</em><br>
<em>&gt; find more with &quot;confabulation&quot; in Google.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Confabulation -The New England Skeptical Society's Encyclopedia of ...
</em><br>
<em>&gt; ... Confabulation. Description: Confabulation is the filling in of gaps in 
</em><br>
<em>&gt; memory
</em><br>
<em>&gt; to make a coherent story. ... Confabulation often occurs during hypnosis. ...
</em><br>
<em>&gt; www.theness.com/encyc/confabulation-encyc.html - 2k - Cached - Similar pages
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;Directly engineering human-level AI cognitive ability will require
</em><br>
<em>&gt; &gt;substantially less computing power than reverse-engineering the brain or
</em><br>
<em>&gt; &gt;uploading.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This *looks* like a valid assumption.  Ralph even supports it at the end of 
</em><br>
<em>&gt; his paper:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;      &quot;. . . his estimate of memory capacity suggests that the capabilities 
</em><br>
<em>&gt; of the human brain are more approachable than we had thought. While this 
</em><br>
<em>&gt; might come as a blow to our egos, it suggests that we could build a device 
</em><br>
<em>&gt; with the skills and abilities of a human being with little more hardware 
</em><br>
<em>&gt; than we now have--if only we knew the correct way to 
</em><br>
<em>&gt; organize that hardware.&quot;
</em><br>
<p>I think you are underestimating the human brain based on experimental evidence of poorly used brains. Brains with better programming (or abmormalities, in some cases) are known to perform 'impossible' feats of memory and calculation, and some of these methods are teachable.
<br>
In this connection I STRONGLY suggest a look at winwenger.com which offers various remarkable techniques for using the 'unconscious' abilities of the brain. For example, Wenger recommends a regimen of underwater swimming, as holding the breath and the 'dive response' gradually enlarge the carotid arteries, leading to much clearer thinking. (It's a bit embarrassing to think that the brain is sensitive to a simple thing like increased blood flow, however.)
<br>
This is not to suggest that machine AI can never catch up with human capacity, only that it will take a few days or weeks longer.
<br>
Tom Buckner
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8175.html">Keith Henson: "Note for Philip Sutton"</a>
<li><strong>Previous message:</strong> <a href="8173.html">Keith Henson: "RE: AI hardware was 'Singularity Realism'"</a>
<li><strong>Maybe in reply to:</strong> <a href="8162.html">Keith Henson: "AI hardware was 'Singularity Realism'"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8176.html">Keith Henson: "Looking for a name (bit off topic perhaps)"</a>
<li><strong>Reply:</strong> <a href="8176.html">Keith Henson: "Looking for a name (bit off topic perhaps)"</a>
<li><strong>Reply:</strong> <a href="8186.html">Daniel Alexandre: "There are few books on AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8174">[ date ]</a>
<a href="index.html#8174">[ thread ]</a>
<a href="subject.html#8174">[ subject ]</a>
<a href="author.html#8174">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
