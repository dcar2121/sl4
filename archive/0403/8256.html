<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: De-Anthropomorphizing SL3 to SL4.</title>
<meta name="Author" content="Michael Anissimov (michael@acceleratingfuture.com)">
<meta name="Subject" content="Re: De-Anthropomorphizing SL3 to SL4.">
<meta name="Date" content="2004-03-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: De-Anthropomorphizing SL3 to SL4.</h1>
<!-- received="Mon Mar 15 11:16:17 2004" -->
<!-- isoreceived="20040315181617" -->
<!-- sent="Mon, 15 Mar 2004 10:16:12 -0800" -->
<!-- isosent="20040315181612" -->
<!-- name="Michael Anissimov" -->
<!-- email="michael@acceleratingfuture.com" -->
<!-- subject="Re: De-Anthropomorphizing SL3 to SL4." -->
<!-- id="4055F2EC.3000003@acceleratingfuture.com" -->
<!-- charset="windows-1252" -->
<!-- inreplyto="20040315090127.19994.qmail@web11708.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Anissimov (<a href="mailto:michael@acceleratingfuture.com?Subject=Re:%20De-Anthropomorphizing%20SL3%20to%20SL4."><em>michael@acceleratingfuture.com</em></a>)<br>
<strong>Date:</strong> Mon Mar 15 2004 - 11:16:12 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8257.html">Michael Roy Ames: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<li><strong>Previous message:</strong> <a href="8255.html">Bill Hibbard: "Re: Psychedelic experience and the Singularity [was : De-Anthropomorphizing SL3 to SL4.]"</a>
<li><strong>In reply to:</strong> <a href="8252.html">Paul Hughes: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8257.html">Michael Roy Ames: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<li><strong>Reply:</strong> <a href="8257.html">Michael Roy Ames: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<li><strong>Reply:</strong> <a href="8258.html">Ben Goertzel: "RE: De-Anthropomorphizing SL3 to SL4."</a>
<li><strong>Reply:</strong> <a href="8260.html">Paul Hughes: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<li><strong>Reply:</strong> <a href="8268.html">Samantha Atkins: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8256">[ date ]</a>
<a href="index.html#8256">[ thread ]</a>
<a href="subject.html#8256">[ subject ]</a>
<a href="author.html#8256">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Whoops! Looks like I misinterpreted some of what Pesce was trying to 
<br>
say. I was under the impression that McKenna construed the Singularity 
<br>
as this benevolent agent we are collectively becoming, so when Pesce 
<br>
says stuff like,
<br>
<p>&quot;In 1993, a writer named Vernor Vinge gave a talk to NASA 
<br>
&lt;<a href="http://singularity.manilasites.com/stories/storyReader$35">http://singularity.manilasites.com/stories/storyReader$35</a>&gt;, in which he 
<br>
described the architecture of an event he called the “Singularity”, 
<br>
which is identical in every feature to McKenna’s Eschaton.&quot;
<br>
<p>it makes me think that they're talking about the same thing. The 
<br>
Singularity is radically different than McKenna's Eschaton. When people 
<br>
like Pesce say things like &quot;the bios might not be prepared for the 
<br>
emergence of the logos&quot;, I tend to visualize them visualizing some sort 
<br>
of societal/psychic chaos rather than outright destruction at the hands 
<br>
of a paperclip SI. I might be wrong about this, but it's the impression 
<br>
I've gotten from reading some of McKenna's articles. Humans are fragile 
<br>
creatures; we require a very precise environment to survive, and in the 
<br>
absence of an SI that specifically cares about preserving it, we would 
<br>
probably be swept aside (read: extinct) by the first SI(s) with goals 
<br>
that meant the rearrangement of matter on any appreciable scale. &quot;Care 
<br>
for humans&quot; is not a quality we should expect to emerge in an arbitrary 
<br>
SI of a certain level of intelligence.
<br>
<p>When you say &quot;will this new emergent complexity be friendly in any sense 
<br>
we can fathom, or will we be consumed or destroyed by it?&quot;, I think the 
<br>
answer lies in the initial motivations and goal structure we instill 
<br>
within the first seed AI. As Nick Bostrom writes in 
<br>
<a href="http://www.nickbostrom.com/ethics/ai.html">http://www.nickbostrom.com/ethics/ai.html</a>,
<br>
<p>&quot;The option to defer many decisions to the superintelligence does not 
<br>
mean that we can afford to be complacent in how we construct the 
<br>
superintelligence. On the contrary, the setting up of initial 
<br>
conditions, and in particular the selection of a top-level goal for the 
<br>
superintelligence, is of the utmost importance. Our entire future may 
<br>
hinge on how we solve these problems.&quot;
<br>
<p>The *first* seed AI seems to be especially important because it would 
<br>
likely have cognitive hardware advantages that allow it to bootstrap to 
<br>
superintelligence before anyone or anything else. This means that the 
<br>
entire human race will be at the mercy of whatever goal system or 
<br>
philosophy this first seed AI has after many iterations of recursive 
<br>
self-improvement. The information pattern that determines the fate of 
<br>
humanity after the Singularity is not be within us as individuals, or 
<br>
predetermined by meta-evolution, or encoded into the Timewave; it will 
<br>
be in the source code of the first recursive self-improver. If some 
<br>
idiot walks into the AI lab just as hard takeoff is about to commence, 
<br>
and spills coffee on the AI's mainframe, driving it a bit nutty, then 
<br>
the whole of humanity might be destroyed by that tiny mistake. Also, 
<br>
novel events prior to the Singularity are liable to have negligible 
<br>
impact upon it. If someone has a really great trip where they visualize 
<br>
all sorts of wonderful worlds, shapes, and entities, it will have 
<br>
absolutely no impact on whether humanity survives the Singularity. I 
<br>
have a feeling that Pesce and others would be turned off by this 
<br>
interpretation of the Singularity because it is so impersonal and 
<br>
arbitrary-seeming.
<br>
<p>So when Pesce says stuff like,
<br>
<p>&quot;So we have three waves, biological, linguistic, and
<br>
technological, which are rapidly moving to
<br>
concrescence, and on their way, as they interact,
<br>
produce such a tsunami of novelty as has never before
<br>
been experienced in the history of this planet.&quot;
<br>
<p>or
<br>
<p>&quot;Anything you see, anywhere, animate, or inanimate, will have within it
<br>
the capacity to be entirely transformed by a rearrangement of its atoms
<br>
into another form, a form which obeys the dictates of linguistic intent.&quot;
<br>
<p>it makes me feel like he has a false sense of hope, that the Singularity is more about embarking on a successful diplomatic relationship with the self-transforming machine elves, rather than solving a highly technical issue involving the design of AI goal systems.  I doubt that Pesce realizes the forces responsible for the rise of complexity and novelty in human society correspond to an immensely sophisticated set of cognitive tools unique to Homo sapiens, not to any underlying feature of the universe.  Fail to pass these tools onto the next stage, and the next stage will fail to carry on the tradition of increasing novelty.
<br>
<p>The vast majority of biological complexity on this planet will be irrelevant to the initial Singularity event, because it will play no part in building the first seed AI, except insofar as it indirectly gave rise to humanity.  Linguistic; irrelevant except insofar as the language the first AGI designers are using to plan their design and launch.  Technological; also, only a small portion of the technological complexity on our planet today will be used to create transhuman intelligence.  The *simplest constructable AIs* are likely to have correspondingly simple goal systems; so the *easiest* AIs to launch into recursive self-improvement are also likely to be the ones bringing on the most boring arrangements of matter, such as multitudes of paper clips.  Simple, boring, cruel, easy.  Instead of the Timewave hitting the bottom level of the graph, it goes to the top, reverting to whatever level of interestingness corresponds to quadrillions of paper clips and no intelligence but one obsessed with them.
<br>
<p>Given a *benevolent* Singularity, yes, biological, linguistic and technological forces might indeed intertwine with one another and produce a &quot;tsunami of novelty&quot; in much the way that he describes, but it seems to be that he's regarding this tsunami of novelty as basically coming for free.  &quot;Novelty&quot;, in the sense that Terence McKenna uses it, has an unambiguously positive connotation.
<br>
<p>Michael Anissimov
<br>
<p><p>Paul Hughes wrote:
<br>
<p><em>&gt;Hi Michael,
</em><br>
<em>&gt;
</em><br>
<em>&gt;You wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;&gt;My first problem with the analogy is that it subtly
</em><br>
<em>&gt;&gt;&gt;      
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;implies a developmentally predetermined positive
</em><br>
<em>&gt;outcome to the Singularity, when this needn't be the
</em><br>
<em>&gt;case. The first recursively self-improving
</em><br>
<em>&gt;intelligence could easily be selfish, or obsessively
</em><br>
<em>&gt;focused on a goal whose accomplishment entails the
</em><br>
<em>&gt;destruction of humanity.&lt;&lt;&lt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Yes, this is definitely true, and Mark Pesce and I
</em><br>
<em>&gt;agree with you.  Mark has said that the bios was not
</em><br>
<em>&gt;prepared for the emergence of the logos.  We could
</em><br>
<em>&gt;look at current species extinction as analagous
</em><br>
<em>&gt;evidence of what you are saying.
</em><br>
<em>&gt;
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;&gt;&gt;In this scenario, the arrival of the Singularity
</em><br>
<em>&gt;&gt;&gt;&gt;        
</em><br>
<em>&gt;&gt;&gt;&gt;
</em><br>
<em>&gt;needn't represent the &quot;next stage of intelligence and
</em><br>
<em>&gt;complexity in the universe&quot; in the positive, 
</em><br>
<em>&gt;uplifting sense at all. A superintelligence devoted
</em><br>
<em>&gt;solely to manufacturing as many paper clips as
</em><br>
<em>&gt;possible could easily delegate all of its complexity
</em><br>
<em>&gt;and intelligence towards plans for the complete 
</em><br>
<em>&gt;material conversion of the universe into
</em><br>
<em>&gt;self-sustaining paperclip manufacturing facilities,
</em><br>
<em>&gt;for example.&lt;&lt;&lt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;I agree this is a possibility, however I think the
</em><br>
<em>&gt;reality will be vastly more complex than that.  The
</em><br>
<em>&gt;question then is will this new emergent complexity be
</em><br>
<em>&gt;friendly in any sense we can fathom, or will we be
</em><br>
<em>&gt;consumed or destroyed by it?  I don’t think anyone
</em><br>
<em>&gt;knows what the answer to that is, and as Mark Pesce
</em><br>
<em>&gt;says it is the greatest challenge we face.&lt;&lt;&lt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;&gt;&gt;The universe has steadily been increasing in
</em><br>
<em>&gt;&gt;&gt;&gt;        
</em><br>
<em>&gt;&gt;&gt;&gt;
</em><br>
<em>&gt;complexity, yes. From the anthropic point of view,
</em><br>
<em>&gt;this makes perfect sense; a threshold level of 
</em><br>
<em>&gt;complexity is clearly required for a universe to
</em><br>
<em>&gt;generate agents capable of observing it to begin with.
</em><br>
<em>&gt;But once the agents have come into being, 
</em><br>
<em>&gt;there are not necessarily any promises for continued
</em><br>
<em>&gt;survival. The forces responsible for building stable
</em><br>
<em>&gt;forms in this universe across the eons do not give a
</em><br>
<em>&gt;damn about us, and would continue chugging along 
</em><br>
<em>&gt;should we become extinct one day.&lt;&lt;&lt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Again, agreed.  Nothing in Mark Pesce’s thinking would
</em><br>
<em>&gt;disagree with that.  This is precisely the problem
</em><br>
<em>&gt;that worrying him the most.
</em><br>
<em>&gt;
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;&gt;My second problem with the analogy is that it
</em><br>
<em>&gt;&gt;&gt;      
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;actually seems to understate the potential magnitude
</em><br>
<em>&gt;of a successful Singularity.&lt;&lt;&lt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;I don’t see where he does that.  All Mark says is that
</em><br>
<em>&gt;the next change is beyond anything our language can
</em><br>
<em>&gt;fathom, and that is at least as big as the jump from
</em><br>
<em>&gt;bios to logos.  His 10-million fold figure was only
</em><br>
<em>&gt;pertaining to the speed of logos over bios, not techne
</em><br>
<em>&gt;over logos.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;&gt;&gt;My point here is that the Singularity radically
</em><br>
<em>&gt;&gt;&gt;&gt;        
</em><br>
<em>&gt;&gt;&gt;&gt;
</em><br>
<em>&gt;outclasses any historical event, even when we make
</em><br>
<em>&gt;incredibly conservative assumptions. To me, making an
</em><br>
<em>&gt;analogy between the rise of general intelligence and 
</em><br>
<em>&gt;the Singularity sounds like trying to make an analogy
</em><br>
<em>&gt;between a firecracker and the Big Bang. Any analogy
</em><br>
<em>&gt;will be appealing, but as far as I can tell, the
</em><br>
<em>&gt;Singularity seems to be an event which is *genuinely* 
</em><br>
<em>&gt;new. Consider a hypothetical world where a large
</em><br>
<em>&gt;transition that occurs can objectively be said to lack
</em><br>
<em>&gt;any analogies. If humans lived in such a 
</em><br>
<em>&gt;world, don't you think they would grasp for whatever
</em><br>
<em>&gt;analogies they could, just to achieve the sensation of
</em><br>
<em>&gt;cognitive closure? Obviously the ancestral environment
</em><br>
<em>&gt;didn't have anything remotely like the 
</em><br>
<em>&gt;Singularity in it, so we're clearly adapted poorly to
</em><br>
<em>&gt;modeling changes of this size. Therefore I think it
</em><br>
<em>&gt;makes sense to be extremely careful in which analogies
</em><br>
<em>&gt;we choose to use, if any.&lt;&lt;&lt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Exactly.  That was Mark Pesce’s whole point!  He said
</em><br>
<em>&gt;the coming changes are so great the completely
</em><br>
<em>&gt;outstrip ANY linguisitic model we could possibly hope
</em><br>
<em>&gt;to attach to them.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Quoting him again here,
</em><br>
<em>&gt;&quot;
</em><br>
<em>&gt;And that search for a language to describe the world
</em><br>
<em>&gt;we’re entering is, I think, the grand project of the
</em><br>
<em>&gt;present civilization. We know that something new is
</em><br>
<em>&gt;approaching.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt;&quot;So we have three waves, biological, linguistic, and
</em><br>
<em>&gt;technological, which are rapidly moving to
</em><br>
<em>&gt;concrescence, and on their way, as they interact,
</em><br>
<em>&gt;produce such a tsunami of novelty as has never before
</em><br>
<em>&gt;been experienced in the history of this planet.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Paul Hughes
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;__________________________________
</em><br>
<em>&gt;Do you Yahoo!?
</em><br>
<em>&gt;Yahoo! Mail - More reliable, more storage, less spam
</em><br>
<em>&gt;<a href="http://mail.yahoo.com">http://mail.yahoo.com</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8257.html">Michael Roy Ames: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<li><strong>Previous message:</strong> <a href="8255.html">Bill Hibbard: "Re: Psychedelic experience and the Singularity [was : De-Anthropomorphizing SL3 to SL4.]"</a>
<li><strong>In reply to:</strong> <a href="8252.html">Paul Hughes: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8257.html">Michael Roy Ames: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<li><strong>Reply:</strong> <a href="8257.html">Michael Roy Ames: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<li><strong>Reply:</strong> <a href="8258.html">Ben Goertzel: "RE: De-Anthropomorphizing SL3 to SL4."</a>
<li><strong>Reply:</strong> <a href="8260.html">Paul Hughes: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<li><strong>Reply:</strong> <a href="8268.html">Samantha Atkins: "Re: De-Anthropomorphizing SL3 to SL4."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8256">[ date ]</a>
<a href="index.html#8256">[ thread ]</a>
<a href="subject.html#8256">[ subject ]</a>
<a href="author.html#8256">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:46 MDT
</em></small></p>
</body>
</html>
