<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Deliver Us from Evil...?</title>
<meta name="Author" content="Brian Atkins (brian@posthuman.com)">
<meta name="Subject" content="Re: Deliver Us from Evil...?">
<meta name="Date" content="2001-04-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Deliver Us from Evil...?</h1>
<!-- received="Fri Apr 06 03:10:26 2001" -->
<!-- isoreceived="20010406091026" -->
<!-- sent="Fri, 06 Apr 2001 03:07:16 -0400" -->
<!-- isosent="20010406070716" -->
<!-- name="Brian Atkins" -->
<!-- email="brian@posthuman.com" -->
<!-- subject="Re: Deliver Us from Evil...?" -->
<!-- id="3ACD6B24.1F9B8A9D@posthuman.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="4.3.2.7.2.20010405211353.00b646e0@mail.earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brian Atkins (<a href="mailto:brian@posthuman.com?Subject=Re:%20Deliver%20Us%20from%20Evil...?"><em>brian@posthuman.com</em></a>)<br>
<strong>Date:</strong> Fri Apr 06 2001 - 01:07:16 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0921.html">Eliezer S. Yudkowsky: "Re: Si definition of Friendliess"</a>
<li><strong>Previous message:</strong> <a href="0919.html">Brian Atkins: "Re: Si definition of Friendliess"</a>
<li><strong>In reply to:</strong> <a href="0913.html">James Higgins: "Re: Deliver Us from Evil...?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0927.html">James Higgins: "Re: Deliver Us from Evil...?"</a>
<li><strong>Reply:</strong> <a href="0927.html">James Higgins: "Re: Deliver Us from Evil...?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#920">[ date ]</a>
<a href="index.html#920">[ thread ]</a>
<a href="subject.html#920">[ subject ]</a>
<a href="author.html#920">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
James Higgins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; At 01:39 PM 4/5/2001 -0400, Brian Atkins wrote:
</em><br>
<em>&gt; &gt;Samantha Atkins wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You two related or something?
</em><br>
<p>No
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; &gt; I don't see how this follows.  If you upload a human who quickly
</em><br>
<em>&gt; &gt; &gt; self-improves ver capabilities and becomes an SI and if
</em><br>
<em>&gt; &gt; &gt; super-intelligence brings with it expanded moral/ethical understanding
</em><br>
<em>&gt; &gt; &gt; then I see no reason this combination is less trustworthy than starting
</em><br>
<em>&gt; &gt; &gt; from scratch and only putting in what you believe should be there in the
</em><br>
<em>&gt; &gt; &gt; beginning.  Yes a lot of evolved complicated behavior and conditioning
</em><br>
<em>&gt; &gt; &gt; is not present in the AI.  But some of that complicated behavior and
</em><br>
<em>&gt; &gt; &gt; conditioning is also the bed of universal compassion and utter
</em><br>
<em>&gt; &gt; &gt; Friendliness.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;Lot of ifs there...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, everything we talk about would need to be qualified a hundred
</em><br>
<em>&gt; different ways if we got down to the nitty gritty.  At this point, this is
</em><br>
<em>&gt; more SF than a science.  That will change, however, over the next few years.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;What it seems to come down to is you are either relying on objective
</em><br>
<em>&gt; &gt;morality (in which an AI should do better since it has less evolved
</em><br>
<em>&gt; &gt;crap to deal with), or a natural convergence to the &quot;Friendly zone&quot;. In
</em><br>
<em>&gt; &gt;which case we also argue that a properly designed AI should easily
</em><br>
<em>&gt; &gt;outperform a human attempting to upgrade him/herself. The reason I think
</em><br>
<em>&gt; &gt;is easy to see: you can't really predict in advance which particular
</em><br>
<em>&gt; &gt;human will become utterly Friendly vs. which particular human will become
</em><br>
<em>&gt; &gt;the next Hitler when presented with the total power uploading/becoming a
</em><br>
<em>&gt; &gt;SI would give them. History has shown a tendency for power to corrupt
</em><br>
<em>&gt; &gt;humans. At least with an AI we can sharply reduce the risks by a) designing
</em><br>
<em>&gt; &gt;it right b) testing testing testing
</em><br>
<em>&gt; 
</em><br>
<em>&gt; BS.  Testing means jack when you are talking about something that will
</em><br>
<em>&gt; complete reinvent itself a thousand times.  And I have a feeling that the
</em><br>
<em>&gt; design will have little long-term impact also.  Certainly, the design will
</em><br>
<em>&gt; influence how well the AI starts off, and how well it upgrades itself the
</em><br>
<em>&gt; first few times or so.  But beyond the first dozen or less redesigns, it
</em><br>
<em>&gt; will have little, if any, resemblance to the original.  So excuse me if I
</em><br>
<em>&gt; don't have much faith.
</em><br>
<p>Feel free to debate the technical points with Eliezer, he'd love to hear
<br>
some real criticism of the details...
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt;You want to talk about who designs the first AI? Well who decides who gets
</em><br>
<em>&gt; &gt;to be the first upload?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Hey, that is easy.  I'll be first.  ;)
</em><br>
<p>Well James we don't see didley squat in terms of new ideas from you, so I
<br>
doubt you would be the one the world would choose (if they were picking), and
<br>
I doubt you will be the one to invent uploading technology. I asked you for
<br>
your ideal scenario and got nothing except further knee jerk reactions to
<br>
ours. I point out the flaws in uploading humans, and instead of coming up
<br>
with some argument against that you choose to make seat of the pants judgements
<br>
about our AI plans without (apparently) even reading our online documentation.
<br>
<pre>
-- 
Brian Atkins
Director, Singularity Institute for Artificial Intelligence
<a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0921.html">Eliezer S. Yudkowsky: "Re: Si definition of Friendliess"</a>
<li><strong>Previous message:</strong> <a href="0919.html">Brian Atkins: "Re: Si definition of Friendliess"</a>
<li><strong>In reply to:</strong> <a href="0913.html">James Higgins: "Re: Deliver Us from Evil...?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0927.html">James Higgins: "Re: Deliver Us from Evil...?"</a>
<li><strong>Reply:</strong> <a href="0927.html">James Higgins: "Re: Deliver Us from Evil...?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#920">[ date ]</a>
<a href="index.html#920">[ thread ]</a>
<a href="subject.html#920">[ subject ]</a>
<a href="author.html#920">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
