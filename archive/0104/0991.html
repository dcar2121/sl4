<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Uploads and AIs (was: Deliver us from...)</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="Re: Uploads and AIs (was: Deliver us from...)">
<meta name="Date" content="2001-04-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Uploads and AIs (was: Deliver us from...)</h1>
<!-- received="Sat Apr 07 12:51:23 2001" -->
<!-- isoreceived="20010407185123" -->
<!-- sent="Sat, 07 Apr 2001 09:59:15 -0700" -->
<!-- isosent="20010407165915" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="Re: Uploads and AIs (was: Deliver us from...)" -->
<!-- id="4.3.2.7.2.20010407093444.00b6cf08@mail.earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3ACE550E.6E8D302F@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=Re:%20Uploads%20and%20AIs%20(was:%20Deliver%20us%20from...)"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Sat Apr 07 2001 - 10:59:15 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0992.html">James Higgins: "Re: Moravec's estimates?"</a>
<li><strong>Previous message:</strong> <a href="0990.html">Eliezer S. Yudkowsky: "Re: Singularity intros (was: Si definition of Friendliess)"</a>
<li><strong>In reply to:</strong> <a href="0968.html">Eliezer S. Yudkowsky: "Uploads and AIs (was: Deliver us from...)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0999.html">Brian Atkins: "Re: Uploads and AIs (was: Deliver us from...)"</a>
<li><strong>Reply:</strong> <a href="0999.html">Brian Atkins: "Re: Uploads and AIs (was: Deliver us from...)"</a>
<li><strong>Reply:</strong> <a href="1001.html">Spike Jones: "Re: Uploads and AIs (was: Deliver us from...)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#991">[ date ]</a>
<a href="index.html#991">[ thread ]</a>
<a href="subject.html#991">[ subject ]</a>
<a href="author.html#991">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
This is exactly the type of discussion I wanted to initiate.  This very 
<br>
much helps to explain why you believe your Friendly Seed AI is the best 
<br>
approach.
<br>
<p>At 07:45 PM 4/6/2001 -0400, Eliezer S. Yudkowsky wrote:
<br>
<em>&gt;For me, the superiority of AI over uploading lies chiefly in two facts:
</em><br>
<em>&gt;First, uploading is a technology years ahead of AI *or* military
</em><br>
<em>&gt;nanotechnology.  If you're postulating that uploading was naturally
</em><br>
<em>&gt;developed before both of the &quot;alternatives&quot;, I want to know how.  If
</em><br>
<em>&gt;you're trusting a transhuman AI, a sort of limited Transition Guide, to
</em><br>
<em>&gt;upload the first humans and leave it to them from there, I want to know
</em><br>
<em>&gt;why this path doesn't subsume almost all the risk of straightforward seed
</em><br>
<em>&gt;AI development.
</em><br>
<p>Ok, could someone please explain the difference between nanotechnology and 
<br>
&quot;military grade&quot; nanotechnology?
<br>
<p>It is still possible that some form of uploading may be possible before 
<br>
AI.  They are already working on exactly simulating very small portions of 
<br>
the brain.  This will continue to expand until they are able to simulate a 
<br>
brain in general.  The big question, of course, is how do they scan 
<br>
someone's brain into such a simulation.  Nanotechnology makes this much 
<br>
easier, of course.  And, depending on the description of military grade 
<br>
nano, such tech may exist before AI.  It may also be possible to do an 
<br>
external scan similar to an MRI, only much, much more sensitive.  I'm not 
<br>
an expert in this field by any means, and AI may arrive before brain 
<br>
scanning ability, but it may not.
<br>
<p><em>&gt;That said, if I had both an uploading device and a seed AI in front of me
</em><br>
<em>&gt;- *which is not the case* - which one I'd choose would depend on how good
</em><br>
<em>&gt;the AI was.  If ve'd been run through a few rounds of wisdom tournaments
</em><br>
<em>&gt;(see _FAI_), and just looked better than human at handling both
</em><br>
<em>&gt;philosophical crises and self-modification, I'd go with the AI, of
</em><br>
<em>&gt;course.  Ve'd be starting out with a much higher level of ability and
</em><br>
<em>&gt;morality.
</em><br>
<p>Question, why not do both?
<br>
<p>This leads to some critical questions that I have not seen information 
<br>
about (not that it isn't written somewhere).  In what environment do you 
<br>
launch the seed AI?  If the SAI was on a single, non-networked computer it 
<br>
should be unable to effect the physical world, no matter how smart it 
<br>
became.  Or is the plan to give the seed AI complete access to the 
<br>
Internet?  or ?
<br>
<p>Also, it is obviously expected that this seed AI, after upgrading many 
<br>
fold, will invent and gain access to nanotechnology.  How does this 
<br>
occur?  Even if it invents nanotech, the first assembler has to get 
<br>
physically built.  How is this expected to occur?
<br>
<p>Depending on these bounds, it may be completely reasonable to launch an 
<br>
array of seed AIs or upload any number of humans.
<br>
<p><em>&gt;As I currently see it, it only takes a finite amount of effort to create a
</em><br>
<em>&gt;threshold level of Friendliness - and more importantly, structural
</em><br>
<em>&gt;Friendliness - beyond which you can be pretty sure that the AI has the
</em><br>
<em>&gt;same moral structure as a human; or rather, a moral structure which can
</em><br>
<em>&gt;handle anything a human can.  Then the human's inexperience at
</em><br>
<em>&gt;self-modification, and emotional problems, become disadvantages.
</em><br>
<p>Hmm, interesting.  Is this covered in some reasonably compact section of 
<br>
FAI?  I really wish I had time to read everything, but I'm unbelievably 
<br>
busy (and probably will be for the next year+).
<br>
<p><em>&gt;However, it seems to me nearly certain that the potential for a hard
</em><br>
<em>&gt;takeoff - supersaturated computing power - will exist years before
</em><br>
<em>&gt;uploading becomes possible.  Thus, the question is simply one of Friendly
</em><br>
<em>&gt;AI, unFriendly AI, or someone blowing up the world.
</em><br>
<p>Those are good arguments for seed AI.
<br>
<p><em>&gt;I'd probably go with one human, three at the most.  I'd be mostly
</em><br>
<em>&gt;concerned about finding a human who was (a) willing to hold off on the
</em><br>
<em>&gt;emotional modifications and concentrate on just increasing intelligence
</em><br>
<em>&gt;for a while, and (b) finding someone who, at least overtly and explicitly
</em><br>
<em>&gt;and as a surface-level decision, thinks that rationalization and
</em><br>
<em>&gt;irrationality and non-normative cognition is a bad thing.  I doubt that
</em><br>
<em>&gt;Christian L. *believes himself* to tolerate irrationality, and that is
</em><br>
<em>&gt;perhaps the single most important quality to start out with.
</em><br>
<p>Actualy, I was thinking about whole research teams.  Preferably exiting 
<br>
teams that work well together.  Just upload them into hardware so they can 
<br>
work faster and build on that.  Leverage the hardware so the best minds can 
<br>
spend vastly more time trying to solve the hard problems.
<br>
<p>Imagine if you, Eliezer (do you have a team?) could be uploaded and think 
<br>
at 2x your current speed.  Then consider that without needing to sleep, 
<br>
eat, etc. you probably gain an additional 2x worth of available time.  And 
<br>
you can work 4x as fast as previously without distraction.  A year and a 
<br>
half later you could think 8x as fast, etc.
<br>
<p>Plus, thanks to the way computers work, you get a previously impossible 
<br>
ability.  There is absolutely no reason why we couldn't have 100 Eliezers 
<br>
in software.  And since they would all be the same person, they should work 
<br>
very well together.  So now you can focus on 100 different problems at the 
<br>
same time.  So, for instance, you could have one of them who just writes 
<br>
papers and talks to us without delaying the project any.  :)
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0992.html">James Higgins: "Re: Moravec's estimates?"</a>
<li><strong>Previous message:</strong> <a href="0990.html">Eliezer S. Yudkowsky: "Re: Singularity intros (was: Si definition of Friendliess)"</a>
<li><strong>In reply to:</strong> <a href="0968.html">Eliezer S. Yudkowsky: "Uploads and AIs (was: Deliver us from...)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0999.html">Brian Atkins: "Re: Uploads and AIs (was: Deliver us from...)"</a>
<li><strong>Reply:</strong> <a href="0999.html">Brian Atkins: "Re: Uploads and AIs (was: Deliver us from...)"</a>
<li><strong>Reply:</strong> <a href="1001.html">Spike Jones: "Re: Uploads and AIs (was: Deliver us from...)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#991">[ date ]</a>
<a href="index.html#991">[ thread ]</a>
<a href="subject.html#991">[ subject ]</a>
<a href="author.html#991">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
