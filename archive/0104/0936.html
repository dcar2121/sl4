<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Si definition of Friendliess</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Si definition of Friendliess">
<meta name="Date" content="2001-04-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Si definition of Friendliess</h1>
<!-- received="Fri Apr 06 16:45:18 2001" -->
<!-- isoreceived="20010406224518" -->
<!-- sent="Fri, 06 Apr 2001 16:42:58 -0400" -->
<!-- isosent="20010406204258" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Si definition of Friendliess" -->
<!-- id="3ACE2A52.2F1DCAFD@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3ACE1FCB.97E21200@earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Si%20definition%20of%20Friendliess"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri Apr 06 2001 - 14:42:58 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0937.html">Chris Cooper: "Re: Si definition of Friendliess"</a>
<li><strong>Previous message:</strong> <a href="0935.html">Arona Ndiaye: "Re: Si definition of Friendliess"</a>
<li><strong>In reply to:</strong> <a href="0934.html">Chris Cooper: "Re: Si definition of Friendliess"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0938.html">Chris Cooper: "Re: Si definition of Friendliess"</a>
<li><strong>Reply:</strong> <a href="0938.html">Chris Cooper: "Re: Si definition of Friendliess"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#936">[ date ]</a>
<a href="index.html#936">[ thread ]</a>
<a href="subject.html#936">[ subject ]</a>
<a href="author.html#936">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Chris Cooper wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Arona Ndiaye wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;A 'Singularity for Dummies' sounds like a joke to me. Do not get me wrong,
</em><br>
<em>&gt; &gt;but with all due respect: why should dummies need
</em><br>
<em>&gt; &gt;to understand the Singularity ?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This attitude is EXACTLY why it is important to explain these ideas to
</em><br>
<em>&gt; laypeople as we approach the Singularity. The kind of arrogant attitude that...
</em><br>
<p>I would say that I want (a) a sufficiently large group of supporters to
<br>
succeed, and (b) a sufficiently small group of opposition that the project
<br>
isn't halted.
<br>
<p>I feel that the rest of the planet should have the opportunity to find out
<br>
more about the Singularity if they become interested, because the
<br>
Singularity is a part of human destiny.  But if someone wants to just not
<br>
know anything about it - shove the whole issue off onto someone else -
<br>
well, I may disagree with them, but that's their privilege.  I no longer
<br>
have the attitude that people MUST care whether they like it or not.  You
<br>
know the saying:  A fanatic is someone who can't change his mind and won't
<br>
change the subject.
<br>
<p>I just write informative articles.  I've given up on (i.e., do not
<br>
currently plan on) determining the rates of memetic propagation for an
<br>
entire planet.  Most likely, the issue will be determined largely by the
<br>
pro-technology and anti-technology factions fighting it out, a relatively
<br>
small percentage of the global population.  There is a small probability
<br>
of a &quot;Baylor Jihad&quot; (antitechnology crusade), and a smaller probability of
<br>
a planetwide crusade for the Singularity.
<br>
<p>I would LOVE to see a planetwide crusade for the Singularity.  I would
<br>
LOVE to see the public really &quot;getting&quot; it, all of it, and doing the right
<br>
thing for the right reasons.  And not just from a utilitarian perspective,
<br>
either - it's a personal, powerful, and emotional dream that I've been
<br>
forced to give up.  If it happens, I'll feel awed and humbled and a bit
<br>
sorry about underestimating people.  But I currently don't expect it to
<br>
happen and don't believe I can make it happen, so that project is on a
<br>
targets-of-opportunity basis.
<br>
<p><em>&gt; Making the general public aware of the very
</em><br>
<em>&gt; scary things that are possible as technology advances so quickly, (such as the
</em><br>
<em>&gt; consequences of out-of-control nanotech) will only help us to reach the
</em><br>
<em>&gt; Singularity, ultimately.
</em><br>
<p>If the public is interested, great, that's why SIAI has a public website,
<br>
but I'm not going to shove anyone's nose in it.  I am certainly not going
<br>
to go peddle technophobia.  It is not emotionally realistic to expect
<br>
someone to accept the end of the world *before* you tell them how to save
<br>
it - it has to happen the other way around.  Similarly, if you tell
<br>
someone about a threat, their emotional reaction is to run like blazes the
<br>
other way even if it kills them, not make a 10-degree course change to
<br>
take the path of least comparative risk.
<br>
<p>&quot;Dangers of technology&quot; propagates selectively, and faster than
<br>
pro-technology memes, so I'm not about to use it as a carrier; it makes a
<br>
Baylor Jihad more likely.
<br>
<p><em>&gt; The Singularity
</em><br>
<em>&gt; is supposed to help EVERYONE, not just those who got in line first.
</em><br>
<p>And it will, regardless of whether they were SIAI programmers, or people
<br>
who never heard of the Singularity, or even people who did hear about it
<br>
and decided to let someone else worry about it.  Why would a Friendly AI
<br>
care?
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0937.html">Chris Cooper: "Re: Si definition of Friendliess"</a>
<li><strong>Previous message:</strong> <a href="0935.html">Arona Ndiaye: "Re: Si definition of Friendliess"</a>
<li><strong>In reply to:</strong> <a href="0934.html">Chris Cooper: "Re: Si definition of Friendliess"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0938.html">Chris Cooper: "Re: Si definition of Friendliess"</a>
<li><strong>Reply:</strong> <a href="0938.html">Chris Cooper: "Re: Si definition of Friendliess"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#936">[ date ]</a>
<a href="index.html#936">[ thread ]</a>
<a href="subject.html#936">[ subject ]</a>
<a href="author.html#936">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
