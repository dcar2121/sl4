<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Uploads and AIs (was: Deliver us from...)</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Uploads and AIs (was: Deliver us from...)">
<meta name="Date" content="2001-04-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Uploads and AIs (was: Deliver us from...)</h1>
<!-- received="Fri Apr 06 19:45:47 2001" -->
<!-- isoreceived="20010407014547" -->
<!-- sent="Fri, 06 Apr 2001 19:45:18 -0400" -->
<!-- isosent="20010406234518" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Uploads and AIs (was: Deliver us from...)" -->
<!-- id="3ACE550E.6E8D302F@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3ACE4F32.640B40BB@posthuman.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Uploads%20and%20AIs%20(was:%20Deliver%20us%20from...)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri Apr 06 2001 - 17:45:18 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0969.html">Christian L.: "Re: Singularity and the general public"</a>
<li><strong>Previous message:</strong> <a href="0967.html">xgl: "Re: Singularity and the general public"</a>
<li><strong>In reply to:</strong> <a href="0963.html">Brian Atkins: "Re: Deliver Us from Evil...?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0971.html">Brian Atkins: "We need a new term?"</a>
<li><strong>Reply:</strong> <a href="0971.html">Brian Atkins: "We need a new term?"</a>
<li><strong>Maybe reply:</strong> <a href="0974.html">Christian L.: "Re: Uploads and AIs (was: Deliver us from...)"</a>
<li><strong>Reply:</strong> <a href="0991.html">James Higgins: "Re: Uploads and AIs (was: Deliver us from...)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#968">[ date ]</a>
<a href="index.html#968">[ thread ]</a>
<a href="subject.html#968">[ subject ]</a>
<a href="author.html#968">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Brian Atkins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Power is power... The ability to do the things a superintelligence can do
</em><br>
<em>&gt; is way way more power than anyone on Earth has ever possessed. The human
</em><br>
<em>&gt; may be able to upgrade him/herself carefully enough to posthumanity/
</em><br>
<em>&gt; superintelligence such that they never are really tempted by it. Or they
</em><br>
<em>&gt; might not. The simple fact that they will be the first person trying to
</em><br>
<em>&gt; upgrade their mind (an evolved, and probably still undeciphered organ if
</em><br>
<em>&gt; uploading becomes available somehow before AI) by trying various hacks
</em><br>
<em>&gt; is dangerous in and of itself. An AI that understands its own source code
</em><br>
<em>&gt; would seem inherently safer.
</em><br>
<p>For me, the superiority of AI over uploading lies chiefly in two facts: 
<br>
First, uploading is a technology years ahead of AI *or* military
<br>
nanotechnology.  If you're postulating that uploading was naturally
<br>
developed before both of the &quot;alternatives&quot;, I want to know how.  If
<br>
you're trusting a transhuman AI, a sort of limited Transition Guide, to
<br>
upload the first humans and leave it to them from there, I want to know
<br>
why this path doesn't subsume almost all the risk of straightforward seed
<br>
AI development.
<br>
<p>That said, if I had both an uploading device and a seed AI in front of me
<br>
- *which is not the case* - which one I'd choose would depend on how good
<br>
the AI was.  If ve'd been run through a few rounds of wisdom tournaments
<br>
(see _FAI_), and just looked better than human at handling both
<br>
philosophical crises and self-modification, I'd go with the AI, of
<br>
course.  Ve'd be starting out with a much higher level of ability and
<br>
morality.
<br>
<p>If I had to pick between Hugo de Garis's AI design and Brian, I'd pick
<br>
Brian in an instant.  Heck, I'd pick Christian L. over a non-Friendly AI,
<br>
because Christian L. has built-in causal rewrite semantics and the NFAI
<br>
doesn't.
<br>
<p>As I currently see it, it only takes a finite amount of effort to create a
<br>
threshold level of Friendliness - and more importantly, structural
<br>
Friendliness - beyond which you can be pretty sure that the AI has the
<br>
same moral structure as a human; or rather, a moral structure which can
<br>
handle anything a human can.  Then the human's inexperience at
<br>
self-modification, and emotional problems, become disadvantages.
<br>
<p>However, it seems to me nearly certain that the potential for a hard
<br>
takeoff - supersaturated computing power - will exist years before
<br>
uploading becomes possible.  Thus, the question is simply one of Friendly
<br>
AI, unFriendly AI, or someone blowing up the world.
<br>
<p><em>&gt; &gt; &gt; You want to talk about who designs the first AI? Well who decides who gets
</em><br>
<em>&gt; &gt; &gt; to be the first upload?
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Actually, no, I don't want to talk about either one.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well you kind of have to if you are going to champion the uploading path
</em><br>
<em>&gt; to Singularity...
</em><br>
<p>I agree with Brian.  Anyone who wants to talk about uploading has to
<br>
explain who goes first, how they're selected, how many people go first,
<br>
where - computationally - they live, how they vote, and what happens in
<br>
the case of a rogue upload, or if I, as an upload, want to write a seed
<br>
AI.  If it's a single human doing self-enhancement, I want to know how the
<br>
changes are tested out (at least, until some decent level of transhumanity
<br>
is reached).
<br>
<p>I'd probably go with one human, three at the most.  I'd be mostly
<br>
concerned about finding a human who was (a) willing to hold off on the
<br>
emotional modifications and concentrate on just increasing intelligence
<br>
for a while, and (b) finding someone who, at least overtly and explicitly
<br>
and as a surface-level decision, thinks that rationalization and
<br>
irrationality and non-normative cognition is a bad thing.  I doubt that
<br>
Christian L. *believes himself* to tolerate irrationality, and that is
<br>
perhaps the single most important quality to start out with.
<br>
<p>It might work.  I just think it would be an unnecessary risk if you have a
<br>
structurally complete AI standing in front of you, and *definitely* an
<br>
inferior risk if you have a wisdom-tournamented, philosophically
<br>
transhuman AI on hand.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0969.html">Christian L.: "Re: Singularity and the general public"</a>
<li><strong>Previous message:</strong> <a href="0967.html">xgl: "Re: Singularity and the general public"</a>
<li><strong>In reply to:</strong> <a href="0963.html">Brian Atkins: "Re: Deliver Us from Evil...?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0971.html">Brian Atkins: "We need a new term?"</a>
<li><strong>Reply:</strong> <a href="0971.html">Brian Atkins: "We need a new term?"</a>
<li><strong>Maybe reply:</strong> <a href="0974.html">Christian L.: "Re: Uploads and AIs (was: Deliver us from...)"</a>
<li><strong>Reply:</strong> <a href="0991.html">James Higgins: "Re: Uploads and AIs (was: Deliver us from...)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#968">[ date ]</a>
<a href="index.html#968">[ thread ]</a>
<a href="subject.html#968">[ subject ]</a>
<a href="author.html#968">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
