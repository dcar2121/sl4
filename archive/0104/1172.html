<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: FW: More on Singularity and group releases Friendly AI guidelines</title>
<meta name="Author" content="Patrick McCuller (patrick@kia.net)">
<meta name="Subject" content="FW: More on Singularity and group releases Friendly AI guidelines">
<meta name="Date" content="2001-04-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>FW: More on Singularity and group releases Friendly AI guidelines</h1>
<!-- received="Thu Apr 19 22:56:31 2001" -->
<!-- isoreceived="20010420045631" -->
<!-- sent="Thu, 19 Apr 2001 22:55:55 -0400" -->
<!-- isosent="20010420025555" -->
<!-- name="Patrick McCuller" -->
<!-- email="patrick@kia.net" -->
<!-- subject="FW: More on Singularity and group releases Friendly AI guidelines" -->
<!-- id="LOBBLDGHBFLPJDFBIPFEKEPHGKAA.patrick@kia.net" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="More on Singularity and group releases Friendly AI guidelines" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Patrick McCuller (<a href="mailto:patrick@kia.net?Subject=FW:%20More%20on%20Singularity%20and%20group%20releases%20Friendly%20AI%20guidelines"><em>patrick@kia.net</em></a>)<br>
<strong>Date:</strong> Thu Apr 19 2001 - 20:55:55 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1173.html">Arona Ndiaye: "Re: Making HAL Your Pal"</a>
<li><strong>Previous message:</strong> <a href="1171.html">Ben Goertzel: "RE: FW: Group releases &quot;Friendly AI&quot;"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1172">[ date ]</a>
<a href="index.html#1172">[ thread ]</a>
<a href="subject.html#1172">[ subject ]</a>
<a href="author.html#1172">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
More forwarding of politech articles to SL4. These seem relevant, to me.
<br>
<p>Patrick McCuller
<br>
<p>-----Original Message-----
<br>
From: <a href="mailto:owner-politech@politechbot.com?Subject=FW:%20More%20on%20Singularity%20and%20group%20releases%20Friendly%20AI%20guidelines">owner-politech@politechbot.com</a>
<br>
[mailto:<a href="mailto:owner-politech@politechbot.com?Subject=FW:%20More%20on%20Singularity%20and%20group%20releases%20Friendly%20AI%20guidelines">owner-politech@politechbot.com</a>]On Behalf Of Declan McCullagh
<br>
Sent: Thursday, April 19, 2001 11:02 PM
<br>
To: <a href="mailto:politech@politechbot.com?Subject=FW:%20More%20on%20Singularity%20and%20group%20releases%20Friendly%20AI%20guidelines">politech@politechbot.com</a>
<br>
Cc: <a href="mailto:sentience@pobox.com?Subject=FW:%20More%20on%20Singularity%20and%20group%20releases%20Friendly%20AI%20guidelines">sentience@pobox.com</a>
<br>
Subject: FC: More on Singularity and group releases Friendly AI
<br>
guidelines
<br>
<p><p>[This is a response to my article today about the Singularity Institute and
<br>
Friendly AI, which is at <a href="http://www.politechbot.com/p-01934.html">http://www.politechbot.com/p-01934.html</a> -- it's an
<br>
excerpt from an email exchange, snipped and forwarded with
<br>
permission. --Declan
<br>
<p>**********
<br>
<p>Date: Thu, 19 Apr 2001 13:19:43 -0400
<br>
From: &quot;Eliezer S. Yudkowsky&quot; &lt;<a href="mailto:sentience@pobox.com?Subject=FW:%20More%20on%20Singularity%20and%20group%20releases%20Friendly%20AI%20guidelines">sentience@pobox.com</a>&gt;
<br>
To: Declan McCullagh &lt;<a href="mailto:declan@well.com?Subject=FW:%20More%20on%20Singularity%20and%20group%20releases%20Friendly%20AI%20guidelines">declan@well.com</a>&gt;
<br>
Subject: Re: My reaction...
<br>
<p>[...]
<br>
<p>Page title:  &quot;Making HAL your Pal&quot;.  Good title.
<br>
<p>Page summary:  &quot;What happens when artificial intelligence becomes far
<br>
smarter than humans? What will keep it friendly? The Singularity Institute
<br>
says it as the answers for what happens during the next stage of
<br>
humanity's evolution. By Declan McCullagh.&quot;  Great summary.
<br>
<p>First sentence:  &quot;Eliezer Yudkowsky has devoted his young life to an
<br>
undeniably unusual pursuit...&quot;
<br>
<p>I guess I felt that there was too much about me, and who produced
<br>
&quot;Friendly AI&quot; and why and whether he was a nice person, rather than the
<br>
actual topic of Friendly AI and the Singularity.
<br>
<p>I'm young, Declan.  I didn't go to college or even high school.  What I
<br>
rely on, for my credentials, is not the fact that I once got an absurdly
<br>
high SAT score at age eleven; what I rely on is people looking at my
<br>
present-day work and seeing that it is good.  I have no objection to your
<br>
including that quote from whichever AI researcher it was, but I also wish
<br>
you'd gone into at least a little detail on something - said something
<br>
about probabilistic supergoals, for example - so that anyone with a
<br>
knowledge of cognitive science who reads the article can say:  &quot;Hm, that
<br>
sounds like an interesting idea for Friendly AI, I'd like to know a bit
<br>
more about it.&quot;  I didn't see anything in the article that would enable
<br>
people to do that.  There are things about my history, and about what
<br>
we're trying to do, and what other people think of it, and what people
<br>
think of the Singularity, and who invented the Singularity, but not
<br>
anything about what the Singularity *is*, or what, *specifically*,
<br>
&quot;Friendly AI&quot; suggests.  Even the words &quot;Friendliness is not imposed, it's
<br>
what the AI *wants* to do&quot;, would have been enough to intrigue people -
<br>
show them a little of the future shock, the strangeness of the
<br>
Singularity.
<br>
<p>That's why I was enthused by the title and summary, but worried when I
<br>
read the first sentence.  I'd as soon turn grey and faceless and be just
<br>
another author's name on the page then permit my admittedly interesting
<br>
life story to get in the way of SIAI.
<br>
<p>Part of all this was simply my inexperience, of course.  In the future,
<br>
for example, I'll be careful to say:  &quot;Any damn fool can design a system
<br>
that will work right if nothing goes wrong.  That's why Friendly AI is
<br>
740K long.&quot;
<br>
<p>And no, the Singularity Institute doesn't have any code.  We *say* we
<br>
don't have any code.  We plaster that fact all over the place.  We *beg*
<br>
people for the funding we need to start writing code, and in the meantime,
<br>
we do what we can to make the Singularity safer.  And yet people, dammit,
<br>
don't just advise us to start writing code, which is one thing, but
<br>
actually get all *angry* at us for not having code?  How the devil do they
<br>
think code gets written?  By hiding out in the basement until you've
<br>
written a complete working AI?  How would people know we were worth
<br>
funding if we didn't do what we could in the meanwhile?
<br>
<p>[...]
<br>
<p>Of course I expect a lot of hostile reactions.  The most I can hope for is
<br>
that I can accumulate a bunch of equal and opposite good reactions to use
<br>
to oppose it.  Any crackpot can say things like &quot;Einstein was mocked!
<br>
Drexler was mocked!&quot;, and the frequent use by crackpots of that line is
<br>
exactly why I try never to use it.  Still, a hostile reaction by a
<br>
distinguished scientist neither proves nor disproves a theory that would
<br>
be expected to be controversial.  You have to judge by looking at the
<br>
content.
<br>
<p>[...]
<br>
<p>I forecast a good chance that the goverment will interfere and mess up the
<br>
Singularity.  Remember, Declan, I'm not necessarily doing this because I
<br>
forecast a large probability of success, but because these are the actions
<br>
that lead to the greatest available probability of success.
<br>
<p>--              --              --              --              --
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><p><p><p>-------------------------------------------------------------------------
<br>
POLITECH -- Declan McCullagh's politics and technology mailing list
<br>
You may redistribute this message freely if it remains intact.
<br>
To subscribe, visit <a href="http://www.politechbot.com/info/subscribe.html">http://www.politechbot.com/info/subscribe.html</a>
<br>
This message is archived at <a href="http://www.politechbot.com/">http://www.politechbot.com/</a>
<br>
-------------------------------------------------------------------------
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1173.html">Arona Ndiaye: "Re: Making HAL Your Pal"</a>
<li><strong>Previous message:</strong> <a href="1171.html">Ben Goertzel: "RE: FW: Group releases &quot;Friendly AI&quot;"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1172">[ date ]</a>
<a href="index.html#1172">[ thread ]</a>
<a href="subject.html#1172">[ subject ]</a>
<a href="author.html#1172">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
