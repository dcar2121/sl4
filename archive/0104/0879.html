<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Fragile Feelings of an AI WAS: Gender Neutral Pronouns</title>
<meta name="Author" content="Durant Schoon (durant@ilm.com)">
<meta name="Subject" content="Re: Fragile Feelings of an AI WAS: Gender Neutral Pronouns">
<meta name="Date" content="2001-04-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Fragile Feelings of an AI WAS: Gender Neutral Pronouns</h1>
<!-- received="Mon Apr 02 19:13:41 2001" -->
<!-- isoreceived="20010403011341" -->
<!-- sent="Mon, 2 Apr 2001 16:09:57 -0700 (PDT)" -->
<!-- isosent="20010402230957" -->
<!-- name="Durant Schoon" -->
<!-- email="durant@ilm.com" -->
<!-- subject="Re: Fragile Feelings of an AI WAS: Gender Neutral Pronouns" -->
<!-- id="durant-1010402160957.A0B659044@sleeper" -->
<!-- inreplyto="3AC8E68F.402013C@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Durant Schoon (<a href="mailto:durant@ilm.com?Subject=Re:%20Fragile%20Feelings%20of%20an%20AI%20WAS:%20Gender%20Neutral%20Pronouns"><em>durant@ilm.com</em></a>)<br>
<strong>Date:</strong> Mon Apr 02 2001 - 17:09:57 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0880.html">James Higgins: "Books"</a>
<li><strong>Previous message:</strong> <a href="0878.html">Durant Schoon: "Re: Eliezer and spoilers"</a>
<li><strong>In reply to:</strong> <a href="0876.html">Eliezer S. Yudkowsky: "Re: Fragile Feelings of an AI WAS: Gender Neutral Pronouns"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0880.html">James Higgins: "Books"</a>
<li><strong>Reply:</strong> <a href="0880.html">James Higgins: "Books"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#879">[ date ]</a>
<a href="index.html#879">[ thread ]</a>
<a href="subject.html#879">[ subject ]</a>
<a href="author.html#879">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; From: &quot;Eliezer S. Yudkowsky&quot; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Fragile%20Feelings%20of%20an%20AI%20WAS:%20Gender%20Neutral%20Pronouns">sentience@pobox.com</a>&gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I would never, ever include a spoiler without great big flaming warning
</em><br>
<em>&gt; signs three pages in advance.  
</em><br>
<p>Hmm, I just skimmed something about Carter-Zimmerman [a polis] and some
<br>
characters named Paolo and Elena...If the book is good, I'd rather avoid 
<br>
everything, even character relationships. (there's also a whole thread in 
<br>
the sl4 archive waiting for me when I finish the book).
<br>
<p>Ok, I'm not listening any more &quot;La-la-la-la&quot;
<br>
<p><em>&gt; There's an already-written section of Friendly AI that explains all of
</em><br>
<em>&gt; this, with diagrams... that hasn't been uploaded yet.  For now... um, I'm
</em><br>
<em>&gt; sorry to say this, but the question is so orthogonal to the proposed
</em><br>
<em>&gt; architecture that I'm not even sure where to start.  The AI treats
</em><br>
<em>&gt; programmer statements as sensory information about supergoal content.  If
</em><br>
<em>&gt; the AI takes an action and the action fails to achieve its purpose, the AI
</em><br>
<em>&gt; is less likely to try it again, but that's because the hypothesis that
</em><br>
<em>&gt; &quot;Action X will lead to Parent Goal Y&quot; has been disconfirmed by the new
</em><br>
<em>&gt; data (i.e, backpropagation of negative reinforcement and positive
</em><br>
<em>&gt; reinforcement can be shown to arise automatically from the Bayesian
</em><br>
<em>&gt; Probability Theorem plus the goal system architecture - this is where the
</em><br>
<em>&gt; diagrams come in).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It's possible to derive quantities like &quot;self-confidence&quot; (the degree to
</em><br>
<em>&gt; which an AI thinks that vis own beliefs have implications about reality),
</em><br>
<em>&gt; &quot;self-worth&quot; (the AI's estimate of vis own value to the present or future
</em><br>
<em>&gt; achievement of supergoals), and so on, but these quantities wouldn't play
</em><br>
<em>&gt; the same role as they do in humans - or a role anywhere near as important,
</em><br>
<em>&gt; given the lack of hardware social connotations.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What I expect to be the most important quantities for a Friendly AI are
</em><br>
<em>&gt; things like &quot;unity of will&quot; (the degree to which the need to use the
</em><br>
<em>&gt; programmers as auxiliary brains outweighs any expected real goal
</em><br>
<em>&gt; divergence), &quot;trust&quot; (to what degree a given programmer affirmation is
</em><br>
<em>&gt; expected to correspond to reality), &quot;a priori trust&quot; (the Bayesian priors
</em><br>
<em>&gt; for how much the programmers can be trusted, independent of any
</em><br>
<em>&gt; programmer-affirmed content), and so on.
</em><br>
<p>Actually this precisely answers my question. There is no need to model
<br>
human emotions (or emotional responses: panic, urgency, fondness, self
<br>
loathing). Using Bayes theorem for a quick and dirty heuristic solution
<br>
gets around the Framing Problem(*) assuming the AI has seen similar 
<br>
problems before. 
<br>
<p>(Well, there might be a need to model emotions in order to *understand* 
<br>
humans better, but there is probably no need to *use* that model for 
<br>
actual problem solving),
<br>
<p>(*) The Framing Problem: Correct me if I get this wrong, I'm not that
<br>
much of an AI buff, but I think the problem can be illustrated by an 
<br>
automaton trying to diffuse a bomb before it goes off. The problem is
<br>
there are so many solutions to choose from, how does ve pick the right
<br>
one in the alloted amount of time without thinking through and weighing
<br>
all the solutions first...or that's a description of the Framing Problem 
<br>
that I read once.
<br>
<p><p>On Human Emotions:
<br>
<p>(Warning: spoliers bout Steven Pinker's &quot;How the Mind Works&quot; and Matt
<br>
Ridley's &quot;The Origins of Virtue&quot; follow)
<br>
<p>I found that Steven Pinker's &quot;How the Mind Works&quot; did a great job of 
<br>
explaining human emotions in the context the computation model of mind.
<br>
The Origins of Virtue touched upon the emergence of emotions as a way out
<br>
of the Kidnapper's Dilemma(**). Pinker goes further and explains why people
<br>
genuinely feel generosity, love and compassion (basically, generous 
<br>
strategies evolve, and then cheaters, then cheater-detecters to &quot;out&quot; 
<br>
the cheaters, then strong, *genuinely* generous feelings emerge from the
<br>
selection pressure to get past the cheater-detectors - or something like 
<br>
that, it's been a couple years since I've read the book). 
<br>
<p>Pinker also describes why emotions can drive people to go berzerk and all 
<br>
sorts of fascinating examples of food taboos. If you thought OoV was too 
<br>
short, pick up HTMW. Great book, also (though not a replacement). I think
<br>
Pinker wrote the book right after spending a year with Cosmides and Tooby. 
<br>
I've also read &quot;Descarte's Error&quot; by Antonio Damasio. It was good, but not 
<br>
great like OoV and HTMW, were.
<br>
<p><p>(**) The Kidnapper's Dilamma: You've kidnapped someone, but you've
<br>
changed your mind. You can release your captive on the condition that
<br>
she won't tell anyone. The dilemma is really your captive's because
<br>
there is no way to prove to you that once she's free, she won't tell
<br>
the police. One way out is to confess an equal crime so that neither 
<br>
of you has incentive to defect (ie. snitch). Since it's probably 
<br>
unlikely that the victim has any such skeleton in her closet (or it's 
<br>
hard to convince the kidnapper), the kidnapper has no choice but to
<br>
kill her. Emotions are a way of persuading the kindapper, when in
<br>
fact there is no logical reason to let the victim go (see The Origins
<br>
of Virtue for a less-mangled description). 
<br>
<p><pre>
--
Durant
x2789
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0880.html">James Higgins: "Books"</a>
<li><strong>Previous message:</strong> <a href="0878.html">Durant Schoon: "Re: Eliezer and spoilers"</a>
<li><strong>In reply to:</strong> <a href="0876.html">Eliezer S. Yudkowsky: "Re: Fragile Feelings of an AI WAS: Gender Neutral Pronouns"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0880.html">James Higgins: "Books"</a>
<li><strong>Reply:</strong> <a href="0880.html">James Higgins: "Books"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#879">[ date ]</a>
<a href="index.html#879">[ thread ]</a>
<a href="subject.html#879">[ subject ]</a>
<a href="author.html#879">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
