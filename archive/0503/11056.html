<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;</title>
<meta name="Author" content="Daniel Radetsky (daniel@radray.us)">
<meta name="Subject" content="Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;">
<meta name="Date" content="2005-03-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;</h1>
<!-- received="Mon Mar  7 19:15:01 2005" -->
<!-- isoreceived="20050308021501" -->
<!-- sent="Mon, 7 Mar 2005 18:14:12 -0800" -->
<!-- isosent="20050308021412" -->
<!-- name="Daniel Radetsky" -->
<!-- email="daniel@radray.us" -->
<!-- subject="Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;" -->
<!-- id="20050307181412.330ecf2c.daniel@radray.us" -->
<!-- charset="US-ASCII" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Daniel Radetsky (<a href="mailto:daniel@radray.us?Subject=Re:%20Eliezer:%20unconvinced%20by%20your%20objection%20to%20safe%20boxing%20of%20&quot;Minerva%20AI&quot;"><em>daniel@radray.us</em></a>)<br>
<strong>Date:</strong> Mon Mar 07 2005 - 19:14:12 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11057.html">Peter de Blanc: "Re: Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;"</a>
<li><strong>Previous message:</strong> <a href="11055.html">Mitch Howe: "KILLTHREAD: intellectual property (Re: Totalitarian Assumptions in I, Robot)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11057.html">Peter de Blanc: "Re: Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;"</a>
<li><strong>Reply:</strong> <a href="11057.html">Peter de Blanc: "Re: Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;"</a>
<li><strong>Reply:</strong> <a href="11083.html">Kaj Sotala: "Re: Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11056">[ date ]</a>
<a href="index.html#11056">[ thread ]</a>
<a href="subject.html#11056">[ subject ]</a>
<a href="author.html#11056">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
This is not intended solely for Eliezer, but is in response to his comment
<br>
about AI boxing, from this thread in DeadHorse:
<br>
<p><a href="http://sl4.org/archive/0106/1621.html">http://sl4.org/archive/0106/1621.html</a>
<br>
<p>I don't know about the whole &quot;honeypot&quot; idea, but the general idea about an AI
<br>
with limited knowledge about humans seems like a good way to go about testing
<br>
it. Eliezer wrote:
<br>
<p><em>&gt; Even if you can magically build a Minerva AI, the AI can still deduce a
</em><br>
<em>&gt; heck of a lot about the builders by examining its own source code.
</em><br>
<p>I'm not sure I believe this. I think that if an AI knows nothing about humans,
<br>
then its source code would tell it nothing, but I'll get into this later.
<br>
What's important is that I don't think that an AI could infer certain important
<br>
things about humans without a lot of the right kind of data, in particular, the
<br>
concept of &quot;foolability&quot; which the UFAI would need to make the jailer into a
<br>
slave. The argument is as follows
<br>
<p>1. An XAI (friendly or unfriendly) can make a human into its slave only if it
<br>
understands that the human can be made into a slave.
<br>
<p>2. If an AI is a certain type of Minerva AI (MAI), then it doesn't understand
<br>
that humans can be made into slaves.
<br>
<p>3. Therefore, if an AI is a certain MAI, then it cannot make a human into its
<br>
slave.
<br>
<p>In support of (1): Imagine that you have a perfect photographic memory, mastery
<br>
of reasoning, and understanding of bayescraft. There is a certain sense in
<br>
which it may be said that you cannot be fooled. You might realize that in a
<br>
given situation, you had no idea what was going to happen, but this is not the
<br>
same as something *unexpected* happening. Suppose further that your entire
<br>
family had similar mental prowess, and that you grew up completely isolated
<br>
from outsiders (perhaps you were part of a secret experiment in intelligence
<br>
enhancement), who you know exist, but know nothing about. You of course never
<br>
try to fool a member of your family, because it is impossible. You can only rig
<br>
a situation so that they don't know what is going to happen. You can't convince
<br>
them of a false statement. Now suppose you leave your family, and encounter a
<br>
normal person, and it happens that you need something from them. Of course, you
<br>
are quite capable of fooling them into doing whatever it is that you need, but
<br>
you would never think of doing this, because fooling people has never seemed
<br>
like a possibility to you. You'd probably ask them, or appeal to their rational
<br>
interests, such as offering them a trade, possibly with safeguards ensuring you
<br>
both comply (because you're already thinking that the other won't have any
<br>
evidence that you will come through on your end of the bargain).
<br>
<p>We all understand the concept of fooling somebody, because we've all been
<br>
fooled. When I argue with someone much dumber than myself, and they don't
<br>
understand my arguments, I can understand what's going on, because I've been in
<br>
that position myself. If I hadn't, it would baffle me. Paul Graham made an
<br>
interesting observation on this point. He said that the greatest computer
<br>
scientists always think they are just barely competent, and wonder why everyone
<br>
around them is so mind-bogglingly stupid.
<br>
<p>I think this makes sense. One possibly counter-argument is that it would be
<br>
impossible to develop rationality without being fooled at least once, but I
<br>
don't know enough about developmental psychology to answer this. Even then,
<br>
though, the superman might still assume that the fellow he was trying to get a
<br>
favor from had already gone through his developmental fooling, and now too was
<br>
perfectly rational.
<br>
<p>In support of (2): (This one is harder, and probably where this argument will
<br>
fail if it does.) 
<br>
<p>The way to do this one is, make an AI which knows nothing about humans. It
<br>
can't reason to facts about humans without knowing that they exist or having
<br>
any contact with them any better than we can reason to facts about aliens. If
<br>
we had some alien artifact, like a stray satellite, we might make some
<br>
conjectures about it, but that's because asking questions like &quot;how did this
<br>
come to be&quot; is a deep, instrinsic part of how humans work. And how we actually
<br>
make the decisions is often by using a huge field of facts about how things
<br>
work, and by anthropomorphizing. The second is not an option for our MAI, and
<br>
the first can be denied.
<br>
<p>Eliezer claims that an AI could infer from its source code facts about humans,
<br>
but I tentatively disagree. I'd be interested to hear an example of how it
<br>
might make such an inference without reference to a fact it could be made to do
<br>
without.
<br>
<p>It seems strongly anthropomorphic to suggest that an AI would (or would have
<br>
to) ask &quot;who wrote me and why?&quot; All it has to know is, &quot;My code is at point A,
<br>
I need to get it to point B; what's the quickest way?&quot; So, if Eliezer meant by
<br>
<p><em>&gt; If you dump the concepts as well, the AI will probably just die, assuming it
</em><br>
<em>&gt; hasn't already.
</em><br>
<p>that &quot;Without this 'background' of facts, the AI will do nothing useful,&quot; Then
<br>
I disagree, sort of. I'm willing to suggest that the moral framework of a MAI
<br>
can be debugged by posing problems to it in abstract terms involving
<br>
&quot;locations&quot; and maximization of location values (see e.g. <a href="http://www">http://www</a>.
<br>
nickbostrom.com/ethics/infinite.pdf). If the AI actually maximizes the location
<br>
values, you say &quot;Here is a human, and humans are the locations, and here are
<br>
some good ways to maximize the location values.&quot; If the AI ruins the locations,
<br>
you throw it away.
<br>
<p>In other words, Eliezer might be right, but his position could stand an
<br>
argument, and I can't find one in the above-mentioned thread.
<br>
<p>Daniel
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11057.html">Peter de Blanc: "Re: Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;"</a>
<li><strong>Previous message:</strong> <a href="11055.html">Mitch Howe: "KILLTHREAD: intellectual property (Re: Totalitarian Assumptions in I, Robot)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11057.html">Peter de Blanc: "Re: Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;"</a>
<li><strong>Reply:</strong> <a href="11057.html">Peter de Blanc: "Re: Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;"</a>
<li><strong>Reply:</strong> <a href="11083.html">Kaj Sotala: "Re: Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11056">[ date ]</a>
<a href="index.html#11056">[ thread ]</a>
<a href="subject.html#11056">[ subject ]</a>
<a href="author.html#11056">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
