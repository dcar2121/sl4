<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: There is No Altruism</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: There is No Altruism">
<meta name="Date" content="2005-03-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: There is No Altruism</h1>
<!-- received="Wed Mar 23 15:05:15 2005" -->
<!-- isoreceived="20050323220515" -->
<!-- sent="Wed, 23 Mar 2005 14:05:09 -0800" -->
<!-- isosent="20050323220509" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: There is No Altruism" -->
<!-- id="4241E815.9030300@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="JNEIJCJJHIEAILJBFHILIEJNEEAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20There%20is%20No%20Altruism"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Mar 23 2005 - 15:05:09 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10958.html">Christopher Healey: "RE: There is No Altruism"</a>
<li><strong>Previous message:</strong> <a href="10956.html">Ben Goertzel: "RE: There is No Altruism"</a>
<li><strong>In reply to:</strong> <a href="10956.html">Ben Goertzel: "RE: There is No Altruism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10968.html">p3: "Re: There is No Altruism"</a>
<li><strong>Reply:</strong> <a href="10968.html">p3: "Re: There is No Altruism"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10957">[ date ]</a>
<a href="index.html#10957">[ thread ]</a>
<a href="subject.html#10957">[ subject ]</a>
<a href="author.html#10957">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;From <a href="http://sl4.org/wiki/DialogueOnFriendliness">http://sl4.org/wiki/DialogueOnFriendliness</a>
<br>
(Eliezer-2003 era, partially superceded by CollectiveVolition)
<br>
<p>Cathryn: All right. Suppose I wished for the genie to grab an ice cream 
<br>
cone from a little girl and give it to me. Now it might be a really 
<br>
delicious and satisfying ice cream cone, but it would still be wrong to 
<br>
take the ice cream cone away from the little girl. Isn't your definition 
<br>
of satisfaction fundamentally selfish?
<br>
<p>Dennis: I'll say! *I* should get the ice cream cone.
<br>
<p>Bernard: Well, of course, the so-called altruist is also really selfish. 
<br>
It's just that the altruist is made happy by other people's happiness, 
<br>
so he tries to make other people happy in order to increase his own 
<br>
happiness.
<br>
<p>Cathryn: That sounds like a silly definition. It sounds like a bunch of 
<br>
philosophers trying to get rid of the inconvenient square peg of 
<br>
altruism by stuffing it into an ill-fitting round hole. That is just not 
<br>
how altruism actually works in real people, Bernard.
<br>
<p>Autrey: I wouldn't dismiss the thought entirely. The philosopher Raymond 
<br>
Smullyan once asked: &quot;Is altruism sacrificing your own happiness for the 
<br>
happiness of others, or gaining your happiness through the happiness of 
<br>
others?&quot; I think that's a penetrating question.
<br>
<p>Eileen: I would say that altruism is making choices so as to maximize 
<br>
the expected happiness of others. My favorite definition of altruism is 
<br>
one I found in a glossary of Zen: &quot;Altruistic behavior: An act done 
<br>
without any intent for personal gain in any form. Altruism requires that 
<br>
there is no want for material, physical, spiritual, or egoistic gain.&quot;
<br>
<p>Cathryn: No *spiritual* gain?
<br>
<p>Eileen: That's right.
<br>
<p>Bernard: That sounds like Zen, all right - self-contradictory, 
<br>
inherently impossible of realization. Different people are made happy by 
<br>
different things, but everyone does what makes them happy. If the 
<br>
altruist were not made happy by the thought of helping others, he 
<br>
wouldn't do it.
<br>
<p>Autrey: I may be made happy by the thought of helping others. That 
<br>
doesn't mean it's the reason I help others.
<br>
<p>Cathryn: Yes, how would you account for someone who sacrifices her life 
<br>
to save someone else's? She can't possibly anticipate being happy once 
<br>
she's dead.
<br>
<p>Autrey: Some people do.
<br>
<p>Cathryn: I don't. And yet there are still things I would give my life 
<br>
for. I think. You can't ever be sure until you face the crunch.
<br>
<p>Eileen: There you go, Cathryn. There's your counterexample.
<br>
<p>Cathryn: Huh?
<br>
<p>Eileen: If your wish is to sacrifice your life so that someone else may 
<br>
live, you can't say &quot;Yes, I'm satisfied&quot; afterward.
<br>
<p>Autrey: If you have a genie on hand, you really should be able to think 
<br>
of a better solution than that.
<br>
<p>Eileen: Perhaps. Regardless, it demonstrates at least one hole in that 
<br>
definition of volition.
<br>
<p>Bernard: It is not a hole in the definition. It is never rational to 
<br>
sacrifice your life for something, precisely because you will not be 
<br>
around to experience the satisfaction you anticipate. A genie should not 
<br>
fulfill irrational wishes.
<br>
<p>Autrey: Cathryn knows very well that she cannot feel anything after she 
<br>
dies, and yet there are still things she would die for, as would I. We 
<br>
are not being tricked into that decision, we are making the choice in 
<br>
full awareness of its consequences. To quote Tyrone Pow, &quot;An atheist 
<br>
giving his life for something is a profound gesture.&quot; Where is the 
<br>
untrue thing that we must believe in order to make that decision? Where 
<br>
is the inherent irrationality? We do not make that choice in 
<br>
anticipation of feeling satisfied. We make it because some things are 
<br>
more important to us than feeling satisfaction.
<br>
<p>Bernard: Like what?
<br>
<p>Cathryn: Like ten other people living to fulfill their own wishes. All 
<br>
sentients have the same intrinsic value. If I die, and never get to 
<br>
experience any satisfaction, that's more than made up for by ten other 
<br>
people living to experience their own satisfactions.
<br>
<p>Bernard: Okay, what you're saying is that other people's happiness is 
<br>
weighted by your goal system the same as your own happiness, so that 
<br>
when ten other people are happy, you experience ten times as much 
<br>
satisfaction as when you yourself are happy. This can make it rational 
<br>
to sacrifice for other people - for example, you donate a thousand 
<br>
dollars to a charity that helps the poor, because the thousand dollars 
<br>
can create ten times as much happiness in that charity as it could 
<br>
create if you spent it on yourself. What can never be rational is 
<br>
sacrificing your life, even to save ten other lives, because you won't 
<br>
get to experience the satisfaction.
<br>
<p>Cathryn: What? You're saying that you wouldn't sacrifice your own life 
<br>
even to save the entire human species?
<br>
<p>Bernard: (Laughs.) Well, I don't always do the rational thing.
<br>
<p>Cathryn: Argh. You deserve to be locked in a cell for a week with Ayn Rand.
<br>
<p>Autrey: Bernard, I'm not altruistic because I anticipate feeling 
<br>
satisfaction. The reward is that other people benefit, not that I 
<br>
experience the realization that they benefit. Given that, it is 
<br>
perfectly rational to sacrifice my life to save ten people.
<br>
<p>Bernard: But you won't ever know those ten people lived.
<br>
<p>Autrey: So what? What I value is not &quot;the fact that Autrey knows ten 
<br>
people lived&quot;, what I value is &quot;the fact that ten people lived&quot;. I care 
<br>
about the territory, not the map. You know, this reminds me of a 
<br>
conversation I once had with Greg Stock. He thought that drugs would 
<br>
eventually become available that could simulate any feeling of 
<br>
satisfaction, not just simple ecstasy - for example, drugs that 
<br>
simulated the feeling of scientific discovery. He then went on to say 
<br>
that he thought that once this happened, everyone would switch over to 
<br>
taking the drugs, because real scientific discovery wouldn't be able to 
<br>
compare.
<br>
<p>Cathryn: Yikes. I wouldn't go near a drug like that with a ten-lightyear 
<br>
pole.
<br>
<p>Autrey: That's what I said, too - that I wanted to genuinely help 
<br>
people, not just feel like I was doing so. &quot;No,&quot; said Greg Stock, &quot;you'd 
<br>
take them anyway, because no matter how much you helped people, the 
<br>
drugs would still make you feel ten times better.&quot;
<br>
<p>Cathryn: That assumes I'd take the drugs to begin with, which I wouldn't 
<br>
ever do. I don't want to be addicted. I don't want to be transformed 
<br>
into the person those drugs would make me.
<br>
<p>Autrey: The strange thing was that Greg Stock didn't seem to mind the 
<br>
prospect. It sounded like he saw it as a natural development.
<br>
<p>Cathryn: So where'd the conversation go after that?
<br>
<p>Autrey: I wanted to talk about the difference between psychological 
<br>
egoism and psychological altruism. But it was a bit too much territory 
<br>
to cover in the thirty seconds of time I had available.
<br>
<p>Dennis: Psychological egoism and psychological altruism? Eh?
<br>
<p>Eileen: The difference between a goal system that optimizes an internal 
<br>
state and a goal system that optimizes an external state.
<br>
<p>Cathryn: There's a formal difference?
<br>
<p>Eileen: Yes.
<br>
<p>Bernard: No.
<br>
<p>Cathryn: Interesting.
<br>
<p>Autrey: In philosophy, this is known as the egoism debate. It's been 
<br>
going on for a while. I don't really agree with the way the arguments 
<br>
are usually phrased, but I can offer a quick summary anyway. You want one?
<br>
<p>Dennis: Yeah.
<br>
<p>Autrey: Okay. Psychological egoism is the position that all our ultimate 
<br>
ends are self-directed. That is, we can want external things as means to 
<br>
an end, but all our ultimate ends - all things that we desire in 
<br>
themselves rather than for their consequences - are self-directed in the 
<br>
sense that their propositional content is about our own states.
<br>
<p>Eileen: Propositional content? Sounds rather GOFAI-ish.
<br>
<p>Autrey: Maybe, but it's the way the standard debate is phrased. Anyway, 
<br>
let's say I want it to be the case that I have a chocolate bar. This 
<br>
desire is purely self-directed, since the propositional content mentions 
<br>
me and no other agent. On the other hand, suppose I want it to be the 
<br>
case that Jennie has a candy bar. This desire is other-directed, since 
<br>
the propositional content mentions another person, Jennie, but not 
<br>
myself. Psychological egoism claims that all our ultimate desires are 
<br>
self-directed; psychological altruism says that at least some of our 
<br>
ultimate desires are other-directed.
<br>
<p>Bernard: If you want Jennie to have a candy bar, it means that you would 
<br>
be happy if Jennie got a candy bar. Your real end is always happiness.
<br>
<p>Autrey: That's known as psychological hedonism, which is a special case 
<br>
of psychological egoism. As Sober and Wilson put it, &quot;The hedonist says 
<br>
that the only ultimate desires that people have are attaining pleasure 
<br>
and avoding pain... the salient fact about hedonism is its claim that 
<br>
people are motivational solipsists; the only things they care about 
<br>
ultimately are states of their own consciousness. Although hedonists 
<br>
must be egoists, the reverse isn't true. For example, if people desire 
<br>
their own survival as an end in itself, they may be egoists, but they 
<br>
are not hedonists.&quot; Another quote from the same authors: &quot;Avoiding pain 
<br>
is one of our ultimate goals. However, many people realize that being in 
<br>
pain reduces their ability to concentrate, so they may sometimes take an 
<br>
aspirin in part because they want to remove a source of distraction. 
<br>
This shows that the things we want as ends in themselves we may also 
<br>
want for instrumental reasons... When psychological egoism seeks to 
<br>
explain why one person helped another, it isn't enough to show that one 
<br>
of the reasons for helping was self-benefit; this is quite consistent 
<br>
with there being another, purely altruistic, reason that the individual 
<br>
had for helping. Symmetrically, to refute egoism, one need not cite 
<br>
examples of helping in which only other-directed motives play a role. If 
<br>
people sometimes help for both egoistic and altruistic ultimate reasons, 
<br>
then psychological egoism is false.&quot;
<br>
<p>Dennis: The very notion of altruism is incoherent.
<br>
<p>Autrey: That argument is indeed the chief reason why some philosophers 
<br>
espouse psychological hedonism.
<br>
<p>Cathryn: Sounds like a lot of silly philosophizing to me. Does it really 
<br>
matter whether I'm considered a &quot;motivational solipsist&quot; or whatever, as 
<br>
long as I actually help people?
<br>
<p>Bernard: That's just it! It doesn't make any operational difference - 
<br>
all goal systems operate to maximize their internal satisfaction, no 
<br>
matter what external events cause satisfaction.
<br>
<p>Eileen: That's not true; it does make an operational difference. If 
<br>
Autrey values the solipsistic psychological event of knowing he saved 
<br>
ten lives, he will never sacrifice his own life to save ten other lives; 
<br>
if he values those ten lives in themselves, he may. You told him that, 
<br>
remember?
<br>
<p>Bernard: Well, I guess Autrey might value the instantaneous happiness of 
<br>
knowing he chose to save ten lives, more than he values all the 
<br>
happiness he might achieve in the rest of his life.
<br>
<p>Cathryn: That doesn't sound anything remotely like the way real people 
<br>
think. Square peg, round hole.
<br>
<p>Autrey: Do you have anything new to contribute to the debate, Eileen? 
<br>
It's a pretty ancient issue in philosophy.
<br>
<p>Eileen: The basic equation for a Bayesian decision system is usually 
<br>
phrased something like D(a) = Sum U(x)P(x|a). This is known as the 
<br>
expected utility equation, and it was derived by von Neumann and 
<br>
Morgenstern in 1944 as a unique constraint on preference orderings for 
<br>
all systems that obey certain consistency axioms -
<br>
<p>Dennis: Start over.
<br>
<p>Eileen: Okay. Imagine that D(a) stands for the &quot;desirability&quot; of an 
<br>
action A, that U(x) stands for the &quot;utility&quot; of a state of the universe 
<br>
X, and P(x|a) is your assigned &quot;probability&quot; that the state X occurs, 
<br>
given that you take action A. For example, let's say that I show you two 
<br>
spinner wheels, the red spinner and the green spinner. One-third of the 
<br>
red spinner wheel is black, while two-thirds of the green spinner wheel 
<br>
is white. Both spinners have a dial that I'm going to spin around until 
<br>
it settles at random into a red or black area (for the red spinner) or a 
<br>
white or green area (for the green spinner). The red spinner has a 
<br>
one-third chance of turning up black, while the green spinner has a 
<br>
two-thirds chance of turning up white. Let's say that I offer you one of 
<br>
two choices; you can pick the red spinner and get a chocolate ice cream 
<br>
cone if the spinner turns up black, or you can pick the green spinner 
<br>
and get a vanilla ice cream cone if the spinner turns up white.
<br>
<p>(And then it gets complicated...)
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10958.html">Christopher Healey: "RE: There is No Altruism"</a>
<li><strong>Previous message:</strong> <a href="10956.html">Ben Goertzel: "RE: There is No Altruism"</a>
<li><strong>In reply to:</strong> <a href="10956.html">Ben Goertzel: "RE: There is No Altruism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10968.html">p3: "Re: There is No Altruism"</a>
<li><strong>Reply:</strong> <a href="10968.html">p3: "Re: There is No Altruism"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10957">[ date ]</a>
<a href="index.html#10957">[ thread ]</a>
<a href="subject.html#10957">[ subject ]</a>
<a href="author.html#10957">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:22:55 MST
</em></small></p>
</body>
</html>
