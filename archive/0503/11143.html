<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Overconfidence and meta-rationality</title>
<meta name="Author" content="Tennessee Leeuwenburg (tennessee@tennessee.id.au)">
<meta name="Subject" content="Re: Overconfidence and meta-rationality">
<meta name="Date" content="2005-03-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Overconfidence and meta-rationality</h1>
<!-- received="Tue Mar 22 17:39:42 2005" -->
<!-- isoreceived="20050323003942" -->
<!-- sent="Wed, 23 Mar 2005 11:35:37 +1100" -->
<!-- isosent="20050323003537" -->
<!-- name="Tennessee Leeuwenburg" -->
<!-- email="tennessee@tennessee.id.au" -->
<!-- subject="Re: Overconfidence and meta-rationality" -->
<!-- id="4240B9D9.4070200@tennessee.id.au" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="423E7C63.5000400@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tennessee Leeuwenburg (<a href="mailto:tennessee@tennessee.id.au?Subject=Re:%20Overconfidence%20and%20meta-rationality"><em>tennessee@tennessee.id.au</em></a>)<br>
<strong>Date:</strong> Tue Mar 22 2005 - 17:35:37 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11144.html">Stephen Tattum: "Universal Replicators..."</a>
<li><strong>Previous message:</strong> <a href="11142.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<li><strong>In reply to:</strong> <a href="11142.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11143">[ date ]</a>
<a href="index.html#11143">[ thread ]</a>
<a href="subject.html#11143">[ subject ]</a>
<a href="author.html#11143">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
-----BEGIN PGP SIGNED MESSAGE-----
<br>
Hash: SHA1
<br>
<p><p>| This is why, when people accusingly say the Singularity is a
<br>
| religious concept, or claim that hard takeoff is inspired by
<br>
| apocalyptic dreaming, I feel that my best reply remains my
<br>
| arguments about the dynamics of recursively self-improving AI.
<br>
| That question stands in closer causal proximity to the matter of
<br>
| interest.  If I establish that we can (or cannot) expect a
<br>
| recursively self-improving AI to go FOOM based on arguments purely
<br>
| from the dynamics of cognition, that renders the matter of interest
<br>
|  conditionally irrelevant on arguments about psychological
<br>
| apocalyptism.
<br>
<p>Obviously. Understanding the Singularity really only takes a few steps.
<br>
<p>1.) AI is possible.
<br>
2.) Superhuman AI is possible given (1)
<br>
3.) AI will seek to improve its' IQ indefinately
<br>
<p>All attacks on the singularity attack (1), (2) or (3) either
<br>
qualitatively or quantitatively, and frequently the attacks are not
<br>
based on science or strict rationality. Most Singularists, I would
<br>
say, would order those steps, in order of likelihood or being correct,
<br>
2, 3, 1. That is, most people think that if AI is possible, getting AI
<br>
even smarter should be pretty easy. Also, no-one can really see why
<br>
anyone would prevent themselves from getting smarter, so they give in
<br>
fairly readily to (3) also. Most challenges come in to the nature and
<br>
quantity of (1). Sometimes these arguments are a reaction directly to
<br>
(1), but may come as people disagree with any later stage, or the
<br>
concept of the Singularity itself.
<br>
<p>I'm sure I don't need to go into it, but I will outline some of those
<br>
arguments by heading -- the argument from qualia, the argument from
<br>
behaviourism, the argument from religion/spirituality, the argument
<br>
from feasibility. For whatever reason, people who argue against the
<br>
Singularity can be broadly split into those who don't like the idea of
<br>
the Singularity, and those who just don't think it will work.
<br>
<p>Your argument about the Wright brothers is a simple analogy dealing
<br>
with the practical elements of (1). However, the Wright brothers now
<br>
have something you still lack - proof by example. To use your language
<br>
- - arguments about the feasibility of AI are conditionally independant
<br>
of the AI once it has been measured.
<br>
<p>The world is big enough, and has enough money, that it can afford to
<br>
wait for proof. There is no reason to deny your arguments absolutely,
<br>
nor to accept them unconditionally. I accept in principle the
<br>
fundamental thinkings of this list, but we still lack even good
<br>
neuroscience of IQ, let alone truly understanding qualia or anything
<br>
like that. It's like there exists a proper framework for discussing
<br>
intelligence, but no &quot;physics&quot; of intelligence. Cognitive science
<br>
still lacks the ability to describe and predict many things.
<br>
<p>I don't think that the feasibility of the Singularity can be
<br>
established /a priori/, because I don't think it's obviously true than
<br>
machines will achieve both AI and the experience of qualia. Nor do I
<br>
think it's obviously true that machines will achieve the indefinite
<br>
improvement in IQ suggested by by Singularist positions.
<br>
<p>I don't have a drawing tablet, so let me &quot;describe&quot; a graph to you.
<br>
<p>The Y axis represents IQ, and the X axis represents complexity of
<br>
configuration. As the complexity of configuration increases, it
<br>
becomes more and more difficult to build or understand that kind of
<br>
brain. A point on the graph represents the IQ of a brain built at that
<br>
level of complexity.
<br>
<p>Now, what configurations might lead to superhuman AI? Is there a
<br>
steady rise of intelligence with complexity? If the answer is yes,
<br>
there will be consistently increasing levels of IQ with complexity.
<br>
But what if the answer is no? Surely we can imagine some highly
<br>
complex configurations which do not produce good minds, and some good
<br>
minds which will be produced by configurations of lesser complexity
<br>
that this?
<br>
<p>If that is the case, let us imagine a graph with many turning points,
<br>
and many peaks and troughs, covering the area. This, perhaps,
<br>
represents the true map for possible brains to the resulting IQ.
<br>
<p>If this is the case, it's possible for the Singularity to get trapped
<br>
in relative peaks - too stupid to understand how to move from its
<br>
existing configuration to a more complex one beyond its' current
<br>
understanding. We currently sit in an evolutionary peak - there is
<br>
little selective pressure to change our biology, but perhaps we are
<br>
also in a cognitive one. Or, more likely, perhaps our artificial
<br>
descendants will become trapped in a cognitive peak.
<br>
<p>I would argue this is probably *true*. Imagine that the line between
<br>
IQ and complexity takes a random walk. Also imagine a line going up at
<br>
45 degrees from the origin. This line represents the complexity of
<br>
mind comprehensible to a particular IQ.
<br>
<p>This means :
<br>
<p>* It's possible to have a simple, effective mind which can understand
<br>
minds more complex than itself, to a point. These other minds may or
<br>
may not have greater IQ.
<br>
<p>* It's possible to have a complex mind, which has a low IQ, and does
<br>
not properly understand itself
<br>
<p>* To find out what other minds a particular mind can understand, find
<br>
out where the &quot;complexity line&quot; intersects the current IQ. Anything
<br>
less complex than this intersection point can be understood.
<br>
Complexity is defined in such a way as to make this a truism.
<br>
<p>The alternative configurations available to mind (A) are anything
<br>
equal or less than the complexity allowed by its' IQ. This *may* allow
<br>
it to envision alternative, more complex minds with IQs that will be
<br>
higher still, or it *may not*.
<br>
<p>This is how I suppose things to really be, with one qualification:
<br>
There is a *general tendency* for minds with a higher IQ to be more
<br>
complex
<br>
<p>As a result, I challenge (3) more than either (1) or (2). That is, I
<br>
believe some degree of AI is possible. I think humans are clearly
<br>
biological machines, and am not here challenging the religious
<br>
position. I also think that any intelligence is likely to explore
<br>
superhuman intelligences. However, I don't think it's necessarily true
<br>
that this is always achievable, because there is nothing &quot;automatic&quot;
<br>
about understanding how to reach ever higher levels of intelligence.
<br>
<p>Given the current levels of human self-understanding, it might well be
<br>
that our own minds are too complex for us to understand fully.
<br>
However, given our success in creating useful machines, it might be
<br>
that a less complex artificial mind of superhuman intelligence is
<br>
possible. In fact, it is this possibility of increased
<br>
self-understanding that makes superhuman AI so attractive.
<br>
<p>Hmm, well, I'd better get back to work. Sorry if there's too much
<br>
sloppy thinking in the above email, it is a response, not a paper.
<br>
<p>Cheers,
<br>
- -Tennessee
<br>
-----BEGIN PGP SIGNATURE-----
<br>
Version: GnuPG v1.2.5 (GNU/Linux)
<br>
Comment: Using GnuPG with Thunderbird - <a href="http://enigmail.mozdev.org">http://enigmail.mozdev.org</a>
<br>
<p>iD8DBQFCQLnZFp/Peux6TnIRArpHAJ9F6NbPm/Qqpf5HYxn5KHrTQ8FTtQCfQWsh
<br>
CghDDl1E9cFCK+T3sAu+HMU=
<br>
=uwBN
<br>
-----END PGP SIGNATURE-----
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11144.html">Stephen Tattum: "Universal Replicators..."</a>
<li><strong>Previous message:</strong> <a href="11142.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<li><strong>In reply to:</strong> <a href="11142.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11143">[ date ]</a>
<a href="index.html#11143">[ thread ]</a>
<a href="subject.html#11143">[ subject ]</a>
<a href="author.html#11143">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
