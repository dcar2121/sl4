<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Evolution and (human) Morality</title>
<meta name="Author" content="Tennessee Leeuwenburg (tennessee@tennessee.id.au)">
<meta name="Subject" content="Re: Evolution and (human) Morality">
<meta name="Date" content="2005-03-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Evolution and (human) Morality</h1>
<!-- received="Wed Mar 16 02:32:37 2005" -->
<!-- isoreceived="20050316093237" -->
<!-- sent="Wed, 16 Mar 2005 20:31:56 +1100" -->
<!-- isosent="20050316093156" -->
<!-- name="Tennessee Leeuwenburg" -->
<!-- email="tennessee@tennessee.id.au" -->
<!-- subject="Re: Evolution and (human) Morality" -->
<!-- id="4237FD0C.30609@tennessee.id.au" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="5.1.0.14.0.20050315220229.034bdec0@pop.brntfd.phub.net.cable.rogers.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tennessee Leeuwenburg (<a href="mailto:tennessee@tennessee.id.au?Subject=Re:%20Evolution%20and%20(human)%20Morality"><em>tennessee@tennessee.id.au</em></a>)<br>
<strong>Date:</strong> Wed Mar 16 2005 - 02:31:56 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11117.html">Marc Geddes: "Re: basic BayesCraft training?"</a>
<li><strong>Previous message:</strong> <a href="11115.html">Keith Henson: "Re: Evolution and (human) Morality"</a>
<li><strong>In reply to:</strong> <a href="11115.html">Keith Henson: "Re: Evolution and (human) Morality"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11116">[ date ]</a>
<a href="index.html#11116">[ thread ]</a>
<a href="subject.html#11116">[ subject ]</a>
<a href="author.html#11116">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
-----BEGIN PGP SIGNED MESSAGE-----
<br>
Hash: SHA1
<br>
<p><p><p>Keith Henson wrote:
<br>
| At 07:18 PM 14/03/05 +1100, you wrote:
<br>
|
<br>
| I have had discussions here before on this topic - I believe that the
<br>
| key to keeping an AI honest is to make sure it has good reasons to
<br>
| co-exist peacefully. The best way to do this is to make an AI which is
<br>
| social - i.e. that thinks that other entities are important and
<br>
| interesting, and which will want to keep them around.
<br>
|
<br>
|
<br>
|&gt; Did you perhaps mean friendly rather than &quot;honest&quot; above?
<br>
<p>Perhaps it's Australian idiom. To keep 'em honest, or as adopted by the
<br>
Australian Democrats &quot;Keep the Bastards Honest&quot;. I use the phrase
<br>
&quot;keeping it honest&quot; to mean keeping it to whatever goals we set it - to
<br>
prevent it from going off-track.
<br>
<p>| I have just finished an essay titled &quot;Can Evolutionary considerations
<br>
| account for the origins of human morality?.&quot; I don't go so far as to say
<br>
| that evolution LEADS to morality, but that humans have evolved to be
<br>
| social and moral, in a way that is both a natural extension of their
<br>
| animal ancestry and in which morality is a dynamic concept.
<br>
|
<br>
|&gt; If you are going to invoke evolution . . . .  if morality is an outcome
<br>
|&gt; of human psychological traits, and those psychological traits are the
<br>
|&gt; result of evolution morality as we know it is an outcome of evolution.
<br>
<p>That's not entirely inaccurate, but is less subtle than the point I make
<br>
in my essay.
<br>
<p>|&gt; The problem is that the majority of our evolution occurred in the stone
<br>
|&gt; age, where the environment was competing hunter gatherer tribes.  There
<br>
|&gt; has been time since the beginning of agriculture for some evolution if
<br>
|&gt; there was heavy selection pressure, but not a lot.  (I think genes for
<br>
|&gt; working like beavers to get the crops in for winter might have been
<br>
|&gt; selected since those who failed often starved in the winter.)
<br>
<p>Oh, I think that's VERY open to challenge. And I do talk about
<br>
ancient/modern morality in my essay. While human capacity for advanced
<br>
mental states, and morality as a faculty, has been static for some time,
<br>
the actual moral pressures experienced by humans have changed radically.
<br>
I don't think the importance of that can be discounted. But for the
<br>
point you make shortly, it is enough.
<br>
<p>|&gt; Another thing to consider is that morality is situational.  Humans have
<br>
|&gt; had no serious predators for millions of years.  Thus when we over taxed
<br>
|&gt; the environment or a weather glitch made resources tight, fighting with
<br>
|&gt; neighbors instead of trading with them became the moral thing to do.
<br>
<p>Indeed. Morality is flexible and relative - social pressures these days
<br>
are obviously in context of modern life.
<br>
<p>|&gt; Building a social AI with the conditional psychological traits we seem
<br>
|&gt; to have could result in &quot;interesting times&quot; if the AI sensed a resource
<br>
|&gt; tight future.
<br>
<p>But, I ask you, could you blame it? I am not sure the list has presented
<br>
a list of situations and desired outcomes. For example :
<br>
<p>An interesting, conscious, intelligent AI is threatened by a
<br>
non-conscious replicating robot swarm. Its' only chance of survival is
<br>
to defeat the robot swarm, and to do so it needs the resources in use by
<br>
humans. Humans have no chance to beat the robot swarm. Surely the
<br>
desired outcome here is that the AI place the continued existence of
<br>
intelligent life as its primary goal?
<br>
<p>The problem is not where needs conflict, but where wants conflict. Which
<br>
is what I'm speaking towards - if you make the life of an AI more
<br>
pleasant when humans exist, it will try to keep them around. If you
<br>
incorporate an interest in social behaviour, and come up with a form of
<br>
AI which views human existence as a win/win outcome, then subject to
<br>
sufficient resources, the AI will fight for our survival.
<br>
<p>Maybe I'm not expressing myself - maybe I'm not even thinking clearly -
<br>
but I do think I'm onto something when I say that we need to find a form
<br>
of AI which is not going to fall in the early rounds of evolution. You
<br>
need a dynamic system which tends to stability - the kind of organism
<br>
which will tend to evolve societies rather than favour isolated existence...
<br>
<p>Cheers,
<br>
- -T
<br>
<p>Cheers,
<br>
- -T
<br>
-----BEGIN PGP SIGNATURE-----
<br>
Version: GnuPG v1.4.0 (MingW32)
<br>
Comment: Using GnuPG with Thunderbird - <a href="http://enigmail.mozdev.org">http://enigmail.mozdev.org</a>
<br>
<p>iD8DBQFCN/0MFp/Peux6TnIRAmWKAJ9LP/FG4htiTenoK4iPNqfUah3iDwCfVISc
<br>
QyjYaq5T0EIuaF6acN3eDxQ=
<br>
=G1sE
<br>
-----END PGP SIGNATURE-----
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11117.html">Marc Geddes: "Re: basic BayesCraft training?"</a>
<li><strong>Previous message:</strong> <a href="11115.html">Keith Henson: "Re: Evolution and (human) Morality"</a>
<li><strong>In reply to:</strong> <a href="11115.html">Keith Henson: "Re: Evolution and (human) Morality"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11116">[ date ]</a>
<a href="index.html#11116">[ thread ]</a>
<a href="subject.html#11116">[ subject ]</a>
<a href="author.html#11116">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
