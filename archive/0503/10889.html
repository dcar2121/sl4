<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Overconfidence and meta-rationality</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Overconfidence and meta-rationality">
<meta name="Date" content="2005-03-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Overconfidence and meta-rationality</h1>
<!-- received="Sat Mar 12 23:00:55 2005" -->
<!-- isoreceived="20050313060055" -->
<!-- sent="Sat, 12 Mar 2005 21:57:32 -0800" -->
<!-- isosent="20050313055732" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Overconfidence and meta-rationality" -->
<!-- id="4233D64C.1060706@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="6.2.1.2.2.20050312125254.02de9100@mail.gmu.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Overconfidence%20and%20meta-rationality"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Mar 12 2005 - 22:57:32 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10890.html">Kaj Sotala: "Re: Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;"</a>
<li><strong>Previous message:</strong> <a href="10888.html">Ben Goertzel: "Lojban and AI"</a>
<li><strong>Maybe in reply to:</strong> <a href="../0502/10764.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10901.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10889">[ date ]</a>
<a href="index.html#10889">[ thread ]</a>
<a href="subject.html#10889">[ subject ]</a>
<a href="author.html#10889">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Robin Hanson wrote:
<br>
<em>&gt; Eliezer, you are just writing far too much for me to comment on all of 
</em><br>
<em>&gt; it.
</em><br>
<p>Yes.  I know.  You don't have to comment on all of it.  I just thought I 
<br>
should say all of it before you wrote your book, rather than afterward.
<br>
<p>I don't think that this issue is simple - you did say you wanted to 
<br>
write a book on it - so I don't think that the volume of discussion is 
<br>
inappropriate to the question.  I understand that your time is 
<br>
constrained, as is mine.
<br>
<p>If you allege that I don't seem interested in the math, you have to 
<br>
expect a certain probability of a long answer.
<br>
<p><em>&gt; If you give me an indication of what your key points are, I will 
</em><br>
<em>&gt; try to respond to those points.
</em><br>
<p>If I had to select out two points as most important, they would be:
<br>
<p>1) Just because perfect Bayesians, or even certain formally imperfect 
<br>
Bayesians that are still not like humans, *will* always agree; it does 
<br>
not follow that a human rationalist can obtain a higher Bayesian score 
<br>
(truth value), or the maximal humanly feasible score, by deliberately 
<br>
*trying* to agree more with other humans, even other human rationalists.
<br>
<p>2) Just because, if everyone agreed to do X without further argument or 
<br>
modification (where X is not agreeing to disagree), the average Bayesian 
<br>
score would increase relative to its current position, it does not 
<br>
follow that X is the *optimal* strategy.
<br>
<p><em>&gt; For now, I will just make a few 
</em><br>
<em>&gt; comments on specific claims.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; At 06:40 PM 3/9/2005, Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; The modesty argument uses Aumann's Agreement Theorem and AAT's 
</em><br>
<em>&gt;&gt; extensions as plugins, but the modesty argument itself is not formal 
</em><br>
<em>&gt;&gt; from start to finish.  I know of no *formal* extension of Aumann's 
</em><br>
<em>&gt;&gt; Agreement Theorem such that its premises are plausibly applicable to 
</em><br>
<em>&gt;&gt; humans.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Then see:  &lt;<a href="http://hanson.gmu.edu/disagree.pdf">http://hanson.gmu.edu/disagree.pdf</a>&gt;For Bayesian Wannabes, 
</em><br>
<em>&gt; Are Disagreements Not About Information? 
</em><br>
<em>&gt; &lt;<a href="http://www.kluweronline.com/issn/0040-5833/">http://www.kluweronline.com/issn/0040-5833/</a>&gt;Theory and Decision 
</em><br>
<em>&gt; 54(2):105-123, March 2003.
</em><br>
<p>(I immediately notice that your proof of Lemma 1 describes a Bayesian 
<br>
Wannabe as wishing to minimize her expected squared error.  Orthodox 
<br>
statisticians minimize their expected squared error because, like, 
<br>
that's what orthodox statisticians do all day.  As described in TechExp, 
<br>
Bayesians maximize their expectation of the logarithm of the probability 
<br>
assigned to the actual outcome, which equates to minimizing expected 
<br>
squared error when the error is believed to possess a Gaussian 
<br>
distribution and the prior probability density is uniform.
<br>
<p>I don't think this is really important to the general thrust of your 
<br>
paper, but it deserves noting.)
<br>
<p>On to the main issue.  These Bayesian Wannabes are still unrealistically 
<br>
skilled rationalists; no human is a Bayesian Wannabe as so defined.  BWs 
<br>
do not self-deceive.  They approximate their estimates of deterministic 
<br>
computations via guesses whose error they treat as random variables.
<br>
<p>I remark on the wisdom of Jaynes who points out that 'randomness' exists 
<br>
in the map rather than the territory; random variables are variables of 
<br>
which we are ignorant.  I remark on the wisdom of Pearl, who points out 
<br>
that when our map sums up many tiny details we can't afford to compute, 
<br>
it is advantageous to retain the Markov property, and hence humans 
<br>
regard any map without the Markov property as unsatisfactory; we say it 
<br>
possesses unexplained correlations and hence is incomplete.
<br>
<p>If the errors in BWs computations are uncorrelated random errors, the 
<br>
BWs are, in effect, simple measuring instruments, and they can treat 
<br>
each other as such, combining their two measurements to obtain a third, 
<br>
more reliable measurement.
<br>
<p>If we assume the computation errors follow a bell curve, we obtain a 
<br>
constructive procedure for combining the computations of any number of 
<br>
agents; the best group guess is the arithmetic mean of the individual 
<br>
guesses.
<br>
<p>How long is the Emperor of China's nose?
<br>
<p><em>&gt;&gt; you say:  &quot;If people mostly disagree because they systematically 
</em><br>
<em>&gt;&gt; violate the rationality standards that they profess, and hold up for 
</em><br>
<em>&gt;&gt; others, then we will say that their disagreements are dishonest.&quot;  (I 
</em><br>
<em>&gt;&gt; would disagree with your terminology; they might be dishonest *or* 
</em><br>
<em>&gt;&gt; they might be self-deceived. ...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I was taking self-deception to be a kind of dishonesty.
</em><br>
<p>Life would be so much simpler if it were.  Being honest is difficult and 
<br>
often socially unrewarding, but halting self-deception is harder.
<br>
<p><em>&gt;&gt; ... if Aumann's Agreement Theorem is wrong (goes wrong reliably in the 
</em><br>
<em>&gt;&gt; long run, not just failing 1 time out of 100 when the consensus belief 
</em><br>
<em>&gt;&gt; is 99% probability) then we can readily compare the premises of AAT 
</em><br>
<em>&gt;&gt; against the dynamics of the agents, their updating, their prior 
</em><br>
<em>&gt;&gt; knowledge, etc., and track down the mistaken assumption that caused 
</em><br>
<em>&gt;&gt; AAT (or the extension of AAT) to fail to match physical reality. ...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This actually seems to me rather hard, as it is hard to observe people's 
</em><br>
<em>&gt; priors.
</em><br>
<p>Is it hard to observe the qualitative fact of whether or not humans' 
<br>
priors agree?  Well, yes, I suppose, as humans, not being Bayesians, 
<br>
possess no distinguished priors.
<br>
<p>But it is a relatively straightforward matter to tell whether humans 
<br>
behave like Aumann agents.  They don't.  Similarly, I think it would be 
<br>
a relatively straightforward matter to sample whether Aumann agents 
<br>
indeed all had the same priors, as they believed, if they had agreed to 
<br>
disagree and therefore the premises stood in doubt.  Since Aumann agents 
<br>
know their priors and can presumably report them.
<br>
<p><em>&gt;&gt; ... You attribute the great number of extensions of AAT to the 
</em><br>
<em>&gt;&gt; following underlying reason:  &quot;His [Aumann's] results are robust 
</em><br>
<em>&gt;&gt; because they are based on the simple idea that when seeking to 
</em><br>
<em>&gt;&gt; estimate the truth, you should realize you might be wrong; others may 
</em><br>
<em>&gt;&gt; well know things that you do not.&quot;
</em><br>
<em>&gt;&gt; I disagree; this is *not* what Aumann's results are based on.
</em><br>
<em>&gt;&gt; Aumann's results are based on the underlying idea that if other 
</em><br>
<em>&gt;&gt; entities behave in a way understandable to you, then their observable 
</em><br>
<em>&gt;&gt; behaviors are relevant Bayesian evidence to you.  This includes the 
</em><br>
<em>&gt;&gt; behavior of assigning probabilities according to understandable 
</em><br>
<em>&gt;&gt; Bayesian cognition.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The paper I cite above is not based on having a specific model of the 
</em><br>
<em>&gt; other's behavior.
</em><br>
<p>The paper you cite above does not yield a constructive method of 
<br>
agreement without additional assumptions.  But then the paper does not 
<br>
prove agreement *given* a set of assumptions.  As far as I can tell, the 
<br>
paper says that Bayesian Wannabes who agree to disagree about 
<br>
state-independent computations and who treat their computation error as 
<br>
a state-independent &quot;random&quot; variable - presumably meaning, a variable 
<br>
of whose exact value they are to some degree ignorant - must agree to 
<br>
disagree about a state-independent random variable.
<br>
<p><em>&gt;&gt; So A and B are *not* compromising between their previous positions; 
</em><br>
<em>&gt;&gt; their consensus probability assignment is *not* a linear weighting of 
</em><br>
<em>&gt;&gt; their previous assignments.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes, of course, who ever said it was?
</em><br>
<p>If two people who find that they disagree immediately act to eliminate 
<br>
their disagreement (which should be &quot;much easier&quot;), what should they 
<br>
compromise on, if not a weighted mix of their probability distributions 
<br>
weighted by an agreed-upon estimate of relative rationality on that problem?
<br>
<p><em>&gt;&gt; ... If this were AAT, rather than a human conversation, then as Fred 
</em><br>
<em>&gt;&gt; and I exchanged probability assignments our actual knowledge of the 
</em><br>
<em>&gt;&gt; moon would steadily increase; our models would concentrate into an 
</em><br>
<em>&gt;&gt; ever-smaller set of possible worlds.  So in this sense the dynamics of 
</em><br>
<em>&gt;&gt; the modesty argument are most unlike the dynamics of Aumann's 
</em><br>
<em>&gt;&gt; Agreement Theorem, from which the modesty argument seeks to derive its 
</em><br>
<em>&gt;&gt; force.  AAT drives down entropy (sorta); the modesty argument 
</em><br>
<em>&gt;&gt; doesn't.  This is a BIG difference.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; AAT is *not* about dynamics at all.  It might require a certain dynamics 
</em><br>
<em>&gt; to reach the state where AAT applies, but this paper of mine applies at 
</em><br>
<em>&gt; any point during any conversation:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &lt;<a href="http://hanson.gmu.edu/unpredict.pdf">http://hanson.gmu.edu/unpredict.pdf</a>&gt;Disagreement Is Unpredictable. 
</em><br>
<em>&gt; &lt;<a href="http://www.sciencedirect.com/science/journal/01651765">http://www.sciencedirect.com/science/journal/01651765</a>&gt;Economics Letters 
</em><br>
<em>&gt; 77(3):365-369, November 2002.
</em><br>
<p>I agree that rational agents will not be able to predict the direction 
<br>
of the other agent's disagreement.  But I don't see what that has to do 
<br>
with my observation, that human beings who attempt to immediately agree 
<br>
with each other will not necessarily know more after compromising than 
<br>
they started out knowing.
<br>
<p><em>&gt;&gt; The AATs I know are constructive; they don't just prove that agents 
</em><br>
<em>&gt;&gt; will agree as they acquire common knowledge, they describe *exactly 
</em><br>
<em>&gt;&gt; how* agents arrive at agreement.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Again, see my Theory and Decision paper cited above.
</em><br>
<p>As far as I can see, this paper is not constructive, but that is because 
<br>
it does not start from some set of premises and prove agent agreement. 
<br>
Rather the paper proves that if Bayesian Wannabes treat their 
<br>
computation errors as state-independent random variables, then if they 
<br>
agree to disagree about computations, they must agree to disagree about 
<br>
state-independent random variables.  So in that sense, the paper proves 
<br>
a non-constructive result that is unlike the usual class of Aumann 
<br>
Agreement theorems.  Unless I'm missing something?
<br>
<p><em>&gt;&gt;&gt; ... people uphold rationality standards that prefer logical 
</em><br>
<em>&gt;&gt;&gt; consistency...
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Is the Way to have beliefs that are consistent among themselves?  This 
</em><br>
<em>&gt;&gt; is not the Way, though it is often mistaken for the Way by logicians 
</em><br>
<em>&gt;&gt; and philosophers. ...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Preferring consistency, all else equal, is not the same as requiring 
</em><br>
<em>&gt; it.  Surely you also prefer it all else equal.
</em><br>
<p>No!  No, I do not prefer consistency, all else equal.  I prefer *only* 
<br>
that my map match the territory.  If I have two maps that are unrelated 
<br>
to the territory, I care not whether they are consistent.  Within the 
<br>
Way, fit to the territory is the *only* thing that I am permitted to 
<br>
consider.
<br>
<p>Michael Wilson remarked to me that general relativity and quantum 
<br>
mechanics are widely believed to be inconsistent in their present forms, 
<br>
yet they both yield excellent predictions of physical phenomena.  This 
<br>
is a challenge to find a unified theory because underlying reality is 
<br>
consistent and therefore there is presumably some *specific* consistent 
<br>
unified theory that would yield better predictions.  It is *not* a 
<br>
problem because I prefer 'all else being equal' that my map be consistent.
<br>
<p><em>&gt;&gt; ... agree that when two humans disagree and have common knowledge of 
</em><br>
<em>&gt;&gt; each other's opinion ... *at least one* human must be doing something 
</em><br>
<em>&gt;&gt; wrong.  ...
</em><br>
<em>&gt;&gt; One possible underlying fact of the matter might be that one person is 
</em><br>
<em>&gt;&gt; right and the other person is wrong and that is all there ever was to it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is *not* all there is too it.  There is also the crucial question 
</em><br>
<em>&gt; of what exactly one of them did wrong.
</em><br>
<p>Okay.
<br>
<p><em>&gt;&gt; Trying to estimate your own rationality or meta-rationality involves 
</em><br>
<em>&gt;&gt; severe theoretical problems ... &quot;Beliefs&quot; ... are not ontological 
</em><br>
<em>&gt;&gt; parts of our universe, ...  if you know the purely abstract fact that 
</em><br>
<em>&gt;&gt; the other entity is a Bayesian reasoner (implements a causal process 
</em><br>
<em>&gt;&gt; with a certain Bayesian structure),... how do you integrate it?  If 
</em><br>
<em>&gt;&gt; there's a mathematical solution it ought to be constructive.  Second, 
</em><br>
<em>&gt;&gt; attaching this kind of *abstract* confidence to the output of a 
</em><br>
<em>&gt;&gt; cognitive system runs into formal problems.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think you exaggerate the difficulties.  Again see the above papers.
</em><br>
<p>I think I need to explain the difficulties at greater length.  Nevermind.
<br>
<p><em>&gt;&gt; It seems to me that you have sometimes argued that I should 
</em><br>
<em>&gt;&gt; foreshorten my chain of reasoning, saying, &quot;But why argue and defend 
</em><br>
<em>&gt;&gt; yourself, and give yourself a chance to deceive yourself?  Why not 
</em><br>
<em>&gt;&gt; just accept the modesty argument?  Just stop fighting, dammit!&quot;  ...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I would not put my advice that way.  I'd say that whatever your 
</em><br>
<em>&gt; reasoning, you should realize that if you disagree, that has certain 
</em><br>
<em>&gt; general implications you should note.
</em><br>
<p>Perhaps we disagree about what those general implications are?
<br>
<p><em>&gt;&gt; It happens every time a scientific illiterate argues with a scientific 
</em><br>
<em>&gt;&gt; literate about natural selection.  ...  How does the scientific 
</em><br>
<em>&gt;&gt; literate guess that he is in the right, when he ... is also aware of 
</em><br>
<em>&gt;&gt; studies of human ... biases toward self-overestimation of relative 
</em><br>
<em>&gt;&gt; competence? ... I try to estimate my rationality in detail, instead of 
</em><br>
<em>&gt;&gt; using unchanged my mean estimate for the rationality of an average 
</em><br>
<em>&gt;&gt; human.  And maybe an average person who tries to do that will fail 
</em><br>
<em>&gt;&gt; pathetically.  Doesn't mean *I'll* fail, cuz, let's face it, I'm a 
</em><br>
<em>&gt;&gt; better-than-average rationalist.  ... If you, Robin Hanson, go about 
</em><br>
<em>&gt;&gt; saying that you have no way of knowing that you know more about 
</em><br>
<em>&gt;&gt; rationality than a typical undergraduate philosophy student because 
</em><br>
<em>&gt;&gt; you *might* be deceiving yourself, then you have argued yourself into 
</em><br>
<em>&gt;&gt; believing the patently ridiculous, making your estimate correct
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You claim to look in detail, but in this conversation on this the key 
</em><br>
<em>&gt; point you continue to be content to just cite the existence of a few 
</em><br>
<em>&gt; extreme examples, though you write volumes on various digressions.  This 
</em><br>
<em>&gt; is what I meant when I said that you don't seem very interested in 
</em><br>
<em>&gt; formal analysis.
</em><br>
<p>I don't regard this as the key point.  If you regard it as the key 
<br>
point, then this is my reply: while there are risks in not 
<br>
foreshortening the chain of logic, I think that foreshortening the 
<br>
reasoning places an upper bound on predictive power and that there exist 
<br>
alternate strategies which exceed the upper bound, even after the human 
<br>
biases are taken into account.  To sum up my reply, I think I can 
<br>
generate an estimate of my rationality that is predictively better than 
<br>
the estimate I would get by substituting unchanged my judgment of the 
<br>
average human rationality on the present planet Earth, even taking into 
<br>
account the known biases that have been discovered to affect 
<br>
self-estimates of rationality.  And this explains my persistent 
<br>
disagreement with that majority of the population which believes in God 
<br>
- how do you justify this disagreement for yourself?
<br>
<p>The formal math I can find does not deal at all with questions of 
<br>
self-deceptive reasoning or the choice of when to foreshorten a chain of 
<br>
reasoning with error-prone links.  Which is the formal analysis that you 
<br>
feel I am ignoring?
<br>
<p><em>&gt; Maybe there are some extreme situations where it is &quot;obvious&quot; that one 
</em><br>
<em>&gt; side is right and the other is a fool.
</em><br>
<p>How do these extreme situations fit into what you seem to feel is a 
<br>
mathematical result requiring agreement?  The more so, as, measuring 
<br>
over Earth's present population, most cases of &quot;obviousness&quot; will be 
<br>
wrong.  Most people think God obviously exists.
<br>
<p><em>&gt; This possibility does not 
</em><br>
<em>&gt; justify your just disagreeing as you always have.
</em><br>
<p>I started disagreeing differently after learning that Bayesians could 
<br>
not agree to disagree, though only when arguing with people I regarded 
<br>
as aspiring rationalists who had indicated explicit knowledge of 
<br>
Aumann-ish results.  Later I would launch a project to break my mind of 
<br>
the habit of disagreeing with domain experts unless I had a very strong 
<br>
reason.  Perhaps I did not adjust my behavior enough; I do not say, 
<br>
&quot;See, I adjusted my behavior!&quot; as my excuse.  Let the observation just 
<br>
be noted for whatever the information is worth.
<br>
<p><em>&gt; The question is what 
</em><br>
<em>&gt; reliable clues you have to justify disagreement in your typical 
</em><br>
<em>&gt; practice.  When you decide that your beliefs are better than theirs, 
</em><br>
<em>&gt; what reasoning are you going through at the meta-level?  Yes, you have 
</em><br>
<em>&gt; specific arguments on the specific topic, but so do they - why exactly 
</em><br>
<em>&gt; is your process for producing an estimate more likely to be accurate 
</em><br>
<em>&gt; than their process?
</em><br>
<p>Sometimes it isn't.  Then I try to substitute their judgment for my 
<br>
judgment.  Then there isn't a disagreement any more.  Then nobody 
<br>
remembers this event because it flashed by too quickly compared to the 
<br>
extended disagreements, and they call me stubborn.
<br>
<p>I do reserve to myself the judgment of when to overwrite my own opinion 
<br>
with someone else's.  Maybe if someone who knew and understood Aumann's 
<br>
result, and knew also to whom they spoke, said to me, &quot;I know and 
<br>
respect your power, Eliezer, but I judge that in this case you must 
<br>
overwrite your opinion with my own,&quot; I would have to give it serious 
<br>
consideration.
<br>
<p>If you're asking after specifics, then I'd have to start describing the 
<br>
art of specific cases, and that would be a long answer.  The most recent 
<br>
occasion where I recall attempting to overwrite my own opinion with 
<br>
someone else's was with an opinion of James Rogers's.  That was a case 
<br>
of domain-specific expertise; James Rogers is a decent rationalist with 
<br>
explicit knowledge of Bayesianity but he hasn't indicated any knowledge 
<br>
of Aumannish things.  Maybe I'll describe the incident later if I have 
<br>
the time to write a further reply detailing what I feel to be the 
<br>
constructive art of resolving disagreements between aspiring rationalists.
<br>
<p>Michael Raimondi and I formed a meta-rational pair from 2001 to 2003. 
<br>
We might still be a meta-rational pair now, but I'm not sure.
<br>
<p><em>&gt; In the above you put great weight on literacy/education, presuming that 
</em><br>
<em>&gt; when two people disagree the much more educated person is more likely to 
</em><br>
<em>&gt; be correct.
</em><br>
<p>In this day and age, people rarely go about disagreeing as to whether 
<br>
the Earth goes around the Sun.  I would attribute the argument over 
<br>
evolution to education about a simple and enormously overdetermined 
<br>
scientific fact, set against tremendous stupidity and self-deception 
<br>
focused on that particular question.
<br>
<p>It's not a general rule about the superiority of education - just one 
<br>
example scenario.  If you want an art of resolving specific 
<br>
disagreements between rationalists, or cues to help you estimate how 
<br>
likely you are to be correct on a particular question, then the art is 
<br>
specific and complicated.
<br>
<p>That said, I do indeed assign tremendous weight to education.  Degree of 
<br>
education is domain-specific; an educated biologist is not an educated 
<br>
physicist.  The value of education is domain-specific; not all education 
<br>
is equally worthwhile.  If a physicist argues with a biologist about 
<br>
physics then the biologist's opinion has no weight.  If a clinical 
<br>
psychologist argues with a physicist about psychology then, as far as 
<br>
any experimental tests have been able to determine, the clinical 
<br>
psychologist has no particular advantage.
<br>
<p><em>&gt; Setting aside the awkward fact of not actually having hard 
</em><br>
<em>&gt; data to support this, do you ever disagree with people who have a lot
</em><br>
<em>&gt; more literacy/education than you?
</em><br>
<p>Define &quot;literacy&quot;.  I strive to know the basics of a pretty damn broad 
<br>
assortment of fields.  *You* might (or might not) have greater breadth 
<br>
than I, but most of your colleagues' publication records haven't nearly 
<br>
your variety.
<br>
<p><em>&gt; If so, what indicators are you using 
</em><br>
<em>&gt; there, and what evidence is there to support them?
</em><br>
<p>When I disagree with an 'educated' person, it may be because I feel the 
<br>
other person to be ignorant of specific known results; overreaching his 
<br>
domain competence into an external domain; affected by wishful thinking; 
<br>
affected by political ideology; educated but not very bright; a 
<br>
well-meaning but incompetent rationalist; or any number of reasons.  Why 
<br>
are the specific cues important to this argument?  You seem to be 
<br>
arguing that there are mathematical results which a priori rule out the 
<br>
usefulness of this digression.
<br>
<p><em>&gt; A formal Bayesian analysis of such an indicator would be to construct a 
</em><br>
<em>&gt; likelihood and a prior, find some data, and then do the math.  It is not 
</em><br>
<em>&gt; enough to just throw out the possibility of various indicators being 
</em><br>
<em>&gt; useful.
</em><br>
<p>I lack the cognitive resources for a formal Bayesian analysis, but my 
<br>
best guess is that I can do better with informal analysis than with no 
<br>
analysis.  As the Way renounces consistency for its own sake, so do I 
<br>
renounce formality, save in the service of arriving to the correct answer.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10890.html">Kaj Sotala: "Re: Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;"</a>
<li><strong>Previous message:</strong> <a href="10888.html">Ben Goertzel: "Lojban and AI"</a>
<li><strong>Maybe in reply to:</strong> <a href="../0502/10764.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10901.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10889">[ date ]</a>
<a href="index.html#10889">[ thread ]</a>
<a href="subject.html#10889">[ subject ]</a>
<a href="author.html#10889">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:22:55 MST
</em></small></p>
</body>
</html>
