<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Overconfidence and meta-rationality</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Overconfidence and meta-rationality">
<meta name="Date" content="2005-03-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Overconfidence and meta-rationality</h1>
<!-- received="Mon Mar 21 00:48:56 2005" -->
<!-- isoreceived="20050321074856" -->
<!-- sent="Sun, 20 Mar 2005 23:48:51 -0800" -->
<!-- isosent="20050321074851" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Overconfidence and meta-rationality" -->
<!-- id="423E7C63.5000400@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="Overconfidence and meta-rationality" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Overconfidence%20and%20meta-rationality"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Mar 21 2005 - 00:48:51 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11143.html">Tennessee Leeuwenburg: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Previous message:</strong> <a href="11141.html">Daniel Radetsky: "Re: Model Starvation and AI Navel-Gazing"</a>
<li><strong>Maybe in reply to:</strong> <a href="../0502/10957.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11143.html">Tennessee Leeuwenburg: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Reply:</strong> <a href="11143.html">Tennessee Leeuwenburg: "Re: Overconfidence and meta-rationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11142">[ date ]</a>
<a href="index.html#11142">[ thread ]</a>
<a href="subject.html#11142">[ subject ]</a>
<a href="author.html#11142">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
One of the arts I now espouse as being pragmatically useful for human 
<br>
rationality is an art of sticking as close to the question as possible, 
<br>
in terms of causal proximity and sufficient indicators.
<br>
<p>To steal an example from Judea Pearl (a Pearl of wisdom, in Emil's 
<br>
hideous phrase), suppose we draw a causal graph as follows:  The warue 
<br>
of the SEASON variable affects the probability of the SPRINKLER being on 
<br>
and the probability of RAIN falling, which in turn can make the sidewalk 
<br>
WET, which means the sidewalk might be SLIPPERY.  So which SEASON it is, 
<br>
has a definite effect on whether the sidewalk is SLIPPERY.  But if we 
<br>
measure the variables RAIN and SPRINKLER, or even just the variable WET, 
<br>
then the variable SEASON can provide us no *additional* information 
<br>
about the variable SLIPPERY.  SEASON becomes conditionally independent 
<br>
of SLIPPERY once WET is measured.
<br>
<p>Similarly, suppose that the Wright Brothers are about to launch the 
<br>
Wright Flyer and someone walks up and says:  &quot;Human flight is a 
<br>
religious concept.  There's no evidence that human beings can fly; the 
<br>
only instances of flying human beings are angels in religious paintings. 
<br>
&nbsp;&nbsp;Since this is obviously a religiously driven enterprise based on pure 
<br>
faith, that Flyer will never fly.  Every past plane has crashed, and my 
<br>
empirical generalization is that your plane will crash too; that's the 
<br>
scientific method.&quot;
<br>
<p>If every previous plane has crashed, then induction does suggest that 
<br>
this plane will crash too.  But &quot;every previous plane has crashed&quot; is a 
<br>
vague and semitechnical hypothesis; it can't compete with a technical 
<br>
theory of aerodynamics that predicts quantitatively when, where, and how 
<br>
hard a plane will crash.  And this same *technical* theory of 
<br>
aerodynamics predicts the Wright Flyer will fly.  (See _A Technical 
<br>
Explanation of Technical Explanation_.)  From a Bayesian standpoint the 
<br>
technical theory eats the semitechnical theory, and swallows it 
<br>
entirely, leaving no scraps of data for the semitechnical theory to 
<br>
explain.  So there's no use in standing around indignantly repeating, 
<br>
&quot;But every previous plane has crashed!  Yours must crash too!&quot;
<br>
<p>It's an empirically undeniable fact that enterprises based on pure faith 
<br>
tend not to fly.  The accusation of religious thinking is not an 
<br>
inferentially irrelevant argument.
<br>
<p>But once I produce a theory of aerodynamics with which to analyze the 
<br>
Wright Flyer, I render irrelevant any information about the Wright 
<br>
Brothers' motives.  Once we have the aerodynamic analysis, we have 
<br>
measured a variable standing in much closer causal proximity to the 
<br>
matter of interest than the Wright Brothers' psychology.  The flying or 
<br>
non-flying of the Wright Flyer is conditionally independent of the 
<br>
Wright Brothers' religious beliefs given that we have analyzed the 
<br>
aerodynamics of the Wright Flyer.  Nature doesn't care directly about 
<br>
whether the Wrights are driven by religious faith or a properly gloating 
<br>
atheism; Nature only checks the proximal indicator of how the plane is 
<br>
put together.  Religious thinking only affects the plane through the 
<br>
intermediate cause of the plane's design.
<br>
<p>This is why, when people accusingly say the Singularity is a religious 
<br>
concept, or claim that hard takeoff is inspired by apocalyptic dreaming, 
<br>
I feel that my best reply remains my arguments about the dynamics of 
<br>
recursively self-improving AI.  That question stands in closer causal 
<br>
proximity to the matter of interest.  If I establish that we can (or 
<br>
cannot) expect a recursively self-improving AI to go FOOM based on 
<br>
arguments purely from the dynamics of cognition, that renders the matter 
<br>
of interest conditionally irrelevant on arguments about psychological 
<br>
apocalyptism.
<br>
<p>Of course the people who originally launched the argument still stand 
<br>
around afterward indignantly saying &quot;But... but... it sounds 
<br>
apocalyptic!&quot;  That's human nature.  &quot;You can't tell me the sidewalk 
<br>
isn't slippery!  It's fall!  It often rains in the fall!&quot;
<br>
<p>There's an art of sticking as close to the question as possible - 
<br>
arguing about issues that stand in the closest possible inferential 
<br>
proximity to the main question; trying to settle questions that, if we 
<br>
knew the answers to them, would render more distant questions irrelevant.
<br>
<p>And this is a valuable habit, because where anyone can argue about the 
<br>
other guy's psychology, or which ideas match a vague category that tends 
<br>
to fail, arguing in close proximity to the question tends to force you 
<br>
to study technical things - to learn something about science, something 
<br>
you'll hopefully remember even when the issue has passed.  Yes, I know, 
<br>
that argument isn't relevant to the Way of cutting through to the 
<br>
correct answer on only this one specific question.  But getting into the 
<br>
habit of arguing technical things instead of arguing psychology is a 
<br>
learned behavior that, over time, ends up mattering a great deal in the 
<br>
pragmatic human business of rationality.
<br>
<p>That's another reason why I don't trust the modesty argument.  It seems 
<br>
to me that you can argue indefinitely over who's more rational, without 
<br>
ever touching on the meat of a question.  Robin Hanson and I have been 
<br>
tossing arguments back and forth at each other.  Imagine if, instead of 
<br>
doing that, we just argued about which of us was more inherently 
<br>
rational and therefore should be assigned the greater weight on the 
<br>
question of modesty, *without* ever touching on our reasons for 
<br>
approving modesty or not.  (This is not to be confused with our separate 
<br>
argument over whether I (Eliezer) can rationally estimate myself to be 
<br>
more rational than average; I am arguing the affirmative, but I am not 
<br>
saying that Hanson should therefore accept my opinion on the modesty 
<br>
argument, reasons unseen.  In that sub-argument my (estimate of my own) 
<br>
rationality is a direct matter of interest, not being argued in order to 
<br>
infer something else.  Such are the hazards of choosing 
<br>
&quot;meta-rationality&quot; as the main question.)
<br>
<p>If I am rational, then I should have decent reasons - Bayesian causes - 
<br>
for believing as I do.  Once I have disgorged my reasons for believing 
<br>
something, my rationality becomes much less inferentially relevant to 
<br>
whether my belief is probably correct.  My causes for belief, if I have 
<br>
told them truly and completely, stand as a variable in closer causal 
<br>
proximity to the matter of interest than my 'rationality'.  My 
<br>
'rationality' is expressed only in the causes that influence my beliefs. 
<br>
&nbsp;&nbsp;If, despite being rational most of the time, I admit unusually stupid 
<br>
causes for belief on one occasion, I will probably end up being wrong on 
<br>
that occasion.  Or a usually irrational person, who happens to admit 
<br>
rigorous reasoning on one occasion, will probably be right on that occasion.
<br>
<p>Thus, I still think that people who disagree should, pragmatically, go 
<br>
on arguing with each other about the matter of interest, instead of 
<br>
immediately compromising based on a belief in the probable rationality 
<br>
of the other.  If two people really do happen to agree on their 
<br>
estimates of each other's psychological rationality, then sure, they can 
<br>
go ahead and compromise their probabilities; but they ought still to 
<br>
tell their reasons to one another, just in case, so that they learn 
<br>
something.  If two people each think the other is being irrational, then 
<br>
at least one of them must not be very meta-rational - but if so, they 
<br>
can still learn more by arguing with each other about the direct facts 
<br>
of the matter than by arguing over which of them is the 
<br>
non-meta-rational one.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11143.html">Tennessee Leeuwenburg: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Previous message:</strong> <a href="11141.html">Daniel Radetsky: "Re: Model Starvation and AI Navel-Gazing"</a>
<li><strong>Maybe in reply to:</strong> <a href="../0502/10957.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11143.html">Tennessee Leeuwenburg: "Re: Overconfidence and meta-rationality"</a>
<li><strong>Reply:</strong> <a href="11143.html">Tennessee Leeuwenburg: "Re: Overconfidence and meta-rationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11142">[ date ]</a>
<a href="index.html#11142">[ thread ]</a>
<a href="subject.html#11142">[ subject ]</a>
<a href="author.html#11142">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
