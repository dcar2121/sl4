<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Overconfidence and meta-rationality</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Overconfidence and meta-rationality">
<meta name="Date" content="2005-03-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Overconfidence and meta-rationality</h1>
<!-- received="Sun Mar 20 13:43:47 2005" -->
<!-- isoreceived="20050320204347" -->
<!-- sent="Sat, 19 Mar 2005 22:25:06 -0800" -->
<!-- isosent="20050320062506" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Overconfidence and meta-rationality" -->
<!-- id="423D1742.8030900@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="6.2.1.2.2.20050315195121.01ee09a0@mail.gmu.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Overconfidence%20and%20meta-rationality"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Mar 19 2005 - 23:25:06 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10946.html">Mitchell Howe: "Marc Geddes Request"</a>
<li><strong>Previous message:</strong> <a href="10944.html">Jeff Medina: "Reminder: SIAI Volunteers Meeting Tomorrow @ 3pm EST"</a>
<li><strong>Maybe in reply to:</strong> <a href="../0502/10764.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10949.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10945">[ date ]</a>
<a href="index.html#10945">[ thread ]</a>
<a href="subject.html#10945">[ subject ]</a>
<a href="author.html#10945">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Robin Hanson wrote:
<br>
<em>&gt; At 12:57 AM 3/13/2005, Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; If I had to select out two points as most important, they would be:
</em><br>
<em>&gt;&gt; 1) Just because perfect Bayesians, or even certain formally imperfect 
</em><br>
<em>&gt;&gt; Bayesians that are still not like humans, *will* always agree; it does 
</em><br>
<em>&gt;&gt; not follow that a human rationalist can obtain a higher Bayesian score 
</em><br>
<em>&gt;&gt; (truth value), or the maximal humanly feasible score, by deliberately 
</em><br>
<em>&gt;&gt; *trying* to agree more with other humans, even other human rationalists.
</em><br>
<p><em>&gt;&gt; 2) Just because, if everyone agreed to do X without further argument 
</em><br>
<em>&gt;&gt; or modification (where X is not agreeing to disagree), the average 
</em><br>
<em>&gt;&gt; Bayesian score would increase relative to its current position, it 
</em><br>
<em>&gt;&gt; does not follow that X is the *optimal* strategy.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; These points are stated very weakly, basically just inviting me to 
</em><br>
<em>&gt; *prove* my claims with mathematical precision.  I may yet rise to that 
</em><br>
<em>&gt; challenge when I get more back into this.
</em><br>
<p>We are at odds about what the math here actually *says*.  I don't regard 
<br>
the sequiturs (the points above where I say, &quot;it does not follow&quot;) as 
<br>
things that are a trivial distance from previously accomplished math. 
<br>
They seem to me almost wholly unrelated to all work on Aumann Agreement 
<br>
Theorems done so far.
<br>
<p>Since state information is irrelevant to our dispute, it would seem that 
<br>
we disagree about the results of a computation.
<br>
<p>Here's at least one semisolid mathematical result that I scribbled down 
<br>
in a couple of lines of calculus left to the reader, a result which I 
<br>
intuitively expected to find, and which I would have found to be a much 
<br>
more compelling argument in 2003.  It is that, when two Bayesianitarians 
<br>
disagree about probabilities P and ~P, they can always immediately 
<br>
improve the expectation of the sum of their Bayesian scores by averaging 
<br>
together their probability estimates for P and ~P, *regardless of the 
<br>
real value*.
<br>
<p>Let the average probability equal X.  Let the individual pre-averaging 
<br>
probabilities equal X+d and X-d.  Let the actual objective frequency 
<br>
equal P.  The function:
<br>
<p>f(d) = p*[log(x+d) + log(x-d)] + (1-p)*[log(1-x-d) + log(1-x+d)]
<br>
<p>has a maximum at d=0, regardless of the value of p.  f'(0)=0 and f''(0) 
<br>
is negative.  If my math hasn't misled me there's a couple of other 
<br>
points where f'(d)=0 but they're presumably inflection points or minima 
<br>
or some such.  I didn't bother checking which is why I call this a 
<br>
semisolid result.
<br>
<p>Therefore, if two Bayesianitarian *altruists* find that they disagree, 
<br>
and they have no better algorithm to resolve their disagreement, they 
<br>
should immediately average together their probability estimates.
<br>
<p>I would have found this argument compelling in 2003 because at that 
<br>
time, I was thinking in terms of a &quot;Categorical Imperative&quot; foundation 
<br>
for probability theory, i.e., a rule that, if all observers follow it, 
<br>
will maximize their collective Bayesian score.  I thought this solved 
<br>
some anthropic problems, but I was mistaken, though it did shed light. 
<br>
Never mind, long story.
<br>
<p>To try and translate my problem with my former foundation without going 
<br>
into a full-blown lecture on the Way:  Suppose that a creationist comes 
<br>
to me and is genuinely willing to update his belief in evolution from ~0 
<br>
to .5, providing that I update my belief from ~1 to .5.  This will 
<br>
necessarily improve the expectation of the sum of our Bayesian scores.
<br>
<p>But:
<br>
<p>1)  I can't just change my beliefs any time I please.  I can't cause 
<br>
myself not to believe in evolution by an act of will.  I can't look up 
<br>
at a blue sky and believe it to be green.  I account this a strength of 
<br>
a rationalist.
<br>
2)  Evolution is still correct regardless of what the two of us do about 
<br>
our probability assignments.  I would need to update my belief because 
<br>
of a cause that I believe to be uncorrelated with the state of the 
<br>
actual world.
<br>
3)  Just before I make my Bayesianitarian act of sacrifice, I will know 
<br>
even as I do so, correctly and rationally, that evolution is true.  And 
<br>
afterward I'll still know, deep down, whatever my lips say...
<br>
4)  I have other beliefs about biology that would be inconsistent with a 
<br>
probability assignment to evolution of 0.5.
<br>
5)  I do wish to be an altruist, but in the service of that wish, it is 
<br>
rather more important that I get my beliefs right about evolution than 
<br>
that J. Random Creationist do so, because JRC is less likely to like 
<br>
blow up the world an' stuff if he gets his cognitive science wrong.
<br>
<p>If I were an utterly selfish Bayesianitarian, how would I maximize only 
<br>
my own Bayesian score?  It is this question that is the foundation of 
<br>
*probability* theory, the what-we-deep-down-know-will-happen-next, 
<br>
whatever decisions we make in the service of altruistic utilities.
<br>
<p>Point (2) from my previous post should now also be clearer:
<br>
<p><em> &gt;&gt; 2) Just because, if everyone agreed to do X without further argument
</em><br>
<em> &gt;&gt; or modification (where X is not agreeing to disagree), the average
</em><br>
<em> &gt;&gt; Bayesian score would increase relative to its current position, it
</em><br>
<em> &gt;&gt; does not follow that X is the *optimal* strategy.
</em><br>
<p>Two Aumann agents, to whom other agents' probability estimates are 
<br>
highly informative with respect to the state of the actual world, could 
<br>
also theoretically follow the average-together-probability-estimates 
<br>
algorithm and thereby, yes, improve their summed Bayesian scores from 
<br>
the former status quo - but they would do worse that way than by 
<br>
following the actual Aumann rules, which do not lend credence as such to 
<br>
the other agent's beliefs, but simply treat those beliefs as another 
<br>
kind of Bayesian signal about the state of the actual world.
<br>
<p>Thus, if you want to claim a mathematical result about an expected 
<br>
individual benefit (let alone optimality!) for rationalists deliberately 
<br>
*trying* to agree with each other, I think you need to specify what 
<br>
algorithm they should follow to agreement - Aumann agent? 
<br>
Bayesianitarian altruist?  In the absence of any specification of how 
<br>
rationalists try to agree with each other, I don't see how you could 
<br>
prove this would be an expected individual improvement.  Setting both 
<br>
your probability estimates to zero is an algorithm for trying to agree, 
<br>
but it will not improve your Bayesian score.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10946.html">Mitchell Howe: "Marc Geddes Request"</a>
<li><strong>Previous message:</strong> <a href="10944.html">Jeff Medina: "Reminder: SIAI Volunteers Meeting Tomorrow @ 3pm EST"</a>
<li><strong>Maybe in reply to:</strong> <a href="../0502/10764.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10949.html">Eliezer S. Yudkowsky: "Re: Overconfidence and meta-rationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10945">[ date ]</a>
<a href="index.html#10945">[ thread ]</a>
<a href="subject.html#10945">[ subject ]</a>
<a href="author.html#10945">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:22:55 MST
</em></small></p>
</body>
</html>
