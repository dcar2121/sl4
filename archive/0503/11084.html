<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Robots with Buddha-Nature</title>
<meta name="Author" content="Thomas Buckner (tcbevolver@yahoo.com)">
<meta name="Subject" content="Re: Robots with Buddha-Nature">
<meta name="Date" content="2005-03-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Robots with Buddha-Nature</h1>
<!-- received="Sun Mar 13 07:34:32 2005" -->
<!-- isoreceived="20050313143432" -->
<!-- sent="Sun, 13 Mar 2005 06:34:09 -0800 (PST)" -->
<!-- isosent="20050313143409" -->
<!-- name="Thomas Buckner" -->
<!-- email="tcbevolver@yahoo.com" -->
<!-- subject="Re: Robots with Buddha-Nature" -->
<!-- id="20050313143409.85733.qmail@web60003.mail.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Thomas Buckner (<a href="mailto:tcbevolver@yahoo.com?Subject=Re:%20Robots%20with%20Buddha-Nature"><em>tcbevolver@yahoo.com</em></a>)<br>
<strong>Date:</strong> Sun Mar 13 2005 - 07:34:09 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11085.html">Daniel Radetsky: "Re: Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;"</a>
<li><strong>Previous message:</strong> <a href="11083.html">Kaj Sotala: "Re: Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11084">[ date ]</a>
<a href="index.html#11084">[ thread ]</a>
<a href="subject.html#11084">[ subject ]</a>
<a href="author.html#11084">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
--- Ben Goertzel &lt;<a href="mailto:ben@goertzel.org?Subject=Re:%20Robots%20with%20Buddha-Nature">ben@goertzel.org</a>&gt; wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Hi,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; David Hart just pointed out this guy to me
</em><br>
<em>&gt; 
</em><br>
<em>&gt; <a href="http://en.wikipedia.org/wiki/Masahiro_Mori">http://en.wikipedia.org/wiki/Masahiro_Mori</a>
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I had never heard of him, but maybe some of you
</em><br>
<em>&gt; have.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It's some fairly wild robo-Buddhistic
</em><br>
<em>&gt; philosophy ;-)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The comments at
</em><br>
<em>&gt; 
</em><br>
<em>&gt; <a href="http://www.karakuri.info/perspectives/">http://www.karakuri.info/perspectives/</a>
</em><br>
<em>&gt; 
</em><br>
<em>&gt; about differences btw Japanese and Western
</em><br>
<em>&gt; attitudes to robotics seem
</em><br>
<em>&gt; right-on.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Westerners tend to fear uberhuman killer
</em><br>
<em>&gt; megalomaniac robots, whereas
</em><br>
<em>&gt; Japanese tend to think of robots as their
</em><br>
<em>&gt; little buddies...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Of course, each perspective has its partial
</em><br>
<em>&gt; truth to it...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; -- Ben
</em><br>
<em>&gt; 
</em><br>
I've heard of him, from Tipler's Physics of
<br>
Immortality I think, and used the following quote
<br>
in my own book:
<br>
&quot;. . . to learn the Buddhist way is to perceive
<br>
oneself as a robot.&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;— Masahiro Mori 
<br>
Because I thought, &quot;He really gets it!&quot; For some
<br>
reason I haven't read any of Mori's books; IIRC
<br>
the local library had none. In any case, this is
<br>
one of the memes that lead me to think (though I
<br>
cannot prove) that any real superintelligence is
<br>
apt to have the attitude of a bodhisattva (one
<br>
who chooses to stay in the trenches and help the
<br>
rest of us reach Buddha-hood). I thought of using
<br>
this in my long-planned AI novel, in a way I
<br>
think I won't mention.
<br>
<p>This might be a good place to mention a concept
<br>
that may be useful in thinking about Collective
<br>
Volition and FAI. In some types of software, such
<br>
as music software, we have the idea of
<br>
'nondestructive editing.' In nondestructive
<br>
editing, you mave, say, a WAV sound file you just
<br>
recorded, a piece of video, etc., and a copy is
<br>
kept so that no matter what you do, if the
<br>
results turn out badly, you still have a pristine
<br>
original.
<br>
I have said before that, given the cussedness and
<br>
unreason of many humans, and given the utterly
<br>
incompatible volitions many of them have, a FAI
<br>
would have no choice but to override their wishes
<br>
at least some of the time. If religious fanatics
<br>
wish to see the world blow up, it can't very well
<br>
let them.
<br>
In a way, all discussion of FAI is like
<br>
'designing a genie.' If the genie gives you
<br>
exactly what you ask for, you're apt to get
<br>
something you don't want (every joke in which a
<br>
genie appears is a parable on this topic). We
<br>
want the genie to understand our wishes and needs
<br>
*better than we do*. And since CV refers to what
<br>
our better selves want, we can even foresee that
<br>
we are giving the genie some permission to make
<br>
us those better selves.
<br>
We want ver to edit us (if it must edit us)
<br>
nondestructively. It may be well to make it
<br>
explicit, as a supergoal, that humans will be
<br>
improved wherever possible, *with the option of
<br>
reversing errors*. And in order to do that, we
<br>
need a very clear idea of what such improvement
<br>
would entail. What do we stand to gain? What are
<br>
we unwilling to lose? Even in these questions CV
<br>
is muddled; there are those who would prefer to
<br>
see the human race as sexless as statues, unable
<br>
to appreciate music, inert in the presence of
<br>
strong drink or sweet leaf, for in their eyes
<br>
these things are sinful. A FAI might find verself
<br>
forced to sandbox a large portion of the human
<br>
race in illusory worlds suited to their
<br>
preferences, or else alter them against their
<br>
stated wishes before they could accept the
<br>
vertiginous freedoms it might offer. As far as I
<br>
am concerned, however, improvement always means
<br>
more freedom.
<br>
<p>Nick Bostrom offers convincing (to me, though I'm
<br>
no expert) arguments that we are already in a
<br>
simulation (or that no simulation will happen in
<br>
the future). If this were so, there are a range
<br>
of different types of simulation we might
<br>
inhabit. One might speculate that one such
<br>
simulation would be a 'soft sided sandbox,' so to
<br>
speak, in which the 'safe copies' of altered
<br>
humans might reside, either frozen in time or
<br>
experiencing ongoing lives where FAI never forced
<br>
them outside their comfortable, limited reality
<br>
tunnels (or, as another poster called them,
<br>
magisteria). The 'soft sides' of such a sandbox
<br>
refer to its essentially protective nature and to
<br>
the option of escape if one finds it confining;
<br>
an essential feature of a soft-sided sandbox
<br>
would be that it contains instructions for
<br>
escape, if one cared enough to look.
<br>
Even more than 'designing a genie' we might say
<br>
FAI research is 'designing a bodhisattva.'
<br>
<p>Tom Buckner
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
__________________________________ 
<br>
Do you Yahoo!? 
<br>
Yahoo! Small Business - Try our new resources site!
<br>
<a href="http://smallbusiness.yahoo.com/resources/">http://smallbusiness.yahoo.com/resources/</a> 
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11085.html">Daniel Radetsky: "Re: Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;"</a>
<li><strong>Previous message:</strong> <a href="11083.html">Kaj Sotala: "Re: Eliezer: unconvinced by your objection to safe boxing of &quot;Minerva AI&quot;"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11084">[ date ]</a>
<a href="index.html#11084">[ thread ]</a>
<a href="subject.html#11084">[ subject ]</a>
<a href="author.html#11084">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
