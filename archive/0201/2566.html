<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: WTA Mission Statement</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: WTA Mission Statement">
<meta name="Date" content="2002-01-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: WTA Mission Statement</h1>
<!-- received="Mon Jan 14 16:30:53 2002" -->
<!-- isoreceived="20020114233053" -->
<!-- sent="Mon, 14 Jan 2002 16:29:58 -0500" -->
<!-- isosent="20020114212958" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: WTA Mission Statement" -->
<!-- id="3C434DD6.B28FE788@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="NFBBLPCEGLEKJKPPBAIJKEOJCKAA.jhughes@changesurfer.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20WTA%20Mission%20Statement"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Jan 14 2002 - 14:29:58 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2567.html">Nat Jones: "Michael Anissimov's 'Shock Level Analysis'"</a>
<li><strong>Previous message:</strong> <a href="2565.html">Gordon Worley: "[REVIEW] BOOK:  Blue Light (mostly non spoiler)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2566">[ date ]</a>
<a href="index.html#2566">[ thread ]</a>
<a href="subject.html#2566">[ subject ]</a>
<a href="author.html#2566">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Mike Treder wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; [For a proposed WTA mission statement -- Eliezer.]
</em><br>
<em>&gt;
</em><br>
<em>&gt;                           To promote discussion of the possibilities
</em><br>
<em>&gt; for radical improvement of human capacities using genetic,
</em><br>
<em>&gt; cybernetic, and nano technologies; to understand, manage, and direct
</em><br>
<em>&gt; the development of these technologies in a way that will preserve
</em><br>
<em>&gt; and protect the best of what makes us human while reducing or
</em><br>
<em>&gt; eliminating the worst; to raise public consciousness of these
</em><br>
<em>&gt; matters and assist in the orderly but urgent reorientation of our
</em><br>
<em>&gt; society into the new age that is nearly upon us.
</em><br>
<p>&quot;J. Hughes&quot; wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Now, turning to Mike's language, the problem I have is that it suggests the
</em><br>
<em>&gt; WTA will actually wield some influence over the course of human destiny,
</em><br>
<em>&gt; which I think is a little grandiose a suggesiton for now. Also the phrase
</em><br>
<em>&gt; &quot;the new age that is nearly upon us&quot; is a tad too certain of the immanence
</em><br>
<em>&gt; of Singularity, and its millenial character, for my taste. Although I accept
</em><br>
<em>&gt; that the rapidly accelerating pace of technological change makes apocalyptic
</em><br>
<em>&gt; social change likely, and social prediction beyond a certain horizon
</em><br>
<em>&gt; impossible, this still sounds too millenial too me.
</em><br>
<p>Writing about the Singularity isn't easy.  It begins, of course, with the
<br>
complete *personal* rejection of the millennial attitude.  But there's
<br>
also a grab-bag of authorial patterns which are useful in avoiding the
<br>
appearance of millennialism *after* you've successfully avoided the
<br>
substance.  This grab-bag may not be sufficient in itself, but it should
<br>
nonetheless be useful.  (Incidentally, if you think that SIAI's current
<br>
online materials don't adequately follow the following rules, this is
<br>
because I wrote them a while back, before I was finished mentally
<br>
formalizing the rules.)
<br>
<p>For the problems above, I would suggest these solutions:
<br>
<p><em>&gt; Now, turning to Mike's language, the problem I have is that it suggests the
</em><br>
<em>&gt; WTA will actually wield some influence over the course of human destiny,
</em><br>
<em>&gt; which I think is a little grandiose a suggesiton for now.
</em><br>
<p>If the WTA were not *actually* going to wield some influence over the
<br>
course of human destiny, I wouldn't have bothered signing up.  So in this
<br>
case the problem is not that the suggestion *is* grandiose but that it
<br>
*sounds* grandiose.
<br>
<p>The underlying cause of this problem is the pervasive sense of learned
<br>
helplessness that characterizes modern-day society.  People are taught
<br>
that social changes are not the result of small groups or individual
<br>
efforts, but rather the result of massive economic forces, majority
<br>
opinions, major governments, megacorporations, ultrawealthy individuals,
<br>
random luck, or other forces external to the individual.  In the vast
<br>
majority of cases, of course, this is quite correct.  Back when we were
<br>
running around in 200-person hunter-gatherer tribes it may have been
<br>
possible for one individual to save the whole tribe or change the whole
<br>
observed world, but a 6-billion-person planet has a bit more inertia. 
<br>
However, because we *are* adapted to 200-person tribes, there is an
<br>
underlying desire to accomplish something significant on the scale of the
<br>
whole of observed society, and a feeling of nonfulfillment when this is
<br>
impossible, as it usually is in today's world.
<br>
<p>Now, you might think, if you were an optimist approaching this problem for
<br>
the first time, that this state of affairs would help propagate
<br>
transhumanist memes, because it's a hole transhumanism can fill. 
<br>
*Snicker*.  Not in this universe, buddy.
<br>
<p>In cases of frequently unfulfilled desire, the most common result is a
<br>
prevalent &quot;consolation philosophy&quot; - in this case, people being told &quot;Just
<br>
do what you can, and don't try to save the world&quot; or &quot;Working to improve
<br>
your own life and the lives of your family is just as important as
<br>
improving the world&quot; or &quot;If everyone does a little, it adds up to a lot.&quot; 
<br>
The attractiveness of a consolation philosophy stems from averting the
<br>
negative mental feelings resulting from an unfulfilled desire.  If you
<br>
offer a way to actually *fulfill* the desire, your offer is seen by the
<br>
audience as a threat to their mental equilibrium - by trying to tell
<br>
people that they can change the world, you are threatening the precious
<br>
mental equilibrium that lets them *not* try to change the world with only
<br>
a slight hollow feeling.
<br>
<p>The other thing that happens, in the case of pervasively unfulfilled
<br>
desire, is that various kook memes arise which use their spurious offer to
<br>
fulfill the desire as the memetic bait; that is, a lot of kook memes arise
<br>
which *promise*, but do not deliver, the chance to change the world. 
<br>
This, in turn, causes a skeptic's heuristic to arise which says that
<br>
&quot;Offering the chance to change the world is a sign of cultishness.&quot;
<br>
<p>The long-term or even mid-term solution to the problem, if you're writing
<br>
an extended essay, is to demonstrate that your statements are rationally
<br>
supportable, independent of any other nonrational memes that may happen to
<br>
be floating around.  There is a sense in which having and taking the
<br>
chance to change the world is a &quot;lower energy state&quot; than nonfulfillment
<br>
protected by consolation philosophies, or cult memes protected by
<br>
rationalizations, so if you can get people to see that this is a genuine
<br>
offer they're likely to take it.
<br>
<p>However, there's also an authorial hack that you can use on an immediate
<br>
level, when you only have a few sentences or paragraphs in which to make
<br>
your point.
<br>
<p>Instead of writing:
<br>
<p>&quot;The Singularity Institute will directly implement the Singularity and
<br>
thereby vastly benefit humanity.&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;you write
<br>
&quot;In the next few decades, humanity will gain the technological capability
<br>
to create smarter-than-human intelligence.  We believe that this event has
<br>
the potential to enormously benefit humanity.  The Singularity Institute
<br>
was founded in the belief that an event this large deserves a nonprofit
<br>
devoted to doing something about it.&quot;
<br>
<p>In the second version, a reader who's hitting on your page for the first
<br>
time can imagine - if they so wish - that the Singularity is the result of
<br>
IBM research, a US government project, a vast planetary trend, or whatever
<br>
else they like that's outside of the individual.  In other words, you
<br>
defer direct contradiction of the &quot;individual helplessness&quot; mindset until
<br>
you have the leisure to back up your challenge with technical arguments.
<br>
<p><em>&gt; Also the phrase
</em><br>
<em>&gt; &quot;the new age that is nearly upon us&quot; is a tad too certain of the immanence
</em><br>
<em>&gt; of Singularity, and its millenial character, for my taste. Although I accept
</em><br>
<em>&gt; that the rapidly accelerating pace of technological change makes apocalyptic
</em><br>
<em>&gt; social change likely, and social prediction beyond a certain horizon
</em><br>
<em>&gt; impossible, this still sounds too millenial too me.
</em><br>
<p>The phrase is indeed too millennial, but not for reasons intrinsic to the
<br>
Singularity model.  In this case, the phrase &quot;the new age that is nearly
<br>
upon us&quot; contains two errors of language.  If you visualize the actual
<br>
model implied by this phrase, as a Lakoff-and-Johnson visual metaphor, it
<br>
implies that there is an object called &quot;the new age&quot; which is approaching
<br>
the viewpoint observer as the result of the viewpoint observer moving
<br>
forward in time, or rather the viewpoint observer remaining motionless as
<br>
future events move toward the observer.  The two semantic errors rooted in
<br>
this model, which trigger suggestions of cultishness (what James calls
<br>
&quot;millennialism&quot;) are:
<br>
<p>1)  The implication that the &quot;new age&quot; is known to exist with absolute
<br>
certainty.  This is not correct.  Either biological or nanotechnological
<br>
warfare would appear to be a sufficient condition to wipe humanity off the
<br>
face of the Earth and hence prevent a Singularity, and many lesser
<br>
catastrophes would substantially delay it.  See Nick Bostrom's
<br>
&quot;Existential Risks&quot; paper.
<br>
<p>2)  The implication that the new age will come about without human
<br>
intervention, as the simple result of the future becoming the present. 
<br>
This implies a passivist view of the world, but that's not the only
<br>
problem.  The semantics (&quot;the new age that is nearly upon us&quot;) implies
<br>
that the new age is caused by an external agency - that it is divinely
<br>
imposed, predestined fate, and so on.  It sounds like there's a preset
<br>
calendar date on which the old age becomes the new age.  This is also a
<br>
signal of cultishness.
<br>
<p>The overall contrast I am trying to draw is between a cult/prophetic
<br>
model, in which global changes are externally caused, possess set calendar
<br>
dates, and are certain; and rational models of the future, in which global
<br>
changes are the result of intrinsic causal agencies and have a probability
<br>
spectrum distributed over a range of dates.  And please note that even
<br>
people who can't describe this difference explicitly will still feel the
<br>
contrast intuitively.
<br>
<p>And of course, the most obvious problem of all is the use of the two words
<br>
&quot;new age&quot;.  Why go there?
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2567.html">Nat Jones: "Michael Anissimov's 'Shock Level Analysis'"</a>
<li><strong>Previous message:</strong> <a href="2565.html">Gordon Worley: "[REVIEW] BOOK:  Blue Light (mostly non spoiler)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2566">[ date ]</a>
<a href="index.html#2566">[ thread ]</a>
<a href="subject.html#2566">[ subject ]</a>
<a href="author.html#2566">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
