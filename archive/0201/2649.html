<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Ethical basics</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Ethical basics">
<meta name="Date" content="2002-01-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Ethical basics</h1>
<!-- received="Wed Jan 23 23:33:24 2002" -->
<!-- isoreceived="20020124063324" -->
<!-- sent="Wed, 23 Jan 2002 23:29:18 -0500" -->
<!-- isosent="20020124042918" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Ethical basics" -->
<!-- id="3C4F8D9E.E1278208@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJKEIACCAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Ethical%20basics"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Jan 23 2002 - 21:29:18 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2650.html">Gordon Worley: "Re: Ethical basics"</a>
<li><strong>Previous message:</strong> <a href="2648.html">Eliezer S. Yudkowsky: "Re: Ethical basics"</a>
<li><strong>In reply to:</strong> <a href="2643.html">Ben Goertzel: "RE: Ethical basics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2652.html">Ben Goertzel: "RE: Ethical basics"</a>
<li><strong>Reply:</strong> <a href="2652.html">Ben Goertzel: "RE: Ethical basics"</a>
<li><strong>Reply:</strong> <a href="2657.html">Jerry Mitchell: "RE: Ethical basics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2649">[ date ]</a>
<a href="index.html#2649">[ thread ]</a>
<a href="subject.html#2649">[ subject ]</a>
<a href="author.html#2649">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Yes, this is a silly topic of conversation...
</em><br>
<p>Rational altruism?  Why would it be?  I've often considered starting a
<br>
third mailing list devoted solely to that.
<br>
<p><em>&gt; It seems to me that you take a certain pleasure in being more altruistic
</em><br>
<em>&gt; than most others.  Doesn't this mean that your apparent altruism is actually
</em><br>
<em>&gt; partially ego gratification ;&gt;  And if you think you don't take this
</em><br>
<em>&gt; pleasure, how do you know you don't do it unconsciously?  Unlike a
</em><br>
<em>&gt; superhuman AI, &quot;you&quot; (i.e. the conscious, reasoning component of Eli) don't
</em><br>
<em>&gt; have anywhere complete knowledge of your own mind-state...
</em><br>
<p>No offense, Ben, but this is very simple stuff - in fact, it's right there
<br>
in the Zen definition of altruism I quoted.  This is a very
<br>
straightforward trap by comparison with any of the political-emotion
<br>
mindtwisters, much less the subtle emergent phenomena that show up in a
<br>
pleasure-pain architecture.
<br>
<p>Ego gratification as a de facto supergoal (if I may be permitted to
<br>
describe the flaw in CFAImorphic terms) is a normal emotion, leaves a
<br>
normal subjective trace, and is fairly easy to learn to identify
<br>
throughout the mind if you can manage to deliberately &quot;catch&quot; yourself
<br>
doing it even once.  Once you have the basic ability to notice the
<br>
emotion, you confront the emotion directly whenever you notice it in
<br>
action, and you go through your behavior routines to check if there are
<br>
any cases where altruism is behaving as a de facto child goal of ego
<br>
gratification; i.e., avoidance of altruistic behavior where it would
<br>
conflict with ego gratification, or a bias towards a particular form of
<br>
altruistic behavior that results in ego gratification.
<br>
<p>I don't take pleasure in being more altruistic than others.  I do take a
<br>
certain amount of pleasure in the possession and exercise of my skills; it
<br>
took an extended effort to acquire them, I acquired them successfully, and
<br>
now that I have them, they're really cool.
<br>
<p>As for my incomplete knowledge of my mind-state, I have a lot of practice
<br>
dealing with incomplete knowledge of my mind-state - enough that I have a
<br>
feel for how incomplete it is, where, and why.  There is a difference
<br>
between having incomplete knowledge of something and being completely
<br>
clueless.
<br>
<p><em>&gt; Eliezer, given the immense capacity of the human mind for self-delusion, it
</em><br>
<em>&gt; is entirely possible for someone to genuinely believe they're being 100%
</em><br>
<em>&gt; altruistic even when it's not the case.  Since you know this, how then can
</em><br>
<em>&gt; you be so sure that you're being entirely altruistic?
</em><br>
<p>Because I didn't wake up one morning and decide &quot;Gee, I'm entirely
<br>
altruistic&quot;, or follow any of the other patterns that are the
<br>
straightforward and knowable paths into delusive self-overestimation, nor
<br>
do I currently exhibit any of the straightforward external signs which are
<br>
the distinguishing marks of such a pattern.  I know a lot about the way
<br>
that the human mind tends to overestimate its own altruism.
<br>
<p>I took a couple of years of effort to clean up the major emotions (ego
<br>
gratification and so on), after which I was pretty much entirely
<br>
altruistic in terms of raw motivations, although if you'd asked me I would
<br>
have said something along the lines of:  &quot;Well, of course I'm still
<br>
learning... there's still probably all this undiscovered stuff to clean
<br>
up...&quot; - which there was, of course; just a different kind of stuff. 
<br>
Anyway, after I in *retrospect* reached the point of effectively complete
<br>
strategic altruism, it took me another couple of years after that to
<br>
accumulate enough skill that I could begin to admit to myself that maybe,
<br>
just maybe, I'd actually managed to clean up most of the debris in this
<br>
particular area.
<br>
<p>This started to happen when I learned to describe the reasons why
<br>
altruists tend to be honestly self-deprecating about their own altruism,
<br>
such as the Bayesian puzzle you describe above.  After that, when I
<br>
understood not just motivations but also the intuitions used to reason
<br>
about motivations, was when I started saying openly that yes, dammit, I'm
<br>
a complete strategic altruist; you can insert all the little qualifiers
<br>
you want, but at the end of the day I'm still a complete strategic
<br>
altruist.
<br>
<p>And one answer to the little Bayesian puzzle you pose is that if you look
<br>
at the internal memories of the people who so easily claim to be 100%
<br>
altruistic, there's a great deal of emotional confidence in that altruism,
<br>
but not much declarative knowledge about altruism.  If you look at my
<br>
memories, there's a picture of several years worth of work in carefully
<br>
cleaning up the mind, with the point of effectively complete strategic
<br>
altruism being reached several years in advance of the emotional and
<br>
rational confidence where I could admit to myself that I'd succeeded.
<br>
<p>Let me turn the question around another way, Ben:  Suppose that I build a
<br>
roughly human (~human) Friendly AI.  Ve really is a complete altruist, but
<br>
ve's surrounded by people who claim to be altruists but aren't.  How does
<br>
ve know that ve's a complete altruist?  If you're tempted to answer back
<br>
&quot;Ve doesn't&quot;, then consider the problem as applying to a superintelligence
<br>
and ask again.
<br>
<p>To condense one particular distinguishing factor into plain English:  I
<br>
did not get where I am today by trusting myself.  I got here by
<br>
distrusting myself for an extended period of time.
<br>
<p><em>&gt; &gt; It's the first piece of the puzzle.  You start with a description of
</em><br>
<em>&gt; &gt; fitness maximization in game theory; then shift to describing ESS
</em><br>
<em>&gt; &gt; adaptation-executers; then move from ESS in social organisms to the
</em><br>
<em>&gt; &gt; population-genetics description of political adaptations in communicating
</em><br>
<em>&gt; &gt; rational-rationalizing entities; and then describe the (co)evolution of
</em><br>
<em>&gt; &gt; memes on top of the political adaptations.  As far as I know, though, that
</em><br>
<em>&gt; &gt; *is* the whole picture.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I suppose it's the whole picture if you construe the terms broadly
</em><br>
<em>&gt; enough....
</em><br>
<p>As I explicitly specified.
<br>
<p><em>&gt; In my view, though, the books you mention take an overly neo-Darwinist view
</em><br>
<em>&gt; of evolution, without giving enough credence to self-organizing and
</em><br>
<em>&gt; ecological phenomena.  Ever read the book &quot;Evolution Without Selection&quot; by
</em><br>
<em>&gt; A. Lima de Faria?  He doesn't discuss evolution of morals, but if he did, he
</em><br>
<em>&gt; would claim that common moral structures have natural symmetries and other
</em><br>
<em>&gt; patterns that cause them to serve as &quot;attractors&quot; for a variety of
</em><br>
<em>&gt; evolutionary processes.  In other words, he sees evolution as mostly a way
</em><br>
<em>&gt; of driving organisms and populations into attractors defined by &quot;natural
</em><br>
<em>&gt; forms.&quot;  This view doesn't necessarily contradict a standard evolutionary
</em><br>
<em>&gt; view, but it deepens it and adds a different angle.
</em><br>
<p>It's amazing how many things the modern synthesis in neo-Darwinism doesn't
<br>
fail to take into account.  Constraints on evolvability (or more
<br>
generally, fitness landscapes for evolvability) is most certainly one of
<br>
them.  I can't recall reading anything explicitly about, e.g., local
<br>
adaptations mirroring local rules and thereby giving rise to
<br>
un-selected-for global mirroring of Platonic forms - but I have explicitly
<br>
written about it, and specifically in the context of the implications for
<br>
human morality.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2650.html">Gordon Worley: "Re: Ethical basics"</a>
<li><strong>Previous message:</strong> <a href="2648.html">Eliezer S. Yudkowsky: "Re: Ethical basics"</a>
<li><strong>In reply to:</strong> <a href="2643.html">Ben Goertzel: "RE: Ethical basics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2652.html">Ben Goertzel: "RE: Ethical basics"</a>
<li><strong>Reply:</strong> <a href="2652.html">Ben Goertzel: "RE: Ethical basics"</a>
<li><strong>Reply:</strong> <a href="2657.html">Jerry Mitchell: "RE: Ethical basics"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2649">[ date ]</a>
<a href="index.html#2649">[ thread ]</a>
<a href="subject.html#2649">[ subject ]</a>
<a href="author.html#2649">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
