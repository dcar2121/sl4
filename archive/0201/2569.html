<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Michael Anissimov's 'Shock Level Analysis'</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Michael Anissimov's 'Shock Level Analysis'">
<meta name="Date" content="2002-01-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Michael Anissimov's 'Shock Level Analysis'</h1>
<!-- received="Wed Jan 16 21:08:40 2002" -->
<!-- isoreceived="20020117040840" -->
<!-- sent="Wed, 16 Jan 2002 17:33:25 -0700" -->
<!-- isosent="20020117003325" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Michael Anissimov's 'Shock Level Analysis'" -->
<!-- id="LAEGJLOGJIOELPNIOOAJMECDCCAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3C4612CD.B6431D03@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Michael%20Anissimov's%20'Shock%20Level%20Analysis'"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Wed Jan 16 2002 - 17:33:25 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2570.html">Gordon Worley: "Re: Michael Anissimov's 'Shock Level Analysis'"</a>
<li><strong>Previous message:</strong> <a href="2568.html">Eliezer S. Yudkowsky: "Re: Michael Anissimov's 'Shock Level Analysis'"</a>
<li><strong>In reply to:</strong> <a href="2568.html">Eliezer S. Yudkowsky: "Re: Michael Anissimov's 'Shock Level Analysis'"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2570.html">Gordon Worley: "Re: Michael Anissimov's 'Shock Level Analysis'"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2569">[ date ]</a>
<a href="index.html#2569">[ thread ]</a>
<a href="subject.html#2569">[ subject ]</a>
<a href="author.html#2569">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eli,
<br>
<p>I agree with your sentiments fully, however I also find a huge amount of
<br>
humor value in Anissimov's essay....  It's been a while since I laughed out
<br>
loud in front of the computer like this ;-D ... for this I am grateful!!
<br>
<p>I mean, on the one hand we have,
<br>
&quot;Amish ...  Humans who live like animals.&quot;
<br>
(Now, now.  The Amish actually *don't* live at all like nonhuman animals...
<br>
this is silly...)
<br>
<p>And on the other hand,
<br>
&quot;...the human embodiments of the Singularity. ...  The only activities this
<br>
person engages in are those that will bring about a quicker Singularity.
<br>
Sex is only used as an anti-stress mechanism.  ...&quot;
<br>
<p>Uh oh.  I thought I was SL5 but recently I caught myself using sex for a
<br>
purpose other than as an anti-stress mechanism.  I guess I'm gonna be
<br>
demoted... darn...
<br>
<p>well MY goodness!!! ;&gt;
<br>
<p>The problem with this sort of thing is that it makes the whole
<br>
Singularitarian idea look awfully silly.  And worse than that, as Eli says,
<br>
it makes Singularitarianism look like an attempt to boost up one segment of
<br>
human society at the expense of another, which is really not what it's
<br>
about.
<br>
<p>Compared to what will come after the Singularity, we're ALL &quot;living like
<br>
animals&quot; -- and none of us are in any strong sense &quot;human embodiments of the
<br>
Singularity.&quot;  (No, not even you, Eli ;)  Knowledge of the splendor of what
<br>
will come after should make us *humble* about the severe limitations of our
<br>
human brains and bodies, not egotistical because we understand a *little*
<br>
more about the future than most people... or most nonhuman animals ;&gt;
<br>
<p>I won't be shocked if Anissimov's ass-tute analysis winds up in &quot;Wired&quot; ;-p
<br>
<p><p>Don't worry ... be happy...
<br>
-- Ben G
<br>
<p><p><p><p><p><p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20Michael%20Anissimov's%20'Shock%20Level%20Analysis'">owner-sl4@sysopmind.com</a> [mailto:<a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20Michael%20Anissimov's%20'Shock%20Level%20Analysis'">owner-sl4@sysopmind.com</a>]On Behalf
</em><br>
<em>&gt; Of Eliezer S. Yudkowsky
</em><br>
<em>&gt; Sent: Wednesday, January 16, 2002 4:55 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sysopmind.com?Subject=RE:%20Michael%20Anissimov's%20'Shock%20Level%20Analysis'">sl4@sysopmind.com</a>
</em><br>
<em>&gt; Subject: Re: Michael Anissimov's 'Shock Level Analysis'
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Nat Jones wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; <a href="http://www.geocities.com/imminentsingularity/Shock_Level_Analysis.html">http://www.geocities.com/imminentsingularity/Shock_Level_Analysis.html</a>
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; A fun read. Thanks Michael. Anyone on this mailing list fit the
</em><br>
<em>&gt; following
</em><br>
<em>&gt; &gt; description, if only in spirit, of an SL5'er?
</em><br>
<em>&gt;
</em><br>
<em>&gt; What is this strange fascination that the word &quot;SL5&quot; seems to exert over
</em><br>
<em>&gt; people?  Why do so many people, confronted with SL0 through SL4, feel this
</em><br>
<em>&gt; impulse to top it by coming up with SL5?  Oh, never mind.  Anyway...
</em><br>
<em>&gt;
</em><br>
<em>&gt; To be a Singularitarian, or even just a good-old-fashioned transhumanist,
</em><br>
<em>&gt; is to tap into some very powerful ideas.  It is an unfortunate but true
</em><br>
<em>&gt; fact that humans tend to malfunction in the presence of powerful ideas, so
</em><br>
<em>&gt; if you want to incorporate powerful ideas and remain rational, those ideas
</em><br>
<em>&gt; have to be balanced by self-awareness and mental discipline.  At the core,
</em><br>
<em>&gt; all of this is about *intelligence*.  Intelligence is more important than
</em><br>
<em>&gt; fanaticism.  You can only permit yourself that degree of fanaticism that
</em><br>
<em>&gt; does not, at your current level of mental discipline, interfere with your
</em><br>
<em>&gt; intelligence.  An extreme effort in the service of a counterproductive
</em><br>
<em>&gt; goal is worse than nothing.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The absolute primacy of rationality and intelligence is what must be
</em><br>
<em>&gt; preserved, above all, as Singularitarian ideas begin to reach out beyond
</em><br>
<em>&gt; the core audience of scientifically literate aggressive rationalists.
</em><br>
<em>&gt; Compromise that in the name of &quot;recruiting&quot; and the reins will simply be
</em><br>
<em>&gt; torn out of your hands by the people you tried to recruit.  It has
</em><br>
<em>&gt; happened before and may happen to us even if we do everything *right*; we
</em><br>
<em>&gt; cannot afford to make deliberate compromises.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The powerful ideas have to be coupled to equally powerful drives toward
</em><br>
<em>&gt; rationality, or the actual *goal* isn't going to get done.  And I do not,
</em><br>
<em>&gt; by the way, believe in the model of a few rational people remaining sane
</em><br>
<em>&gt; in order to provide the strategic direction for a larger group of insane
</em><br>
<em>&gt; fanatics; I am not aware of any historical instance of this model
</em><br>
<em>&gt; working.  Everything I know about human nature says that the insane
</em><br>
<em>&gt; fanatics would drop the rationalists like a rotting moose carcass and
</em><br>
<em>&gt; promote the most insane of their number.  Rationality has to be for
</em><br>
<em>&gt; *everyone*, the whole membership; if you recruit someone that is not a
</em><br>
<em>&gt; scientifically literate aggressive rationalist, then you have to make sure
</em><br>
<em>&gt; that your literature tells people &quot;We think rationality is good... try and
</em><br>
<em>&gt; make yourself more rational.&quot;  You have to preserve the part of the
</em><br>
<em>&gt; message that says:  &quot;Unlike other ideas, Singularitarianism doesn't say
</em><br>
<em>&gt; it's a sin to be skeptical or to question the leaders.&quot;  Because while
</em><br>
<em>&gt; some people may take this for granted, everyone else may not.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Now I am an extremist, by anyone's standard, and I know it.  I make no
</em><br>
<em>&gt; excuses about having devoted my life completely to the Singularity; I have
</em><br>
<em>&gt; done so, I admit it, I'm proud of it, and I'd do it again in a minute.
</em><br>
<em>&gt; The six billion lives that I *know* are at stake, to say nothing of all
</em><br>
<em>&gt; the future lives that may someday come to be, outweigh my own life by an
</em><br>
<em>&gt; indescribably enormous factor.  I do acknowledge that I need to have fun
</em><br>
<em>&gt; every now and then in order to stay in mental shape, but this is not the
</em><br>
<em>&gt; same as living a &quot;normal&quot; life in which fun is pursued for its own sake.
</em><br>
<em>&gt; And it may be that fun pursued for the sake of efficiency is not as much
</em><br>
<em>&gt; fun as fun which is pursued for its own sake.  It may even be that this
</em><br>
<em>&gt; makes me less efficient.  *But*, and this is the key point, to pursue fun
</em><br>
<em>&gt; for its own sake I'd have to change my actual picture of the world, and
</em><br>
<em>&gt; I'm not willing to do that because it would compromise *rationality*.  I
</em><br>
<em>&gt; will not compromise rationality even if it makes my life more efficient!
</em><br>
<em>&gt; If I keep to the rational course, then eventually I expect to discover
</em><br>
<em>&gt; some way to have utilitarian fun that is just as healthy as intrinsic
</em><br>
<em>&gt; fun.  I'm not going to lie to myself, because that would cut off the
</em><br>
<em>&gt; possibility of future progress, even if it delivered a small (or a large)
</em><br>
<em>&gt; short-term benefit.  If you're going to be a good fanatic, then you have
</em><br>
<em>&gt; to remember that good fanaticism is just a very intense effort, and effort
</em><br>
<em>&gt; has no intrinsic worth.  Dedication proves nothing.  It has no value on
</em><br>
<em>&gt; its own.  It is nothing to be proud of.  It is useful only insofar as it
</em><br>
<em>&gt; achieves the actual goals, and to compromise rationality removes the
</em><br>
<em>&gt; possibility of achieving those goals.  Now I do think that complete
</em><br>
<em>&gt; dedication is, in fact, very useful, and for that reason I am completely
</em><br>
<em>&gt; dedicated, but &quot;dedication&quot; is not the point.  It's not something to be
</em><br>
<em>&gt; proud of for its own sake.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If you're going to be a Singularitarian fanatic, then you have to remember
</em><br>
<em>&gt; that the Singularity is more important than your own fanaticism.  I expect
</em><br>
<em>&gt; this sounds obvious to most of us, but it is not, in fact, obvious.  The
</em><br>
<em>&gt; vast majority of fanatics talk and act as if fanaticism is inherently a
</em><br>
<em>&gt; sign of moral worth.  The vast majority of fanatics do not know how to
</em><br>
<em>&gt; safely handle this mental plutonium in order to use it as fuel, which is
</em><br>
<em>&gt; why extremism, even extremism in altruism, has such a lousy reputation
</em><br>
<em>&gt; among rationalists.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Unlike most other aggressive rationalists, I don't think that extremism is
</em><br>
<em>&gt; bad, I just think it has to be handled carefully.  But if you *don't know*
</em><br>
<em>&gt; how to do that, then you really would be better off distrusting your own
</em><br>
<em>&gt; passion, even if it means less mental energy and getting less work done.
</em><br>
<em>&gt; You can always come back to the problem later, when you have a little more
</em><br>
<em>&gt; practice at rationality.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If you're going to be an extremist, you have to be aware of the forces
</em><br>
<em>&gt; underlying human nature.  And you have to dodge them, successfully, even
</em><br>
<em>&gt; at the price of diminishing your own extremism if that's what it takes.
</em><br>
<em>&gt;
</em><br>
<em>&gt; At this time, and from the &quot;Shock Level Analysis&quot; page, I don't think
</em><br>
<em>&gt; Michael Anissimov dodged successfully.  There are some obvious PR
</em><br>
<em>&gt; problems, but PR problems generally go away with sufficient writing
</em><br>
<em>&gt; experience.  The main thing that I think represents a real problem is the
</em><br>
<em>&gt; &quot;us vs. them&quot; mindset - what I usually call the &quot;group polarization&quot;
</em><br>
<em>&gt; dynamic, the tribalism instincts from evolutionary psychology.
</em><br>
<em>&gt;
</em><br>
<em>&gt; People are not divided into groups by future shock level.  At most, you
</em><br>
<em>&gt; can use FSL to divide your readers into audiences.  You should not use it
</em><br>
<em>&gt; to divide humanity into tribes, place your own tribe at the top, and heap
</em><br>
<em>&gt; hatred on the rest.  That'd be too... human.  Seriously.  The original
</em><br>
<em>&gt; &quot;Future Shock&quot; essay I wrote contains an explicit admonishment against
</em><br>
<em>&gt; dividing people into feuding tribes by FSL, and this is exactly why;
</em><br>
<em>&gt; because I know something about how human psychology works, and I know that
</em><br>
<em>&gt; if I establish an ascending scale that could theoretically be applied to
</em><br>
<em>&gt; people, even if it's meant to be applied only to audiences in a specific
</em><br>
<em>&gt; context, then people are going to see a social hierarchy, and they're
</em><br>
<em>&gt; going to try and place themselves at the top.  (Hence all the attempts to
</em><br>
<em>&gt; top the scale with SL5, of course.)
</em><br>
<em>&gt;
</em><br>
<em>&gt; For example, let's take Anissimov's description of SL -1 and SL -2.
</em><br>
<em>&gt; There's some pretty darned hateful language in there, which I will avoid
</em><br>
<em>&gt; quoting permanently in the SL4 archives, in case Anissimov decides to
</em><br>
<em>&gt; change it.  The point is:  Why hate?  What does it accomplish?  What good
</em><br>
<em>&gt; does it do?  It sure doesn't help you outthink an opponent.  It is
</em><br>
<em>&gt; instinctive to hate, and it may even be politically useful to the
</em><br>
<em>&gt; individual to whip up hate against opponents (it worked for McCarthy), but
</em><br>
<em>&gt; the point is that it doesn't accomplish anything useful in terms of the
</em><br>
<em>&gt; actual *goal*.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I wrote the original Shock Level essay very carefully in order to
</em><br>
<em>&gt; establish that I was not establishing a new social hierarchy and putting
</em><br>
<em>&gt; myself at the top.  That would be stupid, boring, and very human, and is
</em><br>
<em>&gt; unworthy of anyone with the slightest smattering of evolutionary
</em><br>
<em>&gt; psychology.  And now, not to mince words about it, someone has gone out
</em><br>
<em>&gt; and done *exactly this* and it looks every bit as awful as I thought it
</em><br>
<em>&gt; would.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The Singularity needs more people who are willing to dedicate themselves
</em><br>
<em>&gt; completely and to hell with the usual reserve; with this I agree.  But you
</em><br>
<em>&gt; can't just take that dedication and not take the other things that go
</em><br>
<em>&gt; along with it and keep it rational.
</em><br>
<em>&gt;
</em><br>
<em>&gt; --              --              --              --              --
</em><br>
<em>&gt; Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt; Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2570.html">Gordon Worley: "Re: Michael Anissimov's 'Shock Level Analysis'"</a>
<li><strong>Previous message:</strong> <a href="2568.html">Eliezer S. Yudkowsky: "Re: Michael Anissimov's 'Shock Level Analysis'"</a>
<li><strong>In reply to:</strong> <a href="2568.html">Eliezer S. Yudkowsky: "Re: Michael Anissimov's 'Shock Level Analysis'"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2570.html">Gordon Worley: "Re: Michael Anissimov's 'Shock Level Analysis'"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2569">[ date ]</a>
<a href="index.html#2569">[ thread ]</a>
<a href="subject.html#2569">[ subject ]</a>
<a href="author.html#2569">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
