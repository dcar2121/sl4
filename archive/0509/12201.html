<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Retrenchment</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Retrenchment">
<meta name="Date" content="2005-09-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Retrenchment</h1>
<!-- received="Mon Sep  5 15:16:50 2005" -->
<!-- isoreceived="20050905211650" -->
<!-- sent="Mon, 05 Sep 2005 14:14:49 -0700" -->
<!-- isosent="20050905211449" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Retrenchment" -->
<!-- id="431CB549.8020101@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="430E7B4B.5010608@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Retrenchment"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Sep 05 2005 - 15:14:49 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12202.html">Tennessee Leeuwenburg: "Bayes and the conceptual framework"</a>
<li><strong>Previous message:</strong> <a href="12200.html">Eliezer S. Yudkowsky: "META: No removes, please."</a>
<li><strong>In reply to:</strong> <a href="../0508/12109.html">Richard Loosemore: "Re: Retrenchment"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12202.html">Tennessee Leeuwenburg: "Bayes and the conceptual framework"</a>
<li><strong>Reply:</strong> <a href="12202.html">Tennessee Leeuwenburg: "Bayes and the conceptual framework"</a>
<li><strong>Reply:</strong> <a href="12203.html">H C: "Kolmogorov + Solomonoff"</a>
<li><strong>Reply:</strong> <a href="12204.html">Phil Goetz: "Re: Retrenchment"</a>
<li><strong>Reply:</strong> <a href="12227.html">Richard Loosemore: "The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12201">[ date ]</a>
<a href="index.html#12201">[ thread ]</a>
<a href="subject.html#12201">[ subject ]</a>
<a href="author.html#12201">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Richard Loosemore wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Ahh!  Eric B. Baum.  You were impressed by him, huh?  So you must be one 
</em><br>
<em>&gt; of those people who were fooled when he tried to explain qualia as a 
</em><br>
<em>&gt; form of mechanism, calling this an answer to the &quot;hard problem&quot; [of 
</em><br>
<em>&gt; consciousness] and making all the people who defined the term &quot;hard 
</em><br>
<em>&gt; problem of consciousness&quot; piss themselves with laughter at his stupidity?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Baum really looks pretty impressive if you don't read his actual words 
</em><br>
<em>&gt; too carefully, doesn't he?
</em><br>
<p>Sure, I was pleasantly surprised by Baum.  Baum had at least one new 
<br>
idea and said at least one sensible thing about it, a compliment I'd pay 
<br>
also to Jeff Hawkins.  I don't expect anyone to get everything right.  I 
<br>
try to credit people for getting a single thing right, or making 
<br>
progress on a problem, as otherwise I'd never be able to look favorably 
<br>
on anyone.
<br>
<p>Did Baum's explanation of consciousness dissipate the mystery?  No, it 
<br>
did not; everything Baum said was factually correct, but people confused 
<br>
by the hard problem of consciousness would be just as confused after 
<br>
hearing Baum's statements.  I agree that Baum failed to dissipate the 
<br>
apparent mystery of Chalmers's hard problem.  Baum said some sensible 
<br>
things about Occam's Razor, and introduced me to the notion of VC 
<br>
dimension; VC dimension isn't important for itself, but it got me to 
<br>
think about Occam's Razor in terms of the range of possibilities a 
<br>
hypothesis class can account for, rather than the bits required to 
<br>
describe an instance of a hypothesis class.
<br>
<p><em>&gt;&gt; COMMUNITY (H)
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; The students of an ancient art devised by Laplace, which is therefore 
</em><br>
<em>&gt;&gt; called Bayesian.  Probability theory, decision theory, information 
</em><br>
<em>&gt;&gt; theory, statistics; Kolmogorov and Solomonoff, Jaynes and Shannon.  
</em><br>
<em>&gt;&gt; The masters of this art can describe ignorance more precisely than 
</em><br>
<em>&gt;&gt; most folk can describe their knowledge, and if you don't realize 
</em><br>
<em>&gt;&gt; that's a pragmatically useful mathematics then you aren't in community 
</em><br>
<em>&gt;&gt; (H).  These are the people to whom &quot;intelligence&quot; is not a sacred 
</em><br>
<em>&gt;&gt; mystery... not to some of us, anyway.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Boy, you are so right there!  They don't think of intelligence as a 
</em><br>
<em>&gt; sacred mystery, they think it is so simple, it only involves Bayesian 
</em><br>
<em>&gt; Inference!
</em><br>
<p>Not at all.  The enlightened use Bayesian inference and expected utility 
<br>
maximization to measure the power of an intelligence.  Like the 
<br>
difference between understanding how to measure aerodynamic lift, and 
<br>
knowing how to build an airplane.  If you don't know how to measure 
<br>
aerodynamic lift, good luck building an airplane.  Knowing how to 
<br>
measure success isn't enough to succeed, but it sure helps.
<br>
<p><em>&gt; Like Behaviorists and Ptolemaic Astronomers, they mistake a formalism 
</em><br>
<em>&gt; that approximately describes a system for the mechanism that is actually 
</em><br>
<em>&gt; inside the system.  They can carry on like this for centuries, adding 
</em><br>
<em>&gt; epicycles onto their models in order to refine them.  When Bayesian 
</em><br>
<em>&gt; Inference does not seem to cut it, they assert that *in principle* a 
</em><br>
<em>&gt; sufficiently complex Bayesian Inference system really would be able to 
</em><br>
<em>&gt; cut it ... but they are not able to understand that the &quot;in principle&quot; 
</em><br>
<em>&gt; bit of their argument depends on subtleties that they don't think much 
</em><br>
<em>&gt; about.
</em><br>
<p>There are subtleties to real-world intelligence that don't appear in 
<br>
standard Bayesian decision theory (he said controversially), but 
<br>
Bayesian decision theory can describe a hell of a lot more than naive 
<br>
students think.  I bet that if you name three subtleties, I can describe 
<br>
how Bayes plus expected utility plus Solomonoff (= AIXI) would do it 
<br>
given infinite computing power.
<br>
<p><em>&gt; In particular, they don't notice when the mechanism that is supposed to 
</em><br>
<em>&gt; do the mapping between internal symbols and external referents, in their 
</em><br>
<em>&gt; kind of system, turns out to require more intelligence than the 
</em><br>
<em>&gt; reasoning engine itself .... and they usually don't notice this because 
</em><br>
<em>&gt; they write all their programs with programmer-defined symbols/concepts 
</em><br>
<em>&gt; (implictly inserting the intelligence themselves, you see), thus sparing 
</em><br>
<em>&gt; their system the pain of doing the work necessary to ground itself.
</em><br>
<p>Historically true.  I remember reading Jaynes and snorting mentally to 
<br>
myself as Jaynes described a &quot;robot&quot; which came complete with 
<br>
preformulated hypotheses.  But it's not as if Jaynes was trying to build 
<br>
a Friendly AI.  I don't expect Jaynes to know that stuff, just to get 
<br>
his probability theory right.
<br>
<p>I note that the mechanism that maps from internal symbols to external 
<br>
referents is readily understandable in Bayesian terms.  AIXI can learn 
<br>
to walk across a room.
<br>
<p>I also note that I have written extensively about this very problem in 
<br>
&quot;Levels of Organization in General Intelligence&quot;.
<br>
<p><em>&gt; If these people understood what was going on in the other communities, 
</em><br>
<em>&gt; they might understand these issues.  Typically, they don't.
</em><br>
<p>Again, historically true.  Jaynes was a physicist before he was a 
<br>
Bayesian, but I've no reason to believe he ever studied, say, visual 
<br>
neurology.
<br>
<p>So far as I've heard, in modern-day science, individuals are polymaths, 
<br>
not communities.
<br>
<p>If I despised the communities for that, I couldn't assemble the puzzle 
<br>
pieces from each isolated community.
<br>
<p><em>&gt; Here you remind me of John Searle, famous Bete Noir of the AI community, 
</em><br>
<em>&gt; who will probably never understand the &quot;levels&quot; difference between 
</em><br>
<em>&gt; systems that *are* intelligent (e.g. humans) and systems that are 
</em><br>
<em>&gt; collections of interacting intelligences (e.g. human societies) and, 
</em><br>
<em>&gt; jumping up almost but not quite a whole level again, systems that are 
</em><br>
<em>&gt; interacting species of &quot;intelligences&quot; (evolution).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is one of the silliest mistakes that a person interested in AI 
</em><br>
<em>&gt; could make.   You know Conway's game of Life?  I've got a dinky little 
</em><br>
<em>&gt; simulation here on this machine that will show me a whole zoo of gliders 
</em><br>
<em>&gt; and loaves and glider guns and traffic lights and whatnot.  Demanding 
</em><br>
<em>&gt; that an AI person should study &quot;evolutionary biology with math&quot; is about 
</em><br>
<em>&gt; as stupid as demanding that someone interested in the structure of 
</em><br>
<em>&gt; computers should study every last detail of the gliders, loaves, glider 
</em><br>
<em>&gt; guns and traffic lights, etc. in Conway's Life.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Level of description fallacy.  Searle fell for it when he invented his 
</em><br>
<em>&gt; ridiculous Chinese Room.  Why would yo make the same dumb mistake?
</em><br>
<p>Ho, ho, ho!  Let me look up the appropriate response in my handbook... 
<br>
ah, yes, here it is:  &quot;You speak from a profound lack of depth in the 
<br>
one field where depth of understanding is most important.&quot;  Evolutionary 
<br>
biology with math isn't 'most important', but it sure as hell is important.
<br>
<p>To try to understand human intelligence without understanding natural 
<br>
selection is hopeless.
<br>
<p>To try to understand optimization, you should study more than one kind 
<br>
of powerful optimization process.  Natural selection is one powerful 
<br>
optimization process.  Human intelligence is another.  Until you have 
<br>
studied both, you have no appreciation of how different two optimization 
<br>
processes can be.  &quot;A barbarian is one who thinks the customs of his 
<br>
island and tribe are the laws of nature.&quot;  To cast off the human island, 
<br>
you study evolutionary biology with math.
<br>
<p>I know about separate levels of description.  I'm not telling you that 
<br>
ev-bio+math is how intelligence works.  I'm telling you to study 
<br>
ev-bio+math anyway, because it will help you understand human 
<br>
intelligence and general optimization.  After you have studied, you will 
<br>
understand why you needed to study.
<br>
<p>I should note that despite my skepticism, I'm quite open to the 
<br>
possibility that if I study complexity theory, I will afterward slap 
<br>
myself on the forehead and say, &quot;I can't believe I tried to do this 
<br>
without studying complexity theory.&quot;  The question I deal with is 
<br>
deciding where to spend limited study time - otherwise I'd study it all.
<br>
<p><em>&gt; I did read the documents.  I knew about Bayes Theorem already.
</em><br>
<p>Good for you.  You do realize that Bayesian probability theory 
<br>
encompasses a lot more territory than Bayes's Theorem?
<br>
<p><em>&gt; A lot of sound and fury, signifying nothing.  You have no real 
</em><br>
<em>&gt; conception of the limitations of mathematics, do you?
</em><br>
<p>Yeah, right, a 21st-century human is going to know the &quot;limitations&quot; of 
<br>
mathematics.  After that, he'll tell me the limitations of science, 
<br>
rationality, skepticism, observation, and reason.  Because if he doesn't 
<br>
see how to do something with mathematics, it can't be done.
<br>
<p><em>&gt; You don't seem to 
</em><br>
<em>&gt; understand that the forces that shape thought and the forces that shape 
</em><br>
<em>&gt; our evaluation of technical theories (each possibly separate forces, 
</em><br>
<em>&gt; though related) might not be governed by your post-hoc bayesian analysis 
</em><br>
<em>&gt; of them.  That entire concept of the separation between an approximate 
</em><br>
<em>&gt; description of a process and the mechanisms that actually *is* the 
</em><br>
<em>&gt; process, is completely lost on you.
</em><br>
<p>I understand quite well the difference between an approximation and an 
<br>
ideal, or the difference between a design goal and a design.  I won't 
<br>
say it's completely futile to try to do without knowing what you're 
<br>
doing, because some technology does get built that way.  But my concern 
<br>
is Friendly AI, not AI, so I utterly abjured and renounced my old ideas 
<br>
of blind exploration.  From now on, I said to myself, I understand 
<br>
exactly what I'm doing *before* I do it.
<br>
<p><em>&gt; As I said at the outset, this is foolishness.  I have read &quot;Judgment 
</em><br>
<em>&gt; Under Uncertainty&quot;, and Lakoff's &quot;Women, Fire and Dangerous Things&quot; ... 
</em><br>
<em>&gt; sitting there on the shelf behind me.
</em><br>
<p>Okay, you passed a couple of spot-checks, you're not a complete waste of 
<br>
time.
<br>
<p>Though you still seem unclear on the realization that polymaths all 
<br>
study *different* fields, so there's nothing impressive about being able 
<br>
to name different communities.  Anyone can rattle off the names of some 
<br>
obscure books they've read.  It's being able to answer at least some of 
<br>
the time when someone else picks the question, that implies you're 
<br>
getting at least a little real coverage.  You seem to have some myopia 
<br>
with respect to this, asking me why I was telling you to study 
<br>
additional fields when I hadn't even studied every single one you'd 
<br>
already studied.  Some roads go on a long, long way.  That *you* can 
<br>
name things you've studied isn't impressive.  Having Lakoff on the shelf 
<br>
behind you does imply you're not a total n00bie, not because Lakoff is 
<br>
so important, but because I selected the question instead of you.
<br>
<p><em>&gt;&gt; Also known as Mainstream AI: the predicate logic users, 
</em><br>
<em>&gt;&gt; connectionists, and artificial evolutionists.  What they know about 
</em><br>
<em>&gt;&gt; goal hierarchies and planning systems is quite different from what 
</em><br>
<em>&gt;&gt; decision theorists know about expected utility maximization, though of 
</em><br>
<em>&gt;&gt; course there's some overlap.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I was conjoining the decision theorists with the hard AI group, since 
</em><br>
<em>&gt; most of the AI people I know are perfectly familiar with the latter.
</em><br>
<p>Non sequitur.  AI people may know some decision theory, it doesn't mean 
<br>
that decision theory is identical to AI.
<br>
<p>I would guess that not many AI people can spot-read the difference between:
<br>
<p>p(B|A)
<br>
p(A []-&gt; B)
<br>
<p><em>&gt;&gt;&gt; COMMUNITY (C)
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; Those to whom the term &quot;edge of chaos&quot; is not just something they 
</em><br>
<em>&gt;&gt;&gt; learned from James Gleick.  These people are comfortable with the 
</em><br>
<em>&gt;&gt;&gt; idea that mathematics is a fringe activity that goes on at the 
</em><br>
<em>&gt;&gt;&gt; tractable edge of a vast abyss of completely intractable systems and 
</em><br>
<em>&gt;&gt;&gt; equations.  When they use the term &quot;non-linear&quot; they don't mean 
</em><br>
<em>&gt;&gt;&gt; something that is not a straight line, nor are they talking about 
</em><br>
<em>&gt;&gt;&gt; finding tricks that yield analytic solutions to certain nonlinear 
</em><br>
<em>&gt;&gt;&gt; equations.  They are equally comfortable talking about a national 
</em><br>
<em>&gt;&gt;&gt; economy and a brain as a &quot;CAS&quot; and they can point to meaningful 
</em><br>
<em>&gt;&gt;&gt; similarities in the behavior of these two sorts of system.  Almost 
</em><br>
<em>&gt;&gt;&gt; all of these people are seriously well versed in mathematics, but 
</em><br>
<em>&gt;&gt;&gt; unlike the main body of mathematicians proper, they understand the 
</em><br>
<em>&gt;&gt;&gt; limitations of analytic attempts to characterize systems in the real 
</em><br>
<em>&gt;&gt;&gt; world.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; I'm not part of community C and I maintain an extreme skepticism of 
</em><br>
<em>&gt;&gt; its popular philosophy, as opposed to particular successful technical 
</em><br>
<em>&gt;&gt; applications, for reasons given in &quot;A Technical Explanation of 
</em><br>
<em>&gt;&gt; Technical Explanation&quot;.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You speak from a profound lack of depth in the one field where depth of 
</em><br>
<em>&gt; understanding is most important.  You mistake &quot;particular successful 
</em><br>
<em>&gt; technical applications&quot; for the issues of most importance to AI.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There is nothing wrong with skepticism.  If it is based on 
</em><br>
<em>&gt; understanding, rather than wilful, studied ignorance.
</em><br>
<p>&quot;Shut up and learn&quot; is a plaint to which I am, in general, prepared to 
<br>
be sympathetic.  But you're not the only one with recommendations.  Give 
<br>
me one example of a technical understanding, one useful for making 
<br>
better-than-random guesses about specific observable outcomes, which 
<br>
derives from chaos theory.  Phil Goetz, making stuff up at random, gave 
<br>
decent examples of what I'm looking for.  Make a good enough case, and 
<br>
I'll put chaos theory on the head of my menu.
<br>
<p>Many people are easily fooled into thinking they have attained some 
<br>
tremendously important and significant understanding of something they 
<br>
are still giving maxentropy probability distributions about, the qualia 
<br>
crowd being one obvious example.  Show me this isn't so of chaos theory.
<br>
<p><em>&gt;&gt;&gt; Those who could give you a reasonable account of where Penrose, 
</em><br>
<em>&gt;&gt;&gt; Chalmers and Dennett would stand with respect to one another.  They 
</em><br>
<em>&gt;&gt;&gt; could easily distinguish the Hard Problem from other versions of the 
</em><br>
<em>&gt;&gt;&gt; consciousness issue, even if they might disagree with Chalmers about 
</em><br>
<em>&gt;&gt;&gt; the conclusion to be drawn.  They know roughly what supervenience 
</em><br>
<em>&gt;&gt;&gt; is.  The could certainly distinguish functionalism (various breeds 
</em><br>
<em>&gt;&gt;&gt; thereof) from epiphenomenalism and physicalism, and they could talk 
</em><br>
<em>&gt;&gt;&gt; about what various camps thought about the issues of dancing, 
</em><br>
<em>&gt;&gt;&gt; inverted and absent qualia.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Sadly I recognize every word and phrase in this paragraph, legacy of a 
</em><br>
<em>&gt;&gt; wasted childhood, like being able to sing the theme song from 
</em><br>
<em>&gt;&gt; Thundercats.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Shame.  There is valuable stuff buried in among the dross.
</em><br>
<p>If you've read _Technical Explanation_, you know my objection. 
<br>
Mysterious answers to mysterious questions.  &quot;Qualia&quot; reifies the 
<br>
confusion into a substance, as did &quot;phlogiston&quot; and &quot;elan vital&quot;.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12202.html">Tennessee Leeuwenburg: "Bayes and the conceptual framework"</a>
<li><strong>Previous message:</strong> <a href="12200.html">Eliezer S. Yudkowsky: "META: No removes, please."</a>
<li><strong>In reply to:</strong> <a href="../0508/12109.html">Richard Loosemore: "Re: Retrenchment"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12202.html">Tennessee Leeuwenburg: "Bayes and the conceptual framework"</a>
<li><strong>Reply:</strong> <a href="12202.html">Tennessee Leeuwenburg: "Bayes and the conceptual framework"</a>
<li><strong>Reply:</strong> <a href="12203.html">H C: "Kolmogorov + Solomonoff"</a>
<li><strong>Reply:</strong> <a href="12204.html">Phil Goetz: "Re: Retrenchment"</a>
<li><strong>Reply:</strong> <a href="12227.html">Richard Loosemore: "The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12201">[ date ]</a>
<a href="index.html#12201">[ thread ]</a>
<a href="subject.html#12201">[ subject ]</a>
<a href="author.html#12201">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
