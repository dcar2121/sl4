<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Remove</title>
<meta name="Author" content="jogdendavis@mail.utexas.edu (jogdendavis@mail.utexas.edu)">
<meta name="Subject" content="Remove">
<meta name="Date" content="2005-09-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Remove</h1>
<!-- received="Fri Sep  2 14:12:39 2005" -->
<!-- isoreceived="20050902201239" -->
<!-- sent="Fri, 02 Sep 2005 15:12:36 -0500" -->
<!-- isosent="20050902201236" -->
<!-- name="jogdendavis@mail.utexas.edu" -->
<!-- email="jogdendavis@mail.utexas.edu" -->
<!-- subject="Remove" -->
<!-- id="1125691956.4318b2341398c@webmailapp4.cc.utexas.edu" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="BAY108-F13C5E0A83CC6C4ECB43F11FBA10@phx.gbl" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> <a href="mailto:jogdendavis@mail.utexas.edu?Subject=Re:%20Remove"><em>jogdendavis@mail.utexas.edu</em></a><br>
<strong>Date:</strong> Fri Sep 02 2005 - 14:12:36 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12199.html">eldras@london.com: "Remove"</a>
<li><strong>Previous message:</strong> <a href="12197.html">Tennessee Leeuwenburg: "Re: Julian Jaynes (Re: JOIN: Olie NcLean)"</a>
<li><strong>In reply to:</strong> <a href="../0508/12170.html">Joshua Amy: "remove"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0508/12174.html">Phil Goetz: "Bayesians &amp; Pascal's wager"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12198">[ date ]</a>
<a href="index.html#12198">[ thread ]</a>
<a href="subject.html#12198">[ subject ]</a>
<a href="author.html#12198">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;From: &quot;Michael Vassar&quot; &lt;<a href="mailto:michaelvassar@hotmail.com?Subject=Re:%20Remove">michaelvassar@hotmail.com</a>&gt;
</em><br>
<em>&gt; &gt;Reply-To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Remove">sl4@sl4.org</a>
</em><br>
<em>&gt; &gt;To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Remove">sl4@sl4.org</a>
</em><br>
<em>&gt; &gt;Subject: Re: drives ABC &gt; XYZ
</em><br>
<em>&gt; &gt;Date: Tue, 30 Aug 2005 22:06:14 -0400
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&gt;We're already
</em><br>
<em>&gt; &gt;&gt;assuming that.  The A B C -&gt; X Y Z example shows how, one step at
</em><br>
<em>&gt; &gt;&gt;a time, the system can take actions that provide greater utility
</em><br>
<em>&gt; &gt;&gt;from the perspective of its top-level goals, that nonetheless end
</em><br>
<em>&gt; &gt;&gt;up replacing all those top-level goals.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;Well then, so long as the ultimate goals are higher utility, from the 
</em><br>
<em>&gt; &gt;perspective of the original goals, than the original goals were, why is 
</em><br>
<em>&gt; &gt;this a problem?  A human would typically not be able to predict the long 
</em><br>
<em>&gt; &gt;term expected utility of a change to its top level goals, but a FAI 
</em><br>
<em>&gt; &gt;wouldn't make such changes unless it could.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&gt;Another question entirely is whether, if the AI is told to maximize
</em><br>
<em>&gt; &gt;&gt;a score relating to the attainment of its top-level goals, and is
</em><br>
<em>&gt; &gt;&gt;given write access to those goals, it will rewrite those goals into
</em><br>
<em>&gt; &gt;&gt;ones more easily attainable?  (We could call this the &quot;Buddhist AI&quot;,
</em><br>
<em>&gt; &gt;&gt;perhaps?)  The REAL top-level goal in that case
</em><br>
<em>&gt; &gt;&gt;is &quot;maximize a score defined by the contents of memory locations X&quot;,
</em><br>
<em>&gt; &gt;&gt;but it doesn't help us to say that &quot;maximization&quot; won't be replaced.
</em><br>
<em>&gt; &gt;&gt;The kinds of goals we don't want to be replaced have referents
</em><br>
<em>&gt; &gt;&gt;in the real world.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;This really is a very very old insight for this list.  Try to familiarize 
</em><br>
<em>&gt; &gt;yourself with the list archive or at least with the major articles.  That 
</em><br>
<em>&gt; &gt;really applies to everyone who hasn't done so.  Suffice it to say that such
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;concerns were addressed very thoroughly years ago.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&gt;You seem to be proposing that an AI will never make mistakes.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;In the human sense, yes.  If an AI is superintelligent and Friendly for any
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;significant time it will reach a state from which it will not ever make the
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;sort of errors of reasoning which humans mean by mistakes.  In fact, any 
</em><br>
<em>&gt; &gt;well calibrated Bayesian built on a sufficiently redundant substrate should
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;never make mistakes in the sense of either acting on implicit beliefs other
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;than its explicit beliefs or holding a belief with unjustified confidence. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;Obviously, computing power, architectural details, and knowledge will 
</em><br>
<em>&gt; &gt;determine the degree to which it will or will not act in the manner which 
</em><br>
<em>&gt; &gt;actually maximized its utility function, but that is not what we humans 
</em><br>
<em>&gt; &gt;mean by a mistake.  We are used to constantly taking actions which we have 
</em><br>
<em>&gt; &gt;every reason to expect to regret.  A FAI shouldn't do that.  This is an 
</em><br>
<em>&gt; &gt;important distinction and not at all a natural one.  It shouldn't be 
</em><br>
<em>&gt; &gt;terribly shocking, but is.  But by now we should be used to the idea that 
</em><br>
<em>&gt; &gt;computers can perform long series of mathematical operations without error,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt;and that performing the right long series of mathematical operations is 
</em><br>
<em>&gt; &gt;equivalent to making a decision under uncertainty, so they should be able 
</em><br>
<em>&gt; &gt;to make decisions under uncertainty without error, though due to the 
</em><br>
<em>&gt; &gt;uncertainty such decisions will usually be less optimal that the decisions 
</em><br>
<em>&gt; &gt;that would have been available given more information.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&gt;Making mistakes is a second way in which top-level goals can
</em><br>
<em>&gt; &gt;&gt;drift away from where they started.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;Making sub-optimal decisions can cause top-level goals to drift, but this 
</em><br>
<em>&gt; &gt;problem is absolutely unoavoidable, but should not be critical (and if it 
</em><br>
<em>&gt; &gt;is critical, that is, fundamental to the way reason works, we will just 
</em><br>
<em>&gt; &gt;have to do as well as we can).  Account must be taken of it when designing 
</em><br>
<em>&gt; &gt;an FAI, but this only requires an incremental development beyond that 
</em><br>
<em>&gt; &gt;needed to protect it from Pascal's Wagers.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12199.html">eldras@london.com: "Remove"</a>
<li><strong>Previous message:</strong> <a href="12197.html">Tennessee Leeuwenburg: "Re: Julian Jaynes (Re: JOIN: Olie NcLean)"</a>
<li><strong>In reply to:</strong> <a href="../0508/12170.html">Joshua Amy: "remove"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0508/12174.html">Phil Goetz: "Bayesians &amp; Pascal's wager"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12198">[ date ]</a>
<a href="index.html#12198">[ thread ]</a>
<a href="subject.html#12198">[ subject ]</a>
<a href="author.html#12198">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
