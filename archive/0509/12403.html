<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Is complex emergence necessary for AGI?</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="RE: Is complex emergence necessary for AGI?">
<meta name="Date" content="2005-09-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Is complex emergence necessary for AGI?</h1>
<!-- received="Tue Sep 20 04:08:15 2005" -->
<!-- isoreceived="20050920100815" -->
<!-- sent="Tue, 20 Sep 2005 11:08:11 +0100 (BST)" -->
<!-- isosent="20050920100811" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="RE: Is complex emergence necessary for AGI?" -->
<!-- id="20050920100811.49403.qmail@web26701.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="JNEIJCJJHIEAILJBFHILAELOGDAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=RE:%20Is%20complex%20emergence%20necessary%20for%20AGI?"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Tue Sep 20 2005 - 04:08:11 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12404.html">Ben Goertzel: "RE: Is complex emergence necessary for AGI?"</a>
<li><strong>Previous message:</strong> <a href="12402.html">BillK: "Re: Retrenchment"</a>
<li><strong>In reply to:</strong> <a href="12393.html">Ben Goertzel: "RE: Is complex emergence necessary for intelligence under limited resources?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12404.html">Ben Goertzel: "RE: Is complex emergence necessary for AGI?"</a>
<li><strong>Reply:</strong> <a href="12404.html">Ben Goertzel: "RE: Is complex emergence necessary for AGI?"</a>
<li><strong>Reply:</strong> <a href="12406.html">Eliezer S. Yudkowsky: "Re: Is complex emergence necessary for AGI?"</a>
<li><strong>Reply:</strong> <a href="12407.html">Richard Loosemore: "Re: Is complex emergence necessary for AGI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12403">[ date ]</a>
<a href="index.html#12403">[ thread ]</a>
<a href="subject.html#12403">[ subject ]</a>
<a href="author.html#12403">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote: 
<br>
<em>&gt; An unpredictable emergent phenomenon in a system is a behavior in a whole
</em><br>
<em>&gt; system that we know can in principle be predicted from the behavior of the
</em><br>
<em>&gt; parts of the system -- but carrying out this prediction in practice is
</em><br>
<em>&gt; extremely computationally difficult.
</em><br>
<p>I'm not going to continue criticising the 'we must use Complexity theory'
<br>
position, as I think that debate is past the point of diminishing returns.
<br>
However given the potential for confusion in the extended back-and-forth
<br>
I think I should clarify my (and to a lesser extent, the SIAI's) position.
<br>
<p>1. The requirement for a certain amount of strong predictability comes from
<br>
the need for Friendliness, analysis that suggests that unless you strongly
<br>
constrain goal system evolution it will be highly unpredictable, and the
<br>
simple fact that when humans are confident something will work, without
<br>
having a technical argument for why it will work, we're usually wrong.
<br>
<p>2. Thus the SIAI has the design requirement; goal system trajectory must
<br>
reliably stay within certain bounds, which is to say that the optimisation
<br>
targets of the overall optimisation process must not drift out of a
<br>
certain region. This is a very specific and limited kind of predictability;
<br>
we don't need specific AI behaviour or cognitive content. I agree that the
<br>
task would be impossible if one were trying to predict much more than just
<br>
the optimisation targets. I am happy to have all kinds of emergence and
<br>
Complexity occuring as long as they stay within the overall constraints,
<br>
though theory and limited experimental experience suggests to me that there
<br>
will be a lot less of this than most people would expect.
<br>
<p>3. If that turns out to be impossible, then we'd agree that AGI development
<br>
should just go ahead using the best probabilistic methods available (maybe;
<br>
it might make sense to develop IA first in that case). But we shouldn't
<br>
write something this important off as impossible without trying really
<br>
hard first, and I think that many people are far too quick to dismiss this
<br>
so that they can get on with the 'fun stuff' i.e. actual AGI design.
<br>
<p>4. Various researchers including Eliezer have spent a fair amount of time
<br>
on this, and so far it looks probable that it is possible given arbitrary
<br>
AGI design that have access to unbounded computing power. The critical
<br>
question is whether there is a tractable design for an AGI that satisfies
<br>
the structural requirements of these theories. This is something that I'm
<br>
working on; unfortunately I'm not aware of anyone else working on it at
<br>
present, though I certainly wish there was.
<br>
<p>5. Any system compatible with the known approaches to strong verification
<br>
of Friendliness will need to be consistently rational, which is to say
<br>
Bayesian from the ground up and have the structural property of being
<br>
'causally clean', although not necessarily driven by expected utility.
<br>
When I first accepted these constraints, they seemed onerous to the point
<br>
of making a tractabale architecture impossible; all the 'powerful'
<br>
techniques I knew of (improved GAs, stochastic codelets, dynamic-toplogy
<br>
NNs, agent systems etc) were thoroughly probabilistic* and hence difficult
<br>
to use or completely unusable. But after a period of research I now
<br>
believe that there are acceptable and even superior replacements for all
<br>
of these that are compatible with strong verification of Friendliness.
<br>
I'm not going to defend that as anything more than a personal opinion at
<br>
this time.
<br>
<p>* Annoying terminology conflict; 'probabilistic methods' are not the same
<br>
thing as 'probabilistic logic'. The former are problem-solving techniques
<br>
that don't reliably obey constraints and/or fail to show a reliable
<br>
minimum performance in relation to normative decision theory; an analogy
<br>
could be drawn to 'soft real time' instead of 'hard real time'. This is
<br>
why saying 'Bayesian logic' to mean 'probabilistic logic' is not too bad
<br>
an idea even if it causes people to fixate on one particular derrivation.
<br>
<p>6. Basically, a rational system of this kind avoids unwanted interactions
<br>
that would violate top-down constraints by constraining the way in which
<br>
components can inteact as you string them together. The resulting
<br>
structure could reasonably be called fractal; combining any set of
<br>
rational components in a rational framework produces a combined system
<br>
that is still rational. Yes, I mean something specific and moderately
<br>
complicated by 'rational' which I don't have space to fully describe.
<br>
Yes, doing this without sacraficing tractability is hard, but at present
<br>
I am optimistic that it will not turn out to be impossible. Yes, I am
<br>
working on a practical experiment/demonstration, this will take some
<br>
time, and I wish I had more resources to do it.
<br>
<p>7. Note that this introduces the notion of 'kinds of Complexity'; a
<br>
system of this kind would be 'Complex' in some respects and non-Complex
<br>
in others. There are plenty of existing technological systems that
<br>
already look like this, so I see no reason to object to it.
<br>
<p>8. Neither I nor the SIAI have claimed that this is the only way to build
<br>
AGI; in fact if it was we'd sleep a lot safer at night. Unfortunately it
<br>
seems entirely possible to build an AI using 'emergence', given enough
<br>
brute force, neuroscience and/or luck. The SIAI's claim is that this is
<br>
a /really bad idea/, because the result is highly likely to be iminical
<br>
to human goals and morals. The claims that any transhuman intelligence
<br>
will renormalise to a rational basis, and that this is actually a better
<br>
way to develop AGI regardless of Friendliness concerns, are weaker ones
<br>
and again stand only as opinion in public at this time.
<br>
<p>9. No-one associated with the SIAI denies that the brain is an example
<br>
of a 'Complex system', or that emergence as a concept won't be useful
<br>
for studying it. We do claim that it is a horrible mess from and that it
<br>
isn't terribly relevant to the task of building an AGI compatible with
<br>
strong Friendliness verification. The position that closely mimicking
<br>
the brain isn't a good way to build AGI regardless of Friendliness is
<br>
again opinion, but the position that an AGI built in this fashion will
<br>
probably be Unfriendly is strongly justified from the previously
<br>
mentioned arguments.
<br>
<p>10. The issue of 'Friendliness content' is genuinely seperate from
<br>
'Friendliness structure' and hence 'strong Friendliness verification'.
<br>
The latter is perhaps a misnomer, as there is some theory that is
<br>
applicable to any attempt to verify that an RPOP will do something
<br>
specific, though it is true that there are some things we would probably
<br>
want an FAI to do that require additional theory to describe and verify.
<br>
Arguments about whether CV, or 'joy, choice and growth', or domain
<br>
protection, or hedonism or Sysops or anything similar are a good idea
<br>
are debates about Friendliness content. This is important, but it's
<br>
well seperated from issues of structural verification and tractable
<br>
implementation, and different in character (because it involves what
<br>
we want instead of how to do it).
<br>
<p>11. Personally I am quite skeptical of Eliezer's ideas about
<br>
Friendliness content, but I support his very important and (as far as
<br>
I can see) valid work on structural verification. I do wish he'd
<br>
publish more, but that criticism can be levelled against most people
<br>
working on AGI, including me. It's true that neither Eliezer nor the
<br>
SIAI has done much work on tractability, which is the main reason
<br>
why I'm working on it. However I agree that the question of how to
<br>
build something in the real world should follow that of how to build
<br>
it in principle, and that people need to be convinced about the
<br>
desireability and theoretical possibility of structural verification
<br>
(of AGIs as general optimisers) before it makes sense to argue about
<br>
if we can do it with real software on contemporary hardware.
<br>
<p>12. Finally, my objection to claims about the value of Complexity theory
<br>
were summed up by one critic's comment that &quot;Wolfram's 'A New Kind of
<br>
Science' would have been fine if it had been called 'Fun With Graph
<br>
Paper'&quot;. The field has produced a vast amount of hype, a small amount
<br>
of interesting maths and very few useful predictive theories in other
<br>
domains. Its proponents are quick to claim that their ideas apply to
<br>
virtually everything, when in practice they seem to have been actually
<br>
useful in rather few cases. This opinion is based on coverage in the
<br>
science press and would be easy to change via evidence, but to date
<br>
no-one has responded to Eliezer's challenge with real examples of
<br>
complexity theory doing something useful. That said, general opinions
<br>
such as this are a side issue; the specifics of AGI are the important
<br>
part.
<br>
<p><em>&gt; It may be that intelligence given limited resources intrinsically
</em><br>
<em>&gt; requires stochastic algorithms, but that is a whole other issue.
</em><br>
<em>&gt; Stochastic algorithms are not all that closely related to emergent
</em><br>
<em>&gt; phenomena -- one can get both emergence and non-emergence from both
</em><br>
<em>&gt; stochastic and non-stochastic algorithms.
</em><br>
<p>I agree, but in practice it does seem that stochastic systems are more
<br>
likely to show/use emergence and vice versa.
<br>
<p>That said, I really must stop spending so much time writing emails.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p><p><p><p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
To help you stay safe and secure online, we've developed the all new Yahoo! Security Centre. <a href="http://uk.security.yahoo.com">http://uk.security.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12404.html">Ben Goertzel: "RE: Is complex emergence necessary for AGI?"</a>
<li><strong>Previous message:</strong> <a href="12402.html">BillK: "Re: Retrenchment"</a>
<li><strong>In reply to:</strong> <a href="12393.html">Ben Goertzel: "RE: Is complex emergence necessary for intelligence under limited resources?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12404.html">Ben Goertzel: "RE: Is complex emergence necessary for AGI?"</a>
<li><strong>Reply:</strong> <a href="12404.html">Ben Goertzel: "RE: Is complex emergence necessary for AGI?"</a>
<li><strong>Reply:</strong> <a href="12406.html">Eliezer S. Yudkowsky: "Re: Is complex emergence necessary for AGI?"</a>
<li><strong>Reply:</strong> <a href="12407.html">Richard Loosemore: "Re: Is complex emergence necessary for AGI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12403">[ date ]</a>
<a href="index.html#12403">[ thread ]</a>
<a href="subject.html#12403">[ subject ]</a>
<a href="author.html#12403">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
