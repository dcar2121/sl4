<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Is complex emergence necessary for intelligence under limited resources?</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Re: Is complex emergence necessary for intelligence under limited resources?">
<meta name="Date" content="2005-09-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Is complex emergence necessary for intelligence under limited resources?</h1>
<!-- received="Tue Sep 20 13:06:20 2005" -->
<!-- isoreceived="20050920190620" -->
<!-- sent="Tue, 20 Sep 2005 15:05:31 -0400" -->
<!-- isosent="20050920190531" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Re: Is complex emergence necessary for intelligence under limited resources?" -->
<!-- id="43305D7B.1080806@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="JNEIJCJJHIEAILJBFHILKEKPGDAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Is%20complex%20emergence%20necessary%20for%20intelligence%20under%20limited%20resources?"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Tue Sep 20 2005 - 13:05:31 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12409.html">Ben Goertzel: "RE: Is complex emergence necessary for AGI?"</a>
<li><strong>Previous message:</strong> <a href="12407.html">Richard Loosemore: "Re: Is complex emergence necessary for AGI?"</a>
<li><strong>In reply to:</strong> <a href="12390.html">Ben Goertzel: "Is complex emergence necessary for intelligence under limited resources?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12410.html">Ben Goertzel: "RE: Is complex emergence necessary for intelligence under limited resources?"</a>
<li><strong>Reply:</strong> <a href="12410.html">Ben Goertzel: "RE: Is complex emergence necessary for intelligence under limited resources?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12408">[ date ]</a>
<a href="index.html#12408">[ thread ]</a>
<a href="subject.html#12408">[ subject ]</a>
<a href="author.html#12408">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em> &gt; Richard,
</em><br>
<em> &gt;
</em><br>
<em> &gt;
</em><br>
<em> &gt;&gt;The answers that others offer to your questions are, pretty much:  no
</em><br>
<em> &gt;&gt;you cannot really avoid complex systems, and mathematical verification
</em><br>
<em> &gt;&gt;of their friendliness is the very last thing you would be able to do.
</em><br>
<em> &gt;&gt;The main defining characteristic of complex systems is that such
</em><br>
<em> &gt;&gt;mathematical verification is out of reach.
</em><br>
<em> &gt;
</em><br>
<em> &gt;
</em><br>
<em> &gt; There is no question that human mind/brain is a complex system which
</em><br>
<em> &gt; achieves its intelligence via emergence, self-organization, strange
</em><br>
<em> &gt; attractors, terminal attractors, and all that great stuff....
</em><br>
<em> &gt;
</em><br>
<em> &gt; And there is little question that emergence-based intelligence is
</em><br>
<em> &gt; intrinsically very difficult to predict and control with a high
</em><br>
<em> &gt; degree of reliability, thus rendering verified Friendliness an
</em><br>
<em> &gt; unlikely outcome.
</em><br>
<em> &gt;
</em><br>
<em> &gt; However, these observations don't tell you much about whether it's
</em><br>
<em> &gt; possible to use digital computers to create an intelligence that
</em><br>
<em> &gt; DOESN'T rely critically on the emergent phenomena typically
</em><br>
<em> &gt; associated with biological complex dynamical systems.
</em><br>
<em> &gt;
</em><br>
<em> &gt; Semi-similarly, the human mind/brain probably uses complex emergent
</em><br>
<em> &gt; phenomena to add 2+2 and get 4, but, a calculator doesn't, and a
</em><br>
<em> &gt; calculator does a better job of arithmetic anyway.
</em><br>
<em> &gt;
</em><br>
<em> &gt; One may argue that flexible, creative intelligence is fundamentally
</em><br>
<em> &gt; different than arithmetic, and is not achievable within limited
</em><br>
<em> &gt; computational resources except via complex, unpredictable emergent
</em><br>
<em> &gt; dynamics.  In fact I strongly SUSPECT this is true, but I haven't SHOWN
</em><br>
<em> &gt; that it's true in a convincing way, and I'm not 100% convinced
</em><br>
<em> &gt; it's true.
</em><br>
<em> &gt;
</em><br>
<em> &gt; If you have a strong argument why this contention is true, I'd be
</em><br>
<em> &gt; very eager to hear it.
</em><br>
<em> &gt;
</em><br>
<em> &gt; On the other hand, some others seems pretty sure that the opposite is 
</em><br>
true,
<br>
<em> &gt; and that it IS possible to achieve powerful intelligence under
</em><br>
<em> &gt; limited resources without requiring unpredictable emergent phenomena.
</em><br>
<em> &gt;
</em><br>
<em> &gt; However, I haven't seen any strong arguments in this direction either.
</em><br>
<em> &gt;
</em><br>
<em> &gt; -- Ben Goertzel
</em><br>
<em> &gt;
</em><br>
<em> &gt;
</em><br>
<p><p><p>Ben,
<br>
<p>In reply to your question, I'll see if I can outline my argument in more
<br>
detail than previously.
<br>
<p>[I am targetting this argument at people who actually understand what a 
<br>
complex system is:  I am beyond the point of trying to educate people 
<br>
who not only do not understand, but are scornful of even making the 
<br>
effort to understand, and who repeatedly throw out false arguments based 
<br>
on caricatures of what I and the complex systems people have claimed.]
<br>
<p><p><p><p>Preliminary Remark 1:  The best I am going to be able to do is offer
<br>
convincing empirical reasons why the thesis is true;  certain proof is
<br>
beyond reach, alas.  So it will always be something of a judgement call
<br>
whether one accepts these arguments or not.
<br>
<p>*Overview.*
<br>
<p>The overall direction of the argument is going to be this:  that in all
<br>
the work on AGI to date, there has been relatively little emphasis on
<br>
getting the &quot;symbols&quot; [please interpret the word loosely: these are just
<br>
the basic representational units that encode the smallest chunks of
<br>
knowledge about the world] to be constructed entirely without programmer
<br>
intervention.  In other words, we tend not to let our systems develop
<br>
their symbols entirely as a result of the interaction of a learning
<br>
mechanism with a stream of environmental input.  Rather, we tend to put
<br>
&quot;ungrounded&quot; symbols in, which we interpret.  The argument is going to
<br>
be that there are indications that this facet of an AGI (whatever
<br>
apparatus allows the symbols to be fully grounded) is going to be more
<br>
important than we suppose, and that it will introduce a great deal of
<br>
complexity, and that that, in turn, will be impossible to avoid.
<br>
<p>*Detailed Version of this Argument*
<br>
<p>We all know that in the Good Old Days, a lot of AI folks would build 
<br>
systems in which they inserted simple tokens, labelled them &quot;arch&quot; 
<br>
&quot;hand&quot; &quot;table&quot; &quot;red&quot; and so on, then wrapped a mechanism around those 
<br>
symbols so the system could manipulate the symbols as a representation 
<br>
of a world, and as a result dispaly some intelligent behavior (like, be 
<br>
able to manipulate, and answer questions about, the placement of blocks 
<br>
on a table).
<br>
<p>What we now know is that these kinds of systems had problems, not the 
<br>
least of which was the fact that the symbols were not grounded:  the 
<br>
system never created the symbols itself, so it was up to the programmer 
<br>
to interpret them manually (so to speak).
<br>
<p>What was the solution?  Clearly, one part of the solution had to be to 
<br>
give the system a learning mechanism, so it could build new symbols out 
<br>
of simpler ones.  Or rather, so it could build symbols from scratch, out 
<br>
of real-world raw input of some kind.  Or rather... well, you can see 
<br>
that it isn't entirely clear, so let's unwrap this in a bit more detail.
<br>
<p>Question:  Do we have to get the system to build all its symbols ab 
<br>
initio, from a completely blank slate?  Can't we give it some starter 
<br>
symbols and ask it to develop from there?  Surely it would be reasonable 
<br>
if the system had some innate knowledge of the world, rather than none 
<br>
at all?  The consensus view on this questtion is that it should not be 
<br>
necessary to go all the way back to signals coming from raw nerve 
<br>
endings in eyes, ears, hands, etc, but that we should be able to put in 
<br>
some primitive symbols and get the rest of them to be generated by the 
<br>
system.  (Aside:  the connectionists were pretty much defined to be the 
<br>
group that broke ranks at this point and insisted that we go down to 
<br>
much deeper levels ... we will avoid talking about them for a moment, 
<br>
however).
<br>
<p>So learning mechanisms should start with some simple symbols and create 
<br>
more complex (more abstract) ones as a result of observing and 
<br>
interacting with the world.  Where do we draw the line, though?  Which 
<br>
primitives are acceptable, and which do we think are too high-level? 
<br>
People disagree.  Many who work in the machine learning field do not 
<br>
really accept any constraints on the high-levelness of their primitive 
<br>
symbols, and are happy to get any symbols to develop into any others, 
<br>
not caring if the primitives look low-level enough that we can believe 
<br>
they escape the grounding problem.
<br>
<p>We, however (from the perspective of this essay) care about the 
<br>
grounding problem and see a learning mechanism as a way to justify our 
<br>
usage of symbols:  we believe that if we find a plausible learning 
<br>
mechanism (or set of mechanisms) it would be capable of going all the 
<br>
way from very primitive sensorimotor signals, or very primitive innate 
<br>
symbols, all the way up to the most abstract symbols the system could 
<br>
ever use.  If we found something as plausible as that, we would believe 
<br>
we had escaped the grounding problem.
<br>
<p>Next step.  We notice that, whatever learning mechanism we put our money 
<br>
on, it is going to complicate our symbols.
<br>
<p>[Terminology Note:  I will use &quot;complicate&quot; to mean just what it seems, 
<br>
and *nothing whatsoever* to do with &quot;Complex&quot; as in Complex Systems. 
<br>
Hopefully this will avoid confusion.  If I use &quot;complex&quot; it will mean 
<br>
&quot;as in 'complex systems'&quot;].
<br>
<p>What do I mean by complicating our symbols?  Only that if they are going 
<br>
to develop, they need more stuff inside them.  They might become 
<br>
&quot;frames&quot; or &quot;scripts&quot; or they might be clusters of features, or they 
<br>
might have prototypes stored in them.... whatever the details, there 
<br>
seems to be a need to put more apparatus in a symbol if it is going to 
<br>
develop.  Or, if not passive data inside the symbol (the way I have 
<br>
implied so far), then more mechanism inside it.  It seems quite hard, 
<br>
for a variety of subtle reasons, to build a good learning mechanism that 
<br>
involves utterly simple, passive symbols (just a token, an activation 
<br>
level and links to other tokens) and a completely separate learning 
<br>
mechanism that is outside the symbols.
<br>
<p>Now, I might be being unfair here by implying that the reason our 
<br>
symbols became more complicated was because, and only because, we needed 
<br>
learning mechanisms.  I don't think I really mean to put it that 
<br>
strongly (and it might be interesting only to future historians of AI 
<br>
anyhow).  We had other reasons for making them more complicated, on of 
<br>
them being that we wanted non-serial models of cognition in which less 
<br>
power was centralized in a single learning mechanism and more power 
<br>
distributed out to the (now active, not passive) symbols.
<br>
<p>So perhaps the best way to summarize is this:  we found, for a variety 
<br>
of reasons, that we were pushed toward more complicated mechanisms 
<br>
(and/or data structures) inside our symbols, in order to get them to do 
<br>
more interesting things, or in order to get over problems that they 
<br>
clearly had.
<br>
<p>This is a very subtle point, so although a lot of people reading this 
<br>
will be right along with me, accepting all of this as obvious, I know 
<br>
there are going to be some voices that dispute it.  For that reason, I 
<br>
am going to dwell on it for just a moment more.
<br>
<p>Sometimes you may think that you do not need complicated, active 
<br>
symbols, and that in fact you can get away with quite simple structures, 
<br>
allied with a sophisticated learning mechanism that builds new symbols 
<br>
and connects them to other symbols in just the right, subtle way that 
<br>
allows the system as a whole to be generally intelligent.  In response 
<br>
to this position, I will say that there is a trap here:  you can always 
<br>
rearrange one of my complicated-symbol systems to make it look as if the 
<br>
symbols are actually simple (and maybe passive also), at the cost of 
<br>
making the learning and thinking mechanism more complicated.  You know 
<br>
the kind of thing I mean:  someone proposes that symbols should be 
<br>
active neuron-like things, and then some anti-neural-net contrarian 
<br>
insists that they can do the same thing with a centralised mechanism 
<br>
acting on a matrix of passive data values.  We have all seen these kinds 
<br>
of disputes, so let's just cut through all the nonsense and point out 
<br>
that you can always reformulate anything to look like anything else, and 
<br>
that some types of formulation look more natural, more efficient and 
<br>
(generally) more parsimonious.  So when I argue that there is a tendency 
<br>
towards more complicated symbols, I mean that the consensus intuition of 
<br>
the community is that the simplest, most parsimonious AGI systems tend 
<br>
to work with symbols that have more complicated apparatus inside them 
<br>
than simply a token plus a couple other bits.
<br>
<p>So I want to wrap up all of this and put it in a Hypothesis:
<br>
<p>*Complicated Symbol Hypothesis*
<br>
<p>&nbsp;&nbsp;&nbsp;&quot;To the extent that AGI researchers acknowledge the need to capture 
<br>
sophisticted learning capabilities in their systems, they discover that 
<br>
they need their symbols to be more complicated.&quot;
<br>
<p>Corollary:
<br>
<p>&nbsp;&nbsp;&nbsp;&quot;The only people who believe that symbols do not need to be 
<br>
complicated are the ones who are in denial about the need for learning 
<br>
mechanisms.&quot;
<br>
<p>***
<br>
Please note the status of this claim.  It is not a provable contention: 
<br>
&nbsp;&nbsp;it is an observation about the way things have been going.  It is 
<br>
based on much thought and intuition about the different kinds of AI 
<br>
systems and their faults and strengths.  The term &quot;complicated&quot; is open 
<br>
to quite wide interpretation, and it does not mean that there is an 
<br>
linear growth on complicatedness as sophistication of learning mechanism 
<br>
goes up, only that we seem to need relatively complicated symbols (as 
<br>
compared with simple passive tokens with activation levels and simple 
<br>
links) in order to capture realistic amounts of learning.
<br>
***
<br>
<p>But now, what of it?  What does it matter that we need more stuff in the 
<br>
symbols?  How is this relevant to the original question about complexity 
<br>
in AGI design?
<br>
<p>To answer this, I am going to try to unpack that idea of &quot;complicated 
<br>
symbols&quot; to get at some of the details.
<br>
<p>When we observe humans going through the process of learning the 
<br>
knowledge that they learn, we notice that they have some extremely 
<br>
powerful mechanisms in there.  I mean &quot;powerful&quot; in the sense of being 
<br>
clever and subtle.  They seem to use analogy a lot, for example.  To get 
<br>
some idea of the subtlety, just go back and look at the stuff Hofstadter 
<br>
comes up with in GEB and in Metamagical Themas.  When I talk about 
<br>
&quot;learning&quot; I don't mean the restricted, narrow sense in which AI folks 
<br>
usually talk about learning systems, I mean the whole shebang:  the 
<br>
full-up, flexible kind of learning that people engage in, where jumping 
<br>
up a level of representation and pulling analogies around seems to be 
<br>
the almost the norm, rather than the exception.
<br>
<p>Getting this kind of learning capability into an AGI is the goal, as far 
<br>
as the present discussion is concerned.  Anything less is not good 
<br>
enough.  I think we can all agree that there is a big gap between 
<br>
current machine capabilities and the awesome generality of the human system.
<br>
<p>But how to do it?  How do we close that gap?
<br>
<p>At this point, everyone has a different philosophy.  I look at language 
<br>
learning in a 2-to-6 year old child and I think I observe that when I 
<br>
talk to that child, I can define pretty much anything in the whole world 
<br>
by referring to loose examples and analogies, and the chold does an 
<br>
astonishing job of getting what I am saying.  I even define grammatical 
<br>
subtleties that way, when teaching how to talk.  But to someone like 
<br>
Chomsky, Fodor or Pinker, this entire process may be governed by a 
<br>
massive amount of innate machinery [and, yes, Fodor at least seems to 
<br>
believe that innateness does not just apply to the grammatical machinery 
<br>
but to most of the conceptual apparatus as well, with just a little 
<br>
content filling in to be done during maturation].
<br>
<p>Then there are folks who don't go for the Chomskian innateness idea, but 
<br>
who do insist that there is nothing wrong with our current ideas about 
<br>
the basic format (data and mechanisms) inside symbols, all we need to do 
<br>
is build a system that is big enough and fast enough, and connect it up 
<br>
to sufficiently much real world input (and realistic motor output 
<br>
systems) and it will develop the same rich repertoire of knowledge that 
<br>
we see in humans.  These people believe that all we need to do is take 
<br>
symbols the way they are currently conceived, add some richer, improved, 
<br>
to-be-determined learning mechanisms that browse on those symbols, and 
<br>
all will eventually be well.
<br>
<p>So what do I think?  I disagree with the position taken in that last 
<br>
paragraph.  Now I am now going to try to focus in on exactly why I disagree.
<br>
<p>Imagine a hypothetical AGI researcher who first decides what the format 
<br>
of a symbol should be and then tries to hunt for a learning mechanism 
<br>
that will allow symbols of that sort to develop as a result of 
<br>
interaction with the world.  Just for the sake of argument (which means 
<br>
don't ask me to defend this!) let's be really naive and throw down some 
<br>
example symbol structure:  maybe each symbol is a token with activation 
<br>
level, truth value, labelled connections to some other symbols (with the 
<br>
labels coming from a fixed set of twenty possible labels) and maybe an 
<br>
instance number.  Who knows, something like that.
<br>
<p>First the researcher convinces themself that the proposed system can 
<br>
work if given ungrounded, programmer-interpreted symbols.  They knock up 
<br>
a system for reasoning about a little knowledge domain and show that 
<br>
given a stream of predigested, interpreted information, the system can 
<br>
come to some interesting conclusions within the domain.  Maybe the 
<br>
system gets loaded on a spacecraft bound for the outer solar system and 
<br>
it can &quot;think&quot; about some of the ship's technical troubles and come up 
<br>
with strategies for resolving them.  And it does a reasonable job, we'll 
<br>
suppose.
<br>
<p>So far, not much learning.  It didn't learn about spaceraft repair from 
<br>
a textbook, or from getting out a wrenc and trying to build spacecraft, 
<br>
or from long conversations with the engineers, it was just preloaded 
<br>
with symbols and information.
<br>
<p>But the researcher is hopeful, so they start adding learning mechanisms 
<br>
in an attempt to get the system to augment itself.  The idea is not just 
<br>
to get it to add to its existing knowledge, but to start with less 
<br>
knowledge and get to where it is now, by doing its own learning.  We 
<br>
are, after all, on a quest to eliminate most of that preloaded knowledge 
<br>
because we want to ground the system in the real world.
<br>
<p>But as the researcher tries to devise more and more powerful kinds of 
<br>
learning mechanisms, they discover a trend.  Those more powerful 
<br>
mechanisms need more complicated stuff inside the symbols.  Let's 
<br>
suppose that they are trying to get analogy to happen:  they find that 
<br>
the existing symbol structure is too limited and they need to add on a 
<br>
bunch of extra doohickeys that represent..... well, they don't represent 
<br>
anything that easily has a name at the symbol level, but they are needed 
<br>
anyhow to get the system to do flexible, tangled kinds of stuff that 
<br>
leads to the building of new symbols out of old.
<br>
<p>When and if the researcher tries to avoid this - tries to keep the 
<br>
symbols nice and clean, like they were before - they discover something 
<br>
rather annoying:  the only way they can do this is to put more stuff in 
<br>
the learning mechanisms (outside of the symbols) instead.  Keep the 
<br>
symbols clean, but make the (non-symbol-internal) learning mechanisms a 
<br>
lot more complex.  And then there is even more trouble, because it turns 
<br>
out that the learning mechanism itself starts to need, not just more 
<br>
machinery, but its own knowledge content!  Now there are two places 
<br>
where knowledge is being acquired:  symbol system and learning engine. 
<br>
And they don't talk, these two systems.  In the process of trying to 
<br>
keep the symbols clean and simple, the learning system had to invent new 
<br>
strategies for learning, and (this is the real cause of the trouble) 
<br>
some of those new learning mechanisms really seemed to be dependent on 
<br>
the content of the world knowledge.
<br>
<p>Something difficult-to-explain is happening here.  It is because 
<br>
learning (knowledge acauisition and refinement) is apparently so 
<br>
flexible and reflexive and tangled in humans, that we have reason to 
<br>
believe that the (human) learning mechanism that is the generator of 
<br>
this behavior must itself involve some quite tangled mechanisms.
<br>
<p>What does this seem to imply for the design of an AGI?  It seems to 
<br>
indicate that if we want a natural, parsimonious design, we are going to 
<br>
inevitably head towards a type of system in which the symbols are 
<br>
allowed to grow in a tangled way right from the outset, with knowledge 
<br>
_about_ the world and knowledge about _how to understand the world_ 
<br>
being inextricably intertwined.  And then, when we try to get those 
<br>
systems to actually work with real world I/O, we will discover that we 
<br>
have to add tweaks and mechanisms inside the symbols, and in the 
<br>
surrounding architecture, to keep the system stable.  And sooner or 
<br>
later we discover that we have a system that seems to learn new concepts 
<br>
from [almost] scratch quite well, but we have lost our ability to 
<br>
exactly interpret what is the meaning of the apparatus inside and around 
<br>
the symbols.  We might find that there is no such thing as a symbol that 
<br>
represents &quot;cup&quot;, there is only a cluster of units and operators that 
<br>
can be used to stand for the cup concept wherever it is needed, and the 
<br>
cluster manifests in different ways depending on whether we are picking 
<br>
up a cup, describing a cup, trying to defining a cup, trying to catch a 
<br>
falling cup, trying to design an artistic looking cup, and so on.
<br>
<p>In short, we may end up discovering, in the process of trying to get a 
<br>
realistic set of human-like, powerul, recursive learning mechanisms to 
<br>
actually work, that all the original apparatus we put in those symbols 
<br>
becomes completely redundant!  All the reasons the system now functions 
<br>
might actually be enshrined in the extra apparatus we had to introduce 
<br>
to (a) get it to learn powerfully and (b) get it to be stable in spite 
<br>
of the tangledness.
<br>
<p>But wait, that sounds like a presumption on my part:  why jump to the 
<br>
conclusion that &quot;all the original apparatus we put in those symbols 
<br>
becomes completely redundant&quot;?  Why on earth should we believe this 
<br>
would happen?  Isn't this just a random piece of guesswork?
<br>
<p>The reason it is not a wild guess is that when the complicated learning 
<br>
mechanisms were introduced, they were so tangled and recursive (with 
<br>
data and mechanism being intertwined) that they forced the system away 
<br>
from the &quot;simple system&quot; regime and smack bang in the middle of the 
<br>
&quot;complex system&quot; regime.  In other words, when you put that kind of 
<br>
reflexivity and adaptiveness in such a system, it is quite likely that 
<br>
the low level mechanisms needed to make its stable will look different 
<br>
from the high-level behavior.  We *want* something that approximates our 
<br>
conventional understanding of symbols to appear in the top level 
<br>
behavior - that is our design goal - and we want enormously powerful 
<br>
adaptive mechanisms.  The experience of teh complex systems community is 
<br>
that you can't start with design goals and easily get mechanisms that do 
<br>
that.  And you especially cannot start with low level mechanisms that 
<br>
look like the desired high-level ones, add a soupcon of adpativeness, 
<br>
and then expect the high and low levels to still be the same.
<br>
<p>Now, the diehard AGI researcher listens to me say this and replies: 
<br>
&quot;But I am not *trying* to emulate the messy human design:  I believe 
<br>
that a completely different design can succeed, involving fairly clean 
<br>
symbols and a very limited amount of tangling.  Just because humans do 
<br>
it that way, doesn't mean that a machine has to.&quot;
<br>
<p>The reply is this.  As you put more powerful learning mechanisms in your 
<br>
designs, I see your symbols getting more complicated.  I see an enormous 
<br>
gap, still, between the power of human learning mechanisms and the power 
<br>
of existing AGI mechanisms.  There is a serious possibility that you can 
<br>
only keep a clean, non-complex AGI design by steering clear of extremely 
<br>
tangled, recursive, powerful knowledge acquisition mechanisms.  And if 
<br>
you steer clear of them, you may find that you never get the AGI to 
<br>
actually work.
<br>
<p>To the complex systems person, observing the only known example of an 
<br>
[non-A]GI (the human mind), it appears to be a matter of faith that real 
<br>
learning can happen in a clean, non-complex AGI design.  Their point of 
<br>
view (my point of view) is:  go straight for the jugular please, and 
<br>
produce mechanisms of awesome, human level learning power, but *without* 
<br>
sending the system into Complex territory, and I will have some reason 
<br>
to believe it possible.
<br>
<p>At the moment, I see no compelling reason to believe, in the face of the 
<br>
complexity I see in the human design.
<br>
<p><p><p>I am sure I could articulate this argument better.  But that is my best 
<br>
shot for the moment.
<br>
<p>Richard Loosemore.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12409.html">Ben Goertzel: "RE: Is complex emergence necessary for AGI?"</a>
<li><strong>Previous message:</strong> <a href="12407.html">Richard Loosemore: "Re: Is complex emergence necessary for AGI?"</a>
<li><strong>In reply to:</strong> <a href="12390.html">Ben Goertzel: "Is complex emergence necessary for intelligence under limited resources?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12410.html">Ben Goertzel: "RE: Is complex emergence necessary for intelligence under limited resources?"</a>
<li><strong>Reply:</strong> <a href="12410.html">Ben Goertzel: "RE: Is complex emergence necessary for intelligence under limited resources?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12408">[ date ]</a>
<a href="index.html#12408">[ thread ]</a>
<a href="subject.html#12408">[ subject ]</a>
<a href="author.html#12408">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
