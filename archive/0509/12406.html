<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Is complex emergence necessary for AGI?</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Is complex emergence necessary for AGI?">
<meta name="Date" content="2005-09-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Is complex emergence necessary for AGI?</h1>
<!-- received="Tue Sep 20 13:02:05 2005" -->
<!-- isoreceived="20050920190205" -->
<!-- sent="Tue, 20 Sep 2005 12:00:06 -0700" -->
<!-- isosent="20050920190006" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Is complex emergence necessary for AGI?" -->
<!-- id="43305C36.9030100@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="20050920100811.49403.qmail@web26701.mail.ukl.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Is%20complex%20emergence%20necessary%20for%20AGI?"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Sep 20 2005 - 13:00:06 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12407.html">Richard Loosemore: "Re: Is complex emergence necessary for AGI?"</a>
<li><strong>Previous message:</strong> <a href="12405.html">Stephen Reed: "Re: Guidelines on Friendly AI"</a>
<li><strong>In reply to:</strong> <a href="12403.html">Michael Wilson: "RE: Is complex emergence necessary for AGI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12409.html">Ben Goertzel: "RE: Is complex emergence necessary for AGI?"</a>
<li><strong>Reply:</strong> <a href="12409.html">Ben Goertzel: "RE: Is complex emergence necessary for AGI?"</a>
<li><strong>Reply:</strong> <a href="12413.html">Michael Wilson: "Re: Is complex emergence necessary for AGI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12406">[ date ]</a>
<a href="index.html#12406">[ thread ]</a>
<a href="subject.html#12406">[ subject ]</a>
<a href="author.html#12406">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michael Wilson wrote:
<br>
<em>&gt; The claims that any transhuman intelligence
</em><br>
<em>&gt; will renormalise to a rational basis, and that this is actually a better
</em><br>
<em>&gt; way to develop AGI regardless of Friendliness concerns, are weaker ones
</em><br>
<em>&gt; and again stand only as opinion in public at this time.
</em><br>
<p>1)  Say &quot;Bayesian basis&quot; not &quot;rational basis&quot;.  In many philosophical 
<br>
systems, the word &quot;rational&quot; takes on meanings that you and I would 
<br>
regard as specific moral content.
<br>
<p>2)  *Any* transhuman intelligence?  That's a generalization over all 
<br>
possible minds.  Did you spend at least a week of solid thought trying 
<br>
to design a counterexample?
<br>
<p>I would guess that *most* other proposed architectures, if they worked 
<br>
well enough to achieve serious optimization power, and turned that 
<br>
optimization power upon themselves, and did not self-destruct, would 
<br>
renormalize to expected utility maximizers or something closely akin. 
<br>
Why?  Because of all the coherence properties which are necessary and 
<br>
often sufficient unto expected utility, same as with probability theory.
<br>
<p><em>&gt; 9. No-one associated with the SIAI denies that the brain is an example
</em><br>
<em>&gt; of a 'Complex system', or that emergence as a concept won't be useful
</em><br>
<em>&gt; for studying it. 
</em><br>
<p>That generalizes over everyone associated with SIAI, and you haven't 
<br>
polled them all...  It seems to me that &quot;emergence&quot; as a concept has 
<br>
proven actively harmful.  Whether there would be a residuum of 
<br>
usefulness if all conceptually harmful aspects were eliminated... 
<br>
probably.  But I get along quite fine without ever attributing anything 
<br>
to &quot;emergence&quot; or calling it an &quot;emergent property&quot;, though from time to 
<br>
time I must say &quot;Y arises from X&quot; or &quot;Y emerges from X&quot;.
<br>
<p><em>&gt; 10. The issue of 'Friendliness content' is genuinely seperate from
</em><br>
<em>&gt; 'Friendliness structure' and hence 'strong Friendliness verification'.
</em><br>
<p>CEV blends content and structure in some ways.
<br>
<p><em>&gt; Arguments about whether CV, or 'joy, choice and growth', or domain
</em><br>
<em>&gt; protection, or hedonism or Sysops or anything similar are a good idea
</em><br>
<em>&gt; are debates about Friendliness content. This is important, but it's
</em><br>
<em>&gt; well seperated from issues of structural verification and tractable
</em><br>
<em>&gt; implementation, and different in character (because it involves what
</em><br>
<em>&gt; we want instead of how to do it).
</em><br>
<p>Problems with &quot;domain protection&quot; or &quot;joyous growth&quot; are structural, not 
<br>
content-only.  Domain protection attempts to use AI as a means to 
<br>
implement world-changes for the sake of desired consequences, without 
<br>
any attempt to have the FAI verify that the changes really do match up 
<br>
with the desired consequences.  &quot;Joyous growth&quot; tries to transfer over a 
<br>
small chunk of moral complexity as direct programming, without setting 
<br>
up a dynamic to transfer over all necessary humane complexity.  These 
<br>
are both distinctly structural issues.
<br>
<p><em>&gt; 12. Finally, my objection to claims about the value of Complexity theory
</em><br>
<em>&gt; were summed up by one critic's comment that &quot;Wolfram's 'A New Kind of
</em><br>
<em>&gt; Science' would have been fine if it had been called 'Fun With Graph
</em><br>
<em>&gt; Paper'&quot;. The field has produced a vast amount of hype, a small amount
</em><br>
<em>&gt; of interesting maths and very few useful predictive theories in other
</em><br>
<em>&gt; domains. Its proponents are quick to claim that their ideas apply to
</em><br>
<em>&gt; virtually everything, when in practice they seem to have been actually
</em><br>
<em>&gt; useful in rather few cases. This opinion is based on coverage in the
</em><br>
<em>&gt; science press and would be easy to change via evidence, but to date
</em><br>
<em>&gt; no-one has responded to Eliezer's challenge with real examples of
</em><br>
<em>&gt; complexity theory doing something useful.
</em><br>
<p>I asked for Complexity math applicable to *cognition* in humans or 
<br>
elsewhere, which produces specific predictions better than 
<br>
maximum-entropy distributions over the same phenomena.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12407.html">Richard Loosemore: "Re: Is complex emergence necessary for AGI?"</a>
<li><strong>Previous message:</strong> <a href="12405.html">Stephen Reed: "Re: Guidelines on Friendly AI"</a>
<li><strong>In reply to:</strong> <a href="12403.html">Michael Wilson: "RE: Is complex emergence necessary for AGI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12409.html">Ben Goertzel: "RE: Is complex emergence necessary for AGI?"</a>
<li><strong>Reply:</strong> <a href="12409.html">Ben Goertzel: "RE: Is complex emergence necessary for AGI?"</a>
<li><strong>Reply:</strong> <a href="12413.html">Michael Wilson: "Re: Is complex emergence necessary for AGI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12406">[ date ]</a>
<a href="index.html#12406">[ thread ]</a>
<a href="subject.html#12406">[ subject ]</a>
<a href="author.html#12406">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
