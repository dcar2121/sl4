<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The Relevance of Complex Systems</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="Re: The Relevance of Complex Systems">
<meta name="Date" content="2005-09-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The Relevance of Complex Systems</h1>
<!-- received="Wed Sep  7 20:28:57 2005" -->
<!-- isoreceived="20050908022857" -->
<!-- sent="Thu, 8 Sep 2005 03:28:54 +0100 (BST)" -->
<!-- isosent="20050908022854" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="Re: The Relevance of Complex Systems" -->
<!-- id="20050908022854.6294.qmail@web26703.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="431F87AE.4080701@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20The%20Relevance%20of%20Complex%20Systems"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Wed Sep 07 2005 - 20:28:54 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12232.html">Ben Goertzel: "RE: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>Previous message:</strong> <a href="12230.html">Eliezer S. Yudkowsky: "Re: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>In reply to:</strong> <a href="12227.html">Richard Loosemore: "The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12234.html">Eliezer S. Yudkowsky: "Re: The Relevance of Complex Systems"</a>
<li><strong>Reply:</strong> <a href="12234.html">Eliezer S. Yudkowsky: "Re: The Relevance of Complex Systems"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12231">[ date ]</a>
<a href="index.html#12231">[ thread ]</a>
<a href="subject.html#12231">[ subject ]</a>
<a href="author.html#12231">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Richard Loosemore  wrote:
<br>
<em>&gt; An interesting subset (those sometimes referred to as being &quot;on the edge
</em><br>
<em>&gt; of chaos&quot;) can show very ordered behavior.  These are Complex Systems.
</em><br>
<em>&gt; Capital &quot;C&quot;, notice, to distinguish them from &quot;complex&quot; in the sense of
</em><br>
<em>&gt; merely complicated.
</em><br>
<p>The fact that the world contains a lot of systems with high-level
<br>
predictive regularities, which are probabilistic rather than
<br>
deterministic for any practical analysis, is hardly news to anyone.
<br>
<p><em>&gt; What is interesting about these is that they often show global
</em><br>
<em>&gt; regularities that do not appear to be derivable (using any form of
</em><br>
<em>&gt; analytic mathematics) from the local rules that govern the unit
</em><br>
<em>&gt; behaviors.
</em><br>
<p>However, the large number of computer simulations you mention have
<br>
quite clearly demonstrated their susceptability to monte-carlo
<br>
simulation and induction of probabilistic predictive rules (which
<br>
are present any time you see 'order in the chaos') from the results
<br>
of such simulations. Frankly whether analytic methods work or not
<br>
is irrelevant to the question of whether you have to use 'emergence'
<br>
and abandon all hope of understanding your AGI; an AGI does not have
<br>
to be an causally tangled system itself to be able to model and
<br>
understand complex systems in the world.
<br>
<p><em>&gt; In other words, you can build alll sorts of systems with enormously
</em><br>
<em>&gt; different local rules and global architectures, and the same patterns
</em><br>
<em>&gt; of global behavior seem to crop up time and again.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What to conclude from this?
</em><br>
<p>Well, let's see. The computer I am typing on has several hundred million
<br>
transistors in it. Several hundred million electronic components, each
<br>
capable of simple computation. But the rules the rules that govern
<br>
their interaction aren't even nice and simple; they're fiendishly
<br>
complex, with millions of overlaid patterns and context-dependent /local/
<br>
behaviours. Surely the global behavior of such a system will be totally
<br>
and utterly incomprehensible! It must be many orders of magnitude beyond
<br>
the difficult of predicting what will happen when say a mere million
<br>
game of life cells are run for a few billion generations!
<br>
<p>In practice of course the computer satisfies a huge number of very
<br>
specific high level constraints with near perfect reliability. Why?
<br>
Because it was designed by deliberative intelligences that chose
<br>
causal mechanisms that would mesh in ways that maintain various
<br>
high-level constraints while manipulating hidden, internal degrees of
<br>
freedom to improve performance. This is how engineering works, and
<br>
it's something that nature has a devil of a time doing due to the
<br>
complete lack of planning and refactoring capability in natural
<br>
selection.
<br>
<p>As you say, most large-scale iterative systems picked at random
<br>
are either chaotic or monotonous. The natural processes that
<br>
produced and sustain humans, including the humain brain, are indeed
<br>
'Complex systems' by this definition. Natural selection had the
<br>
brute force to do massive tial and error search for rule sets that
<br>
worked, and due to its complete dependence on incremental paths
<br>
was doomed to repeat this at one level of organisation after
<br>
another.
<br>
<p><p><em>&gt; Namely: if you look at Mathematics as a whole you can see that
</em><br>
<em>&gt; the space of soluble, analytic, tractable problems is and always
</em><br>
<em>&gt; has been a pitiably small corner of the space of all possible systems.
</em><br>
<p>Yet mathematics has been incredibly useful in understanding how the
<br>
universe works and building useful artefacts, because nature keeps
<br>
throwing up patterns again and again that can be closely approximated
<br>
with maths. The fact that maths can't tractably work out those
<br>
patterns from first principles is irrelevant. We are not attempting to
<br>
extrapolate the entire structure of the universe from a piece of fairy
<br>
cake, we are merely attempting to replicate learning (which means the
<br>
Bayesian superset of the scientific method for rationalists).
<br>
<p><em>&gt; The default assumption made by some people is that Mathematics as a
</em><br>
<em>&gt; domain of inquiry is gradually pushing back the frontiers and that
</em><br>
<em>&gt; in an infinite universe there may come a time when all possible
</em><br>
<em>&gt; problems (equations/systems) become tractable (i.e. analytically
</em><br>
<em>&gt; solvable) BUT there is a substantial body of thought, especially
</em><br>
<em>&gt; post-Godel, that believes that those systems are not just difficult
</em><br>
<em>&gt; to solve, but actually impossible.
</em><br>
<p>To make this relevant you're going to have to state why intelligence
<br>
can only be implemented by a system which is utterly unable to
<br>
provably follow high-level constraints. You've already made one
<br>
stab at explaining why AGIs must be Compex, which was simply wrong.
<br>
Feel free to try again, because without such a proof all this
<br>
glorification of the strict intractability of various arbitrary
<br>
systems is irrelevant. Why is it that this particular class of
<br>
engineered artefact can't benefit from the controlled, selective
<br>
determinism that we have been able to put into all of our other IT
<br>
systems? Why should nature have the only answers here, when we are
<br>
steadily overtaking her everywhere else?
<br>
&nbsp;
<br>
<em>&gt; When people try to cook up formalisms that are supposed to be the core
</em><br>
<em>&gt; of an intelligence, they often refer to systems of interacting parts in
</em><br>
<em>&gt; which they (the designers) think they know (a) what the parts look like
</em><br>
<em>&gt; and (b) how the parts interact and (c) what the system architecture and
</em><br>
<em>&gt; environmental input/output connection amounts to. A CAS person looks at
</em><br>
<em>&gt; these systems and says &quot;Wait, that's a recipe for Complexity&quot;.
</em><br>
<p>If we could somehow import a 'CAS person' from a world where there were
<br>
no digital computers, I'm sure they'd declare the entire idea ridiculous,
<br>
and claim that an economy based on billions of lines of crystaline,
<br>
deterministic, and hideously complex code was utterly impossible.
<br>
<p>Some, possibly most, past attempts at AGI fit your bill. Certainly all
<br>
the connectionist ones do. Given that none of these attempts have
<br>
actually worked I don't think that says much about whether working
<br>
AGI designs can have high-level predictability or not.
<br>
<p><em>&gt; And what they mean is that the designer may *think* that a system can
</em><br>
<em>&gt; be built with (e.g.) bayesian local rules etc., but until they actually
</em><br>
<em>&gt; build a complete working version that grows up whilst interacting with
</em><br>
<em>&gt; a real environment, it is by no means certain that what they will get
</em><br>
<em>&gt; globally is what they thought they were going to get when they invented
</em><br>
<em>&gt; the local aspects of the design.
</em><br>
<p>In engineering, the ability to predict in advance how a device will
<br>
behave increases with competence, the overall level of knowledge in
<br>
the field and the care taken in the design process. I would say that
<br>
applies very well to AGI. Things apparently work in reverse under
<br>
your development paradigm.
<br>
<p><em>&gt; In practice, it just never works that way. The connection between
</em><br>
<em>&gt; local and global is not usually very simple.
</em><br>
<p>Not if you don't know how to design causally clean architectures
<br>
and stable goal systems, then no it doesn't.
<br>
&nbsp;
<br>
<em>&gt; So you may find that if a few well-structured pieces of knowledge are
</em><br>
<em>&gt; set up in the AGI system by the programmer, the Bayesian-inspired local
</em><br>
<em>&gt; mechanism can allow the system to hustle along quite comfortably for a
</em><br>
<em>&gt; while .... until it gradually seizes up.
</em><br>
<p>Rational systems don't 'sieze up'. They may reach a fitness plateau,
<br>
but a competent design will not get itself into states where no
<br>
useful work is being done. I know perfectly well the various sorts
<br>
of behaviour you're alluding to, I used to work on systems like that
<br>
too, including trying to fix them by patching and poking and
<br>
generally carrying out trial and error without being able to reliably
<br>
trace the causes of the problem. If you really want to stick with
<br>
that because you feel that mysterious problems can only be solved by
<br>
mysterious solutions, then so be it.
<br>
<p><em>&gt; (This is a more general version of what was previously called the
</em><br>
<em>&gt; Grounding Problem, of course).
</em><br>
<p>No, it's a rather strange analogy for (I think) the referant drift
<br>
that irrational systems can experience.
<br>
&nbsp;
<br>
<em>&gt; *So this is the lesson that the CAS folks are trying to bring to the
</em><br>
<em>&gt; table.*  (1) They know that most of the time when someone puts together
</em><br>
<em>&gt; a real system of interacting, adaptive units, there can be global
</em><br>
<em>&gt; regularities that are not identical to the local mechanisms.  (2) They
</em><br>
<em>&gt; see AGI people coming up with proposals regarding the mechanisms of
</em><br>
<em>&gt; thought, but those ideas are inspired by certain aspects of what the
</em><br>
<em>&gt; high-level behavior *ought* to be (e.g. Bayesian reasoning), and the AGI
</em><br>
<em>&gt; people often talk as if it is obvious that these are also the underlying
</em><br>
<em>&gt; local mechanisms...... but this jump from local to global is simply not
</em><br>
<em>&gt; warranted!
</em><br>
<p>I'll grant that it's not obvious. In humans, Bayesian reasoning is a
<br>
very high level behaviour. Making it the starting place for intelligence
<br>
is a radical step, but a very well supported one.
<br>
<p><p><em>&gt; These points are so crucial to the issues being discussed on this list,
</em><br>
<em>&gt; that at the very least they need to be taken seriously, rather than
</em><br>
<em>&gt; dismissed out of hand by people who are unbelievably scornful of the
</em><br>
<em>&gt; Complex Systems community.
</em><br>
<p>The dismissal comes from the fact that the CS people keep insisting
<br>
that this is essential when no such thing has been established. The
<br>
scorn comes from the fact that most of the commentary from that
<br>
quarter has been along the lines of 'everything is intractable!
<br>
maths is a dead end! all is fuzzy patterns! look, here is one I
<br>
made on my computer last night, isn't it pretty?', rather than
<br>
descriptions of how specific discoveries from their field can be
<br>
used to design specific mechanisms that make a verifiable contribution
<br>
to AGI.
<br>
<p><em>&gt; I did not describe the details in my last post, only the general
</em><br>
<em>&gt; approach. Not enough for you to dismiss it as closely matching anything.
</em><br>
<p>I extrapolated from your other posts. Frankly there are so many papers
<br>
on potential post-von-Neumann computing paradigms that it's usually a
<br>
fair bet to say that any general approach has already been tried. But
<br>
that's just a prior, please do demonstrate your orginality by describing
<br>
some technical ideas.
<br>
<p><em>&gt; I looked at Flare a few years back.  I was not impressed.
</em><br>
<p>Even a technical criticism of Flare might be interesting.
<br>
<p><em>&gt; later discussed my ideas in detail with the head of a government
</em><br>
<em>&gt; agency that was charged with fostering innovation in this arena
</em><br>
<p>Sorry, large chunks of this list have been there, done that. Why
<br>
bother with annecdotes about anonymous VIPs when you could just wow
<br>
us with a taste of the actual material?
<br>
<p><em>&gt;&gt; This is a phase most people go through at some point in their AI
</em><br>
<em>&gt;&gt; career, a cheap belief that makes it easy to avoid doing hard work.
</em><br>
<em>&gt;
</em><br>
<em>&gt; So that's why I stopped:  I was afraid of hard work.  Darn.
</em><br>
<p>While I agree that James was assuming a lot, his point is valid in
<br>
the general case. Part of the problem is that state-of-the-art tools
<br>
are usually at the difficultly level of being challenging and fun,
<br>
not utterly frustrating. You can really get stuck into writing them,
<br>
work 16 hour days, churn out lots of code and end up wasting a lot of
<br>
time if you don't know precisely how those tools will be used to make
<br>
an actual AI system.
<br>
<p><em>&gt; Prior to starting that AI Ph.D. I had worked long and hard on
</em><br>
<em>&gt; Inmos Transputers (do you know what they were?  massively parallel
</em><br>
<em>&gt; hardware with a novel parallel programming language integrated in
</em><br>
<em>&gt; the chip design), so my comments about the difficulty level were
</em><br>
<em>&gt; based on real world experience of massively parallel systems.
</em><br>
<p>As it happens, I did a project on transputers when I was at university
<br>
(the lecturer for that course was one of the original designers and
<br>
seemd nogalistic about them). The technology seemed cool, at least by
<br>
the standards of mid-1980s computer science, but it struck me as a
<br>
solution looking for a problem. Modern technology can support similar
<br>
architectural designs without all the akward sacrafices and
<br>
limitations; we have decent support libraries and properly general
<br>
purpose computing nodes that make massively parallel clusters
<br>
applicable to a nontrivial number of real world problems.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
To help you stay safe and secure online, we've developed the all new Yahoo! Security Centre. <a href="http://uk.security.yahoo.com">http://uk.security.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12232.html">Ben Goertzel: "RE: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>Previous message:</strong> <a href="12230.html">Eliezer S. Yudkowsky: "Re: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>In reply to:</strong> <a href="12227.html">Richard Loosemore: "The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12234.html">Eliezer S. Yudkowsky: "Re: The Relevance of Complex Systems"</a>
<li><strong>Reply:</strong> <a href="12234.html">Eliezer S. Yudkowsky: "Re: The Relevance of Complex Systems"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12231">[ date ]</a>
<a href="index.html#12231">[ thread ]</a>
<a href="subject.html#12231">[ subject ]</a>
<a href="author.html#12231">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
