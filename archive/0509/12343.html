<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Non-black non-ravens etc.</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Re: Non-black non-ravens etc.">
<meta name="Date" content="2005-09-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Non-black non-ravens etc.</h1>
<!-- received="Tue Sep 13 08:54:25 2005" -->
<!-- isoreceived="20050913145425" -->
<!-- sent="Tue, 13 Sep 2005 10:53:04 -0400" -->
<!-- isosent="20050913145304" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Re: Non-black non-ravens etc." -->
<!-- id="4326E7D0.4030006@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="737b61f30509121754738a8a2e@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Non-black%20non-ravens%20etc."><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Tue Sep 13 2005 - 08:53:04 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12344.html">Phil Goetz: "Re: Logics at multiple levels of abstraction"</a>
<li><strong>Previous message:</strong> <a href="12342.html">Chris Capel: "Re: Non-black non-ravens etc."</a>
<li><strong>In reply to:</strong> <a href="12342.html">Chris Capel: "Re: Non-black non-ravens etc."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12345.html">Ben Goertzel: "CAS, symbolic and subsymbolic AI, etc."</a>
<li><strong>Reply:</strong> <a href="12345.html">Ben Goertzel: "CAS, symbolic and subsymbolic AI, etc."</a>
<li><strong>Reply:</strong> <a href="12347.html">Michael Wilson: "Re: Non-black non-ravens etc."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12343">[ date ]</a>
<a href="index.html#12343">[ thread ]</a>
<a href="subject.html#12343">[ subject ]</a>
<a href="author.html#12343">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Chris Capel wrote:
<br>
<em>&gt; On 9/12/05, Richard Loosemore &lt;<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Non-black%20non-ravens%20etc.">rpwl@lightlink.com</a>&gt; wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;Ben Goertzel wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;I don't think that logical reasoning can serve as the sole basis for an AGI
</em><br>
<em>&gt;&gt;&gt;design, but I think it can serve as one of the primary bases.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;You raise an interesting question.  If you were assuming that &quot;logical
</em><br>
<em>&gt;&gt;reasoning&quot; (in a fairly general sense, not committed to Bayes or
</em><br>
<em>&gt;&gt;whatever) was THE basic substrate of the AGI system, then I would be
</em><br>
<em>&gt;&gt;skeptical of it succeeding.  If, as you suggest, you are only hoping to
</em><br>
<em>&gt;&gt;give logic a more primary role than it has in humans (but not exclusive
</em><br>
<em>&gt;&gt;rights to the whole show), then that I am sure is feasible.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; [...]
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;Lastly, you say:  &quot;However, I suggest that in an AGI system, logical
</em><br>
<em>&gt;&gt;reasoning may exist BOTH as a low-level wired-in subsystem AND as a
</em><br>
<em>&gt;&gt;high-level emergent phenomenon, and that these two aspects of logic in
</em><br>
<em>&gt;&gt;the AGI system may be coordinated closely together.&quot;  If it really did
</em><br>
<em>&gt;&gt;that, it would (as I understand it) be quite a surprise (to put it
</em><br>
<em>&gt;&gt;mildly) ... CAS systems do not as a rule show that kind of weird
</em><br>
<em>&gt;&gt;reflection, as I said in my earlier posts.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'm not sure I can reconcile these two opinions. If you think it's
</em><br>
<em>&gt; feasible to use some sort of logical reasoning, (whether rational
</em><br>
<em>&gt; probability analysis or something else,) as part of the basic
</em><br>
<em>&gt; substrate of a generally intelligent system, and given that any
</em><br>
<em>&gt; successful AI project would necessarily result with a system that
</em><br>
<em>&gt; *does* exhibit logical reasoning at a high level, how could you find
</em><br>
<em>&gt; it unlikely that a system would combine both features? I probably
</em><br>
<em>&gt; misunderstand you.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Oh, and do fractal patterns not emerge in many complex systems? (Curious.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Chris Capel
</em><br>
<p>Chris,
<br>
<p>You are right to point this out:  when I wrote those words I knew I 
<br>
would risk obscuring my point by saying it that way.
<br>
<p>In the first part I was imagining a logic engine sitting side by side 
<br>
with some other system - let's call it the 'symbol engine' - that is 
<br>
able to find the 'things' out of which the world is made and represent 
<br>
them as internal symbols.  The symbol engine is assumed to be a complex 
<br>
system, while the logic engine is not.  The symbol engine has low level 
<br>
mechanisms that may look nothing like symbols (to take a very crude 
<br>
example (that I don't want to imply I am committed to!) consider the raw 
<br>
neural signals in a distributed-representation connectionist net, which 
<br>
are low-level and very non-symbolic), and high-level symbols that emerge 
<br>
out of those low-level mechanisms (the way that distributed patterns can 
<br>
act like whole symbols in the neural net example).  So the symbol engine 
<br>
has layers to it.  The logic engine, on the other hand, is somehow 
<br>
independent of that layering and the basic components of the logic 
<br>
engine *are* the high level symbols created by the symbol engine.  Do we 
<br>
call the logic engine high-levl or low-level?  I am not sure: as I say, 
<br>
it operates on the high-level symbols of the symbol engine, not the low 
<br>
level mechanisms.  But it is kind of a &quot;basic substrate&quot; because it is 
<br>
implemented at just the one level.
<br>
<p>Now the only thing I was saying was that the logic engine would not 
<br>
&quot;emerge&quot; from that system:  it was there from the beginning.
<br>
<p>But of course, another logic engine could arise as an even higher level 
<br>
of the symbol engine (the way it does in our own minds).  Then there 
<br>
would be two of them, one emergent and another basic-substrate.
<br>
<p>You know what?  On reflection this looks like *my* misunderstanding of 
<br>
Ben's original point, because I think he was only saying exactly what I 
<br>
just said.  Apologies.  I had thought he was implying that the first 
<br>
logic engine would be somehow responsible for the emergence of the 
<br>
second one.  I don't think he meant to say that, so I was tilting at a 
<br>
ghost.
<br>
<p><p>But NOW here is an interesting question.
<br>
<p>If that basic-substrate logic engine were to interact with the symbols 
<br>
created by the symbol engine, how would it do it?
<br>
<p>I am referring now to Ben's comment:
<br>
<p><em>&gt; In the human mind, arguably, abstract logical reasoning exists ONLY as a
</em><br>
<em>&gt; high-level emergent phenomenon.  However, I suggest that in an AGI system,
</em><br>
<em>&gt; logical reasoning may exist BOTH as a low-level wired-in subsystem AND as a
</em><br>
<em>&gt; high-level emergent phenomenon, and that these two aspects of logic in the
</em><br>
<em>&gt; AGI system may be coordinated closely together.
</em><br>
<p>Let's overlook the deficiencies of connectionism (aka Neural Nets) for a 
<br>
moment and push my previous example a little further.
<br>
<p>The (Neural Net) symbol engine generates these distributed patterns that 
<br>
correspond to symbols.  The logic engine uses these to reason with.  Now 
<br>
imagine that the logic engine does something (I am not sure what) to 
<br>
cause there to be a need for a new symbol.  This would be difficult or 
<br>
impossible, because there is no way for you to impose a new symbol on 
<br>
the symbol engine; the symbols emerge, so to create a new one you have 
<br>
to set up the right pattern of connections across a big chunk of 
<br>
network, you can't just write another symbol to memory the way you would 
<br>
in a conventional system.  The logic engine doesn't know about neural 
<br>
signals, only high level symbols.
<br>
<p>This question hinges on my suggestion that a logic engine would somehow 
<br>
need to create or otherwise modify the symbols themselves.  So tell me 
<br>
folks:  can we guarantee that the symbol engine can get along without 
<br>
ever touching any symbols?  You know more about this than I do.  Is 
<br>
there going to be a firewall between the logic engine and whatever 
<br>
creates and maintains symbols?  You can look but you can't touch, so to 
<br>
speak?  This all speaks to the question of what exactly such a built-in 
<br>
logic engine would be for, exactly?
<br>
<p>I could stand to be enlightened on this point.  In my world, I wouldn't 
<br>
try to connect them, so I have not yet considered the problem.
<br>
<p>Richard Loosemore
<br>
<p><p><p><p>P.S.  About fractals in CAS:  that was what was in the back of my mind 
<br>
as I wrote .... I don't think they do.  If they do, I suspect the CAS 
<br>
would be a weird one.  I'll try to do a little research on that one.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12344.html">Phil Goetz: "Re: Logics at multiple levels of abstraction"</a>
<li><strong>Previous message:</strong> <a href="12342.html">Chris Capel: "Re: Non-black non-ravens etc."</a>
<li><strong>In reply to:</strong> <a href="12342.html">Chris Capel: "Re: Non-black non-ravens etc."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12345.html">Ben Goertzel: "CAS, symbolic and subsymbolic AI, etc."</a>
<li><strong>Reply:</strong> <a href="12345.html">Ben Goertzel: "CAS, symbolic and subsymbolic AI, etc."</a>
<li><strong>Reply:</strong> <a href="12347.html">Michael Wilson: "Re: Non-black non-ravens etc."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12343">[ date ]</a>
<a href="index.html#12343">[ thread ]</a>
<a href="subject.html#12343">[ subject ]</a>
<a href="author.html#12343">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
