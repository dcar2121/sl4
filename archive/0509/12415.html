<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Is complex emergence necessary for intelligence under limited resources?</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Re: Is complex emergence necessary for intelligence under limited resources?">
<meta name="Date" content="2005-09-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Is complex emergence necessary for intelligence under limited resources?</h1>
<!-- received="Wed Sep 21 09:25:18 2005" -->
<!-- isoreceived="20050921152518" -->
<!-- sent="Wed, 21 Sep 2005 11:24:26 -0400" -->
<!-- isosent="20050921152426" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Re: Is complex emergence necessary for intelligence under limited resources?" -->
<!-- id="43317B2A.1030605@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="JNEIJCJJHIEAILJBFHILMEPDGDAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Is%20complex%20emergence%20necessary%20for%20intelligence%20under%20limited%20resources?"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Wed Sep 21 2005 - 09:24:26 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12416.html">D Goel: "Re: [sl4] Singularity, &quot;happiness&quot;, suffering, Mars"</a>
<li><strong>Previous message:</strong> <a href="12414.html">Richard Loosemore: "Complexity Theory vs Complex Systems"</a>
<li><strong>In reply to:</strong> <a href="12410.html">Ben Goertzel: "RE: Is complex emergence necessary for intelligence under limited resources?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12418.html">Ben Goertzel: "RE: Is complex emergence necessary for intelligence under limited resources?"</a>
<li><strong>Reply:</strong> <a href="12418.html">Ben Goertzel: "RE: Is complex emergence necessary for intelligence under limited resources?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12415">[ date ]</a>
<a href="index.html#12415">[ thread ]</a>
<a href="subject.html#12415">[ subject ]</a>
<a href="author.html#12415">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; Richard,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think you have made a reasonably convincing argument why AGI should
</em><br>
<em>&gt; proceed via allowing a proto-AGI system to engage with a world via sensors
</em><br>
<em>&gt; and actuators, and construct its own symbols to represent the world around
</em><br>
<em>&gt; it.  I agree that these symbols will generally not be simple, and also that
</em><br>
<em>&gt; sophisticated learning mechanisms will be required to learn them.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What is not clear to me is why the &quot;complexity&quot; of these symbols and
</em><br>
<em>&gt; learning  mechanisms necessarily has to entail &quot;complexity&quot; in the sense of
</em><br>
<em>&gt; complex systems theory (as  opposed to just &quot;complexity&quot; in the sense of
</em><br>
<em>&gt; complicatedness and sophistication).
</em><br>
<p>Your question makes me fear that I muddied the waters when I talked 
<br>
about the complicatedness of symbols.  And I was trying so hard, too!
<br>
<p>The answer to your question was in the text, but alas it may have gotten 
<br>
buried.  When I talked about the &quot;complicatedness&quot; of the symbols, I was 
<br>
summarizing something that I then (later on) tried to spell out in more 
<br>
detail:  I was postulating that when we try to get serious amounts of 
<br>
high-level learning into an AGI (analogy-making and so on), we are 
<br>
forced to put in precisely the kinds of tangled, reflexive, 
<br>
self-modifying mechanisms that leads to complexity in the system as a 
<br>
whole, and in complicatedness in the symbols.
<br>
<p>Even without any other arguments, I was claiming, we should be deeply 
<br>
worried that AGI programmers tend to shy away from trying to build those 
<br>
very high-level learning mechanisms.  Is it fair of me to say this?  Do 
<br>
they really shy away from them?  I think they do.  Many AGI folks talk 
<br>
about such stuff as if it is next on the list after they get the basic 
<br>
mechanisms sorted out - but it might also be that the real reason people 
<br>
avoid them is that nobody has much idea how to build them *without* 
<br>
straying into the domain of complex, self-modifying, tangled, recursive 
<br>
(etc) systems.  Let me put the same point, but coming from the other 
<br>
direction:  do you see any research groups throwing themselves full-tilt 
<br>
into the problem of understanding those high-level learning (aka 
<br>
concept-building or structure-finding) mechanisms?  Oh boy, yes!  For 
<br>
just one example, look at the FARG group at Indiana U 
<br>
(<a href="http://www.cogsci.indiana.edu/index.html">http://www.cogsci.indiana.edu/index.html</a>).  But these folks take the 
<br>
complex-systems approach.  They eat, drink and breathe complexity.
<br>
<p>So it is not that the complicatedness of symbols means anything by 
<br>
itself (I apologize for misleading you there) - a symbol, after all, is 
<br>
a local mechanism, and the point of a complex system is that the 
<br>
complexity is in the system as a whole, not in the local units.  No, 
<br>
what I meant was:  the symbols have to be complicated precisely because 
<br>
we need to make them develop by themselves using powerful (tangled, 
<br>
complex) learning mechanisms.  [The complicatedness of the symbols 
<br>
raises a slightly different issue that I started to discuss, but for 
<br>
clarity I will leave it aside here and come back to it if you wish].
<br>
<p>I submit that most AGI people assume that they are going to be able to 
<br>
crack the learning problem later, *without* having to resort to tangled 
<br>
complexity.  They believe that they will be able to invent all the 
<br>
learning mechanisms required in an AGI without having to give those 
<br>
learning mechanisms the power to trasnform the system as a whole into a 
<br>
complex system.
<br>
<p>I further submit that they believe the format for the symbols that they 
<br>
are using now (relatively uncomplicated and interpretable, in my 
<br>
terminology) will not be substantially affected by the later 
<br>
introduction of those learning mechanisms.  When I talked about the 
<br>
symbols becoming more complicated, I was referring to this assumption, 
<br>
saying that I believe that the later introduction of proper learning 
<br>
mechanisms will actually affect the format of the symbols, and that the 
<br>
change may be so huge that we may discover that the only way to get the 
<br>
symbols to develop by themselves (supplied only with real world I/O and 
<br>
no programmer intervention) is to to give them so much freedom to 
<br>
develop that all the apparatus we put into in the symbols, that we 
<br>
thought was so important, turns out to be redundant.
<br>
<p>So all of this is about observing a process within the AI community. 
<br>
Specifically, these observations.  (a) I see people (over the course of 
<br>
at least four decades now) concentrating on the mechanisms-of-thought in 
<br>
non-grounded systems, and postponing the problem of building the kind of 
<br>
powerful learning mechanisms that could generate the symbols that are 
<br>
used in those mechanisms-of-thought.  (b) I see a few people embracing 
<br>
the problem of those powerful learning mechanisms, but those people take 
<br>
a complex systems approach because all the indications are that the kind 
<br>
of reflexive, self-modifying characteristics needed in such learning 
<br>
mechanisms will lead to complex systems.  Now, why do the latter group 
<br>
go straight for complex systems?  We need to be careful not to 
<br>
trivialise their reasons for doing so:  they don't do it just because 
<br>
it's fun;  they don't do it because they don't know any better;  they 
<br>
don't do it because they are mathematically naive wimps who have no 
<br>
faith in the power of mathematics to grow until it can describe things 
<br>
that people previously dismissed as to difficult to describe .... they 
<br>
do it because they have an extremely broad range of knowledge, have come 
<br>
at the problem from a number of angles, and have decided that there is a 
<br>
very profound message coming from all the studies that have been done on 
<br>
different sorts of complex system.  And the message, as far as they are 
<br>
concerned, is that *if* you are going to build a mechanism that captures 
<br>
what appears to be an extremely reflexive, self-modifying and tangled 
<br>
ability as the kinds of learning and concept building that are important 
<br>
to them, *then* you had jolly well better get used to the idea that the 
<br>
mechanism is going to be complex, because in the thousands upon 
<br>
thousands of other examples of systems with that kind of tangledness, we 
<br>
always observe an element of complexity.
<br>
<p>So, from these two observations, I take away this conclusion.  The 
<br>
people who insist that we will be able to build powerful learning 
<br>
mechanisms in an AGI *without* recourse to complexity, are precisely 
<br>
those people who have not tried to build such mechanisms.  They offer a 
<br>
firm conviction that they will be able to do so, but they can offer 
<br>
nothing except their blind faith in the future.  [And where they do 
<br>
attempt to build learning mechanisms, they only try relatively simple 
<br>
kinds of learning (concept building) and they have never demonstrated 
<br>
that their mechanisms are powerful enough to ground a broad-based 
<br>
intelligence].  The only people who have ventured into this domain have 
<br>
accepted the evidence that complexity is unavoidable, and they give 
<br>
reasons why they think so.
<br>
<p>The standard response to this argument (at least from some quarters in 
<br>
this list), is that I have not given a demonstration why an AGI *cannot* 
<br>
be built without complexity, or a demonstration of why complexity *must* 
<br>
be necessary.  This is a completely nonsensical demand:  a rigorous 
<br>
proof or demonstration is not possible: that is the whole point of my 
<br>
argument!  If I could give a rigorous mathematical or logical proof why 
<br>
you cannot build an AGI without complexity, the very rigor of that proof 
<br>
would invalidate my argument!
<br>
<p>I most certainly have given a demonstration:  it is an empirical one. 
<br>
Look at all the examples of learning systems that work; look at the way 
<br>
the non-complex AGI researchers run away from powerful learning 
<br>
mechanisms and never demonstrate any convincing reasons to believe their 
<br>
systems can be grounded;  look at all the evidence from natural systems 
<br>
that are intelligent, but which do not use crystalline, non-complex 
<br>
thinking and learning mechanisms;  look at all the systems which have 
<br>
large numbers of interacting, self-modifying, adaptive components that 
<br>
interact with the world (not just intelligent systems, but all the 
<br>
others) and ask yourself why it is that we cannot find any examples 
<br>
where someone could start by observing the global behavior of the system 
<br>
and then reason back to the local mechanisms that must have given rise 
<br>
to that behavior.
<br>
<p>And at the end of all this observing, ask yourself what reason an AGI 
<br>
researcher has to denigrate the human mind as a lousy design (even 
<br>
thoughthey don't understand the design!) and say that they can do it 
<br>
better without introducing complexity, without offering any examples of 
<br>
working (grounded) systems to back up their claim, and without offering 
<br>
any mathematical proofs or demonstrations that their approach will one 
<br>
day work, when they get the learning and grounding mechanisms fully 
<br>
worked out.
<br>
<p>The boot, I submit, is on the other foot.  The rest of the community is 
<br>
asking the non-complex AGI folks [apologies for the awkward term:  I am 
<br>
not sure what to call people who eschew complexity, except perhaps the 
<br>
Old Guard] why *we* should go along with what looks like their blind 
<br>
faith in being able to build a fully capable, grounded AGI without 
<br>
resorting to complexity.
<br>
<p>*****
<br>
<p>I have concentrated on one aspect of the learning mechanisms (the 
<br>
expected tangledness, if you like) because this is the most obvious 
<br>
thing that would lead to complexity.  However, this is not the only 
<br>
plank of the argument.  I have brought up some of these other issues 
<br>
elsewhere, but it might be better for me to organize them more 
<br>
systematically, rather than throw them into the pot right now.
<br>
<p>In closing, let me say that I look very negative when presenting this 
<br>
argument, even though I actually do have concrete suggestions for what 
<br>
we should do instead.  Some people have jumped to false conclusions 
<br>
about what I would recommend us doing, if the above were true:  the fact 
<br>
is, I have barely even mentioned what I think we should be doing.  Right 
<br>
now, my goal is to suggest that here we have an issue of truly enormous 
<br>
importance, and that we should first of all accept that it really is an 
<br>
issue, then go on to talk about what can be done about it.  But I want 
<br>
to get to first base first and get people to agree that there is an issue.
<br>
<p>Richard Loosemore.
<br>
<p><p><p><p><p><p><em>&gt; Clearly, it does in the human brain.
</em><br>
<em>&gt; But you haven't demonstrated that it does in an AGI system, nor have you
</em><br>
<em>&gt; really given any arguments in this direction.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Personally I suspect you are probably right and that given limited resources
</em><br>
<em>&gt; complex-systems-style &quot;complexity&quot; probably IS necessary for effective
</em><br>
<em>&gt; symbol grounding.  But I don't have a strong argument in this direction,
</em><br>
<em>&gt; unfortunately, just an intuition.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; -- Ben
</em><br>
<em>&gt; 
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12416.html">D Goel: "Re: [sl4] Singularity, &quot;happiness&quot;, suffering, Mars"</a>
<li><strong>Previous message:</strong> <a href="12414.html">Richard Loosemore: "Complexity Theory vs Complex Systems"</a>
<li><strong>In reply to:</strong> <a href="12410.html">Ben Goertzel: "RE: Is complex emergence necessary for intelligence under limited resources?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12418.html">Ben Goertzel: "RE: Is complex emergence necessary for intelligence under limited resources?"</a>
<li><strong>Reply:</strong> <a href="12418.html">Ben Goertzel: "RE: Is complex emergence necessary for intelligence under limited resources?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12415">[ date ]</a>
<a href="index.html#12415">[ thread ]</a>
<a href="subject.html#12415">[ subject ]</a>
<a href="author.html#12415">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
