<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The Relevance of Complex Systems [was: Re: Retrenchment]</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: The Relevance of Complex Systems [was: Re: Retrenchment]">
<meta name="Date" content="2005-09-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The Relevance of Complex Systems [was: Re: Retrenchment]</h1>
<!-- received="Wed Sep  7 20:13:41 2005" -->
<!-- isoreceived="20050908021341" -->
<!-- sent="Wed, 07 Sep 2005 19:11:44 -0700" -->
<!-- isosent="20050908021144" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: The Relevance of Complex Systems [was: Re: Retrenchment]" -->
<!-- id="431F9DE0.6090503@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="431F87AE.4080701@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20The%20Relevance%20of%20Complex%20Systems%20[was:%20Re:%20Retrenchment]"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Sep 07 2005 - 20:11:44 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12231.html">Michael Wilson: "Re: The Relevance of Complex Systems"</a>
<li><strong>Previous message:</strong> <a href="12229.html">Eliezer S. Yudkowsky: "Re: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>In reply to:</strong> <a href="12227.html">Richard Loosemore: "The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12232.html">Ben Goertzel: "RE: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>Reply:</strong> <a href="12232.html">Ben Goertzel: "RE: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12230">[ date ]</a>
<a href="index.html#12230">[ thread ]</a>
<a href="subject.html#12230">[ subject ]</a>
<a href="author.html#12230">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Richard Loosemore wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I want to conclude by quoting one extract from your message below that
</em><br>
<em>&gt; sums up the whole argument:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; [Richard Loosemore wrote:]
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;&gt; Like Behaviorists and Ptolemaic Astronomers, they mistake a
</em><br>
<em>&gt;&gt;&gt; formalism that approximately describes a system for the mechanism
</em><br>
<em>&gt;&gt;&gt; that is actually inside the system.  They can carry on like this
</em><br>
<em>&gt;&gt;&gt; for centuries, adding epicycles onto their models in order to
</em><br>
<em>&gt;&gt;&gt; refine them.  When Bayesian Inference does not seem to cut it,
</em><br>
<em>&gt;&gt;&gt; they assert that *in principle* a sufficiently complex Bayesian 
</em><br>
<em>&gt;&gt;&gt; Inference system really would be able to cut it ... but they are
</em><br>
<em>&gt;&gt;&gt; not able to understand that the &quot;in principle&quot; bit of their argument
</em><br>
<em>&gt;&gt;&gt; depends on subtleties that they don't think much about.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; There are subtleties to real-world intelligence that don't
</em><br>
<em>&gt;&gt; appear in standard Bayesian decision theory (he said controversially),
</em><br>
<em>&gt;&gt; but Bayesian decision theory can describe a hell of a lot more than
</em><br>
<em>&gt;&gt; naive students think.  I bet that if you name three subtleties, I can 
</em><br>
<em>&gt;&gt; describe how Bayes plus expected utility plus Solomonoff
</em><br>
<em>&gt;&gt; (= AIXI) would do it given infinite computing power. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You make my point for me.  The Ptolemaic astronomers would have used
</em><br>
<em>&gt; exactly the same argument that you do:  &quot;Name some subtle ways in which
</em><br>
<em>&gt; the heavenly bodies do not move according to the standard set of
</em><br>
<em>&gt; epicycles, and I can describe how an infinite number of epicycles would
</em><br>
<em>&gt; do it....&quot;   Yes, yes yes!  But they were wrong, because the *real*
</em><br>
<em>&gt; mechanism for planetary movement was not actually governed by epicycles,
</em><br>
<em>&gt; it was governed by something completely different, and all the Ptolemaic
</em><br>
<em>&gt; folks were barking up the wrong tree when though their system was in
</em><br>
<em>&gt; principle capable of covering the data.
</em><br>
<p>First off, you didn't answer my challenge.  Name three subtleties, heck, 
<br>
name one subtlety, and see what I make of it.
<br>
<p>Your Ptolemaic argument misses the point.  Ellipses are not superior to 
<br>
epicycles because they are local, or cheaply computable, or any such 
<br>
computational advantage.  Ellipses are superior to epicycles because 
<br>
they are simpler.  If planets really moved in ellipses, but we had to 
<br>
try and approximate ellipses using epicycles because ellipses were just 
<br>
too darn expensive to compute directly, that would be an appropriate 
<br>
analogy to Bayesian probability versus cheaper approximations.  If 
<br>
planets really moved in ellipses, but we had to approximate ellipses 
<br>
with epicycles because ellipses were intractable, then you'd damn well 
<br>
better understand that planets *really* move in ellipses.  And use that 
<br>
knowledge to develop your fast epicycle algorithms and your programs 
<br>
that find good, simple epicyclic approximations when looking at elliptic 
<br>
data.  It wouldn't be good to stare at the computer screen for months, 
<br>
go nuts, and start thinking that planets *really* moved in epicycles.
<br>
<p>Your claim amounts to saying that nobody can actually build a Carnot 
<br>
engine, therefore thermodynamics is invalid as a description of the 
<br>
physical universe because it fails to take into account the subtleties 
<br>
of real engines, and obviously Carnot was an idiot who never got his 
<br>
hands dirty on a real engineering project.
<br>
<p>If you want to argue that Bayesian probability can't do something *in 
<br>
principle*, or that it's the wrong system to describe the ideal we're 
<br>
trying to approximate, you have to say something other than &quot;It's 
<br>
computationally intractable&quot;.  You have to show me a specific case where 
<br>
Bayesian reasoning breaks down - where some other system produces 
<br>
answers that are better.  Perhaps you can find a case where you can 
<br>
produce only slightly inferior answers using simpler methods, a la 
<br>
Gigerenzer - those are fascinating and useful.  But much more important 
<br>
would be a case where an alternate method produces answers that are 
<br>
*better* than Solomonoff induction using infinite computing power.
<br>
<p><em>&gt; And a vital corollary to the above arguments about how to build an AGI
</em><br>
<em>&gt; is the fact that _absolutely guaranteeing_ a Friendly AI is impossible
</em><br>
<em>&gt; the way you are trying to do it.  If AGI systems that actually work are
</em><br>
<em>&gt; Complex (and all the indications are that they are indeed Complex), then
</em><br>
<em>&gt; guarantees are impossible.  It's a waste of time to look for absolute
</em><br>
<em>&gt; guarantees.  (Other indications of Friendliness .... now that's a
</em><br>
<em>&gt; different matter).
</em><br>
<p>That's another non-sequitur.  Suppose you have to write localizable, 
<br>
cheaply computable approximations to an ideal.  If the localizable, 
<br>
cheaply computable approximations are generated by the AI itself, the AI 
<br>
can guarantee that the approximation is an approximation *of* the ideal, 
<br>
rather than something else.
<br>
<p>The wrong sequence of cosmic-ray transistor flips might turn any FAI bad 
<br>
- in the limiting case, the bitflips rewrite your whole AI from scratch. 
<br>
&nbsp;&nbsp;In that sense, there are no guarantees.  (Though if you formally 
<br>
guarantee that no *possible* three bitflips can corrupt your AI, you'll 
<br>
have gone a long way toward ensuring that any *random* thousand bitflips 
<br>
is extremely unlikely to corrupt your AI.)
<br>
<p>What I want is no significant *independent* sources of failure that 
<br>
apply to each round of recursive self-improvement.  Each transistor in 
<br>
your computer is less than 99% likely to operate over the next year - 
<br>
you might drop your computer out a window, or accidentally spoon ice 
<br>
cream onto the motherboard.  But, such catastrophes destroy large groups 
<br>
of transistors simultaneously.  Any given transistor has less than a 99% 
<br>
chance of working for one year - but that doesn't mean that, in a chip 
<br>
of twenty million transistors, you can raise 0.99 to the power of twenty 
<br>
million and calculate a negligible probability of the whole chip lasting 
<br>
a year.  The probability that transistor A works for a year is not 
<br>
independent of the probability that transistor B works for a year, so 
<br>
you can't just multiply P(A) and P(B) to get P(AB).  If each transistor 
<br>
in your computer had a 99% *independent* chance of failing each year, it 
<br>
wouldn't work for a day.
<br>
<p>A seed AI needs to rewrite its own source code, then that new source 
<br>
code may rewrite itself again, and so on.  If there are independent 
<br>
sources of error, then 99% reliability on one rewrite is nearly sure to 
<br>
fail on a thousand sequential rewrites.  A Friendly AI needs some way to 
<br>
assert with extremely high probability that each rewrite maintains some 
<br>
invariant.
<br>
<p>Somehow human mathematicians manage to scale up abstract mathematical 
<br>
reasoning to theorems far larger than computers have yet succeeded in 
<br>
proving.  No known algorithm could independently prove a CPU design 
<br>
correct in the age of the universe, but with human-chosen *lemmas* we 
<br>
can get machine-*verified* correctness proofs.  The critical property of 
<br>
proof in an axiomatic system is not that it's certain, since the system 
<br>
might be inconsistent for all we know or can formally prove.  The 
<br>
critical property is that, *if* the system is consistent, then a proof 
<br>
of ten thousand steps is as reliable as a proof of ten steps.  There are 
<br>
no independent sources of failure.  I hope that provably correct 
<br>
rewrites, like provably correct CPUs, will be managable if the AI can 
<br>
put forth deductive reasoning with efficiency, tractability, and 
<br>
scalability at least equalling that of a human mathematician.  An 
<br>
AI-complete problem?  Sure, but let's not forget - we *are* trying to 
<br>
design an AI.
<br>
<p>I would like to be able to say that if the framework we're using to 
<br>
understand what our guarantees *mean* doesn't contain some deep, 
<br>
concealed conceptual flaw - that's the part of the risk that can never 
<br>
be wholly eliminated - then the system will not fail catastrophically, 
<br>
guaranteed with p&gt;0.999 reliability.  Barring improbably huge 
<br>
conjunctions of cosmic-ray transistor flips, or the Simulators reaching 
<br>
into the Matrix to tweak the AI code.
<br>
<p><em>&gt; These points are so crucial to the issues being discussed on this list,
</em><br>
<em>&gt; that at the very least they need to be taken seriously, rather than
</em><br>
<em>&gt; dismissed out of hand by people who are unbelievably scornful of the
</em><br>
<em>&gt; Complex Systems community.
</em><br>
<p>Give me one good, solid, predictive equation applying to cognitive 
<br>
systems that stems from CAS.  I am interested in knowledge, not proud 
<br>
statements of ignorance.  Don't tell me &quot;some things are intractable&quot;. 
<br>
I already knew that.  Show me a tractable approximation of something 
<br>
important.  Show me something better than raw Bayes on bounded computing 
<br>
power, and I'll be interested in that, as I'm interested in Gigerenzer's 
<br>
stuff, despite the silly things that some of the Fast and Frugal crowd 
<br>
have said about the value of Bayesian theory.  Of course I'll go on 
<br>
measuring the efficacy of your shiny new method in Bayesian terms, but 
<br>
that can't be helped.
<br>
<p>Again, Phil Goetz gave an excellent example of what I was looking for 
<br>
when he made up the fake example of power laws applying to the 
<br>
distribution of goals and subgoals of various sizes, as related to the 
<br>
tractability of the system.  Can you show me something like that, but 
<br>
non-fake?
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12231.html">Michael Wilson: "Re: The Relevance of Complex Systems"</a>
<li><strong>Previous message:</strong> <a href="12229.html">Eliezer S. Yudkowsky: "Re: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>In reply to:</strong> <a href="12227.html">Richard Loosemore: "The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12232.html">Ben Goertzel: "RE: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>Reply:</strong> <a href="12232.html">Ben Goertzel: "RE: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12230">[ date ]</a>
<a href="index.html#12230">[ thread ]</a>
<a href="subject.html#12230">[ subject ]</a>
<a href="author.html#12230">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
