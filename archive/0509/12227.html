<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: The Relevance of Complex Systems [was: Re: Retrenchment]</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="The Relevance of Complex Systems [was: Re: Retrenchment]">
<meta name="Date" content="2005-09-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>The Relevance of Complex Systems [was: Re: Retrenchment]</h1>
<!-- received="Wed Sep  7 18:37:42 2005" -->
<!-- isoreceived="20050908003742" -->
<!-- sent="Wed, 07 Sep 2005 20:37:02 -0400" -->
<!-- isosent="20050908003702" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="The Relevance of Complex Systems [was: Re: Retrenchment]" -->
<!-- id="431F87AE.4080701@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="431CB549.8020101@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20The%20Relevance%20of%20Complex%20Systems%20[was:%20Re:%20Retrenchment]"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Wed Sep 07 2005 - 18:37:02 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12228.html">Peter Voss: "RE: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>Previous message:</strong> <a href="12226.html">rpwl@lightlink.com: "Re: Introduction"</a>
<li><strong>In reply to:</strong> <a href="12201.html">Eliezer S. Yudkowsky: "Re: Retrenchment"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12228.html">Peter Voss: "RE: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>Reply:</strong> <a href="12228.html">Peter Voss: "RE: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>Reply:</strong> <a href="12229.html">Eliezer S. Yudkowsky: "Re: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>Reply:</strong> <a href="12230.html">Eliezer S. Yudkowsky: "Re: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>Reply:</strong> <a href="12231.html">Michael Wilson: "Re: The Relevance of Complex Systems"</a>
<li><strong>Reply:</strong> <a href="12241.html">Phil Goetz: "Re: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>Maybe reply:</strong> <a href="12242.html">rpwl@lightlink.com: "Re: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12227">[ date ]</a>
<a href="index.html#12227">[ thread ]</a>
<a href="subject.html#12227">[ subject ]</a>
<a href="author.html#12227">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer,
<br>
<p>I won't do a line-by-line on your post (copied below).
<br>
<p>Instead I want to address the background issue.  This is part of the 
<br>
essay I promised when I wrote the Retrenchment post, explaining in more 
<br>
detail the parts that were not clear before. There has been much 
<br>
confusion about what I meant by trying to advocate the relevance of 
<br>
Complex Systems Theory, but until now I have despaired of trying to give 
<br>
an exact description of what I meant, short of writing an entire book.
<br>
<p>But now I have to say something, because you wrote this:
<br>
<p><em>&gt; &quot;Shut up and learn&quot; is a plaint to which I am, in general, prepared to
</em><br>
<em>&gt; be sympathetic.  But you're not the only one with recommendations.
</em><br>
<em>&gt; Give me one example of a technical understanding, one useful for
</em><br>
<em>&gt; making better-than-random guesses about specific observable outcomes,
</em><br>
<em>&gt; which derives from chaos theory.  Phil Goetz, making stuff up at
</em><br>
<em>&gt; random, gave decent examples of what I'm looking for.  Make a good
</em><br>
<em>&gt; enough case, and I'll put chaos theory on the head of my menu.
</em><br>
<p>You called it &quot;chaos theory&quot;.
<br>
<p>Chaos theory is not Complex Systems.  They are related, but chaos theory 
<br>
has no relevance to AI.  I now have no idea what you thought I was 
<br>
talking about all along.
<br>
<p>So here, as briefly as I can, is what I meant by Complex Systems and
<br>
its relevance for AI.  And for anyone following the other current thread 
<br>
about tools for building an AGI, *this* is one of the reasons why all 
<br>
those attempts to hack an AGI without serious consideration to tool 
<br>
building are likely to end in tears.
<br>
<p>[I really hope that people meet me half way here:  I am trying to convey 
<br>
a lot of stuff very concisely, so I am opening myself up to the danger 
<br>
of yet another spate of line-by-line misconstruals.]
<br>
<p><p><p><p>*Complex Adaptive Systems*        (aka &quot;Complex Systems&quot;)
<br>
<p>If one builds systems that are composed of many (more or less identical)
<br>
elements, each of which is relatively simple, but able to do some
<br>
moderately interesting amount of computation (with messages being
<br>
exchanged between them, some influences coming in from outside and
<br>
adaptation going on), then one observes that such systems sometimes
<br>
exhibit interesting behaviors, as follows.
<br>
<p>Sometimes they evolve in chaotic ways.  In fact, *usually* they evolve
<br>
in chaotic ways.  Not interesting.
<br>
<p>Sometimes they head straight toward a latchup state after being switched
<br>
on, and stay there.  Not chaos, just boring.
<br>
<p>An interesting subset (those sometimes referred to as being &quot;on the edge
<br>
of chaos&quot;) can show very ordered behavior.  These are Complex Systems.
<br>
Capital &quot;C&quot;, notice, to distinguish them from &quot;complex&quot; in the sense of
<br>
merely complicated.
<br>
<p>What is interesting about these is that they often show global
<br>
regularities that do not appear to be derivable (using any form of
<br>
analytic mathematics) from the local rules that govern the unit
<br>
behaviors.  This is what a CAS (&quot;Complex Adaptive Systems&quot;) person would
<br>
refer to as &quot;emergent&quot; behaviors.  More than that, some of these global
<br>
regularities appear to be common to many types of CAS.  In other words,
<br>
you can build alll sorts of systems with enormously different local
<br>
rules and global architectures, and the same patterns of global behavior
<br>
seem to crop up time and again.
<br>
<p>What to conclude from this?
<br>
<p>First, that bit about &quot;do not appear to be derivable (using any form of
<br>
analytic mathematics)&quot; is something that a lot of people have thought
<br>
deeply about .... this is no mere statement of inability, but a profound
<br>
realization about what it means to do math.  Namely:  if you look at
<br>
Mathematics as a whole you can see that the space of soluble, analytic,
<br>
tractable problems is and always has been a pitiably small corner of the
<br>
space of all possible systems.  It is trivially easy to write down
<br>
equations (or systems, speaking more generally) that are completely
<br>
intractable.  The default assumption made by some people is that
<br>
Mathematics as a domain of inquiry is gradually pushing back the
<br>
frontiers and that in an infinite universe there may come a time when
<br>
all possible problems (equations/systems) become tractable (i.e.
<br>
analytically solvable) BUT there is a substantial body of thought,
<br>
especially post-Godel, that believes that those systems are not just
<br>
difficult to solve, but actually impossible.  When I talk about &quot;the
<br>
limitations of mathematics&quot; I mean precisely that point of view.
<br>
<p>All that the CAS people did was to come up with some fabulously
<br>
interesting types of regularity (the emergent properties of Complex
<br>
Adaptive Systems), and then point out that the tractability of the
<br>
problem of accounting for these regularities is way, way, way beyond
<br>
anything else.  They allude to a philosophical/methodological position
<br>
in the math community, not to mere &quot;difficulty&quot;.  Heck, if there are
<br>
nonlinear DEs that the math folks declare to be &quot;ridiculously hard and
<br>
probably impossible to solve&quot;, then what are these Complex Systems,
<br>
which are a gazillion times more complex?
<br>
<p>Take the regularities observed in one of the most trivial systems that
<br>
we can think about, Conway's Life.  Can we find a set of equations that
<br>
will generate the &quot;regular&quot; forms that emerge in that game?  All of the
<br>
regular forms, not just some.  We should plug in the algorithm that
<br>
defines the game, and out the other end should come descriptions of the
<br>
glider guns etc.  Maybe there are optimists who think this is possible.
<br>
&nbsp;&nbsp;There are many people, I submit, who consider this kind of solution to
<br>
be impossible.  The function that generates regularities given local
<br>
rules, in the Comway system, is *never* going to be found.  It does not
<br>
exist.
<br>
<p>What is the relevance for AI?
<br>
<p>When people try to cook up formalisms that are supposed to be the core
<br>
of an intelligence, they often refer to systems of interacting parts in
<br>
which they (the designers) think they know (a) what the parts look like
<br>
and (b) how the parts interact and (c) what the system architecture and
<br>
environmental input/output connection amounts to.  A CAS person looks at
<br>
these systems and says &quot;Wait, that's a recipe for Complexity&quot;.  And what
<br>
they mean is that the designer may *think* that a system can be built
<br>
with (e.g.) bayesian local rules etc., but until they actually build a
<br>
complete working version that grows up whilst interacting with a real
<br>
environment, it is by no means certain that what they will get globally
<br>
is what they thought they were going to get when they invented the local
<br>
aspects of the design.  In practice, it just never works that way.  The
<br>
connection between local and global is not usually very simple.
<br>
<p>So you may find that if a few well-structured pieces of knowledge are
<br>
set up in the AGI system by the programmer, the Bayesian-inspired local
<br>
mechanism can allow the system to hustle along quite comfortably for a
<br>
while .... until it gradually seizes up.  To bring in an analogy here,
<br>
the Complex Systems person would say this is like trying to tile a
<br>
gently curved noneuclidean space .... it looks euclidean on a local
<br>
scale, but it would be a mistake to think you can tile it with a
<br>
euclidean pattern.
<br>
<p>(This is a more general version of what was previously called the
<br>
Grounding Problem, of course).
<br>
<p>*So this is the lesson that the CAS folks are trying to bring to the
<br>
table.*  (1) They know that most of the time when someone puts together
<br>
a real system of interacting, adaptive units, there can be global
<br>
regularities that are not identical to the local mechanisms.  (2) They
<br>
see AGI people coming up with proposals regarding the mechanisms of
<br>
thought, but those ideas are inspired by certain aspects of what the
<br>
high-level behavior *ought* to be (e.g. Bayesian reasoning), and the AGI
<br>
people often talk as if it is obvious that these are also the underlying
<br>
local mechanisms...... but this jump from local to global is simply not
<br>
warranted!
<br>
<p>I want to conclude by quoting one extract from your message below that
<br>
sums up the whole argument:
<br>
<p>[Richard Loosemore wrote:]
<br>
<em>&gt;&gt; Like Behaviorists and Ptolemaic Astronomers, they mistake a
</em><br>
<em>&gt;&gt; formalism that approximately describes a system for the mechanism
</em><br>
<em>&gt;&gt; that is actually inside the system.  They can carry on like this
</em><br>
<em>&gt;&gt; for centuries, adding epicycles onto their models in order to
</em><br>
<em>&gt;&gt; refine them.  When Bayesian Inference does not seem to cut it,
</em><br>
<em>&gt;&gt; they assert that *in principle* a sufficiently complex Bayesian 
</em><br>
<em>&gt;&gt; Inference system really would be able to cut it ... but they are
</em><br>
<em>&gt;&gt; not able to understand that the &quot;in principle&quot; bit of their argument
</em><br>
<em>&gt;&gt; depends on subtleties that they don't think much about.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There are subtleties to real-world intelligence that don't
</em><br>
<em>&gt; appear in standard Bayesian decision theory (he said controversially),
</em><br>
<em>&gt; but Bayesian decision theory can describe a hell of a lot more than
</em><br>
<em>&gt; naive students think.  I bet that if you name three subtleties, 
</em><br>
<em>&gt; I can describe how Bayes plus expected utility plus Solomonoff
</em><br>
<em>&gt; (= AIXI) would do it given infinite computing power. 
</em><br>
<p>You make my point for me.  The Ptolemaic astronomers would have used
<br>
exactly the same argument that you do:  &quot;Name some subtle ways in which
<br>
the heavenly bodies do not move according to the standard set of
<br>
epicycles, and I can describe how an infinite number of epicycles would
<br>
do it....&quot;   Yes, yes yes!  But they were wrong, because the *real*
<br>
mechanism for planetary movement was not actually governed by epicycles,
<br>
it was governed by something completely different, and all the Ptolemaic
<br>
folks were barking up the wrong tree when though their system was in
<br>
principle capable of covering the data.
<br>
<p>I have not said exactly how to proceed from here on out (although I do
<br>
have many thoughts to share with people about how, given the above
<br>
situation, we should really try to do AI), because at the moment all I
<br>
am trying to establish is that there is a big, serious problem, coming
<br>
in from the Complex Systems community, that says that this Bayesian kind
<br>
of approach (along with many others) to building an AGI is based on
<br>
faith and wishful thinking.
<br>
<p>And a vital corollary to the above arguments about how to build an AGI
<br>
is the fact that _absolutely guaranteeing_ a Friendly AI is impossible
<br>
the way you are trying to do it.  If AGI systems that actually work are
<br>
Complex (and all the indications are that they are indeed Complex), then
<br>
guarantees are impossible.  It's a waste of time to look for absolute
<br>
guarantees.  (Other indications of Friendliness .... now that's a
<br>
different matter).
<br>
<p>These points are so crucial to the issues being discussed on this list,
<br>
that at the very least they need to be taken seriously, rather than
<br>
dismissed out of hand by people who are unbelievably scornful of the
<br>
Complex Systems community.  That was the reason that I originally sent
<br>
the &quot;Retrenchment&quot; post.
<br>
<p>If anyone understands what I am saying here, it would be good to hear
<br>
from you.
<br>
<p><p>Richard Loosemore
<br>
<p><p><p><p><p><p><p><p><p><p>Eliezer S. Yudkowsky wrote:
<br>
<em>&gt; Richard Loosemore wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Ahh!  Eric B. Baum.  You were impressed by him, huh?  So you must be 
</em><br>
<em>&gt;&gt; one of those people who were fooled when he tried to explain qualia as 
</em><br>
<em>&gt;&gt; a form of mechanism, calling this an answer to the &quot;hard problem&quot; [of 
</em><br>
<em>&gt;&gt; consciousness] and making all the people who defined the term &quot;hard 
</em><br>
<em>&gt;&gt; problem of consciousness&quot; piss themselves with laughter at his stupidity?
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Baum really looks pretty impressive if you don't read his actual words 
</em><br>
<em>&gt;&gt; too carefully, doesn't he?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Sure, I was pleasantly surprised by Baum.  Baum had at least one new 
</em><br>
<em>&gt; idea and said at least one sensible thing about it, a compliment I'd pay 
</em><br>
<em>&gt; also to Jeff Hawkins.  I don't expect anyone to get everything right.  I 
</em><br>
<em>&gt; try to credit people for getting a single thing right, or making 
</em><br>
<em>&gt; progress on a problem, as otherwise I'd never be able to look favorably 
</em><br>
<em>&gt; on anyone.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Did Baum's explanation of consciousness dissipate the mystery?  No, it 
</em><br>
<em>&gt; did not; everything Baum said was factually correct, but people confused 
</em><br>
<em>&gt; by the hard problem of consciousness would be just as confused after 
</em><br>
<em>&gt; hearing Baum's statements.  I agree that Baum failed to dissipate the 
</em><br>
<em>&gt; apparent mystery of Chalmers's hard problem.  Baum said some sensible 
</em><br>
<em>&gt; things about Occam's Razor, and introduced me to the notion of VC 
</em><br>
<em>&gt; dimension; VC dimension isn't important for itself, but it got me to 
</em><br>
<em>&gt; think about Occam's Razor in terms of the range of possibilities a 
</em><br>
<em>&gt; hypothesis class can account for, rather than the bits required to 
</em><br>
<em>&gt; describe an instance of a hypothesis class.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;&gt; COMMUNITY (H)
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; The students of an ancient art devised by Laplace, which is therefore 
</em><br>
<em>&gt;&gt;&gt; called Bayesian.  Probability theory, decision theory, information 
</em><br>
<em>&gt;&gt;&gt; theory, statistics; Kolmogorov and Solomonoff, Jaynes and Shannon.  
</em><br>
<em>&gt;&gt;&gt; The masters of this art can describe ignorance more precisely than 
</em><br>
<em>&gt;&gt;&gt; most folk can describe their knowledge, and if you don't realize 
</em><br>
<em>&gt;&gt;&gt; that's a pragmatically useful mathematics then you aren't in 
</em><br>
<em>&gt;&gt;&gt; community (H).  These are the people to whom &quot;intelligence&quot; is not a 
</em><br>
<em>&gt;&gt;&gt; sacred mystery... not to some of us, anyway.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Boy, you are so right there!  They don't think of intelligence as a 
</em><br>
<em>&gt;&gt; sacred mystery, they think it is so simple, it only involves Bayesian 
</em><br>
<em>&gt;&gt; Inference!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Not at all.  The enlightened use Bayesian inference and expected utility 
</em><br>
<em>&gt; maximization to measure the power of an intelligence.  Like the 
</em><br>
<em>&gt; difference between understanding how to measure aerodynamic lift, and 
</em><br>
<em>&gt; knowing how to build an airplane.  If you don't know how to measure 
</em><br>
<em>&gt; aerodynamic lift, good luck building an airplane.  Knowing how to 
</em><br>
<em>&gt; measure success isn't enough to succeed, but it sure helps.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; Like Behaviorists and Ptolemaic Astronomers, they mistake a formalism 
</em><br>
<em>&gt;&gt; that approximately describes a system for the mechanism that is 
</em><br>
<em>&gt;&gt; actually inside the system.  They can carry on like this for 
</em><br>
<em>&gt;&gt; centuries, adding epicycles onto their models in order to refine 
</em><br>
<em>&gt;&gt; them.  When Bayesian Inference does not seem to cut it, they assert 
</em><br>
<em>&gt;&gt; that *in principle* a sufficiently complex Bayesian Inference system 
</em><br>
<em>&gt;&gt; really would be able to cut it ... but they are not able to understand 
</em><br>
<em>&gt;&gt; that the &quot;in principle&quot; bit of their argument depends on subtleties 
</em><br>
<em>&gt;&gt; that they don't think much about.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There are subtleties to real-world intelligence that don't appear in 
</em><br>
<em>&gt; standard Bayesian decision theory (he said controversially), but 
</em><br>
<em>&gt; Bayesian decision theory can describe a hell of a lot more than naive 
</em><br>
<em>&gt; students think.  I bet that if you name three subtleties, I can describe 
</em><br>
<em>&gt; how Bayes plus expected utility plus Solomonoff (= AIXI) would do it 
</em><br>
<em>&gt; given infinite computing power.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; In particular, they don't notice when the mechanism that is supposed 
</em><br>
<em>&gt;&gt; to do the mapping between internal symbols and external referents, in 
</em><br>
<em>&gt;&gt; their kind of system, turns out to require more intelligence than the 
</em><br>
<em>&gt;&gt; reasoning engine itself .... and they usually don't notice this 
</em><br>
<em>&gt;&gt; because they write all their programs with programmer-defined 
</em><br>
<em>&gt;&gt; symbols/concepts (implictly inserting the intelligence themselves, you 
</em><br>
<em>&gt;&gt; see), thus sparing their system the pain of doing the work necessary 
</em><br>
<em>&gt;&gt; to ground itself.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Historically true.  I remember reading Jaynes and snorting mentally to 
</em><br>
<em>&gt; myself as Jaynes described a &quot;robot&quot; which came complete with 
</em><br>
<em>&gt; preformulated hypotheses.  But it's not as if Jaynes was trying to build 
</em><br>
<em>&gt; a Friendly AI.  I don't expect Jaynes to know that stuff, just to get 
</em><br>
<em>&gt; his probability theory right.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I note that the mechanism that maps from internal symbols to external 
</em><br>
<em>&gt; referents is readily understandable in Bayesian terms.  AIXI can learn 
</em><br>
<em>&gt; to walk across a room.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I also note that I have written extensively about this very problem in 
</em><br>
<em>&gt; &quot;Levels of Organization in General Intelligence&quot;.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; If these people understood what was going on in the other communities, 
</em><br>
<em>&gt;&gt; they might understand these issues.  Typically, they don't.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Again, historically true.  Jaynes was a physicist before he was a 
</em><br>
<em>&gt; Bayesian, but I've no reason to believe he ever studied, say, visual 
</em><br>
<em>&gt; neurology.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So far as I've heard, in modern-day science, individuals are polymaths, 
</em><br>
<em>&gt; not communities.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If I despised the communities for that, I couldn't assemble the puzzle 
</em><br>
<em>&gt; pieces from each isolated community.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; Here you remind me of John Searle, famous Bete Noir of the AI 
</em><br>
<em>&gt;&gt; community, who will probably never understand the &quot;levels&quot; difference 
</em><br>
<em>&gt;&gt; between systems that *are* intelligent (e.g. humans) and systems that 
</em><br>
<em>&gt;&gt; are collections of interacting intelligences (e.g. human societies) 
</em><br>
<em>&gt;&gt; and, jumping up almost but not quite a whole level again, systems that 
</em><br>
<em>&gt;&gt; are interacting species of &quot;intelligences&quot; (evolution).
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; This is one of the silliest mistakes that a person interested in AI 
</em><br>
<em>&gt;&gt; could make.   You know Conway's game of Life?  I've got a dinky little 
</em><br>
<em>&gt;&gt; simulation here on this machine that will show me a whole zoo of 
</em><br>
<em>&gt;&gt; gliders and loaves and glider guns and traffic lights and whatnot.  
</em><br>
<em>&gt;&gt; Demanding that an AI person should study &quot;evolutionary biology with 
</em><br>
<em>&gt;&gt; math&quot; is about as stupid as demanding that someone interested in the 
</em><br>
<em>&gt;&gt; structure of computers should study every last detail of the gliders, 
</em><br>
<em>&gt;&gt; loaves, glider guns and traffic lights, etc. in Conway's Life.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Level of description fallacy.  Searle fell for it when he invented his 
</em><br>
<em>&gt;&gt; ridiculous Chinese Room.  Why would yo make the same dumb mistake?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Ho, ho, ho!  Let me look up the appropriate response in my handbook... 
</em><br>
<em>&gt; ah, yes, here it is:  &quot;You speak from a profound lack of depth in the 
</em><br>
<em>&gt; one field where depth of understanding is most important.&quot;  Evolutionary 
</em><br>
<em>&gt; biology with math isn't 'most important', but it sure as hell is important.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; To try to understand human intelligence without understanding natural 
</em><br>
<em>&gt; selection is hopeless.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; To try to understand optimization, you should study more than one kind 
</em><br>
<em>&gt; of powerful optimization process.  Natural selection is one powerful 
</em><br>
<em>&gt; optimization process.  Human intelligence is another.  Until you have 
</em><br>
<em>&gt; studied both, you have no appreciation of how different two optimization 
</em><br>
<em>&gt; processes can be.  &quot;A barbarian is one who thinks the customs of his 
</em><br>
<em>&gt; island and tribe are the laws of nature.&quot;  To cast off the human island, 
</em><br>
<em>&gt; you study evolutionary biology with math.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I know about separate levels of description.  I'm not telling you that 
</em><br>
<em>&gt; ev-bio+math is how intelligence works.  I'm telling you to study 
</em><br>
<em>&gt; ev-bio+math anyway, because it will help you understand human 
</em><br>
<em>&gt; intelligence and general optimization.  After you have studied, you will 
</em><br>
<em>&gt; understand why you needed to study.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I should note that despite my skepticism, I'm quite open to the 
</em><br>
<em>&gt; possibility that if I study complexity theory, I will afterward slap 
</em><br>
<em>&gt; myself on the forehead and say, &quot;I can't believe I tried to do this 
</em><br>
<em>&gt; without studying complexity theory.&quot;  The question I deal with is 
</em><br>
<em>&gt; deciding where to spend limited study time - otherwise I'd study it all.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; I did read the documents.  I knew about Bayes Theorem already.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Good for you.  You do realize that Bayesian probability theory 
</em><br>
<em>&gt; encompasses a lot more territory than Bayes's Theorem?
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; A lot of sound and fury, signifying nothing.  You have no real 
</em><br>
<em>&gt;&gt; conception of the limitations of mathematics, do you?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yeah, right, a 21st-century human is going to know the &quot;limitations&quot; of 
</em><br>
<em>&gt; mathematics.  After that, he'll tell me the limitations of science, 
</em><br>
<em>&gt; rationality, skepticism, observation, and reason.  Because if he doesn't 
</em><br>
<em>&gt; see how to do something with mathematics, it can't be done.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; You don't seem to understand that the forces that shape thought and 
</em><br>
<em>&gt;&gt; the forces that shape our evaluation of technical theories (each 
</em><br>
<em>&gt;&gt; possibly separate forces, though related) might not be governed by 
</em><br>
<em>&gt;&gt; your post-hoc bayesian analysis of them.  That entire concept of the 
</em><br>
<em>&gt;&gt; separation between an approximate description of a process and the 
</em><br>
<em>&gt;&gt; mechanisms that actually *is* the process, is completely lost on you.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I understand quite well the difference between an approximation and an 
</em><br>
<em>&gt; ideal, or the difference between a design goal and a design.  I won't 
</em><br>
<em>&gt; say it's completely futile to try to do without knowing what you're 
</em><br>
<em>&gt; doing, because some technology does get built that way.  But my concern 
</em><br>
<em>&gt; is Friendly AI, not AI, so I utterly abjured and renounced my old ideas 
</em><br>
<em>&gt; of blind exploration.  From now on, I said to myself, I understand 
</em><br>
<em>&gt; exactly what I'm doing *before* I do it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; As I said at the outset, this is foolishness.  I have read &quot;Judgment 
</em><br>
<em>&gt;&gt; Under Uncertainty&quot;, and Lakoff's &quot;Women, Fire and Dangerous Things&quot; 
</em><br>
<em>&gt;&gt; ... sitting there on the shelf behind me.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Okay, you passed a couple of spot-checks, you're not a complete waste of 
</em><br>
<em>&gt; time.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Though you still seem unclear on the realization that polymaths all 
</em><br>
<em>&gt; study *different* fields, so there's nothing impressive about being able 
</em><br>
<em>&gt; to name different communities.  Anyone can rattle off the names of some 
</em><br>
<em>&gt; obscure books they've read.  It's being able to answer at least some of 
</em><br>
<em>&gt; the time when someone else picks the question, that implies you're 
</em><br>
<em>&gt; getting at least a little real coverage.  You seem to have some myopia 
</em><br>
<em>&gt; with respect to this, asking me why I was telling you to study 
</em><br>
<em>&gt; additional fields when I hadn't even studied every single one you'd 
</em><br>
<em>&gt; already studied.  Some roads go on a long, long way.  That *you* can 
</em><br>
<em>&gt; name things you've studied isn't impressive.  Having Lakoff on the shelf 
</em><br>
<em>&gt; behind you does imply you're not a total n00bie, not because Lakoff is 
</em><br>
<em>&gt; so important, but because I selected the question instead of you.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;&gt; Also known as Mainstream AI: the predicate logic users, 
</em><br>
<em>&gt;&gt;&gt; connectionists, and artificial evolutionists.  What they know about 
</em><br>
<em>&gt;&gt;&gt; goal hierarchies and planning systems is quite different from what 
</em><br>
<em>&gt;&gt;&gt; decision theorists know about expected utility maximization, though 
</em><br>
<em>&gt;&gt;&gt; of course there's some overlap.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; I was conjoining the decision theorists with the hard AI group, since 
</em><br>
<em>&gt;&gt; most of the AI people I know are perfectly familiar with the latter.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Non sequitur.  AI people may know some decision theory, it doesn't mean 
</em><br>
<em>&gt; that decision theory is identical to AI.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I would guess that not many AI people can spot-read the difference between:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; p(B|A)
</em><br>
<em>&gt; p(A []-&gt; B)
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;&gt;&gt; COMMUNITY (C)
</em><br>
<em>&gt;&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;&gt; Those to whom the term &quot;edge of chaos&quot; is not just something they 
</em><br>
<em>&gt;&gt;&gt;&gt; learned from James Gleick.  These people are comfortable with the 
</em><br>
<em>&gt;&gt;&gt;&gt; idea that mathematics is a fringe activity that goes on at the 
</em><br>
<em>&gt;&gt;&gt;&gt; tractable edge of a vast abyss of completely intractable systems and 
</em><br>
<em>&gt;&gt;&gt;&gt; equations.  When they use the term &quot;non-linear&quot; they don't mean 
</em><br>
<em>&gt;&gt;&gt;&gt; something that is not a straight line, nor are they talking about 
</em><br>
<em>&gt;&gt;&gt;&gt; finding tricks that yield analytic solutions to certain nonlinear 
</em><br>
<em>&gt;&gt;&gt;&gt; equations.  They are equally comfortable talking about a national 
</em><br>
<em>&gt;&gt;&gt;&gt; economy and a brain as a &quot;CAS&quot; and they can point to meaningful 
</em><br>
<em>&gt;&gt;&gt;&gt; similarities in the behavior of these two sorts of system.  Almost 
</em><br>
<em>&gt;&gt;&gt;&gt; all of these people are seriously well versed in mathematics, but 
</em><br>
<em>&gt;&gt;&gt;&gt; unlike the main body of mathematicians proper, they understand the 
</em><br>
<em>&gt;&gt;&gt;&gt; limitations of analytic attempts to characterize systems in the real 
</em><br>
<em>&gt;&gt;&gt;&gt; world.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; I'm not part of community C and I maintain an extreme skepticism of 
</em><br>
<em>&gt;&gt;&gt; its popular philosophy, as opposed to particular successful technical 
</em><br>
<em>&gt;&gt;&gt; applications, for reasons given in &quot;A Technical Explanation of 
</em><br>
<em>&gt;&gt;&gt; Technical Explanation&quot;.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; You speak from a profound lack of depth in the one field where depth 
</em><br>
<em>&gt;&gt; of understanding is most important.  You mistake &quot;particular 
</em><br>
<em>&gt;&gt; successful technical applications&quot; for the issues of most importance 
</em><br>
<em>&gt;&gt; to AI.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; There is nothing wrong with skepticism.  If it is based on 
</em><br>
<em>&gt;&gt; understanding, rather than wilful, studied ignorance.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &quot;Shut up and learn&quot; is a plaint to which I am, in general, prepared to 
</em><br>
<em>&gt; be sympathetic.  But you're not the only one with recommendations.  Give 
</em><br>
<em>&gt; me one example of a technical understanding, one useful for making 
</em><br>
<em>&gt; better-than-random guesses about specific observable outcomes, which 
</em><br>
<em>&gt; derives from chaos theory.  Phil Goetz, making stuff up at random, gave 
</em><br>
<em>&gt; decent examples of what I'm looking for.  Make a good enough case, and 
</em><br>
<em>&gt; I'll put chaos theory on the head of my menu.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Many people are easily fooled into thinking they have attained some 
</em><br>
<em>&gt; tremendously important and significant understanding of something they 
</em><br>
<em>&gt; are still giving maxentropy probability distributions about, the qualia 
</em><br>
<em>&gt; crowd being one obvious example.  Show me this isn't so of chaos theory.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;&gt;&gt; Those who could give you a reasonable account of where Penrose, 
</em><br>
<em>&gt;&gt;&gt;&gt; Chalmers and Dennett would stand with respect to one another.  They 
</em><br>
<em>&gt;&gt;&gt;&gt; could easily distinguish the Hard Problem from other versions of the 
</em><br>
<em>&gt;&gt;&gt;&gt; consciousness issue, even if they might disagree with Chalmers about 
</em><br>
<em>&gt;&gt;&gt;&gt; the conclusion to be drawn.  They know roughly what supervenience 
</em><br>
<em>&gt;&gt;&gt;&gt; is.  The could certainly distinguish functionalism (various breeds 
</em><br>
<em>&gt;&gt;&gt;&gt; thereof) from epiphenomenalism and physicalism, and they could talk 
</em><br>
<em>&gt;&gt;&gt;&gt; about what various camps thought about the issues of dancing, 
</em><br>
<em>&gt;&gt;&gt;&gt; inverted and absent qualia.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; Sadly I recognize every word and phrase in this paragraph, legacy of 
</em><br>
<em>&gt;&gt;&gt; a wasted childhood, like being able to sing the theme song from 
</em><br>
<em>&gt;&gt;&gt; Thundercats.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Shame.  There is valuable stuff buried in among the dross.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If you've read _Technical Explanation_, you know my objection. 
</em><br>
<em>&gt; Mysterious answers to mysterious questions.  &quot;Qualia&quot; reifies the 
</em><br>
<em>&gt; confusion into a substance, as did &quot;phlogiston&quot; and &quot;elan vital&quot;.
</em><br>
<em>&gt; 
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12228.html">Peter Voss: "RE: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>Previous message:</strong> <a href="12226.html">rpwl@lightlink.com: "Re: Introduction"</a>
<li><strong>In reply to:</strong> <a href="12201.html">Eliezer S. Yudkowsky: "Re: Retrenchment"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12228.html">Peter Voss: "RE: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>Reply:</strong> <a href="12228.html">Peter Voss: "RE: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>Reply:</strong> <a href="12229.html">Eliezer S. Yudkowsky: "Re: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>Reply:</strong> <a href="12230.html">Eliezer S. Yudkowsky: "Re: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>Reply:</strong> <a href="12231.html">Michael Wilson: "Re: The Relevance of Complex Systems"</a>
<li><strong>Reply:</strong> <a href="12241.html">Phil Goetz: "Re: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<li><strong>Maybe reply:</strong> <a href="12242.html">rpwl@lightlink.com: "Re: The Relevance of Complex Systems [was: Re: Retrenchment]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12227">[ date ]</a>
<a href="index.html#12227">[ thread ]</a>
<a href="subject.html#12227">[ subject ]</a>
<a href="author.html#12227">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
