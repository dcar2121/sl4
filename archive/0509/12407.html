<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Is complex emergence necessary for AGI?</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Re: Is complex emergence necessary for AGI?">
<meta name="Date" content="2005-09-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Is complex emergence necessary for AGI?</h1>
<!-- received="Tue Sep 20 13:03:16 2005" -->
<!-- isoreceived="20050920190316" -->
<!-- sent="Tue, 20 Sep 2005 15:02:20 -0400" -->
<!-- isosent="20050920190220" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Re: Is complex emergence necessary for AGI?" -->
<!-- id="43305CBC.3090800@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="20050920100811.49403.qmail@web26701.mail.ukl.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Is%20complex%20emergence%20necessary%20for%20AGI?"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Tue Sep 20 2005 - 13:02:20 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12408.html">Richard Loosemore: "Re: Is complex emergence necessary for intelligence under limited resources?"</a>
<li><strong>Previous message:</strong> <a href="12406.html">Eliezer S. Yudkowsky: "Re: Is complex emergence necessary for AGI?"</a>
<li><strong>In reply to:</strong> <a href="12403.html">Michael Wilson: "RE: Is complex emergence necessary for AGI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12408.html">Richard Loosemore: "Re: Is complex emergence necessary for intelligence under limited resources?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12407">[ date ]</a>
<a href="index.html#12407">[ thread ]</a>
<a href="subject.html#12407">[ subject ]</a>
<a href="author.html#12407">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I am not going to reply to any more of this stuff, because almost 
<br>
everything said here about Complex Systems is based in a complete 
<br>
misunderstanding of what Complex Systems actually are.
<br>
<p><p><p>Michael Wilson wrote:
<br>
<em>&gt; Ben Goertzel wrote: 
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;An unpredictable emergent phenomenon in a system is a behavior in a whole
</em><br>
<em>&gt;&gt;system that we know can in principle be predicted from the behavior of the
</em><br>
<em>&gt;&gt;parts of the system -- but carrying out this prediction in practice is
</em><br>
<em>&gt;&gt;extremely computationally difficult.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'm not going to continue criticising the 'we must use Complexity theory'
</em><br>
<em>&gt; position, as I think that debate is past the point of diminishing returns.
</em><br>
<em>&gt; However given the potential for confusion in the extended back-and-forth
</em><br>
<em>&gt; I think I should clarify my (and to a lesser extent, the SIAI's) position.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1. The requirement for a certain amount of strong predictability comes from
</em><br>
<em>&gt; the need for Friendliness, analysis that suggests that unless you strongly
</em><br>
<em>&gt; constrain goal system evolution it will be highly unpredictable, and the
</em><br>
<em>&gt; simple fact that when humans are confident something will work, without
</em><br>
<em>&gt; having a technical argument for why it will work, we're usually wrong.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 2. Thus the SIAI has the design requirement; goal system trajectory must
</em><br>
<em>&gt; reliably stay within certain bounds, which is to say that the optimisation
</em><br>
<em>&gt; targets of the overall optimisation process must not drift out of a
</em><br>
<em>&gt; certain region. This is a very specific and limited kind of predictability;
</em><br>
<em>&gt; we don't need specific AI behaviour or cognitive content. I agree that the
</em><br>
<em>&gt; task would be impossible if one were trying to predict much more than just
</em><br>
<em>&gt; the optimisation targets. I am happy to have all kinds of emergence and
</em><br>
<em>&gt; Complexity occuring as long as they stay within the overall constraints,
</em><br>
<em>&gt; though theory and limited experimental experience suggests to me that there
</em><br>
<em>&gt; will be a lot less of this than most people would expect.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 3. If that turns out to be impossible, then we'd agree that AGI development
</em><br>
<em>&gt; should just go ahead using the best probabilistic methods available (maybe;
</em><br>
<em>&gt; it might make sense to develop IA first in that case). But we shouldn't
</em><br>
<em>&gt; write something this important off as impossible without trying really
</em><br>
<em>&gt; hard first, and I think that many people are far too quick to dismiss this
</em><br>
<em>&gt; so that they can get on with the 'fun stuff' i.e. actual AGI design.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 4. Various researchers including Eliezer have spent a fair amount of time
</em><br>
<em>&gt; on this, and so far it looks probable that it is possible given arbitrary
</em><br>
<em>&gt; AGI design that have access to unbounded computing power. The critical
</em><br>
<em>&gt; question is whether there is a tractable design for an AGI that satisfies
</em><br>
<em>&gt; the structural requirements of these theories. This is something that I'm
</em><br>
<em>&gt; working on; unfortunately I'm not aware of anyone else working on it at
</em><br>
<em>&gt; present, though I certainly wish there was.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 5. Any system compatible with the known approaches to strong verification
</em><br>
<em>&gt; of Friendliness will need to be consistently rational, which is to say
</em><br>
<em>&gt; Bayesian from the ground up and have the structural property of being
</em><br>
<em>&gt; 'causally clean', although not necessarily driven by expected utility.
</em><br>
<em>&gt; When I first accepted these constraints, they seemed onerous to the point
</em><br>
<em>&gt; of making a tractabale architecture impossible; all the 'powerful'
</em><br>
<em>&gt; techniques I knew of (improved GAs, stochastic codelets, dynamic-toplogy
</em><br>
<em>&gt; NNs, agent systems etc) were thoroughly probabilistic* and hence difficult
</em><br>
<em>&gt; to use or completely unusable. But after a period of research I now
</em><br>
<em>&gt; believe that there are acceptable and even superior replacements for all
</em><br>
<em>&gt; of these that are compatible with strong verification of Friendliness.
</em><br>
<em>&gt; I'm not going to defend that as anything more than a personal opinion at
</em><br>
<em>&gt; this time.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; * Annoying terminology conflict; 'probabilistic methods' are not the same
</em><br>
<em>&gt; thing as 'probabilistic logic'. The former are problem-solving techniques
</em><br>
<em>&gt; that don't reliably obey constraints and/or fail to show a reliable
</em><br>
<em>&gt; minimum performance in relation to normative decision theory; an analogy
</em><br>
<em>&gt; could be drawn to 'soft real time' instead of 'hard real time'. This is
</em><br>
<em>&gt; why saying 'Bayesian logic' to mean 'probabilistic logic' is not too bad
</em><br>
<em>&gt; an idea even if it causes people to fixate on one particular derrivation.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 6. Basically, a rational system of this kind avoids unwanted interactions
</em><br>
<em>&gt; that would violate top-down constraints by constraining the way in which
</em><br>
<em>&gt; components can inteact as you string them together. The resulting
</em><br>
<em>&gt; structure could reasonably be called fractal; combining any set of
</em><br>
<em>&gt; rational components in a rational framework produces a combined system
</em><br>
<em>&gt; that is still rational. Yes, I mean something specific and moderately
</em><br>
<em>&gt; complicated by 'rational' which I don't have space to fully describe.
</em><br>
<em>&gt; Yes, doing this without sacraficing tractability is hard, but at present
</em><br>
<em>&gt; I am optimistic that it will not turn out to be impossible. Yes, I am
</em><br>
<em>&gt; working on a practical experiment/demonstration, this will take some
</em><br>
<em>&gt; time, and I wish I had more resources to do it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 7. Note that this introduces the notion of 'kinds of Complexity'; a
</em><br>
<em>&gt; system of this kind would be 'Complex' in some respects and non-Complex
</em><br>
<em>&gt; in others. There are plenty of existing technological systems that
</em><br>
<em>&gt; already look like this, so I see no reason to object to it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 8. Neither I nor the SIAI have claimed that this is the only way to build
</em><br>
<em>&gt; AGI; in fact if it was we'd sleep a lot safer at night. Unfortunately it
</em><br>
<em>&gt; seems entirely possible to build an AI using 'emergence', given enough
</em><br>
<em>&gt; brute force, neuroscience and/or luck. The SIAI's claim is that this is
</em><br>
<em>&gt; a /really bad idea/, because the result is highly likely to be iminical
</em><br>
<em>&gt; to human goals and morals. The claims that any transhuman intelligence
</em><br>
<em>&gt; will renormalise to a rational basis, and that this is actually a better
</em><br>
<em>&gt; way to develop AGI regardless of Friendliness concerns, are weaker ones
</em><br>
<em>&gt; and again stand only as opinion in public at this time.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 9. No-one associated with the SIAI denies that the brain is an example
</em><br>
<em>&gt; of a 'Complex system', or that emergence as a concept won't be useful
</em><br>
<em>&gt; for studying it. We do claim that it is a horrible mess from and that it
</em><br>
<em>&gt; isn't terribly relevant to the task of building an AGI compatible with
</em><br>
<em>&gt; strong Friendliness verification. The position that closely mimicking
</em><br>
<em>&gt; the brain isn't a good way to build AGI regardless of Friendliness is
</em><br>
<em>&gt; again opinion, but the position that an AGI built in this fashion will
</em><br>
<em>&gt; probably be Unfriendly is strongly justified from the previously
</em><br>
<em>&gt; mentioned arguments.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 10. The issue of 'Friendliness content' is genuinely seperate from
</em><br>
<em>&gt; 'Friendliness structure' and hence 'strong Friendliness verification'.
</em><br>
<em>&gt; The latter is perhaps a misnomer, as there is some theory that is
</em><br>
<em>&gt; applicable to any attempt to verify that an RPOP will do something
</em><br>
<em>&gt; specific, though it is true that there are some things we would probably
</em><br>
<em>&gt; want an FAI to do that require additional theory to describe and verify.
</em><br>
<em>&gt; Arguments about whether CV, or 'joy, choice and growth', or domain
</em><br>
<em>&gt; protection, or hedonism or Sysops or anything similar are a good idea
</em><br>
<em>&gt; are debates about Friendliness content. This is important, but it's
</em><br>
<em>&gt; well seperated from issues of structural verification and tractable
</em><br>
<em>&gt; implementation, and different in character (because it involves what
</em><br>
<em>&gt; we want instead of how to do it).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 11. Personally I am quite skeptical of Eliezer's ideas about
</em><br>
<em>&gt; Friendliness content, but I support his very important and (as far as
</em><br>
<em>&gt; I can see) valid work on structural verification. I do wish he'd
</em><br>
<em>&gt; publish more, but that criticism can be levelled against most people
</em><br>
<em>&gt; working on AGI, including me. It's true that neither Eliezer nor the
</em><br>
<em>&gt; SIAI has done much work on tractability, which is the main reason
</em><br>
<em>&gt; why I'm working on it. However I agree that the question of how to
</em><br>
<em>&gt; build something in the real world should follow that of how to build
</em><br>
<em>&gt; it in principle, and that people need to be convinced about the
</em><br>
<em>&gt; desireability and theoretical possibility of structural verification
</em><br>
<em>&gt; (of AGIs as general optimisers) before it makes sense to argue about
</em><br>
<em>&gt; if we can do it with real software on contemporary hardware.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 12. Finally, my objection to claims about the value of Complexity theory
</em><br>
<em>&gt; were summed up by one critic's comment that &quot;Wolfram's 'A New Kind of
</em><br>
<em>&gt; Science' would have been fine if it had been called 'Fun With Graph
</em><br>
<em>&gt; Paper'&quot;. The field has produced a vast amount of hype, a small amount
</em><br>
<em>&gt; of interesting maths and very few useful predictive theories in other
</em><br>
<em>&gt; domains. Its proponents are quick to claim that their ideas apply to
</em><br>
<em>&gt; virtually everything, when in practice they seem to have been actually
</em><br>
<em>&gt; useful in rather few cases. This opinion is based on coverage in the
</em><br>
<em>&gt; science press and would be easy to change via evidence, but to date
</em><br>
<em>&gt; no-one has responded to Eliezer's challenge with real examples of
</em><br>
<em>&gt; complexity theory doing something useful. That said, general opinions
</em><br>
<em>&gt; such as this are a side issue; the specifics of AGI are the important
</em><br>
<em>&gt; part.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;It may be that intelligence given limited resources intrinsically
</em><br>
<em>&gt;&gt;requires stochastic algorithms, but that is a whole other issue.
</em><br>
<em>&gt;&gt;Stochastic algorithms are not all that closely related to emergent
</em><br>
<em>&gt;&gt;phenomena -- one can get both emergence and non-emergence from both
</em><br>
<em>&gt;&gt;stochastic and non-stochastic algorithms.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I agree, but in practice it does seem that stochastic systems are more
</em><br>
<em>&gt; likely to show/use emergence and vice versa.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; That said, I really must stop spending so much time writing emails.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;  * Michael Wilson
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 		
</em><br>
<em>&gt; ___________________________________________________________ 
</em><br>
<em>&gt; To help you stay safe and secure online, we've developed the all new Yahoo! Security Centre. <a href="http://uk.security.yahoo.com">http://uk.security.yahoo.com</a>
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12408.html">Richard Loosemore: "Re: Is complex emergence necessary for intelligence under limited resources?"</a>
<li><strong>Previous message:</strong> <a href="12406.html">Eliezer S. Yudkowsky: "Re: Is complex emergence necessary for AGI?"</a>
<li><strong>In reply to:</strong> <a href="12403.html">Michael Wilson: "RE: Is complex emergence necessary for AGI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12408.html">Richard Loosemore: "Re: Is complex emergence necessary for intelligence under limited resources?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12407">[ date ]</a>
<a href="index.html#12407">[ thread ]</a>
<a href="subject.html#12407">[ subject ]</a>
<a href="author.html#12407">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
