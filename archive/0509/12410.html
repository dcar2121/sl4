<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Is complex emergence necessary for intelligence under limited resources?</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Is complex emergence necessary for intelligence under limited resources?">
<meta name="Date" content="2005-09-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Is complex emergence necessary for intelligence under limited resources?</h1>
<!-- received="Tue Sep 20 13:30:15 2005" -->
<!-- isoreceived="20050920193015" -->
<!-- sent="Tue, 20 Sep 2005 15:30:13 -0400" -->
<!-- isosent="20050920193013" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Is complex emergence necessary for intelligence under limited resources?" -->
<!-- id="JNEIJCJJHIEAILJBFHILMEPDGDAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="43305D7B.1080806@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Is%20complex%20emergence%20necessary%20for%20intelligence%20under%20limited%20resources?"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Tue Sep 20 2005 - 13:30:13 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12411.html">Ben Goertzel: "RE: Is complex emergence necessary for AGI?"</a>
<li><strong>Previous message:</strong> <a href="12409.html">Ben Goertzel: "RE: Is complex emergence necessary for AGI?"</a>
<li><strong>In reply to:</strong> <a href="12408.html">Richard Loosemore: "Re: Is complex emergence necessary for intelligence under limited resources?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12415.html">Richard Loosemore: "Re: Is complex emergence necessary for intelligence under limited resources?"</a>
<li><strong>Reply:</strong> <a href="12415.html">Richard Loosemore: "Re: Is complex emergence necessary for intelligence under limited resources?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12410">[ date ]</a>
<a href="index.html#12410">[ thread ]</a>
<a href="subject.html#12410">[ subject ]</a>
<a href="author.html#12410">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Richard,
<br>
<p>I think you have made a reasonably convincing argument why AGI should
<br>
proceed via allowing a proto-AGI system to engage with a world via sensors
<br>
and actuators, and construct its own symbols to represent the world around
<br>
it.  I agree that these symbols will generally not be simple, and also that
<br>
sophisticated learning mechanisms will be required to learn them.
<br>
<p>What is not clear to me is why the &quot;complexity&quot; of these symbols and
<br>
learning  mechanisms necessarily has to entail &quot;complexity&quot; in the sense of
<br>
complex systems theory (as  opposed to just &quot;complexity&quot; in the sense of
<br>
complicatedness and sophistication).  Clearly, it does in the human brain.
<br>
But you haven't demonstrated that it does in an AGI system, nor have you
<br>
really given any arguments in this direction.
<br>
<p>Personally I suspect you are probably right and that given limited resources
<br>
complex-systems-style &quot;complexity&quot; probably IS necessary for effective
<br>
symbol grounding.  But I don't have a strong argument in this direction,
<br>
unfortunately, just an intuition.
<br>
<p>-- Ben
<br>
<p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20Is%20complex%20emergence%20necessary%20for%20intelligence%20under%20limited%20resources?">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20Is%20complex%20emergence%20necessary%20for%20intelligence%20under%20limited%20resources?">owner-sl4@sl4.org</a>]On Behalf Of Richard
</em><br>
<em>&gt; Loosemore
</em><br>
<em>&gt; Sent: Tuesday, September 20, 2005 3:06 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=RE:%20Is%20complex%20emergence%20necessary%20for%20intelligence%20under%20limited%20resources?">sl4@sl4.org</a>
</em><br>
<em>&gt; Subject: Re: Is complex emergence necessary for intelligence under
</em><br>
<em>&gt; limited resources?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ben Goertzel wrote:
</em><br>
<em>&gt;  &gt; Richard,
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt;&gt;The answers that others offer to your questions are, pretty much:  no
</em><br>
<em>&gt;  &gt;&gt;you cannot really avoid complex systems, and mathematical verification
</em><br>
<em>&gt;  &gt;&gt;of their friendliness is the very last thing you would be able to do.
</em><br>
<em>&gt;  &gt;&gt;The main defining characteristic of complex systems is that such
</em><br>
<em>&gt;  &gt;&gt;mathematical verification is out of reach.
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; There is no question that human mind/brain is a complex system which
</em><br>
<em>&gt;  &gt; achieves its intelligence via emergence, self-organization, strange
</em><br>
<em>&gt;  &gt; attractors, terminal attractors, and all that great stuff....
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; And there is little question that emergence-based intelligence is
</em><br>
<em>&gt;  &gt; intrinsically very difficult to predict and control with a high
</em><br>
<em>&gt;  &gt; degree of reliability, thus rendering verified Friendliness an
</em><br>
<em>&gt;  &gt; unlikely outcome.
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; However, these observations don't tell you much about whether it's
</em><br>
<em>&gt;  &gt; possible to use digital computers to create an intelligence that
</em><br>
<em>&gt;  &gt; DOESN'T rely critically on the emergent phenomena typically
</em><br>
<em>&gt;  &gt; associated with biological complex dynamical systems.
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; Semi-similarly, the human mind/brain probably uses complex emergent
</em><br>
<em>&gt;  &gt; phenomena to add 2+2 and get 4, but, a calculator doesn't, and a
</em><br>
<em>&gt;  &gt; calculator does a better job of arithmetic anyway.
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; One may argue that flexible, creative intelligence is fundamentally
</em><br>
<em>&gt;  &gt; different than arithmetic, and is not achievable within limited
</em><br>
<em>&gt;  &gt; computational resources except via complex, unpredictable emergent
</em><br>
<em>&gt;  &gt; dynamics.  In fact I strongly SUSPECT this is true, but I haven't SHOWN
</em><br>
<em>&gt;  &gt; that it's true in a convincing way, and I'm not 100% convinced
</em><br>
<em>&gt;  &gt; it's true.
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; If you have a strong argument why this contention is true, I'd be
</em><br>
<em>&gt;  &gt; very eager to hear it.
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; On the other hand, some others seems pretty sure that the opposite is
</em><br>
<em>&gt; true,
</em><br>
<em>&gt;  &gt; and that it IS possible to achieve powerful intelligence under
</em><br>
<em>&gt;  &gt; limited resources without requiring unpredictable emergent phenomena.
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; However, I haven't seen any strong arguments in this direction either.
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; -- Ben Goertzel
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ben,
</em><br>
<em>&gt;
</em><br>
<em>&gt; In reply to your question, I'll see if I can outline my argument in more
</em><br>
<em>&gt; detail than previously.
</em><br>
<em>&gt;
</em><br>
<em>&gt; [I am targetting this argument at people who actually understand what a
</em><br>
<em>&gt; complex system is:  I am beyond the point of trying to educate people
</em><br>
<em>&gt; who not only do not understand, but are scornful of even making the
</em><br>
<em>&gt; effort to understand, and who repeatedly throw out false arguments based
</em><br>
<em>&gt; on caricatures of what I and the complex systems people have claimed.]
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Preliminary Remark 1:  The best I am going to be able to do is offer
</em><br>
<em>&gt; convincing empirical reasons why the thesis is true;  certain proof is
</em><br>
<em>&gt; beyond reach, alas.  So it will always be something of a judgement call
</em><br>
<em>&gt; whether one accepts these arguments or not.
</em><br>
<em>&gt;
</em><br>
<em>&gt; *Overview.*
</em><br>
<em>&gt;
</em><br>
<em>&gt; The overall direction of the argument is going to be this:  that in all
</em><br>
<em>&gt; the work on AGI to date, there has been relatively little emphasis on
</em><br>
<em>&gt; getting the &quot;symbols&quot; [please interpret the word loosely: these are just
</em><br>
<em>&gt; the basic representational units that encode the smallest chunks of
</em><br>
<em>&gt; knowledge about the world] to be constructed entirely without programmer
</em><br>
<em>&gt; intervention.  In other words, we tend not to let our systems develop
</em><br>
<em>&gt; their symbols entirely as a result of the interaction of a learning
</em><br>
<em>&gt; mechanism with a stream of environmental input.  Rather, we tend to put
</em><br>
<em>&gt; &quot;ungrounded&quot; symbols in, which we interpret.  The argument is going to
</em><br>
<em>&gt; be that there are indications that this facet of an AGI (whatever
</em><br>
<em>&gt; apparatus allows the symbols to be fully grounded) is going to be more
</em><br>
<em>&gt; important than we suppose, and that it will introduce a great deal of
</em><br>
<em>&gt; complexity, and that that, in turn, will be impossible to avoid.
</em><br>
<em>&gt;
</em><br>
<em>&gt; *Detailed Version of this Argument*
</em><br>
<em>&gt;
</em><br>
<em>&gt; We all know that in the Good Old Days, a lot of AI folks would build
</em><br>
<em>&gt; systems in which they inserted simple tokens, labelled them &quot;arch&quot;
</em><br>
<em>&gt; &quot;hand&quot; &quot;table&quot; &quot;red&quot; and so on, then wrapped a mechanism around those
</em><br>
<em>&gt; symbols so the system could manipulate the symbols as a representation
</em><br>
<em>&gt; of a world, and as a result dispaly some intelligent behavior (like, be
</em><br>
<em>&gt; able to manipulate, and answer questions about, the placement of blocks
</em><br>
<em>&gt; on a table).
</em><br>
<em>&gt;
</em><br>
<em>&gt; What we now know is that these kinds of systems had problems, not the
</em><br>
<em>&gt; least of which was the fact that the symbols were not grounded:  the
</em><br>
<em>&gt; system never created the symbols itself, so it was up to the programmer
</em><br>
<em>&gt; to interpret them manually (so to speak).
</em><br>
<em>&gt;
</em><br>
<em>&gt; What was the solution?  Clearly, one part of the solution had to be to
</em><br>
<em>&gt; give the system a learning mechanism, so it could build new symbols out
</em><br>
<em>&gt; of simpler ones.  Or rather, so it could build symbols from scratch, out
</em><br>
<em>&gt; of real-world raw input of some kind.  Or rather... well, you can see
</em><br>
<em>&gt; that it isn't entirely clear, so let's unwrap this in a bit more detail.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Question:  Do we have to get the system to build all its symbols ab
</em><br>
<em>&gt; initio, from a completely blank slate?  Can't we give it some starter
</em><br>
<em>&gt; symbols and ask it to develop from there?  Surely it would be reasonable
</em><br>
<em>&gt; if the system had some innate knowledge of the world, rather than none
</em><br>
<em>&gt; at all?  The consensus view on this questtion is that it should not be
</em><br>
<em>&gt; necessary to go all the way back to signals coming from raw nerve
</em><br>
<em>&gt; endings in eyes, ears, hands, etc, but that we should be able to put in
</em><br>
<em>&gt; some primitive symbols and get the rest of them to be generated by the
</em><br>
<em>&gt; system.  (Aside:  the connectionists were pretty much defined to be the
</em><br>
<em>&gt; group that broke ranks at this point and insisted that we go down to
</em><br>
<em>&gt; much deeper levels ... we will avoid talking about them for a moment,
</em><br>
<em>&gt; however).
</em><br>
<em>&gt;
</em><br>
<em>&gt; So learning mechanisms should start with some simple symbols and create
</em><br>
<em>&gt; more complex (more abstract) ones as a result of observing and
</em><br>
<em>&gt; interacting with the world.  Where do we draw the line, though?  Which
</em><br>
<em>&gt; primitives are acceptable, and which do we think are too high-level?
</em><br>
<em>&gt; People disagree.  Many who work in the machine learning field do not
</em><br>
<em>&gt; really accept any constraints on the high-levelness of their primitive
</em><br>
<em>&gt; symbols, and are happy to get any symbols to develop into any others,
</em><br>
<em>&gt; not caring if the primitives look low-level enough that we can believe
</em><br>
<em>&gt; they escape the grounding problem.
</em><br>
<em>&gt;
</em><br>
<em>&gt; We, however (from the perspective of this essay) care about the
</em><br>
<em>&gt; grounding problem and see a learning mechanism as a way to justify our
</em><br>
<em>&gt; usage of symbols:  we believe that if we find a plausible learning
</em><br>
<em>&gt; mechanism (or set of mechanisms) it would be capable of going all the
</em><br>
<em>&gt; way from very primitive sensorimotor signals, or very primitive innate
</em><br>
<em>&gt; symbols, all the way up to the most abstract symbols the system could
</em><br>
<em>&gt; ever use.  If we found something as plausible as that, we would believe
</em><br>
<em>&gt; we had escaped the grounding problem.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Next step.  We notice that, whatever learning mechanism we put our money
</em><br>
<em>&gt; on, it is going to complicate our symbols.
</em><br>
<em>&gt;
</em><br>
<em>&gt; [Terminology Note:  I will use &quot;complicate&quot; to mean just what it seems,
</em><br>
<em>&gt; and *nothing whatsoever* to do with &quot;Complex&quot; as in Complex Systems.
</em><br>
<em>&gt; Hopefully this will avoid confusion.  If I use &quot;complex&quot; it will mean
</em><br>
<em>&gt; &quot;as in 'complex systems'&quot;].
</em><br>
<em>&gt;
</em><br>
<em>&gt; What do I mean by complicating our symbols?  Only that if they are going
</em><br>
<em>&gt; to develop, they need more stuff inside them.  They might become
</em><br>
<em>&gt; &quot;frames&quot; or &quot;scripts&quot; or they might be clusters of features, or they
</em><br>
<em>&gt; might have prototypes stored in them.... whatever the details, there
</em><br>
<em>&gt; seems to be a need to put more apparatus in a symbol if it is going to
</em><br>
<em>&gt; develop.  Or, if not passive data inside the symbol (the way I have
</em><br>
<em>&gt; implied so far), then more mechanism inside it.  It seems quite hard,
</em><br>
<em>&gt; for a variety of subtle reasons, to build a good learning mechanism that
</em><br>
<em>&gt; involves utterly simple, passive symbols (just a token, an activation
</em><br>
<em>&gt; level and links to other tokens) and a completely separate learning
</em><br>
<em>&gt; mechanism that is outside the symbols.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Now, I might be being unfair here by implying that the reason our
</em><br>
<em>&gt; symbols became more complicated was because, and only because, we needed
</em><br>
<em>&gt; learning mechanisms.  I don't think I really mean to put it that
</em><br>
<em>&gt; strongly (and it might be interesting only to future historians of AI
</em><br>
<em>&gt; anyhow).  We had other reasons for making them more complicated, on of
</em><br>
<em>&gt; them being that we wanted non-serial models of cognition in which less
</em><br>
<em>&gt; power was centralized in a single learning mechanism and more power
</em><br>
<em>&gt; distributed out to the (now active, not passive) symbols.
</em><br>
<em>&gt;
</em><br>
<em>&gt; So perhaps the best way to summarize is this:  we found, for a variety
</em><br>
<em>&gt; of reasons, that we were pushed toward more complicated mechanisms
</em><br>
<em>&gt; (and/or data structures) inside our symbols, in order to get them to do
</em><br>
<em>&gt; more interesting things, or in order to get over problems that they
</em><br>
<em>&gt; clearly had.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is a very subtle point, so although a lot of people reading this
</em><br>
<em>&gt; will be right along with me, accepting all of this as obvious, I know
</em><br>
<em>&gt; there are going to be some voices that dispute it.  For that reason, I
</em><br>
<em>&gt; am going to dwell on it for just a moment more.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Sometimes you may think that you do not need complicated, active
</em><br>
<em>&gt; symbols, and that in fact you can get away with quite simple structures,
</em><br>
<em>&gt; allied with a sophisticated learning mechanism that builds new symbols
</em><br>
<em>&gt; and connects them to other symbols in just the right, subtle way that
</em><br>
<em>&gt; allows the system as a whole to be generally intelligent.  In response
</em><br>
<em>&gt; to this position, I will say that there is a trap here:  you can always
</em><br>
<em>&gt; rearrange one of my complicated-symbol systems to make it look as if the
</em><br>
<em>&gt; symbols are actually simple (and maybe passive also), at the cost of
</em><br>
<em>&gt; making the learning and thinking mechanism more complicated.  You know
</em><br>
<em>&gt; the kind of thing I mean:  someone proposes that symbols should be
</em><br>
<em>&gt; active neuron-like things, and then some anti-neural-net contrarian
</em><br>
<em>&gt; insists that they can do the same thing with a centralised mechanism
</em><br>
<em>&gt; acting on a matrix of passive data values.  We have all seen these kinds
</em><br>
<em>&gt; of disputes, so let's just cut through all the nonsense and point out
</em><br>
<em>&gt; that you can always reformulate anything to look like anything else, and
</em><br>
<em>&gt; that some types of formulation look more natural, more efficient and
</em><br>
<em>&gt; (generally) more parsimonious.  So when I argue that there is a tendency
</em><br>
<em>&gt; towards more complicated symbols, I mean that the consensus intuition of
</em><br>
<em>&gt; the community is that the simplest, most parsimonious AGI systems tend
</em><br>
<em>&gt; to work with symbols that have more complicated apparatus inside them
</em><br>
<em>&gt; than simply a token plus a couple other bits.
</em><br>
<em>&gt;
</em><br>
<em>&gt; So I want to wrap up all of this and put it in a Hypothesis:
</em><br>
<em>&gt;
</em><br>
<em>&gt; *Complicated Symbol Hypothesis*
</em><br>
<em>&gt;
</em><br>
<em>&gt;    &quot;To the extent that AGI researchers acknowledge the need to capture
</em><br>
<em>&gt; sophisticted learning capabilities in their systems, they discover that
</em><br>
<em>&gt; they need their symbols to be more complicated.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Corollary:
</em><br>
<em>&gt;
</em><br>
<em>&gt;    &quot;The only people who believe that symbols do not need to be
</em><br>
<em>&gt; complicated are the ones who are in denial about the need for learning
</em><br>
<em>&gt; mechanisms.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; ***
</em><br>
<em>&gt; Please note the status of this claim.  It is not a provable contention:
</em><br>
<em>&gt;   it is an observation about the way things have been going.  It is
</em><br>
<em>&gt; based on much thought and intuition about the different kinds of AI
</em><br>
<em>&gt; systems and their faults and strengths.  The term &quot;complicated&quot; is open
</em><br>
<em>&gt; to quite wide interpretation, and it does not mean that there is an
</em><br>
<em>&gt; linear growth on complicatedness as sophistication of learning mechanism
</em><br>
<em>&gt; goes up, only that we seem to need relatively complicated symbols (as
</em><br>
<em>&gt; compared with simple passive tokens with activation levels and simple
</em><br>
<em>&gt; links) in order to capture realistic amounts of learning.
</em><br>
<em>&gt; ***
</em><br>
<em>&gt;
</em><br>
<em>&gt; But now, what of it?  What does it matter that we need more stuff in the
</em><br>
<em>&gt; symbols?  How is this relevant to the original question about complexity
</em><br>
<em>&gt; in AGI design?
</em><br>
<em>&gt;
</em><br>
<em>&gt; To answer this, I am going to try to unpack that idea of &quot;complicated
</em><br>
<em>&gt; symbols&quot; to get at some of the details.
</em><br>
<em>&gt;
</em><br>
<em>&gt; When we observe humans going through the process of learning the
</em><br>
<em>&gt; knowledge that they learn, we notice that they have some extremely
</em><br>
<em>&gt; powerful mechanisms in there.  I mean &quot;powerful&quot; in the sense of being
</em><br>
<em>&gt; clever and subtle.  They seem to use analogy a lot, for example.  To get
</em><br>
<em>&gt; some idea of the subtlety, just go back and look at the stuff Hofstadter
</em><br>
<em>&gt; comes up with in GEB and in Metamagical Themas.  When I talk about
</em><br>
<em>&gt; &quot;learning&quot; I don't mean the restricted, narrow sense in which AI folks
</em><br>
<em>&gt; usually talk about learning systems, I mean the whole shebang:  the
</em><br>
<em>&gt; full-up, flexible kind of learning that people engage in, where jumping
</em><br>
<em>&gt; up a level of representation and pulling analogies around seems to be
</em><br>
<em>&gt; the almost the norm, rather than the exception.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Getting this kind of learning capability into an AGI is the goal, as far
</em><br>
<em>&gt; as the present discussion is concerned.  Anything less is not good
</em><br>
<em>&gt; enough.  I think we can all agree that there is a big gap between
</em><br>
<em>&gt; current machine capabilities and the awesome generality of the
</em><br>
<em>&gt; human system.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But how to do it?  How do we close that gap?
</em><br>
<em>&gt;
</em><br>
<em>&gt; At this point, everyone has a different philosophy.  I look at language
</em><br>
<em>&gt; learning in a 2-to-6 year old child and I think I observe that when I
</em><br>
<em>&gt; talk to that child, I can define pretty much anything in the whole world
</em><br>
<em>&gt; by referring to loose examples and analogies, and the chold does an
</em><br>
<em>&gt; astonishing job of getting what I am saying.  I even define grammatical
</em><br>
<em>&gt; subtleties that way, when teaching how to talk.  But to someone like
</em><br>
<em>&gt; Chomsky, Fodor or Pinker, this entire process may be governed by a
</em><br>
<em>&gt; massive amount of innate machinery [and, yes, Fodor at least seems to
</em><br>
<em>&gt; believe that innateness does not just apply to the grammatical machinery
</em><br>
<em>&gt; but to most of the conceptual apparatus as well, with just a little
</em><br>
<em>&gt; content filling in to be done during maturation].
</em><br>
<em>&gt;
</em><br>
<em>&gt; Then there are folks who don't go for the Chomskian innateness idea, but
</em><br>
<em>&gt; who do insist that there is nothing wrong with our current ideas about
</em><br>
<em>&gt; the basic format (data and mechanisms) inside symbols, all we need to do
</em><br>
<em>&gt; is build a system that is big enough and fast enough, and connect it up
</em><br>
<em>&gt; to sufficiently much real world input (and realistic motor output
</em><br>
<em>&gt; systems) and it will develop the same rich repertoire of knowledge that
</em><br>
<em>&gt; we see in humans.  These people believe that all we need to do is take
</em><br>
<em>&gt; symbols the way they are currently conceived, add some richer, improved,
</em><br>
<em>&gt; to-be-determined learning mechanisms that browse on those symbols, and
</em><br>
<em>&gt; all will eventually be well.
</em><br>
<em>&gt;
</em><br>
<em>&gt; So what do I think?  I disagree with the position taken in that last
</em><br>
<em>&gt; paragraph.  Now I am now going to try to focus in on exactly why
</em><br>
<em>&gt; I disagree.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Imagine a hypothetical AGI researcher who first decides what the format
</em><br>
<em>&gt; of a symbol should be and then tries to hunt for a learning mechanism
</em><br>
<em>&gt; that will allow symbols of that sort to develop as a result of
</em><br>
<em>&gt; interaction with the world.  Just for the sake of argument (which means
</em><br>
<em>&gt; don't ask me to defend this!) let's be really naive and throw down some
</em><br>
<em>&gt; example symbol structure:  maybe each symbol is a token with activation
</em><br>
<em>&gt; level, truth value, labelled connections to some other symbols (with the
</em><br>
<em>&gt; labels coming from a fixed set of twenty possible labels) and maybe an
</em><br>
<em>&gt; instance number.  Who knows, something like that.
</em><br>
<em>&gt;
</em><br>
<em>&gt; First the researcher convinces themself that the proposed system can
</em><br>
<em>&gt; work if given ungrounded, programmer-interpreted symbols.  They knock up
</em><br>
<em>&gt; a system for reasoning about a little knowledge domain and show that
</em><br>
<em>&gt; given a stream of predigested, interpreted information, the system can
</em><br>
<em>&gt; come to some interesting conclusions within the domain.  Maybe the
</em><br>
<em>&gt; system gets loaded on a spacecraft bound for the outer solar system and
</em><br>
<em>&gt; it can &quot;think&quot; about some of the ship's technical troubles and come up
</em><br>
<em>&gt; with strategies for resolving them.  And it does a reasonable job, we'll
</em><br>
<em>&gt; suppose.
</em><br>
<em>&gt;
</em><br>
<em>&gt; So far, not much learning.  It didn't learn about spaceraft repair from
</em><br>
<em>&gt; a textbook, or from getting out a wrenc and trying to build spacecraft,
</em><br>
<em>&gt; or from long conversations with the engineers, it was just preloaded
</em><br>
<em>&gt; with symbols and information.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But the researcher is hopeful, so they start adding learning mechanisms
</em><br>
<em>&gt; in an attempt to get the system to augment itself.  The idea is not just
</em><br>
<em>&gt; to get it to add to its existing knowledge, but to start with less
</em><br>
<em>&gt; knowledge and get to where it is now, by doing its own learning.  We
</em><br>
<em>&gt; are, after all, on a quest to eliminate most of that preloaded knowledge
</em><br>
<em>&gt; because we want to ground the system in the real world.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But as the researcher tries to devise more and more powerful kinds of
</em><br>
<em>&gt; learning mechanisms, they discover a trend.  Those more powerful
</em><br>
<em>&gt; mechanisms need more complicated stuff inside the symbols.  Let's
</em><br>
<em>&gt; suppose that they are trying to get analogy to happen:  they find that
</em><br>
<em>&gt; the existing symbol structure is too limited and they need to add on a
</em><br>
<em>&gt; bunch of extra doohickeys that represent..... well, they don't represent
</em><br>
<em>&gt; anything that easily has a name at the symbol level, but they are needed
</em><br>
<em>&gt; anyhow to get the system to do flexible, tangled kinds of stuff that
</em><br>
<em>&gt; leads to the building of new symbols out of old.
</em><br>
<em>&gt;
</em><br>
<em>&gt; When and if the researcher tries to avoid this - tries to keep the
</em><br>
<em>&gt; symbols nice and clean, like they were before - they discover something
</em><br>
<em>&gt; rather annoying:  the only way they can do this is to put more stuff in
</em><br>
<em>&gt; the learning mechanisms (outside of the symbols) instead.  Keep the
</em><br>
<em>&gt; symbols clean, but make the (non-symbol-internal) learning mechanisms a
</em><br>
<em>&gt; lot more complex.  And then there is even more trouble, because it turns
</em><br>
<em>&gt; out that the learning mechanism itself starts to need, not just more
</em><br>
<em>&gt; machinery, but its own knowledge content!  Now there are two places
</em><br>
<em>&gt; where knowledge is being acquired:  symbol system and learning engine.
</em><br>
<em>&gt; And they don't talk, these two systems.  In the process of trying to
</em><br>
<em>&gt; keep the symbols clean and simple, the learning system had to invent new
</em><br>
<em>&gt; strategies for learning, and (this is the real cause of the trouble)
</em><br>
<em>&gt; some of those new learning mechanisms really seemed to be dependent on
</em><br>
<em>&gt; the content of the world knowledge.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Something difficult-to-explain is happening here.  It is because
</em><br>
<em>&gt; learning (knowledge acauisition and refinement) is apparently so
</em><br>
<em>&gt; flexible and reflexive and tangled in humans, that we have reason to
</em><br>
<em>&gt; believe that the (human) learning mechanism that is the generator of
</em><br>
<em>&gt; this behavior must itself involve some quite tangled mechanisms.
</em><br>
<em>&gt;
</em><br>
<em>&gt; What does this seem to imply for the design of an AGI?  It seems to
</em><br>
<em>&gt; indicate that if we want a natural, parsimonious design, we are going to
</em><br>
<em>&gt; inevitably head towards a type of system in which the symbols are
</em><br>
<em>&gt; allowed to grow in a tangled way right from the outset, with knowledge
</em><br>
<em>&gt; _about_ the world and knowledge about _how to understand the world_
</em><br>
<em>&gt; being inextricably intertwined.  And then, when we try to get those
</em><br>
<em>&gt; systems to actually work with real world I/O, we will discover that we
</em><br>
<em>&gt; have to add tweaks and mechanisms inside the symbols, and in the
</em><br>
<em>&gt; surrounding architecture, to keep the system stable.  And sooner or
</em><br>
<em>&gt; later we discover that we have a system that seems to learn new concepts
</em><br>
<em>&gt; from [almost] scratch quite well, but we have lost our ability to
</em><br>
<em>&gt; exactly interpret what is the meaning of the apparatus inside and around
</em><br>
<em>&gt; the symbols.  We might find that there is no such thing as a symbol that
</em><br>
<em>&gt; represents &quot;cup&quot;, there is only a cluster of units and operators that
</em><br>
<em>&gt; can be used to stand for the cup concept wherever it is needed, and the
</em><br>
<em>&gt; cluster manifests in different ways depending on whether we are picking
</em><br>
<em>&gt; up a cup, describing a cup, trying to defining a cup, trying to catch a
</em><br>
<em>&gt; falling cup, trying to design an artistic looking cup, and so on.
</em><br>
<em>&gt;
</em><br>
<em>&gt; In short, we may end up discovering, in the process of trying to get a
</em><br>
<em>&gt; realistic set of human-like, powerul, recursive learning mechanisms to
</em><br>
<em>&gt; actually work, that all the original apparatus we put in those symbols
</em><br>
<em>&gt; becomes completely redundant!  All the reasons the system now functions
</em><br>
<em>&gt; might actually be enshrined in the extra apparatus we had to introduce
</em><br>
<em>&gt; to (a) get it to learn powerfully and (b) get it to be stable in spite
</em><br>
<em>&gt; of the tangledness.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But wait, that sounds like a presumption on my part:  why jump to the
</em><br>
<em>&gt; conclusion that &quot;all the original apparatus we put in those symbols
</em><br>
<em>&gt; becomes completely redundant&quot;?  Why on earth should we believe this
</em><br>
<em>&gt; would happen?  Isn't this just a random piece of guesswork?
</em><br>
<em>&gt;
</em><br>
<em>&gt; The reason it is not a wild guess is that when the complicated learning
</em><br>
<em>&gt; mechanisms were introduced, they were so tangled and recursive (with
</em><br>
<em>&gt; data and mechanism being intertwined) that they forced the system away
</em><br>
<em>&gt; from the &quot;simple system&quot; regime and smack bang in the middle of the
</em><br>
<em>&gt; &quot;complex system&quot; regime.  In other words, when you put that kind of
</em><br>
<em>&gt; reflexivity and adaptiveness in such a system, it is quite likely that
</em><br>
<em>&gt; the low level mechanisms needed to make its stable will look different
</em><br>
<em>&gt; from the high-level behavior.  We *want* something that approximates our
</em><br>
<em>&gt; conventional understanding of symbols to appear in the top level
</em><br>
<em>&gt; behavior - that is our design goal - and we want enormously powerful
</em><br>
<em>&gt; adaptive mechanisms.  The experience of teh complex systems community is
</em><br>
<em>&gt; that you can't start with design goals and easily get mechanisms that do
</em><br>
<em>&gt; that.  And you especially cannot start with low level mechanisms that
</em><br>
<em>&gt; look like the desired high-level ones, add a soupcon of adpativeness,
</em><br>
<em>&gt; and then expect the high and low levels to still be the same.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Now, the diehard AGI researcher listens to me say this and replies:
</em><br>
<em>&gt; &quot;But I am not *trying* to emulate the messy human design:  I believe
</em><br>
<em>&gt; that a completely different design can succeed, involving fairly clean
</em><br>
<em>&gt; symbols and a very limited amount of tangling.  Just because humans do
</em><br>
<em>&gt; it that way, doesn't mean that a machine has to.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; The reply is this.  As you put more powerful learning mechanisms in your
</em><br>
<em>&gt; designs, I see your symbols getting more complicated.  I see an enormous
</em><br>
<em>&gt; gap, still, between the power of human learning mechanisms and the power
</em><br>
<em>&gt; of existing AGI mechanisms.  There is a serious possibility that you can
</em><br>
<em>&gt; only keep a clean, non-complex AGI design by steering clear of extremely
</em><br>
<em>&gt; tangled, recursive, powerful knowledge acquisition mechanisms.  And if
</em><br>
<em>&gt; you steer clear of them, you may find that you never get the AGI to
</em><br>
<em>&gt; actually work.
</em><br>
<em>&gt;
</em><br>
<em>&gt; To the complex systems person, observing the only known example of an
</em><br>
<em>&gt; [non-A]GI (the human mind), it appears to be a matter of faith that real
</em><br>
<em>&gt; learning can happen in a clean, non-complex AGI design.  Their point of
</em><br>
<em>&gt; view (my point of view) is:  go straight for the jugular please, and
</em><br>
<em>&gt; produce mechanisms of awesome, human level learning power, but *without*
</em><br>
<em>&gt; sending the system into Complex territory, and I will have some reason
</em><br>
<em>&gt; to believe it possible.
</em><br>
<em>&gt;
</em><br>
<em>&gt; At the moment, I see no compelling reason to believe, in the face of the
</em><br>
<em>&gt; complexity I see in the human design.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I am sure I could articulate this argument better.  But that is my best
</em><br>
<em>&gt; shot for the moment.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Richard Loosemore.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12411.html">Ben Goertzel: "RE: Is complex emergence necessary for AGI?"</a>
<li><strong>Previous message:</strong> <a href="12409.html">Ben Goertzel: "RE: Is complex emergence necessary for AGI?"</a>
<li><strong>In reply to:</strong> <a href="12408.html">Richard Loosemore: "Re: Is complex emergence necessary for intelligence under limited resources?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12415.html">Richard Loosemore: "Re: Is complex emergence necessary for intelligence under limited resources?"</a>
<li><strong>Reply:</strong> <a href="12415.html">Richard Loosemore: "Re: Is complex emergence necessary for intelligence under limited resources?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12410">[ date ]</a>
<a href="index.html#12410">[ thread ]</a>
<a href="subject.html#12410">[ subject ]</a>
<a href="author.html#12410">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
