<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))</title>
<meta name="Author" content="Matt Mahoney (matmahoney@yahoo.com)">
<meta name="Subject" content="Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))">
<meta name="Date" content="2009-12-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))</h1>
<!-- received="Mon Dec  7 14:52:14 2009" -->
<!-- isoreceived="20091207215214" -->
<!-- sent="Mon, 7 Dec 2009 13:52:08 -0800 (PST)" -->
<!-- isosent="20091207215208" -->
<!-- name="Matt Mahoney" -->
<!-- email="matmahoney@yahoo.com" -->
<!-- subject="Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))" -->
<!-- id="643178.87363.qm@web51902.mail.re2.yahoo.com" -->
<!-- charset="utf-8" -->
<!-- inreplyto="12902e900912071256q562f253bq85a1b39d8589e50@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Matt Mahoney (<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))"><em>matmahoney@yahoo.com</em></a>)<br>
<strong>Date:</strong> Mon Dec 07 2009 - 14:52:08 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20935.html">Luke: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>Previous message:</strong> <a href="20933.html">Luke: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>In reply to:</strong> <a href="20933.html">Luke: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20935.html">Luke: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>Reply:</strong> <a href="20935.html">Luke: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20934">[ date ]</a>
<a href="index.html#20934">[ thread ]</a>
<a href="subject.html#20934">[ subject ]</a>
<a href="author.html#20934">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Luke wrote:
&gt; Actually, in that situation you'd probably be primarily motivated by a desire not to experience your flesh being burned.

So are you denying that you believe in this nonexistent thing, or are you denying that it doesn't exist?

 -- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">matmahoney@yahoo.com</a>




________________________________
From: Luke &lt;<a href="mailto:wlgriffiths@gmail.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">wlgriffiths@gmail.com</a>&gt;
To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">sl4@sl4.org</a>
Sent: Mon, December 7, 2009 3:56:22 PM
Subject: Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))

&quot;If you got out, then you must believe in this nonexistent thing.&quot;

Actually, in that situation you'd probably be primarily motivated by a desire not to experience your flesh being burned.

Even a zen master will duck if you throw a molotov cocktail at him.  That's the nature of the flesh.

More generally, re: this entire conversation.  Just accept the fact that if you make a conscious copy of yourself you'll both feel the continuity.

That continuity is itself an illusion.  The past doesn't exist, except in memory.  &quot;In&quot; memory - not a spatial relationship.

 - Luke


On Mon, Dec 7, 2009 at 3:09 PM, Matt Mahoney &lt;<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">matmahoney@yahoo.com</a>&gt; wrote:

&quot;M.&gt;h&quot; wrote:
&gt;&gt; ... sorry, but i do not get the whole problem.
&gt;
&gt;
&gt;You understand that I can't define something which doesn't exist. When I say qualia or self awareness or that little person inside your mind that observes the world through your senses, most people know what I mean.
&gt;
&gt;
&gt;Let me put it this way. Consider the AI program that observed everything you did for the last several years until it became so good at predicting your behavior that none of your friends or relatives could distinguish it from you in a Turing test environment.
&gt; Unfortunately the building containing the only copy of the program is on fire.  It is just you and the computer in a room rapidly filling with smoke. There is just enough time either for you to get out, allowing the only copy to be destroyed, or for you to upload a copy of the program to a remote site over the internet with your last dying breath. Which do you do?
&gt;
&gt;
&gt;If you got out, then you must believe in this nonexistent thing. Otherwise you would logically conclude that by preserving your memories in a form that can be backed up, that you become immortal. Furthermore, you have the opportunity to enhance your intelligence and your environment by running on more powerful computers and embodied in better robots in the future. Why would you ever allow one copy to be destroyed now and the only other copy in a few decades?
&gt;
&gt;
&gt;Sorry for my ambiguous use of the word &quot;you&quot;.
&gt;
&gt; -- Matt
&gt; Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">matmahoney@yahoo.com</a>
&gt;
&gt;
&gt;
&gt;
&gt;
________________________________
From: &quot;M.&gt;h&quot; &lt;<a href="mailto:m.transhumanist@gmail.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">m.transhumanist@gmail.com</a>&gt;
&gt;To: &quot;<a href="mailto:sl4@sl4.org?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">sl4@sl4.org</a>&quot; &lt;<a href="mailto:sl4@sl4.org?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">sl4@sl4.org</a>&gt;
&gt;Sent: Mon, December 7, 2009 1:58:00 AM
&gt;
&gt;Subject: Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))
&gt;
&gt;
&gt;
&gt;... sorry, but i do not get the whole problem. even if a clone of me would walk up to me right here and now, having sufficiently enough of my memories and claiming to have my 'identity', i would not care if this 'double' would not use my resources (e.g. credit card) and bureaucrats would
&gt; leave me alone!
&gt;
&gt;
&gt;cheers,
&gt;
&gt;
&gt;miriam
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;Am 06.12.2009 um 22:12 schrieb Thomas Buckner &lt;<a href="mailto:tcbevolver@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">tcbevolver@yahoo.com</a>&gt;:
&gt;
&gt;
&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
________________________________
From: Matt Mahoney &lt;<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">matmahoney@yahoo.com</a>&gt;
&gt;&gt;To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">sl4@sl4.org</a>
&gt;&gt;Sent: Sun, December 6, 2009 2:44:53 PM
&gt;&gt;Subject: Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))
&gt;&gt;
&gt;&gt;
&gt;&gt;Rewot Fetterkey
&gt;&gt; wrote:
&gt;&gt;&gt; Can you clarify that? How, exactly, is consciousness nonexistent?
&gt;&gt;
&gt;&gt;
&gt;&gt;By consciousness, I mean that which makes you different from a philosophical zombie as described in <a href="http://en.wikipedia.org/wiki/Philosophical_zombie">http://en.wikipedia.org/wiki/Philosophical_zombie</a>
&gt;&gt;But by definition, a zombie is not distinguishable from you at all. I really don't know how much more clear the logic could be.
&gt;&gt;
&gt;&gt;
&gt;&gt;The
&gt;&gt; problem arises because all animals, including those that have no concept of death, have evolved a fear of those things that can kill them. Humans do have such a concept, which we associate with a lack of conscious experience. So we all desperately want to preserve this thing that does not exist. We can't help it. We are programmed that way.
&gt;&gt;
&gt;&gt;
&gt;&gt;One way to deal with this conflict is to argue that the zombie argument is wrong and create ever more convoluted arguments to refute it. My preferred approach is as follows:
&gt;&gt;
&gt;&gt;
&gt;&gt;1. I believe that I have conscious experience. (I am programmed to).
&gt;&gt;2. I know that conscious experience does not exist.. (Logic irrefutably says so).
&gt;&gt;3. I realize that 1 and 2 are inconsistent. I leave it at that.
&gt;&gt;
&gt;&gt; -- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">matmahoney@yahoo.com</a>
&gt;&gt;
&gt;&gt;
&gt;&gt;I'm with Daniel Dennet on this: the P-zombie is (to paraphrase an earlier poster)
&gt;&gt; 2+2 = 5. Purely hypothetical, a character in a gendankenexperiment, The Man Who Wasn't There. In practice, any creature with a human brain that could say &quot;Ouch, that hurt&quot; has an internal process isomorphic to what we experience as consciousness. Please see my post of a few hours ago on the Edge talk.  <a href="http://www.edge.org/3rd_culture/dehaene09/dehaene09_index.html">http://www.edge.org/3rd_culture/dehaene09/dehaene09_index.html</a>
&gt;&gt;
&gt;&gt;Consciousness in the human brain is a global pattern of activation and we now have methods of scanning and can say whether that pattern appears or not. This scanning has been applied to comatose/vegetative patients. From Dr. Dehaene's talk: 
&gt;&gt;
&gt;&gt;
&gt;&gt;&quot;Let me just give you a very basic idea about the test. We stimulate the patient with five tones. The first four tones are identical, but the fifth can be different. So you hear something like dit-dit-dit-dit-tat. When you do this, a very banal observation, dating back 25 years, is that the brain reacts to the different tone at the end. That reaction, which is called mismatch negativity, is completely automatic. You get it even in coma, in sleep, or when you do not attend to the stimulus. It's a non-conscious response.
&gt;&gt;&gt;&gt;Following it, however, there is also, typically, a later brain response called the P3. This is exactly the large-scale global response that we found in our
&gt;&gt; previous experiments, that must be specifically associated with consciousness.
&gt;&gt; (snip)
&gt;&gt;The P3 response (a marker is absent in coma patients. It is also gone in most vegetative state patients — but it remains present in most minimally conscious patients. It is always present in locked-in patients and in any other conscious subject.&quot;
&gt;&gt;
&gt;&gt;
&gt;&gt;Consciousness, according to Dr. Dehaene's findings, is how the human brain gets around certain limitations of being an analog computer. If you've read Eliezer Yudkowsky's posts you'll know that his approach to AGI would not necessarily call for the AGI to be conscious in the sense we understand. I recall he said &quot;I'm not looking for the AGI to be a new drinking buddy, at least not at first&quot; or words close to that. Paramount, to him, is that the AGI be Friendly, and not damage us intentionally or otherwise. While the human brain is a kind of analog computer, and much research is now
&gt;&gt; afoot to emulate it on digital computers, our minds are not exactly computer programs. They are certainly not fungible programs running on a general computing machine, but rather embedded in the structure. The mind is not fungible unless the neural structure is made fungible, which may or may not ever be possible.
&gt;&gt;To sum up, there's no real-world way a zombie could react as if conscious, using human brain architecture, without being conscious. Unless you believe in magic. And the subject of zombies, even if such could exist, probably doesn't really apply to the problems of building an AGI.
&gt;&gt;
&gt;&gt;
&gt;&gt;Tom Buckner
&gt;&gt;

<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20935.html">Luke: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>Previous message:</strong> <a href="20933.html">Luke: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>In reply to:</strong> <a href="20933.html">Luke: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20935.html">Luke: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>Reply:</strong> <a href="20935.html">Luke: "Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20934">[ date ]</a>
<a href="index.html#20934">[ thread ]</a>
<a href="subject.html#20934">[ subject ]</a>
<a href="author.html#20934">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
