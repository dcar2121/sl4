<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))</title>
<meta name="Author" content="Matt Mahoney (matmahoney@yahoo.com)">
<meta name="Subject" content="Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))">
<meta name="Date" content="2009-12-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))</h1>
<!-- received="Mon Dec 14 13:50:45 2009" -->
<!-- isoreceived="20091214205045" -->
<!-- sent="Mon, 14 Dec 2009 12:50:14 -0800 (PST)" -->
<!-- isosent="20091214205014" -->
<!-- name="Matt Mahoney" -->
<!-- email="matmahoney@yahoo.com" -->
<!-- subject="Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))" -->
<!-- id="486093.66115.qm@web51906.mail.re2.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="10B6B468E3674EBF8BD481A1A4BD55F8@bradley01c25a6" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Matt Mahoney (<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))"><em>matmahoney@yahoo.com</em></a>)<br>
<strong>Date:</strong> Mon Dec 14 2009 - 13:50:14 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20944.html">Natasha Vita-More: "[sl4]"</a>
<li><strong>Previous message:</strong> <a href="20942.html">Bradley Thomas: "RE: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>In reply to:</strong> <a href="20942.html">Bradley Thomas: "RE: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20909.html">John K Clark: "[sl4] Re: Copying nonsense"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20943">[ date ]</a>
<a href="index.html#20943">[ thread ]</a>
<a href="subject.html#20943">[ subject ]</a>
<a href="author.html#20943">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Bradley Thomas wrote:
<br>
<em>&gt; Maybe we could define an &quot;n-bit consciousness&quot; as one who's dataset never
</em><br>
<em>&gt; copies less than n bits during each transition from one experience to the next.
</em><br>
<p>You are making this more complicated than it needs to be. Human brains are programmed to fear death and grieve the death of others. As an upload, this is a software problem. You could reprogram your goals so that you don't care about death, or you could create an artificial world where death does not exist.
<br>
<p>Care must be taken during the upload process to avoid giving the illusion of death to others. The robot that comes out of the process must look and behave like the person that went in, as far as anyone else can tell. Also, it is important that the human be killed immediately and the body disposed of undetectably. Having two copies at the same time would destroy the illusion of continuity.
<br>
<p>The copy does not have to be exact. You only have about 50% of the memories that you did a few years ago. Yet, friends who had no contact with you during that period won't confuse you with a different person. With that much tolerance, it would be possible to construct a model of your mind given a few years of surveillance (which I believe is the most likely path to providing the training data for AI). The model would be good enough to fool others, which is all you need. You would not be aware of any missing or false memories yourself, and even if you were, you could correct them.
<br>
<p>&nbsp;-- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">matmahoney@yahoo.com</a>
<br>
<p><p><p>----- Original Message ----
<br>
From: Bradley Thomas &lt;<a href="mailto:brad36@gmail.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">brad36@gmail.com</a>&gt;
<br>
To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">sl4@sl4.org</a>
<br>
Sent: Sat, December 12, 2009 7:59:43 PM
<br>
Subject: RE: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))
<br>
<p><em>&gt;Three experiences S1, S2, S3 are subjectively connected if S2 remembers S1
</em><br>
and S3 remembers S2, even though S3 may not remember S1. In this way a
<br>
person can be defined as a series of subjectively connected experiences.
<br>
However, I don't think it is possible to make this definition *exact* if we
<br>
consider events that cause subjective discontinuities, such as copying with
<br>
partial memory loss or memory implantation.
<br>
<p>Is it reasonable to make the analogy between an experience and a dataset? If
<br>
so, I just wonder what minimal amount of data needs to be carried over from
<br>
S1 to S2 in order for S2 to &quot;remember&quot; S1. Is one bit the minimal amount of
<br>
data possible? But to me that would seem to be ridiculous, so I guess it
<br>
would have to be somewhere higher. But where? I can see that if two separate
<br>
computer systems held the same image it is reasonable to say they have some
<br>
kind of subjective connection, but one bit? Its hard to make the case for
<br>
that. 
<br>
<p>Maybe we could define an &quot;n-bit consciousness&quot; as one who's dataset never
<br>
copies less than n bits during each transition from one experience to the
<br>
next. And maybe that could be used to define the points at which human life
<br>
begins and ends (cf. abortion debate). Ie. as soon as a baby copies an
<br>
experience of at least n bits from one experience to the next, then it
<br>
technically becomes a sentient person, and as soon as a person fails to copy
<br>
n bits, then they technically cease to be a sentient person. Perhaps we
<br>
might someday be able to measure that fairly accurately and it becomes
<br>
useful for legal reasons etc.
<br>
<p>I wonder what a reasonable range would be for n, to define such a partial
<br>
requirement for human personhood. It seems like a potentially useful idea to
<br>
me although it's still nothing like a complete definition of human identity
<br>
as far as I can tell because clearly one could apply this definition to
<br>
purely machine systems today.
<br>
<p><p>Brad Thomas
<br>
www.bradleythomas.com
<br>
Twitter @bradleymthomas, @instansa
<br>
<p><p><p>-----Original Message-----
<br>
From: <a href="mailto:owner-sl4@sl4.org?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">owner-sl4@sl4.org</a>] On Behalf Of Stathis
<br>
Papaioannou
<br>
Sent: Thursday, December 10, 2009 8:47 PM
<br>
To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">sl4@sl4.org</a>
<br>
Subject: Re: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))
<br>
<p><p>2009/12/11 Bradley Thomas &lt;<a href="mailto:brad36@gmail.com?Subject=Re:%20Copying%20nonsense%20(was%20Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI))">brad36@gmail.com</a>&gt;:
<br>
<em>&gt;&gt;There is no exact definition.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think there needs to be, otherwise how do we know what we're talking 
</em><br>
<em>&gt; about when we refer to experiences as being &quot;subjectively connected&quot;?
</em><br>
<p>Three experiences S1, S2, S3 are subjectively connected if S2 remembers S1
<br>
and S3 remembers S2, even though S3 may not remember S1. In this way a
<br>
person can be defined as a series of subjectively connected experiences.
<br>
However, I don't think it is possible to make this definition *exact* if we
<br>
consider events that cause subjective discontinuities, such as copying with
<br>
partial memory loss or memory implantation.
<br>
<p><p><pre>
-- 
Stathis Papaioannou
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20944.html">Natasha Vita-More: "[sl4]"</a>
<li><strong>Previous message:</strong> <a href="20942.html">Bradley Thomas: "RE: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<li><strong>In reply to:</strong> <a href="20942.html">Bradley Thomas: "RE: Copying nonsense (was Re: [sl4] Uploading (was : goals of AI))"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20909.html">John K Clark: "[sl4] Re: Copying nonsense"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20943">[ date ]</a>
<a href="index.html#20943">[ thread ]</a>
<a href="subject.html#20943">[ subject ]</a>
<a href="author.html#20943">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
