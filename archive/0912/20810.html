<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] Uploading (was : goals of AI)</title>
<meta name="Author" content="John McNamara (harlequin@novastar.org)">
<meta name="Subject" content="Re: [sl4] Uploading (was : goals of AI)">
<meta name="Date" content="2009-12-01">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] Uploading (was : goals of AI)</h1>
<!-- received="Tue Dec  1 17:21:57 2009" -->
<!-- isoreceived="20091202002157" -->
<!-- sent="Wed, 2 Dec 2009 00:21:32 +0000" -->
<!-- isosent="20091202002132" -->
<!-- name="John McNamara" -->
<!-- email="harlequin@novastar.org" -->
<!-- subject="Re: [sl4] Uploading (was : goals of AI)" -->
<!-- id="6362eea20912011621w186c9115v3f15c30885111695@mail.gmail.com" -->
<!-- charset="UTF-8" -->
<!-- inreplyto="924363.25205.qm@web51901.mail.re2.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> John McNamara (<a href="mailto:harlequin@novastar.org?Subject=Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI)"><em>harlequin@novastar.org</em></a>)<br>
<strong>Date:</strong> Tue Dec 01 2009 - 17:21:32 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20811.html">Frank Adamek: "Re: [sl4] Uploading (was : goals of AI)"</a>
<li><strong>Previous message:</strong> <a href="20809.html">Bradley Thomas: "RE: [sl4] JKC, are your views actually amenable to correction, ever?"</a>
<li><strong>In reply to:</strong> <a href="20795.html">Matt Mahoney: "[sl4] Uploading (was : goals of AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20814.html">John K Clark: "Re: [sl4] Uploading"</a>
<li><strong>Reply:</strong> <a href="20814.html">John K Clark: "Re: [sl4] Uploading"</a>
<li><strong>Reply:</strong> <a href="20818.html">Stathis Papaioannou: "Re: [sl4] Uploading (was : goals of AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20810">[ date ]</a>
<a href="index.html#20810">[ thread ]</a>
<a href="subject.html#20810">[ subject ]</a>
<a href="author.html#20810">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Tue, Dec 1, 2009 at 17:41, Matt Mahoney &lt;<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI)">matmahoney@yahoo.com</a>&gt; wrote:
<br>
<em>&gt; John McNamara wrote:
</em><br>
<em>&gt; &gt; What is the maximum tolerable error that will not result in the failure of your engineering project (ie upload of a live human with no apparent deviations from expected normal thinking patterns (including fuzzy things like emotions/inspiration etc) for at least 1000 years with 99.9999 confidence level etc etc).
</em><br>
<p><em>&gt; Suppose there was a program that simulated you so well that nobody could tell the difference between you and the program in a Turing test environment. What is the probability that the program will be you after you shoot yourself?
</em><br>
<em>&gt; -- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI)">matmahoney@yahoo.com</a>
</em><br>
<p>Hi Matt,
<br>
<p>Exactly zero, in my personal opinion.
<br>
<p>For me, the term &quot;be you&quot; puts the question in a philosophical frame
<br>
as opposed to say an engineering one.
<br>
As such I can give you my personal philosophical reasoning on it but I
<br>
cannot offer a math-based answer.
<br>
If there was a credible science for the &quot;Math of Philosophy&quot; and I had
<br>
a Phd in it, it might be different matter.
<br>
<p>The example engineering project I mentioned was intended as a
<br>
throwaway vague example. There is little or no technical detail in it.
<br>
I think the point I was making would apply to any engineering project
<br>
attempting an error-controlled simulation of a highly complex physical
<br>
system.
<br>
<p>Engineers today have non-perfect information on the systems they build
<br>
and have to use their professions best techniques to determine if the
<br>
thing they build will be OK. This involves saying things like &quot;this
<br>
wall will not develop more than x micro fractures per sq m when
<br>
exposed to cyclical winds between z &amp; y m/s for s mins every day for
<br>
at least 5 years with a probability of .99999&quot; after doing lots of
<br>
testing and maths and (hopefully) very little wild guessing.
<br>
I see future &quot;upload engineers&quot; as working in a similar way. It may
<br>
turn out that they have no practical use for the Turing test as we
<br>
know it now.
<br>
<p>That said I'm happy to discuss your specific scenario further and it
<br>
deserves a more detailed answer than '0'.
<br>
You did after all create a new thread.
<br>
<p>The main benefit of uploading is obviously moving from our current
<br>
situation where the mind 'runs' on the familiar human organic hardware
<br>
to hardware that has superior features, the most important of which is
<br>
to cleanly separate 'data' from 'hardware', with the many attendant
<br>
engineering benefits that brings.
<br>
I realise an upload is effectively &quot;data&quot; but the objective is to be
<br>
able to choose better hardware to 'run' that data on.
<br>
<p>There are theoretically 2 ways to do an upload that I know of.
<br>
<p>1 : build 1 (or more) copies on new hardware, (optionally dispose of
<br>
original or let entropy take it's course)
<br>
<p>2 : Ship of Theseus style migration of live 'running' mind from the
<br>
familiar human organic hardware to new hardware.
<br>
<p>I'll assume they're equally practical and reliable from an engineering POV.
<br>
I personally would only tolerate option 2 (assuming only these 2
<br>
alternatives) for my myself.
<br>
No disrespect to the scientific value (whatever that may be) of the
<br>
Turing test but I couldn't care less about the &quot;nobody could tell the
<br>
difference between you and the program in a Turing test environment&quot;
<br>
bit.
<br>
Other's observations on my status are theirs and are irreverent to my
<br>
philosophical opinion on this matter.
<br>
They can think I'm Santa Claus for all I care.
<br>
I would envision a successful type 2 upload occurring only over a long
<br>
time scale, in the order of years.
<br>
When completed I would think of the final disposal of the last part of
<br>
my original &quot;meat&quot; host like the removal of an appendix is considered
<br>
now. I might keep the parts for sentimental value or to build a robot
<br>
shell with.
<br>
<p>What do I think of option 1 ?
<br>
I think the upload is just a copy (regardless of their personal
<br>
philosophical opinions). Should such copies have the full suite of
<br>
legal etc rights given to sentients in whatever society they are part
<br>
of, Yes.
<br>
I think the original is an entirely separate person logically and legally.
<br>
Them arranging their own death (by whatever means) would be suicide.
<br>
Forceably arranging their death against their will (their will, that
<br>
is, in the last moment of their lucid informed consciousness before
<br>
their death) would be some variation on murder/execution.
<br>
I don't think the time gap between the upload creation and the
<br>
original's destruction have any bearing on this.
<br>
Make it zero or 5.391 24(27)×10^−44 sec or a year, doesn't matter.
<br>
<p>It is easy to imagine a scenario (under the above assumptions) where
<br>
the upload could be legally charged with the murder of their
<br>
&quot;original&quot;. There are lots of possible macabre and entertaining
<br>
scenarios but I consider them &quot;domestic stories&quot; and probably not
<br>
relevant to your question.
<br>
<p>Were a type 1 upload made of me involuntarily (I would not permit it)
<br>
and I survived, I would consider them to be a sort of weird off-spring
<br>
or distant relation. I would not consider them as having any property
<br>
rights over any of my physical or informational assets.
<br>
I would be of the opinion that there should be legal societal rules to
<br>
handle such an awkward situation. The onus would be on them to change
<br>
their life as much as required to minimise harm to both parties. I
<br>
don't think they should be mind wiped or anything like that. It would
<br>
be a horrendously complex and challenging personal, ethical and legal
<br>
problem for both parties and society to manage.
<br>
<p>If a friend had an upload made and they survived, I would consider
<br>
their upload a different new person. I would likely avoid them
<br>
socially.
<br>
If a friend had an upload made and they didn't survive, I would
<br>
consider my friend dead, mourn them and probably avoid the upload to
<br>
preserve my own mental health.
<br>
<p>The really interesting thing is that if the upload were as good as
<br>
stipulated it would have all the same opinions as me on this matter.
<br>
<p>Obviously if both upload types were available and widely used there
<br>
would be political conflict between the proponents of each.
<br>
<p>Best Regards
<br>
John McNamara
<br>
<p>VOTE THESEUS PARTY !
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20811.html">Frank Adamek: "Re: [sl4] Uploading (was : goals of AI)"</a>
<li><strong>Previous message:</strong> <a href="20809.html">Bradley Thomas: "RE: [sl4] JKC, are your views actually amenable to correction, ever?"</a>
<li><strong>In reply to:</strong> <a href="20795.html">Matt Mahoney: "[sl4] Uploading (was : goals of AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20814.html">John K Clark: "Re: [sl4] Uploading"</a>
<li><strong>Reply:</strong> <a href="20814.html">John K Clark: "Re: [sl4] Uploading"</a>
<li><strong>Reply:</strong> <a href="20818.html">Stathis Papaioannou: "Re: [sl4] Uploading (was : goals of AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20810">[ date ]</a>
<a href="index.html#20810">[ thread ]</a>
<a href="subject.html#20810">[ subject ]</a>
<a href="author.html#20810">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
