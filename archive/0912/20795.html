<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: [sl4] Uploading (was : goals of AI)</title>
<meta name="Author" content="Matt Mahoney (matmahoney@yahoo.com)">
<meta name="Subject" content="[sl4] Uploading (was : goals of AI)">
<meta name="Date" content="2009-12-01">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>[sl4] Uploading (was : goals of AI)</h1>
<!-- received="Tue Dec  1 10:41:49 2009" -->
<!-- isoreceived="20091201174149" -->
<!-- sent="Tue, 1 Dec 2009 09:41:43 -0800 (PST)" -->
<!-- isosent="20091201174143" -->
<!-- name="Matt Mahoney" -->
<!-- email="matmahoney@yahoo.com" -->
<!-- subject="[sl4] Uploading (was : goals of AI)" -->
<!-- id="924363.25205.qm@web51901.mail.re2.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="6362eea20912010635r5ce1da01w9374637fba52b23e@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Matt Mahoney (<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI)"><em>matmahoney@yahoo.com</em></a>)<br>
<strong>Date:</strong> Tue Dec 01 2009 - 10:41:43 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20796.html">Robin Lee Powell: "[sl4] JKC, are your views actually amenable to correction, ever?"</a>
<li><strong>Previous message:</strong> <a href="20794.html">Matt Paul: "Re: [sl4] Re: goals of AI"</a>
<li><strong>In reply to:</strong> <a href="20782.html">John McNamara: "Re: [sl4] Re: goals of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20799.html">Johnicholas Hines: "Re: [sl4] Uploading (was : goals of AI)"</a>
<li><strong>Reply:</strong> <a href="20799.html">Johnicholas Hines: "Re: [sl4] Uploading (was : goals of AI)"</a>
<li><strong>Reply:</strong> <a href="20807.html">Mike Dougherty: "Re: [sl4] Uploading (was : goals of AI)"</a>
<li><strong>Reply:</strong> <a href="20810.html">John McNamara: "Re: [sl4] Uploading (was : goals of AI)"</a>
<li><strong>Maybe reply:</strong> <a href="20812.html">lispunit_benrayfield@audivolv.com: "Re: [sl4] Uploading (was : goals of AI)"</a>
<li><strong>Maybe reply:</strong> <a href="20848.html">Frank Adamek: "Re: [sl4] Uploading (was : goals of AI)"</a>
<li><strong>Maybe reply:</strong> <a href="20895.html">Frank Adamek: "Re: [sl4] Uploading (was : goals of AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20795">[ date ]</a>
<a href="index.html#20795">[ thread ]</a>
<a href="subject.html#20795">[ subject ]</a>
<a href="author.html#20795">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
John McNamara wrote:
<br>
<em>&gt; What is the maximum tolerable error that will not result in the failure of your engineering project (ie upload of a live human with no apparent deviations from expected normal thinking patterns (including fuzzy things like emotions/inspiration etc) for at least 1000 years with 99.9999 confidence level etc etc).
</em><br>
<p>Suppose there was a program that simulated you so well that nobody could tell the difference between you and the program in a Turing test environment. What is the probability that the program will be you after you shoot yourself?
<br>
<p>&nbsp;-- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI)">matmahoney@yahoo.com</a>
<br>
<p><p><p><p>________________________________
<br>
From: John McNamara &lt;<a href="mailto:harlequin@novastar.org?Subject=Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI)">harlequin@novastar.org</a>&gt;
<br>
To: <a href="mailto:sl4@sl4.org?Subject=Re:%20[sl4]%20Uploading%20(was%20:%20goals%20of%20AI)">sl4@sl4.org</a>
<br>
Sent: Tue, December 1, 2009 9:35:55 AM
<br>
Subject: Re: [sl4] Re: goals of AI
<br>
<p>Hi Folks,
<br>
<p>First post to list (braces for the bullet) and observation on this thread's debate.
<br>
<p>To me this sounds like a matter of simulation resolution.
<br>
<p>A human mind is the information output 'artefact' of a physical system.
<br>
We can choose to simulate that physical system over a wide range between the following 2 extremes.
<br>
a: extreme low resolution
<br>
1 bit, 1=mind is 'on' 0 = mind is 'off'
<br>
only useful in financial accounting obviously
<br>
<p>b: extreme high resolution
<br>
simulate using _all_ information on the system
<br>
as we don't have a final complete physics theory of everything we obviously cannot even determine if this is even possible
<br>
it would effectively be an absolutely perfect simulation of actual physical reality all the way down past quarks, strings and n dimensions to whatever idea is at the very bottom.
<br>
a bit beyond sl4 i suspect
<br>
<p>between a and b is a large range
<br>
it's possible (pending TOE) that b resolution simulation guarantees a zero error rate in the simulation output, the mind. Barring non-physical influences that would leave no wiggle-room left to say that the simulation isn't perfect in every way.
<br>
<p>Any level of simulation below b introduces errors in the output data all the way down to having just 1 bit of reliable data at simulation level a
<br>
<p>Therefore its a matter of deciding the acceptable error level for your objective.
<br>
All you need is a mastery of the physics and math required to get down to your required level of acceptable error rate.
<br>
<p>b involves all sorts of things we not good at like probability and infinities.
<br>
<p>There are 2 separate questions here.
<br>
<p>1
<br>
Is any non-zero error acceptable in principle ?
<br>
This is a philosophical question I think, not an engineering one.
<br>
<p><p><p>2
<br>
What is the maximum tolerable error that will not result in the failure of your engineering project (ie upload of a live human with no apparent deviations from expected normal thinking patterns (including fuzzy things like emotions/inspiration etc) for at least 1000 years with 99.9999 confidence level etc etc).
<br>
This is a practical engineering problem for a branch of engineering that doesn't exist yet.
<br>
<p>my answers for the curious
<br>
1
<br>
I'm not comfortable with a non-zero error I must admin, now that it's &quot;on the menu&quot; so to speak. That said, the now pre-upload me would jump at any error rate accepted by sane-looking engineering types as an alternative to oblivion. I wouldn't be surprised if the post-upload me wanted a lot of virtual beer to get over the whole thing.
<br>
<p><p>2
<br>
no idea, but I wouldn't be stunned to learn that something more detailed than neural charge levels was required. Which would be unfortunate because it would be harder. Perhaps I'm pessimistic on this one.
<br>
<p><p>Apologies if this has wandered off-topic.
<br>
<p>Best Regards
<br>
John Mc Namara
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20796.html">Robin Lee Powell: "[sl4] JKC, are your views actually amenable to correction, ever?"</a>
<li><strong>Previous message:</strong> <a href="20794.html">Matt Paul: "Re: [sl4] Re: goals of AI"</a>
<li><strong>In reply to:</strong> <a href="20782.html">John McNamara: "Re: [sl4] Re: goals of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20799.html">Johnicholas Hines: "Re: [sl4] Uploading (was : goals of AI)"</a>
<li><strong>Reply:</strong> <a href="20799.html">Johnicholas Hines: "Re: [sl4] Uploading (was : goals of AI)"</a>
<li><strong>Reply:</strong> <a href="20807.html">Mike Dougherty: "Re: [sl4] Uploading (was : goals of AI)"</a>
<li><strong>Reply:</strong> <a href="20810.html">John McNamara: "Re: [sl4] Uploading (was : goals of AI)"</a>
<li><strong>Maybe reply:</strong> <a href="20812.html">lispunit_benrayfield@audivolv.com: "Re: [sl4] Uploading (was : goals of AI)"</a>
<li><strong>Maybe reply:</strong> <a href="20848.html">Frank Adamek: "Re: [sl4] Uploading (was : goals of AI)"</a>
<li><strong>Maybe reply:</strong> <a href="20895.html">Frank Adamek: "Re: [sl4] Uploading (was : goals of AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20795">[ date ]</a>
<a href="index.html#20795">[ thread ]</a>
<a href="subject.html#20795">[ subject ]</a>
<a href="author.html#20795">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
