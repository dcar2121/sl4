<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Singularity Objections: Singularity, Desirability</title>
<meta name="Author" content="Thomas McCabe (pphysics141@gmail.com)">
<meta name="Subject" content="Singularity Objections: Singularity, Desirability">
<meta name="Date" content="2008-01-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Singularity Objections: Singularity, Desirability</h1>
<!-- received="Tue Jan 29 13:36:59 2008" -->
<!-- isoreceived="20080129203659" -->
<!-- sent="Tue, 29 Jan 2008 15:34:39 -0500" -->
<!-- isosent="20080129203439" -->
<!-- name="Thomas McCabe" -->
<!-- email="pphysics141@gmail.com" -->
<!-- subject="Singularity Objections: Singularity, Desirability" -->
<!-- id="b7a9e8680801291234v6aa3e2ccx6fbd61d86e0d1b5c@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Thomas McCabe (<a href="mailto:pphysics141@gmail.com?Subject=Re:%20Singularity%20Objections:%20Singularity,%20Desirability"><em>pphysics141@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Jan 29 2008 - 13:34:39 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17546.html">Thomas McCabe: "Singularity Objections: General AI, Consciousness"</a>
<li><strong>Previous message:</strong> <a href="17544.html">Thomas McCabe: "Singularity Objections: Singularity, exponential growth"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17545">[ date ]</a>
<a href="index.html#17545">[ thread ]</a>
<a href="subject.html#17545">[ subject ]</a>
<a href="author.html#17545">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Desirability / getting there
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* There's no reason for anybody to want to build a superhuman AI.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: A superintelligent AI, by definition,
<br>
would be able to do *anything* faster than any human can. No matter
<br>
what you want to do, superintelligent AI can help you do it better.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* A Singularity through uploading/BCI would be more feasible/desirable.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Life would have no meaning in a universe with AI/advanced
<br>
nanotech (see Bill McKibben).
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: If we wanted to, we could always choose
<br>
not to use advanced technologies, or just keep them running in the
<br>
background to protect us from asteroids and what not.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* A real AI would turn out just like (insert scenario from sci-fi
<br>
book or movie).
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Science fiction is entertainment, not
<br>
an actual prediction of how things will turn out. You can't generalize
<br>
from fictional evidence.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Technology has given us nuclear bombs/industrial slums/etc.; the
<br>
future should involve less technology, not more.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Because of technology, the average
<br>
quality of life is much, much better than it was (say) a thousand
<br>
years ago. If we wanted to, we could throw out all of our computers
<br>
and cellphones tomorrow. We choose not to, because we know that
<br>
technology improves our lives.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* We might live in a computer simulation and it might be too
<br>
computationally expensive for our simulators to simulate our world
<br>
post-Singularity.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: This scenario can be used to argue for,
<br>
or against, any idea whatsoever. For idea X, just say &quot;What if the
<br>
simulators killed us if we did X?&quot;, or &quot;What if the simulators killed
<br>
us if we didn't do X?&quot;.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* AI is too long-term a project, we should focus on short-term
<br>
goals like curing cancer.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: AI could actually wind up being easier
<br>
than curing cancer, at least in terms of money and man-hours involved.
<br>
And the impact of AI is huge- it could cure every disease known to
<br>
humankind, as well as solve a whole bunch of other problems.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Unraveling the mystery of intelligence would demean the value of
<br>
human uniqueness.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Many, many scientific advances have
<br>
made humans seem less special (Copernicus, Darwin, etc.) With
<br>
hindsight, we still see these advances as good things.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* If this was as good as it sounds, someone else would already be
<br>
working on it.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Every great idea was passed over
<br>
thousands of times before someone got around to working on it. Every
<br>
new startup company depends on the principle that an idea can be good,
<br>
and yet not taken by someone else. And startups are now one of the
<br>
main drivers of our economy- all five of the Internet's most-visited
<br>
websites were originally startup companies.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Singularity utopias are all written in an elite Western
<br>
intellectual culture: a Singularity and machines taking over will
<br>
threaten the diversity of other forms of thought, such as religion and
<br>
less technology-based cultures.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o With the Internet, we have seen an explosion of different
<br>
subcultures, including religious and even anti-technology ones -
<br>
better communications allow ideas to spread faster, and a Singularity
<br>
is likely to do that even more effectively. Furthermore, a
<br>
Friendliness model such as CEV would preserve existing cultures to the
<br>
extent that humans ultimately wish to see them preserved.
<br>
<p>&nbsp;- TOm
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17546.html">Thomas McCabe: "Singularity Objections: General AI, Consciousness"</a>
<li><strong>Previous message:</strong> <a href="17544.html">Thomas McCabe: "Singularity Objections: Singularity, exponential growth"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17545">[ date ]</a>
<a href="index.html#17545">[ thread ]</a>
<a href="subject.html#17545">[ subject ]</a>
<a href="author.html#17545">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:01 MDT
</em></small></p>
</body>
</html>
