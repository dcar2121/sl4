<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Singularity Objections: Friendliness, feasibility</title>
<meta name="Author" content="Thomas McCabe (pphysics141@gmail.com)">
<meta name="Subject" content="Singularity Objections: Friendliness, feasibility">
<meta name="Date" content="2008-01-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Singularity Objections: Friendliness, feasibility</h1>
<!-- received="Tue Jan 29 13:41:38 2008" -->
<!-- isoreceived="20080129204138" -->
<!-- sent="Tue, 29 Jan 2008 15:39:19 -0500" -->
<!-- isosent="20080129203919" -->
<!-- name="Thomas McCabe" -->
<!-- email="pphysics141@gmail.com" -->
<!-- subject="Singularity Objections: Friendliness, feasibility" -->
<!-- id="b7a9e8680801291239w3c7b7f9ag1f17e6964499eb1c@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Thomas McCabe (<a href="mailto:pphysics141@gmail.com?Subject=Re:%20Singularity%20Objections:%20Friendliness,%20feasibility"><em>pphysics141@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Jan 29 2008 - 13:39:19 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17551.html">Thomas McCabe: "Singularity Objections: Friendliness, alternatives"</a>
<li><strong>Previous message:</strong> <a href="17549.html">Thomas McCabe: "Singularity Objections: Friendliness, activism"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17550">[ date ]</a>
<a href="index.html#17550">[ thread ]</a>
<a href="subject.html#17550">[ subject ]</a>
<a href="author.html#17550">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;Feasibility of the concept
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Ethics are subjective, not objective: therefore no truly
<br>
Friendly AI can be built.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: If ethics are subjective, we can still
<br>
build a Friendly AI: we just need to program in our collective
<br>
(human-derived) morality, not some external objective morality.
<br>
<p>[edit]
<br>
The idea of a hostile AI is anthropomorphic.
<br>
<p>There is no reason to assume that an AI would be actively hostile, no.
<br>
However, as AIs can become very powerful, their indifference (if they
<br>
haven't purposefully been programmed to be Friendly, that is) becomes
<br>
dangerous in itself. Humans are not actively hostile towards the
<br>
animals living in a forest when they burn down the forest and build
<br>
luxury housing where it once stood. Or as Eliezer Yudkowsky put it:
<br>
the AI does not hate you, nor does it love you, but you are made out
<br>
of atoms which it can use for something else.
<br>
<p>The vast majority of the time, if someone dies, it's not because of
<br>
murder - it's because of something accidental. Some random error in
<br>
DNA replication caused cancer, or some clump of fatty acid caused a
<br>
heart attack. Non-malevolent forces killed more people than every
<br>
genocide in history put together. Even during WWII, the single largest
<br>
mass-killing event in human history, more people died of &quot;natural
<br>
causes&quot; than were killed by government armies. The same principle
<br>
applies on a smaller scale; most of the daily annoyances we live with
<br>
aren't caused by deliberate malice.
<br>
<p>Were an AI not a threat to the very survival of humanity, it could
<br>
threaten our other values. Even among humans, there exist radical
<br>
philosophers whose ideas of a perfect society are repulsive to the
<br>
vast majority of the populace. Even an AI that was built to care about
<br>
many of the things humans value could ignore some values that are
<br>
taken for so granted that they are never programmed into it. This
<br>
could produce a society we considered very repulsive, even though our
<br>
survival was never at stake.
<br>
[edit]
<br>
&quot;Friendliness&quot; is too vaguely defined.
<br>
<p>This is true, because Friendly AI is currently an open research
<br>
subject. It's not that we don't know how it should be implemented,
<br>
it's that we don't even know what exactly should be implemented. If
<br>
anything, this is a reason to spend more resources studying the
<br>
problem.
<br>
<p>Some informal proposals for defining Friendliness do exist. None of
<br>
these are meant to be conclusive - they are open to criticism and are
<br>
subject to change as new information is gathered. The one that
<br>
currently seems most promising is called Coherent Extrapolated
<br>
Volition. In the CEV proposal, an AI will be built (or, to be exact, a
<br>
proto-AI will be built to program another) to extrapolate what the
<br>
ultimate desires of all the humans in the world would be if those
<br>
humans knew everything a superintelligent being could potentially
<br>
know; could think faster and smarter; were more like they wanted to be
<br>
(more altruistic, more hard-working, whatever your ideal self is);
<br>
would have lived with other humans for a longer time; had mainly those
<br>
parts of themselves taken into account that they wanted to be taken
<br>
into account. The ultimate desire - the volition - of everyone is
<br>
extrapolated, with the AI then beginning to direct humanity towards a
<br>
future where everyone's volitions are fulfilled in the best manner
<br>
possible. The desirability of the different futures is weighted by the
<br>
strength of humanity's desire - a smaller group of people with a very
<br>
intense desire to see something happen may &quot;overrule&quot; a larger group
<br>
who'd slightly prefer the opposite alternative but doesn't really care
<br>
all that much either way. Humanity is not instantly &quot;upgraded&quot; to the
<br>
ideal state, but instead gradually directed towards it.
<br>
<p>CEV avoids the problem of its programmers having to define the wanted
<br>
values exactly, as it draws them directly out of the minds of people.
<br>
Likewise it avoids the problem of confusing ends with means, as it'll
<br>
explicitly model society's development and the development of
<br>
different desires as well. Everybody who thinks their favorite
<br>
political model happens to objectively be the best in the world for
<br>
everyone should be happy to implement CEV - if it really turns out
<br>
that it is the best one in the world, CEV will end up implementing it.
<br>
(Likewise, if it is the best for humanity that an AI stays mostly out
<br>
of its affairs, that will happen as well.) A perfect implementation of
<br>
CEV is unbiased in the sense that it will produce the same kind of
<br>
world regardless of who builds it, and regardless of what their
<br>
ideology happens to be - assuming the builders are intelligent enough
<br>
to avoid including their own empirical beliefs (aside for the bare
<br>
minimum required for the mind to function) into the model, and trust
<br>
that if they are correct, the AI will figure them out on its own.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Mainstream researchers don't consider Friendliness an issue.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Mainstream researchers don't have a
<br>
very good record of carefully thinking out the implications of future
<br>
technologies. Even during the Manhattan Project, few of the scientists
<br>
took the time to think about- in detail- the havoc the bomb would
<br>
wreak twenty years down the road. FAIs are much more difficult to
<br>
understand than atomic bombs, and so if anything, the problem will be
<br>
worse.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Human morals/ethics contradict each other, even within individuals.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: We, as humans, have a common enough
<br>
morality to build a system of laws. We share almost all of our brain
<br>
hardware, and we all have most of the same basic drives from
<br>
evolutionary psychology. In fact, within any given society, the moral
<br>
common ground usually far exceeds the variance between any two people.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Most humans are rotten bastards and so basing an FAI morality
<br>
off of human morality is a bad idea anyway.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Eli listed this as a real possibility
<br>
in CEV, so we'll need a serious, possibly technical answer. - Tom
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* The best way to make us happy would be to constantly stimulate
<br>
our pleasure centers, turning us into nothing but experiencers of
<br>
constant orgasms.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Most people would find this morally
<br>
objectionable, and a CEV or CEV-like system would act on our
<br>
objections and prevent this from happening.
<br>
<p>&nbsp;- Tom
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17551.html">Thomas McCabe: "Singularity Objections: Friendliness, alternatives"</a>
<li><strong>Previous message:</strong> <a href="17549.html">Thomas McCabe: "Singularity Objections: Friendliness, activism"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17550">[ date ]</a>
<a href="index.html#17550">[ thread ]</a>
<a href="subject.html#17550">[ subject ]</a>
<a href="author.html#17550">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:01 MDT
</em></small></p>
</body>
</html>
