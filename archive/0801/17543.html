<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Singularity Objections: Singularity, Intelligence</title>
<meta name="Author" content="Thomas McCabe (pphysics141@gmail.com)">
<meta name="Subject" content="Singularity Objections: Singularity, Intelligence">
<meta name="Date" content="2008-01-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Singularity Objections: Singularity, Intelligence</h1>
<!-- received="Tue Jan 29 13:36:13 2008" -->
<!-- isoreceived="20080129203613" -->
<!-- sent="Tue, 29 Jan 2008 15:33:54 -0500" -->
<!-- isosent="20080129203354" -->
<!-- name="Thomas McCabe" -->
<!-- email="pphysics141@gmail.com" -->
<!-- subject="Singularity Objections: Singularity, Intelligence" -->
<!-- id="b7a9e8680801291233u32fd52e7o569ce149b1fd3550@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Thomas McCabe (<a href="mailto:pphysics141@gmail.com?Subject=Re:%20Singularity%20Objections:%20Singularity,%20Intelligence"><em>pphysics141@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Jan 29 2008 - 13:33:54 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17544.html">Thomas McCabe: "Singularity Objections: Singularity, exponential growth"</a>
<li><strong>Previous message:</strong> <a href="17542.html">Thomas McCabe: "Singularity Objections: SIAI, General Objections"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17583.html">Thomas Buckner: "Re: Singularity Objections: Singularity, Intelligence"</a>
<li><strong>Reply:</strong> <a href="17583.html">Thomas Buckner: "Re: Singularity Objections: Singularity, Intelligence"</a>
<li><strong>Maybe reply:</strong> <a href="../0802/17587.html">lucassheehan@gmail.com: "Re: Singularity Objections: Singularity, Intelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17543">[ date ]</a>
<a href="index.html#17543">[ thread ]</a>
<a href="subject.html#17543">[ subject ]</a>
<a href="author.html#17543">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Intelligence isn't everything
<br>
[edit]
<br>
An AI still wouldn't have the resources of humanity.
<br>
<p>Looking at early humans, one wouldn't have expected them to rise to a
<br>
dominant position based on their nearly nonexistent resources and only
<br>
a mental advantage over their environment. All advantages that had so
<br>
far been developed had been built-in ones - poison spikes, sharp
<br>
teeth, acute hearing, while humans had no extraordinary physical
<br>
capabilities. There was no reason to assume that a simple intellect
<br>
would help them out as much as it did.
<br>
<p>When discussing the threat of an advanced AI, it has at its disposal a
<br>
mental advantage over its environment and easy access to all the
<br>
resources it can hack, con or persuade its way to - potentially a lot,
<br>
given that humans are easy to manipulate. If an outside observer
<br>
couldn't have predicted the rise of humanity based on the information
<br>
available so far, and we are capable of coming up with plenty of ways
<br>
that an AI could rise into a position of power... how many ways must
<br>
there be for a superintelligent being to do so, that we aren't capable
<br>
of even imagining?
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Bacteria and insects are more numerous than humans.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Possible rebuttals:
<br>
<p>- Sheer population size isn't a reasonable measure of success. We
<br>
wouldn't consider it a success if the Earth was so jam-packed with
<br>
humans that there was barely enough food. Indeed, overpopulation is
<br>
already considered a serious problem in many countries, particularly
<br>
in non-industrialized nations where birth control isn't readily
<br>
available.
<br>
<p>- Bacteria and insects took hundreds of millions of years to grow and
<br>
adapt to the huge range of environments they currently inhabit. Modern
<br>
man has been around for less than .1% of that timespan, yet we have
<br>
increased our numbers faster than any other species in history.
<br>
<p>- Bacteria, insects, fungi, protists, and other small organisms have
<br>
always existed in much larger numbers than mammals, regardless of how
<br>
successful the mammals were, primarily because of size.
<br>
<p>- Organisms with short life cycles reproduce much faster than
<br>
organisms with long cycles (r-strategy vs. K-strategy). If the human
<br>
race had reproduced faster, people would die much faster than they do
<br>
now.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Superminds won't be solving The Meaning Of Life or breaking the
<br>
laws of physics.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: The Meaning Of Life has already been
<br>
solved (link). As for the laws of physics, we can far exceed the
<br>
bounds of today's civilization without breaking any of them. There's
<br>
no physical law saying humans have to get cancer or travel at .00001%
<br>
of c.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Just because you can think a million times faster doesn't mean
<br>
you can do experiments a million times faster; super AI will not
<br>
invent super nanotech three hours after it awakens.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: All of the world's major laboratories
<br>
are now computerized, and computers aren't secure from human hackers,
<br>
much less superintelligent AIs.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Machines will never be placed in positions of power.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o (Kurzweil paraphrase): If, tomorrow, all the computers in
<br>
the world shut down, the entire planet would be thrown into utter
<br>
chaos. Basic utilities like electricity, cable, phone, etc. would all
<br>
fail. Most cars wouldn't even start. You couldn't get paid, or use a
<br>
credit card, or take money out of a bank. The trillions of dollars
<br>
transferred globally on a daily basis would come to a screeching halt.
<br>
The government and the military would be crippled, unable to
<br>
communicate or take action. All investments, from stocks, to bonds, to
<br>
mutual funds, would suddenly disappear. And on and on it goes...
<br>
<p>[edit]
<br>
On an Intelligence Explosion
<br>
[edit]
<br>
There are limits to everything. You can't get infinite growth.
<br>
<p>For one, this is mainly an objection against the Accelerating Change
<br>
interpretation of the Singularity, most famously advanced by Ray
<br>
Kurzweil. When talking about the Singularity, many people are in fact
<br>
referring to the &quot;Intelligence Explosion&quot; or &quot;Event Horizon&quot;
<br>
interpretations, which are the ones this article is mainly concerned
<br>
with. Neither of these requires infinite growth - they only require us
<br>
to be able to create minds which are smarter than humans. Secondly,
<br>
even Kurzweil's interpretation doesn't contain infinite anything -
<br>
&quot;there are limits, but they are not very limiting&quot;, is what he has
<br>
been quoted saying.
<br>
<p>(Add also: Infinite growth isn't necessary; the potential for finite
<br>
growth is enormous. Even right here on Earth, there's 6 * 10^24 kg of
<br>
available matter; almost none of it has ever been used for anything.
<br>
Could also link to different articles estimating the physical limits
<br>
of computation, or the limits of processing power theoretically
<br>
obtainable with technologies currently under development.)
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* A smarter being is also more complex, and thus cannot
<br>
necessarily improve itself any faster than the previous stage -- no
<br>
exponential spiral.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Does anyone have counter-evidence? This looks like a real
<br>
possibility. - Tom
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Computation takes power. Fast super AI will probably draw
<br>
red-hot power for questionable benefit. (Also, so far fast serial
<br>
computation takes far more power than slow parallel computation
<br>
(brains).)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Power consumption per FLOP has gone
<br>
down with Moore's Law like every other parameter, and there's no
<br>
reason to believe this trend will stop. Human brains only consume
<br>
about 1% of the power currently used by human civilization.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Giant computers and super AI can be obedient tools as easily as
<br>
they can be free-willed rogues, so there's no reason to think humans+
<br>
loyal AI will be upstaged by rogues. The bigger the complex
<br>
intelligence, the less it matters that one part of the complex
<br>
intelligence is a slow meat-brain.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o I don't understand this objection. - Tom
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+ It's basically saying that AI can be used for good
<br>
as well as bad, and there's no reason to assume that the bad designs
<br>
will beat the good ones. Might be useful to say something about a
<br>
first-mover advantage, as well about the fact that it's incredibly
<br>
hard to get right a mind design whose wishes are anywhere near what
<br>
humans would want them to be... - Kaj
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Biology gives us no reason to believe in hard transitions or
<br>
steep levels of intelligence. Computer science does, but puts the
<br>
Singularity as having happened back when language was developed.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: The average human has a frontal cortex
<br>
only around six times larger than the average chimpanzee, and yet the
<br>
result of that change has been huge (civilization, nuclear bombs,
<br>
etc.).
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Strong Drexlerian nanotech seems to be bunk in the mind of most
<br>
chemists, and there's no reason to think AI have any trump advantage
<br>
with regard to it.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Nanosystems, Eric Drexler's 1992
<br>
technical book on nanotechnology, has never been found to contain a
<br>
significant error (link).
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* There is a fundamental limit on intelligence, somewhere close to
<br>
or only slightly above the human level. (Strong AI Footnotes)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o It seems that counter-evidence exists, but I haven't seen it. - Tom
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+ One could note that simply making faster processors
<br>
or larger stores of memory will make a mind more intelligent, and that
<br>
our brains are nowhere near the physical limits for either. Like here,
<br>
for instance. - Kaj
<br>
<p>[edit]
<br>
On Intelligence
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* You can't build a superintelligent machine when we can't even
<br>
define what intelligence means.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Even if we can't define intelligence
<br>
precisely yet, in practical terms, we know what it does: intelligence
<br>
provides optimization power to reshape the world. There are plenty of
<br>
things that could give one more optimization power, for instance,
<br>
faster processing and more memory.
<br>
<p>[edit]
<br>
Intelligence is not linear or one-dimensional, so talking about
<br>
greater- or below-human intelligences doesn't make sense.
<br>
[edit]
<br>
Talking about human-equivalent AI is pointless. A computer mind would
<br>
of necessity be much smarter than humans in some fields: for instance,
<br>
in the field of doing multiplication or addition. Creating a truly
<br>
&quot;human-equivalent&quot; AI would require needless work and involve
<br>
essentially crippling the AI.
<br>
<p>It is true that intelligence is hard to measure with a single, linear
<br>
variable. It is also true that it will probably take a long time
<br>
before there is truly human-equivalent AI, just as there is no
<br>
bird-level flight: humans will have their own strong sides, while AIs
<br>
will have their own strong sides. A simple calculator is already
<br>
superintelligent, if speed of multiplication is the only thing being
<br>
measured.
<br>
<p>However, there are such things as rough human-equivalence and rough
<br>
below-human equivalence. No human adult has exactly the same
<br>
capabilities, yet we still speak of adult-level intelligence. A
<br>
calculator might be superintelligent in a single field, but obviously
<br>
no manager would hire a calculator to be trained as an accountant, nor
<br>
would he hire a monkey. A &quot;human-level intelligence&quot; simply means a
<br>
mind that is roughly capable of learning and carrying out the things
<br>
that humans are capable of learning and doing. It does not mean that
<br>
we'd be aiming to build an AI with exactly the same capabilities as a
<br>
human mind. Likewise, a &quot;superhuman intelligence&quot; is a mind that can
<br>
do all the things humans can at least at a roughly equivalent level,
<br>
as well being considerably better in many of them.
<br>
<p>&nbsp;- Tom
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17544.html">Thomas McCabe: "Singularity Objections: Singularity, exponential growth"</a>
<li><strong>Previous message:</strong> <a href="17542.html">Thomas McCabe: "Singularity Objections: SIAI, General Objections"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17583.html">Thomas Buckner: "Re: Singularity Objections: Singularity, Intelligence"</a>
<li><strong>Reply:</strong> <a href="17583.html">Thomas Buckner: "Re: Singularity Objections: Singularity, Intelligence"</a>
<li><strong>Maybe reply:</strong> <a href="../0802/17587.html">lucassheehan@gmail.com: "Re: Singularity Objections: Singularity, Intelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17543">[ date ]</a>
<a href="index.html#17543">[ thread ]</a>
<a href="subject.html#17543">[ subject ]</a>
<a href="author.html#17543">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:01 MDT
</em></small></p>
</body>
</html>
