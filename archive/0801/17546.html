<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Singularity Objections: General AI, Consciousness</title>
<meta name="Author" content="Thomas McCabe (pphysics141@gmail.com)">
<meta name="Subject" content="Singularity Objections: General AI, Consciousness">
<meta name="Date" content="2008-01-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Singularity Objections: General AI, Consciousness</h1>
<!-- received="Tue Jan 29 13:38:51 2008" -->
<!-- isoreceived="20080129203851" -->
<!-- sent="Tue, 29 Jan 2008 15:36:24 -0500" -->
<!-- isosent="20080129203624" -->
<!-- name="Thomas McCabe" -->
<!-- email="pphysics141@gmail.com" -->
<!-- subject="Singularity Objections: General AI, Consciousness" -->
<!-- id="b7a9e8680801291236w57f2bef8w5200469f457334db@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Thomas McCabe (<a href="mailto:pphysics141@gmail.com?Subject=Re:%20Singularity%20Objections:%20General%20AI,%20Consciousness"><em>pphysics141@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Jan 29 2008 - 13:36:24 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17547.html">Thomas McCabe: "Singularity Objections: General AI, Implementation"</a>
<li><strong>Previous message:</strong> <a href="17545.html">Thomas McCabe: "Singularity Objections: Singularity, Desirability"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17580.html">Thomas Buckner: "Re: Singularity Objections: General AI, Consciousness"</a>
<li><strong>Reply:</strong> <a href="17580.html">Thomas Buckner: "Re: Singularity Objections: General AI, Consciousness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17546">[ date ]</a>
<a href="index.html#17546">[ thread ]</a>
<a href="subject.html#17546">[ subject ]</a>
<a href="author.html#17546">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Consciousness
<br>
<p>A general rebuttal against most of these could be that consciousness
<br>
isn't strictly necessary for AI.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Computation isn't a sufficient prerequisite for consciousness.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Every atom in the human brain obeys the
<br>
laws of physics. These laws are very well understood, and can be
<br>
modeled on any Turing-complete computer system with enough RAM and
<br>
processing power. With sufficient resolution, you could simulate the
<br>
entire brain this way, atom-by-atom. We know that the real atoms and
<br>
the simulated atoms behave identically; the real person and the
<br>
simulated person should, therefore, also behave identically (allowing
<br>
for quantum randomness). Hence, as long as no supernatural or
<br>
spiritual elements are involved, it *must* be possible to build a
<br>
conscious entity inside a computer.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* A computer can never really understand the world the way humans
<br>
can. (Searle's Chinese Room)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: This idea is mainly the result of
<br>
previous, abandoned AI projects, where (say) a cow was represented by
<br>
a single string variable, &quot;COW&quot;. Obviously, using the word &quot;COW&quot; isn't
<br>
going to make the computer understand the full range of experiences we
<br>
associate with real-life cows. However, this problem is specific to
<br>
old-fashioned AI systems, *not* AIs or computers in general.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Human consciousness requires quantum computing, and so no
<br>
conventional computer could match the human brain.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Human neurons are fairly well
<br>
understood, and so far, there's no evidence for any kind of quantum
<br>
computation within the brain. A quantum computer needs to be kept
<br>
isolated from any disturbances to avoid wave function collapse, and
<br>
any atom in the brain is bombarded constantly by photons and other
<br>
atoms.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Human consciousness requires holonomic properties.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: This is still a fringe hypothesis, and
<br>
goes against the bulk of currently-accepted neuroscience.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* A brain isn't enough for an intelligent mind - you also need a
<br>
body/emotions/society.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Humans need a body, emotions, and
<br>
society to function, but there's no real reason an AI would need them.
<br>
AIs and humans are vastly different from each other, and what applies
<br>
to one doesn't automatically translate to the other.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Secondary rebuttal: Even an AI did need a body, there are
<br>
plenty of simulated environments where something like a body could be
<br>
provided. AIs will automatically have a society of sorts, by virtue of
<br>
interacting with the researchers building them.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* As a purely subjective experience, consciousness cannot be
<br>
studied in a reductionist/outside way, nor can its presence be
<br>
verified. (in more detail)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: As in quantum mechanics, we don't need
<br>
to be concerned with unverifiable philosophies. What we can verify is
<br>
that intelligence has had a huge impact on the world, and any increase
<br>
in the level of available intelligence will have huge consequences for
<br>
the human species.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* A computer, even if it could think, wouldn't have human
<br>
intuition and so would be much less capable in many situations.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Human intuition, although it may seem
<br>
mysterious to us, is based on a physical network of subconscious
<br>
memories and observations. This network has been studied by cognitive
<br>
scientists, and there's no reason why it couldn't be programmed in if
<br>
necessary.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* If we do not properly understand feelings and qualia, we could
<br>
accidentally cause our AI systems to suffer immensely when they were
<br>
being developed.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o This is an engineering problem. It *is* true that we could
<br>
cause immense suffering if we screw up. - Tom
<br>
<p>&nbsp;- TOm
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17547.html">Thomas McCabe: "Singularity Objections: General AI, Implementation"</a>
<li><strong>Previous message:</strong> <a href="17545.html">Thomas McCabe: "Singularity Objections: Singularity, Desirability"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17580.html">Thomas Buckner: "Re: Singularity Objections: General AI, Consciousness"</a>
<li><strong>Reply:</strong> <a href="17580.html">Thomas Buckner: "Re: Singularity Objections: General AI, Consciousness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17546">[ date ]</a>
<a href="index.html#17546">[ thread ]</a>
<a href="subject.html#17546">[ subject ]</a>
<a href="author.html#17546">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:01 MDT
</em></small></p>
</body>
</html>
