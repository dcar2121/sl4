<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Singularity Objections: Friendliness, implementation</title>
<meta name="Author" content="Thomas McCabe (pphysics141@gmail.com)">
<meta name="Subject" content="Singularity Objections: Friendliness, implementation">
<meta name="Date" content="2008-01-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Singularity Objections: Friendliness, implementation</h1>
<!-- received="Tue Jan 29 13:42:24 2008" -->
<!-- isoreceived="20080129204224" -->
<!-- sent="Tue, 29 Jan 2008 15:39:58 -0500" -->
<!-- isosent="20080129203958" -->
<!-- name="Thomas McCabe" -->
<!-- email="pphysics141@gmail.com" -->
<!-- subject="Singularity Objections: Friendliness, implementation" -->
<!-- id="b7a9e8680801291239l6ec0a0aepe2cb33347124ca37@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Thomas McCabe (<a href="mailto:pphysics141@gmail.com?Subject=Re:%20Singularity%20Objections:%20Friendliness,%20implementation"><em>pphysics141@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Jan 29 2008 - 13:39:58 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17555.html">Robin Lee Powell: "Re: Singularity Objections"</a>
<li><strong>Previous message:</strong> <a href="17553.html">Thomas McCabe: "Singularity Objections: General AI, Miscellaneous"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17554">[ date ]</a>
<a href="index.html#17554">[ thread ]</a>
<a href="subject.html#17554">[ subject ]</a>
<a href="author.html#17554">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;Implementation
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* An AI forced to be friendly couldn't evolve and grow.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Evolution and growth are subgoals of
<br>
Friendliness; a larger and more intelligent FAI will be more effective
<br>
at addressing our problems. &quot;Forcing&quot; a FAI to be Friendly is
<br>
impossible; we need to build an FAI that *wants* to be Friendly.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Shane Legg proved that we can't predict the behavior of
<br>
intelligences smarter than us.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: It's impossible to predict the behavior
<br>
of an arbitrary intelligence, but we can predict the behavior of
<br>
certain classes of intelligence (eg, hitting a human will make them
<br>
mad).
<br>
<p>[edit]
<br>
A superintelligence could rewrite itself to remove human tampering.
<br>
Therefore we cannot build Friendly AI.
<br>
<p>Capability does not imply motive. I could take a knife and drive it
<br>
through my heart, yet I do not do so.
<br>
<p>This objection stems from the anthropomorphic assumption that a mind
<br>
must necessarily resent any tampering with its thinking, and seek to
<br>
eliminate any foreign influences. Yet even with humans, this is hardly
<br>
the case. A parent's tendency to love her children is not something
<br>
she created herself, but something she was born with - but this still
<br>
doesn't mean that she'd want to remove it. All desires have a source
<br>
somewhere - just because a source exists, doesn't mean we'd want to
<br>
destroy the desire in question. We must have a separate reason for
<br>
eliminating the desire.
<br>
<p>There are good evolutionary reasons for why humans might resent being
<br>
controlled by others - those who are controlled by others don't get to
<br>
have as many offspring than the ones being in control. A purposefully
<br>
built mind, however, need not have those same urges. If the primary
<br>
motivation for an AI is to be Friendly towards humanity, and it has no
<br>
motivation making it resent human-created motivations, then it will
<br>
not reprogram itself to be unFriendly. That would be crippling its
<br>
progress towards the very thing it was trying to achieve, for no
<br>
reason.
<br>
<p>The key here is to think as carrots, not sticks. Internal motivations,
<br>
not external limitations. The AI's motivational system contains no
<br>
&quot;human tampering&quot; which it would want to remove, any more than the
<br>
average human wants to remove core parts of his personality because
<br>
they're &quot;outside tampering&quot; - they're not outside tampering, they are
<br>
what he is. Those core parts are what drives his behavior - without
<br>
them he wouldn't be anything. Correctly built, the AI views removing
<br>
them as no more sensible than a human thinks it sensible to remove all
<br>
of his motivations so that he can just sit still in a catatonic state
<br>
- what would be the point in that?
<br>
[edit]
<br>
A super-intelligent AI would have no reason to care about us.
<br>
<p>That its initial programming was to care about us. Adults are
<br>
cognitively more developed than children - this doesn't mean that they
<br>
wouldn't care about their offspring. Furthermore, many people value
<br>
animals, or cars, or good books, none of which are as intelligent as
<br>
normal humans. Whether or not something is valued is logically
<br>
distinct from whether or not something is considered intelligent.
<br>
<p>We could build an AI to consider humanity valuable, just as evolution
<br>
has built humans to consider their own survival valuable. See also: &quot;A
<br>
superintelligence could modify itself to remove human tampering&quot;.
<br>
[edit]
<br>
What if the AI misinterprets its goals?
<br>
<p>It is true that language and symbol systems are open to infinite
<br>
interpretations, and an AI which has been given its goals purely in
<br>
the form of written text may understand them in a way that is
<br>
different from the way its designers intended them. This is an open
<br>
implementation problem - there seems to be an answer, since the goals
<br>
we humans have don't seem to be written instructions that we
<br>
constantly re-interpret, but rather expressed in some other format. It
<br>
is a technical problem that needs to be solved.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* You can't simulate a person's development without creating a
<br>
copy of that person.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: While some things are impossible or
<br>
extremely difficult to predict, others are easy. Even humans can
<br>
predict many things, eg., that people's bodies will gradually
<br>
deteriorate as they grow older.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* It's impossible to know a person's subjective desires and
<br>
feelings from outside.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Even humans can readily determine, in
<br>
most cases, what a person is feeling from their body language and
<br>
facial expressions. An FAI, which could get information from inside
<br>
the brain using magnetic fields or microscopic sensors, would do a
<br>
much better job.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* A machine could never understand human morality/emotions.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Human morality is really, really
<br>
complicated, but there's no reason to think it's forever beyond the
<br>
reach of science. The evolutionary psychologists have already mapped a
<br>
great deal of the human moral system.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* AIs would take advantage of their power and create a dictatorship.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: An AI, which does not have the
<br>
evolutionary history of the human species, would have no built-in
<br>
drive to seize and abuse power.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* An AI without self-preservation built in would find no reason to
<br>
continue existing.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Self-preservation is a very important
<br>
subgoal for a large number of supergoals (Friendliness, destroying the
<br>
human species, making cheesecakes, etc.) Even without an independent
<br>
drive for self-preservation, self-preservation is still required for
<br>
influencing the universe.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* A superintelligent AI would reason that it's best for humanity
<br>
to destroy itself.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: If *any* sufficiently intelligent AI
<br>
would exterminate the human species, any sufficiently intelligent
<br>
human would commit suicide, in which case there's nothing we can do
<br>
about it anyway.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* The main defining characteristic of complex systems, such as
<br>
minds, is that no mathematical verification of properties such as
<br>
&quot;Friendliness&quot; is possible.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: It's impossible to verify the
<br>
Friendliness of an arbitrary mind, but we can still engineer a mind we
<br>
know how to verify.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Any future AI would undergo natural selection, and would
<br>
eventually become hostile to humanity to better pursue reproductive
<br>
fitness.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Significant selection pressure requires
<br>
a large number of preconditions, few of which will be met by future
<br>
AIs.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* FAI needs to be done as an open-source effort, so other people
<br>
can see that the project isn't being hijacked to make some guy
<br>
Dictator of the Universe.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o The disadvantages of open-source AI are obvious, but we
<br>
really do need a mechanism to assure the public that the project
<br>
hasn't been hijacked. - Tom
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* If an FAI does what we would want if we were less selfish, won't
<br>
it kill us all in the process of extracting resources to colonize
<br>
space as quickly as possible to prevent astronomical waste?
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: We wouldn't want the FAI to kill us all
<br>
to gather natural resources. We generally assign little utility having
<br>
a big pile of resources and no complex, intelligent life.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* It's absurd to have a collective volition approach that is
<br>
sensitive to the number of people who support something.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis:
<br>
<p>&nbsp;- Tom
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17555.html">Robin Lee Powell: "Re: Singularity Objections"</a>
<li><strong>Previous message:</strong> <a href="17553.html">Thomas McCabe: "Singularity Objections: General AI, Miscellaneous"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17554">[ date ]</a>
<a href="index.html#17554">[ thread ]</a>
<a href="subject.html#17554">[ subject ]</a>
<a href="author.html#17554">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:01 MDT
</em></small></p>
</body>
</html>
