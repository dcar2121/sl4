<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Singularity Objections: Friendliness, activism</title>
<meta name="Author" content="Thomas McCabe (pphysics141@gmail.com)">
<meta name="Subject" content="Singularity Objections: Friendliness, activism">
<meta name="Date" content="2008-01-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Singularity Objections: Friendliness, activism</h1>
<!-- received="Tue Jan 29 13:44:17 2008" -->
<!-- isoreceived="20080129204417" -->
<!-- sent="Tue, 29 Jan 2008 15:41:53 -0500" -->
<!-- isosent="20080129204153" -->
<!-- name="Thomas McCabe" -->
<!-- email="pphysics141@gmail.com" -->
<!-- subject="Singularity Objections: Friendliness, activism" -->
<!-- id="b7a9e8680801291241h7748ef70w81e1fdb3e7cbcfd0@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Thomas McCabe (<a href="mailto:pphysics141@gmail.com?Subject=Re:%20Singularity%20Objections:%20Friendliness,%20activism"><em>pphysics141@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Jan 29 2008 - 13:41:53 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17550.html">Thomas McCabe: "Singularity Objections: Friendliness, feasibility"</a>
<li><strong>Previous message:</strong> <a href="17548.html">Thomas McCabe: "Singularity Objections: Friendliness, desirability"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17549">[ date ]</a>
<a href="index.html#17549">[ thread ]</a>
<a href="subject.html#17549">[ subject ]</a>
<a href="author.html#17549">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Activism
<br>
[edit]
<br>
It's too early to start thinking about Friendly AI
<br>
<p>The &quot;it is too early to worry about the dangers of AI&quot; argument has
<br>
some merit, but as Eliezer Yudkowsky notes, there was very little
<br>
discussion about the dangers of AI even back when researchers thought
<br>
it was just around the corner. What is needed is a mindset of caution
<br>
- a way of thinking that makes safety issues the first priority, and
<br>
which is shared by all researchers working on AI. A mindset like that
<br>
does not spontaneously appear - it takes either decades of careful
<br>
cultivation, or sudden catastrophes that shock people into realizing
<br>
the dangers. Environmental activists have been talking about the
<br>
dangers of climate change for decades now, but they are only now
<br>
starting to get taken seriously. Soviet engineers obviously did not
<br>
have a mindset of caution when they designed the Chernobyl power
<br>
plant, nor did its operators when they started the fateful experiment.
<br>
Most AI researchers do not have a mindset of caution that makes them
<br>
consider thrice every detail of their system architectures - or even
<br>
make them realize that there are dangers. If active discussion is
<br>
postponed to the moment when AI is starting to become a real threat,
<br>
then it will be too late to foster that mindset.
<br>
<p>There is also the issue of our current awareness of risks influencing
<br>
our AI engineering techniques. Investors who have only been told of
<br>
the promising sides are likely to pressure the researchers to pursue
<br>
progress at any means available - or if the original researchers are
<br>
aware of the risks and refuse to do so, the investors will hire other
<br>
researchers who are less aware of them. To quote Artificial
<br>
Intelligence as a Positive and Negative Factor in Global Risk:
<br>
<p>&quot;The field of AI has techniques, such as neural networks and
<br>
evolutionary programming, which have grown in power with the slow
<br>
tweaking of decades. But neural networks are opaque - the user has no
<br>
idea how the neural net is making its decisions - and cannot easily be
<br>
rendered un-opaque; the people who invented and polished neural
<br>
networks were not thinking about the long-term problems of Friendly
<br>
AI. Evolutionary programming (EP) is stochastic, and does not
<br>
precisely preserve the optimization target in the generated code; EP
<br>
gives you code that does what you ask, most of the time, under the
<br>
tested circumstances, but the code may also do something else on the
<br>
side. EP is a powerful, still maturing technique that is intrinsically
<br>
unsuited to the demands of Friendly AI. Friendly AI, as I have
<br>
proposed it, requires repeated cycles of recursive self-improvement
<br>
that precisely preserve a stable optimization target.
<br>
<p>The most powerful current AI techniques, as they were developed and
<br>
then polished and improved over time, have basic incompatibilities
<br>
with the requirements of Friendly AI as I currently see them. The Y2K
<br>
problem - which proved very expensive to fix, though not
<br>
global-catastrophic - analogously arose from failing to foresee
<br>
tomorrow's design requirements. The nightmare scenario is that we find
<br>
ourselves stuck with a catalog of mature, powerful, publicly available
<br>
AI techniques which combine to yield non-Friendly AI, but which cannot
<br>
be used to build Friendly AI without redoing the last three decades of
<br>
AI work from scratch.&quot;
<br>
[edit]
<br>
Development towards AI will be gradual. Methods will pop up to deal with it.
<br>
<p>Unfortunately, it is by no means not a given that society will have
<br>
time to adapt to artificial intelligences. Once a roughly-human level
<br>
intelligence has been reached, there are many ways for an AI to become
<br>
vastly more intelligent (and thus more powerful) than humans in a very
<br>
short time:
<br>
<p>Hardware increase/speed-up. Once a certain amount of hardware has
<br>
human-equivalence, it may be possible to make it faster by simply
<br>
adding more hardware. While the increase isn't necessarily linear -
<br>
many systems need to spend an increasing fraction of resources to
<br>
managing overhead as the scale involved increases - it is daunting to
<br>
imagine a mind which is human-equivalent, then has five times as many
<br>
extra processors and memory added on. AIs might also be capable of
<br>
increasing the general speed of development - [Staring into the
<br>
Singularity] has a hypothetical scenario with technological
<br>
development being done by AIs, which themselves double in (hardware)
<br>
speed every two years - two subjective years, which shorten as their
<br>
speed goes up. A Model-1 AI takes two years to develop the Model-2 AI,
<br>
which takes takes a year to develop the Model-3 AI, which takes six
<br>
months to develop the Model-4 AI, which takes three months to develop
<br>
the Model-5 AI...
<br>
<p>Instant reproduction. An AI can &quot;create offspring&quot; very fast, by
<br>
simply copying itself to any system to which it has access. Likewise,
<br>
if the memories and knowledge obtained by the different AIs are in an
<br>
easily transferable format, they can simply be copied, enabling
<br>
computer systems to learn immense amounts of information in an
<br>
instant.
<br>
<p>Software self-improvement involves the computer studying itself and
<br>
applying its intelligence to modifying itself to become more
<br>
intelligent, then using that improved intelligence to modify itself
<br>
further. An AI could make itself more intelligent by, for instance,
<br>
studying its learning algorithms for signs of bias and improving them
<br>
with better ones, developing ways for more effective management of its
<br>
working memory, or creating entirely new program modules for handling
<br>
particular tasks. Each round of improvement would make the AI smarter
<br>
and accelerate continued self-improvement. An early, primitive example
<br>
of this sort of capability was EURISKO, a computer program composed of
<br>
different heuristics (rules of thumb) which it used for learning and
<br>
for creating and modifying its own heuristics. Having been fed
<br>
hundreds of pages of rules for the Traveller science fiction wargame,
<br>
EURISKO began running simulated battles between different fleets of
<br>
its own design, abstracting useful principles into new heuristics and
<br>
modifying old ones with the help of its creator. When EURISKO was
<br>
eventually entered into a tournament, the fleet of its design won the
<br>
contest single-handedly. In response, the organizers of the contest
<br>
revised the rules, releasing the new set of them only a short time
<br>
before the next contest. According to the creator of the program,
<br>
Douglas Lenat, the original EURISKO would not have had the time to
<br>
design a new fleet in such a short time - but now it had learned
<br>
enough general-purpose heuristics from the first contest that it could
<br>
build a fleet that won the contest, even with the modified rules.
<br>
<p>And it is much easier to improve a purely digital entity than it is to
<br>
improve human beings: an electronic being can be built in a modular
<br>
fashion and have bits of it re-written from scratch. The minds of
<br>
human beings are evolved to be hopelessly interdependent and are so
<br>
fragile that they easily develop numerous traumas and disorders even
<br>
without outside tampering.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Friendliness is trivially achieved. People evolved from selfish
<br>
self-replicators; AIs will &quot;evolve&quot; from programs which exist solely
<br>
to fulfill our wishes. Without evolution building them, AIs will
<br>
automatically be Friendly.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Possible rebuttals:
<br>
<p>- An AI doesn't need to be selfish in order to be unsafe. The fact
<br>
that they were not shaped by evolution to have the same moral
<br>
intuitions as we do is precisely the problem. Might be good to link to
<br>
Eli's posts on the topic (1) (2) here. - Most humans, when placed in
<br>
positions of power, are not Friendly in the FAI sense. History is rife
<br>
with abuse of power; just look at Hitler, Stalin, and Mao, who led
<br>
large portions of the world for decades. - Directed evolution on a
<br>
computer is not likely to resemble human evolution. Most of the
<br>
selection pressures which drove human altruism will be absent or
<br>
distorted.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Trying to build Friendly AI is pointless, as a Singularity is by
<br>
definition beyond human understanding and control.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;* Unfriendly AI is much easier than Friendly AI, so we are going
<br>
to be destroyed regardless.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: There's no point in giving up the
<br>
future of humanity just because things seem bleak.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;* Other technologies, such as nanotechnology and bioengineering,
<br>
are much easier than FAI and they have no &quot;Friendly&quot; equivalent that
<br>
could prevent them from being used to destroy humanity.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: There's no point in giving up the
<br>
future of humanity just because things seem bleak. If we achieve FAI
<br>
first, then the risks from the other technologies will be mitigated,
<br>
as well.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;* Any true AI would have a drastic impact on human society,
<br>
including a large number of unpredictable, unintended, probably really
<br>
bad consequences.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: AI will be developed sooner or later
<br>
anyway. Making it Friendly will minimize the bad consequences.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;* We can't start making AIs Friendly until we have AIs around to
<br>
look at and experiment with. (Goertzel's objection)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: then we should research Friendliness
<br>
and general AI theory side by side.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;* Talking about possible dangers would make people much less
<br>
willing to fund needed AI research.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Funding AI research without considering
<br>
the dangers is much worse than AI research being delayed.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;* Any work done on FAI will be hijacked and used to build hostile AI.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: That's a valid fear. It's just a risk
<br>
we have to take.
<br>
<p>&nbsp;- Tom
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17550.html">Thomas McCabe: "Singularity Objections: Friendliness, feasibility"</a>
<li><strong>Previous message:</strong> <a href="17548.html">Thomas McCabe: "Singularity Objections: Friendliness, desirability"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17549">[ date ]</a>
<a href="index.html#17549">[ thread ]</a>
<a href="subject.html#17549">[ subject ]</a>
<a href="author.html#17549">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:01 MDT
</em></small></p>
</body>
</html>
