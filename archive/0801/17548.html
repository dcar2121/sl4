<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Singularity Objections: Friendliness, desirability</title>
<meta name="Author" content="Thomas McCabe (pphysics141@gmail.com)">
<meta name="Subject" content="Singularity Objections: Friendliness, desirability">
<meta name="Date" content="2008-01-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Singularity Objections: Friendliness, desirability</h1>
<!-- received="Tue Jan 29 13:43:51 2008" -->
<!-- isoreceived="20080129204351" -->
<!-- sent="Tue, 29 Jan 2008 15:41:27 -0500" -->
<!-- isosent="20080129204127" -->
<!-- name="Thomas McCabe" -->
<!-- email="pphysics141@gmail.com" -->
<!-- subject="Singularity Objections: Friendliness, desirability" -->
<!-- id="b7a9e8680801291241v13033942n576b65f83cc09c85@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Thomas McCabe (<a href="mailto:pphysics141@gmail.com?Subject=Re:%20Singularity%20Objections:%20Friendliness,%20desirability"><em>pphysics141@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Jan 29 2008 - 13:41:27 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17549.html">Thomas McCabe: "Singularity Objections: Friendliness, activism"</a>
<li><strong>Previous message:</strong> <a href="17547.html">Thomas McCabe: "Singularity Objections: General AI, Implementation"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17548">[ date ]</a>
<a href="index.html#17548">[ thread ]</a>
<a href="subject.html#17548">[ subject ]</a>
<a href="author.html#17548">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;Desirability
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* A post-Singularity mankind won't be anything like the humanity
<br>
we know, regardless of whether it's a positive or negative Singularity
<br>
- therefore it's irrelevant whether we get a positive or negative
<br>
Singularity.
<br>
<p>[edit]
<br>
It's unethical to build AIs as willing slaves.
<br>
<p>(an example of this objection)
<br>
<p>There are two parts to this objection. For one, it could be argued
<br>
that it's unethical to restrict a mind's freedom of choice. But if you
<br>
have the freedom to build a mind with an arbitrary set of desires,
<br>
what level of uncertainty would need to be incorporated before the
<br>
programmed choice no longer was a programmed choice? Would it have a
<br>
true choice if you estimated that it chose things in a certain way 90%
<br>
of the time? 70%? 50%? Is it only ethical to craft minds for as long
<br>
as you are lousy in the art of mindcraft, and don't even know how to
<br>
estimate those probabilities? But that would be saying that it's only
<br>
ethical to build minds when you have no clue of what they will do to
<br>
their environment and others. That wouldn't be ethical - that would be
<br>
criminally irresponsible.
<br>
<p>It could be suggested that it'd be more ethical to simply treat the
<br>
created AI well, so that it would find the choice of helping humanity
<br>
attractive. But that argument only works if you can only build a
<br>
certain kind of mind - for instance, if you can only build very
<br>
human-like minds. When you are free to define all of a mind's
<br>
preferences, what's the difference between making it an attractive
<br>
option to assist humans and programming it into a certain decision? We
<br>
easily think that certain things are more &quot;natural&quot; for minds to
<br>
prefer than others, because our we have evolved to consider them
<br>
inherently natural. But ultimately there's no reason for why it'd be
<br>
right or wrong to make a mind prefer a certain sort of treatment over
<br>
the other, or why it'd be right or wrong to make a mind prefer acting
<br>
in certain ways.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* You can't suffer if you're dead, therefore AIs wiping out
<br>
humanity isn't a bad thing.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: it seems very plausible that AIs wiping
<br>
out humanity would cause immense suffering during the process.
<br>
Furthermore, it would be a horrible waste to let humanity be destroyed
<br>
when a positive Singularity could create a humanity with no
<br>
non-voluntary suffering at all.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;* Humanity should be in charge of its own destiny, not machines.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: then we should build AIs to implement a
<br>
program such as CEV that helps humans take charge of their destiny.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;* A perfectly Friendly AI would do everything for us, making life
<br>
boring and not worth living.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: An AI that would make life boring and
<br>
not worth living would by definition not be perfectly Friendly. If
<br>
there is some optimal level of adversity that humans need in order to
<br>
thrive, then a perfectly Friendly AI would create a world where
<br>
everybody faced that optimal level - assuming they didn't want to
<br>
modify their psyches to require a different level of adversity.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;* The solution to the problems that humanity faces cannot involve
<br>
more technology, especially such a dangerous technology as AGI, as
<br>
technology itself is part of the problem.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;* No problems that could possibly be solved through AGI/MNT/the
<br>
Singularity are worth the extreme existential risk incurred through
<br>
developing the relevant technology/triggering the relevant event.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: the Singularity will eventually be
<br>
triggered anyway. We aren't aiming to trigger it as fast as possible,
<br>
we're aiming to trigger it as safely as possible.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;* A human-Friendly AI would ignore the desires of other sentients,
<br>
such as uploads/robots/aliens/animals.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o Rebuttal synopsis: Preferably, Friendly AIs would be built
<br>
to be Friendly towards all sentient life.
<br>
<p>&nbsp;- Tom
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17549.html">Thomas McCabe: "Singularity Objections: Friendliness, activism"</a>
<li><strong>Previous message:</strong> <a href="17547.html">Thomas McCabe: "Singularity Objections: General AI, Implementation"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17548">[ date ]</a>
<a href="index.html#17548">[ thread ]</a>
<a href="subject.html#17548">[ subject ]</a>
<a href="author.html#17548">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:01 MDT
</em></small></p>
</body>
</html>
