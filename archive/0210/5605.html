<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: In defense of Friendliness was RE: [wta-talk] My own A.I project: 'Show-down with Singularity Institute&quot;</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: In defense of Friendliness was RE: [wta-talk] My own A.I project: 'Show-down with Singularity Institute&quot;">
<meta name="Date" content="2002-10-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: In defense of Friendliness was RE: [wta-talk] My own A.I project: 'Show-down with Singularity Institute&quot;</h1>
<!-- received="Thu Oct 17 01:25:32 2002" -->
<!-- isoreceived="20021017072532" -->
<!-- sent="Thu, 17 Oct 2002 03:25:32 -0400" -->
<!-- isosent="20021017072532" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: In defense of Friendliness was RE: [wta-talk] My own A.I project: 'Show-down with Singularity Institute&quot;" -->
<!-- id="3DAE65EC.5050001@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="aoldi5+kuco@eGroups.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20In%20defense%20of%20Friendliness%20was%20RE:%20[wta-talk]%20My%20own%20A.I%20project:%20'Show-down%20with%20Singularity%20Institute&quot;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu Oct 17 2002 - 01:25:32 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5606.html">Reason: "RE: In defense of Friendliness was RE: [wta-talk] My own A.I project: 'Show-down with Singularity Institute&quot;"</a>
<li><strong>Previous message:</strong> <a href="5604.html">mike99: "RE: testing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5606.html">Reason: "RE: In defense of Friendliness was RE: [wta-talk] My own A.I project: 'Show-down with Singularity Institute&quot;"</a>
<li><strong>Reply:</strong> <a href="5606.html">Reason: "RE: In defense of Friendliness was RE: [wta-talk] My own A.I project: 'Show-down with Singularity Institute&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5605">[ date ]</a>
<a href="index.html#5605">[ thread ]</a>
<a href="subject.html#5605">[ subject ]</a>
<a href="author.html#5605">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Marc Geddes wrote:
<br>
<p><em>&gt; Fair enough, but I did kind of get the impression that Eleizer 
</em><br>
<em>&gt; expected the FAI to act like a wish-granting genie.
</em><br>
<p>No, if I inflated to superintelligence, then this is my best current 
<br>
understanding of how I would be ethically obligated to interact with any 
<br>
still-basic humans around.
<br>
<p>Why?  In the absence of an objective morality (an undetermined possibility 
<br>
that needs to be projected both ways) then I don't see any moral reason 
<br>
that would allow a superintelligence to meddle in other people's lives 
<br>
without their consent.  As a human, interacting with other humans, I can 
<br>
try to convince people that they should want different things.  I can do 
<br>
this because I don't have the intelligence to see humans as systems whose 
<br>
actions are effectively determined by which of several sentences I choose 
<br>
to say to them.  At that level, even simple conversation becomes meddling 
<br>
with people's minds if you start choosing between innocent-sounding 
<br>
statements on the basis of a superintelligent understanding of how those 
<br>
statements will affect that person's trajectory, according to goals not 
<br>
shared by that person.  I don't see any privileged moral frame of 
<br>
reference under human morality that would enable one entity's strictly 
<br>
personal goals to justly supervene on another entity's strictly personal 
<br>
goals.  But giving people what *they* want, as opposed to what *you* want 
<br>
or even what you would prefer that they want, is okay.
<br>
<p>Another way of thinking about it is this.  Suppose that you're interacting 
<br>
with an human-derived superintelligence that happens to be a devout 
<br>
Zoroastrian (a thought that makes my brain hurt, but let's suppose for the 
<br>
sake of discussion that this is cognitively possible).  Let's suppose 
<br>
there are no external guards on how this superintelligence interacts with 
<br>
you.  What constraints do you hope this superintelligence feels obliged to 
<br>
obey in interacting with you?  I would feel that my rights had been 
<br>
violated if our conversation seemed innocent on the surface - in 
<br>
accordance with my wishes - but had actually been chosen out of a vast 
<br>
space of possibilities with the intent of making my life serve the overall 
<br>
goals of that ZSI (Zoroastrian superintelligence).  I would not wish to 
<br>
find my life trajectory warped to serve goals I find repugnant.
<br>
<p>But in the absence of objective morality, how can the same ethics not also 
<br>
apply to a rational superintelligence interacting with a human 
<br>
Zoroastrian?  Fair is fair.  If I don't want my life warped to serve a 
<br>
view of destiny that I disagree with, then undoubtedly neither do 
<br>
Zoroastrians.  Now it could be that there is some view that is simply 
<br>
*right* and that I and the Zoroastrians are both wrong.  But in the 
<br>
absence of objective morality, what's left for the ethics of 
<br>
superintelligences interacting with mortals is the idea that sentient 
<br>
beings should get what *they* want.  And &quot;what they want&quot; should be 
<br>
defined however *they* want to define it.
<br>
<p>If you're interacting as an equal, you can try and convince people to want 
<br>
other things, and they can try and convince you back.  But if you have 
<br>
substantially transhuman brainpower and can model people from the design 
<br>
stance, such that you have the power to convince people of arbitrary 
<br>
propositions, at that point what you are doing ceases to be &quot;argument&quot; and 
<br>
becomes &quot;mind control&quot;, unless the discussion is carried out *solely* 
<br>
according to the wishes of the person being talked to.
<br>
<p>Now you might say that an obvious way around this - in ethical terms - is 
<br>
for a superintelligence to create an interacting system on the same level 
<br>
on the human.  And if I were talking with a voluntarily ethical 
<br>
Zoroastrian superintelligence for some reason, this is in fact what I 
<br>
would prefer.  Given my current limited understanding of ethics, that is; 
<br>
obviously you should first ask a more trusted superintelligence about 
<br>
whether this is what a smarter you would regard as the ethical way to 
<br>
communicate...
<br>
<p>But if you're talking about a Transition Guide or Sysop - a 
<br>
superintelligence occupying a unique position with respect to the 
<br>
trajectory of human destiny - then *any* transfer of personal preferences 
<br>
by the original programmer constitutes a hijacking of human destiny. 
<br>
Would you want a transcending upload with personal goals you found 
<br>
repugnant using her unique position as Transition Guide to leverage those 
<br>
goals over all human destiny?  No?  Then, failing an objective morality, 
<br>
the general ethical principle must be that a mind which will occupy a 
<br>
unique position with respect to human destiny, such as the first seed AI, 
<br>
should not be leveraging the personal goals of its programmers.  Not in 
<br>
*any* way, big or small, because you wouldn't like it if someone else did 
<br>
that to you.  Eliminate sensitivities on the choice of original 
<br>
programmers.  If any mind-in-general needs *some* set of highly developed 
<br>
personal preferences, then use the modeled personal preferences of a 
<br>
generic human, not the personal preferences of the original programmers. 
<br>
And that's even *if* the personal preferences of this mind-in-general play 
<br>
no role in how it interacts with not-yet-upgraded intelligences, or play a 
<br>
role only on the specific request of those intelligences.  Don't hijack 
<br>
human destiny.  Being a seed AI programmer doesn't make you morally 
<br>
different from any other human.
<br>
<p>Of course my current understanding of morality may be wrong, but my 
<br>
current understanding of morality is all that I have available to use as 
<br>
raw material to illustrate to an AI how human moral philosophy works.
<br>
<p>Marc Geddes:  Your wish is to achieve your immediate subgoals as a result 
<br>
of personal effort, rather than having them granted directly as wishes. 
<br>
(If you Google on &quot;Singularity Fun Theory&quot;, you'll find out *why* you 
<br>
prefer this... but that's a separate story.)  That wish is certainly 
<br>
suboptimally achieved with respect to your current environment.  You 
<br>
didn't create your shoes, much less your feet.  Your functional components 
<br>
are systems that evolution designed and then handed to you on a silver 
<br>
platter.  The various artifacts around you were created by other humans 
<br>
and operate as independent causal systems divorced from your sensorimotor 
<br>
architecture; you control them as external tools rather than feeling them 
<br>
as parts of yourself.  If your wish is to find happiness in achieving your 
<br>
goals as the result of your personal creative efforts, rather than having 
<br>
things handed to you on a silver platter, then that's certainly a wish 
<br>
that could be achieved to a much greater degree after the Singularity than 
<br>
before.
<br>
<p><em> &gt; All that's needed is an injunction not to violate the rights of
</em><br>
<em> &gt; others.  (i.e. to regard all other sentients as 'ends in themselves'
</em><br>
<em> &gt; and not initiate force against them except in self-defense)
</em><br>
<em> &gt;
</em><br>
<em> &gt; A little bit of self-centeredness should not be a problem if the
</em><br>
<em> &gt; Kantian imperative is backed up by reason.  (i.e. if the A.I reasons
</em><br>
<em> &gt; that it is indeed 'ethically correct' to regard other sentients
</em><br>
<em> &gt; as 'ends in themselves')
</em><br>
[snip]
<br>
<em>&gt; Agreed. In fact as part of my suggested supergoals the A.I would co-
</em><br>
<em>&gt; operate with humans.  (As part of it's quest for knowledge I 
</em><br>
<em>&gt; suggested a desire to 'teach' what it learns, so it would be sharing 
</em><br>
<em>&gt; it's knowledge with us.  In fact this sharing of knowledge is 
</em><br>
<em>&gt; actually probably the best way an A.I could help us)
</em><br>
<p>...
<br>
<p>You can't mess around with AI morality this way - adding a piece here, 
<br>
taking a piece there according to your whims.  Do you construct your own 
<br>
morality using this kind of thinking?  No?  Then why do you believe you 
<br>
can use it to construct an AI's?
<br>
<p>This is not a question of making up random stuff and asking the AI to do 
<br>
it.  What do *you* believe is right?  What do *you* believe is wrong?  No 
<br>
matter how imperfect your moral philosophy is, it's something that can 
<br>
exist at least temporarily in at least one mind.
<br>
<p>By thinking of FAI as something separate from yourself, you are destroying 
<br>
your ability to even get started on understanding metamorality.  We 
<br>
interact with other humans by cajoling them, giving them orders, making 
<br>
alliances, wondering about their other loyalty.  We *create* only 
<br>
ourselves.  Building AI is an act of creation, not a matter of giving 
<br>
orders.  How do *you* choose between actions?  What are *your* supergoals?
<br>
<p>If you're wondering how this gets reconciled with &quot;no sensitivity on 
<br>
choice of initial programmer&quot;, it's because opening a channel to your own 
<br>
moral substance is how you provide the AI with an *interim* approximation 
<br>
of the substance of humanity.  In other words, because your supergoals are 
<br>
good enough for you, doesn't mean they're good enough for the AI.  But if 
<br>
they're not good enough for *you*, they're *certainly* not good enough for 
<br>
the AI.  That doesn't mean inventing whatever you want piecemeal, then 
<br>
rationalizing why each piece *would be* good enough for you.  It means 
<br>
using what actually genuinely *was* good enough for you.
<br>
<p><em>&gt; It may be that a 'non-self 
</em><br>
<em>&gt; oriented supergoal' might turn out to harmful in some way that we are 
</em><br>
<em>&gt; currently unaware of.
</em><br>
<p>You're going to have to offer something more specific than &quot;might&quot; if you 
<br>
want to convince me that observer-centered supergoals are necessary to the 
<br>
integrity of minds-in-general.  I don't believe that having ten fingers is 
<br>
necessary to all possible minds-in-general.  I don't believe that being 
<br>
built from amino acid chains twisted into complex shapes by van der Waals 
<br>
forces is a requirement of minds-in-general.  Why should the feature you 
<br>
cite be anything except another contingent product of our evolutionary 
<br>
origins?  You are arguing from your own ignorance of the evolutionary 
<br>
psychology of goal systems.  I suppose this can be an effective argument 
<br>
if the audience doesn't know either - i.e., Jeremy &quot;How do we *know* 
<br>
genetically engineered organisms won't spontaneously explode?&quot; Rifkin - 
<br>
but it ill behooves a wannabe specialist.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5606.html">Reason: "RE: In defense of Friendliness was RE: [wta-talk] My own A.I project: 'Show-down with Singularity Institute&quot;"</a>
<li><strong>Previous message:</strong> <a href="5604.html">mike99: "RE: testing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5606.html">Reason: "RE: In defense of Friendliness was RE: [wta-talk] My own A.I project: 'Show-down with Singularity Institute&quot;"</a>
<li><strong>Reply:</strong> <a href="5606.html">Reason: "RE: In defense of Friendliness was RE: [wta-talk] My own A.I project: 'Show-down with Singularity Institute&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5605">[ date ]</a>
<a href="index.html#5605">[ thread ]</a>
<a href="subject.html#5605">[ subject ]</a>
<a href="author.html#5605">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
