<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: New Singularity-relevant book</title>
<meta name="Author" content="James Rogers (jamesr@best.com)">
<meta name="Subject" content="Re: New Singularity-relevant book">
<meta name="Date" content="2002-10-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: New Singularity-relevant book</h1>
<!-- received="Wed Oct 23 12:11:35 2002" -->
<!-- isoreceived="20021023181135" -->
<!-- sent="23 Oct 2002 11:32:16 -0700" -->
<!-- isosent="20021023183216" -->
<!-- name="James Rogers" -->
<!-- email="jamesr@best.com" -->
<!-- subject="Re: New Singularity-relevant book" -->
<!-- id="1035397953.1057.80.camel@avalon" -->
<!-- inreplyto="Pine.SOL.4.33.0210230457280.12497-100000@doll.ssec.wisc.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Rogers (<a href="mailto:jamesr@best.com?Subject=Re:%20New%20Singularity-relevant%20book"><em>jamesr@best.com</em></a>)<br>
<strong>Date:</strong> Wed Oct 23 2002 - 12:32:16 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5643.html">Ben Goertzel: "Cancer and evolution"</a>
<li><strong>Previous message:</strong> <a href="5641.html">Ben Goertzel: "RE: New Singularity-relevant book"</a>
<li><strong>In reply to:</strong> <a href="5640.html">Bill Hibbard: "Re: New Singularity-relevant book"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5642">[ date ]</a>
<a href="index.html#5642">[ thread ]</a>
<a href="subject.html#5642">[ subject ]</a>
<a href="author.html#5642">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Wed, 2002-10-23 at 03:39, Bill Hibbard wrote:
<br>
<em>&gt; On Tue, 22 Oct 2002, James Rogers wrote:
</em><br>
<em>&gt; &gt; This is a nonsensical assertion on a number of levels, and I fear that it
</em><br>
<em>&gt; &gt; effectively pollutes those things derived from the assumption that this
</em><br>
<em>&gt; &gt; makes any kind of sense.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I agree that it is nonsense to say that intelligence is
</em><br>
<em>&gt; learned, but that's not what I said. I said intelligent
</em><br>
<em>&gt; behaviors are learned.
</em><br>
<p><p>The way it was written, you also said that &quot;Intelligent behavior cannot
<br>
be programmed&quot;, which contradicts &quot;[Intelligent behavior] must be
<br>
learned&quot;.  Reading these two statements one after the other, DOES read
<br>
as nonsense to me.  Granted, it was late and I was tired when I
<br>
originally wrote that email, but it reads the same this morning.
<br>
<p>&nbsp;
<br>
<em>&gt; &gt; Second, you CAN program intelligent
</em><br>
<em>&gt; &gt; behavior; its so obvious I'm not even sure where that came from.  Granted,
</em><br>
<em>&gt; &gt; for extremely complex learning tasks it becomes less wise to let monkeys
</em><br>
<em>&gt; &gt; program machines for intelligent behavior if you expect to maintain some
</em><br>
<em>&gt; &gt; average quality of results, but it is certainly doable.  Third, any designs
</em><br>
<em>&gt; &gt; to &quot;mimic the reinforcement learning of human brains&quot; seem misguided,
</em><br>
<em>&gt; &gt; largely because ANY system that can learn has these properties (ignoring the
</em><br>
<em>&gt; &gt; edge case of parrots); there is nothing categorically special about human
</em><br>
<em>&gt; &gt; brains in this regard and don't see where it buys anything, at least not as
</em><br>
<em>&gt; &gt; a checklist item.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; As to your second point, a programmed rather than learned
</em><br>
<em>&gt; implementation of intelligent behavior is only slightly
</em><br>
<em>&gt; less absurd than Searle's Chinese Room. Perhaps I should
</em><br>
<em>&gt; not have used the absolute word &quot;cannot&quot;, but in any
</em><br>
<em>&gt; pratical sense what I said is true.
</em><br>
<p><p>Intelligence is a property of the behavior, and even a strictly
<br>
programmed &quot;intelligent&quot; behavior can be defined as having some level of
<br>
intelligence within that context.  This is not a good argument though,
<br>
as &quot;intelligent&quot; as applied to behavior is observer dependent, and can
<br>
only be objectively applied to the intrinsic properties of the
<br>
machinery.  An intelligent machine can learn ANY behavior, so whether or
<br>
not it is stupid or intelligent from a particular viewpoint is
<br>
immaterial. &quot;It isn't that the bear waltzes so gracefully, it is that it
<br>
waltzes at all.&quot;  From a theoretical standpoint, observer-independent
<br>
intelligence is far more interesting than observer-dependent
<br>
intelligence.  This whole argument has similarities to the argument that
<br>
results from the conflict between pedestrian definitions of
<br>
&quot;information&quot; and mathematical definitions of the same.
<br>
<p>Just to throw in the point, Searle's Chinese Room does define a system
<br>
capable of expressing measurable intelligence.  It just seems absurd
<br>
because the context is narrow by definition and it violates our
<br>
intuition (which is wrong as often as not about these things).  As has
<br>
been pointed out in various forums, all programs expressible on finite
<br>
state machinery can be expressed as Giant Look-Up Tables (&quot;GLUTs&quot;). 
<br>
Therefore, if we accept the premise that general intelligence is
<br>
expressible on a FSM, we must also accept that it can be implemented as
<br>
a GLUT.
<br>
<p>&nbsp;
<br>
<em>&gt; Third, I used the phrase &quot;mimic the reinforcement learning
</em><br>
<em>&gt; of human brains&quot; just to make the point that intelligent
</em><br>
<em>&gt; machines have more in common with human brains than with
</em><br>
<em>&gt; current machines.
</em><br>
<p><p>I could mostly agree with this with some qualification.
<br>
<p>&nbsp;
<br>
<em>&gt; I am following Francis Crick and Gerald Edelman in my use of
</em><br>
<em>&gt; the word &quot;emotion&quot;. They both say that emotions are essential
</em><br>
<em>&gt; for intelligence based on the role of emotions for reinforcing
</em><br>
<em>&gt; or selecting intelligent behaviors. Of course, &quot;emotion&quot; is an
</em><br>
<em>&gt; overloaded term and you can find different neuroscientists who
</em><br>
<em>&gt; use it in different ways.
</em><br>
<p><p>I don't disagree but I do have a different perspective, mostly derived
<br>
from intelligence in the context of behavior being observer dependent. 
<br>
<em>&gt;From an evolutionary standpoint, you need emotion to bootstrap the
</em><br>
feedback loops that lead to learning advantageous behaviors in an
<br>
evolutionary sense.  The behaviors are learned, but I wouldn't classify
<br>
them as &quot;intelligent&quot; being merely the consequences of a biasing system
<br>
selected by evolutionary pressures.  As I stated previously, if you are
<br>
going to classify a behavior as &quot;intelligent&quot;, you have to qualify it
<br>
with the observer context.  The intelligence exists in the system, and
<br>
the emotion makes it &quot;do something&quot;, smart or stupid not being a
<br>
consideration.
<br>
<p>&nbsp;
<br>
<em>&gt; The difference between human and animal consciousness can be
</em><br>
<em>&gt; described in terms of whether animal minds include models of
</em><br>
<em>&gt; other animals minds, of events tommorrow, etc. Similarly, I think
</em><br>
<em>&gt; a key difference between human and machine consciousness will
</em><br>
<em>&gt; be the machines' detailed model of billions of human minds, in
</em><br>
<em>&gt; contrast to our detailed model of about 200 human minds.
</em><br>
<p><p>My point of contention is primarily with the idea that a
<br>
super-intelligent machine's consciousness is the result of interaction
<br>
with humans.  Intelligent machines develop themselves by interacting
<br>
with other machinery, of which humans are an interesting and relatively
<br>
complex form.  But the entire universe is filled with machinery that
<br>
will do the job; having a machine interact with human machinery is
<br>
largely useful if you want it to interact well with humans (which we
<br>
presumably do want -- I'm merely asserting it isn't strictly
<br>
necessary).  &quot;Consciousness&quot; on a machine capable of it comes from the
<br>
interaction with other machinery, but there is no requirement that the
<br>
other machinery be particularly intelligent.
<br>
<p>&nbsp;
<br>
<em>&gt; The statement &quot;the essential property of consciousness in humans
</em><br>
<em>&gt; and animals is that it enables brains to process experiences that
</em><br>
<em>&gt; are not actually occuring&quot; says something pretty rigorous. The
</em><br>
<em>&gt; simplest animal brains can only process events as they happen.
</em><br>
<em>&gt; But at some level of evolution, brains break free of &quot;now&quot;.
</em><br>
<p><p>The ability to work with models in the abstract is a limited only by
<br>
resources.  Human ability to do this exists solely as a consequence of
<br>
having more available resources for the machine.  The emergence of
<br>
something we call &quot;consciousness&quot; is essentially a function of the size
<br>
of the machinery, and therefore not really crucial to interesting
<br>
intelligence per se.  At the very least, there is a clear mathematical
<br>
relationship between resources and the computational ability to
<br>
manipulate model abstractions.
<br>
<p>&nbsp;
<br>
<em>&gt; And the temporal credit assignment problem is a well known
</em><br>
<em>&gt; and rigorous problem. There has been some very exciting
</em><br>
<em>&gt; neuroscience into how brains solve this problem, at least when
</em><br>
<em>&gt; delays between behaviors and rewards are short and predictable,
</em><br>
<em>&gt; in the paper:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;   Brown, J., Bullock, D., and Grossberg, S. How the Basal Ganglia
</em><br>
<em>&gt;   Use Parallel Excitatory and Inhibitory Learning Pathways to
</em><br>
<em>&gt;   Selectively Respond to Unexpected Rewarding Cues. Journal of
</em><br>
<em>&gt;   Neuroscience 19(23), 10502-10511. 1999.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think that the need to solve the temporal credit assignment
</em><br>
<em>&gt; problem when delays between behaviors and rewards are not
</em><br>
<em>&gt; short and predictable was the selectional force behind the
</em><br>
<em>&gt; evolution of consciousness. Any known effective solution to
</em><br>
<em>&gt; this problem requires a simulation model of the world.
</em><br>
<p><p>Interesting paper, it is my first time seeing it.  I am pleased to see
<br>
that it seems to suggest a system that works in a very similar fashion
<br>
to the model we derived from our computational theory.  I don't follow
<br>
neuroscience to closely because I feel it has a tendency to pollute
<br>
research on the more theoretical side of intelligence, but I do like to
<br>
occasionally checkpoint neuroscience against the theoretical models that
<br>
I work with.  Trying to derive computational theory from neuroscience
<br>
has never seemed to be a particularly productive endeavor, but that is
<br>
another topic for another day. 
<br>
<p>&nbsp;
<br>
Cheers,
<br>
<p>-James Rogers
<br>
&nbsp;<a href="mailto:jamesr@best.com?Subject=Re:%20New%20Singularity-relevant%20book">jamesr@best.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5643.html">Ben Goertzel: "Cancer and evolution"</a>
<li><strong>Previous message:</strong> <a href="5641.html">Ben Goertzel: "RE: New Singularity-relevant book"</a>
<li><strong>In reply to:</strong> <a href="5640.html">Bill Hibbard: "Re: New Singularity-relevant book"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5642">[ date ]</a>
<a href="index.html#5642">[ thread ]</a>
<a href="subject.html#5642">[ subject ]</a>
<a href="author.html#5642">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
