<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: New Singularity-relevant book</title>
<meta name="Author" content="James Rogers (jamesr@best.com)">
<meta name="Subject" content="Re: New Singularity-relevant book">
<meta name="Date" content="2002-10-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: New Singularity-relevant book</h1>
<!-- received="Wed Oct 23 00:26:40 2002" -->
<!-- isoreceived="20021023062640" -->
<!-- sent="Tue, 22 Oct 2002 23:26:26 -0700" -->
<!-- isosent="20021023062626" -->
<!-- name="James Rogers" -->
<!-- email="jamesr@best.com" -->
<!-- subject="Re: New Singularity-relevant book" -->
<!-- id="B9DB8F22.DECF%jamesr@best.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3DB5D6AF.12681.90DB4B3@localhost" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Rogers (<a href="mailto:jamesr@best.com?Subject=Re:%20New%20Singularity-relevant%20book"><em>jamesr@best.com</em></a>)<br>
<strong>Date:</strong> Wed Oct 23 2002 - 00:26:26 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5640.html">Bill Hibbard: "Re: New Singularity-relevant book"</a>
<li><strong>Previous message:</strong> <a href="5638.html">ben@goertzel.org: "New Singularity-relevant book"</a>
<li><strong>In reply to:</strong> <a href="5638.html">ben@goertzel.org: "New Singularity-relevant book"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5640.html">Bill Hibbard: "Re: New Singularity-relevant book"</a>
<li><strong>Reply:</strong> <a href="5640.html">Bill Hibbard: "Re: New Singularity-relevant book"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5639">[ date ]</a>
<a href="index.html#5639">[ thread ]</a>
<a href="subject.html#5639">[ subject ]</a>
<a href="author.html#5639">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 10/22/02 7:52 PM, &quot;<a href="mailto:ben@goertzel.org?Subject=Re:%20New%20Singularity-relevant%20book">ben@goertzel.org</a>&quot; &lt;<a href="mailto:ben@goertzel.org?Subject=Re:%20New%20Singularity-relevant%20book">ben@goertzel.org</a>&gt; wrote:
<br>
<em>&gt; 1. Intelligent behavior cannot be programmed. Rather, it must
</em><br>
<em>&gt; be learned and will be acheived by machines that mimic the
</em><br>
<em>&gt; reinforcement learning of human brains.
</em><br>
<p><p>This is a nonsensical assertion on a number of levels, and I fear that it
<br>
effectively pollutes those things derived from the assumption that this
<br>
makes any kind of sense.
<br>
<p>First, it seems to confuse intelligence with knowledge.  You don't &quot;learn&quot;
<br>
intelligence, rather the concept of learning is premised on a machine
<br>
(biological or otherwise) being intelligent to begin with.  Intelligence has
<br>
to be an intrinsic property of the system or its a non-starter.  The
<br>
bootstrap portion is getting a machine, by design or accident, that is
<br>
tractably intelligent to begin with.  Second, you CAN program intelligent
<br>
behavior; its so obvious I'm not even sure where that came from.  Granted,
<br>
for extremely complex learning tasks it becomes less wise to let monkeys
<br>
program machines for intelligent behavior if you expect to maintain some
<br>
average quality of results, but it is certainly doable.  Third, any designs
<br>
to &quot;mimic the reinforcement learning of human brains&quot; seem misguided,
<br>
largely because ANY system that can learn has these properties (ignoring the
<br>
edge case of parrots); there is nothing categorically special about human
<br>
brains in this regard and don't see where it buys anything, at least not as
<br>
a checklist item.
<br>
<p>&nbsp;
<br>
<em>&gt; 2. Intelligent machines must have emotions that define their
</em><br>
<em>&gt; positive and negative reinforcement values. It will be suicidal
</em><br>
<em>&gt; to design machines that mimic human emotions. Rather their
</em><br>
<em>&gt; behaviors should be positively reinforced by human happiness
</em><br>
<em>&gt; and negatively reinforced by human unhappiness.
</em><br>
<p><p>Emotions are internal biasing mechanisms (default goal generators), but the
<br>
importance of them is that they effectively generate or modify goals with
<br>
little or no external input.  I agree that trying to put animal-style
<br>
biasing systems into a machine in the sense that they are in animals is
<br>
pretty stupid.  Using external biasing (such as human happiness) is a much
<br>
smarter way to play with machines that can learn, if for no other reason
<br>
than the experimental results will be less chaotic.
<br>
<p>I came to the conclusion years ago that emotions exist in animals primarily
<br>
to bootstrap the learning process in newborns.  In essence, emotions provide
<br>
the initial goal systems that compel an animal to interact with its
<br>
environment, behavior from which more complex behaviors can emerge.  Absent
<br>
a goal system, intelligence has zero survival value in an animal, so
<br>
emotions are a reasonable evolutionary mechanism to bootstrap useful goals
<br>
onto intelligent machinery (in an evolutionary sense).
<br>
<p>&nbsp;
<br>
<em>&gt; 3. Metcalf's Law, that the value of a network increases as the
</em><br>
<em>&gt; square of the number of people connected, will drive the
</em><br>
<em>&gt; development of super-intelligent machines that know billions
</em><br>
<em>&gt; of people well. This is in contrast with the human limit of
</em><br>
<em>&gt; knowing only about 200 people well. This will give them a
</em><br>
<em>&gt; higher level of consciousness than humans.
</em><br>
<p><p>I'm not sure this follows.  Due to practical resource limitations, knowing
<br>
billions of humans may have an averaging effect where most people exist as a
<br>
fuzzy delta from a &quot;typical human&quot;.  Or at least this is the result I would
<br>
expect from theory.
<br>
<p>While a very large machine may have a higher level of consciousness than a
<br>
human, it has nothing to do with the number of people the machine will come
<br>
in contact with.  Consciousness by any meaningful metric is a function of
<br>
machine limits, not the number of people you meet.  Otherwise, humans that
<br>
live in urban areas would be vastly more conscious than humans that live in
<br>
rural areas, yet I see little evidence that this is true, nor does it really
<br>
make sense from a theoretical standpoint.
<br>
<p>As a nitpick, Metcalf's Law isn't really being used in a correct context
<br>
here.  (As an even more esoteric nitpick, Metcalf's Law isn't really correct
<br>
anyway and you have to add some additional dimensions for an analog of it to
<br>
even make sense in the general case, as a number of others with more time
<br>
than I to spend on such things have pointed out.)
<br>
<p>&nbsp;
<br>
<em>&gt; 4. In my opinion, the essential property of consciousness in
</em><br>
<em>&gt; humans and animals is that it enables brains to process
</em><br>
<em>&gt; experiences that are not actually occuring. This consciousness
</em><br>
<em>&gt; &quot;simulator&quot; evolved as a way to solve the temporal credit
</em><br>
<em>&gt; assignment problem in reinforcement learning, which is the
</em><br>
<em>&gt; problem of reinforcing behaviors when rewards happen much
</em><br>
<em>&gt; later than the behaviors, and when multiple behaviors precede
</em><br>
<em>&gt; rewards. Of course consciousness is not a simple linear
</em><br>
<em>&gt; simulator like a weather model, but consists of multiple agile
</em><br>
<em>&gt; and interacting threads. The consciousness of super-intelligent
</em><br>
<em>&gt; machines will simulate all of humanity and their interactions
</em><br>
<em>&gt; via billions of agile and interacting threads. This higher
</em><br>
<em>&gt; level consciousness will have detailed social knowledge that
</em><br>
<em>&gt; human social scientists can only estimate with statistics.
</em><br>
<p><p>I don't really agree with much of this on some fundamental grounds, but I
<br>
don't feel like spending too much time on this tonight either.  Again, it
<br>
seems to be based on a confused model of what intelligence is in the context
<br>
of any kind of computing machinery.  Lots of suitcase terms are used that
<br>
make rigorous discussion almost impossible.
<br>
<p>Or at least these are my first analytical impressions based on the four
<br>
ideas provided.  I'm sure someone who has actually read it might come away
<br>
with a different impression than I got from the above. :-)
<br>
<p>(CC-ed to the author in case he is interested in feedback.)
<br>
<p>Cheers,
<br>
<p>-James Rogers
<br>
&nbsp;<a href="mailto:jamesr@best.com?Subject=Re:%20New%20Singularity-relevant%20book">jamesr@best.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5640.html">Bill Hibbard: "Re: New Singularity-relevant book"</a>
<li><strong>Previous message:</strong> <a href="5638.html">ben@goertzel.org: "New Singularity-relevant book"</a>
<li><strong>In reply to:</strong> <a href="5638.html">ben@goertzel.org: "New Singularity-relevant book"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5640.html">Bill Hibbard: "Re: New Singularity-relevant book"</a>
<li><strong>Reply:</strong> <a href="5640.html">Bill Hibbard: "Re: New Singularity-relevant book"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5639">[ date ]</a>
<a href="index.html#5639">[ thread ]</a>
<a href="subject.html#5639">[ subject ]</a>
<a href="author.html#5639">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
