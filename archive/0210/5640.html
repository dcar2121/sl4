<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: New Singularity-relevant book</title>
<meta name="Author" content="Bill Hibbard (test@doll.ssec.wisc.edu)">
<meta name="Subject" content="Re: New Singularity-relevant book">
<meta name="Date" content="2002-10-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: New Singularity-relevant book</h1>
<!-- received="Wed Oct 23 04:39:19 2002" -->
<!-- isoreceived="20021023103919" -->
<!-- sent="Wed, 23 Oct 2002 05:39:18 -0500 (CDT)" -->
<!-- isosent="20021023103918" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@doll.ssec.wisc.edu" -->
<!-- subject="Re: New Singularity-relevant book" -->
<!-- id="Pine.SOL.4.33.0210230457280.12497-100000@doll.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="B9DB8F22.DECF%jamesr@best.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@doll.ssec.wisc.edu?Subject=Re:%20New%20Singularity-relevant%20book"><em>test@doll.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Wed Oct 23 2002 - 04:39:18 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5641.html">Ben Goertzel: "RE: New Singularity-relevant book"</a>
<li><strong>Previous message:</strong> <a href="5639.html">James Rogers: "Re: New Singularity-relevant book"</a>
<li><strong>In reply to:</strong> <a href="5639.html">James Rogers: "Re: New Singularity-relevant book"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5641.html">Ben Goertzel: "RE: New Singularity-relevant book"</a>
<li><strong>Reply:</strong> <a href="5641.html">Ben Goertzel: "RE: New Singularity-relevant book"</a>
<li><strong>Reply:</strong> <a href="5642.html">James Rogers: "Re: New Singularity-relevant book"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5640">[ date ]</a>
<a href="index.html#5640">[ thread ]</a>
<a href="subject.html#5640">[ subject ]</a>
<a href="author.html#5640">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
(Ben, I am not subscribed to <a href="mailto:sl4@sl4.org?Subject=Re:%20New%20Singularity-relevant%20book">sl4@sl4.org</a>, so if my message doesn't
<br>
get posted there will you please forward it? Thanks, Bill)
<br>
<p>On Tue, 22 Oct 2002, James Rogers wrote:
<br>
<p><em>&gt; On 10/22/02 7:52 PM, &quot;<a href="mailto:ben@goertzel.org?Subject=Re:%20New%20Singularity-relevant%20book">ben@goertzel.org</a>&quot; &lt;<a href="mailto:ben@goertzel.org?Subject=Re:%20New%20Singularity-relevant%20book">ben@goertzel.org</a>&gt; wrote:
</em><br>
<em>&gt; &gt; 1. Intelligent behavior cannot be programmed. Rather, it must
</em><br>
<em>&gt; &gt; be learned and will be acheived by machines that mimic the
</em><br>
<em>&gt; &gt; reinforcement learning of human brains.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is a nonsensical assertion on a number of levels, and I fear that it
</em><br>
<em>&gt; effectively pollutes those things derived from the assumption that this
</em><br>
<em>&gt; makes any kind of sense.
</em><br>
<p>I agree that it is nonsense to say that intelligence is
<br>
learned, but that's not what I said. I said intelligent
<br>
behaviors are learned.
<br>
<p><em>&gt; First, it seems to confuse intelligence with knowledge.  You don't &quot;learn&quot;
</em><br>
<em>&gt; intelligence, rather the concept of learning is premised on a machine
</em><br>
<em>&gt; (biological or otherwise) being intelligent to begin with.  Intelligence has
</em><br>
<em>&gt; to be an intrinsic property of the system or its a non-starter.  The
</em><br>
<em>&gt; bootstrap portion is getting a machine, by design or accident, that is
</em><br>
<em>&gt; tractably intelligent to begin with.  Second, you CAN program intelligent
</em><br>
<em>&gt; behavior; its so obvious I'm not even sure where that came from.  Granted,
</em><br>
<em>&gt; for extremely complex learning tasks it becomes less wise to let monkeys
</em><br>
<em>&gt; program machines for intelligent behavior if you expect to maintain some
</em><br>
<em>&gt; average quality of results, but it is certainly doable.  Third, any designs
</em><br>
<em>&gt; to &quot;mimic the reinforcement learning of human brains&quot; seem misguided,
</em><br>
<em>&gt; largely because ANY system that can learn has these properties (ignoring the
</em><br>
<em>&gt; edge case of parrots); there is nothing categorically special about human
</em><br>
<em>&gt; brains in this regard and don't see where it buys anything, at least not as
</em><br>
<em>&gt; a checklist item.
</em><br>
<p>As to your second point, a programmed rather than learned
<br>
implementation of intelligent behavior is only slightly
<br>
less absurd than Searle's Chinese Room. Perhaps I should
<br>
not have used the absolute word &quot;cannot&quot;, but in any
<br>
pratical sense what I said is true.
<br>
<p>Third, I used the phrase &quot;mimic the reinforcement learning
<br>
of human brains&quot; just to make the point that intelligent
<br>
machines have more in common with human brains than with
<br>
current machines.
<br>
<p><em>&gt; &gt; 2. Intelligent machines must have emotions that define their
</em><br>
<em>&gt; &gt; positive and negative reinforcement values. It will be suicidal
</em><br>
<em>&gt; &gt; to design machines that mimic human emotions. Rather their
</em><br>
<em>&gt; &gt; behaviors should be positively reinforced by human happiness
</em><br>
<em>&gt; &gt; and negatively reinforced by human unhappiness.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Emotions are internal biasing mechanisms (default goal generators), but the
</em><br>
<em>&gt; importance of them is that they effectively generate or modify goals with
</em><br>
<em>&gt; little or no external input.  I agree that trying to put animal-style
</em><br>
<em>&gt; biasing systems into a machine in the sense that they are in animals is
</em><br>
<em>&gt; pretty stupid.  Using external biasing (such as human happiness) is a much
</em><br>
<em>&gt; smarter way to play with machines that can learn, if for no other reason
</em><br>
<em>&gt; than the experimental results will be less chaotic.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I came to the conclusion years ago that emotions exist in animals primarily
</em><br>
<em>&gt; to bootstrap the learning process in newborns.  In essence, emotions provide
</em><br>
<em>&gt; the initial goal systems that compel an animal to interact with its
</em><br>
<em>&gt; environment, behavior from which more complex behaviors can emerge.  Absent
</em><br>
<em>&gt; a goal system, intelligence has zero survival value in an animal, so
</em><br>
<em>&gt; emotions are a reasonable evolutionary mechanism to bootstrap useful goals
</em><br>
<em>&gt; onto intelligent machinery (in an evolutionary sense).
</em><br>
<p>I am following Francis Crick and Gerald Edelman in my use of
<br>
the word &quot;emotion&quot;. They both say that emotions are essential
<br>
for intelligence based on the role of emotions for reinforcing
<br>
or selecting intelligent behaviors. Of course, &quot;emotion&quot; is an
<br>
overloaded term and you can find different neuroscientists who
<br>
use it in different ways.
<br>
<p><em>&gt; &gt; 3. Metcalf's Law, that the value of a network increases as the
</em><br>
<em>&gt; &gt; square of the number of people connected, will drive the
</em><br>
<em>&gt; &gt; development of super-intelligent machines that know billions
</em><br>
<em>&gt; &gt; of people well. This is in contrast with the human limit of
</em><br>
<em>&gt; &gt; knowing only about 200 people well. This will give them a
</em><br>
<em>&gt; &gt; higher level of consciousness than humans.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I'm not sure this follows.  Due to practical resource limitations, knowing
</em><br>
<em>&gt; billions of humans may have an averaging effect where most people exist as a
</em><br>
<em>&gt; fuzzy delta from a &quot;typical human&quot;.  Or at least this is the result I would
</em><br>
<em>&gt; expect from theory.
</em><br>
<em>&gt;
</em><br>
<em>&gt; While a very large machine may have a higher level of consciousness than a
</em><br>
<em>&gt; human, it has nothing to do with the number of people the machine will come
</em><br>
<em>&gt; in contact with.  Consciousness by any meaningful metric is a function of
</em><br>
<em>&gt; machine limits, not the number of people you meet.  Otherwise, humans that
</em><br>
<em>&gt; live in urban areas would be vastly more conscious than humans that live in
</em><br>
<em>&gt; rural areas, yet I see little evidence that this is true, nor does it really
</em><br>
<em>&gt; make sense from a theoretical standpoint.
</em><br>
<p>The point isn't how many people we bump into on the street, or
<br>
even our number of acquaintances, but how many we can know well.
<br>
As I said &quot;the human limit of knowing only about 200 people well&quot;,
<br>
which is discussed in the excellent book &quot;Biology of Mind&quot; by
<br>
Deric Bownds, available at <a href="http://dericbownds.net/">http://dericbownds.net/</a>.
<br>
<p>The difference between human and animal consciousness can be
<br>
described in terms of whether animal minds include models of
<br>
other animals minds, of events tommorrow, etc. Similarly, I think
<br>
a key difference between human and machine consciousness will
<br>
be the machines' detailed model of billions of human minds, in
<br>
contrast to our detailed model of about 200 human minds.
<br>
<p>Because of the physical limits of human brains, our models of
<br>
billions of human minds are averaged out. But machine brains
<br>
that exceed our physical limits will be have detailed models
<br>
of billions of human minds.
<br>
<p><em>&gt; As a nitpick, Metcalf's Law isn't really being used in a correct context
</em><br>
<em>&gt; here.  (As an even more esoteric nitpick, Metcalf's Law isn't really correct
</em><br>
<em>&gt; anyway and you have to add some additional dimensions for an analog of it to
</em><br>
<em>&gt; even make sense in the general case, as a number of others with more time
</em><br>
<em>&gt; than I to spend on such things have pointed out.)
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; 4. In my opinion, the essential property of consciousness in
</em><br>
<em>&gt; &gt; humans and animals is that it enables brains to process
</em><br>
<em>&gt; &gt; experiences that are not actually occuring. This consciousness
</em><br>
<em>&gt; &gt; &quot;simulator&quot; evolved as a way to solve the temporal credit
</em><br>
<em>&gt; &gt; assignment problem in reinforcement learning, which is the
</em><br>
<em>&gt; &gt; problem of reinforcing behaviors when rewards happen much
</em><br>
<em>&gt; &gt; later than the behaviors, and when multiple behaviors precede
</em><br>
<em>&gt; &gt; rewards. Of course consciousness is not a simple linear
</em><br>
<em>&gt; &gt; simulator like a weather model, but consists of multiple agile
</em><br>
<em>&gt; &gt; and interacting threads. The consciousness of super-intelligent
</em><br>
<em>&gt; &gt; machines will simulate all of humanity and their interactions
</em><br>
<em>&gt; &gt; via billions of agile and interacting threads. This higher
</em><br>
<em>&gt; &gt; level consciousness will have detailed social knowledge that
</em><br>
<em>&gt; &gt; human social scientists can only estimate with statistics.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't really agree with much of this on some fundamental grounds, but I
</em><br>
<em>&gt; don't feel like spending too much time on this tonight either.  Again, it
</em><br>
<em>&gt; seems to be based on a confused model of what intelligence is in the context
</em><br>
<em>&gt; of any kind of computing machinery.  Lots of suitcase terms are used that
</em><br>
<em>&gt; make rigorous discussion almost impossible.
</em><br>
<p>The statement &quot;the essential property of consciousness in humans
<br>
and animals is that it enables brains to process experiences that
<br>
are not actually occuring&quot; says something pretty rigorous. The
<br>
simplest animal brains can only process events as they happen.
<br>
But at some level of evolution, brains break free of &quot;now&quot;.
<br>
<p>And the temporal credit assignment problem is a well known
<br>
and rigorous problem. There has been some very exciting
<br>
neuroscience into how brains solve this problem, at least when
<br>
delays between behaviors and rewards are short and predictable,
<br>
in the paper:
<br>
<p>&nbsp;&nbsp;Brown, J., Bullock, D., and Grossberg, S. How the Basal Ganglia
<br>
&nbsp;&nbsp;Use Parallel Excitatory and Inhibitory Learning Pathways to
<br>
&nbsp;&nbsp;Selectively Respond to Unexpected Rewarding Cues. Journal of
<br>
&nbsp;&nbsp;Neuroscience 19(23), 10502-10511. 1999.
<br>
<p>This is available on-line at:
<br>
<p>&nbsp;&nbsp;<a href="http://cns-web.bu.edu/pub/diana/BroBulGro99.pdf">http://cns-web.bu.edu/pub/diana/BroBulGro99.pdf</a>
<br>
<p>I think that the need to solve the temporal credit assignment
<br>
problem when delays between behaviors and rewards are not
<br>
short and predictable was the selectional force behind the
<br>
evolution of consciousness. Any known effective solution to
<br>
this problem requires a simulation model of the world.
<br>
<p><em>&gt; Or at least these are my first analytical impressions based on the four
</em><br>
<em>&gt; ideas provided.  I'm sure someone who has actually read it might come away
</em><br>
<em>&gt; with a different impression than I got from the above. :-)
</em><br>
<em>&gt;
</em><br>
<em>&gt; (CC-ed to the author in case he is interested in feedback.)
</em><br>
<p>Thanks for your comments.
<br>
<p>Cheers,
<br>
Bill
<br>
----------------------------------------------------------
<br>
Bill Hibbard, SSEC, 1225 W. Dayton St., Madison, WI  53706
<br>
<a href="mailto:test@doll.ssec.wisc.edu?Subject=Re:%20New%20Singularity-relevant%20book">test@doll.ssec.wisc.edu</a>  608-263-4427  fax: 608-263-6738
<br>
<a href="http://www.ssec.wisc.edu/~billh/vis.html">http://www.ssec.wisc.edu/~billh/vis.html</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5641.html">Ben Goertzel: "RE: New Singularity-relevant book"</a>
<li><strong>Previous message:</strong> <a href="5639.html">James Rogers: "Re: New Singularity-relevant book"</a>
<li><strong>In reply to:</strong> <a href="5639.html">James Rogers: "Re: New Singularity-relevant book"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5641.html">Ben Goertzel: "RE: New Singularity-relevant book"</a>
<li><strong>Reply:</strong> <a href="5641.html">Ben Goertzel: "RE: New Singularity-relevant book"</a>
<li><strong>Reply:</strong> <a href="5642.html">James Rogers: "Re: New Singularity-relevant book"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5640">[ date ]</a>
<a href="index.html#5640">[ thread ]</a>
<a href="subject.html#5640">[ subject ]</a>
<a href="author.html#5640">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
