<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: &quot;Supergoal&quot; considered harmful; New term: &quot;Quality Function&quot;</title>
<meta name="Author" content="pdugan (pdugan@vt.edu)">
<meta name="Subject" content="RE: &quot;Supergoal&quot; considered harmful; New term: &quot;Quality Function&quot;">
<meta name="Date" content="2005-07-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: &quot;Supergoal&quot; considered harmful; New term: &quot;Quality Function&quot;</h1>
<!-- received="Sun Jul 17 15:35:59 2005" -->
<!-- isoreceived="20050717213559" -->
<!-- sent="Sun, 17 Jul 2005 17:35:06 -0400" -->
<!-- isosent="20050717213506" -->
<!-- name="pdugan" -->
<!-- email="pdugan@vt.edu" -->
<!-- subject="RE: &quot;Supergoal&quot; considered harmful; New term: &quot;Quality Function&quot;" -->
<!-- id="43A56CEF@zathras" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="&quot;Supergoal&quot; considered harmful; New term: &quot;Quality Function&quot;" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> pdugan (<a href="mailto:pdugan@vt.edu?Subject=RE:%20&quot;Supergoal&quot;%20considered%20harmful;%20New%20term:%20&quot;Quality%20Function&quot;"><em>pdugan@vt.edu</em></a>)<br>
<strong>Date:</strong> Sun Jul 17 2005 - 15:35:06 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11391.html">Philip Sutton: "Universalising an AGI's duty of care"</a>
<li><strong>Previous message:</strong> <a href="11389.html">Eliezer S. Yudkowsky: "Re: &quot;Supergoal&quot; considered harmful"</a>
<li><strong>Maybe in reply to:</strong> <a href="11384.html">Thomas Buckner: "&quot;Supergoal&quot; considered harmful; New term: &quot;Quality Function&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11386.html">William Chapin: "Re: &quot;Supergoal&quot; considered harmful"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11390">[ date ]</a>
<a href="index.html#11390">[ thread ]</a>
<a href="subject.html#11390">[ subject ]</a>
<a href="author.html#11390">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Maybe a robust quality function can only be kept consistently through rampant 
<br>
self-improvement by means of quantum gravitational computation, in other words 
<br>
perhaps the logic/math associated with an ideal quality function are incumbent 
<br>
on non-turing computable numbers. If this were true it would suggest that not 
<br>
only would the utility function help effect a universe whose value is 
<br>
corroborated by the quality function, but the utility function may actually 
<br>
serve to instantiate a robust quality function. For example say the utility 
<br>
function were to figure out how to increase computation resources from the 
<br>
original substrate to a level where the Penrose Hypothesis could be reasonably 
<br>
tested and a quantum gravity computation substrate could be instantiated to 
<br>
process the nessecary qualia a quality function would require. The causal 
<br>
validity semantics would keep this transition from killing us in the process, 
<br>
I imagine in my ignorance that a mind using less than a square meter of 
<br>
computronium should be able to figure out the required basis for computing a 
<br>
quality function, particularily figuring the degree to which the Penrose 
<br>
Hypothesis is true. Its quite reasonable to assume the possibility Mr. Penrose 
<br>
was just being anthropomorphic and his hypothesis is totally false, in which 
<br>
case the FAI should default to inductively creating a quality function based 
<br>
on Friendliness content reasoned over with good ol' fashioned 
<br>
turing-compatable processes. Any utility function aiming for this Holy Grail 
<br>
should have a shadow utility function which has proven consistency of 
<br>
Friendliness provided the Holy Grail doesn't exist.
<br>
<p>Patrick Dugan
<br>
<p><em>&gt;===== Original Message From Thomas Buckner &lt;<a href="mailto:tcbevolver@yahoo.com?Subject=RE:%20&quot;Supergoal&quot;%20considered%20harmful;%20New%20term:%20&quot;Quality%20Function&quot;">tcbevolver@yahoo.com</a>&gt; =====
</em><br>
<em>&gt;--- &quot;Eliezer S. Yudkowsky&quot; &lt;<a href="mailto:sentience@pobox.com?Subject=RE:%20&quot;Supergoal&quot;%20considered%20harmful;%20New%20term:%20&quot;Quality%20Function&quot;">sentience@pobox.com</a>&gt;
</em><br>
<em>&gt;wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; Should the AI be a good predictor, it will
</em><br>
<em>&gt;&gt; systematically steer reality into
</em><br>
<em>&gt;&gt; regions which its utility function assigns high
</em><br>
<em>&gt;&gt; utilities.  Thus, the term
</em><br>
<em>&gt;&gt; &quot;supergoal&quot; that I used in CFAI means simply
</em><br>
<em>&gt;&gt; &quot;utility function&quot;.  And if the
</em><br>
<em>&gt;&gt; AI is a good predictor, its utility function
</em><br>
<em>&gt;&gt; also serves as a good description
</em><br>
<em>&gt;&gt; of the target of the optimization process, the
</em><br>
<em>&gt;&gt; regions of reality into which
</em><br>
<em>&gt;&gt; the AI does in fact steer the future.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I think this is an appropriate time to apply my
</em><br>
<em>&gt;previous post about the NLP method of identifying
</em><br>
<em>&gt;'values' in the sense of 'what does this utility
</em><br>
<em>&gt;function achieve?' Perhaps for sl4 purposes a new
</em><br>
<em>&gt;word is needed, since mathematicians already mean
</em><br>
<em>&gt;something else by the word 'values', and it's
</em><br>
<em>&gt;apparently too late to decide that 'supergoal'
</em><br>
<em>&gt;can fill this function.
</em><br>
<em>&gt;
</em><br>
<em>&gt;So let me propose &quot;quality function&quot; as the new
</em><br>
<em>&gt;term; it implies 'qualia' since (frothy, dodgy
</em><br>
<em>&gt;word though it is) we can agree that pleasant
</em><br>
<em>&gt;qualia are what we humans are hoping to get from
</em><br>
<em>&gt;FAI; it also implies 'quality' in the sense of
</em><br>
<em>&gt;'quality of life' which, again, we are hoping to
</em><br>
<em>&gt;get from FAI.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I propose this since you (Eliezer, and perhaps
</em><br>
<em>&gt;others) seem to have punted the question of &quot;What
</em><br>
<em>&gt;is the optimum utility function *in humans* that
</em><br>
<em>&gt;the FAI utility function is supposed to achieve?&quot;
</em><br>
<em>&gt;It seems to me that you have found flaws with all
</em><br>
<em>&gt;pre-existing ideas about what this ultimate human
</em><br>
<em>&gt;goal or set of goals could be, which presents a
</em><br>
<em>&gt;seemingly intractable Alphonse-and-Gaston dilemma
</em><br>
<em>&gt;(You first! No, you first! No, I insist, sir! And
</em><br>
<em>&gt;so on.) The seed AI programmer dare not make any
</em><br>
<em>&gt;assumptions beforehand about CV without first
</em><br>
<em>&gt;letting the superior mind of the AI judge the
</em><br>
<em>&gt;likelihood of success, but if the AI gets ahead
</em><br>
<em>&gt;of the human programmer, ve may not be Friendly
</em><br>
<em>&gt;or fully informed about the needs of humans, and
</em><br>
<em>&gt;offer wrong answers, and around and around we go.
</em><br>
<em>&gt;
</em><br>
<em>&gt;So, like it or not, we need more confidence that
</em><br>
<em>&gt;we understand the quality function, from which we
</em><br>
<em>&gt;derive the AI's utility function.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I'm thinking of Monty Python and the Holy Grail.
</em><br>
<em>&gt;King Arthur, at the end, are convinced that the
</em><br>
<em>&gt;French knights in the castle have the Grail, and
</em><br>
<em>&gt;one of their failed gambits is the building of a
</em><br>
<em>&gt;wooden rabbit, which is catapulted back and
</em><br>
<em>&gt;crushes one or two Englishmen. A nested view of
</em><br>
<em>&gt;their utility functions (UF), with each step
</em><br>
<em>&gt;intended to achieve the next, higher value, might
</em><br>
<em>&gt;run as follows:
</em><br>
<em>&gt;
</em><br>
<em>&gt;UF Build wooden rabbit =&gt; UF infiltrate castle =&gt;
</em><br>
<em>&gt;UF overpower French knights =&gt; UF search castle
</em><br>
<em>&gt;=&gt; UF find Grail =&gt; QF serve Greater Glory of
</em><br>
<em>&gt;God.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Fans of the film will know that the plan broke
</em><br>
<em>&gt;down at the second step, due to the failure to
</em><br>
<em>&gt;put anybody in the wooden rabbit (obviously, any
</em><br>
<em>&gt;utility function might depend on more than one
</em><br>
<em>&gt;lower function, so the actual nested view could
</em><br>
<em>&gt;be a tree converging on a single trunk function;
</em><br>
<em>&gt;when you are down to one trunk or a set of equal
</em><br>
<em>&gt;and indispensable trunks, you've found your QF).
</em><br>
<em>&gt;But there were many assumptions underlying the
</em><br>
<em>&gt;English plan; once inside, they might have lost
</em><br>
<em>&gt;the fight; the French might not really have had
</em><br>
<em>&gt;the Grail; Jehovah's original geas to go find the
</em><br>
<em>&gt;Grail might have been a mass hallucination!
</em><br>
<em>&gt;Perhaps, in order to serve the highest quality
</em><br>
<em>&gt;function (Greater Glory of God), Arthur and his
</em><br>
<em>&gt;knights should have just gone home and helped
</em><br>
<em>&gt;feed the poor.
</em><br>
<em>&gt;
</em><br>
<em>&gt;This is a point where I think I misunderstood
</em><br>
<em>&gt;Eliezer all along; when he wrote 'supergoal (UF)'
</em><br>
<em>&gt;I thought he was talking about something a lot
</em><br>
<em>&gt;closer to 'supergoal (QF)'.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Now I expect Eliezer to fill my backside with
</em><br>
<em>&gt;buckshot for proposing a quality function that
</em><br>
<em>&gt;floats atop the utility function with no
</em><br>
<em>&gt;mathematical means of support, but it can't be
</em><br>
<em>&gt;helped (yet?) The quality function, in the end,
</em><br>
<em>&gt;would be what all discussion of CV, domain
</em><br>
<em>&gt;protection, singularity fun theory, and so on,
</em><br>
<em>&gt;are intended to produce: a
</em><br>
<em>&gt;mathematically/logically sturdy formulation of
</em><br>
<em>&gt;what it is that the utility function is meant *by
</em><br>
<em>&gt;us* to achieve. A paperclip maximizer, in this
</em><br>
<em>&gt;light, is an AI that performs the utility
</em><br>
<em>&gt;function flawlessly but makes a total wreck of
</em><br>
<em>&gt;the quality function (or punishes us for not
</em><br>
<em>&gt;having nailed down the quality function in time
</em><br>
<em>&gt;for the AI to make use of it).
</em><br>
<em>&gt;
</em><br>
<em>&gt;Tom Buckner
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;____________________________________________________
</em><br>
<em>&gt;Start your day with Yahoo! - make it your home page
</em><br>
<em>&gt;<a href="http://www.yahoo.com/r/hs">http://www.yahoo.com/r/hs</a>
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11391.html">Philip Sutton: "Universalising an AGI's duty of care"</a>
<li><strong>Previous message:</strong> <a href="11389.html">Eliezer S. Yudkowsky: "Re: &quot;Supergoal&quot; considered harmful"</a>
<li><strong>Maybe in reply to:</strong> <a href="11384.html">Thomas Buckner: "&quot;Supergoal&quot; considered harmful; New term: &quot;Quality Function&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11386.html">William Chapin: "Re: &quot;Supergoal&quot; considered harmful"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11390">[ date ]</a>
<a href="index.html#11390">[ thread ]</a>
<a href="subject.html#11390">[ subject ]</a>
<a href="author.html#11390">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:22:58 MST
</em></small></p>
</body>
</html>
