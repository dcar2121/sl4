<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: A Glimpse of the Future -- Or, Has SF Gone Blind?</title>
<meta name="Author" content="Ralph Cerchione (figment@boone.net)">
<meta name="Subject" content="A Glimpse of the Future -- Or, Has SF Gone Blind?">
<meta name="Date" content="2005-07-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>A Glimpse of the Future -- Or, Has SF Gone Blind?</h1>
<!-- received="Sun Jul 17 22:17:00 2005" -->
<!-- isoreceived="20050718041700" -->
<!-- sent="Mon, 18 Jul 2005 00:16:47 -0400" -->
<!-- isosent="20050718041647" -->
<!-- name="Ralph Cerchione" -->
<!-- email="figment@boone.net" -->
<!-- subject="A Glimpse of the Future -- Or, Has SF Gone Blind?" -->
<!-- id="008801c58b4f$84b663b0$599e133f@ralph" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="3ad827f305071313595979b0fc@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ralph Cerchione (<a href="mailto:figment@boone.net?Subject=Re:%20A%20Glimpse%20of%20the%20Future%20--%20Or,%20Has%20SF%20Gone%20Blind?"><em>figment@boone.net</em></a>)<br>
<strong>Date:</strong> Sun Jul 17 2005 - 22:16:47 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11589.html">Ralph Cerchione: "A Host of Variables -- Part I -- Computers, Biotech, and Who Will Transcend First..."</a>
<li><strong>Previous message:</strong> <a href="11587.html">Ralph Cerchione: "Augmentation: Is the Train Leaving the Station?"</a>
<li><strong>In reply to:</strong> <a href="11473.html">justin corwin: "Re: Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11471.html">Eliezer S. Yudkowsky: "Re: Fighting UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11588">[ date ]</a>
<a href="index.html#11588">[ thread ]</a>
<a href="subject.html#11588">[ subject ]</a>
<a href="author.html#11588">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hello, all. Here's another article I originally posted yesterday on my blog 
<br>
at
<br>
<a href="http://futureimperative.blogspot.com/2005/07/glimpse-of-future-or-has-sf-gone-blind.html">http://futureimperative.blogspot.com/2005/07/glimpse-of-future-or-has-sf-gone-blind.html</a>
<br>
<p>Unfortunately, all of the links were stripped out of this copy as well. See 
<br>
the above link if you want those connections.
<br>
<p>Comments on the following piece are more than welcome.
<br>
<p>Ralph
<br>
<p><pre>
----
The question &quot;Is Science Fiction About to Go Blind?&quot; has arisen more and 
more often in recent years as the idea of a &quot;technological Singularity&quot; has 
caught on in SF -- literally a rate of accelerating technological change so 
swift as to be beyond modern human comprehension. Much less our ability to 
meaningfully predict its course. For obvious reasons, if you believe that we 
will experience technological progress that pronounced in the near future, 
the range of future scenarios you can meaningfully write about is 
correspondingly diminished. Many science fiction writers who anticipate such 
an era are constantly trying to expand that spectrum of possibilities, but 
it often proves challenging.
The article linked above describes some of the problems faced by the SF 
field. Situations from Charles Stross' novel Accelerando (which is given 
away free online here) are used to illustrate some of the radical changes 
that could take place in the event of runaway AI and nanotech breakthroughs. 
While that article is interesting and well worth reading, I'd like to look 
at a slightly different problem -- what do we lose if science fiction stops 
being a lens that surveys the future for the rest of humanity, if it loses 
the predictive power that its best examples have had over the last two 
centuries?
Consider Brave New World, 1984, R.U.R. or even modern films such as 
Gattacca. Or, for that matter, the venerable novels of Verne or Shelley or 
visionaries of human evolution from Olaf Stapledon to William Gibson and 
Vernor Vinge. Works such as these often introduce a wider audience to 
critical issues they had no idea existed.
It's been said that the greatest contribution that Brave New World and 1984 
made in describing their respective dystopias was in insuring those futures 
never came to pass. These two potential worlds -- warped, respectively, by 
massive, misguided human social engineering and a ruthless, all-controlling 
totalitarian state -- are now classics in the genre that asks &quot;What would be 
so bad about doing ___?&quot; R.U.R., of course, coined the word &quot;robot&quot; while 
simultaneously asking what happens in a world where all human labor has been 
replaced by the efforts of intelligent machines.
Gattacca looked at how radically American society could change with just a 
single technology -- exceptionally cheap, fast and accurate genetic scans... 
which would enable the selection of superior embryos, the screening of the 
&quot;genetically unfit&quot; and the use of DNA analysis in every forensic crime 
scene. How quickly the future has come upon us.
And Gibson and Vinge, of course, are known for their respective visions of 
cyberpunk and technological Singularities -- both of which relate to this 
site's focus of radical human enhancement and which have, more importantly, 
influenced many futurists, philosophers and artificial intelligence 
researchers. There are more obscure works, such as Greg Egan's novel Blood 
Music, which anticipated nanotechnology well before Engines of Creation (as 
did the character of Warlock in the comic book The New Mutants, though no 
one wants to discuss that fact =) ), or Arthur C. Clarke's Fountains of 
Paradise, which envisioned a geostationary elevator out of Earth's gravity 
well... not to mention Clarke's non-fiction explanation of how to put 
geostationary communications satellites in orbit to revolutionize 
telecommunications. Which they did.
What's my point? Science fiction propogates otherwise obscure ideas about 
the future among many different audiences -- whether among potential 
nanotech innovators reading Blood Music, ordinary American voters watching 
Gattacca, early 20th Century labor organizers taking in R.U.R., early 
telecom or aerospace engineers reading Clarke or future civil rights 
activists contemplating 1984. In each case, the critical audience may differ 
dramatically -- a few scientists or inventors spurred to develop a 
technology in one case may serve as the idea's &quot;critical mass,&quot; in other 
cases, it may be the widespread comprehension of millions regarding a 
technology's implications will change the course of history.
When science fiction dramatically restricts its vision to narrowly defined 
possibilities -- whether space opera stories, post-apocalyptic realities or 
your choice of post-Singularity/ post-humanity futures -- the field as a 
whole loses much of its ability to surprise as it treads and retreads the 
same overtaxed plot of ground. That's not to say that there aren't plenty of 
great stories left involving nanites or AIs (or space fleets or holocaust 
aftermaths), but if every &quot;serious SF writer&quot; ends up tramping down the same 
path, we're going to end up mssing a lot of insights.
In fairness to writers fascinated by Singularities, it's worth nothing that 
many writers, while their technological timescales may be greatly 
accelerated, do consider the impact of radically advanced technology on 
human society. It's just that they anticipate its arrival being just around 
the corner, and they generally don't expect &quot;society as we know it&quot; to last 
very long thereafter. Nevertheless, there are some interesting stories 
packed into those compressed timespans.
Perhaps more intriguing in this vein are writers such as Ken Macleod who 
anticipate the survival of some kind of human civilization in their 
stories -- if one that is much shakier and less populous than the one we 
have today. And which exists in the shadow of incomprehensibly powerful 
intelligences.
These are interesting scenarios to contemplate. However, lest the field one 
day devolve to a &quot;cheesy space opera&quot;/&quot;bug-eyed monsters&quot; level of recycled 
plots, I thought I'd do my part by pointing out just a few of the questions 
worthy of the SF's serious consideration, particularly at this stage of 
history. A few of which actually fit in pretty well in Singularity SF, if 
you think about them.
In what ways can human beings be enhanced, whether in terms of intelligence, 
health, speed, looks, whatever? To what degree can they be enhanced while 
still remaining fundamentally &quot;human&quot;? (And what does &quot;human&quot; mean, while 
we're at it?)
To what degree can various methods of radical human enhancement be 
synergized? Will biological beings be able to compete at all with 
non-biological intelligences? Or -- heresy though it is to ask -- will AIs 
be able to compete at all with biological or cyborged intellects?
Will recursively self-improving intelligences result in the development of 
unspeakably powerful AIs -- or unspeakably powerful human/post-human minds, 
if the world's computational resources and scientific innovation are turned 
towards the refinement of human/biological intelligence instead of 
artificial thought?
How many different versions (or factions) of &quot;superior beings&quot; might a 
technologically evolving Earth/solar system/galaxy en up playing host to? 
How might they get along? How might they learn to get along, if the only 
alternative were wasteful (if not genocidal) conflict?
How does ordinary humanity maintain its rights and independence in the face 
of a newly evolved &quot;higher intelligence&quot;? Will humanity (or a large 
proportion of it) be forced to self-evolve in response in a kind of &quot;arms 
race&quot; or at least a push to blunt the most dramatic advantages a superior 
intellect might hold over &quot;masses of ordinary men&quot;?
Will human beings -- either normal modern ones, geniuses, or significantly 
more advanced near-future near-humans -- be able to offer higher 
intelligences anything? Here's a fictional comparison of where various 
intelligences fall on one imaginary scale. Consider how far down even the 
most advanced of modern humans would sit on this measure of sentience, and 
then consider this yardstick was specifically designed to make &quot;mere 
mortals&quot; a measurable quantity next to the celestial minds it conemplates.
What happens if the difference between &quot;transhuman&quot; minds and conventional 
geniuses becomes as great as between ordinary genius and the severely 
retarded? Even if there are no issues of wealth, power and recognition, what 
happens if &quot;the rest of us&quot; become keenly aware of how irrelevent we are to 
the next step on the evolutionary ladder?
Is there anything ordinary humanity can do to effectively influence human 
evolution, whether dramatically hastening it, delaying it, redirecting it or 
&quot;putting the genie back in the bottle&quot;? How can national/international 
education, government and/or R&amp;D funding shape these unfolding 
possibilities?
Whew. There's actually a lot more to talk about, most of which has nothing 
directly to do with human enhancement at all. But I think I've described 
enough already to illustrate my point. Even without journeying too far from 
&quot;ye olde Singularity territory&quot; I've found quite a bit of material that is 
foreboden in a strict, &quot;the AI gods shall rule all flesh,&quot; AI/nano, &quot;hard 
takeoff&quot; Singularity SF. (An exhausting definition just to write. But oddly 
enough, an accurate one.)
Future Imperative -- A broad look at human enhancement, from gene therapy to
accelerated learning, from neural implants to smart drugs, from posthuman
evolution to the wildest flights of human imagination.
<a href="http://futureimperative.blogspot.com">http://futureimperative.blogspot.com</a> 
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11589.html">Ralph Cerchione: "A Host of Variables -- Part I -- Computers, Biotech, and Who Will Transcend First..."</a>
<li><strong>Previous message:</strong> <a href="11587.html">Ralph Cerchione: "Augmentation: Is the Train Leaving the Station?"</a>
<li><strong>In reply to:</strong> <a href="11473.html">justin corwin: "Re: Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11471.html">Eliezer S. Yudkowsky: "Re: Fighting UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11588">[ date ]</a>
<a href="index.html#11588">[ thread ]</a>
<a href="subject.html#11588">[ subject ]</a>
<a href="author.html#11588">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
