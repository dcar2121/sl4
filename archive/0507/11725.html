<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Collective Volition, next take</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Collective Volition, next take">
<meta name="Date" content="2005-07-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Collective Volition, next take</h1>
<!-- received="Sat Jul 23 12:55:50 2005" -->
<!-- isoreceived="20050723185550" -->
<!-- sent="Sat, 23 Jul 2005 11:55:49 -0700" -->
<!-- isosent="20050723185549" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Collective Volition, next take" -->
<!-- id="42E292B5.9040308@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="8d71341e05072311123b1be271@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Collective%20Volition,%20next%20take"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Jul 23 2005 - 12:55:49 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11726.html">Martin Striz: "Re: hostile slightly enhanced humans"</a>
<li><strong>Previous message:</strong> <a href="11724.html">Russell Wallace: "Re: Collective Volition, next take"</a>
<li><strong>In reply to:</strong> <a href="11722.html">Russell Wallace: "Re: Collective Volition, next take"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11728.html">Russell Wallace: "Re: Collective Volition, next take"</a>
<li><strong>Reply:</strong> <a href="11728.html">Russell Wallace: "Re: Collective Volition, next take"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11725">[ date ]</a>
<a href="index.html#11725">[ thread ]</a>
<a href="subject.html#11725">[ subject ]</a>
<a href="author.html#11725">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Russell Wallace wrote:
<br>
<em>&gt; CV throws vast resources of intelligence - information-processing ability -
</em><br>
<em>&gt; behind moral axioms evolved for survival on the plains of Africa, and then
</em><br>
<em>&gt; - this is the problem - proceeds as though with unlimited power comes 
</em><br>
<em>&gt; unlimited moral authority.
</em><br>
<p>I'm not sure what you mean by &quot;moral axioms&quot;.  Human goal systems don't 
<br>
decompose cleanly and orthogonally into moral axioms + everything else.  If 
<br>
they did, my life would be a lot simpler.
<br>
<p>In CV - which, by the way, I really should have called &quot;Collective 
<br>
Extrapolated Volition&quot; - I called for defining a family of enhancements 
<br>
applicable to abstractions of human minds and human society, such that the 
<br>
extrapolation of abstract interacting enhanced humans could get far enough to 
<br>
return a legitimate answer to the question, &quot;What sort of AI would we want if 
<br>
we were smarter?&quot;  This is one question.  It could have more than one answer, 
<br>
depending on how you define the extrapolation process.  But if you do multiple 
<br>
extrapolations, you have to define some way for the extrapolations to 
<br>
interact, and you can't execute a single rewrite on the interaction framework, 
<br>
meaning the basic level is now permanently hardcoded at the programmers' level 
<br>
of intelligence and wisdom, which is a bad bad bad thing.  So the Collective 
<br>
Extrapolated Volition returns one answer to one question, one AI rewrite to 
<br>
start off the next round.  It doesn't mean that the one answer is &quot;We want an 
<br>
AI that will mess with our individual destinies according to a uniform set of 
<br>
averaged-out moral rules!&quot;
<br>
<p>Look, from the outside - to anyone who's not on the SIAI programming team - 
<br>
what the programmers are doing (forget about how they do it) is supposed to be 
<br>
intuitively simple.  The programmers create an enormously powerful question 
<br>
mark whose question is &quot;What AI do we want to happen next?&quot;  I frankly do not 
<br>
understand exactly where you think an error inevitably occurs in this 
<br>
framework.  Are you afraid of getting what you want?  Are you afraid that most 
<br>
other people want something different (if so, why should SIAI listen to you, 
<br>
not them?)  Or are you worried that building a Collective Extrapolated 
<br>
Volition as the fleshed-out, real-world implementation of the question mark 
<br>
inherently defines 'wanting' in some sense other than the intuitive, the sense 
<br>
in which you don't 'want' the future to be a giant ball of worms or whatever? 
<br>
&nbsp;&nbsp;You've got to mean one of those three and it's not clear which.
<br>
<p><em>&gt; In reality, a glut of intelligence/power 
</em><br>
<em>&gt; combined with confinement - a high ratio of force to space - triggers the
</em><br>
<em>&gt; K-strategist elements of said axiom system, applying selective pressure in
</em><br>
<em>&gt; favor of memes corresponding to the moral concept of &quot;evil&quot;. (Consider the
</em><br>
<em>&gt; trend in ratio of lawyers to engineers in the population over the last
</em><br>
<em>&gt; century for an infinitesimal foreshadowing.)
</em><br>
<p>Dude, what the *heck* are you talking about?
<br>
<p><em>&gt; In pursuit of a guiding light that isn't there, the RPOP would extrapolate
</em><br>
<em>&gt; the interaction between K-strategist genes and parasite memes and force the
</em><br>
<em>&gt; result, with utter indifference to the consequences, on the entire human
</em><br>
<em>&gt; race. There will be no goal system for any element of the Collective but
</em><br>
<em>&gt; power - not clean power over the material world (which will effectively
</em><br>
<em>&gt; have ceased to exist except as the RPOP's substrate) but power always and
</em><br>
<em>&gt; only over others - a regime of the most absolute, perfectly distilled evil
</em><br>
<em>&gt; ever contemplated by the human mind. (Watch an old movie called &quot;Zardoz&quot;
</em><br>
<em>&gt; for a little more foreshadowing.)
</em><br>
<p>Is this what you think would inevitably happen if, starting with present human 
<br>
society, the average IQ began climbing by three points per year?  At what 
<br>
point - which decade, say - do you think humans would be so intelligent, know 
<br>
so much and think so quickly, that their society would turn utterly evil?  Or 
<br>
if this is not what you think would happen, why would the AI mistakenly 
<br>
extrapolate that such a society would turn utterly evil?  I don't understand 
<br>
what you conceive to be the chain of cause and effect.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11726.html">Martin Striz: "Re: hostile slightly enhanced humans"</a>
<li><strong>Previous message:</strong> <a href="11724.html">Russell Wallace: "Re: Collective Volition, next take"</a>
<li><strong>In reply to:</strong> <a href="11722.html">Russell Wallace: "Re: Collective Volition, next take"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11728.html">Russell Wallace: "Re: Collective Volition, next take"</a>
<li><strong>Reply:</strong> <a href="11728.html">Russell Wallace: "Re: Collective Volition, next take"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11725">[ date ]</a>
<a href="index.html#11725">[ thread ]</a>
<a href="subject.html#11725">[ subject ]</a>
<a href="author.html#11725">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
