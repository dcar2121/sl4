<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Magic means large search spaces</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Magic means large search spaces">
<meta name="Date" content="2005-07-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Magic means large search spaces</h1>
<!-- received="Thu Jul 21 11:23:34 2005" -->
<!-- isoreceived="20050721172334" -->
<!-- sent="Thu, 21 Jul 2005 10:23:42 -0700" -->
<!-- isosent="20050721172342" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Magic means large search spaces" -->
<!-- id="42DFDA1E.2040703@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Magic%20means%20large%20search%20spaces"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu Jul 21 2005 - 11:23:42 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11677.html">pdugan: "RE: Fighting UFAI"</a>
<li><strong>Previous message:</strong> <a href="11675.html">Norm Wilson: "RE: Objective versus subjective reality: which is primary?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11679.html">Phil Goetz: "&quot;Hostile&quot; transhumans (Re: Magic means large search spaces)"</a>
<li><strong>Reply:</strong> <a href="11679.html">Phil Goetz: "&quot;Hostile&quot; transhumans (Re: Magic means large search spaces)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11676">[ date ]</a>
<a href="index.html#11676">[ thread ]</a>
<a href="subject.html#11676">[ subject ]</a>
<a href="author.html#11676">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michael Vassar correctly wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; c) &quot;magic&quot; has to be accounted for.  How many things can you do that a dog 
</em><br>
<em>&gt; would simply NEVER think of?
</em><br>
<p>Daniel Radetsky wrote:
<br>
<em>&gt; &quot;Ben Goertzel&quot; &lt;<a href="mailto:ben@goertzel.org?Subject=Re:%20Magic%20means%20large%20search%20spaces">ben@goertzel.org</a>&gt; wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; How about the argument that every supposedly final and correct theory of
</em><br>
<em>&gt;&gt; physics we humans have come up with, has turned out to be drastically 
</em><br>
<em>&gt;&gt; wrong....
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This provides an infinitesimal degree of support to the claim that the real
</em><br>
<em>&gt; final and correct theory would permit magic...
</em><br>
<p>Only if 'magic' is interpreted as drawing mystic circles and waving your 
<br>
hands.  'Magic' takes on a different meaning here - it means, simply, anything 
<br>
you didn't think of.  Not just anything a human would simply NEVER think of, 
<br>
but also anything YOU didn't think of - the former being a subset of the 
<br>
latter and therefore also 'magic'.  The point of the AI-Box Experiments is 
<br>
that I can do 'magic' in the latter sense relative to some people who firmly 
<br>
stated that NOTHING could possibly persuade them to let an AI out of the box. 
<br>
&nbsp;&nbsp;Obviously, being human, I did nothing that was *strongly* magical.
<br>
<p>The problem of magic is the problem of a very large search space, in a case 
<br>
where we not only lack the brainpower to search each element of the space, we 
<br>
may lack the brainpower to properly delineate the search space.  The AI is not 
<br>
limited to breaking out via any particular method you consider.  Neither you 
<br>
nor the AI have enough computing power to consider the *entire* search space, 
<br>
but the AI may search much more efficiently than you do (not to mention a lot 
<br>
faster).  Thus, your inabiliity to think of a way out yourself, is only slight 
<br>
evidence that the AI will be unable to think of a way out.  Similarly, the 
<br>
conviction of certain people that no possible mind, even a transhuman, would 
<br>
be unable to persuade them to let the AI out of a box; was not strong evidence 
<br>
that I would be unable to persuade them to let the AI out of the box. 
<br>
Creativity has the appearance of magic when, even after the fact, you don't 
<br>
know how it worked.  The AI-Box Experiment is a lesson in the tendency of 
<br>
sufficiently large search spaces to contain magic.  That is why I refuse to 
<br>
publish transcripts.  Get used to the existence of magic.
<br>
<p>The argument which Ben Goertzel cites is that, since physics has changed over 
<br>
the last few generations, we should anticipate that we have stated the search 
<br>
space incorrectly when we consider all physical means by which the AI might 
<br>
break out of the box.  This does not mean that the AI *has* to go outside 
<br>
known physics to break out, because there might also be an escape route in 
<br>
known physics that you did not think of.  Consider OpenBSD, the most secure OS 
<br>
you can obtain on an open market.  OpenBSD is constantly changing as people 
<br>
discover new bugs and fix them.  Our fundamental physics is in less flux than 
<br>
OpenBSD, though arguably over history fundamental physics has gone through 
<br>
more total change than OpenBSD.  I don't know how to break into an OpenBSD 
<br>
box, since I am not a security expert, as you are not a physicist.  I 
<br>
anticipate that even some people whose job title says &quot;system administrator&quot; 
<br>
wouldn't be able to break into a patched OpenBSD box.  I have no idea whether 
<br>
NSA spooks can break into an OpenBSD box but my money would be on 'yes'.  The 
<br>
fact that OpenBSD is often discovering new bugs is a strong argument that 
<br>
additional security flaws exist.  Of course there are *presently* no security 
<br>
flaws known to the open-source community or they would be fixed, just as there 
<br>
are presently no known predictive breakdowns in modern physics.  But we expect 
<br>
there are more bugs in OpenBSD and we expect there are more bugs in our model 
<br>
of physics.
<br>
<p>Even though I can think of absolutely no way to break into an OpenBSD box 
<br>
using only an Internet connection, I anticipate that an AI could break into an 
<br>
OpenBSD box using only an Internet connection, using 'magic'.  Maybe this 
<br>
would involve going outside the search space that I or NSA spooks think about; 
<br>
though probably there are answers within the search space too.  Who can guess 
<br>
which would be more efficient?  Going outside the search space involves some 
<br>
extra trouble in any particular case, but building the infrastructure for 
<br>
magic might save the AI effort in the long run.  From our perspective the 
<br>
results are the same whether the loophole is creative (comprehensible in 
<br>
retrospect) or magic (inexplicable even after we witness the results).
<br>
<p><em>&gt; For example, we might say that the AI might be able to manipulate its 
</em><br>
<em>&gt; hardware in such a way as to create electromagnetic fields (or some such 
</em><br>
<em>&gt; thing) and &quot;break the box.&quot; Given the AI's degree of flexibility in the 
</em><br>
<em>&gt; hardware, there is some chance that it will succeed. If the chance is 
</em><br>
<em>&gt; vanishingly small, we say that this exploit in the box's solidity is not 
</em><br>
<em>&gt; reasonably exploitable.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So, I want to know why you believe there is a exploit which is reasonably 
</em><br>
<em>&gt; exploitable.
</em><br>
<p>If you consider any *single* element of a large search space, the probability 
<br>
remains infinitesimal that *that single element* is an escape route.  It is 
<br>
the probability of the whole search space that is the problem.  If I consider 
<br>
a single bitstring targeted at a single TCP/IP port, its probability of 
<br>
breaking the OpenBSD box is very low.  If an AI sends out that exact bitstring 
<br>
then the probability is still very low, presuming there are no free variables 
<br>
to manipulate such as the time of attack.  Similarly the probability that 
<br>
classical magic, drawing mystical circles, will work, remains low even if it 
<br>
is an AI drawing the mystic circles.  But if the AI can send arbitrarily 
<br>
formed bitstrings to any port, then the probability of a working exploit 
<br>
existing is high, and the probability of a seed AI being able to find at least 
<br>
one such exploit, I also estimate to be high.
<br>
<p>When you cite particular physical means of breaking a box and their apparent 
<br>
implausibility to you, you are simply saying that some particular bitstring 
<br>
probably does not obtain root on an OpenBSD box.  What of it?  How many things 
<br>
can you do that a dog would simply NEVER think of?
<br>
<p>Daniel Radetsky wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I can't, but I submit that no one on this list has any basis to assess the 
</em><br>
<em>&gt; probability either. So if I claim that the probability is infinitesimal,
</em><br>
<em>&gt; then your only basis for disagreement is pure paranoia, which I feel
</em><br>
<em>&gt; comfortable dismissing.
</em><br>
<p>That's not how rationality works.  If you don't know the answer you are not 
<br>
free to pick a particular answer and demand that someone disprove it.  It is 
<br>
analogous to finding a blank spot on your map of the world and rejoicing, not 
<br>
because you have new knowledge to discover, but because you can draw whatever 
<br>
you want to be there.  And once you have drawn your dragon or your comfortable 
<br>
absence of dragons, you become committed to your ignorance, and all knowledge 
<br>
is your enemy, for that it might wipe away that comfortable blank spot on the 
<br>
map, over which you drew... you have not made this error too greatly, but 
<br>
there have been others on SL4 committed to defending their comfortable 
<br>
ignorance.  There is no freedom in the way of cutting through to the correct 
<br>
answer.  It is a dance, not a walk.  On each step of that dance your foot must 
<br>
come down in exactly the right spot, neither to the left nor to the right.  If 
<br>
you say that the probability of this very large search space containing no 
<br>
exploit is 'infinitesimal', you must give reason for it.  If I say that the 
<br>
probability is 'certain', I must give reason for it.  You cannot hole up with 
<br>
your preferred answer and wait for someone to provide positive disproof; that 
<br>
may comfort you but it is not how truthseeking works.
<br>
<p>When there is a blank spot on the map our best guess is that it is &quot;similar&quot; 
<br>
to past experience.  The art consists of a detailed understanding of what it 
<br>
means to be &quot;similar&quot;.  Similarity of fundamental laws takes precedence over 
<br>
similarity of surface characteristics.  Many would-be flyers failed before the 
<br>
Wright Flyer flew, but if you could make a physical prediction of exactly when 
<br>
the other flyers would hit the ground, you could use the same quantitative 
<br>
model of aerodynamics to predict the Wright Flyer would fly.  So we should 
<br>
assume the blank spot on the map follows the same rules as the territory we 
<br>
know, interpreted at the most fundamental level possible.  Does this mean we 
<br>
assume that the blank spot on the map obeys known physics exactly?  Yes and 
<br>
no.  If we have any particular question of physics in which an exact, 
<br>
quantitative prediction is desired, then we have to assume that the prediction 
<br>
of present physics is the point of maximum probability.  If you have a 
<br>
computer containing a superintelligent AI, and you throw it off a roof 78.4 
<br>
meters high, and you want to know the computer's downward velocity when it 
<br>
hits the ground, the best *quantitative* guess is 39.2 meters/second.  If the 
<br>
computer does not hit the ground due to 'magic', i.e., some action performed 
<br>
by an intelligence that searches a space we cannot search as well ourselves 
<br>
nor correctly formulate, we have no idea where it will go or how fast it will 
<br>
be moving.  Hence the prediction of modern physics is by far the best *exact* 
<br>
guess.  That is one sense in which we presume the blank spot on the map 
<br>
resembles known territory.  But we are not committed to the absurd statement 
<br>
that we expect every one of our physical generalizations to prove correct in 
<br>
every possible experiment in the future, even though at any particular point 
<br>
any particular generalization is our best exact guess.  This is no more 
<br>
paradoxical than my simultaneous expectation that any specific ticket will not 
<br>
win the lottery and that some ticket will win the lottery.  My beliefs are 
<br>
probabilistic, so that any large number of individual statements can have a 
<br>
high probability, yet their conjunction a low probability.
<br>
<p>It is an interesting question how *exactly* to formulate the generalization, 
<br>
'All past models of physics except one have already proven incorrect, so I 
<br>
estimate a low probability that we are at the final step currently'.  Or how 
<br>
to note the facts that present physics has persisted over a subjective time 
<br>
consistent with past generalizations that were eventually disproven (i.e. not 
<br>
an unusually long time) or that there are known problems in the modern theory 
<br>
(i.e. reconciling quantum mechanics and general relativity).  This literally 
<br>
&quot;meta-physical&quot; generalization yields no specific predictions so it can't 
<br>
override physics in any specific case.
<br>
<p>But in the case where we have a superintelligent AI then we may need to think 
<br>
about a cognitive system that systematically searches a large space for *any* 
<br>
useful breakdown in our physical model (or anything we didn't think of that 
<br>
can be done within modern physics).  It's sort of like a man falling out of a 
<br>
plane.  How does a dog that knows physics, predict the unfolding of a 
<br>
parachute?  The answer is that the dog cannot calculate when the parachute 
<br>
will hit the ground, but the dog would be wise to allocate less probability 
<br>
mass than usual to the proposition that the man hits the ground at the time 
<br>
predicted by Galileo.  The dog may be justly confident that if the man waves 
<br>
his hands and chants &quot;Booga booga&quot; or if the man straps anvils to his feet 
<br>
that the man will still hit the ground.  But for the man to actually, reliably 
<br>
hit the ground, requires the truth of the enormous conjunction, &quot;No matter 
<br>
*what* the man does he will still hit the ground.&quot;
<br>
<p>To guess that physics might break down *somewhere*, or that known physics 
<br>
might contain some way to break out of the box, presumes that the blank spot 
<br>
is similar to known territory; but the presumption takes place at a higher 
<br>
level.  It is a generalization about intelligence, goals, creativity, and what 
<br>
happens when a higher intelligence encounters a space blank to us.  This last 
<br>
generalized mostly from human past civilizations contrasted to human future 
<br>
civilizations, because if we compared chimpanzees or lizards to humans we 
<br>
would have to conclude that the answer was just pure incomprehensible magic. 
<br>
But since the degree by which the human future outsmarted the human past is 
<br>
impressive enough to rule out AI-boxing as a good idea, there is no need to 
<br>
appeal to strong magic.
<br>
<p>If you consider in your security model a branch describing the existence of a 
<br>
hostile mind that is smarter than you are, you must assume that this branch of 
<br>
your security model is a total loss.  How many things can you do that a dog 
<br>
would simply NEVER think of?
<br>
<p>It may still make sense to try and make precautions against hostile 
<br>
transhumans because this is a likely failure mode, even of the branches of 
<br>
your security model that don't explicitly expect it.  A hope in hell is better 
<br>
than no hope, and the precautions may make sense in any case - be useful 
<br>
against nascent pre-transhumans particularly.  But if a branch of your 
<br>
security model involves an unknown probability of creating a hostile 
<br>
transhuman, you have to assume that this is an unknown probability of total 
<br>
loss, not rely on your dog to invent countermeasures.
<br>
<p>The problem with the AI-Box paradigm is that it assumes that the existence of 
<br>
a hostile transhuman is a manageable problem and makes this the foundation of 
<br>
the strategy.  Typically you assume in your security model that if the 
<br>
terrorists smuggle a nuke into New York City, set it up in the UN Building, 
<br>
escape to safe distance, take the trigger out of their pockets, and put their 
<br>
finger on the trigger, well, you've sorta lost at that point.  Stop them if 
<br>
you can, any way you can, but your security model is supposed to rely on 
<br>
stopping the nuclear weapon EARLIER.  Maybe in Hollywood the hero crashes in 
<br>
through the door at this point, but that's not what security experts assume. 
<br>
Countries don't allow known terrorists through customs carrying nuclear 
<br>
weapons on the theory that, hey, the hero can always shoot them if they look 
<br>
like they're going to pull the trigger.  Allowing the existence of a hostile 
<br>
transhuman is just plain STUPID, end of story.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11677.html">pdugan: "RE: Fighting UFAI"</a>
<li><strong>Previous message:</strong> <a href="11675.html">Norm Wilson: "RE: Objective versus subjective reality: which is primary?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11679.html">Phil Goetz: "&quot;Hostile&quot; transhumans (Re: Magic means large search spaces)"</a>
<li><strong>Reply:</strong> <a href="11679.html">Phil Goetz: "&quot;Hostile&quot; transhumans (Re: Magic means large search spaces)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11676">[ date ]</a>
<a href="index.html#11676">[ thread ]</a>
<a href="subject.html#11676">[ subject ]</a>
<a href="author.html#11676">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
