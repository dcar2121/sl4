<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Fighting UFAI</title>
<meta name="Author" content="Tennessee Leeuwenburg (hamptonite@gmail.com)">
<meta name="Subject" content="Re: Fighting UFAI">
<meta name="Date" content="2005-07-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Fighting UFAI</h1>
<!-- received="Wed Jul 13 20:43:24 2005" -->
<!-- isoreceived="20050714024324" -->
<!-- sent="Thu, 14 Jul 2005 12:42:17 +1000" -->
<!-- isosent="20050714024217" -->
<!-- name="Tennessee Leeuwenburg" -->
<!-- email="hamptonite@gmail.com" -->
<!-- subject="Re: Fighting UFAI" -->
<!-- id="4f293f4050713194256a25bf1@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="42D5CDE7.9070204@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tennessee Leeuwenburg (<a href="mailto:hamptonite@gmail.com?Subject=Re:%20Fighting%20UFAI"><em>hamptonite@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Jul 13 2005 - 20:42:17 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11290.html">Eliezer S. Yudkowsky: "Re: Fighting UFAI"</a>
<li><strong>Previous message:</strong> <a href="11288.html">Eliezer S. Yudkowsky: "Re: Fighting UFAI"</a>
<li><strong>In reply to:</strong> <a href="11288.html">Eliezer S. Yudkowsky: "Re: Fighting UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11290.html">Eliezer S. Yudkowsky: "Re: Fighting UFAI"</a>
<li><strong>Reply:</strong> <a href="11290.html">Eliezer S. Yudkowsky: "Re: Fighting UFAI"</a>
<li><strong>Reply:</strong> <a href="11312.html">Dani Eder: "Re: Paperclip scenario"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11289">[ date ]</a>
<a href="index.html#11289">[ thread ]</a>
<a href="subject.html#11289">[ subject ]</a>
<a href="author.html#11289">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 7/14/05, Eliezer S. Yudkowsky &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Fighting%20UFAI">sentience@pobox.com</a>&gt; wrote:
<br>
<em>&gt; Tennessee Leeuwenburg wrote:
</em><br>
<em>&gt; &gt; What do people suppose the goals of a UFAI might be? Other than our
</em><br>
<em>&gt; &gt; destruction, of course. I'm assuming that UFAI isn't going to want our
</em><br>
<em>&gt; &gt; destruction just for its own sake, but consequentially, for other
</em><br>
<em>&gt; &gt; reasons.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I usually assume paperclips, for the sake of argument.  More realistically the
</em><br>
<em>&gt; UFAI might want to tile the universe with tiny smiley faces (if, as Bill
</em><br>
<em>&gt; Hibbard suggested, we were to use reinforcement learning on smiling humans) or
</em><br>
<em>&gt; most likely of all, circuitry that holds an ever-increasing representation of
</em><br>
<em>&gt; the pleasure counter.  It doesn't seem to make much of a difference.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; --
</em><br>
<em>&gt; Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt; Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<em>&gt; 
</em><br>
<p>I suppose I'm unwilling to accept the paperclips position to some
<br>
extent, for a variety of reasons.
<br>
<p>Is a truly intelligent AI ever going to make the kind of monumental
<br>
slip-up required to decide to do something so blatantly dumb as just
<br>
cover the universe in paperclips?
<br>
<p>The paperclip scenario, I always thought, was a danger posed by a
<br>
second-rate intelligence - a kind of incredibly powerful child.
<br>
Something which is given the tools to achieve goals easily, but not
<br>
given the rationality to reason its goals out.
<br>
<p>Does it really make sense that something as intelligent as a
<br>
post-singularity AI would miss such an obvious point?
<br>
<p>I know people have posed race conditions between FAI and paperclips,
<br>
but there seems to me to be a kind of contradiction inherent in any AI
<br>
which is intelligent enough to achieve one of these worst-case
<br>
outcomes, but is still capable of making stupid mistakes.
<br>
<p>Does it make sense that something so intelligent could have such mindless goals?
<br>
<p>I'm fairly willing to accept that UFAI might see a need for human
<br>
destruction in achieving its own goals, but I think that those are
<br>
likely to be interesting, complex goals, not simple mindless goals.
<br>
<p>I'm also willing to accept the risk-in-principle posed by advanced
<br>
nanotech, or some kind of &quot;subverted&quot; power which destroys humanity,
<br>
but I'm both reluctant to tag it as truly intelligent, and also
<br>
doubtful about the real possiblity.
<br>
<p>To some extent, there is a trade-off between efficiency and efficacy.
<br>
For example, the energy requirements might be too high to sustain
<br>
existence across the void of space. Just as lions in the sahara starve
<br>
when there is no food, so being powerful is not always a survival
<br>
advantage. I'm sure this point may have been up before, but I don't
<br>
know that it's a given that evil nanotech is really a universal
<br>
threat. It's clearly a planetwide threat, which is probably enough for
<br>
the argument anyway, given the lack of evidence of offworld life.
<br>
<p>Cheers,
<br>
-T
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11290.html">Eliezer S. Yudkowsky: "Re: Fighting UFAI"</a>
<li><strong>Previous message:</strong> <a href="11288.html">Eliezer S. Yudkowsky: "Re: Fighting UFAI"</a>
<li><strong>In reply to:</strong> <a href="11288.html">Eliezer S. Yudkowsky: "Re: Fighting UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11290.html">Eliezer S. Yudkowsky: "Re: Fighting UFAI"</a>
<li><strong>Reply:</strong> <a href="11290.html">Eliezer S. Yudkowsky: "Re: Fighting UFAI"</a>
<li><strong>Reply:</strong> <a href="11312.html">Dani Eder: "Re: Paperclip scenario"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11289">[ date ]</a>
<a href="index.html#11289">[ thread ]</a>
<a href="subject.html#11289">[ subject ]</a>
<a href="author.html#11289">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:22:57 MST
</em></small></p>
</body>
</html>
