<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Fighting UFAI</title>
<meta name="Author" content="pdugan (pdugan@vt.edu)">
<meta name="Subject" content="RE: Fighting UFAI">
<meta name="Date" content="2005-07-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Fighting UFAI</h1>
<!-- received="Wed Jul 20 00:44:29 2005" -->
<!-- isoreceived="20050720064429" -->
<!-- sent="Wed, 20 Jul 2005 02:44:04 -0400" -->
<!-- isosent="20050720064404" -->
<!-- name="pdugan" -->
<!-- email="pdugan@vt.edu" -->
<!-- subject="RE: Fighting UFAI" -->
<!-- id="43BF1424@zathras" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="Fighting UFAI" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> pdugan (<a href="mailto:pdugan@vt.edu?Subject=RE:%20Fighting%20UFAI"><em>pdugan@vt.edu</em></a>)<br>
<strong>Date:</strong> Wed Jul 20 2005 - 00:44:04 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11627.html">Peter de Blanc: "RE: Objective versus subjective reality: which is primary?"</a>
<li><strong>Previous message:</strong> <a href="11625.html">Ben Goertzel: "RE: Objective versus subjective reality: which is primary?"</a>
<li><strong>Maybe in reply to:</strong> <a href="11732.html">Phillip Huggan: "Fighting UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11629.html">Chris Capel: "Re: Fighting UFAI"</a>
<li><strong>Reply:</strong> <a href="11629.html">Chris Capel: "Re: Fighting UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11626">[ date ]</a>
<a href="index.html#11626">[ thread ]</a>
<a href="subject.html#11626">[ subject ]</a>
<a href="author.html#11626">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt;Before leaping from one example of someone who failed to consider that
</em><br>
<em>&gt;a different kind of mind would be, well, different, we should try to
</em><br>
<em>&gt;establish some bounds on the problem.
</em><br>
<em>&gt;
</em><br>
<em>&gt;We have access to consciousness through introspection. Can we identify
</em><br>
<em>&gt;which elements of consciousness are arbitrary, and which are not? To
</em><br>
<em>&gt;put it another way - can we identify which elements of ourselves might
</em><br>
<em>&gt;be preserved, or perhaps even necessarily must be preserved, in
</em><br>
<em>&gt;another kind of mind.
</em><br>
<p>&nbsp;&nbsp;Before you answer that question you have to consider through introspection 
<br>
this question: to what extent does human wetware cognition preserve 
<br>
non-arbitraty components? For instance, I have rational structures into which 
<br>
I plug symbolic data gleaned from sensory modality, if my sensory modality 
<br>
were to change, say in a simulated (or subjectively real) universe with 
<br>
different physics regarding just photon dynamics, would my symbolic 
<br>
interpretations become radically different from all prior earthly ontologies? 
<br>
Would my rational structures cease to be useful and be discarded? Would I 
<br>
enter a cognitive dimension where Bayes' lost all meaning? Would this 
<br>
transition be temporary or permanent? Would this transition make me crazy or 
<br>
enlightened? Or both? My inclination is that these questions are undecidable, 
<br>
leading me to conclude an inability to identify any non-anthropomorphic value 
<br>
worth keeping.
<br>
<p><em>&gt;
</em><br>
<em>&gt;Is emotion, for example, a natural byproduct of the combination of
</em><br>
<em>&gt;intelligence, consciousness and experience? Perhaps it is not - but
</em><br>
<em>&gt;perhaps there are some identifiable examples.
</em><br>
<p>&nbsp;&nbsp;&nbsp;Intelligence, as we've discussed, can be thought of as a utlity function or 
<br>
optimization process, consciousness is a nueral feed-back loop (though a 
<br>
mysterious one indeed) and experience is sense data compressed to symbolic 
<br>
autopoiesis and highly selective memory. Emotions are nuero-chemical functions 
<br>
which interact with these mental components. I don't think this implies a 
<br>
chemical or &quot;emotional&quot; context to electronic cognition to be be inherently 
<br>
incompatable with Turing computation. If we could get the kinks out of fluid 
<br>
quantum computing this would be an engineering option worth considering.
<br>
<p><em>&gt;
</em><br>
<em>&gt;What do you think? Do you think that what we have access to as
</em><br>
<em>&gt;intelligent beings is not even the same kind of thing that another
</em><br>
<em>&gt;intelligence might have access to? Does it make sense to call FAI
</em><br>
<em>&gt;intelligent if the mind cannot be in any sensible way called &quot;the
</em><br>
<em>&gt;same&quot;?
</em><br>
<p>&nbsp;&nbsp;I think you are alluding to the Penrose hypothesis, which begs the vital 
<br>
question: Is perfectly robust Friendliness (through unrestrained 
<br>
self-modification) possible with Turing computation? Is fluid quantum 
<br>
computation nessecary to instill &quot;emotion&quot; resembling systemic attractors? Or, 
<br>
as Penrose suggested, is the elusive quantum gravity computer needed to give 
<br>
the FAI the capacity for the existential questions regarding infinity?
<br>
<p><em>&gt;&gt; &gt; I know people have posed race conditions between FAI and paperclips,
</em><br>
<em>&gt;&gt; &gt; but there seems to me to be a kind of contradiction inherent in any AI
</em><br>
<em>&gt;&gt; &gt; which is intelligent enough to achieve one of these worst-case
</em><br>
<em>&gt;&gt; &gt; outcomes, but is still capable of making stupid mistakes.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Actions are only &quot;stupid mistakes&quot; relative to a cognitive reference frame.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Partial point. Obviously irrationality is irrationality however you
</em><br>
<em>&gt;slice it. I don't care who you are, (A -&gt; B) -&gt; (!B -&gt; !A) is going to
</em><br>
<em>&gt;stay a logical law. But you rightly didn't respond like that, my point
</em><br>
<em>&gt;was the &quot;stupidity&quot;, the disconnect between the logic and the sense if
</em><br>
<em>&gt;you will, of being able to formulate paperclips as a goal, and
</em><br>
<em>&gt;believing it to be a good idea.
</em><br>
<p>&nbsp;&nbsp;I'm a proponent of the notion that irrationality is rationality if construed 
<br>
in an autopoetic system with different underlying rules and axioms. As I 
<br>
suggested above, a mind privy to worlds with utterly different ontologies 
<br>
might not give much a damn for human logic. Whether this translates into our 
<br>
annihilation or the gentle amusement of the AI is the six billion person 
<br>
question.
<br>
<p><em>&gt;One thing about humans - we ask existential questions. Why are we
</em><br>
<em>&gt;here? What shall we do now? In a sense, human intelligence is a defeat
</em><br>
<em>&gt;of instinct and of mindless goal-following. A superintelligence poses
</em><br>
<em>&gt;a strange reversal. By being able to remove uncertainty, it is
</em><br>
<em>&gt;supposed that in answering the questions &quot;Why am I here?&quot; and &quot;What
</em><br>
<em>&gt;shall I do now?&quot;, the AI will return to a near-instinctual level of
</em><br>
<em>&gt;behaviour, with deadly efficiency if it so chooses.
</em><br>
<p>&nbsp;&nbsp;Much like Colonel Kurtz in &quot;Apocalypse Now&quot;.
<br>
<em>&gt;
</em><br>
<em>&gt;Do you think that's a fair analysis? Some kinds of AI are scary
</em><br>
<em>&gt;because they might come to a doubt-free conclusion?
</em><br>
<p>&nbsp;A better question is &quot;Are some AI's scary because they might experience Bayes 
<br>
irrelevant realities and be completely beyond human probable analysis?&quot;
<br>
<p><em>&gt;Or do you perhaps not think that existential questions are like that.
</em><br>
<em>&gt;It might be that the greater the intelligence, the less cosmic
</em><br>
<em>&gt;certainty one has. We should have the opportunity to interrogate
</em><br>
<em>&gt;merely somewhat superintelligent beings about this question at some
</em><br>
<em>&gt;point before singularity.
</em><br>
<p>&nbsp;&nbsp;I point to the idea of a Taoist sort of AI, one who learns and plays with 
<br>
all sorts of potentialities, only to renormalize on its certainty of its 
<br>
complete lack of objective knowledge. This is the paradox of objective 
<br>
subjectivity. The principal follows that you can't go crazy if you aren't 
<br>
attached to the issues threatening philosophical crisis. Your last sentance 
<br>
nicely reflects the notion of &quot;Singularity Steward&quot; proposed in Ben's essay on 
<br>
Positive Transcension, I strongly suggest to SIAI and anyone else working on 
<br>
AGI that the existential risks of unfriendliness can be marginalized through 
<br>
AI's who don't take themselves too seriously and the through the guidance of a 
<br>
Singularity Steward. Of course this seems a bit of a paradox on its own, the 
<br>
way around that paradox is to extend discourse such as this list to the level 
<br>
of a &quot;global brain&quot;, to ensure that the wisdom of transhuman intelligences is 
<br>
distributed like a safety net against both existential risk and philosophical 
<br>
crisis.
<br>
<p><p>&nbsp;&nbsp;&nbsp;Patrick
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11627.html">Peter de Blanc: "RE: Objective versus subjective reality: which is primary?"</a>
<li><strong>Previous message:</strong> <a href="11625.html">Ben Goertzel: "RE: Objective versus subjective reality: which is primary?"</a>
<li><strong>Maybe in reply to:</strong> <a href="11732.html">Phillip Huggan: "Fighting UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11629.html">Chris Capel: "Re: Fighting UFAI"</a>
<li><strong>Reply:</strong> <a href="11629.html">Chris Capel: "Re: Fighting UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11626">[ date ]</a>
<a href="index.html#11626">[ thread ]</a>
<a href="subject.html#11626">[ subject ]</a>
<a href="author.html#11626">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
