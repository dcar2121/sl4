<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Fighting UFAI</title>
<meta name="Author" content="justin corwin (outlawpoet@gmail.com)">
<meta name="Subject" content="Re: Fighting UFAI">
<meta name="Date" content="2005-07-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Fighting UFAI</h1>
<!-- received="Wed Jul 13 12:02:17 2005" -->
<!-- isoreceived="20050713180217" -->
<!-- sent="Wed, 13 Jul 2005 11:01:30 -0700" -->
<!-- isosent="20050713180130" -->
<!-- name="justin corwin" -->
<!-- email="outlawpoet@gmail.com" -->
<!-- subject="Re: Fighting UFAI" -->
<!-- id="3ad827f3050713110119556f62@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="003401c587cf$505e1410$f0f84c55@jumala" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> justin corwin (<a href="mailto:outlawpoet@gmail.com?Subject=Re:%20Fighting%20UFAI"><em>outlawpoet@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Jul 13 2005 - 12:01:30 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11276.html">Chris Capel: "Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<li><strong>Previous message:</strong> <a href="11274.html">Kaj Sotala: "Re: Fighting UFAI"</a>
<li><strong>In reply to:</strong> <a href="11274.html">Kaj Sotala: "Re: Fighting UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11276.html">Chris Capel: "Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<li><strong>Reply:</strong> <a href="11276.html">Chris Capel: "Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<li><strong>Reply:</strong> <a href="11278.html">Eliezer S. Yudkowsky: "Re: Fighting UFAI"</a>
<li><strong>Reply:</strong> <a href="11437.html">D. Alex: "Re: Fighting UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11275">[ date ]</a>
<a href="index.html#11275">[ thread ]</a>
<a href="subject.html#11275">[ subject ]</a>
<a href="author.html#11275">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Appreciation to E for the override. 
<br>
<p>In deference to Mitch Howe, I think that the idea of containment is
<br>
played out. But we're not exactly discussing keeping an AI in a box
<br>
against its will, but whether or not its existence in the world means
<br>
our immediate destruction, or if we have some game-theoretic chance of
<br>
defending ourselves.
<br>
<p>I tend to think there is a chance that a self-improving AI could be so
<br>
smart so fast that it doesn't make sense to try to evaluate its
<br>
'power' relative to us. We could be entirely within it's mercy(if it
<br>
had any). This bare possibility is enough to warp the planning of any
<br>
AI theorist, as it's not very good to continue with plans that have
<br>
big open existential risks in them.
<br>
<p>The question is, how likely is that? This is important, quite aside
<br>
from the problem that it's only one part of the equation. Suppose I,
<br>
Eliezer and other self improvement enthusiasts are quite wrong about
<br>
the scaling speed of self improving cognition. We might see AGI
<br>
designs stuck at infrahuman intelligence for ten years(or a thousand,
<br>
but that's a different discussion). In that ten years, do you think
<br>
that even a project that started out as friendly-compliant(whatever
<br>
that means) would remain so? I imagine even I might have trouble
<br>
continuing to treat it with the respect and developmental fear tht it
<br>
deserves. To be frank, if AGIs are stuck at comparatively low levels
<br>
of intelligence for any amount of time, they're going to be
<br>
productized, and used everywhere. That's an entirely different kind of
<br>
safety problem. Which one is the first up the knee of the curve?
<br>
Yours? IBM's? An unknown? There are safety concerns here that must be
<br>
applied societally, or at least not to  a single AI.
<br>
<p>There are other possible scenarios. Broken AGIs may exist in the
<br>
future, which could be just as dangerous as unFriendly ones. Another
<br>
spectre worried about is something I call a commandline intelligence,
<br>
with no upperlevel goal system, just mindlessly optimizing against
<br>
input commands of a priviliged user. Such a system would be
<br>
fantastically dangerous, even before it started unintended
<br>
optimization. It would be the equivalent of a stunted human upload.
<br>
all the powers, none of the intelligence upgrade.
<br>
<p>These and other scenarios may not be realistic, but I have thought
<br>
about them. If they are possible, they deserve consideration, just as
<br>
much(relative to their possiblity, severity, and possibility of
<br>
resolution) as the possibility of a single super-fast self improving
<br>
AI, which will be either Friendly or unFriendly, depending on who
<br>
writes it.
<br>
<p>So what's likely? What can be plan for, and what solutions may there
<br>
be? None of the above are particularly well served by planning a
<br>
Friendly architecture, and nothing else. Someone else talked about
<br>
safety measures, I think that may be a realistic thing to think about
<br>
*for some classes of error*. The question is, how likely are those
<br>
classes, and does it make sense to worry about them more than the edge
<br>
cases.
<br>
<p>For those of you who are still shaking your heads at the impossibility
<br>
of defending against a transhuman intelligence, let me point out some
<br>
scale. If you imagine that an ascendant AI might take 2 hours from
<br>
first getting out to transcension, that's more than enough time for a
<br>
forewarned military from one of the superpowers to physically destroy
<br>
a signifance portion of internet infrastructure(mines, perhaps), and
<br>
EMP the whole world into the 17th century(ICBMs set for high altitute
<br>
airburst would take less than 45 minutes from anywhere in the
<br>
world(plus America, at least, has EMP weapons), the amount of shielded
<br>
computing centers is miniscule). We may be stupid monkeys, but we've
<br>
spent a lot of time preparing for the use of force. Arguing that we
<br>
would be impotent in front of a new threat requires some fancy
<br>
stepping. I, for one, tend to think there might be classes of danger
<br>
we could defend against, which are worth defending against.
<br>
<p>&nbsp;I am open to persuasion, of course.
<br>
<p><p><pre>
-- 
Justin Corwin
<a href="mailto:outlawpoet@hell.com?Subject=Re:%20Fighting%20UFAI">outlawpoet@hell.com</a>
<a href="http://outlawpoet.blogspot.com">http://outlawpoet.blogspot.com</a>
<a href="http://www.adaptiveai.com">http://www.adaptiveai.com</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11276.html">Chris Capel: "Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<li><strong>Previous message:</strong> <a href="11274.html">Kaj Sotala: "Re: Fighting UFAI"</a>
<li><strong>In reply to:</strong> <a href="11274.html">Kaj Sotala: "Re: Fighting UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11276.html">Chris Capel: "Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<li><strong>Reply:</strong> <a href="11276.html">Chris Capel: "Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<li><strong>Reply:</strong> <a href="11278.html">Eliezer S. Yudkowsky: "Re: Fighting UFAI"</a>
<li><strong>Reply:</strong> <a href="11437.html">D. Alex: "Re: Fighting UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11275">[ date ]</a>
<a href="index.html#11275">[ thread ]</a>
<a href="subject.html#11275">[ subject ]</a>
<a href="author.html#11275">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:22:57 MST
</em></small></p>
</body>
</html>
