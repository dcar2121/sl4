<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: The return of the revenge of qualia, part VI.</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="RE: The return of the revenge of qualia, part VI.">
<meta name="Date" content="2005-07-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: The return of the revenge of qualia, part VI.</h1>
<!-- received="Tue Jul 26 08:04:02 2005" -->
<!-- isoreceived="20050726140402" -->
<!-- sent="Tue, 26 Jul 2005 15:03:58 +0100 (BST)" -->
<!-- isosent="20050726140358" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="RE: The return of the revenge of qualia, part VI." -->
<!-- id="20050726140358.68967.qmail@web26703.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="BAY103-F338F6A9FA6A0536358A1DCCACD0@phx.gbl" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=RE:%20The%20return%20of%20the%20revenge%20of%20qualia,%20part%20VI."><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Tue Jul 26 2005 - 08:03:58 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11776.html">Thomas Buckner: "Re: The return of the revenge of qualia, part VI."</a>
<li><strong>Previous message:</strong> <a href="11774.html">Mitchell Porter: "RE: The return of the revenge of qualia, part VI."</a>
<li><strong>In reply to:</strong> <a href="11774.html">Mitchell Porter: "RE: The return of the revenge of qualia, part VI."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11788.html">Ben Goertzel: "RE: The return of the revenge of qualia, part VI."</a>
<li><strong>Reply:</strong> <a href="11788.html">Ben Goertzel: "RE: The return of the revenge of qualia, part VI."</a>
<li><strong>Reply:</strong> <a href="11794.html">Mitchell Porter: "RE: The return of the revenge of qualia, part VI."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11775">[ date ]</a>
<a href="index.html#11775">[ thread ]</a>
<a href="subject.html#11775">[ subject ]</a>
<a href="author.html#11775">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Looking at the responses, I think several people haven't actually
<br>
seriously considered a materialist analysis of subjective sensation.
<br>
Yes, they have the general notion that it's all biology and hence
<br>
physics, but they haven't considered what sort of intermediate detail
<br>
there is or what the corresponding causal mechanisms might look like.
<br>
I admitt that if I hadn't thought about this detail the idea that
<br>
subjective experience is something special/nonphysical/noncomputable
<br>
would seem more plausible.
<br>
<p>A lot of the problems here are the result of sticking stubbornly to
<br>
folk psychology terms and failing to decompose to a finer resolution.
<br>
Ok, we don't have the details to decompose all the way, but we can do
<br>
a lot better than generic amateur philosophising. A case in point is
<br>
the question 'does actually seeing a colour for the first time bring
<br>
new knowledge'. 'Knowledge' here is not well-defined, yet the word is
<br>
so familiar we don't even notice. If the question is rephrased in
<br>
terms of cognitive capabilities, things become clearer; the human
<br>
didn't acquire any new axioms for deductive reasoning (though they
<br>
might be able to extract some from their updated sensory subsystem
<br>
via indirect reflection), but acquired some other capabilities (e.g.
<br>
the ability to immediately recognise the colour, the ability to
<br>
visualise objects of that colour). Worse problems come when people
<br>
start to talk about 'meaning'; the 'meaning as an ineffable substance'
<br>
metaphor seems to persist despite commendable efforts by Eliezer and
<br>
others to stamp it out. You can't reasonably discuss the cognitive
<br>
basis of subjective experience until you know exactly what your terms
<br>
mean (or at least where the areas of ambiguity are). Again 'meaning'
<br>
can be translated into specific effects and capabilites (and
<br>
information, though 'information' itself is a moderately complex term
<br>
to adequately define).
<br>
<p>Tennessee Leeuwenburg wrote:
<br>
<em>&gt;&gt;&gt; But that is precisely what is interesting. A human cannot
</em><br>
<em>&gt;&gt;&gt; understand logically everything that they can learn, nor can they
</em><br>
<em>&gt;&gt;&gt; describe with phyics everything that is immanent (loosely, &quot;real&quot;)
</em><br>
<em>&gt;&gt;&gt; to them.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Why are you intent on glamourising this relatively straightforward
</em><br>
<em>&gt;&gt; cognitive architecture limitation with metaphysics?
</em><br>
<em>&gt;
</em><br>
<em>&gt; In answering that question, I would be implicity accepting your
</em><br>
<em>&gt; believe that I *am* doing that. I reject said belief. I truly
</em><br>
<em>&gt; believe it to be a genuine possibility that an articifial
</em><br>
<em>&gt; intelligence might have no consciousness, or awareness of
</em><br>
<em>&gt; immanence.
</em><br>
<p>You've switched arguments. I said that you're making the human
<br>
inability to access everything they 'know' in declarative form
<br>
unnecessarily mysterious. The question of whether an AI can lack
<br>
conscious experience is related but distinct, and I do in fact agree
<br>
that consciousness and subjective experience in the human sense are
<br>
not necessary in an AGI, or indeed desireable in an FAI.
<br>
<p><em>&gt; Firstly, while I'm happy to accept your tone as being
</em><br>
<em>&gt; argumentatively efficient, the blanket claim &quot;this statement is
</em><br>
<em>&gt; incorrect&quot; is not really the kind of thing which is uncontroversial
</em><br>
<em>&gt; or proven.
</em><br>
<p>I untrained myself from using the 'I belive', 'In my opinion', 'I
<br>
am fairly certain of' etc prefixes on SL4 because frankly people here
<br>
are generally competent enough to infer them correctly and it wastes
<br>
a lot of time and attention in extended arguments.
<br>
<p><em>&gt; Let me accept, temporarily, that the brain is capable of perfect
</em><br>
<em>&gt; simulation (and here's the important qualifier) at the physical
</em><br>
<em>&gt; level. All predictions are similarly restricted to the physical
</em><br>
<em>&gt; level. Meaning is not predicted -- only brain state. If the
</em><br>
<em>&gt; predicting being does not understand the meaning of its prediction
</em><br>
<em>&gt; of physical state, then it is a meaningless prediction
</em><br>
<p>What /exactly/ do you want to know that a perfect physical simulation
<br>
wouldn't tell you? Obviously the simulation includes a complete
<br>
record of the person's behaviour for the period simulated. Usually
<br>
when people talk about 'meaning' in this context they're reffering to
<br>
some sort of compressed description that they find convenient for
<br>
recall and inference. I would guess that the 'meaning' you want is a
<br>
high-level (i.e. highly lossly, but extremely efficient) description
<br>
of the person's brain state defined in terms of other high level
<br>
concepts you already know and understand, so that you can easily infer
<br>
what that person's past, future or otherwise subjunctive behaviour
<br>
might be using your own mental toolkit. Amusingly enough the procedure
<br>
for brute-forcing the problem is quite straightforward; we simply
<br>
simulate /your/ brain after telling you every plausible combination of
<br>
words that might be a concise description of the subject's brain, and
<br>
then simulate both the subject's brain reacting to various related
<br>
situations and your brain trying to guess how they will react.
<br>
Whichever description string produced the most predictive success was
<br>
the most meaningful for you (by this definition of meaning, but I
<br>
challenge you to think up a definition I can't brute force given an
<br>
arbitrary but finite amount of computing power). Practically of course
<br>
an AGI would extract 'meaning' by performing pattern/compressibility
<br>
analysis of the physical level simulation, or more likely skip the
<br>
low-level simulation altogether and run a multi-level simulation
<br>
extending to whatever bottom level of detail is required for the
<br>
desired inferential accuracy.
<br>
<p><em>&gt; I am not a dualist. I believe that mental states do arise from the
</em><br>
<em>&gt; physical nature of the brain...
</em><br>
<em>&gt;
</em><br>
<em>&gt; 2) That qualia are real, and that physics as such does not capture
</em><br>
<em>&gt; the full meaning of state.
</em><br>
<p>This is what you need to pin down. Most materialists do not claim that
<br>
the laws of physics include rules about qualia, they claim that qualia
<br>
exist as regularities within the structure of the universe as
<br>
determined by physics. They are higher-order consequences of physics
<br>
in the same way as raindrops, galaxies, natural selection and futures
<br>
trading. 'Meaning' isn't even that tangible; it's a human cognitive
<br>
construct that's convenient for reasoning about some things but that
<br>
has no coherent set of physical referents.
<br>
<p>Given a complete physical description of everything you want to
<br>
predict, it is /always/ possible to produce the most accurate possible
<br>
description given enough computing power. 'Meaning' only comes into it
<br>
when we can't afford such extravagent expenditures, and have to reason
<br>
about regularities in physics rather than physics itself. AIXI doesn't
<br>
even need 'meaning' to be superintelligent, because it has infinite
<br>
computing power. Any practical AGI would have to use highly compressed
<br>
representations, but hopefully it wouldn't be as confused about the
<br>
process as humans tend to be.
<br>
<p><em>&gt;&gt;&gt; Physics, for example, doesn't enable to me understand what
</em><br>
<em>&gt;&gt;&gt; language means, nor does merely understanding the grammar and
</em><br>
<em>&gt;&gt;&gt; syntax and symbolism of a language allow me to use it.
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt; This is a limit of your inferential capability, not any flaw in the
</em><br>
<em>&gt;&gt; materialist position.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Possibly true. Care to point out the specific error? Or do you just
</em><br>
<em>&gt; mean that another person *could* use physics to understand etc etc.
</em><br>
<p>'Your' was a bad (ambigious) choice of word; 'human' would be better
<br>
as no single human could take a printout of the details of every
<br>
atom in your body and work out the rules of English grammer from it.
<br>
Maybe an entire civilisation of transhumans with computers far in
<br>
advance of ours could do so, as a major public science project. Maybe
<br>
a Power could do it in a few milliseconds as a background task. It is
<br>
possible in principle, but not in practice; the important point is
<br>
/why/ it is not possible in practice. It's not ineffability, it's
<br>
simple intractability.
<br>
<p><em>&gt; Let me broaden the claim :: physics, in principle, allows no being
</em><br>
<em>&gt; or potential being, to understand etc etc, where physics is the
</em><br>
<em>&gt; study of matter and its behaviour.
</em><br>
<p>This statement is definitely incorrect, assuming you are not barring
<br>
the reasoner from finding more compact descriptions of (i.e.
<br>
inferentially useful regularities in) their initial data.
<br>
<p><em>&gt; Indeed -- by definition. I would simply argue that it is important
</em><br>
<em>&gt; to humans that meaning be preserved.
</em><br>
<p>'Meaning' in the sense of compressed descriptions isn't going to be
<br>
under threat while computing power is reasonably bounded, and isn't
<br>
really what we want anyway. 'The illusion of qualia' is another issue
<br>
and one I am actually concerned about, though I acknowledge that this
<br>
may seem terribly quaint and silly to far transhumans. Actually the
<br>
term 'illusion' is somewhat unfortunate, as I do acknowledge that
<br>
qualia can usefully be given a physical (neuroarchitectural) referant.
<br>
What's illusory is all the mystery and seeming irreducability involved
<br>
that causes people to believe that qualia are ontological primitives,
<br>
simply because our brain wasn't designed to answer the question 'so
<br>
what /is/ blue?'. A (contemporary) AGI would get the answer 'high
<br>
values of byte 3 in RGB format, mapped to flux density of photons in
<br>
frequency range X to Y, as transcribed by sensory mechanism Z...' 
<br>
<p>Note that it's fairly pointless to make statements about how building
<br>
one sort of AGI or another would be a bad thing for humanity unless
<br>
you actually intend to do something about it, either by building an
<br>
AGI yourself first or more realistically convincing people who you
<br>
believe are most likely to build an AGI that your ideas are right.
<br>
While I disagree with Russell Wallace's domain protection scheme and
<br>
his criticisms of CV, I do appreciate the fact that he is trying to
<br>
follow up on his principles by convincing at least a few of the
<br>
relevant people that he is right.
<br>
<p><em>&gt; I think it is a dangerous leap of faith to assume that *all* good
</em><br>
<em>&gt; physical modelling programs will be conscious just because
</em><br>
<em>&gt; they are imbued with goals.
</em><br>
<p>Aargh. 'Conscious' != 'has qualia'. Do dogs have qualia? Do salmon
<br>
have qualia? Do spiders? Are dogs or salmon or spiders conscious?
<br>
Do you think 'self aware' == 'conscious' or is it something different?
<br>
See the futility of trying to do precision reasoning with the crude,
<br>
broken blunt instruments that are folk psychology concepts?
<br>
<p>AIs are not 'self-aware' unless they have a sophisticated self-model
<br>
with a list of specific capabilities, the extent of which is a subject
<br>
for active debate. Qualia are a necessary component of a much smaller
<br>
set of capabilities, assuming that you can have qualia without having
<br>
the ability to argue about 'the true nature of blue' (a description
<br>
that applies to quite a few humans, never mind dogs or AIs).
<br>
<p>Mitchell Porter wrote:
<br>
<em>&gt; However, epistemologically, qualia are the starting point, and
</em><br>
<em>&gt; the material universe is the thing posited.
</em><br>
<p>I read this section as; 'qualia are what we call the primitive data
<br>
structures which all of our consciously accessible sensory data is
<br>
constructed from. Note that a self-modifying AGI can create new
<br>
'qualia' simply by adding some new sensory processing code.
<br>
<p><em>&gt; I ask you to conceive of point particles possessing a specified
</em><br>
<em>&gt; mass, a specified charge, a specified  location, and no other
</em><br>
<em>&gt; properties. Sprinkle them about in space as you will, you will not
</em><br>
<em>&gt; create a 'sensation of color'. Equip them with a certain dynamics,
</em><br>
<em>&gt; and you may be able to construct an 'environment with properties'
</em><br>
<em>&gt; and a 'stimulus classifier'; name some of those environmental
</em><br>
<em>&gt; properties 'colors' and some of the classifier's states 'sensations
</em><br>
<em>&gt; of color', and you  may be able to mimic the apparent causal
</em><br>
<em>&gt; relations between our environment and our sensations of color; but
</em><br>
<em>&gt; the possible world you have thereby specified does not contain
</em><br>
<em>&gt; sensations of color as we know them, and therefore cannot be the
</em><br>
<em>&gt; world we are inhabiting.
</em><br>
<p>I'm not clear if you're building a classifier out of the particles
<br>
and embedding it in the universe, or tacking something on to the base
<br>
physics. I agree that the latter wouldn't work anything like our
<br>
universe, where secondary properties are entirely higher-level
<br>
regularities in the group of particles that constitute the perceiver.
<br>
But in either case, if you immitate /all/ the causal properties of
<br>
the human concepts of colour (including intermeidate sensory
<br>
processing details that affect our conscious reasoning but which we
<br>
can't clearly describe), how is that not sensations of color?
<br>
Substrate indepedendence includes both the internal details of
<br>
black-box lower level algorithms (assuming they don't systematically
<br>
effect the output) and the details of whatever physics you are
<br>
implemented it. The more coherent proposals of the people who want
<br>
qualia to be part of physics are superficially plausible because in
<br>
principle qualia could really work as ontological primitives; it's
<br>
just that there's overwhelming evidence that our universe does not
<br>
work like that.
<br>
<p><em>&gt; We are faced, not just with a self-denying sensibility which wishes
</em><br>
<em>&gt; to assert that colorless matter in motion is all that exists (in
</em><br>
<em>&gt; which case the secondary properties - the qualia - are either
</em><br>
<em>&gt; mysteriously identical with certain unspecified conjunctive
</em><br>
<em>&gt; properties of large numbers of these particles, or even more 
</em><br>
<em>&gt; mysteriously do not exist at all),
</em><br>
<p>People (well, most of them) don't go around saying that 'thoughts do
<br>
not exist' because they're happy with the idea that thoughts are
<br>
inside their head, and that they exist as patterns that the basic
<br>
elements of their brain (e.g. synapses, activation spikes) adopt. The
<br>
confusion about qualia exists because people intuitively believe that
<br>
qualia are 'out there' rather than 'in here'. This is sensible for
<br>
normal reasoning; it saves a deference to just store 'the bus is red'
<br>
rather than 'there is an object that reliably causes the red detector
<br>
to fire'. As usual though direct intuition is worse than useless -
<br>
actively misleading - when trying to unravel how human cognition
<br>
works.
<br>
<p><em>&gt; Mathematical physics, as we know it, is both an apex and a dead
</em><br>
<em>&gt; end. No amount of quantitative predictive progress through better
</em><br>
<em>&gt; model-building is going to explain consciousness, because the
</em><br>
<em>&gt; models in question exclude certain aspects of reality *by
</em><br>
<em>&gt; construction*.
</em><br>
<p>I disagree, assuming you allow the search for and use of higher level
<br>
regularities (and hence all parts of the unified causal model other
<br>
than the physical foundation). I have yet to hear any coherent
<br>
question about consciousness on this list which cannot be irrefutably
<br>
answered given enough research and computer time (though that amount
<br>
may well be sufficient that it won't be so answered any time soon).
<br>
<p><em>&gt; But I do regard it to be a kind of arrow to be shot at physical
</em><br>
<em>&gt; reductionists - which is to say people who believe that talking
</em><br>
<em>&gt; about brain states is the same thing as talking about mental states.
</em><br>
<p>So why isn't this just a question of level of description and
<br>
reflective accessibility?
<br>
<p><em>&gt; There is something which pain is like which is not described by
</em><br>
<em>&gt; physics equations, even if physics equations can account for the
</em><br>
<em>&gt; progress of the state of the world.
</em><br>
<p>People are going to keep saying this until we can in fact tell you
<br>
exactly how pain works, what structures it effects, and the entire
<br>
causal chain from stubbing your toe to saying 'pain sucks, why can't
<br>
we engineer it out' at the most convenient abstraction level. Right
<br>
now people see physics, see that their mental representation of
<br>
physics is nothing like their mental representation of pain, and
<br>
automatically disregard the possibility that one could include the
<br>
other. Actually some people will keep saying it anyway, but that's
<br>
the peverse attraction of the unknowable for you. I realise that it's
<br>
difficult to accept 'yes, we can describe it' before we actually have
<br>
the description, but that is itself a more direct logical consequence
<br>
of the science we already have.
<br>
<p><em>&gt; A stock example of an inexact or vague predicate is baldness. You're
</em><br>
<em>&gt; not bald when you have a full head of hair; you are bald if you have
</em><br>
<em>&gt; none; but what if you have one hair, ten hairs, a thousand hairs;
</em><br>
<em>&gt; where is the dividing line between bald and not-bald? There is no
</em><br>
<em>&gt; reason to think that there is any way to answer that question
</em><br>
<em>&gt; without arbitrary stipulation that, say, 1000 hairs is the most you
</em><br>
<em>&gt; can have and still be bald.
</em><br>
<p>I don't think you need to be so concerned about using exact
<br>
predicates. Fuzzy predicates are a direct consequence of the fact that
<br>
tractable classifiers are usually unreliable, particularly on human
<br>
neural hardware; the probability of returning 'bald' on seeing a head
<br>
varies smoothly from 'nearly 0' at some lower threshold of hair to
<br>
'nearly 1' beyond a higher threshold. I agree that exact predicates
<br>
are usually preferable when constructing a precise theory, but we have
<br>
a /long/ way to go before we're at that point with subjective
<br>
sensation. Right now the terms people are using have much more serious
<br>
clarity issues than being based on simple probabilistic classifiers.
<br>
<p><em>&gt; But if we consider all possible distributions of electrons
</em><br>
<em>&gt; throughout a transistor, there will clearly be marginal cases.
</em><br>
<p>We solve this by adding a third 'undefined' state. We set the
<br>
thresholds for '0' and '1' such that all states in those categories
<br>
result in predictable behaviour. All marginal cases and in practice
<br>
some predictable states for which the distance to the threshold is
<br>
within the range of measurement error go in the 'undefined' category.
<br>
<p><em>&gt; You have to draw an exact boundary in configuration space and say,
</em><br>
<em>&gt; these are the ones, these are the zeroes; these are the blue qualia,
</em><br>
<em>&gt; these are the green qualia; this is the thought of an apple, this is
</em><br>
<em>&gt; the thought of a red apple.
</em><br>
<p>In the real world, probabilistic inference is generally Good Enough.
<br>
Though I admitt that determined dualists might try to hide behind any
<br>
remaining margin of error; on the plus side, that will drive efforts
<br>
towards ever-increasing accuracy. :)
<br>
<p><em>&gt; I take it as given, not just that there are qualia, but that there
</em><br>
<em>&gt; is awareness of qualia, and an associated capacity to reason about
</em><br>
<em>&gt; their nature, and that this is what makes phenomenological
</em><br>
<em>&gt; reflection possible in human beings.
</em><br>
<p>Since the human brain is an evolved structure, do you believe that
<br>
natural selection discovered qualia or invented them? Either way, if
<br>
you think qualia are indivisible primitives please answer the
<br>
question 'what use is half a qualia' ?
<br>
<p>Ben Goertzel wrote:
<br>
<em>&gt; I tend to think that if one builds a software program with the
</em><br>
<em>&gt; right cognitive structures and dynamics attached to it, the qualia
</em><br>
<em>&gt; will come along &quot;for free&quot;.  Qualia don't need to be explicitly
</em><br>
<em>&gt; engineered as part of AI design, but this doesn't make them any
</em><br>
<em>&gt; less real or any less important.
</em><br>
<p>True on two counts. Firstly 'qualia' in the sense of sensory data
<br>
structures exist whenever there is a sensory modality feeing the
<br>
reasoning system of a general intelligence. Secondly qualia with
<br>
something like human sensation's mysterious primitiveness are likely
<br>
to result from any cognitive architecture which uses an opaque design
<br>
(or rather, transparent enough at the high level to not be entirely
<br>
brittle, but opaque at the lower sensory processing levels), and Ben
<br>
generally goes for fairly opaque designs. That said you're not going
<br>
to get closely humanlike sensation from anything other than a close
<br>
copy of the human sensory processing stack. Again, think of the causal
<br>
mechanisms that produce inference and behaviour, and how sensitive
<br>
the conclusions of (particularly reflective) inference and resulting 
<br>
behaviour are to changes in those mechanisms.
<br>
<p>Norm Wilson wrote:
<br>
<em>&gt; What level of modeling is necessary and sufficient to build a
</em><br>
<em>&gt; program with the &quot;right cognitive structures and dynamics&quot;?
</em><br>
<p>A sufficiently flexible layering system to draw a wide range of
<br>
possible inferences from a limited set of object 'secondary
<br>
properties'.
<br>
<p><em>&gt; Is it sufficient to model the brain at the level of neurons, atoms,
</em><br>
<em>&gt; quantum mechanics (e.g., Penrose-Hammeroff), or sub-quantum
</em><br>
<em>&gt; features of reality?
</em><br>
<p>All of those are progressively massive levels of overkill for the
<br>
problem; I know Ben at least strongly suspects this otherwise he
<br>
wouldn't believe that his much-more-abstracted-than-NNs AGI design
<br>
would have a chance of producing 'qualia'.
<br>
<p><em>&gt; How can we (or an AI) know, let alone prove, that a sufficient
</em><br>
<em>&gt; model for consciousness has been created?  Can we define a
</em><br>
<em>&gt; &quot;Turing Test&quot; for qualia?
</em><br>
<p>Yes, and in principle we can look at the functional structures in the
<br>
AGI and compare them to a detailed functional analysis of the human
<br>
brain (once we have one).
<br>
<p><em>&gt; I'd like to reformulate the hard problem in purely materialistic
</em><br>
<em>&gt; terms as one of &quot;completeness&quot;, in which the burden is on the
</em><br>
<em>&gt; materialists to demonstrate that his or her particular model of
</em><br>
<em>&gt; consciousness is complete.
</em><br>
<p>Generally I put the burden of proof on whoever can't formulate a
<br>
well-defined question or answer. If all the model can generate answers
<br>
that are well-defined enough to be verifiably correct for all the
<br>
well-defined questions people can think of, then it's the best model
<br>
we have. Right now you want the materialists to do your homework for
<br>
you in translating wishy washy stuff such as 'what is meaning' into
<br>
something concrete; we hope to be able to say 'this is the structure
<br>
in your brain for the concept of 'meaning', here are the kind of
<br>
things it matches, here are what those things do, here is what life
<br>
would be like if you didn't have that concept, here is why you
<br>
generate the answers you do to questions about meaning'. If you want
<br>
more than that you're going to have to specify the question better,
<br>
lest I just get frusrated and ask my Oracle which 27 words will
<br>
instantly and reliably transform you into a qualified materialist ;)
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
Yahoo! Messenger - NEW crystal clear PC to PC calling worldwide with voicemail <a href="http://uk.messenger.yahoo.com">http://uk.messenger.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11776.html">Thomas Buckner: "Re: The return of the revenge of qualia, part VI."</a>
<li><strong>Previous message:</strong> <a href="11774.html">Mitchell Porter: "RE: The return of the revenge of qualia, part VI."</a>
<li><strong>In reply to:</strong> <a href="11774.html">Mitchell Porter: "RE: The return of the revenge of qualia, part VI."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11788.html">Ben Goertzel: "RE: The return of the revenge of qualia, part VI."</a>
<li><strong>Reply:</strong> <a href="11788.html">Ben Goertzel: "RE: The return of the revenge of qualia, part VI."</a>
<li><strong>Reply:</strong> <a href="11794.html">Mitchell Porter: "RE: The return of the revenge of qualia, part VI."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11775">[ date ]</a>
<a href="index.html#11775">[ thread ]</a>
<a href="subject.html#11775">[ subject ]</a>
<a href="author.html#11775">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
