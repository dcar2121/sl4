<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: &quot;Hostile&quot; transhumans (Re: Magic means large search spaces)</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: &quot;Hostile&quot; transhumans (Re: Magic means large search spaces)">
<meta name="Date" content="2005-07-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: &quot;Hostile&quot; transhumans (Re: Magic means large search spaces)</h1>
<!-- received="Thu Jul 21 15:43:50 2005" -->
<!-- isoreceived="20050721214350" -->
<!-- sent="Thu, 21 Jul 2005 14:43:59 -0700" -->
<!-- isosent="20050721214359" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: &quot;Hostile&quot; transhumans (Re: Magic means large search spaces)" -->
<!-- id="42E0171F.5050207@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="20050721202834.61545.qmail@web54501.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20&quot;Hostile&quot;%20transhumans%20(Re:%20Magic%20means%20large%20search%20spaces)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu Jul 21 2005 - 15:43:59 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11682.html">p3: "Re: positivism (was Re: On Geddes)"</a>
<li><strong>Previous message:</strong> <a href="11680.html">pdugan: "Global Brain as Steward"</a>
<li><strong>In reply to:</strong> <a href="11679.html">Phil Goetz: "&quot;Hostile&quot; transhumans (Re: Magic means large search spaces)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11694.html">Phil Goetz: "Re: &quot;Hostile&quot; transhumans (Re: Magic means large search spaces)"</a>
<li><strong>Reply:</strong> <a href="11694.html">Phil Goetz: "Re: &quot;Hostile&quot; transhumans (Re: Magic means large search spaces)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11681">[ date ]</a>
<a href="index.html#11681">[ thread ]</a>
<a href="subject.html#11681">[ subject ]</a>
<a href="author.html#11681">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Phil Goetz wrote:
<br>
<em>&gt; --- &quot;Eliezer S. Yudkowsky&quot; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20&quot;Hostile&quot;%20transhumans%20(Re:%20Magic%20means%20large%20search%20spaces)">sentience@pobox.com</a>&gt; wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;Allowing the existence of a hostile 
</em><br>
<em>&gt;&gt;transhuman is just plain STUPID, end of story.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Can you define what it means for a transhuman to be hostile
</em><br>
<em>&gt; clearly enough that you don't have to kill all transhumans?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Suppose that transhumans had no particular ill will to
</em><br>
<em>&gt; ordinary humans, but they were very successful in
</em><br>
<em>&gt; everything that they did, so that transhumans were seen
</em><br>
<em>&gt; to be on their way to becoming the only dominant economic
</em><br>
<em>&gt; force, soon to legally control 95% of the cash, property,
</em><br>
<em>&gt; and land in the world, and ordinary humans would be the
</em><br>
<em>&gt; equivalent of Americans without college degrees.
</em><br>
<em>&gt; Would the ordinary humans see these transhumans as hostile?
</em><br>
<p>If they're smarter than Eliezer their only activity of any long-term 
<br>
importance is building an AI.  Sucking up all the money in the world is a side 
<br>
issue that no one will care about in two hundred million years.  And please 
<br>
note this situation holds if you've got even one &gt;E intelligence in the crowd, 
<br>
starting from day one of &gt;E's existence.  So the &quot;cash, property, and land&quot; or 
<br>
using the plural in &quot;transhumans&quot;, is a distraction that would lead you to 
<br>
expect the crisis to materialize much later than it actually would.
<br>
<p>Note also the creativity, or even apparent magical quality, of building an AI. 
<br>
&nbsp;&nbsp;Didn't think of that, didja?
<br>
<p>*Any* hostile transhuman is a huge problem, even if s/he is merely a slightly 
<br>
augmented human, holed up in an apartment in Venezuela coding apparent 
<br>
gibberish on a laptop with no Internet connection.  Though not as much of a 
<br>
problem as an UFAI running at a million times the human subjective rate.  Nor 
<br>
can we make the default presumption of hostility for the Venezuelan.
<br>
<p><em>&gt; I don't see how Eliezer's viewpoint can pragmatically
</em><br>
<em>&gt; permit the co-existence of humans and transhumans.
</em><br>
<p>Let's rephrase.  For *humans* to deliberately allow the existence of a hostile 
<br>
transhuman is just plain STUPID.  (Once again, gotta keep that two-place 
<br>
predicate from becoming a one-place predicate; no action is &quot;stupid&quot; except 
<br>
relative to some goal system.)
<br>
<p>On the other hand, if I build a CV-type FAI and it goes ahead and permits the 
<br>
existence of hostile transhumans, I can see any number of ways that could work 
<br>
out okay.  Maybe they all live on Neptune, etc.  *You* don't permit something 
<br>
with hostile goals that is smarter than you are.  It's okay to build a 
<br>
Friendly-type thing that can then permit the existence of hostile transhumans 
<br>
that are smarter than you and dumber than it.  Maybe the rules change with 
<br>
increasing intelligence, so that an SI can knowably safely permit the 
<br>
existence of hostile minds bigger than itself, if it has a positional 
<br>
advantage over them such as root permission on their operating system - 
<br>
because it *knows* its code is flawless and its model of physics is as good as 
<br>
it gets, etc.  Superior intelligence might or might not always be magic, but 
<br>
it's magic at this particular level of intelligence.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11682.html">p3: "Re: positivism (was Re: On Geddes)"</a>
<li><strong>Previous message:</strong> <a href="11680.html">pdugan: "Global Brain as Steward"</a>
<li><strong>In reply to:</strong> <a href="11679.html">Phil Goetz: "&quot;Hostile&quot; transhumans (Re: Magic means large search spaces)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11694.html">Phil Goetz: "Re: &quot;Hostile&quot; transhumans (Re: Magic means large search spaces)"</a>
<li><strong>Reply:</strong> <a href="11694.html">Phil Goetz: "Re: &quot;Hostile&quot; transhumans (Re: Magic means large search spaces)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11681">[ date ]</a>
<a href="index.html#11681">[ thread ]</a>
<a href="subject.html#11681">[ subject ]</a>
<a href="author.html#11681">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
