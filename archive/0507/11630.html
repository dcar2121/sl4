<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Fighting UFAI</title>
<meta name="Author" content="D. Alex (adsl7iie@tpg.com.au)">
<meta name="Subject" content="Re: Fighting UFAI">
<meta name="Date" content="2005-07-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Fighting UFAI</h1>
<!-- received="Wed Jul 20 08:37:40 2005" -->
<!-- isoreceived="20050720143740" -->
<!-- sent="Wed, 20 Jul 2005 22:34:44 +0800" -->
<!-- isosent="20050720143444" -->
<!-- name="D. Alex" -->
<!-- email="adsl7iie@tpg.com.au" -->
<!-- subject="Re: Fighting UFAI" -->
<!-- id="003001c58d38$2c724b20$0301010a@handle" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="3ad827f3050713110119556f62@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> D. Alex (<a href="mailto:adsl7iie@tpg.com.au?Subject=Re:%20Fighting%20UFAI"><em>adsl7iie@tpg.com.au</em></a>)<br>
<strong>Date:</strong> Wed Jul 20 2005 - 08:34:44 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11631.html">Jef Allbright: "Re: Objective versus subjective reality: which is primary?"</a>
<li><strong>Previous message:</strong> <a href="11629.html">Chris Capel: "Re: Fighting UFAI"</a>
<li><strong>In reply to:</strong> <a href="11468.html">justin corwin: "Re: Fighting UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11487.html">Marc Geddes: "Re: Fighting UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11630">[ date ]</a>
<a href="index.html#11630">[ thread ]</a>
<a href="subject.html#11630">[ subject ]</a>
<a href="author.html#11630">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Despite reading all the old posts I can find on &quot;AI boxing&quot;, I cannot see
<br>
that the &quot;idea of containment is played out&quot;. The arguments against
<br>
containment seem to be based on unproven assertions and inapropriate
<br>
comparisons - and I would love to have someone correct me here and PROVE
<br>
that AI boxing is not feasible.
<br>
<p>I also cannot accept that an AI that would be required to PROVE its benign
<br>
nature (prove mathematically while staying fully within the box) would be
<br>
able to pull the wool over the eyes of an appropriate gatekeeper commettee.
<br>
A proof is by definition sufficient, and just because it can be understood
<br>
and validated by such a committee does not make it invalid, even though a
<br>
much higher intelligence had the opportunity to bend it to its ends. There
<br>
need not be room for any subtelties beyond human understanding in a proof
<br>
like that.
<br>
<p>Finally, why would a proof provided by the AI itself be less reliable than a
<br>
set of principles developed by humans to build friendly AI? It would seem
<br>
easier by far to check than to effectively develop a proof.
<br>
<p>D. Alex
<br>
<p><p>----- Original Message -----
<br>
From: &quot;justin corwin&quot; &lt;<a href="mailto:outlawpoet@gmail.com?Subject=Re:%20Fighting%20UFAI">outlawpoet@gmail.com</a>&gt;
<br>
To: &lt;<a href="mailto:sl4@sl4.org?Subject=Re:%20Fighting%20UFAI">sl4@sl4.org</a>&gt;
<br>
Sent: Thursday, July 14, 2005 2:01 AM
<br>
Subject: Re: Fighting UFAI
<br>
<p><p><em>&gt; Appreciation to E for the override.
</em><br>
<em>&gt;
</em><br>
<em>&gt; In deference to Mitch Howe, I think that the idea of containment is
</em><br>
<em>&gt; played out. But we're not exactly discussing keeping an AI in a box
</em><br>
<em>&gt; against its will, but whether or not its existence in the world means
</em><br>
<em>&gt; our immediate destruction, or if we have some game-theoretic chance of
</em><br>
<em>&gt; defending ourselves.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I tend to think there is a chance that a self-improving AI could be so
</em><br>
<em>&gt; smart so fast that it doesn't make sense to try to evaluate its
</em><br>
<em>&gt; 'power' relative to us. We could be entirely within it's mercy(if it
</em><br>
<em>&gt; had any). This bare possibility is enough to warp the planning of any
</em><br>
<em>&gt; AI theorist, as it's not very good to continue with plans that have
</em><br>
<em>&gt; big open existential risks in them.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The question is, how likely is that? This is important, quite aside
</em><br>
<em>&gt; from the problem that it's only one part of the equation. Suppose I,
</em><br>
<em>&gt; Eliezer and other self improvement enthusiasts are quite wrong about
</em><br>
<em>&gt; the scaling speed of self improving cognition. We might see AGI
</em><br>
<em>&gt; designs stuck at infrahuman intelligence for ten years(or a thousand,
</em><br>
<em>&gt; but that's a different discussion). In that ten years, do you think
</em><br>
<em>&gt; that even a project that started out as friendly-compliant(whatever
</em><br>
<em>&gt; that means) would remain so? I imagine even I might have trouble
</em><br>
<em>&gt; continuing to treat it with the respect and developmental fear tht it
</em><br>
<em>&gt; deserves. To be frank, if AGIs are stuck at comparatively low levels
</em><br>
<em>&gt; of intelligence for any amount of time, they're going to be
</em><br>
<em>&gt; productized, and used everywhere. That's an entirely different kind of
</em><br>
<em>&gt; safety problem. Which one is the first up the knee of the curve?
</em><br>
<em>&gt; Yours? IBM's? An unknown? There are safety concerns here that must be
</em><br>
<em>&gt; applied societally, or at least not to  a single AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; There are other possible scenarios. Broken AGIs may exist in the
</em><br>
<em>&gt; future, which could be just as dangerous as unFriendly ones. Another
</em><br>
<em>&gt; spectre worried about is something I call a commandline intelligence,
</em><br>
<em>&gt; with no upperlevel goal system, just mindlessly optimizing against
</em><br>
<em>&gt; input commands of a priviliged user. Such a system would be
</em><br>
<em>&gt; fantastically dangerous, even before it started unintended
</em><br>
<em>&gt; optimization. It would be the equivalent of a stunted human upload.
</em><br>
<em>&gt; all the powers, none of the intelligence upgrade.
</em><br>
<em>&gt;
</em><br>
<em>&gt; These and other scenarios may not be realistic, but I have thought
</em><br>
<em>&gt; about them. If they are possible, they deserve consideration, just as
</em><br>
<em>&gt; much(relative to their possiblity, severity, and possibility of
</em><br>
<em>&gt; resolution) as the possibility of a single super-fast self improving
</em><br>
<em>&gt; AI, which will be either Friendly or unFriendly, depending on who
</em><br>
<em>&gt; writes it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; So what's likely? What can be plan for, and what solutions may there
</em><br>
<em>&gt; be? None of the above are particularly well served by planning a
</em><br>
<em>&gt; Friendly architecture, and nothing else. Someone else talked about
</em><br>
<em>&gt; safety measures, I think that may be a realistic thing to think about
</em><br>
<em>&gt; *for some classes of error*. The question is, how likely are those
</em><br>
<em>&gt; classes, and does it make sense to worry about them more than the edge
</em><br>
<em>&gt; cases.
</em><br>
<em>&gt;
</em><br>
<em>&gt; For those of you who are still shaking your heads at the impossibility
</em><br>
<em>&gt; of defending against a transhuman intelligence, let me point out some
</em><br>
<em>&gt; scale. If you imagine that an ascendant AI might take 2 hours from
</em><br>
<em>&gt; first getting out to transcension, that's more than enough time for a
</em><br>
<em>&gt; forewarned military from one of the superpowers to physically destroy
</em><br>
<em>&gt; a signifance portion of internet infrastructure(mines, perhaps), and
</em><br>
<em>&gt; EMP the whole world into the 17th century(ICBMs set for high altitute
</em><br>
<em>&gt; airburst would take less than 45 minutes from anywhere in the
</em><br>
<em>&gt; world(plus America, at least, has EMP weapons), the amount of shielded
</em><br>
<em>&gt; computing centers is miniscule). We may be stupid monkeys, but we've
</em><br>
<em>&gt; spent a lot of time preparing for the use of force. Arguing that we
</em><br>
<em>&gt; would be impotent in front of a new threat requires some fancy
</em><br>
<em>&gt; stepping. I, for one, tend to think there might be classes of danger
</em><br>
<em>&gt; we could defend against, which are worth defending against.
</em><br>
<em>&gt;
</em><br>
<em>&gt;  I am open to persuasion, of course.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; --
</em><br>
<em>&gt; Justin Corwin
</em><br>
<em>&gt; <a href="mailto:outlawpoet@hell.com?Subject=Re:%20Fighting%20UFAI">outlawpoet@hell.com</a>
</em><br>
<em>&gt; <a href="http://outlawpoet.blogspot.com">http://outlawpoet.blogspot.com</a>
</em><br>
<em>&gt; <a href="http://www.adaptiveai.com">http://www.adaptiveai.com</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11631.html">Jef Allbright: "Re: Objective versus subjective reality: which is primary?"</a>
<li><strong>Previous message:</strong> <a href="11629.html">Chris Capel: "Re: Fighting UFAI"</a>
<li><strong>In reply to:</strong> <a href="11468.html">justin corwin: "Re: Fighting UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11487.html">Marc Geddes: "Re: Fighting UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11630">[ date ]</a>
<a href="index.html#11630">[ thread ]</a>
<a href="subject.html#11630">[ subject ]</a>
<a href="author.html#11630">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
