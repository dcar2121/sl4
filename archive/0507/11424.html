<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Universalising an AGI's duty of care</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Universalising an AGI's duty of care">
<meta name="Date" content="2005-07-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Universalising an AGI's duty of care</h1>
<!-- received="Tue Jul 19 15:18:38 2005" -->
<!-- isoreceived="20050719211838" -->
<!-- sent="Tue, 19 Jul 2005 14:18:43 -0700" -->
<!-- isosent="20050719211843" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Universalising an AGI's duty of care" -->
<!-- id="42DD6E33.9010207@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="8d71341e05071914093a8f438e@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Universalising%20an%20AGI's%20duty%20of%20care"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Jul 19 2005 - 15:18:43 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11425.html">Robin Lee Powell: "Re: ET AGI is only a marginal concern"</a>
<li><strong>Previous message:</strong> <a href="11423.html">Philip Sutton: "Re: ET AGI is only a marginal concern"</a>
<li><strong>In reply to:</strong> <a href="11422.html">Russell Wallace: "Re: Universalising an AGI's duty of care"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11427.html">Russell Wallace: "Re: Universalising an AGI's duty of care"</a>
<li><strong>Reply:</strong> <a href="11427.html">Russell Wallace: "Re: Universalising an AGI's duty of care"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11424">[ date ]</a>
<a href="index.html#11424">[ thread ]</a>
<a href="subject.html#11424">[ subject ]</a>
<a href="author.html#11424">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Russell Wallace wrote:
<br>
<em>&gt; On 7/18/05, Philip Sutton &lt;<a href="mailto:Philip.Sutton@green-innovations.asn.au?Subject=Re:%20Universalising%20an%20AGI's%20duty%20of%20care">Philip.Sutton@green-innovations.asn.au</a>&gt; wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;I reckon we should start from the perspective of a person who is advising
</em><br>
<em>&gt;&gt;the makers of AGIs in another galaxy.  What friendliness goals would we
</em><br>
<em>&gt;&gt;recommend that they adopt?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, whatever they choose to do to themselves, what I'd want them to
</em><br>
<em>&gt; do regarding us is simply leave us alone, at least until we're at the
</em><br>
<em>&gt; point where we can talk to them at their own level. That makes sense
</em><br>
<em>&gt; to me as a policy for our FAI if we manage to build one: if hostile
</em><br>
<em>&gt; aliens are encountered, fight back, but if non-hostile intelligent
</em><br>
<em>&gt; life is encountered, leave it alone unless/until it gets to the same
</em><br>
<em>&gt; level as us.
</em><br>
<p>That would explain the Fermi Paradox.  But would you really want aliens to 
<br>
permit the Holocaust?  Would you let it occur on some alien world, if you 
<br>
could see, and knew, and had the power to stop it?  If third parties stepped 
<br>
in to help those who wished help, would you fight to stop them?  If there is 
<br>
any force that interdicts our world, preventing even others from helping us as 
<br>
we would wish to be helped, then I cannot consider them as friends.  It goes 
<br>
back to the disturbing question of aliens with alien motives successfully 
<br>
constructing FAI, a useless hypothesis that explains anything and everything 
<br>
and makes no further predictions even if it's true.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11425.html">Robin Lee Powell: "Re: ET AGI is only a marginal concern"</a>
<li><strong>Previous message:</strong> <a href="11423.html">Philip Sutton: "Re: ET AGI is only a marginal concern"</a>
<li><strong>In reply to:</strong> <a href="11422.html">Russell Wallace: "Re: Universalising an AGI's duty of care"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11427.html">Russell Wallace: "Re: Universalising an AGI's duty of care"</a>
<li><strong>Reply:</strong> <a href="11427.html">Russell Wallace: "Re: Universalising an AGI's duty of care"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11424">[ date ]</a>
<a href="index.html#11424">[ thread ]</a>
<a href="subject.html#11424">[ subject ]</a>
<a href="author.html#11424">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:22:58 MST
</em></small></p>
</body>
</html>
