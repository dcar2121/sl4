<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI boxing</title>
<meta name="Author" content="Michael Vassar (michaelvassar@hotmail.com)">
<meta name="Subject" content="Re: AI boxing">
<meta name="Date" content="2005-07-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI boxing</h1>
<!-- received="Thu Jul 21 13:06:47 2005" -->
<!-- isoreceived="20050721190647" -->
<!-- sent="Thu, 21 Jul 2005 15:06:37 -0400" -->
<!-- isosent="20050721190637" -->
<!-- name="Michael Vassar" -->
<!-- email="michaelvassar@hotmail.com" -->
<!-- subject="Re: AI boxing" -->
<!-- id="BAY101-F7480208B3C46AD1F20CEAACD60@phx.gbl" -->
<!-- inreplyto="20050720211305.07a76f38@localhost.localdomain" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Vassar (<a href="mailto:michaelvassar@hotmail.com?Subject=Re:%20AI%20boxing"><em>michaelvassar@hotmail.com</em></a>)<br>
<strong>Date:</strong> Thu Jul 21 2005 - 13:06:37 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11679.html">Phil Goetz: "&quot;Hostile&quot; transhumans (Re: Magic means large search spaces)"</a>
<li><strong>Previous message:</strong> <a href="11677.html">pdugan: "RE: Fighting UFAI"</a>
<li><strong>In reply to:</strong> <a href="11650.html">Daniel Radetsky: "Re: AI boxing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11664.html">pdugan: "RE: AI boxing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11678">[ date ]</a>
<a href="index.html#11678">[ thread ]</a>
<a href="subject.html#11678">[ subject ]</a>
<a href="author.html#11678">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
No, proof means a proposal for a FAI combined with a formal demonstration 
<br>
that said FAI will be Friendly.
<br>
How about an appeal to ignorance backed by historical precedent.  How many 
<br>
of the things we can do today are magic by earlier standards?  How many are 
<br>
only possible due to our utilization of principles earlier people didn't 
<br>
know about?  Thinkers such as Bacon, Franklin, Hooke and others essentially 
<br>
used the appeal to ignorance in their arguments that we would eventually 
<br>
live forever and travel to the moon, and they were blatantly right.  So did 
<br>
early cryonicists.  The appeal to ignorance does NOT justify saying that we 
<br>
will some day do things that seem impossible according to physics as we know 
<br>
it, but it DOES justify saying we will probably some day do any particular 
<br>
desirable thing that we currently have no idea how to do.  With SI, someday 
<br>
probably means in 15 minutes.  Remember, any given method of doing something 
<br>
may be impossible (like it really may not be possible to fly by breeding and 
<br>
riding giant birds) but the goal can often still be achieved.  At any rate, 
<br>
I find it very unlikely that a SI could not build a crude internal 
<br>
radio-transmitter.  It is obvious that this particular risk could be 
<br>
prevented by prior counter-measures, but if you can figure out 
<br>
counter-measures for EVERY option available to your AI it must not really be 
<br>
smarter than you.  Anyway, it's a safe bet that in actual implementation, 
<br>
even countermeasures against techniques that I can think up will not be 
<br>
implemented.
<br>
<p><em>&gt;From: Daniel Radetsky &lt;<a href="mailto:daniel@radray.us?Subject=Re:%20AI%20boxing">daniel@radray.us</a>&gt;
</em><br>
<em>&gt;Reply-To: <a href="mailto:sl4@sl4.org?Subject=Re:%20AI%20boxing">sl4@sl4.org</a>
</em><br>
<em>&gt;To: <a href="mailto:sl4@sl4.org?Subject=Re:%20AI%20boxing">sl4@sl4.org</a>
</em><br>
<em>&gt;Subject: Re: AI boxing
</em><br>
<em>&gt;Date: Wed, 20 Jul 2005 21:13:05 -0700
</em><br>
<em>&gt;
</em><br>
<em>&gt;On Wed, 20 Jul 2005 17:31:49 -0400
</em><br>
<em>&gt;&quot;Michael Vassar&quot; &lt;<a href="mailto:michaelvassar@hotmail.com?Subject=Re:%20AI%20boxing">michaelvassar@hotmail.com</a>&gt; wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; I agree that no convincing argument has been made that a deceptive proof
</em><br>
<em>&gt; &gt; could be made, or that a UFAI could exploit holes in our mathematical 
</em><br>
<em>&gt;logic
</em><br>
<em>&gt; &gt; and present us with a false proof.  However,
</em><br>
<em>&gt;
</em><br>
<em>&gt;I'm sorry: &quot;proof&quot; means an argument that that the AI should be unboxed?
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; c) &quot;magic&quot; has to be accounted for.  How many things can you do that a 
</em><br>
<em>&gt;dog
</em><br>
<em>&gt; &gt; would simply NEVER think of?  This doesn't have to be &quot;quantum cheat 
</em><br>
<em>&gt;codes&quot;.
</em><br>
<em>&gt; &gt;   It could be something as simple as using the electromagnetic fields 
</em><br>
<em>&gt;within
</em><br>
<em>&gt; &gt; the microchip to trap CO2 molecules in Bose-Einstein condensates and 
</em><br>
<em>&gt;build a
</em><br>
<em>&gt; &gt; quantum medium for itself and/or use electromagnetic fields to guide
</em><br>
<em>&gt; &gt; particles into the shape of a controlled assembler or limited assembler. 
</em><br>
<em>&gt;  It
</em><br>
<em>&gt; &gt; could involve using internal electronics to hack local radio traffic.  
</em><br>
<em>&gt;But
</em><br>
<em>&gt; &gt; it probably involves doing things I haven't thought of.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I'm no physicist, so if you think that those are reasonable possibilities, 
</em><br>
<em>&gt;then
</em><br>
<em>&gt;I'll have to take your word for it. However, I don't see how you can 
</em><br>
<em>&gt;justify
</em><br>
<em>&gt;positing magic on the grounds that we haven't considered every logical
</em><br>
<em>&gt;possibility. It is true that what we believe is a box may not be a box 
</em><br>
<em>&gt;under
</em><br>
<em>&gt;magic, if there exists some magic, but you'll have to give a better 
</em><br>
<em>&gt;argument
</em><br>
<em>&gt;for the existence of this magic than an appeal to ignorance.
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11679.html">Phil Goetz: "&quot;Hostile&quot; transhumans (Re: Magic means large search spaces)"</a>
<li><strong>Previous message:</strong> <a href="11677.html">pdugan: "RE: Fighting UFAI"</a>
<li><strong>In reply to:</strong> <a href="11650.html">Daniel Radetsky: "Re: AI boxing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11664.html">pdugan: "RE: AI boxing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11678">[ date ]</a>
<a href="index.html#11678">[ thread ]</a>
<a href="subject.html#11678">[ subject ]</a>
<a href="author.html#11678">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
