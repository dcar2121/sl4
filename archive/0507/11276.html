<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Intelligence roadblocks (was Re: Fighting UFAI)</title>
<meta name="Author" content="Chris Capel (pdf23ds@gmail.com)">
<meta name="Subject" content="Intelligence roadblocks (was Re: Fighting UFAI)">
<meta name="Date" content="2005-07-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Intelligence roadblocks (was Re: Fighting UFAI)</h1>
<!-- received="Wed Jul 13 12:49:38 2005" -->
<!-- isoreceived="20050713184938" -->
<!-- sent="Wed, 13 Jul 2005 13:49:35 -0500" -->
<!-- isosent="20050713184935" -->
<!-- name="Chris Capel" -->
<!-- email="pdf23ds@gmail.com" -->
<!-- subject="Intelligence roadblocks (was Re: Fighting UFAI)" -->
<!-- id="737b61f3050713114966a15f6d@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="3ad827f3050713110119556f62@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Chris Capel (<a href="mailto:pdf23ds@gmail.com?Subject=Re:%20Intelligence%20roadblocks%20(was%20Re:%20Fighting%20UFAI)"><em>pdf23ds@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Jul 13 2005 - 12:49:35 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11277.html">Eliezer S. Yudkowsky: "Re: Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<li><strong>Previous message:</strong> <a href="11275.html">justin corwin: "Re: Fighting UFAI"</a>
<li><strong>In reply to:</strong> <a href="11275.html">justin corwin: "Re: Fighting UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11277.html">Eliezer S. Yudkowsky: "Re: Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<li><strong>Reply:</strong> <a href="11277.html">Eliezer S. Yudkowsky: "Re: Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<li><strong>Reply:</strong> <a href="11280.html">justin corwin: "Re: Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11276">[ date ]</a>
<a href="index.html#11276">[ thread ]</a>
<a href="subject.html#11276">[ subject ]</a>
<a href="author.html#11276">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 7/13/05, justin corwin &lt;<a href="mailto:outlawpoet@gmail.com?Subject=Re:%20Intelligence%20roadblocks%20(was%20Re:%20Fighting%20UFAI)">outlawpoet@gmail.com</a>&gt; wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Suppose I,
</em><br>
<em>&gt; Eliezer and other self improvement enthusiasts are quite wrong about
</em><br>
<em>&gt; the scaling speed of self improving cognition. We might see AGI
</em><br>
<em>&gt; designs stuck at infrahuman intelligence for ten years(or a thousand,
</em><br>
<em>&gt; but that's a different discussion). In that ten years, do you think
</em><br>
<em>&gt; that even a project that started out as friendly-compliant(whatever
</em><br>
<em>&gt; that means) would remain so? 
</em><br>
<p>Sorry to fork the thread, but this really interests me. This seems
<br>
unlikely, but what if the limited intelligence of humans is due not
<br>
mainly to the firing speed limitations of neurons, but due to some
<br>
architectural limitation in the human brain that would persist even in
<br>
a super-fast thinking AI? Just because an AI has thousands of
<br>
subjective years to think in every minute of our time doesn't mean
<br>
that the AI would necessarily have a memory able to contain thousands
<br>
of years worth of memories, or that it would be able to scale up in
<br>
synthesizing and organizing the vastly larger amounts of information
<br>
than current humans do. It doesn't mean that the AI wouldn't fall prey
<br>
to the same problems of boredom and inertial thinking and the myriad
<br>
rational errors that humans commit that get them believing in really
<br>
confused theories.
<br>
<p>Granted, given what I know about the hodge-podge nature of the
<br>
organization of the brain, it's unlikely that an AI programmer would
<br>
duplicate most of the same problems that humans have in an AI. But if
<br>
it's the case that some of humans' intelligence shortcomings are due
<br>
to a rather fundamental architectural problem, so fundamental that
<br>
it's hard for us to even comprehend it, so fundamental that
<br>
intelligences of different architectures would be unrecognizable as
<br>
intelligences to us, then that could be a huge crimp in developing an
<br>
AI that actually has transhuman intelligence. (Of course, the problems
<br>
manifested would be of a different nature than the ones I mentioned in
<br>
the first paragraph.) Now, these hypothetical limitations might not
<br>
even be ones that actual humans would never run into, because actual
<br>
humans don't live that long, and have slow neurons. But an extremely
<br>
long-lived human, or a human upload, or perhaps only an AI built with
<br>
a very human-patterned architecture, could run into the limitation,
<br>
providing a cap on intelligence that would take many times more effort
<br>
to overcome than the effort that lead to the initial offering. Or not.
<br>
<p>I suppose this is much to speculative to be useful (except in the
<br>
context of justin's original post), unless someone has any data that
<br>
would actually support these ideas. If we do eventually come across
<br>
something like this limiting progress, we'll deal with it then, when
<br>
we've understood enough to even duplicate human intelligence, and
<br>
certainly not before then. In the meantime, even a pessimistic
<br>
estimate of the intelligence of an uploaded human (10-50 times their
<br>
intelligence before they were uploaded) might make them quite
<br>
dangerous to the world if malicious. So I'll stop wasting everyone's
<br>
time now.
<br>
<p>Chris Capel
<br>
<pre>
-- 
&quot;What is it like to be a bat? What is it like to bat a bee? What is it
like to be a bee being batted? What is it like to be a batted bee?&quot;
-- The Mind's I (Hofstadter, Dennet)
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11277.html">Eliezer S. Yudkowsky: "Re: Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<li><strong>Previous message:</strong> <a href="11275.html">justin corwin: "Re: Fighting UFAI"</a>
<li><strong>In reply to:</strong> <a href="11275.html">justin corwin: "Re: Fighting UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11277.html">Eliezer S. Yudkowsky: "Re: Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<li><strong>Reply:</strong> <a href="11277.html">Eliezer S. Yudkowsky: "Re: Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<li><strong>Reply:</strong> <a href="11280.html">justin corwin: "Re: Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11276">[ date ]</a>
<a href="index.html#11276">[ thread ]</a>
<a href="subject.html#11276">[ subject ]</a>
<a href="author.html#11276">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:22:57 MST
</em></small></p>
</body>
</html>
