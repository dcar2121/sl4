<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Intelligence roadblocks (was Re: Fighting UFAI)</title>
<meta name="Author" content="justin corwin (outlawpoet@gmail.com)">
<meta name="Subject" content="Re: Intelligence roadblocks (was Re: Fighting UFAI)">
<meta name="Date" content="2005-07-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Intelligence roadblocks (was Re: Fighting UFAI)</h1>
<!-- received="Wed Jul 13 15:00:14 2005" -->
<!-- isoreceived="20050713210014" -->
<!-- sent="Wed, 13 Jul 2005 13:59:36 -0700" -->
<!-- isosent="20050713205936" -->
<!-- name="justin corwin" -->
<!-- email="outlawpoet@gmail.com" -->
<!-- subject="Re: Intelligence roadblocks (was Re: Fighting UFAI)" -->
<!-- id="3ad827f305071313595979b0fc@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="737b61f3050713114966a15f6d@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> justin corwin (<a href="mailto:outlawpoet@gmail.com?Subject=Re:%20Intelligence%20roadblocks%20(was%20Re:%20Fighting%20UFAI)"><em>outlawpoet@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Jul 13 2005 - 14:59:36 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11474.html">justin corwin: "Re: Fighting UFAI"</a>
<li><strong>Previous message:</strong> <a href="11472.html">Carl Shulman: "Re: Fighting UFAI"</a>
<li><strong>In reply to:</strong> <a href="11469.html">Chris Capel: "Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11587.html">Ralph Cerchione: "Augmentation: Is the Train Leaving the Station?"</a>
<li><strong>Reply:</strong> <a href="11587.html">Ralph Cerchione: "Augmentation: Is the Train Leaving the Station?"</a>
<li><strong>Reply:</strong> <a href="11588.html">Ralph Cerchione: "A Glimpse of the Future -- Or, Has SF Gone Blind?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11473">[ date ]</a>
<a href="index.html#11473">[ thread ]</a>
<a href="subject.html#11473">[ subject ]</a>
<a href="author.html#11473">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 7/13/05, Chris Capel &lt;<a href="mailto:pdf23ds@gmail.com?Subject=Re:%20Intelligence%20roadblocks%20(was%20Re:%20Fighting%20UFAI)">pdf23ds@gmail.com</a>&gt; wrote:
<br>
<em>&gt; Sorry to fork the thread, but this really interests me. This seems
</em><br>
<em>&gt; unlikely, but what if the limited intelligence of humans is due not
</em><br>
<em>&gt; mainly to the firing speed limitations of neurons, but due to some
</em><br>
<em>&gt; architectural limitation in the human brain that would persist even in
</em><br>
<em>&gt; a super-fast thinking AI? Just because an AI has thousands of
</em><br>
<em>&gt; subjective years to think in every minute of our time doesn't mean...
</em><br>
<p>&lt;snip&gt;
<br>
<p>It is an interesting subject, and well worth discussing, quite aside
<br>
from my line of reasoning.
<br>
<p>It's important here to define the playing field and terms. For
<br>
example, note that there is currently only theory, and weak theory
<br>
predicting what AI will actually look like. Anyone who says anything
<br>
is either expressing opinions based on prescientific AI theory (non
<br>
functional, non experimental incomplete stuff) or projective opinions
<br>
based on cognitive science (or more likely, personally extended
<br>
cognitive science). So keep where people are coming from in mind.
<br>
<p>It's common knowledge that AI is different from human intelligence.
<br>
It's not a simple comparison. But there are things we know will be
<br>
different. AI's will be parallel computing, but by human standards,
<br>
they are serial speed demons. Most people interpret this as AIs
<br>
running at great subjective speed compared to humans, but that's not
<br>
quite true. It's valid to say that under most AI designs I've ever
<br>
heard of, the entity would be quite fast at evaluating abstract
<br>
concepts, orders of magnitude above humans. But this doesn't hold true
<br>
for all things. How long would it take the AI to recognize something
<br>
it has seen before?(dependent search through memory) What about how
<br>
long will it take to set actuator outputs?(serially considered
<br>
values?) What does it's 'native timescale' actually look like, and how
<br>
much does translation to physical timescales cost?(learned waits?
<br>
preset actions? what about processes that happen faster than it
<br>
decides?)
<br>
<p>My answers to these kinds of questions will be different than another
<br>
AI theorist, and depend not only on proposed AI architecture, but on
<br>
interactions within architectures we don't actually know about, like
<br>
what the 'intelligence' of a system with human-like architecture but
<br>
perfect memory looks like, or what a completely alien statistical
<br>
intelligence looks like with a poor perceptual model, versus a deep
<br>
modality? &lt;big shrug&gt; I'm not confident in my understanding of these
<br>
concepts within my own proposed design, much less the space of
<br>
human-possible designs. That's why I say more work is necessary, or
<br>
somebody needs to share.
<br>
<p><em>&gt; I suppose this is much to speculative to be useful (except in the
</em><br>
<em>&gt; context of justin's original post), unless someone has any data that
</em><br>
<em>&gt; would actually support these ideas. If we do eventually come across
</em><br>
<em>&gt; something like this limiting progress, we'll deal with it then, when
</em><br>
<em>&gt; we've understood enough to even duplicate human intelligence, and
</em><br>
<em>&gt; certainly not before then. In the meantime, even a pessimistic
</em><br>
<em>&gt; estimate of the intelligence of an uploaded human (10-50 times their
</em><br>
<em>&gt; intelligence before they were uploaded) might make them quite
</em><br>
<em>&gt; dangerous to the world if malicious. So I'll stop wasting everyone's
</em><br>
<em>&gt; time now.
</em><br>
<p>It's true that it's speculative, but it's important to establish some
<br>
kind of possibility space so that people can make informed judgements
<br>
about relevant factors and probability.
<br>
<p>Uploaded humans are of course another question entirely. I tend to be
<br>
of the opinion that they are dangerous, but in predictable ways, and
<br>
not immediately existentially terminal, although pretty suboptimal.
<br>
Assuming they don't kill themselves or change themselves into
<br>
something broken or nonhuman very quickly(which is also a big
<br>
possibility).
<br>
<p>AIs can be dangerous because they're different, and uploads are
<br>
dangerous because they are the same.  pithy, no?
<br>
<p><pre>
-- 
Justin Corwin
<a href="mailto:outlawpoet@hell.com?Subject=Re:%20Intelligence%20roadblocks%20(was%20Re:%20Fighting%20UFAI)">outlawpoet@hell.com</a>
<a href="http://outlawpoet.blogspot.com">http://outlawpoet.blogspot.com</a>
<a href="http://www.adaptiveai.com">http://www.adaptiveai.com</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11474.html">justin corwin: "Re: Fighting UFAI"</a>
<li><strong>Previous message:</strong> <a href="11472.html">Carl Shulman: "Re: Fighting UFAI"</a>
<li><strong>In reply to:</strong> <a href="11469.html">Chris Capel: "Intelligence roadblocks (was Re: Fighting UFAI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11587.html">Ralph Cerchione: "Augmentation: Is the Train Leaving the Station?"</a>
<li><strong>Reply:</strong> <a href="11587.html">Ralph Cerchione: "Augmentation: Is the Train Leaving the Station?"</a>
<li><strong>Reply:</strong> <a href="11588.html">Ralph Cerchione: "A Glimpse of the Future -- Or, Has SF Gone Blind?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11473">[ date ]</a>
<a href="index.html#11473">[ thread ]</a>
<a href="subject.html#11473">[ subject ]</a>
<a href="author.html#11473">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
