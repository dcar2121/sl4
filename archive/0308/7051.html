<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [SL4] brainstorm: a new vision for uploading</title>
<meta name="Author" content="Nick Hay (nickjhay@hotmail.com)">
<meta name="Subject" content="Re: [SL4] brainstorm: a new vision for uploading">
<meta name="Date" content="2003-08-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [SL4] brainstorm: a new vision for uploading</h1>
<!-- received="Thu Aug 14 04:16:37 2003" -->
<!-- isoreceived="20030814101637" -->
<!-- sent="Thu, 14 Aug 2003 22:16:10 +1200" -->
<!-- isosent="20030814101610" -->
<!-- name="Nick Hay" -->
<!-- email="nickjhay@hotmail.com" -->
<!-- subject="Re: [SL4] brainstorm: a new vision for uploading" -->
<!-- id="200308142216.10482.nickjhay@hotmail.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="200308140047.02569.samantha@objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Nick Hay (<a href="mailto:nickjhay@hotmail.com?Subject=Re:%20[SL4]%20brainstorm:%20a%20new%20vision%20for%20uploading"><em>nickjhay@hotmail.com</em></a>)<br>
<strong>Date:</strong> Thu Aug 14 2003 - 04:16:10 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7052.html">Ben Goertzel: "RE: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Previous message:</strong> <a href="7050.html">Samantha Atkins: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>In reply to:</strong> <a href="7050.html">Samantha Atkins: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7076.html">Samantha Atkins: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Reply:</strong> <a href="7076.html">Samantha Atkins: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7051">[ date ]</a>
<a href="index.html#7051">[ thread ]</a>
<a href="subject.html#7051">[ subject ]</a>
<a href="author.html#7051">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Samantha Atkins wrote:
<br>
<em>&gt; On Wednesday 13 August 2003 07:52, Gordon Worley wrote:
</em><br>
<em>&gt; &gt; Many AGI projects is, in my opinion, a bad idea.  Each one is more than
</em><br>
<em>&gt; &gt; another chance to create the Singularity.  Each one is a chance for
</em><br>
<em>&gt; &gt; existential disaster.  Even a Friendly AI project has a significant
</em><br>
<em>&gt; &gt; risk of negative outcome because Earth has no AI experts.  Rather we
</em><br>
<em>&gt; &gt; have a lot of smart people flopping around, some flopping in the right
</em><br>
<em>&gt; &gt; direction more than others, hoping they'll hit the right thing.  But no
</em><br>
<em>&gt; &gt; one knows how to do it with great confidence.  It could be that one day
</em><br>
<em>&gt; &gt; 10 or 20 years from now the universe just doesn't wake up because it
</em><br>
<em>&gt; &gt; was eaten during the night.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The entire universe?  Naw.  Many AGI projects is a great idea precisely
</em><br>
<em>&gt; because we don't know which path is the most fruitful with least danger at
</em><br>
<em>&gt; this time.  If humanity is facing almost certain disaster without an AGI
</em><br>
<em>&gt; and only with the right kind of AGI is the likelihood of survival/thriving
</em><br>
<em>&gt; high, then even risky possible paths are reasonble in light of the certain
</em><br>
<em>&gt; of doom without AGI.
</em><br>
<p>Risky paths are reasonable only if there are no knowable faults with the path. 
<br>
Creating an AI without a concrete theory of Friendliness, perhaps because you 
<br>
don't think it's necessary or possible to work out anything beforehand, is a 
<br>
knowable fault. It is both necessary and possible to work out essential 
<br>
things beforehand (eg. identifying &quot;silent death&quot; scenarios where the AI 
<br>
you're experimenting with appears to perfectly pick up Friendliness, but 
<br>
become unFriendly as soon as it's not dependant on humans). You can't work 
<br>
out every detail so you'll update and test your theories as evidence from AI 
<br>
development comes in.
<br>
<p>Creating an AI with the belief that no special or non-anthropomorphic efforts 
<br>
are needed for Friendliness, perhaps assuming it'll be an emergent behaviour 
<br>
of interaction between altruistic humans and the developing AI or that you 
<br>
need only raise the AI 'child' right, is another knowable fault. There a 
<br>
bunch of them, since there are always more ways to go wrong than right.
<br>
<p>An AI effort is only a necessary risk given that it has no knowable faults. 
<br>
The project must have a complete theory of Friendliness, for instance. If you 
<br>
don't know exactly how your AI's going to be Friendly, it probably won't be, 
<br>
so you shouldn't start coding until you do. Even then you have to be careful 
<br>
to have a design that'll actually work out, which requires you to be 
<br>
sufficenlty rational and take efforts to &quot;debug&quot; as many human 
<br>
irrationalities and flaws as you can.
<br>
<p>&quot;AGI project&quot; -&gt; &quot;Friendly AGI project&quot; is not a trivial transformation. Most 
<br>
AI projects I know of have not taken sufficent upfront effort towards 
<br>
Friendliness, and are therefore &quot;unFriendly AGI projects&quot; (in the sense of 
<br>
non-Friendly, not explictly evil) until they do. You have to have pretty 
<br>
strong evidence that there is nothing that can be discovered upfront to not 
<br>
take the conservative decision to work out as much Friendliness as possible 
<br>
before starting. 
<br>
<p>Since an unFriendly AI is one of the top (if not the top) existential risk, 
<br>
we're doomed both with and without AGI. For an AGI to have a good chance not 
<br>
to destroy us, Friendliness is necessary. Ergo, Friendly AIs are better than 
<br>
our default condition. By default an AGI is unFriendly: humane morality is 
<br>
not something that simply emerges from code. If the AGI project hasn't taken 
<br>
significant up front measures to understand Friendliness, along with 
<br>
continuing measures whilst developing the AI, it's not likely to be Friendly.
<br>
<p><em>&gt; It would be a stupid
</em><br>
<em>&gt; AI that would literally wipe out &quot;everything&quot; which would belie it being
</em><br>
<em>&gt; smart enough to grab and retain control of &quot;everything&quot;.
</em><br>
<p>The unFriendly AI wouldn't destroy literally everything, but optimise nearby 
<br>
matter to better fufill it's goal system. An unFriendly AI doesn't care for 
<br>
humans, and so we get used as raw matter for computronium. It's this kind of 
<br>
scenario that's the risk.
<br>
<p>- Nick
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7052.html">Ben Goertzel: "RE: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Previous message:</strong> <a href="7050.html">Samantha Atkins: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>In reply to:</strong> <a href="7050.html">Samantha Atkins: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7076.html">Samantha Atkins: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Reply:</strong> <a href="7076.html">Samantha Atkins: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7051">[ date ]</a>
<a href="index.html#7051">[ thread ]</a>
<a href="subject.html#7051">[ subject ]</a>
<a href="author.html#7051">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
