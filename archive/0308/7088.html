<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Singularity function.</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="Singularity function.">
<meta name="Date" content="2003-08-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Singularity function.</h1>
<!-- received="Thu Aug 28 14:24:31 2003" -->
<!-- isoreceived="20030828202431" -->
<!-- sent="Thu, 28 Aug 2003 21:24:24 +0100 (BST)" -->
<!-- isosent="20030828202424" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="Singularity function." -->
<!-- id="20030828202424.62116.qmail@web21504.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20Singularity%20function."><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Thu Aug 28 2003 - 14:24:24 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7089.html">Alexander Mayboroda: "JOIN:Generally thinking;"</a>
<li><strong>Previous message:</strong> <a href="7087.html">Samantha Atkins: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7088">[ date ]</a>
<a href="index.html#7088">[ thread ]</a>
<a href="subject.html#7088">[ subject ]</a>
<a href="author.html#7088">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hello. My discovery of this community and the associated
<br>
writings on practical aspects of the singularity was less
<br>
than a month ago; due to time constraints I'm still building
<br>
a well-justified stance on these issues. However I do have one
<br>
pressing question which I have not succeeded in locating an
<br>
answer to elsewhere. There is a mention of it on the
<br>
'singularity holes' SL4 wiki page
<br>
(<a href="http://www.sl4.org/bin/wiki.pl?SingularityHoles">http://www.sl4.org/bin/wiki.pl?SingularityHoles</a>) and
<br>
several occurrences of superficially similar arguments about
<br>
self-improvement rate constraints in the archives, but the
<br>
following question was not addressed in detail.
<br>
&nbsp;
<br>
The singularity argument assumes that the technological
<br>
development of intelligence capable of direct
<br>
self-improvement will result in the next phase of the
<br>
exponential increase in intelligence seen over the lifespan
<br>
of the local universe so far. As I see it the rate of
<br>
intelligence change is determined by some function that takes
<br>
the resources available and the desired step in intelligence
<br>
as parameters and generates a probability distribution
<br>
describing the likelihood of the intelligence increase
<br>
occurring over time. To date the rate of increase has been
<br>
improving as biological evolution has become more efficient,
<br>
followed by a recent sharp spike as cultural evolution became
<br>
possible (and rapidly more effective). Both these processes
<br>
would appear to have sharp limits on their ultimate
<br>
effectiveness; the question of whether they might work
<br>
together to produce hyperintelligence is rendered moot by
<br>
the (likely endemic) high instability of minimally sentient
<br>
organic intelligence society over evolutionary meaningful
<br>
timescales.
<br>
&nbsp;
<br>
The heart of the singularity is clearly the sudden creation
<br>
of a feedback loop that makes the current level of
<br>
intelligence an important parameter of the function giving
<br>
the probability/time distribution for the creation of an
<br>
arbitrarily higher intelligence. When this function is
<br>
differentiated with respect to time it rapidly becomes the
<br>
only significant parameter. The critical question is
<br>
therefore 'what can we say about the relationship between
<br>
the current intelligence level and the difficulty (median of
<br>
the probability/time curve) of reaching the next incremental
<br>
level?'. The first part breaks down into a secondary
<br>
relationship between current intelligence and the total
<br>
computing power available (ops/second); the premise of
<br>
self-improving intelligence implies that with increasing
<br>
intelligence these operations will also be more efficiently
<br>
directed. In other words at any one point on the graph
<br>
developing intelligence is a non-deterministic O^n
<br>
computational problem where n is inversely proportional to
<br>
the current intelligence level.
<br>
&nbsp;
<br>
If that assumption is correct then anything we can say about
<br>
the function relating base difficulty O to target
<br>
intelligence level I is critically important to determining
<br>
if singularity will occur. I'm assuming that it may well
<br>
change at arbitrary transition points as intelligence
<br>
increases; boundaries similar to the development of tool use,
<br>
cultural evolution and complete self-awareness (seed AI).
<br>
If the relationship is linear then geometric takeoff will
<br>
occur; a hard takeoff towards singularity if that's the
<br>
initial relationship. If the relationship is quadratic then
<br>
growth will be exponential; possibly a hard or a soft
<br>
takeoff depending on the exponent, but singularity all the
<br>
same. If the relationship is cubic then growth is linear;
<br>
a posthuman era certainly, but not a singularity as I
<br>
understand it. If the relationship gets much worse than
<br>
cubic at any point intelligence plataeus until computational
<br>
power increases enough to get things back into gear; if
<br>
it can't be increased fast enough then progress halts and
<br>
singularity doesn't occur.
<br>
&nbsp;
<br>
I intuitively feel that it should be possible to place some
<br>
bounds on this function using complexity theory and/or
<br>
existing data points (evolution produced humanity in a few
<br>
billion years, humanity produced artificial intelligence
<br>
equivalent to insects within decades of the computing power
<br>
becoming available). However I don't have the relevant
<br>
training to follow this line of reasoning much further.
<br>
Following review of the archives and wiki pages it seems
<br>
likely to me that someone here has thought this through
<br>
and arrived at a more complete answer, so I would
<br>
appreciate comments on the validity and implications of
<br>
the above.
<br>
&nbsp;
<br>
&nbsp;* Michael Wilson
<br>
<p><p><p>---------------------------------
<br>
Want to chat instantly with your online friends? Get the FREE Yahoo!Messenger
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7089.html">Alexander Mayboroda: "JOIN:Generally thinking;"</a>
<li><strong>Previous message:</strong> <a href="7087.html">Samantha Atkins: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7088">[ date ]</a>
<a href="index.html#7088">[ thread ]</a>
<a href="subject.html#7088">[ subject ]</a>
<a href="author.html#7088">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
