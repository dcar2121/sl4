<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [SL4] brainstorm: a new vision for uploading</title>
<meta name="Author" content="Nick Hay (nickjhay@hotmail.com)">
<meta name="Subject" content="Re: [SL4] brainstorm: a new vision for uploading">
<meta name="Date" content="2003-08-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [SL4] brainstorm: a new vision for uploading</h1>
<!-- received="Thu Aug 14 17:31:57 2003" -->
<!-- isoreceived="20030814233157" -->
<!-- sent="Fri, 15 Aug 2003 11:31:20 +1200" -->
<!-- isosent="20030814233120" -->
<!-- name="Nick Hay" -->
<!-- email="nickjhay@hotmail.com" -->
<!-- subject="Re: [SL4] brainstorm: a new vision for uploading" -->
<!-- id="200308151131.20448.nickjhay@hotmail.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="AHCKKOMPBFCFGCAA@mailcity.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Nick Hay (<a href="mailto:nickjhay@hotmail.com?Subject=Re:%20[SL4]%20brainstorm:%20a%20new%20vision%20for%20uploading"><em>nickjhay@hotmail.com</em></a>)<br>
<strong>Date:</strong> Thu Aug 14 2003 - 17:31:20 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7056.html">Gordon Worley: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Previous message:</strong> <a href="7054.html">king-yin yan: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>In reply to:</strong> <a href="7054.html">king-yin yan: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7056.html">Gordon Worley: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7055">[ date ]</a>
<a href="index.html#7055">[ thread ]</a>
<a href="subject.html#7055">[ subject ]</a>
<a href="author.html#7055">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
king-yin yan wrote:
<br>
<em>&gt; I don't have a clear understanding of the big picture yet, but I think I've
</em><br>
<em>&gt; spotted a mistake here: &quot;Robust&quot; Friendliness requires non-anthropomorphic,
</em><br>
<em>&gt; mathematical / logical precision. Anything less than that would be risky.
</em><br>
<p>Formal mathematial systems are one tool humans use to solve problems. I don't 
<br>
think it's well suited to the task of describing and transferring human moral 
<br>
structure and content to an AI. I don't think this process can be well 
<br>
described mathematically.
<br>
<p>*** UPDATE, . formal systems humans create directly via axioms
<br>
<p><em>&gt; However, Friendly to whom? We seem to have already excluded other
</em><br>
<em>&gt; primates from consideration, not to mention animals. Even if that is OK,
</em><br>
<em>&gt; the defintion of Friendliness will become more problematic when uploading
</em><br>
<em>&gt; becomes available. Are uploads humans? Do copies have separate votes or
</em><br>
<em>&gt; 1 vote? I'm not sure how a formal system of Friendliness is capable of
</em><br>
<em>&gt; dealing with such questions.
</em><br>
<p>&quot;Friendliness&quot; is very different to &quot;friendliness&quot;. A Friendly AI is one that 
<br>
shares the moral complexity we all share - the adapatations we use to argue 
<br>
and think about morality as we are now. The Friendly AI doesn't quite share 
<br>
all human moral complexity, not all parts are desirable (eg. selfish aspects 
<br>
of morality), but humane moral complexity the kind of morality structure (and 
<br>
content) we'd want to have. 
<br>
<p>So primates, animals are certainly not clearly ruled out - plenty of human 
<br>
moralities judge that to be a bad thing, all other things equal. The aim is 
<br>
not to answer these questions at the start, which is a far more political 
<br>
solution, but to give the FAI itself the ability to think about the issues 
<br>
and answer these questions as it gets smarter and more humane.
<br>
<p>Friendliness isn't a formal system, certainly not in the moral law sense - 
<br>
they're far too fragile. Typically manipulating or adding axioms vastly 
<br>
change the system. Formal systems in general lack the flexibity and structure 
<br>
of the human thoughts that create them. We don't want to transfer the moral 
<br>
codes of law that humans can create, but the ability to create those code in 
<br>
the first place. The programmers don't decide what is right and wrong.
<br>
<p>For an example of the kind of theory of Friendliness I mean, albeit an 
<br>
incomplete and outdated one, have a look at <a href="http://intelligence.org/CFAI">http://intelligence.org/CFAI</a> . It 
<br>
describes various useful structures: those needed to understand the idea of 
<br>
approximating a goal (External Reference Semantics), of representing and 
<br>
acquiring the forces underneath human morality (Shaper/Anchor Semantics), and 
<br>
generally examining all causes behind its creation (Causal Validity 
<br>
Semantics). These are the AI equivalent of certain kinds of moral structure 
<br>
we all share (all being members of a single species).
<br>
<p><em>&gt; The second problem is that Friendliness will be designed by a group of
</em><br>
<em>&gt; *human* programmers, not by an AI. If we then concentrate all our
</em><br>
<em>&gt; computational resources to the FAI, then Friendliness will effectively
</em><br>
<em>&gt; become some sort of universal political solution. In other words:
</em><br>
<em>&gt; A group of human programmers will have to design a perfect political
</em><br>
<em>&gt; system. That sounds very unrealistic...
</em><br>
<p>The programmers don't design the solution 1) they aren't smart/rational/humane 
<br>
enough 2) it isn't programmer indepedent. A FAI written by the Marquis de 
<br>
Sade should not different from one written by a Buddist monk, or any other 
<br>
human, because FAI's are designed to be not sensitive to the particular moral 
<br>
conclusions or opinions that differnet between potential programmers, but the 
<br>
things they (and non-programmers alike) all share.
<br>
<p><em>&gt; The question is why put ourselves in a vulnerable position, sub-
</em><br>
<em>&gt; -ordinating to a superintelligence which doesn't even exist now.
</em><br>
<p>You should really read CFAI: Beyond Anthropomorphism :) Our position right 
<br>
now, with a whole bunch of near-equals getting more and more powerful 
<br>
weapons, is vulnerable. Indeed every existential risk makes us vulnerable, 
<br>
that's 1/2 the point of a Singularity in the first place. We can't eliminate 
<br>
all risks, or remove all vulnerablity, but decrease it.
<br>
<p>FAI orginated superintelligences aren't like a tribal leaders, or tribal 
<br>
councils, or governments, or any other [human] structure which is 
<br>
superordinate to other sentients. The SI doesn't have, nor does it want, 
<br>
political control as humans do. It wants sufficent control to ensure bullets 
<br>
simply don't hit anyone who doesn't want to be shot, for instance, but it 
<br>
doesn't want sufficent control to ensure everyone &quot;agrees with it&quot;, for 
<br>
instance. Anthropomorphisms, that is almost any comparison between AIs and 
<br>
humans, don't help understanding.
<br>
<p><em>&gt; Personally I think the most appealing solution is to let people augment
</em><br>
<em>&gt; themselves rather than create autonomous intelligent entities. But we
</em><br>
<em>&gt; don't have a direct neural interface to connect our brains to computers.
</em><br>
<p>Personally I think that's one of the least appealing solutions. Humans are 
<br>
autonomous intelligence entities with reams of known flaws. Fears about an 
<br>
entity, or group of humans, rising among the rest and subordinating them are 
<br>
far more founded than those about AIs because, historically speaking, that's 
<br>
what humans *do*. Often they proclaim they're doing the best for everyone, 
<br>
and often they'll believe it, but rationalisation distorts actions in a 
<br>
self-biased manner. Unless there's some way to augment everyone at the same 
<br>
rate, and in fact even then, it doesn't look good.
<br>
<p>Part of the appeal of the Friendly AI approach is starting from a blank slate. 
<br>
Making a mind focussed about rationality and altruism, not politics. 
<br>
<p>However, there is a matter of time here.  think it's far easier to spark a 
<br>
superintelligence from an AI than from a human brain, in the sense that I 
<br>
imagine it'll be possible to do the former first. So attempts at solely 
<br>
augmenting humans will be too late, since I can't see everyone stopping their 
<br>
AI projects. However things would be very different if the human augmentation 
<br>
route to superintelligence was significantly faster than the AI route.
<br>
<p>(for further details here, see <a href="http://intelligence.org/intro/whyAI.html">http://intelligence.org/intro/whyAI.html</a>)
<br>
<p>Mind you, various human augmentations could certainly help things - perhaps a 
<br>
little device that alerted humans when they're rationalising. Or something 
<br>
that increased the level of mental energy without compromising the ability to 
<br>
think properly. But augmenting or uploading humans, as the sole route, 
<br>
doesn't seem either desirable or practical. 
<br>
<p><em>&gt; Unless we get uploaded otherwise we'll have to rely on LANGUAGE to
</em><br>
<em>&gt; communicate with computers. This *linguistic bottleneck* is the hardest
</em><br>
<em>&gt; problem I think.
</em><br>
<p>We'll have to rely on thoughts, and the things they do. Using human language 
<br>
to directly communicate with an AI is more of a final step - the AI has to be 
<br>
quite mature to understand human language directly, I suspect. But there are 
<br>
other ways to communicate, or more generally transfer information to the AI. 
<br>
For instance, posing simple problems for the AI to solve.
<br>
<p><em>&gt; PS Thanks for everyone else's reply...
</em><br>
<p>Thanks for your questions. 
<br>
<p>- Nick
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7056.html">Gordon Worley: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Previous message:</strong> <a href="7054.html">king-yin yan: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>In reply to:</strong> <a href="7054.html">king-yin yan: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7056.html">Gordon Worley: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7055">[ date ]</a>
<a href="index.html#7055">[ thread ]</a>
<a href="subject.html#7055">[ subject ]</a>
<a href="author.html#7055">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
