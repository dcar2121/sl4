<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [SL4] brainstorm: a new vision for uploading</title>
<meta name="Author" content="Nick Hay (nickjhay@hotmail.com)">
<meta name="Subject" content="Re: [SL4] brainstorm: a new vision for uploading">
<meta name="Date" content="2003-08-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [SL4] brainstorm: a new vision for uploading</h1>
<!-- received="Tue Aug 12 18:44:32 2003" -->
<!-- isoreceived="20030813004432" -->
<!-- sent="Wed, 13 Aug 2003 12:38:46 +1200" -->
<!-- isosent="20030813003846" -->
<!-- name="Nick Hay" -->
<!-- email="nickjhay@hotmail.com" -->
<!-- subject="Re: [SL4] brainstorm: a new vision for uploading" -->
<!-- id="200308131238.46814.nickjhay@hotmail.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="HHEMEAPPBIDMFCAA@mailcity.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Nick Hay (<a href="mailto:nickjhay@hotmail.com?Subject=Re:%20[SL4]%20brainstorm:%20a%20new%20vision%20for%20uploading"><em>nickjhay@hotmail.com</em></a>)<br>
<strong>Date:</strong> Tue Aug 12 2003 - 18:38:46 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7041.html">Philip Sutton: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Previous message:</strong> <a href="7039.html">Mitchell Porter: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>In reply to:</strong> <a href="7038.html">king-yin yan: "[SL4] brainstorm: a new vision for uploading"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7043.html">Philip Sutton: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Reply:</strong> <a href="7043.html">Philip Sutton: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7040">[ date ]</a>
<a href="index.html#7040">[ thread ]</a>
<a href="subject.html#7040">[ subject ]</a>
<a href="author.html#7040">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
king-yin yan wrote:
<br>
<em>&gt; I think Eliezer's vision of a single superAI is rather problematic. I
</em><br>
<em>&gt; think a diversity of specific-purpose AI's is the more likely scenario.
</em><br>
<em>&gt; The reasons are as follows:
</em><br>
<p>Below, I've tried to address separately (1) whether a single FAI is a good 
<br>
idea and (2) whether it more likely there'll be (in the near term) one AI far 
<br>
superior to the rest, or a community of rough equals.
<br>
<p><em>&gt; 1. The definition of Friendliness is a political issue. There is no such
</em><br>
<em>&gt; thing as value-free, &quot;objective&quot; morality; the Friendly AI can only
</em><br>
<em>&gt; *inherit* the moral system of its creators and share-holders (if done
</em><br>
<em>&gt; right at all!) and the Friendly-AI-as-a-God-like-moral-figure is unsound.
</em><br>
<em>&gt; The debate about Friendliness will itself start a political war, rather
</em><br>
<em>&gt; than solve all political problems.
</em><br>
<p>The political issues are the human differences, we share a great degree of 
<br>
moral structure that is left unnoticed in political arguments. The structures 
<br>
which allow us to reason about morality at all, particular forces that shape 
<br>
what counts as a morality, are human universals. This is the target of 
<br>
Friendly AI, what all (neurologically normal) humans share, not the surface 
<br>
political arguments of any one group.
<br>
<p>Because of this independence of the FAI from any programmer peculiar opinions 
<br>
or morals is a central design aim. It shouldn't matter who made the FAI, as 
<br>
long as they designed it with this independence in mind (and they succeed in 
<br>
creating it!). Interim approximations will use the programmers beliefs, 
<br>
probably, since they're the nearby humans, but the final result will not 
<br>
depend on that - we don't want programmer dependance any more than anyone 
<br>
else. 
<br>
<p>You're right. Analogies between Friendly AIs and anything human, especially 
<br>
Gods, are unsound and misleading.
<br>
<p><em>&gt; 2. One may argue that superAI will be very powerful and everyone
</em><br>
<em>&gt; would want to be on &quot;our&quot; side. But this also is an unlikely scenario
</em><br>
<em>&gt; because it does not resolve the problem of *who* will have more
</em><br>
<em>&gt; power within our &quot;party&quot;. Once again this would depend on the
</em><br>
<em>&gt; definition of Friendliness and thus start a war. (I'm actually quite
</em><br>
<em>&gt; pacifist by the way =))
</em><br>
<p>The definiton of Friendliness is independent of any one human. We're all 
<br>
members of the same species, and that accounts for most of our features. 
<br>
However, the tiny individual differences between human are adaptively 
<br>
important and are thus what we tend to see and argue about. We don't tend to 
<br>
argue so much about the things all humans share which other animals lack.
<br>
<p>There are no sides. If the FAI can be seen to be on a side at all ve's on 
<br>
everyone's side, or humanity's side. This really isn't a political battle. 
<br>
<p><em>&gt; 3. Safety. It is better to diversify the risk by building several AIs
</em><br>
<em>&gt; so in case one goes awry the others (perhaps many others) will be
</em><br>
<em>&gt; able to suppress it -- fault-tolerance of distributive systems. It
</em><br>
<em>&gt; seems the best way is to let a whole lot of people augment their
</em><br>
<em>&gt; intelligence via uploading or *cyborganization*.
</em><br>
<p>Not necessarily. Firstly such an effort can only reduce independant risks - 
<br>
there are failure modes in which all AIs fail. Given that as an aim, what's 
<br>
the best distribution of effort between seperate AI projects? It's seems that 
<br>
we shouldn't spread our effort thin and create a whole bunch of low-quality 
<br>
AIs, but concentrate it on creating a high-quality AI.
<br>
<p>The idea of policing a community against offending individuals is only 
<br>
feasible among groups of equals. It seems unlikely that different AIs will be 
<br>
equal unless the ones ahead held back to wait for the others to catch up. An 
<br>
unFriendly AI is unlikely to slow down under such a situation, giving it a 
<br>
head start. 
<br>
<p>Fault-tolerance can be built into a single mind itself. The distinction 
<br>
between one mind and a group is especially clear in humans, who have a fixed 
<br>
non-agglomerative amount of brainware, but less so in AIs. If needed, an AI 
<br>
can take multiple points of view, with the final outcome depending on some 
<br>
form of internal consensus. 
<br>
<p><em>&gt; 4. The superAI is unfathomable (hence unpredictable) to us, so
</em><br>
<em>&gt; what's the difference between this and other techno-catastrophies?
</em><br>
<p>The difference between Friendly AI and most other scenarios is that other 
<br>
scenarios are predictably bad. Friendly AI is unpredictable, but only 
<br>
unpredictable in the sene that we don't actually know what right will turn 
<br>
out to be. We've already seen development of human morality in recent times 
<br>
(equal rights independant of race and sex, seeing slavery as evil), we can't 
<br>
know where we'd end up if we were actually getting more intelligent (our 
<br>
brains have remained constant over recorded history) at the same time.
<br>
<p>The most likely predictably bad scenarios I can think of:
<br>
* unFriendly AI - all dead, lay waste to galaxy and outward (possibly). Since 
<br>
this involve greater than human intelligence this is unpredictable in 
<br>
details, but the important moral consequences are predictable.
<br>
* nanotech war - all, or most, dead.
<br>
<p>Of course I'm skipping over a whole lot of details here as in the rest of this 
<br>
post.
<br>
<p><em>&gt; 5. Even if we have FAI, it probably will not stop some people from
</em><br>
<em>&gt; uploading themselves destructively (They have their rights). This will
</em><br>
<em>&gt; still create inequality between uploaders and those remaining
</em><br>
<em>&gt; flesh-and-blood.
</em><br>
<p>But what does this 'inequality' matter? Just as they have the right to upload 
<br>
themselves destructively, you have your right not to worry about being 
<br>
harmed. As far as I can see, there is no race.
<br>
<p><em>&gt; Therefore the superAI scenario will not happen UNLESS there are
</em><br>
<em>&gt; some compelling reasons to build it. The fear is that destructive
</em><br>
<em>&gt; uploading will create too much of a first-move advantage to the
</em><br>
<em>&gt; effect that everyone would be compelled to follow suit immediately.
</em><br>
<p>There are other dangers which have to been taking into account before 
<br>
decisions are made. A significant one being existential risks - how can we 
<br>
minimise them? 
<br>
<p><em>&gt; So the goal should be clear: Create a technology for humans that
</em><br>
<em>&gt; would allow them to be on-par with uploads. And I think that
</em><br>
<em>&gt; answer would be: &quot;personal AI&quot;. The PAI starts off like a baby
</em><br>
<em>&gt; and shares the users experience, like a dual existence. By the
</em><br>
<em>&gt; time cyborganization is available, the cyborganization process
</em><br>
<em>&gt; would be like merging with one's personal AI.
</em><br>
<p>This is something to do when existential risks have been dealt with. What's to 
<br>
stop someone creating an unFriendly AI from their personal AI? Or from 
<br>
starting nanotechnological war? What stops present day suffering and death? 
<br>
It's not a bad idea, it's just that I don't think this is a good first step.
<br>
<p><em>&gt; Thus, the rights to transhuman intelligence is distributed to all
</em><br>
<em>&gt; those who can afford it. If you think about it, that is probably
</em><br>
<em>&gt; the only sensible way to deal with computational power
</em><br>
<em>&gt; explosion... ie to create a broadly distributed balance-of-power.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It doesn't matter that many people may not be techno-savy
</em><br>
<em>&gt; enough to use the AI -- that depends on user-friendliness and
</em><br>
<em>&gt; the best AI should be quite transparent and easy to use.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Well, this still sounds very vague and difficult, but it's more
</em><br>
<em>&gt; plausible than the superAI scenario already (I think).
</em><br>
<p>How's that? It'd be helpful to keep plausibility separate from desirability.
<br>
<p><em>&gt; One last problem that remains is poverty. I predict that
</em><br>
<em>&gt; some people will be maginalized from cyborganization, rather
</em><br>
<em>&gt; inevitable. Who am I to save humanity? We have to accept
</em><br>
<em>&gt; this and the next best thing is to maximize availability
</em><br>
<em>&gt; through education and perhaps redistribution of wealth,
</em><br>
<em>&gt; creation of more jobs etc.
</em><br>
<p>As you might imagine this is only the tip of an iceberg. For futher details 
<br>
I'd recommend reading the <a href="http://intelligence.org/">http://intelligence.org/</a> materials:
<br>
* <a href="http://intelligence.org/CFAI/">http://intelligence.org/CFAI/</a> - Creating Friendly AI (in particular part 2 - 
<br>
beyond anthropomorphism)
<br>
* the introduction documents on the Singularity there
<br>
* anything else that looks interesting
<br>
<p>In particular this will clear up misunderstandings on what SIAI means by 
<br>
creating a Friendly AI, and why we think it's an all-round good idea. 
<br>
<p>- Nick
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7041.html">Philip Sutton: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Previous message:</strong> <a href="7039.html">Mitchell Porter: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>In reply to:</strong> <a href="7038.html">king-yin yan: "[SL4] brainstorm: a new vision for uploading"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7043.html">Philip Sutton: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Reply:</strong> <a href="7043.html">Philip Sutton: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7040">[ date ]</a>
<a href="index.html#7040">[ thread ]</a>
<a href="subject.html#7040">[ subject ]</a>
<a href="author.html#7040">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
