<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [SL4] brainstorm: a new vision for uploading</title>
<meta name="Author" content="Nick Hay (nickjhay@hotmail.com)">
<meta name="Subject" content="Re: [SL4] brainstorm: a new vision for uploading">
<meta name="Date" content="2003-08-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [SL4] brainstorm: a new vision for uploading</h1>
<!-- received="Fri Aug 15 22:20:55 2003" -->
<!-- isoreceived="20030816042055" -->
<!-- sent="Sat, 16 Aug 2003 16:20:04 +1200" -->
<!-- isosent="20030816042004" -->
<!-- name="Nick Hay" -->
<!-- email="nickjhay@hotmail.com" -->
<!-- subject="Re: [SL4] brainstorm: a new vision for uploading" -->
<!-- id="200308161620.05353.nickjhay@hotmail.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="IDAEPCANJCIKGCAA@mailcity.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Nick Hay (<a href="mailto:nickjhay@hotmail.com?Subject=Re:%20[SL4]%20brainstorm:%20a%20new%20vision%20for%20uploading"><em>nickjhay@hotmail.com</em></a>)<br>
<strong>Date:</strong> Fri Aug 15 2003 - 22:20:04 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7064.html">Philip Sutton: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Previous message:</strong> <a href="7062.html">Nick Hay: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>In reply to:</strong> <a href="7060.html">king-yin yan: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7064.html">Philip Sutton: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Reply:</strong> <a href="7064.html">Philip Sutton: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Reply:</strong> <a href="7065.html">Paul Fidika: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7063">[ date ]</a>
<a href="index.html#7063">[ thread ]</a>
<a href="subject.html#7063">[ subject ]</a>
<a href="author.html#7063">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
king-yin yan wrote:
<br>
<em>&gt; There is a dilemma in here, on the one hand a formal system (made of
</em><br>
<em>&gt; simplistic rules and thus mathematically analysable) will be predictable
</em><br>
<em>&gt; and safe, but it can't handle the moral complexities that we would want.
</em><br>
<p>Right, it'll be predicable. It won't necessarily be safe - a lot of those 
<br>
moral complexities are necessary for safety.
<br>
<p><em>&gt; On the other hand, the complex moral structure that you described
</em><br>
<em>&gt; above will require a connectionist approach or something equivalent.
</em><br>
<em>&gt; Meaning that it has distributed representations, graded response,
</em><br>
<em>&gt; generalization, and being able to be *trained*. 
</em><br>
<p>Connectionism, in the sense of building everything out of neurologically 
<br>
inspired networks, has at least two problems. Firstly it is substrate that 
<br>
may not be best suited for the kind of computational hardware we have - fast, 
<br>
digital, serial. It seems more suited to implementing a mind on meat - cells 
<br>
with a slow rate of computation, which need to be massively parallelised to 
<br>
get anything done. Secondly it conflates the levels of organisation - you 
<br>
don't introduce all information on the lowest level, code, but on various 
<br>
different level built on top of code. You don't design things solely on the 
<br>
atomic level.
<br>
<p>See <a href="http://www.intelligence.org/seedAI/">http://www.intelligence.org/seedAI/</a> for a more sensible AI design.
<br>
<p>Although I'm not quite sure what you mean by &quot;connectionist approach or 
<br>
something equivalent&quot;. Can you elaborate? What AI methods is it contrasted 
<br>
with?
<br>
<p><em>&gt; Then you have a big
</em><br>
<em>&gt; problem. Practically such a connectionist network is quite similar to a
</em><br>
<em>&gt; human being, but much smarter. 
</em><br>
<p>Similar to humans in what way? Humans, like all species, contain a lot of 
<br>
specific complexity. A lot of this is important complexity that needs to be 
<br>
explictly introduced (eg. the structure beneath humane morality). 
<br>
<p>Why will connectionist networks be smarter?
<br>
<p><em>&gt;Every human would end up trying to
</em><br>
<em>&gt; talk to this AI like crazy in order to influence its behavior in their
</em><br>
<em>&gt; favor...
</em><br>
<p>This is unnecessary in a Friendly AI, in the sense that it won't make any 
<br>
difference (this is a good thing!) - the FAI's final state should not depend 
<br>
on who programmed it, or who talked to the AI in ver youth, except in so much 
<br>
as deciding the binary issue &quot;Friendly or unFriendly&quot;. This independence is a 
<br>
desirable goal towards which the FAI can direct its intelligence. Ensuring 
<br>
the FAI remains Friendly is a complex and important design consideration.
<br>
<p><em>&gt; I can understand why you're alarmed by intelligence augmentation, what
</em><br>
<em>&gt; you say is basically: &quot;Computational power is dangerous, let's concentrate
</em><br>
<em>&gt; all the power in one AI and let it rule&quot;. But you seemed to downplay the
</em><br>
<em>&gt; fact that 1) the Friendliness system is designed by human programmers;
</em><br>
<em>&gt; 2) it needs to be trained by humans. I'm afraid a lot of people will be
</em><br>
<em>&gt; skeptical about this.
</em><br>
<p>There is no distinction between one AI and many - unless the many have 
<br>
divergent goal systems ie. they're not all Friendly. If &quot;let the AI rule&quot; 
<br>
uses &quot;rule&quot; in the same sense that &quot;physics rules humans&quot; then sure. If 
<br>
you're imagining a benevolent dictator who exerts social control, then no. 
<br>
<p>Actually, I don't think that phrase by itself is a good summary of what I'm 
<br>
trying to say. And I can't think of a single sentence that'd accurately 
<br>
describe it :)
<br>
<p>Of course the number of skeptical people is often independent of the truth of 
<br>
a given statement, but you don't mean that. A Friendly AI is designed by 
<br>
human programmers, its *initial* training is specified by human programmers. 
<br>
This is true for all AIs. A Friendly AI is specifically designed to not be 
<br>
sensitive to the differences between humans or to its particular programmers, 
<br>
not sensitive to the various classes of mistakes the programmers can make, 
<br>
etc. This is not an issue that is downplayed, although perhaps I have in my 
<br>
posts, but one that is explicitly recognised. Most of CFAI describes 
<br>
particular structures needed for a solution of this problem.
<br>
<p><em>&gt; &gt;FAI orginated superintelligences aren't like a tribal leaders, or tribal
</em><br>
<em>&gt; &gt;councils, or governments, or any other [human] structure which is
</em><br>
<em>&gt; &gt;superordinate to other sentients. The SI doesn't have, nor does it want,
</em><br>
<em>&gt; &gt;political control as humans do. It wants sufficent control to ensure
</em><br>
<em>&gt; &gt; bullets simply don't hit anyone who doesn't want to be shot, for
</em><br>
<em>&gt; &gt; instance, but it doesn't want sufficent control to ensure everyone
</em><br>
<em>&gt; &gt; &quot;agrees with it&quot;, for instance. Anthropomorphisms, that is almost any
</em><br>
<em>&gt; &gt; comparison between AIs and humans, don't help understanding.
</em><br>
<em>&gt;
</em><br>
<em>&gt; That sounds like a universal political solution. The FAI will decide
</em><br>
<em>&gt; whether wars should be fought or not, who are criminals and deserve what
</em><br>
<em>&gt; kinds of punishment, etc.
</em><br>
<p>Fight wars? Punish criminals? *Have* criminals? Why is any of this necessary?
<br>
<p>&quot;universal political solution&quot;?
<br>
<p><em>&gt; &gt;Personally I think that's one of the least appealing solutions. Humans are
</em><br>
<em>&gt; &gt;autonomous intelligence entities with reams of known flaws. Fears about an
</em><br>
<em>&gt; &gt;entity, or group of humans, rising among the rest and subordinating them
</em><br>
<em>&gt; &gt; are far more founded than those about AIs because, historically speaking,
</em><br>
<em>&gt; &gt; that's what humans *do*. Often they proclaim they're doing the best for
</em><br>
<em>&gt; &gt; everyone, and often they'll believe it, but rationalisation distorts
</em><br>
<em>&gt; &gt; actions in a self-biased manner. Unless there's some way to augment
</em><br>
<em>&gt; &gt; everyone at the same rate, and in fact even then, it doesn't look good.
</em><br>
<em>&gt;
</em><br>
<em>&gt; What you're depicting here is dangerously close to dictatorship. On the
</em><br>
<em>&gt; other hand, free augmentation is actually not that bad. 
</em><br>
<p>Do you think I'm suggesting we supress human augmentation technologies? Is 
<br>
that what you mean by &quot;dicatorship&quot;? If so, I wasn't clear: I was arguing 
<br>
that *accelerating* human augmentation isn't the best use of our efforts and 
<br>
that accelerating *Friendly* AI is, at least at present, a far better 
<br>
investment. Both because Friendly AIs are safer and more desirable that your 
<br>
typical human augment or upload, and because Friendly AIs of a given 
<br>
intelligence should be easier to get and thus exist earlier. This is 
<br>
important because that means unFriendly AIs (ie. any AI that isn't Friendly) 
<br>
can come before human augments/upload with enough intelligence to protect 
<br>
against them.
<br>
<p><em>&gt; Just because
</em><br>
<em>&gt; humans are free to augment their intelligence does not mean that they
</em><br>
<em>&gt; will start using that intelligence to harm others. Most likely a kind of
</em><br>
<em>&gt; morality will emerge in the population so no one will have an absolute
</em><br>
<em>&gt; advantage over others.
</em><br>
<p>You're right, people won't start using their intelligence to harm others, 
<br>
deliberately or not. The risk is they'll continue as they have throughout 
<br>
human history. 
<br>
<p>Of course I think humans should be free to augment their intelligence, I don't 
<br>
think we shouldn't supress people trying to augment humans. However I don't 
<br>
think it's a practical or desirable route to the Singularity - a human mind 
<br>
isn't the best seed for a superintelligence, nor is a human brain an easy to 
<br>
expand substrate (compared to that of an AI). And all those other reasons 
<br>
I've mentioned elsewhere.
<br>
<p><em>&gt; &gt;Part of the appeal of the Friendly AI approach is starting from a blank
</em><br>
<em>&gt; &gt; slate. Making a mind focussed about rationality and altruism, not
</em><br>
<em>&gt; &gt; politics.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It's much more complicated than that, if you look closer...
</em><br>
<p>Some things are suprisingly simple. What complexities am I ignoring?
<br>
<p><em>&gt; &gt;However, there is a matter of time here.  think it's far easier to spark a
</em><br>
<em>&gt; &gt;superintelligence from an AI than from a human brain, in the sense that I
</em><br>
<em>&gt; &gt;imagine it'll be possible to do the former first. So attempts at solely
</em><br>
<em>&gt; &gt;augmenting humans will be too late, since I can't see everyone stopping
</em><br>
<em>&gt; &gt; their AI projects. However things would be very different if the human
</em><br>
<em>&gt; &gt; augmentation route to superintelligence was significantly faster than the
</em><br>
<em>&gt; &gt; AI route.
</em><br>
<em>&gt;
</em><br>
<em>&gt; There's an even more important question: Whether the AI can really be
</em><br>
<em>&gt; controlled by its own designer. 
</em><br>
<p>&quot;Control&quot;? If you mean control in the sense of &quot;the AI obeys our commands&quot; 
<br>
kind of control, then this is not what we want. This is termed the 
<br>
&quot;adversarial attitude&quot; and is not a workable solution to Friendly AI. 
<br>
<p>If you mean control in the sense of &quot;best ensure the FAI remains 
<br>
humane/Friendly&quot; then this question has complex answer. For many AI designs 
<br>
this is not possible. Friendly AI is an effort to explore this possibility - 
<br>
it should be possible to have at least as much &quot;control&quot; (in this sense) over 
<br>
FAIs as over any group of humans, although you can't simply assume that's 
<br>
true. This is a complex matter than cannot be simply decided. CFAI (of 
<br>
course) goes into more details.
<br>
<p><em>&gt; On the one hand you want the AI to have
</em><br>
<em>&gt; common sense. That requires a connectionist appraoch (or something
</em><br>
<em>&gt; similar). 
</em><br>
<p>Why is something similar to a connectionist approach necessary to implement 
<br>
and transfer common sense?
<br>
<p><em>&gt; Once you have connectionism then the AI is pretty much
</em><br>
<em>&gt; autonomous.. Then it is somewhat like a human child. That would be like
</em><br>
<em>&gt; all humanity having only *1* kid and giving him/her all the power.
</em><br>
<p>Not really, it's more like having only *1* set of physical laws. There's no 
<br>
reason to consider and FAI more like a single human child than an entire 
<br>
human civilisation. It's like neither, but the important point is that you 
<br>
can't use your intuitions about humans abusing power to judge the likelihood 
<br>
of an FAI abusing power - your intuitions transparently assume too much. When 
<br>
you reason about minds in this manner you are using adaptations specialised 
<br>
for dealing with humans - they were the only class of mind around in your 
<br>
evolutionary history. As such they assume specific things about minds that 
<br>
don't hold in general since they didn't need to work for minds-in-general and 
<br>
could specialise (or rather, were specialised from the start) eg. single 
<br>
minds are most likely to abuse power than multiple minds, or indeed that 
<br>
minds-in-general are likely to abuse power at all.
<br>
<p><em>&gt; Now why are you so sure that a connectionist system will behave as
</em><br>
<em>&gt; you want it, given all its complex characteristics?
</em><br>
<p>I don't, because I don't suggest we use anything like a connectionist system. 
<br>
I suggest we use a far more complex, and well-specified, solution.  
<br>
Friendliness is specifically designed to work in this kind of situation, it's 
<br>
not designed to need a human safety net.
<br>
<p>We can't guarentee it'll work, but we can compare the likelihood of it working 
<br>
to other navigational scenarios. Why do you think a society of augmented 
<br>
humans will behave as you want, given all its complex characteristics? Why do 
<br>
you think it's more likely to behave as you want than a Friendly AI?
<br>
<p><em>&gt; &gt;(for further details here, see <a href="http://intelligence.org/intro/whyAI.html">http://intelligence.org/intro/whyAI.html</a>)
</em><br>
<em>&gt;
</em><br>
<em>&gt; Thanks, I've read that, and I've browsed through CFAI briefly.
</em><br>
<p>CFAI is one of those documents you don't understand even if you read it 
<br>
closely. I personally found I had to read it multiple times and I still don't 
<br>
think I understand it all. It doesn't appear that you understand it, 
<br>
otherwise you wouldn't be casually mentioning Friendly AIs &quot;taking over&quot; or 
<br>
the need for humans to &quot;control the AI&quot; etc without justifaction (ie. 
<br>
anthropomorphisms specifically dealt with in CFAI). It really is well worth 
<br>
the effort.
<br>
<p><em>&gt; The problem is AI's are likely to take over rather than care about us.
</em><br>
<em>&gt; Unless we figure out a way to control them. If we do, then it is a kind
</em><br>
<em>&gt; of augmentation (external rather than implanted).
</em><br>
<p>This is anthropomorphic. If you create an unFriendly AI it uses us as 
<br>
resources, it doesn't rule us like a human would. An unFriendly AI has no 
<br>
reason to treat us differently from any other arrangement of matter, expect 
<br>
in so much as our behaviour may be specifically tuned to counteract its 
<br>
goals. Perhaps that's what you meant by &quot;take over&quot;. 
<br>
<p>What's this distinction you see between AIs and humans? Why can't an AI be as 
<br>
moral or more moral than a human? Humans have specific adapatations for 
<br>
taking over the tribe, for abusing power when it suits them, etc. A FAI will 
<br>
not. Why will augmented humans care about us rather than take over?
<br>
<p><em>&gt; Augmenting/uploading is not necessarily undesirable. 
</em><br>
<p>You're right: it's not necessarily undesirable in itself. As a route to the 
<br>
Singularity, explictly contrasted with the Friendly AI route, I supect it is. 
<br>
There is the added difficulty of AI probably being easier than augmentation, 
<br>
and far more probably easier than uploading. &quot;easier&quot; essentially meaning 
<br>
&quot;will come first&quot;. 
<br>
<p><em>&gt; Sure, some people
</em><br>
<em>&gt; will end up more intelligent than others. But that's just the way human
</em><br>
<em>&gt; diversity is always like. No one is likely to attain absolute power, so I
</em><br>
<em>&gt; think that's fine.
</em><br>
<p>Why isn't anyone likely to attain power? What probability would you attach to 
<br>
an upload/augment turning into a world dictator, or some such thing, as 
<br>
humans often do given enough power? How about a large group taking power? Or 
<br>
simply destroying everything with nanoweapons? etc.
<br>
<p><em>&gt; Question: How can you have an AI understand you, without letting it be an
</em><br>
<em>&gt; autonomous entity? On the one hand we want a tool, on the other hand
</em><br>
<em>&gt; we want to make sure it will not become the master. And actually the crux
</em><br>
<em>&gt; of the problem comes from the linguistic bottleneck. Imagine if we have
</em><br>
<em>&gt; direct neural interfaces on the back of our necks, then we'll all be busy
</em><br>
<em>&gt; playing with add-on modules now, with magazines advertising all sorts of
</em><br>
<em>&gt; gadgets, like body-building etc.
</em><br>
<p>You and the AI don't have to be separate minds, so the dichotomy of autonomous 
<br>
vs. tool is false. But I won't go into that here. We don't want a tool, as I 
<br>
think I illustrated in a previous post, and we don't want a master. Howver an 
<br>
AI is extremely unlikely to want to become our master, unless you plan to 
<br>
build in a &quot;human-like social dominance&quot; module, which would be an incredibly 
<br>
stupid move. Social dominance is an undesirable human trait which an AI will 
<br>
lack without it being explicitly introduced.
<br>
<p>It'd be nice to have direct neural interfaces, and to the extent they're 
<br>
developed and are useful they'll be used. But, especially since it appears AI 
<br>
will be feasible before significant neural interfaces, it's not a good idea 
<br>
to tie ourselves to this.
<br>
<p>What problems do you see with non-neurologial methods of information transfer? 
<br>
How will neural interfaces solve this problem, and what kind of neural 
<br>
interface is necessary? Perhaps the problems can be solved in alternate 
<br>
manners? For instance, one can notice that linguistically we have problems 
<br>
describing unambiguous external referents, problems pinning down the meaning. 
<br>
We can study this seperately, to find more feasible solutions.
<br>
<p><p><p>Note I'm speaking specifically about Friendly AIs and not an arbitary AI. CFAI 
<br>
describes one particular FAI design, or rather a class, and the kind of 
<br>
structures that are necessary. Notice the details (eg. external reference 
<br>
semantics, shaper/anchor semantics) are much more specific than &quot;a 
<br>
connectionist AI&quot; or &quot;training an X&quot; and it's this specificity that allows 
<br>
particular statements not warrented for AIs in general, or minds-in-general, 
<br>
to be made. (minds-in-general refer to the class of all possible minds, 
<br>
humans, AIs, and humane superintelligence being particular sub-classes of 
<br>
this space).
<br>
<p>I think our disagreements are caused by disagreement on more fundamental 
<br>
premises eg. what AIs can be, what Friendly AIs are. In particular, here is a 
<br>
pair of contrasting views, the former is my approximation to your view, the 
<br>
latter to mine:
<br>
<p>* Friendly AIs can be no more moral than humans (or aren't likely to be), or 
<br>
groups there of, and quite likely far less. Human control, or supervision, of 
<br>
(mature) FAIs would increase their safety. 
<br>
<p>* Friendly AIs can be far more moral than humans. Humans weren't designed to 
<br>
be altruistic, rational, or good. They were designed to be selfish (in the 
<br>
sense of increasing *their* inclusive fitness) in a hunter-gatherer 
<br>
lifestyle, it's largely by accident that we can be altruistic at all (except 
<br>
in the limited sense of reciprocal altruism - selfish trade). Augmented 
<br>
humans can change this, but a human mind is not suited to major revision 
<br>
unlike that of a Seed AI, and there's an additional risk that the selfishness 
<br>
won't be successfully removed (note that I'm not speaking just about 
<br>
deliberative desires to be selfish, but all the aspects of the mind that lead 
<br>
to selfish behaviour - rationalisation, etc)
<br>
<p>(this is not very complete, and possibly not very accurate, but it's a start)
<br>
<p>These issues are discussed at length in CFAI and LOGI. One can only fit a 
<br>
small amount of detail into an email. In particular, you might like to read 
<br>
these sections first:
<br>
<p>CFAI:
<br>
* 1: Challenges of Friendly AI, 1.1: Envisioning perfection 
<br>
* Appendix A.1: Indexed FAQ
<br>
* 2: Beyond anthropomorphism; Interlude: Beyond the adversarial attitude
<br>
<p>LOGI: 
<br>
* 3.1: Advantages of minds-in-general
<br>
<p>As you might guess, these issues have been discussed frequently and more 
<br>
throughly in the past. In addition to the above documents, and the singinst 
<br>
site, there's the SL4 achive. However I'm not sure how interested you are, 
<br>
and how much reading you're willing to do :)
<br>
<p>- Nick
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7064.html">Philip Sutton: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Previous message:</strong> <a href="7062.html">Nick Hay: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>In reply to:</strong> <a href="7060.html">king-yin yan: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7064.html">Philip Sutton: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Reply:</strong> <a href="7064.html">Philip Sutton: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Reply:</strong> <a href="7065.html">Paul Fidika: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7063">[ date ]</a>
<a href="index.html#7063">[ thread ]</a>
<a href="subject.html#7063">[ subject ]</a>
<a href="author.html#7063">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
