<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [SL4] brainstorm: a new vision for uploading</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: [SL4] brainstorm: a new vision for uploading">
<meta name="Date" content="2003-08-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [SL4] brainstorm: a new vision for uploading</h1>
<!-- received="Wed Aug 20 20:06:16 2003" -->
<!-- isoreceived="20030821020616" -->
<!-- sent="Wed, 20 Aug 2003 16:33:17 -0700" -->
<!-- isosent="20030820233317" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: [SL4] brainstorm: a new vision for uploading" -->
<!-- id="200308201633.17706.samantha@objectent.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="200308142216.10482.nickjhay@hotmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20[SL4]%20brainstorm:%20a%20new%20vision%20for%20uploading"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Wed Aug 20 2003 - 17:33:17 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7077.html">James Rogers: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Previous message:</strong> <a href="7075.html">Samantha Atkins: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>In reply to:</strong> <a href="7051.html">Nick Hay: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7042.html">Mitchell Porter: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7076">[ date ]</a>
<a href="index.html#7076">[ thread ]</a>
<a href="subject.html#7076">[ subject ]</a>
<a href="author.html#7076">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Thursday 14 August 2003 03:16, Nick Hay wrote:
<br>
<p><em>&gt; &gt; The entire universe?  Naw.  Many AGI projects is a great idea precisely
</em><br>
<em>&gt; &gt; because we don't know which path is the most fruitful with least danger
</em><br>
<em>&gt; &gt; at this time.  If humanity is facing almost certain disaster without an
</em><br>
<em>&gt; &gt; AGI and only with the right kind of AGI is the likelihood of
</em><br>
<em>&gt; &gt; survival/thriving high, then even risky possible paths are reasonble in
</em><br>
<em>&gt; &gt; light of the certain of doom without AGI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Risky paths are reasonable only if there are no knowable faults with the
</em><br>
<em>&gt; path. Creating an AI without a concrete theory of Friendliness, perhaps
</em><br>
<em>&gt; because you don't think it's necessary or possible to work out anything
</em><br>
<em>&gt; beforehand, is a knowable fault. It is both necessary and possible to work
</em><br>
<em>&gt; out essential things beforehand (eg. identifying &quot;silent death&quot; scenarios
</em><br>
<em>&gt; where the AI you're experimenting with appears to perfectly pick up
</em><br>
<em>&gt; Friendliness, but become unFriendly as soon as it's not dependant on
</em><br>
<em>&gt; humans). You can't work out every detail so you'll update and test your
</em><br>
<em>&gt; theories as evidence from AI development comes in.
</em><br>
<p>Well, that is fine except you did not address the primary point I attempted to 
<br>
make.  The measure of risk is relative to alternatives.  If we face almost 
<br>
certain death of humanity if we do not create AGI then the amount of risk 
<br>
that is tolerable enough to move forward needs to be adjusted accordingly.
<br>
<p><em>&gt;
</em><br>
<em>&gt; An AI effort is only a necessary risk given that it has no knowable faults.
</em><br>
<p>I disagree.  What is an is not knowable comes into play and what is and isn't 
<br>
a survivable fault is also germane.
<br>
<p><em>&gt; The project must have a complete theory of Friendliness, for instance. If
</em><br>
<em>&gt; you don't know exactly how your AI's going to be Friendly, it probably
</em><br>
<em>&gt; won't be, so you shouldn't start coding until you do. Even then you have to
</em><br>
<em>&gt; be careful to have a design that'll actually work out, which requires you
</em><br>
<em>&gt; to be sufficenlty rational and take efforts to &quot;debug&quot; as many human
</em><br>
<em>&gt; irrationalities and flaws as you can.
</em><br>
<p>I am sorry but I see no way to fully work out the theory of Friendliness to 
<br>
such a state of completion given the limitations of human minds at this time.  
<br>
In particular there are many parts of an AI that have nothing to do with 
<br>
Friendliness whose coding certainly doesn't require waiting for such a theory 
<br>
to be complete.   As we need some of these subparts to be available in order 
<br>
to be bright enough to carry out the rest of the work including getting a 
<br>
more airtight theory of Friendliness and testing its implementation, it would 
<br>
obviously be suicidal to put off all coding until we had the complete theory.
<br>
<p><em>&gt;
</em><br>
<em>&gt; &quot;AGI project&quot; -&gt; &quot;Friendly AGI project&quot; is not a trivial transformation.
</em><br>
<em>&gt; Most AI projects I know of have not taken sufficent upfront effort towards
</em><br>
<em>&gt; Friendliness, and are therefore &quot;unFriendly AGI projects&quot; (in the sense of
</em><br>
<em>&gt; non-Friendly, not explictly evil) until they do. You have to have pretty
</em><br>
<em>&gt; strong evidence that there is nothing that can be discovered upfront to not
</em><br>
<em>&gt; take the conservative decision to work out as much Friendliness as possible
</em><br>
<em>&gt; before starting.
</em><br>
<p>Your &quot;therefore&quot; does not follow.  It has some assumptions packed into it that 
<br>
are questionable.   What is &quot;as much as possible&quot;?   To whose satisfaction?
<br>
<p><em>&gt;
</em><br>
<em>&gt; Since an unFriendly AI is one of the top (if not the top) existential risk,
</em><br>
<em>&gt; we're doomed both with and without AGI. For an AGI to have a good chance
</em><br>
<em>&gt; not to destroy us, Friendliness is necessary. Ergo, Friendly AIs are better
</em><br>
<em>&gt; than our default condition. By default an AGI is unFriendly: humane
</em><br>
<em>&gt; morality is not something that simply emerges from code. If the AGI project
</em><br>
<em>&gt; hasn't taken significant up front measures to understand Friendliness,
</em><br>
<em>&gt; along with continuing measures whilst developing the AI, it's not likely to
</em><br>
<em>&gt; be Friendly.
</em><br>
<em>&gt;
</em><br>
<p>I disagree that unFriendly AI is the top existential risk.  The top risk is 
<br>
our own stupidity using the technology at our command to destroy ourselves 
<br>
accidentally or on purpose.  We are not bright enough individually or 
<br>
collectively to successfully navigate the issues and problems that we 
<br>
presently face indefinitely much less as they speed up and become more 
<br>
complex. So the hightest priority toward saving humanity is to develop and 
<br>
deploy greater intelligence.   One of the things this includes is various 
<br>
levels of AI leading up to AGI.    I very much agree with the importance of 
<br>
making AGI friendly.  But I very much doubt that we are capable of fully 
<br>
developing AGI much less friendly AGI in theory or practice without 
<br>
considerable AI and IA along the way.   
<br>
&nbsp;
<br>
- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7077.html">James Rogers: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Previous message:</strong> <a href="7075.html">Samantha Atkins: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>In reply to:</strong> <a href="7051.html">Nick Hay: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7042.html">Mitchell Porter: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7076">[ date ]</a>
<a href="index.html#7076">[ thread ]</a>
<a href="subject.html#7076">[ subject ]</a>
<a href="author.html#7076">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
