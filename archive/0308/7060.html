<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [SL4] brainstorm: a new vision for uploading</title>
<meta name="Author" content="king-yin yan (y.k.y@lycos.com)">
<meta name="Subject" content="Re: [SL4] brainstorm: a new vision for uploading">
<meta name="Date" content="2003-08-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [SL4] brainstorm: a new vision for uploading</h1>
<!-- received="Fri Aug 15 15:21:20 2003" -->
<!-- isoreceived="20030815212120" -->
<!-- sent="Fri, 15 Aug 2003 17:21:06 -0400" -->
<!-- isosent="20030815212106" -->
<!-- name="king-yin yan" -->
<!-- email="y.k.y@lycos.com" -->
<!-- subject="Re: [SL4] brainstorm: a new vision for uploading" -->
<!-- id="IDAEPCANJCIKGCAA@mailcity.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="[SL4] brainstorm: a new vision for uploading" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> king-yin yan (<a href="mailto:y.k.y@lycos.com?Subject=Re:%20[SL4]%20brainstorm:%20a%20new%20vision%20for%20uploading"><em>y.k.y@lycos.com</em></a>)<br>
<strong>Date:</strong> Fri Aug 15 2003 - 15:21:06 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7061.html">Tommy McCabe: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Previous message:</strong> <a href="7059.html">king-yin yan: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Maybe in reply to:</strong> <a href="7038.html">king-yin yan: "[SL4] brainstorm: a new vision for uploading"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7063.html">Nick Hay: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Reply:</strong> <a href="7063.html">Nick Hay: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7060">[ date ]</a>
<a href="index.html#7060">[ thread ]</a>
<a href="subject.html#7060">[ subject ]</a>
<a href="author.html#7060">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi Nick,
<br>
<p><em>&gt;Formal mathematial systems are one tool humans use to solve problems. I don't 
</em><br>
<em>&gt;think it's well suited to the task of describing and transferring human moral 
</em><br>
<em>&gt;structure and content to an AI. I don't think this process can be well 
</em><br>
<em>&gt;described mathematically.
</em><br>
<em>&gt;
</em><br>
<em>&gt;*** UPDATE, . formal systems humans create directly via axioms
</em><br>
<em>&gt;
</em><br>
<em>&gt;&quot;Friendliness&quot; is very different to &quot;friendliness&quot;. A Friendly AI is one that 
</em><br>
<em>&gt;shares the moral complexity we all share - the adapatations we use to argue 
</em><br>
<em>&gt;and think about morality as we are now. The Friendly AI doesn't quite share 
</em><br>
<em>&gt;all human moral complexity, not all parts are desirable (eg. selfish aspects 
</em><br>
<em>&gt;of morality), but humane moral complexity the kind of morality structure (and 
</em><br>
<em>&gt;content) we'd want to have. 
</em><br>
<em>&gt;
</em><br>
<em>&gt;Friendliness isn't a formal system, certainly not in the moral law sense - 
</em><br>
<em>&gt;they're far too fragile. Typically manipulating or adding axioms vastly 
</em><br>
<em>&gt;change the system. Formal systems in general lack the flexibity and structure 
</em><br>
<em>&gt;of the human thoughts that create them. We don't want to transfer the moral 
</em><br>
<em>&gt;codes of law that humans can create, but the ability to create those code in 
</em><br>
<em>&gt;the first place. The programmers don't decide what is right and wrong.
</em><br>
<p>There is a dilemma in here, on the one hand a formal system (made of
<br>
simplistic rules and thus mathematically analysable) will be predictable
<br>
and safe, but it can't handle the moral complexities that we would want.
<br>
On the other hand, the complex moral structure that you described
<br>
above will require a connectionist approach or something equivalent.
<br>
Meaning that it has distributed representations, graded response,
<br>
generalization, and being able to be *trained*. Then you have a big
<br>
problem. Practically such a connectionist network is quite similar to a
<br>
human being, but much smarter. Every human would end up trying to
<br>
talk to this AI like crazy in order to influence its behavior in their favor...
<br>
<p><em>&gt; [...]
</em><br>
<em>&gt;
</em><br>
<em>&gt;You should really read CFAI: Beyond Anthropomorphism :) Our position right 
</em><br>
<em>&gt;now, with a whole bunch of near-equals getting more and more powerful 
</em><br>
<em>&gt;weapons, is vulnerable. Indeed every existential risk makes us vulnerable, 
</em><br>
<em>&gt;that's 1/2 the point of a Singularity in the first place. We can't eliminate 
</em><br>
<em>&gt;all risks, or remove all vulnerablity, but decrease it.
</em><br>
<p>I can understand why you're alarmed by intelligence augmentation, what
<br>
you say is basically: &quot;Computational power is dangerous, let's concentrate
<br>
all the power in one AI and let it rule&quot;. But you seemed to downplay the
<br>
fact that 1) the Friendliness system is designed by human programmers;
<br>
2) it needs to be trained by humans. I'm afraid a lot of people will be
<br>
skeptical about this.
<br>
<p><em>&gt;FAI orginated superintelligences aren't like a tribal leaders, or tribal 
</em><br>
<em>&gt;councils, or governments, or any other [human] structure which is 
</em><br>
<em>&gt;superordinate to other sentients. The SI doesn't have, nor does it want, 
</em><br>
<em>&gt;political control as humans do. It wants sufficent control to ensure bullets 
</em><br>
<em>&gt;simply don't hit anyone who doesn't want to be shot, for instance, but it 
</em><br>
<em>&gt;doesn't want sufficent control to ensure everyone &quot;agrees with it&quot;, for 
</em><br>
<em>&gt;instance. Anthropomorphisms, that is almost any comparison between AIs and 
</em><br>
<em>&gt;humans, don't help understanding.
</em><br>
<p>That sounds like a universal political solution. The FAI will decide whether
<br>
wars should be fought or not, who are criminals and deserve what kinds of
<br>
punishment, etc.
<br>
<p><em>&gt;&gt; Personally I think the most appealing solution is to let people augment
</em><br>
<em>&gt;&gt; themselves rather than create autonomous intelligent entities. But we
</em><br>
<em>&gt;&gt; don't have a direct neural interface to connect our brains to computers.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Personally I think that's one of the least appealing solutions. Humans are 
</em><br>
<em>&gt;autonomous intelligence entities with reams of known flaws. Fears about an 
</em><br>
<em>&gt;entity, or group of humans, rising among the rest and subordinating them are 
</em><br>
<em>&gt;far more founded than those about AIs because, historically speaking, that's 
</em><br>
<em>&gt;what humans *do*. Often they proclaim they're doing the best for everyone, 
</em><br>
<em>&gt;and often they'll believe it, but rationalisation distorts actions in a 
</em><br>
<em>&gt;self-biased manner. Unless there's some way to augment everyone at the same 
</em><br>
<em>&gt;rate, and in fact even then, it doesn't look good.
</em><br>
<p>What you're depicting here is dangerously close to dictatorship. On the
<br>
other hand, free augmentation is actually not that bad. Just because
<br>
humans are free to augment their intelligence does not mean that they
<br>
will start using that intelligence to harm others. Most likely a kind of
<br>
morality will emerge in the population so no one will have an absolute
<br>
advantage over others.
<br>
<p><em>&gt;Part of the appeal of the Friendly AI approach is starting from a blank slate. 
</em><br>
<em>&gt;Making a mind focussed about rationality and altruism, not politics. 
</em><br>
<p>It's much more complicated than that, if you look closer...
<br>
<p><em>&gt;However, there is a matter of time here.  think it's far easier to spark a 
</em><br>
<em>&gt;superintelligence from an AI than from a human brain, in the sense that I 
</em><br>
<em>&gt;imagine it'll be possible to do the former first. So attempts at solely 
</em><br>
<em>&gt;augmenting humans will be too late, since I can't see everyone stopping their 
</em><br>
<em>&gt;AI projects. However things would be very different if the human augmentation 
</em><br>
<em>&gt;route to superintelligence was significantly faster than the AI route.
</em><br>
<p>There's an even more important question: Whether the AI can really be
<br>
controlled by its own designer. On the one hand you want the AI to have
<br>
common sense. That requires a connectionist appraoch (or something
<br>
similar). Once you have connectionism then the AI is pretty much
<br>
autonomous. Then it is somewhat like a human child. That would be like
<br>
all humanity having only *1* kid and giving him/her all the power.
<br>
Now why are you so sure that a connectionist system will behave as
<br>
you want it, given all its complex characteristics?
<br>
<p><em>&gt;(for further details here, see <a href="http://intelligence.org/intro/whyAI.html">http://intelligence.org/intro/whyAI.html</a>)
</em><br>
<p>Thanks, I've read that, and I've browsed through CFAI briefly.
<br>
<p><em>&gt;Mind you, various human augmentations could certainly help things - perhaps a 
</em><br>
<em>&gt;little device that alerted humans when they're rationalising. Or something 
</em><br>
<em>&gt;that increased the level of mental energy without compromising the ability to 
</em><br>
<em>&gt;think properly. But augmenting or uploading humans, as the sole route, 
</em><br>
<em>&gt;doesn't seem either desirable or practical. 
</em><br>
<p>The problem is AI's are likely to take over rather than care about us.
<br>
Unless we figure out a way to control them. If we do, then it is a kind
<br>
of augmentation (external rather than implanted).
<br>
<p>Augmenting/uploading is not necessarily undesirable. Sure, some people
<br>
will end up more intelligent than others. But that's just the way human
<br>
diversity is always like. No one is likely to attain absolute power, so I
<br>
think that's fine.
<br>
<p><em>&gt;&gt; Unless we get uploaded otherwise we'll have to rely on LANGUAGE to
</em><br>
<em>&gt;&gt; communicate with computers. This *linguistic bottleneck* is the hardest
</em><br>
<em>&gt;&gt; problem I think.
</em><br>
<em>&gt;
</em><br>
<em>&gt;We'll have to rely on thoughts, and the things they do. Using human language 
</em><br>
<em>&gt;to directly communicate with an AI is more of a final step - the AI has to be 
</em><br>
<em>&gt;quite mature to understand human language directly, I suspect. But there are 
</em><br>
<em>&gt;other ways to communicate, or more generally transfer information to the AI. 
</em><br>
<em>&gt;For instance, posing simple problems for the AI to solve.
</em><br>
<p>Question: How can you have an AI understand you, without letting it be an
<br>
autonomous entity? On the one hand we want a tool, on the other hand
<br>
we want to make sure it will not become the master. And actually the crux
<br>
of the problem comes from the linguistic bottleneck. Imagine if we have
<br>
direct neural interfaces on the back of our necks, then we'll all be busy
<br>
playing with add-on modules now, with magazines advertising all sorts of
<br>
gadgets, like body-building etc.
<br>
<p>YKY
<br>
<p><p>____________________________________________________________
<br>
Get advanced SPAM filtering on Webmail or POP Mail ... Get Lycos Mail!
<br>
<a href="http://login.mail.lycos.com/r/referral?aid=27005">http://login.mail.lycos.com/r/referral?aid=27005</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7061.html">Tommy McCabe: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Previous message:</strong> <a href="7059.html">king-yin yan: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Maybe in reply to:</strong> <a href="7038.html">king-yin yan: "[SL4] brainstorm: a new vision for uploading"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7063.html">Nick Hay: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<li><strong>Reply:</strong> <a href="7063.html">Nick Hay: "Re: [SL4] brainstorm: a new vision for uploading"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7060">[ date ]</a>
<a href="index.html#7060">[ thread ]</a>
<a href="subject.html#7060">[ subject ]</a>
<a href="author.html#7060">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
