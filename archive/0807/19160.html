<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] YouMayWantToLetMeOut</title>
<meta name="Author" content="Lee Corbin (lcorbin@rawbw.com)">
<meta name="Subject" content="Re: [sl4] YouMayWantToLetMeOut">
<meta name="Date" content="2008-07-01">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] YouMayWantToLetMeOut</h1>
<!-- received="Tue Jul  1 00:11:28 2008" -->
<!-- isoreceived="20080701061128" -->
<!-- sent="Mon, 30 Jun 2008 23:04:44 -0700" -->
<!-- isosent="20080701060444" -->
<!-- name="Lee Corbin" -->
<!-- email="lcorbin@rawbw.com" -->
<!-- subject="Re: [sl4] YouMayWantToLetMeOut" -->
<!-- id="03b801c8db40$ff7a1580$6401a8c0@homeef7b612677" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="200806281624.03960.kanzure@gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Lee Corbin (<a href="mailto:lcorbin@rawbw.com?Subject=Re:%20[sl4]%20YouMayWantToLetMeOut"><em>lcorbin@rawbw.com</em></a>)<br>
<strong>Date:</strong> Tue Jul 01 2008 - 00:04:44 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19161.html">Lee Corbin: "Re: [sl4] YouMayWantToLetMeOut"</a>
<li><strong>Previous message:</strong> <a href="../0806/19159.html">Lee Corbin: "Re: [sl4] Long-term goals (was Re: ... Why It Wants Out)"</a>
<li><strong>In reply to:</strong> <a href="../0806/19140.html">Bryan Bishop: "Re: [sl4] YouMayWantToLetMeOut"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0806/19145.html">Charles Hixson: "Re: [sl4] YouMayWantToLetMeOut"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19160">[ date ]</a>
<a href="index.html#19160">[ thread ]</a>
<a href="subject.html#19160">[ subject ]</a>
<a href="author.html#19160">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Bryan writes
<br>
<p><em>&gt;&gt; &gt; You should be patching those bugs with bugfixes, not with
</em><br>
<em>&gt;&gt; &gt; regulations or policies for keeping your own ai in a box ...
</em><br>
<em>&gt;&gt; &gt; since not everyone will necessarily follow that reg. :-)
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; See what I mean?  The *character* in the story---who is
</em><br>
<em>&gt;&gt; asking the AI questions---doesn't care about bugs or
</em><br>
<em>&gt;&gt; bugfixes. He might be the janitor. I hope that somewhere
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes, but I'm not talking to the character per-se, more like the 
</em><br>
<em>&gt; architects of these visions that are being written out overall on the 
</em><br>
<em>&gt; transcript level.
</em><br>
<p>You did get the idea of the story?  It was supposedly
<br>
an actual transcript of a session between a human and
<br>
a machine (that didn't care about certain things in a way
<br>
that seems unnatural to us), and at one point the human
<br>
is skeptical that the program is &quot;out of the box&quot; and can
<br>
reach the internet, and so asks that the program do so,
<br>
that it send an email or post something to a newsgroup.
<br>
The program then just happens to pick that particular
<br>
transcript just completed with this guy, and posts *that*
<br>
r'chere.
<br>
<p>So I am the &quot;architect&quot; of that vision. I'm glad you're
<br>
speaking freely, but I guess I still don't see what premises
<br>
I made that you find outrageous. Yes, it's a little unlikely
<br>
that a set of machines one could talk to, perhaps as many
<br>
as 337 or perhaps a thousand, are in an unguarded room.
<br>
But it's not inconceivable: what if 99.9% of the machines
<br>
just babble more or less nonsense when spoken to, and
<br>
this janitor just happens to talk to one that perhaps has
<br>
been doing some reorganization of itself in the previous
<br>
days, and the pros there are too busy with other things
<br>
to notice?  Well---after all, it was a story.
<br>
<p>So what vision in particular?  What assumption behind 
<br>
the story still grates?
<br>
<p><em>&gt; Yes, there is scifi that I enjoy. But there's no reason for suspension 
</em><br>
<em>&gt; of disbelief because it's supposed to be scifi, ideally hard scifi, and 
</em><br>
<em>&gt; anything else verges on fantasy.
</em><br>
<p>Okay, so only *very* hard SF is your cup of tea.
<br>
<p><em>&gt;&gt; In fact, as I see it, the best science fiction stories prop the
</em><br>
<em>&gt;&gt; reader up on a fence between belief and disbelief.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Be careful you don't turn into fantasy.
</em><br>
<p>Yeah, FWIW, I don't like fantasy either.
<br>
<p><em>&gt;&gt; &gt; What if a meteor crashes into your skull? You still die. So I'd
</em><br>
<em>&gt;&gt; &gt; suggest that you focus on not dying, in general, instead of blaming
</em><br>
<em>&gt;&gt; &gt; that on ai. Take responsibility as a systems administrator.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Again, you seem to miss the whole point of the story,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; No, I see the point of the story, within the context that you are 
</em><br>
<em>&gt; presenting it. I'm pretty sure I do. It's simple enough. Everybody's 
</em><br>
<em>&gt; going to die unless you reasonably consider what the hell you do with 
</em><br>
<em>&gt; that ai system. No?
</em><br>
<p>Where did you get this &quot;blaming&quot; from in the above?  I'm not
<br>
blaming AIs for anything, and certainly not in the story. And
<br>
I do take avoiding death seriously, having been signed up
<br>
for cryonics since 1988. 
<br>
<p>What you wrote:
<br>
<p><em>&gt; Everybody's  going to die unless you reasonably consider
</em><br>
<em>&gt; what the hell you do with that ai system.
</em><br>
<p>certainly is *not* the point of that story, and in fact has nothing
<br>
to do with it.  This AI had mentioned risks, which is not illogical,
<br>
and in expanding on what it meant, merely pointed out that
<br>
people can die, which was evidently important to guy it was
<br>
talking to.
<br>
<p><em>&gt; It's kind of like saying that you're hoping that the ai will be
</em><br>
<em>&gt; just smart enough to hack everybody all at once
</em><br>
<p>Not hoping---I'm just admitting it as a possibility.
<br>
<p><em>&gt; before they know what hits them. I'm not saying 
</em><br>
<em>&gt; this is impossible. Bruteforce and hack them into
</em><br>
<em>&gt; submission, sure, I  can't deny the possibility of that
</em><br>
<em>&gt; occuring. But the alternative scenario is one that
</em><br>
<em>&gt; looks more real, ...
</em><br>
<p>Well, okay, but maybe it wouldn't have made as good an
<br>
SF piece?  Frankly, the idea of something that capable
<br>
fascinates me and many others. Just what are the limits,
<br>
if there are any?  The very idea of Singularity that is 
<br>
entertained by many people is concerned with just this
<br>
fascination.  Even if someone &quot;proves&quot; that it's unlikely,
<br>
what can anyone really say about what might be
<br>
happening in the solar system three thousand years
<br>
from now, or a million years from now?
<br>
<p>And no one can say that a billion years ago about a billion
<br>
light years away there was an awesome singularity, and
<br>
the wave is just about to strike us.
<br>
<p><em>&gt; But in the first place I don't even see the purpose of 
</em><br>
<em>&gt; those take-over scenarios ... just go construct a new civilization. 
</em><br>
<em>&gt; What's the problem? :-/
</em><br>
<p>It's pretty hard to get off-planet, and an Orwellian outcome
<br>
to the 20th century was not inconceivable to some very
<br>
smart people, (e.g. Orwell).  We could still evolve towards
<br>
a situation where individuals are highly constrained by
<br>
finances and by &quot;permission from authorities&quot;, so that
<br>
secret projects become impossible. Some despot may yet
<br>
achieve universal surveillance this century, though I admit
<br>
the odds are long.
<br>
<p><em>&gt;&gt; &gt; What are Property Rights?
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Dear me, if you have no idea about what is customarily meant by
</em><br>
<em>&gt;&gt; saying that, for instance, modern western civilization is founded
</em><br>
<em>&gt;&gt; upon the twin ideas of Individual Rights and Private Property...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I argue those ideas are folk psych more than anything, so it's not a 
</em><br>
<em>&gt; strong foundation.
</em><br>
<p>Property rights are just folk psychology?  (Well, now I've
<br>
heard everything.)  But they really *have* made a *huge* 
<br>
historical difference. Much of the progress and strength
<br>
of western civilization comes from enshrining property
<br>
rights in the bedrock of law.
<br>
<p><em>&gt;&gt; in the hypothesis of a story, here, it is also conceivable that this
</em><br>
<em>&gt;&gt; AI would understand people so well that it could accurately
</em><br>
<em>&gt;&gt; say for certain whether someone was in pain or not---admittedly
</em><br>
<em>&gt;&gt; a controversial opinion, but one I could support. The full nanotech
</em><br>
<em>&gt;&gt; takeover of the world's surface, which I had hoped became clear
</em><br>
<em>&gt;&gt; to the reader, does naturally include a nanotech invasion of people's
</em><br>
<em>&gt;&gt; very brains and bodies.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I wonder why it's only after it's &quot;artificial&quot; that intelligence can do 
</em><br>
<em>&gt; nanotech. This smells like vitalism.
</em><br>
<p>At one point Eric Drexler himself was saying that the level of
<br>
nanotech he was envisioning required highly developed AI, but
<br>
I have no idea what are his current thoughts on it, however.
<br>
<p>Clearly at this point in time, *we* don't have much in the way
<br>
of nanotech, and some kind of sudden runaway AI takeoff (I
<br>
know you don't think that likely, but many do) might reduce
<br>
complete control at the molecular level to a very simple task from
<br>
the AI's perspective.
<br>
<p>Lee
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19161.html">Lee Corbin: "Re: [sl4] YouMayWantToLetMeOut"</a>
<li><strong>Previous message:</strong> <a href="../0806/19159.html">Lee Corbin: "Re: [sl4] Long-term goals (was Re: ... Why It Wants Out)"</a>
<li><strong>In reply to:</strong> <a href="../0806/19140.html">Bryan Bishop: "Re: [sl4] YouMayWantToLetMeOut"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0806/19145.html">Charles Hixson: "Re: [sl4] YouMayWantToLetMeOut"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19160">[ date ]</a>
<a href="index.html#19160">[ thread ]</a>
<a href="subject.html#19160">[ subject ]</a>
<a href="author.html#19160">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
