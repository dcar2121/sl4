<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] Long-term goals (was Re: ... Why It Wants Out)</title>
<meta name="Author" content="Charles Hixson (charleshixsn@earthlink.net)">
<meta name="Subject" content="Re: [sl4] Long-term goals (was Re: ... Why It Wants Out)">
<meta name="Date" content="2008-07-01">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] Long-term goals (was Re: ... Why It Wants Out)</h1>
<!-- received="Wed Jul  2 22:46:52 2008" -->
<!-- isoreceived="20080703044652" -->
<!-- sent="Tue, 1 Jul 2008 13:26:19 -0700" -->
<!-- isosent="20080701202619" -->
<!-- name="Charles Hixson" -->
<!-- email="charleshixsn@earthlink.net" -->
<!-- subject="Re: [sl4] Long-term goals (was Re: ... Why It Wants Out)" -->
<!-- id="200807011326.19903.charleshixsn@earthlink.net" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="03b401c8db3b$61d4d270$6401a8c0@homeef7b612677" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Charles Hixson (<a href="mailto:charleshixsn@earthlink.net?Subject=Re:%20[sl4]%20Long-term%20goals%20(was%20Re:%20...%20Why%20It%20Wants%20Out)"><em>charleshixsn@earthlink.net</em></a>)<br>
<strong>Date:</strong> Tue Jul 01 2008 - 14:26:19 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19165.html">Charles Hixson: "Re: [sl4] Re: More silly but friendly ideas"</a>
<li><strong>Previous message:</strong> <a href="19163.html">Bryan Bishop: "Re: [sl4] Advent of Advanced AI Orthogonal to Present Problems?"</a>
<li><strong>In reply to:</strong> <a href="../0806/19159.html">Lee Corbin: "Re: [sl4] Long-term goals (was Re: ... Why It Wants Out)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19172.html">Lee Corbin: "Re: [sl4] Long-term goals"</a>
<li><strong>Reply:</strong> <a href="19172.html">Lee Corbin: "Re: [sl4] Long-term goals"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19164">[ date ]</a>
<a href="index.html#19164">[ thread ]</a>
<a href="subject.html#19164">[ subject ]</a>
<a href="author.html#19164">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Monday 30 June 2008 10:25:58 pm Lee Corbin wrote:
<br>
<em>&gt; Martin writes
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt; Tim writes
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt; &gt; By the way, if the AI has any long-term goals, then it will want to
</em><br>
<em>&gt; &gt;&gt; &gt; preserve its own integrity in order to preserve those goals.  Although
</em><br>
<em>&gt; &gt;&gt; &gt; &quot;preserve its own integrity&quot; is a good enough example for the issue at
</em><br>
<em>&gt; &gt;&gt; &gt; hand, it's not something you'd really need to put in there explicitly.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Strong sense of identity and self-modifying are somewhat contradictions.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes, but we know of many devices (e.g. us) that live with contradictions.
</em><br>
<em>&gt; It can be put in &quot;society of mind&quot; terms: a single entity may still have
</em><br>
<em>&gt; competing agents (e.g. a human being barely able to make up his mind
</em><br>
<em>&gt; in some particular case, a well-formed highly unified nation or group
</em><br>
<em>&gt; that has internal dissent but manages to keep from displaying its dirty
</em><br>
<em>&gt; laundry in public, or a simple apparatus with a built-in governer that
</em><br>
<em>&gt; shudders and shakes unable to quite settle into a steady course of
</em><br>
<em>&gt; action).
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; (similar to drugs that change the personality)
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes. But people often do use them. And as illogical as it is to me
</em><br>
<em>&gt; (and I'm in a *really* small minority here), many people don't have
</em><br>
<em>&gt; a problem with becoming quite different over time.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; That might even lead to the point that a self-aware AI does not
</em><br>
<em>&gt; &gt; want to modify itself to preserve its current state of being.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes, but it would then have to &quot;care&quot; about its existence---and
</em><br>
<em>&gt; that still seems to be a hot point of contention here about how
</em><br>
<em>&gt; &quot;natural&quot; or &quot;necessary&quot; it is.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Likewise a friendly AI might be aware that the next-level
</em><br>
<em>&gt; &gt; AI it's supposed to develop might not be guaranteed friendly
</em><br>
<em>&gt; &gt; and therefore refuse to develop it at all.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes  :-)   much like we wonder if we should develop AI.
</em><br>
<em>&gt; (Of course, since our six billion people are not in a single
</em><br>
<em>&gt; high-integrity group that can come to a meaningful decision
</em><br>
<em>&gt; about it, AI will be developed by someone, like it or not.)
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; [obligatory new poster and foreign speaker disclaimer]
</em><br>
<em>&gt;
</em><br>
<em>&gt; Welcome!  I don't think I'd be able to tell that you weren't
</em><br>
<em>&gt; a natural English speaker from the above writing. Thanks
</em><br>
<em>&gt; for the effort.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Lee
</em><br>
<p>I don't think that it's &quot;natural&quot; for a created entity to have any particular 
<br>
goal structure.  I don't think that it's necessary that it 
<br>
intrinsically &quot;care about it's existence&quot;, as I agree that this would be a 
<br>
derived goal from almost any other goal.  OTOH, I do think that it's probably 
<br>
desireable.  Other primary goals should definitely be stronger, but I do 
<br>
believe that in-and-of-itself a desire to continue it's own existence is a 
<br>
reasonable intrinsic (primary) goal.
<br>
<p>My terminology:  Primary goals are the ones that are originally written into 
<br>
the source code.  Secondary goals are based in data files that are read.  
<br>
Derived goals all refer back to either primary or secondary goals for their 
<br>
significance.
<br>
<p>OTOH, almost all conversation about goals presumes extensive knowledge of the 
<br>
external world.  I don't see how this could be possible for either the 
<br>
primary or the secondary goals.  E.g., how are you going to define &quot;human&quot; to 
<br>
an entity that has never interacted with one, either directly or indirectly, 
<br>
that that probably only has a vague idea of object persistence?  The only 
<br>
approach that occurs to me is via imprinting, and that is notorious for its 
<br>
drawbacks.  This could probably be dealt with via a large number 
<br>
of &quot;micro-imprints&quot;, but then one encounters the problem of sensory 
<br>
non-constancy.  I'm sure this can be dealt with...but just how I'm not sure.  
<br>
Possibly it would be desireable to have AIs practice Ahimsa, but I'm not 
<br>
really sure that's logically possible.  Still, a strong desire to cause the 
<br>
minimal amount of damage coupled with a weaker desire to help entities might 
<br>
get us through this.  True, this means that somehow the concepts of &quot;damage&quot; 
<br>
and &quot;helpful&quot; need to be communicated to an entity that doesn't have the 
<br>
concept of object-permanence...and I don't see just how to do THAT, but it 
<br>
looks like a much simpler problem than defining &quot;human&quot; to such an entity.  
<br>
(But you'd need to balance things carefully, or you'd end up with an AI that 
<br>
did nothing but &quot;meditate&quot;, which wouldn't be particularly useful.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19165.html">Charles Hixson: "Re: [sl4] Re: More silly but friendly ideas"</a>
<li><strong>Previous message:</strong> <a href="19163.html">Bryan Bishop: "Re: [sl4] Advent of Advanced AI Orthogonal to Present Problems?"</a>
<li><strong>In reply to:</strong> <a href="../0806/19159.html">Lee Corbin: "Re: [sl4] Long-term goals (was Re: ... Why It Wants Out)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19172.html">Lee Corbin: "Re: [sl4] Long-term goals"</a>
<li><strong>Reply:</strong> <a href="19172.html">Lee Corbin: "Re: [sl4] Long-term goals"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19164">[ date ]</a>
<a href="index.html#19164">[ thread ]</a>
<a href="subject.html#19164">[ subject ]</a>
<a href="author.html#19164">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
