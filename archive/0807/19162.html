<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: [sl4] Advent of Advanced AI Orthogonal to Present Problems?</title>
<meta name="Author" content="Lee Corbin (lcorbin@rawbw.com)">
<meta name="Subject" content="[sl4] Advent of Advanced AI Orthogonal to Present Problems?">
<meta name="Date" content="2008-07-01">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>[sl4] Advent of Advanced AI Orthogonal to Present Problems?</h1>
<!-- received="Wed Jul  2 21:30:46 2008" -->
<!-- isoreceived="20080703033046" -->
<!-- sent="Tue, 1 Jul 2008 16:32:51 -0700" -->
<!-- isosent="20080701233251" -->
<!-- name="Lee Corbin" -->
<!-- email="lcorbin@rawbw.com" -->
<!-- subject="[sl4] Advent of Advanced AI Orthogonal to Present Problems?" -->
<!-- id="043401c8dbd2$dd3592f0$6401a8c0@homeef7b612677" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Lee Corbin (<a href="mailto:lcorbin@rawbw.com?Subject=Re:%20[sl4]%20Advent%20of%20Advanced%20AI%20Orthogonal%20to%20Present%20Problems?"><em>lcorbin@rawbw.com</em></a>)<br>
<strong>Date:</strong> Tue Jul 01 2008 - 17:32:51 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19163.html">Bryan Bishop: "Re: [sl4] Advent of Advanced AI Orthogonal to Present Problems?"</a>
<li><strong>Previous message:</strong> <a href="19161.html">Lee Corbin: "Re: [sl4] YouMayWantToLetMeOut"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19163.html">Bryan Bishop: "Re: [sl4] Advent of Advanced AI Orthogonal to Present Problems?"</a>
<li><strong>Reply:</strong> <a href="19163.html">Bryan Bishop: "Re: [sl4] Advent of Advanced AI Orthogonal to Present Problems?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19162">[ date ]</a>
<a href="index.html#19162">[ thread ]</a>
<a href="subject.html#19162">[ subject ]</a>
<a href="author.html#19162">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Bryan wrote
<br>
<p><em>&gt; [Lee wrote]
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; suppose that there could exist an entity so many millions of times
</em><br>
<em>&gt;&gt; smarter than humans that it could dominate the world exactly as this
</em><br>
<em>&gt;&gt; story proposes. I will in fact be so bold as to suggest that when he
</em><br>
<em>&gt;&gt; started this list, Eliezer was making exactly this conjecture, and
</em><br>
<em>&gt;&gt; that the whole issue of &quot;whether or not to let an AI out&quot;, or, better
</em><br>
<em>&gt;&gt; &quot;whether or not an AI could be contained&quot; still motivates a huge
</em><br>
<em>&gt;&gt; amount of discourse on this list to this very day.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I suspect this particular portion of our discussion should be forked off 
</em><br>
<em>&gt; into a new thread,
</em><br>
<p>Here it is.
<br>
<p><em>&gt; but arguably since I've presented arguments and evidence for
</em><br>
<em>&gt; alternative solutions to the problems of death, problems 
</em><br>
<em>&gt; of the survival of humanity, that do not involve the requirement of 
</em><br>
<em>&gt; FAI,
</em><br>
<p>Honestly, I didn't think that that was the point of AI or FAI at all.
<br>
<p>In fact, I think that our chances of surviving the development of
<br>
any kind of superhuman AI are less than 50 percent.  But I'm
<br>
not really worried, because if we do survive, then we'll probably
<br>
survive in incredibly good style, minor problems like death,
<br>
poverty, boredom, true suffering, and so on being easily handled
<br>
for the likes of us primitive types.  (And that's whether or not some
<br>
versions of one fork off and become superhuman themselves.)
<br>
<p>So for me, AI or FAI is hardly a solution:  it's merely a most
<br>
likely... occurrence.
<br>
<p><em>&gt; then I then argue that the discussion can return to actual 
</em><br>
<em>&gt; implementation and elucidation of ai or other SL4-ish topics
</em><br>
<em>&gt; instead of  the spooky ai domination scenarios.
</em><br>
<p>How do these ideas that *you* have for quickly dispatching death
<br>
and human survival have any effect upon the problem of how we
<br>
should try to cope with an inevitable or nearly inevitable rise of
<br>
superhuman intelligence?
<br>
<p>Lee
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19163.html">Bryan Bishop: "Re: [sl4] Advent of Advanced AI Orthogonal to Present Problems?"</a>
<li><strong>Previous message:</strong> <a href="19161.html">Lee Corbin: "Re: [sl4] YouMayWantToLetMeOut"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19163.html">Bryan Bishop: "Re: [sl4] Advent of Advanced AI Orthogonal to Present Problems?"</a>
<li><strong>Reply:</strong> <a href="19163.html">Bryan Bishop: "Re: [sl4] Advent of Advanced AI Orthogonal to Present Problems?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19162">[ date ]</a>
<a href="index.html#19162">[ thread ]</a>
<a href="subject.html#19162">[ subject ]</a>
<a href="author.html#19162">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
