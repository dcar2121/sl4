<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Ethical experimentation on AIs</title>
<meta name="Author" content="PETER JENKINS (peterjenkins@rogers.com)">
<meta name="Subject" content="Re: Ethical experimentation on AIs">
<meta name="Date" content="2007-10-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Ethical experimentation on AIs</h1>
<!-- received="Sun Oct 21 20:12:50 2007" -->
<!-- isoreceived="20071022021250" -->
<!-- sent="Sun, 21 Oct 2007 22:11:16 -0400" -->
<!-- isosent="20071022021116" -->
<!-- name="PETER JENKINS" -->
<!-- email="peterjenkins@rogers.com" -->
<!-- subject="Re: Ethical experimentation on AIs" -->
<!-- id="001b01c81450$d48711a0$6401a8c0@PETERVAIO" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="79ecaa350710211013u4d232adfte3c61c6e7b41d39d@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> PETER JENKINS (<a href="mailto:peterjenkins@rogers.com?Subject=Re:%20Ethical%20experimentation%20on%20AIs"><em>peterjenkins@rogers.com</em></a>)<br>
<strong>Date:</strong> Sun Oct 21 2007 - 20:11:16 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16909.html">Stathis Papaioannou: "Re: Ethical experimentation on AIs"</a>
<li><strong>Previous message:</strong> <a href="16907.html">Norman Noman: "Re: The Meaning That Immortality Gives to Life"</a>
<li><strong>In reply to:</strong> <a href="16902.html">Rolf Nelson: "Re: Ethical experimentation on AIs"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16910.html">Lee Corbin: "Personal Decision Making In Simulations"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16908">[ date ]</a>
<a href="index.html#16908">[ thread ]</a>
<a href="subject.html#16908">[ subject ]</a>
<a href="author.html#16908">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Rolf Asks: 
<br>
<p><em>&gt; Did you mean to only say that self-motivated long range planning would be futile? I would argue that altruistic long-range planning is not necessarily affected by the analysis. Example:
</em><br>
<p><em>&gt;(Making up figures here, the actual figures don't matter much to the analysis) Suppose that every non-simulated civilization produces, on average, self-aware simulations equivalent to 10^100 civilizations, .001% of which are flawless ancestor simulations. A basic vanilla Simulation Argument would say that the odds are 10^95 to 1 that you live in a simulation. However, any decisions you make will affect 10^100 simulated civilizations. So in basic vanilla utilitarianism, your actions are still dominated by a factor of 10,000 to 1 by considerations of &quot;what should I do if we do *not* live in an ancestor simulation?&quot; 
</em><br>
<p>You are using an expected utility analysis which says that if there is a 1 in 10^95 chance that you will benefit  10^100 simulated civilizations with long-range planning then you should undertake such planning, since the remote chance of there being any effect at all would be outweighed by the large extent of the effect if it occurs. Although this is an intriguing form of analysis in theory, I think that it is unlikely that the flawed sims (where presumably it is obvious to the AI inhabitants that they are in a sim) would outnumber the flawless sims by such a huge amount. Alternatively if these flawed sims were created in such great numbers, they would be quickly terminated as inflicting unnecessary suffering on the inhabitants and moreover not providing any valuable experimental data, so they would not factor much at all into the equation. I am currently working on a paper on the issue of constraining rogue AI through the use of the simulation argument, so thanks for raising this point though.
<br>
&nbsp;&nbsp;----- Original Message ----- 
<br>
&nbsp;&nbsp;From: Rolf Nelson 
<br>
&nbsp;&nbsp;To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Ethical%20experimentation%20on%20AIs">sl4@sl4.org</a> 
<br>
&nbsp;&nbsp;Sent: Sunday, October 21, 2007 1:13 PM
<br>
&nbsp;&nbsp;Subject: Re: Ethical experimentation on AIs
<br>
<p><p>&nbsp;&nbsp;Thanks for the link to your paper, Peter. Your paper states that we are likely to be living in a simulation, and therefore...
<br>
<p><em>  &gt; Long range planning beyond... [2050] would therefore be futile. 
</em><br>
<p>&nbsp;&nbsp;Did you mean to only say that self-motivated long range planning would be futile? I would argue that altruistic long-range planning is not necessarily affected by the analysis. Example:
<br>
<p>&nbsp;&nbsp;(Making up figures here, the actual figures don't matter much to the analysis) Suppose that every non-simulated civilization produces, on average, self-aware simulations equivalent to 10^100 civilizations, .001% of which are flawless ancestor simulations. A basic vanilla Simulation Argument would say that the odds are 10^95 to 1 that you live in a simulation. However, any decisions you make will affect 10^100 simulated civilizations. So in basic vanilla utilitarianism, your actions are still dominated by a factor of 10,000 to 1 by considerations of &quot;what should I do if we do *not* live in an ancestor simulation?&quot; 
<br>
<p>&nbsp;&nbsp;Of course there are arguments against basic vanilla utilitarianism, when large numbers are concerned. But, I would argue that for every valid argument that &quot;basic utilitarianism shouldn't apply to large numbers because it produces silly results&quot;, there's an equal argument that &quot;the basic Simulation Argument shouldn't apply to large numbers because it produces silly results.&quot; 
<br>
<p>&nbsp;&nbsp;(Caveat: I don't believe in the Simulation Argument. For example, if it comes down to a choice between something like Wei Dai's UDASSA, or believing I live in a simulation, I consider UDASSA more likely. That said, I consider the Simulation Argument a deep and noteworthy argument, and UDASSA is the only model I know of that might allow an escape that I personally find &quot;satisfactory&quot;. I'm still holding out, though, that post-Singularity when we have centuries to leisurely think about it, we'll be able to come up with a better model.) 
<br>
<p><p>&nbsp;&nbsp;On 10/20/07, Peter S Jenkins &lt;<a href="mailto:peterjenkins@rogers.com?Subject=Re:%20Ethical%20experimentation%20on%20AIs">peterjenkins@rogers.com</a>&gt; wrote:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=929327">http://papers.ssrn.com/sol3/papers.cfm?abstract_id=929327</a>
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;Here is a link to my paper on this issue that was mentioned in the NY Times last August -- comments welcome 
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16909.html">Stathis Papaioannou: "Re: Ethical experimentation on AIs"</a>
<li><strong>Previous message:</strong> <a href="16907.html">Norman Noman: "Re: The Meaning That Immortality Gives to Life"</a>
<li><strong>In reply to:</strong> <a href="16902.html">Rolf Nelson: "Re: Ethical experimentation on AIs"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16910.html">Lee Corbin: "Personal Decision Making In Simulations"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16908">[ date ]</a>
<a href="index.html#16908">[ thread ]</a>
<a href="subject.html#16908">[ subject ]</a>
<a href="author.html#16908">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:58 MDT
</em></small></p>
</body>
</html>
