<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI, just do what I tell you to</title>
<meta name="Author" content="Stathis Papaioannou (stathisp@gmail.com)">
<meta name="Subject" content="Re: AI, just do what I tell you to">
<meta name="Date" content="2007-10-30">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI, just do what I tell you to</h1>
<!-- received="Tue Oct 30 07:58:26 2007" -->
<!-- isoreceived="20071030135826" -->
<!-- sent="Wed, 31 Oct 2007 00:56:54 +1100" -->
<!-- isosent="20071030135654" -->
<!-- name="Stathis Papaioannou" -->
<!-- email="stathisp@gmail.com" -->
<!-- subject="Re: AI, just do what I tell you to" -->
<!-- id="f21c22e30710300656h4cca342m2c2d69fa5f21c2e9@mail.gmail.com" -->
<!-- charset="UTF-8" -->
<!-- inreplyto="c528628a0710300503h66ae45b3yb215750be6f1fab5@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Stathis Papaioannou (<a href="mailto:stathisp@gmail.com?Subject=Re:%20AI,%20just%20do%20what%20I%20tell%20you%20to"><em>stathisp@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Oct 30 2007 - 07:56:54 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16967.html">Peter de Blanc: "Re: AI, just do what I tell you to"</a>
<li><strong>Previous message:</strong> <a href="16965.html">Nick Hay: "AI, just do what I tell you to"</a>
<li><strong>In reply to:</strong> <a href="16965.html">Nick Hay: "AI, just do what I tell you to"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16967.html">Peter de Blanc: "Re: AI, just do what I tell you to"</a>
<li><strong>Reply:</strong> <a href="16967.html">Peter de Blanc: "Re: AI, just do what I tell you to"</a>
<li><strong>Reply:</strong> <a href="16968.html">Nick Hay: "Re: AI, just do what I tell you to"</a>
<li><strong>Reply:</strong> <a href="16969.html">Matt Mahoney: "Re: AI, just do what I tell you to"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16966">[ date ]</a>
<a href="index.html#16966">[ thread ]</a>
<a href="subject.html#16966">[ subject ]</a>
<a href="author.html#16966">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 30/10/2007, Nick Hay &lt;<a href="mailto:nickjhay@gmail.com?Subject=Re:%20AI,%20just%20do%20what%20I%20tell%20you%20to">nickjhay@gmail.com</a>&gt; wrote:
<br>
<p><em>&gt; &gt; Are hired human experts intelligent? The idea is that they provide
</em><br>
<em>&gt; &gt; advice and other services without letting any competing motives of
</em><br>
<em>&gt; &gt; their own interfere.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If you've built this AI, why did you build in competing motives?
</em><br>
<p>The AI should always remain in character as the ideal expert. If you
<br>
want it to fix your plumbing it should not concern itself with
<br>
charging you as much as possible, or finishing in time to watch a
<br>
football game. It should listen to what you want it to do, advise you
<br>
of what it thinks it ought to do (given that it knows more about
<br>
plumbing than you do), then go ahead and follow your instructions. If
<br>
you tell it to do something stupid...
<br>
<p><em>&gt; I think future predictable horror should be at least allowed as a
</em><br>
<em>&gt; veto.  Suppose someone really really wants to destroy their brain,
</em><br>
<em>&gt; just to see what happens.  They think they're implemented by an
</em><br>
<em>&gt; immortal soul, so this seems harmless enough.  If the AI didn't grant
</em><br>
<em>&gt; this wish they'd be indignant:  who are you to refuse my order?
</em><br>
<em>&gt; However, if they found out souls don't exists they would predictably
</em><br>
<em>&gt; be horrified, and wish that the AI had ignored their previous order.
</em><br>
<em>&gt;
</em><br>
<em>&gt; In this scenario, it is not helpful for the AI to shut up and do what they say.
</em><br>
<p>But we have the same problem with any tool, like a kitchen knife. It
<br>
might be a good idea to make smart knives that can never be used in
<br>
suicide or murder attempts, if this were possible, and ban
<br>
old-fashioned knives. But with an AI it would be rather difficult to
<br>
build in rules like this. Fixing the plumbing might result in
<br>
increased water use, which a decade down the track may somehow cause
<br>
one excess human death. It would render the AI almost useless if it
<br>
reviewed every command on the basis of such considerations.
<br>
<p><em>&gt; A more extreme example, the wisher commands the AI to make the sun go
</em><br>
<em>&gt; nova, because they've always wondered what it looks like.  For some
</em><br>
<em>&gt; reason they do not understand that this would destroy humanity (which
</em><br>
<em>&gt; they do not want), even if this is explained carefully to them.  In
</em><br>
<em>&gt; some sense it will be &quot;their fault&quot; that the human species dies, and
</em><br>
<em>&gt; yet making the sun go nova doesn't seem like a good idea.
</em><br>
<p>We've had to deal with problems like this throughout history anyway.
<br>
Consider nuclear weapons, and the people who might have urged their
<br>
use in the belief that a pre-emptive strike against the other side was
<br>
a good idea. What we rely on is that humanity collectively will do the
<br>
right thing. This will apply to AI's as well, when there are many of
<br>
them all monitoring and policing each other, as they have been
<br>
programmed to do. If one rogue human with an AI can easily do terrible
<br>
things despite this, then we are doomed, and no attempt to ensure that
<br>
each AI comes out of the factory friendly will work.
<br>
<p><p><p><pre>
-- 
Stathis Papaioannou
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16967.html">Peter de Blanc: "Re: AI, just do what I tell you to"</a>
<li><strong>Previous message:</strong> <a href="16965.html">Nick Hay: "AI, just do what I tell you to"</a>
<li><strong>In reply to:</strong> <a href="16965.html">Nick Hay: "AI, just do what I tell you to"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16967.html">Peter de Blanc: "Re: AI, just do what I tell you to"</a>
<li><strong>Reply:</strong> <a href="16967.html">Peter de Blanc: "Re: AI, just do what I tell you to"</a>
<li><strong>Reply:</strong> <a href="16968.html">Nick Hay: "Re: AI, just do what I tell you to"</a>
<li><strong>Reply:</strong> <a href="16969.html">Matt Mahoney: "Re: AI, just do what I tell you to"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16966">[ date ]</a>
<a href="index.html#16966">[ thread ]</a>
<a href="subject.html#16966">[ subject ]</a>
<a href="author.html#16966">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:58 MDT
</em></small></p>
</body>
</html>
