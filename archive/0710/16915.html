<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Is there evidence for, &quot;Humans are going to create an AI,&quot; to be a probable hypothesis?</title>
<meta name="Author" content="Thomas McCabe (pphysics141@gmail.com)">
<meta name="Subject" content="Re: Is there evidence for, &quot;Humans are going to create an AI,&quot; to be a probable hypothesis?">
<meta name="Date" content="2007-10-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Is there evidence for, &quot;Humans are going to create an AI,&quot; to be a probable hypothesis?</h1>
<!-- received="Tue Oct 23 19:07:35 2007" -->
<!-- isoreceived="20071024010735" -->
<!-- sent="Tue, 23 Oct 2007 21:05:53 -0400" -->
<!-- isosent="20071024010553" -->
<!-- name="Thomas McCabe" -->
<!-- email="pphysics141@gmail.com" -->
<!-- subject="Re: Is there evidence for, &quot;Humans are going to create an AI,&quot; to be a probable hypothesis?" -->
<!-- id="b7a9e8680710231805o40186bc2i8bba78308ebacae1@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="ab5bcc90710231658y238c894cva44c64d72f8983f9@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Thomas McCabe (<a href="mailto:pphysics141@gmail.com?Subject=Re:%20Is%20there%20evidence%20for,%20&quot;Humans%20are%20going%20to%20create%20an%20AI,&quot;%20to%20be%20a%20probable%20hypothesis?"><em>pphysics141@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Oct 23 2007 - 19:05:53 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16916.html">Eliezer S. Yudkowsky: "Re: Is there evidence for, &quot;Humans are going to create an AI,&quot; to be a probable hypothesis?"</a>
<li><strong>Previous message:</strong> <a href="16914.html">William Pearson: "Is there evidence for, &quot;Humans are going to create an AI,&quot; to be a probable hypothesis?"</a>
<li><strong>In reply to:</strong> <a href="16914.html">William Pearson: "Is there evidence for, &quot;Humans are going to create an AI,&quot; to be a probable hypothesis?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16916.html">Eliezer S. Yudkowsky: "Re: Is there evidence for, &quot;Humans are going to create an AI,&quot; to be a probable hypothesis?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16915">[ date ]</a>
<a href="index.html#16915">[ thread ]</a>
<a href="subject.html#16915">[ subject ]</a>
<a href="author.html#16915">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 10/23/07, William Pearson &lt;<a href="mailto:wil.pearson@gmail.com?Subject=Re:%20Is%20there%20evidence%20for,%20&quot;Humans%20are%20going%20to%20create%20an%20AI,&quot;%20to%20be%20a%20probable%20hypothesis?">wil.pearson@gmail.com</a>&gt; wrote:
<br>
<em>&gt; I can't currently get around the problem that we haven't had any
</em><br>
<em>&gt; instances of this happening. In a way we have had negative instances
</em><br>
<em>&gt; of some hypotheses involving AI, e.g. each planck time we don't create
</em><br>
<em>&gt; a realistic intelligence could be counted as evidence that we won't
</em><br>
<em>&gt; create one in the next planck second (and this hypothesis is very
</em><br>
<em>&gt; reliable, to date). And by induction it is not probable to create one
</em><br>
<em>&gt; in any planck time. And until we do create one, we shouldn't have a
</em><br>
<em>&gt; reason for increasing the probability of one being created, and we
</em><br>
<em>&gt; should be forever decreasing it.
</em><br>
<p>If you look at the 21st century as yet another random hundred-year
<br>
interval, yes, the prior probability is very low due to Laplace's Law
<br>
of Succession. However, the prior probability does not equal the
<br>
posterior probability; there is a great deal of strong evidence for
<br>
the &quot;AGI during 21st century&quot; hypothesis, such as the presence of a
<br>
pre-existing general intelligence (humans), and the tools to create
<br>
AGI (computers).
<br>
<p><em>&gt; Now you could argue that the probability of creating an AI in any
</em><br>
<em>&gt; given time period is independent of one another.
</em><br>
<p>Each additional time period without AGI does count as evidence against
<br>
AGI's feasibility. But there is a huge amount of other evidence to
<br>
consider, a lot of which has already been documented and posted
<br>
online. See <a href="http://www.intelligence.org/AIRisk.pdf">http://www.intelligence.org/AIRisk.pdf</a> for some basics.
<br>
<p><em>&gt; We have no evidence
</em><br>
<em>&gt; for this meta-hypothesis either, due to not having created an AI for
</em><br>
<em>&gt; us to analyse the distributions of how they are created. Although we
</em><br>
<em>&gt; have a fair amount of evidence that is consistent with the hypothesis
</em><br>
<em>&gt; that the probability of creating an AI not independent of time, and
</em><br>
<em>&gt; just very low.
</em><br>
<p>Suppose I have a black box, inside which is a colored block. The block
<br>
is either red or blue, but the box only has a hole wide enough for
<br>
individual photons to escape, one at a time. I want to show the
<br>
physics community that the block is red, so I turn on a light bulb,
<br>
and count the colors of the photons coming out. At the end of the day,
<br>
ten million are blue, and one million are red. I then publish data on
<br>
the red photons in a journal, and surely everyone must now agree that
<br>
the block is red; after all, we have a million published pieces of
<br>
evidence that it is so.
<br>
<p><em>&gt; Possibly you could look at the number of people that have put there
</em><br>
<em>&gt; mind to creating something new, and see how many actually achieved
</em><br>
<em>&gt; there goal.
</em><br>
<p>It's &quot;their&quot;, please use proper grammar. And please try to understand
<br>
probability theory better- if ten million people dream of creating
<br>
AGI, and one succeeds, this does not mean that the probability of the
<br>
human race creating AGI is one in ten million!
<br>
<p><em>&gt; How to get a good delineation of what to include as
</em><br>
<em>&gt; evidence would be problematic in this case (e.g. should the alchemists
</em><br>
<em>&gt; and there philosopher's stone be counted), and it is likely that we
</em><br>
<em>&gt; will have far more evidence of people being successful compared to the
</em><br>
<em>&gt; number of unknown failures.
</em><br>
<p>The unknown failures are unknown for a reason- they have little effect
<br>
on history. Thomas Edison tried two thousand different filaments for
<br>
the incandescent light bulb. What were they? I have no idea, because
<br>
in the grand historical calculus the total failures don't count.
<br>
Partial failures (non-Friendly AGIs) may have huge negative impacts,
<br>
though.
<br>
<p><em>&gt; Or the kurzweil way, which I will paraphrase as: Defining AI as part
</em><br>
<em>&gt; of the type of computer system with a high resource usage and showing
</em><br>
<em>&gt; that the hypothesis that we have been increasing the resources
</em><br>
<em>&gt; available of computer systems by a certain rate over time has a lot of
</em><br>
<em>&gt; evidence.
</em><br>
<p>This is not Ray Kurzweil's hypothesis.
<br>
<p><em>&gt; Now I don't like this one much, because while we have
</em><br>
<em>&gt; evidence we will increase resources available to computers, there is
</em><br>
<em>&gt; no evidence we will create the right computer system for intelligence
</em><br>
<em>&gt; given sufficient resources.
</em><br>
<p>We *know* that neurons and silicon are equivalent (in computability
<br>
theory terms). For a proof-of-concept AGI, you could simply do out the
<br>
QED calculations for the wave function of all 10^25-odd atoms in a
<br>
human brain- it would take longer than the age of the universe with
<br>
current hardware, but it would be fully human-equivalent.
<br>
<p><em>&gt; Is there any principled way of deciding which way of calculating the
</em><br>
<em>&gt; probability of humans creating AI is the better to base decisions off?
</em><br>
<em>&gt; PTL?
</em><br>
<p>Yes, there is- Bayesian probability theory, see
<br>
<a href="http://www.yudkowsky.net/bayes/bayes.html">http://www.yudkowsky.net/bayes/bayes.html</a>.
<br>
<p><em>&gt; Now my knowledge of bayesian decision theory is rusty, so it may well
</em><br>
<em>&gt; be that I am missing something or my analyses are faulty. Any pointers
</em><br>
<em>&gt; to things already written? And note I am looking for a body of data I
</em><br>
<em>&gt; could feed to a Bayesian classifier, so no general human type
</em><br>
<em>&gt; arguments for AI.
</em><br>
<p>You must always feed all the data you have to a Bayesian classifier,
<br>
or you get nonsense like the red block example above. If you know some
<br>
of the data is faulty, the knowledge &quot;Data XYZ is faulty&quot; is itself
<br>
data, and should be fed in along with the original data. If you're
<br>
trying to decide what &quot;set&quot; of data to feed into the classifier,
<br>
you're going about it wrong. See
<br>
<a href="http://omega.albany.edu:8008/JaynesBook.html">http://omega.albany.edu:8008/JaynesBook.html</a>.
<br>
<p><em>&gt;  Will Pearson
</em><br>
<em>&gt;
</em><br>
<p>&nbsp;- Tom
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16916.html">Eliezer S. Yudkowsky: "Re: Is there evidence for, &quot;Humans are going to create an AI,&quot; to be a probable hypothesis?"</a>
<li><strong>Previous message:</strong> <a href="16914.html">William Pearson: "Is there evidence for, &quot;Humans are going to create an AI,&quot; to be a probable hypothesis?"</a>
<li><strong>In reply to:</strong> <a href="16914.html">William Pearson: "Is there evidence for, &quot;Humans are going to create an AI,&quot; to be a probable hypothesis?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16916.html">Eliezer S. Yudkowsky: "Re: Is there evidence for, &quot;Humans are going to create an AI,&quot; to be a probable hypothesis?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16915">[ date ]</a>
<a href="index.html#16915">[ thread ]</a>
<a href="subject.html#16915">[ subject ]</a>
<a href="author.html#16915">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:58 MDT
</em></small></p>
</body>
</html>
