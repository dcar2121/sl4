<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Definition of strong recursive self-improvement</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Definition of strong recursive self-improvement">
<meta name="Date" content="2005-01-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Definition of strong recursive self-improvement</h1>
<!-- received="Sun Jan  2 10:37:03 2005" -->
<!-- isoreceived="20050102173703" -->
<!-- sent="Sun, 02 Jan 2005 11:38:54 -0600" -->
<!-- isosent="20050102173854" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Definition of strong recursive self-improvement" -->
<!-- id="41D831AE.3070507@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="8d71341e050101230936e06ac3@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Definition%20of%20strong%20recursive%20self-improvement"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jan 02 2005 - 10:38:54 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10514.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Previous message:</strong> <a href="10512.html">Eliezer S. Yudkowsky: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>In reply to:</strong> <a href="10508.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10515.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10515.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10513">[ date ]</a>
<a href="index.html#10513">[ thread ]</a>
<a href="subject.html#10513">[ subject ]</a>
<a href="author.html#10513">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Russell Wallace wrote:
<br>
<em>&gt; On Sat, 01 Jan 2005 22:48:31 -0600, Eliezer S. Yudkowsky
</em><br>
<em>&gt; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Definition%20of%20strong%20recursive%20self-improvement">sentience@pobox.com</a>&gt; wrote:
</em><br>
<em>&gt;&gt;I intend to comprehend how it is theoretically possible that humans
</em><br>
<em>&gt;&gt;should write code, and then come up with a deterministic or
</em><br>
<em>&gt;&gt;calibrated-very-high-probability way of doing &quot;the same thing&quot; or
</em><br>
<em>&gt;&gt;better.  It is not logically necessary that this be possible, but I
</em><br>
<em>&gt;&gt;expect it to be possible.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I can explain that one for you. Humans write code the same way we do
</em><br>
<em>&gt; other things: a combination of semi-formal reasoning and empirical
</em><br>
<em>&gt; testing.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; By &quot;semi-formal reasoning&quot;, I mean where we think things along the
</em><br>
<em>&gt; lines of &quot;A -&gt; B&quot;, ignoring the fact that in strict logic it would
</em><br>
<em>&gt; read &quot;A &amp;!C &amp;!D &amp;!E ... -&gt; B&quot;, where C, D, E and an indefinitely long
</em><br>
<em>&gt; list of other things we haven't thought of, could intervene to stop A
</em><br>
<em>&gt; from working. We ignore C, D, E etc because if we had to stop to take
</em><br>
<em>&gt; them all into account, we'd starve to death in our beds because we
</em><br>
<em>&gt; couldn't prove it was a good idea to get up in the morning. In
</em><br>
<em>&gt; practice, we're good enough at taking into account only those things
</em><br>
<em>&gt; that are important enough to make a practical difference, that we can
</em><br>
<em>&gt; survive in the real world.
</em><br>
<p>Thank you for your helpful explanation; go forth and implement it in an 
<br>
AI code-writing system and put all the programmers out of business.
<br>
<p>I do not intend to achieve (it is theoretically impossible to achieve, I 
<br>
think) a 1.00000... expected probability of success.  However, such 
<br>
outside causes as you name are not *independent* causes of failure among 
<br>
all the elements of a complex system.  A CPU works because all the 
<br>
individual transistors have extremely low failure rates, much lower than 
<br>
the real-world probability of the CPU being tossed into a bowl of ice 
<br>
cream.  The probability of a transistor working might be only 99%, when 
<br>
ice cream is taken into account, yet the probability of the entire CPU 
<br>
working is scarcely less than 99% (despite the millions of transistors) 
<br>
because it's not an independent probability for each transistor.
<br>
<p>I do not say:  If I could solve the code-writing problem for 
<br>
deterministic transistors, I would be done.  For there is still more 
<br>
safety that may be wrung from such a system; with probabilistic 
<br>
reasoning it may be proofed against cosmic rays.  And the problem of 
<br>
guarding against errors of human interface is more complex still, to 
<br>
which I devote much thought trying to make it a solvable technical 
<br>
problem instead of a sterile philosphical one.
<br>
<p>But if I knew how to build an FAI that worked so long as no one tossed 
<br>
its CPU into a bowl of ice cream, I would count myself as having made 
<br>
major progress.
<br>
<p>Meanwhile, saying that humans use &quot;semi-formal reasoning&quot; to write code 
<br>
is not, I'm afraid, a Technical Explanation.  Imagine someone who knew 
<br>
naught of Bayes, pointing to probabilistic reasoning and saying it was 
<br>
all &quot;guessing&quot; and therefore would inevitably fail at one point or 
<br>
another.  In that vague and verbal model you could not express the 
<br>
notion of a more reliable, better-discriminating probabilistic guesser, 
<br>
powered by Bayesian principles and a better implementation, that could 
<br>
achieve a calibrated probability of 0.0001% for the failure of an entire 
<br>
system over, say, ten millennia.  (For I do now regard FAI as an interim 
<br>
measure, to be replaced by some other System when humans have grown up a 
<br>
little.)
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10514.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Previous message:</strong> <a href="10512.html">Eliezer S. Yudkowsky: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>In reply to:</strong> <a href="10508.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10515.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10515.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10513">[ date ]</a>
<a href="index.html#10513">[ thread ]</a>
<a href="subject.html#10513">[ subject ]</a>
<a href="author.html#10513">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
