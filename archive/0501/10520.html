<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Definition of strong recursive self-improvement</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Definition of strong recursive self-improvement">
<meta name="Date" content="2005-01-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Definition of strong recursive self-improvement</h1>
<!-- received="Sun Jan  2 23:24:54 2005" -->
<!-- isoreceived="20050103062454" -->
<!-- sent="Mon, 03 Jan 2005 00:26:42 -0600" -->
<!-- isosent="20050103062642" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Definition of strong recursive self-improvement" -->
<!-- id="41D8E5A2.5020309@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="8d71341e050102152377ad7e9b@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Definition%20of%20strong%20recursive%20self-improvement"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jan 02 2005 - 23:26:42 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10521.html">Ben Goertzel: "Android science conference"</a>
<li><strong>Previous message:</strong> <a href="10519.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>In reply to:</strong> <a href="10519.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10523.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10523.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10520">[ date ]</a>
<a href="index.html#10520">[ thread ]</a>
<a href="subject.html#10520">[ subject ]</a>
<a href="author.html#10520">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Russell Wallace wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Well, I originally had the impression you believed it would be
</em><br>
<em>&gt; possible to create a seed AI which:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; - Would provably undergo hard takeoff (running on a supercomputer in a basement)
</em><br>
<em>&gt; - Or else, would provably have e.g. a 99% probability of doing so
</em><br>
<p>Good heavens, no!  I was needing to create a system that would provably 
<br>
remain &lt;Friendly&gt; (according to some well-specified target) *if* it 
<br>
underwent hard takeoff.  Proving in advance that a system undergoes hard 
<br>
takeoff might be possible but it isn't nearly so *important*.
<br>
<p><em>&gt; I'm confident both of these are self-evidently wrong; the things we're
</em><br>
<em>&gt; dealing with here are simply not in the domain of formal proof.
</em><br>
<p>An informal argument is just formal probabilistic reasoning you don't 
<br>
know how to formalize.
<br>
<p><em>&gt; Do I now understand correctly that your position is a slightly weaker
</em><br>
<em>&gt; one: it would be possible to create a seed AI which:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; - In fact has a 99% chance of undergoing a hard takeoff, even though
</em><br>
<em>&gt; we can't mathematically prove it has?
</em><br>
<p>Or 80%, whatever, so long as it has a guarantee of staying-on-target 
<br>
&lt;Friendly&gt; if it does undergo hard takeoff.
<br>
<p><em>&gt; If so, then I'm still inclined to think this is incorrect, but I'm not
</em><br>
<em>&gt; as confident. My intuition says each step might have a 99% chance of
</em><br>
<em>&gt; being successfully taken, but the overall process of hard takeoff
</em><br>
<em>&gt; would be .99^N; I gather your intuition says otherwise.
</em><br>
<p>Correct.  If one path doesn't work out, take another.
<br>
<p><em>&gt;&gt;My studies so
</em><br>
<em>&gt;&gt;far indicate that humans do these things very poorly
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Compared to what standard?
</em><br>
<p>When a car has three flat tires and a broken windshield and is leaking 
<br>
oil all over the pavement, you don't need to see a new car to know this 
<br>
one is broken.  But since you ask:  A Bayesian standard, of course.  Why 
<br>
do you think cognitive psychologists talk about Bayes?  It's so that 
<br>
they have a standard by which to say humans perform poorly.
<br>
<p><em>&gt;&gt;yet because we can
</em><br>
<em>&gt;&gt;try, there must be some component of our effort that works, that
</em><br>
<em>&gt;&gt;reflects Bayes-structure or logic-structure or *something*.  At the
</em><br>
<em>&gt;&gt;least it should be possible to obtain huge performance increases over
</em><br>
<em>&gt;&gt;humans.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Bear in mind that for any evidence we have to the contrary, human
</em><br>
<em>&gt; ability at strongly recursive self-improvement is zero.
</em><br>
<p>Which is why I pointed out that your argument applies equally to the 
<br>
impossibility of writing code, for which we do possess evidence to the 
<br>
contrary.
<br>
<p><em>&gt;&gt;Why should a system that works probabilistically, not be refinable to
</em><br>
<em>&gt;&gt;yield very low failure probabilities?  Or at least I may hope.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I hope so too, but the refining has to be done by something other than
</em><br>
<em>&gt; the system itself.
</em><br>
<p>This sounds like the old fallacy of a modular system not being able to 
<br>
copy a module into a static form, refine it, and execute a controlled swap.
<br>
<p><em>&gt;&gt;But at least a volition-extrapolating FAI would refract through humans
</em><br>
<em>&gt;&gt;on the way to deciding which options our world will offer us, unlike
</em><br>
<em>&gt;&gt;natural selection or the uncaring universe.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There may be something to be said for that idea, if it can actually be
</em><br>
<em>&gt; made to work.
</em><br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10521.html">Ben Goertzel: "Android science conference"</a>
<li><strong>Previous message:</strong> <a href="10519.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>In reply to:</strong> <a href="10519.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10523.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10523.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10520">[ date ]</a>
<a href="index.html#10520">[ thread ]</a>
<a href="subject.html#10520">[ subject ]</a>
<a href="author.html#10520">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
