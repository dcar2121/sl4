<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Novamente [ was RE: When does it pay to play (lottery)?]</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="Novamente [ was RE: When does it pay to play (lottery)?]">
<meta name="Date" content="2005-01-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Novamente [ was RE: When does it pay to play (lottery)?]</h1>
<!-- received="Sun Jan 23 08:09:50 2005" -->
<!-- isoreceived="20050123150950" -->
<!-- sent="Sun, 23 Jan 2005 10:09:22 -0500" -->
<!-- isosent="20050123150922" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="Novamente [ was RE: When does it pay to play (lottery)?]" -->
<!-- id="JNEIJCJJHIEAILJBFHILOEALDDAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="41F38D6B.6070901@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=Re:%20Novamente%20[%20was%20RE:%20When%20does%20it%20pay%20to%20play%20(lottery)?]"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sun Jan 23 2005 - 08:09:22 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10614.html">maru: "Re: When does it pay to play (lottery)?"</a>
<li><strong>Previous message:</strong> <a href="10612.html">Harvey Newstrom: "Re: When does it pay to play (lottery)?"</a>
<li><strong>In reply to:</strong> <a href="10611.html">Eliezer Yudkowsky: "Re: When does it pay to play (lottery)?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10619.html">Thomas Buckner: "Re: Novamente"</a>
<li><strong>Reply:</strong> <a href="10619.html">Thomas Buckner: "Re: Novamente"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10613">[ date ]</a>
<a href="index.html#10613">[ thread ]</a>
<a href="subject.html#10613">[ subject ]</a>
<a href="author.html#10613">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; &gt; If I were a betting man (and I am on occasion), I'd put my money on Ben.
</em><br>
<em>&gt; &gt; Seems to me he's got Novamente halfway around the track while Eliezer's
</em><br>
<em>&gt; &gt; still trying to decide which horse he's going to ride.
</em><br>
<em>&gt;
</em><br>
<em>&gt; And if Novamente should ever cross the finish line, we all die.  That is
</em><br>
<em>&gt; what I believe or I would be working for Ben this instant.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Aside from that, I don't object to your statement of fact.  You
</em><br>
<em>&gt; can indeed
</em><br>
<em>&gt; move faster the less you care about safety.  We'll all die when you cross
</em><br>
<em>&gt; the finish line, but hey, you were first!  Yay!  That is how
</em><br>
<em>&gt; people think,
</em><br>
<em>&gt; and that is what makes the planet itself unsafe, at this point in time.
</em><br>
<em>&gt;
</em><br>
<em>&gt;--
</em><br>
<em>&gt;Eliezer Yudkowsky
</em><br>
<p><p>I'll take this opportunity to say a few things about Novamente and its
<br>
current situation.. [I'll get to Friendliness at the end]
<br>
<p>1)
<br>
After many years of experimenting with AI components and doing prototype
<br>
system-building and related mathematical and conceptual theory, I'm pretty
<br>
confident we have an AGI design that is workable.  Workable in the sense of:
<br>
Can yield human-level-and-beyond intelligence in principle; is tractable
<br>
enough to do so on a plausible-sized network of contemporary Linux boxes;
<br>
and is simple enough to be tuned, debugged and tested by a small team of
<br>
only modestly extraordinary mortals.  Furthermore, we have a software design
<br>
for our AGI and have a pretty good percentage of it implemented (though
<br>
there's still plenty work to be done) and have done plenty of tuning of AI
<br>
components on various test problems.
<br>
<p>2)
<br>
While we may well be halfway or a third of the way around the track to true
<br>
AGI, alas our pace of progress is more like a fast walk than a gallop or
<br>
trot, at this point.  The reason is that the core Novamente team -- the
<br>
handful of folks who really understand the Novamente system -- are spending
<br>
most of their time working on Novamente-based commercial software consulting
<br>
projects, rather than on directly AGI-oriented work.  This was OK for a
<br>
while, but we have now reached the point where we have initial versions of
<br>
the basic learning/reasoning/memory components of Novamente, and a good
<br>
initial version of the overall &quot;Mind OS&quot; framework in which they cooperate.
<br>
This is the point at which Novamente-AI-component-based commercial
<br>
development necessarily DIVERGES from AGI work.  When we were building the
<br>
basic learning/reasoning tools, the commercial work and AGI work were
<br>
somewhat overlapping, because the same tools can be used for AGI and for
<br>
narrow AI apps.  But what we need to do now for AGI is work on integrating
<br>
the different AI tools together in a more sophisticated way in the context
<br>
of having Novamente control an embodied agent in a simulated environment.
<br>
And this is not work that any of our current commercial applications
<br>
supports.
<br>
<p>3)
<br>
Thankfully, due to a recent $7000 investment, I've been able to hire one
<br>
person to focus solely on AGI.  What he's doing at the moment is building
<br>
the simulation environment in which the embodied agent will live, and
<br>
hooking this sim-world up to Novamente.  But alas, one person isn't
<br>
enough....
<br>
<p>4)
<br>
To really do the Novamente project right, at a reasonable rate of speed, I'd
<br>
need something like $500K/year for something like 3 years.  I say 3 years
<br>
because that is enough time that, ABSOLUTELY FOR CERTAIN, before that time
<br>
is up we'd have results SO IMPRESSIVE that getting much more development
<br>
money would be no problem.  This money would be used to pay a bunch of the
<br>
current Novamente gurus, some of whom are in the US and some in Brazil, to
<br>
work full-time on nothing but AGI.
<br>
<p>5)
<br>
To raise this money there are two avenues open:
<br>
5a)
<br>
Make enough $$ from Novamente-based businesses to fund it ourselves.  This
<br>
is not going to happen in 2005, but it could happen in 2006 or 2007, if all
<br>
goes well.  Our bioinformatics work (www.biomind.com) has yielded some
<br>
really nice scientific results in the area of gene expression analysis,
<br>
we're working with the CDC and the NIH, and over the next couple years (with
<br>
a lot of effort) it should be possible to turn this into a reasonably
<br>
profitable business in the biopharma market.
<br>
5b)
<br>
Get someone to donate money for AGI research.  Here there are two
<br>
categories:
<br>
5b1) Government research grants.  Unfortunately the US government
<br>
research-funding establishment is extremely conservative where AGI is
<br>
concerned, and nearly all AGI-ish funding seems to go to the likes of Cyc,
<br>
SOAR and ACT-R.  I have been banging my head against the
<br>
government-grant-funding wall for some time, and who knows, I may succeed
<br>
eventually, it's partly a matter of statistics.  At the moment I have some
<br>
collaborators in this regard who have a lot of experience getting gov't
<br>
research grants.
<br>
5b2) Private donations.  This just depends on meeting the right person who
<br>
has a substantial amount of money and an interest in using it to move
<br>
forward toward AGI.  I have some contacts who meet these conditions, but am
<br>
waiting for the right moment to approach them.
<br>
<p>6)
<br>
There are some specific things we can do to get ourselves in a better
<br>
position in order to raise private donation or government grant money.
<br>
These are:
<br>
6a)
<br>
Finally publish the long-in-process books on Novamente.  This will happen in
<br>
2005, for real!  Two of the 3 books in the trilogy are quite close to being
<br>
ready to go out to the publisher!! ;)  [Please note, the reason these books
<br>
have been so long in coming is basically that I, the lead author, have been
<br>
spending so much of my time on commercial narrow-AI projects -- including
<br>
very cool and scientifically valuable stuff like Biomind....]
<br>
6b)
<br>
Put together a reasonably wizzy demo of Novamente doing something cool.  I
<br>
really hope this will happen in 2005, but I'm not positive it will, due to
<br>
lack of human resources devoted to it.  What I want to do here is have
<br>
Novamente control an agent in our AGI-SIM sim world, according to
<br>
instructions given to it in English.  We have a good, interactive
<br>
English-language comprehension interface (which relies on a mix of learning
<br>
and inelegant but effective rule-based AI, which we built for a commercial
<br>
AI contract), and in a couple months the sim-world will be in good shape.
<br>
What I want to demonstrate initially is just some simple learning and
<br>
reasoning.  Teach it what the word &quot;on&quot; means by giving it a bunch of
<br>
examples of objects on other objects.  Once it knows what &quot;Put the cup on
<br>
the table&quot; means and knows what cups and bowls are, then show that it
<br>
automatically learns what &quot;Put the bowl on the table means.&quot;   And a whole
<br>
bunch of other analogous examples, some a bit more complex.  Simple stuff --
<br>
but visually demonstrable, within a framework constructed with AGI in mind
<br>
and with detailed mathematical, conceptual and software documentation
<br>
backing up its AGI ambitions.
<br>
<p>7)
<br>
What stands between us and our wizzy, fundraising-friendly Novamente demo
<br>
right now is simply time and money.  We have the AI software framework, we
<br>
have the AI learning and reasoning tools within that framework, we have the
<br>
language-processing front end (which doesn't embody truly humanlike language
<br>
processing -- though we do know how to do that, we just haven't had time
<br>
yet -- but is still very useful for practical communication purposes, as
<br>
after a bit of interaction it does succeed in correctly translating English
<br>
sentences into Novamente's internal knowledge-representing nodes and links).
<br>
I guess that about $80K in investment or donation money would get us there
<br>
for sure, during 2005.  Quite possibly less.  (However, this $80K would have
<br>
to come from a source other than US government grants, because it would have
<br>
to be spent mostly outside the US in order get the needed bang for buck.  If
<br>
the money has to be spent in the US then the pricetag is higher, more like
<br>
$160K.)
<br>
<p>8)
<br>
Now, about Friendliness.  I agree with Eliezer that it's a very important
<br>
thing to worry about.  However, as I've stated oft before, I just don't
<br>
think we know nearly enough about AGI to meaningfully concoct theories of
<br>
AGI friendliness at this point in time.  I enjoy Eli's thoughts on AGI
<br>
Friendliness very much -- but as far as I'm concerned, CFAI and Collective
<br>
Volition and so forth fall into the domain of *very interesting philosophy,
<br>
and very interesting scientific speculations*.  There is nothing
<br>
*constructive* in there that's either very pragmatic or very convincing.
<br>
The main thing that Eliezer has demonstrated convincingly, IMO, is that
<br>
Friendly AI is a very hard problem!  Of course, this demonstration is a
<br>
worthwhile thing.  But my feeling is that, in order to get a decent feel for
<br>
the Friendliness problem, we're going to need to actually experiment with
<br>
some simple AGI systems -- systems with awareness of self, ability to
<br>
communicate with humans and to learn.  Based on experimenting with such
<br>
systems in a safe and simple context, we will be able to create the elements
<br>
of a science of intelligence -- which it's hard to say we have right now.
<br>
Then we will be able to grapple with the problem of Friendly AI in a
<br>
primarily scientific rather than speculative way.  Of course, at that point
<br>
the conclusion may well be that Friendly AI is impossible -- at which point
<br>
I'll shift my efforts from AGI-creation to AGI-prevention ;-)  But my
<br>
*guess* is that this won't be the conclusion...
<br>
<p>-- Ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10614.html">maru: "Re: When does it pay to play (lottery)?"</a>
<li><strong>Previous message:</strong> <a href="10612.html">Harvey Newstrom: "Re: When does it pay to play (lottery)?"</a>
<li><strong>In reply to:</strong> <a href="10611.html">Eliezer Yudkowsky: "Re: When does it pay to play (lottery)?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10619.html">Thomas Buckner: "Re: Novamente"</a>
<li><strong>Reply:</strong> <a href="10619.html">Thomas Buckner: "Re: Novamente"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10613">[ date ]</a>
<a href="index.html#10613">[ thread ]</a>
<a href="subject.html#10613">[ subject ]</a>
<a href="author.html#10613">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
