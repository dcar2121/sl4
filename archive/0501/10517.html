<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Definition of strong recursive self-improvement</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Definition of strong recursive self-improvement">
<meta name="Date" content="2005-01-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Definition of strong recursive self-improvement</h1>
<!-- received="Sun Jan  2 14:26:51 2005" -->
<!-- isoreceived="20050102212651" -->
<!-- sent="Sun, 02 Jan 2005 15:28:41 -0600" -->
<!-- isosent="20050102212841" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Definition of strong recursive self-improvement" -->
<!-- id="41D86789.7000204@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="8d71341e050102114157985a2a@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Definition%20of%20strong%20recursive%20self-improvement"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jan 02 2005 - 14:28:41 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10518.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Previous message:</strong> <a href="10516.html">Mike Williams: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>In reply to:</strong> <a href="10515.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10519.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10519.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10517">[ date ]</a>
<a href="index.html#10517">[ thread ]</a>
<a href="subject.html#10517">[ subject ]</a>
<a href="author.html#10517">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Russell Wallace wrote:
<br>
<em>&gt; On Sun, 02 Jan 2005 11:38:54 -0600, Eliezer S. Yudkowsky
</em><br>
<em>&gt; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Definition%20of%20strong%20recursive%20self-improvement">sentience@pobox.com</a>&gt; wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;But if I knew how to build an FAI that worked so long as no one tossed
</em><br>
<em>&gt;&gt;its CPU into a bowl of ice cream, I would count myself as having made
</em><br>
<em>&gt;&gt;major progress.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes, I think it's safe to say that would qualify as progress alright.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Do you still believe in the &quot;hard takeoff in a basement&quot; scenario, though?
</em><br>
<p>Leaving aside the choice of verbs, yes, I still guess that.
<br>
<p><em>&gt;&gt;Meanwhile, saying that humans use &quot;semi-formal reasoning&quot; to write code
</em><br>
<em>&gt;&gt;is not, I'm afraid, a Technical Explanation.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; No, really? I'm shocked :) (Good article that, btw.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If either of us were at the point of being able to provide a Technical
</em><br>
<em>&gt; Explanation for this stuff, this conversation would be taking a very
</em><br>
<em>&gt; different form. (For one thing, the side that had it could probably
</em><br>
<em>&gt; let their AI do a lot of the debating for them!) But my semi-technical
</em><br>
<em>&gt; explanation does answer the question you asked, which is how _in
</em><br>
<em>&gt; principle_ it can be possible for human programmers to ever write
</em><br>
<em>&gt; working code;
</em><br>
<p>No, your answer is at best an *argument that* in principle it is 
<br>
possible for human programmers to write code - and if we did not have 
<br>
the example before our eyes, the argument wouldn't convince.
<br>
<p>There are specific things about how humans write code that I do not 
<br>
presently understand, even as to matters of fundamental principle.  If I 
<br>
had never seen humans write code, I wouldn't know to expect that they 
<br>
could.  I have read your answer and my questions, even the fundamental 
<br>
questions, are still unsolved to me.  So either I missed something in 
<br>
your response, or it doesn't count as an explanation.
<br>
<p>I'm sorry if this seems harsh, but, you have read the page, you know the 
<br>
rules.  &quot;Semi-formal reasoning&quot; is not an answer.  You have to say what 
<br>
specifically are the dynamics of semi-formal reasoning, why it works to 
<br>
describe the universe reliably enough to permit (at least) the observed 
<br>
level of human competence in writing code... the phrase &quot;semi-formal&quot; 
<br>
reasoning doesn't tell me what kind of code humans write, or even what 
<br>
kind of code humans do not write.  I'm not trying to annoy you, this is 
<br>
a generally strict standard that I try to apply.  How humans write code 
<br>
is not something that you have answered me, nor have you explained why 
<br>
the phrase &quot;semi-formal&quot; excepts your previous impossibility argument. 
<br>
Should not semi-formal reasoning be even less effective than formal 
<br>
reasoning?  Unless it contains some additional component, not present in 
<br>
formal reasoning, that works well and reliably - perhaps not perfectly, 
<br>
but still delivering reliably better performance than random numbers, 
<br>
while not being the same as a formal proof.  Can we not calibrate such a 
<br>
system according to its strength?  Can we not wring from it a calibrated 
<br>
probability of 99%?
<br>
<p><em>&gt; and it therefore suffices to answer your objection that
</em><br>
<em>&gt; if I was right about the problems, there could be no such thing even
</em><br>
<em>&gt; in principle.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;Imagine someone who knew
</em><br>
<em>&gt;&gt;naught of Bayes, pointing to probabilistic reasoning and saying it was
</em><br>
<em>&gt;&gt;all &quot;guessing&quot; and therefore would inevitably fail at one point or
</em><br>
<em>&gt;&gt;another.  In that vague and verbal model you could not express the
</em><br>
<em>&gt;&gt;notion of a more reliable, better-discriminating probabilistic guesser,
</em><br>
<em>&gt;&gt;powered by Bayesian principles and a better implementation, that could
</em><br>
<em>&gt;&gt;achieve a calibrated probability of 0.0001% for the failure of an entire
</em><br>
<em>&gt;&gt;system over, say, ten millennia.
</em><br>
<p><em>&gt; How do you get a calibrated probability of failure, or even calculate
</em><br>
<em>&gt; P(E|H) for a few H's, in a situation where calculating P(E|H) for one
</em><br>
<em>&gt; H would take computer time measured in teraexaflop-eons, and plenty of
</em><br>
<em>&gt; them?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; (These are not rhetorical questions. I'm asking them because answers
</em><br>
<em>&gt; would be of great practical value.)
</em><br>
<p>I am still trying to figure out the answers myself.  What I do not 
<br>
understand is your confidence that there is no answer.  My studies so 
<br>
far indicate that humans do these things very poorly; yet because we can 
<br>
try, there must be some component of our effort that works, that 
<br>
reflects Bayes-structure or logic-structure or *something*.  At the 
<br>
least it should be possible to obtain huge performance increases over 
<br>
humans.
<br>
<p>Why should a system that works probabilistically, not be refinable to 
<br>
yield very low failure probabilities?  Or at least I may hope.
<br>
<p><em>&gt;&gt;(For I do now regard FAI as an interim
</em><br>
<em>&gt;&gt;measure, to be replaced by some other System when humans have grown up a
</em><br>
<em>&gt;&gt;little.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So you want to take humans out of the loop for awhile, then put them
</em><br>
<em>&gt; back in after a few millennia? (Whereas I'm inclined to think humans
</em><br>
<em>&gt; will need to stay in the loop all the way along.)
</em><br>
<p>Humans *never were* in the loop on most questions, like what kind of 
<br>
brain designs humans should have, or what kind of environment and 
<br>
environmental rules we exist in, or what kind of decisions humans should 
<br>
make.  We were each born into a world we did not design, decided for us 
<br>
by alien forces like natural selection.  I am not so proud of my human 
<br>
stupidity as to think that handing foundational decisions to modern-day 
<br>
humans would accomplish anything but death by unintended consequences. 
<br>
But at least a volition-extrapolating FAI would refract through humans 
<br>
on the way to deciding which options our world will offer us, unlike 
<br>
natural selection or the uncaring universe.
<br>
<p>Someday we will become wise enough to understand which decisions we dare 
<br>
make, instead of indignantly rejecting any suggestion that we are not so 
<br>
wise.  At that point, perhaps, I will have earned the right to choose 
<br>
whether to choose.  Yet I might wait a thousand subjective years after 
<br>
that threshold before there was *not* too great a danger.  If I want to 
<br>
live to be a billion years old, I'd best not take that sort of risk when 
<br>
I'm still young.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10518.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Previous message:</strong> <a href="10516.html">Mike Williams: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>In reply to:</strong> <a href="10515.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10519.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10519.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10517">[ date ]</a>
<a href="index.html#10517">[ thread ]</a>
<a href="subject.html#10517">[ subject ]</a>
<a href="author.html#10517">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
