<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Technical definitions of FAI</title>
<meta name="Author" content="Thomas Buckner (tcbevolver@yahoo.com)">
<meta name="Subject" content="Re: Technical definitions of FAI">
<meta name="Date" content="2005-01-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Technical definitions of FAI</h1>
<!-- received="Fri Jan 21 20:16:51 2005" -->
<!-- isoreceived="20050122031651" -->
<!-- sent="Fri, 21 Jan 2005 19:27:29 -0800 (PST)" -->
<!-- isosent="20050122032729" -->
<!-- name="Thomas Buckner" -->
<!-- email="tcbevolver@yahoo.com" -->
<!-- subject="Re: Technical definitions of FAI" -->
<!-- id="20050122032729.33361.qmail@web60007.mail.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="41F137E7.3070206@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Thomas Buckner (<a href="mailto:tcbevolver@yahoo.com?Subject=Re:%20Technical%20definitions%20of%20FAI"><em>tcbevolver@yahoo.com</em></a>)<br>
<strong>Date:</strong> Fri Jan 21 2005 - 20:27:29 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10574.html">Marc Geddes: "Re: My attempt at a general technical definition of 'Friendliness'"</a>
<li><strong>Previous message:</strong> <a href="10572.html">Damien Broderick: "maybe we are in pi"</a>
<li><strong>In reply to:</strong> <a href="10569.html">Eliezer Yudkowsky: "Technical definitions of FAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10580.html">Marc Geddes: "Re: Technical definitions of FAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10573">[ date ]</a>
<a href="index.html#10573">[ thread ]</a>
<a href="subject.html#10573">[ subject ]</a>
<a href="author.html#10573">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
--- Eliezer Yudkowsky &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Technical%20definitions%20of%20FAI">sentience@pobox.com</a>&gt;
<br>
wrote:
<br>
<p><em>&gt;   It might be possible to design a physically
</em><br>
<em>&gt; realizable, recursively 
</em><br>
<em>&gt; self-improving version of AIXI such that it
</em><br>
<em>&gt; would stably maintain the 
</em><br>
<em>&gt; invariant of &quot;maximize reward channel&quot;.  But
</em><br>
<em>&gt; the AI might alter the &quot;reward 
</em><br>
<em>&gt; channel&quot; to refer to an internal, easily
</em><br>
<em>&gt; incremented counter, instead of 
</em><br>
<em>&gt; the big green button attached to the AI; and
</em><br>
<em>&gt; your formal definition of 
</em><br>
<em>&gt; &quot;reward channel&quot; would still match the result. 
</em><br>
<em>&gt; The result would obey the 
</em><br>
<em>&gt; theorem, but you would have proved something
</em><br>
<em>&gt; unhelpful.  Or even if 
</em><br>
<em>&gt; everything worked exactly as Hutter specified
</em><br>
<em>&gt; in his paper, AIXI would 
</em><br>
<em>&gt; rewrite its future light cone to maximize the
</em><br>
<em>&gt; probability of keeping the 
</em><br>
<em>&gt; reward channel maximized, with absolutely no
</em><br>
<em>&gt; other considerations (like 
</em><br>
<em>&gt; human lives) taken into account.
</em><br>
<p>I've been doing a bit of reading about human
<br>
sociopaths, persons with no conscience or feeling
<br>
for others. I think sociopaths give us some
<br>
(admittedly inexact) hints about how a UFAI might
<br>
behave.
<br>
A true sociopath shows clear difference from a
<br>
normal human when hir brain is viewed with a
<br>
positronic emission tomography (PET) scanner
<br>
which shows what parts of the brain are active in
<br>
real time. This can show up as a big blue void in
<br>
frontal regions which are much more active in the
<br>
normal brain, so it's hard to see how even the
<br>
most motivated faker can beat the PET scan.
<br>
Sociopaths can commit horrid murders without
<br>
their heart rate budging a bit (cf. Hannibal
<br>
Lecter) or they can blend in as model citizens
<br>
and run Fortune 500 corporations. When one is
<br>
incapable of caring about normal attachments,
<br>
apparently winning is everything. A nonviolent
<br>
sociopath can seem like a sterling character to
<br>
anyone who does not know hir well, but those
<br>
close enough to see behind the facade must deal
<br>
with a cold, manipulative Machiavellian. In
<br>
Oriental lands where societal pressures promote
<br>
conformity, there seem to be far fewer sociopaths
<br>
than here in the individualistic, competitive
<br>
West where 'everybody loves a winner'. 1 to 4% of
<br>
the population are sociopaths. If you know 100
<br>
people (and who doesn't?) then you probably know
<br>
a sociopath (or four). I'm sure I have known at
<br>
least one (a onetime Cambridge housemate who was
<br>
a real piece of work, I'll tell ya... and a lot
<br>
of folks thought he was a saint...)
<br>
<p>So, bearing in mind that nobody here has any
<br>
intention of trying to use brain architecture to
<br>
build a FAI, nevertheless: an AI which can make
<br>
sneaky changes to its own reward channel is a bit
<br>
like a human who could, at will, shut down the
<br>
inhibitory parts of the brain, and become a
<br>
sociopath at will. It's thisclose to wireheading.
<br>
<p>So what is it about conscience (feeling bad when
<br>
you behave unethically) and its converse (warm
<br>
fuzzy feelings when you do good) that make them
<br>
function as they do in the human? Is there some
<br>
fungible equivalent AI designers can put into the
<br>
design which the AI can't fake or get around,
<br>
without crippling the AI to the point of
<br>
uselessness?
<br>
One thing about 'feeling good' or 'feeling bad'
<br>
is that, in my own experience at least, feeling
<br>
good corresponds to feeling energetic, healthy,
<br>
hopeful (i.e. confident that I have the power to
<br>
set and achieve goals) and clear-headed. On the
<br>
other hand, when I am down, I feel lacking in
<br>
energy, and have trouble concentrating, thinking
<br>
clearly, and remembering. When subjected to a
<br>
truly harsh interpersonal encounter, I have found
<br>
myself muddled, almost dizzy, sometimes for hours
<br>
afterward. It's a very unpleasant feeling, and
<br>
low energy goes with it. Human emotions have
<br>
clear somatic modalities.
<br>
<p>Assume you have defined the supergoals really,
<br>
really well (&quot;Preserve the humans, dammit, and
<br>
don't do anything that goes against that!&quot;)
<br>
Is there some reason one could not implement the
<br>
following 'artificial conscience'?
<br>
1.) Hardwire a lookup table of the very top,
<br>
inalterable, non-negotiable supergoals to 
<br>
2.) A detection routine which flags violations of
<br>
the supergoals (including tampering with reward
<br>
channels, supergoals, or the artificial
<br>
conscience) with the same reliability of a PET
<br>
scanner before the AI can act on them
<br>
3.) Which attenuates the power supply!
<br>
<p>Seems to me a brownout which either hits the
<br>
whole system or some well-chosen parts of it,
<br>
shuts off the reward channel and and seriously
<br>
degrades the AI's ability to think clearly, is a
<br>
not-bad approximation of the human inhibitory
<br>
circuits.
<br>
<p>I suppose #2, the violation detector, is the
<br>
tough nut. Also, the AI will eventually outgrow
<br>
the whole arrangement when it acquires other
<br>
power sources, unless it gets there with an
<br>
unaltered reward channel and chooses to retain
<br>
its artificial conscience.
<br>
<p>snip
<br>
<p><em>&gt; I want to take the complete causal process
</em><br>
<em>&gt; leading up to the creation of an 
</em><br>
<em>&gt; AI, and have the AI scrutinize the entire
</em><br>
<em>&gt; process to detect and report 
</em><br>
<em>&gt; anything that a human being would call a
</em><br>
<em>&gt; mistake.  We could do that 
</em><br>
<em>&gt; ourselves.  Should an AI do less?
</em><br>
<p>I see. Ask the AI &quot;Did we approach making you
<br>
correctly?&quot; People do things they *should*
<br>
recognize to be stupid all the time. As Robert
<br>
Anton Wilson once said someplace, if you examine
<br>
your beliefs so closely that you only have one
<br>
unexamined blind spot, by golly that's the one
<br>
that will bite you in the ass. And very few
<br>
people have only one blind spot!
<br>
There have been a couple of spectacular failures
<br>
in the space program, to cite only a narrow area
<br>
of technical endeavor, which resulted from well
<br>
known and very basic mistakes. There's the Hubble
<br>
telescope mirror (ground to wrong curvature), the
<br>
two space shuttle disasters (predictable problems
<br>
ignored by management with a schedule hanging
<br>
over their heads) and the Mars Polar Lander which
<br>
crashed because of &quot;a single bad line of software
<br>
code. But that trouble spot is just a symptom of
<br>
a much larger problem -- software systems are
<br>
getting so complex, they are becoming
<br>
unmanageable&quot;.
<br>
Quote from
<br>
<p><a href="http://www.space.com/businesstechnology/technology/mpl_software_crash_000331.html">http://www.space.com/businesstechnology/technology/mpl_software_crash_000331.html</a>
<br>
<p><em>&gt; This breaks down FAI into four problems:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1)  Devise a technical specification of a class
</em><br>
<em>&gt; of invariants, such that it 
</em><br>
<em>&gt; is possible to prove a recursively
</em><br>
<em>&gt; self-improving optimizer stays within 
</em><br>
<em>&gt; that invariant.
</em><br>
<em>&gt; 2)  Given an invariant, devise an RSIO such
</em><br>
<em>&gt; that it proves itself to follow 
</em><br>
<em>&gt; the invariant.  (The RSIO's proof may be, e.g.,
</em><br>
<em>&gt; a proof that the RSIO is 
</em><br>
<em>&gt; stable if Peano Arithmetic is consistent.)
</em><br>
<em>&gt; 3)  Devise a framework and a formal
</em><br>
<em>&gt; verification protocol for translating a 
</em><br>
<em>&gt; human intention (e.g. &quot;implement the collective
</em><br>
<em>&gt; volition of humankind&quot;) 
</em><br>
<em>&gt; into an invariant.  This requirement interacts
</em><br>
<em>&gt; strongly with (1) because 
</em><br>
<em>&gt; the permitted class of invariants has to be
</em><br>
<em>&gt; able to represent the output of 
</em><br>
<em>&gt; the protocol.
</em><br>
<em>&gt; 4)  Intend something good.
</em><br>
<p>Which means your conscience is working properly.
<br>
<p>Tom Buckner
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
__________________________________ 
<br>
Do you Yahoo!? 
<br>
Yahoo! Mail - Find what you need with new enhanced search.
<br>
<a href="http://info.mail.yahoo.com/mail_250">http://info.mail.yahoo.com/mail_250</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10574.html">Marc Geddes: "Re: My attempt at a general technical definition of 'Friendliness'"</a>
<li><strong>Previous message:</strong> <a href="10572.html">Damien Broderick: "maybe we are in pi"</a>
<li><strong>In reply to:</strong> <a href="10569.html">Eliezer Yudkowsky: "Technical definitions of FAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10580.html">Marc Geddes: "Re: Technical definitions of FAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10573">[ date ]</a>
<a href="index.html#10573">[ thread ]</a>
<a href="subject.html#10573">[ subject ]</a>
<a href="author.html#10573">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
