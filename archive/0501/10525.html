<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Definition of strong recursive self-improvement</title>
<meta name="Author" content="Billy Brown (bbrown@transcient.com)">
<meta name="Subject" content="RE: Definition of strong recursive self-improvement">
<meta name="Date" content="2005-01-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Definition of strong recursive self-improvement</h1>
<!-- received="Mon Jan  3 11:28:29 2005" -->
<!-- isoreceived="20050103182829" -->
<!-- sent="Mon, 3 Jan 2005 12:28:35 -0600" -->
<!-- isosent="20050103182835" -->
<!-- name="Billy Brown" -->
<!-- email="bbrown@transcient.com" -->
<!-- subject="RE: Definition of strong recursive self-improvement" -->
<!-- id="200501031828.j03ISS013164@tick.javien.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="8d71341e050102152377ad7e9b@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Billy Brown (<a href="mailto:bbrown@transcient.com?Subject=RE:%20Definition%20of%20strong%20recursive%20self-improvement"><em>bbrown@transcient.com</em></a>)<br>
<strong>Date:</strong> Mon Jan 03 2005 - 11:28:35 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10526.html">Emil Gilliam: "Ray Solomonoff lectures at MIT"</a>
<li><strong>Previous message:</strong> <a href="10524.html">Dani Eder: "Re: self improvement"</a>
<li><strong>In reply to:</strong> <a href="10519.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10510.html">maru: "Re: Definition of strong recursive self-improvement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10525">[ date ]</a>
<a href="index.html#10525">[ thread ]</a>
<a href="subject.html#10525">[ subject ]</a>
<a href="author.html#10525">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I think you (and some other posters on this thread) are implicitly assuming
<br>
here that &quot;intelligence&quot; is a single monolithic entity, which can only be
<br>
improved through a wholesale overhaul that changes every part of the AI at
<br>
once. This is actually a very unlikely scenario.
<br>
<p>More plausible is an AI in which &quot;intelligence&quot; is the output of a very
<br>
large system containing many interacting subsystems, with each subsystem
<br>
containing a great deal of internal complexity. The performance of each
<br>
subsystem can be described in much less nebulous terms (data retrieval
<br>
speeds, reasoning speeds, success rates of internal algorithms at various
<br>
micro-tasks, and so on).
<br>
<p><em>&gt;From this perspective there are many ways to improve the overall performance
</em><br>
of the AI that do not run afoul of your objections. On a low level you can
<br>
tune internal heuristics of individual subsystems, do performance tuning,
<br>
add local enhancements to a module's capabilities, etc. On a higher level
<br>
you can better allocate resources between subsystems, improve the interfaces
<br>
between them, write new subsystems to deal with new types of tasks, and so
<br>
on. You will occasionally need to port the whole system to a new
<br>
architecture to solve specific problems or provide new capabilities, but
<br>
even then the analysis required to validate the change can stay at the level
<br>
of system internals rather than getting stuck in the squishy semantics of
<br>
&quot;smarter&quot; and &quot;better&quot;. 
<br>
<p>Billy Brown
<br>
<p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20Definition%20of%20strong%20recursive%20self-improvement">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20Definition%20of%20strong%20recursive%20self-improvement">owner-sl4@sl4.org</a>] On Behalf Of Russell
</em><br>
<em>&gt; Wallace
</em><br>
<em>&gt; Sent: Sunday, January 02, 2005 5:23 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=RE:%20Definition%20of%20strong%20recursive%20self-improvement">sl4@sl4.org</a>
</em><br>
<em>&gt; Subject: Re: Definition of strong recursive self-improvement
</em><br>
<em>&gt; 
</em><br>
<em>&gt; On Sun, 02 Jan 2005 15:28:41 -0600, Eliezer S. Yudkowsky
</em><br>
<em>&gt; &lt;<a href="mailto:sentience@pobox.com?Subject=RE:%20Definition%20of%20strong%20recursive%20self-improvement">sentience@pobox.com</a>&gt; wrote:
</em><br>
<em>&gt; &gt; There are specific things about how humans write code that I do not
</em><br>
<em>&gt; &gt; presently understand, even as to matters of fundamental principle.  If I
</em><br>
<em>&gt; &gt; had never seen humans write code, I wouldn't know to expect that they
</em><br>
<em>&gt; &gt; could.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Just to check, do you mean:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; a) That if you had seen humans plan journeys, construction projects,
</em><br>
<em>&gt; military campaigns etc, you wouldn't know to expect that they could
</em><br>
<em>&gt; also write code on the grounds that a program and a plan are the same
</em><br>
<em>&gt; sort of thing wearing different hats,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; or
</em><br>
<em>&gt; 
</em><br>
<em>&gt; b) That you wouldn't know to expect that humans would be capable of
</em><br>
<em>&gt; any sort of effective planning in a world where perfect planning is
</em><br>
<em>&gt; impossible?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; (If you mean a) I disagree with you, if you mean b) I agree.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; I'm sorry if this seems harsh, but, you have read the page, you know the
</em><br>
<em>&gt; &gt; rules.  &quot;Semi-formal reasoning&quot; is not an answer.  You have to say what
</em><br>
<em>&gt; &gt; specifically are the dynamics of semi-formal reasoning, why it works to
</em><br>
<em>&gt; &gt; describe the universe reliably enough to permit (at least) the observed
</em><br>
<em>&gt; &gt; level of human competence in writing code... the phrase &quot;semi-formal&quot;
</em><br>
<em>&gt; &gt; reasoning doesn't tell me what kind of code humans write, or even what
</em><br>
<em>&gt; &gt; kind of code humans do not write.  I'm not trying to annoy you, this is
</em><br>
<em>&gt; &gt; a generally strict standard that I try to apply.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; No problem, I'm just trying to point out that if either of us were in
</em><br>
<em>&gt; a position to answer that question, this conversation wouldn't be
</em><br>
<em>&gt; necessary in the first place. I'm in the position of a Renaissance
</em><br>
<em>&gt; alchemist telling a colleague &quot;I think life works because it's made of
</em><br>
<em>&gt; zillions of clockwork-like components that are themselves made of
</em><br>
<em>&gt; atoms, rather because of vital force&quot; and getting the reply &quot;That
</em><br>
<em>&gt; still doesn't count as a Technical Explanation&quot; - in a sense the reply
</em><br>
<em>&gt; is correct, but the explanation is still the best available at the
</em><br>
<em>&gt; current time.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; How humans write code
</em><br>
<em>&gt; &gt; is not something that you have answered me, nor have you explained why
</em><br>
<em>&gt; &gt; the phrase &quot;semi-formal&quot; excepts your previous impossibility argument.
</em><br>
<em>&gt; &gt; Should not semi-formal reasoning be even less effective than formal
</em><br>
<em>&gt; &gt; reasoning?  Unless it contains some additional component, not present in
</em><br>
<em>&gt; &gt; formal reasoning, that works well and reliably - perhaps not perfectly,
</em><br>
<em>&gt; &gt; but still delivering reliably better performance than random numbers,
</em><br>
<em>&gt; &gt; while not being the same as a formal proof.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The additional component is the ability to ignore the indefinitely
</em><br>
<em>&gt; large set of things that in principle could matter but in practice
</em><br>
<em>&gt; probably don't in the particular context, in favor of the smallish set
</em><br>
<em>&gt; of things that are likely to matter, which makes a problem tractable,
</em><br>
<em>&gt; while forgoing certainty. (And no, I don't at this time have a
</em><br>
<em>&gt; technical explanation of _how_ this additional component works.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; Can we not calibrate such a
</em><br>
<em>&gt; &gt; system according to its strength?  Can we not wring from it a calibrated
</em><br>
<em>&gt; &gt; probability of 99%?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 99% probability of success _at a particular, specified task_, maybe
</em><br>
<em>&gt; so. Self-improvement isn't a particular, specified task though.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; I am still trying to figure out the answers myself.  What I do not
</em><br>
<em>&gt; &gt; understand is your confidence that there is no answer.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, I originally had the impression you believed it would be
</em><br>
<em>&gt; possible to create a seed AI which:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; - Would provably undergo hard takeoff (running on a supercomputer in a
</em><br>
<em>&gt; basement)
</em><br>
<em>&gt; - Or else, would provably have e.g. a 99% probability of doing so
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'm confident both of these are self-evidently wrong; the things we're
</em><br>
<em>&gt; dealing with here are simply not in the domain of formal proof.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Do I now understand correctly that your position is a slightly weaker
</em><br>
<em>&gt; one: it would be possible to create a seed AI which:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; - In fact has a 99% chance of undergoing a hard takeoff, even though
</em><br>
<em>&gt; we can't mathematically prove it has?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If so, then I'm still inclined to think this is incorrect, but I'm not
</em><br>
<em>&gt; as confident. My intuition says each step might have a 99% chance of
</em><br>
<em>&gt; being successfully taken, but the overall process of hard takeoff
</em><br>
<em>&gt; would be .99^N; I gather your intuition says otherwise.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; My studies so
</em><br>
<em>&gt; &gt; far indicate that humans do these things very poorly
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Compared to what standard?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; yet because we can
</em><br>
<em>&gt; &gt; try, there must be some component of our effort that works, that
</em><br>
<em>&gt; &gt; reflects Bayes-structure or logic-structure or *something*.  At the
</em><br>
<em>&gt; &gt; least it should be possible to obtain huge performance increases over
</em><br>
<em>&gt; &gt; humans.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Bear in mind that for any evidence we have to the contrary, human
</em><br>
<em>&gt; ability at strongly recursive self-improvement is zero.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; Why should a system that works probabilistically, not be refinable to
</em><br>
<em>&gt; &gt; yield very low failure probabilities?  Or at least I may hope.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I hope so too, but the refining has to be done by something other than
</em><br>
<em>&gt; the system itself.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; But at least a volition-extrapolating FAI would refract through humans
</em><br>
<em>&gt; &gt; on the way to deciding which options our world will offer us, unlike
</em><br>
<em>&gt; &gt; natural selection or the uncaring universe.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There may be something to be said for that idea, if it can actually be
</em><br>
<em>&gt; made to work.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; - Russell
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10526.html">Emil Gilliam: "Ray Solomonoff lectures at MIT"</a>
<li><strong>Previous message:</strong> <a href="10524.html">Dani Eder: "Re: self improvement"</a>
<li><strong>In reply to:</strong> <a href="10519.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10510.html">maru: "Re: Definition of strong recursive self-improvement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10525">[ date ]</a>
<a href="index.html#10525">[ thread ]</a>
<a href="subject.html#10525">[ subject ]</a>
<a href="author.html#10525">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
