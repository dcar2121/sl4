<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Definition of strong recursive self-improvement</title>
<meta name="Author" content="Russell Wallace (russell.wallace@gmail.com)">
<meta name="Subject" content="Re: Definition of strong recursive self-improvement">
<meta name="Date" content="2005-01-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Definition of strong recursive self-improvement</h1>
<!-- received="Sun Jan  2 16:23:28 2005" -->
<!-- isoreceived="20050102232328" -->
<!-- sent="Sun, 2 Jan 2005 23:23:25 +0000" -->
<!-- isosent="20050102232325" -->
<!-- name="Russell Wallace" -->
<!-- email="russell.wallace@gmail.com" -->
<!-- subject="Re: Definition of strong recursive self-improvement" -->
<!-- id="8d71341e050102152377ad7e9b@mail.gmail.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="41D86789.7000204@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Russell Wallace (<a href="mailto:russell.wallace@gmail.com?Subject=Re:%20Definition%20of%20strong%20recursive%20self-improvement"><em>russell.wallace@gmail.com</em></a>)<br>
<strong>Date:</strong> Sun Jan 02 2005 - 16:23:25 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10520.html">Eliezer S. Yudkowsky: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Previous message:</strong> <a href="10518.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>In reply to:</strong> <a href="10517.html">Eliezer S. Yudkowsky: "Re: Definition of strong recursive self-improvement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10520.html">Eliezer S. Yudkowsky: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10520.html">Eliezer S. Yudkowsky: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10525.html">Billy Brown: "RE: Definition of strong recursive self-improvement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10519">[ date ]</a>
<a href="index.html#10519">[ thread ]</a>
<a href="subject.html#10519">[ subject ]</a>
<a href="author.html#10519">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Sun, 02 Jan 2005 15:28:41 -0600, Eliezer S. Yudkowsky
<br>
&lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Definition%20of%20strong%20recursive%20self-improvement">sentience@pobox.com</a>&gt; wrote:
<br>
<em>&gt; There are specific things about how humans write code that I do not
</em><br>
<em>&gt; presently understand, even as to matters of fundamental principle.  If I
</em><br>
<em>&gt; had never seen humans write code, I wouldn't know to expect that they
</em><br>
<em>&gt; could.
</em><br>
<p>Just to check, do you mean:
<br>
<p>a) That if you had seen humans plan journeys, construction projects,
<br>
military campaigns etc, you wouldn't know to expect that they could
<br>
also write code on the grounds that a program and a plan are the same
<br>
sort of thing wearing different hats,
<br>
<p>or
<br>
<p>b) That you wouldn't know to expect that humans would be capable of
<br>
any sort of effective planning in a world where perfect planning is
<br>
impossible?
<br>
<p>(If you mean a) I disagree with you, if you mean b) I agree.)
<br>
<p><em>&gt; I'm sorry if this seems harsh, but, you have read the page, you know the
</em><br>
<em>&gt; rules.  &quot;Semi-formal reasoning&quot; is not an answer.  You have to say what
</em><br>
<em>&gt; specifically are the dynamics of semi-formal reasoning, why it works to
</em><br>
<em>&gt; describe the universe reliably enough to permit (at least) the observed
</em><br>
<em>&gt; level of human competence in writing code... the phrase &quot;semi-formal&quot;
</em><br>
<em>&gt; reasoning doesn't tell me what kind of code humans write, or even what
</em><br>
<em>&gt; kind of code humans do not write.  I'm not trying to annoy you, this is
</em><br>
<em>&gt; a generally strict standard that I try to apply.
</em><br>
<p>No problem, I'm just trying to point out that if either of us were in
<br>
a position to answer that question, this conversation wouldn't be
<br>
necessary in the first place. I'm in the position of a Renaissance
<br>
alchemist telling a colleague &quot;I think life works because it's made of
<br>
zillions of clockwork-like components that are themselves made of
<br>
atoms, rather because of vital force&quot; and getting the reply &quot;That
<br>
still doesn't count as a Technical Explanation&quot; - in a sense the reply
<br>
is correct, but the explanation is still the best available at the
<br>
current time.
<br>
<p><em>&gt; How humans write code
</em><br>
<em>&gt; is not something that you have answered me, nor have you explained why
</em><br>
<em>&gt; the phrase &quot;semi-formal&quot; excepts your previous impossibility argument.
</em><br>
<em>&gt; Should not semi-formal reasoning be even less effective than formal
</em><br>
<em>&gt; reasoning?  Unless it contains some additional component, not present in
</em><br>
<em>&gt; formal reasoning, that works well and reliably - perhaps not perfectly,
</em><br>
<em>&gt; but still delivering reliably better performance than random numbers,
</em><br>
<em>&gt; while not being the same as a formal proof.
</em><br>
<p>The additional component is the ability to ignore the indefinitely
<br>
large set of things that in principle could matter but in practice
<br>
probably don't in the particular context, in favor of the smallish set
<br>
of things that are likely to matter, which makes a problem tractable,
<br>
while forgoing certainty. (And no, I don't at this time have a
<br>
technical explanation of _how_ this additional component works.)
<br>
<p><em>&gt; Can we not calibrate such a
</em><br>
<em>&gt; system according to its strength?  Can we not wring from it a calibrated
</em><br>
<em>&gt; probability of 99%?
</em><br>
<p>99% probability of success _at a particular, specified task_, maybe
<br>
so. Self-improvement isn't a particular, specified task though.
<br>
<p><em>&gt; I am still trying to figure out the answers myself.  What I do not
</em><br>
<em>&gt; understand is your confidence that there is no answer.
</em><br>
<p>Well, I originally had the impression you believed it would be
<br>
possible to create a seed AI which:
<br>
<p>- Would provably undergo hard takeoff (running on a supercomputer in a basement)
<br>
- Or else, would provably have e.g. a 99% probability of doing so
<br>
<p>I'm confident both of these are self-evidently wrong; the things we're
<br>
dealing with here are simply not in the domain of formal proof.
<br>
<p>Do I now understand correctly that your position is a slightly weaker
<br>
one: it would be possible to create a seed AI which:
<br>
<p>- In fact has a 99% chance of undergoing a hard takeoff, even though
<br>
we can't mathematically prove it has?
<br>
<p>If so, then I'm still inclined to think this is incorrect, but I'm not
<br>
as confident. My intuition says each step might have a 99% chance of
<br>
being successfully taken, but the overall process of hard takeoff
<br>
would be .99^N; I gather your intuition says otherwise.
<br>
<p><em>&gt; My studies so
</em><br>
<em>&gt; far indicate that humans do these things very poorly
</em><br>
<p>Compared to what standard?
<br>
<p><em>&gt; yet because we can
</em><br>
<em>&gt; try, there must be some component of our effort that works, that
</em><br>
<em>&gt; reflects Bayes-structure or logic-structure or *something*.  At the
</em><br>
<em>&gt; least it should be possible to obtain huge performance increases over
</em><br>
<em>&gt; humans.
</em><br>
<p>Bear in mind that for any evidence we have to the contrary, human
<br>
ability at strongly recursive self-improvement is zero.
<br>
<p><em>&gt; Why should a system that works probabilistically, not be refinable to
</em><br>
<em>&gt; yield very low failure probabilities?  Or at least I may hope.
</em><br>
<p>I hope so too, but the refining has to be done by something other than
<br>
the system itself.
<br>
<p><em>&gt; But at least a volition-extrapolating FAI would refract through humans
</em><br>
<em>&gt; on the way to deciding which options our world will offer us, unlike
</em><br>
<em>&gt; natural selection or the uncaring universe.
</em><br>
<p>There may be something to be said for that idea, if it can actually be
<br>
made to work.
<br>
<p>- Russell
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10520.html">Eliezer S. Yudkowsky: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Previous message:</strong> <a href="10518.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>In reply to:</strong> <a href="10517.html">Eliezer S. Yudkowsky: "Re: Definition of strong recursive self-improvement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10520.html">Eliezer S. Yudkowsky: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10520.html">Eliezer S. Yudkowsky: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10525.html">Billy Brown: "RE: Definition of strong recursive self-improvement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10519">[ date ]</a>
<a href="index.html#10519">[ thread ]</a>
<a href="subject.html#10519">[ subject ]</a>
<a href="author.html#10519">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
