<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Human intelligence is obviously absurd</title>
<meta name="Author" content="Keith Henson (hkhenson@rogers.com)">
<meta name="Subject" content="Re: Human intelligence is obviously absurd">
<meta name="Date" content="2005-01-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Human intelligence is obviously absurd</h1>
<!-- received="Sat Jan 29 11:19:25 2005" -->
<!-- isoreceived="20050129181925" -->
<!-- sent="Sat, 29 Jan 2005 13:17:38 -0500" -->
<!-- isosent="20050129181738" -->
<!-- name="Keith Henson" -->
<!-- email="hkhenson@rogers.com" -->
<!-- subject="Re: Human intelligence is obviously absurd" -->
<!-- id="5.1.0.14.0.20050129121330.033651c0@pop.brntfd.phub.net.cable.rogers.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="41FB96EC.4000008@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Keith Henson (<a href="mailto:hkhenson@rogers.com?Subject=Re:%20Human%20intelligence%20is%20obviously%20absurd"><em>hkhenson@rogers.com</em></a>)<br>
<strong>Date:</strong> Sat Jan 29 2005 - 11:17:38 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10713.html">Martin Striz: "Re: Human intelligence is obviously absurd"</a>
<li><strong>Previous message:</strong> <a href="10711.html">Martin Striz: "artificial evolution of AI"</a>
<li><strong>In reply to:</strong> <a href="10706.html">Eliezer S. Yudkowsky: "Human intelligence is obviously absurd"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10713.html">Martin Striz: "Re: Human intelligence is obviously absurd"</a>
<li><strong>Reply:</strong> <a href="10713.html">Martin Striz: "Re: Human intelligence is obviously absurd"</a>
<li><strong>Reply:</strong> <a href="10721.html">Phil Goetz: "Neural darwinism (Re: Human intelligence is obviously absurd)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10712">[ date ]</a>
<a href="index.html#10712">[ thread ]</a>
<a href="subject.html#10712">[ subject ]</a>
<a href="author.html#10712">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 06:00 AM 29/01/05 -0800, you wrote:
<br>
<em>&gt;Suppose that humanity, instead of evolving intelligence on a hundred 
</em><br>
<em>&gt;trillion 200Hz synapses,
</em><br>
<p>Excuse me if I am preaching to the converted, but if you don't know Dr. 
<br>
Calvin's proposed mechanisms for how thinking occurs, it might be worth 
<br>
taking a look here:
<br>
<p><a href="http://williamcalvin.com/bk9/bk9ch2.htm">http://williamcalvin.com/bk9/bk9ch2.htm</a>
<br>
<p>The problem might not be as large as implied.
<br>
<p>Much of what Calvin thinks generates mental activity and eventually 
<br>
intelligence is in the communication mechanism between cortical &quot;elements,&quot; 
<br>
groups of roughly 100 cells in a bundle about 0.03 mm in diameter.  (There 
<br>
are detailed drawings in this URL)  I think some substantial number of 
<br>
cells operating in concert is required to get consistent activity.  The 
<br>
jitter in the firing of a single nerve cell may have to be reduced by 
<br>
putting a mess of them in parallel.   The synapse may just be too low a 
<br>
level, like a transistor in a processor chip.
<br>
<p>Essentially Calvin's model involves copying (with errors) of patterns of 
<br>
activity in the cortical elements, the spreading of these patterns over the 
<br>
cortical surface, and a full scale Darwinian evolutionary process being 
<br>
applied to the patterns with sub second time constants.
<br>
<p>The physical mechanism is that the sidewise branches of a column near the 
<br>
cortical surface inhibit columns out to 0.5 mm and excite those at that 
<br>
distance.  Thus if two columns spaced 0.5 mm start singing the same song, 
<br>
they will entrain others on a triangular grid at the intersections of 0.5 
<br>
mm circles.  This gives rise to a transitory spreading hexagonal pattern of 
<br>
activation.
<br>
<p>Now Calvin might not be right in how this works, but his is the only model 
<br>
I know of that proposes to explain our mental processes.  (Please provide 
<br>
pointers in this thread for me if you know of other models.  I would like 
<br>
to look at them.)
<br>
<p>I am not sure how much of this applies to AI.  Ultimately the same laws of 
<br>
physics apply to birds and aircraft.  Perhaps the same processes we use to 
<br>
get natural intelligence will be required for AI though implemented on 
<br>
different substrates.
<br>
<p>*If* Darwinian selection is an essential part of the process, then the 
<br>
emergence of intelligence is going to be inherently wasteful of 
<br>
computational resources.  If this feature of intelligence could be 
<br>
*demonstrated* as being essential, it might comfort those worried about a 
<br>
transcendent AI emerging on a 386 machine.
<br>
<p><em>&gt;had instead evolved essentially equivalent intelligence on a million 2 GHz 
</em><br>
<em>&gt;processors using slightly more efficient serial algorithms (my example 
</em><br>
<em>&gt;postulates a factor-of-ten efficiency improvement, no more).  Let's call 
</em><br>
<em>&gt;these alternate selves Humans.
</em><br>
<p>At a square cm per processor, this would be a ten meter on a side square of 
<br>
silicon.  This is about 100 times off my last wild guess of what it would 
<br>
take to implement human level processing or ten given your assumption.  At 
<br>
least the power bill would not be a line item in the national budget.  :-)
<br>
<p>Keith Henson
<br>
<p><em>&gt;Would anyone here dare to predict, in advance, that it was even 
</em><br>
<em>&gt;*theoretically possible* to achieve Human-equivalent intelligence on 200Hz 
</em><br>
<em>&gt;processors no matter *how* many of them you had?
</em><br>
<em>&gt;
</em><br>
<em>&gt;Even I wouldn't dare.  Trying my best to be conservative and to widen my 
</em><br>
<em>&gt;confidence interval, my guess is that I would guess 10KHz, or 1KHz given a 
</em><br>
<em>&gt;superintelligent programmer, and I would probably have the lowest guess in 
</em><br>
<em>&gt;the crowd - both because of my guess that intelligence doesn't require 
</em><br>
<em>&gt;much crunch, and because I knew to widen my confidence intervals.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Ben Goertzel would laugh at me, saying that Human-equivalent intelligence 
</em><br>
<em>&gt;carried out with one thousand sequential serial operations per second was 
</em><br>
<em>&gt;obviously impossible.  Perhaps Ben would suggest that I try writing code 
</em><br>
<em>&gt;that executed with a bound of ten thousand sequential serial operations, 
</em><br>
<em>&gt;to get a feel for how restrictive that limit was.
</em><br>
<em>&gt;
</em><br>
<em>&gt;And if you suggested two hundred serial instructions per second - pfft! 
</em><br>
<em>&gt;Now you're just being silly, they would say; and while I might credit you 
</em><br>
<em>&gt;for fearless audacity, I probably wouldn't defend you, lest I be tarred 
</em><br>
<em>&gt;with the same brush.  Like the reaction you might get if you suggested 
</em><br>
<em>&gt;that intelligence could run on a 286, or use less than 4KB of RAM, or be 
</em><br>
<em>&gt;produced by natural selection.
</em><br>
<em>&gt;
</em><br>
<em>&gt;If any computational neurobiologists were present, they might even be able 
</em><br>
<em>&gt;to provide a quantitative mathematical argument, showing that some of the 
</em><br>
<em>&gt;basic algorithms known to be used in Human neurobiology intrinsically 
</em><br>
<em>&gt;required more than ten thousand serial steps per second.  So too did Lord 
</em><br>
<em>&gt;Kelvin prove by quantitative calculation that the Sun could not have 
</em><br>
<em>&gt;burned for more than a few tens of millions of years.
</em><br>
<em>&gt;
</em><br>
<em>&gt;One of the great lessons of history is that &quot;absurd&quot; is not a scientific 
</em><br>
<em>&gt;argument.  The future is usually &quot;absurd&quot; relative to the past.  Reality 
</em><br>
<em>&gt;is very tightly restrained in the kinds of absurdity it presents you with; 
</em><br>
<em>&gt;the human history of the 20th century might be absurd from the perspective 
</em><br>
<em>&gt;of the 19th century, but not one of those absurdities violated the law of 
</em><br>
<em>&gt;conservation of momentum.  Even so, &quot;absurd&quot; is not good evidence because 
</em><br>
<em>&gt;of the historical observation that the answers we now know were &quot;absurd&quot; 
</em><br>
<em>&gt;to people who didn't grow up with our background assumptions.  &quot;Obvious&quot; 
</em><br>
<em>&gt;is often wrong and &quot;absolutely certain&quot; isn't remotely close to 1.0 
</em><br>
<em>&gt;calibration.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Widen the bounds of your confidence interval.  Spread the wings of your 
</em><br>
<em>&gt;probability distribution, and fly.
</em><br>
<em>&gt;
</em><br>
<em>&gt;--
</em><br>
<em>&gt;Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt;Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10713.html">Martin Striz: "Re: Human intelligence is obviously absurd"</a>
<li><strong>Previous message:</strong> <a href="10711.html">Martin Striz: "artificial evolution of AI"</a>
<li><strong>In reply to:</strong> <a href="10706.html">Eliezer S. Yudkowsky: "Human intelligence is obviously absurd"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10713.html">Martin Striz: "Re: Human intelligence is obviously absurd"</a>
<li><strong>Reply:</strong> <a href="10713.html">Martin Striz: "Re: Human intelligence is obviously absurd"</a>
<li><strong>Reply:</strong> <a href="10721.html">Phil Goetz: "Neural darwinism (Re: Human intelligence is obviously absurd)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10712">[ date ]</a>
<a href="index.html#10712">[ thread ]</a>
<a href="subject.html#10712">[ subject ]</a>
<a href="author.html#10712">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
