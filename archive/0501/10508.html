<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Definition of strong recursive self-improvement</title>
<meta name="Author" content="Russell Wallace (russell.wallace@gmail.com)">
<meta name="Subject" content="Re: Definition of strong recursive self-improvement">
<meta name="Date" content="2005-01-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Definition of strong recursive self-improvement</h1>
<!-- received="Sun Jan  2 00:09:20 2005" -->
<!-- isoreceived="20050102070920" -->
<!-- sent="Sun, 2 Jan 2005 07:09:17 +0000" -->
<!-- isosent="20050102070917" -->
<!-- name="Russell Wallace" -->
<!-- email="russell.wallace@gmail.com" -->
<!-- subject="Re: Definition of strong recursive self-improvement" -->
<!-- id="8d71341e050101230936e06ac3@mail.gmail.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="41D77D1F.2060402@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Russell Wallace (<a href="mailto:russell.wallace@gmail.com?Subject=Re:%20Definition%20of%20strong%20recursive%20self-improvement"><em>russell.wallace@gmail.com</em></a>)<br>
<strong>Date:</strong> Sun Jan 02 2005 - 00:09:17 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="10509.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Previous message:</strong> <a href="10507.html">Eliezer S. Yudkowsky: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>In reply to:</strong> <a href="10507.html">Eliezer S. Yudkowsky: "Re: Definition of strong recursive self-improvement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10509.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10509.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10511.html">Mike Williams: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10513.html">Eliezer S. Yudkowsky: "Re: Definition of strong recursive self-improvement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10508">[ date ]</a>
<a href="index.html#10508">[ thread ]</a>
<a href="subject.html#10508">[ subject ]</a>
<a href="author.html#10508">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Sat, 01 Jan 2005 22:48:31 -0600, Eliezer S. Yudkowsky
<br>
&lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Definition%20of%20strong%20recursive%20self-improvement">sentience@pobox.com</a>&gt; wrote:
<br>
<em>&gt; It seems to me that you have just proved that Marcus Hutter's AIXI can
</em><br>
<em>&gt; be no smarter than a human
</em><br>
<p>AIXI can't be any smarter than a rock, if run on physically feasible
<br>
hardware. There's a reason AIXI et al are PDFware rather than running
<br>
code.
<br>
<p><em>&gt; when AIXI could tear apart a human like
</em><br>
<em>&gt; tinfoil. We can specify computations which no human mind nor physically
</em><br>
<em>&gt; realizable computer can run, yet which, if they were computed, would
</em><br>
<em>&gt; rule the universe.
</em><br>
<p>It's easy to say &quot;tear apart a human like tinfoil&quot; and &quot;would rule the
<br>
universe&quot; - is there any stronger basis than intuition for believing
<br>
AIXI could compete with human performance at practical real-world
<br>
tasks even if it had an infinitely powerful computer to run on?
<br>
<p>Mind you, my claim isn't about what could be done in the limit of
<br>
infinite computing power; it's about what could be done with a mere
<br>
few million exaflops - about the limit of what we can expect to get
<br>
from nanotech supercomputers. With that sort of physically possible
<br>
hardware, the sort of proof by exhaustive search that AIXI et al rely
<br>
on is completely infeasible.
<br>
<p>But even if you had infinite computing power, the very notion of
<br>
formal proof relies on a formal specification, so the results could be
<br>
no better than said specification.
<br>
<p><em>&gt; I only partially understand - I am presently working on understanding -
</em><br>
<em>&gt; how humans write code without either simulating every step of every
</em><br>
<em>&gt; possible run of the program, nor employing contemporary slow
</em><br>
<em>&gt; theorem-proving techniques.  Nonetheless it is evident that we write
</em><br>
<em>&gt; code.  Your proof against recursive self-improvement, which denies even
</em><br>
<em>&gt; the first step of writing a single line of functioning code, is equally
</em><br>
<em>&gt; strong against the existence of human programmers.
</em><br>
<p>Not at all. It is, however, a proof that human programmers can't write
<br>
_guaranteed correct_ code. And indeed we see that is the case: code
<br>
written by human programmers is notoriously unreliable, must be
<br>
extensively tested before even partial confidence is placed in it, and
<br>
is never regarded as 100% trustworthy.
<br>
<p>An AI will be in the same boat when it tries to improve a program:
<br>
sure, it might get some modifications right, but because it can't be
<br>
100% sure, it will get some of them wrong. And if the program it's
<br>
trying to improve is itself, with no restrictions on what parts it can
<br>
modify, then some errors will be impossible to recover from, because
<br>
the ability they will degrade will be the ability to recover from
<br>
errors. (This isn't pure speculation - in the one case where it was
<br>
put to the test, we note that EURISKO ran into exactly the sort of
<br>
problems I describe.)
<br>
<p><em>&gt; I intend to comprehend how it is theoretically possible that humans
</em><br>
<em>&gt; should write code, and then come up with a deterministic or
</em><br>
<em>&gt; calibrated-very-high-probability way of doing &quot;the same thing&quot; or
</em><br>
<em>&gt; better.  It is not logically necessary that this be possible, but I
</em><br>
<em>&gt; expect it to be possible.
</em><br>
<p>I can explain that one for you. Humans write code the same way we do
<br>
other things: a combination of semi-formal reasoning and empirical
<br>
testing.
<br>
<p>By &quot;semi-formal reasoning&quot;, I mean where we think things along the
<br>
lines of &quot;A -&gt; B&quot;, ignoring the fact that in strict logic it would
<br>
read &quot;A &amp;!C &amp;!D &amp;!E ... -&gt; B&quot;, where C, D, E and an indefinitely long
<br>
list of other things we haven't thought of, could intervene to stop A
<br>
from working. We ignore C, D, E etc because if we had to stop to take
<br>
them all into account, we'd starve to death in our beds because we
<br>
couldn't prove it was a good idea to get up in the morning. In
<br>
practice, we're good enough at taking into account only those things
<br>
that are important enough to make a practical difference, that we can
<br>
survive in the real world.
<br>
<p>This is the &quot;frame problem&quot; of classical AI - how to get a computer to
<br>
reliably take into account those things that it needs to take into
<br>
account, and ignore those things that it's adequately safe to ignore.
<br>
Now, like you I believe it should be possible to solve the frame
<br>
problem well enough to get into the ballpark of human performance. But
<br>
it can't be possible to _absolutely_ solve it, because the whole
<br>
_point_ of the frame problem is that it's a problem of how to _give up
<br>
on_ the idea of _absolutely_ solving problems.
<br>
<p>And indeed, we find humans are fallible. We make mistakes. We come up
<br>
with plans that fail, we write programs that crash, we go &quot;oops, I
<br>
didn't think of that&quot;. And unlike our propensity to make mistakes in
<br>
arithmetic (which is just because our brains aren't designed for it),
<br>
our propensity to make mistakes in general problem solving is inherent
<br>
in the nature of things. An AI can't be infallible any more than we
<br>
can.
<br>
<p>Of course, an AI doesn't have to be infallible to be useful. An AI
<br>
that writes buggy code that still needs testing, could still be useful
<br>
if, for example, its code wasn't _as_ buggy as that produced by human
<br>
programmers.
<br>
<p>Until you look at recursive self-improvement... and note that if the
<br>
code you're modifying is yourself, then it's too late to go &quot;oops, I
<br>
didn't thought of that, I'll do a patch&quot;, when the bug has destroyed
<br>
your ability to do the patch.
<br>
<p>- Russell
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10509.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Previous message:</strong> <a href="10507.html">Eliezer S. Yudkowsky: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>In reply to:</strong> <a href="10507.html">Eliezer S. Yudkowsky: "Re: Definition of strong recursive self-improvement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="10509.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10509.html">Russell Wallace: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10511.html">Mike Williams: "Re: Definition of strong recursive self-improvement"</a>
<li><strong>Reply:</strong> <a href="10513.html">Eliezer S. Yudkowsky: "Re: Definition of strong recursive self-improvement"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#10508">[ date ]</a>
<a href="index.html#10508">[ thread ]</a>
<a href="subject.html#10508">[ subject ]</a>
<a href="author.html#10508">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:50 MDT
</em></small></p>
</body>
</html>
