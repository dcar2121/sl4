<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: META: Dangers of Superintelligence</title>
<meta name="Author" content="Chris Healey (chealey@unicom-inc.com)">
<meta name="Subject" content="Re: META: Dangers of Superintelligence">
<meta name="Date" content="2004-08-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: META: Dangers of Superintelligence</h1>
<!-- received="Sun Aug 29 14:51:26 2004" -->
<!-- isoreceived="20040829205126" -->
<!-- sent="Sun, 29 Aug 2004 16:50:35 -0400" -->
<!-- isosent="20040829205035" -->
<!-- name="Chris Healey" -->
<!-- email="chealey@unicom-inc.com" -->
<!-- subject="Re: META: Dangers of Superintelligence" -->
<!-- id="001601c48e09$f0552be0$0400a8c0@OBSIDIAN" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="META: Dangers of Superintelligence" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Chris Healey (<a href="mailto:chealey@unicom-inc.com?Subject=Re:%20META:%20Dangers%20of%20Superintelligence"><em>chealey@unicom-inc.com</em></a>)<br>
<strong>Date:</strong> Sun Aug 29 2004 - 14:50:35 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9768.html">Eliezer Yudkowsky: "Re: META: Dangers of Superintelligence"</a>
<li><strong>Previous message:</strong> <a href="9766.html">fatherjohn@club-corsica.com: "Re: META: Dangers of Superintelligence"</a>
<li><strong>Maybe in reply to:</strong> <a href="9754.html">Emil Gilliam: "META: Dangers of Superintelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9755.html">Eugen Leitl: "Re: Dangers of Superintelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9767">[ date ]</a>
<a href="index.html#9767">[ thread ]</a>
<a href="subject.html#9767">[ subject ]</a>
<a href="author.html#9767">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I found it constructive, in my own consideration of the AI-Box
<br>
experiments, to ask questions regarding the motives of a person likely
<br>
to find themselves in the role of an AI-jailer:
<br>
<p>1.  Why create an AI at all?  To stick it in a box and poke it a bit?
<br>
Presumably we want a safe AI so that we can utilize it for SOMETHING.
<br>
Is our aim to create and confine benevolent prisoner intelligences, or
<br>
to guard against human-stompers?
<br>
<p>2.  How much are you willing to sacrifice on your convictions?  If an
<br>
existentially catastrophic event was pending that would wipe out
<br>
humanity and was beyond our power to control, would you refrain from
<br>
releasing the AI?  
<br>
<p>What if the AI could provably demonstrate that our chances of survival
<br>
were 0.00001% without its assistance, and a comparitively whopping 3%
<br>
with its assistance?  50%?  As we continue to progress
<br>
technologically, how many of these risks will we fail to indentify?
<br>
How many of these would a superintelligent AI be likely to identify?
<br>
<p>3.  Reframing question 2, what if that truly-present existential risk
<br>
was another AGI, obviously of the human-stomping sort and not confined
<br>
by a jailer, which was exponentially converting the Earth into
<br>
paperclips?  Would you dig a fire-line in front of it, or nuke it?  Or
<br>
would you take the only chance you really had?  Would you wait until
<br>
100,000 people were dead, or 5 billion (maybe not long thereafter)?
<br>
What if it had not happened yet, but the AGI had identified, from
<br>
provided information, that another AGI team would have the means to
<br>
achieve takeoff in ~2 months with no FAI safeguards?
<br>
<p>4.  What is the real difference between &quot;friendly&quot; and &quot;Friendly&quot;?
<br>
Can you introspect that difference in a foreign mind through a VT100?
<br>
If it &quot;truly&quot; coverged on Friendly goal content (non-deceitfully), how
<br>
sure can we be that it's not an unstable convergence due to deep
<br>
structural issues?  It seems to be doing surgery with a spoon.
<br>
<p><pre>
---
If you're confining the AI irregardless of reality and you do not let
it out, then maybe the point shouldn't be that you're a good jailer. 
You've started with a volition of protecting the world from a threat,
you've composed a strategy, and finally you've turned yourself into
your own little paperclip-level-AI implementing your strategy
irregardless of it's continued ability to serve that volition.
This strategy produces a wide array of degenerate results.  It is
effectively useless for protecting us in any meaningful way. 
-Chris Healey
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9768.html">Eliezer Yudkowsky: "Re: META: Dangers of Superintelligence"</a>
<li><strong>Previous message:</strong> <a href="9766.html">fatherjohn@club-corsica.com: "Re: META: Dangers of Superintelligence"</a>
<li><strong>Maybe in reply to:</strong> <a href="9754.html">Emil Gilliam: "META: Dangers of Superintelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9755.html">Eugen Leitl: "Re: Dangers of Superintelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9767">[ date ]</a>
<a href="index.html#9767">[ thread ]</a>
<a href="subject.html#9767">[ subject ]</a>
<a href="author.html#9767">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:48 MDT
</em></small></p>
</body>
</html>
