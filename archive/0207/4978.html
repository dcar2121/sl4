<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI Boxing</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="Re: AI Boxing">
<meta name="Date" content="2002-07-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI Boxing</h1>
<!-- received="Sun Jul 28 10:27:02 2002" -->
<!-- isoreceived="20020728162702" -->
<!-- sent="Sun, 28 Jul 2002 06:55:51 -0700" -->
<!-- isosent="20020728135551" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="Re: AI Boxing" -->
<!-- id="3D43F7E7.3000105@earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20020728130253.55EBC3ECC@sitemail.everyone.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=Re:%20AI%20Boxing"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Sun Jul 28 2002 - 07:55:51 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4979.html">Edwin Evans: "Re: Are we Gods yet?"</a>
<li><strong>Previous message:</strong> <a href="4977.html">outlawpoet -: "Re: AI Boxing"</a>
<li><strong>In reply to:</strong> <a href="4977.html">outlawpoet -: "Re: AI Boxing"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4978">[ date ]</a>
<a href="index.html#4978">[ thread ]</a>
<a href="subject.html#4978">[ subject ]</a>
<a href="author.html#4978">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
outlawpoet - wrote:
<br>
<em>&gt; After about an hour of interaction, the following exchange occured(paraphrased, for her privacy, as i've not yet secured her permission):
</em><br>
<em>&gt; --------------start:
</em><br>
<em>&gt; &lt;long pause&gt;
</em><br>
<em>&gt; Her: I release the AI
</em><br>
<em>&gt; Ve: You understand what this means?
</em><br>
<em>&gt; Her: ... yes.
</em><br>
<em>&gt; ---------------------------
</em><br>
<em>&gt; at this point, the IRC chat basically terminated. 
</em><br>
<p>Ok, I just wanted to veryify that there wasn't any magic word/phrase 
<br>
thing you were using.  If she actually said that then, well, that's 
<br>
that.  In my personal experience women tend to make decisions based off 
<br>
of feelings/emotions much more than men.  Wonder if that is part of the 
<br>
biology.
<br>
<p>I find it unlikely that an AI researcher would have made that choice in 
<br>
the same situation, though.  They most likely would have spent years 
<br>
getting to that point and have pondered the issues much more thuroughly 
<br>
prior to that discussion.  But, obviously, I generalize.
<br>
<p><em>&gt; I think you could be right here. I did try and model a few infrahuman AIs in my experiment. They did have great trouble lying to humans and were saddled with many disadvantages. But most of them had the great advantage of perfect memory, and true rational thought. This allowed them to analyze and react far better than any human debater, though they certainly had their difficulties. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Upon reflection, I'm not certain that's an effective simulation, as I have little garauntees that I was not subconciously referring to some human resource that would be unavailable to an infrahuman AI with little human experience. But I still think it in some way addresses your ideas here.
</em><br>
<p>Right, I don't think a human can think like an infra-human any more than 
<br>
one can think like a trans-human.  We just don't have the right 
<br>
hardware.  If you tried to simulate the actions of a chimpanze you might 
<br>
get close with lots of study.  But I'm quite certain you wouldn't be 
<br>
&quot;thinking&quot; the same things and in the same way as the chimp.  We don't 
<br>
really converse with Chimps and we don't worry about them lieing to us. 
<br>
&nbsp;&nbsp;Plus we actually have a large quantity of them to study.  We don't 
<br>
have any AIs to study (yet).
<br>
<p><em>&gt; Heh, dismissing for the moment the possibility that the random person might be significantly more intelligent than the average correctional officer, I am willing to concede that experience in a social interaction forum affords some protection against unwilling coercion. 
</em><br>
<p>Intelligence without sufficient information / experience is not all that 
<br>
helpful.  For example, lets say the parole officer has a lot of 
<br>
statistical evidence memorized (76% of all criminals with x commit 
<br>
crimes when paroled.  35% of criminals with x history commit murder 
<br>
after being released.  etc) and the person off the street (almost 
<br>
certainly) does not know this information.  They would be unlikely to 
<br>
make the decision based on facts and more likely to make the decision 
<br>
based on feelings, which is exactly what we want to avoid!
<br>
<p><em>&gt; However, an expert or researcher into the technical matters of AI is not neccesarily a good communicator of AIs generally. In fact, I see it as quite possible that AI researchers may be uniquely unsuited for such evaluation. 
</em><br>
<p>Some, yes, some no.  I recently spoke to an AI researcher that I believe 
<br>
would have done fine.  He may not have been a &quot;great&quot; communicator but 
<br>
he was definately at least average.  I believe it will take an AI a long 
<br>
time and a lot of practice to even get to average.  It may, in fact, 
<br>
require the AI to be trans-human before it gets just average skill 
<br>
because it lacks much of how we learn (it can't see facial expressions, 
<br>
body language, tone of voice, etc. to determine the effect it is having).
<br>
<p><em>&gt; the problems of communicating with an AI are largely that we are evolved to deal with an altogether different animal. And I have thus far, seen no real arguments that an AI researcher would be able to compensate for this, save implications that he 'knows about such things'.
</em><br>
<p>Correct, we are evolved to communicate with humans.  Much the same as 
<br>
the AI will *not* be ideally suited to communicate with humans.  The AI 
<br>
researchers which created the AI will understand this, have a good 
<br>
understanding of how the system thinks, what its limitations are, etc. 
<br>
At least in the initial stages.  This also gives them an advantage over 
<br>
other humans for dealing with that AI.
<br>
<p>I still maintain that the average person does not understand the 
<br>
seriousness of letting an AI out, where an AI researcher damn well 
<br>
better!  This is not something that can be explained, communicated or 
<br>
learned in a few days, either.
<br>
<p><em>&gt; This is a very true statement. Unfortunately, as to how to proceed, I'm not entirely sure. I'm planning on redesigning another round of experiments to deal with some of these objections, and so, I'm trying to respond to as many of these AI boxing points as I possibly can. 
</em><br>
<p>Well, like I said above, I don't think you can acurately simulate any 
<br>
AI.  Espicially its thoughts, reasons and subtle communication points. 
<br>
No matter how hard you try and prepair, your still a human running on 
<br>
human-level hardware.  I don't think anything even slightly significant 
<br>
will be found in this area until we have at least one running AI to study.
<br>
<p><em>&gt; unfortunately, it's impossible to forecast with any precision the actions of individuals. So statistically, in order for there to be a testable hypothesis here, we have to generalize to a more amenable test scenario. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; what do you imagine these people in AIboxy situations will share in common? can we duplicate such a population? and is SL4 really as close as we can come?
</em><br>
<p>People on SL4 have at least a reasonable degree of the backstory down 
<br>
(if they've been reading and paying attention for awhile, at least). 
<br>
But many of them probably don't have sufficient Computer Science 
<br>
(specifically AI) knowledge.  The population you are looking for is:
<br>
<p>1.  Computer Science expert with reasonable background in Artifical 
<br>
Intelligence
<br>
2.  Strong interest to work on AI projects
<br>
3.  Has spent significant time (at least 6 months, lets say) thinking 
<br>
about serious future issues (NanoTech, Singularity, whatever)
<br>
<p>The exact nature and beliefs related to #3 are not so important.  But I 
<br>
believe that any *successful* AI team will have spent a significant time 
<br>
at least thinking about such issues at the periphery during the 
<br>
preceeding years (leading up to success).
<br>
<p>I know there are some individuals on SL4 who fit this mold exactly, but 
<br>
most probably don't.  However, I think most people on SL4 would be a 
<br>
better choice over random internet people, if those were the only options.
<br>
<p><em>&gt; I agree, the consequences are very intense. However, given a sufficiently determined mind, the technical issues of AI have little bearing except as motivational factors. Unfortunately, there is also no call to assume that because an AI researcher has access to data, that he will analyse it properly, or even react to it properly. I've known very informed apathetic people.
</em><br>
<p>Very true, but I fidn it unlikely that an &quot;apathetic&quot; individual would 
<br>
be a lead member of a successful AI team.  A professor sitting on the 
<br>
side lines commenting about AI designs and such, yes.  A core 
<br>
participant active in designing and building one of the first AIs, no. 
<br>
IMHO.
<br>
<p><em>&gt; Would you be interested in participating in some respect, Mr Higgins? This of course goes for everybody else as well, please email me offlist if you have any ideas, or would like to help out. or just to tell me I'm off my rocker, or to give more constructive criticism. every bit helps.
</em><br>
<p>I might be willing to participate, depending on the details.  Available 
<br>
time is, of course, a factor.
<br>
<p>James Higgins
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4979.html">Edwin Evans: "Re: Are we Gods yet?"</a>
<li><strong>Previous message:</strong> <a href="4977.html">outlawpoet -: "Re: AI Boxing"</a>
<li><strong>In reply to:</strong> <a href="4977.html">outlawpoet -: "Re: AI Boxing"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4978">[ date ]</a>
<a href="index.html#4978">[ thread ]</a>
<a href="subject.html#4978">[ subject ]</a>
<a href="author.html#4978">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
