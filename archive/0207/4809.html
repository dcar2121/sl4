<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI Options.</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: AI Options.">
<meta name="Date" content="2002-07-11">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI Options.</h1>
<!-- received="Thu Jul 11 03:05:36 2002" -->
<!-- isoreceived="20020711090536" -->
<!-- sent="Thu, 11 Jul 2002 02:50:51 -0400" -->
<!-- isosent="20020711065051" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: AI Options." -->
<!-- id="3D2D2ACB.3010203@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="000a01c22890$a8e55a40$f040da0c@mchsi.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20AI%20Options."><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu Jul 11 2002 - 00:50:51 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4810.html">Mike & Donna Deering: "Re: AI Options."</a>
<li><strong>Previous message:</strong> <a href="4808.html">Michael Anissimov: "Re: AI Options."</a>
<li><strong>In reply to:</strong> <a href="4807.html">Mike & Donna Deering: "AI Options."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4810.html">Mike & Donna Deering: "Re: AI Options."</a>
<li><strong>Reply:</strong> <a href="4810.html">Mike & Donna Deering: "Re: AI Options."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4809">[ date ]</a>
<a href="index.html#4809">[ thread ]</a>
<a href="subject.html#4809">[ subject ]</a>
<a href="author.html#4809">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Mike &amp; Donna Deering wrote:
<br>
<em> &gt; Lets look at the options for AI:
</em><br>
<em> &gt;
</em><br>
<em> &gt; 1.  We never develop AI, or so far into the future that it doesn't
</em><br>
<em> &gt; matter to any of us alive today, unless we are signed up for
</em><br>
<em> &gt; cryonics.
</em><br>
<p>You can eliminate this option.  A million years in the future is not so
<br>
far ahead that it &quot;doesn't matter&quot; to me.
<br>
<p><em> &gt; 2.  We develop AI of a non-sentient tool nature and implement the
</em><br>
<em> &gt; solutions using human level institutions, in other words we screw
</em><br>
<em> &gt; everything up.
</em><br>
<p>Equivalent to nondevelopment of AI.  Prune this branch too.
<br>
<p><em> &gt; 3.  We develop conscious sentient UAI, game over.
</em><br>
<p>This should be option 1.
<br>
<p><em> &gt; 4.  We develop conscious sentient FAI that thinks it shouldn't
</em><br>
<em> &gt; interfere with anyone's volition and we destroy ourselves while it
</em><br>
<em> &gt; stands by and advises us that this is not the most logical course of
</em><br>
<em> &gt; action.
</em><br>
<p>I can't see this happening under any self-consistent interpretation of
<br>
morality, and would tend to prune this branch entirely along with
<br>
similarly anthropomorphic branches like &quot;We develop FAI that decides it
<br>
wants animal sacrifices as proof of our loyalty.&quot;  Maybe some people
<br>
want to destroy themselves.  I want off Earth before that happens.
<br>
<p><em> &gt; 5.  We develop conscious sentient FAI that thinks it should keep us
</em><br>
<em> &gt; from destroying ourselves or each other.  This involves preserving
</em><br>
<em> &gt; each person's volition up to the point where it would be a threat to
</em><br>
<em> &gt; ourselves or someone else.  This involves maintaining the capability
</em><br>
<em> &gt; to control the volition of every being on the planet.  This involves
</em><br>
<em> &gt; taking over the world.  This involves maintaining a comfortable lead
</em><br>
<em> &gt; in intelligence over every other being on the planet.
</em><br>
(*)
<br>
<em> &gt; This involves limiting the intelligence advancement of all of us.
</em><br>
<em> &gt; Understandably, a limit that is suffiently far away is not of much
</em><br>
<em> &gt; practical effect, but we are still left in the philosophical position
</em><br>
<em> &gt;  compared with the AI, of pets.  In the future of non-biological
</em><br>
<em> &gt; intelligence, biologically derived and protected entities are little
</em><br>
<em> &gt; more than pets.
</em><br>
<p>I've marked with a (*) above the point where this extended chain of
<br>
reasoning breaks down.  If you assume that 1% of all computing resources
<br>
are allocated to the underlying intelligence of the substrate, and that
<br>
no other one entity has more than one millionth of all the wealth in the
<br>
universe, there's no need to limit the intelligence advancement of
<br>
others.  It is also a fallacious assumption that an intelligent
<br>
substrate must necessarily remain more intelligent than all other
<br>
parties; this could be the case but it could also be the case that an
<br>
absolute threshold of intelligence combined with the asymmetry in
<br>
physical access is sufficient to defend against any and all attacks.
<br>
<p>Building a real tree of all possible options would be very tough work,
<br>
and nobody has constructed one that I agree with as yet, but Mechanus
<br>
(on Anissimov's forum) has gotten a lot farther than this:
<br>
<p><a href="http://bjklein.com/sing/forum/topic.asp?TOPIC_ID=489">http://bjklein.com/sing/forum/topic.asp?TOPIC_ID=489</a>
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4810.html">Mike & Donna Deering: "Re: AI Options."</a>
<li><strong>Previous message:</strong> <a href="4808.html">Michael Anissimov: "Re: AI Options."</a>
<li><strong>In reply to:</strong> <a href="4807.html">Mike & Donna Deering: "AI Options."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4810.html">Mike & Donna Deering: "Re: AI Options."</a>
<li><strong>Reply:</strong> <a href="4810.html">Mike & Donna Deering: "Re: AI Options."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4809">[ date ]</a>
<a href="index.html#4809">[ thread ]</a>
<a href="subject.html#4809">[ subject ]</a>
<a href="author.html#4809">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
