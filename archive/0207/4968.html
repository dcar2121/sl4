<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI Boxing</title>
<meta name="Author" content="outlawpoet - (outlawpoet@stealth.hell.com)">
<meta name="Subject" content="Re: AI Boxing">
<meta name="Date" content="2002-07-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI Boxing</h1>
<!-- received="Sat Jul 27 17:43:01 2002" -->
<!-- isoreceived="20020727234301" -->
<!-- sent="Sat, 27 Jul 2002 13:30:04 -0700 (PDT)" -->
<!-- isosent="20020727203004" -->
<!-- name="outlawpoet -" -->
<!-- email="outlawpoet@stealth.hell.com" -->
<!-- subject="Re: AI Boxing" -->
<!-- id="20020727203005.089F43958@sitemail.everyone.net" -->
<!-- inreplyto="AI Boxing" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> outlawpoet - (<a href="mailto:outlawpoet@stealth.hell.com?Subject=Re:%20AI%20Boxing"><em>outlawpoet@stealth.hell.com</em></a>)<br>
<strong>Date:</strong> Sat Jul 27 2002 - 14:30:04 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4969.html">outlawpoet -: "Re: Are we Gods yet?"</a>
<li><strong>Previous message:</strong> <a href="4967.html">outlawpoet -: "Re: AI boxing"</a>
<li><strong>Maybe in reply to:</strong> <a href="4935.html">Justin Corwin: "AI Boxing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4972.html">James Higgins: "Re: AI Boxing"</a>
<li><strong>Reply:</strong> <a href="4972.html">James Higgins: "Re: AI Boxing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4968">[ date ]</a>
<a href="index.html#4968">[ thread ]</a>
<a href="subject.html#4968">[ subject ]</a>
<a href="author.html#4968">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
THanks for the reply James, I reply to a couple of the points below..
<br>
<p><p>--- James Higgins &lt;<a href="mailto:jameshiggins@earthlink.net?Subject=Re:%20AI%20Boxing">jameshiggins@earthlink.net</a>&gt; wrote:
<br>
<em>&gt;Justin Corwin wrote:
</em><br>
<em>&gt;&gt; On a related note, I believe this experiment can be generalized to most 
</em><br>
<p><em>&gt;&gt; humans, and should be seen as applicable even to highly intelligent and 
</em><br>
<p><em>&gt;&gt; prepared individuals, as some of these people were, and I think this 
</em><br>
<p><em>&gt;&gt; illustrates some universal principles.
</em><br>
<p>I would like to clarify my belief here. I did not mean to imply that the 
<br>
pattern of conversation, or the method of convincing could be generalized 
<br>
to most humans. The problem with humans as voluntary jailers is that most 
<br>
human have flaws or 'hooks' in their thinking. Even thinking that they've 
<br>
mulled over and formalized. An AI(or in this case, me roleplaying an AI) 
<br>
can seek out and grab those hooks in whatever kind of conversaiton you have 
<br>
with it, and exploit them. The most convincing kinds of hooks can be used 
<br>
against each person, such as their logical flaws in religion, or philosophy, 
<br>
or even AI perspective. It is this process that I believe can be generalized 
<br>
from this series of experiments. Humans are too bad at thinking to accurately 
<br>
defend against a determined and rational opponnent, particularly an unFriendly, 
<br>
lying, transhuman one. 
<br>
<p>And another point I think got lost here, but that 
<br>
the original aim was not to show how cool I am, or how many convinces I 
<br>
can bag, but that the AI Box theory is not sound. Humans cannot determine 
<br>
accurately how Friendly or unFriendly an AI is. Humans can, as it was demonstrated 
<br>
in my experiment, leave an AI in jail just by not listening, or not interacting, 
<br>
but that defeats the whole stated purpose of the AI Box, which is a testing 
<br>
ground for the AI to be released. 
<br>
<p><em>&gt;
</em><br>
<em>&gt;I agree that this experiment could be generalized to most humans, but I 
</em><br>
<p><em>&gt;do not believe it is applicable to highly intelligent &amp; prepared 
</em><br>
<em>&gt;individuals.  
</em><br>
You said &quot;only 2 were transhumanists&quot; and &quot;1 could be 
<br>
<em>&gt;considered a nascent 
</em><br>
Singularitarian&quot;.  Therefor your sample may be 
<br>
<em>&gt;indicitive of the general 
</em><br>
population but not of transhumanists, 
<br>
<em>&gt;singularitians or AI researchers 
</em><br>
(the most important).
<br>
<p>Most of the participants were quite intelligent. I would say none much over 
<br>
3 sigmas above the mean, but very intelligent on the whole compared to the 
<br>
base population. Also, these people realized what they were getting into. 
<br>
They took the experiment seriously, and reacted seriously. In fact, some 
<br>
of the hardest convinces I had were from people far removed from the transhumanist 
<br>
arena. One lady actually wanted to destroy the AI, not just leave it in 
<br>
jail, when i finally convinced her to let ve out, she did so, then realized 
<br>
what she had done, burst into tears, and demanded I destroy the log of her 
<br>
doing so. 
<br>
<p>On a less emotional note, as I noted earlier, 22 of the involved 
<br>
claimed beforehand that nothing could make them let the AI out. 4 said they 
<br>
would evaluate the AI in the interview and decide for themselves. None were 
<br>
predisposed to simply let it out. 
<br>
<p><em>&gt;
</em><br>
<em>&gt;The vast majority of the population is not equiped for such a challenge. 
</em><br>
<p><em>&gt;  To make an intelligent decision in this one would, at least, need to 
</em><br>
<p><em>&gt;understand the concepts of NanoTechnology &amp; The Singularity.  If they 
</em><br>
<p><em>&gt;don't truly understand, or believe in, the potential threat they may 
</em><br>
<em>&gt;arbitrarily 
</em><br>
release the AI.  Even if NanoTech &amp; The Singularity were 
<br>
<em>&gt;impossible, a 
</em><br>
transhuman AI could still wreak havoc on the Internet. 
<br>
<em>&gt;You don't even 
</em><br>
qualify the level of Computer Science knowledge of most 
<br>
<em>&gt;participants (prefering 
</em><br>
to specify their religous beliefs in detail, 
<br>
<em>&gt;which (IMHO) is far less 
</em><br>
important).
<br>
<p>As I mentioned in my first email, I qualified the dangers and relevant technologies 
<br>
for these people, telling them why the AI was boxed, and the kinds of things 
<br>
it might be able to do. 
<br>
<p>All of the participants communicated via IRC, 
<br>
none were newcomers to computers. 10 worked in Information Technology. 2 
<br>
did not believe AI was possible, but accepted it for the purposes of the 
<br>
interview. All had some time to prepare for the interview, (in an interesting 
<br>
twist one was doing research on AI, came across Yudkowsky's site, and noticed 
<br>
the AI-boxing protocol) 
<br>
<p>I quantify the religious beliefs in detail becuase 
<br>
that was the factor that overridingly decided how long of a convince this 
<br>
would be. In general, it came down to moral beliefs. So the religion or 
<br>
beliefs of people tended to decide in general how people went, what they 
<br>
wanted to talk about, and how they were convinced. I think within a single 
<br>
person Computer Science Knowledge could be the deciding factor, rather than 
<br>
religious beliefs, but that has not been my experience. 
<br>
<p><p><em>&gt;
</em><br>
<em>&gt;You also don't specify the conviction of the participants.  Were they 
</em><br>
<p><em>&gt;just playing along because they thought it was fun or had some excess 
</em><br>
<p><em>&gt;time?  Did they take this seriously?
</em><br>
<p>I'm sure most did not take it as seriously as I did, but most tried their 
<br>
best to be objective, 14 did research before hand(of what level I did not 
<br>
inquire) Most had strong feelings on the subject. 
<br>
<p>I would also like to 
<br>
make some things clear. In many cases you seem to refer to these people 
<br>
as if they're of no consequence, or inapplicable to the problem. These were 
<br>
intelligent, vibrant people who think things through, know about lots of 
<br>
things, and care a great deal. They're internet people (else how would I 
<br>
find them) and they were the most interested ones I could find. Don't let 
<br>
my quantifying the differences fool you, they were on the whole, more alike 
<br>
than unalike. and the picture is one of above average intelligence, interest 
<br>
in the issues at hand, and computer savvy, if not proffessional. But as 
<br>
the saying goes, sheep have no trouble telling each other apart.
<br>
&nbsp;
<br>
And when I say only 2 were transhumanist, I mean REAL transhumanists, 
<br>
with the desire to transcend themselves, upwing beliefs, rational objectors. 
<br>
Hell, most of the people I interviewed would be more than comfortable on 
<br>
the Extropians list, or Transhumantech. They're just not yet ready to go 
<br>
all the way. (or at least as far as I have, i hesitate to call that all 
<br>
the way, even if ELiezer doesn't see anything past sl4.)
<br>
<p><em>&gt;
</em><br>
<em>&gt;As for Eliezer's rules I do agree that the 2 hour minimum is not 
</em><br>
<em>&gt;realistic.
</em><br>
<p>I saw Eliezer's rule to minimum at 2 hours as a hack to try and approximate 
<br>
transhuman arguing ability, but I felt the experiment would be more meaningful 
<br>
without it. I suppose it might have changed my mind if more thatn one person 
<br>
had just 'hung up' on me. 
<br>
<em>&gt;
</em><br>
<em>&gt;James Higgins
</em><br>
<p>Interesting stuff james, Thanks for the reply, again.
<br>
<p>Justin Corwin
<br>
<a href="mailto:outlawpoet@hell.com?Subject=Re:%20AI%20Boxing">outlawpoet@hell.com</a>
<br>
<p>&quot;One more time....Celebrate...One more time.&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;~Daft Punk
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4969.html">outlawpoet -: "Re: Are we Gods yet?"</a>
<li><strong>Previous message:</strong> <a href="4967.html">outlawpoet -: "Re: AI boxing"</a>
<li><strong>Maybe in reply to:</strong> <a href="4935.html">Justin Corwin: "AI Boxing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4972.html">James Higgins: "Re: AI Boxing"</a>
<li><strong>Reply:</strong> <a href="4972.html">James Higgins: "Re: AI Boxing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4968">[ date ]</a>
<a href="index.html#4968">[ thread ]</a>
<a href="subject.html#4968">[ subject ]</a>
<a href="author.html#4968">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
