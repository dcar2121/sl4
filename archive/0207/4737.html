<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Suggested AI-Box protocol &amp; AI-Honeypots</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Suggested AI-Box protocol &amp; AI-Honeypots">
<meta name="Date" content="2002-07-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Suggested AI-Box protocol &amp; AI-Honeypots</h1>
<!-- received="Sat Jul 06 20:29:49 2002" -->
<!-- isoreceived="20020707022949" -->
<!-- sent="Sat, 06 Jul 2002 19:47:58 -0400" -->
<!-- isosent="20020706234758" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Suggested AI-Box protocol &amp; AI-Honeypots" -->
<!-- id="3D2781AE.9030905@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="E17Qxn2-0005eo-00@freyja.in-orbit.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Suggested%20AI-Box%20protocol%20&amp;%20AI-Honeypots"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Jul 06 2002 - 17:47:58 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4738.html">Mike & Donna Deering: "Re: AI Jailer."</a>
<li><strong>Previous message:</strong> <a href="4736.html">Michael Warnock: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<li><strong>In reply to:</strong> <a href="4736.html">Michael Warnock: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4745.html">Tomaz Kristan: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4737">[ date ]</a>
<a href="index.html#4737">[ thread ]</a>
<a href="subject.html#4737">[ subject ]</a>
<a href="author.html#4737">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michael Warnock wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; This seems very complete.  Most of my ideas for how the AI party has
</em><br>
<em>&gt; been successful are not within the protocol.  The only remaining notion
</em><br>
<em>&gt; I think to be reasonable is that Eli is convincing the Gatekeeper party
</em><br>
<em>&gt; that letting him out now increases the chances of real FAI by increasing
</em><br>
<em>&gt; the thought and self-doubt surrounding AI-Boxes and Friendliness.
</em><br>
<em>&gt; This too may be judged to be using a real-world Eli tactic such as a
</em><br>
<em>&gt; secret bribe, which breaks the first of the AI protocols.
</em><br>
<p>That's why the protocol says &quot;no real-world *material* stakes&quot; - bribes 
<br>
are ruled out but not other means of convincing the Gatekeeper to break 
<br>
character, if you can manage to do so.
<br>
<p><em>&gt; I tend to think that an AI-Box with a single perimeter and an on/off
</em><br>
<em>&gt; switch would not do the job intended, but possibly influence the AI's
</em><br>
<em>&gt; perception of our own friendliness negatively.
</em><br>
<p>That shouldn't make any difference.  We are not dealing with an evolved 
<br>
retaliator.
<br>
<p><em> &gt; On the other hand I
</em><br>
<em>&gt; think it's important that we not run programs on any substrate capable
</em><br>
<em>&gt; of unaided self-extension at the hardware level without extremely well
</em><br>
<em>&gt; thought-out safe guards.
</em><br>
<p>There are no safeguards against a transhuman.  If you want to win, do it 
<br>
before then.
<br>
<p><em>&gt; If a transhuman AI could exist purely as software on more conventional
</em><br>
<em>&gt; hardware (barring quantum mechanisms in our own brains or extremely
</em><br>
<em>&gt; high requirements), then it should be possible to maintain hardware parity
</em><br>
<em>&gt; that is not under the influence of said software.  This parity hardware
</em><br>
<em>&gt; would allow inspection of any and all code.  The usefullness of this,
</em><br>
<em>&gt; as I see it, depends upon how algorythmic vs connectionist the program
</em><br>
<em>&gt; is and its level of complexity in general.  Even given the most readable
</em><br>
<em>&gt; code, however, a transhuman AI, which has spent most of its existance
</em><br>
<em>&gt; working with code, including its own, would likely be able to slip one
</em><br>
<em>&gt; by even the most expert panel of code-reviewers.
</em><br>
<p>Yep.
<br>
<p><em>&gt; So what else could be done along these lines?  How about the
</em><br>
<em>&gt; equivalent of a honey-pot.  In a traditional *N*X honeypot a
</em><br>
<em>&gt; system image is placed within a host environment to which it has
</em><br>
<em>&gt; no access or even indication that it is there.  All outside communication
</em><br>
<em>&gt; is routed to the contained system image.  Checks are made of the
</em><br>
<em>&gt; integrity of the image to alert of the hoped-for intrusion.  Much can
</em><br>
<em>&gt; be learned about a particular cracker by tricking him into your
</em><br>
<em>&gt; honeypot.
</em><br>
<em>&gt; To produce the equivalent for a transhuman AI may prove
</em><br>
<em>&gt; impossible if it is able to harness superior hardware technology
</em><br>
<em>&gt; within its own perimeter.  Also, if an UAI deduced the existance
</em><br>
<em>&gt; of the honeypot it would likely pretend to respect the first
</em><br>
<em>&gt; boundary so as to be let out or wait for an appropriate time to
</em><br>
<em>&gt; blow through the first and attempt the second.  At least I think there
</em><br>
<em>&gt; is some possible use in this, though, if an UAI is let into what it
</em><br>
<em>&gt; thinks is the base level of reality and makes its character known,
</em><br>
<em>&gt; but is still contained.
</em><br>
<p>IMO it would be very easy for a transhuman AI (UAI or FAI) to deduce the 
<br>
existence of the honeypot.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4738.html">Mike & Donna Deering: "Re: AI Jailer."</a>
<li><strong>Previous message:</strong> <a href="4736.html">Michael Warnock: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<li><strong>In reply to:</strong> <a href="4736.html">Michael Warnock: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4745.html">Tomaz Kristan: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4737">[ date ]</a>
<a href="index.html#4737">[ thread ]</a>
<a href="subject.html#4737">[ subject ]</a>
<a href="author.html#4737">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
