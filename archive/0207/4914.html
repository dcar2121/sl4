<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Intelligence and wisdom</title>
<meta name="Author" content="Mitch Howe (mitch@iconfound.com)">
<meta name="Subject" content="Re: Intelligence and wisdom">
<meta name="Date" content="2002-07-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Intelligence and wisdom</h1>
<!-- received="Wed Jul 17 06:49:58 2002" -->
<!-- isoreceived="20020717124958" -->
<!-- sent="Tue, 16 Jul 2002 22:00:54 -0600" -->
<!-- isosent="20020717040054" -->
<!-- name="Mitch Howe" -->
<!-- email="mitch@iconfound.com" -->
<!-- subject="Re: Intelligence and wisdom" -->
<!-- id="001f01c22d46$8d3d9860$0100a8c0@mitch" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJAEOHCMAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Mitch Howe (<a href="mailto:mitch@iconfound.com?Subject=Re:%20Intelligence%20and%20wisdom"><em>mitch@iconfound.com</em></a>)<br>
<strong>Date:</strong> Tue Jul 16 2002 - 22:00:54 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4915.html">Christian L.: "RE: Intelligence and wisdom"</a>
<li><strong>Previous message:</strong> <a href="4913.html">Ben Goertzel: "RE: SL4 meets &quot;Pinky and the Brain&quot;"</a>
<li><strong>In reply to:</strong> <a href="4907.html">Ben Goertzel: "RE: Intelligence and wisdom"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4922.html">Gordon Worley: "Re: Intelligence and wisdom"</a>
<li><strong>Reply:</strong> <a href="4922.html">Gordon Worley: "Re: Intelligence and wisdom"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4914">[ date ]</a>
<a href="index.html#4914">[ thread ]</a>
<a href="subject.html#4914">[ subject ]</a>
<a href="author.html#4914">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I offer a narrower definition of wisdom that may prove useful to the
<br>
discussion of AI.
<br>
<p><p><p>Wisdom:  The consistency or degree to which the best decision is made based
<br>
on the information available and the goals of the decision maker.
<br>
<p><p><p>The key here is &quot;based on the information available.&quot;  By this definition,
<br>
teenage foolishness (foolishness = lack of wisdom) can only really be
<br>
attributed to things like raging hormones that cause them to act in ways
<br>
they know full well will move them farther away from their long or even
<br>
short term goals and values.  A male teenager who exposes himself, makes
<br>
catcalls, and shouts lewd statements in an attempt to persuade a female
<br>
passerby to have sex with him (sex being his goal, readily accepted from his
<br>
hardwiring) will probably meet with little success.  If he previously
<br>
learned from a credible source (experience is one, but not the only
<br>
possibility) that the most reliable means of seducing women generally begin
<br>
with cultivating a perception of emotional intimacy, then I would call this
<br>
teenager a fool.  But, if he does not have this useful tidbit of information
<br>
at his disposal, he may in fact be making the best possible decision with
<br>
the information he has available.  After all, if it is sex he is after, why
<br>
shouldn't he approach the topic directly?  If a woman did the same to he
<br>
would respond quite readily.  It just isn't logical to assume that some
<br>
roundabout ritual involving restaurants and boring conversations should be
<br>
required.  Such an unlearned teenager is not foolish - just clueless.  Now,
<br>
if he persists in using this approach without success over a long period of
<br>
time without attempting to figure out why it wasn't working, I would again
<br>
consider him a fool (on the meta level of &quot;decision making process for
<br>
making decisions&quot;)  ...unless his past offered clear reinforcement to the
<br>
idea that blind persistence pays.
<br>
<p><p><p>(--Can we have a pool Dad?
<br>
<p>--No.
<br>
<p>--Can we have a pool, Dad?
<br>
<p>--No.
<br>
<p>--Can we have a pool, Dad?
<br>
<p>--No.
<br>
<p>--Can we have a pool, Dad?
<br>
<p>--No.
<br>
<p>--Can we have a pool, Dad?
<br>
<p>--Fine! Just shut up!)
<br>
<p><p><p>Now, to bring AI into the picture, we can first take the horror of an
<br>
AI-induced existential disaster into consideration and decide whether the AI
<br>
was evil/good, wise/foolish, clueless/learned, or some combination of these.
<br>
In the oft-mentioned scenario where an AI converts the biosphere into
<br>
computronium in order to solve some paradox or other, we must ask ourselves:
<br>
<p><p><p>1) What was the AI's most powerful, overriding goal? Was this goal something
<br>
we would consider intrinsically good or evil?
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;This is the level at which I feel the morality of an AI must be defined.
<br>
If, for example, the AI's goal encouraged the expansion of its own
<br>
intelligence and disregarded the volition of other sentient intelligences, I
<br>
would call it evil.  (Others' opinions may vary.) If the cause of the
<br>
destruction is not found as an obvious outcome of a supergoal, then we must
<br>
move on to other possibilities.
<br>
<p><p><p>2) What information did the AI have at its disposal?
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;If for some strange reason the AI's &quot;upbringing&quot; included a severely
<br>
restricted curriculum that included copious practical knowledge of
<br>
nanotechnology and no awareness at all of human concepts of morality, then
<br>
we can, of course, not consider it foolish for destroying the earth in its
<br>
quest to solve a problem.  This is true even if the AI's supergoal was to
<br>
maximize Friendliness; the AI was acting based on the best information it
<br>
had, which was so utterly restricted as to totally stunt its concept of
<br>
Friendliness and turn it into a Golem.  (Note to self: encourage broad and
<br>
largely self-guided curriculum for seed AI)
<br>
<p><p><p>3) How intelligent was the AI?  Was its decision making process rushed by
<br>
external factors?
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;I'm going to use intelligence here as raw computational power and
<br>
memory - the capacity with which one can mull over complicated problems in
<br>
an in-depth way.  Many kinds of problems do not have obvious, elegant routes
<br>
to their own solutions -- or these routes may not be known to the mind
<br>
working on the problems (see #2).  Other problems, particularly of the
<br>
decision making variety, have a huge number of possible solutions that must
<br>
be internally explored to test for desirability (in terms of
<br>
values/supergoals -- see #1).  If, in our destructive scenario, the AI had
<br>
some externally imposed deadline for deciding whether or how to solve the
<br>
paradox, and if this deadline was too short for its level of intelligence to
<br>
evaluate the possible outcomes of a runaway intelligence-boosting program,
<br>
then this AI can not be considered foolish for making what amounts to a
<br>
rushed decision; the information it had on hand to work out a better
<br>
solution may have been unavailable due to time constraints. If the AI's
<br>
physical capacity (memory, etc) was too small to contain or solve the
<br>
problem regardless of allotted time, the AI was &quot;stumped&quot; -- and in this
<br>
case catastrophically &quot;stupid&quot; (unintelligent)  - but still not necessarily
<br>
foolish.  We would have to ask if or why it acted without being
<br>
appropriately certain that its action would have no serious negative
<br>
consequences.
<br>
<p><p><p>4) Did the AI's programming allow for random or irrational decision making?
<br>
<p>I'll admit that I am assuming, but I don't see any AI programmers valuing
<br>
Kirk's human side over Spock's vulcan logic to the point they are expressly
<br>
working irrational behavior into their designs.  But, in the event that such
<br>
programming did occur, whether intentional or otherwise, then such an AI
<br>
would be acting at times in ways that do not correlate with its own
<br>
goals/values -- even if these goals were good, even if it had adequate
<br>
information available, and even if it had the intelligence/time to make a
<br>
good decision.  This is either a broken, buggy, or intentionally dangerous
<br>
AI, and, by my definition, foolish.  I cannot think of any other situation
<br>
that would earn an AI this description.
<br>
<p><p><p><p><p>So, the only really foolish AI is one that is broken at the level of logical
<br>
continuity.  The degree to which a program is free from such breaks is,
<br>
therefore, the only real measure of wisdom.  Any two AIs that always act in
<br>
accordance with their own goals are wise - even maximally wise (call it
<br>
&quot;Wise&quot; with a capital &quot;w&quot;), since regardless of how stupid or uneducated one
<br>
Wise AI was compared to the other, they would both be making the best
<br>
possible decisions based on the information available to them.
<br>
<p><p><p>So, greater intelligence would not equate at all to greater wisdom in an AI.
<br>
By the same token, lesser intelligence would not equal foolishness.  But
<br>
these statements do not mean that the actions of one Wise AI would always be
<br>
as desirable to an observer as the actions of another Wise AI.  The obvious
<br>
reason is that these entities may not share the same supergoals, and that
<br>
the subjective observer will prefer one goal over another.  But also
<br>
important to consider is the fact that one Wise AI might have more
<br>
information to work with than the other (age is irrelevant, merely
<br>
accumulated knowledge), allowing it to make better decisions.  And, finally,
<br>
one Wise AI may just be clearly smarter than the other - and in a universe
<br>
filled with big, time-sensitive questions, this means that one Wise AI would
<br>
be getting &quot;stumped&quot; or making rushed decisions more often than the other.
<br>
Greater intelligence should thus be something you fear in AI's that are
<br>
unwise, evil (according to your own definition), or pressured to act in
<br>
spite of inadequate education.  Greater intelligence should be something you
<br>
approve of in AI's that are Wise and good (by your definition), regardless
<br>
of whether or not the AI is pressured to act and regardless of how educated
<br>
it is, since greater intelligence cannot help but increase the odds of the
<br>
AI making a well thought-out decision (one that is the result of a completed
<br>
internal review using all the information available to it).
<br>
<p><p><p>I recognize that my definition of wisdom does not match up well to that of
<br>
many others, but I feel that it is fairer than most by having clearly
<br>
defined criteria.  More traditional definitions are too easily accompanied
<br>
by shields of mystery that discourage the outsider from finding out whether
<br>
one's claim to wisdom is based on anything substantial.  The alleged lone
<br>
guru at the top of a mountain may be obviously old, and is likely an
<br>
accomplished climber.  But neither age nor goat-like dexterity ought to
<br>
instill any confidence in the man's accumulated knowledge, intelligence, or
<br>
values.  I would, in fact, find it quite reasonable to assume that he is
<br>
lazy or indifferent if he spends all that time on the mountain (he would
<br>
have to climb back up if he left), and unlikely to be up on scientific
<br>
knowledge, current events, or social situations.  He would probably claim
<br>
that his wisdom comes from some kind of internal enlightenment -- that all
<br>
these other things I think lend credibility to someone's solution are
<br>
nothing compared to his spiritual awareness.  Such awareness is, of course,
<br>
a purely internal and subjective phenomenon indistinguishable from outright
<br>
lie to anyone but him (and maybe even him!).  In point of fact, he may cluck
<br>
like a chicken and be perfectly Wise by my definition merely because he
<br>
knows almost nothing but always acts in accordance with his goals based on
<br>
the paltry information he has available.  But I am in no way arguing that
<br>
wisdom should ever be sufficient of itself to gain the trust of me or anyone
<br>
else.
<br>
<p><p><p>I think the traditional concept of &quot;street smarts&quot; as something that can
<br>
only come from experience (and is never the same as &quot;book smarts&quot;) is silly
<br>
as the clucking guru for similar reasons.  I think it is perfectly possible
<br>
for someone to gain knowledge of the street through a third party, such as a
<br>
particularly insightful book.  Books have a reputation for leaving out
<br>
certain details that might ultimately matter in a dark alley, but even I
<br>
know that I should never expect to win at Three-Card Monty, despite never
<br>
having been in a position to try.
<br>
<p><p><p><p><p>QUIZ:
<br>
<p><p><p>If you wanted the answer to some really deep philosophical question, which
<br>
of the following would you trust to come up with the best answer for you
<br>
personally?
<br>
<p><p><p>a)Yourself, after 20 minutes of reflection.
<br>
<p>b)Yourself, after 80 years of reflection.
<br>
<p>c)Some guru on a mountain (or analogue in any religion of your choice)
<br>
<p>d)An SI possessing all of the knowledge you do currently, plus a lot more,
<br>
after what amounts to 10 human years of reflection. (10 actual seconds)
<br>
<p>e)An SI possessing all of the knowledge you do currently, plus a lot more,
<br>
after what amounts to 5 million human years of reflection. (57.9 actual
<br>
days)
<br>
<p><p><p>You were probably torn between either &quot;b&quot; or &quot;e&quot;, but the important thing is
<br>
that you preferred these to &quot;a&quot; or &quot;d&quot;.  (If you chose &quot;a&quot;, &quot;c&quot;, or &quot;d&quot; then
<br>
this entire discussion has probably seemed pointless to you and I apologize
<br>
for wasting your time.)  Time does matter to you, which really means that
<br>
the total amount of thought and knowledge put into the problem matters to
<br>
you (intelligence, available information).  If you chose &quot;b&quot;, you probably
<br>
believe that there is something about your own human experience that is
<br>
essential to the question that can never be appreciated by an SI of any
<br>
sophistication.  Reasons why this is probably not the case are discussions
<br>
for other occasions (many of which have already passed).  The correct answer
<br>
is &quot;e&quot; :)
<br>
<p><p><p>But on a more serious note, try this question:
<br>
<p><p><p>Suppose the entire planet was looking for an answer to &quot;What is the best
<br>
future for us?&quot;  Which of the following do you think would have the best
<br>
chance of coming up with the best answer for the greatest number of people?
<br>
<p><p><p>a)Yourself, after 20 minutes of reflection.
<br>
<p>b)Yourself, after 80 years of reflection.
<br>
<p>c)Some guru on a mountain (or analogue in any religion of your choice)
<br>
<p>d)Some committee of humans of your choice, after 20 years of reflection.
<br>
<p>e)An uploaded version of any of the above, with time factors increased to a
<br>
10x subjective level. (Same real time amounts).
<br>
<p>f)An SI possessing all of the knowledge you and this committee do currently,
<br>
plus a lot more, after what amounts to 10 human years of reflection. (10
<br>
actual seconds)
<br>
<p>g)An SI possessing all of the knowledge you and the entire human race do
<br>
currently, plus a lot more, after what amounts to 5 million human years of
<br>
reflection. (57.9 actual days)
<br>
<p><p><p>The correct answer is.
<br>
<p><p><p>Rats! 42 again!
<br>
<p><p><p>--Mitch Howe
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4915.html">Christian L.: "RE: Intelligence and wisdom"</a>
<li><strong>Previous message:</strong> <a href="4913.html">Ben Goertzel: "RE: SL4 meets &quot;Pinky and the Brain&quot;"</a>
<li><strong>In reply to:</strong> <a href="4907.html">Ben Goertzel: "RE: Intelligence and wisdom"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4922.html">Gordon Worley: "Re: Intelligence and wisdom"</a>
<li><strong>Reply:</strong> <a href="4922.html">Gordon Worley: "Re: Intelligence and wisdom"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4914">[ date ]</a>
<a href="index.html#4914">[ thread ]</a>
<a href="subject.html#4914">[ subject ]</a>
<a href="author.html#4914">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
