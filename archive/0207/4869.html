<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Intelligence and wisdom</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Intelligence and wisdom">
<meta name="Date" content="2002-07-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Intelligence and wisdom</h1>
<!-- received="Tue Jul 16 11:19:25 2002" -->
<!-- isoreceived="20020716171925" -->
<!-- sent="Tue, 16 Jul 2002 11:09:15 -0400" -->
<!-- isosent="20020716150915" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Intelligence and wisdom" -->
<!-- id="3D34371B.8060309@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="C7AC37DB-98C7-11D6-8A82-000A27B4DEFC@rbisland.cx" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Intelligence%20and%20wisdom"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Jul 16 2002 - 09:09:15 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4870.html">Eliezer S. Yudkowsky: "META: Re: AI Jailer."</a>
<li><strong>Previous message:</strong> <a href="4868.html">Gordon Worley: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<li><strong>In reply to:</strong> <a href="4863.html">Gordon Worley: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4872.html">Ben Goertzel: "RE: Intelligence and wisdom"</a>
<li><strong>Reply:</strong> <a href="4872.html">Ben Goertzel: "RE: Intelligence and wisdom"</a>
<li><strong>Reply:</strong> <a href="4884.html">James Higgins: "Re: Intelligence and wisdom"</a>
<li><strong>Maybe reply:</strong> <a href="4915.html">Christian L.: "RE: Intelligence and wisdom"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4869">[ date ]</a>
<a href="index.html#4869">[ thread ]</a>
<a href="subject.html#4869">[ subject ]</a>
<a href="author.html#4869">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Gordon Worley wrote:
<br>
<em>&gt; Q:  So, does intelligence equal wisdom?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; A:  It has become cliché on SL4 to say that intelligence does not equal 
</em><br>
<em>&gt; wisdom.  Many of us have been well aware of this for quite some time.  
</em><br>
<em>&gt; Please, avoid pointing this out unless the alternative is being drawn 
</em><br>
<em>&gt; and quartered (and maybe not even then).  This is also a rather silly 
</em><br>
<em>&gt; thing to say, since for all you know greater intelligence *does* equal 
</em><br>
<em>&gt; greater wisdom. With humans we get the opinion that the two are 
</em><br>
<em>&gt; uncorrelated, but the sample is too small to make non trivial factual 
</em><br>
<em>&gt; statements about greater intelligences (aside from the errors of 
</em><br>
<em>&gt; extrapolation).
</em><br>
<p>I don't *think* that intelligence equates to the characteristics we call 
<br>
&quot;wisdom&quot; for all minds in general, but in humans, it looks like if you 
<br>
throw more intelligence and knowledge at the brain than it is 
<br>
evolutionarily prepared to handle, it turns wise.  Evolution is 
<br>
accustomed to puppeteering most of the gaussian curve for intelligence, 
<br>
but it doesn't know how to control people who are actively aware of 
<br>
evolutionary psychology.  People like that weren't around in the 
<br>
environment of evolutionary adaptedness.  Arguably the entire presence 
<br>
of cultural knowledge and historical book-learning about human nature, 
<br>
as opposed to sung legends, is a condition which evolution is not 
<br>
prepared to handle.
<br>
<p>An example:  Evolution is set up to create altruistic justifications for 
<br>
selfish actions; it was an evolutionary advantage, in the ancestral 
<br>
environment, to have altruistic principles for PR purposes but not live 
<br>
up to them.  (Oversimplified, I know...)  If you take those same 
<br>
adaptations in an environment where there exists explicit cultural 
<br>
knowledge of the way in which evolution puppets humans, a human with 
<br>
sufficient native computational ability at reflectivity (self-awareness) 
<br>
can use that cultural knowledge to perceive evolution's puppet strings 
<br>
and cut them.  From our perspective, the ability of evolution to distort 
<br>
our altruistic principles to selfish ends is reduced.  Evolution relies 
<br>
on us believing certain things that aren't true in order to puppet us. 
<br>
If you have enough intelligence and some of the right knowledge, it 
<br>
supervenes to cut those strings.  That's wisdom.
<br>
<p>I can't prove that this is a necessary chain of events given sufficient 
<br>
intelligence.  It might be so, especially given transhuman intelligence, 
<br>
but I can't prove it.  There may be a sensitive dependency on the 
<br>
initial conditions of a conscious decision to prefer altruism to 
<br>
selfishness, live up to your principles, care about the truth, and 
<br>
prefer rationality to rationalization.  But given those decisions - and 
<br>
I don't think they need to be particularly well-enforced to begin with - 
<br>
increased intelligence should supervene to produce increased wisdom in 
<br>
humans.
<br>
<p>Evolution is set up to turn our own intelligence against itself by 
<br>
rationalizing instead of ratiocinating - a particularly perverted aspect 
<br>
of being human, I've always felt - and this may not change, much, across 
<br>
most of the gaussian curve.  Increased computational abilities may not 
<br>
result in an increase in real intelligence, if better ratiocination is 
<br>
simply matched by better rationalization.  But there's no reason why 
<br>
evolution would be set up to successfully turn transhuman intelligence 
<br>
against itself, or to operate against a human mind actively aware of 
<br>
evolution and fighting it.  Neither of these causes are present in an 
<br>
ancestral environment.
<br>
<p>Some people have only met &quot;intelligent&quot; people whose rationalization 
<br>
matched pace with their ratiocination, and they come away with the idea 
<br>
that intelligence is just a greater ability to rationalize wrong ideas; 
<br>
increased facility with verbal argument but no actual increase in 
<br>
smartness.  In my experience this is what people have in mind when they 
<br>
say &quot;Intelligence does not equal wisdom&quot;, and for this reason I really 
<br>
dislike that statement.  It's a way of giving up on rationality itself. 
<br>
&nbsp;&nbsp;If macho rationality consists of mistaking facility with verbal 
<br>
argument for intelligence, then this is the equally wrong mirror image 
<br>
of macho rationality - the idea that intelligence has nothing to do with 
<br>
wisdom.
<br>
<p>The problem of discussing &quot;wisdom&quot; is that everyone's had at least a few 
<br>
thoughts they consider &quot;wise&quot;.  In many cases they've gone through a lot 
<br>
of hell to acquire whatever lesson they learned, and they really don't 
<br>
want to hear that they've learned the wrong lesson from it.  If you have 
<br>
the misfortune to meet an &quot;intelligent&quot; person who's learned the lesson 
<br>
that facility with verbal argument isn't smartness but rather &quot;a fancier 
<br>
kind of stupid&quot;, as Spider Robinson put it, good luck ever convincing 
<br>
them that there's any kind of intelligence that actually works!  They've 
<br>
given up their pride to come to that conclusion, they've been hit over 
<br>
the head by stark reality, they've gone through hell to arrive at that 
<br>
one crumb of wisdom and by damn they've earned the right to be smug 
<br>
about it.  If you're foolish enough to think there's any kind of 
<br>
intelligence that really works, you must not have gone through what they 
<br>
did; you must be too proud to admit your own folly; you must not be 
<br>
wise.  I don't know how to dig people out of that trap.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4870.html">Eliezer S. Yudkowsky: "META: Re: AI Jailer."</a>
<li><strong>Previous message:</strong> <a href="4868.html">Gordon Worley: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<li><strong>In reply to:</strong> <a href="4863.html">Gordon Worley: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4872.html">Ben Goertzel: "RE: Intelligence and wisdom"</a>
<li><strong>Reply:</strong> <a href="4872.html">Ben Goertzel: "RE: Intelligence and wisdom"</a>
<li><strong>Reply:</strong> <a href="4884.html">James Higgins: "Re: Intelligence and wisdom"</a>
<li><strong>Maybe reply:</strong> <a href="4915.html">Christian L.: "RE: Intelligence and wisdom"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4869">[ date ]</a>
<a href="index.html#4869">[ thread ]</a>
<a href="subject.html#4869">[ subject ]</a>
<a href="author.html#4869">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
