<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI Boxing</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="Re: AI Boxing">
<meta name="Date" content="2002-07-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI Boxing</h1>
<!-- received="Sun Jul 28 10:26:47 2002" -->
<!-- isoreceived="20020728162647" -->
<!-- sent="Sat, 27 Jul 2002 16:18:27 -0700" -->
<!-- isosent="20020727231827" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="Re: AI Boxing" -->
<!-- id="3D432A43.4010101@earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20020727203005.089F43958@sitemail.everyone.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=Re:%20AI%20Boxing"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Sat Jul 27 2002 - 17:18:27 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4973.html">outlawpoet -: "Re: AI Boxing"</a>
<li><strong>Previous message:</strong> <a href="4971.html">James Higgins: "Re: AI boxing"</a>
<li><strong>In reply to:</strong> <a href="4968.html">outlawpoet -: "Re: AI Boxing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4973.html">outlawpoet -: "Re: AI Boxing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4972">[ date ]</a>
<a href="index.html#4972">[ thread ]</a>
<a href="subject.html#4972">[ subject ]</a>
<a href="author.html#4972">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
outlawpoet - wrote:
<br>
<em>&gt; Most of the participants were quite intelligent. I would say none much over 
</em><br>
<em>&gt; 3 sigmas above the mean, but very intelligent on the whole compared to the 
</em><br>
<em>&gt; base population. Also, these people realized what they were getting into. 
</em><br>
<em>&gt; They took the experiment seriously, and reacted seriously. In fact, some 
</em><br>
<em>&gt; of the hardest convinces I had were from people far removed from the transhumanist 
</em><br>
<em>&gt; arena. One lady actually wanted to destroy the AI, not just leave it in 
</em><br>
<em>&gt; jail, when i finally convinced her to let ve out, she did so, then realized 
</em><br>
<em>&gt; what she had done, burst into tears, and demanded I destroy the log of her 
</em><br>
<em>&gt; doing so. 
</em><br>
<p>Please explain this as I don't understand how she released the AI 
<br>
without knowing she did so.
<br>
<p><em>&gt; On a less emotional note, as I noted earlier, 22 of the involved 
</em><br>
<em>&gt; claimed beforehand that nothing could make them let the AI out. 4 said they 
</em><br>
<em>&gt; would evaluate the AI in the interview and decide for themselves. None were 
</em><br>
<em>&gt; predisposed to simply let it out. 
</em><br>
<p>And what did this matter to them?  It was just an experiment, and some 
<br>
of them may not have even realized the significance of letting the AI 
<br>
out.  Last weekend I, quite unexpectedly, ended up in a Casino for a 
<br>
short while with a friend.  I don't enjoy gambling and consider it a 
<br>
waste of money, and not having a bunch of excess cash recently I quite 
<br>
seriously decided not to gamble any money.  Yet I did, in fact, end up 
<br>
gambling $5.  Why did I change my mind?  Becuase $5 isn't that 
<br>
significant, despite my feelings on the matter (I got bored just 
<br>
standing there).
<br>
<p>Sitting in front of a console doing a &quot;real&quot; AI Box Test would have much 
<br>
more significance, at least to someone who really understood the problem 
<br>
and what was taking place.  In my opinion it is likely that at least 
<br>
some of your participants didn't fully understand the situation (it 
<br>
takes months to start coping with SL4 issues) or didn't take it very 
<br>
seriously.  In any case, none of the participants (as far as I've heard) 
<br>
had a vested interest in keeping the AI in the box.  With Eliezer's 
<br>
tests it costs the person money to let the AI out, which is what 
<br>
surprises me when he wins.
<br>
<p><em>&gt; As I mentioned in my first email, I qualified the dangers and relevant technologies 
</em><br>
<em>&gt; for these people, telling them why the AI was boxed, and the kinds of things 
</em><br>
<em>&gt; it might be able to do. 
</em><br>
<p>It is commonly understood on SL4 (correct me if I'm wrong) that it takes 
<br>
considerable time (months to years) to start to fully understand and 
<br>
come to terms with SL4 level issues.  A few hours, at best, doesn't make 
<br>
much difference.
<br>
<p>Let say you:
<br>
1.  Put a $100 bill in a test tube
<br>
2.  Affix a bio-hazard sticker to the test tube
<br>
3.  Walk up to a random person on the street
<br>
4.  Say &quot;If you open this you can have what is inside, but there
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;may be a potentially hazerdous chemical in there.  You have 2
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hours to decide.&quot;.
<br>
5.  Give them the test tube.
<br>
<p>How long do you think it would take to find someone to open the bottle 
<br>
in less than 5 minutes?  Why?
<br>
<p>Repeat the same experiment, but only use bio-chemists and give them 
<br>
access to a decent labratory.  Think there will be any difference in how 
<br>
the participants react?
<br>
<p>Experts in the domain will treat a problem differently than the general 
<br>
population.
<br>
<p><em>&gt; All of the participants communicated via IRC, 
</em><br>
<em>&gt; none were newcomers to computers. 10 worked in Information Technology.
</em><br>
<p>Working in IT hardly indicates knowledge on these topics.  I know a lot 
<br>
of people that work in IT that I wouldn't trust to setup email 
<br>
encryption much less something of this magnitude.
<br>
<p><em>&gt; 2 did not believe AI was possible, but accepted it for the purposes of the interview
</em><br>
<p>So at least 2 participants could not possibly have taken the test seriously.
<br>
<p><em>&gt; I quantify the religious beliefs in detail becuase 
</em><br>
<em>&gt; that was the factor that overridingly decided how long of a convince this 
</em><br>
<em>&gt; would be. In general, it came down to moral beliefs. So the religion or 
</em><br>
<em>&gt; beliefs of people tended to decide in general how people went, what they 
</em><br>
<em>&gt; wanted to talk about, and how they were convinced. I think within a single 
</em><br>
<em>&gt; person Computer Science Knowledge could be the deciding factor, rather than 
</em><br>
<em>&gt; religious beliefs, but that has not been my experience. 
</em><br>
<p>This may well be an artifact of who you choose to conduct the experiment 
<br>
on.  At least if all the tested individuals were SL4 readers (not just 
<br>
subscribers) or active transhumanists it would have been a more 
<br>
representitive sample.
<br>
<p><em>&gt; I'm sure most did not take it as seriously as I did, but most tried their 
</em><br>
<em>&gt; best to be objective, 14 did research before hand(of what level I did not 
</em><br>
<em>&gt; inquire) Most had strong feelings on the subject. 
</em><br>
<p>People who would base such decisions on feelings should be avoided at 
<br>
all costs.
<br>
<p><em>&gt; I would also like to 
</em><br>
<em>&gt; make some things clear. In many cases you seem to refer to these people 
</em><br>
<em>&gt; as if they're of no consequence, or inapplicable to the problem. These were 
</em><br>
<em>&gt; intelligent, vibrant people who think things through, know about lots of 
</em><br>
<em>&gt; things, and care a great deal. They're internet people (else how would I 
</em><br>
<em>&gt; find them) and they were the most interested ones I could find. Don't let 
</em><br>
<p>Most (possibly not all) of the people you choose to test are irrelevant 
<br>
for this purpose.
<br>
<p>Bill Clinton and George Bush are intelligent, vibrant people who think 
<br>
things through, know about lots of things, and care a great deal.  But I 
<br>
believe very few people on SL4 would trust their decision on a Box Test.
<br>
<p>You could give me a survey to see which of two drugs I would most likely 
<br>
perscribe to a patient, but that would be completely irrelevant since 
<br>
I'm not a doctor and can't/don't perscribe drugs.  If you found me at a 
<br>
Doctor's convention and I was very interested I *still* wouldn't be 
<br>
relevant for such a survey!
<br>
<p>None of these people are likely to ever conduct a real AI Box Test, and 
<br>
even if they did they would certainly not have the power to release the 
<br>
AI.  Further, none of them appear to be experts in the domain.  The 
<br>
people you choose *are* inapplicable to this problem.
<br>
<p><em>&gt;&gt;As for Eliezer's rules I do agree that the 2 hour minimum is not 
</em><br>
<em>&gt;&gt;realistic.
</em><br>
<p>FYI - I had intended to remove that point, which I've explained 
<br>
previously on this list.
<br>
<p>James Higgins
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4973.html">outlawpoet -: "Re: AI Boxing"</a>
<li><strong>Previous message:</strong> <a href="4971.html">James Higgins: "Re: AI boxing"</a>
<li><strong>In reply to:</strong> <a href="4968.html">outlawpoet -: "Re: AI Boxing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4973.html">outlawpoet -: "Re: AI Boxing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4972">[ date ]</a>
<a href="index.html#4972">[ thread ]</a>
<a href="subject.html#4972">[ subject ]</a>
<a href="author.html#4972">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
