<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI Jailer.</title>
<meta name="Author" content="Mike & Donna Deering (deering9@mchsi.com)">
<meta name="Subject" content="Re: AI Jailer.">
<meta name="Date" content="2002-07-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI Jailer.</h1>
<!-- received="Sat Jul 06 20:29:49 2002" -->
<!-- isoreceived="20020707022949" -->
<!-- sent="Sat, 6 Jul 2002 18:40:21 -0500" -->
<!-- isosent="20020706234021" -->
<!-- name="Mike & Donna Deering" -->
<!-- email="deering9@mchsi.com" -->
<!-- subject="Re: AI Jailer." -->
<!-- id="000b01c22546$7efacfa0$f040da0c@mchsi.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="3D275D07.2000405@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Mike & Donna Deering (<a href="mailto:deering9@mchsi.com?Subject=Re:%20AI%20Jailer."><em>deering9@mchsi.com</em></a>)<br>
<strong>Date:</strong> Sat Jul 06 2002 - 17:40:21 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4739.html">Eliezer S. Yudkowsky: "Re: AI Jailer."</a>
<li><strong>Previous message:</strong> <a href="4737.html">Eliezer S. Yudkowsky: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<li><strong>In reply to:</strong> <a href="4734.html">Eliezer S. Yudkowsky: "Re: AI Jailer."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4739.html">Eliezer S. Yudkowsky: "Re: AI Jailer."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4738">[ date ]</a>
<a href="index.html#4738">[ thread ]</a>
<a href="subject.html#4738">[ subject ]</a>
<a href="author.html#4738">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer writes: &quot;If anyone wishes to administer a lie-detector test on this, such as lie 
<br>
detection technology is in modern times, I'll take it.  If anyone comes 
<br>
up with a better lie detector test before the Singularity I'll take that 
<br>
as well.  I am not an AI; if I were lying you would have a reasonable 
<br>
expectation of catching it using your native human abilities, and the 
<br>
fact that you have not is evidence, though not proof.  I currently 
<br>
visualize, although I am not completely sure and can make no promises, 
<br>
that the grounding of the Friendly AI structure will be such that the 
<br>
other programmers on the project can verify that, despite being 
<br>
sensitive to the motives of the programmers, the AI will have been told 
<br>
to resist hidden selfish motives.&quot;
<br>
<p>The offer to take a lie detector test is not evidence until one is actually administered.  I could make this offer without much expectation that one would ever be given.  And native human abilities have failed to catch many a con man (which I'm not saying you are, probably haven't had time to learn a skill like that) or a psychopath which I have no reason to think you are not.  As for the other programmers on the project, I expect that you could easily influence their selection to those suitable for your purpose.  In my opinion these two options are at least equally plausible:
<br>
<p>1:  A nine year old genius is struck by the realization that the world is need of saving and that he can make a major contribution to this effort decides to code the first AI.
<br>
<p>2:  A nine year old genius suffers a traumatic rejection by someone of the opposite sex and decides to take over the world in order to payback the world for the pain suffered and to secure his position of superiority.  Searching around for a way to implement this decides to code the first AI as his tool.
<br>
<p>But for the record, I admit that I tend toward paranoia, seeing hidden motives and conspiracies everywhere.
<br>
<p>Eliezer also writes: &quot;I don't think selfish AI programmers executing multiyear altruistic 
<br>
masquerades constitute a major threat to the Singularity, but I am 
<br>
always on the lookout for ways to reduce threats to the Singularity.&quot;
<br>
<p>Although I can't logically differentiate between these two options, am still left with the options of existential risks through knowledge enabled weapons or letting someone initiate the Singularity.
<br>
<p>Mike.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4739.html">Eliezer S. Yudkowsky: "Re: AI Jailer."</a>
<li><strong>Previous message:</strong> <a href="4737.html">Eliezer S. Yudkowsky: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<li><strong>In reply to:</strong> <a href="4734.html">Eliezer S. Yudkowsky: "Re: AI Jailer."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4739.html">Eliezer S. Yudkowsky: "Re: AI Jailer."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4738">[ date ]</a>
<a href="index.html#4738">[ thread ]</a>
<a href="subject.html#4738">[ subject ]</a>
<a href="author.html#4738">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
