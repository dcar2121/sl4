<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Suggested AI-Box protocol &amp; AI-Honeypots</title>
<meta name="Author" content="Michael Warnock (michael@in-orbit.net)">
<meta name="Subject" content="Re: Suggested AI-Box protocol &amp; AI-Honeypots">
<meta name="Date" content="2002-07-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Suggested AI-Box protocol &amp; AI-Honeypots</h1>
<!-- received="Sat Jul 06 19:32:10 2002" -->
<!-- isoreceived="20020707013210" -->
<!-- sent="Sat, 06 Jul 2002 16:20:58 -0700" -->
<!-- isosent="20020706232058" -->
<!-- name="Michael Warnock" -->
<!-- email="michael@in-orbit.net" -->
<!-- subject="Re: Suggested AI-Box protocol &amp; AI-Honeypots" -->
<!-- id="E17Qxn2-0005eo-00@freyja.in-orbit.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="Suggested AI-Box protocol &amp; AI-Honeypots" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Warnock (<a href="mailto:michael@in-orbit.net?Subject=Re:%20Suggested%20AI-Box%20protocol%20&amp;%20AI-Honeypots"><em>michael@in-orbit.net</em></a>)<br>
<strong>Date:</strong> Sat Jul 06 2002 - 17:20:58 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4737.html">Eliezer S. Yudkowsky: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<li><strong>Previous message:</strong> <a href="4735.html">Eliezer S. Yudkowsky: "Suggested AI-Box protocol"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4737.html">Eliezer S. Yudkowsky: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<li><strong>Reply:</strong> <a href="4737.html">Eliezer S. Yudkowsky: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<li><strong>Maybe reply:</strong> <a href="4745.html">Tomaz Kristan: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<li><strong>Maybe reply:</strong> <a href="../0107/4749.html">Michael Warnock: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4736">[ date ]</a>
<a href="index.html#4736">[ thread ]</a>
<a href="subject.html#4736">[ subject ]</a>
<a href="author.html#4736">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
7/6/2002 2:17:28 PM, &quot;Eliezer S. Yudkowsky&quot; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Suggested%20AI-Box%20protocol%20&amp;%20AI-Honeypots">sentience@pobox.com</a>&gt; wrote:
<br>
<p><em>&gt;Based on my two experiences so far, I've created a suggested protocol 
</em><br>
<em>&gt;for future AI-Box tests.  This should also answer the questions Higgins 
</em><br>
<em>&gt;asked earlier.
</em><br>
<em>&gt;
</em><br>
<em>&gt;<a href="http://sysopmind.com/essays/aibox.html">http://sysopmind.com/essays/aibox.html</a>
</em><br>
<p>This seems very complete.  Most of my ideas for how the AI party has
<br>
been successful are not within the protocol.  The only remaining notion
<br>
I think to be reasonable is that Eli is convincing the Gatekeeper party
<br>
that letting him out now increases the chances of real FAI by increasing
<br>
the thought and self-doubt surrounding AI-Boxes and Friendliness.
<br>
This too may be judged to be using a real-world Eli tactic such as a
<br>
secret bribe, which breaks the first of the AI protocols.
<br>
<p>I tend to think that an AI-Box with a single perimeter and an on/off
<br>
switch would not do the job intended, but possibly influence the AI's
<br>
perception of our own friendliness negatively.  On the other hand I
<br>
think it's important that we not run programs on any substrate capable
<br>
of unaided self-extension at the hardware level without extremely well
<br>
thought-out safe guards.
<br>
If a transhuman AI could exist purely as software on more conventional
<br>
hardware (barring quantum mechanisms in our own brains or extremely
<br>
high requirements), then it should be possible to maintain hardware parity
<br>
that is not under the influence of said software.  This parity hardware
<br>
would allow inspection of any and all code.  The usefullness of this,
<br>
as I see it, depends upon how algorythmic vs connectionist the program
<br>
is and its level of complexity in general.  Even given the most readable
<br>
code, however, a transhuman AI, which has spent most of its existance
<br>
working with code, including its own, would likely be able to slip one
<br>
by even the most expert panel of code-reviewers.
<br>
So what else could be done along these lines?  How about the
<br>
equivalent of a honey-pot.  In a traditional *N*X honeypot a
<br>
system image is placed within a host environment to which it has
<br>
no access or even indication that it is there.  All outside communication
<br>
is routed to the contained system image.  Checks are made of the
<br>
integrity of the image to alert of the hoped-for intrusion.  Much can
<br>
be learned about a particular cracker by tricking him into your
<br>
honeypot.
<br>
To produce the equivalent for a transhuman AI may prove
<br>
impossible if it is able to harness superior hardware technology
<br>
within its own perimeter.  Also, if an UAI deduced the existance
<br>
of the honeypot it would likely pretend to respect the first
<br>
boundary so as to be let out or wait for an appropriate time to
<br>
blow through the first and attempt the second.  At least I think there
<br>
is some possible use in this, though, if an UAI is let into what it
<br>
thinks is the base level of reality and makes its character known,
<br>
but is still contained.
<br>
Perhaps we are in a similar situation as humans. ;)
<br>
<p>Michael Warnock
<br>
Wherever you go; &amp;this;
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4737.html">Eliezer S. Yudkowsky: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<li><strong>Previous message:</strong> <a href="4735.html">Eliezer S. Yudkowsky: "Suggested AI-Box protocol"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4737.html">Eliezer S. Yudkowsky: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<li><strong>Reply:</strong> <a href="4737.html">Eliezer S. Yudkowsky: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<li><strong>Maybe reply:</strong> <a href="4745.html">Tomaz Kristan: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<li><strong>Maybe reply:</strong> <a href="../0107/4749.html">Michael Warnock: "Re: Suggested AI-Box protocol &amp; AI-Honeypots"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4736">[ date ]</a>
<a href="index.html#4736">[ thread ]</a>
<a href="subject.html#4736">[ subject ]</a>
<a href="author.html#4736">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
