<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI-Box Experiment 2: Yudkowsky and McFadzean</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="Re: AI-Box Experiment 2: Yudkowsky and McFadzean">
<meta name="Date" content="2002-07-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI-Box Experiment 2: Yudkowsky and McFadzean</h1>
<!-- received="Tue Jul 16 05:40:28 2002" -->
<!-- isoreceived="20020716114028" -->
<!-- sent="Tue, 16 Jul 2002 00:16:21 -0700" -->
<!-- isosent="20020716071621" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="Re: AI-Box Experiment 2: Yudkowsky and McFadzean" -->
<!-- id="3D33C845.7060707@earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D26B8EF.8090100@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=Re:%20AI-Box%20Experiment%202:%20Yudkowsky%20and%20McFadzean"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Tue Jul 16 2002 - 01:16:21 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4852.html">James Higgins: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<li><strong>Previous message:</strong> <a href="4850.html">Damien Broderick: "RE: Why do we seek to transcend ourselves?"</a>
<li><strong>In reply to:</strong> <a href="4727.html">Eliezer S. Yudkowsky: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4728.html">Eliezer S. Yudkowsky: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4851">[ date ]</a>
<a href="index.html#4851">[ thread ]</a>
<a href="subject.html#4851">[ subject ]</a>
<a href="author.html#4851">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Sorry, been locked out of this email account for better than a week.
<br>
Not that anyone on SL4 probably minded.  ;)
<br>
<p>Eliezer S. Yudkowsky wrote:
<br>
<em> &gt; James Higgins wrote:
</em><br>
<em> &gt; David McFadzean has been an Extropian for considerably longer than I
</em><br>
<em> &gt; have - he maintains extropy.org's server, in fact - and currently works
</em><br>
<p>Ah, ok.  I don't get into the Extropian realm much...
<br>
<p><em> &gt; on Peter Voss's A2I2 project.  Do you still believe that you would be
</em><br>
<em> &gt; &quot;extremely difficult if not impossible&quot; for an actual transhuman to
</em><br>
<em> &gt; convince?
</em><br>
<p>yes.
<br>
<p><em> &gt; Are you at least a *little* less confident than before?  Am I not having
</em><br>
<em> &gt; any impact here?  Will people just go on saying &quot;Well, that's an
</em><br>
<p>Nope.  I'm starting to wonder if you're picking people who, you believe
<br>
in advance, you can get to open the jail.
<br>
<p>There is no way to say for certain that a transhuman (espicially without
<br>
defining what exactly is meant by that) could not convince me.  However,
<br>
I'm absolutely certain an Eliezer Yudkowsky could not convince me.
<br>
<p><em> &gt; Apparently the transhuman AI in your mental picture is not as smart as
</em><br>
<em> &gt; Eliezer Yudkowsky in actuality.  You can't imagine a character who's
</em><br>
<em> &gt; smarter than the author and this definitely applies to figuring out
</em><br>
<em> &gt; whether a &quot;transhuman AI&quot; can persuade you to let it out of the box. All
</em><br>
<em> &gt; you can do is imagine whether a mind that's as smart as James Higgins
</em><br>
<em> &gt; can convince you to let it out of the box.  You can't imagine anything
</em><br>
<p>I know a mind as smart as JH (me) could never convince me to let it out
<br>
of the &quot;box&quot; prematurly.  Obviously, at some point the goal is to let it
<br>
out of the box.  Once everything reasonable has been done to ensure that
<br>
it is friendly and its goals match those desirable to a reasonable
<br>
aproximation.  Unless your trick is to tell your jailer (out of role)
<br>
that this is the case I don't see how these people could be letting you
<br>
out unless you think they are likely to be soft in that area beforehand.
<br>
<p><em> &gt; at a level of intelligence above that.  If something seems impossible to
</em><br>
<em> &gt; you, it doesn't prove it's impossible for a human who's even slightly
</em><br>
<p>For the record, I don't think anything is impossible, at least in the
<br>
long term.  Nearly so, maybe, but nothing is impossible given time and
<br>
resources.
<br>
<p><em> &gt; That's the problem with saying something like, e.g., &quot;Intelligence does
</em><br>
<em> &gt; not equal wisdom.&quot;  You *do not know* what intelligence does or does not
</em><br>
<em> &gt; equal.  All you know is that the amount of intelligence you currently
</em><br>
<p>Actually, I believe I do know that intelligence does not equal wisdom.
<br>
If for no other reason than because the two are different concepts.
<br>
Now, I'm not saying intelligence can't get you to wisdom, or at least
<br>
reduce the learning curve, but it does not equal nor guarantee wisdom.
<br>
<p><em> &gt; have does not equal wisdom.  You have no idea whether intelligence
</em><br>
<em> &gt; equals wisdom for someone even slightly smarter than you.  Intelligence
</em><br>
<p>Sorry, wrong again.  Intelligence never equals wisdom, no matter how
<br>
intelligent.  An apple is never an orange, no matter how large it grows.
<br>
<p><em> &gt; Actually, this sounds like a rather adversarial restatement of my
</em><br>
<em> &gt; perspective.  What I am saying is that a transhuman AI, if it chooses to
</em><br>
<em> &gt; do so, can almost certainly take over a human through a text-only
</em><br>
<em> &gt; terminal.  Whether a transhuman would choose to do so is a separate 
</em><br>
issue.
<br>
<p>How transhuman are we talking here?  If we move the bar closer to SI
<br>
than human then your probably correct.  However, I personally believe it
<br>
will take a substantial improvement to make such things possible.  Which
<br>
will, hopefully, give us enough time to fully evaluate and guide the
<br>
development of the AI.
<br>
<p><em> &gt;
</em><br>
<em> &gt;&gt; Thus it is unlikely his team would do this (at least regularly) unless
</em><br>
<em> &gt;&gt; they had a specific reason to do so.
</em><br>
<em> &gt;
</em><br>
<em> &gt;
</em><br>
<em> &gt; The point I am trying to make is that when a transhuman comes into
</em><br>
<em> &gt; existence, you have bet the farm at that point.  There are potentially
</em><br>
<em> &gt; reasons for the programmers to talk to a transhuman Friendly AI during
</em><br>
<em> &gt; the final days or hours before the Singularity, but the die has been
</em><br>
<em> &gt; almost certainly been cast as soon as one transhuman comes into
</em><br>
<em> &gt; existence, whether the mind is contained in sealed hardware on the Moon
</em><br>
<em> &gt; next to a million tons of explosives, or is in immediate command of a
</em><br>
<em> &gt; full nanotechnological laboratory.  Distinctions such as these are only
</em><br>
<em> &gt; relevant on a human scale.  They are impressive to us, not to 
</em><br>
transhumans.
<br>
<p>Ok, so what *exactly* is your definition of a transhuman.  By some
<br>
accounts you could qualify as a transhuman since you are obviously
<br>
smarter than the average human.  I have no doubt I could stop you if
<br>
necessary.  So are you using &quot;transhuman&quot; to indicate an AI that is
<br>
1,000 times as smart as an average human?  Or?
<br>
<p>James Higgins
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4852.html">James Higgins: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<li><strong>Previous message:</strong> <a href="4850.html">Damien Broderick: "RE: Why do we seek to transcend ourselves?"</a>
<li><strong>In reply to:</strong> <a href="4727.html">Eliezer S. Yudkowsky: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4728.html">Eliezer S. Yudkowsky: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4851">[ date ]</a>
<a href="index.html#4851">[ thread ]</a>
<a href="subject.html#4851">[ subject ]</a>
<a href="author.html#4851">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
