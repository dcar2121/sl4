<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re[2]: Intelligence without Awareness? (was Re: The dumb  SAI and the Semiautomatic Singularity)</title>
<meta name="Author" content="Cliff Stabbert (cps46@earthlink.net)">
<meta name="Subject" content="Re[2]: Intelligence without Awareness? (was Re: The dumb  SAI and the Semiautomatic Singularity)">
<meta name="Date" content="2002-07-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re[2]: Intelligence without Awareness? (was Re: The dumb  SAI and the Semiautomatic Singularity)</h1>
<!-- received="Mon Jul 08 21:07:30 2002" -->
<!-- isoreceived="20020709030730" -->
<!-- sent="Mon, 8 Jul 2002 21:05:59 -0400" -->
<!-- isosent="20020709010559" -->
<!-- name="Cliff Stabbert" -->
<!-- email="cps46@earthlink.net" -->
<!-- subject="Re[2]: Intelligence without Awareness? (was Re: The dumb  SAI and the Semiautomatic Singularity)" -->
<!-- id="10816557778.20020708210559@earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D2A233F.5050400@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Cliff Stabbert (<a href="mailto:cps46@earthlink.net?Subject=Re[2]:%20Intelligence%20without%20Awareness?%20(was%20Re:%20The%20dumb%20%20SAI%20and%20the%20Semiautomatic%20Singularity)"><em>cps46@earthlink.net</em></a>)<br>
<strong>Date:</strong> Mon Jul 08 2002 - 19:05:59 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4779.html">Tomaz Kristan: "Re: Re[6]: The dumb  SAI and the Semiautomatic Singularity"</a>
<li><strong>Previous message:</strong> <a href="4777.html">Eliezer S. Yudkowsky: "Re: Intelligence without Awareness? (was Re: The dumb  SAI and the Semiautomatic Singularity)"</a>
<li><strong>In reply to:</strong> <a href="4777.html">Eliezer S. Yudkowsky: "Re: Intelligence without Awareness? (was Re: The dumb  SAI and the Semiautomatic Singularity)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4761.html">Brian Phillips: "Re: The dumb  SAI and the Semiautomatic Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4778">[ date ]</a>
<a href="index.html#4778">[ thread ]</a>
<a href="subject.html#4778">[ subject ]</a>
<a href="author.html#4778">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Monday, July 8, 2002, 7:41:51 PM, Eliezer S. Yudkowsky wrote:
<br>
<p>ESY&gt; Cliff Stabbert wrote:
<br>
ESY&gt;  &gt;
<br>
<em>&gt;&gt; From GISAI (<a href="http://intelligence.org/GISAI.html#mind_thought_I">http://intelligence.org/GISAI.html#mind_thought_I</a> ):
</em><br>
<p>ESY&gt; The updated version of this in LOGI (Levels of Organization in General 
<br>
ESY&gt; Intelligence) is:
<br>
<p>ESY&gt; <a href="http://intelligence.org/DGI/levels/deliberation.html#self">http://intelligence.org/DGI/levels/deliberation.html#self</a>
<br>
<p>ESY&gt; Unlike GISAI, this contains at least one concrete example of how a mind 
<br>
ESY&gt; modeling itself in the first person is different from a mind modeling 
<br>
ESY&gt; itself in the third person.
<br>
<p>Thanks for that link.  I think that the more extended discussion you
<br>
offer there of when it's valid to use &quot;I&quot; offers further support for
<br>
the position that intelligence above a certain level of complexity is
<br>
not possible without awareness.  Of course, we cannot prove awareness:
<br>
<p>You write that
<br>
&nbsp;&nbsp;Legitimate use of &quot;I&quot; is explicitly not offered as a necessary and
<br>
&nbsp;&nbsp;sufficient condition for the &quot;hard problem of conscious experience&quot;
<br>
&nbsp;&nbsp;[Chalmers95] or social, legal, and moral personhood.
<br>
<p>...which I'm not sure I even follow.  My understanding of the &quot;hard
<br>
problem&quot; as Chalmers sketches it is not &quot;which conditions are
<br>
necessary and sufficient to give rise to awareness,&quot; but rather &quot;what
<br>
is consciousness/experience and how come it arises/accompanies certain
<br>
phenomena?&quot;
<br>
<p>Two quotes from Chalmers' paper may be in order here:
<br>
[it's at <a href="http://www.u.arizona.edu/~chalmers/papers/facing.html">http://www.u.arizona.edu/~chalmers/papers/facing.html</a>, for
<br>
&nbsp;those playing along at home]
<br>
<p>&nbsp;&nbsp;What makes the hard problem hard and almost unique is that it goes
<br>
&nbsp;&nbsp;beyond problems about the performance of functions. To see this,
<br>
&nbsp;&nbsp;note that even when we have explained the performance of all the
<br>
&nbsp;&nbsp;cognitive and behavioral functions in the vicinity of experience -
<br>
&nbsp;&nbsp;perceptual discrimination, categorization, internal access, verbal
<br>
&nbsp;&nbsp;report - there may still remain a further unanswered question: **Why
<br>
&nbsp;&nbsp;is the performance of these functions accompanied by experience?**
<br>
&nbsp;&nbsp;[his emphasis]
<br>
<p>The emphasized question is what I take as the essence of his hard
<br>
problem.  *That* there is something it is like to be a bat, he
<br>
does not seem to dispute:
<br>
<p>&nbsp;&nbsp;It is undeniable that some organisms are subjects of experience.
<br>
<p>In which case, it seems to me that the question of whether an AI,
<br>
another person, or other creature &quot;actually&quot; experiences consciousness
<br>
is formally undecidable, and irrelevant: whether they have awareness
<br>
seems more of a Consensus question, i.e., &quot;we can tell&quot; (leaving aside
<br>
boundary cases) can serve as a working definition.
<br>
<p>To repeat: I very strongly suspect there are fundamental reasons why
<br>
intelligence of a certain order must be accompanied by what we would
<br>
agree, from the outside, is awareness.  I cannot yet formulate those
<br>
reasons as well as I'd like but the section you link above would serve
<br>
as an excellent beginning.
<br>
<p><pre>
--
Cliff
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4779.html">Tomaz Kristan: "Re: Re[6]: The dumb  SAI and the Semiautomatic Singularity"</a>
<li><strong>Previous message:</strong> <a href="4777.html">Eliezer S. Yudkowsky: "Re: Intelligence without Awareness? (was Re: The dumb  SAI and the Semiautomatic Singularity)"</a>
<li><strong>In reply to:</strong> <a href="4777.html">Eliezer S. Yudkowsky: "Re: Intelligence without Awareness? (was Re: The dumb  SAI and the Semiautomatic Singularity)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4761.html">Brian Phillips: "Re: The dumb  SAI and the Semiautomatic Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4778">[ date ]</a>
<a href="index.html#4778">[ thread ]</a>
<a href="subject.html#4778">[ subject ]</a>
<a href="author.html#4778">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
