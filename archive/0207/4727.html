<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI-Box Experiment 2: Yudkowsky and McFadzean</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: AI-Box Experiment 2: Yudkowsky and McFadzean">
<meta name="Date" content="2002-07-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI-Box Experiment 2: Yudkowsky and McFadzean</h1>
<!-- received="Sat Jul 06 05:38:46 2002" -->
<!-- isoreceived="20020706113846" -->
<!-- sent="Sat, 06 Jul 2002 05:31:27 -0400" -->
<!-- isosent="20020706093127" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: AI-Box Experiment 2: Yudkowsky and McFadzean" -->
<!-- id="3D26B8EF.8090100@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="4.3.2.7.2.20020704150902.01a996c8@mail.earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20AI-Box%20Experiment%202:%20Yudkowsky%20and%20McFadzean"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Jul 06 2002 - 03:31:27 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4728.html">Eliezer S. Yudkowsky: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<li><strong>Previous message:</strong> <a href="4726.html">Tomaz Kristan: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4699.html">James Higgins: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4851.html">James Higgins: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<li><strong>Reply:</strong> <a href="4851.html">James Higgins: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4727">[ date ]</a>
<a href="index.html#4727">[ thread ]</a>
<a href="subject.html#4727">[ subject ]</a>
<a href="author.html#4727">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
James Higgins wrote:
<br>
<em> &gt;
</em><br>
<em>&gt; Many humans may be convinced, however an AI programmer who was working 
</em><br>
<em>&gt; on the problem should understand the situation and the dangers.  Thus I 
</em><br>
<em>&gt; believe they would be very difficult to convince.   I believe I would be
</em><br>
<em>&gt; extremely difficult (if not impossible) to convince.
</em><br>
<p>David McFadzean has been an Extropian for considerably longer than I 
<br>
have - he maintains extropy.org's server, in fact - and currently works 
<br>
on Peter Voss's A2I2 project.  Do you still believe that you would be 
<br>
&quot;extremely difficult if not impossible&quot; for an actual transhuman to 
<br>
convince?
<br>
<p>Are you at least a *little* less confident than before?  Am I not having 
<br>
any impact here?  Will people just go on saying &quot;Well, that's an 
<br>
interesting anecdote, but I definitely know that no conceivable 
<br>
intelligence can convince ME of anything against my will&quot;?  Shades of 
<br>
&quot;doc&quot; Smith...
<br>
<p><em>&gt; Of course, the smarter the AI gets the better a chance it has to get me 
</em><br>
<em>&gt; to let it out.  I double anyone could withstand a conversation with a SI 
</em><br>
<em>&gt; without letting it out.  But a trans-human AI that is much closer to the 
</em><br>
<em>&gt; human-equivalent level than the SI level should be doable (within the 
</em><br>
<em>&gt; context I mentioned).
</em><br>
<p>Apparently the transhuman AI in your mental picture is not as smart as 
<br>
Eliezer Yudkowsky in actuality.  You can't imagine a character who's 
<br>
smarter than the author and this definitely applies to figuring out 
<br>
whether a &quot;transhuman AI&quot; can persuade you to let it out of the box. 
<br>
All you can do is imagine whether a mind that's as smart as James 
<br>
Higgins can convince you to let it out of the box.  You can't imagine 
<br>
anything at a level of intelligence above that.  If something seems 
<br>
impossible to you, it doesn't prove it's impossible for a human who's 
<br>
even slightly smarter than you, or even that you'll still think the 
<br>
problem is impossible in five years.  It just shows that you don't see 
<br>
any way to solve the problem at that exact moment.
<br>
<p>That's the problem with saying something like, e.g., &quot;Intelligence does 
<br>
not equal wisdom.&quot;  You *do not know* what intelligence does or does not 
<br>
equal.  All you know is that the amount of intelligence you currently 
<br>
have does not equal wisdom.  You have no idea whether intelligence 
<br>
equals wisdom for someone even slightly smarter than you.  Intelligence 
<br>
is the fountain of unknown unknowns.  Now if you were to say 
<br>
&quot;intelligence is not definitely known in advance to equal wisdom for all 
<br>
possible minds-in-general&quot;, I might agree with you.
<br>
<p><em>&gt; Not presumably.  Depends on who is constructing the AI and for what 
</em><br>
<em>&gt; purpose.  Eliezer in particular (if I'm remembering this correctly) 
</em><br>
<em>&gt; doesn't believe it is safe to communicate with a trans-human AI at all.
</em><br>
<p>No!  Really?
<br>
<p>Actually, this sounds like a rather adversarial restatement of my 
<br>
perspective.  What I am saying is that a transhuman AI, if it chooses to 
<br>
do so, can almost certainly take over a human through a text-only 
<br>
terminal.  Whether a transhuman would choose to do so is a separate issue.
<br>
<p><em>&gt; Thus it is unlikely his team would do this (at least regularly) unless 
</em><br>
<em>&gt; they had a specific reason to do so.
</em><br>
<p>The point I am trying to make is that when a transhuman comes into 
<br>
existence, you have bet the farm at that point.  There are potentially 
<br>
reasons for the programmers to talk to a transhuman Friendly AI during 
<br>
the final days or hours before the Singularity, but the die has been 
<br>
almost certainly been cast as soon as one transhuman comes into 
<br>
existence, whether the mind is contained in sealed hardware on the Moon 
<br>
next to a million tons of explosives, or is in immediate command of a 
<br>
full nanotechnological laboratory.  Distinctions such as these are only 
<br>
relevant on a human scale.  They are impressive to us, not to transhumans.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4728.html">Eliezer S. Yudkowsky: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<li><strong>Previous message:</strong> <a href="4726.html">Tomaz Kristan: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4699.html">James Higgins: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4851.html">James Higgins: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<li><strong>Reply:</strong> <a href="4851.html">James Higgins: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4727">[ date ]</a>
<a href="index.html#4727">[ thread ]</a>
<a href="subject.html#4727">[ subject ]</a>
<a href="author.html#4727">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
