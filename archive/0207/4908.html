<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: SL4 meets &quot;Pinky and the Brain&quot;</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: SL4 meets &quot;Pinky and the Brain&quot;">
<meta name="Date" content="2002-07-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: SL4 meets &quot;Pinky and the Brain&quot;</h1>
<!-- received="Tue Jul 16 19:01:07 2002" -->
<!-- isoreceived="20020717010107" -->
<!-- sent="Tue, 16 Jul 2002 17:01:32 -0600" -->
<!-- isosent="20020716230132" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: SL4 meets &quot;Pinky and the Brain&quot;" -->
<!-- id="LAEGJLOGJIOELPNIOOAJMEOGCMAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D3464CE.1020409@earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20SL4%20meets%20&quot;Pinky%20and%20the%20Brain&quot;"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Tue Jul 16 2002 - 17:01:32 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4909.html">Ben Goertzel: "RE: SL4 meets &quot;Pinky and the Brain&quot;"</a>
<li><strong>Previous message:</strong> <a href="4907.html">Ben Goertzel: "RE: Intelligence and wisdom"</a>
<li><strong>In reply to:</strong> <a href="4889.html">James Higgins: "Re: SL4 meets &quot;Pinky and the Brain&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4912.html">Doug Keenan: "Re: SL4 meets &quot;Pinky and the Brain&quot;"</a>
<li><strong>Reply:</strong> <a href="4912.html">Doug Keenan: "Re: SL4 meets &quot;Pinky and the Brain&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4908">[ date ]</a>
<a href="index.html#4908">[ thread ]</a>
<a href="subject.html#4908">[ subject ]</a>
<a href="author.html#4908">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
James wrote:
<br>
<em>&gt; Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt;  &gt; James Higgins wrote:
</em><br>
<em>&gt;  &gt;  &gt; This isn't the place to get into the details but having conversed
</em><br>
<em>&gt;  &gt;  &gt; with you and Ben for awhile I believe he is significantly wiser than
</em><br>
<em>&gt;  &gt;  &gt; you (not necessarily more intelligent).
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; If that's true, it doesn't change the fact that Ben Goertzel has
</em><br>
<em>&gt;  &gt; posted a mathematical definition of how he would either ask an AI to
</em><br>
<em>&gt;  &gt; optimize the world according to his goal system C, or else create a
</em><br>
<em>&gt;  &gt; population of entities with goal systems N such that the
</em><br>
<em>&gt;  &gt; population-level effect would be to optimize C.  Now this can be argued
</em><br>
<em>&gt;
</em><br>
<em>&gt; You know, actually, I don't remember ever having seen such a post by
</em><br>
<em>&gt; Ben.  For a period of some months I didn't read many of the posts on SL4
</em><br>
<em>&gt; so I'm guessing this is why.  Could you refer me to the post/thread in
</em><br>
<em>&gt; question, I would very much like to read that thread...
</em><br>
<p>I don't remember posting anything exactly like that.
<br>
<p>What I did say is that I would create an AGI with a goal system embodying
<br>
some approximation to my own morality, which is similar to the &quot;generic
<br>
morality&quot; of the transhumanist community, and different from the morality of
<br>
(e.g.) the Christian Scientist or Zoroastrian communities.
<br>
<p>I asserted that it was going to be necessary to teach a baby AGI an
<br>
approximation to some particular human moral system.  Eli seemed to disagree
<br>
with this, arguing that the baby AGI should be taught to treat all human
<br>
moral systems equally (or something like that).  I don't think this makes
<br>
sense, since there are some human moral systems that say AGI and uploading
<br>
are evil, others that say AGI is a waste of resources, etc.
<br>
<p>I do not intend to ask any AGI system to optimize the world, and I doubt if
<br>
I ever said anything like that.
<br>
<p>I do think that once an AGI is created, it is going to affect the world.
<br>
But I think that an AGI should be taught a &quot;live and let live&quot; value, not a
<br>
&quot;manipulate every molecule of the universe&quot; value.  (Yes, these are very
<br>
crude terms, and should be clarified, but not today, I don't have time.)
<br>
<p>I think that, as human-level AGI gets nearer, we'll need to do a lot of
<br>
research aimed at figuring out how to increase the odds that the AGI's we
<br>
create, when they become superhuman, will NOT forcibly reprogram the brains
<br>
or molecules of lower life-forms.
<br>
<p>So my view is:
<br>
<p>1) In creating an AGI, we have no choice but to instill it with a particular
<br>
initial moral system, which not all humans will agree with
<br>
<p>2) Part of this moral system should be a value that causes it to respect the
<br>
freedom and autonomy of other sentient and living beings
<br>
<p>If I did not state this clearly enough before, I'm sorry; as I've emphasized
<br>
repeatedly, I have never written a manifesto on this stuff, just some
<br>
unsystematic e-mails and notes.  I will address these issues seriously in
<br>
writing once I'm done with the mathematical treatment of the Novamente
<br>
design I'm now in the middle of revising...
<br>
<p>The &quot;Sysop Scenario&quot; is a little ambiguous to me.  If you have an AI with
<br>
Sysop-level powers, it may still decide to allow autonomy to humans and
<br>
other lower life forms to live freely in their regions of space.  This is
<br>
something to work towards; I'd consider it a very positive scenario given
<br>
many of the other alternatives...
<br>
<p>-- Ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4909.html">Ben Goertzel: "RE: SL4 meets &quot;Pinky and the Brain&quot;"</a>
<li><strong>Previous message:</strong> <a href="4907.html">Ben Goertzel: "RE: Intelligence and wisdom"</a>
<li><strong>In reply to:</strong> <a href="4889.html">James Higgins: "Re: SL4 meets &quot;Pinky and the Brain&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4912.html">Doug Keenan: "Re: SL4 meets &quot;Pinky and the Brain&quot;"</a>
<li><strong>Reply:</strong> <a href="4912.html">Doug Keenan: "Re: SL4 meets &quot;Pinky and the Brain&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4908">[ date ]</a>
<a href="index.html#4908">[ thread ]</a>
<a href="subject.html#4908">[ subject ]</a>
<a href="author.html#4908">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
