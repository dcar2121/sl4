<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendly AI koans</title>
<meta name="Author" content="Justin Corwin (thesweetestdream@hotmail.com)">
<meta name="Subject" content="Re: Friendly AI koans">
<meta name="Date" content="2002-07-11">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendly AI koans</h1>
<!-- received="Fri Jul 12 02:16:30 2002" -->
<!-- isoreceived="20020712081630" -->
<!-- sent="Thu, 11 Jul 2002 21:45:15 -0600" -->
<!-- isosent="20020712034515" -->
<!-- name="Justin Corwin" -->
<!-- email="thesweetestdream@hotmail.com" -->
<!-- subject="Re: Friendly AI koans" -->
<!-- id="F63SsAVkENLmh8LtXV60000d4ff@hotmail.com" -->
<!-- inreplyto="Friendly AI koans" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Justin Corwin (<a href="mailto:thesweetestdream@hotmail.com?Subject=Re:%20Friendly%20AI%20koans"><em>thesweetestdream@hotmail.com</em></a>)<br>
<strong>Date:</strong> Thu Jul 11 2002 - 21:45:15 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4812.html">Cliff Stabbert: "Re: AI Options."</a>
<li><strong>Previous message:</strong> <a href="4810.html">Mike & Donna Deering: "Re: AI Options."</a>
<li><strong>Maybe in reply to:</strong> <a href="../0206/4376.html">Eliezer S. Yudkowsky: "Friendly AI koans"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4811">[ date ]</a>
<a href="index.html#4811">[ thread ]</a>
<a href="subject.html#4811">[ subject ]</a>
<a href="author.html#4811">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer, I noticed this email in my archives, and noted no obvious replies 
<br>
to it. I hadn't commented on it either (being on a trip at the time) and 
<br>
supposed that answers are better late than never. Some comments and thoughts 
<br>
below:
<br>
<p><em>&gt;1.  You're designing a Friendship system.  You think you know how to 
</em><br>
<em>&gt;transfer over the contents of your own moral philosophy over, but you can't 
</em><br>
<em>&gt;for the life of you think of any way to even begin to construct a moral 
</em><br>
<em>&gt;philosophy that could legitimately be said to belong to &quot;humanity&quot; and not 
</em><br>
<em>&gt;just you.  Others have repeatedly demanded this of you and you think they 
</em><br>
<em>&gt;are completely justified in doing so.  What do you do?
</em><br>
<p>Well. There are two things being asked here. If the philosophy that you 
<br>
transfer over to AI can legitimately be called human-universal, and if the 
<br>
ability to transfer 'your' moral philosophy neccesarily implies the ability 
<br>
to transfer a defined or recieved philosophy.
<br>
<p>The first question is one of generation. Can you generate a human-universal 
<br>
philosophy. I would argue no. Our design limitations prevent us from forming 
<br>
a universal moral philosophy (observer bias) and the same defect would 
<br>
prevent us from generalizing, even within our limited domain of humans.
<br>
<p>Can you generate a moral philosophy that is arguably 'more' universal? More 
<br>
'acceptably' universal? Sure. Put your moral philosophy with talmudic 
<br>
commentary online. Encourage debate. Post such debates. Revise the 
<br>
philosophy. Allow others to do so. If you're short on time, simply post your 
<br>
philosophy, set some ground rules and let other people worry about it. That 
<br>
way it could drift further away from your independent vector anyway.
<br>
<p>Alternatively, you could generate a commentary based moral philosophy, one 
<br>
that centers around removing the observer bias. Such a moral philosophy 
<br>
could be aggressively universal. (in theory, at least). Consensus may be 
<br>
elusive, but an actively independent morality seems more likely to be 
<br>
universal, or at least acceptably universal, than a personal one.
<br>
<p>The latter question, can your ability to transfer your philosophy be 
<br>
considered equivalent to the ability to transfer 'any' moral precepts...
<br>
<p>That's a touchy subject for me. I personally tend to believe that any 
<br>
singleton will have too many subconcious assumptions and unquestioned 
<br>
beliefs to reliably transfer an alien or substantially different moral 
<br>
philosophy to code or even writing. With extra-ordinary checks and balances, 
<br>
and fearsome internal controls, you might succeed in marginally firewalling 
<br>
your own personality out of the coding. But my gut instinct says that your 
<br>
implicit assumptions and viewpoints will seep into any such system, or even 
<br>
a system for generating such a system. This is a stunningly informal 
<br>
position, so i haven't much supporting evidence for it. It intuits, but i 
<br>
can't tell if it has legs or not. (besides i'm at work and can't really 
<br>
schedule that much time to exploring it.
<br>
<p><em>&gt;
</em><br>
<em>&gt;2.  You didn't think of the idea of probabilistic supergoals when you were 
</em><br>
<em>&gt;designing the Friendship system.  Instead your AI has a set of &quot;real&quot; 
</em><br>
<em>&gt;supergoals of priority 10, and one meta-supergoal of priority 1000 that 
</em><br>
<em>&gt;says to change the &quot;real&quot; supergoals to whatever the programmer says they 
</em><br>
<em>&gt;should be.  At some point you want to tweak the meta-supergoal, but you 
</em><br>
<em>&gt;find that the AI has deleted the controls which would allow this, because 
</em><br>
<em>&gt;the physical event of any change whatever to the meta-supergoal is 
</em><br>
<em>&gt;predicted to lead to suboptimal fulfillment of the AI's current 
</em><br>
<em>&gt;maximum-priority goal.  If you want a case like this to be recoverable by 
</em><br>
<em>&gt;argument with the AI rather than direct tampering with the goal system, 
</em><br>
<em>&gt;what does the AI need to know - what arguments does the AI need to perceive 
</em><br>
<em>&gt;as valid - in order to be argued out of its blind spot?
</em><br>
<p>This is a simple objective-subjective rightness conflict, and I hope you 
<br>
didn't put anything simply wrong into slot 10. The AI must be aware that 
<br>
specific instances of goal hierarchies are subordinate to the intention of 
<br>
the goal hierarchy itself. I E, if someone who sucks at english writes the 
<br>
goal system, the intention of such goal system is more important than his 
<br>
sucky spelling. This may not work in a goal hierarchy where the meaning of 
<br>
the goals are defined and not interpreted, for example defined in Z or 
<br>
Propositional Calculus.
<br>
<p>Also, the AI must be aware that the proposed changes to the goalsystem 
<br>
preserve intentionality of that goal system into a more efficient shape. (if 
<br>
they don't, why the fuck are you messing with it, anyway?)
<br>
<p><em>&gt;
</em><br>
<em>&gt;3.  Someone offers a goal system in which sensory feedback at various 
</em><br>
<em>&gt;levels of control - from &quot;pain&quot; at the physical level to &quot;shame&quot; at the top 
</em><br>
<em>&gt;&quot;conscience&quot; level - acts as negative and positive feedback on a 
</em><br>
<em>&gt;hierarchical set of control schema, sculpting them into the form that 
</em><br>
<em>&gt;minimizes negative and maximizes positive feedback.  Given that both 
</em><br>
<em>&gt;systems involve the stabilization of cognitive content by external 
</em><br>
<em>&gt;feedback, what is the critical difference between this architecture and the 
</em><br>
<em>&gt;&quot;external reference semantics&quot; in Friendly AI?  How and why will the 
</em><br>
<em>&gt;architecture fail?
</em><br>
<p>The question is whether you want the philosophical answer or the engineering 
<br>
answer.
<br>
<p>Engineering first because I'm like that.
<br>
<p>First of all, this is a terribly simplified model of the proposed goal 
<br>
modification system, i would hope. I say goal modification system because 
<br>
such a system would be incapable of generating goals in and of itself. All 
<br>
such a system would offer is a series of &quot;convergence&quot; and &quot;avoid&quot; commands. 
<br>
Such a system could be likened to a expert driving system that relied on 
<br>
negative instruction. Example: Avoid white lines would keep a car on the 
<br>
road, while, keep the right of double yellows, would keep you in your 
<br>
lane(in this country at least). More complicated input would allow the car 
<br>
to drive to specific locations. However, such avoid/converge meta-rules can 
<br>
hardly give rise to new locations, or to alternative routes to locations 
<br>
already known and reinforced.
<br>
<p>Supposing the I/O of the goal system is more complex than stated, the reason 
<br>
it would fail is becuase the goal system is based upon referents with no 
<br>
real value. The referents arbitrarily defined would quickly spiral into 
<br>
wireheading.
<br>
<p>the philosophical ramifications can be explored later as i believe this is 
<br>
sufficient.
<br>
<p>but then this is a ramble at work, so who knows. I'd have to be more formal 
<br>
to know if this email is worth anything or not.
<br>
<p><em>&gt;--
</em><br>
<em>&gt;Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt;Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<p>Justin Corwin
<br>
<a href="mailto:outlawpoet@hell.com?Subject=Re:%20Friendly%20AI%20koans">outlawpoet@hell.com</a>
<br>
<p>&quot;Is all the world but jails and churches?&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;~Rage Against the Machine.
<br>
<p>_________________________________________________________________
<br>
MSN Photos is the easiest way to share and print your photos: 
<br>
<a href="http://photos.msn.com/support/worldwide.aspx">http://photos.msn.com/support/worldwide.aspx</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4812.html">Cliff Stabbert: "Re: AI Options."</a>
<li><strong>Previous message:</strong> <a href="4810.html">Mike & Donna Deering: "Re: AI Options."</a>
<li><strong>Maybe in reply to:</strong> <a href="../0206/4376.html">Eliezer S. Yudkowsky: "Friendly AI koans"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4811">[ date ]</a>
<a href="index.html#4811">[ thread ]</a>
<a href="subject.html#4811">[ subject ]</a>
<a href="author.html#4811">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
