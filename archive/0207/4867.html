<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: SL4 meets &quot;Pinky and the Brain&quot;</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: SL4 meets &quot;Pinky and the Brain&quot;">
<meta name="Date" content="2002-07-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: SL4 meets &quot;Pinky and the Brain&quot;</h1>
<!-- received="Tue Jul 16 11:19:23 2002" -->
<!-- isoreceived="20020716171923" -->
<!-- sent="Tue, 16 Jul 2002 09:07:40 -0600" -->
<!-- isosent="20020716150740" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: SL4 meets &quot;Pinky and the Brain&quot;" -->
<!-- id="LAEGJLOGJIOELPNIOOAJOENFCMAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D33F127.3060700@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20SL4%20meets%20&quot;Pinky%20and%20the%20Brain&quot;"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Tue Jul 16 2002 - 09:07:40 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4868.html">Gordon Worley: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<li><strong>Previous message:</strong> <a href="4866.html">Ben Goertzel: "RE: Why do we seek to transcend ourselves?"</a>
<li><strong>In reply to:</strong> <a href="4858.html">Eliezer S. Yudkowsky: "SL4 meets &quot;Pinky and the Brain&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4883.html">James Higgins: "Re: SL4 meets &quot;Pinky and the Brain&quot;"</a>
<li><strong>Reply:</strong> <a href="4883.html">James Higgins: "Re: SL4 meets &quot;Pinky and the Brain&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4867">[ date ]</a>
<a href="index.html#4867">[ thread ]</a>
<a href="subject.html#4867">[ subject ]</a>
<a href="author.html#4867">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eli wrote:
<br>
<em>&gt; James Higgins wrote:
</em><br>
<em>&gt; &gt; Mike &amp; Donna Deering wrote:
</em><br>
<em>&gt; &gt;  &gt; a human.  Or for that matter the status of an AI programmer.
</em><br>
<em>&gt;  How do we
</em><br>
<em>&gt; &gt;  &gt; know that Eliezer isn't trying to take over the world for his own
</em><br>
<em>&gt; &gt; purposes?
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Um, what else do you think Eliezer IS trying to do if not take over the
</em><br>
<em>&gt; &gt; world for his own purposes?  That may not be the way he states it but
</em><br>
<em>&gt; &gt; that is obviously his goal.  His previously stated goal is to initiate
</em><br>
<em>&gt; &gt; the Sysop which would, by proxy, take over the world.  He states that
</em><br>
<em>&gt; &gt; this is to help all people on the Earth, but it is still him taking over
</em><br>
<em>&gt; &gt; the world for his own purposes.
</em><br>
<em>&gt;
</em><br>
<em>&gt; James, this is pure slander.
</em><br>
...
<br>
<em>&gt;  Ben, on
</em><br>
<em>&gt; the other hand, has stated in clear mathematical terms his intention to
</em><br>
<em>&gt; optimize the world according to his own personal goal system, yet you
</em><br>
<em>&gt; don't seem to worry about this at all.  I'm not trying to attack Ben,
</em><br>
<em>&gt; just pointing out that your priorities are insane.  Apparently you don't
</em><br>
<em>&gt; listen to what either Ben or I say.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Oh, well.  I've been genuinely, seriously accused of trying to take over
</em><br>
<em>&gt; the world.  There probably aren't many people in the world who can put
</em><br>
<em>&gt; that on their CV.
</em><br>
<p>World domination, huh?  Sounds like a blast!  Where do I sign up?  ;&gt;
<br>
<p>Seriously: I think I'm going to have to side with Eli on this topic.
<br>
<p>&quot;Taking over the world&quot; has the flavor of trying to make oneself,
<br>
personally, the ruler of the world, so that one can enforce one's whims and
<br>
desires and plans on the world in detail.  This is not what Eliezer is
<br>
proposing, exactly.
<br>
<p>In fact, he is not even proposing to create software that will definitely
<br>
&quot;take over the world&quot;.
<br>
<p>I think he is proposing to create software that will have a *huge influence*
<br>
on the world, but not necessarily control it in any full &amp; complete way.
<br>
<p>And, I am proposing to do effectively the same thing.  Anyone seeking to
<br>
produce superhuman AI is really pushing in this direction, whether they
<br>
admit it to themselves or not.  It's only to be expected that a superhumanly
<br>
intelligent mind is going to
<br>
<p>1) have the capability to &quot;rule the world.&quot;
<br>
<p>2) exercise at least its capability to *strongly influence* the world
<br>
[understanding that it may lack the inclination to actually *rule* the
<br>
world]
<br>
<p>To illustrate this point, let's consider a science-fictional &quot;semi-hard
<br>
takeoff&quot; scenario.  Suppose in 2040 we have a world with lots of advanced
<br>
tech, including a superhuman mind living in a data warehouse in Peoria.
<br>
Suppose some human loonies try to hijack a plane and fly it into the data
<br>
warehouse.  What's the AI gonna do?  Ok, it's going to stop the plane from
<br>
making impact.  But after that, what?  It has three choices
<br>
<p>1) take over the world, enforcing a benevolent dictatorship to prevent
<br>
stupid humans from doing future stupid things to it and to each other
<br>
2) make itself super-secure and hide out, letting us humans maul each other
<br>
as we wish, but making itself impervious to damage
<br>
3) try to nudge and influence the human world, to make it a better place
<br>
(while making itself more secure at the same time)...
<br>
<p>Let's say it mulls things over and decides it has a responsiblity to help
<br>
humans as well as itself, so it chooses path 3).  But it doesn't want to be
<br>
too intrusive.  It decides that releasing drugs into the water supply that
<br>
would make us less violent would be too controlling and intrusive, too
<br>
dictatorial.  So it decides to release a global advertising campaign,
<br>
calculated with superhuman intelligence to affect human attitudes in a
<br>
certain way.  It creates movies, video games, ad spots, teledildonic fantasy
<br>
VR scenarios.  It discovers it can control our minds highly effectively in
<br>
this way, without resorting to direct brain control or to physical violence
<br>
based control.
<br>
<p>This sci-fi scenario is intended to illustrate that there's a fine line
<br>
between &quot;influencing the world&quot; and &quot;ruling the world&quot;... and that there
<br>
will potentially be pressure for a superhuman mind -- even a Friendly one --
<br>
to do a dance on this fine line.
<br>
<p>Of course, there are *many* possible future scenarios, I've just given this
<br>
one as an example.  It's a scenario from an interim period between
<br>
human-level AI and Sysop-level AI, and in some theories of the hard takeoff,
<br>
this interim period will not exist.
<br>
<p>It's not a question of trying to take over the world, it's a question of
<br>
trying to build and bias future beings that are going to either take over
<br>
the world or strongly influence it.
<br>
<p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4868.html">Gordon Worley: "Re: AI-Box Experiment 2: Yudkowsky and McFadzean"</a>
<li><strong>Previous message:</strong> <a href="4866.html">Ben Goertzel: "RE: Why do we seek to transcend ourselves?"</a>
<li><strong>In reply to:</strong> <a href="4858.html">Eliezer S. Yudkowsky: "SL4 meets &quot;Pinky and the Brain&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4883.html">James Higgins: "Re: SL4 meets &quot;Pinky and the Brain&quot;"</a>
<li><strong>Reply:</strong> <a href="4883.html">James Higgins: "Re: SL4 meets &quot;Pinky and the Brain&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4867">[ date ]</a>
<a href="index.html#4867">[ thread ]</a>
<a href="subject.html#4867">[ subject ]</a>
<a href="author.html#4867">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
