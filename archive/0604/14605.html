<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: AI Goals  [WAS Re: The Singularity vs. the Wall]</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="AI Goals  [WAS Re: The Singularity vs. the Wall]">
<meta name="Date" content="2006-04-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>AI Goals  [WAS Re: The Singularity vs. the Wall]</h1>
<!-- received="Tue Apr 25 08:01:19 2006" -->
<!-- isoreceived="20060425140119" -->
<!-- sent="Tue, 25 Apr 2006 09:56:06 -0400" -->
<!-- isosent="20060425135606" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="AI Goals  [WAS Re: The Singularity vs. the Wall]" -->
<!-- id="444E2A76.1080902@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="6fdad3790604242010h1ff4f85t7655fdf56c219846@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20AI%20Goals%20%20[WAS%20Re:%20The%20Singularity%20vs.%20the%20Wall]"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Tue Apr 25 2006 - 07:56:06 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14606.html">Ben Goertzel: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<li><strong>Previous message:</strong> <a href="14604.html">Phillip Huggan: "Re: The Singularity vs. the Wall"</a>
<li><strong>In reply to:</strong> <a href="14603.html">Philip Goetz: "Re: The Singularity vs. the Wall"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14606.html">Ben Goertzel: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<li><strong>Reply:</strong> <a href="14606.html">Ben Goertzel: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<li><strong>Reply:</strong> <a href="14614.html">Mikko Särelä: "Re: AI Goals  [WAS Re: The Singularity vs. the Wall]"</a>
<li><strong>Reply:</strong> <a href="14637.html">Mike Dougherty: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14605">[ date ]</a>
<a href="index.html#14605">[ thread ]</a>
<a href="subject.html#14605">[ subject ]</a>
<a href="author.html#14605">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Philip Goetz wrote:
<br>
<em>&gt; On 4/24/06, Richard Loosemore &lt;<a href="mailto:rpwl@lightlink.com?Subject=Re:%20AI%20Goals%20%20[WAS%20Re:%20The%20Singularity%20vs.%20the%20Wall]">rpwl@lightlink.com</a>&gt; wrote:
</em><br>
<em>&gt;&gt; I could pick up on several different examples, but let me grab the most
</em><br>
<em>&gt;&gt; important one.  You seem to be saying (correct me if I am wrong) that an
</em><br>
<em>&gt;&gt; AGI will go around taking the same sort of risks with its technological
</em><br>
<em>&gt;&gt; experiments that we take with ours, and that because its experiments
</em><br>
<em>&gt;&gt; could hold existential risks for us, we should be very afraid.  But
</em><br>
<em>&gt;&gt; there is no reason to suppose that it would take such risks, and many,
</em><br>
<em>&gt;&gt; many reasons why it would specifically not do the kind of risky stuff
</em><br>
<em>&gt;&gt; that we do:  if you look at all the societal and other pressures that
</em><br>
<em>&gt;&gt; cause humans to engage in engineering ventures that might have serious
</em><br>
<em>&gt;&gt; side effects, you find that all the drivers behind those proessures come
</em><br>
<em>&gt;&gt; from internal psychological factors that would not be present in the
</em><br>
<em>&gt;&gt; AGI.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The problem is that the AI's goals might not be our goals.  At all.
</em><br>
<em>&gt; Most people would say we haven't taken many outrageous risks in
</em><br>
<em>&gt; settling America, and yet the number of wolves has declined by a
</em><br>
<em>&gt; factor of as much as a thousand!  How did that happen?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; (Aside: It is worth asking whether a free AI or an AI controlled by
</em><br>
<em>&gt; humans would be more dangerous.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<p>I think that the question of an AI's &quot;goals&quot; is the most important issue 
<br>
lurking beneath many of the discussions that take place on this list.
<br>
<p>The problem is, most people plunge into this question without stopping 
<br>
to consider what it is they are actually talking about.  I don't mean 
<br>
this in a critical or offensive way, I mean that we have spent so little 
<br>
time thinking about what it means for a thing to have goals or 
<br>
motivations, that we don't have a clear concept of what &quot;goals&quot; actually 
<br>
are.  We don't have a science of AGI motivation yet, we only have wild 
<br>
guesswork.
<br>
<p>And there are many traps that guessing will get us into:
<br>
<p>One trap is to do a simple extrapolation from the way that humans behave 
<br>
towards other species:  we had our goals, the wolves had theirs, and 
<br>
look what happened to the worlves, as you put it above.  But why would 
<br>
an AGI be motivated the way the human species is collectively motivated?
<br>
<p>A second trap is to suppose that when we build an AGI, the goals and 
<br>
motivations of the AGI will be something we discover afterwards, when it 
<br>
is too late.  Almost every comment on this list that expresses concern 
<br>
about what an AGI would do, has this assumption lurking out back.  There 
<br>
are good reasons to believe that we, the designers of the AGI, would 
<br>
have complete control over what its motivations would be.  Worried that 
<br>
it might wipe us out without caring?  Then don't design it without a 
<br>
&quot;caring&quot; module!  (I am oversimplifying for rhetorical effect, but you 
<br>
know what I mean).
<br>
<p>A third trap is to suppose that it could be an AGI and also be in some 
<br>
sense completely dumb, like being superintelligent but also being under 
<br>
the control of an idiot human dictator.  Sure, maybe this is possible: 
<br>
but we shouldn't just assume it can happen and start ranting about how 
<br>
bad it would be, we should talk technical details about exactly how it 
<br>
might work (and FWIW, I don't think it could be made to work at all).
<br>
<p>Here is another subtle issue:  is there going to be one AGI, or are 
<br>
there going to be thousands/millions/billions of them?  The assumption 
<br>
always seems to be &quot;lots of them,&quot; but is this realistic?  It might well 
<br>
be only on AGI, with large numbers of drones that carry out dumb donkey 
<br>
work for the central AGI.  Now in that case, you suddenly get a 
<br>
situation in which there are no collective effects of conflicting 
<br>
motivations among the members of the AGI species.  At the very least, 
<br>
all the questions about goals and species dominance get changed by this 
<br>
one-AGI scenario, and yet people make the default assumption that this 
<br>
is not going to happen:  I think it very likely indeed.
<br>
<p>Finally, there are many people (especially on this list) who assume that 
<br>
an AGI will be an RPOP whose motivations are nothing more than a 
<br>
sophisticated goal stack:  this begs many questions about whether such a 
<br>
primitive goal system would actually work.  These questions are 
<br>
enormously technical, and some of them may not be resolvable without a 
<br>
good deal more hard coding on big systems, but the answers to the 
<br>
questions are utterly huge, when it comes to deciding what kind of 
<br>
creature we might be dealing with.  If RPOPs are stable and easy to 
<br>
build, we get (IMO) a very dangerous kind of AGI.  If RPOPs are 
<br>
difficult to get to work, or if they don't work at all, and if 
<br>
human-style motivational systems are used instead, (again, IMO) we could 
<br>
have an extremely safe kind of AGI that would never, ever be vlunerable 
<br>
to any of the issues that people tear their hair about.  Enormous 
<br>
difference in outcomes, between these two possibilities:  but how many 
<br>
people are discussing the distinctions I am making here?  Practically 
<br>
zero!  [I don't have very good discussions with myself, you see :-)]
<br>
<p>Overall message:  let's have some serious and detailed discussion to 
<br>
explore the space of possibilities.  Let's not make one default 
<br>
assumption about what would motivate an AGI, and then run with it; 
<br>
let's find out what the options are, then debate the ramifactions.
<br>
<p>I don't think we can easily do that on this list, because things get too 
<br>
heated, alas.
<br>
<p><p>Richard Loosemore
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14606.html">Ben Goertzel: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<li><strong>Previous message:</strong> <a href="14604.html">Phillip Huggan: "Re: The Singularity vs. the Wall"</a>
<li><strong>In reply to:</strong> <a href="14603.html">Philip Goetz: "Re: The Singularity vs. the Wall"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14606.html">Ben Goertzel: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<li><strong>Reply:</strong> <a href="14606.html">Ben Goertzel: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<li><strong>Reply:</strong> <a href="14614.html">Mikko Särelä: "Re: AI Goals  [WAS Re: The Singularity vs. the Wall]"</a>
<li><strong>Reply:</strong> <a href="14637.html">Mike Dougherty: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14605">[ date ]</a>
<a href="index.html#14605">[ thread ]</a>
<a href="subject.html#14605">[ subject ]</a>
<a href="author.html#14605">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
