<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI Goals [WAS Re: The Singularity vs. the Wall]</title>
<meta name="Author" content="micah glasser (micahglasser@gmail.com)">
<meta name="Subject" content="Re: AI Goals [WAS Re: The Singularity vs. the Wall]">
<meta name="Date" content="2006-04-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI Goals [WAS Re: The Singularity vs. the Wall]</h1>
<!-- received="Tue Apr 25 12:09:33 2006" -->
<!-- isoreceived="20060425180933" -->
<!-- sent="Tue, 25 Apr 2006 14:09:30 -0400" -->
<!-- isosent="20060425180930" -->
<!-- name="micah glasser" -->
<!-- email="micahglasser@gmail.com" -->
<!-- subject="Re: AI Goals [WAS Re: The Singularity vs. the Wall]" -->
<!-- id="23bd28ec0604251109q11d0aefdh385eaf2a6d69ba6d@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="444E56C4.8050208@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> micah glasser (<a href="mailto:micahglasser@gmail.com?Subject=Re:%20AI%20Goals%20[WAS%20Re:%20The%20Singularity%20vs.%20the%20Wall]"><em>micahglasser@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Apr 25 2006 - 12:09:30 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14617.html">Phillip Huggan: "Re: AI Goals definition"</a>
<li><strong>Previous message:</strong> <a href="14615.html">Ben Goertzel: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<li><strong>In reply to:</strong> <a href="14612.html">Richard Loosemore: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14618.html">Jef Allbright: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14616">[ date ]</a>
<a href="index.html#14616">[ thread ]</a>
<a href="subject.html#14616">[ subject ]</a>
<a href="author.html#14616">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
All intelligent life on this planet have at least one goal in common: to
<br>
continue the process of replication. From this fact comes scarcity,
<br>
competition, and the threat of extinction from competing replication systems
<br>
(life forms).
<br>
Taking this into consideration it would seem that any entity (including AGI)
<br>
that has reproductive replication as a super-goal could conceivably be a
<br>
threat to the human super-goal of reproductive replication/ genetic
<br>
preservation. Unfortunately this circumstance sets up a catch 22. Any system
<br>
that is intelligent enough to self-recursively improve its own design and
<br>
level of intelligence is inherently a system with the goal of replication
<br>
and is a threat to human existence so long as there is scarcity of resources
<br>
shared between the two competing replication systems (humans and AGI
<br>
respectively).
<br>
So the question I raise is whether or not friendly AGI is even a theoretical
<br>
possibility. If it is a possibility then the next question we must ask is
<br>
how can we program an AGI that is recursively self-improving yet not in
<br>
competition with human civilization for vital resources. Probably the
<br>
solution to this conundrum is simply that any AGI should have human
<br>
preservation and human flourishing as its super goal with recursive
<br>
reproductive self-improvement taking place only to the extent, and at a
<br>
rate, that is conducive to this goal.
<br>
If this kind of algorithm can be successfully implemented then it would not
<br>
only constitute a benevolent AGI but it would also prevent a violently hard
<br>
take-off because the AGI would not just recursively self-improve its
<br>
hardware and programming as fast as it could but would moderate its own
<br>
development according to what is best for human survival and flourishing.
<br>
<p>On 4/25/06, Richard Loosemore &lt;<a href="mailto:rpwl@lightlink.com?Subject=Re:%20AI%20Goals%20[WAS%20Re:%20The%20Singularity%20vs.%20the%20Wall]">rpwl@lightlink.com</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; Jef Allbright wrote:
</em><br>
<em>&gt; &gt; On 4/25/06, Ben Goertzel &lt;<a href="mailto:ben@goertzel.org?Subject=Re:%20AI%20Goals%20[WAS%20Re:%20The%20Singularity%20vs.%20the%20Wall]">ben@goertzel.org</a>&gt; wrote:
</em><br>
<em>&gt; &gt;&gt;&gt; I think that the question of an AI's &quot;goals&quot; is the most important
</em><br>
<em>&gt; issue
</em><br>
<em>&gt; &gt;&gt;&gt; lurking beneath many of the discussions that take place on this list.
</em><br>
<em>&gt; &gt;&gt;&gt;
</em><br>
<em>&gt; &gt;&gt;&gt; The problem is, most people plunge into this question without stopping
</em><br>
<em>&gt; &gt;&gt;&gt; to consider what it is they are actually talking about.
</em><br>
<em>&gt; &gt;&gt; Richard, this is a good point.
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt; &quot;Goal&quot;, like &quot;free will&quot; or &quot;consciousness&quot; or &quot;memory&quot;, is
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Building upon Ben's points, much of the confusion with regard to
</em><br>
<em>&gt; &gt; consciousness, free will, etc., is that we tend to fall into the trap
</em><br>
<em>&gt; &gt; of thinking that there is some independent entity to which we attach
</em><br>
<em>&gt; &gt; these attributes.  If we think in terms of describing the behavior of
</em><br>
<em>&gt; &gt; systems, with the understanding that each level of system necessarily
</em><br>
<em>&gt; &gt; exists and interacts within a larger context, then this whole class of
</em><br>
<em>&gt; &gt; confusion falls away.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; - Jef
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; Hmmmm.... I wasn't sure I would go along with the idea that goals are in
</em><br>
<em>&gt; the same category of misunderstoodness as free will, consciousness and
</em><br>
<em>&gt; memory.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I agree that when these terms are used in a very general way they are
</em><br>
<em>&gt; often misused.
</em><br>
<em>&gt;
</em><br>
<em>&gt; BUt in the case of goals and motivations, would we not agree that an AGI
</em><br>
<em>&gt; would have some system that was responsible for maintaining and
</em><br>
<em>&gt; governing goals and motivations?
</em><br>
<em>&gt;
</em><br>
<em>&gt; I am happy to let it be a partially distributed system, so that the
</em><br>
<em>&gt; actual moment to moment state of the goal system might be determined by
</em><br>
<em>&gt; a collective, rather than one single mechanism, but would it make sense
</em><br>
<em>&gt; to say that there is no mechanism at all?
</em><br>
<em>&gt;
</em><br>
<em>&gt; If your point were about free will, I would agree completely with your
</em><br>
<em>&gt; comment.  About consciousness .... well, not so much (but I am writing a
</em><br>
<em>&gt; paper on that right now, so I am prejudiced).  About memory?  That
</em><br>
<em>&gt; sounds much more of a real thing than free will, surely?  I don't think
</em><br>
<em>&gt; that is a fiction.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Richard Loosemore.
</em><br>
<em>&gt;
</em><br>
<p><p><p><pre>
--
I swear upon the alter of God, eternal hostility to every form of tyranny
over the mind of man. - Thomas Jefferson
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14617.html">Phillip Huggan: "Re: AI Goals definition"</a>
<li><strong>Previous message:</strong> <a href="14615.html">Ben Goertzel: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<li><strong>In reply to:</strong> <a href="14612.html">Richard Loosemore: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14618.html">Jef Allbright: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14616">[ date ]</a>
<a href="index.html#14616">[ thread ]</a>
<a href="subject.html#14616">[ subject ]</a>
<a href="author.html#14616">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
