<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI Goals [WAS Re: The Singularity vs. the Wall]</title>
<meta name="Author" content="Jef Allbright (jef@jefallbright.net)">
<meta name="Subject" content="Re: AI Goals [WAS Re: The Singularity vs. the Wall]">
<meta name="Date" content="2006-04-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI Goals [WAS Re: The Singularity vs. the Wall]</h1>
<!-- received="Tue Apr 25 13:12:38 2006" -->
<!-- isoreceived="20060425191238" -->
<!-- sent="Tue, 25 Apr 2006 12:12:12 -0700" -->
<!-- isosent="20060425191212" -->
<!-- name="Jef Allbright" -->
<!-- email="jef@jefallbright.net" -->
<!-- subject="Re: AI Goals [WAS Re: The Singularity vs. the Wall]" -->
<!-- id="22360fa10604251212j32f3cd17se891a261a2b52ada@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="444E56C4.8050208@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Jef Allbright (<a href="mailto:jef@jefallbright.net?Subject=Re:%20AI%20Goals%20[WAS%20Re:%20The%20Singularity%20vs.%20the%20Wall]"><em>jef@jefallbright.net</em></a>)<br>
<strong>Date:</strong> Tue Apr 25 2006 - 13:12:12 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14619.html">Woody Long: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<li><strong>Previous message:</strong> <a href="14617.html">Phillip Huggan: "Re: AI Goals definition"</a>
<li><strong>In reply to:</strong> <a href="14612.html">Richard Loosemore: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14621.html">Phillip Huggan: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<li><strong>Reply:</strong> <a href="14621.html">Phillip Huggan: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14618">[ date ]</a>
<a href="index.html#14618">[ thread ]</a>
<a href="subject.html#14618">[ subject ]</a>
<a href="author.html#14618">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 4/25/06, Richard Loosemore &lt;<a href="mailto:rpwl@lightlink.com?Subject=Re:%20AI%20Goals%20[WAS%20Re:%20The%20Singularity%20vs.%20the%20Wall]">rpwl@lightlink.com</a>&gt; wrote:
<br>
<em>&gt; Jef Allbright wrote:
</em><br>
<em>&gt; &gt; On 4/25/06, Ben Goertzel &lt;<a href="mailto:ben@goertzel.org?Subject=Re:%20AI%20Goals%20[WAS%20Re:%20The%20Singularity%20vs.%20the%20Wall]">ben@goertzel.org</a>&gt; wrote:
</em><br>
<em>&gt; &gt;&gt;&gt; I think that the question of an AI's &quot;goals&quot; is the most important issue
</em><br>
<em>&gt; &gt;&gt;&gt; lurking beneath many of the discussions that take place on this list.
</em><br>
<em>&gt; &gt;&gt;&gt;
</em><br>
<em>&gt; &gt;&gt;&gt; The problem is, most people plunge into this question without stopping
</em><br>
<em>&gt; &gt;&gt;&gt; to consider what it is they are actually talking about.
</em><br>
<em>&gt; &gt;&gt; Richard, this is a good point.
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt; &quot;Goal&quot;, like &quot;free will&quot; or &quot;consciousness&quot; or &quot;memory&quot;, is
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Building upon Ben's points, much of the confusion with regard to
</em><br>
<em>&gt; &gt; consciousness, free will, etc., is that we tend to fall into the trap
</em><br>
<em>&gt; &gt; of thinking that there is some independent entity to which we attach
</em><br>
<em>&gt; &gt; these attributes.  If we think in terms of describing the behavior of
</em><br>
<em>&gt; &gt; systems, with the understanding that each level of system necessarily
</em><br>
<em>&gt; &gt; exists and interacts within a larger context, then this whole class of
</em><br>
<em>&gt; &gt; confusion falls away.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; - Jef
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; Hmmmm.... I wasn't sure I would go along with the idea that goals are in
</em><br>
<em>&gt; the same category of misunderstoodness as free will, consciousness and
</em><br>
<em>&gt; memory.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I agree that when these terms are used in a very general way they are
</em><br>
<em>&gt; often misused.
</em><br>
<em>&gt;
</em><br>
<p>Each of these topics is an attractor for confusion when people aren't
<br>
clear to distinguish between subjective and objective descriptions and
<br>
neglect the larger context without which a description is necessarily
<br>
incomplete.
<br>
<p>Goals can be described precisely only within a specified context.  We
<br>
can speak precisely about the &quot;goals&quot; of a feedback loop, or multiple
<br>
loops in a complex physical system, whether it be electronic,
<br>
mechanical, chemical, or some combination. Note that we can speak
<br>
precisely even of those parameters that we can't currently quantify. 
<br>
There's no confusion about what we mean by goals when the context is
<br>
clearly understood.  However, when we try to speak of goals in
<br>
relation to a subjective agent, we must be very careful, because the
<br>
subjective element doesn't have the same kind of first order existence
<br>
as the physical system and it's encompassing environment.  Confusion
<br>
arises because the subjective element is already a description of an
<br>
aspect of the system, removed from the actual system itself.  If I
<br>
were to speak of &quot;my goals&quot;, it must be understood in common terms,
<br>
but the closer one looks, the more one sees that they're really not
<br>
&quot;my&quot; goals because the subjective &quot;I&quot; doesn't have an independent
<br>
existence.  &quot;I&quot; am more precisely defined as a behavior of a system
<br>
within a given context.
<br>
<p>Similarly with &quot;free-will&quot;.  Certainly we can all speak of free-will
<br>
within the context of common human social interactions and it makes
<br>
sense.  In fact our legal and judicial system, as well as
<br>
moral/ethical beliefs and behavior depend on it.  However, just as
<br>
with the self, the closer one looks, the more it is apparent that
<br>
there is no ultimate free-will, and that all interactions can be
<br>
described precisely (including describing the degree of uncertaintly)
<br>
within a deterministic framework of explanation.  In fact, if our
<br>
behavior were not deterministic, we would lose the &quot;free-will&quot;--the
<br>
ability to choose--that we do have.
<br>
<p>Similarly with memory.  When we speak of our memory there is an
<br>
inherent subjective aspect.  We do not often acknowledge this,
<br>
especially since ones memories are an important part of ones personal
<br>
identity. We can say truthfully that we remember events from our past,
<br>
but the closer we look, the more we see that memories are subject to
<br>
distortion, gaps, confabulation, and outright fabrication.  We can
<br>
speak precisely of memory in a well-defined objective context, such as
<br>
a memory device in a computer, but when we speak of the memory of a
<br>
subjective agent, even an AI with the capability of accurate
<br>
introspection, we must be clear to distinguish between subjective and
<br>
objective descriptions.
<br>
<p>[Note that there is no truly &quot;objective&quot; description since none of us
<br>
can observe from a vantage point completely outside the system, but we
<br>
can use the term effectively as long as we always recognize the
<br>
importance of context.]
<br>
<p><p><em>&gt; BUt in the case of goals and motivations, would we not agree that an AGI
</em><br>
<em>&gt; would have some system that was responsible for maintaining and
</em><br>
<em>&gt; governing goals and motivations?
</em><br>
<p>Goals are always about controlling some (complex) parameter relative
<br>
to something else.  Given a well-specified context, then we can
<br>
precisely define goals. Goals are necessary for an AGI, but I believe
<br>
they must evolve. Within an evolving model of an evolving environment,
<br>
to be invariant is to die.
<br>
<p>With regard to developing safe AI, I don't think there can be any
<br>
guarantee.  The best we can do is to incorporate a model of human
<br>
values as broad-based as possible, and to promote the growth of our
<br>
evolving values based on principles rather than ends.
<br>
<p><em>&gt;
</em><br>
<em>&gt; I am happy to let it be a partially distributed system, so that the
</em><br>
<em>&gt; actual moment to moment state of the goal system might be determined by
</em><br>
<em>&gt; a collective, rather than one single mechanism, but would it make sense
</em><br>
<em>&gt; to say that there is no mechanism at all?
</em><br>
<em>&gt;
</em><br>
<em>&gt; If your point were about free will, I would agree completely with your
</em><br>
<em>&gt; comment.  About consciousness .... well, not so much (but I am writing a
</em><br>
<em>&gt; paper on that right now, so I am prejudiced).  About memory?  That
</em><br>
<em>&gt; sounds much more of a real thing than free will, surely?  I don't think
</em><br>
<em>&gt; that is a fiction.
</em><br>
<p>As many list members know, I often point out that Self,
<br>
Conciousness/Qualia, Free-will, Morality, and Social Decision-making
<br>
(politics) all have an inherent subjective element that commonly, but
<br>
not necessarily, leads to confusion.
<br>
<p>To take it up a further level, we can never overcome the problem of
<br>
induction as described by Hume, but I see no reason why we should want
<br>
to.  We need only remain aware of the importance of context to any
<br>
description.
<br>
<p>- Jef
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14619.html">Woody Long: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<li><strong>Previous message:</strong> <a href="14617.html">Phillip Huggan: "Re: AI Goals definition"</a>
<li><strong>In reply to:</strong> <a href="14612.html">Richard Loosemore: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14621.html">Phillip Huggan: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<li><strong>Reply:</strong> <a href="14621.html">Phillip Huggan: "Re: AI Goals [WAS Re: The Singularity vs. the Wall]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14618">[ date ]</a>
<a href="index.html#14618">[ thread ]</a>
<a href="subject.html#14618">[ subject ]</a>
<a href="author.html#14618">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
