<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The Singularity vs. the Wall</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Re: The Singularity vs. the Wall">
<meta name="Date" content="2006-04-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The Singularity vs. the Wall</h1>
<!-- received="Mon Apr 24 07:48:14 2006" -->
<!-- isoreceived="20060424134814" -->
<!-- sent="Mon, 24 Apr 2006 09:47:13 -0400" -->
<!-- isosent="20060424134713" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Re: The Singularity vs. the Wall" -->
<!-- id="444CD6E1.1000300@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="20060424045328.33966.qmail@web61315.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20The%20Singularity%20vs.%20the%20Wall"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Mon Apr 24 2006 - 07:47:13 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14590.html">Charles D Hixson: "Re: Fwd: We Can Understand Anything, But are Just a Bit Slow"</a>
<li><strong>Previous message:</strong> <a href="14588.html">Richard Loosemore: "Re: The Singularity vs. the Wall"</a>
<li><strong>In reply to:</strong> <a href="14586.html">Phillip Huggan: "Re: The Singularity vs. the Wall"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14603.html">Philip Goetz: "Re: The Singularity vs. the Wall"</a>
<li><strong>Reply:</strong> <a href="14603.html">Philip Goetz: "Re: The Singularity vs. the Wall"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14589">[ date ]</a>
<a href="index.html#14589">[ thread ]</a>
<a href="subject.html#14589">[ subject ]</a>
<a href="author.html#14589">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Some of your points are well taken (there might be hugely dramatic 
<br>
technologies that come along after the Singularity, like time travel, 
<br>
which make us forget how fabulous the Singularity itself once seemed), 
<br>
but on some other points you and I are talking about different things 
<br>
while using the same word.
<br>
<p>I do not assume &quot;AGI&quot; to be just another form of computer engineering. 
<br>
I believe there are strong reasons to argue that it would arrive in 
<br>
something close to human-mind form, not RPOP form.  (The argument in a 
<br>
nutshell:  it is just *easier* to do it that way, so this is the first 
<br>
way that it will be done).  The main thing that would be different about 
<br>
this AGI is that it would not have all the dangerous stuff that 
<br>
evolution (in her sincere but cockeyed wisdom) designed into the human 
<br>
mind:  no aggression, no jealousy, no thirst for dominance, etc etc.
<br>
<p>&nbsp;From that perspective, some of the things you say about AGI simply do 
<br>
not make sense to me.
<br>
<p>I could pick up on several different examples, but let me grab the most 
<br>
important one.  You seem to be saying (correct me if I am wrong) that an 
<br>
AGI will go around taking the same sort of risks with its technological 
<br>
experiments that we take with ours, and that because its experiments 
<br>
could hold existential risks for us, we should be very afraid.  But 
<br>
there is no reason to suppose that it would take such risks, and many, 
<br>
many reasons why it would specifically not do the kind of risky stuff 
<br>
that we do:  if you look at all the societal and other pressures that 
<br>
cause humans to engage in engineering ventures that might have serious 
<br>
side effects, you find that all the drivers behind those proessures come 
<br>
from internal psychological factors that would not be present in the 
<br>
AGI.  For example, we do stuff at breakneck speed and cut corners 
<br>
because we don't want to wait a hundred years for every new technology, 
<br>
and we do that because we individually don't live that long.  If we 
<br>
lived for a million years, we could afford to hang on with a new 
<br>
technology and get it to the point where we were extremely sure of its 
<br>
safety.
<br>
<p>It would take a long essay to reinforce the point I am making here, but 
<br>
the bottom line is that from the moment that someone builds the kind of 
<br>
AGI I am talking about, we will be vastly safer than we are at the moment.
<br>
<p><p>Richard Loosemore.
<br>
<p><p><p>Phillip Huggan wrote:
<br>
<em>&gt; It's not fair to call AGI the only Singularity.  At the end AGI is just 
</em><br>
<em>&gt; another engineering
</em><br>
<em>&gt; technology; you still have to specify some goal for it to serve.  It 
</em><br>
<em>&gt; just so happens that a good chunk of our global industrial base is 
</em><br>
<em>&gt; devoted towards both improving computer hardware and programming bigger 
</em><br>
<em>&gt; software, so this powerful engineering technology may appear &quot;ahead of 
</em><br>
<em>&gt; its time&quot;.  But even post-AGI there should still be progress.  A 
</em><br>
<em>&gt; time-machine would give you direct access to the machine's whole future 
</em><br>
<em>&gt; light-cone volume instantly.  The ability to manufacture black-holes, 
</em><br>
<em>&gt; travel near C or even harvest antimatter may be a bigger accelerant 
</em><br>
<em>&gt; post-AGI, than AGI would be now.  In this context a technology like MNT 
</em><br>
<em>&gt; or indeed any exponential manufacturing method, can be seen as 
</em><br>
<em>&gt; Singularity-ish.
</em><br>
<em>&gt;  
</em><br>
<em>&gt; AGI spacecraft might not blow apart but their time-machines or their 
</em><br>
<em>&gt; particle accelerators might.  This is why I think humans should have the 
</em><br>
<em>&gt; ultimate say on any AGI action.  If ve is just allowed to force vis 
</em><br>
<em>&gt; engineering concerns, ve will eventually come across problems that may 
</em><br>
<em>&gt; be as intractable (from the PoV of us meatty fragile humans that will 
</em><br>
<em>&gt; suffer extinction from any AGI engineering-gone-wrong) to it as our 
</em><br>
<em>&gt; societal problems are to us.  With AGI you are attempting to take on all 
</em><br>
<em>&gt; human extinction risks in one giant bite.  I'm not saying don't do it, 
</em><br>
<em>&gt; I'm just saying don't hold achieving AGI to be the be all and the end 
</em><br>
<em>&gt; all (even though it might be), especially if a superior pathway presents 
</em><br>
<em>&gt; itself in the decades ahead.  Don't turn on SkyNet if we got a good 
</em><br>
<em>&gt; thing going in the decades ahead.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; */Richard Loosemore &lt;<a href="mailto:rpwl@lightlink.com?Subject=Re:%20The%20Singularity%20vs.%20the%20Wall">rpwl@lightlink.com</a>&gt;/* wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     This is an important thread.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     I think that Kurzweil's tendency to emphasize exponential curves of
</em><br>
<em>&gt;     general technology is really a nuisance. I don't care how many curves
</em><br>
<em>&gt;     he finds, I think that without AGI we simply cannot get more than a
</em><br>
<em>&gt;     certain rate of new technological development. Frankly, I think it will
</em><br>
<em>&gt;     hit a plateau. (I even have my suspicions that we already did hit the
</em><br>
<em>&gt;     plateau).
</em><br>
<em>&gt; 
</em><br>
<em>&gt;     And as for AGI, this for me is *the* definition of the Singularity.
</em><br>
<em>&gt;     None of the rest is very important. Without it, I see so many
</em><br>
<em>&gt;     limitations on the ability of human engineers to handle the complexity
</em><br>
<em>&gt;     of ordinary technology that I do not believe there will be any dramatic
</em><br>
<em>&gt;     advances in the near future. I think spacecraft will continue to blow
</em><br>
<em>&gt;     up because of single-line errors in their code, and the IRS will fail
</em><br>
<em>&gt;     again and again to upgrade their computer software.
</em><br>
<em>&gt;     &lt;SNIP&gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ------------------------------------------------------------------------
</em><br>
<em>&gt; Blab-away for as little as 1¢/min. Make PC-to-Phone Calls 
</em><br>
<em>&gt; &lt;<a href="http://us.rd.yahoo.com/mail_us/taglines/postman2/*http://us.rd.yahoo.com/evt=39663/*http://voice.yahoo.com">http://us.rd.yahoo.com/mail_us/taglines/postman2/*http://us.rd.yahoo.com/evt=39663/*http://voice.yahoo.com</a>&gt; 
</em><br>
<em>&gt; using Yahoo! Messenger with Voice.
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14590.html">Charles D Hixson: "Re: Fwd: We Can Understand Anything, But are Just a Bit Slow"</a>
<li><strong>Previous message:</strong> <a href="14588.html">Richard Loosemore: "Re: The Singularity vs. the Wall"</a>
<li><strong>In reply to:</strong> <a href="14586.html">Phillip Huggan: "Re: The Singularity vs. the Wall"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14603.html">Philip Goetz: "Re: The Singularity vs. the Wall"</a>
<li><strong>Reply:</strong> <a href="14603.html">Philip Goetz: "Re: The Singularity vs. the Wall"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14589">[ date ]</a>
<a href="index.html#14589">[ thread ]</a>
<a href="subject.html#14589">[ subject ]</a>
<a href="author.html#14589">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
