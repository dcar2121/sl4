<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Immoral optimisation - killing AIs</title>
<meta name="Author" content="H C (lphege@hotmail.com)">
<meta name="Subject" content="Re: Immoral optimisation - killing AIs">
<meta name="Date" content="2005-11-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Immoral optimisation - killing AIs</h1>
<!-- received="Wed Nov 16 12:19:40 2005" -->
<!-- isoreceived="20051116191940" -->
<!-- sent="Wed, 16 Nov 2005 19:19:38 +0000" -->
<!-- isosent="20051116191938" -->
<!-- name="H C" -->
<!-- email="lphege@hotmail.com" -->
<!-- subject="Re: Immoral optimisation - killing AIs" -->
<!-- id="BAY101-F142D82BAE9312D5DC9BC69DC5C0@phx.gbl" -->
<!-- inreplyto="BAY106-F113534D298EE0D7D7B7B55BA5C0@phx.gbl" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> H C (<a href="mailto:lphege@hotmail.com?Subject=Re:%20Immoral%20optimisation%20-%20killing%20AIs"><em>lphege@hotmail.com</em></a>)<br>
<strong>Date:</strong> Wed Nov 16 2005 - 12:19:38 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12868.html">Richard Loosemore: "Re: Einstein, Edison, &amp; IQ"</a>
<li><strong>Previous message:</strong> <a href="12866.html">Michael Vassar: "RE: IQ - A cautionary tale (Re: The ways of child prodigies)"</a>
<li><strong>In reply to:</strong> <a href="12861.html">Olie L: "Re: Immoral optimisation - killing AIs"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12867">[ date ]</a>
<a href="index.html#12867">[ thread ]</a>
<a href="subject.html#12867">[ subject ]</a>
<a href="author.html#12867">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt;From: &quot;Olie L&quot; &lt;<a href="mailto:neomorphy@hotmail.com?Subject=Re:%20Immoral%20optimisation%20-%20killing%20AIs">neomorphy@hotmail.com</a>&gt;
</em><br>
<em>&gt;Reply-To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Immoral%20optimisation%20-%20killing%20AIs">sl4@sl4.org</a>
</em><br>
<em>&gt;To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Immoral%20optimisation%20-%20killing%20AIs">sl4@sl4.org</a>
</em><br>
<em>&gt;Subject: Re: Immoral optimisation - killing AIs
</em><br>
<em>&gt;Date: Wed, 16 Nov 2005 16:16:33 +1100
</em><br>
<em>&gt;
</em><br>
<em>&gt;I've searched through the archives, and noticed that there is very little 
</em><br>
<em>&gt;said here about the meta-ethics of the value of AIs' existences. This is 
</em><br>
<em>&gt;kinda important stuff, and although I respect that 1) it's a hard area 2) 
</em><br>
<em>&gt;It's not as much fun as probability theory, it's kinda important as far as 
</em><br>
<em>&gt;friendly-AI is concerned.  Without a decent understanding of why an entity 
</em><br>
<em>&gt;(humans) shouldn't be destroyed, all the practicalities of how to ensure 
</em><br>
<em>&gt;their continued existence is kinda castles-in-the-air.
</em><br>
<em>&gt;
</em><br>
<em>&gt;*Warning! Warning! Typical introductory paragraph for a nutbag to prattle 
</em><br>
<em>&gt;on about &quot;Universal Morality&quot;*
</em><br>
<em>&gt;
</em><br>
<em>&gt;Yeah, we don't want a Sysop to convert us into processing units, or to 
</em><br>
<em>&gt;decide that the best way to solve our problems is to dope us up to the 
</em><br>
<em>&gt;eyeballs with sedatives, but what's the reasoning for &quot;vim&quot; not to do so 
</em><br>
<em>&gt;from ver perspective?  Note that asking &quot;why shouldn't?&quot; is an entirely 
</em><br>
<em>&gt;different question from &quot;why wouldn't,&quot; which some of the people here are 
</em><br>
<em>&gt;doing admirable work on (cue applause).
</em><br>
<em>&gt;
</em><br>
<em>&gt;The &quot;would&quot; can be addressed by goal-system examination etc.  
</em><br>
<em>&gt;Unfortunately, the &quot;should&quot; issue can /only/ addressed with Morality, which 
</em><br>
<em>&gt;attracts people to spout exceptional amounts of drivel.  I'm going to 
</em><br>
<em>&gt;venture out, and try to make a few meaningful statements about meta-ethics.
</em><br>
<em>&gt;
</em><br>
<em>&gt;First, a primer on what I mean by &quot;meta-ethics&quot;.  Ethics studies tend to 
</em><br>
<em>&gt;fall into three categories: Meta, normative and practical.
</em><br>
<em>&gt;*Meta-ethics* looks at what moral statement are: what does &quot;good&quot; mean; can 
</em><br>
<em>&gt;one derive &quot;ought&quot; from &quot;is&quot;; problems of subjectivity and whether or not 
</em><br>
<em>&gt;moral propositions can even have any truth-value.
</em><br>
<em>&gt;*Normative ethics* looks at systems for going about achieving good - these 
</em><br>
<em>&gt;tend to be focussed around variations of utilitarianism, justice or 
</em><br>
<em>&gt;rule-based systems.  Typical concerns:  Is it ok to do some bad stuff to 
</em><br>
<em>&gt;achieve lots of good stuff?
</em><br>
<em>&gt;*Practical ethics* is... you guessed it... applying normative ethics in 
</em><br>
<em>&gt;practice.  &quot;We've got $10B to spend on healthcare.  What do we do with it?&quot; 
</em><br>
<em>&gt;or &quot;Under what conditions are 5th trimester abortions permissable?&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt;One would expect that non-singularity AIs - I'm thinking advanced weak AIs 
</em><br>
<em>&gt;- would still be useful/good at dealing with practical ethical problems, 
</em><br>
<em>&gt;however, it will take some pretty savvy intelligences to get much ground on 
</em><br>
<em>&gt;the meta-ethics end.  Nb: don't get cocky with meta-ethics.  Many of the 
</em><br>
<em>&gt;best philosophers have taken a plunge and sunk.  Anyhoo:
</em><br>
<em>&gt;
</em><br>
<em>&gt;*Puts on Moral Philosopher hat*
</em><br>
<em>&gt;
</em><br>
<em>&gt;Why might it be wrong to turn off an AI?
</em><br>
<em>&gt;
</em><br>
<em>&gt;Most moral philosophers have a pretty hard time dealing with the 
</em><br>
<em>&gt;meta-ethics of killing people (contastively, the meta ethics on making 
</em><br>
<em>&gt;living people suffer is pretty straightforward - suffering is bad, bad is 
</em><br>
<em>&gt;Bad, avoid causing suffering).
</em><br>
<em>&gt;
</em><br>
<em>&gt;Apart from issues of making friends and family suffer, the meta-ethical 
</em><br>
<em>&gt;grounding for proscribing killing usually comes down to (1) sanctity, which 
</em><br>
<em>&gt;doesn't hold for non-religious types (2) Divine command - same problem (3) 
</em><br>
<em>&gt;Rights - based approach (4) Actions-based approach.  There are also a few 
</em><br>
<em>&gt;others, such as social order considerations, but... I can't be stuffed 
</em><br>
<em>&gt;wading through that.  I'll focus on 3 and 4.
</em><br>
<em>&gt;
</em><br>
<em>&gt;The main idea behind these is that people have plans and intentions, and 
</em><br>
<em>&gt;that disrupting these plans is &quot;bad&quot;.  The rights-approach says that there 
</em><br>
<em>&gt;are certain qualities that give an entity a &quot;Right&quot; that shouldn't be 
</em><br>
<em>&gt;violated - the qualities often stem back to the plans and intentions, so an 
</em><br>
<em>&gt;examination of these is relevant...
</em><br>
<em>&gt;
</em><br>
<em>&gt;A key question for the issue of killing / turning off an AI is whether or 
</em><br>
<em>&gt;not the AI has any plans, any desire to continue operating.
</em><br>
<em>&gt;
</em><br>
<em>&gt;There a few senses of the phrase &quot;nothing to do&quot; - if an intelligence is 
</em><br>
<em>&gt;bored but wants to do stuff, that's a desire that off-ness (death) 
</em><br>
<em>&gt;interferes with.  If, on the other hand, an intelligence has no desire to 
</em><br>
<em>&gt;do anything, no desire to think or feel, feels quite content to desist 
</em><br>
<em>&gt;being intelligent, then death does not interfere with any desires.  An 
</em><br>
<em>&gt;intelligence that has no desire to continue existing won't mind being 
</em><br>
<em>&gt;&quot;offed&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt;/I'm not doing a fantastic job of justifying any of these positions, 
</em><br>
<em>&gt;largely because I disagree with most of these meta-ethical approaches.  For 
</em><br>
<em>&gt;the others, I lack a complete technical understanding.  I'll therefore 
</em><br>
<em>&gt;resort to the customary ethical technique of providing analogies, reductio 
</em><br>
<em>&gt;ad absurdum, and relying on intuitions to invent rules (sigh)./
</em><br>
<em>&gt;
</em><br>
<em>&gt;Imagine someone wishing to commit suicide.  Is this an ethically acceptable 
</em><br>
<em>&gt;course of action?  I think so, particularly if they're generally having a 
</em><br>
<em>&gt;rough time (terminal illness etc).  Just imagine they've put their affairs 
</em><br>
<em>&gt;in order, said goodbye to their family, are about to put a plastic bag full 
</em><br>
<em>&gt;of happy gas over their head... when somebody else shoots them in the back 
</em><br>
<em>&gt;of the head, killing them instantly.  Is the assassin here doing something 
</em><br>
<em>&gt;ethically unacceptable?  Are the intentions/ actions bad?  Is the result 
</em><br>
<em>&gt;bad?  If the assasin is aware of the suicide-attempter's plans, does that 
</em><br>
<em>&gt;make a difference?
</em><br>
<em>&gt;
</em><br>
<em>&gt;I would suggest that although the killer's intentions could be immoral, the 
</em><br>
<em>&gt;result ain't bad.  Whether the means of death is self-inflicted, 
</em><br>
<em>&gt;other-inflicted or nature-inflicted, the suicidor's wish is granted.  
</em><br>
<em>&gt;Killing a person with no desire to live is not necessarily such a terrible 
</em><br>
<em>&gt;thing.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Drag the analogy accross to AIs: if the AI has no desire to live, is 
</em><br>
<em>&gt;killing them/ turning them off bad?  Not really.  An AI with no desire to 
</em><br>
<em>&gt;continue operating would seem to be, necessarily, an AI with no intentions 
</em><br>
<em>&gt;and no purpose.  One can imagine this happening if the AI has a purpose 
</em><br>
<em>&gt;that is completed.
</em><br>
<em>&gt;
</em><br>
<em>&gt;The interesting counter to this is: would it be extremely immoral to 
</em><br>
<em>&gt;influence an entity to cease their intentions to live?  By whatever means, 
</em><br>
<em>&gt;causing a person to give up their desire to do, achieve and continue to 
</em><br>
<em>&gt;live?  How about comparing the goodness of creating a person with a high 
</em><br>
<em>&gt;likelihood of will-to-live-cessation against creating a person more likely 
</em><br>
<em>&gt;to want to keep living?  My intuition says this is dancing around a very 
</em><br>
<em>&gt;fine line between OK and hideous.
</em><br>
<p>Morality is probably more difficult to understand than anything else. As 
<br>
such, I will discuss how the frame of reference that came to mind while 
<br>
reading this.
<br>
<p>&quot;Intention&quot; and &quot;Purpose&quot; are very loaded words. Our intentions our derived 
<br>
from intentions which are derived from beliefs which are derived from 
<br>
reality.
<br>
<p>Ultimately you have to reference some pre-programmed goal system, to really 
<br>
refer to any *absolute* intentions or purposes. In terms of creating an AGI, 
<br>
there are three different characteristics that must be accounted for: A 
<br>
complicated probabilistic process, a knowledge representation schema (which 
<br>
includes a goal system schema), and a seed (which is the preprogrammed 
<br>
probabilistic process, the actual scheme of knowledge representation, some 
<br>
initial preprogrammed values within these, and any information which is 
<br>
added that modifies these)
<br>
<p>The big question for the Singularity is whether or not the *true* 
<br>
optimization of some &quot;seed&quot; within reality can be convergent for more than 
<br>
one seed. The big question for Friendy AI is exactly what would a &quot;seed&quot; 
<br>
look like, that, resulting from a RPOP in which this seed is based, whether 
<br>
or not it could **account** (in it's RPOP) for all possible seeds in 
<br>
existence (or at least within the realm of human seeds).
<br>
<p>Or something like that.
<br>
<p>Th3Hegem0n
<br>
<p><p><p><em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;--Olie
</em><br>
<em>&gt;
</em><br>
<em>&gt;H C wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;&quot;You've specified an AGI which feels desire, and stated it doesn't mimic 
</em><br>
<em>&gt;&gt;human desires&quot;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;It wants to be Friendly, but it doesn't want to have sex with people or 
</em><br>
<em>&gt;&gt;eat food.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Or get jealous when you ask for a second opinion, or react 
</em><br>
<em>&gt;agressive-violently to actions it finds threatening?
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;&gt;From: Phillip Huggan &lt;<a href="mailto:cdnprodigy@yahoo.com?Subject=Re:%20Immoral%20optimisation%20-%20killing%20AIs">cdnprodigy@yahoo.com</a>&gt;
</em><br>
<em>&gt;&gt;&gt;Reply-To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Immoral%20optimisation%20-%20killing%20AIs">sl4@sl4.org</a>
</em><br>
<em>&gt;&gt;&gt;To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Immoral%20optimisation%20-%20killing%20AIs">sl4@sl4.org</a>
</em><br>
<em>&gt;&gt;&gt;Subject: Re: Immorally optimized? - alternate observation points
</em><br>
<em>&gt;&gt;&gt;Date: Fri, 9 Sep 2005 11:15:04 -0700 (PDT)
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;H C &lt;<a href="mailto:lphege@hotmail.com?Subject=Re:%20Immoral%20optimisation%20-%20killing%20AIs">lphege@hotmail.com</a>&gt; wrote:
</em><br>
<em>&gt;&gt;&gt; &gt;Imagine (attempted) Friendly AGI named X, who resides in some computer
</em><br>
<em>&gt;&gt;&gt; &gt;simulation. X observes things, gives meaning, feels desire, 
</em><br>
<em>&gt;&gt;&gt;hypothesizes,
</em><br>
<em>&gt;&gt;&gt; &gt;and is capable of creating tests for vis hypotheses. In other words, 
</em><br>
<em>&gt;&gt;&gt;AGI X
</em><br>
<em>&gt;&gt;&gt; &gt;is actually a *real* intelligent AGI, intelligent in the human sense 
</em><br>
<em>&gt;&gt;&gt;(but
</em><br>
<em>&gt;&gt;&gt; &gt;without athropomorphizing human thought procedures and desires).
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; &gt;Now imagine that AGI X has the capability to run &quot;alternate observation
</em><br>
<em>&gt;&gt;&gt; &gt;points&quot; in which ve creates another &quot;instance&quot; of the [observation 
</em><br>
<em>&gt;&gt;&gt;program -
</em><br>
<em>&gt;&gt;&gt; &gt;aka intelligence program] and runs this intelligence program on one
</em><br>
<em>&gt;&gt;&gt; &gt;particular problem... and this instance exists independently of the X,
</em><br>
<em>&gt;&gt;&gt; &gt;except it modifies the same memory base. In other words &quot;I need a 
</em><br>
<em>&gt;&gt;&gt;program to
</em><br>
<em>&gt;&gt;&gt; &gt;fly a helicopter&quot; *clicks in disk recorded where an alternate 
</em><br>
<em>&gt;&gt;&gt;observation
</em><br>
<em>&gt;&gt;&gt;point already learned/experienced flying helicopter* &quot;Ok thanks.&quot;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; &gt;Now if you optimize this concept, given some problem like &quot;Program this
</em><br>
<em>&gt;&gt;&gt; &gt;application&quot;, X could create several different AOPs and solve 5 
</em><br>
<em>&gt;&gt;&gt;different
</em><br>
<em>&gt;&gt;&gt; &gt;parts of the problem at the same time, shut them down, and start 
</em><br>
<em>&gt;&gt;&gt;solving the
</em><br>
<em>&gt;&gt;&gt;main problem of the application with all of the detailed trial and error
</em><br>
<em>&gt;&gt;&gt;learning that took place in creating the various parts of the application
</em><br>
<em>&gt;&gt;&gt;already done.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; &gt;The problem is, is it *immoral* to create these &quot;parallel 
</em><br>
<em>&gt;&gt;&gt;intelligences&quot; and
</em><br>
<em>&gt;&gt;&gt; &gt;arbitrarily destory them when they've fulfilled their purpose? Also, if 
</em><br>
<em>&gt;&gt;&gt;you
</em><br>
<em>&gt;&gt;&gt;decide to respond, try to give explanation for your answers.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;You've specified an AGI which feels desire, and stated it doesn't mimic 
</em><br>
<em>&gt;&gt;&gt;human desires.  Which is it?  If the AGI itself cannot answer this moral 
</em><br>
<em>&gt;&gt;&gt;dillemma, it is not friendly and we are all in big trouble.  I suspect 
</em><br>
<em>&gt;&gt;&gt;the answer depends upon how important the application is you are telling 
</em><br>
<em>&gt;&gt;&gt;the AGI to solve.  If solving the application requires creating and 
</em><br>
<em>&gt;&gt;&gt;destroying 5 sentient AIs, we are setting a precedent for computronium.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Good point. You could focus on suboptimal performance while waiting for 
</em><br>
<em>&gt;&gt;the AGI to Singularitize itself and tell you the answer.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;__________________________________________________
</em><br>
<em>&gt;&gt;&gt;Do You Yahoo!?
</em><br>
<em>&gt;&gt;&gt;Tired of spam?  Yahoo! Mail has the best spam protection around
</em><br>
<em>&gt;&gt;&gt;<a href="http://mail.yahoo.com">http://mail.yahoo.com</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12868.html">Richard Loosemore: "Re: Einstein, Edison, &amp; IQ"</a>
<li><strong>Previous message:</strong> <a href="12866.html">Michael Vassar: "RE: IQ - A cautionary tale (Re: The ways of child prodigies)"</a>
<li><strong>In reply to:</strong> <a href="12861.html">Olie L: "Re: Immoral optimisation - killing AIs"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12867">[ date ]</a>
<a href="index.html#12867">[ thread ]</a>
<a href="subject.html#12867">[ subject ]</a>
<a href="author.html#12867">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:53 MDT
</em></small></p>
</body>
</html>
