<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: threat analysis (RE: IQ - A cautionary tale)</title>
<meta name="Author" content="Phillip Huggan (cdnprodigy@yahoo.com)">
<meta name="Subject" content="threat analysis (RE: IQ - A cautionary tale)">
<meta name="Date" content="2005-11-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>threat analysis (RE: IQ - A cautionary tale)</h1>
<!-- received="Wed Nov 16 15:54:55 2005" -->
<!-- isoreceived="20051116225455" -->
<!-- sent="Wed, 16 Nov 2005 14:54:46 -0800 (PST)" -->
<!-- isosent="20051116225446" -->
<!-- name="Phillip Huggan" -->
<!-- email="cdnprodigy@yahoo.com" -->
<!-- subject="threat analysis (RE: IQ - A cautionary tale)" -->
<!-- id="20051116225446.45460.qmail@web61316.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="BAY101-F2644FB28751F9D2647833FAC5C0@phx.gbl" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Phillip Huggan (<a href="mailto:cdnprodigy@yahoo.com?Subject=Re:%20threat%20analysis%20(RE:%20IQ%20-%20A%20cautionary%20tale)"><em>cdnprodigy@yahoo.com</em></a>)<br>
<strong>Date:</strong> Wed Nov 16 2005 - 15:54:46 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12678.html">Marc Geddes: "'The Revenge of Geddes'! - New Theory Of Everything! Ha!"</a>
<li><strong>Previous message:</strong> <a href="12676.html">Phillip Huggan: "Re: Einstein, Edison, &amp; IQ"</a>
<li><strong>In reply to:</strong> <a href="12673.html">Michael Vassar: "RE: IQ - A cautionary tale (Re: The ways of child prodigies)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12712.html">mike99: "RE: IQ - A cautionary tale (Re: The ways of child prodigies)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12677">[ date ]</a>
<a href="index.html#12677">[ thread ]</a>
<a href="subject.html#12677">[ subject ]</a>
<a href="author.html#12677">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;&nbsp;Compiling reliable estimates of &quot;group behaviour&quot; risks is necessary in deciding whether or not to activate an AGI of type #3 below.  The AGI in question must be more likely to enact a favourable world than would be a world without the AGI.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;Bioterror existential risks are overstated because they require a &quot;doomsday rapture&quot; mindset to carry out, not merely contemplation of suicide.  The infrastructure to manufacture germs will be easier to attain in the years ahead, so this will ever so slightly lower the required safety threshold required for unleashing an AGI over time, until other pandemic countermeasures are devised.  The inertia in government weapons research programs suggests MNT arms-races are a very real possibility.  Nano-terror won't happen.  Terrorists won't be capable of manufacturing the 1st nano-assembler, and if assemblers ever become available at the level black-market nukes currently are, we will already be killed or enslaved by more skilled assembler handlers.
<br>
&nbsp;&nbsp;&nbsp;
<br>
<em>  &gt;&quot;IS complex emergence necessary for AI&quot;
</em><br>
&nbsp;&nbsp;&nbsp;&nbsp;From: Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20threat%20analysis%20(RE:%20IQ%20-%20A%20cautionary%20tale)">mwdestinystar@yahoo.co.uk</a>)
<br>
Date: Tue Sep 20 2005 - 04:08:11 MDT
<br>
&lt;SNIP&gt;
<br>
&nbsp;&nbsp;2. Thus the SIAI has the design requirement; goal system trajectory must 
<br>
reliably stay within certain bounds, which is to say that the optimisation 
<br>
targets of the overall optimisation process must not drift out of a 
<br>
certain region. This is a very specific and limited kind of predictability; 
<br>
we don't need specific AI behaviour or cognitive content. I agree that the 
<br>
task would be impossible if one were trying to predict much more than just 
<br>
the optimisation targets. I am happy to have all kinds of emergence and 
<br>
Complexity occuring as long as they stay within the overall constraints, 
<br>
though theory and limited experimental experience suggests to me that there 
<br>
will be a lot less of this than most people would expect. 
<br>
&nbsp;&nbsp;3. If that turns out to be impossible, then we'd agree that AGI development 
<br>
should just go ahead using the best probabilistic methods available (maybe; 
<br>
it might make sense to develop IA first in that case). But we shouldn't 
<br>
write something this important off as impossible without trying really 
<br>
hard first, and I think that many people are far too quick to dismiss this 
<br>
so that they can get on with the 'fun stuff' i.e. actual AGI design. 
<br>
<p>&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;Michael Vassar &lt;<a href="mailto:michaelvassar@hotmail.com?Subject=Re:%20threat%20analysis%20(RE:%20IQ%20-%20A%20cautionary%20tale)">michaelvassar@hotmail.com</a>&gt; wrote:
<br>
<p>&nbsp;&nbsp;&lt;SNIP&gt;  For all of our 
<br>
arrogance, most Transhumanists grossly overestimate the abilities of 
<br>
ordinary humans. This is substantially a consequence of how folk psychology 
<br>
works, and fails to work for outliers, but also a consequence of typically 
<br>
limited and iscolated life experience. Unfortunately, it has serious 
<br>
consequences when predicting the future. Our estimates of the likely 
<br>
behavior of large scale groups, the effort that will be devoted to a 
<br>
particular research objective, or the time until some task is accomplished 
<br>
are all grossly distorted. For many transhumanists this means that boogie 
<br>
men such as &quot;terrorists&quot; are imagined as something that never was, 
<br>
disutility maximizers, and the resultant threats of bioterror and nanoterror 
<br>
are overestimated by many orders of magnitude. For almost all 
<br>
transhumanists this means an underestimation of inertia, leading Chris 
<br>
Phoenix's fears of pre-emptory arms races and Nick Bostrom's utopian dreams 
<br>
of world government and benign regulation of dangerous tech. At SL4, it 
<br>
probably means a serious overestimate of the immediacy of the existential 
<br>
risk associated with more powerful hardware.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
---------------------------------
<br>
&nbsp;Yahoo! FareChase - Search multiple travel sites in one click.  
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12678.html">Marc Geddes: "'The Revenge of Geddes'! - New Theory Of Everything! Ha!"</a>
<li><strong>Previous message:</strong> <a href="12676.html">Phillip Huggan: "Re: Einstein, Edison, &amp; IQ"</a>
<li><strong>In reply to:</strong> <a href="12673.html">Michael Vassar: "RE: IQ - A cautionary tale (Re: The ways of child prodigies)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12712.html">mike99: "RE: IQ - A cautionary tale (Re: The ways of child prodigies)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12677">[ date ]</a>
<a href="index.html#12677">[ thread ]</a>
<a href="subject.html#12677">[ subject ]</a>
<a href="author.html#12677">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:19 MST
</em></small></p>
</body>
</html>
