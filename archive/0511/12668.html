<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Immoral optimisation - killing AIs</title>
<meta name="Author" content="Olie L (neomorphy@hotmail.com)">
<meta name="Subject" content="Re: Immoral optimisation - killing AIs">
<meta name="Date" content="2005-11-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Immoral optimisation - killing AIs</h1>
<!-- received="Tue Nov 15 22:16:36 2005" -->
<!-- isoreceived="20051116051636" -->
<!-- sent="Wed, 16 Nov 2005 16:16:33 +1100" -->
<!-- isosent="20051116051633" -->
<!-- name="Olie L" -->
<!-- email="neomorphy@hotmail.com" -->
<!-- subject="Re: Immoral optimisation - killing AIs" -->
<!-- id="BAY106-F113534D298EE0D7D7B7B55BA5C0@phx.gbl" -->
<!-- inreplyto="Immoral optimisation - killing AIs" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Olie L (<a href="mailto:neomorphy@hotmail.com?Subject=Re:%20Immoral%20optimisation%20-%20killing%20AIs"><em>neomorphy@hotmail.com</em></a>)<br>
<strong>Date:</strong> Tue Nov 15 2005 - 22:16:33 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12669.html">Phil Goetz: "Re: Einstein, Edison, &amp; IQ"</a>
<li><strong>Previous message:</strong> <a href="12667.html">Michael Wilson: "Re: Extrapolated volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12674.html">H C: "Re: Immoral optimisation - killing AIs"</a>
<li><strong>Reply:</strong> <a href="12674.html">H C: "Re: Immoral optimisation - killing AIs"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12668">[ date ]</a>
<a href="index.html#12668">[ thread ]</a>
<a href="subject.html#12668">[ subject ]</a>
<a href="author.html#12668">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I've searched through the archives, and noticed that there is very little 
<br>
said here about the meta-ethics of the value of AIs' existences. This is 
<br>
kinda important stuff, and although I respect that 1) it's a hard area 2) 
<br>
It's not as much fun as probability theory, it's kinda important as far as 
<br>
friendly-AI is concerned.  Without a decent understanding of why an entity 
<br>
(humans) shouldn't be destroyed, all the practicalities of how to ensure 
<br>
their continued existence is kinda castles-in-the-air.
<br>
<p>*Warning! Warning! Typical introductory paragraph for a nutbag to prattle on 
<br>
about &quot;Universal Morality&quot;*
<br>
<p>Yeah, we don't want a Sysop to convert us into processing units, or to 
<br>
decide that the best way to solve our problems is to dope us up to the 
<br>
eyeballs with sedatives, but what's the reasoning for &quot;vim&quot; not to do so 
<br>
from ver perspective?  Note that asking &quot;why shouldn't?&quot; is an entirely 
<br>
different question from &quot;why wouldn't,&quot; which some of the people here are 
<br>
doing admirable work on (cue applause).
<br>
<p>The &quot;would&quot; can be addressed by goal-system examination etc.  Unfortunately, 
<br>
the &quot;should&quot; issue can /only/ addressed with Morality, which attracts people 
<br>
to spout exceptional amounts of drivel.  I'm going to venture out, and try 
<br>
to make a few meaningful statements about meta-ethics.
<br>
<p>First, a primer on what I mean by &quot;meta-ethics&quot;.  Ethics studies tend to 
<br>
fall into three categories: Meta, normative and practical.
<br>
*Meta-ethics* looks at what moral statement are: what does &quot;good&quot; mean; can 
<br>
one derive &quot;ought&quot; from &quot;is&quot;; problems of subjectivity and whether or not 
<br>
moral propositions can even have any truth-value.
<br>
*Normative ethics* looks at systems for going about achieving good - these 
<br>
tend to be focussed around variations of utilitarianism, justice or 
<br>
rule-based systems.  Typical concerns:  Is it ok to do some bad stuff to 
<br>
achieve lots of good stuff?
<br>
*Practical ethics* is... you guessed it... applying normative ethics in 
<br>
practice.  &quot;We've got $10B to spend on healthcare.  What do we do with it?&quot; 
<br>
or &quot;Under what conditions are 5th trimester abortions permissable?&quot;
<br>
<p>One would expect that non-singularity AIs - I'm thinking advanced weak AIs - 
<br>
would still be useful/good at dealing with practical ethical problems, 
<br>
however, it will take some pretty savvy intelligences to get much ground on 
<br>
the meta-ethics end.  Nb: don't get cocky with meta-ethics.  Many of the 
<br>
best philosophers have taken a plunge and sunk.  Anyhoo:
<br>
<p>*Puts on Moral Philosopher hat*
<br>
<p>Why might it be wrong to turn off an AI?
<br>
<p>Most moral philosophers have a pretty hard time dealing with the meta-ethics 
<br>
of killing people (contastively, the meta ethics on making living people 
<br>
suffer is pretty straightforward - suffering is bad, bad is Bad, avoid 
<br>
causing suffering).
<br>
<p>Apart from issues of making friends and family suffer, the meta-ethical 
<br>
grounding for proscribing killing usually comes down to (1) sanctity, which 
<br>
doesn't hold for non-religious types (2) Divine command - same problem (3) 
<br>
Rights - based approach (4) Actions-based approach.  There are also a few 
<br>
others, such as social order considerations, but... I can't be stuffed 
<br>
wading through that.  I'll focus on 3 and 4.
<br>
<p>The main idea behind these is that people have plans and intentions, and 
<br>
that disrupting these plans is &quot;bad&quot;.  The rights-approach says that there 
<br>
are certain qualities that give an entity a &quot;Right&quot; that shouldn't be 
<br>
violated - the qualities often stem back to the plans and intentions, so an 
<br>
examination of these is relevant...
<br>
<p>A key question for the issue of killing / turning off an AI is whether or 
<br>
not the AI has any plans, any desire to continue operating.
<br>
<p>There a few senses of the phrase &quot;nothing to do&quot; - if an intelligence is 
<br>
bored but wants to do stuff, that's a desire that off-ness (death) 
<br>
interferes with.  If, on the other hand, an intelligence has no desire to do 
<br>
anything, no desire to think or feel, feels quite content to desist being 
<br>
intelligent, then death does not interfere with any desires.  An 
<br>
intelligence that has no desire to continue existing won't mind being 
<br>
&quot;offed&quot;
<br>
<p>/I'm not doing a fantastic job of justifying any of these positions, largely 
<br>
because I disagree with most of these meta-ethical approaches.  For the 
<br>
others, I lack a complete technical understanding.  I'll therefore resort to 
<br>
the customary ethical technique of providing analogies, reductio ad 
<br>
absurdum, and relying on intuitions to invent rules (sigh)./
<br>
<p>Imagine someone wishing to commit suicide.  Is this an ethically acceptable 
<br>
course of action?  I think so, particularly if they're generally having a 
<br>
rough time (terminal illness etc).  Just imagine they've put their affairs 
<br>
in order, said goodbye to their family, are about to put a plastic bag full 
<br>
of happy gas over their head... when somebody else shoots them in the back 
<br>
of the head, killing them instantly.  Is the assassin here doing something 
<br>
ethically unacceptable?  Are the intentions/ actions bad?  Is the result 
<br>
bad?  If the assasin is aware of the suicide-attempter's plans, does that 
<br>
make a difference?
<br>
<p>I would suggest that although the killer's intentions could be immoral, the 
<br>
result ain't bad.  Whether the means of death is self-inflicted, 
<br>
other-inflicted or nature-inflicted, the suicidor's wish is granted.  
<br>
Killing a person with no desire to live is not necessarily such a terrible 
<br>
thing.
<br>
<p>Drag the analogy accross to AIs: if the AI has no desire to live, is killing 
<br>
them/ turning them off bad?  Not really.  An AI with no desire to continue 
<br>
operating would seem to be, necessarily, an AI with no intentions and no 
<br>
purpose.  One can imagine this happening if the AI has a purpose that is 
<br>
completed.
<br>
<p>The interesting counter to this is: would it be extremely immoral to 
<br>
influence an entity to cease their intentions to live?  By whatever means, 
<br>
causing a person to give up their desire to do, achieve and continue to 
<br>
live?  How about comparing the goodness of creating a person with a high 
<br>
likelihood of will-to-live-cessation against creating a person more likely 
<br>
to want to keep living?  My intuition says this is dancing around a very 
<br>
fine line between OK and hideous.
<br>
<p><p>--Olie
<br>
<p>H C wrote:
<br>
<p><em>&gt;&quot;You've specified an AGI which feels desire, and stated it doesn't mimic 
</em><br>
<em>&gt;human desires&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt;It wants to be Friendly, but it doesn't want to have sex with people or eat 
</em><br>
<em>&gt;food.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<p>Or get jealous when you ask for a second opinion, or react 
<br>
agressive-violently to actions it finds threatening?
<br>
<p><em>&gt;&gt;From: Phillip Huggan &lt;<a href="mailto:cdnprodigy@yahoo.com?Subject=Re:%20Immoral%20optimisation%20-%20killing%20AIs">cdnprodigy@yahoo.com</a>&gt;
</em><br>
<em>&gt;&gt;Reply-To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Immoral%20optimisation%20-%20killing%20AIs">sl4@sl4.org</a>
</em><br>
<em>&gt;&gt;To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Immoral%20optimisation%20-%20killing%20AIs">sl4@sl4.org</a>
</em><br>
<em>&gt;&gt;Subject: Re: Immorally optimized? - alternate observation points
</em><br>
<em>&gt;&gt;Date: Fri, 9 Sep 2005 11:15:04 -0700 (PDT)
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;H C &lt;<a href="mailto:lphege@hotmail.com?Subject=Re:%20Immoral%20optimisation%20-%20killing%20AIs">lphege@hotmail.com</a>&gt; wrote:
</em><br>
<em>&gt;&gt; &gt;Imagine (attempted) Friendly AGI named X, who resides in some computer
</em><br>
<em>&gt;&gt; &gt;simulation. X observes things, gives meaning, feels desire, 
</em><br>
<em>&gt;&gt;hypothesizes,
</em><br>
<em>&gt;&gt; &gt;and is capable of creating tests for vis hypotheses. In other words, AGI 
</em><br>
<em>&gt;&gt;X
</em><br>
<em>&gt;&gt; &gt;is actually a *real* intelligent AGI, intelligent in the human sense 
</em><br>
<em>&gt;&gt;(but
</em><br>
<em>&gt;&gt; &gt;without athropomorphizing human thought procedures and desires).
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; &gt;Now imagine that AGI X has the capability to run &quot;alternate observation
</em><br>
<em>&gt;&gt; &gt;points&quot; in which ve creates another &quot;instance&quot; of the [observation 
</em><br>
<em>&gt;&gt;program -
</em><br>
<em>&gt;&gt; &gt;aka intelligence program] and runs this intelligence program on one
</em><br>
<em>&gt;&gt; &gt;particular problem... and this instance exists independently of the X,
</em><br>
<em>&gt;&gt; &gt;except it modifies the same memory base. In other words &quot;I need a 
</em><br>
<em>&gt;&gt;program to
</em><br>
<em>&gt;&gt; &gt;fly a helicopter&quot; *clicks in disk recorded where an alternate 
</em><br>
<em>&gt;&gt;observation
</em><br>
<em>&gt;&gt;point already learned/experienced flying helicopter* &quot;Ok thanks.&quot;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; &gt;Now if you optimize this concept, given some problem like &quot;Program this
</em><br>
<em>&gt;&gt; &gt;application&quot;, X could create several different AOPs and solve 5 
</em><br>
<em>&gt;&gt;different
</em><br>
<em>&gt;&gt; &gt;parts of the problem at the same time, shut them down, and start solving 
</em><br>
<em>&gt;&gt;the
</em><br>
<em>&gt;&gt;main problem of the application with all of the detailed trial and error
</em><br>
<em>&gt;&gt;learning that took place in creating the various parts of the application
</em><br>
<em>&gt;&gt;already done.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; &gt;The problem is, is it *immoral* to create these &quot;parallel intelligences&quot; 
</em><br>
<em>&gt;&gt;and
</em><br>
<em>&gt;&gt; &gt;arbitrarily destory them when they've fulfilled their purpose? Also, if 
</em><br>
<em>&gt;&gt;you
</em><br>
<em>&gt;&gt;decide to respond, try to give explanation for your answers.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;You've specified an AGI which feels desire, and stated it doesn't mimic 
</em><br>
<em>&gt;&gt;human desires.  Which is it?  If the AGI itself cannot answer this moral 
</em><br>
<em>&gt;&gt;dillemma, it is not friendly and we are all in big trouble.  I suspect the 
</em><br>
<em>&gt;&gt;answer depends upon how important the application is you are telling the 
</em><br>
<em>&gt;&gt;AGI to solve.  If solving the application requires creating and destroying 
</em><br>
<em>&gt;&gt;5 sentient AIs, we are setting a precedent for computronium.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Good point. You could focus on suboptimal performance while waiting for the 
</em><br>
<em>&gt;AGI to Singularitize itself and tell you the answer.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;__________________________________________________
</em><br>
<em>&gt;&gt;Do You Yahoo!?
</em><br>
<em>&gt;&gt;Tired of spam?  Yahoo! Mail has the best spam protection around
</em><br>
<em>&gt;&gt;<a href="http://mail.yahoo.com">http://mail.yahoo.com</a>
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12669.html">Phil Goetz: "Re: Einstein, Edison, &amp; IQ"</a>
<li><strong>Previous message:</strong> <a href="12667.html">Michael Wilson: "Re: Extrapolated volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12674.html">H C: "Re: Immoral optimisation - killing AIs"</a>
<li><strong>Reply:</strong> <a href="12674.html">H C: "Re: Immoral optimisation - killing AIs"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12668">[ date ]</a>
<a href="index.html#12668">[ thread ]</a>
<a href="subject.html#12668">[ subject ]</a>
<a href="author.html#12668">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:19 MST
</em></small></p>
</body>
</html>
