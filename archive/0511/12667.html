<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Extrapolated volition</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="Re: Extrapolated volition">
<meta name="Date" content="2005-11-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Extrapolated volition</h1>
<!-- received="Tue Nov 15 02:54:29 2005" -->
<!-- isoreceived="20051115095429" -->
<!-- sent="Tue, 15 Nov 2005 09:54:25 +0000 (GMT)" -->
<!-- isosent="20051115095425" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="Re: Extrapolated volition" -->
<!-- id="20051115095425.27929.qmail@web26713.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="8d71341e0511101206s28aab2c2h5b8e8cc1acdf8225@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20Extrapolated%20volition"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Tue Nov 15 2005 - 02:54:25 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12668.html">Olie L: "Re: Immoral optimisation - killing AIs"</a>
<li><strong>Previous message:</strong> <a href="12666.html">Robin Lee Powell: "Re: Indoctrinating my family"</a>
<li><strong>In reply to:</strong> <a href="12643.html">Russell Wallace: "Re: Extrapolated volition: oops"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12667">[ date ]</a>
<a href="index.html#12667">[ thread ]</a>
<a href="subject.html#12667">[ subject ]</a>
<a href="author.html#12667">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Russell Wallace wrote:
<br>
<em>&gt;&gt; I would not write (domain protection) off as useless; some of the
</em><br>
<em>&gt;&gt; problems may actually be solvable, and it is conceivable that it
</em><br>
<em>&gt;&gt; could form an important component of a workable hybrid strategy. 
</em><br>
<em>&gt;
</em><br>
<em>&gt; Speaking of hybrid strategies, I'm starting to think DP and CEV
</em><br>
<em>&gt; may not be as far apart as they appear at first glance.
</em><br>
<p>They are quite different. As I see it, there are five basic
<br>
approaches to designing the goal system for a Friendly AI;
<br>
<p>1. Lay down immutable pronouncements that you intend to shape the rest
<br>
&nbsp;&nbsp;&nbsp;of human history. Domain protection and any attempts at 'objective
<br>
&nbsp;&nbsp;&nbsp;morality' are in this category.
<br>
2. Lay down ground rules plus metagoals that will modify the basic
<br>
&nbsp;&nbsp;&nbsp;goals as the FAI considers the problem and interacts with the world.
<br>
&nbsp;&nbsp;&nbsp;Depending on the design, this may or may not settle rapidly to a
<br>
&nbsp;&nbsp;&nbsp;static consistent-under-reflection goal system. Eliezer's model in
<br>
&nbsp;&nbsp;&nbsp;'Creating a Friendly AI' is in this category, and AFAIK Ben's
<br>
&nbsp;&nbsp;&nbsp;current model does too.
<br>
3. Lay down ground rules plus specific rules about how humans can
<br>
&nbsp;&nbsp;&nbsp;change the FAI's basic goal system content (e.g. the 'AGI implements
<br>
&nbsp;&nbsp;&nbsp;any majority vote of the UN' failure scenario, assuming that
<br>
&nbsp;&nbsp;&nbsp;includes obeying directives to stop obeying the UN in future). This
<br>
&nbsp;&nbsp;&nbsp;is a special case of (2).
<br>
4. Specify the absolute minimum possible yourself; get the AGI to
<br>
&nbsp;&nbsp;&nbsp;generate goal system content by analysing some other source, i.e.
<br>
&nbsp;&nbsp;&nbsp;scanning the brain state of all extant humans and extrapolating
<br>
&nbsp;&nbsp;&nbsp;their future desires. This differs from (2) in that effectively
<br>
&nbsp;&nbsp;&nbsp;there are no 'ground rules', just metagoals; the AGI has to go
<br>
&nbsp;&nbsp;&nbsp;through the goal system generation process before it starts
<br>
&nbsp;&nbsp;&nbsp;affecting the world. CV is almost in this category; it retains a
<br>
&nbsp;&nbsp;&nbsp;couple of basic rules such as 'don't/try not to create any
<br>
&nbsp;&nbsp;&nbsp;sentient beings in the extrapolation process'.
<br>
5. Instruct the AGI to do something specific and then shut down;
<br>
&nbsp;&nbsp;&nbsp;this means specifying a strong bound on how long the AGI is going
<br>
&nbsp;&nbsp;&nbsp;to be around. In principle you could add a fixed temporal bound
<br>
&nbsp;&nbsp;&nbsp;to scenarios 2, 3 and 4, but usually this is proposed as an extra
<br>
&nbsp;&nbsp;&nbsp;constraint on 1, i.e. having the AGI take some specific actions
<br>
&nbsp;&nbsp;&nbsp;to mitigate existential risks and then cease activity.
<br>
<p><em>&gt; For the EV part, while I think I'd want it toned down some from
</em><br>
<em>&gt; the way Eliezer seems to see it, _some_ form of intelligent
</em><br>
<em>&gt; interpretation of volition is needed to avoid the murder by genie
</em><br>
<em>&gt; bottle problem;
</em><br>
<p>Scenario (1) is just generally a bad idea. I wouldn't rule it out
<br>
utterly and completely, but we want to avoid inflicting badly thought
<br>
out rules (and there's a sharp limit to how well thought out anything
<br>
designed by humans, particularly a single human, is going to be) on
<br>
the entirety of future history if at all possible.
<br>
<p><em>&gt; the real stumbling block is the C part.
</em><br>
<p>There is a strong tendency to have a knee-jerk reaction against the
<br>
term 'collective' just because we're all such devoted individualists
<br>
on SL4. Now it is arguable that Eliezer is in fact excessively
<br>
concerned about inclusiveness-above-all-else, but CEV is not some
<br>
dastardly communist plot. I encourage everyone to think about the
<br>
kinds of things in a society that should be dependent on a
<br>
non-unanimous consensus, and the most effective, least harmful ways
<br>
to generate that consensus. Then consider how you might push as
<br>
many parts of /that/ problem onto transhuman intelligences as
<br>
possible, and how it might need fixing if you got it wrong. Finally
<br>
you have to do a risk analysis on how the course of events could
<br>
deviate from the general constraints you're trying to impose, should
<br>
you make mistakes (and you probably will). This is (I think) how
<br>
Eliezer came up with the concept of CV.
<br>
<p><em>&gt; If an escape clause were added - the right for people to say &quot;I
</em><br>
<em>&gt; don't want anything to do with path X that that lot are following,
</em><br>
<em>&gt; my volition is to go down path Y instead&quot; - then I'd have far
</em><br>
<em>&gt; fewer problems with it.
</em><br>
<p>That sounds like domain protection with everyone in the 'CV' domain
<br>
to start with. In theory, if this was a good idea then the CV would
<br>
implement such an escape clause for you. In practice, I think CV is
<br>
unavoidably dependent on initial conditions and structure to a far
<br>
greater extent than Eliezer might like. I remain open to the
<br>
possibility of some dazzling insight on the 'right way to do it',
<br>
but I am not holding my breath. Regardless, I am more amenable to
<br>
engineering the structural properties and base assumptions of a
<br>
CV process to change the ease with which some conclusions can be
<br>
generated, than to start specifying rules directly.
<br>
<p><em>&gt; (I know Eliezer's afraid of letting people start adding
</em><br>
<em>&gt; preconditions - but slippery slope arguments aren't always valid.
</em><br>
<em>&gt; If 100 Xs are bad, that doesn't always mean the right number of
</em><br>
<em>&gt; Xs is 0.)
</em><br>
<p>I certainly agree there. Eliezer appears to be obsessively concerned
<br>
with finding a 'single unique solution' to the FAI problem, where
<br>
the uniqueness criterion will effectively be 'meets some personal
<br>
and probably highly abstract criterion of perfection'. While this is
<br>
not the field I have been focusing on, and it is possible that I
<br>
have a mistaken impression, this seems like suboptimal Singularity
<br>
strategy coloured by ill-chosen axioms to me. I would rather that
<br>
humanity makes it into the future with a few problems to deal with,
<br>
than we let the world be destroyed by UFAI/etc because we spent so
<br>
long trying to be 'perfect'. This isn't a contradiction of the
<br>
standard Yudkowsky rheotic about accepting less than your best
<br>
effort being fatal and rushing things leading to disaster (which
<br>
still applies, for the most part); it's about avoiding the folly of
<br>
chasing pointless and illusory standards of perfection.
<br>
<p>That said, clearly all current FAI theory still has a /long/ way to
<br>
go.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
To help you stay safe and secure online, we've developed the all new Yahoo! Security Centre. <a href="http://uk.security.yahoo.com">http://uk.security.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12668.html">Olie L: "Re: Immoral optimisation - killing AIs"</a>
<li><strong>Previous message:</strong> <a href="12666.html">Robin Lee Powell: "Re: Indoctrinating my family"</a>
<li><strong>In reply to:</strong> <a href="12643.html">Russell Wallace: "Re: Extrapolated volition: oops"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12667">[ date ]</a>
<a href="index.html#12667">[ thread ]</a>
<a href="subject.html#12667">[ subject ]</a>
<a href="author.html#12667">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:19 MST
</em></small></p>
</body>
</html>
