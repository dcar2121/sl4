<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Domain Protection</title>
<meta name="Author" content="Russell Wallace (russell.wallace@gmail.com)">
<meta name="Subject" content="Re: Domain Protection">
<meta name="Date" content="2005-05-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Domain Protection</h1>
<!-- received="Wed May 18 19:14:52 2005" -->
<!-- isoreceived="20050519011452" -->
<!-- sent="Thu, 19 May 2005 02:14:50 +0100" -->
<!-- isosent="20050519011450" -->
<!-- name="Russell Wallace" -->
<!-- email="russell.wallace@gmail.com" -->
<!-- subject="Re: Domain Protection" -->
<!-- id="8d71341e0505181814191f2702@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="428BAAC9.4000400@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Russell Wallace (<a href="mailto:russell.wallace@gmail.com?Subject=Re:%20Domain%20Protection"><em>russell.wallace@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed May 18 2005 - 19:14:50 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11119.html">Damien Broderick: "Eliezer vs. Jaron Lanier"</a>
<li><strong>Previous message:</strong> <a href="11117.html">Nick Bostrom: "RE: Robot that thinks like a human"</a>
<li><strong>In reply to:</strong> <a href="11115.html">Eliezer S. Yudkowsky: "Re: Domain Protection"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11119.html">Damien Broderick: "Eliezer vs. Jaron Lanier"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11118">[ date ]</a>
<a href="index.html#11118">[ thread ]</a>
<a href="subject.html#11118">[ subject ]</a>
<a href="author.html#11118">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 5/18/05, Eliezer S. Yudkowsky &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Domain%20Protection">sentience@pobox.com</a>&gt; wrote:
<br>
[feedback on my proposal, thanks]
<br>
<p>...Okay, it seems to me you claim that Domain Protection is inferior
<br>
to Collective Volition in the following ways; correct me if I'm
<br>
misstating or omitting any:
<br>
<p>1. It has more moving parts: more things to be specified, more things
<br>
to go wrong.
<br>
<p>No, it only seems that way because I'm giving specific answers to
<br>
things you're sweeping under the carpet. For all that your paper was
<br>
longer than mine (and indeed better written; frankly, I was tired and
<br>
in a hurry when I wrote mine, because unlike you I don't get paid for
<br>
writing about AI; but I've tried to clarify anything that wasn't
<br>
initially clear), it's a lot less specific about details.
<br>
<p><em>&gt; The age of majority... laws of transit... minimum
</em><br>
<em>&gt; group size to define an environment... communication between domains...
</em><br>
<em>&gt; debts... bankruptcy... division of resources...
</em><br>
<p>...are all decisions that have to be made, whichever proposal is
<br>
followed. I've suggested how to make them (in most cases by people
<br>
other than myself, see my earlier replies in this thread); you
<br>
haven't.
<br>
<p><em>&gt; You are not being more conservative about what will be possible.  You are not
</em><br>
<em>&gt; being more conservative about what you are trying to solve.  I very carefully
</em><br>
<em>&gt; limited myself to trying to solve only one problem.  It was a big problem, but
</em><br>
<em>&gt; there was still only one of it, and it wasn't a fun political issue such as
</em><br>
<em>&gt; humans love to get stuck in and underestimate the complexity of.
</em><br>
<p>It only looks like one problem because you've found a sentence in
<br>
English that describes it as one. In fact you're trying to solve
<br>
essentially _all_ the hardest problems simultaneously, and doing it in
<br>
one big lump with everything including the kitchen sink thrown in, and
<br>
throwing a carpet of English words over the lump to obscure the
<br>
details so they can't be seen well enough to have a chance of solving
<br>
any of them.
<br>
<p>In real life, political issues have to be dealt with, however much us
<br>
geeks wish it weren't so. I've tried to analyze them and boil them
<br>
down to a bare minimum that just might have a prayer of being
<br>
solvable. I can't promise my solution will work - but it has a much
<br>
better chance than the head-in-the-sand approach.
<br>
<p>2. You're better qualified than me to be thinking about this stuff.
<br>
<p>Well, you have a high opinion of your abilities. Cool; I have a high
<br>
opinion of my abilities too. The proof of both those puddings will be
<br>
in the eating. You think certain things are feasible, I think they
<br>
aren't; neither one of us has mathematical proof, nor yet a line of
<br>
working AI code to serve as experimental evidence, so right now all we
<br>
can say for sure is that your intuition differs from mine (and believe
<br>
me, mine was no more casually arrived at than yours).
<br>
<p><em>&gt; And then you go on to confidently declare, &quot;Smarter yes, wiser and
</em><br>
<em>&gt; more benevolent no; it's a Godel problem, you can't do better than your
</em><br>
<em>&gt; axioms.&quot;  Wow!  Where'd you get the firm theoretical ground from to
</em><br>
<em>&gt; confidently declare this problem unsolvable?  Didn't you just get through
</em><br>
<em>&gt; saying you didn't have one?
</em><br>
<p>Of course, and neither have you (or if you have, there's no sign of it
<br>
in what you've published). If you can come up with proof that you're
<br>
right, great!
<br>
<p>But what if you're wrong?
<br>
<p>3. It doesn't have certain features you want.
<br>
<p>Okay, I'll address these in more detail below.
<br>
<p><em>&gt; Let's review the Domain Protection according to the seven motivations listed
</em><br>
<em>&gt; in the original &quot;Collective Volition&quot; page.  You may wish to refer to the
</em><br>
<em>&gt; original discussion:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; <a href="http://www.intelligence.org/friendly/collective-volition.html#motivations">http://www.intelligence.org/friendly/collective-volition.html#motivations</a>
</em><br>
<p>Okay.
<br>
<p><em>&gt; 1. Defend humans, the future of humankind, and humane nature.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; * I don't see anything in the DP framework that covers any of those three
</em><br>
<em>&gt; things, except possibly &quot;defend humans&quot; - but even that would depend on the
</em><br>
<em>&gt; specific domains created, or the mechanism for creating domains.  If all
</em><br>
<em>&gt; domains are wildernesses in which death is permanent, that is not &quot;defend
</em><br>
<em>&gt; humans&quot;.  Since you didn't specify the exact mechanism which determines which
</em><br>
<em>&gt; domains are created, I don't know what kind of domains will be created.  Since
</em><br>
<em>&gt; I don't know what kind of domains will be created, I don't know what will
</em><br>
<em>&gt; happen to the people in them.
</em><br>
<p>See my replies earlier in the thread for a suggested mechanism for
<br>
creating domains. Defending the future of humankind and humane nature
<br>
is the primary purpose of Domain Protection, and it is addressed by
<br>
creating domains in which they are protected, where boundaries are
<br>
defined in state space that confine the domain to a region in which
<br>
humanity exists.
<br>
<p><em>&gt; As for defending the 'future of humankind'
</em><br>
<em>&gt; (which in CEV is dealt with by trying to extrapolate the spectrum of probable
</em><br>
<em>&gt; futures and our reactions to them), it's not clear how any progress at all is
</em><br>
<em>&gt; made on using AI superintelligence to guard against dangers we did not know;
</em><br>
<em>&gt; you propose a fixed unalterable structure that you think might deal with a
</em><br>
<em>&gt; small set of dangers to humanity that you foresee, such as 'loss of
</em><br>
<em>&gt; diversity'.
</em><br>
<p>You're missing the other point of diversity - it's the only way to
<br>
deal with dangers we didn't foresee. If you only have one domain, as
<br>
CEV proposes, then when an unforeseen danger comes up that kills that
<br>
domain (and I've already pointed at least one out that you didn't
<br>
foresee - who's to say there aren't others that neither of us has
<br>
foreseen?), it's goodnight forever. Having many domains gives the best
<br>
chance of at least some surviving.
<br>
<p><em>&gt; With &quot;humane nature&quot; the proposal contains nothing that would
</em><br>
<em>&gt; tend to solve the chicken-and-egg problem of humans who are not wise enough to
</em><br>
<em>&gt; upgrade themselves upgrading themselves to where they are wise enough to
</em><br>
<em>&gt; upgrade themselves.
</em><br>
<p>Nor does it preclude this. If you think you have a solution, by all
<br>
means get 99 like-minded folk together and create a domain where
<br>
you're free to upgrade yourselves to your heart's content. Meanwhile
<br>
if you get it wrong, you've only killed yourselves and not all of
<br>
humanity.
<br>
<p><em>&gt; CEV is an attempt to get the best definition we can of
</em><br>
<em>&gt; that-which-we-wish-to-preserve-through-the-transition that isn't limited to
</em><br>
<em>&gt; our present-day abilities to define it
</em><br>
<p>Whereas DP doesn't rely that we gamble on one such definition being correct.
<br>
<p><em>&gt; and returns deliberately spread-out
</em><br>
<em>&gt; answers in the case our possible decisions aren't strongly concentrated.
</em><br>
<p>What if they're strongly concentrated in the wrong place? What if
<br>
they're not concentrated but decisions still have to be made?
<br>
<p><em>&gt; 2.  Encapsulate Moral Growth
</em><br>
<em>&gt; 
</em><br>
<em>&gt; * Domain Protection doesn't do this at all.  Period.
</em><br>
<p>Yep, it doesn't aspire to being a theory of encapsulating moral
<br>
growth. Nor does it preclude such; again, if you think you have a
<br>
solution to this, by all means create a domain where you can implement
<br>
your proposed solution.
<br>
<p><em>&gt; 3.  Humankind should not spend the rest of eternity desperately wishing that
</em><br>
<em>&gt; the programmers had done something differently.  &quot;This seems obvious, until
</em><br>
<em>&gt; you realize that only the Singularity Institute has even tried to address this
</em><br>
<em>&gt; issue. I haven't seen a single other proposal for AI morality out there, not
</em><br>
<em>&gt; even a casual guess, that takes the possibility of Singularity Regret into
</em><br>
<em>&gt; account. Not one. Everyone has their brilliant idea for the Four Great Moral
</em><br>
<em>&gt; Principles That Are All We Need To Program Into AIs, and not one says, &quot;But
</em><br>
<em>&gt; wait, what if I got the Four Great Moral Principles wrong?&quot;  They don't think
</em><br>
<em>&gt; of writing any escape clause, any emergency exit if the programmers made the
</em><br>
<em>&gt; wrong decision. They don't wonder if the original programmers of the AI might
</em><br>
<em>&gt; not be the wisest members of the human species; or if even the wisest
</em><br>
<em>&gt; human-level minds might flunk the test; or if humankind might outgrow the
</em><br>
<em>&gt; programmers' brilliantly insightful moral philosophy a few million years hence.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; * I didn't see *anywhere* in your Domain Protection page where you talked
</em><br>
<em>&gt; about a revocation mechanism and a framework for controlled transition to
</em><br>
<em>&gt; something else.  Don't tell me it was a minor oversight.
</em><br>
<p>Of course it wasn't an oversight, it's the core feature! Part of the
<br>
point of having multiple domains is that humanity does _not_ have to
<br>
trust me or you or anyone to get things right. If you think I'm wrong
<br>
about the way I want to live for the next billion years, go to a
<br>
domain where your way applies.
<br>
<p>The point of DP is that you can turn yourself into a deranged
<br>
superintelligence in the name of a misguided theory of moral growth
<br>
and then go on to destroy everything of value in _your_ domain if you
<br>
want to - you just don't ever get to destroy _my_ domain, neither
<br>
tomorrow nor in a billion years, no matter what arguments you come up
<br>
with as to why you should be allowed to do so.
<br>
<p>But you don't think your theory of moral growth will be misguided, and
<br>
you're not planning to become deranged or destroy everything of value?
<br>
Of course. How do you know your proposed upgrade program won't have
<br>
that result anyway? You don't. We have two choices:
<br>
<p>1) Gamble the survival of all humanity on _one_ path being right...
<br>
and keep gambling until the dice go against us.
<br>
<p>2) Lay down in stone that some things are protected forever, no matter
<br>
what else goes wrong.
<br>
<p>CEV proposes the first option, DP proposes the second.
<br>
<p><em>&gt; 4. Avoid hijacking the destiny of humankind.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; * You aren't proposing to define the domains yourself (as far as I can tell),
</em><br>
<em>&gt; so you aren't being a jerk.  You could end up destroying the destiny of
</em><br>
<em>&gt; humankind but you don't appear to be trying to hijack it per se.
</em><br>
<p>*nods* CEV and DP both meet this criterion; I'm inclined to agree with
<br>
a comment you made once, that having a shot at designing an AI that
<br>
wouldn't just turn everything in reach into grey goo, requires
<br>
qualities of mind that are at least unlikely to be compatible with not
<br>
having risen above that level.
<br>
<p><em>&gt; 5. Avoid creating a motive for modern-day humans to fight over the initial
</em><br>
<em>&gt; dynamic.
</em><br>
<p>Right, DP meets this criterion far better than CEV does.
<br>
<p><em>&gt; * Along comes an AI project that wants to define the minimum age to move
</em><br>
<em>&gt; freely between domains as 13, instead of 18, in according with the laws of the
</em><br>
<em>&gt; Torah.  How do the two of you settle your differences?
</em><br>
<p>That's easy; they get to define a domain where 13 year olds are free
<br>
to emigrate, other people get to define one where you have to be 18 to
<br>
be allowed to emigrate.
<br>
<p><em>&gt; Would you advise the
</em><br>
<em>&gt; al-Qaeda programmers to make as many decisions (irrevocable decisions!) as you
</em><br>
<em>&gt; wish to allocate to yourself?
</em><br>
<p>I am proposing to make very few decisions myself - considerably fewer
<br>
and less consequential ones than you are, in fact. In DP, the al-Qaeda
<br>
guys can create a domain where the Koran is enforced to the letter or
<br>
whatever if that's what they want. Under CEV, because everyone is
<br>
forced down the same path, if al-Qaeda think they are outnumbered they
<br>
have to take a truck bomb to the Singularity Institute or see their
<br>
philosophy extinguished.
<br>
<p><em>&gt; 7. Help people.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; * DP suggests no Last Judge or equivalent, and no stated dependency on
</em><br>
<em>&gt; people's actual reactions or extrapolated judgments.  There's no mechanism for
</em><br>
<em>&gt; revoking DP if its consequences turn out to be horrible.
</em><br>
<p>What if the consequences of having a mechanism to revoke DP would turn
<br>
out to be horrible?
<br>
<p>But DP is compatible with having a Last Judge; if you can come up with
<br>
a coherent proposal for a mechanism by which the information provided
<br>
to the Judge would have significant causal dependence on something
<br>
other than the prejudices of the programmer, perhaps that should be
<br>
added.
<br>
<p><em>&gt; ...I wish people understood just how non-arbitrary the CEV proposal is.  If
</em><br>
<em>&gt; you understand all the motivations here that must needs be satisfied, you will
</em><br>
<em>&gt; see how really difficult it is to come up with anything that works half as
</em><br>
<em>&gt; well, let alone better.
</em><br>
<p>Oh, I understand your motivations, perhaps better than you do
<br>
yourself; they are similar to mine before I learned to add caution and
<br>
humility into the mix. Still, we probably have time for you to learn
<br>
more caution.
<br>
<p><em>&gt; Think meta, and keep thinking.  Let me worry about what's technically
</em><br>
<em>&gt; impossible.  Try to say what you care about, what you want as the consequence,
</em><br>
<em>&gt; not what means you think will achieve the consequence.  Both judgments are
</em><br>
<em>&gt; fallible, but the latter is considerably more fallible.
</em><br>
<p>As I remarked above, statements to the effect that other researchers
<br>
shouldn't worry their pretty little heads about technical things are
<br>
all the better for proof ^.~ But here's what I care about:
<br>
<p>1. Safety
<br>
<p>Above all else, to preserve humanity (not so much the protein and DNA
<br>
hardware technology, but our values and modes of experience). DP is
<br>
the best way I can think of to do that.
<br>
<p>CEV fails here; I've already pointed out ways in which it's liable to
<br>
exterminate all sentient life, and there are probably others I haven't
<br>
thought of.
<br>
<p>Here's another: doesn't CEV require the AI to unilaterally take over
<br>
the world (so that it can start doing what it thinks we ought to want,
<br>
rather than what we actually want)? If not, how are you proposing to
<br>
get it accepted? If so, have you considered the issues of feasibility
<br>
and safety involved?
<br>
<p>2. Diversity
<br>
<p>CEV proposes that every man, woman and child is forced down the same
<br>
path, whatever their feelings on the matter. That's _not_ a good
<br>
starting point for diversity.
<br>
<p>3. Fairness
<br>
<p>Apart from the little taking over the world problem above, there's the
<br>
problem of forcing everyone down the same path again - which means
<br>
minority groups are faced with the prospect of having to fight you or
<br>
be extinguished.
<br>
<p>&quot;If all mankind minus one were of one opinion, mankind would be no
<br>
more justified in silencing that one person than he, if he had the
<br>
power, would be justified in silencing mankind.&quot; - John Stuart Mill
<br>
<p>- Russell
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11119.html">Damien Broderick: "Eliezer vs. Jaron Lanier"</a>
<li><strong>Previous message:</strong> <a href="11117.html">Nick Bostrom: "RE: Robot that thinks like a human"</a>
<li><strong>In reply to:</strong> <a href="11115.html">Eliezer S. Yudkowsky: "Re: Domain Protection"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11119.html">Damien Broderick: "Eliezer vs. Jaron Lanier"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11118">[ date ]</a>
<a href="index.html#11118">[ thread ]</a>
<a href="subject.html#11118">[ subject ]</a>
<a href="author.html#11118">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:22:56 MST
</em></small></p>
</body>
</html>
