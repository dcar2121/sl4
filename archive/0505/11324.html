<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Robot that thinks like a human</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="Re: Robot that thinks like a human">
<meta name="Date" content="2005-05-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Robot that thinks like a human</h1>
<!-- received="Thu May 19 19:29:01 2005" -->
<!-- isoreceived="20050520012901" -->
<!-- sent="Thu, 19 May 2005 21:28:52 -0400" -->
<!-- isosent="20050520012852" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="Re: Robot that thinks like a human" -->
<!-- id="008701c55cdb$49409690$0f010a0a@CUTIOIDE" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20050520010136.13866.qmail@web26705.mail.ukl.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=Re:%20Robot%20that%20thinks%20like%20a%20human"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Thu May 19 2005 - 19:28:52 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11325.html">Marc Geddes: "Re: Proposed Universal data types"</a>
<li><strong>Previous message:</strong> <a href="11323.html">Michael Wilson: "Re: Robot that thinks like a human"</a>
<li><strong>In reply to:</strong> <a href="11323.html">Michael Wilson: "Re: Robot that thinks like a human"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11329.html">Dani Eder: "Re: Robot that thinks like a human"</a>
<li><strong>Reply:</strong> <a href="11329.html">Dani Eder: "Re: Robot that thinks like a human"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11324">[ date ]</a>
<a href="index.html#11324">[ thread ]</a>
<a href="subject.html#11324">[ subject ]</a>
<a href="author.html#11324">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; You claim that you've made finding a workable FAI design the central goal
</em><br>
<em>&gt; of your project, but have you sat down and designed a set of experiments
</em><br>
<em>&gt; that will generate the information you need?
</em><br>
<p>Largely, yes.
<br>
<p><em>&gt; Have you worked out what the
</em><br>
<em>&gt; blocker set of questions you need answers to is?
</em><br>
<p>Largely...
<br>
<p><em>&gt;If your objective is to
</em><br>
<em>&gt; (safely) generate data for FAI design, then you should be planning in
</em><br>
<em>&gt; depth how you're going to go about getting it. Reality will probably
</em><br>
<em>&gt; deviate from the plan, but without the plan you're just groping about
</em><br>
<em>&gt; aimlessly. The only good reason I can think of not to put a core team
</em><br>
<em>&gt; member on this full time is that you don't have enough funding yet.
</em><br>
<p>In fact, we are currently at a very difficult point in terms of funding, and 
<br>
only two people (none of them me) are working full-time on Novamente AGI.
<br>
<p><em>&gt; I've had someone seriously propose to me that we use limited AI to
</em><br>
<em>&gt; rapidly develop nanotech, which would then be used to take over the world
</em><br>
<em>&gt; and shut down all other AI/nanotech/biotech projects to prevent anything
</em><br>
<em>&gt; bad from happening (things got rather hazy after that).
</em><br>
<p>Hmmm... I think I can guess who ;-)
<br>
<p><em>&gt;I don't worry
</em><br>
<em>&gt; about it because 'highly restricted human-level AGI' is very, very hard
</em><br>
<em>&gt; and ultimately pointless (if you know how to make a 'human-level AGI'
</em><br>
<em>&gt; controllable, then you know how to make a transhuman AGI controllable).
</em><br>
<em>&gt; People less convinced about hard takeoff will doubtless be more concerned
</em><br>
<em>&gt; about this sort of thing.
</em><br>
<p>I am more concerned about this kind of thing than you are.  I think 
<br>
&quot;human-level&quot; is a very vague term, and that someone could make an AI that 
<br>
was superhuman in enough ways to successfully help them achieve definitive 
<br>
world domination, yet not self-reflective enough to achieve hard takeoff.
<br>
<p><em>&gt;&gt; Orwell may have been overoptimistic in his time estimate, but his
</em><br>
<em>&gt;&gt; basic point about what technology can do if we let it, still seems
</em><br>
<em>&gt;&gt; to me right-on
</em><br>
<em>&gt;
</em><br>
<em>&gt; You don't need AGI to take over the world, particularly if you already
</em><br>
<em>&gt; have the resources of a nation state or multinational.
</em><br>
<p>Taking over the world quickly enough and well enough to prevent development 
<br>
of AGI's would be difficult given current technologies, though.
<br>
<p><em>&gt; 'Self-organising' is a bit of a vauge term. It can mean 'self-modifying,
</em><br>
<em>&gt; but with less seed complexity than a fully specified seed AI'
</em><br>
<p>The distinction between self-modification and learning is not well-defined.
<br>
<p><em>&gt;or 'AGI
</em><br>
<em>&gt; in which learning operations are probabilistic and/or stochastic' or
</em><br>
<em>&gt; 'AGI in which self-modification is pervasive, local and without central
</em><br>
<em>&gt; control'. As far as I can tell Novamente falls in the first and third
</em><br>
<em>&gt; categories and may or may not be in the second depending on how much
</em><br>
<em>&gt; you've moved on from the 'bubbling broth' approach.
</em><br>
<p>Novamente never had a &quot;bubbling broth&quot; approach.  Webmind sorta did. 
<br>
Novamente relies centrally on Probabilistic Term Logic.
<br>
<p>In Novamente, significant self-modification is done under central control. 
<br>
But as I said, it's not really possible to distinguish minor 
<br>
self-modification from learning --- and in this sense, there is plenty of 
<br>
self-modification occurring in Novamente all the time without explicit 
<br>
central control.
<br>
<p><em>&gt;Thus the solution combines both (a) and (b), though
</em><br>
<em>&gt; I wouldn't say that what is needed is new mathematics so much as a novel
</em><br>
<em>&gt; means of describing AGI functionality that allows us to tractably apply
</em><br>
<em>&gt; the formal methods we've already got.
</em><br>
<p>Well, it's an interesting idea and I'll be eager to see some details 
<br>
someday...
<br>
<em>&gt; This is where we part ways again. I think that the best approach is to
</em><br>
<em>&gt; develop progressively more advanced AI systems, using each existing system
</em><br>
<em>&gt; as a formal prover to develop the next design.
</em><br>
<p>Egads!  How much work have you guys ever done with real formal theorem 
<br>
provers?
<br>
<p>I worked a bit with HOL and Otter, and boy do they suck, though they're 
<br>
great by comparison to the competition....
<br>
<p>Unfortunately, it seems that making theorem-provers that really work 
<br>
probably requires a fairly high level of AGI....
<br>
<p>I agree that IF you can get around this problem and make a nonsentient, 
<br>
highly specialized but highly powerful theorem-proving AI, THEN this is the 
<br>
best route to FAI.
<br>
<p>I just don't believe it is possible....
<br>
<p><em>&gt;&gt; 1) toddler AGI's interacting with each other in simulated environments,
</em><br>
<em>&gt;&gt; which will pose no significant hard-takeoff danger but will let us begin
</em><br>
<em>&gt;&gt; learning empirically about AGI morality in practice
</em><br>
<em>&gt;
</em><br>
<em>&gt; This makes no sense if your AGI does in fact possess a causally clean
</em><br>
<em>&gt; goal system. 'AGI morality' should be something you inscribe on your
</em><br>
<em>&gt; code (or startup KB);
</em><br>
<p>Yah, but since in reality not all system operations can be directly chosen 
<br>
based on the goals, there is still some teaching needed, to be sure the 
<br>
system correctly learns how to combine its explicitly goal-oriented 
<br>
higher-level control with its lower-level goal-regulated but not 
<br>
in-detail-goal-dictated activity...
<br>
<p><em>&gt;&gt; Note that these two goals intersect, if you buy the Lakoff-Nunez argument
</em><br>
<em>&gt;&gt; that human math is based on metaphors of human physical perception and
</em><br>
<em>&gt;&gt; action.
</em><br>
<em>&gt;
</em><br>
<em>&gt; How does that require interaction between AGI instances? The only bit of
</em><br>
<em>&gt; maths that requires an idea of other intelligences is game theory.
</em><br>
<p>What understanding of human math requires, according to Lakoff and Nunez, is 
<br>
not interaction with other intelligences but rather embodiment, or at least 
<br>
the intuition about simple &quot;prepositional&quot; type relationships that humans 
<br>
gain from embodiment...
<br>
<p><em>&gt;&gt; Once we have 1 and 2 we will have much better knowledge and tools for
</em><br>
<em>&gt;&gt; making the big decision, i.e. whether to launch a hard-takeoff or
</em><br>
<em>&gt;&gt; impose a global thought-police AGI-suppression apparatus....
</em><br>
<em>&gt;
</em><br>
<em>&gt; Are you going on the record saying that AGIRI will take it upon themselves
</em><br>
<em>&gt; to try and implement (2) ?
</em><br>
<p>Indeed, at the  moment we are already experimenting with simple 
<br>
theorem-proving using Novamente.  It's not our current top priority though.
<br>
<p><em>&gt;&gt; I have a pretty good idea what I'm looking for, in fact. I'm looking for
</em><br>
<em>&gt;&gt; dynamical laws governing the probabilistic grammars that emerge from
</em><br>
<em>&gt;&gt; discretizing the state space of an interactive learning system. I have
</em><br>
<em>&gt;&gt; some hypotheses regarding what these dynamical laws should look like.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I could make an educated guess at what you mean by this, but experience
</em><br>
<em>&gt; has taught me to avoid guessing about other people's AGI ideas if
</em><br>
<em>&gt; possible. Would you care to expand?
</em><br>
<p>I will, but later... I've spent enough time emailing this morning ;-)
<br>
&nbsp;[morning because I'm in Australia at the moment]
<br>
<p><em>&gt;&gt; Which makes the overall behavior less easily predictable...
</em><br>
<em>&gt;
</em><br>
<em>&gt; Technically, yes. Practically, not necessarily, because a good design
</em><br>
<em>&gt; should be able to enforce absolute constraints on the 'freedom of
</em><br>
<em>&gt; action' of local dynamics that prevent them from altering anything
</em><br>
<em>&gt; important.
</em><br>
<p>That is hard, if the local dynamics are organizing the knowledge base that 
<br>
guides the high-level goal system's interpretation of important terms and 
<br>
concepts
<br>
<p>-- Ben 
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11325.html">Marc Geddes: "Re: Proposed Universal data types"</a>
<li><strong>Previous message:</strong> <a href="11323.html">Michael Wilson: "Re: Robot that thinks like a human"</a>
<li><strong>In reply to:</strong> <a href="11323.html">Michael Wilson: "Re: Robot that thinks like a human"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11329.html">Dani Eder: "Re: Robot that thinks like a human"</a>
<li><strong>Reply:</strong> <a href="11329.html">Dani Eder: "Re: Robot that thinks like a human"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11324">[ date ]</a>
<a href="index.html#11324">[ thread ]</a>
<a href="subject.html#11324">[ subject ]</a>
<a href="author.html#11324">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
