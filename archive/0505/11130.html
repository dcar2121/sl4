<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Robot that thinks like a human</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="Re: Robot that thinks like a human">
<meta name="Date" content="2005-05-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Robot that thinks like a human</h1>
<!-- received="Thu May 19 19:01:39 2005" -->
<!-- isoreceived="20050520010139" -->
<!-- sent="Fri, 20 May 2005 02:01:36 +0100 (BST)" -->
<!-- isosent="20050520010136" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="Re: Robot that thinks like a human" -->
<!-- id="20050520010136.13866.qmail@web26705.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="00ff01c55cc3$fc4007c0$0f010a0a@CUTIOIDE" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20Robot%20that%20thinks%20like%20a%20human"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Thu May 19 2005 - 19:01:36 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11131.html">Ben Goertzel: "Re: Robot that thinks like a human"</a>
<li><strong>Previous message:</strong> <a href="11129.html">Michael Wilson: "Re: Systems engineering"</a>
<li><strong>In reply to:</strong> <a href="11128.html">Ben Goertzel: "Re: Robot that thinks like a human"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11131.html">Ben Goertzel: "Re: Robot that thinks like a human"</a>
<li><strong>Reply:</strong> <a href="11131.html">Ben Goertzel: "Re: Robot that thinks like a human"</a>
<li><strong>Reply:</strong> <a href="11134.html">Russell Wallace: "Re: Robot that thinks like a human"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11130">[ date ]</a>
<a href="index.html#11130">[ thread ]</a>
<a href="subject.html#11130">[ subject ]</a>
<a href="author.html#11130">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; Michael, I don't disagree with this.  What I object to is the implication 
</em><br>
<em>&gt; that SIAI is the only group that takes these issues seriously.
</em><br>
<p>The SIAI is the only group that is taking these issues seriously /enough/.
<br>
The SIAI has made Friendliness its primary research objective and has
<br>
dedicated 100% of its self-funded research effort to solving that problem.
<br>
You claim that you've made finding a workable FAI design the central goal
<br>
of your project, but have you sat down and designed a set of experiments
<br>
that will generate the information you need? Have you worked out what the
<br>
blocker set of questions you need answers to is? If your objective is to
<br>
(safely) generate data for FAI design, then you should be planning in
<br>
depth how you're going to go about getting it. Reality will probably
<br>
deviate from the plan, but without the plan you're just groping about
<br>
aimlessly. The only good reason I can think of not to put a core team
<br>
member on this full time is that you don't have enough funding yet.
<br>
<p><em>&gt; For one thing, it is clear to me that world domination using a
</em><br>
<em>&gt; human-level  but highly restricted AGI (together with other advanced
</em><br>
<em>&gt; technologies) is possible...
</em><br>
<p>I've had someone seriously propose to me that we use limited AI to
<br>
rapidly develop nanotech, which would then be used to take over the world
<br>
and shut down all other AI/nanotech/biotech projects to prevent anything
<br>
bad from happening (things got rather hazy after that). I don't worry
<br>
about it because 'highly restricted human-level AGI' is very, very hard
<br>
and ultimately pointless (if you know how to make a 'human-level AGI'
<br>
controllable, then you know how to make a transhuman AGI controllable).
<br>
People less convinced about hard takeoff will doubtless be more concerned
<br>
about this sort of thing.
<br>
<p><em>&gt; Orwell may have been overoptimistic in his time estimate, but his
</em><br>
<em>&gt; basic point about what technology can do if we let it, still seems
</em><br>
<em>&gt; to me right-on
</em><br>
<p>You don't need AGI to take over the world, particularly if you already
<br>
have the resources of a nation state or multinational. There are several
<br>
highly disruptive technologies coming up that could potentially do it,
<br>
particularly in combination. I don't worry about this either because
<br>
there isn't much I can do about it.
<br>
<p><em>&gt; It may happen (I hope not!) that investigation of FAI reveals that
</em><br>
<em>&gt; it's so hard that the only way to avoid non-Friendly superhuman AI
</em><br>
<em>&gt; is to enforce a global dictatorship that forbids AGI research.
</em><br>
<p>Both the futility and the unlikeliness of this have been previously
<br>
discussed at length on SL4.
<br>
<p><em>&gt; I don't agree, but I don't see what I would gain by posting to this
</em><br>
<em>&gt; list a detailed plan for how to use human-level but highly restricted
</em><br>
<em>&gt; AGI to achieve world domination.
</em><br>
<p>So you too amuse yourself in the shower by thinking up fiendishly
<br>
intricate plans for world domination? Alas the SIAI people tell me
<br>
that this is a bad habit that I will have to kick ;)
<br>
<p><em>&gt; This is where we disagree.  There is a lot of middle ground between 
</em><br>
<em>&gt; experiments with individual system components (which we've already been 
</em><br>
<em>&gt; doing for a while) and  &quot;random-access interaction pattern over every
</em><br>
<em>&gt; single functional component&quot; ....
</em><br>
<p>Yes, there is. Technically speaking the implementatin project I'm working
<br>
on now falls into that middle ground. However that's a commercial product
<br>
and a proof of concept, not an exploratory prototype. I believe that
<br>
exploratory prototyping of anything above small components is neither
<br>
necessary nor advisable. I'll make a serious attempt to convince you of
<br>
that if and when I have a nonesoteric demonstration.
<br>
<p><em>&gt; The relevant questions pertain to what the dynamics of an AGI system
</em><br>
<em>&gt; will be when put into various complex situations involving interaction
</em><br>
<em>&gt; with other minds.
</em><br>
<p>Actually agent modelling and game theory seem to be one of the less
<br>
complicated parts of AGI to me; the latter isn't strictly speaking AGI
<br>
at all, though it's an AGI competence. This is exactly the sort of
<br>
thing that you should be able to fully specify in advance; not the
<br>
accuracy of the AGI's external agent model, which is essentially a
<br>
performance issue (unless you're silly enough to introduce reasoning
<br>
pathologies that prevent the modelling of whole classes of cognitive
<br>
architecture), but the important issue of what it will do with the
<br>
outputs of those models.
<br>
<p><em>&gt; So we then need either;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; a) a design for an AGI that is not a complex, self-organizing
</em><br>
<em>&gt; system, but is more predictable and tractably mathematically modelable
</em><br>
<em>&gt; b) a really powerful new mathematics of (intelligent) complex, 
</em><br>
<em>&gt; self-organizing systems
</em><br>
<p>'Self-organising' is a bit of a vauge term. It can mean 'self-modifying,
<br>
but with less seed complexity than a fully specified seed AI' or 'AGI
<br>
in which learning operations are probabilistic and/or stochastic' or
<br>
'AGI in which self-modification is pervasive, local and without central
<br>
control'. As far as I can tell Novamente falls in the first and third
<br>
categories and may or may not be in the second depending on how much
<br>
you've moved on from the 'bubbling broth' approach. Designs produced by
<br>
the approach the SIAI advocates will not be any of these things.
<br>
However all seed AIs use radical self-modification and thus the problem
<br>
of proving that abstract constraints hold for highly complex functional
<br>
mechanisms remains. Thus the solution combines both (a) and (b), though
<br>
I wouldn't say that what is needed is new mathematics so much as a novel
<br>
means of describing AGI functionality that allows us to tractably apply
<br>
the formal methods we've already got.
<br>
<p><em>&gt; It may be that the best way to achieve b is to create a
</em><br>
<em>&gt; &quot;specialized AGI&quot; whose only domains are mathematics and scientific
</em><br>
<em>&gt; data analysis. The question then becomes whether it's possible to
</em><br>
<em>&gt; create an AGI with enough power to help us rapidly achieve b, yet
</em><br>
<em>&gt; without giving this AGI enough autonomy and self-modifying capability
</em><br>
<em>&gt; to achieve a surprise hard takeoff.
</em><br>
<p>Now this is the interesting part. I am very much in favour of this; it
<br>
seems to me that the difficultly of deliberative design increases
<br>
steadily as you go up from limited domain non-general AI to a seed FAI.
<br>
Furthermore I don't think general intelligence is necessary at all to
<br>
produce a really useful tool.
<br>
<p><em>&gt; I believe that this is possible to do, for instance using Novamente. I 
</em><br>
<em>&gt; don't think it's an easy problem but I think it's a vastly easier problem 
</em><br>
<em>&gt; than creating an AGI that remains Friendly through a takeoff.
</em><br>
<p>This is where we part ways again. I think that the best approach is to
<br>
develop progressively more advanced AI systems, using each existing system
<br>
as a formal prover to develop the next design. The goal is a system powerful
<br>
enough to support the design of an FAI, which may range from nothing (if
<br>
Eliezer is right and he can design a perfect FAI on paper first time) to
<br>
a Bostrom Oracle (if FAI design is only possible for Powers). However I
<br>
think that the only safe and effective way to bootstrap this is to use the
<br>
same methods to design the initial system by hand. You think that it's
<br>
possible and sensible to use educated guessing and exploratory prototyping
<br>
to develop an opaque initial system; I think that this is folly (albeit one
<br>
I emphasise with, as I held the same views not too long ago).
<br>
<p><em>&gt; 1) toddler AGI's interacting with each other in simulated environments, 
</em><br>
<em>&gt; which will pose no significant hard-takeoff danger but will let us begin 
</em><br>
<em>&gt; learning empirically about AGI morality in practice
</em><br>
<p>This makes no sense if your AGI does in fact possess a causally clean
<br>
goal system. 'AGI morality' should be something you inscribe on your
<br>
code (or startup KB); it isn't something that 'emerges' unless the goal
<br>
system has highly unstable definitions that don't track specific targets
<br>
in reality. The only empirical questions are what are the self-modification
<br>
trajectories of various sorts of goal system and how effective is your AGI
<br>
at cognitive task of predicting how other agents will react. If you are
<br>
truely in the position of an 'experimental behaviourist', trying out opaque
<br>
configurations to see what behaviour comes out, you are sunk before you
<br>
begin.
<br>
&nbsp;
<br>
<em>&gt; Note that these two goals intersect, if you buy the Lakoff-Nunez argument 
</em><br>
<em>&gt; that human math is based on metaphors of human physical perception and 
</em><br>
<em>&gt; action.
</em><br>
<p>How does that require interaction between AGI instances? The only bit of
<br>
maths that requires an idea of other intelligences is game theory.
<br>
<p><em>&gt; Once we have 1 and 2 we will have much better knowledge and tools for
</em><br>
<em>&gt; making the big decision, i.e. whether to launch a hard-takeoff or
</em><br>
<em>&gt; impose a global thought-police AGI-suppression apparatus....
</em><br>
<p>Are you going on the record saying that AGIRI will take it upon themselves
<br>
to try and implement (2) ?
<br>
<p><em>&gt; I have a pretty good idea what I'm looking for, in fact. I'm looking for 
</em><br>
<em>&gt; dynamical laws governing the probabilistic grammars that emerge from 
</em><br>
<em>&gt; discretizing the state space of an interactive learning system. I have
</em><br>
<em>&gt; some hypotheses regarding what these dynamical laws should look like.
</em><br>
<p>I could make an educated guess at what you mean by this, but experience
<br>
has taught me to avoid guessing about other people's AGI ideas if
<br>
possible. Would you care to expand?
<br>
<p><em>&gt; I don't believe it's computationally feasible to have every last
</em><br>
<em>&gt; computation done in the AGI system follow directly from the goal system.
</em><br>
<p>I agree that goal-relevance is a guess and it's not possible to make
<br>
that guess on an individual basis for every computation, and the bulk
<br>
of computation is local and not deliberative in the decision sense...
<br>
<p><em>&gt; So it becomes a matter of having the goal system regulate some more
</em><br>
<em>&gt; efficient but less predictable self-organizing learning dynamics.
</em><br>
<p>...but I consider 'self-organising' to be harmful and 'regulate' to
<br>
be a poor cousin to 'specify'.
<br>
<p><em>&gt; Which makes the overall behavior less easily predictable...
</em><br>
<p>Technically, yes. Practically, not necessarily, because a good design
<br>
should be able to enforce absolute constraints on the 'freedom of
<br>
action' of local dynamics that prevent them from altering anything
<br>
important.
<br>
<p><em>&gt; The efficiency workarounds seem to inevitably increase unpredictability.
</em><br>
<p>Again it all comes down to having a mechanism that can prove specific
<br>
constraints hold for the behaviour of complex (possibly self-modifying)
<br>
systems.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p><p><p><p><p><p><p><p>.
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
Yahoo! Messenger - want a free and easy way to contact your friends online? <a href="http://uk.messenger.yahoo.com">http://uk.messenger.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11131.html">Ben Goertzel: "Re: Robot that thinks like a human"</a>
<li><strong>Previous message:</strong> <a href="11129.html">Michael Wilson: "Re: Systems engineering"</a>
<li><strong>In reply to:</strong> <a href="11128.html">Ben Goertzel: "Re: Robot that thinks like a human"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11131.html">Ben Goertzel: "Re: Robot that thinks like a human"</a>
<li><strong>Reply:</strong> <a href="11131.html">Ben Goertzel: "Re: Robot that thinks like a human"</a>
<li><strong>Reply:</strong> <a href="11134.html">Russell Wallace: "Re: Robot that thinks like a human"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11130">[ date ]</a>
<a href="index.html#11130">[ thread ]</a>
<a href="subject.html#11130">[ subject ]</a>
<a href="author.html#11130">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:22:56 MST
</em></small></p>
</body>
</html>
