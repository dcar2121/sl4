<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Robot that thinks like a human</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="Re: Robot that thinks like a human">
<meta name="Date" content="2005-05-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Robot that thinks like a human</h1>
<!-- received="Thu May 19 16:42:14 2005" -->
<!-- isoreceived="20050519224214" -->
<!-- sent="Thu, 19 May 2005 18:42:05 -0400" -->
<!-- isosent="20050519224205" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="Re: Robot that thinks like a human" -->
<!-- id="00ff01c55cc3$fc4007c0$0f010a0a@CUTIOIDE" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20050519132438.11568.qmail@web26704.mail.ukl.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=Re:%20Robot%20that%20thinks%20like%20a%20human"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Thu May 19 2005 - 16:42:05 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11322.html">Michael Wilson: "Re: Systems engineering"</a>
<li><strong>Previous message:</strong> <a href="11320.html">Dani Eder: "Re: Systems engineering"</a>
<li><strong>In reply to:</strong> <a href="11316.html">Michael Wilson: "Re: Robot that thinks like a human"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11323.html">Michael Wilson: "Re: Robot that thinks like a human"</a>
<li><strong>Reply:</strong> <a href="11323.html">Michael Wilson: "Re: Robot that thinks like a human"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11321">[ date ]</a>
<a href="index.html#11321">[ thread ]</a>
<a href="subject.html#11321">[ subject ]</a>
<a href="author.html#11321">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; True. We say that it looks like it is possible, you say that it looks
</em><br>
<em>&gt; like it isn't possible, neither of us have published any formal
</em><br>
<em>&gt; reasoning to support our position. We think you're rationalising, you
</em><br>
<em>&gt; think we're indulding in wishful thinking. For now we can only keep
</em><br>
<em>&gt; working towards proof that resolves the issue one way or the other.
</em><br>
<p>Michael, I don't disagree with this.  What I object to is the implication 
<br>
that SIAI is the only group that takes these issues seriously.
<br>
<p>I agree that SIAI is taking a different approach from Novamente or anyone 
<br>
else to exploring Friendly-AI issues; and that no one, including SIAI, has 
<br>
made any really convincing arguments why their approach to Friendly-AI 
<br>
issues is superior.
<br>
<p><em>&gt; Reliable world domination is of the same
</em><br>
<em>&gt; structural difficultly as Friendliness; it's perhaps a little easier to
</em><br>
<em>&gt; specify what you want, but no easier to get an AGI to do it.
</em><br>
<p>I'm not at all sure this is correct.
<br>
<p>For one thing, it is clear to me that world domination using a human-level 
<br>
but highly restricted AGI (together with other advanced technologies) is 
<br>
possible ... whereas it's not yet clear to me that Friendly AI is even 
<br>
possible in any strong sense.
<br>
<p>Eliezer's writings contain a lot of nice arguments as to why FAI is either 
<br>
impossible or really, really hard.  I have not seen comparable arguments as 
<br>
to the difficulty or potential impossibility of technology-powered world 
<br>
domination.
<br>
<p>I'm afraid that here again you are indulging in wishful thinking ;-)
<br>
<p>Orwell may have been overoptimistic in his time estimate, but his basic 
<br>
point about what technology can do if we let it, still seems to me right-on
<br>
<p>It may happen (I hope not!) that investigation of FAI reveals that it's so 
<br>
hard that the only way to avoid non-Friendly superhuman AI is to enforce a 
<br>
global dictatorship that forbids AGI research.  Then an aesthetic/moral 
<br>
decision needs to be made, whether it's better to enforce such a 
<br>
dictatorship or just let the non-Friendly uber-AI romp and toss the cosmic 
<br>
coin...
<br>
<p><em>&gt; Even the
</em><br>
<em>&gt; people who think that AGIs will automatically have self-centered human
</em><br>
<em>&gt; like goal systems should agree with this. Anyone foolish enough to try
</em><br>
<em>&gt; and take over the world using AGI, and who manages to beat the very
</em><br>
<em>&gt; harsh negative prior for AGI project success, will still almost certainly
</em><br>
<em>&gt; fail (we'd say by destroying the world, people with anthropomorphic views
</em><br>
<em>&gt; of AI would say because the AGI revolts and rules the world itself or
</em><br>
<em>&gt; discovers objective morality and becomes nice, but still failure).
</em><br>
<p>I don't agree, but I don't see what I would gain by posting to this list a 
<br>
detailed plan for how to use human-level but highly restricted AGI to 
<br>
achieve world domination.
<br>
<p>I think I'll keep my thoughts on this subject to myself, heh heh heh ;-D
<br>
<p><em>&gt;&gt; IMO a more productive direction is think about how to design an AGI
</em><br>
<em>&gt;&gt; that will teach us a lot about AGI and Friendly AGI, but won't have
</em><br>
<em>&gt;&gt; much potential of hard takeoff.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You don't need to build a whole AGI for that. Any algorithms or dynamics
</em><br>
<em>&gt; of interest can be investigated by a limited prototype. The results of
</em><br>
<em>&gt; these experiments can be fed back into your overall model of how the
</em><br>
<em>&gt; design will perform. AGI is hard to modularise, but if your design
</em><br>
<em>&gt; requires a random-access interaction pattern over every single functional
</em><br>
<em>&gt; component before it displays recognisable behaviour then you are on a
</em><br>
<em>&gt; wild goose chase.
</em><br>
<p>This is where we disagree.  There is a lot of middle ground between 
<br>
experiments with individual system components (which we've already been 
<br>
doing for a while) and  &quot;random-access interaction pattern over every single 
<br>
functional component&quot; ....
<br>
<p>The relevant questions pertain to what the dynamics of an AGI system will be 
<br>
when put into various complex situations involving interaction with other 
<br>
minds.  If the AGI system at hand is a complex, self-organizing system, then 
<br>
these questions become extremely hard to resolve via pure mathematics.  So 
<br>
we then need either
<br>
<p>a) a design for an AGI that is not a complex, self-organizing system, but is 
<br>
more predictable and tractably mathematically modelable
<br>
b) a really powerful new mathematics of (intelligent) complex, 
<br>
self-organizing systems
<br>
<p>I don't think a is possible.
<br>
<p>I think b might be possible but we're nowhere near having it now, and based 
<br>
on my knowledge of the background of the folks involved with SIAI, I don't 
<br>
think you guys have much chance of coming up with such a thing in the 
<br>
reasonably near future.
<br>
<p>It may be that the best way to achieve b is to create a &quot;specialized AGI&quot; 
<br>
whose only domains are mathematics and scientific data analysis.  The 
<br>
question then becomes whether it's possible to create an AGI with enough 
<br>
power to help us rapidly achieve b, yet without giving this AGI enough 
<br>
autonomy and self-modifying capability to achieve a surprise hard takeoff. 
<br>
I believe that this is possible to do, for instance using Novamente.  I 
<br>
don't think it's an easy problem but I think it's a vastly easier problem 
<br>
than creating an AGI that remains Friendly through a takeoff.
<br>
<p>Thus my vision of a way forward is to create
<br>
<p>1) toddler AGI's interacting with each other in simulated environments, 
<br>
which will pose no significant hard-takeoff danger but will let us begin 
<br>
learning empirically about AGI morality in practice
<br>
<p>2) narrow-focused scientist/mathematician AGI's that can help us create the 
<br>
now-missing math of evolving intelligent systems
<br>
<p>Note that these two goals intersect, if you buy the Lakoff-Nunez argument 
<br>
that human math is based on metaphors of human physical perception and 
<br>
action.  According to this view the toddler AI's are probably a necessary 
<br>
intermediate stage before you get to the robust scientist/mathematician 
<br>
AI's.
<br>
<p>Once we have 1 and 2 we will have much better knowledge and tools for making 
<br>
the big decision, i.e. whether to launch a hard-takeoff or impose a global 
<br>
thought-police AGI-suppression apparatus....  Or conceivably another 
<br>
alternative will become apparent...
<br>
<p><em>&gt;&gt; I think this is much more promising than trying to make a powerful
</em><br>
<em>&gt;&gt; theory of Friendly AI based on a purely theoretical rathern than
</em><br>
<em>&gt;&gt; empirical approach.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Well, lets face it, experimenting is more fun, less frustrating and
</em><br>
<em>&gt; potentially money-spinning.
</em><br>
<p>In fact I prefer theorizing, on a personal level.  But I guess it's a matter 
<br>
of taste.
<br>
<p>And money doesn't interest me very much.
<br>
<p>If I weren't convinced building an AGI was highly important, I'd go back to 
<br>
being a professor, which was a quite comfortable lifestyle, and spend the 
<br>
rest of my life happily philosophizing, or working on bioinformatics with a 
<br>
view toward life extension ;-)
<br>
<p><em>&gt;&gt; The Novamente project seeks to build a benevolent, superhuman AGI
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ben, you started off trying to build an AGI with the assumption that it
</em><br>
<em>&gt; would automatically be Friendly,
</em><br>
<p>Certainly not; I read too much SF in my youth to ever have believed that ;-)
<br>
<p><em>&gt; or that at most it would take a good
</em><br>
<em>&gt; 'upbringing' to make it Friendly.
</em><br>
<p>A good upbringing in the context of the right design (which I think I have).
<br>
<p>I still believe this may be true, but I'm far from certain of it.  I've been 
<br>
convinced of the wisdom of trying to mathematically validate this intuition 
<br>
;)
<br>
<p>My views have shifted over the years, but nowhere near as drastically as 
<br>
Eli's....
<br>
<p><p><em>&gt;This required a different design
</em><br>
<em>&gt; approach, which we initially adopted with trepidation and resignation
</em><br>
<em>&gt; because formal methods had a pretty bad track record in GOFAI. As it
</em><br>
<em>&gt; turned out the problem wasn't formal methods, the problem was GOFAI
</em><br>
<em>&gt; foolishness giving them a bad name, and that design approach was
</em><br>
<em>&gt; actually far preferable even without the Friendliness constraint.
</em><br>
<p>I basically agree with this, which is why one of the key components of 
<br>
Novamente is Probabilistic Term Logic (a probabilistic formal approach to 
<br>
learning, memory, reasoning, etc...).
<br>
<p>In fact I recall strenuously trying to convince Eliezer of this point back 
<br>
in 2001 or so....  It may be that my arguments had some impact, though I'm 
<br>
sure he primarily came to the conclusion from his own direction.
<br>
<p><em>&gt;&gt; a) we think such a theory will come only from experimenting with
</em><br>
<em>&gt;&gt; appropriately constructed AGI's
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't think you can actually get such a theory from experimenting
</em><br>
<em>&gt; with AGIs unless you know exactly what you're looking for.
</em><br>
<p>I have a pretty good idea what I'm looking for, in fact.  I'm looking for 
<br>
dynamical laws governing the probabilistic grammars that emerge from 
<br>
discretizing the state space of an interactive learning system.  I have some 
<br>
hypotheses regarding what these dynamical laws should look like.  But I 
<br>
don't know how to prove these hypotheses using pure math....
<br>
<p><em>&gt; Inventing
</em><br>
<em>&gt; a theory to explain the behaviour shown in some set of simple
</em><br>
<em>&gt; experiments will probably be simultaneously easier yet result in a
</em><br>
<em>&gt; theory will a lot of cruft compared to a proper theory of the
</em><br>
<em>&gt; dynamics of causally clean goal systems. If your AGI doesn't have
</em><br>
<em>&gt; a causally clean goal system then it's pretty much a write off in
</em><br>
<em>&gt; terms of our ability to predict the results of self-modification.
</em><br>
<p>My AGI design has a causally clean goal system ---- but, I don't believe 
<br>
it's computationally feasible to have every last computation done in the AGI 
<br>
system follow directly from the goal system.  So it becomes a matter of 
<br>
having the goal system regulate some more efficient but less predictable 
<br>
self-organizing learning dynamics.  Which makes the overall behavior less 
<br>
easily predictable...
<br>
<p>The problem is that having every last thought the system takes follow from 
<br>
the system ubergoal directly, leads one to totally computationally 
<br>
infeasible designs like ITSSIM.  The efficiency workarounds seem to 
<br>
inevitably increase unpredictability.
<br>
<p>I have seen nothing remotely resembling a solution to this fundamental 
<br>
problem in any of the SIAI literature (or anywhere else).
<br>
<p><em>&gt; Virtually no-one wants to destory the world on purpose,
</em><br>
<p>Unfortunately I know a few counterexamples to that assertion ;-)
<br>
<p>yours
<br>
ben 
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11322.html">Michael Wilson: "Re: Systems engineering"</a>
<li><strong>Previous message:</strong> <a href="11320.html">Dani Eder: "Re: Systems engineering"</a>
<li><strong>In reply to:</strong> <a href="11316.html">Michael Wilson: "Re: Robot that thinks like a human"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11323.html">Michael Wilson: "Re: Robot that thinks like a human"</a>
<li><strong>Reply:</strong> <a href="11323.html">Michael Wilson: "Re: Robot that thinks like a human"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11321">[ date ]</a>
<a href="index.html#11321">[ thread ]</a>
<a href="subject.html#11321">[ subject ]</a>
<a href="author.html#11321">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
