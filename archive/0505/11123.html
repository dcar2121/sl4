<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Robot that thinks like a human</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="Re: Robot that thinks like a human">
<meta name="Date" content="2005-05-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Robot that thinks like a human</h1>
<!-- received="Thu May 19 07:24:41 2005" -->
<!-- isoreceived="20050519132441" -->
<!-- sent="Thu, 19 May 2005 14:24:38 +0100 (BST)" -->
<!-- isosent="20050519132438" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="Re: Robot that thinks like a human" -->
<!-- id="20050519132438.11568.qmail@web26704.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20Robot%20that%20thinks%20like%20a%20human"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Thu May 19 2005 - 07:24:38 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11124.html">Dani Eder: "Re: Systems engineering"</a>
<li><strong>Previous message:</strong> <a href="11122.html">Ben Goertzel: "Re: Robot that thinks like a human"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11124.html">Dani Eder: "Re: Systems engineering"</a>
<li><strong>Reply:</strong> <a href="11124.html">Dani Eder: "Re: Systems engineering"</a>
<li><strong>Reply:</strong> <a href="11128.html">Ben Goertzel: "Re: Robot that thinks like a human"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11123">[ date ]</a>
<a href="index.html#11123">[ thread ]</a>
<a href="subject.html#11123">[ subject ]</a>
<a href="author.html#11123">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; I have a great deal of doubt that it's possible for anyone to
</em><br>
<em>&gt; achieve a good understanding of AGI Friendliness prior to building
</em><br>
<em>&gt; and experimenting with some AGI's.
</em><br>
<p>You don't appear to believe that it's possible to achieve a good
<br>
understanding of how AGIs work full stop prior to actually building
<br>
them. This is the fundamental reason why I don't think any of your
<br>
designs will succeed; you're trying things that sound as if they
<br>
might work, but the design space is too large and the zone of success
<br>
too small for even educated guessing to work. And at five years or so
<br>
per 'project generation' (global architecture), human-driven
<br>
incremental trial and error isn't going to work any time soon.
<br>
<p>The SIAI approach relies on achieving complete understanding of how
<br>
the design works before building it. This means specifying global
<br>
behaviour first and breaking that down into progressively more local
<br>
behaviour until you have code. My own project has skipped ahead a bit
<br>
and is applying that approach to a limited AI domain which is
<br>
<p><em>&gt; So far none of the ideas published online by the SIAI staff have done 
</em><br>
<em>&gt; anything to assuage this doubt.
</em><br>
<p>True. We say that it looks like it is possible, you say that it looks
<br>
like it isn't possible, neither of us have published any formal
<br>
reasoning to support our position. We think you're rationalising, you
<br>
think we're indulding in wishful thinking. For now we can only keep
<br>
working towards proof that resolves the issue one way or the other.
<br>
<p><em>&gt; Sure, you can argue that it's better to spend 10-20 years trying to 
</em><br>
<em>&gt; construct theoretical foundations of Friendly AGI in the absence of
</em><br>
<em>&gt; any AGI systems to play with.  But the risk then is that in the interim
</em><br>
<em>&gt; someone else who's less conservative is going to build a nasty AI to
</em><br>
<em>&gt; ensure their own world domination.
</em><br>
<p>Frankly that's highly unlikely. Reliable world domination is of the same
<br>
structural difficultly as Friendliness; it's perhaps a little easier to
<br>
specify what you want, but no easier to get an AGI to do it. Even the
<br>
people who think that AGIs will automatically have self-centered human
<br>
like goal systems should agree with this. Anyone foolish enough to try
<br>
and take over the world using AGI, and who manages to beat the very
<br>
harsh negative prior for AGI project success, will still almost certainly
<br>
fail (we'd say by destroying the world, people with anthropomorphic views
<br>
of AI would say because the AGI revolts and rules the world itself or
<br>
discovers objective morality and becomes nice, but still failure).
<br>
<p><em>&gt; IMO a more productive direction is think about how to design an AGI
</em><br>
<em>&gt; that will teach us a lot about AGI and Friendly AGI, but won't have
</em><br>
<em>&gt; much potential of hard takeoff.
</em><br>
<p>You don't need to build a whole AGI for that. Any algorithms or dynamics
<br>
of interest can be investigated by a limited prototype. The results of
<br>
these experiments can be fed back into your overall model of how the
<br>
design will perform. AGI is hard to modularise, but if your design
<br>
requires a random-access interaction pattern over every single functional
<br>
component before it displays recognisable behaviour then you are on a
<br>
wild goose chase.
<br>
<p><em>&gt; I think this is much more promising than trying to make a powerful
</em><br>
<em>&gt; theory of Friendly AI based on a purely theoretical rathern than
</em><br>
<em>&gt; empirical approach.
</em><br>
<p>Well, lets face it, experimenting is more fun, less frustrating and
<br>
potentially money-spinning. I've previously detailed the reasons why 
<br>
experimenting with proto AGIs (particularly those lacking takeoff
<br>
protection) is a bad idea at some length, so I won't do so again now.
<br>
<p><em>&gt; The Novamente project seeks to build a benevolent, superhuman AGI
</em><br>
<p>Ben, you started off trying to build an AGI with the assumption that it
<br>
would automatically be Friendly, or that at most it would take a good
<br>
'upbringing' to make it Friendly. So did Eliezer and by extension the
<br>
SIAI. Eliezer realised some time around 2001 that Friendliness is not
<br>
automatic, it's a very specific class of behaviours which will only be
<br>
achievable and stable in a very specific class of cognitive
<br>
architectures. The SIAI essentially threw everything away and started
<br>
from scratch, because 'AGI that must be Friendly' is a very different
<br>
spec from 'self-improving AGI'. This required a different design
<br>
approach, which we initially adopted with trepidation and resignation
<br>
because formal methods had a pretty bad track record in GOFAI. As it
<br>
turned out the problem wasn't formal methods, the problem was GOFAI
<br>
foolishness giving them a bad name, and that design approach was
<br>
actually far preferable even without the Friendliness constraint.
<br>
<p>The fundamental problem with Novamente is that you didn't reboot the
<br>
project when you realised that Friendliness was both hard and
<br>
essential. You're still thinking in terms of 'we're going to build
<br>
an AGI, which we will find a way to make Friendly'. I think it may
<br>
actually take that radical statement of 'we /must/ find a way to
<br>
reliably predict how the AGI will behave before building it' to force
<br>
people to abandon probabilistic, emergentist and other methods that
<br>
are essentially guesswork.
<br>
<p><em>&gt; (I'm not using the word Friendly because in fact I'm not entirely
</em><br>
<em>&gt; sure what Eli means by that defined term these days).
</em><br>
<p>Regardless of whether you agree with Eliezer's rather contraversial
<br>
ideas about Friendliness content, the problem of maintaining stable
<br>
optimisation targets, or more generally how to translate desired
<br>
behaviour into decision functions that are stable under reflection,
<br>
is one all designs that claim to be Friendly must solve.
<br>
<p><em>&gt; We are committed not to create an AGI that appears likely capable
</em><br>
<em>&gt; of hard takeoff unless
</em><br>
<p>Note use of the word 'appears'. Without a predictive model of the
<br>
system's dynamics you are making a personal intuitive judgement, with
<br>
no representative experience to calibrate against and under strong
<br>
pressure to believe that you don't need to delay your schedule and
<br>
that you're not an existential risk. 'Appears' is utterly unreliable.
<br>
I don't think your AGI design has significent takeoff risk either,
<br>
but I still think you should implement every practical safety
<br>
mechanism (though again without the predictive model, you can't be
<br>
sure how effective they will be). As Eliezer says, if nothing else
<br>
it will get you in the habit.
<br>
<p><em>&gt; it seems highly clear that this AGI will be benevolent.
</em><br>
<p>I could dissect that 'seems' as well, but that would be beating a
<br>
dead horse. It seems harsh to criticise you so much Ben when you're
<br>
way ahead of almost all of your contemporaries in realising that
<br>
Friendliness is important and difficult, but unfortunately you're
<br>
still only a small fraction of the way towards the elements needed
<br>
for a realistic chance of success.
<br>
<p><em>&gt; We are not committed to avoid building *any* AGI until we have a 
</em><br>
<em>&gt; comprehensive theory of Friendliness/benevolence, because
</em><br>
<em>&gt; 
</em><br>
<em>&gt; a) we think such a theory will come only from experimenting with 
</em><br>
<em>&gt; appropriately constructed AGI's
</em><br>
<p>I don't think you can actually get such a theory from experimenting
<br>
with AGIs unless you know exactly what you're looking for. Inventing
<br>
a theory to explain the behaviour shown in some set of simple
<br>
experiments will probably be simultaneously easier yet result in a
<br>
theory will a lot of cruft compared to a proper theory of the
<br>
dynamics of causally clean goal systems. If your AGI doesn't have
<br>
a causally clean goal system then it's pretty much a write off in
<br>
terms of our ability to predict the results of self-modification.
<br>
<p><em>&gt; So anyway, it is just not true that the SIAI is the only group
</em><br>
<em>&gt; seeking to build a demonstrably/arguably benevolent AGI system.
</em><br>
<p>True, but that's not what I said;
<br>
<p><em>&gt;&gt; The only realistic way for humanity to win is for the AGI race
</em><br>
<em>&gt;&gt; to be won by a project that explicitly sets out to build an
</em><br>
<em>&gt;&gt; AGI that can be /proven/ to be Friendly (to a high degree of
</em><br>
<em>&gt;&gt; confidence, prior to actually building it).
</em><br>
<p>Predicting system behaviour via formal probabilistic reasoning from
<br>
the spec is /not/ the same as being able to 'argue' that it will
<br>
behave in certain ways, or simply claiming that your demo version's
<br>
behaviour is bound to remain stable under self-modification.
<br>
<p>Virtually no-one wants to destory the world on purpose, and most AI
<br>
researchers want to make the world a better place. The problem
<br>
isn't one of desires, but of vision (to see the scope of possible
<br>
consequences) and technique.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
How much free photo storage do you get? Store your holiday 
<br>
snaps for FREE with Yahoo! Photos <a href="http://uk.photos.yahoo.com">http://uk.photos.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11124.html">Dani Eder: "Re: Systems engineering"</a>
<li><strong>Previous message:</strong> <a href="11122.html">Ben Goertzel: "Re: Robot that thinks like a human"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11124.html">Dani Eder: "Re: Systems engineering"</a>
<li><strong>Reply:</strong> <a href="11124.html">Dani Eder: "Re: Systems engineering"</a>
<li><strong>Reply:</strong> <a href="11128.html">Ben Goertzel: "Re: Robot that thinks like a human"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11123">[ date ]</a>
<a href="index.html#11123">[ thread ]</a>
<a href="subject.html#11123">[ subject ]</a>
<a href="author.html#11123">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:22:56 MST
</em></small></p>
</body>
</html>
