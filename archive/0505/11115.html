<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Domain Protection</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Domain Protection">
<meta name="Date" content="2005-05-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Domain Protection</h1>
<!-- received="Wed May 18 14:49:21 2005" -->
<!-- isoreceived="20050518204921" -->
<!-- sent="Wed, 18 May 2005 13:51:21 -0700" -->
<!-- isosent="20050518205121" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Domain Protection" -->
<!-- id="428BAAC9.4000400@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="427F8D27.4050701@gmx.de" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Domain%20Protection"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed May 18 2005 - 14:51:21 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11116.html">Michael Wilson: "RE: Robot that thinks like a human"</a>
<li><strong>Previous message:</strong> <a href="11114.html">Stuart, Ian: "RE: Robot that thinks like a human"</a>
<li><strong>In reply to:</strong> <a href="11081.html">Sebastian Hagen: "Re: Domain Protection"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11118.html">Russell Wallace: "Re: Domain Protection"</a>
<li><strong>Reply:</strong> <a href="11118.html">Russell Wallace: "Re: Domain Protection"</a>
<li><strong>Reply:</strong> <a href="11119.html">Damien Broderick: "Eliezer vs. Jaron Lanier"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11115">[ date ]</a>
<a href="index.html#11115">[ thread ]</a>
<a href="subject.html#11115">[ subject ]</a>
<a href="author.html#11115">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Sebastian Hagen wrote:
<br>
<em>&gt; Russell Wallace wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;I'm assuming the Sysop will _not_ know what constitutes a sentient
</em><br>
<em>&gt;&gt;being, and we won't be able to formally define it either. This is the
</em><br>
<em>&gt;&gt;big difference between domain protection and both of Eliezer's sysop
</em><br>
<em>&gt;&gt;scenarios; I'm making more conservative assumptions about what will be
</em><br>
<em>&gt;&gt;possible, and being more modest in the problems I try to solve.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;For purposes of setting up the domains, the rule can be simple: each
</em><br>
<em>&gt;&gt;and every human on Earth (the ones who are old enough to make a
</em><br>
<em>&gt;&gt;choice, at least) gets to decide what domain they want to move to (or
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Who decides what &quot;old enough&quot; means? And why is biological age the
</em><br>
<em>&gt; critical metric, as opposed to, for example, a certain measure of
</em><br>
<em>&gt; intelligence?
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;stay on Earth, of course); that's an operationally adequate definition
</em><br>
<em>&gt;&gt;of &quot;sentient&quot; for that purpose.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Even assuming it was, there would still be a lot of other important
</em><br>
<em>&gt; decisions to be made; for example:
</em><br>
<em>&gt; What domains with what rules are actually created? How are the available
</em><br>
<em>&gt; ressources divided between them? How is interdomain communication
</em><br>
<em>&gt; regulated? Do 'sentient beings' only get to choose their domain once,
</em><br>
<em>&gt; are they free to move between them at any time, or are parameters of the
</em><br>
<em>&gt; individual domains responsible for deciding who is allowed to enter or
</em><br>
<em>&gt; leave?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If you allow unupgraded humans to make final decisions about these
</em><br>
<em>&gt; matters, suboptimal to catastrophic results are likely.
</em><br>
<em>&gt; Keeping all of these settings adjustable into the indefinite future, on
</em><br>
<em>&gt; the other hand, would provide possibilities for attacking individual
</em><br>
<em>&gt; domains.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Do you have a better method of determining these parameters?
</em><br>
<p>Sebastian Hagen has pinpointed the foundational flaw with the Domain 
<br>
Protection proposal - the missed point, even.
<br>
<p>Each and every question Sebastian asked is an additional decision that you 
<br>
would allocate to the programmers.  It is hence an additional point for the 
<br>
programmers to potentially get wrong.  Furthermore you propose no mechanism 
<br>
for fixing things you get wrong.  There are two things missing here that are 
<br>
the core motivations behind Collective Extrapolated Volition: limited initial 
<br>
complexity and self-healing.  Although there are decisions that must be made 
<br>
to define an initial dynamic of extrapolated volition, there's a bounded 
<br>
number of such decisions and they tend to center around very fundamental 
<br>
questions.  Once you finish defining only this one thing of collective 
<br>
extrapolated volition, you need not define the ground rules for an entire 
<br>
sociopolitical system, as you are attempting to do.  Defining a 
<br>
socialpolitical system is a tempting failure because it is so much endless 
<br>
fun.  Every time you have a little more fun, you raise a few more questions to 
<br>
potentially get wrong.  The age of majority... laws of transit... minimum 
<br>
group size to define an environment... communication between domains... 
<br>
debts... bankruptcy... division of resources...
<br>
<p>You are not being more conservative about what will be possible.  You are not 
<br>
being more conservative about what you are trying to solve.  I very carefully 
<br>
limited myself to trying to solve only one problem.  It was a big problem, but 
<br>
there was still only one of it, and it wasn't a fun political issue such as 
<br>
humans love to get stuck in and underestimate the complexity of.
<br>
<p>The second problem is that if you make up all your own rules for a 
<br>
sociopolitical system - or pardon me, a framework for creating sociopolitical 
<br>
domains and regulating interactions between them - then, even if you succeed 
<br>
in creating a system that does what you told it to do, it is fixed in place. 
<br>
If the consequences are not what you imagined, it is still fixed in place.  It 
<br>
does not self-heal.  It does not self-correct.  You have to get every single 
<br>
thing exactly right on the first try because there is no mechanism to fix even 
<br>
a single thing wrong.  The Collective Extrapolated Volition proposal takes the 
<br>
form it does because, if I succeed at CEV, it isn't a random sociopolitical 
<br>
system, it's a mechanism for correcting errors including errors I made in the 
<br>
initial specification of CEV, providing that the initial idea is not *perfect* 
<br>
but rather *right enough*.  I won't call it error-tolerant, but there would at 
<br>
least be hope.
<br>
<p>You say:  &quot;In any case, consider the arrows in state space: because you're 
<br>
following a single volition, they all point to a single region of state space. 
<br>
Again loss of diversity, and horrendous danger - all we can say about the 
<br>
nature of that single region is that we don't know anything about it.&quot;
<br>
<p>A CEV can implement a multi-domain region if that's what people want.  The 
<br>
Domain Protection system runs through a single volition, your own, pointing to 
<br>
the single framework for the system; and that volition is considerably weaker 
<br>
than a CEV.  You're trying to achieve results, such as &quot;protection of 
<br>
diversity&quot;, by taking specific complex actions, such as a domain framework, 
<br>
that you think will have the consequence of protecting diversity.   But your 
<br>
volition is too weak to firmly state the expected consequences of your actions 
<br>
- even in this our human world, let alone the full spread of possibilities for 
<br>
the next million years.
<br>
<p>***
<br>
<p>Let's review the Domain Protection according to the seven motivations listed 
<br>
in the original &quot;Collective Volition&quot; page.  You may wish to refer to the 
<br>
original discussion:
<br>
<p><a href="http://www.intelligence.org/friendly/collective-volition.html#motivations">http://www.intelligence.org/friendly/collective-volition.html#motivations</a>
<br>
<p>1. Defend humans, the future of humankind, and humane nature.
<br>
<p>* I don't see anything in the DP framework that covers any of those three 
<br>
things, except possibly &quot;defend humans&quot; - but even that would depend on the 
<br>
specific domains created, or the mechanism for creating domains.  If all 
<br>
domains are wildernesses in which death is permanent, that is not &quot;defend 
<br>
humans&quot;.  Since you didn't specify the exact mechanism which determines which 
<br>
domains are created, I don't know what kind of domains will be created.  Since 
<br>
I don't know what kind of domains will be created, I don't know what will 
<br>
happen to the people in them.  As for defending the 'future of humankind' 
<br>
(which in CEV is dealt with by trying to extrapolate the spectrum of probable 
<br>
futures and our reactions to them), it's not clear how any progress at all is 
<br>
made on using AI superintelligence to guard against dangers we did not know; 
<br>
you propose a fixed unalterable structure that you think might deal with a 
<br>
small set of dangers to humanity that you foresee, such as 'loss of 
<br>
diversity'.  With &quot;humane nature&quot; the proposal contains nothing that would 
<br>
tend to solve the chicken-and-egg problem of humans who are not wise enough to 
<br>
upgrade themselves upgrading themselves to where they are wise enough to 
<br>
upgrade themselves.  CEV is an attempt to get the best definition we can of 
<br>
that-which-we-wish-to-preserve-through-the-transition that isn't limited to 
<br>
our present-day abilities to define it, and returns deliberately spread-out 
<br>
answers in the case our possible decisions aren't strongly concentrated.
<br>
<p>2.  Encapsulate Moral Growth
<br>
<p>* Domain Protection doesn't do this at all.  Period.  You say &quot;AI smarter than 
<br>
human is on much firmer theoretical ground than AI wiser than human&quot;, and in 
<br>
one sense this is true, because today I have an understanding verging on the 
<br>
truly technical for &quot;smarter than human&quot;, whereas my apprehension of &quot;wiser 
<br>
than human&quot; is roughly where my apprehension of &quot;smarter than human&quot; was in 
<br>
2000 or thereabouts.  But I didn't just poof into technical understanding; I 
<br>
got there by steadily gnawing away on a vague understanding.  And, no offense, 
<br>
but unless it's what you plan to do with your whole life, I doubt that you 
<br>
have any firm theoretical ground with which to describe &quot;AI smarter than 
<br>
human&quot;.  And then you go on to confidently declare, &quot;Smarter yes, wiser and 
<br>
more benevolent no; it's a Godel problem, you can't do better than your 
<br>
axioms.&quot;  Wow!  Where'd you get the firm theoretical ground from to 
<br>
confidently declare this problem unsolvable?  Didn't you just get through 
<br>
saying you didn't have one?  A True AI Researcher is someone who can take 
<br>
ill-defined comparisons like &quot;humans are smarter than chimps&quot; or &quot;Einstein was 
<br>
smarter than a village idiot&quot; or &quot;Leo Szilard was more altruistic than Adolf 
<br>
Hitler&quot; and cash out the ill-defined terms like &quot;smarter&quot; or &quot;wiser and more 
<br>
benevolent&quot; into solid predicates and creatable technology.  Lots of people 
<br>
will tell you that smartness or even intelligence is forever and eternally 
<br>
undefinable, and that Artificial Intelligence is therefore unworkable.  In 
<br>
fact I just recently had to argue about that with Jaron Lanier.
<br>
<p>3.  Humankind should not spend the rest of eternity desperately wishing that 
<br>
the programmers had done something differently.  &quot;This seems obvious, until 
<br>
you realize that only the Singularity Institute has even tried to address this 
<br>
issue. I haven't seen a single other proposal for AI morality out there, not 
<br>
even a casual guess, that takes the possibility of Singularity Regret into 
<br>
account. Not one. Everyone has their brilliant idea for the Four Great Moral 
<br>
Principles That Are All We Need To Program Into AIs, and not one says, &quot;But 
<br>
wait, what if I got the Four Great Moral Principles wrong?&quot;  They don't think 
<br>
of writing any escape clause, any emergency exit if the programmers made the 
<br>
wrong decision. They don't wonder if the original programmers of the AI might 
<br>
not be the wisest members of the human species; or if even the wisest 
<br>
human-level minds might flunk the test; or if humankind might outgrow the 
<br>
programmers' brilliantly insightful moral philosophy a few million years hence.&quot;
<br>
<p>* I didn't see *anywhere* in your Domain Protection page where you talked 
<br>
about a revocation mechanism and a framework for controlled transition to 
<br>
something else.  Don't tell me it was a minor oversight.
<br>
<p>4. Avoid hijacking the destiny of humankind.
<br>
<p>* You aren't proposing to define the domains yourself (as far as I can tell), 
<br>
so you aren't being a jerk.  You could end up destroying the destiny of 
<br>
humankind but you don't appear to be trying to hijack it per se.
<br>
<p>5. Avoid creating a motive for modern-day humans to fight over the initial 
<br>
dynamic.
<br>
<p>* Along comes an AI project that wants to define the minimum age to move 
<br>
freely between domains as 13, instead of 18, in according with the laws of the 
<br>
Torah.  How do the two of you settle your differences?  Would you advise the 
<br>
al-Qaeda programmers to make as many decisions (irrevocable decisions!) as you 
<br>
wish to allocate to yourself?
<br>
<p>6. Keep humankind ultimately in charge of its own destiny.
<br>
<p>* All descendants of humankind are subject to your initial choices in defining 
<br>
the exact form of the domain interaction framework, even a million years 
<br>
hence.  However you do not propose to create a god.  Mixed score.
<br>
<p>7. Help people.
<br>
<p>* DP suggests no Last Judge or equivalent, and no stated dependency on 
<br>
people's actual reactions or extrapolated judgments.  There's no mechanism for 
<br>
revoking DP if its consequences turn out to be horrible.
<br>
<p>***
<br>
<p>If you'd proposed a revocation mechanism, it would have shown more moral 
<br>
caution than any other non-SIAI AI morality proposal up until this point.  But 
<br>
it still wouldn't be enough, not nearly.
<br>
<p>...I wish people understood just how non-arbitrary the CEV proposal is.  If 
<br>
you understand all the motivations here that must needs be satisfied, you will 
<br>
see how really difficult it is to come up with anything that works half as 
<br>
well, let alone better.
<br>
<p>Think meta, and keep thinking.  Let me worry about what's technically 
<br>
impossible.  Try to say what you care about, what you want as the consequence, 
<br>
not what means you think will achieve the consequence.  Both judgments are 
<br>
fallible, but the latter is considerably more fallible.
<br>
<p>Sincerely,
<br>
Eliezer.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11116.html">Michael Wilson: "RE: Robot that thinks like a human"</a>
<li><strong>Previous message:</strong> <a href="11114.html">Stuart, Ian: "RE: Robot that thinks like a human"</a>
<li><strong>In reply to:</strong> <a href="11081.html">Sebastian Hagen: "Re: Domain Protection"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11118.html">Russell Wallace: "Re: Domain Protection"</a>
<li><strong>Reply:</strong> <a href="11118.html">Russell Wallace: "Re: Domain Protection"</a>
<li><strong>Reply:</strong> <a href="11119.html">Damien Broderick: "Eliezer vs. Jaron Lanier"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11115">[ date ]</a>
<a href="index.html#11115">[ thread ]</a>
<a href="subject.html#11115">[ subject ]</a>
<a href="author.html#11115">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:22:56 MST
</em></small></p>
</body>
</html>
