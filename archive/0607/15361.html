<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: strong and weakly self improving processes</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: strong and weakly self improving processes">
<meta name="Date" content="2006-07-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: strong and weakly self improving processes</h1>
<!-- received="Sat Jul 15 15:54:10 2006" -->
<!-- isoreceived="20060715215410" -->
<!-- sent="Sat, 15 Jul 2006 14:52:50 -0700" -->
<!-- isosent="20060715215250" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: strong and weakly self improving processes" -->
<!-- id="44B963B2.9060600@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="17592.4103.411053.280199@pc-eric.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20strong%20and%20weakly%20self%20improving%20processes"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Jul 15 2006 - 15:52:50 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15362.html">Damien Broderick: "intelligence explosion"</a>
<li><strong>Previous message:</strong> <a href="15360.html">Ola Bini: "Re: programming"</a>
<li><strong>Maybe in reply to:</strong> <a href="15358.html">Eliezer S. Yudkowsky: "Re: strong and weakly self improving processes"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15362.html">Damien Broderick: "intelligence explosion"</a>
<li><strong>Reply:</strong> <a href="15362.html">Damien Broderick: "intelligence explosion"</a>
<li><strong>Reply:</strong> <a href="15363.html">Eliezer S. Yudkowsky: "Re: strong and weakly self improving processes"</a>
<li><strong>Reply:</strong> <a href="15366.html">Damien Broderick: "Re: intelligence runaway (was explosion)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15361">[ date ]</a>
<a href="index.html#15361">[ thread ]</a>
<a href="subject.html#15361">[ subject ]</a>
<a href="author.html#15361">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eric Baum wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer&gt; It should be emphasized that I wrote LOGI in 2002; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Didn't know that. Are the rest of the papers in that 2005 book as old?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer&gt; Nonetheless, calling something &quot;complex&quot; doesn't explain it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Methinks you protest too much, although I take the point.
</em><br>
<p>You may be right.  Still, better to protest too much than too little.
<br>
<p><em>&gt; But I did 
</em><br>
<em>&gt; like the presentation-- you didn't just say it was complex, you
</em><br>
<em>&gt; pointed out it was layered, which some in the AI community had failed
</em><br>
<em>&gt; to adequately credit (cf your critique of semantic nets).
</em><br>
<p>Oh, I'm willing enough to say that *human* intelligence is complex, 
<br>
because I have a specific image of human intelligence as including a 
<br>
hugely subdivided cerebral cortex, layers of organization, multiple 
<br>
centers of gravity, many individually evolved instincts and intuitions 
<br>
in conflict, et cetera.
<br>
<p>Saying that *intelligence* is complex is a whole different story.
<br>
<p><em>&gt; Eliezer&gt; A giant lookup table is a simple process that may know an
</em><br>
<em>&gt; Eliezer&gt; arbitrarily large amount, depending on the incompressibility
</em><br>
<em>&gt; Eliezer&gt; of the lookup table.  A human programmer turned loose on the
</em><br>
<em>&gt; Eliezer&gt; purely abstract form of a simple problem (e.g. stacking
</em><br>
<em>&gt; Eliezer&gt; towers of blocks), who invents a purely abstract algorithm
</em><br>
<em>&gt; Eliezer&gt; (e.g. mergesort) without knowing anything about which
</em><br>
<em>&gt; Eliezer&gt; specific blocks need to be moved, is an example of a complex
</em><br>
<em>&gt; Eliezer&gt; process that used very little specific knowledge about that
</em><br>
<em>&gt; Eliezer&gt; specific problem to come up with a good general solution.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I respectfully suggest that the human programmer couldn't do that
</em><br>
<em>&gt; unless he knew a lot, in fact unless he had most of the program
</em><br>
<em>&gt; (in chunks, not exactly assembled, capable of being assembled in
</em><br>
<em>&gt; different ways to solve different problems) already in his head before
</em><br>
<em>&gt; attacking the problem.
</em><br>
<p>But that is not knowledge *specifically* about the blocks problem.  It 
<br>
is not like having a giant lookup table in your head that says how to 
<br>
solve all possible blocks problems up to 10 blocks.  The existence of 
<br>
&quot;knowledge&quot; that is very generally applicable, far beyond the domains 
<br>
over which it was generalized, is what makes general intelligence 
<br>
possible.  It is why the problem is not NP-hard.
<br>
<p><em>&gt; Even an untrained human couldn't do it, and an untrained human
</em><br>
<em>&gt; is 10^44 creatures worth of evolution away from a tabula rasa.
</em><br>
<em> &gt;
</em><br>
<em>&gt; Eliezer&gt; Is the term &quot;top level&quot; really all that useful for describing
</em><br>
<em>&gt; Eliezer&gt; evolutionary designs?  The human brain has more than one
</em><br>
<em>&gt; Eliezer&gt; center of gravity.  The limbic system, the ancient goal
</em><br>
<em>&gt; Eliezer&gt; system at the center, is a center of gravity; everything grew
</em><br>
<em>&gt; Eliezer&gt; up around it.  The prefrontal cortex, home of reflection and
</em><br>
<em>&gt; Eliezer&gt; the self-model, is a center of gravity.  The cerebellum,
</em><br>
<em>&gt; Eliezer&gt; which learns the realtime skill of &quot;thinking&quot; and projects
</em><br>
<em>&gt; Eliezer&gt; massively to the cortex, is a center of gravity.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What do you mean by center of gravity?
</em><br>
<p>About the same thing you mean by &quot;top module&quot;?  An axis around which 
<br>
cognition turns?  A major command-and-control outpost?  I'm not sure 
<br>
that I have a better definition than the intuitive sound of the words 
<br>
plus the examples given.
<br>
<p>The center of a self-modifying AI would be none of these things; it 
<br>
would be the criterion against which self-written code is checked.
<br>
<p><em>&gt; I talked about levels in part because it was the subject of
</em><br>
<em>&gt; your paper :^) , and because the comment I was discussing seemed to 
</em><br>
<em>&gt; have a two level nature (the human and the culture);
</em><br>
<em>&gt; but I do tend to think that big hierarchic programs tend to
</em><br>
<em>&gt; have top modules to a (potentially somewhat fuzzy) extent, 
</em><br>
<em>&gt; and I tend to think of information as
</em><br>
<em>&gt; being filtered and processed up to a point where decisions are made,
</em><br>
<em>&gt; and the brain certainly has a somewhat layered structure.
</em><br>
<p>I have my skepticism about the proper design for an AI being a big 
<br>
hierarchic program.  Or a lot of little agents.  Or a lot of little 
<br>
agents controlled by an incorruptible central broker.  My current 
<br>
thinking tends to turn around stages of processing - *not* necessarily 
<br>
layers of organization as in the human idiom.  Does information have to 
<br>
be filtered *up* to the decision-making level?  Or is making the 
<br>
decision just one more *stage* of processing?  Ultimately, a mind is the 
<br>
cognition that happens between sense input and motor output.  If we 
<br>
write down the stages of an AI, and find a natural mountain - beginning 
<br>
with complex sense information, being processed toward a peak of 
<br>
simplicity and a direct decision, then increasingly complex translation 
<br>
toward motor stages - then we might call the peak of simplicity the &quot;top&quot;.
<br>
<p>But it doesn't follow that the optimal stages between sense and motor 
<br>
*must* obey any such neat progression.  Maybe, when you design the 
<br>
system properly - as opposed to blindly accreting it by natural 
<br>
selection - some stages are more complicated and some are less 
<br>
complicated, and there's no natural top.
<br>
<p>*Within humans*, the evolutionary idiom of levels of organization, and 
<br>
the actual design of the architecture, are such that we can speak 
<br>
comprehensibly of humans having a &quot;top&quot; level.  In fact, I can think of 
<br>
at least three of them: the limbic system, the prefrontal cortex, and 
<br>
the cerebellum.
<br>
<p><em>&gt; Eliezer&gt;  From my perspective, this argument over &quot;top levels&quot; doesn't
</em><br>
<em>&gt; Eliezer&gt; have much to do with the question of recursive
</em><br>
<em>&gt; Eliezer&gt; self-improvement!  It's the agent's entire intelligence that
</em><br>
<em>&gt; Eliezer&gt; may be turned to improving itself.  Whether the greatest
</em><br>
<em>&gt; Eliezer&gt; amount of heavy lifting happens at a &quot;top level&quot;, or lower
</em><br>
<em>&gt; Eliezer&gt; levels, or systems that don't modularize into levels of
</em><br>
<em>&gt; Eliezer&gt; organization; and whether the work done improves upon the
</em><br>
<em>&gt; Eliezer&gt; AI's top layers or lower layers; doesn't seem to me to
</em><br>
<em>&gt; Eliezer&gt; impinge much upon the general thrust of I.  J. Good's
</em><br>
<em>&gt; Eliezer&gt; &quot;intelligence explosion&quot; concept.  &quot;The AI improves itself.&quot;
</em><br>
<em>&gt; Eliezer&gt; Why does this stop being an interesting idea if you further
</em><br>
<em>&gt; Eliezer&gt; specify that the AI is structured into levels of organization
</em><br>
<em>&gt; Eliezer&gt; with a simple level describable as &quot;top&quot;?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; As I said:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;even if there would be some way to keep modifying the top level 
</em><br>
<em>&gt;&gt;to make it better, one could presumably achieve just as powerful an 
</em><br>
<em>&gt;&gt;ultimate intelligence by keeping it fixed and adding more powerful 
</em><br>
<em>&gt;&gt;lower levels (or maybe better yet, middle levels) or more or better 
</em><br>
<em>&gt;&gt;chunks and modules within a middle or lower level.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You had posed a 2 level system: humans and culture,
</em><br>
<em>&gt; and said this was different from a seed AI, because the humans modify
</em><br>
<em>&gt; the culture, and that's not as powerful as the whole AI modifying
</em><br>
<em>&gt; itself.
</em><br>
<p>Okay.  I would still defend that.  Not sure how the internal structure 
<br>
of the AI directly relates to the above issue.
<br>
<p><em>&gt; But what I'm arguing is that there is no such distinction,
</em><br>
<em>&gt; the humans modifying the culture really does modify the humans in
</em><br>
<em>&gt; a potentially arbitrarily powerful way;
</em><br>
<em>&gt; within most AI's I can conceive, there will in any case be some
</em><br>
<em>&gt; fixed top level, even within an AIXI or in
</em><br>
<em>&gt; Schmidhuber's OOPS or whatever to the extent I understand them,
</em><br>
<p>AIXI has an unalterably fixed top level; it cannot conceive of the 
<br>
possibility of modifying itself.
<br>
<p>Schmidhuber's OOPS, if I recall correctly, supposedly has no invariants 
<br>
at all.  If you can prove the new code has &quot;greater expected utility&quot;, 
<br>
according to the current utility function (even if the new code includes 
<br>
changes to the utility function), and taking into account all changes 
<br>
that will be adopted by the new code, the new code gets adopted.  But 
<br>
Schmidhuber is very vague about exactly how this proof takes place.
<br>
<p>My own thinking tends to the idea of a preserved optimization target, 
<br>
preserved preferences over outcomes, rather than protected bits in memory.
<br>
<p><em>&gt; yet this doesn't preclude these things from powerful self modification,
</em><br>
<em>&gt; having a 2 level system where the top level can't modify its very
</em><br>
<em>&gt; top level (eg the humans can't modify their genome-- positing for the 
</em><br>
<em>&gt; sake of argument they don't and we only talk about progress that's
</em><br>
<em>&gt; occurred to date) does not make it weakly self-improving in some sense
</em><br>
<em>&gt; that bars it from gaining as much power as a &quot;strongly self-improving&quot;
</em><br>
<em>&gt; alternative.
</em><br>
<p>It is written in the _Twelve Virtues of Rationality_ that the sixth 
<br>
virtue is empiricism:  &quot;Do not ask which beliefs to profess, but which 
<br>
experiences to anticipate.  Always know which difference of experience 
<br>
you argue about.&quot;
<br>
<p>So let's see if we can figure out where we anticipate differently, and 
<br>
organize the conversation around that.
<br>
<p>The main experience I anticipate may be described intuitively as &quot;AI go 
<br>
FOOM&quot;.  Past some threshold point - definitely not much above human 
<br>
intelligence, and probably substantially below it - a self-modifying AI 
<br>
undergoes an enormously rapid accession of optimization power (unless 
<br>
the AI has been specifically constructed so as to prefer an ascent which 
<br>
is slower than the maximum potential speed).  This is a testable 
<br>
prediction, though its consequences render it significant beyond the 
<br>
usual clash of scientific theories.
<br>
<p>The basic concept is not original with me and is usually attributed to a 
<br>
paper by I. J. Good in 1965, &quot;Speculations Concerning the First 
<br>
Ultraintelligent Machine&quot;.  (Pp. 31-88 in Advances in Computers, vol 6, 
<br>
eds. F. L. Alt and M. Rubinoff.  New York: Academic Press.)  Good 
<br>
labeled this an &quot;intelligence explosion&quot;.  I have recently been trying 
<br>
to consistently use the term &quot;intelligence explosion&quot; rather than 
<br>
&quot;Singularity&quot; because the latter term has just been abused too much.
<br>
<p>Now there are many different imaginable ways that an intelligence 
<br>
explosion could occur.  As a physicist, you are probably familiar with 
<br>
the history of the first nuclear pile, which achieved criticality on 
<br>
December 2nd, 1942.  Szilard, Fermi, and friends built the first nuclear 
<br>
pile, in the open air of a squash court beneath Stagg Field at the 
<br>
University of Chicago, by stacking up alternating layers of uranium 
<br>
bricks and graphite bricks.  The nuclear pile didn't exhibit its 
<br>
qualitative behavior change as a result of any qualitative change in the 
<br>
behavior of the underlying atoms and neutrons, nor as a result of the 
<br>
builders suddenly piling on a huge number of bricks.  As the pile 
<br>
increased in size, there was a corresponding quantitative change in the 
<br>
effective neutron multiplication factor (k), which rose slowly toward 1. 
<br>
&nbsp;&nbsp;The actual first fission chain reaction had k of 1.0006 and ran in a 
<br>
delayed critical regime.
<br>
<p>If Fermi et. al. had not possessed the ability to quantitatively 
<br>
calculate the behavior of this phenomenon in advance, but instead had 
<br>
just piled on the bricks hoping for something interesting to happen, it 
<br>
would not have been a good year to attend the University of Chicago.
<br>
<p>We can imagine an analogous cause of an intelligence explosion in which 
<br>
the key parameter is not the qualitative ability to self-modify, but a 
<br>
critical value for a smoothly changing quantitative parameter which 
<br>
measures how many additional self-improvements are triggered by an 
<br>
average self-improvement.
<br>
<p>But this isn't the only potential cause of behavior that empirically 
<br>
looks like &quot;AI go FOOM&quot;.  The species Homo sapiens showed a sharp jump 
<br>
in the effectiveness of intelligence, as the result of natural selection 
<br>
exerting a more-or-less steady optimization pressure on hominids for 
<br>
millions of years, gradually expanding the brain and prefrontal cortex, 
<br>
tweaking the software architecture.  A few tens of thousands of years 
<br>
ago, hominid intelligence crossed some key threshold and made a huge 
<br>
leap in real-world effectiveness; we went from caves to skyscrapers in 
<br>
the blink of an evolutionary eye.  This happened with a continuous 
<br>
underlying selection pressure - there wasn't a huge jump in the 
<br>
optimization power of evolution when humans came along.  The underlying 
<br>
brain architecture was also continuous - our cranial capacity didn't 
<br>
suddenly increase by two orders of magnitude.  So it might be that, even 
<br>
if the AI is being elaborated from outside by human programmers, the 
<br>
curve for effective intelligence will jump sharply.  It's certainly 
<br>
plausible that *the* key threshold was culture, but because we wiped out 
<br>
all our nearest relatives, it's hard to disentangle exactly which 
<br>
improvements to human cognition were responsible for what.
<br>
<p>Or perhaps someone builds an AI prototype that shows some promising 
<br>
results, and the demo attracts another $100 million in venture capital, 
<br>
and this money purchases a thousand times as much supercomputing power. 
<br>
&nbsp;&nbsp;I doubt a thousandfold increase in hardware would purchase anything 
<br>
like a thousandfold increase in effective intelligence - but mere doubt 
<br>
is not reliable in the absence of any ability to perform an analytical 
<br>
calculation.  Compared to chimps, humans have a threefold advantage in 
<br>
brain and a sixfold advantage in prefrontal cortex, which suggests (a) 
<br>
firmware is more important than hardware and (b) small increases in 
<br>
hardware can support large improvements in firmware.
<br>
<p>Humans, thinking, certainly cause changes to their neurons; and it may 
<br>
even be possible that with a theoretically perfect series of 
<br>
instructions to our introspective levers, we could reprogram the 
<br>
firmware into whatever we liked.  Just as it's theoretically possible 
<br>
that the genome could contain a series of DNA instructions which built 
<br>
something that built something that built diamondoid nanotechnology and 
<br>
placed it under the control of our high-level decision process, thus 
<br>
obviating all discussion of protected levels.  But the genome *doesn't* 
<br>
contain those instructions, and naive humans don't even know the visual 
<br>
cortex exists, let alone have the power to reprogram it, and this is not 
<br>
coincidence.  In theory, a sub-critical nuclear pile could have every 
<br>
single emitted neutron just happen to strike another nucleus, and so 
<br>
explode; but it's not very *probable*.
<br>
<p>There is a level at which an AI is doing exactly the same thing as a 
<br>
human, who in turn is doing exactly the same thing as a chimp, who is 
<br>
doing exactly the same thing as a bacterium, who is doing exactly the 
<br>
same thing as a rock.  This level is called physics.  There'll be some 
<br>
level on which the behavior of the system is smoothly continuous with 
<br>
all its past history, changing neither qualitatively nor quantitatively.
<br>
<p>I do not insist that an AI reaching down to its hardware and firmware 
<br>
levels must change *everything*.  It doesn't have to violate the laws of 
<br>
physics.  The important point of debate is not that the AI is 
<br>
&quot;different&quot; in some sense of how we describe it; the question is 
<br>
observed behavior.  If the pragmatic result of an AI being able to 
<br>
modify and improve its own hardware and firmware is that the AI 
<br>
increases its effective self-improvement multiplication factor past 1 - 
<br>
metaphorically speaking - and goes &quot;critical&quot;, then that's the important 
<br>
thing from my perspective.  Or, if humans have already achieved cultural 
<br>
criticality, but the AI goes prompt critical (metaphorically speaking) 
<br>
and ascends at rates far faster than human culture, then again I regard 
<br>
that as the important empirical consequence.
<br>
<p>I don't think there should be a question that being able to improve your 
<br>
hardware (possibly by millionfold or greater factors) and rewrite your 
<br>
firmware should provide *some* benefit.  *How much* benefit is the issue 
<br>
here.  Whether the change I'm describing is &quot;qualitatively different&quot; is 
<br>
a proxy question, which may turn on matters of mere definition; the key 
<br>
issue is what we observe in real life.
<br>
<p>Now, if you said that humans are already self-modifying to such a degree 
<br>
that we should expect *no substantial additional benefit* from an AI 
<br>
having direct access to its own source code, *then* I'd know what 
<br>
difference of empirical anticipation we were arguing about.
<br>
<p><em>&gt;&gt;&gt;I think the hard problem about achieving intelligence is crafting
</em><br>
<em>&gt;&gt;&gt;the software, which problem is &quot;hard&quot; in a technical sense of being
</em><br>
<em>&gt;&gt;&gt;NP-hard and requiring major computational effort,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer&gt; As I objected at the AGI conference, if intelligence were
</em><br>
<em>&gt; Eliezer&gt; hard in the sense of being NP-hard, a mere 10^44 nodes
</em><br>
<em>&gt; Eliezer&gt; searched would be nowhere near enough to solve an environment
</em><br>
<em>&gt; Eliezer&gt; as complex as the world, nor find a solution anywhere near as
</em><br>
<em>&gt; Eliezer&gt; large as the human brain.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer&gt; *Optimal* intelligence is NP-hard and probably
</em><br>
<em>&gt; Eliezer&gt; Turing-incomputable.  This we all know.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer&gt; But if intelligence had been a problem in which *any*
</em><br>
<em>&gt; Eliezer&gt; solution whatsoever were NP-hard, it would imply a world in
</em><br>
<em>&gt; Eliezer&gt; which all organisms up to the first humans would have had
</em><br>
<em>&gt; Eliezer&gt; zero intelligence, and then, by sheer luck, evolution would
</em><br>
<em>&gt; Eliezer&gt; have hit on the optimal solution of human intelligence.  What
</em><br>
<em>&gt; Eliezer&gt; makes NP-hard problems difficult is that you can't gather
</em><br>
<em>&gt; Eliezer&gt; information about a rare solution by examining the many
</em><br>
<em>&gt; Eliezer&gt; common attempts that failed.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer&gt; Finding successively better approximations to intelligence is
</em><br>
<em>&gt; Eliezer&gt; clearly not an NP-hard problem, or we would look over our
</em><br>
<em>&gt; Eliezer&gt; evolutionary history and find exponentially more evolutionary
</em><br>
<em>&gt; Eliezer&gt; generations separating linear increments of intelligence.
</em><br>
<em>&gt; Eliezer&gt; Hominid history may or may not have been &quot;accelerating&quot;, but
</em><br>
<em>&gt; Eliezer&gt; it certainly wasn't logarithmic!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer&gt; If you are really using NP-hard in the technical sense, and
</em><br>
<em>&gt; Eliezer&gt; not just a colloquial way of saying &quot;bloody hard&quot;, then I
</em><br>
<em>&gt; Eliezer&gt; would have to say I flatly disagree: Over the domain where
</em><br>
<em>&gt; Eliezer&gt; hominid evolution searched, it was not an NP-hard problem to
</em><br>
<em>&gt; Eliezer&gt; find improved approximations to intelligence by local search
</em><br>
<em>&gt; Eliezer&gt; from previous solutions.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I am using the term NP-hard to an extent metaphorically, but
</em><br>
<em>&gt; drawing on real complexity notions that problems can really be hard.
</em><br>
<em>&gt; I'm not claiming that constructing an intelligence is a decision problem 
</em><br>
<em>&gt; with a yes-no answer; in fact I'm not claiming it's an infinite class
</em><br>
<em>&gt; of problems, which is necessary to talk about asymptotic behavior
</em><br>
<em>&gt; altogether. 
</em><br>
<em>&gt; It's a particular instance-- we are trying to construct
</em><br>
<em>&gt; one particular program that works in this particular world,
</em><br>
<em>&gt; meaning solves a large collection of problems of certain types.
</em><br>
<p>Okay, problems *can* be hard; what reason do you have to believe that 
<br>
this particular problem *is* hard?
<br>
<p><em>&gt; (I don't buy into the notion of &quot;general intelligence&quot; that solves
</em><br>
<em>&gt; any possible world or any possible problem.)
</em><br>
<p>I agree.  An AI is supposed to work in the unusual special case of own 
<br>
low-entropy universe, not all possible worlds.  No-Free-Lunch theorems etc.
<br>
<p><em>&gt; I think the problem of constructing the right code
</em><br>
<em>&gt; for intelligence is a problem like finding a very short tour in
</em><br>
<em>&gt; a particular huge TSP instance. A human can't solve it by hand, (for
</em><br>
<em>&gt; reasons that are best understood by thinking about complexity theory
</em><br>
<em>&gt; results about infinite problem classes and in the limit behavior,
</em><br>
<em>&gt; which is why I appeal to that understanding). 
</em><br>
<em>&gt; To solve it, you are going to have to construct a good algorithm,
</em><br>
<em>&gt; *and run it for a long time*. If you do that, you can get a better
</em><br>
<em>&gt; and better solution, just like if you run Lin-Kernighan on a huge
</em><br>
<em>&gt; TSP instance, you will find a pretty short tour.
</em><br>
<p>Finding a *short*, but not *optimal*, tour in a particular huge TSP 
<br>
instance, is not an NP-hard problem - there are algorithms that do it, 
<br>
as you mention.  And much more importantly from the perspective of AI 
<br>
design, it was not an NP-hard problem for a programmer to find those 
<br>
algorithms.
<br>
<p>I furthermore note that the problem of constructing intelligent code 
<br>
doesn't seem to me at all like the problem of finding a short tour in a 
<br>
huge TSP instance.  The world has a vast number of exploitable 
<br>
regularities, which have similarities and differences between 
<br>
themselves; there are meta-regularities in the regularities which can in 
<br>
turn be exploited.  You can eat them one at a time, or swallow 
<br>
metaproblems in whole gulps.
<br>
<p>Magic takes many forms.  When you don't know how to do something, you 
<br>
can appeal to complexity, to emergence, to huge heaps of hardware, to 
<br>
vague similarities to the human brain...  Are you sure that you aren't 
<br>
saying &quot;We'll need to run the code for a long time&quot; in order to 
<br>
generate, within yourself, a feeling of having thrown something really 
<br>
powerful at the problem?  Like de Garis talking about ten thousand 
<br>
neural-net-module-engineers constructing an intelligent being?  Do you 
<br>
know specifically what is the algorithm that you think *must* be run to 
<br>
generate an intelligence, and can you calculate quantitatively how long 
<br>
it takes to run?
<br>
<p>We know that natural selection took a long time to run, but natural 
<br>
selection is a bloody inefficient algorithm.  Natural selection is so 
<br>
ridiculously simple that we can even calculate quantitatively how 
<br>
inefficient it is, and come up with estimates like 2 ln(N) / s 
<br>
generations to fix a single mutation with advantage s in population N.
<br>
<p>I wouldn't be surprised if, in the course of building an AI, there were 
<br>
points where I found it convenient to run simple algorithms for a long 
<br>
time.  But too much of this would signify that I was trying to 
<br>
brute-force the problem and failing to exploit important regularities in it.
<br>
<p><em>&gt; Evolution ran for a heck of a lot of computation on the problem.
</em><br>
<em>&gt; It is possible that humans will be able to jump start a lot of
</em><br>
<em>&gt; that, but its also true we are not going to be able to run
</em><br>
<em>&gt; for as much computation. Its an open question whether we can get
</em><br>
<em>&gt; there, but I suggest it may take a composite algorithm-- both 
</em><br>
<em>&gt; jump starting the code design and then running a lot to improve it.
</em><br>
<p>Actually, I very much like the idea of running simple programs for a 
<br>
long time to boot up an intelligence.  Not because it's the only way to 
<br>
get intelligence, or even because it's convenient, but because it means 
<br>
that the humans have less complexity to potentially get wrong.  I 
<br>
wouldn't use an evolutionary program because then I'd lose control of 
<br>
the resulting complexity, thus obviating the whole point of starting out 
<br>
simple.
<br>
<p><em>&gt; Eliezer&gt; Now as Justin Corwin pointed out to me, this does not mean
</em><br>
<em>&gt; Eliezer&gt; that intelligence is not *ultimately* NP-hard.  Evolution
</em><br>
<em>&gt; Eliezer&gt; could have been searching at the bottom of the design space,
</em><br>
<em>&gt; Eliezer&gt; coming up with initial solutions so inefficient that there
</em><br>
<em>&gt; Eliezer&gt; were plenty of big wins.  From a pragmatic standpoint, this
</em><br>
<em>&gt; Eliezer&gt; still implies I. J. Good's intelligence explosion in
</em><br>
<em>&gt; Eliezer&gt; practice; the first AI to search effectively enough to run up
</em><br>
<em>&gt; Eliezer&gt; against NP-hard problems in making further improvements, will
</em><br>
<em>&gt; Eliezer&gt; make an enormous leap relative to evolved intelligence before
</em><br>
<em>&gt; Eliezer&gt; running out of steam.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't know what you mean here at all.
</em><br>
<p>Did previous paragraphs clear it up?  In other words, Corwin's notion is 
<br>
that a *properly designed* intelligence is good enough that making 
<br>
further improvements is NP-hard, but human intelligences are operating 
<br>
far short of the level where this happens.  Like starting out with a 
<br>
random traversal of the TSP graph; there'll be plenty of low-hanging 
<br>
fruit, and if you only take them one at a time, they'll last quite a 
<br>
while - you might start thinking it was an easy problem.  Corwin's 
<br>
notion is that human intelligence is so poorly designed as to still 
<br>
occupy this regime; single mutations can still lift us up.
<br>
<p><em>&gt;&gt;&gt;so the ability to make sequential small improvements, and bring to
</em><br>
<em>&gt;&gt;&gt;bear the computation of millions or billions of (sophisticated,
</em><br>
<em>&gt;&gt;&gt;powerful) brains, led to major improvements.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer&gt; This is precisely the behavior that does *not* characterize
</em><br>
<em>&gt; Eliezer&gt; NP-hard problems.  Improvements on NP-hard problems don't add
</em><br>
<em>&gt; Eliezer&gt; up; when you tweak a local subproblem it breaks something
</em><br>
<em>&gt; Eliezer&gt; else.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;&gt;I suggest these improvements are not merely &quot;external&quot;, but
</em><br>
<em>&gt;&gt;&gt;fundamentally affect thought itself. For example, one of the
</em><br>
<em>&gt;&gt;&gt;distinctions between human and ape cognition is said to be that we
</em><br>
<em>&gt;&gt;&gt;have &quot;theory of mind&quot; whereas they don't (or do much more
</em><br>
<em>&gt;&gt;&gt;weakly). But I suggest that &quot;theory of mind&quot; must already be a
</em><br>
<em>&gt;&gt;&gt;fairly complex program, built out of many sub-units, and that we
</em><br>
<em>&gt;&gt;&gt;have built additional components and capabilities on what came
</em><br>
<em>&gt;&gt;&gt;evolutionarily before by virtue of thinking about the problem and
</em><br>
<em>&gt;&gt;&gt;passing on partial progress, for example in the mode of bed-time
</em><br>
<em>&gt;&gt;&gt;stories and fiction. Both for language itself and things like
</em><br>
<em>&gt;&gt;&gt;theory of mind, one can imagine some evolutionary improvements in
</em><br>
<em>&gt;&gt;&gt;ability to use it through the Baldwin effect, but the main point
</em><br>
<em>&gt;&gt;&gt;here seems to be the use of external storage in &quot;culture&quot; in
</em><br>
<em>&gt;&gt;&gt;developing the algorithms and passing them on. Other examples of
</em><br>
<em>&gt;&gt;&gt;modules that directly effect thinking prowess would be the
</em><br>
<em>&gt;&gt;&gt;axiomatic method, and recursion, which are specific human
</em><br>
<em>&gt;&gt;&gt;discoveries of modes of thinking, that are passed on using language
</em><br>
<em>&gt;&gt;&gt;and improve &quot;intelligence&quot; in a core way.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer&gt; Considering the infinitesimal amount of information that
</em><br>
<em>&gt; Eliezer&gt; evolution can store in the genome per generation, on the
</em><br>
<em>&gt; Eliezer&gt; order of one bit, 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Actually, with sex its theoretically possible to gain something like 
</em><br>
<em>&gt; sqrt(P) bits per generation (where P is population size), cf Baum, Boneh paper
</em><br>
<em>&gt; could be found on whatisthought.com and also Mackay paper. (This is
</em><br>
<em>&gt; a digression, since I'm not claiming huge evolution since chimps).
</em><br>
<p>That's for human-built genetic algorithms, not natural selection.  For 
<br>
natural selection see e.g. 
<br>
<a href="http://dspace.dial.pipex.com/jcollie/sle/index.htm">http://dspace.dial.pipex.com/jcollie/sle/index.htm</a>.  (I don't buy some 
<br>
of the author's claims here, but the central principle of which he gives 
<br>
a heuristic explanation is something I've heard of before in 
<br>
evolutionary biology; I think it goes back to Kimura.)  Natural 
<br>
selection does run on O(1) bits per generation.
<br>
<p>I furthermore note that gaining one standard deviation per generation, 
<br>
which is what your paper describes, is not obviously like gaining 
<br>
sqrt(P) bits of Shannon information per generation.  Yes, the standard 
<br>
deviation is proportional to sqrt(N), but it's not clear how you're 
<br>
going from that to gaining sqrt(N) bits of Shannon information in the 
<br>
gene pool per generation.  It would seem heuristically obvious that if 
<br>
your algorithm eliminates roughly half the population on each round, it 
<br>
can produce at most one bit of negentropy per round in allele 
<br>
frequencies.  I only skimmed the referenced paper, though; so if there's 
<br>
a particular paragraph I ought to read, feel free to direct me to it.
<br>
<p><em>&gt; Eliezer&gt; it's certainly plausible that a lot of our
</em><br>
<em>&gt; Eliezer&gt; software is cultural.  This proposition, if true to a
</em><br>
<em>&gt; Eliezer&gt; sufficiently extreme degree, strongly impacts my AI ethics
</em><br>
<em>&gt; Eliezer&gt; because it means we can't read ethics off of generic human
</em><br>
<em>&gt; Eliezer&gt; brainware.  But it has very little to do with my AGI theory
</em><br>
<em>&gt; Eliezer&gt; as such.  Programs are programs.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It has to do with the subject of my post, which was that by modifying
</em><br>
<em>&gt; the culture, humans have modified their core intelligence, so
</em><br>
<em>&gt; there is no distinction from strongly self improving.
</em><br>
<p>You might as well say that, since evolution built humans, evolution is 
<br>
intelligent, therefore humans are nothing new... but pragmatically 
<br>
speaking, there seems to be a large qualitative difference in there 
<br>
somewhere.  Ultimately it's all just the same 'ol physics.  You could 
<br>
equally well argue that if we build powerful AIs that shows the power of 
<br>
human intelligence, but again, it seems like the system went through an 
<br>
important transition somewhere.
<br>
<p>Humans may have modified their core intelligence a *little*, but what 
<br>
about all the results showing the perseverance of cognitive biases 
<br>
against self-willed remediation attempts?
<br>
<p><em>&gt; Eliezer&gt; But try to teach the human operating system to a chimp, and
</em><br>
<em>&gt; Eliezer&gt; you realize that firmware counts for *a lot*.  Kanzi seems to
</em><br>
<em>&gt; Eliezer&gt; have picked up some interesting parts of the human operating
</em><br>
<em>&gt; Eliezer&gt; system - but Kanzi won't be entering college anytime soon.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'm not claiming there was 0 evolution between chimp and man--
</em><br>
<em>&gt; our brains are 4 times bigger.
</em><br>
<p>(Terrence Deacon, in _The Symbolic Species_, says our brains are three 
<br>
times too large for an ape our size, and that our prefrontal cortex is 
<br>
relatively six times too large.)
<br>
<p><em>&gt; I'm claiming that the hard part--
</em><br>
<em>&gt; discovering the algorithms-- was mostly done by humans using storage
</em><br>
<em>&gt; and culture. Then there was some simple tuning up in brain size,
</em><br>
<em>&gt; and some slightly more complex Baldwin-effect etc tuning up
</em><br>
<em>&gt; programming grammar into the genome in large measure, so we become
</em><br>
<em>&gt; much more facile at learning the stuff quickly, and maybe other
</em><br>
<em>&gt; similar stuff. I don't deny that if you turn all that other stuff
</em><br>
<em>&gt; off you get an idiot, I'm just claiming it was computationally
</em><br>
<em>&gt; easy.
</em><br>
<p>Arguably, in a certain sense it *must* have been computationally easy 
<br>
because natural selection is incapable of doing anything computationally 
<br>
*hard*; evolution can't sit back and design complex interdependent 
<br>
machinery with hundreds of interlocking parts in a single afternoon, 
<br>
like a human programmer.
<br>
<p>However, chimps can recognize themselves in mirrors and implement 
<br>
complex political strategies in which A anticipates B's reaction to C, 
<br>
so there's clearly some level of hardware support among chimps for 
<br>
empathy and theory of mind, despite the (presumable) lack of 
<br>
sufficiently complex culture to give rise to a proper Baldwin effect.
<br>
<p>The real-world impressive power of human culture dates back largely to 
<br>
the last hundred thousand years which is an eyeblink of evolutionary 
<br>
time.  Space shuttles are pure products of accumulated culture without 
<br>
much in the way of space-shuttle-specific adaptive support.  Science is 
<br>
so much larger than the genome that even if we didn't know the answer in 
<br>
advance, we could guess that most scientific information *had* to be on 
<br>
paper somewhere, not in the genes.
<br>
<p>The question is, when all that lovely knowledge gets written down on 
<br>
paper, what is the force that does the writing?  What is the generator 
<br>
that produces all this lovely knowledge we're accumulating?  Could a 
<br>
more powerful generator produce knowledge orders of magnitude faster? 
<br>
Obviously yes, because human neurons run at speeds that are at least six 
<br>
orders of magnitude short of what we know to be physically possible. 
<br>
(Drexler's _Nanosystems_ describes sensory inputs and motor outputs that 
<br>
operate at a similar speedup.)  What about better firmware?  Would that 
<br>
buy us many additional orders of magnitude?
<br>
<p>If most of the generator complexity lay in a culturally transmitted 
<br>
human operating system that was open to introspection, then further 
<br>
improvements to firmware might be trivial.  But then scientists would 
<br>
have a much better understanding of how science works; but most 
<br>
scientists proceed mostly by instinct, and they don't have to learn 
<br>
rituals on anything remotely approaching the complexity of a human 
<br>
brain.  Most people would find learning the workings of the human brain 
<br>
a hugely intimidating endeavor - rather than being an easier and simpler 
<br>
version of something they did unwittingly as children, in the course of 
<br>
absorbing the larger and more important &quot;human operating system&quot; you 
<br>
postulate.  This human operating system, this modular theory of mind 
<br>
that gets transmitted - where is it written down?  There's a sharp limit 
<br>
on how much information you can accumulate without digital fidelity of 
<br>
transmission between generations.  The vast majority of human evolution 
<br>
took place long before the invention of writing.
<br>
<p>I don't believe in a culturally transmitted operating system, that 
<br>
existed over evolutionary periods, which contains greater total useful 
<br>
complexity than that specified in the brain-constructing portions of the 
<br>
human genome itself.  And even if such a thing existed, the fact that we 
<br>
haven't written it down implies that it is largely inaccessible to 
<br>
introspection and hence to deliberative, intelligent self-modification.
<br>
<p><em>&gt;&gt;&gt;I don't understand any real distinction between &quot;weakly self
</em><br>
<em>&gt;&gt;&gt;improving processes&quot; and &quot;strongly self improving processes&quot;, and
</em><br>
<em>&gt;&gt;&gt;hence, if there is such a distinction, I would be happy for
</em><br>
<em>&gt;&gt;&gt;clarification.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer&gt; The &quot;cheap shot&quot; reply is: Try thinking your neurons into
</em><br>
<em>&gt; Eliezer&gt; running at 200MHz instead of 200Hz.  Try thinking your
</em><br>
<em>&gt; Eliezer&gt; neurons into performing noiseless arithmetic operations.  Try
</em><br>
<em>&gt; Eliezer&gt; thinking your mind onto a hundred times as much brain, the
</em><br>
<em>&gt; Eliezer&gt; way you get a hard drive a hundred times as large every 10
</em><br>
<em>&gt; Eliezer&gt; years or so.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer&gt; Now that's just hardware, of course.  But evolution, the same
</em><br>
<em>&gt; Eliezer&gt; designer, wrote the hardware and the firmware.  Why shouldn't
</em><br>
<em>&gt; Eliezer&gt; there be equally huge improvements waiting in firmware?  We
</em><br>
<em>&gt; Eliezer&gt; understand human hardware better than human firmware, so we
</em><br>
<em>&gt; Eliezer&gt; can clearly see how restricted we are by not being able to
</em><br>
<em>&gt; Eliezer&gt; modify the hardware level.  Being unable to reach down to
</em><br>
<em>&gt; Eliezer&gt; firmware may be less visibly annoying, but it's a good bet
</em><br>
<em>&gt; Eliezer&gt; that the design idiom is just as powerful.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer&gt; &quot;The further down you reach, the more power.&quot;  This is the
</em><br>
<em>&gt; Eliezer&gt; idiom of strong self-improvement and I think the hardware
</em><br>
<em>&gt; Eliezer&gt; reply is a valid illustration of this.  It seems so simple
</em><br>
<em>&gt; Eliezer&gt; that it sounds like a cheap shot, but I think it's a valid
</em><br>
<em>&gt; Eliezer&gt; cheap shot.  We were born onto badly designed processors and
</em><br>
<em>&gt; Eliezer&gt; we can't fix that by pulling on the few levers exposed by our
</em><br>
<em>&gt; Eliezer&gt; introspective API.  The firmware is probably even more
</em><br>
<em>&gt; Eliezer&gt; important; it's just harder to explain.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer&gt; And merely the potential hardware improvements still imply
</em><br>
<em>&gt; Eliezer&gt; I. J. Good's intelligence explosion.  So is there a practical
</em><br>
<em>&gt; Eliezer&gt; difference?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The cheapshot reply to your cheapshot reply, is that if we construct
</em><br>
<em>&gt; an AI, that AI is just another part of the lower level in the weakly
</em><br>
<em>&gt; self-improving process, its part of our &quot;culture&quot;, so we can indeed
</em><br>
<em>&gt; realize the hardware improvement. This may sound cheap, but it 
</em><br>
<em>&gt; shows there is no real difference between the 2 layered system
</em><br>
<em>&gt; and the entirely self-recursive one.
</em><br>
<p>The cheap-cheap-cheap-reply is that if a self-improving AI goes off and 
<br>
builds a Dyson Sphere, and that is &quot;no real difference&quot;, I'm not sure I 
<br>
want to see what a &quot;real difference&quot; looks like.  Again, the cheap^3 
<br>
reply seems to me valid because it asks what difference of experience we 
<br>
anticipate.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15362.html">Damien Broderick: "intelligence explosion"</a>
<li><strong>Previous message:</strong> <a href="15360.html">Ola Bini: "Re: programming"</a>
<li><strong>Maybe in reply to:</strong> <a href="15358.html">Eliezer S. Yudkowsky: "Re: strong and weakly self improving processes"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15362.html">Damien Broderick: "intelligence explosion"</a>
<li><strong>Reply:</strong> <a href="15362.html">Damien Broderick: "intelligence explosion"</a>
<li><strong>Reply:</strong> <a href="15363.html">Eliezer S. Yudkowsky: "Re: strong and weakly self improving processes"</a>
<li><strong>Reply:</strong> <a href="15366.html">Damien Broderick: "Re: intelligence runaway (was explosion)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15361">[ date ]</a>
<a href="index.html#15361">[ thread ]</a>
<a href="subject.html#15361">[ subject ]</a>
<a href="author.html#15361">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
