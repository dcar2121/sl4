<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: strong and weakly self improving processes</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: strong and weakly self improving processes">
<meta name="Date" content="2006-07-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: strong and weakly self improving processes</h1>
<!-- received="Fri Jul 14 11:57:41 2006" -->
<!-- isoreceived="20060714175741" -->
<!-- sent="Fri, 14 Jul 2006 10:57:22 -0700" -->
<!-- isosent="20060714175722" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: strong and weakly self improving processes" -->
<!-- id="44B7DB02.40608@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="17587.47525.362109.662912@pc-eric.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20strong%20and%20weakly%20self%20improving%20processes"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri Jul 14 2006 - 11:57:22 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15359.html">J. Andrew Rogers: "Re: programming"</a>
<li><strong>Previous message:</strong> <a href="15357.html">Michael Vassar: "programming"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15361.html">Eliezer S. Yudkowsky: "Re: strong and weakly self improving processes"</a>
<li><strong>Maybe reply:</strong> <a href="15361.html">Eliezer S. Yudkowsky: "Re: strong and weakly self improving processes"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15358">[ date ]</a>
<a href="index.html#15358">[ thread ]</a>
<a href="subject.html#15358">[ subject ]</a>
<a href="author.html#15358">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eric Baum wrote:
<br>
<p><em>&gt; These matters are discussed in more detail in &quot;What is Thought?&quot;, 
</em><br>
<em>&gt; particularly the later chapters.
</em><br>
<p>You may assume I've read it.
<br>
<p><em>&gt; Eliezer,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I enjoyed &quot;Levels of Organization in General Intelligence&quot;. I very 
</em><br>
<em>&gt; much agree that there must be depth and complexity in the 
</em><br>
<em>&gt; computation.
</em><br>
<p>It should be emphasized that I wrote LOGI in 2002; many of my opinions
<br>
have updated since then, although I still think that LOGI stands as a
<br>
decent hypothesis about the evolutionary psychology of *human* general
<br>
intelligence.
<br>
<p>One particular disagreement of Eliezer-2006 with Eliezer-2002 is that I
<br>
would no longer resort to using &quot;complexity&quot; as an adjective for any
<br>
process for which I do not know *specifically* how it is complex.  As I
<br>
mention in a later essay, &quot;A Technical Explanation of Technical
<br>
Explanation&quot;, the following statements are all roughly equivalent in how
<br>
much they shape our anticipated experiences:
<br>
<p>&quot;Human intelligence is an emergent product of neurons firing.&quot;
<br>
&quot;Human intelligence is a magical product of neurons firing.&quot;
<br>
&quot;Human intelligence is a product of neurons firing.&quot;
<br>
<p>There are many synonyms for magic - words which seem to explain how
<br>
something interesting is done, without telling us what to anticipate,
<br>
without even being able to *exclude* experiences and tell us what we
<br>
should *not* see.  &quot;Phlogiston&quot; is magic.  &quot;Elan vital&quot; is magic.  And
<br>
if you don't have a specific complex process in mind, saying that
<br>
&quot;complexity&quot; does something is magic.  So I no longer go around saying
<br>
that intelligence is complicated, unless I have something specific in
<br>
mind which happens to be complicated.  Anything I don't understand is
<br>
not &quot;rich and deep and complex&quot;, it is something I don't understand.
<br>
<p>In retrospect, saying that intelligence was rich and deep and complex
<br>
was the right mistake to make at the time, because it got me to go out
<br>
and study many subjects; read much more broadly than I would have done,
<br>
if I had made the complementary mistake of thinking that I had definite
<br>
knowledge to the effect that intelligence was simple.  Nonetheless,
<br>
calling something &quot;complex&quot; doesn't explain it.
<br>
<p><em>&gt; There is one point, however, I wish to clarify.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You state &quot;The accelerating development of the hominid family and the
</em><br>
<em>&gt;  exponential increase in human culture
</em><br>
<p>Today I wouldn't call the increase in human culture &quot;exponential&quot;,
<br>
though I'd call it &quot;accelerating&quot;.  As for hominids, I'm not sure that
<br>
the last seven million years can even be fairly characterized as
<br>
&quot;accelerating&quot;.  We went over some kind of critical threshold, but
<br>
there's no evidence that the trip up to the threshold was an
<br>
accelerating journey.
<br>
<p><em>&gt; are both instances of *weakly self-improving processes*, 
</em><br>
<em>&gt; characterized by an externally constant process (evolution, modern 
</em><br>
<em>&gt; human brains) acting on a complexity pool (hominid genes, cultural 
</em><br>
<em>&gt; knowledge) whose elements interact synergetically. If we divide the 
</em><br>
<em>&gt; process into an improver and a content base, then weakly 
</em><br>
<em>&gt; self-improving processes are characterized by an external improving 
</em><br>
<em>&gt; process with roughly constant characteristic intelligence, and a 
</em><br>
<em>&gt; content base within which positive feedback takes place under the 
</em><br>
<em>&gt; dynamics imposed by the external process.&quot; (477)... &quot;A seed AI is a 
</em><br>
<em>&gt; *strongly self improving process*, characterized by improvements to 
</em><br>
<em>&gt; the content base that exert direct positive feedback on the 
</em><br>
<em>&gt; intelligence of the underlying improving process.&quot; (478) [italics in 
</em><br>
<em>&gt; original] and go on to suggest the possiblity that a seed AI may thus
</em><br>
<em>&gt; accelerate its progress in ways beyond what has happened to human
</em><br>
<em>&gt; intelligence.
</em><br>
<p>My words above, I would still defend.  I'd phrase it slightly
<br>
differently, here and there - for example, &quot;synergetically&quot; is
<br>
synonymous with &quot;magically&quot;.  And I wouldn't say &quot;a seed AI is a
<br>
strongly self-improving process&quot;, but, &quot;I think it should be possible to
<br>
construct some classes of minds that are strongly self-improving&quot;.
<br>
Nonetheless, overall I still hold the above position.
<br>
<p><em>&gt; I would like to respectfully suggest the possibility that this 
</em><br>
<em>&gt; overlooks a ramification of the layered and complex nature of 
</em><br>
<em>&gt; intelligence. It seems that the very top level of an intelligent 
</em><br>
<em>&gt; system (including a human) may be (or indeed to some extent may 
</em><br>
<em>&gt; intriniscally have to be) a module or system that actually knows very
</em><br>
<em>&gt;  little.
</em><br>
<p>Knows very little, or does something very simple?  Though I appreciate
<br>
that in your system, appropriate behavior is considered &quot;knowledge&quot;.
<br>
Still, there are such things as complex processes that know very little,
<br>
and simple processes that know a great deal.
<br>
<p>A giant lookup table is a simple process that may know an arbitrarily
<br>
large amount, depending on the incompressibility of the lookup table.  A
<br>
human programmer turned loose on the purely abstract form of a simple
<br>
problem (e.g. stacking towers of blocks), who invents a purely abstract
<br>
algorithm (e.g. mergesort) without knowing anything about which specific
<br>
blocks need to be moved, is an example of a complex process that used
<br>
very little specific knowledge about that specific problem to come up
<br>
with a good general solution.
<br>
<p><em>&gt; An example would be the auctioneer in a Hayek system (which only 
</em><br>
<em>&gt; knows to compare bids and choose the highest) or some other kind of 
</em><br>
<em>&gt; test module that simply tries out alternative lower modules and 
</em><br>
<em>&gt; receives a simple measure of what works and keeps what works, such as
</em><br>
<em>&gt;  various proposals of universal algorithms etc. Such a top layer 
</em><br>
<em>&gt; doesn't know anything about what it is comparing or how it is 
</em><br>
<em>&gt; computed. Its a chunk of fixed code. One reason why it makes sense to
</em><br>
<em>&gt;  assert there can't be some very smart top level is basically the 
</em><br>
<em>&gt; same reason why Friedrich Hayek asserted you couldn't run a control 
</em><br>
<em>&gt; economy.
</em><br>
<p>I think Hayek's assertion analogously argues that the &quot;top level&quot; - if
<br>
such a term even makes sense relative to a specific architecture - can't
<br>
be as smart as the entire AI, unless the &quot;top level&quot; is identical with
<br>
the entire AI, or the rest of the AI doesn't do any cognitive work.
<br>
<p>However, the Hayek analogy doesn't necessarily argue that the top level
<br>
must be simple.  In fact, the &quot;uppermost level&quot; of a large market
<br>
economy is so much more complex than any single human decisionmaker that
<br>
replacing the &quot;uppermost level&quot; with human strategizing results in
<br>
catastrophe, albeit for more reasons than simple cognitive incapacity.
<br>
One could just as easily argue that having an overly simple top level
<br>
was exactly the *mistake* of a planned economy.
<br>
<p>Socialism doesn't work for human economies, but that doesn't show that
<br>
all good design looks like an emergent level of organization atop units
<br>
that were optimized for fitness in a strictly local context.  There are
<br>
things that human societies can't get away with that would be perfectly
<br>
possible if we were designing the social agents from scratch.  The cells
<br>
in a multicellular organism are more cooperative than ants in a colony,
<br>
which are more cooperative than a human society; owing to the levels on
<br>
which selection acted and the effectiveness of evolved regulatory
<br>
mechanisms against selfishness on lower levels of organization.
<br>
<p>Is the term &quot;top level&quot; really all that useful for describing
<br>
evolutionary designs?  The human brain has more than one center of
<br>
gravity.  The limbic system, the ancient goal system at the center, is a
<br>
center of gravity; everything grew up around it.  The prefrontal cortex,
<br>
home of reflection and the self-model, is a center of gravity.  The
<br>
cerebellum, which learns the realtime skill of &quot;thinking&quot; and projects
<br>
massively to the cortex, is a center of gravity.
<br>
<p><em>&gt; But even if there would be some way to keep modifying the top level 
</em><br>
<em>&gt; to make it better, one could presumably achieve just as powerful an 
</em><br>
<em>&gt; ultimate intelligence by keeping it fixed and adding more powerful 
</em><br>
<em>&gt; lower levels (or maybe better yet, middle levels) or more or better 
</em><br>
<em>&gt; chunks and modules within a middle or lower level.
</em><br>
<p>The notion of a &quot;top level&quot;, to me, suggests that there's some behavior
<br>
such that it's useful to implement it over everything done by other
<br>
modules.  In the human mind, three such behaviors are reinforcement,
<br>
reflection, and realtime control.  A &quot;top level&quot; behavior can be simple
<br>
or complex, and it can embody a lot or a little knowledge.
<br>
<p><em>&gt; Along these lines, I tend to think that creatures evolved 
</em><br>
<em>&gt; intelligence and &quot;consciousness&quot; in this fashion: a decision making 
</em><br>
<em>&gt; unit that didn't know much but picked the best alternative (&quot;best&quot; 
</em><br>
<em>&gt; according to simple pain/reward signals passed to it) evolved first 
</em><br>
<em>&gt; (already in bacteria), followed by evolution in the sophistication of
</em><br>
<em>&gt;  the information calculated below the top level decision unit. No 
</em><br>
<em>&gt; doubt there was some evolution in &quot;the top level&quot; to better interface
</em><br>
<em>&gt;  with the better information being passed up,  but this was not 
</em><br>
<em>&gt; necessarily the crux of the matter. So in some sense, &quot;wanting&quot; and 
</em><br>
<em>&gt; &quot;will&quot; may have came first evolutionarily, and consciousness simply 
</em><br>
<em>&gt; became more sophisticated and nuanced as evolution progressed. This 
</em><br>
<em>&gt; also seems different than your picture.
</em><br>
<p>I did later have the thought that organisms needed to start out with
<br>
complete circuits - simple neural thermostats that implemented extremely
<br>
primitive modality structure and category structure and event structure
<br>
all at once.  A better evolutionary theory would state how
<br>
organizational structure differentiated out from there.  Obviously, a
<br>
system that is too incomplete to guide the organism cannot exist as an
<br>
evolutionary intermediate.  I am not sure that this is equivalent to
<br>
what you are thinking, but as an amendment, it seems to have some of the
<br>
same spirit.
<br>
<p>It is certainly an intriguing suggestion that, like ATP synthase, the
<br>
fast and frugal heuristic &quot;Take the Best&quot; is older than eukaryotic life
<br>
and was passed down to humans without substantial modification along the
<br>
way.  But I'm not sure I believe it.
<br>
<p>However...
<br>
<p>&nbsp;From my perspective, this argument over &quot;top levels&quot; doesn't have much
<br>
to do with the question of recursive self-improvement!  It's the agent's
<br>
entire intelligence that may be turned to improving itself.  Whether the
<br>
greatest amount of heavy lifting happens at a &quot;top level&quot;, or lower
<br>
levels, or systems that don't modularize into levels of organization;
<br>
and whether the work done improves upon the AI's top layers or lower
<br>
layers; doesn't seem to me to impinge much upon the general thrust of I.
<br>
J. Good's &quot;intelligence explosion&quot; concept.  &quot;The AI improves itself.&quot;
<br>
Why does this stop being an interesting idea if you further specify that
<br>
the AI is structured into levels of organization with a simple level
<br>
describable as &quot;top&quot;?
<br>
<p><em>&gt; I further think that a sufficient explanation (which is also the 
</em><br>
<em>&gt; simplest explanation, and is in accord with various data including 
</em><br>
<em>&gt; all known to me, and is thus my working assumption) for the 
</em><br>
<em>&gt; divergence between human and ape intelligence is that the discovery 
</em><br>
<em>&gt; of language allowed greatly increased &quot;culture&quot;, ie allowed 
</em><br>
<em>&gt; thought-programs to be passed down from one human to another and thus
</em><br>
<em>&gt;  to be discovered and improved by a cumulative process, involving the
</em><br>
<em>&gt;  efforts of numerous humans.
</em><br>
<p>If chimps had language, would they achieve human levels of technological
<br>
sophistication?  This is not a rhetorical question.  A human raised in
<br>
isolation, a wolfling child, may not be much more than a chimp.  So is
<br>
most of the difference our software?
<br>
<p>A human raised in total darkness would end up blind.  That doesn't mean
<br>
that the software of the visual cortex is stored primarily in the
<br>
environment and in the laws of optics - that you can apply a simple
<br>
decryptor to the photons as they strike, and end up with the code of a
<br>
visual cortex.  It means that the software of the visual cortex evolved
<br>
to treat the visual environment as an invariant and does not properly
<br>
develop in the absence of that invariant.  Tooby and Cosmides wrote
<br>
about this at length in &quot;The Psychological Foundations of Culture&quot;.
<br>
Genes can store information in the environment, but it's still the genes
<br>
deciding what to store and how to retrieve it.
<br>
<p><em>&gt; I think the hard problem about achieving intelligence is crafting the
</em><br>
<em>&gt;  software, which problem is &quot;hard&quot; in a technical sense of being 
</em><br>
<em>&gt; NP-hard and requiring major computational effort,
</em><br>
<p>As I objected at the AGI conference, if intelligence were hard in the
<br>
sense of being NP-hard, a mere 10^44 nodes searched would be nowhere
<br>
near enough to solve an environment as complex as the world, nor find a
<br>
solution anywhere near as large as the human brain.
<br>
<p>*Optimal* intelligence is NP-hard and probably Turing-incomputable.
<br>
This we all know.
<br>
<p>But if intelligence had been a problem in which *any* solution
<br>
whatsoever were NP-hard, it would imply a world in which all organisms
<br>
up to the first humans would have had zero intelligence, and then, by
<br>
sheer luck, evolution would have hit on the optimal solution of human
<br>
intelligence.  What makes NP-hard problems difficult is that you can't
<br>
gather information about a rare solution by examining the many common
<br>
attempts that failed.
<br>
<p>Finding successively better approximations to intelligence is clearly
<br>
not an NP-hard problem, or we would look over our evolutionary history
<br>
and find exponentially more evolutionary generations separating linear
<br>
increments of intelligence.  Hominid history may or may not have been
<br>
&quot;accelerating&quot;, but it certainly wasn't logarithmic!
<br>
<p>If you are really using NP-hard in the technical sense, and not just a
<br>
colloquial way of saying &quot;bloody hard&quot;, then I would have to say I
<br>
flatly disagree:  Over the domain where hominid evolution searched, it
<br>
was not an NP-hard problem to find improved approximations to
<br>
intelligence by local search from previous solutions.
<br>
<p>Now as Justin Corwin pointed out to me, this does not mean that
<br>
intelligence is not *ultimately* NP-hard.  Evolution could have been
<br>
searching at the bottom of the design space, coming up with initial
<br>
solutions so inefficient that there were plenty of big wins.  From a
<br>
pragmatic standpoint, this still implies I. J. Good's intelligence
<br>
explosion in practice; the first AI to search effectively enough to run
<br>
up against NP-hard problems in making further improvements, will make an
<br>
enormous leap relative to evolved intelligence before running out of steam.
<br>
<p><em>&gt; so the ability to make sequential small improvements, and bring to 
</em><br>
<em>&gt; bear the computation of millions or billions of (sophisticated, 
</em><br>
<em>&gt; powerful) brains, led to major improvements.
</em><br>
<p>This is precisely the behavior that does *not* characterize NP-hard
<br>
problems.  Improvements on NP-hard problems don't add up; when you tweak
<br>
a local subproblem it breaks something else.
<br>
<p><em>&gt; I suggest these improvements are not merely &quot;external&quot;, but 
</em><br>
<em>&gt; fundamentally affect thought itself. For example, one of the 
</em><br>
<em>&gt; distinctions between human and ape cognition is said to be that we 
</em><br>
<em>&gt; have &quot;theory of mind&quot; whereas they don't (or do much more weakly). 
</em><br>
<em>&gt; But I suggest that &quot;theory of mind&quot; must already be a fairly complex 
</em><br>
<em>&gt; program, built out of many sub-units, and that we have built 
</em><br>
<em>&gt; additional components and capabilities on what came evolutionarily 
</em><br>
<em>&gt; before by virtue of thinking about the problem and passing on partial
</em><br>
<em>&gt;  progress, for example in the mode of bed-time stories and fiction. 
</em><br>
<em>&gt; Both for language itself and things like theory of mind, one can 
</em><br>
<em>&gt; imagine some evolutionary improvements in ability to use it through 
</em><br>
<em>&gt; the Baldwin effect, but the main point here seems to be the use of 
</em><br>
<em>&gt; external storage in &quot;culture&quot; in developing the algorithms and 
</em><br>
<em>&gt; passing them on. Other examples of modules that directly effect 
</em><br>
<em>&gt; thinking prowess would be the axiomatic method, and recursion, which 
</em><br>
<em>&gt; are specific human discoveries of modes of thinking, that are passed 
</em><br>
<em>&gt; on using language and improve &quot;intelligence&quot; in a core way.
</em><br>
<p>Considering the infinitesimal amount of information that evolution can
<br>
store in the genome per generation, on the order of one bit, it's
<br>
certainly plausible that a lot of our software is cultural.  This
<br>
proposition, if true to a sufficiently extreme degree, strongly impacts
<br>
my AI ethics because it means we can't read ethics off of generic human
<br>
brainware.  But it has very little to do with my AGI theory as such.
<br>
Programs are programs.
<br>
<p>But try to teach the human operating system to a chimp, and you realize
<br>
that firmware counts for *a lot*.  Kanzi seems to have picked up some
<br>
interesting parts of the human operating system - but Kanzi won't be
<br>
entering college anytime soon.
<br>
<p>The instructions that human beings communicate to one another are
<br>
instructions for pulling sequences of levers on an enormously complex
<br>
system, the brain, which we never built.  If the machine is not there,
<br>
the levers have nothing to activate.  When the first explorers of AI
<br>
tried to write down their accessible knowledge of &quot;How to be a
<br>
scientist&quot; as code, they failed to create a scientist.  They could not
<br>
introspect on, and did not see, the vast machine their levers
<br>
controlled.  If you tell a human, &quot;Try to falsify your theories, rather
<br>
than trying to prove them,&quot; they may learn something important about how
<br>
to think.  If you inscribe the same words on a rock, nothing happens.
<br>
<p>Don't get me wrong - those lever-pulling sequences are important.  But
<br>
the true power, I think, lies in the firmware.  Could a human culture
<br>
with a sufficiently different &quot;operating system&quot; be more alien to us
<br>
than bonobos?  More alien than a species that evolved on another planet?
<br>
&nbsp;&nbsp;If most of the critical complexity is in the OS, then you'd expect this
<br>
to be the case.  Maybe I just lack imagination, but I have difficulty
<br>
seeing it.
<br>
<p><em>&gt; Another ramification of this layered picture are all the ways that 
</em><br>
<em>&gt; evolution evolves to evolve better, including finding meaningful 
</em><br>
<em>&gt; chunks that can then be put together into programs in novel ways. 
</em><br>
<em>&gt; These are analogous to adding or improving lower layers on an 
</em><br>
<em>&gt; intelligent system, which may make it as intelligent as modifying the
</em><br>
<em>&gt;  top layers would in any conceivable way. Evolution, which 
</em><br>
<em>&gt; constructed our ability to &quot;rationally design&quot;, may apply very much 
</em><br>
<em>&gt; the same processes on itself.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't understand any real distinction between &quot;weakly self 
</em><br>
<em>&gt; improving processes&quot; and &quot;strongly self improving processes&quot;, and 
</em><br>
<em>&gt; hence, if there is such a distinction, I would be happy for 
</em><br>
<em>&gt; clarification.
</em><br>
<p>The &quot;cheap shot&quot; reply is:  Try thinking your neurons into running at
<br>
200MHz instead of 200Hz.  Try thinking your neurons into performing
<br>
noiseless arithmetic operations.  Try thinking your mind onto a hundred
<br>
times as much brain, the way you get a hard drive a hundred times as
<br>
large every 10 years or so.
<br>
<p>Now that's just hardware, of course.  But evolution, the same designer,
<br>
wrote the hardware and the firmware.  Why shouldn't there be equally
<br>
huge improvements waiting in firmware?  We understand human hardware
<br>
better than human firmware, so we can clearly see how restricted we are
<br>
by not being able to modify the hardware level.  Being unable to reach
<br>
down to firmware may be less visibly annoying, but it's a good bet that
<br>
the design idiom is just as powerful.
<br>
<p>&quot;The further down you reach, the more power.&quot;  This is the idiom of
<br>
strong self-improvement and I think the hardware reply is a valid
<br>
illustration of this.  It seems so simple that it sounds like a cheap
<br>
shot, but I think it's a valid cheap shot.  We were born onto badly
<br>
designed processors and we can't fix that by pulling on the few levers
<br>
exposed by our introspective API.  The firmware is probably even more
<br>
important; it's just harder to explain.
<br>
<p>And merely the potential hardware improvements still imply I. J. Good's
<br>
intelligence explosion.  So is there a practical difference?
<br>
<p><em>&gt; Eric Baum
</em><br>
<em>&gt; <a href="http://whatisthought.com">http://whatisthought.com</a>
</em><br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15359.html">J. Andrew Rogers: "Re: programming"</a>
<li><strong>Previous message:</strong> <a href="15357.html">Michael Vassar: "programming"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15361.html">Eliezer S. Yudkowsky: "Re: strong and weakly self improving processes"</a>
<li><strong>Maybe reply:</strong> <a href="15361.html">Eliezer S. Yudkowsky: "Re: strong and weakly self improving processes"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15358">[ date ]</a>
<a href="index.html#15358">[ thread ]</a>
<a href="subject.html#15358">[ subject ]</a>
<a href="author.html#15358">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
