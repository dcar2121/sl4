<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ESSAY: Program length, Omega and Friendliness</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="Re: ESSAY: Program length, Omega and Friendliness">
<meta name="Date" content="2006-02-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ESSAY: Program length, Omega and Friendliness</h1>
<!-- received="Wed Feb 22 13:55:06 2006" -->
<!-- isoreceived="20060222205506" -->
<!-- sent="Wed, 22 Feb 2006 15:55:05 -0500" -->
<!-- isosent="20060222205505" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="Re: ESSAY: Program length, Omega and Friendliness" -->
<!-- id="638d4e150602221255w4ef78698uaa26a6ac64fe330f@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="43FCBF3F.5010900@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=Re:%20ESSAY:%20Program%20length,%20Omega%20and%20Friendliness"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Wed Feb 22 2006 - 13:55:05 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14187.html">David Picon Alvarez: "Re: ESSAY: Program length, Omega and Friendliness"</a>
<li><strong>Previous message:</strong> <a href="14185.html">Eliezer S. Yudkowsky: "Re: ESSAY: Program length, Omega and Friendliness"</a>
<li><strong>In reply to:</strong> <a href="14185.html">Eliezer S. Yudkowsky: "Re: ESSAY: Program length, Omega and Friendliness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14189.html">Eliezer S. Yudkowsky: "Re: ESSAY: Program length, Omega and Friendliness"</a>
<li><strong>Reply:</strong> <a href="14189.html">Eliezer S. Yudkowsky: "Re: ESSAY: Program length, Omega and Friendliness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14186">[ date ]</a>
<a href="index.html#14186">[ thread ]</a>
<a href="subject.html#14186">[ subject ]</a>
<a href="author.html#14186">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; &gt; What I suggested is that it is impossible for a program/computer
</em><br>
<em>&gt; &gt; combination to recursively self-improve its hardware and software in
</em><br>
<em>&gt; &gt; such a way that it can both
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; a) increase dramatically the algorithmic information of its software
</em><br>
<em>&gt;
</em><br>
<em>&gt; I see no reason why this is desirable.  It is desirable to improve the
</em><br>
<em>&gt; veracity of one's knowledge base in ways that accurately mirror external
</em><br>
<em>&gt; reality, which may increase algorithmic information.  Recursive
</em><br>
<em>&gt; self-improvement to make your internal algorithms smarter and more
</em><br>
<em>&gt; efficient at manipulating knowledge about the external world, does not
</em><br>
<em>&gt; require increasing algorithmic information.  I think you are confusing
</em><br>
<em>&gt; technical algorithmic complexity with the intuitive sense of complexity.
</em><br>
<p>I'm not confusing these senses of complexity.
<br>
<p>I conjecture that achieving powerful general intelligence within
<br>
plausible computational resources involves integrating a variety of
<br>
components involving differing levels of specialization.  (This is
<br>
different from AIXItl or godel machine type architectures, which are
<br>
very simple but do not operate well within plausible computational
<br>
resources.)   If this is true then making a vastly more intelligent AI
<br>
may involve integrating a large number of different components, at
<br>
various levels of specialization.  In this case the &quot;knowledge about
<br>
the external world&quot; is present in the AI system not only explicitly as
<br>
data but implicitly in the detailed design of the specialized
<br>
components.  The more specialized components, the greater the
<br>
algorithmic information.
<br>
<p><em>&gt;  A cellular automaton could give rise to a whole universe full of
</em><br>
<em>&gt; sentient creatures and superintelligent AIs, while still having almost
</em><br>
<em>&gt; trivial algorithmic complexity.
</em><br>
<p>Yes, but it cannot do so within a brief period of time.  A CA-universe
<br>
is more comparable to AIXItl than to a practical AGI architecture
<br>
(though of course both comparisons are loose ones): it gives rise to
<br>
interesting things via a kind of crude enumerative exploration of a
<br>
vast number of possibilities, until it hits on something good.  (Yes,
<br>
there's more to it than that, but that is a significant aspect.)
<br>
<p>To create a system giving rise to a universe full of sentient
<br>
creatures and superintelligent AI's within a brief period of time, I
<br>
conjecture that one would need to build a system with a pretty high
<br>
algorithmic information, not at all like a simple CA rule with a
<br>
low-algorithmic-information initial condition.  This is related of
<br>
course to my conjecture that to achieve general intelligence within
<br>
feasible space and time constraints one needs relatively
<br>
high-algorithmic-information systems integrating various components at
<br>
multiple levels of specialization.
<br>
<p><em>&gt;An AI can do extraordinarily complex
</em><br>
<em>&gt; things in the service of its goals, which to humans would look like
</em><br>
<em>&gt; immense complexity on the surface, without increasing its algorithmic
</em><br>
<em>&gt; complexity beyond that of the search which looked for good methods to
</em><br>
<em>&gt; accomplish its goals.  Anything you can do which predictably makes a
</em><br>
<em>&gt; program better at serving the utility function has no greater
</em><br>
<em>&gt; *algorithmic* complexity than the criterion you used to decide that the
</em><br>
<em>&gt; program would be better, even though it may look immensely more complex,
</em><br>
<em>&gt; and be immensely more efficient.  Like a cellular automaton producing a
</em><br>
<em>&gt; universe, or natural selection coughing up a human, such a process can
</em><br>
<em>&gt; produce an AI of vast surface complexity and fine-tuned efficiency
</em><br>
<em>&gt; without increasing algorithmic complexity in the technical sense.
</em><br>
<p>Yes, but what you are alluding to is an intelligence process that is
<br>
like AIXItl or evolutionary learning in that it is a simple algorithm
<br>
carrying out a sort of semi-exhaustive, heuristically-guided program
<br>
space search.
<br>
<p>I think that AGI's need to have this aspect, but they also need a
<br>
whole bunch of more specialized and space-intensive code, in order
<br>
that their intelligent behavior may have reasonable time-complexity.
<br>
<p>None of your comments are addressing the issue of tradeoffs between
<br>
space and time complexity, which I believe are conceptually
<br>
fundamental.
<br>
<p><em>&gt; Furthermore, if we imagine - I don't think this way, but it's the sort
</em><br>
<em>&gt; of thing you keep suggesting -
</em><br>
<p>Actually, this does not sound to me like the sort of thing I keep suggesting...
<br>
<p><em>&gt;that an outside source presents a
</em><br>
<em>&gt; Friendly design plus a proof that the design is Friendly; then the AI
</em><br>
<em>&gt; can verify the proof and the outside Friendly design can have greater
</em><br>
<em>&gt; algorithmic complexity than the original AI.  The original AI doesn't
</em><br>
<em>&gt; even need to keep the whole design or the whole proof in RAM, so long as
</em><br>
<em>&gt; it can keep all the intermediate results necessary to verify that each
</em><br>
<em>&gt; proof step is valid.
</em><br>
<p>I will need to think about this point more, it's an interesting one. 
<br>
On the face of it, it seems to me that there might be some proofs that
<br>
would lend themselves to this kind of incremental understanding, and
<br>
others that would not (the difference being the algorithmic
<br>
information of the set of intermediate results needed to be stored at
<br>
each stage).
<br>
<p>However, as you note, this is not the most likely-sounding option.
<br>
<p>-- Ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14187.html">David Picon Alvarez: "Re: ESSAY: Program length, Omega and Friendliness"</a>
<li><strong>Previous message:</strong> <a href="14185.html">Eliezer S. Yudkowsky: "Re: ESSAY: Program length, Omega and Friendliness"</a>
<li><strong>In reply to:</strong> <a href="14185.html">Eliezer S. Yudkowsky: "Re: ESSAY: Program length, Omega and Friendliness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14189.html">Eliezer S. Yudkowsky: "Re: ESSAY: Program length, Omega and Friendliness"</a>
<li><strong>Reply:</strong> <a href="14189.html">Eliezer S. Yudkowsky: "Re: ESSAY: Program length, Omega and Friendliness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14186">[ date ]</a>
<a href="index.html#14186">[ thread ]</a>
<a href="subject.html#14186">[ subject ]</a>
<a href="author.html#14186">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
