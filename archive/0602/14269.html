<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: How do you know when to stop? (was Re: Why playing it safe is dangerous)</title>
<meta name="Author" content="Charles D Hixson (charleshixsn@earthlink.net)">
<meta name="Subject" content="Re: How do you know when to stop? (was Re: Why playing it safe is dangerous)">
<meta name="Date" content="2006-02-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: How do you know when to stop? (was Re: Why playing it safe is dangerous)</h1>
<!-- received="Sun Feb 26 20:33:39 2006" -->
<!-- isoreceived="20060227033339" -->
<!-- sent="Sun, 26 Feb 2006 19:33:11 -0800" -->
<!-- isosent="20060227033311" -->
<!-- name="Charles D Hixson" -->
<!-- email="charleshixsn@earthlink.net" -->
<!-- subject="Re: How do you know when to stop? (was Re: Why playing it safe is dangerous)" -->
<!-- id="200602261933.11960.charleshixsn@earthlink.net" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="638d4e150602260802n39e9825aw76f72c22f549021c@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Charles D Hixson (<a href="mailto:charleshixsn@earthlink.net?Subject=Re:%20How%20do%20you%20know%20when%20to%20stop?%20(was%20Re:%20Why%20playing%20it%20safe%20is%20dangerous)"><em>charleshixsn@earthlink.net</em></a>)<br>
<strong>Date:</strong> Sun Feb 26 2006 - 20:33:11 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14270.html">Philip Goetz: "Re: ESSAY: Program length, Omega and Friendliness"</a>
<li><strong>Previous message:</strong> <a href="14268.html">William Pearson: "Re: ESSAY: Program length, Omega and Friendliness"</a>
<li><strong>In reply to:</strong> <a href="14259.html">Ben Goertzel: "Re: How do you know when to stop? (was Re: Why playing it safe is dangerous)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14261.html">Philip Goetz: "Re: How do you know when to stop? (was Re: Why playing it safe is dangerous)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14269">[ date ]</a>
<a href="index.html#14269">[ thread ]</a>
<a href="subject.html#14269">[ subject ]</a>
<a href="author.html#14269">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Sunday 26 February 2006 08:02 am, Ben Goertzel wrote:
<br>
<em>&gt; &gt; We have actually ALREADY reached the point where many computer programs
</em><br>
<em>&gt; &gt; self-modify.  I can practically guarantee that they aren't friendly, as
</em><br>
<em>&gt; &gt; they don't even have any model at all of the world external to the
</em><br>
<em>&gt; &gt; computer.  It's hard to be friendly if you don't know that anyone's
</em><br>
<em>&gt; &gt; there.  (A chain of increasingly less relevant examples of self modifying
</em><br>
<em>&gt; &gt; code follows.)
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; P.S.:  Check out: Evolutionary algorithms, Genetic Programming, Self
</em><br>
<em>&gt; &gt; Optimizing Systems, and Self Organizing systems.  You might also look
</em><br>
<em>&gt; &gt; into Core War and Tierra.  These are all (currently) quite primitive, but
</em><br>
<em>&gt; &gt; they are actual self-modifying code.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Hi,
</em><br>
<em>&gt;
</em><br>
<em>&gt; I'm very familiar with these techniques and in fact my Novamente AI
</em><br>
<em>&gt; system uses a sort of evolutionary algorithm (a novel variant of
</em><br>
<em>&gt; Estimation of Distribution algorithm) for hypothesis generation.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Technically, most evolutionary algorithms do not involve
</em><br>
<em>&gt; self-modifying code, rather they are learning-based automated code
</em><br>
<em>&gt; generation.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Tierra does involve self-modifying code of a very simple sort, but not
</em><br>
<em>&gt; of a sort that could ever lead to mouse-level intelligence let alone
</em><br>
<em>&gt; human-level or superhuman.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Of course, not all self-modifying code has nontrivial hard-takeoff
</em><br>
<em>&gt; potential; and an AI system need not possess self-modifying code in
</em><br>
<em>&gt; order to have nontrivial hard-takeoff potential.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Sorry if I spoke too imprecisely in my prior e-mail,
</em><br>
<em>&gt; Ben
</em><br>
<p>I'm not sure that you spoke too hurriedly, when speaking for yourself...but 
<br>
you aren't alone. 
<br>
<p>Perhaps I'm projecting from my own experiences.  I tend to consider that a 
<br>
system which generates code automatically is modifying it's code if it then 
<br>
runs the code.  Perhaps you mean that you read over the code between each set 
<br>
of runs.  I did something like that in land-use modeling during the 1980's.   
<br>
[The approach was retired when we switched to PCs that didn't have enough 
<br>
horsepower to run the models...and though now they do, the organization has 
<br>
been restructured.]  In this case all that was done was adjusting the weights 
<br>
of &quot;trip costs&quot; between zones, but the computer did it automatically, and 
<br>
even though it was technically feasible to examine them, only scattered 
<br>
checks were ever made.  Occasionally we would stack several runs in series to 
<br>
do repeated projections to estimate further into the future, and I thought of 
<br>
this repeated series of runs as a single program run to multiple decades.  
<br>
(We weren't right, but we captured the general trends.)  I'll grant (EASILY) 
<br>
that this program wasn't intelligent in any meaningful sense, but it was 
<br>
definitely &quot;self modifying&quot; as I think of the term.
<br>
<p>It is my expectation that when a company runs a project, the managers won't 
<br>
understand the details, and the programmers won't understand the context.  
<br>
This will frequently result in problems that could be avoided, but aren't.  
<br>
When I'm assuming that the companies are building increasingly intelligent 
<br>
programs...thinking things work this was causes me to be nervous.  Corners 
<br>
regularly get cut where they won't show, as people skip parts of what they 
<br>
are supposed to do because it's boring, or late for a bus.  This causes me to 
<br>
expect that when mandated examination of code becomes routine, if corrections 
<br>
are very rarely needed, it will tend to be skipped.  Particularly if the 
<br>
errors are subtle, so that skipping isn't even the right word.  &quot;Done without 
<br>
sufficient attention&quot; might be better.   Think of the scene in &quot;The Andromeda 
<br>
Strain&quot; where the researchers are testing samples...and one slips through.  
<br>
Naturally, most such mistakes won't cause any significant problem, so they'll 
<br>
be repeatedly detected and the result will be &quot;Well, no harm done&quot;.  (I'm 
<br>
assuming that quality control catches anything that blatantly wrong.  Every 
<br>
iteration passes the &quot;stress test&quot;s.)  However ...
<br>
<p>I also assume that anything people do has an error rate.  That there comes a 
<br>
point when one decides that the machine's judgement about the quality of a 
<br>
product is better than the human expert.  And that THAT has an error rate.  
<br>
To me an engineer's &quot;This appears to be a friendly AI&quot; is better assurance 
<br>
that a government's stamp of approval, but certainty won't be available.  I'm 
<br>
not sure that I trust a DoD stamp of approval.  One of the things they want 
<br>
is a robot soldier, and that's a bit hard to square with friendliness.  
<br>
Similar considerations apply to the other branches of the government...and 
<br>
they're doing, or at least funding, most of the work.
<br>
<p>So... I see the singularity coming, and I'm not thrilled.  The dangers 
<br>
inherent in it are so great that the only thing comparable is having a bunch 
<br>
of psychotic apes controlling planet-killing weapons of mass destruction.  
<br>
(Oops!)  Well, if I had a choice, I'd choose the singularity.  But I don't 
<br>
find it a happy choice.   (OTOH, I don't have a choice.  I get to live with 
<br>
one condition until the second one occurs.  Then I get to hope to live 
<br>
through it.)  I suppose seeing things this way causes me to be a bit 
<br>
dyspeptic.
<br>
<p>Friendly AI is the big hope.  It's not the only one...some neutral AGIs 
<br>
wouldn't be impossible to live with.  (Well, not really neutral, but also not 
<br>
measuring up to &quot;Friendly&quot;, either. Consider the &quot;Accelerando&quot; scenario.  The 
<br>
AIs there weren't exactly friendly, but it was possible to live with them. I 
<br>
feel that this kind of occurrence is more probable, and less desireable [to 
<br>
us], than a genuinely friendly AI.)
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14270.html">Philip Goetz: "Re: ESSAY: Program length, Omega and Friendliness"</a>
<li><strong>Previous message:</strong> <a href="14268.html">William Pearson: "Re: ESSAY: Program length, Omega and Friendliness"</a>
<li><strong>In reply to:</strong> <a href="14259.html">Ben Goertzel: "Re: How do you know when to stop? (was Re: Why playing it safe is dangerous)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14261.html">Philip Goetz: "Re: How do you know when to stop? (was Re: Why playing it safe is dangerous)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14269">[ date ]</a>
<a href="index.html#14269">[ thread ]</a>
<a href="subject.html#14269">[ subject ]</a>
<a href="author.html#14269">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
