<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Hard takeoff [WAS Re: JOIN: Joshua Fox]</title>
<meta name="Author" content="Olie L (neomorphy@hotmail.com)">
<meta name="Subject" content="Hard takeoff [WAS Re: JOIN: Joshua Fox]">
<meta name="Date" content="2006-02-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Hard takeoff [WAS Re: JOIN: Joshua Fox]</h1>
<!-- received="Tue Feb  7 18:33:18 2006" -->
<!-- isoreceived="20060208013318" -->
<!-- sent="Wed, 08 Feb 2006 12:33:16 +1100" -->
<!-- isosent="20060208013316" -->
<!-- name="Olie L" -->
<!-- email="neomorphy@hotmail.com" -->
<!-- subject="Hard takeoff [WAS Re: JOIN: Joshua Fox]" -->
<!-- id="BAY106-F2627ADF3D21473C9AEAA06BA000@phx.gbl" -->
<!-- inreplyto="43E8B92D.3080705@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Olie L (<a href="mailto:neomorphy@hotmail.com?Subject=Re:%20Hard%20takeoff%20[WAS%20Re:%20JOIN:%20Joshua%20Fox]"><em>neomorphy@hotmail.com</em></a>)<br>
<strong>Date:</strong> Tue Feb 07 2006 - 18:33:16 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14019.html">Charles D Hixson: "Re: JOIN: Joshua Fox"</a>
<li><strong>Previous message:</strong> <a href="14017.html">Richard Loosemore: "Re: About discussing Social Impact etc. [WAS Re: KILL-subTHREAD: Joshua Fox]"</a>
<li><strong>In reply to:</strong> <a href="14004.html">Richard Loosemore: "Re: JOIN: Joshua Fox"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14020.html">Phillip Huggan: "Re: Hard takeoff"</a>
<li><strong>Reply:</strong> <a href="14020.html">Phillip Huggan: "Re: Hard takeoff"</a>
<li><strong>Reply:</strong> <a href="14021.html">Russell Wallace: "Re: Hard takeoff [WAS Re: JOIN: Joshua Fox]"</a>
<li><strong>Reply:</strong> <a href="14025.html">H C: "RE: Hard takeoff [WAS Re: JOIN: Joshua Fox]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14018">[ date ]</a>
<a href="index.html#14018">[ thread ]</a>
<a href="subject.html#14018">[ subject ]</a>
<a href="author.html#14018">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt;From: Richard Loosemore &lt;<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Hard%20takeoff%20[WAS%20Re:%20JOIN:%20Joshua%20Fox]">rpwl@lightlink.com</a>&gt;
</em><br>
<em>&gt;Subject: Re: JOIN: Joshua Fox
</em><br>
<em>&gt;Date: Tue, 07 Feb 2006 10:13:49 -0500
</em><br>
<em>&gt;
</em><br>
<em>&gt;In short, I am of the opinion that the approach to AGI they espouse is 
</em><br>
<em>&gt;going to lead to an AGD (Artificial General Dumbtelligence), which will 
</em><br>
<em>&gt;never reach the level of human intelligence even after many more decades of 
</em><br>
<em>&gt;painstaking work, and hence will never be a threat.
</em><br>
<p>Decades?  I thought you said never?
<br>
<p>If powerful AI (~AGI) doesn't eventuate within decades - for whatever 
<br>
unanticipated engineering difficulties there may be - , work on it won't 
<br>
necessarily stop.
<br>
<p>Furthermore, the longer it takes to develop an AI that can improve AI (~~ 
<br>
Seed AI), the more likely it is to create a faster take-off.  Which is more 
<br>
likely to create a &quot;bad&quot; situation.
<br>
<p>One issue that will continue to be a problem (likely within decades) is that 
<br>
although no single methodology will create an AGI, if enough separately 
<br>
created tools get thrown together... I don't know.  I can't /show/ what's 
<br>
possible.
<br>
<p>(Much more...)
<br>
<p><em>&gt;From: Russell Wallace &lt;<a href="mailto:russell.wallace@gmail.com?Subject=Re:%20Hard%20takeoff%20[WAS%20Re:%20JOIN:%20Joshua%20Fox]">russell.wallace@gmail.com</a>&gt;
</em><br>
<em>&gt;Subject: Re: JOIN: Joshua Fox
</em><br>
<em>&gt;Date: Tue, 7 Feb 2006 15:23:30 +0000
</em><br>
<em>&gt;
</em><br>
<em>&gt;On 2/7/06, Joshua Fox &lt;<a href="mailto:joshua@joshuafox.com?Subject=Re:%20Hard%20takeoff%20[WAS%20Re:%20JOIN:%20Joshua%20Fox]">joshua@joshuafox.com</a>&gt; wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Russell Wallace wrote:
</em><br>
<em>&gt; &gt; &gt; I don't think the Singularity is inevitable.
</em><br>
<p>Key word: Inevitable.
<br>
<p>Very very different from &quot;highly likely&quot;
<br>
<p><em>&gt;In fact, I can think of
</em><br>
<em>&gt; &gt; three plausible scenarios in which it never happens, and there might be
</em><br>
<em>&gt; &gt; a fourth and fifth . . .
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; What are those? The only ones I have seen in the literature are:
</em><br>
(4)
<br>
<em>&gt; &gt;    - total catastrophe for human civilization
</em><br>
(5)
<br>
<em>&gt; &gt;    - some unknown factor that puts limits on exponential progress for
</em><br>
<em>&gt; &gt; intelligence and technology.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;...
</em><br>
<em>&gt;1. De facto world government forms, with the result that progress goes the
</em><br>
<em>&gt;way of the Qeng Ho fleets. ...
</em><br>
<p>We'll put this under &quot;regulation&quot;, then, shall we?
<br>
<p>(There are &quot;black-market&quot;/&quot;underground&quot; research considerations for 
<br>
practicality of regulating various researchy forms... but anyhoo...)
<br>
<p><em>&gt;2. Continuing population crash renders progress unsustainable. (Continued
</em><br>
<em>&gt;progress from a technology base as complex as today's requires very large
</em><br>
<em>&gt;populations to be economically feasible.)
</em><br>
<p>This could be categorised more generally as a contributing factor to severe 
<br>
economic recession.
<br>
<p>Similarly (4) - &quot;total catastrophe&quot; - doesn't have to be anything like an 
<br>
existential threat.  Sufficient economic recession will impede technological 
<br>
development, particularly AI development.
<br>
<p>Hell, all it takes to cause a huge setback for computer - related research 
<br>
(~AI) is damage to very small areas of the Earth, particularly around SF bay 
<br>
area (Silicon Valley), Massachusetts, Bangalore, Tokyo and Dresden.  (No, 
<br>
I'm not suggesting this would &quot;wipe out&quot; the computer industry.  But think 
<br>
of how much Comp tech is concentrated in small tech centres around the 
<br>
world, and how much a couple of small disasters could set the industry back)
<br>
<p>The computer industry is only... uh... &quot;supportable&quot; thanks to the large 
<br>
number of stable, supporting industries.
<br>
<p><em>&gt;
</em><br>
<em>&gt;3. Future political crisis leading to large scale war with nuclear or other
</em><br>
<em>&gt;(e.g. biotech or nanotech) weapons of mass destruction results in a
</em><br>
<em>&gt;fast-forward version of 2.
</em><br>
<p>Yeah, I think this is the same thing, broadly, as (4) - total catastrophe.
<br>
<p>There are other possibilities:
<br>
<p>6)  Cultural shift away from (specific forms of) technological development.
<br>
<p>Although this is unlikely to be sufficient, given the strength of tech 
<br>
development, culturally-inspired technological regression has happened 
<br>
before.
<br>
<p>It doesn't even have to be against all technology (generalist Luddism) - it 
<br>
could be very specific anti-computer tech.
<br>
<p>It is concievable (neither likely nor good) that over 50 years, most 
<br>
societies will adopt a position advocating natural, slow food.  Slow food is 
<br>
yummy (benefit), so there is a &quot;motivation&quot; for anti-GM food, anti 
<br>
fast-cooking tech.
<br>
<p>7) Engineering challenges on AGI - a variant on (5) - unforseen limit
<br>
<p>I can't say.  I don't know that anyone else can reasonably deny with 
<br>
sufficient knowledge:  There may be impediments that slow the development of 
<br>
AGI by many many decades.  By this stage, other forms of technological 
<br>
development may be advanced enough so that the &quot;rapid takeoff&quot; element of 
<br>
AGI won't have the same disjunctive impact that it would in the next 
<br>
century.
<br>
<p>If we already have open life-expectancy, enhanced biological intelligence, 
<br>
nano-assembly, cyberware, (!) hyperdrive, each of which has arisen slowly, 
<br>
would the slow implementation of AGI create a technological disjunction for 
<br>
humans?
<br>
<p>Sort of.
<br>
<p>There's still the predictability problem.  But it's not as much of a 
<br>
disjunction - it's a much smoother bump.
<br>
<p>Again, this seems /unlikely/ given current trends of development.  But for a 
<br>
different society, with a different order of inventions...
<br>
<p><em>&gt;From: &quot;H C&quot; &lt;<a href="mailto:lphege@hotmail.com?Subject=Re:%20Hard%20takeoff%20[WAS%20Re:%20JOIN:%20Joshua%20Fox]">lphege@hotmail.com</a>&gt;
</em><br>
<em>&gt;Subject: Re: Hard Take-off Re: JOIN: Joshua Fox
</em><br>
<em>&gt;Date: Tue, 07 Feb 2006 02:09:37 +0000
</em><br>
<em>&gt;
</em><br>
<em>&gt;You can't really agree or disagree about hard take-off.
</em><br>
<em>&gt;
</em><br>
<em>&gt;If the resources are available for hard take-off, then it happens. If 
</em><br>
<em>&gt;computational resources are more limiting, then it won't be so hard of a 
</em><br>
<em>&gt;take-off.
</em><br>
<p>It's not just the computational resources - referring to the AI Jail 
<br>
stuff... you can't argue from the inability to demonstrate the impossibility 
<br>
of something that it is guaranteed to happen.
<br>
<p>You can't guarantee that an AI bound to a computer, with no interaction with 
<br>
the wider world can't escape (dissenters will be told to shut up and read 
<br>
up)- but you can't use that fact to predict that it will escape to create a 
<br>
hard take-off.
<br>
<p>Also,
<br>
<p>Computational resources are not the only limiting factor.
<br>
<p>Factors that influence how hard the takeoff &quot;knee&quot; is include:
<br>
<p>1) Computational resources
<br>
2) Other resources - particularly nanotech.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;- it doesn't have to be replicators.  Tunnelling electron 
<br>
microscope-level nanotools etc will make it much easier for a &quot;runaway AI&quot; 
<br>
to create replicators
<br>
3) &quot;first instance efficiency&quot; - I know there's a better term, but I can't 
<br>
remember it.  If the first code only just gets over the line, and is slow 
<br>
and clunky --&gt; slower takeoff
<br>
4) AI goals (how much it wants to improve)
<br>
<p>And in Goals lies the rub
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14019.html">Charles D Hixson: "Re: JOIN: Joshua Fox"</a>
<li><strong>Previous message:</strong> <a href="14017.html">Richard Loosemore: "Re: About discussing Social Impact etc. [WAS Re: KILL-subTHREAD: Joshua Fox]"</a>
<li><strong>In reply to:</strong> <a href="14004.html">Richard Loosemore: "Re: JOIN: Joshua Fox"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14020.html">Phillip Huggan: "Re: Hard takeoff"</a>
<li><strong>Reply:</strong> <a href="14020.html">Phillip Huggan: "Re: Hard takeoff"</a>
<li><strong>Reply:</strong> <a href="14021.html">Russell Wallace: "Re: Hard takeoff [WAS Re: JOIN: Joshua Fox]"</a>
<li><strong>Reply:</strong> <a href="14025.html">H C: "RE: Hard takeoff [WAS Re: JOIN: Joshua Fox]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14018">[ date ]</a>
<a href="index.html#14018">[ thread ]</a>
<a href="subject.html#14018">[ subject ]</a>
<a href="author.html#14018">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
