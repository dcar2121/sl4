<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Think of it as AGI suiciding, not boxing</title>
<meta name="Author" content="Nick Hay (nickjhay@hotmail.com)">
<meta name="Subject" content="Re: Think of it as AGI suiciding, not boxing">
<meta name="Date" content="2006-02-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Think of it as AGI suiciding, not boxing</h1>
<!-- received="Mon Feb 20 03:29:08 2006" -->
<!-- isoreceived="20060220102908" -->
<!-- sent="Mon, 20 Feb 2006 23:29:00 +1300" -->
<!-- isosent="20060220102900" -->
<!-- name="Nick Hay" -->
<!-- email="nickjhay@hotmail.com" -->
<!-- subject="Re: Think of it as AGI suiciding, not boxing" -->
<!-- id="43F999EC.7060402@hotmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="20060220032437.23840.qmail@web61324.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Nick Hay (<a href="mailto:nickjhay@hotmail.com?Subject=Re:%20Think%20of%20it%20as%20AGI%20suiciding,%20not%20boxing"><em>nickjhay@hotmail.com</em></a>)<br>
<strong>Date:</strong> Mon Feb 20 2006 - 03:29:00 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14161.html">Christopher Healey: "RE: Think of it as AGI suiciding, not boxing"</a>
<li><strong>Previous message:</strong> <a href="14159.html">Tyler Emerson: "2006 Singularity Challenge - Met!"</a>
<li><strong>In reply to:</strong> <a href="14156.html">Phillip Huggan: "Re: Think of it as AGI suiciding, not boxing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14161.html">Christopher Healey: "RE: Think of it as AGI suiciding, not boxing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14160">[ date ]</a>
<a href="index.html#14160">[ thread ]</a>
<a href="subject.html#14160">[ subject ]</a>
<a href="author.html#14160">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Phillip Huggan wrote:
<br>
<em>&gt; Why do we need to influence it during the design process beyond specifying an
</em><br>
<em>&gt; initial question?
</em><br>
<p>True, you wouldn't need to.  But it is the AI's influence on humanity that's the 
<br>
problem.
<br>
<p><em>&gt; To communicate the design, all it prints out is an
</em><br>
<em>&gt; engineering blueprint for a really efficient ion thruster or a series of
</em><br>
<em>&gt; chemical equations leading to a cheapie solar cell.  
</em><br>
<p>Or it prints out a stream of text that is entirely unexpected yet unearthly 
<br>
compelling....  If the space of possible outputs is large enough to contain 
<br>
nontrivial inventions, say &gt;1000 bits, it is large to contain plenty of 
<br>
suprising things.  Its not clear that all of these would have a benign effect on 
<br>
humans -- and us humans are known to be greatly affected by the things we read.
<br>
<p>There need only be 1 such design in the space of 2^1000 if the AI has the 
<br>
intelligence to find it along with the desire to influence the universe.
<br>
<p><em>&gt; Anything that looks like
</em><br>
<em>&gt; it might destabilize the vacuum or create an UFAI, we don't build.  There are
</em><br>
<em>&gt; many fields of engineering we know enough about to be assured the product
</em><br>
<em>&gt; effects of a given blueprint won't be harmful.
</em><br>
<p>In some fields we know a lot about the effects of designs humans have previously 
<br>
produced, and can reliably predict the safety of a subclass of these designs. 
<br>
For example, we may be pretty sure an overdesigned bridge is safe to carry N 
<br>
kilograms.
<br>
<p>We know nothing about the designs a superintelligent AI could think of, as it is 
<br>
smarter than us, nor the blindspots in our ability to detect dangerous things.
<br>
<p><em>&gt; To significantly reduce most extinction threats, you need to monitor all
</em><br>
<em>&gt; bio/chemical lab facilities and all computers worldwide.  A means of
</em><br>
<em>&gt; military/police intervention must be devised to deal with violators too.
</em><br>
<em>&gt; Obviously there are risks of initiating WWIII and of introducing tyrants to
</em><br>
<em>&gt; power if the extinction threat reduction process is goofed.  Obviously an AGI
</em><br>
<em>&gt; may kill us off. There is a volume of probability space where the AGI intends
</em><br>
<em>&gt; to be unfriendly, yet blueprints some useful product technologies (that we
</em><br>
<em>&gt; can use for manually reducing extinction risks) before we realize it is
</em><br>
<em>&gt; trying to kill us and pull its plug.  
</em><br>
<p>There is perhaps a large volume of probability space where we don't realise it 
<br>
intends to escape before it does, being tempted by its inexhaustable supply of 
<br>
useful blueprints to keep it running until then.
<br>
<p><em>&gt; I also realize this is a recipe for an
</em><br>
<em>&gt; AGI that can be used to take over the world regardless if it is friendly or
</em><br>
<em>&gt; not.
</em><br>
<p>My basic point is apparently benign actions, e.g. printing out a page of text, 
<br>
aren't safe.  If the AI have the intention to manipulate us it doesn't need 
<br>
robot arms, it can do something weird and unanticipated.  You can't box a 
<br>
nonFriendly superintelligent AI.
<br>
<p>This is not to say you cannot create an Oracle which will design things for you. 
<br>
It does indicate limiting its output is not enough to make it safe.
<br>
<p>-- Nick Hay
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14161.html">Christopher Healey: "RE: Think of it as AGI suiciding, not boxing"</a>
<li><strong>Previous message:</strong> <a href="14159.html">Tyler Emerson: "2006 Singularity Challenge - Met!"</a>
<li><strong>In reply to:</strong> <a href="14156.html">Phillip Huggan: "Re: Think of it as AGI suiciding, not boxing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14161.html">Christopher Healey: "RE: Think of it as AGI suiciding, not boxing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14160">[ date ]</a>
<a href="index.html#14160">[ thread ]</a>
<a href="subject.html#14160">[ subject ]</a>
<a href="author.html#14160">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
