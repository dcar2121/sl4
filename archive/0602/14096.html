<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Singularity Institute: Likely to win the race to build GAI?</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Singularity Institute: Likely to win the race to build GAI?">
<meta name="Date" content="2006-02-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Singularity Institute: Likely to win the race to build GAI?</h1>
<!-- received="Wed Feb 15 10:45:25 2006" -->
<!-- isoreceived="20060215174525" -->
<!-- sent="Wed, 15 Feb 2006 09:47:28 -0800" -->
<!-- isosent="20060215174728" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Singularity Institute: Likely to win the race to build GAI?" -->
<!-- id="43F36930.3010001@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="43F1D9C2.2060302@joshuafox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Feb 15 2006 - 10:47:28 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14097.html">Eliezer S. Yudkowsky: "Re: Singularity Institute: Likely to win the race to build GAI?"</a>
<li><strong>Previous message:</strong> <a href="14095.html">Philip Goetz: "Rate of change (was Re: 'a process of non-thinking called faith')"</a>
<li><strong>In reply to:</strong> <a href="14074.html">Joshua Fox: "Re: Singularity Institute: Likely to win the race to build GAI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14079.html">pdugan: "RE: Singularity Institute: Likely to win the race to build GAI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14096">[ date ]</a>
<a href="index.html#14096">[ thread ]</a>
<a href="subject.html#14096">[ subject ]</a>
<a href="author.html#14096">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Joshua Fox wrote:
<br>
<em>&gt; Yes, I know that they are working on _Friendly_ GAI. But my question is: 
</em><br>
<em>&gt; What reason is there to think that the Institute has any real chance of 
</em><br>
<em>&gt; winning the race to General Artificial Intelligence of any sort, beating 
</em><br>
<em>&gt; out those thousands of very smart GAI researchers?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Though it might be a very bad thing for nonFriendly GAI to emerge first, 
</em><br>
<em>&gt; it seems to me by far more likely for someone else --there are a lot of 
</em><br>
<em>&gt; smart people out there -- to beat the Institute to the goal of GAI. 
</em><br>
<p>Through no fault on the part of the poster, who has asked a question 
<br>
that seems ordinary enough from his perspective, this is a &quot;wrong 
<br>
question&quot; from the perspective of anyone trying to build an AI - or do 
<br>
anything difficult to a scientist or engineer.  You don't want to come 
<br>
up with convincing reasons why you can solve the problem.  You just want 
<br>
to solve the problem.  Any attention you devote to comparing yourself to 
<br>
other people is wasted neural firings.
<br>
<p>As HC also pointed out, you play the cards you're dealt.  If you've got 
<br>
to beat a thousand other researchers to the punch to prevent the world 
<br>
from blowing up, then that's what you gotta do.  You should not mistake 
<br>
the Singularity Institute for futurists.  We are not here to prophesy 
<br>
that AI *will be* Friendly.  It's an easy point to become confused over, 
<br>
because most people talking in this mindspace are futurists; they want 
<br>
to convince you that things *will* turn out nicely.  We make no such claim.
<br>
<p>I will try to answer anyway, as best I can, the question as you put it. 
<br>
&nbsp;&nbsp;If I thought the probability of winning was negligible, I'd look for 
<br>
other cards to play.
<br>
<p>Suppose I walk into a ballroom full of PhD physicists.  Can I, a 
<br>
nonphysicist, tell who in that room has the best likelihood of making 
<br>
significant advances in physics?
<br>
<p>I can try to sort the physicists into a stratum of relatively uninspired 
<br>
people who picked up a PhD in college, and a stratum of dedicated 
<br>
geniuses.  This sorting will not be perfectly reliable but it may be 
<br>
discriminating enough to make it worthwhile.
<br>
<p>Competence is made up of fluid intelligence, crystallized skill, and 
<br>
background knowledge.  I can't detect crystallized skill or background 
<br>
knowledge in the domain of physics without possessing it myself.  I can 
<br>
try to detect fluid intelligence.  But short of becoming a physicist 
<br>
myself, I may have no luck at all in discriminating among the people who 
<br>
strike me as smart, unless they possess crystallized skill or background 
<br>
knowledge which I happen to share.  If a physicist launches into a 
<br>
lecture on cognitive science, I can label him as &quot;+1 Polymath&quot;, or 
<br>
detect a mistake if he makes one.  Similarly for people who start 
<br>
talking about Bayes, biases, or the philosophy of science, and get &quot;+1 
<br>
Rational&quot; bonuses.
<br>
<p>I've run into people whom others described as &quot;very smart&quot;, who not only 
<br>
struck me as not &quot;very smart&quot;, but as quite noticeably less smart than 
<br>
other people I know.  I strongly suspect that everyone significantly 
<br>
smarter than a given perceiver tends to be perceived as just &quot;very 
<br>
smart&quot;.  The hypothesis here is that if you've got IQ 130, you can 
<br>
distinguish grades of intelligence up to IQ 140, and everyone smarter 
<br>
than that is just &quot;very smart&quot;.  I don't think this is actually true, 
<br>
but I think there's a grain of truth in it, meaning that your ability to 
<br>
detect differences in grades of intelligence decreases as the distance 
<br>
above you increases.  Someone once asked me if I considered myself a 
<br>
genius.  I immediately inquired how rare a level of intelligence is 
<br>
required to qualify as &quot;genius&quot;.  The person thought for a moment and 
<br>
replied, &quot;1 in 300&quot;.  I laughed involuntarily and assured him that, yes, 
<br>
I was a &quot;genius&quot;.
<br>
<p>There are not thousands of AGI researchers in the world.  I doubt there 
<br>
are so many as a hundred.  And they are &quot;very smart&quot; to widely different 
<br>
degrees.
<br>
<p>Observed behavior can set upper bounds on competence.  When you make a 
<br>
specific observation of this type, you automatically offend people who 
<br>
are underneath the upper bound.  My father, a Ph.D. physicist and 
<br>
believing Orthodox Jew, would not agree that his acceptance of the Old 
<br>
Testament as the factual word of God sets an upper bound on his 
<br>
rationality skills.  But we are not talking about a subtle mistake.
<br>
<p>My father is more rational than the average human; for example, he 
<br>
taught me some simple magic tricks to make sure I wasn't taken in by 
<br>
supposed psychics.  My father looks with contempt upon Jewish sects 
<br>
which practice what he regards as superstition - Kabbalah and so on. 
<br>
But my father cannot possibly be a *world-class* rationalist.  That's 
<br>
beyond the bounds of possibility given his observed behavior.
<br>
<p>Many atheists, maybe even most atheists, would be reluctant to say that 
<br>
there is a limit to how smart you can be and still be religious.  Wasn't 
<br>
Isaac Newton religious?  It is historical fact that Newton wasted most 
<br>
of his life on Christian mysticism.  The notion of observed behavior 
<br>
setting an upper bound on competence should be understood as a 3D 
<br>
surface over fluid intelligence, relevant crystallized skill, and 
<br>
relevant factual knowledge.  Newton lived before Darwin, in an era when 
<br>
humanity's grasp on science and scientific procedure were both much 
<br>
weaker.  If Newton lived today and was still a Christian, I'd penalize 
<br>
him a lot more points for the mistake.  Also, physics is not as directly 
<br>
relevant to religion or rationality as other disciplines.  Darwin's 
<br>
observations forcibly stripped him of his belief in a personal loving 
<br>
God, but he remained a deist.  Laplace, the inventor of what is now 
<br>
known as Bayesian probability theory, was questioned by Napoleon as to 
<br>
whether his astronomy book made mention of God.  Laplace famously 
<br>
replied, &quot;Your Highness, I have no need of that hypothesis.&quot;
<br>
<p>Many atheists, probably a majority, arrived to that conclusion through 
<br>
some degree of luck; not because their rationality skills lay above the 
<br>
upper bound that *forces* someone to become an atheist.  There are 
<br>
atheists who profess themselves as having unsupported faith in a 
<br>
proposition, the nonexistence of God, which strikes them as more 
<br>
pleasant than its negation; atheists who try to keep an open mind about 
<br>
the healing powers of crystals; and atheists who are atheists because 
<br>
their parents raised them as atheists.
<br>
<p>People who are not sufficiently competent themselves, may be very 
<br>
skeptical about the idea of competence *forcing* you to a particular 
<br>
position.  People who know the probabilities and still buy lottery 
<br>
tickets set upper bounds on how well they can have internalized the 
<br>
concept of a hundred-million-to-one probability; good luck explaining 
<br>
that to them.  People whose mix of fluid intelligence, relevant 
<br>
crystallized skill, and relevant knowledge, does not *force* them to 
<br>
believe in evolution, have a hard time understanding that evolution is 
<br>
not &quot;just a theory&quot;.  And of course you can't convince them that their 
<br>
&quot;openmindedness&quot; is the result of insufficient competence, or that their 
<br>
&quot;openmindedness&quot; sets a hard upper bound on how competent they could be. 
<br>
&nbsp;&nbsp;They are not willing to believe - to really, emotionally believe, as 
<br>
opposed to claiming to entertain the theoretical possibility - that 
<br>
someone else could have a mind stronger than their own, which is 
<br>
inevitably forced to a single verdict favoring evolution.
<br>
<p>Now let's consider an AGI researcher working on a &quot;human-level&quot; AGI 
<br>
project in which Friendliness is not a first-class technical requirement 
<br>
actively shaping the AI.  Unless the researchers are setting out in 
<br>
deliberate intent to destroy the world, there is an upper bound on how 
<br>
competent they can be *at AGI*.  That is, there is a 2D surface which 
<br>
bounds the combination of crystallized skill at AGI research, and 
<br>
knowledge of related sciences, which they can possibly be using to 
<br>
challenge the problem.  (Unfortunately, in this domain, I don't think 
<br>
there's any associated bound on raw g-factor.  Lacking knowledge and 
<br>
skill and rationality, you can be at the human limits of fluid 
<br>
intelligence and still get it wrong, a la Newton on religion.)
<br>
<p>Trying to explain exactly which AGI skills they can't possibly have, 
<br>
stumbles over the problem of the skills themselves being harder to 
<br>
explain than any one issue that rests on them.  If you look at a dynamic 
<br>
computational process, and you expect it to be Friendly for no good 
<br>
reason, then that bounds your skill at noticing what a piece of code 
<br>
really does, and the rigor of the standards to which you hold yourself 
<br>
in predicting that a piece of code does pleasant things.  If you were 
<br>
sufficiently skilled at AGI thought, you'd write a walkthrough showing 
<br>
exactly how the nice thing happened, or else you wouldn't expect it to 
<br>
happen.  This, whether the nice thing you wanted consisted of something 
<br>
&quot;Friendly&quot; or something &quot;intelligent&quot;.
<br>
<p>Trying to explain this in words, I see that it sounds very vague - not 
<br>
more vague than most AI discussion, perhaps, but much too vague for an 
<br>
FAI researcher to accept.  Some of these concepts are explained more 
<br>
clearly in &quot;A Technical Explanation of Technical Explanation&quot;.  If 
<br>
you've read that, you remember that people will invent magical 
<br>
explanations like &quot;phlogiston&quot; or &quot;elan vital&quot; or &quot;emergence&quot;, and not 
<br>
notice that they are magical; it is not an error that humans notice by 
<br>
instinct, which it is why it is so common in history.  If you've read 
<br>
_Technical Explanation_ plus Judea Pearl, then you will understand when 
<br>
I say that bad explanations for intelligence consist of causal graphs 
<br>
labeled with portentous words: leaf nodes for desired outcomes such as 
<br>
&quot;intelligence&quot; or &quot;benevolence&quot;, and parent nodes for causes such as 
<br>
&quot;emergence&quot; or &quot;spontaneous order&quot;, with arcs reinforced by perceived 
<br>
correlation (the one says, humans are &quot;emergent&quot;, humans are 
<br>
&quot;intelligent&quot;, from this correlation I infer necessary and sufficient 
<br>
causation).  If you come up with a bad explanation for intelligence, and 
<br>
you are sufficiently enthusiastic about it, you can declare yourself an 
<br>
AGI researcher.  That's most of the AGI researchers out there, at least 
<br>
right now.  People who can't give you a walkthrough of how their program 
<br>
will behave intelligently (let alone nicely), but they have a bright 
<br>
idea about intelligence, and they want to test it experimentally. 
<br>
That's what their teachers told them science was all about.
<br>
<p>There's a good amount of knowledge you can acquire, such as evolutionary 
<br>
biology, heuristics and biases, experimental study of anthropomorphism, 
<br>
evolutionary psychology, etc. etc., which will make it *more difficult* 
<br>
to stare at a computer program and think that it will magically do nice 
<br>
things.  Unfortunately, the possible lack of such knowledge in AGI 
<br>
researchers doesn't give FAI researchers any significant advantage, 
<br>
since evolutionary biology is not directly relevant to constructing an 
<br>
AGI.  Worse, you can know quite a few individual disciplines before they 
<br>
*combine* to *force* a correct answer, since it only takes a *single* 
<br>
mistake not to get there.
<br>
<p>In this domain, I doubt there is any humanly possible level of raw fluid 
<br>
intelligence, which would *force* you to get the answer right in the 
<br>
absence of skill and knowledge.  I.e., Newton was extraordinarily 
<br>
intelligent but still failed on easy tests of rationality because he 
<br>
lacked knowledge we take for granted.  Relative to the background of 
<br>
modern science, AGI and FAI are hard enough as problems that no humanly 
<br>
possible level of g-factor alone will force you to get it right.  This 
<br>
is bad because it means you can get incredibly talented mathematicians 
<br>
trying to build an AGI, without them even realizing that FAI is a 
<br>
problem.  But they are still limited in how deeply they can understand 
<br>
intelligence; they can grasp facets and combine powerful tools and 
<br>
that's it.
<br>
<p>What an FAI researcher can theoretically do, which would require 
<br>
competence above the bound implied by trying to write an AGI *without* 
<br>
FAI, is write an AI based on a complete understanding of intelligence. 
<br>
An FAI researcher knows they are forbidden to invoke and use concepts 
<br>
that they don't fully and nonmagically understand (again, see TechExp to 
<br>
gain a clearer grasp on what this means).  When you're staring at a 
<br>
blank sheet of paper, trying to reason out how an aspect of cognition 
<br>
works, in advance of designing your FAI, then your thoughts may bounce 
<br>
off the rubber walls of magical things.  But you will be aware of your 
<br>
own lack of understanding, and you will be aware that you are prohibited 
<br>
from making use of the magic until it has ceased to be magical to you. 
<br>
And that's not just an FAI skill, it's an AGI skill - although realizing 
<br>
that you need to do FAI causes you to elevate this skill to a much 
<br>
higher level of importance, because you are no longer *allowed* to just 
<br>
try stuff and see if something unexpectedly works.
<br>
<p>If an FAI project comes first, it will be because the researchers of 
<br>
that project had a much deeper understanding.
<br>
<p>Again, I am not saying that you *can't* build an AGI without being 
<br>
sufficiently competent that your theory grabs you by the throat and 
<br>
forces you to elevate FAI to a first-class technical requirement. 
<br>
Natural selection produced humans without exercising any design 
<br>
intelligence whatsoever.  But there's a limit to how well you can 
<br>
visualize and understand your AI, and yet make elementary, gaping, 
<br>
obvious mistakes about whether the AI will be nice.  (Unfortunately, I 
<br>
suspect that you can understand your AI fully, and still make more 
<br>
subtle mistakes, more dignified forms of failure that are just as lethal.)
<br>
<p>If you're a researcher building an F-less AGI and you're not 
<br>
deliberately out to destroy the world, there are things you can't know, 
<br>
skills you can't have, and a limit on how well you can understand the AI 
<br>
you're trying to build.  You can be a tremendous genius, possibly at the 
<br>
limits of human intelligence, but if so that sets an even stricter upper 
<br>
bound on your crystallized skill and relevant knowledge.  Most such 
<br>
folks *won't* be tremendous geniuses.  World-class geniuses will be rare 
<br>
among AGI researchers, simply because world-class geniuses are rare in 
<br>
general.  There is no physical law that prohibits a non-world-class 
<br>
genius from declaring themselves an AGI researcher; even the Mentifexes 
<br>
and Marc Geddeses of the world do it.
<br>
<p>So it is not that FAI and AGI projects are racing at the same speed, 
<br>
toward a goal which is miles more distant for FAI projects because FAI 
<br>
projects have additional requirements.  The AGI projects are bounded in 
<br>
their competence, or they will turn into FAI projects; if their vision 
<br>
grows clear enough it will *force* the realization that to develop a 
<br>
nonlethal design they must junk their theories and start over with a 
<br>
higher standard of understanding.  FAI projects can continue on past 
<br>
that point of realization, and develop the skills which come afterward 
<br>
in the order of learning.  The advantage is not entirely to the 
<br>
ignorant, nor to the careless.
<br>
<p>I am sure that it is possible to spend years thinking about FAI, hold 
<br>
yourself to the standard of understanding every concept which you invoke 
<br>
and being able to walk through every nice behavior you expect, and yet 
<br>
make a nonobvious lethal mistake, and so fail.  But the projects whose 
<br>
AGIs would *automatically* kill off humanity, the projects who must fail 
<br>
at FAI by *default* - are, yes, genuinely limited in their competence. 
<br>
To reduce it to a slogan that fits on a T-Shirt:
<br>
<p>&nbsp;&nbsp;There is a limit to how competent you can be, and still be that stupid.
<br>
<p>It's a *very high* limit.  There's more to it than raw g-factor.  People 
<br>
can be *that stupid*, and still look &quot;very smart&quot;.  I can even conceive 
<br>
that they might *genuinely* be very smart, though I've yet to encounter 
<br>
a failing-by-default AGI researcher who strikes me as being on a level 
<br>
with, say, Judea Pearl.
<br>
<p>So the life-or-death problem reduces to whether people permissibly 
<br>
smarter than an upper bound can accomplish a colossally difficult task 
<br>
by means of exactly understanding it; before at least one member of a 
<br>
much larger pool of people, a few of whom are &quot;very smart&quot;, but none of 
<br>
them more competent than the upper bound, can accomplish a task, whose 
<br>
difficulty is merely huge, through work that includes a substantial 
<br>
component of guessing, vagueness, and luck.
<br>
<p>And remember also this.  This is the pass-fail test for the human 
<br>
species itself.  The other AGI projects are, most of them, not fighting 
<br>
that battle.  Sadly there is no monopoly on enthusiasm; perhaps some of 
<br>
the other AGI projects will work 20 hours per day on our doom.  I wish I 
<br>
could say that any sufficiently intelligent human being *must* see the 
<br>
pass-fail test of our survival, and devote all their resources to 
<br>
passing it.  Unfortunately this again requires crystallized skill and 
<br>
background knowledge, not just g-factor.  When I look over my own past 
<br>
history, I find that there was, dismayingly, a significant component of 
<br>
luck to noticing the pass-fail test.  That is not something I am 
<br>
comfortable with, and I will try to do better in the future, win on 
<br>
purpose instead of by accident.
<br>
<p>It still strikes me as endlessly strange that so few people should care 
<br>
about this matter, the hinge of time around which all else turns.  Yet I 
<br>
cannot boast that my course here was inevitable - I wonder sometimes if 
<br>
I *could* have made it without a component of luck, but that is not what 
<br>
actually happened.  But if I had been smart enough to bulldoze through 
<br>
life on sheer raw intelligence, as opposed to my grand-uncle loaning me 
<br>
a copy of &quot;Great Mambo Chicken and the Transhuman Condition&quot; when I was 
<br>
11, then there would be many others of slightly lesser intelligence, 
<br>
some of whom would succeed through an event path that included a 
<br>
component of luck.  And there would be many FAI researchers; the 
<br>
battlefield would not be so deserted.
<br>
<p>It would seem that sufficient fluid intelligence, crystallized skill at 
<br>
rationality, and background knowledge of science, is the price only of 
<br>
it being *possible* to find the hinge of time and move to defend it, if 
<br>
you are also lucky.  Yet even if there is a component of luck, we may 
<br>
hope that it will be some of the very best of the human species who move 
<br>
to prevent the world from destruction.  That it will *not* be an equal 
<br>
dispute between bands of people who happened to major in computer 
<br>
science, or even an equal dispute between scientific geniuses who fell 
<br>
into the particular field of AI.  The FAI project may draw on resources 
<br>
and brainpower allocated to the defense of the human species.  Which, in 
<br>
this time, is practically nothing, because the understanding is so rare. 
<br>
&nbsp;&nbsp;But among *very very* smart individuals, if the FAI project has need 
<br>
of them, that rationality may not be so rare.
<br>
<p>And it may be that there will not be enough smart people among the human 
<br>
species who have also the luck to discover the one important problem; or 
<br>
the smart people will not be able to raise enough of an advantage in 
<br>
competence to overcome the greater intrinsic difficulty of their 
<br>
problem; or the human species will not have sufficient wisdom among its 
<br>
six billions to allocate even the tiny proportion of its resources that 
<br>
it would need to defend itself.  And that will be the end of the human 
<br>
story.
<br>
<p>But it is not a foregone conclusion.  It is worth putting up a fight.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14097.html">Eliezer S. Yudkowsky: "Re: Singularity Institute: Likely to win the race to build GAI?"</a>
<li><strong>Previous message:</strong> <a href="14095.html">Philip Goetz: "Rate of change (was Re: 'a process of non-thinking called faith')"</a>
<li><strong>In reply to:</strong> <a href="14074.html">Joshua Fox: "Re: Singularity Institute: Likely to win the race to build GAI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14079.html">pdugan: "RE: Singularity Institute: Likely to win the race to build GAI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14096">[ date ]</a>
<a href="index.html#14096">[ thread ]</a>
<a href="subject.html#14096">[ subject ]</a>
<a href="author.html#14096">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
