<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: FW: RE: Singularity Institute: Likely to win the race to build GAI?</title>
<meta name="Author" content="Olie L (neomorphy@hotmail.com)">
<meta name="Subject" content="FW: RE: Singularity Institute: Likely to win the race to build GAI?">
<meta name="Date" content="2006-02-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>FW: RE: Singularity Institute: Likely to win the race to build GAI?</h1>
<!-- received="Wed Feb 15 03:43:59 2006" -->
<!-- isoreceived="20060215104359" -->
<!-- sent="Wed, 15 Feb 2006 21:43:57 +1100" -->
<!-- isosent="20060215104357" -->
<!-- name="Olie L" -->
<!-- email="neomorphy@hotmail.com" -->
<!-- subject="FW: RE: Singularity Institute: Likely to win the race to build GAI?" -->
<!-- id="BAY106-F48404A22AECB903DDF588BAFA0@phx.gbl" -->
<!-- inreplyto="RE: Singularity Institute: Likely to win the race to build GAI?" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Olie L (<a href="mailto:neomorphy@hotmail.com?Subject=FW:%20RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?"><em>neomorphy@hotmail.com</em></a>)<br>
<strong>Date:</strong> Wed Feb 15 2006 - 03:43:57 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14092.html">dave last: "Re: Singularity Institute: Likely to win the race to build GAI?"</a>
<li><strong>Previous message:</strong> <a href="14090.html">David Picon Alvarez: "Re: Faith-based thought vs thinkers"</a>
<li><strong>Maybe in reply to:</strong> <a href="14106.html">H C: "RE: Singularity Institute: Likely to win the race to build GAI?"</a>
<!-- nextthread="start" -->
<li><strong>Reply:</strong> <a href="14093.html">Tyler Emerson: "RE: RE: Singularity Institute: Likely to win the race to build GAI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14091">[ date ]</a>
<a href="index.html#14091">[ thread ]</a>
<a href="subject.html#14091">[ subject ]</a>
<a href="author.html#14091">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt;From: &quot;Tyler Emerson&quot; &lt;<a href="mailto:emerson@intelligence.org?Subject=FW:%20RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">emerson@intelligence.org</a>&gt;
</em><br>
<em>&gt;To: &quot;'pdugan'&quot; &lt;<a href="mailto:pdugan@vt.edu?Subject=FW:%20RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">pdugan@vt.edu</a>&gt;, &quot;'Olie L'&quot; &lt;<a href="mailto:neomorphy@hotmail.com?Subject=FW:%20RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">neomorphy@hotmail.com</a>&gt;
</em><br>
<em>&gt;Subject: RE: Singularity Institute: Likely to win the race to build GAI?
</em><br>
<em>&gt;Date: Tue, 14 Feb 2006 19:57:36 -0800
</em><br>
<em>&gt;
</em><br>
<em>&gt;Our chief goal is to create Friendly AI through our own project. Acquiring
</em><br>
<em>&gt;the funding and researchers to sustain an eight- to ten-person team is
</em><br>
<em>&gt;challenging but sufficiently achievable. I am not against influencing other
</em><br>
<em>&gt;projects, but that is much less optimal, based on my present assessment. I
</em><br>
<em>&gt;don't see enough appreciation from other projects on how *hard* it will be
</em><br>
<em>&gt;to achieve Friendly AI, and how critical it is to have a mathematical
</em><br>
<em>&gt;understanding of Friendly AI *before* building AGI. I don't know why this 
</em><br>
<em>&gt;is
</em><br>
<em>&gt;so hard for most projects to understand. Based on my present understanding,
</em><br>
<em>&gt;AGI projects are playing with fire the likes of which the world has never
</em><br>
<em>&gt;seen, and I haven't seen a sufficient appreciation of this. The Institute
</em><br>
<em>&gt;must find and develop brilliant researchers one individual at a time. We're
</em><br>
<em>&gt;presently looking for our second full-time Research Fellow. If and when we
</em><br>
<em>&gt;find that person, we'll be in a stronger position to find the 3rd and 4th.
</em><br>
<em>&gt;
</em><br>
<em>&gt;TE
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; -----Original Message-----
</em><br>
<em>&gt; &gt; From: pdugan [mailto:<a href="mailto:pdugan@vt.edu?Subject=FW:%20RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">pdugan@vt.edu</a>]
</em><br>
<em>&gt; &gt; Sent: Tuesday, February 14, 2006 7:31 PM
</em><br>
<em>&gt; &gt; To: emerson; Olie L
</em><br>
<em>&gt; &gt; Cc: <a href="mailto:pdugan@vt.edu?Subject=FW:%20RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">pdugan@vt.edu</a>
</em><br>
<em>&gt; &gt; Subject: RE: Singularity Institute: Likely to win the race to build GAI?
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I think you have a good idea of what SIAI's role could be, though I
</em><br>
<em>&gt; &gt; suppose
</em><br>
<em>&gt; &gt; Tyler should corroborate (I'm only affilied as a volunteer and not much 
</em><br>
<em>&gt;of
</em><br>
<em>&gt; &gt; an
</em><br>
<em>&gt; &gt; authority). I remember Goertzel saying something about Eliezer's 
</em><br>
<em>&gt;writings,
</em><br>
<em>&gt; &gt; how
</em><br>
<em>&gt; &gt; they made him take the Friendliness problem more seriously. I think the
</em><br>
<em>&gt; &gt; Institute could operate in the mediatory way you describe without
</em><br>
<em>&gt; &gt; requiring
</em><br>
<em>&gt; &gt; the fifty million dollar budget needed for build an AGI themselves. I
</em><br>
<em>&gt; &gt; think it
</em><br>
<em>&gt; &gt; would be most benificial for SIAI to gear themselves toward fostering an
</em><br>
<em>&gt; &gt; underlying forum for ensuring Friendliness in the AGI community.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Now, though this should definetly be a role the institute plays, I can't
</em><br>
<em>&gt; &gt; say
</em><br>
<em>&gt; &gt; whether it would be a primary or secondary role, Eliezer seems very
</em><br>
<em>&gt; &gt; commited
</em><br>
<em>&gt; &gt; toward engineering an AGI himself.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;    Patrick
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt;===== Original Message From Olie L &lt;<a href="mailto:neomorphy@hotmail.com?Subject=FW:%20RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">neomorphy@hotmail.com</a>&gt; =====
</em><br>
<em>&gt; &gt; &gt;Hi Patrick, Tyler
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;I'd like to bounce this off you, first - Could you check and verify 
</em><br>
<em>&gt;that
</em><br>
<em>&gt; &gt; I'm
</em><br>
<em>&gt; &gt; &gt;not just reiterating lame info or stepping over any lines of
</em><br>
<em>&gt; &gt; &gt;appropriateness?  Thankye...
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;As I see it, the SIAI's stated goals of
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;  the &quot;advancement of beneficial artificial intelligence and ethical
</em><br>
<em>&gt; &gt; &gt;cognitive enhancement&quot;
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;can be perfectly well achieved by having other institutions &quot;win the
</em><br>
<em>&gt; &gt; race&quot;
</em><br>
<em>&gt; &gt; &gt;to AGI.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;Their role is not necessarily to be the most advanced group on the path
</em><br>
<em>&gt; &gt; &gt;toward an AGI implementation.  Their role -as I see it - is to work
</em><br>
<em>&gt; &gt; towards
</em><br>
<em>&gt; &gt; &gt;the creation of beneficial AI.  That is different from creating
</em><br>
<em>&gt; &gt; beneficial
</em><br>
<em>&gt; &gt; &gt;AI themselves.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;Many other NGOs have created powerful positions for themselves, where
</em><br>
<em>&gt; &gt; they
</em><br>
<em>&gt; &gt; &gt;can work with commercial institutions to achieve their stated goals.  A
</em><br>
<em>&gt; &gt; good
</em><br>
<em>&gt; &gt; &gt;example is the RSPCA (Royal Soc. for Prevention of Cruelty to Animals) 
</em><br>
<em>&gt;-
</em><br>
<em>&gt; &gt; &gt;which has become (1) a de-facto enforcer of government legislation (2) 
</em><br>
<em>&gt;An
</em><br>
<em>&gt; &gt; &gt;powerful lobby for creating government legislation (3) an operation 
</em><br>
<em>&gt;that
</em><br>
<em>&gt; &gt; &gt;directly provides shelter to some animals (4) An organisation that 
</em><br>
<em>&gt;works
</em><br>
<em>&gt; &gt; &gt;collaboratively with many businesses.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;Not only do businesses provide the RSPCA with resources and money, they
</em><br>
<em>&gt; &gt; also
</em><br>
<em>&gt; &gt; &gt;engage in joint projects and give them unusual access to commercially
</em><br>
<em>&gt; &gt; &gt;sensitive information.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;As a non-profit organisation, the Institute may often be given far more
</em><br>
<em>&gt; &gt; &gt;access to proprietary information than other groups - such as
</em><br>
<em>&gt; &gt; universities
</em><br>
<em>&gt; &gt; &gt;or even investors.  Such access relies on the Institute developing an
</em><br>
<em>&gt; &gt; &gt;appropriate reputation, including having the right skills to be able to
</em><br>
<em>&gt; &gt; &gt;provide consultative services.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;No commercial AGI project would want to create an unfriendly AI.  It is
</em><br>
<em>&gt; &gt; &gt;against their interests to do so.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;If a business believes that the Institute can provide a service - such 
</em><br>
<em>&gt;as
</em><br>
<em>&gt; &gt; &gt;improving the friendliness of the business's AI project - there is a
</em><br>
<em>&gt; &gt; strong
</em><br>
<em>&gt; &gt; &gt;incentive to work with the Institute, advancing the Institute's goals.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;As far as I can see, by carving out a niche for itself - FAI theory - 
</em><br>
<em>&gt;the
</em><br>
<em>&gt; &gt; &gt;Institute has already done much to advance its reputation.  Even if its
</em><br>
<em>&gt; &gt; own
</em><br>
<em>&gt; &gt; &gt;projects are not the furthest towards demonstrating AGI potential, any
</em><br>
<em>&gt; &gt; &gt;efforts will hopefully assist in improving the Institute's FAI 
</em><br>
<em>&gt;expertise.
</em><br>
<em>&gt; &gt; &gt;If there are demonstrable successes, these will also greatly advance 
</em><br>
<em>&gt;the
</em><br>
<em>&gt; &gt; &gt;Institute's reputation.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;I would like to see the Institute expand its capacity to provide
</em><br>
<em>&gt; &gt; &gt;consultative services.  This is only my opinion.  But it has already 
</em><br>
<em>&gt;had
</em><br>
<em>&gt; &gt; &gt;substantial influence on a number of projects other than those of its
</em><br>
<em>&gt; &gt; staff.
</em><br>
<em>&gt; &gt; &gt;  Let us hope that more AGI projects will take their advice.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;-- Olie
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;&gt;From: pdugan &lt;<a href="mailto:pdugan@vt.edu?Subject=FW:%20RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">pdugan@vt.edu</a>&gt;
</em><br>
<em>&gt; &gt; &gt;&gt;Reply-To: <a href="mailto:sl4@sl4.org?Subject=FW:%20RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">sl4@sl4.org</a>
</em><br>
<em>&gt; &gt; &gt;&gt;To: sl4 &lt;<a href="mailto:sl4@sl4.org?Subject=FW:%20RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">sl4@sl4.org</a>&gt;
</em><br>
<em>&gt; &gt; &gt;&gt;CC: <a href="mailto:pdugan@vt.edu?Subject=FW:%20RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">pdugan@vt.edu</a>
</em><br>
<em>&gt; &gt; &gt;&gt;Subject: RE: Singularity Institute: Likely to win the race to build 
</em><br>
<em>&gt;GAI?
</em><br>
<em>&gt; &gt; &gt;&gt;Date: Tue, 14 Feb 2006 18:25:04 -0500
</em><br>
<em>&gt; &gt; &gt;&gt;
</em><br>
<em>&gt; &gt; &gt;&gt;Well I'd say its worth evaluating the prospective Friendliness of 
</em><br>
<em>&gt;these
</em><br>
<em>&gt; &gt; &gt;&gt;systems, for the obvious reasons. This is probably fairly difficult to
</em><br>
<em>&gt; &gt; do,
</em><br>
<em>&gt; &gt; &gt;&gt;particularly for projects based on proprietary information. I think a
</em><br>
<em>&gt; &gt; &gt;&gt;useful
</em><br>
<em>&gt; &gt; &gt;&gt;hueristic when gauging the risks associated with an AGI is to evaluate
</em><br>
<em>&gt; &gt; the
</em><br>
<em>&gt; &gt; &gt;&gt;likelyhood of a hard take-off. From what I gather about Novaemente, 
</em><br>
<em>&gt;you
</em><br>
<em>&gt; &gt; &gt;&gt;seem
</em><br>
<em>&gt; &gt; &gt;&gt;to see soft take-off as much more likely. If Novamente does prove 
</em><br>
<em>&gt;robust
</em><br>
<em>&gt; &gt; &gt;&gt;enough to be deemed a &quot;general intelligence&quot; would it possible for
</em><br>
<em>&gt; &gt; someone
</em><br>
<em>&gt; &gt; &gt;&gt;else, possibly SIAI, to conceive of a more &quot;powerful&quot; system that
</em><br>
<em>&gt; &gt; enganges
</em><br>
<em>&gt; &gt; &gt;&gt;in
</em><br>
<em>&gt; &gt; &gt;&gt;hard take-off while Novamente spends its &quot;childhood&quot;? Or one the other
</em><br>
<em>&gt; &gt; &gt;&gt;hand,
</em><br>
<em>&gt; &gt; &gt;&gt;what sort of Friendliness constraints does Novamente possess?
</em><br>
<em>&gt; &gt; &gt;&gt;
</em><br>
<em>&gt; &gt; &gt;&gt;   Patrick
</em><br>
<em>&gt; &gt; &gt;&gt;
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;===== Original Message From <a href="mailto:ben@goertzel.org?Subject=FW:%20RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">ben@goertzel.org</a> =====
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;In fact I know of a number of individuals/groups in addition to 
</em><br>
<em>&gt;myself
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;who fall into this category (significant progress made toward
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;realizing a software implementation whose design has apparent AGI
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;potential), though I'm not sure which of them are list members.
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;In addition to my Novamente project (www.novamente.net), I would
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;mention Steve Omohundro
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;<a href="http://home.att.net/~om3/selfawaresystems.html">http://home.att.net/~om3/selfawaresystems.html</a>
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;(who is working on a self-modifying AI system using his own variant 
</em><br>
<em>&gt;of
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;Bayesian learning) and James Rogers with his
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;algorithmic-information-theory related AGI design (James is a list
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;member, but his work has been kept sufficiently proprietary that I
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;can't say much about it).  There are many others as well...
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;Based on crude considerations, it would seem SIAI is nowhere near 
</em><br>
<em>&gt;the
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;most advanced group on the path toward an AGI implementation.  On 
</em><br>
<em>&gt;the
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;other hand, it's of course possible that those of us who are 
</em><br>
<em>&gt;&quot;further
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;along&quot; all have wrong ideas (though I doubt it!) and SIAI will come 
</em><br>
<em>&gt;up
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;with the right idea in 2008 or whenever and then proceed rapidly
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;toward the end goal.
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;ben
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;ben
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;On 2/14/06, pdugan &lt;<a href="mailto:pdugan@vt.edu?Subject=FW:%20RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">pdugan@vt.edu</a>&gt; wrote:
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; There is a certain list member who already has an AGI model more
</em><br>
<em>&gt; &gt; than
</em><br>
<em>&gt; &gt; &gt;&gt;half
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; implemented, making it a few years from testablility to see if it
</em><br>
<em>&gt; &gt; &gt;&gt;classifies
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; as a genuine AGI, and if so then maybe another half a decade 
</em><br>
<em>&gt;before
</em><br>
<em>&gt; &gt; &gt;&gt;something
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; like recursive self-improvement becomes possible.
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt;
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt;   Patrick
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt;
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;===== Original Message From P K &lt;<a href="mailto:kpete1@hotmail.com?Subject=FW:%20RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">kpete1@hotmail.com</a>&gt; =====
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;&gt;Yes, I know that they are working on _Friendly_ GAI. But my
</em><br>
<em>&gt; &gt; question
</em><br>
<em>&gt; &gt; &gt;&gt;is:
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;&gt;What reason is there to think that the Institute has any real
</em><br>
<em>&gt; &gt; chance
</em><br>
<em>&gt; &gt; &gt;&gt;of
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;&gt;winning the race to General Artificial Intelligence of any sort,
</em><br>
<em>&gt; &gt; &gt;&gt;beating
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;&gt;out those thousands of very smart GAI researchers?
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;&gt;
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;There is no particular reason(s) I can think of that make the
</em><br>
<em>&gt; &gt; &gt;&gt;Institute
</em><br>
<em>&gt; &gt; &gt;&gt;more
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;likely to develop AGI than any other organization with skilled
</em><br>
<em>&gt; &gt; &gt;&gt;developers.
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;It's all a fog. The only way to see if their ideas have any merit
</em><br>
<em>&gt; &gt; is
</em><br>
<em>&gt; &gt; &gt;&gt;to
</em><br>
<em>&gt; &gt; &gt;&gt;try
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;them out. Also, I suspect their donations would increase if they
</em><br>
<em>&gt; &gt; &gt;&gt;showed
</em><br>
<em>&gt; &gt; &gt;&gt;some
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;proofs of concept. It's all speculative at this point.
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;As for predicting success or failure, the best calibrated answer 
</em><br>
<em>&gt;is
</em><br>
<em>&gt; &gt; to
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;predict failure to anyone attempting to build a GAI. You would be
</em><br>
<em>&gt; &gt; &gt;&gt;right
</em><br>
<em>&gt; &gt; &gt;&gt;most
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;of the time and wrong probably only once or right all the time (o
</em><br>
<em>&gt; &gt; &gt;&gt;dear,
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;heresy).
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;That doesn't mean it isn't worth trying. By analogy, think of AGI
</em><br>
<em>&gt; &gt; &gt;&gt;developers
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;as individual sperm trying to reach the egg. The odds of any
</em><br>
<em>&gt; &gt; &gt;&gt;individual
</em><br>
<em>&gt; &gt; &gt;&gt;are
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;incredibly small but the reward is so good it would be a shame 
</em><br>
<em>&gt;not
</em><br>
<em>&gt; &gt; to
</em><br>
<em>&gt; &gt; &gt;&gt;try.
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;Also, FAI has to be developed only once for all to benefit.
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;_________________________________________________________________
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;MSN(r) Calendar keeps you organized and takes the effort out of
</em><br>
<em>&gt; &gt; &gt;&gt;scheduling
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;get-togethers.
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt;
</em><br>
<em>&gt; &gt; &gt;&gt;
</em><br>
<em>&gt; &gt; &gt;<a href="http://join.msn.com/?pgmarket=en">http://join.msn.com/?pgmarket=en</a>-
</em><br>
<em>&gt; &gt; ca&amp;page=byoa/prem&amp;xAPID=1994&amp;DI=1034&amp;SU=http
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; ://hotmail.com/enca&amp;HL=Market_MSNIS_Taglines
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;  Start enjoying all the benefits of MSN(r) Premium right now and
</em><br>
<em>&gt; &gt; get
</em><br>
<em>&gt; &gt; &gt;&gt;the
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt; &gt;first two months FREE*.
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt;
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt;
</em><br>
<em>&gt; &gt; &gt;&gt; &gt;&gt;
</em><br>
<em>&gt; &gt; &gt;&gt;
</em><br>
<em>&gt; &gt; &gt;&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14092.html">dave last: "Re: Singularity Institute: Likely to win the race to build GAI?"</a>
<li><strong>Previous message:</strong> <a href="14090.html">David Picon Alvarez: "Re: Faith-based thought vs thinkers"</a>
<li><strong>Maybe in reply to:</strong> <a href="14106.html">H C: "RE: Singularity Institute: Likely to win the race to build GAI?"</a>
<!-- nextthread="start" -->
<li><strong>Reply:</strong> <a href="14093.html">Tyler Emerson: "RE: RE: Singularity Institute: Likely to win the race to build GAI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14091">[ date ]</a>
<a href="index.html#14091">[ thread ]</a>
<a href="subject.html#14091">[ subject ]</a>
<a href="author.html#14091">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
