<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendliness not an Add-on</title>
<meta name="Author" content="Marcello Mathias Herreshoff (m@marcello.gotdns.com)">
<meta name="Subject" content="Re: Friendliness not an Add-on">
<meta name="Date" content="2006-02-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendliness not an Add-on</h1>
<!-- received="Mon Feb 20 18:05:31 2006" -->
<!-- isoreceived="20060221010531" -->
<!-- sent="Mon, 20 Feb 2006 17:05:26 -0800" -->
<!-- isosent="20060221010526" -->
<!-- name="Marcello Mathias Herreshoff" -->
<!-- email="m@marcello.gotdns.com" -->
<!-- subject="Re: Friendliness not an Add-on" -->
<!-- id="20060221010526.GA3312@marcello.gotdns.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="638d4e150602190437y1c6256fauc2875526e53488a@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marcello Mathias Herreshoff (<a href="mailto:m@marcello.gotdns.com?Subject=Re:%20Friendliness%20not%20an%20Add-on"><em>m@marcello.gotdns.com</em></a>)<br>
<strong>Date:</strong> Mon Feb 20 2006 - 18:05:26 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14165.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<li><strong>Previous message:</strong> <a href="14163.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<li><strong>In reply to:</strong> <a href="14150.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14165.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<li><strong>Reply:</strong> <a href="14165.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<li><strong>Reply:</strong> <a href="14166.html">BillK: "Re: Friendliness not an Add-on"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14164">[ date ]</a>
<a href="index.html#14164">[ thread ]</a>
<a href="subject.html#14164">[ subject ]</a>
<a href="author.html#14164">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi,
<br>
Because this message is getting long, I'm putting all my responses at the top
<br>
of the message with headers for our convenience.
<br>
<p>--- Rice's Theorem
<br>
Here's the problem.  If you didn't know what algorithm the AI proper was
<br>
using, and you had no log file, you would run up against Rice's theorem here.
<br>
However, we do know what algorithm the AI is using and we might have a log
<br>
file.  These are the only things preventing us from running into Rice's theorem.
<br>
<p>Therefore, this means there must exist some reasonably efficient translation
<br>
algorithm for our AI's algorithm which will take in a conclusion and
<br>
optionally a  log file and output a deductive justification, which can then
<br>
be checked.  But, this is precisely what I meant by verifiable, so you can't
<br>
do that for a non-verifiable architecture.
<br>
<p>If when you said &quot;verifiable&quot; you meant something other than this, then
<br>
what did you mean?
<br>
<p>--- Danger
<br>
A non-verifiable subsystem is more dangerous than a verifiable one, because a
<br>
non-verifiable subsystem would not necessarily be caught if it made a
<br>
mistake, whereas a verifiable subsystem would.
<br>
<p>Further, if a mistake fell into the reasoning involved in constructing the
<br>
next version of the AI it could cause horrible damage.
<br>
<p>If I were given a genie lamp which made a mistake 1/10 of the time, I'd lock
<br>
it in a box and hide it somewhere.  A mistake could mean anything from
<br>
everyone losing all their left shoes to South America turning into a giant
<br>
apple strudel.
<br>
<p>A mistake in the construction of the AI's next version is pretty much a
<br>
random genie wish.
<br>
<p>--- Verifiability
<br>
Given all that's at stake, we must pick the very strongest criteria for
<br>
verifiability.  Should it become apparent that these criteria are too strong
<br>
to make the AI possible to build, they should be scaled back accordingly.
<br>
However, unless or until that happens, we should only be content with the
<br>
very best.
<br>
<p>--- Fluid Analogies
<br>
If I can't provide this level of justification for fluid analogies I won't
<br>
put them in.  Should analogies really be one of the keystones of
<br>
intelligence, and not a special case of something more fundamental, I want to
<br>
understand precisely what they are and be able to see each link in the chain
<br>
reasoning involved in constructing them.
<br>
<p>Until I do that, I won't have really understood what makes analogies tick,
<br>
and have no hope of writing a piece of code which rates them, let alone
<br>
finds good ones.
<br>
<p>--- Chess
<br>
But, I chose the example of chess to show that Alice has to be just as
<br>
creative as Bob.  Suppose, as you claim, that there's some brilliant move
<br>
that Bob will notice and Alice won't.  Now, consider Bob's previous move.
<br>
Alice has to notice the brilliant move, or she won't be able to criticize
<br>
suboptimal moves that do not set up the situation that the really brilliant
<br>
move depended on.
<br>
<p>Real life is a game where the AI gets to take more than one turn.
<br>
<p>--- Conclusion
<br>
I hope I've stated my arguments here sufficiently clearly.
<br>
<p>Remember that if you do end up building a powerful enough AI system, the
<br>
burden of proof regarding its safety lies with you.  If you don't use a nicely
<br>
formalized architecture, this step looks way harder.
<br>
<p>-=+Marcello Mathias Herreshoff
<br>
<p>On Sun, Feb 19, 2006 at 07:37:48AM -0500, Ben Goertzel wrote:
<br>
<em>&gt; About Rice's theorem... Sorry, I did not phrase my argument against
</em><br>
<em>&gt; the relevance of this theorem very carefully.  Here goes again. 
</em><br>
<em>&gt; Hopefully this reformulation is sufficiently precise.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What that theorem says (as you know) is that for any nontrivial
</em><br>
<em>&gt; property P (roughly: any property that holds for some arguments and
</em><br>
<em>&gt; not others) it is impossible to make a program that will tell you, for
</em><br>
<em>&gt; all algorithms A, whether A has property P.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In other words, it says
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It is not true that:
</em><br>
<em>&gt; &lt;snip to other message&gt;
</em><br>
<em>&gt; For all nontrivial properties P
</em><br>
<em>&gt;  { It is not true that
</em><br>
<em>&gt;  { there exists a program Q so that
</em><br>
<em>&gt;  { for all algorithms A
</em><br>
<em>&gt;  { Q will tell you whether A has property P
</em><br>
<em>&gt;  }}}}
</em><br>
<em>&gt; &lt;end snip&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; But a Friendliness verifier does not need to do this.  A Friendliness
</em><br>
<em>&gt; verifier just needs to verify whether
</em><br>
<em>&gt; 
</em><br>
<em>&gt; * a certain class of algorithms A (the ones that it is plausibly
</em><br>
<em>&gt; likely the AI system in question will ultimately self-modify into)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; satisfy
</em><br>
<em>&gt; 
</em><br>
<em>&gt; * a particular property P: Friendliness
</em><br>
<em>&gt;
</em><br>
<em>&gt; The existence of a Friendliness verifier of this nature is certainly
</em><br>
<em>&gt; not ruled out by Rice's Theorem.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The problems are in formulating what is meant by Friendliness, and
</em><br>
<em>&gt; defining the class of algorithms A.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; A log of the complete history of an AI system is not necessary in
</em><br>
<em>&gt; order to define the plausible algorithm-class; this definition may be
</em><br>
<em>&gt; given potentially by a priori knowledge about the nature of the AI
</em><br>
<em>&gt; system in question.
</em><br>
(see Rice's Theorem)
<br>
<em>&gt; &gt; &gt; &gt; To put it less formally, we'd be giving our Friendliness module the use
</em><br>
<em>&gt; &gt; &gt; &gt; of a genie which is somewhat unreliable and whose reliability in any
</em><br>
<em>&gt; &gt; &gt; &gt; particular decision is, for all intents and purposes, difficult to check.
</em><br>
<em>&gt; &gt; &gt; True
</em><br>
<em>&gt; &gt; Right.  Doesn't this stike you as dangerous?
</em><br>
<em>&gt; It strikes me as potentially but not necessarily dangerous -- it all
</em><br>
<em>&gt; depends on the details of the AI architecture.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is not the same as the &quot;AI boxing&quot; issue, in which the AI in the
</em><br>
<em>&gt; box is like a genie giving suggestions to the human out of the box. 
</em><br>
<em>&gt; In that case, the genie is proposed to be potentially a sentient mind
</em><br>
<em>&gt; with its own goals and motivations and with a lot of flexibility of
</em><br>
<em>&gt; behavior.  In the case I'm discussing, the &quot;genie&quot; is a
</em><br>
<em>&gt; hard-to-predict hypothesis-suggester giving suggestions to a logical
</em><br>
<em>&gt; cognition component controlled by a Friendliness verifier.  And the
</em><br>
<em>&gt; hard-to-predict hypothesis-suggester does not not need to be a
</em><br>
<em>&gt; sentient mind on its own: it does not need flexible goals,
</em><br>
<em>&gt; motivations, feelings, or the ability to self-modify in any general
</em><br>
<em>&gt; way.  It just needs to be a specialized learning component, similar in
</em><br>
<em>&gt; some ways to Eliezer's proposed Very Powerful Optimization Process
</em><br>
<em>&gt; used for world-simulation inside his Collective Volition proposal (I'm
</em><br>
<em>&gt; saying that it's similar in being powerful at problem-solving without
</em><br>
<em>&gt; having goals, motivations, feelings or strong self-modification; of
</em><br>
<em>&gt; course the problem being solved by my hard-to-predict
</em><br>
<em>&gt; hypothesis-suggester (hypothesis generation) is quite different than
</em><br>
<em>&gt; the problem being solved by Eliezer's VPOP (future prediction)).
</em><br>
(See Danger)
<br>
<p><em>&gt; &gt; I never said evolutionary programming was &quot;nontraceable&quot;.  What I said was
</em><br>
<em>&gt; &gt; &quot;nonverifiable&quot;.  I am not splitting hairs, as these are completely different
</em><br>
<em>&gt; &gt; things.  No program is nontraceable!  You can just emulate the CPU and
</em><br>
<em>&gt; &gt; observe the contents of the registers and memory at any point in time.  With
</em><br>
<em>&gt; &gt; that said though, you can probably see why this sort of traceability is not
</em><br>
<em>&gt; &gt; good enough.  What needs to be traceable is not the how the bits were
</em><br>
<em>&gt; &gt; shuffled but how the conclusion reached was justified.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; a)
</em><br>
<em>&gt; You have not presented any argument as to why verifiability in this
</em><br>
<em>&gt; sense is needed for Friendliness verification.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; b)
</em><br>
<em>&gt; Your criterion of verifiability seems to me to be unreasonably strong,
</em><br>
<em>&gt; and to effectively rule out all metaphorical and heuristic inference. 
</em><br>
<em>&gt; But maybe I have misunderstood your meaning.
</em><br>
(see Verifiability)
<br>
<p><em>&gt; Please consider the following scenario.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Suppose we have a probabilistic-logical theorem-proving system, which
</em><br>
<em>&gt; arrives at a conclusion.  We can then trace the steps that it took to
</em><br>
<em>&gt; arrive at this conclusion.  But suppose that one of these steps was a
</em><br>
<em>&gt; metaphorical ANALOGY, to some other situation -- a loose and fluid
</em><br>
<em>&gt; analogy, of the sort that humans make all the time but current AI
</em><br>
<em>&gt; reasoning software is bad at making (as Douglas Hofstadter has pointed
</em><br>
<em>&gt; out in detail).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Then, it seems to me that what your verifiability criterion demands is
</em><br>
<em>&gt; not just that the conclusion arrived at through metaphorical analogy
</em><br>
<em>&gt; be checked for correctness and usefulness -- but that a justification
</em><br>
<em>&gt; be given as to why *that particular analogy* was chosen instead of
</em><br>
<em>&gt; some other one.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This means that according to your requirement of verifiability (as I
</em><br>
<em>&gt; understand it) a stochastic method can't be used to grab one among
</em><br>
<em>&gt; many possible analogies for handling a situation.  Instead, according
</em><br>
<em>&gt; to your requirement, some kind of verifiable logical inference needs
</em><br>
<em>&gt; to be used to choose the possible analogy.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In Novamente, right now, the way this kind of thing would be handled
</em><br>
<em>&gt; would be (roughly speaking):
</em><br>
<em>&gt; 
</em><br>
<em>&gt; a) a table would be made of the possible analogies, each one
</em><br>
<em>&gt; quantified with a number indicating its contextual desirability
</em><br>
<em>&gt; 
</em><br>
<em>&gt; b) one of the analogies would be chosen from the table, with a
</em><br>
<em>&gt; probability proportional to the desirability number
</em><br>
<em>&gt; 
</em><br>
<em>&gt; According to your definition of verifiability this is a bad approach
</em><br>
<em>&gt; because of the use of a stochastic selection mechanism in Step b.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; However, I have  my doubts whether it is really possible to achieve
</em><br>
<em>&gt; significant levels of general intelligence under severely finite
</em><br>
<em>&gt; resources without making this kind of stochastic selection in one form
</em><br>
<em>&gt; or another.  (I'm not claiming it is necessary to resort to
</em><br>
<em>&gt; pseudorandom number generation; just that I suspect it's necessary to
</em><br>
<em>&gt; resort to something equally arbitrary for selecting among options in
</em><br>
<em>&gt; cases where there are  many possibly relevant pieces of knowledge in
</em><br>
<em>&gt; memory and not much information to go on regarding which one to use in
</em><br>
<em>&gt; a given inference.)
</em><br>
(see Fluid Analogies)
<br>
<em>&gt; &gt; What I meant was that when B proposes an action, A can either verify that B
</em><br>
<em>&gt; &gt; did the correct thing or point out a flaw in B's choice.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The statment above is sufficient but not necessary to show that A is smarter
</em><br>
<em>&gt; &gt; than B, in the colloquial sence of the phrase.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I find this interpretation of the &quot;smarter&quot; concept very inadequate.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; For instance, suppose I have a collaborator who is more reliable in
</em><br>
<em>&gt; judgment than me but less creative than me.  For sake of concretness,
</em><br>
<em>&gt; let's call this individual by the name &quot;Cassio.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Let A=Ben, B=Cassio
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Now, it may be true that &quot;When  Ben proposes an action, Cassio can
</em><br>
<em>&gt; either verify that Ben proposed the correct thing, or point out a flaw
</em><br>
<em>&gt; in his choice&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This does not necessarily imply that Cassio is smarter than Ben -- it
</em><br>
<em>&gt; may be that Ben is specialized for hypothesis generation and Cassio is
</em><br>
<em>&gt; specialized for quality-verification.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The colloquial notion of &quot;smartness&quot; is not really sufficient for
</em><br>
<em>&gt; discussing situations like this, IMO.
</em><br>
(see Chess)
<br>
<em>&gt; &gt; To illustrate, suppose Alice is helping Bob play chess.  Whenever Bob
</em><br>
<em>&gt; &gt; suggests a move, she always says something like &quot;I agree with your move&quot; or
</em><br>
<em>&gt; &gt; &quot;Yikes!  If you go there, he'll fork your rooks in two moves! You overlooked
</em><br>
<em>&gt; &gt; this move here.&quot; If she can always do this, it should be absolutely clear
</em><br>
<em>&gt; &gt; that Alice is a better chess player than Bob.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes, but I can also imagine two chess masters, where master A was
</em><br>
<em>&gt; better at coming up with bold new ideas and master B was better at
</em><br>
<em>&gt; pointing out subtle flaws in ideas (be they bold new ones or not). 
</em><br>
<em>&gt; These two masters, if they were able to cooperate very closely (e.g.
</em><br>
<em>&gt; through mental telepathy), might be able to play much better than
</em><br>
<em>&gt; either one on their own.  This situation is more like the one at hand.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; (i.e., I think your internal quasirandom selection mechanism has
</em><br>
<em>&gt; chosen a suboptimal analogy here ;-)
</em><br>
(see Chess)
<br>
<em>&gt; 
</em><br>
<em>&gt; This discussion has gotten fairly in-depth, but the crux of it is, I
</em><br>
<em>&gt; don't feel you have made a convincing argument in favor of your point
</em><br>
<em>&gt; that it is implausible-in-principle to add Friendliness on to an AGI
</em><br>
<em>&gt; architecture designed without a detailed theory of Friendliness on
</em><br>
<em>&gt; hand.  I don't feel Eliezer has ever made a convincing argument in
</em><br>
<em>&gt; favor of this point either.  It may be true but you guys seem far from
</em><br>
<em>&gt; demonstrating it...
</em><br>
(see Conclusion)
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14165.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<li><strong>Previous message:</strong> <a href="14163.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<li><strong>In reply to:</strong> <a href="14150.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14165.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<li><strong>Reply:</strong> <a href="14165.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<li><strong>Reply:</strong> <a href="14166.html">BillK: "Re: Friendliness not an Add-on"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14164">[ date ]</a>
<a href="index.html#14164">[ thread ]</a>
<a href="subject.html#14164">[ subject ]</a>
<a href="author.html#14164">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
