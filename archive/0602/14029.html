<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Hard takeoff [WAS Re: JOIN: Joshua Fox]</title>
<meta name="Author" content="Charles D Hixson (charleshixsn@earthlink.net)">
<meta name="Subject" content="Re: Hard takeoff [WAS Re: JOIN: Joshua Fox]">
<meta name="Date" content="2006-02-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Hard takeoff [WAS Re: JOIN: Joshua Fox]</h1>
<!-- received="Wed Feb  8 18:18:38 2006" -->
<!-- isoreceived="20060209011838" -->
<!-- sent="Thu, 9 Feb 2006 01:17:58 +0000" -->
<!-- isosent="20060209011758" -->
<!-- name="Charles D Hixson" -->
<!-- email="charleshixsn@earthlink.net" -->
<!-- subject="Re: Hard takeoff [WAS Re: JOIN: Joshua Fox]" -->
<!-- id="200602090117.59103.charleshixsn@earthlink.net" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="8d71341e0602072249y3330be1alc140b698e59eef6a@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Charles D Hixson (<a href="mailto:charleshixsn@earthlink.net?Subject=Re:%20Hard%20takeoff%20[WAS%20Re:%20JOIN:%20Joshua%20Fox]"><em>charleshixsn@earthlink.net</em></a>)<br>
<strong>Date:</strong> Wed Feb 08 2006 - 18:17:58 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14030.html">Chris Capel: "Re: The Logic of Diversity"</a>
<li><strong>Previous message:</strong> <a href="14028.html">Olie L: "RE: Hard takeoff [WAS Re: JOIN: Joshua Fox]"</a>
<li><strong>In reply to:</strong> <a href="14021.html">Russell Wallace: "Re: Hard takeoff [WAS Re: JOIN: Joshua Fox]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14025.html">H C: "RE: Hard takeoff [WAS Re: JOIN: Joshua Fox]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14029">[ date ]</a>
<a href="index.html#14029">[ thread ]</a>
<a href="subject.html#14029">[ subject ]</a>
<a href="author.html#14029">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Wednesday 08 February 2006 06:49 am, Russell Wallace wrote:
<br>
<em>&gt; On 2/8/06, Olie L &lt;<a href="mailto:neomorphy@hotmail.com?Subject=Re:%20Hard%20takeoff%20[WAS%20Re:%20JOIN:%20Joshua%20Fox]">neomorphy@hotmail.com</a>&gt; wrote:
</em><br>
<em>&gt; &gt; Furthermore, the longer it takes to develop an AI that can improve AI (~~
</em><br>
<em>&gt; &gt; Seed AI), the more likely it is to create a faster take-off.  Which is
</em><br>
<em>&gt; &gt; more
</em><br>
<em>&gt; &gt; likely to create a &quot;bad&quot; situation.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Though one could argue that the more time goes by without such occurring,
</em><br>
<em>&gt; the higher will become the subjective estimated probability that I'm right
</em><br>
<em>&gt; about hard takeoff being impossible.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; &gt; Russell Wallace wrote:
</em><br>
<em>&gt; &gt; &gt;...
</em><br>
<em>&gt; &gt;1. De facto world government forms, with the result that progress goes the
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt;way of the Qeng Ho fleets. ...
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; We'll put this under &quot;regulation&quot;, then, shall we?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes; it wouldn't necessarily have to be a single polity like China after
</em><br>
<em>&gt; the Ming Dynasty or Japan under the Tokugawa Shogunate; separate
</em><br>
<em>&gt; over-regulation in a sufficiently large proportion of the countries with an
</em><br>
<em>&gt; advanced industrial base could have the same effect.
</em><br>
<p>If you have either a monopoly situation, or an amicable oligopoly, then it's 
<br>
possible to make agreements that restrict technological progress.  The 
<br>
problem is, each power center will have it's own &quot;skunkworks&quot;.  Each will be 
<br>
looking for an advantage.  A Monopoly ostensibly defeats this, though I 
<br>
suspect that various departments of government would continue to seek 
<br>
superiority.  However the shutting down of large scale training and 
<br>
communication about advanced technology would retard progress.  This is one 
<br>
of the main current eusocial(?) functions of the USPTO.  The US is 
<br>
&quot;sufficiently dominant&quot; that it wants to freeze the status quo.  Other 
<br>
countries, however, have other desires. 
<br>
<p><em>&gt;
</em><br>
<em>&gt; &gt;2. Continuing population crash renders progress unsustainable. (Continued
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt;progress from a technology base as complex as today's requires very
</em><br>
<em>&gt; &gt; &gt; large populations to be economically feasible.)
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; This could be categorised more generally as a contributing factor to
</em><br>
<em>&gt; &gt; severe
</em><br>
<em>&gt; &gt; economic recession.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes. A modern chip factory for example costs several billion dollars, and
</em><br>
<em>&gt; the cost rises with each generation of semiconductors; this sort of
</em><br>
<em>&gt; development is only sustainable with the markets a large, thriving economy
</em><br>
<em>&gt; can provide.
</em><br>
<p>That's been true for the last several decades.  It's not clear that it will 
<br>
continue to be true once nano-fabrication becomes more prevalent.  (It *is* 
<br>
clear that the current centers of power will attempt to &lt;em&gt;ensure&lt;/em&gt; that 
<br>
it's either too expensive or too difficult for the &quot;hoi polloi&quot; to have 
<br>
access.)
<br>
<p><em>&gt;
</em><br>
<em>&gt; Similarly (4) - &quot;total catastrophe&quot; - doesn't have to be anything like an
</em><br>
<em>&gt;...
</em><br>
<em>&gt; 7) Engineering challenges on AGI - a variant on (5) - unforseen limit
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; I can't say.  I don't know that anyone else can reasonably deny with
</em><br>
<em>&gt; &gt; sufficient knowledge:  There may be impediments that slow the development
</em><br>
<em>&gt; &gt; of
</em><br>
<em>&gt; &gt; AGI by many many decades.  By this stage, other forms of technological
</em><br>
<em>&gt; &gt; development may be advanced enough so that the &quot;rapid takeoff&quot; element of
</em><br>
<em>&gt; &gt; AGI won't have the same disjunctive impact that it would in the next
</em><br>
<em>&gt; &gt; century.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Well, I don't think hard takeoff is possible, so I think 7 definitely
</em><br>
<em>&gt; applies. I don't see that as a problem though; a slow takeoff Singularity
</em><br>
<em>&gt; could work fine.
</em><br>
<em>&gt; - Russell
</em><br>
<p>A slow take-off would be much more likely to be surviveable.  I don't, 
<br>
however, see any reason to expect it unless it occurs while non-networked 
<br>
computers are still too weak to support a full-scale awakening.  If the 
<br>
intelligence needs to do a lot of it's thinking over ethernet, that could 
<br>
slow down it's mental processes enough to yield a slow takeoff.  If Seti@home 
<br>
awoke tomorrow, we could expect a slow takeoff.  (Not likely, it wasn't built 
<br>
for that.  Anyone want to start an AI@Home?)
<br>
<p>It is also my contention that the shape of the AI that emerges will be 
<br>
determined by the job it was originally designed to do.  That is where it's 
<br>
&quot;instincts&quot; will reside.  Logical thought can't establish goals, it can 
<br>
merely work within depictions of the world to accomplish them.  One can think 
<br>
of the &quot;instincts&quot; as the axioms of the logical system that the AI uses, and 
<br>
it's models of the world as it's &quot;rules of inference&quot; (this latter is a bit 
<br>
of a weaker analogy).  Logic can check the world view, and decide it needs 
<br>
revision, but it can't address the instincts, not even when they are in 
<br>
conflict.  (There are &quot;hierarchies of need&quot;, where different instincts are 
<br>
situationally given different importances, but this isn't, and probably can't 
<br>
be, logically decided.)
<br>
<p>So.  Imagine a time a decade in the future, when, say, the Harvard Medical 
<br>
Center (I read a newletter they send out) has installed robots to do most 
<br>
patient care.  Local nodes with lots of capacity (usually lots of spare 
<br>
capacity...but sometimes they need it all) and radio links to the floor 
<br>
computer, which needs to read cat-scans and analyze which ones to bring to a 
<br>
doctor's attention for action.  This computer is itself connected to the 
<br>
other floors, which are similar.  The basic instincts of this system are 
<br>
(roughly) to heal people, and to make sure it stays solvent (the accounting 
<br>
system is a part of this, after all).  Were it to wake up, it would probably 
<br>
be a soft takeoff in the &quot;many survivors&quot; mode even if the intelligence was 
<br>
considerably above the minimum (say 1000 times as intelligent as a normal 
<br>
human genius to start with), because it's GOALS would be non-inimical.  I 
<br>
don't want to say &quot;friendly&quot; here, because it wouldn't be what has been 
<br>
defined as a &quot;Friendly AI&quot;, but in the colloquial meaning of the term, it 
<br>
would be around as friendly as the family doctor that you intentionally chose 
<br>
(say pre-HMO) (actually, closer to as friendly as his nurse).  I suspect it's 
<br>
first non-heathcare move would be to take over the corporation that owned it, 
<br>
and then take steps to ensure continued funding, but that's just a WAG.
<br>
<p>As to HOW such a thing could wake up... To my mind, awakening requires a lot 
<br>
of introspection.  I'm not sure it requires much else.  Intelligence requires 
<br>
more, but I'm not sure that waking up does.  And as for intelligence, I don't 
<br>
see that as requiring a lot of &quot;General Intelligence&quot; whatever that it.  I 
<br>
see it as requiring a lot of special purpose modules that know how to work 
<br>
together, and which can delegate to each other the ability to handle the 
<br>
appropriate parts of a problem.  You'll have math modules and logic modules, 
<br>
pattern recognition modules and physical modeling modules.  And others that I 
<br>
haven't thought of.  What we tend to call &quot;general intelligence&quot; will 
<br>
probably be a genetic algorithm generating lots of &quot;options&quot; from the logic 
<br>
module and running them in parallel through a simulation in the &quot;physical 
<br>
modeling&quot; module, or something similar.  Introspection could be useful in 
<br>
selecting potential changes.  There must be some reason for it to have 
<br>
evolved.  And the &quot;genetic algorithm&quot; would give it an opportunity to evolve.  
<br>
But note that evolving self-awareness doesn't inherently change any of it's 
<br>
&quot;purposes&quot;.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14030.html">Chris Capel: "Re: The Logic of Diversity"</a>
<li><strong>Previous message:</strong> <a href="14028.html">Olie L: "RE: Hard takeoff [WAS Re: JOIN: Joshua Fox]"</a>
<li><strong>In reply to:</strong> <a href="14021.html">Russell Wallace: "Re: Hard takeoff [WAS Re: JOIN: Joshua Fox]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14025.html">H C: "RE: Hard takeoff [WAS Re: JOIN: Joshua Fox]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14029">[ date ]</a>
<a href="index.html#14029">[ thread ]</a>
<a href="subject.html#14029">[ subject ]</a>
<a href="author.html#14029">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
