<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Why playing it safe is the most dangerous thing</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="Re: Why playing it safe is the most dangerous thing">
<meta name="Date" content="2006-02-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Why playing it safe is the most dangerous thing</h1>
<!-- received="Fri Feb 24 04:04:26 2006" -->
<!-- isoreceived="20060224110426" -->
<!-- sent="Fri, 24 Feb 2006 06:04:23 -0500" -->
<!-- isosent="20060224110423" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="Re: Why playing it safe is the most dangerous thing" -->
<!-- id="638d4e150602240304g7acc09fr6c368250f17c26cc@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="1140760615.21732.8.camel@localhost.localdomain" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=Re:%20Why%20playing%20it%20safe%20is%20the%20most%20dangerous%20thing"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Fri Feb 24 2006 - 04:04:23 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14230.html">Keith Henson: "Re: DNA as a measure of brain complexity"</a>
<li><strong>Previous message:</strong> <a href="14228.html">Keith Henson: "Re: ESSAY: Program length, Omega and Friendliness"</a>
<li><strong>In reply to:</strong> <a href="14224.html">Peter de Blanc: "Re: Why playing it safe is the most dangerous thing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14231.html">Philip Goetz: "Re: Why playing it safe is the most dangerous thing"</a>
<li><strong>Reply:</strong> <a href="14231.html">Philip Goetz: "Re: Why playing it safe is the most dangerous thing"</a>
<li><strong>Reply:</strong> <a href="14238.html">Peter de Blanc: "Re: Why playing it safe is the most dangerous thing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14229">[ date ]</a>
<a href="index.html#14229">[ thread ]</a>
<a href="subject.html#14229">[ subject ]</a>
<a href="author.html#14229">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; SIAI is not proposing that the US government or the UN should decide how
</em><br>
<em>&gt; to design a Friendly AI. SIAI is not proposing that &quot;we, as a society&quot;
</em><br>
<em>&gt; should be thinking about how to build a Friendly AI. SIAI is trying to
</em><br>
<em>&gt; build a Friendly AI. Believe it or not, individual human beings are
</em><br>
<em>&gt; capable of thinking intelligently about ethics.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; - ... we must conclude that the SAFEST thing to do is to rush into AI
</em><br>
<em>&gt; &gt; and the Singularity blindly, without pause, before the Powers That Be
</em><br>
<em>&gt; &gt; can control and divert it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't see how committing mass suicide is the safest thing to do.
</em><br>
<p>Peter, two points:
<br>
<p>1)
<br>
Eliezer has sometimes proposed that a Singularity not properly planned
<br>
with regard to Friendly AI is almost certain to lead to human
<br>
extinction.  But this has not been convincingly argued for.  He has
<br>
merely shown why this is a significant possibility.
<br>
<p>2)
<br>
&nbsp;Phil is not really suggesting that rushing into the Singularity
<br>
blindly is the best possible option.  He's merely suggesting that the
<br>
*better-in-principle* options are not very plausible, so that we
<br>
should focus on it because it's by far the highest-probability
<br>
plausible option.
<br>
<p>As I understand it, a caricature of Phil's argument would go something like:
<br>
<p>* If we launch a Singularity before the jerks in power figure out
<br>
what's up, we have a 50/50 or so chance of a good outcome (by the
<br>
Principle of Indifference, since what happens after the Singularity is
<br>
totally opaque to us lesser beings)
<br>
<p>* If we don't launch a Singularity before the jerks in power figure
<br>
out what's up, we have a much lower chance of a good outcome, because
<br>
those jerks are likely to find some way to screw things up
<br>
<p>* The truly better-in-principle approach to the Singularity would
<br>
require a long period of peaceful study and experimentation before
<br>
launching the Singularity: but this is just not feasible because once
<br>
the tech gets to a certain point, the jerks in power will pay people
<br>
to develop it quickly and in an unsafe way
<br>
<p>Specialized to AGI, the argument would go something like:
<br>
<p>-- making provably safe AGI is really hard and will take time X
<br>
-- for a dedicated maverick team to make AGI with unknown safety may
<br>
be easier, and will take time Y
<br>
-- after enough time has passed, some jerks will make unsafe and nasty
<br>
AI; this will take time Z
<br>
<p>If
<br>
<p>Y &lt; Z &lt; X
<br>
<p>then it may be optimal to make AGI with unknown safety.
<br>
<p>I am not putting this forth as my own argument, I am merely trying to
<br>
clarify the argument that was made as I understand it, since it seems
<br>
to have been misunderstood.
<br>
<p>I do not think that you or anyone in the SIAI has ever presented a
<br>
convincing refutation of this argument.  It is certainly not a
<br>
watertight argument but IMO it is at least equally plausible as the
<br>
SIAI perspective (which, as I understand it, holds that an AI not
<br>
strongly engineered for Friendliness will almost certainly be very
<br>
dangerous).
<br>
<p><p>-- Ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14230.html">Keith Henson: "Re: DNA as a measure of brain complexity"</a>
<li><strong>Previous message:</strong> <a href="14228.html">Keith Henson: "Re: ESSAY: Program length, Omega and Friendliness"</a>
<li><strong>In reply to:</strong> <a href="14224.html">Peter de Blanc: "Re: Why playing it safe is the most dangerous thing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14231.html">Philip Goetz: "Re: Why playing it safe is the most dangerous thing"</a>
<li><strong>Reply:</strong> <a href="14231.html">Philip Goetz: "Re: Why playing it safe is the most dangerous thing"</a>
<li><strong>Reply:</strong> <a href="14238.html">Peter de Blanc: "Re: Why playing it safe is the most dangerous thing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14229">[ date ]</a>
<a href="index.html#14229">[ thread ]</a>
<a href="subject.html#14229">[ subject ]</a>
<a href="author.html#14229">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
