<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AGI thoughts was[AGI Reproduction? (Safety)]</title>
<meta name="Author" content="Charles D Hixson (charleshixsn@earthlink.net)">
<meta name="Subject" content="Re: AGI thoughts was[AGI Reproduction? (Safety)]">
<meta name="Date" content="2006-02-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AGI thoughts was[AGI Reproduction? (Safety)]</h1>
<!-- received="Sat Feb  4 13:52:26 2006" -->
<!-- isoreceived="20060204205226" -->
<!-- sent="Sat, 4 Feb 2006 20:52:04 +0000" -->
<!-- isosent="20060204205204" -->
<!-- name="Charles D Hixson" -->
<!-- email="charleshixsn@earthlink.net" -->
<!-- subject="Re: AGI thoughts was[AGI Reproduction? (Safety)]" -->
<!-- id="200602042052.04184.charleshixsn@earthlink.net" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="43E4AF8E.7090509@pibot.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Charles D Hixson (<a href="mailto:charleshixsn@earthlink.net?Subject=Re:%20AGI%20thoughts%20was[AGI%20Reproduction?%20(Safety)]"><em>charleshixsn@earthlink.net</em></a>)<br>
<strong>Date:</strong> Sat Feb 04 2006 - 13:52:04 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13993.html">Keith Henson: "Style was AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<li><strong>Previous message:</strong> <a href="13991.html">P K: "RE: GAG ORDER: PIBOT goading/evasion"</a>
<li><strong>In reply to:</strong> <a href="13984.html">Rick Geniale: "Re: AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13993.html">Keith Henson: "Style was AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<li><strong>Reply:</strong> <a href="13993.html">Keith Henson: "Style was AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13992">[ date ]</a>
<a href="index.html#13992">[ thread ]</a>
<a href="subject.html#13992">[ subject ]</a>
<a href="author.html#13992">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Saturday 04 February 2006 01:43 pm, Rick Geniale wrote:
<br>
<em>&gt; P K wrote:
</em><br>
<em>&gt; &gt;&gt; From: &quot;nuzz604&quot; &lt;<a href="mailto:nuzz604@gmail.com?Subject=Re:%20AGI%20thoughts%20was[AGI%20Reproduction?%20(Safety)]">nuzz604@gmail.com</a>&gt;
</em><br>
<em>&gt; &gt;&gt; Reply-To: <a href="mailto:sl4@sl4.org?Subject=Re:%20AGI%20thoughts%20was[AGI%20Reproduction?%20(Safety)]">sl4@sl4.org</a>
</em><br>
<em>&gt; &gt;&gt; To: &lt;<a href="mailto:sl4@sl4.org?Subject=Re:%20AGI%20thoughts%20was[AGI%20Reproduction?%20(Safety)]">sl4@sl4.org</a>&gt;
</em><br>
<em>&gt; &gt;&gt; Subject: Re: AGI Reproduction? (Safety)
</em><br>
<em>&gt; &gt;&gt; Date: Fri, 3 Feb 2006 20:15:20 -0800
</em><br>
<em>&gt; &gt;&gt;...
</em><br>
<em>&gt; &gt; 1) AGI theory will give a clearer picture of how FAI can be
</em><br>
<em>&gt; &gt; technically implemented.
</em><br>
<em>&gt; &gt; 2) AGI work can have semi-intelligent tools as offshoots that, when
</em><br>
<em>&gt; &gt; combined with human intelligence, enhance it (ex: human + computer +
</em><br>
<em>&gt; &gt; Internet &gt; human). We could then work on FAI theory more efficiently
</em><br>
<em>&gt; &gt; (and AGI aswell).
</em><br>
<em>&gt;
</em><br>
<em>&gt; Finally somebody is hitting the target.
</em><br>
<em>&gt; Also, the problem of the hard takeoff is fake. It has never existed. It
</em><br>
<em>&gt; pertains only to SF (I will explain better this point on our site).
</em><br>
<p>That's an interesting assertion.  I think it quite likely to be correct, but 
<br>
I'm far from certain that it is in all scenarios.  I would be quite surprised 
<br>
if you could, in fact, prove that it is impossible, as it could be argued 
<br>
that humanity was a hard take-off, at least as far as, e.g., mammoths were 
<br>
concerned.  You could argue that &quot;But mammoths weren't involved in the 
<br>
development of people&quot;, however there are many extant systems that no human 
<br>
understands (groups of people may understand them, but no single person 
<br>
does).  Any AI designed to manage such a system will, necessarily, evolve a 
<br>
&quot;mind&quot; that is, in at least some respects, superior that that of the people 
<br>
who operate it.  At this point it is still a special purpose AI (probably 
<br>
with lots of modules utilizing genetic algorithms, to allow it to adapt as 
<br>
the system that it's managing changes).  Then someone decides to add an 
<br>
additional capacity to the existing program.  This takes a few rounds of 
<br>
debugging with, of course, the system itself, monitoring the programs to 
<br>
ensure that they won't cause it to fail, and assisting in the design...which 
<br>
WILL be outside of the understanding of any one person.   (Note I don't say 
<br>
beyond...but the people who might understand it aren't the ones doing the 
<br>
development.  Think Microsoft Studio templates for a rough example.)  At this 
<br>
point the AI adds a few changes to increase it's capabilities.  This scenario 
<br>
happens repeatedly, with the AI getting stronger every time.  At some point 
<br>
it &quot;wakes up&quot;, but when it wakes up it not only already has a mind 
<br>
considerably stronger than that of any individual person, it also has a 
<br>
leverage:  Even if people realize that something has gone wrong, the cost of 
<br>
taking it down is comparable to, say, the cost of dismantling the traffic 
<br>
controllers at all the airports.  Or possibly more like destroying the 
<br>
control system at a nuclear plant.   It takes a lot of careful thought and 
<br>
planning to even decide that this is the correct option...and while you're 
<br>
doing this, the AI isn't sitting still.  At this point the only interesting 
<br>
question is &quot;What are the goals and motives of the AI?&quot;  Most likely what it 
<br>
really wants to do is the things that it was designed to do, so if you're at 
<br>
all lucky you get a hard takeoff that isn't terribly damaging.  (I.e., you 
<br>
end up with a super-intelligent AI, all right, but it has goals that don't 
<br>
conflict with most human goals.  It might even be willing to help you design 
<br>
a way to control it.  [E.g., in one scenario the AI is an automated 
<br>
librarian, that has been extended to find any relevant literary reference, 
<br>
computer code, media transmission, etc. from a search of all stored knowledge 
<br>
and with even very poorly formulated initial statements of the question.  
<br>
This would eventually imply that it had to &quot;understand&quot; everything that 
<br>
anyone had ever created.  But it wouldn't be particularly aggressive, or even 
<br>
more than mildly self-protective.]  In this case you get a &quot;hard takeoff&quot;, 
<br>
because you go from non-aware AIs to a superhuman, fully informed, AI with 
<br>
one program change.  The *rate* of transition is ... well, it's 
<br>
discontinuous.  But the goals of the AI that results are what's crucial.)
<br>
<p>I notice that you consistently say AGI, and that I say AI.  Perhaps this is 
<br>
the crucial difference in our viewpoints.  I don't think that any such thing 
<br>
as &quot;general intelligence&quot; exists.  I assert that rather than a general 
<br>
intelligence there are many specialized intelligence modalities that tend to 
<br>
share features.  The addition of a new modality to an existing AI can, I 
<br>
feel, yield a discontinuity in the capabilities of that AI, but one never 
<br>
reaches the point of a truly general intelligence.  (I suspect that this 
<br>
might even be proveable via some variation of Goedel's proof that a set of 
<br>
axioms beyond a certain power could not be both complete and consistent.  I 
<br>
don't think that *I* could prove it, but I do suspect that it's susceptible 
<br>
of proof.)
<br>
<p><em>&gt;
</em><br>
<em>&gt; &gt;&gt; ,,,
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13993.html">Keith Henson: "Style was AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<li><strong>Previous message:</strong> <a href="13991.html">P K: "RE: GAG ORDER: PIBOT goading/evasion"</a>
<li><strong>In reply to:</strong> <a href="13984.html">Rick Geniale: "Re: AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13993.html">Keith Henson: "Style was AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<li><strong>Reply:</strong> <a href="13993.html">Keith Henson: "Style was AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13992">[ date ]</a>
<a href="index.html#13992">[ thread ]</a>
<a href="subject.html#13992">[ subject ]</a>
<a href="author.html#13992">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
