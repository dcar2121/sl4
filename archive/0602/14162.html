<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendliness not an Add-on</title>
<meta name="Author" content="Charles D Hixson (charleshixsn@earthlink.net)">
<meta name="Subject" content="Re: Friendliness not an Add-on">
<meta name="Date" content="2006-02-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendliness not an Add-on</h1>
<!-- received="Mon Feb 20 16:50:52 2006" -->
<!-- isoreceived="20060220235052" -->
<!-- sent="Mon, 20 Feb 2006 15:50:46 -0800" -->
<!-- isosent="20060220235046" -->
<!-- name="Charles D Hixson" -->
<!-- email="charleshixsn@earthlink.net" -->
<!-- subject="Re: Friendliness not an Add-on" -->
<!-- id="200602201550.46786.charleshixsn@earthlink.net" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="638d4e150602190437y1c6256fauc2875526e53488a@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Charles D Hixson (<a href="mailto:charleshixsn@earthlink.net?Subject=Re:%20Friendliness%20not%20an%20Add-on"><em>charleshixsn@earthlink.net</em></a>)<br>
<strong>Date:</strong> Mon Feb 20 2006 - 16:50:46 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14163.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<li><strong>Previous message:</strong> <a href="14161.html">Christopher Healey: "RE: Think of it as AGI suiciding, not boxing"</a>
<li><strong>In reply to:</strong> <a href="14150.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14163.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<li><strong>Reply:</strong> <a href="14163.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14162">[ date ]</a>
<a href="index.html#14162">[ thread ]</a>
<a href="subject.html#14162">[ subject ]</a>
<a href="author.html#14162">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Sunday 19 February 2006 04:37 am, Ben Goertzel wrote:
<br>
<em>&gt; Hi,
</em><br>
<em>&gt;
</em><br>
<em>&gt; About Rice's theorem... Sorry, I did not phrase my argument against
</em><br>
<em>&gt; the relevance of this theorem very carefully.  Here goes again.
</em><br>
<em>&gt; Hopefully this reformulation is sufficiently precise.
</em><br>
<em>&gt;
</em><br>
<em>&gt; What that theorem says (as you know) is that for any nontrivial
</em><br>
<em>&gt; property P (roughly: any property that holds for some arguments and
</em><br>
<em>&gt; not others) it is impossible to make a program that will tell you, for
</em><br>
<em>&gt; all algorithms A, whether A has property P.
</em><br>
<em>&gt;
</em><br>
<em>&gt; In other words, it says
</em><br>
<em>&gt;
</em><br>
<em>&gt; It is not true that:
</em><br>
<em>&gt; { There exists a program so that
</em><br>
<em>&gt; {For All nontrivial properties P and all algorithms A
</em><br>
<em>&gt; { there exists a program Q that will tell you whether A has property P
</em><br>
<em>&gt; }}}
</em><br>
<em>&gt;
</em><br>
<em>&gt; But a Friendliness verifier does not need to do this.  A Friendliness
</em><br>
<em>&gt; verifier just needs to verify whether
</em><br>
<em>&gt;
</em><br>
<em>&gt; * a certain class of algorithms A (the ones that it is plausibly
</em><br>
<em>&gt; likely the AI system in question will ultimately self-modify into)
</em><br>
<em>&gt;
</em><br>
<em>&gt; satisfy
</em><br>
<em>&gt;
</em><br>
<em>&gt; * a particular property P: Friendliness
</em><br>
<em>&gt;
</em><br>
<em>&gt; The existence of a Friendliness verifier of this nature is certainly
</em><br>
<em>&gt; not ruled out by Rice's Theorem.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The problems are in formulating what is meant by Friendliness, and
</em><br>
<em>&gt; defining the class of algorithms A.
</em><br>
<em>&gt;
</em><br>
<em>&gt; A log of the complete history of an AI system is not necessary in
</em><br>
<em>&gt; order to define the plausible algorithm-class; this definition may be
</em><br>
<em>&gt; given potentially by a priori knowledge about the nature of the AI
</em><br>
<em>&gt; system in question.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; &gt; &gt; To put it less formally, we'd be giving our Friendliness module the
</em><br>
<em>&gt; &gt; &gt; &gt; use of a genie which is somewhat unreliable and whose reliability in
</em><br>
<em>&gt; &gt; &gt; &gt; any particular decision is, for all intents and purposes, difficult
</em><br>
<em>&gt; &gt; &gt; &gt; to check.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; True
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Right.  Doesn't this stike you as dangerous?
</em><br>
<em>&gt;
</em><br>
<em>&gt; It strikes me as potentially but not necessarily dangerous -- it all
</em><br>
<em>&gt; depends on the details of the AI architecture.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is not the same as the &quot;AI boxing&quot; issue, in which the AI in the
</em><br>
<em>&gt; box is like a genie giving suggestions to the human out of the box.
</em><br>
<em>&gt; In that case, the genie is proposed to be potentially a sentient mind
</em><br>
<em>&gt; with its own goals and motivations and with a lot of flexibility of
</em><br>
<em>&gt; behavior.  In the case I'm discussing, the &quot;genie&quot; is a
</em><br>
<em>&gt; hard-to-predict hypothesis-suggester giving suggestions to a logical
</em><br>
<em>&gt; cognition component controlled by a Friendliness verifier.  And the
</em><br>
<em>&gt; hard-to-predict hypothesis-suggester does not not need to be a
</em><br>
<em>&gt; sentient mind on its own: it does not need flexible goals,
</em><br>
<em>&gt; motivations, feelings, or the ability to self-modify in any general
</em><br>
<em>&gt; way.  It just needs to be a specialized learning component, similar in
</em><br>
<em>&gt; some ways to Eliezer's proposed Very Powerful Optimization Process
</em><br>
<em>&gt; used for world-simulation inside his Collective Volition proposal (I'm
</em><br>
<em>&gt; saying that it's similar in being powerful at problem-solving without
</em><br>
<em>&gt; having goals, motivations, feelings or strong self-modification; of
</em><br>
<em>&gt; course the problem being solved by my hard-to-predict
</em><br>
<em>&gt; hypothesis-suggester (hypothesis generation) is quite different than
</em><br>
<em>&gt; the problem being solved by Eliezer's VPOP (future prediction)).
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; I never said evolutionary programming was &quot;nontraceable&quot;.  What I said
</em><br>
<em>&gt; &gt; was &quot;nonverifiable&quot;.  I am not splitting hairs, as these are completely
</em><br>
<em>&gt; &gt; different things.  No program is nontraceable!  You can just emulate the
</em><br>
<em>&gt; &gt; CPU and observe the contents of the registers and memory at any point in
</em><br>
<em>&gt; &gt; time.  With that said though, you can probably see why this sort of
</em><br>
<em>&gt; &gt; traceability is not good enough.  What needs to be traceable is not the
</em><br>
<em>&gt; &gt; how the bits were shuffled but how the conclusion reached was justified.
</em><br>
<em>&gt;
</em><br>
<em>&gt; a)
</em><br>
<em>&gt; You have not presented any argument as to why verifiability in this
</em><br>
<em>&gt; sense is needed for Friendliness verification.
</em><br>
<em>&gt;
</em><br>
<em>&gt; b)
</em><br>
<em>&gt; Your criterion of verifiability seems to me to be unreasonably strong,
</em><br>
<em>&gt; and to effectively rule out all metaphorical and heuristic inference.
</em><br>
<em>&gt; But maybe I have misunderstood your meaning.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Please consider the following scenario.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Suppose we have a probabilistic-logical theorem-proving system, which
</em><br>
<em>&gt; arrives at a conclusion.  We can then trace the steps that it took to
</em><br>
<em>&gt; arrive at this conclusion.  But suppose that one of these steps was a
</em><br>
<em>&gt; metaphorical ANALOGY, to some other situation -- a loose and fluid
</em><br>
<em>&gt; analogy, of the sort that humans make all the time but current AI
</em><br>
<em>&gt; reasoning software is bad at making (as Douglas Hofstadter has pointed
</em><br>
<em>&gt; out in detail).
</em><br>
<em>&gt;
</em><br>
<em>&gt; Then, it seems to me that what your verifiability criterion demands is
</em><br>
<em>&gt; not just that the conclusion arrived at through metaphorical analogy
</em><br>
<em>&gt; be checked for correctness and usefulness -- but that a justification
</em><br>
<em>&gt; be given as to why *that particular analogy* was chosen instead of
</em><br>
<em>&gt; some other one.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This means that according to your requirement of verifiability (as I
</em><br>
<em>&gt; understand it) a stochastic method can't be used to grab one among
</em><br>
<em>&gt; many possible analogies for handling a situation.  Instead, according
</em><br>
<em>&gt; to your requirement, some kind of verifiable logical inference needs
</em><br>
<em>&gt; to be used to choose the possible analogy.
</em><br>
<em>&gt;
</em><br>
<em>&gt; In Novamente, right now, the way this kind of thing would be handled
</em><br>
<em>&gt; would be (roughly speaking):
</em><br>
<em>&gt;
</em><br>
<em>&gt; a) a table would be made of the possible analogies, each one
</em><br>
<em>&gt; quantified with a number indicating its contextual desirability
</em><br>
<em>&gt;
</em><br>
<em>&gt; b) one of the analogies would be chosen from the table, with a
</em><br>
<em>&gt; probability proportional to the desirability number
</em><br>
<em>&gt;
</em><br>
<em>&gt; According to your definition of verifiability this is a bad approach
</em><br>
<em>&gt; because of the use of a stochastic selection mechanism in Step b.
</em><br>
<em>&gt;
</em><br>
<em>&gt; However, I have  my doubts whether it is really possible to achieve
</em><br>
<em>&gt; significant levels of general intelligence under severely finite
</em><br>
<em>&gt; resources without making this kind of stochastic selection in one form
</em><br>
<em>&gt; or another.  (I'm not claiming it is necessary to resort to
</em><br>
<em>&gt; pseudorandom number generation; just that I suspect it's necessary to
</em><br>
<em>&gt; resort to something equally arbitrary for selecting among options in
</em><br>
<em>&gt; cases where there are  many possibly relevant pieces of knowledge in
</em><br>
<em>&gt; memory and not much information to go on regarding which one to use in
</em><br>
<em>&gt; a given inference.)
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; What I meant was that when B proposes an action, A can either verify that
</em><br>
<em>&gt; &gt; B did the correct thing or point out a flaw in B's choice.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The statment above is sufficient but not necessary to show that A is
</em><br>
<em>&gt; &gt; smarter than B, in the colloquial sence of the phrase.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I find this interpretation of the &quot;smarter&quot; concept very inadequate.
</em><br>
<em>&gt;
</em><br>
<em>&gt; For instance, suppose I have a collaborator who is more reliable in
</em><br>
<em>&gt; judgment than me but less creative than me.  For sake of concretness,
</em><br>
<em>&gt; let's call this individual by the name &quot;Cassio.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Let A=Ben, B=Cassio
</em><br>
<em>&gt;
</em><br>
<em>&gt; Now, it may be true that &quot;When  Ben proposes an action, Cassio can
</em><br>
<em>&gt; either verify that Ben proposed the correct thing, or point out a flaw
</em><br>
<em>&gt; in his choice&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; This does not necessarily imply that Cassio is smarter than Ben -- it
</em><br>
<em>&gt; may be that Ben is specialized for hypothesis generation and Cassio is
</em><br>
<em>&gt; specialized for quality-verification.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The colloquial notion of &quot;smartness&quot; is not really sufficient for
</em><br>
<em>&gt; discussing situations like this, IMO.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; To illustrate, suppose Alice is helping Bob play chess.  Whenever Bob
</em><br>
<em>&gt; &gt; suggests a move, she always says something like &quot;I agree with your move&quot;
</em><br>
<em>&gt; &gt; or &quot;Yikes!  If you go there, he'll fork your rooks in two moves! You
</em><br>
<em>&gt; &gt; overlooked this move here.&quot; If she can always do this, it should be
</em><br>
<em>&gt; &gt; absolutely clear that Alice is a better chess player than Bob.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes, but I can also imagine two chess masters, where master A was
</em><br>
<em>&gt; better at coming up with bold new ideas and master B was better at
</em><br>
<em>&gt; pointing out subtle flaws in ideas (be they bold new ones or not).
</em><br>
<em>&gt; These two masters, if they were able to cooperate very closely (e.g.
</em><br>
<em>&gt; through mental telepathy), might be able to play much better than
</em><br>
<em>&gt; either one on their own.  This situation is more like the one at hand.
</em><br>
<em>&gt;
</em><br>
<em>&gt; (i.e., I think your internal quasirandom selection mechanism has
</em><br>
<em>&gt; chosen a suboptimal analogy here ;-)
</em><br>
<em>&gt;
</em><br>
<em>&gt; This discussion has gotten fairly in-depth, but the crux of it is, I
</em><br>
<em>&gt; don't feel you have made a convincing argument in favor of your point
</em><br>
<em>&gt; that it is implausible-in-principle to add Friendliness on to an AGI
</em><br>
<em>&gt; architecture designed without a detailed theory of Friendliness on
</em><br>
<em>&gt; hand.  I don't feel Eliezer has ever made a convincing argument in
</em><br>
<em>&gt; favor of this point either.  It may be true but you guys seem far from
</em><br>
<em>&gt; demonstrating it...
</em><br>
<em>&gt;
</em><br>
<em>&gt; -- Ben
</em><br>
<p>A problem is that for trivial pieces of code it's not even possible to define 
<br>
what friendliness consists of, unless you consider a sort routine performing 
<br>
a sort correctly to be friendly.  Even for most more complex parts, taken in 
<br>
isolation, you won't be able to predict their friendliness. 
<br>
E.g.:  If a module for modeling possible outcomes can't model &quot;unfriendly&quot; 
<br>
outcomes, it won't be able to avoid them, but if it can, then it will be able 
<br>
to generate them as portions of a plan to execute them.  So even at the level 
<br>
were friendliness is unambiguously recognizable it doesn't necessarily make 
<br>
sense to exclude unfriendly thoughts.
<br>
<p>More to the point, I sometimes find myself unable to decide which of two 
<br>
proposed actions would reasonably be considered &quot;friendly&quot; in a larger 
<br>
context (i.e., my personal choices of how to relate to people).  How much 
<br>
coercion is it &quot;friendly&quot; to exert to prevent an alcoholic from drinking?  
<br>
Clearly one shouldn't offer them a drink, that would, in this context, be 
<br>
unfriendly.  Is one required to hide the fact that one has alcoholic drinks 
<br>
on the premises?  If they bring one with them, should one refuse to allow 
<br>
them entry?  What is this &quot;friendliness&quot;?   Practically, I generally choose 
<br>
from self-interest, and refuse to allow them in with alcohol or when 
<br>
obviously having drunk...but is this friendly, or merely selfish?
<br>
<p>What, exactly, *is* this friendliness that we wish our AIs to exhibit?
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14163.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<li><strong>Previous message:</strong> <a href="14161.html">Christopher Healey: "RE: Think of it as AGI suiciding, not boxing"</a>
<li><strong>In reply to:</strong> <a href="14150.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14163.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<li><strong>Reply:</strong> <a href="14163.html">Ben Goertzel: "Re: Friendliness not an Add-on"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14162">[ date ]</a>
<a href="index.html#14162">[ thread ]</a>
<a href="subject.html#14162">[ subject ]</a>
<a href="author.html#14162">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
