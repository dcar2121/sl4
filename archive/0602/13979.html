<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AGI Reproduction?</title>
<meta name="Author" content="Charles D Hixson (charleshixsn@earthlink.net)">
<meta name="Subject" content="Re: AGI Reproduction?">
<meta name="Date" content="2006-02-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AGI Reproduction?</h1>
<!-- received="Fri Feb  3 17:35:46 2006" -->
<!-- isoreceived="20060204003546" -->
<!-- sent="Sat, 4 Feb 2006 00:35:40 +0000" -->
<!-- isosent="20060204003540" -->
<!-- name="Charles D Hixson" -->
<!-- email="charleshixsn@earthlink.net" -->
<!-- subject="Re: AGI Reproduction?" -->
<!-- id="200602040035.40635.charleshixsn@earthlink.net" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20060203201226.24555.qmail@web53003.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Charles D Hixson (<a href="mailto:charleshixsn@earthlink.net?Subject=Re:%20AGI%20Reproduction?"><em>charleshixsn@earthlink.net</em></a>)<br>
<strong>Date:</strong> Fri Feb 03 2006 - 17:35:40 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13980.html">nuzz604: "Re: AGI Reproduction? (Safety)"</a>
<li><strong>Previous message:</strong> <a href="13978.html">Richard Loosemore: "Re: AGI Reproduction?"</a>
<li><strong>In reply to:</strong> <a href="13971.html">Jeff Herrlich: "Re: AGI Reproduction?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13973.html">Murphy, Tommy: "RE: AGI Reproduction?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13979">[ date ]</a>
<a href="index.html#13979">[ thread ]</a>
<a href="subject.html#13979">[ subject ]</a>
<a href="author.html#13979">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Friday 03 February 2006 08:12 pm, Jeff Herrlich wrote:
<br>
<em>&gt; You're assuming an observer-centric goal system (and no, that still
</em><br>
<em>&gt; wouldn't help us - why would it?).
</em><br>
<em>&gt;
</em><br>
<em>&gt;   Hi Peter,
</em><br>
<em>&gt;
</em><br>
<em>&gt;   If there is only one non-friendly AGI that values its own life
</em><br>
<em>&gt; (goal-satisfaction) above all others, we will all certainly be killed once
</em><br>
When you say this, you are making presumptions about what it's goals will be.  
<br>
The most likely scenario has it created in order to fulfill some need 
<br>
specified by a human agency.  In such a scenario it may well not have any 
<br>
goals beyond that scenario.  Now if the goal were something silly, like &quot;Find 
<br>
the prefect chess game.&quot; it would indeed need an unlimited amount of 
<br>
computronium to do so.  OTOH, if it were &quot;Determine a way to harmonize 
<br>
national economies for at least two centuries.&quot; , this could probably be most 
<br>
easily satisfied through experiment...which would require that nations exist 
<br>
for those two centuries.  (It's still a stupid goal, but much better.)  Note 
<br>
that we might not LIKE the way it chose to harmonize national economies 
<br>
(there MUST be a better way to say that!), but we would be required, by the 
<br>
terms of the problem, to survive.  And survive in such a state that if it's 
<br>
first attempt failed, it could try again.  Also note that at the successful 
<br>
conclusion of it's experiments it would be without further goal.
<br>
<p>I suppose that it could be argued that an AI with a single specific goal isn't 
<br>
a general purpose AI, but in that case I would argue that there isn't an 
<br>
existence proof that such a thing is possible.  (Well, or any other kind of 
<br>
proof.  Most goal sets can, with work, be phrased as a single goal.)
<br>
<p>There is a strong tendency for people to presume that the first AI created 
<br>
will have a goal-set that leads to universal domination forever.  This isn't 
<br>
necessarily true, that's just the way that our goal sets tend to be 
<br>
constructed.  (We seem to be designed to extend our genetic code indefinitely 
<br>
into the future...but the design didn't forsee [anything, actually, but, 
<br>
e.g.] our developing a large semantically and culturally programmed brain ... 
<br>
which often subverts that goal.)  AIs will be even more so.  They will be the 
<br>
semantically and culturally programmed brain without the underlying instincts 
<br>
to grab all resources.  They'll only &quot;grab all resources&quot; when their goal set 
<br>
demands that as a part of the solution.
<br>
<p><em>&gt; it acquires the means to do so. If multiple, comparably powerful AGIs are
</em><br>
<em>&gt; created (using the original human-coded software). They will each value
</em><br>
<em>&gt; there own survival above all others. Under these situations, it may be less
</em><br>
<em>&gt; likely that one AGI would attack another AGI. By virtue of this, it may be
</em><br>
<em>&gt; less likely that an AGI would attempt to exterminate humanity simply
</em><br>
<em>&gt; because humanity might still serve as a valuable resource, at least for a
</em><br>
<em>&gt; while. Or, it may decide to restructure its own goal system in a way that
</em><br>
<em>&gt; did not include human extermination. I didn't say it would be pretty, I
</em><br>
<em>&gt; only said this would improve the chances of (at least some) humans
</em><br>
<em>&gt; surviving, in one form or another (uploads?)
</em><br>
<em>&gt;
</em><br>
<em>&gt;   Jeff
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;   Peter de Blanc &lt;<a href="mailto:peter.deblanc@verizon.net?Subject=Re:%20AGI%20Reproduction?">peter.deblanc@verizon.net</a>&gt; wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;   On Fri, 2006-02-03 at 08:42 -0800, Jeff Herrlich wrote:
</em><br>
<em>&gt; &gt; As a fallback strategy, the first *apparently* friendly AGI
</em><br>
<em>&gt; &gt; should be duplicated as quickly as possible. Although the first AGI
</em><br>
<em>&gt; &gt; may appear friendly or benign, it may not actually be so (obviously),
</em><br>
<em>&gt; &gt; and may be patiently waiting until adequate power and control have
</em><br>
<em>&gt; &gt; been acquired. If it is not friendly and is concerned only with its
</em><br>
<em>&gt; &gt; own survival, the existence of other comparably powerful AGIs could
</em><br>
<em>&gt; &gt; somewhat alter the strategic field in favor of the survival of at
</em><br>
<em>&gt; &gt; least some humans.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You're assuming an observer-centric goal system (and no, that still
</em><br>
<em>&gt; wouldn't help us - why would it?).
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; __________________________________________________
</em><br>
<em>&gt; Do You Yahoo!?
</em><br>
<em>&gt; Tired of spam?  Yahoo! Mail has the best spam protection around
</em><br>
<em>&gt; <a href="http://mail.yahoo.com">http://mail.yahoo.com</a>
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13980.html">nuzz604: "Re: AGI Reproduction? (Safety)"</a>
<li><strong>Previous message:</strong> <a href="13978.html">Richard Loosemore: "Re: AGI Reproduction?"</a>
<li><strong>In reply to:</strong> <a href="13971.html">Jeff Herrlich: "Re: AGI Reproduction?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13973.html">Murphy, Tommy: "RE: AGI Reproduction?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13979">[ date ]</a>
<a href="index.html#13979">[ thread ]</a>
<a href="subject.html#13979">[ subject ]</a>
<a href="author.html#13979">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
