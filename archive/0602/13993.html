<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Style was AGI thoughts was[AGI Reproduction? (Safety)]</title>
<meta name="Author" content="Keith Henson (hkhenson@rogers.com)">
<meta name="Subject" content="Style was AGI thoughts was[AGI Reproduction? (Safety)]">
<meta name="Date" content="2006-02-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Style was AGI thoughts was[AGI Reproduction? (Safety)]</h1>
<!-- received="Sun Feb  5 10:08:42 2006" -->
<!-- isoreceived="20060205170842" -->
<!-- sent="Sun, 05 Feb 2006 12:11:48 -0500" -->
<!-- isosent="20060205171148" -->
<!-- name="Keith Henson" -->
<!-- email="hkhenson@rogers.com" -->
<!-- subject="Style was AGI thoughts was[AGI Reproduction? (Safety)]" -->
<!-- id="5.1.0.14.0.20060205114050.04f4c048@pop.bloor.is.net.cable.rogers.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="200602042052.04184.charleshixsn@earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Keith Henson (<a href="mailto:hkhenson@rogers.com?Subject=Re:%20Style%20was%20AGI%20thoughts%20was[AGI%20Reproduction?%20(Safety)]"><em>hkhenson@rogers.com</em></a>)<br>
<strong>Date:</strong> Sun Feb 05 2006 - 10:11:48 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13994.html">Peter de Blanc: "Re: Style was AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<li><strong>Previous message:</strong> <a href="13992.html">Charles D Hixson: "Re: AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<li><strong>In reply to:</strong> <a href="13992.html">Charles D Hixson: "Re: AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13994.html">Peter de Blanc: "Re: Style was AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<li><strong>Reply:</strong> <a href="13994.html">Peter de Blanc: "Re: Style was AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13993">[ date ]</a>
<a href="index.html#13993">[ thread ]</a>
<a href="subject.html#13993">[ subject ]</a>
<a href="author.html#13993">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 08:52 PM 2/4/2006 +0000, Charles D Hixson wrote:
<br>
<p>snip
<br>
<p>This is good, but I have a readability request, more paragraph breaks.
<br>
<p><em>&gt;That's an interesting assertion.  I think it quite likely to be correct, but
</em><br>
<em>&gt;I'm far from certain that it is in all scenarios.  I would be quite surprised
</em><br>
<em>&gt;if you could, in fact, prove that it is impossible, as it could be argued
</em><br>
<em>&gt;that humanity was a hard take-off, at least as far as, e.g., mammoths were
</em><br>
<em>&gt;concerned.
</em><br>
<p><em>&gt;You could argue that &quot;But mammoths weren't involved in the
</em><br>
<em>&gt;development of people&quot;, however there are many extant systems that no human
</em><br>
<em>&gt;understands (groups of people may understand them, but no single person
</em><br>
<em>&gt;does).
</em><br>
<p><em>&gt;Any AI designed to manage such a system will, necessarily, evolve a
</em><br>
<em>&gt;&quot;mind&quot; that is, in at least some respects, superior that that of the people
</em><br>
<em>&gt;who operate it.  At this point it is still a special purpose AI (probably
</em><br>
<em>&gt;with lots of modules utilizing genetic algorithms, to allow it to adapt as
</em><br>
<em>&gt;the system that it's managing changes).
</em><br>
<p><em>&gt;Then someone decides to add an
</em><br>
<em>&gt;additional capacity to the existing program.  This takes a few rounds of
</em><br>
<em>&gt;debugging with, of course, the system itself, monitoring the programs to
</em><br>
<em>&gt;ensure that they won't cause it to fail, and assisting in the design...which
</em><br>
<em>&gt;WILL be outside of the understanding of any one person.   (Note I don't say
</em><br>
<em>&gt;beyond...but the people who might understand it aren't the ones doing the
</em><br>
<em>&gt;development.  Think Microsoft Studio templates for a rough example.)
</em><br>
<p><em>&gt;At this
</em><br>
<em>&gt;point the AI adds a few changes to increase it's capabilities.  This scenario
</em><br>
<em>&gt;happens repeatedly, with the AI getting stronger every time.  At some point
</em><br>
<em>&gt;it &quot;wakes up&quot;, but when it wakes up it not only already has a mind
</em><br>
<em>&gt;considerably stronger than that of any individual person, it also has a
</em><br>
<em>&gt;leverage:  Even if people realize that something has gone wrong, the cost of
</em><br>
<em>&gt;taking it down is comparable to, say, the cost of dismantling the traffic
</em><br>
<em>&gt;controllers at all the airports.  Or possibly more like destroying the
</em><br>
<em>&gt;control system at a nuclear plant.
</em><br>
<p><em>&gt;It takes a lot of careful thought and
</em><br>
<em>&gt;planning to even decide that this is the correct option...and while you're
</em><br>
<em>&gt;doing this, the AI isn't sitting still.  At this point the only interesting
</em><br>
<em>&gt;question is &quot;What are the goals and motives of the AI?&quot;
</em><br>
<p><em>&gt;Most likely what it
</em><br>
<em>&gt;really wants to do is the things that it was designed to do, so if you're at
</em><br>
<em>&gt;all lucky you get a hard takeoff that isn't terribly damaging.  (I.e., you
</em><br>
<em>&gt;end up with a super-intelligent AI, all right, but it has goals that don't
</em><br>
<em>&gt;conflict with most human goals.
</em><br>
<p><em>&gt;It might even be willing to help you design
</em><br>
<em>&gt;a way to control it.  [E.g., in one scenario the AI is an automated
</em><br>
<em>&gt;librarian, that has been extended to find any relevant literary reference,
</em><br>
<em>&gt;computer code, media transmission, etc. from a search of all stored knowledge
</em><br>
<em>&gt;and with even very poorly formulated initial statements of the question.
</em><br>
<em>&gt;This would eventually imply that it had to &quot;understand&quot; everything that
</em><br>
<em>&gt;anyone had ever created.  But it wouldn't be particularly aggressive, or even
</em><br>
<em>&gt;more than mildly self-protective.]
</em><br>
<p><em>&gt;In this case you get a &quot;hard takeoff&quot;,
</em><br>
<em>&gt;because you go from non-aware AIs to a superhuman, fully informed, AI with
</em><br>
<em>&gt;one program change.  The *rate* of transition is ... well, it's
</em><br>
<em>&gt;discontinuous.  But the goals of the AI that results are what's crucial.)
</em><br>
<em>&gt;
</em><br>
<em>&gt;I notice that you consistently say AGI, and that I say AI.  Perhaps this is
</em><br>
<em>&gt;the crucial difference in our viewpoints.  I don't think that any such thing
</em><br>
<em>&gt;as &quot;general intelligence&quot; exists.  I assert that rather than a general
</em><br>
<em>&gt;intelligence there are many specialized intelligence modalities that tend to
</em><br>
<em>&gt;share features.
</em><br>
<p>While I see your point, and there is no doubt humans have *many* 
<br>
specialized  brain modules, I think there is such a thing as &quot;general 
<br>
intelligence.&quot;  It is what we use when we don't have a specialized module 
<br>
to deal with some problem.
<br>
<p>It also isn't very good compared to the specialized modules.
<br>
<p>We can see this in autistic people--some of whom have high general 
<br>
intelligence and seem to be lacking some of the modules we use for social 
<br>
interactions.  Using GI in place of specialized modules is really 
<br>
clunky.  But if GI is being used to solve a problem never faced before , it 
<br>
is better than having noting at all.
<br>
<p><em>&gt;The addition of a new modality to an existing AI can, I
</em><br>
<em>&gt;feel, yield a discontinuity in the capabilities of that AI, but one never
</em><br>
<em>&gt;reaches the point of a truly general intelligence.  (I suspect that this
</em><br>
<em>&gt;might even be proveable via some variation of Goedel's proof that a set of
</em><br>
<em>&gt;axioms beyond a certain power could not be both complete and consistent.  I
</em><br>
<em>&gt;don't think that *I* could prove it, but I do suspect that it's susceptible
</em><br>
<em>&gt;of proof.)
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13994.html">Peter de Blanc: "Re: Style was AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<li><strong>Previous message:</strong> <a href="13992.html">Charles D Hixson: "Re: AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<li><strong>In reply to:</strong> <a href="13992.html">Charles D Hixson: "Re: AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13994.html">Peter de Blanc: "Re: Style was AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<li><strong>Reply:</strong> <a href="13994.html">Peter de Blanc: "Re: Style was AGI thoughts was[AGI Reproduction? (Safety)]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13993">[ date ]</a>
<a href="index.html#13993">[ thread ]</a>
<a href="subject.html#13993">[ subject ]</a>
<a href="author.html#13993">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
