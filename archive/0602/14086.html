<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Singularity Institute: Likely to win the race to build GAI?</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="Re: Singularity Institute: Likely to win the race to build GAI?">
<meta name="Date" content="2006-02-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Singularity Institute: Likely to win the race to build GAI?</h1>
<!-- received="Tue Feb 14 17:37:08 2006" -->
<!-- isoreceived="20060215003708" -->
<!-- sent="Tue, 14 Feb 2006 19:37:06 -0500" -->
<!-- isosent="20060215003706" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="Re: Singularity Institute: Likely to win the race to build GAI?" -->
<!-- id="638d4e150602141637l279b3d1bia74a3767c4a49d72@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="440D165E@zathras" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=Re:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Tue Feb 14 2006 - 17:37:06 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14087.html">Charles D Hixson: "Re: 'a process of non-thinking called faith'"</a>
<li><strong>Previous message:</strong> <a href="14085.html">pdugan: "RE: Singularity Institute: Likely to win the race to build GAI?"</a>
<li><strong>In reply to:</strong> <a href="14085.html">pdugan: "RE: Singularity Institute: Likely to win the race to build GAI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14094.html">Kaj Sotala: "Re: Singularity Institute: Likely to win the race to build GAI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14086">[ date ]</a>
<a href="index.html#14086">[ thread ]</a>
<a href="subject.html#14086">[ subject ]</a>
<a href="author.html#14086">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi,
<br>
<p>About Novamente and &quot;hard takeoff&quot;
<br>
<p>I find the hard vs. soft takeoff terminology somewhat imprecise.  Or, at
<br>
least, it may be precise as used by some individuals, but some individuals
<br>
use it imprecisely, and among those who use it precisely, there is not that
<br>
much consistency in the meaning.
<br>
<p>I think that a hard takeoff could occur with a Novamente system -- but only
<br>
once the system had achieved a certain, clearly recognizable, point of
<br>
maturity.
<br>
<p>There will be a period of &quot;soft&quot;, gradual cognitive development as we raise
<br>
the system through its virtual toddlerhood and childhood. At this stage it
<br>
will not have strong self-modifying capabilities, nor enough understanding
<br>
of computing, mathematics and cognitive science to usefully revise its own
<br>
code.  Hard takeoff in this phase is extremely unlikely.
<br>
<p>Then (assuming we are comfortable with the related, difficult Friendliness
<br>
issues -- but that's another topic, which I won't digress to in this
<br>
message) there will be a point at which we allow the system to modify its
<br>
own source-code, once it understands a lot of supporting science.  After
<br>
this point, a hard takeoff is possible at any time... though certainly not
<br>
guaranteed.  We really don't have enough knowledge right now to say how fast
<br>
progress will be at such a stage.
<br>
<p>-- Ben G
<br>
<p><p>On 2/14/06, pdugan &lt;<a href="mailto:pdugan@vt.edu?Subject=Re:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">pdugan@vt.edu</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; Well I'd say its worth evaluating the prospective Friendliness of these
</em><br>
<em>&gt; systems, for the obvious reasons. This is probably fairly difficult to do,
</em><br>
<em>&gt; particularly for projects based on proprietary information. I think a
</em><br>
<em>&gt; useful
</em><br>
<em>&gt; hueristic when gauging the risks associated with an AGI is to evaluate the
</em><br>
<em>&gt; likelyhood of a hard take-off. From what I gather about Novaemente, you
</em><br>
<em>&gt; seem
</em><br>
<em>&gt; to see soft take-off as much more likely. If Novamente does prove robust
</em><br>
<em>&gt; enough to be deemed a &quot;general intelligence&quot; would it possible for someone
</em><br>
<em>&gt; else, possibly SIAI, to conceive of a more &quot;powerful&quot; system that enganges
</em><br>
<em>&gt; in
</em><br>
<em>&gt; hard take-off while Novamente spends its &quot;childhood&quot;? Or one the other
</em><br>
<em>&gt; hand,
</em><br>
<em>&gt; what sort of Friendliness constraints does Novamente possess?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Patrick
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;===== Original Message From <a href="mailto:ben@goertzel.org?Subject=Re:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">ben@goertzel.org</a> =====
</em><br>
<em>&gt; &gt;In fact I know of a number of individuals/groups in addition to myself
</em><br>
<em>&gt; &gt;who fall into this category (significant progress made toward
</em><br>
<em>&gt; &gt;realizing a software implementation whose design has apparent AGI
</em><br>
<em>&gt; &gt;potential), though I'm not sure which of them are list members.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;In addition to my Novamente project (www.novamente.net), I would
</em><br>
<em>&gt; &gt;mention Steve Omohundro
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;<a href="http://home.att.net/~om3/selfawaresystems.html">http://home.att.net/~om3/selfawaresystems.html</a>
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;(who is working on a self-modifying AI system using his own variant of
</em><br>
<em>&gt; &gt;Bayesian learning) and James Rogers with his
</em><br>
<em>&gt; &gt;algorithmic-information-theory related AGI design (James is a list
</em><br>
<em>&gt; &gt;member, but his work has been kept sufficiently proprietary that I
</em><br>
<em>&gt; &gt;can't say much about it).  There are many others as well...
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;Based on crude considerations, it would seem SIAI is nowhere near the
</em><br>
<em>&gt; &gt;most advanced group on the path toward an AGI implementation.  On the
</em><br>
<em>&gt; &gt;other hand, it's of course possible that those of us who are &quot;further
</em><br>
<em>&gt; &gt;along&quot; all have wrong ideas (though I doubt it!) and SIAI will come up
</em><br>
<em>&gt; &gt;with the right idea in 2008 or whenever and then proceed rapidly
</em><br>
<em>&gt; &gt;toward the end goal.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;ben
</em><br>
<em>&gt; &gt;ben
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;On 2/14/06, pdugan &lt;<a href="mailto:pdugan@vt.edu?Subject=Re:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">pdugan@vt.edu</a>&gt; wrote:
</em><br>
<em>&gt; &gt;&gt; There is a certain list member who already has an AGI model more than
</em><br>
<em>&gt; half
</em><br>
<em>&gt; &gt;&gt; implemented, making it a few years from testablility to see if it
</em><br>
<em>&gt; classifies
</em><br>
<em>&gt; &gt;&gt; as a genuine AGI, and if so then maybe another half a decade before
</em><br>
<em>&gt; something
</em><br>
<em>&gt; &gt;&gt; like recursive self-improvement becomes possible.
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt;   Patrick
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt; &gt;===== Original Message From P K &lt;<a href="mailto:kpete1@hotmail.com?Subject=Re:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">kpete1@hotmail.com</a>&gt; =====
</em><br>
<em>&gt; &gt;&gt; &gt;&gt;Yes, I know that they are working on _Friendly_ GAI. But my question
</em><br>
<em>&gt; is:
</em><br>
<em>&gt; &gt;&gt; &gt;&gt;What reason is there to think that the Institute has any real chance
</em><br>
<em>&gt; of
</em><br>
<em>&gt; &gt;&gt; &gt;&gt;winning the race to General Artificial Intelligence of any sort,
</em><br>
<em>&gt; beating
</em><br>
<em>&gt; &gt;&gt; &gt;&gt;out those thousands of very smart GAI researchers?
</em><br>
<em>&gt; &gt;&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt; &gt;There is no particular reason(s) I can think of that make the
</em><br>
<em>&gt; Institute
</em><br>
<em>&gt; more
</em><br>
<em>&gt; &gt;&gt; &gt;likely to develop AGI than any other organization with skilled
</em><br>
<em>&gt; developers.
</em><br>
<em>&gt; &gt;&gt; &gt;It's all a fog. The only way to see if their ideas have any merit is
</em><br>
<em>&gt; to
</em><br>
<em>&gt; try
</em><br>
<em>&gt; &gt;&gt; &gt;them out. Also, I suspect their donations would increase if they
</em><br>
<em>&gt; showed
</em><br>
<em>&gt; some
</em><br>
<em>&gt; &gt;&gt; &gt;proofs of concept. It's all speculative at this point.
</em><br>
<em>&gt; &gt;&gt; &gt;
</em><br>
<em>&gt; &gt;&gt; &gt;As for predicting success or failure, the best calibrated answer is to
</em><br>
<em>&gt; &gt;&gt; &gt;predict failure to anyone attempting to build a GAI. You would be
</em><br>
<em>&gt; right
</em><br>
<em>&gt; most
</em><br>
<em>&gt; &gt;&gt; &gt;of the time and wrong probably only once or right all the time (o
</em><br>
<em>&gt; dear,
</em><br>
<em>&gt; &gt;&gt; &gt;heresy).
</em><br>
<em>&gt; &gt;&gt; &gt;
</em><br>
<em>&gt; &gt;&gt; &gt;That doesn't mean it isn't worth trying. By analogy, think of AGI
</em><br>
<em>&gt; developers
</em><br>
<em>&gt; &gt;&gt; &gt;as individual sperm trying to reach the egg. The odds of any
</em><br>
<em>&gt; individual
</em><br>
<em>&gt; are
</em><br>
<em>&gt; &gt;&gt; &gt;incredibly small but the reward is so good it would be a shame not to
</em><br>
<em>&gt; try.
</em><br>
<em>&gt; &gt;&gt; &gt;Also, FAI has to be developed only once for all to benefit.
</em><br>
<em>&gt; &gt;&gt; &gt;
</em><br>
<em>&gt; &gt;&gt; &gt;_________________________________________________________________
</em><br>
<em>&gt; &gt;&gt; &gt;MSN(r) Calendar keeps you organized and takes the effort out of
</em><br>
<em>&gt; scheduling
</em><br>
<em>&gt; &gt;&gt; &gt;get-togethers.
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; <a href="http://join.msn.com/?pgmarket=en-ca&amp;page=byoa/prem&amp;xAPID=1994&amp;DI=1034&amp;SU=http">http://join.msn.com/?pgmarket=en-ca&amp;page=byoa/prem&amp;xAPID=1994&amp;DI=1034&amp;SU=http</a>gularity Institute: Likely to win the race to build GAI?</em><br>
<em>&gt; &gt;&gt; ://hotmail.com/enca&amp;HL=Market_MSNIS_Taglines
</em><br>
<em>&gt; &gt;&gt; &gt;  Start enjoying all the benefits of MSN(r) Premium right now and get
</em><br>
<em>&gt; the
</em><br>
<em>&gt; &gt;&gt; &gt;first two months FREE*.
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14087.html">Charles D Hixson: "Re: 'a process of non-thinking called faith'"</a>
<li><strong>Previous message:</strong> <a href="14085.html">pdugan: "RE: Singularity Institute: Likely to win the race to build GAI?"</a>
<li><strong>In reply to:</strong> <a href="14085.html">pdugan: "RE: Singularity Institute: Likely to win the race to build GAI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14094.html">Kaj Sotala: "Re: Singularity Institute: Likely to win the race to build GAI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14086">[ date ]</a>
<a href="index.html#14086">[ thread ]</a>
<a href="subject.html#14086">[ subject ]</a>
<a href="author.html#14086">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
