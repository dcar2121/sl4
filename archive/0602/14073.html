<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Fundamental problems</title>
<meta name="Author" content="Mitchell Porter (mitchtemporarily@hotmail.com)">
<meta name="Subject" content="Fundamental problems">
<meta name="Date" content="2006-02-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Fundamental problems</h1>
<!-- received="Tue Feb 14 06:20:51 2006" -->
<!-- isoreceived="20060214132051" -->
<!-- sent="Tue, 14 Feb 2006 13:20:48 +0000" -->
<!-- isosent="20060214132048" -->
<!-- name="Mitchell Porter" -->
<!-- email="mitchtemporarily@hotmail.com" -->
<!-- subject="Fundamental problems" -->
<!-- id="BAY103-F17B9A93FD3E7B94650AA7CCA060@phx.gbl" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Mitchell Porter (<a href="mailto:mitchtemporarily@hotmail.com?Subject=Re:%20Fundamental%20problems"><em>mitchtemporarily@hotmail.com</em></a>)<br>
<strong>Date:</strong> Tue Feb 14 2006 - 06:20:48 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14074.html">Joshua Fox: "Re: Singularity Institute: Likely to win the race to build GAI?"</a>
<li><strong>Previous message:</strong> <a href="14072.html">David Picon Alvarez: "Re: Faith-based thought vs thinkers"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14076.html">Eliezer S. Yudkowsky: "Re: Fundamental problems"</a>
<li><strong>Reply:</strong> <a href="14076.html">Eliezer S. Yudkowsky: "Re: Fundamental problems"</a>
<li><strong>Reply:</strong> <a href="14077.html">Jeff Medina: "Re: Fundamental problems"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14073">[ date ]</a>
<a href="index.html#14073">[ thread ]</a>
<a href="subject.html#14073">[ subject ]</a>
<a href="author.html#14073">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer has posted a job notice at the SIAI website, looking
<br>
for research partners to tackle the problem of rigorously
<br>
ensuring AI goal stability under self-enhancement transformations.
<br>
I would like to see this problem (or perhaps a more refined one)
<br>
stated in the rigorous terms of theoretical computer science; and
<br>
I'd like to see this list try to generate such a formulation.
<br>
<p>There are several reasons why one might wish to refine the
<br>
problem's specification, even at an informal level. Clearly, one
<br>
can achieve goal stability by committing to an architecture
<br>
incapable of modifying its goal-setting components. This is an
<br>
insufficiently general solution for Eliezer's purposes, but he does
<br>
not specify exactly how broad the solution is supposed to be,
<br>
and perhaps he can't. Ideally, one should want a theory of
<br>
Friendliness which can say something (even if only 'irredeemably
<br>
unsafe, do not use') for all possible architectures. More on this
<br>
below.
<br>
<p>What follows has no actual theory. This is mostly a
<br>
methodological preamble. There's a little philosophy, a little
<br>
polemic, some general thoughts on pragmatic strategy, and
<br>
(if I get that far) some actual theory. It's also rather repetitive,
<br>
but I'm shooting this off as-is in order to kickstart some
<br>
discussion. A rough synopsis:
<br>
<p>1. A characteristic statement from me, of skepticism about
<br>
prevailing philosophies of mind and matter, implying that current
<br>
science cannot *even in principle* understand all the issues of
<br>
the Singularity.
<br>
<p>2. But unFriendly RPOPs are an important sub-issue, this is what
<br>
SIAI proposes to deal with, and I endorse their approach, insofar
<br>
as I understand it to be a call for rigor.
<br>
<p>3. Some thoughts on the importance of *general* Friendliness
<br>
theory, as opposed to architecture-specific Friendliess theory.
<br>
<p>So first, the philosophical statement of principle. I do not believe
<br>
that thought is (solely) computation, or that artificial intelligence
<br>
is actually intelligence. I regard the computational description of
<br>
mind and the mathematical description of physical nature as
<br>
ontologically impoverished; they provide a causal account of
<br>
formal changes of state, but say nothing about the 'substance'
<br>
of those states. To do that, one must do phenomenology,
<br>
epistemology, and ontology at a level more profound than
<br>
natural sciences permits. I would point towards Husserl's notion
<br>
of 'eidetic sciences' besides logic and mathematics as the way
<br>
forward.
<br>
<p>The stipulation that mind is more than computation implies that
<br>
the combination of physics and computer science simply cannot
<br>
illuminate all the issues involved with the Singularity. For example,
<br>
they have nothing to say about the possibility of pseudoFriendly
<br>
superintelligences with a wrong philosophy of mind, whose
<br>
universe-optimizing strategy does not respect the actual
<br>
ontological basis of our existence (whatever that may be), but
<br>
only the continued existence of some active causal emulation of
<br>
our minds (for example). In crude terms, this is the scenario
<br>
where *substrate matters*, but the AIs cannot know this,
<br>
because they don't actually know anything; they are just
<br>
self-organizing physical processes with functionally superhuman
<br>
intelligence, homeostatically acting to bring about certain
<br>
conditions in their part of the universe.
<br>
<p>So, while I do not believe that even the solution of all the
<br>
problems that SIAI sets for itself would suffice to ensure a
<br>
happy Singularity, I recognize that unFriendly RPOPs (Really
<br>
Powerful Optimization Processes) are almost certainly possible,
<br>
that they are a threat to our future in ways both blunt and
<br>
subtle (just as Friendly RPOPs would be powerful allies), and that
<br>
computer science is the relevant discipline, just as classical
<br>
mechanics is the basic discipline if you wish to land a rocket on
<br>
the moon. For this reason I personally agree that theoretical
<br>
progress in these matters is overwhelmingly important, if only
<br>
it can be achieved.
<br>
<p>So, now to business. What do we want? A theory of 'goal
<br>
stability under self-enhancement', just as rigorous as (say) the
<br>
theory of computational complexity classes. We want to pose
<br>
exact problems, and solve them. But before we can even pose
<br>
them, we must be able to formalize the basic concepts. What
<br>
are they here? I would nominate 'Friendliness', 'self-enhancement',
<br>
and 'Friendly self-enhancement'. (I suppose even the definition of
<br>
'goal' may prove subtle.) It seems to me that the rigorous
<br>
characterization of 'self-enhancement', especially, has been
<br>
neglected so far - and this tracks the parallel failure to define
<br>
'intelligence'. We have a sort of empirical definition - success at
<br>
prediction - which provides an *empirical* criterion, but we need
<br>
a theoretical one (prediction within possible worlds? but then
<br>
which worlds, and with what a-priori probabilities?): both a way
<br>
to rate the 'intelligence' of an algorithm or a bundle of heuristics,
<br>
and a way to judge whether a given self-modification is actually
<br>
an *enhancement* (although that should follow, given a truly
<br>
rigorous definition of intelligence). When multiple criteria are in
<br>
play, there are usually trade-offs: an improvement in one direction
<br>
will eventually be a diminution in another. One needs to think
<br>
carefully about how to set objective criteria for enhancement,
<br>
without arbitrarily selecting a narrow set of assumptions.
<br>
<p>This meta-problem - of arbitrarily narrowing the problem space,
<br>
achieving tractability at the price of relevance - relates to another
<br>
issue, a question of priorities. By now, there must be very many
<br>
research groups who think they have a chance of being the first
<br>
to achieve human-equivalent or superhuman AI. If they care
<br>
about 'Friendliness' at all (regardless of whether they know that
<br>
term), they are presumably trying to solve that problem in private,
<br>
and only for the cognitive architecture which they have settled
<br>
upon for their own experiments.
<br>
<p>Now, the probability that YOU win the race is less than 1, probably
<br>
much less; not necessarily because you're making an obvious
<br>
mistake, but just because we do not know (and perhaps cannot
<br>
know, in advance) the most efficient route to superintelligence.
<br>
Given the uncertainties and the number of researchers, it's fair to
<br>
say that the odds of any given research group being the first are
<br>
LOW, but the odds that *someone* gets there are HIGH. But this
<br>
implies that one should be working, not just privately on a
<br>
Friendliness theory for one's preferred architecture, but publicly on
<br>
a Friendliness theory general enough to say something about all
<br>
possible architectures. That sounds like a huge challenge, but it's
<br>
best to know what the ideal would be, and it's important to see
<br>
this in game-theoretic terms. By contributing to a publicly available,
<br>
general theory of Friendliness, you are hedging your bets;
<br>
accounting for the contingency that someone else, with a
<br>
different AI philosophy, will win the race.
<br>
<p>To expand on this: the priority of public research should be to
<br>
achieve a rigorous theoretical conception of Friendliness, to develop
<br>
a practical criterion for evaluating whether a proposed AI
<br>
architecture is Friendly or not, and then to make this a *standard*
<br>
in the world of AI research, or at least &quot;seed AI&quot; research.
<br>
<p>So, again, what would I say the research problems are? To
<br>
develop behavioral criteria of Friendliness in an 'agent', whether
<br>
natural or artificial; to develop a theory of Friendly cognitive
<br>
architecture (examples - an existence proof - would be useful;
<br>
rigorous proof that these *and only these* architectures exhibit
<br>
unconditional Friendliness would be even better); to develop
<br>
*criteria* of self-enhancement (what sort of modifications
<br>
constitute an enhancement?); to develop a knowledge of
<br>
what sort of algorithms will *actually self-enhance*.
<br>
<p>Then one can tackle questions like, which initial conditions
<br>
lead to stably Friendly self-enhancement; and which
<br>
self-enhancing algorithms get smarter fastest, when
<br>
launched at the same time.
<br>
<p>The aim should always be, to turn all of these into
<br>
well-posed problems of theoretical computer science, just
<br>
as well-posed as, say, &quot;Is P equal to NP?&quot; Beyond that, the
<br>
aim should be to *answer* those problems (although I
<br>
suspect that in some cases the answer will be an unhelpful
<br>
'undecidable', i.e. trial and error is all that can be advised,
<br>
and so luck and raw computational speed are all that will
<br>
matter), and to establish standards - standards of practice,
<br>
perhaps even standards of implementation - in the global
<br>
AI development community.
<br>
<p>Furthermore, as I said, every AI project that aims to
<br>
produce human-equivalent or superhuman intelligence
<br>
should devote some fraction of its efforts to the establishment
<br>
of universal safe standards among its peers and rivals - or at
<br>
least, devote some fraction of its efforts to thinking about
<br>
what 'universal safe standards' could possibly mean. The odds
<br>
are it is in your interest, not just to try to secretly crack the
<br>
seed AI problem in your bedroom, but to contribute to
<br>
developing a *public* understanding of Friendliness theory.
<br>
(What fraction of efforts should be spent on private project,
<br>
versus on public discussion, I leave for individual researchers
<br>
to decide.)
<br>
<p>One more word on what public development of Friendliness
<br>
standards would require - more than just having a
<br>
Friendliness-stabilizing strategy for your preferred architecture,
<br>
the one by means of which you hope that your team will win
<br>
the mind race. Public Friendliness standards must have
<br>
something to say on *every possible cognitive architecture* -
<br>
that it is irrelevant because it cannot achieve superintelligence
<br>
(although Friendliness is also relevant to the coexistence of
<br>
humans with non-enhancing human-equivalent AIs); that it
<br>
cannot be made safe, must not win the race, and should
<br>
never be implemented; that it can be made safe, but only
<br>
if you do it like so.
<br>
<p>And since in the real world, candidates for first
<br>
superintelligence will include groups of humans, enhanced
<br>
individual humans, enhanced animals, and all sorts of AI-human
<br>
symbioses, as well as exercises such as massive experiments in
<br>
Tierra-like darwinism - a theory of Friendliness, ideally, would
<br>
have a principled evaluation of all of these, along the lines I
<br>
already sketched. It sounds like a tall order, it certainly is, and
<br>
it may even be unattainable, pre-Singularity. But it's worth
<br>
having an idea of what the ideal looks like.
<br>
<p>That's all I have to say for now; as I said, this was just preamble.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14074.html">Joshua Fox: "Re: Singularity Institute: Likely to win the race to build GAI?"</a>
<li><strong>Previous message:</strong> <a href="14072.html">David Picon Alvarez: "Re: Faith-based thought vs thinkers"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14076.html">Eliezer S. Yudkowsky: "Re: Fundamental problems"</a>
<li><strong>Reply:</strong> <a href="14076.html">Eliezer S. Yudkowsky: "Re: Fundamental problems"</a>
<li><strong>Reply:</strong> <a href="14077.html">Jeff Medina: "Re: Fundamental problems"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14073">[ date ]</a>
<a href="index.html#14073">[ thread ]</a>
<a href="subject.html#14073">[ subject ]</a>
<a href="author.html#14073">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
