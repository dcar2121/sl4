<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Singularity Institute: Likely to win the race to build GAI?</title>
<meta name="Author" content="pdugan (pdugan@vt.edu)">
<meta name="Subject" content="RE: Singularity Institute: Likely to win the race to build GAI?">
<meta name="Date" content="2006-02-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Singularity Institute: Likely to win the race to build GAI?</h1>
<!-- received="Tue Feb 14 16:25:13 2006" -->
<!-- isoreceived="20060214232513" -->
<!-- sent="Tue, 14 Feb 2006 18:25:04 -0500" -->
<!-- isosent="20060214232504" -->
<!-- name="pdugan" -->
<!-- email="pdugan@vt.edu" -->
<!-- subject="RE: Singularity Institute: Likely to win the race to build GAI?" -->
<!-- id="440D165E@zathras" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="Singularity Institute: Likely to win the race to build GAI?" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> pdugan (<a href="mailto:pdugan@vt.edu?Subject=RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?"><em>pdugan@vt.edu</em></a>)<br>
<strong>Date:</strong> Tue Feb 14 2006 - 16:25:04 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14086.html">Ben Goertzel: "Re: Singularity Institute: Likely to win the race to build GAI?"</a>
<li><strong>Previous message:</strong> <a href="14084.html">Michael Vassar: "Re: Faith-based thought vs thinkers"</a>
<li><strong>Maybe in reply to:</strong> <a href="14061.html">Joshua Fox: "Singularity Institute: Likely to win the race to build GAI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14086.html">Ben Goertzel: "Re: Singularity Institute: Likely to win the race to build GAI?"</a>
<li><strong>Reply:</strong> <a href="14086.html">Ben Goertzel: "Re: Singularity Institute: Likely to win the race to build GAI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14085">[ date ]</a>
<a href="index.html#14085">[ thread ]</a>
<a href="subject.html#14085">[ subject ]</a>
<a href="author.html#14085">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Well I'd say its worth evaluating the prospective Friendliness of these 
<br>
systems, for the obvious reasons. This is probably fairly difficult to do, 
<br>
particularly for projects based on proprietary information. I think a useful 
<br>
hueristic when gauging the risks associated with an AGI is to evaluate the 
<br>
likelyhood of a hard take-off. From what I gather about Novaemente, you seem 
<br>
to see soft take-off as much more likely. If Novamente does prove robust 
<br>
enough to be deemed a &quot;general intelligence&quot; would it possible for someone 
<br>
else, possibly SIAI, to conceive of a more &quot;powerful&quot; system that enganges in 
<br>
hard take-off while Novamente spends its &quot;childhood&quot;? Or one the other hand, 
<br>
what sort of Friendliness constraints does Novamente possess?
<br>
<p>&nbsp;&nbsp;Patrick
<br>
<p><em>&gt;===== Original Message From <a href="mailto:ben@goertzel.org?Subject=RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">ben@goertzel.org</a> =====
</em><br>
<em>&gt;In fact I know of a number of individuals/groups in addition to myself
</em><br>
<em>&gt;who fall into this category (significant progress made toward
</em><br>
<em>&gt;realizing a software implementation whose design has apparent AGI
</em><br>
<em>&gt;potential), though I'm not sure which of them are list members.
</em><br>
<em>&gt;
</em><br>
<em>&gt;In addition to my Novamente project (www.novamente.net), I would
</em><br>
<em>&gt;mention Steve Omohundro
</em><br>
<em>&gt;
</em><br>
<em>&gt;<a href="http://home.att.net/~om3/selfawaresystems.html">http://home.att.net/~om3/selfawaresystems.html</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt;(who is working on a self-modifying AI system using his own variant of
</em><br>
<em>&gt;Bayesian learning) and James Rogers with his
</em><br>
<em>&gt;algorithmic-information-theory related AGI design (James is a list
</em><br>
<em>&gt;member, but his work has been kept sufficiently proprietary that I
</em><br>
<em>&gt;can't say much about it).  There are many others as well...
</em><br>
<em>&gt;
</em><br>
<em>&gt;Based on crude considerations, it would seem SIAI is nowhere near the
</em><br>
<em>&gt;most advanced group on the path toward an AGI implementation.  On the
</em><br>
<em>&gt;other hand, it's of course possible that those of us who are &quot;further
</em><br>
<em>&gt;along&quot; all have wrong ideas (though I doubt it!) and SIAI will come up
</em><br>
<em>&gt;with the right idea in 2008 or whenever and then proceed rapidly
</em><br>
<em>&gt;toward the end goal.
</em><br>
<em>&gt;
</em><br>
<em>&gt;ben
</em><br>
<em>&gt;ben
</em><br>
<em>&gt;
</em><br>
<em>&gt;On 2/14/06, pdugan &lt;<a href="mailto:pdugan@vt.edu?Subject=RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">pdugan@vt.edu</a>&gt; wrote:
</em><br>
<em>&gt;&gt; There is a certain list member who already has an AGI model more than half
</em><br>
<em>&gt;&gt; implemented, making it a few years from testablility to see if it 
</em><br>
classifies
<br>
<em>&gt;&gt; as a genuine AGI, and if so then maybe another half a decade before 
</em><br>
something
<br>
<em>&gt;&gt; like recursive self-improvement becomes possible.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;   Patrick
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; &gt;===== Original Message From P K &lt;<a href="mailto:kpete1@hotmail.com?Subject=RE:%20Singularity%20Institute:%20Likely%20to%20win%20the%20race%20to%20build%20GAI?">kpete1@hotmail.com</a>&gt; =====
</em><br>
<em>&gt;&gt; &gt;&gt;Yes, I know that they are working on _Friendly_ GAI. But my question is:
</em><br>
<em>&gt;&gt; &gt;&gt;What reason is there to think that the Institute has any real chance of
</em><br>
<em>&gt;&gt; &gt;&gt;winning the race to General Artificial Intelligence of any sort, beating
</em><br>
<em>&gt;&gt; &gt;&gt;out those thousands of very smart GAI researchers?
</em><br>
<em>&gt;&gt; &gt;&gt;
</em><br>
<em>&gt;&gt; &gt;There is no particular reason(s) I can think of that make the Institute 
</em><br>
more
<br>
<em>&gt;&gt; &gt;likely to develop AGI than any other organization with skilled developers.
</em><br>
<em>&gt;&gt; &gt;It's all a fog. The only way to see if their ideas have any merit is to 
</em><br>
try
<br>
<em>&gt;&gt; &gt;them out. Also, I suspect their donations would increase if they showed 
</em><br>
some
<br>
<em>&gt;&gt; &gt;proofs of concept. It's all speculative at this point.
</em><br>
<em>&gt;&gt; &gt;
</em><br>
<em>&gt;&gt; &gt;As for predicting success or failure, the best calibrated answer is to
</em><br>
<em>&gt;&gt; &gt;predict failure to anyone attempting to build a GAI. You would be right 
</em><br>
most
<br>
<em>&gt;&gt; &gt;of the time and wrong probably only once or right all the time (o dear,
</em><br>
<em>&gt;&gt; &gt;heresy).
</em><br>
<em>&gt;&gt; &gt;
</em><br>
<em>&gt;&gt; &gt;That doesn't mean it isn't worth trying. By analogy, think of AGI 
</em><br>
developers
<br>
<em>&gt;&gt; &gt;as individual sperm trying to reach the egg. The odds of any individual 
</em><br>
are
<br>
<em>&gt;&gt; &gt;incredibly small but the reward is so good it would be a shame not to try.
</em><br>
<em>&gt;&gt; &gt;Also, FAI has to be developed only once for all to benefit.
</em><br>
<em>&gt;&gt; &gt;
</em><br>
<em>&gt;&gt; &gt;_________________________________________________________________
</em><br>
<em>&gt;&gt; &gt;MSN(r) Calendar keeps you organized and takes the effort out of scheduling
</em><br>
<em>&gt;&gt; &gt;get-togethers.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;<a href="http://join.msn.com/?pgmarket=en-ca&amp;page=byoa/prem&amp;xAPID=1994&amp;DI=1034&amp;SU=http">http://join.msn.com/?pgmarket=en-ca&amp;page=byoa/prem&amp;xAPID=1994&amp;DI=1034&amp;SU=http</a></em><br>
<em>&gt;&gt; ://hotmail.com/enca&amp;HL=Market_MSNIS_Taglines
</em><br>
<em>&gt;&gt; &gt;  Start enjoying all the benefits of MSN(r) Premium right now and get the
</em><br>
<em>&gt;&gt; &gt;first two months FREE*.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14086.html">Ben Goertzel: "Re: Singularity Institute: Likely to win the race to build GAI?"</a>
<li><strong>Previous message:</strong> <a href="14084.html">Michael Vassar: "Re: Faith-based thought vs thinkers"</a>
<li><strong>Maybe in reply to:</strong> <a href="14061.html">Joshua Fox: "Singularity Institute: Likely to win the race to build GAI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14086.html">Ben Goertzel: "Re: Singularity Institute: Likely to win the race to build GAI?"</a>
<li><strong>Reply:</strong> <a href="14086.html">Ben Goertzel: "Re: Singularity Institute: Likely to win the race to build GAI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14085">[ date ]</a>
<a href="index.html#14085">[ thread ]</a>
<a href="subject.html#14085">[ subject ]</a>
<a href="author.html#14085">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
