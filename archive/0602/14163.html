<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendliness not an Add-on</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="Re: Friendliness not an Add-on">
<meta name="Date" content="2006-02-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendliness not an Add-on</h1>
<!-- received="Mon Feb 20 17:16:20 2006" -->
<!-- isoreceived="20060221001620" -->
<!-- sent="Mon, 20 Feb 2006 19:16:18 -0500" -->
<!-- isosent="20060221001618" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="Re: Friendliness not an Add-on" -->
<!-- id="638d4e150602201616x6c3b7c24y2183ab8919b52762@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="200602201550.46786.charleshixsn@earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=Re:%20Friendliness%20not%20an%20Add-on"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Mon Feb 20 2006 - 17:16:18 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14164.html">Marcello Mathias Herreshoff: "Re: Friendliness not an Add-on"</a>
<li><strong>Previous message:</strong> <a href="14162.html">Charles D Hixson: "Re: Friendliness not an Add-on"</a>
<li><strong>In reply to:</strong> <a href="14162.html">Charles D Hixson: "Re: Friendliness not an Add-on"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14164.html">Marcello Mathias Herreshoff: "Re: Friendliness not an Add-on"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14163">[ date ]</a>
<a href="index.html#14163">[ thread ]</a>
<a href="subject.html#14163">[ subject ]</a>
<a href="author.html#14163">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Charles,
<br>
<p>Indeed, defining Friendliness in a way that is precise and yet
<br>
captures our intuitions is one major problem.
<br>
<p>However, another, separate problem is that even if this definition
<br>
problem is solved, there is likely no way to make an AI that is in any
<br>
useful sense guaranteed to continue to satisfy Friendliness as it
<br>
self-modifies over time.  This is because once the AI gets
<br>
fundamentally more algorithmically complex than us, it would seem to
<br>
defy our ability to prove anything about it.
<br>
<p>To work around this problem in a very limited way, I last year
<br>
suggested the ITSSIM approach to iterated Friendliness, which
<br>
basically requires that: When an AGI self-modifies, it does so in such
<br>
a way that it believes its self-modified version will still be
<br>
Friendly according to its standards and will also continue with the
<br>
ITSSIM approach.  But this doesn't fully solve the problem either, and
<br>
I have a feeling there will be no solution...
<br>
<p>Singularities and guarantees, it would appear, probably don't mix very well...
<br>
<p>-- Ben G
<br>
<p><p>On 2/20/06, Charles D Hixson &lt;<a href="mailto:charleshixsn@earthlink.net?Subject=Re:%20Friendliness%20not%20an%20Add-on">charleshixsn@earthlink.net</a>&gt; wrote:
<br>
<em>&gt; On Sunday 19 February 2006 04:37 am, Ben Goertzel wrote:
</em><br>
<em>&gt; &gt; Hi,
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; About Rice's theorem... Sorry, I did not phrase my argument against
</em><br>
<em>&gt; &gt; the relevance of this theorem very carefully.  Here goes again.
</em><br>
<em>&gt; &gt; Hopefully this reformulation is sufficiently precise.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; What that theorem says (as you know) is that for any nontrivial
</em><br>
<em>&gt; &gt; property P (roughly: any property that holds for some arguments and
</em><br>
<em>&gt; &gt; not others) it is impossible to make a program that will tell you, for
</em><br>
<em>&gt; &gt; all algorithms A, whether A has property P.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; In other words, it says
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; It is not true that:
</em><br>
<em>&gt; &gt; { There exists a program so that
</em><br>
<em>&gt; &gt; {For All nontrivial properties P and all algorithms A
</em><br>
<em>&gt; &gt; { there exists a program Q that will tell you whether A has property P
</em><br>
<em>&gt; &gt; }}}
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; But a Friendliness verifier does not need to do this.  A Friendliness
</em><br>
<em>&gt; &gt; verifier just needs to verify whether
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; * a certain class of algorithms A (the ones that it is plausibly
</em><br>
<em>&gt; &gt; likely the AI system in question will ultimately self-modify into)
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; satisfy
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; * a particular property P: Friendliness
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The existence of a Friendliness verifier of this nature is certainly
</em><br>
<em>&gt; &gt; not ruled out by Rice's Theorem.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The problems are in formulating what is meant by Friendliness, and
</em><br>
<em>&gt; &gt; defining the class of algorithms A.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; A log of the complete history of an AI system is not necessary in
</em><br>
<em>&gt; &gt; order to define the plausible algorithm-class; this definition may be
</em><br>
<em>&gt; &gt; given potentially by a priori knowledge about the nature of the AI
</em><br>
<em>&gt; &gt; system in question.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt; &gt; To put it less formally, we'd be giving our Friendliness module the
</em><br>
<em>&gt; &gt; &gt; &gt; &gt; use of a genie which is somewhat unreliable and whose reliability in
</em><br>
<em>&gt; &gt; &gt; &gt; &gt; any particular decision is, for all intents and purposes, difficult
</em><br>
<em>&gt; &gt; &gt; &gt; &gt; to check.
</em><br>
<em>&gt; &gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &gt; True
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; Right.  Doesn't this stike you as dangerous?
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; It strikes me as potentially but not necessarily dangerous -- it all
</em><br>
<em>&gt; &gt; depends on the details of the AI architecture.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; This is not the same as the &quot;AI boxing&quot; issue, in which the AI in the
</em><br>
<em>&gt; &gt; box is like a genie giving suggestions to the human out of the box.
</em><br>
<em>&gt; &gt; In that case, the genie is proposed to be potentially a sentient mind
</em><br>
<em>&gt; &gt; with its own goals and motivations and with a lot of flexibility of
</em><br>
<em>&gt; &gt; behavior.  In the case I'm discussing, the &quot;genie&quot; is a
</em><br>
<em>&gt; &gt; hard-to-predict hypothesis-suggester giving suggestions to a logical
</em><br>
<em>&gt; &gt; cognition component controlled by a Friendliness verifier.  And the
</em><br>
<em>&gt; &gt; hard-to-predict hypothesis-suggester does not not need to be a
</em><br>
<em>&gt; &gt; sentient mind on its own: it does not need flexible goals,
</em><br>
<em>&gt; &gt; motivations, feelings, or the ability to self-modify in any general
</em><br>
<em>&gt; &gt; way.  It just needs to be a specialized learning component, similar in
</em><br>
<em>&gt; &gt; some ways to Eliezer's proposed Very Powerful Optimization Process
</em><br>
<em>&gt; &gt; used for world-simulation inside his Collective Volition proposal (I'm
</em><br>
<em>&gt; &gt; saying that it's similar in being powerful at problem-solving without
</em><br>
<em>&gt; &gt; having goals, motivations, feelings or strong self-modification; of
</em><br>
<em>&gt; &gt; course the problem being solved by my hard-to-predict
</em><br>
<em>&gt; &gt; hypothesis-suggester (hypothesis generation) is quite different than
</em><br>
<em>&gt; &gt; the problem being solved by Eliezer's VPOP (future prediction)).
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt; I never said evolutionary programming was &quot;nontraceable&quot;.  What I said
</em><br>
<em>&gt; &gt; &gt; was &quot;nonverifiable&quot;.  I am not splitting hairs, as these are completely
</em><br>
<em>&gt; &gt; &gt; different things.  No program is nontraceable!  You can just emulate the
</em><br>
<em>&gt; &gt; &gt; CPU and observe the contents of the registers and memory at any point in
</em><br>
<em>&gt; &gt; &gt; time.  With that said though, you can probably see why this sort of
</em><br>
<em>&gt; &gt; &gt; traceability is not good enough.  What needs to be traceable is not the
</em><br>
<em>&gt; &gt; &gt; how the bits were shuffled but how the conclusion reached was justified.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; a)
</em><br>
<em>&gt; &gt; You have not presented any argument as to why verifiability in this
</em><br>
<em>&gt; &gt; sense is needed for Friendliness verification.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; b)
</em><br>
<em>&gt; &gt; Your criterion of verifiability seems to me to be unreasonably strong,
</em><br>
<em>&gt; &gt; and to effectively rule out all metaphorical and heuristic inference.
</em><br>
<em>&gt; &gt; But maybe I have misunderstood your meaning.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Please consider the following scenario.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Suppose we have a probabilistic-logical theorem-proving system, which
</em><br>
<em>&gt; &gt; arrives at a conclusion.  We can then trace the steps that it took to
</em><br>
<em>&gt; &gt; arrive at this conclusion.  But suppose that one of these steps was a
</em><br>
<em>&gt; &gt; metaphorical ANALOGY, to some other situation -- a loose and fluid
</em><br>
<em>&gt; &gt; analogy, of the sort that humans make all the time but current AI
</em><br>
<em>&gt; &gt; reasoning software is bad at making (as Douglas Hofstadter has pointed
</em><br>
<em>&gt; &gt; out in detail).
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Then, it seems to me that what your verifiability criterion demands is
</em><br>
<em>&gt; &gt; not just that the conclusion arrived at through metaphorical analogy
</em><br>
<em>&gt; &gt; be checked for correctness and usefulness -- but that a justification
</em><br>
<em>&gt; &gt; be given as to why *that particular analogy* was chosen instead of
</em><br>
<em>&gt; &gt; some other one.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; This means that according to your requirement of verifiability (as I
</em><br>
<em>&gt; &gt; understand it) a stochastic method can't be used to grab one among
</em><br>
<em>&gt; &gt; many possible analogies for handling a situation.  Instead, according
</em><br>
<em>&gt; &gt; to your requirement, some kind of verifiable logical inference needs
</em><br>
<em>&gt; &gt; to be used to choose the possible analogy.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; In Novamente, right now, the way this kind of thing would be handled
</em><br>
<em>&gt; &gt; would be (roughly speaking):
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; a) a table would be made of the possible analogies, each one
</em><br>
<em>&gt; &gt; quantified with a number indicating its contextual desirability
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; b) one of the analogies would be chosen from the table, with a
</em><br>
<em>&gt; &gt; probability proportional to the desirability number
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; According to your definition of verifiability this is a bad approach
</em><br>
<em>&gt; &gt; because of the use of a stochastic selection mechanism in Step b.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; However, I have  my doubts whether it is really possible to achieve
</em><br>
<em>&gt; &gt; significant levels of general intelligence under severely finite
</em><br>
<em>&gt; &gt; resources without making this kind of stochastic selection in one form
</em><br>
<em>&gt; &gt; or another.  (I'm not claiming it is necessary to resort to
</em><br>
<em>&gt; &gt; pseudorandom number generation; just that I suspect it's necessary to
</em><br>
<em>&gt; &gt; resort to something equally arbitrary for selecting among options in
</em><br>
<em>&gt; &gt; cases where there are  many possibly relevant pieces of knowledge in
</em><br>
<em>&gt; &gt; memory and not much information to go on regarding which one to use in
</em><br>
<em>&gt; &gt; a given inference.)
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt; What I meant was that when B proposes an action, A can either verify that
</em><br>
<em>&gt; &gt; &gt; B did the correct thing or point out a flaw in B's choice.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; The statment above is sufficient but not necessary to show that A is
</em><br>
<em>&gt; &gt; &gt; smarter than B, in the colloquial sence of the phrase.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I find this interpretation of the &quot;smarter&quot; concept very inadequate.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; For instance, suppose I have a collaborator who is more reliable in
</em><br>
<em>&gt; &gt; judgment than me but less creative than me.  For sake of concretness,
</em><br>
<em>&gt; &gt; let's call this individual by the name &quot;Cassio.&quot;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Let A=Ben, B=Cassio
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Now, it may be true that &quot;When  Ben proposes an action, Cassio can
</em><br>
<em>&gt; &gt; either verify that Ben proposed the correct thing, or point out a flaw
</em><br>
<em>&gt; &gt; in his choice&quot;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; This does not necessarily imply that Cassio is smarter than Ben -- it
</em><br>
<em>&gt; &gt; may be that Ben is specialized for hypothesis generation and Cassio is
</em><br>
<em>&gt; &gt; specialized for quality-verification.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The colloquial notion of &quot;smartness&quot; is not really sufficient for
</em><br>
<em>&gt; &gt; discussing situations like this, IMO.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt; To illustrate, suppose Alice is helping Bob play chess.  Whenever Bob
</em><br>
<em>&gt; &gt; &gt; suggests a move, she always says something like &quot;I agree with your move&quot;
</em><br>
<em>&gt; &gt; &gt; or &quot;Yikes!  If you go there, he'll fork your rooks in two moves! You
</em><br>
<em>&gt; &gt; &gt; overlooked this move here.&quot; If she can always do this, it should be
</em><br>
<em>&gt; &gt; &gt; absolutely clear that Alice is a better chess player than Bob.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Yes, but I can also imagine two chess masters, where master A was
</em><br>
<em>&gt; &gt; better at coming up with bold new ideas and master B was better at
</em><br>
<em>&gt; &gt; pointing out subtle flaws in ideas (be they bold new ones or not).
</em><br>
<em>&gt; &gt; These two masters, if they were able to cooperate very closely (e.g.
</em><br>
<em>&gt; &gt; through mental telepathy), might be able to play much better than
</em><br>
<em>&gt; &gt; either one on their own.  This situation is more like the one at hand.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; (i.e., I think your internal quasirandom selection mechanism has
</em><br>
<em>&gt; &gt; chosen a suboptimal analogy here ;-)
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; This discussion has gotten fairly in-depth, but the crux of it is, I
</em><br>
<em>&gt; &gt; don't feel you have made a convincing argument in favor of your point
</em><br>
<em>&gt; &gt; that it is implausible-in-principle to add Friendliness on to an AGI
</em><br>
<em>&gt; &gt; architecture designed without a detailed theory of Friendliness on
</em><br>
<em>&gt; &gt; hand.  I don't feel Eliezer has ever made a convincing argument in
</em><br>
<em>&gt; &gt; favor of this point either.  It may be true but you guys seem far from
</em><br>
<em>&gt; &gt; demonstrating it...
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; -- Ben
</em><br>
<em>&gt;
</em><br>
<em>&gt; A problem is that for trivial pieces of code it's not even possible to define
</em><br>
<em>&gt; what friendliness consists of, unless you consider a sort routine performing
</em><br>
<em>&gt; a sort correctly to be friendly.  Even for most more complex parts, taken in
</em><br>
<em>&gt; isolation, you won't be able to predict their friendliness.
</em><br>
<em>&gt; E.g.:  If a module for modeling possible outcomes can't model &quot;unfriendly&quot;
</em><br>
<em>&gt; outcomes, it won't be able to avoid them, but if it can, then it will be able
</em><br>
<em>&gt; to generate them as portions of a plan to execute them.  So even at the level
</em><br>
<em>&gt; were friendliness is unambiguously recognizable it doesn't necessarily make
</em><br>
<em>&gt; sense to exclude unfriendly thoughts.
</em><br>
<em>&gt;
</em><br>
<em>&gt; More to the point, I sometimes find myself unable to decide which of two
</em><br>
<em>&gt; proposed actions would reasonably be considered &quot;friendly&quot; in a larger
</em><br>
<em>&gt; context (i.e., my personal choices of how to relate to people).  How much
</em><br>
<em>&gt; coercion is it &quot;friendly&quot; to exert to prevent an alcoholic from drinking?
</em><br>
<em>&gt; Clearly one shouldn't offer them a drink, that would, in this context, be
</em><br>
<em>&gt; unfriendly.  Is one required to hide the fact that one has alcoholic drinks
</em><br>
<em>&gt; on the premises?  If they bring one with them, should one refuse to allow
</em><br>
<em>&gt; them entry?  What is this &quot;friendliness&quot;?   Practically, I generally choose
</em><br>
<em>&gt; from self-interest, and refuse to allow them in with alcohol or when
</em><br>
<em>&gt; obviously having drunk...but is this friendly, or merely selfish?
</em><br>
<em>&gt;
</em><br>
<em>&gt; What, exactly, *is* this friendliness that we wish our AIs to exhibit?
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14164.html">Marcello Mathias Herreshoff: "Re: Friendliness not an Add-on"</a>
<li><strong>Previous message:</strong> <a href="14162.html">Charles D Hixson: "Re: Friendliness not an Add-on"</a>
<li><strong>In reply to:</strong> <a href="14162.html">Charles D Hixson: "Re: Friendliness not an Add-on"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14164.html">Marcello Mathias Herreshoff: "Re: Friendliness not an Add-on"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14163">[ date ]</a>
<a href="index.html#14163">[ thread ]</a>
<a href="subject.html#14163">[ subject ]</a>
<a href="author.html#14163">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:55 MDT
</em></small></p>
</body>
</html>
