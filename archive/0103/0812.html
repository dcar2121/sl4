<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Deliver Us from Evil...?</title>
<meta name="Author" content="Mark Walker (tap@cgocable.net)">
<meta name="Subject" content="Re: Deliver Us from Evil...?">
<meta name="Date" content="2001-03-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Deliver Us from Evil...?</h1>
<!-- received="Tue Mar 27 12:27:36 2001" -->
<!-- isoreceived="20010327192736" -->
<!-- sent="Tue, 27 Mar 2001 09:32:21 -0500" -->
<!-- isosent="20010327143221" -->
<!-- name="Mark Walker" -->
<!-- email="tap@cgocable.net" -->
<!-- subject="Re: Deliver Us from Evil...?" -->
<!-- id="007801c0b6ca$bc6a3560$9f5be218@hala1.on.home.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="F187APNXRPmcarcR4ZD00000033@hotmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Mark Walker (<a href="mailto:tap@cgocable.net?Subject=Re:%20Deliver%20Us%20from%20Evil...?"><em>tap@cgocable.net</em></a>)<br>
<strong>Date:</strong> Tue Mar 27 2001 - 07:32:21 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0813.html">Mark Walker: "Oops: Deliver Us from Evil...?"</a>
<li><strong>Previous message:</strong> <a href="0811.html">Samantha Atkins: "Re: Deliver Us from Evil...?"</a>
<li><strong>In reply to:</strong> <a href="0809.html">Christian L.: "Re: Deliver Us from Evil...?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0813.html">Mark Walker: "Oops: Deliver Us from Evil...?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#812">[ date ]</a>
<a href="index.html#812">[ thread ]</a>
<a href="subject.html#812">[ subject ]</a>
<a href="author.html#812">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; Personally, I feel that it will probably be impossible to &quot;hardwire&quot;
</em><br>
<em>&gt; anthropomorphic morality and reasoning into a seed AI and expect those
</em><br>
<em>&gt; goal-systems to remain after severe self-enhancement by the transcending
</em><br>
AI.
<br>
<em>&gt; The resulting SI would be an utterly alien thing, and any speculation
</em><br>
about
<br>
<em>&gt; its actions would be futile. Hence my slight irritation regarding
</em><br>
<em>&gt; discussions about the Sysops do:s and don't:s.
</em><br>
<em>&gt; Since it is my belief that the post-singularity world will be unknowable,
</em><br>
my
<br>
<em>&gt; definition of long-term is on the order of 20-25 years. My guiding
</em><br>
<em>&gt; principles is reaching singularity as fast as possible. If you want to
</em><br>
call
<br>
<em>&gt; that ethics, that's fine with me.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; There will ONE relevant entity. This entity will IMO relate to humans as
</em><br>
we
<br>
<em>&gt; relate to bacteria. We do not make stable associations with bacteria.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Again, the unknowability assumption makes it impossible to predict
</em><br>
anything
<br>
<em>&gt; IMO.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
I have some sympathy with your point that epistemology ought to proceed
<br>
ethics but I think a lot more needs to be said about the unknowability
<br>
assumption. Here is a very very very rough schema for fleshing out the
<br>
unknowability assumption:
<br>
Cognitively speaking:
<br>
<p>0. There is no overlap between us and SI
<br>
1. Minimal overlap: We hold to the same principles of logic as SI.
<br>
<p>Beliefs                                                Desires
<br>
2b. Beliefs in basic physics (+1)           2d. game theoretic assumptions
<br>
(+1)
<br>
3b  Belief in basics of special               3d. same ethical concerns (+2d
<br>
+1).
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sciences (biology etc.)                     but also have desires
<br>
about things beyond our ken.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(+1, 2b) but
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;have belief about things
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;beyond our ken.
<br>
4b Share all our beliefs about the         4d. Share all our desires--they
<br>
just process
<br>
&nbsp;&nbsp;&nbsp;&nbsp;world--they just think faster                  them faster.
<br>
<p>Presumably you mean something above the fourth level. (This would be what is
<br>
sometimes called weak super intelligence. The SI can think faster than us
<br>
but we can come up with the same answer if we plod along).
<br>
Presumably you mean something more than level three. The idea here would be
<br>
that our viewpoint might be (roughly) a proper subset of the SI viewpoint in
<br>
the way that say an average 10 year old's viewpoint is (roughly) a proper
<br>
subset of an average adult.
<br>
Presumably you mean something more than level two since even here we are
<br>
imaging there is a partial overlap in our most basic beliefs and desires.
<br>
Presumably cannot mean level one either since even at this point we would
<br>
share knowledge of some logical truths.
<br>
As far as I can tell, many people in the transhumanist network believe that
<br>
SIs must be at least at level two. (For example, Jupiter brain discussions
<br>
seem to presuppose that we have got the basic physics and thus logic right,
<br>
hence 2b. Discussions about wars between SIs seem to assume that
<br>
game-theoretic assumptions hold and thus logic, hence 2d). Although I have
<br>
yet to find a good reason for this assumption.
<br>
Your argument seems to presuppose that the null level best describes our
<br>
cognitive relation--I take this to be the upshot of the bacteria analogy. Do
<br>
you have good evidence that this MUST be the case? Myself, I think that the
<br>
attempt to make SIs is an experiment where we do not know for certain where
<br>
on the 0 to 4 scale our &quot;children&quot; will land. This being the case it makes
<br>
sense to  be anthropomorphic (as you say) and due what we can to ensure
<br>
friendly AI. We can due this even if we believe that it is possible (but not
<br>
certain) that all our efforts to this end may be like the bacteria's attempt
<br>
to determine our kingdom of ends, i.e., that all our efforts may be full of
<br>
sound and fury, signifying nothing.  So, at minimum your argument needs to
<br>
show complete transcendence level 0. If the SI's transcendence is only
<br>
partial then there is still hope for having a hand in the future.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0813.html">Mark Walker: "Oops: Deliver Us from Evil...?"</a>
<li><strong>Previous message:</strong> <a href="0811.html">Samantha Atkins: "Re: Deliver Us from Evil...?"</a>
<li><strong>In reply to:</strong> <a href="0809.html">Christian L.: "Re: Deliver Us from Evil...?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0813.html">Mark Walker: "Oops: Deliver Us from Evil...?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#812">[ date ]</a>
<a href="index.html#812">[ thread ]</a>
<a href="subject.html#812">[ subject ]</a>
<a href="author.html#812">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
