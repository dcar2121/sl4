<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Sentients As Temporary Variables</title>
<meta name="Author" content="Durant Schoon (durant@ilm.com)">
<meta name="Subject" content="Sentients As Temporary Variables">
<meta name="Date" content="2001-03-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Sentients As Temporary Variables</h1>
<!-- received="Sat Mar 17 22:37:34 2001" -->
<!-- isoreceived="20010318053734" -->
<!-- sent="Sat, 17 Mar 2001 19:18:22 -0800 (PST)" -->
<!-- isosent="20010318031822" -->
<!-- name="Durant Schoon" -->
<!-- email="durant@ilm.com" -->
<!-- subject="Sentients As Temporary Variables" -->
<!-- id="durant-1010317191822.A08520294@sleeper" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Durant Schoon (<a href="mailto:durant@ilm.com?Subject=Re:%20Sentients%20As%20Temporary%20Variables"><em>durant@ilm.com</em></a>)<br>
<strong>Date:</strong> Sat Mar 17 2001 - 20:18:22 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0695.html">Durant Schoon: "Re: How To Live In A Simulation"</a>
<li><strong>Previous message:</strong> <a href="0693.html">Eliezer S. Yudkowsky: "Re: The problem of cognitive closure"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0726.html">Declan McCullagh: "Re: Sentients As Temporary Variables"</a>
<li><strong>Reply:</strong> <a href="0726.html">Declan McCullagh: "Re: Sentients As Temporary Variables"</a>
<li><strong>Maybe reply:</strong> <a href="0751.html">Spudboy100@aol.com: "Re: Sentients As Temporary Variables"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#694">[ date ]</a>
<a href="index.html#694">[ thread ]</a>
<a href="subject.html#694">[ subject ]</a>
<a href="author.html#694">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
----------------------------------------------------------------------
<br>
<p>This post is inspired from Samantha's and Eliezer's comments in the
<br>
&quot;Beyond Evolution&quot; thread applied to the &quot;How To Live In A Simulation&quot;
<br>
thread.
<br>
<p>I haven't finished reading the interim version of FAI, so if I'm
<br>
addressing something already covered there, please just let me know.
<br>
<p>----------------------------------------------------------------------
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Sentients As Temporary Variables&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OR
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Memory Leaks in the Caverns of Computronium&quot;
<br>
<p><p>Is the possible future of Friendliness incompatible with the possible
<br>
future in which we are all resimulated?
<br>
<p>The two scenarios of Friendliness and Resimulation might be
<br>
incompatible due to restrictions imposed by the Sysop. (The examples
<br>
in this posting assume one can &quot;create an instance of a sentient&quot; some
<br>
day in the future.) Up until recently, I had thought that these two
<br>
scenarios would both eventually take place, but now I'm wondering if
<br>
they are actually mutually exclusive.
<br>
<p><p>Part I: Actions forbidden by the Sysop
<br>
<p>Say I create two sentients: ABoy and HisDog, who are very good friends
<br>
and are emotionally attatched in a positive relationship.  Suppose I
<br>
create them with simulational complexity equivalent to a modern ten
<br>
year old human boy and a 3 year old dog.
<br>
<p>This is my simulated world, however, the following are not allowed
<br>
because these simulation are sufficiently complex and these sentients
<br>
have rights of their own:
<br>
<p>1) I cannot destroy either ABoy or HisDog (nor torture them, maim
<br>
&nbsp;&nbsp;&nbsp;them, etc.)
<br>
<p>2) I cannot lie to them and completely manipulate their realities to
<br>
&nbsp;&nbsp;&nbsp;make them believe that the other has died horribly or even that the
<br>
&nbsp;&nbsp;&nbsp;one hates the other and wants to be left alone for the rest of
<br>
&nbsp;&nbsp;&nbsp;eternity (which would cause a high degree of mental anguish for
<br>
&nbsp;&nbsp;&nbsp;either of them, though no one had actually been harmed). Presumably
<br>
&nbsp;&nbsp;&nbsp;they have the right to ask and learn the truth from the Sysop.
<br>
<p>3) I cannot separate them and prevent them from meeting up again of
<br>
&nbsp;&nbsp;&nbsp;their own free will, so that they suffer in the absence of the
<br>
&nbsp;&nbsp;&nbsp;other (to fulfill my own sadistic desires). 
<br>
<p>4) I am required to inform them that they were created in a simulation
<br>
&nbsp;&nbsp;&nbsp;and that they have rights enforced by the Sysop (virtual Miranda?)
<br>
<p>Memory Leaks: If every time I create a sentience, I cannot later
<br>
destroy vim since ve has rights, won't huge memory leaks result? I
<br>
assume there will be a bill of sentient rights, so that each sentient
<br>
created is given a minimum of computational resources to be happy
<br>
(ie. you cannot overcrowd them by placing them in a tiny environment
<br>
with no room (resources) to pursue happiness). All simulators will
<br>
have the following label: &quot;We Serve only Free Range Sentients&quot;.
<br>
<p>(Well, maybe this would be ok, if sentients agreed to assist in
<br>
whichever endeavors sentients are needed. For example, if we created
<br>
non-killable humans in the 1400's to build merchant ships for Europe,
<br>
eventually they could be trained to use Emacs in 500 years (well most
<br>
of emacs, 500 years might not be enough time :-))
<br>
<p>In fact, I wonder: Would the Sysop allow me to give ABoy great wealth
<br>
(resources) and then take it all away (except for the minimum
<br>
requirements) for the sole effect of making ABoy miserable? 
<br>
<p>This means that I cannot create any simulation which qualifies as
<br>
sentient without certain consequences and responsibilities. Of course,
<br>
maybe I could create a sentient being with preloaded memories that
<br>
would, in all likelihood, lead vim to commit suicide in two days
<br>
(after filling out my very important marketting questionaire, of
<br>
course). Or can I actually do that? Maybe not...
<br>
<p>Can I simulation the final days of my favorite suicidal pop star?
<br>
<p>Part II: Is simulated suffering the same as real suffering?
<br>
<p>TRUE STORY 1: My mother's mother died of cancer when my mom was 13
<br>
years old. My mother was given the responsibility of informing her
<br>
younger sister and brother. According to her, this whole ordeal was an
<br>
extremely onerous and psycholocally damaging event in her life. My mom
<br>
experienced real suffering.
<br>
<p>TRUE STORY 2: As an adult teacher of elemetary school children my
<br>
mother was given the task of informing a student of hers that his
<br>
father had died that morning. The child burst into tears and my mom
<br>
felt terrible. When the child's mother came to school, she too did not
<br>
know and my mom had to tell her. Again more tears. However, as it
<br>
turned out, the whole thing was a mistake (not my mom's). The person
<br>
who died had a similar name, but was not actually the father of the
<br>
student. Whomever had contacted the school had made an error. The
<br>
student, his mother and my mother had suffered but the cause of the
<br>
suffering was illusory. Of course, until they knew the truth, this
<br>
suffering felt as &quot;real&quot; as real suffering and is arguably
<br>
indistinguishable (while it lasted). 
<br>
<p>Part III: Incompatibility of Friendliness and Resimulation
<br>
<p>If what we call reality, is really a resimulation under the guidlines
<br>
of a future Friendliness, would the sysop allow us all be subjected to
<br>
the anguish and the horrors of modern times? (if not experiencing
<br>
horror first hand, being subjected to it very close up). I have
<br>
memories of being eight years old and being severely and traumatically
<br>
burned (1st to 3rd degree over 20% of my body) when my apartment
<br>
building burned down (you wouldn't be able to tell by looking at me
<br>
today, though).
<br>
<p>If Eliezer's future of Friendliness takes hold, can we conclude that
<br>
this reality cannot be a simulated one, for it would not be allowed
<br>
(according to 1-4 above and probably a lot more out of FAI) due to the
<br>
large sorrow factor of our existence?
<br>
<p>Part IV: Possibly Compatible:
<br>
<p>I can provide some possible counter-examples, I just don't know if
<br>
it's easy to believe any of them.
<br>
<p>How they might be compatible:
<br>
<p>1) Simulated suffering does not count as suffering and is not
<br>
&nbsp;&nbsp;&nbsp;prohibited.
<br>
&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;ie. We could be simulations, so all the Hutu's who were brutally
<br>
&nbsp;&nbsp;&nbsp;bludgeoned to death by machete weilding Tutsi's didn't &quot;really&quot;
<br>
&nbsp;&nbsp;&nbsp;experience actual violence.
<br>
<p>2) The ones who are hurt are not actually sentient (just &quot;background
<br>
&nbsp;&nbsp;&nbsp;characters&quot;) - This line of reasoning would be a terrible
<br>
&nbsp;&nbsp;&nbsp;justification for violence if it turns out to be wrong.
<br>
<p>3) Implanting memories of suffering (and temporarily blocking all
<br>
&nbsp;&nbsp;&nbsp;other memories) is not prohibited.
<br>
<p>&nbsp;&nbsp;&nbsp;ie. We could all be future sentients, reliving &quot;old times&quot; and
<br>
&nbsp;&nbsp;&nbsp;temporarily blocking all our &quot;other&quot; memories, just to make this
<br>
&nbsp;&nbsp;&nbsp;feel more real. We might have agreed to do this for the
<br>
&nbsp;&nbsp;&nbsp;experience, just like going to a movie and forgetting our &quot;real&quot;
<br>
&nbsp;&nbsp;&nbsp;lives for an hour and a half. 
<br>
<p>4) Assuming that this reality is a historically acurate recreation of
<br>
&nbsp;&nbsp;&nbsp;the true, original reality that lead to the Singularity, perhaps
<br>
&nbsp;&nbsp;&nbsp;the following rationale is valid: Since everyone lived through the
<br>
&nbsp;&nbsp;&nbsp;experience once and the end result was &quot;positive&quot;, all the
<br>
&nbsp;&nbsp;&nbsp;&quot;negative&quot; experiences are justified as a matter of historical
<br>
&nbsp;&nbsp;&nbsp;relevance and for the greater good of perpetuating our reality (in
<br>
&nbsp;&nbsp;&nbsp;simulation). 
<br>
<p>5) It is allowable for sentients to experience suffering as long as
<br>
&nbsp;&nbsp;&nbsp;they are told the truth later and are given the option of reverting
<br>
&nbsp;&nbsp;&nbsp;to a former &quot;non-pychologically scarred&quot; version of verself.
<br>
<p>Did I miss any?
<br>
<p><p><pre>
--
Durant
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0695.html">Durant Schoon: "Re: How To Live In A Simulation"</a>
<li><strong>Previous message:</strong> <a href="0693.html">Eliezer S. Yudkowsky: "Re: The problem of cognitive closure"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0726.html">Declan McCullagh: "Re: Sentients As Temporary Variables"</a>
<li><strong>Reply:</strong> <a href="0726.html">Declan McCullagh: "Re: Sentients As Temporary Variables"</a>
<li><strong>Maybe reply:</strong> <a href="0751.html">Spudboy100@aol.com: "Re: Sentients As Temporary Variables"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#694">[ date ]</a>
<a href="index.html#694">[ thread ]</a>
<a href="subject.html#694">[ subject ]</a>
<a href="author.html#694">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
