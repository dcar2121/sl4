<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: How To Live In A Simulation</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="Re: How To Live In A Simulation">
<meta name="Date" content="2001-03-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: How To Live In A Simulation</h1>
<!-- received="Tue Mar 20 21:58:55 2001" -->
<!-- isoreceived="20010321045855" -->
<!-- sent="Tue, 20 Mar 2001 10:08:33 -0700" -->
<!-- isosent="20010320170833" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="Re: How To Live In A Simulation" -->
<!-- id="4.3.2.7.2.20010320094810.00b4e7a0@mail.earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3AB72AD0.E9F50A17@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=Re:%20How%20To%20Live%20In%20A%20Simulation"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Tue Mar 20 2001 - 10:08:33 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0723.html">Eliezer S. Yudkowsky: "VETO (sorry!)"</a>
<li><strong>Previous message:</strong> <a href="0721.html">James Higgins: "Re: How To Live In A Simulation"</a>
<li><strong>In reply to:</strong> <a href="0714.html">Eliezer S. Yudkowsky: "Re: How To Live In A Simulation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0695.html">Durant Schoon: "Re: How To Live In A Simulation"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#722">[ date ]</a>
<a href="index.html#722">[ thread ]</a>
<a href="subject.html#722">[ subject ]</a>
<a href="author.html#722">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 05:02 AM 3/20/2001 -0500, Eliezer S. Yudkowski wrote:
<br>
<em>&gt;Depending on how you frame your assumptions, you get different answers at
</em><br>
<em>&gt;this point.  Assuming that individual volition holds, it's hard to imagine
</em><br>
<em>&gt;all of humanity being moved into VR... not without them being asked
</em><br>
<em>&gt;first.  And it isn't necessarily true that even the transhumanists would
</em><br>
<em>&gt;walk into Diaspora within the first few seconds - there are friends and
</em><br>
<em>&gt;relatives who may get left behind, and once you're a transhuman, you may
</em><br>
<em>&gt;not be allowed to stick around on persuading people - or rather, the
</em><br>
<em>&gt;people on Earth may not want to talk to you.
</em><br>
<p>My idea was designed to give everyone on the order of 20 years to think 
<br>
about, discuss, and persuade.  Plus, it may be difficult to do this on a 
<br>
&quot;real-time&quot; scale, since 20 real years would equate to an unimaginably long 
<br>
time for any SI to run a program.
<br>
<p><em>&gt;I'm not sure how this works out.  I used to think it would just be a mass
</em><br>
<em>&gt;upload followed by a mass upgrade, but that was on the objective morality
</em><br>
<em>&gt;paradigm.  In the Sysop Scenario, the dominant force in the universe is
</em><br>
<em>&gt;volition, and that creates the possibility of commonsense solutions,
</em><br>
<em>&gt;compromises, not the extreme cases that I instinctively feel are more
</em><br>
<em>&gt;realistic.  In an extreme case, there's some kind of rush and no
</em><br>
<em>&gt;milliseconds to waste; all the transhumans vanish instantly and all the
</em><br>
<em>&gt;Pedestrians and Undecided stay behind, untouched, possibly even a bit
</em><br>
<em>&gt;confused (though hopefully it should be obvious enough what happened).  In
</em><br>
<em>&gt;a highly extreme case, everyone, including the Pedestrians, vanishes into
</em><br>
<em>&gt;the Transcend whether they like it or not.  In a compromise solution...
</em><br>
<em>&gt;what happens next will be whatever least violates Earth's massed
</em><br>
<em>&gt;volitions, including our volitions about how conflicting volitions should
</em><br>
<em>&gt;be dealt with, and what is or isn't fair methods of persuasion during the
</em><br>
<em>&gt;Transition period.
</em><br>
<em>&gt;
</em><br>
<em>&gt;The following scenario is a bit Lothlorien, so I distrust it... but to
</em><br>
<em>&gt;continue working out the consequences:  The transhumanists don't instantly
</em><br>
<em>&gt;become superintelligences - most of them don't, anyway - but they do stop
</em><br>
<em>&gt;losing neurons due to aging.  Cognitive enhancement proceeds at a very
</em><br>
<em>&gt;slow pace, the natural or irreduceable level of improvement that comes
</em><br>
<em>&gt;from aging without dying.  If a transhumanist wanted to stick around on
</em><br>
<em>&gt;Old Earth for a millennium it might get dicey, but the Transition
</em><br>
<em>&gt;presumably ends, and Old Earth is handed off exclusively to the
</em><br>
<em>&gt;Pedestrians, in five or twenty years rather than a thousand.  Meanwhile,
</em><br>
<em>&gt;the gently transhuman transhumanists are slowly creeping up the gaussian
</em><br>
<em>&gt;curve, but are not yet directly outside it; so by commonsense, slacky
</em><br>
<em>&gt;application of the second-order rules, gently transhuman transhumanists
</em><br>
<em>&gt;don't pose a contamination threat to the Earth culture the Pedestrians
</em><br>
<em>&gt;will inherit.  Similarly, the gentle transhumans can't instantly turn into
</em><br>
<em>&gt;physical supermen or start casting magical spells; that's probably outside
</em><br>
<em>&gt;the range of what the remaining Pedestrians will want to remember in their
</em><br>
<em>&gt;legends.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Sticking around on Old Earth to persuade your friends doesn't mean
</em><br>
<em>&gt;volunteering to die, or even to get hurt, so some degree of physical
</em><br>
<em>&gt;invulnerability would appear to be necessary - but then, there's more than
</em><br>
<em>&gt;one way to play that.  If the remaining Pedestrians are willing to
</em><br>
<em>&gt;tolerate legends of invulnerability, then OK, guns don't fire or
</em><br>
<em>&gt;whatever.  An alternative would be that your body suffers the expected
</em><br>
<em>&gt;physical damage but you just rematerialize a few milliseconds later
</em><br>
<em>&gt;(presumably out of the line of fire).  Obviously, this kind of freedom
</em><br>
<em>&gt;implies that a gentle transhuman is also willing to accept that the Sysop
</em><br>
<em>&gt;won't let him or her get into fights with the Pedestrians (it's a bit
</em><br>
<em>&gt;unfair if they can die permanently and you can't).  Eating?  Going to the
</em><br>
<em>&gt;bathroom?  And what happens to the economy when a third of the population
</em><br>
<em>&gt;no longer needs a job?  The Sysop can stabilize it easily enough, but only
</em><br>
<em>&gt;if the Pedestrian volition is to temporarily accept that kind of
</em><br>
<em>&gt;existence.
</em><br>
<p>The world economy will collapse at this point.  But, you also need to take 
<br>
into account several other factors.  We pretty much assume nanotech to be 
<br>
available.  The invention of nanotech alone has the potential to cause 
<br>
incredible problems.  First, the economy crashes because A) anyone can 
<br>
clone anything so all manufacturing companies go out of business and B) 
<br>
individual people get massive destructive power.  Everyone would become 
<br>
unemployed within weeks, most likely, after widespread availability of the 
<br>
technology.  Preventing wide availability puts near absolute power into a 
<br>
few hands and thus breeds near absolute corruption.  Let's not even talk 
<br>
about potential accidents.  I spent much time thinking about this before I 
<br>
became aware of the Singularity.  Many people in the know plan to get 
<br>
off-world ASAP at this point due to the danger.
<br>
<p>Now add in bio tech with the ability to alter human genes and fuse in 
<br>
technology.  At this point &quot;humans&quot; start to go in different directions, 
<br>
possibly to great extremes.  Possibly causing serious unforeseen complications.
<br>
<p>Lastly, add in the expected population growth which, I believe, I read 
<br>
about on your pages.  &quot;Supposedly&quot; the world population is supposed to go 
<br>
infinite in 2027 or something.  Obviously impossible, but it will continue 
<br>
to grow.  Of course, transcendence would actually help here.  But if you 
<br>
also believe in all the environmental stuff such as global warming we have 
<br>
problems here too.
<br>
<p>So, I don't think it will be possible to maintain an &quot;Old Earth&quot; at 
<br>
all.  The environment we live in now will most likely cease to exist 
<br>
sometime between 2010 and 2030.  This is partially why I suggested 
<br>
uploading everyone into a VR.  The VR could be kept viable during all of 
<br>
this.  Grey goo along with any potentially destructive discoveries, 
<br>
incidents or accidents simply don't happen.  The scientists may scratch 
<br>
their head and ask why, but at least they are still around to do so.  I 
<br>
truly believe this is the only viable long-term (more than a year or two at 
<br>
most) way to things intact.  Well, unless the Sysop decides to &quot;take over&quot; 
<br>
the real world, in which case why not just do it in a totally immersive VR 
<br>
where you can't tell the difference anyway.
<br>
<p><em>&gt;To some extent, this is a scenario that I worked out for possible use in
</em><br>
<em>&gt;science fiction... it feels too normal and sane to be real, somehow; too
</em><br>
<em>&gt;much like self-indulgent fantasy.  But perhaps that very self-indulgence
</em><br>
<em>&gt;means that the scenario is more likely to happen.  It's not SL4... but
</em><br>
<em>&gt;maybe Old Earth doesn't want SL4.  Maybe Earth wants a commonsense,
</em><br>
<em>&gt;flexible, non-strict set of solutions to all the various dilemnas that pop
</em><br>
<em>&gt;up during the Transition period.  That's the only fundamental reason that
</em><br>
<em>&gt;would allow something so Lothlorien, with so much story potential, to
</em><br>
<em>&gt;happen.
</em><br>
<p>SL4 may be more shocking, but SL3 technologies are far more destructive and 
<br>
dangerous.  They don't have a mind and they are not &quot;friendly&quot; by nature.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0723.html">Eliezer S. Yudkowsky: "VETO (sorry!)"</a>
<li><strong>Previous message:</strong> <a href="0721.html">James Higgins: "Re: How To Live In A Simulation"</a>
<li><strong>In reply to:</strong> <a href="0714.html">Eliezer S. Yudkowsky: "Re: How To Live In A Simulation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0695.html">Durant Schoon: "Re: How To Live In A Simulation"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#722">[ date ]</a>
<a href="index.html#722">[ thread ]</a>
<a href="subject.html#722">[ subject ]</a>
<a href="author.html#722">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
