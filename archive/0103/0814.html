<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Deliver Us from Evil...?</title>
<meta name="Author" content="Christian L. (n95lundc@hotmail.com)">
<meta name="Subject" content="Re: Deliver Us from Evil...?">
<meta name="Date" content="2001-03-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Deliver Us from Evil...?</h1>
<!-- received="Tue Mar 27 18:31:42 2001" -->
<!-- isoreceived="20010328013142" -->
<!-- sent="Tue, 27 Mar 2001 23:10:02 -0000" -->
<!-- isosent="20010327231002" -->
<!-- name="Christian L." -->
<!-- email="n95lundc@hotmail.com" -->
<!-- subject="Re: Deliver Us from Evil...?" -->
<!-- id="F144oY99EXxt5BBBVhy00001738@hotmail.com" -->
<!-- inreplyto="Deliver Us from Evil...?" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Christian L. (<a href="mailto:n95lundc@hotmail.com?Subject=Re:%20Deliver%20Us%20from%20Evil...?"><em>n95lundc@hotmail.com</em></a>)<br>
<strong>Date:</strong> Tue Mar 27 2001 - 16:10:02 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0815.html">Christian L.: "Re: Deliver Us from Evil...?"</a>
<li><strong>Previous message:</strong> <a href="0813.html">Mark Walker: "Oops: Deliver Us from Evil...?"</a>
<li><strong>Maybe in reply to:</strong> <a href="0776.html">Christian L.: "Deliver Us from Evil...?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0815.html">Christian L.: "Re: Deliver Us from Evil...?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#814">[ date ]</a>
<a href="index.html#814">[ thread ]</a>
<a href="subject.html#814">[ subject ]</a>
<a href="author.html#814">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Mark Walker wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; &gt; Personally, I feel that it will probably be impossible to &quot;hardwire&quot;
</em><br>
<em>&gt; &gt; anthropomorphic morality and reasoning into a seed AI and expect those
</em><br>
<em>&gt; &gt; goal-systems to remain after severe self-enhancement by the transcending
</em><br>
<em>&gt;AI.
</em><br>
<em>&gt; &gt; The resulting SI would be an utterly alien thing, and any speculation
</em><br>
<em>&gt;about
</em><br>
<em>&gt; &gt; its actions would be futile. Hence my slight irritation regarding
</em><br>
<em>&gt; &gt; discussions about the Sysops do:s and don't:s.
</em><br>
<em>&gt; &gt; Since it is my belief that the post-singularity world will be 
</em><br>
<em>&gt;unknowable,
</em><br>
<em>&gt;my
</em><br>
<em>&gt; &gt; definition of long-term is on the order of 20-25 years. My guiding
</em><br>
<em>&gt; &gt; principles is reaching singularity as fast as possible. If you want to
</em><br>
<em>&gt;call
</em><br>
<em>&gt; &gt; that ethics, that's fine with me.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; There will ONE relevant entity. This entity will IMO relate to humans as
</em><br>
<em>&gt;we
</em><br>
<em>&gt; &gt; relate to bacteria. We do not make stable associations with bacteria.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Again, the unknowability assumption makes it impossible to predict
</em><br>
<em>&gt;anything
</em><br>
<em>&gt; &gt; IMO.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;I have some sympathy with your point that epistemology ought to proceed
</em><br>
<em>&gt;ethics but I think a lot more needs to be said about the unknowability
</em><br>
<em>&gt;assumption. Here is a very very very rough schema for fleshing out the
</em><br>
<em>&gt;unknowability assumption:
</em><br>
<em>&gt;Cognitively speaking:
</em><br>
<em>&gt;
</em><br>
<em>&gt;0. There is no overlap between us and SI
</em><br>
<em>&gt;1. Minimal overlap: We hold to the same principles of logic as SI.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Beliefs                                                Desires
</em><br>
<em>&gt;2b. Beliefs in basic physics (+1)           2d. game theoretic assumptions
</em><br>
<em>&gt;(+1)
</em><br>
<em>&gt;3b  Belief in basics of special               3d. same ethical concerns 
</em><br>
<em>&gt;(+2d
</em><br>
<em>&gt;+1).
</em><br>
<em>&gt;       sciences (biology etc.)                     but also have desires
</em><br>
<em>&gt;about things beyond our ken.
</em><br>
<em>&gt;       (+1, 2b) but
</em><br>
<em>&gt;       have belief about things
</em><br>
<em>&gt;        beyond our ken.
</em><br>
<em>&gt;4b Share all our beliefs about the         4d. Share all our desires--they
</em><br>
<em>&gt;just process
</em><br>
<em>&gt;     world--they just think faster                  them faster.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Presumably you mean something above the fourth level. (This would be what 
</em><br>
<em>&gt;is
</em><br>
<em>&gt;sometimes called weak super intelligence. The SI can think faster than us
</em><br>
<em>&gt;but we can come up with the same answer if we plod along).
</em><br>
<p>Correct, this does not seem likely. This would be something like an uploaded 
<br>
human slave running at turbo-speed. I think the SI needs new ways of 
<br>
thinking, not just faster thinking.
<br>
<p><p><em>&gt;Presumably you mean something more than level three. The idea here would be
</em><br>
<em>&gt;that our viewpoint might be (roughly) a proper subset of the SI viewpoint 
</em><br>
<em>&gt;in
</em><br>
<em>&gt;the way that say an average 10 year old's viewpoint is (roughly) a proper
</em><br>
<em>&gt;subset of an average adult.
</em><br>
<em>&gt;Presumably you mean something more than level two since even here we are
</em><br>
<em>&gt;imaging there is a partial overlap in our most basic beliefs and desires.
</em><br>
<em>&gt;Presumably cannot mean level one either since even at this point we would
</em><br>
<em>&gt;share knowledge of some logical truths.
</em><br>
<p>Well I do think that logical and mathematical truths are universal, so it 
<br>
would be strange if the SI did not share these. The same would be true for 
<br>
basic physics. However, this has nothing to do with caring about human 
<br>
desires.
<br>
<p><p><p><em>&gt;Your argument seems to presuppose that the null level best describes our
</em><br>
<em>&gt;cognitive relation--I take this to be the upshot of the bacteria analogy. 
</em><br>
<em>&gt;Do
</em><br>
<em>&gt;you have good evidence that this MUST be the case? Myself, I think that the
</em><br>
<em>&gt;attempt to make SIs is an experiment where we do not know for certain where
</em><br>
<em>&gt;on the 0 to 4 scale our &quot;children&quot; will land.
</em><br>
<p>I haven't got a shred of evidence. As far as I can tell, we cannot know 
<br>
where on the scale the SI will come out, like you say, or if the scale is a 
<br>
good representation. This is what I meant by my assumption of unknowability. 
<br>
I did not mean that it would be certain that the resulting SI would be 
<br>
totally alien, even if my personal feeling is that it would. There is quite 
<br>
a difference between a human with an IQ of 70 and another of IQ 170, so I 
<br>
guess there would be a substantial difference between IQ 150 and IQ 10^50.
<br>
<p>This said, I feel that I have to read &quot;Friendly AI&quot; before going any further 
<br>
with the Friendliness issue.
<br>
<p><p><em>&gt;This being the case it makes
</em><br>
<em>&gt;sense to  be anthropomorphic (as you say) and due what we can to ensure
</em><br>
<em>&gt;friendly AI. We can due this even if we believe that it is possible (but 
</em><br>
<em>&gt;not
</em><br>
<em>&gt;certain) that all our efforts to this end may be like the bacteria's 
</em><br>
<em>&gt;attempt
</em><br>
<em>&gt;to determine our kingdom of ends, i.e., that all our efforts may be full of
</em><br>
<em>&gt;sound and fury, signifying nothing.
</em><br>
<p>Sure, I don't think that trying to influence the seed AI in the direction of 
<br>
Sysop can be bad in any way. The worst thing that can happen is that the AI 
<br>
thinks &quot;hmm... I don't need these goals anymore&quot; and that's it.
<br>
<p><p><p><em>&gt;So, at minimum your argument needs to
</em><br>
<em>&gt;show complete transcendence level 0.
</em><br>
<p>Like I said above, my argument was that you cannot be sure about what the SI 
<br>
will be like.
<br>
<p><p><em>&gt;If the SI's transcendence is only
</em><br>
<em>&gt;partial then there is still hope for having a hand in the future.
</em><br>
<p>It never hurts to try...
<br>
<p><p><p>/Christian
<br>
<p>_________________________________________________________________________
<br>
Get Your Private, Free E-mail from MSN Hotmail at <a href="http://www.hotmail.com">http://www.hotmail.com</a>.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0815.html">Christian L.: "Re: Deliver Us from Evil...?"</a>
<li><strong>Previous message:</strong> <a href="0813.html">Mark Walker: "Oops: Deliver Us from Evil...?"</a>
<li><strong>Maybe in reply to:</strong> <a href="0776.html">Christian L.: "Deliver Us from Evil...?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0815.html">Christian L.: "Re: Deliver Us from Evil...?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#814">[ date ]</a>
<a href="index.html#814">[ thread ]</a>
<a href="subject.html#814">[ subject ]</a>
<a href="author.html#814">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
