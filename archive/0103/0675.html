<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: The problem of cognitive closure</title>
<meta name="Author" content="Mitchell Porter (you_have_some_sort_of_bug@yahoo.com)">
<meta name="Subject" content="The problem of cognitive closure">
<meta name="Date" content="2001-03-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>The problem of cognitive closure</h1>
<!-- received="Fri Mar 16 12:37:35 2001" -->
<!-- isoreceived="20010316193735" -->
<!-- sent="Fri, 16 Mar 2001 03:35:29 -0800 (PST)" -->
<!-- isosent="20010316113529" -->
<!-- name="Mitchell Porter" -->
<!-- email="you_have_some_sort_of_bug@yahoo.com" -->
<!-- subject="The problem of cognitive closure" -->
<!-- id="20010316113529.6356.qmail@web1205.mail.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Mitchell Porter (<a href="mailto:you_have_some_sort_of_bug@yahoo.com?Subject=Re:%20The%20problem%20of%20cognitive%20closure"><em>you_have_some_sort_of_bug@yahoo.com</em></a>)<br>
<strong>Date:</strong> Fri Mar 16 2001 - 04:35:29 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0676.html">Ben Goertzel: "RE: The problem of cognitive closure"</a>
<li><strong>Previous message:</strong> <a href="0674.html">Eliezer S. Yudkowsky: "Re: Sysop vs. Liberty (was Re: How To Live In A Simulation)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0676.html">Ben Goertzel: "RE: The problem of cognitive closure"</a>
<li><strong>Reply:</strong> <a href="0676.html">Ben Goertzel: "RE: The problem of cognitive closure"</a>
<li><strong>Reply:</strong> <a href="0678.html">Randal Koene: "Re: The problem of cognitive closure"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#675">[ date ]</a>
<a href="index.html#675">[ thread ]</a>
<a href="subject.html#675">[ subject ]</a>
<a href="author.html#675">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;&nbsp;The problem of cognitive closure;
<br>
&nbsp;&nbsp;or, whether to philosophize or to program. 
<br>
<p>I suspect the majority of people on this list do both.
<br>
<p>But I mean at those times when you are actually trying
<br>
to bring the Singularity closer. 
<br>
<p>The philosophical thing that I see Ben and Eliezer 
<br>
(the two people on this list who are visibly trying
<br>
to create AI) both doing is a sort of phenomenology 
<br>
of cognition - they think about thinking, on the basis
<br>
<p>of personal experience. From this they derive their 
<br>
ideas of how to implement thought in code. The
<br>
question 
<br>
that bothers me is, is this enough? 
<br>
<p>Eliezer has a list of 'hard problems' somewhere: 
<br>
existence (why does anything exist), consciousness
<br>
(what is it), ethics (what should the supergoals be). 
<br>
I believe he expects that superintelligences will be
<br>
able to solve these problems, but that the main design
<br>
problems of seed AI are Friendliness and pragmatic
<br>
self-enhancement. If one really wants AIs to solve
<br>
such hard problems, however, I think one has to solve
<br>
the third design problem of avoiding cognitive closure
<br>
(a term which I learnt from him). 
<br>
<p>Consider a seed AI which starts out as a Friendly goal
<br>
system overseeing a pragmatically self-enhancing core.
<br>
<p>The core contains the famous 'codic cortex', and other
<br>
modules specializing in design of software
<br>
architecture, hardware configuration, and so forth. 
<br>
To write such a core would require a lot of work, 
<br>
but it would not require a revision of fundamental 
<br>
computer science or fundamental cognitive science. 
<br>
This is ultimately what I mean by 'pragmatic 
<br>
self-enhancement': enhancement of abilities that 
<br>
computers or computer programmers already have and 
<br>
use. 
<br>
<p>&nbsp;&nbsp;The results of philosophizing
<br>
<p>Is a pragmatically self-enhancing AI ever going to
<br>
be able to solve the hard problems? There are a number
<br>
of possibilities: 
<br>
<p>1) Yes, by inventing new abilities if necessary. 
<br>
There's no problem of cognitive closure with respect 
<br>
to these problems. 
<br>
<p>2) No; both humans and all AIs which they can create
<br>
are cognitively closed in this way. 
<br>
<p>3a) No, but a 'new physics' AI could solve them, *and*
<br>
a Turing-computable AI could design a new-physics AI. 
<br>
<p>3b) Same as 3a, except that it would take a
<br>
new-physics
<br>
AI to design a new-physics AI. 
<br>
<p>Now in fact my position is something like
<br>
<p>4) Quite possibly they can even be solved by a Turing
<br>
AI, but only formally; the semantics of a solution 
<br>
(i.e. the answers and the arguments for them) would 
<br>
have to be supplied by something that understood what 
<br>
the questions mean in the first place. That requires 
<br>
consciousness, and consciousness is not any
<br>
combination 
<br>
of physics and computation. Rather, the ontologies of 
<br>
physics and of computation are both subsets of the
<br>
full
<br>
ontology of the world, in which such aspects of 
<br>
consciousness as qualia, intentionality and 
<br>
subjectivity are just as primordial as quantity and 
<br>
causality. 
<br>
<p>In principle you could have a new physics which had
<br>
fundamental entities with irreducible intentional 
<br>
states, but given what physics is today, I think it's 
<br>
confusing to refer to such a hypothetical theory as 
<br>
just a physical theory. In my notes to myself I call 
<br>
the hypothetical fundamental science of consciousness 
<br>
'noetics', but that's just a name. 
<br>
<p>Having said all that, I can state my idea more
<br>
succinctly: 
<br>
<p>4') If the answers to the hard problems can be 
<br>
discovered and known to be true, 'noetic' processes
<br>
must occur at some point, because knowledge (of 
<br>
*anything*) is not just the existence of an 
<br>
isomorphism between a fact and a computational state 
<br>
(i.e. possession of a computational representation), 
<br>
it's the existence of a particular type of noetic 
<br>
state. 
<br>
<p>Two comments in passing: 
<br>
<p>a. My working notion of what knowledge is, is
<br>
'justified true belief', a philosophically
<br>
conventional
<br>
answer. Noetics has to enter in the explanation of
<br>
what a belief is. At present I take perception and
<br>
belief to be dual aspects of experience, grounded in
<br>
ontologically elementary relations of 'acquaintance'
<br>
and 'positing' respectively. 
<br>
<p>b. The relationship between noetics and 'new physics'
<br>
(new for cognitive scientists anyway) like quantum 
<br>
theory: quantum theory itself does not introduce all 
<br>
this extra ontology. However, it introduces a few
<br>
extra 
<br>
factors (entanglement, nondeterminism) which on this 
<br>
interpretation are the tip of the noetic iceberg. 
<br>
<p>I could say a lot more about this, but let me return 
<br>
to the issue at hand, which is the implications of 
<br>
a philosophy like this for seed AI. There are a lot of
<br>
possible noetic worlds, but let's pick one. Suppose
<br>
our
<br>
world is fundamentally a collection of 'partless' 
<br>
monads, which have perceptual and belief states, 
<br>
whose worldlines of successive states and causal 
<br>
neighborhoods of interaction constitute physical time 
<br>
and space respectively. The state space of a monad can
<br>
<p>increase or decrease in complexity, through the 
<br>
transfer of degrees of freedom between monads. 
<br>
We interpret these degrees of freedom as elementary 
<br>
particles and we interpret monads as entangled 
<br>
collections of elementary particles, but this is 
<br>
'misplaced concreteness'; monads, not qubits, are 
<br>
basic. Finally, a conscious mind of human scale is 
<br>
a single monad with a very large number of degrees 
<br>
of freedom. 
<br>
<p>In a world like that, a classical computer is not a 
<br>
monad, it's a huge system of interacting but still 
<br>
individuated monads. If we grant that perception, 
<br>
belief, and other conscious states can be properties 
<br>
of individual monads only, then a Turing machine can 
<br>
have none of those things; at best it can reproduce 
<br>
their causal consequences. 
<br>
<p>(Side comment on Penrose and Goedel. The logician 
<br>
Solomon Feferman has studied what he calls the 
<br>
'reflective closure' of a set of axioms, which 
<br>
consists of everything you can infer, not just from 
<br>
the axioms, but from knowing that the axioms are true.
<br>
<p>The reflective closure does in fact include all Goedel
<br>
<p>propositions. This is written up in a highly technical
<br>
<p>paper called 'Turing in the land of O(z)', in
<br>
_The universal Turing machine: a half-century survey_,
<br>
and in other papers, some of which might be on his 
<br>
website: <a href="http://math.stanford.edu/~feferman">http://math.stanford.edu/~feferman</a>. Now it
<br>
might be that reflective closure, since it involves
<br>
semantics and meta-knowledge, can only be implemented 
<br>
by monads with some sort of self-acquaintance. I don't
<br>
<p>understand the nuts and bolts of Feferman's work yet. 
<br>
But I do think that most of Penrose's critics strongly
<br>
<p>underestimate the complexities here.) 
<br>
<p>In the possible world I've described, would a 
<br>
superintelligence have to be a quantum computer - a 
<br>
single monad - to have any chance of solving the hard 
<br>
problems? Not necessarily! Although only monads 
<br>
actually know anything in such a world, they might, 
<br>
through the use of their 'noetic faculties', figure 
<br>
out logical relations between the relevant concepts, 
<br>
formalize them, and write code to manipulate them 
<br>
formally. This is the real meaning of computability 
<br>
in such a world: a problem is computable, if the 
<br>
corresponding *formal* problem can be solved by a 
<br>
Turing machine. For the hard problems to be solved, 
<br>
however, the monads themselves must do at least enough
<br>
<p>ontological reasoning to formalize the problems 
<br>
properly. 
<br>
<p>And here I think I can reach a point of agreement even
<br>
with unreconstructed computationalists and 
<br>
unctionalists who think all this monad talk is crap. 
<br>
If the hard problems never get formalized, they will 
<br>
never get solved, except by sheer luck which we have 
<br>
no reason to expect. A pack of dogs thinking at a 
<br>
trillion times biological speeds is still a pack of 
<br>
dogs, and a pragmatic problem solver which Transcends 
<br>
is still a pragmatic problem solver - just a very 
<br>
powerful one. Which leads to my final section... 
<br>
<p>&nbsp;&nbsp;The results of NOT philosophizing
<br>
<p>What are the consequences of ignoring the problem of
<br>
cognitive closure? 
<br>
<p>One possibility: eventually the programmers realize
<br>
that their program is *really* good at engineering, 
<br>
but hasn't a clue about anything else, and so they 
<br>
start trying to formalize hard ontological problems. 
<br>
<p>Another possibility: the creation of Pragmatic Powers,
<br>
<p>which can do science and technology and that's it. 
<br>
Life under a Friendly Pragmatic Power might be 
<br>
blissful, if the hard problems don't bother you, or 
<br>
it might be hellish, if they did; but unhappy 
<br>
philosophers in paradise would surely be removed 
<br>
from misery by a Friendly Pragmatic Power in *some* 
<br>
way; it should be able to detect their unhappiness, 
<br>
even if it can only model its causes formally. 
<br>
<p>A third possibility (and this is what worries me): 
<br>
built-in 'solutions' to the hard problems, designed
<br>
to fit the assumption that physics and computation are
<br>
the whole ontological story. There are many such 
<br>
solutions available on the philosophy shelves. They 
<br>
are the result of people taking seriously the 
<br>
'naturalistic' worldview, which on my account 
<br>
(Husserl's account, actually) is just an abstraction, 
<br>
a result of taking the most conceptually tractable 
<br>
subontology and saying that's the whole story. 
<br>
<p>In a world of humans, a philosophically forced
<br>
solution
<br>
can only damage culture, really. But turned into the
<br>
algorithm of a superintelligence... I'm sure you get
<br>
the picture. To take just one example: let's just 
<br>
*suppose* that the mainstream philosophy of mind 
<br>
uploading is wrong, and a neural-level emulation is 
<br>
*not* conscious. Nonetheless, a superintelligent 
<br>
automaton armed with sensors, nanotech, and a 
<br>
utilitarian Prime Directive might still freeze, slice
<br>
and scan everyone in the world, as a step towards the
<br>
digital utopia. This is basically a problem of 'Good 
<br>
intentions, bad philosophy.' 
<br>
<p>In the real world, one would hope that such a monster
<br>
would notice quantum mechanics, change its plans, 
<br>
invent quantum mind uploading, and quantum-teleport  
<br>
everyone into their new vessels. Or something similar.
<br>
<p><p>The bottom line: If we *don't* pay detailed attention 
<br>
now to what we *don't* know, we may really, really
<br>
regret it later. 
<br>
<p>Pragmatic consequence: Seed AIs need to be 
<br>
philosophically sophisticated *by design*. 
<br>
<p><p>__________________________________________________
<br>
Do You Yahoo!?
<br>
Get email at your own domain with Yahoo! Mail. 
<br>
<a href="http://personal.mail.yahoo.com/">http://personal.mail.yahoo.com/</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0676.html">Ben Goertzel: "RE: The problem of cognitive closure"</a>
<li><strong>Previous message:</strong> <a href="0674.html">Eliezer S. Yudkowsky: "Re: Sysop vs. Liberty (was Re: How To Live In A Simulation)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0676.html">Ben Goertzel: "RE: The problem of cognitive closure"</a>
<li><strong>Reply:</strong> <a href="0676.html">Ben Goertzel: "RE: The problem of cognitive closure"</a>
<li><strong>Reply:</strong> <a href="0678.html">Randal Koene: "Re: The problem of cognitive closure"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#675">[ date ]</a>
<a href="index.html#675">[ thread ]</a>
<a href="subject.html#675">[ subject ]</a>
<a href="author.html#675">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
