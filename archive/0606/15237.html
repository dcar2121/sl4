<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Two draft papers: AI and existential risk; heuristics and biases</title>
<meta name="Author" content="Christopher Healey (CHealey@unicom-inc.com)">
<meta name="Subject" content="RE: Two draft papers: AI and existential risk; heuristics and biases">
<meta name="Date" content="2006-06-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Two draft papers: AI and existential risk; heuristics and biases</h1>
<!-- received="Thu Jun  8 16:07:00 2006" -->
<!-- isoreceived="20060608220700" -->
<!-- sent="Thu, 8 Jun 2006 18:05:41 -0400" -->
<!-- isosent="20060608220541" -->
<!-- name="Christopher Healey" -->
<!-- email="CHealey@unicom-inc.com" -->
<!-- subject="RE: Two draft papers: AI and existential risk; heuristics and biases" -->
<!-- id="5725663BF245FA4EBDC03E405C85429631224E@w2k3exch.UNICOM-INC.CORP" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="Two draft papers: AI and existential risk; heuristics and biases" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Christopher Healey (<a href="mailto:CHealey@unicom-inc.com?Subject=RE:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases"><em>CHealey@unicom-inc.com</em></a>)<br>
<strong>Date:</strong> Thu Jun 08 2006 - 16:05:41 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15238.html">Bill Hibbard: "RE: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Previous message:</strong> <a href="15236.html">H C: "Confidence in Friendly Singularity"</a>
<li><strong>Maybe in reply to:</strong> <a href="15102.html">Eliezer S. Yudkowsky: "Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15238.html">Bill Hibbard: "RE: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15238.html">Bill Hibbard: "RE: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15237">[ date ]</a>
<a href="index.html#15237">[ thread ]</a>
<a href="subject.html#15237">[ subject ]</a>
<a href="author.html#15237">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Thu, Jun 08, 2006 at 1:58 PM, Bill Hibbard wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; My paper discusses the difference between hedonic and
</em><br>
<em>&gt; eudiamonic ... and makes the point that the SI should use 
</em><br>
<em>&gt; &quot;expression of long-term life satisfaction rather than
</em><br>
<em>&gt; immediate pleasure.&quot;
</em><br>
<p><p>With the SI using the expression of long-term life satisfaction to
<br>
arrive at judgments regarding which actions are appropriate for it to
<br>
take in the present, I wonder what the actual mechanism might look like.
<br>
<p>Presupposing that I have some notion of what might specifically
<br>
contribute to my long-term life satisfaction, should I expect the SI to
<br>
help me execute on that?  Perhaps there is some unstated goal that
<br>
exemplifies a target toward which my pursuit of satisfaction is steering
<br>
me; a pattern I have not yet generalized.  Surely, I would be more
<br>
satisfied in the long-term having
<br>
identified and integrated that information.  Considering this, an overly
<br>
simplistic model of my satisfaction might cause some SI to incorrectly
<br>
configure the environment in a way that thwarts my greater satisfaction.
<br>
As I changed over time as a person, it would be desirable that the SI
<br>
would continue to update its predictive model of my future self, such
<br>
that it would dynamically re-converge on my changing trajectory (and do
<br>
so through better generalizing the process of how I and others change),
<br>
acting accordingly. 
<br>
<p>Now, perhaps we should delineate particular &quot;gate events&quot; (Heinleinian
<br>
cusps) whose traversal might have significantly disproportionate results
<br>
of either a positive or negative nature.  These could be drastic, such
<br>
as getting killed in a foreseeable disaster, or more mundane, such as a
<br>
mild chemical exposure that subtly stunts one's mental performance
<br>
throughout one's life.  Where the SI's predictive capabilities were
<br>
poor, or of relatively minor impact on the volume of one's potential
<br>
developmental space, it would be desirable for it to stand down.
<br>
Perhaps after issuing a strong suggestion, but standing down
<br>
nonetheless.  
<br>
<p>However, along action paths (identified by the SI) where the future
<br>
self's desires were well-defined in the SI's model, in addition to
<br>
either strongly preserving potential development space or strongly
<br>
avoiding the loss of it, would one really have any logical choice but to
<br>
defer to the SI?  From another viewpoint, most people have had the
<br>
experience and benefit, at some point in their lives, of having a
<br>
competent and trusted advisor.  You sometimes think you understand the
<br>
reasons for their advice at the time, but then you later come to
<br>
appreciate many of the subtle ways in which you really had no clue how
<br>
right they were.  Would most people choose to forgo the potential gains
<br>
of being guided down an action path they could not yet understand (but
<br>
whose end result was more-or-less ensured)?  I'd be very surprised if
<br>
they did.
<br>
<p>In the best of worlds, an SI so supporting our notions of long-term
<br>
satisfaction would have the time to loft us at a manageable pace, but in
<br>
a world with various race-conditions present, the SI is eventually (and
<br>
perhaps often) going to face too large a jump between our current and
<br>
future selves, and ultimately have to say: &quot;You'll understand when
<br>
you're older and wiser. Poof!!!&quot;
<br>
<p>These are just a few (perhaps somewhat disjointed) thoughts sparked by
<br>
your comment quoted above, and my overall observation is that (if I'm
<br>
not misinterpreting you) your approach seems to flow down a path similar
<br>
in many ways to an extrapolated volition.  
<br>
<p>I'd be interested in your feedback in this regard.
<br>
<p>-Chris Healey
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15238.html">Bill Hibbard: "RE: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Previous message:</strong> <a href="15236.html">H C: "Confidence in Friendly Singularity"</a>
<li><strong>Maybe in reply to:</strong> <a href="15102.html">Eliezer S. Yudkowsky: "Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15238.html">Bill Hibbard: "RE: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15238.html">Bill Hibbard: "RE: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15237">[ date ]</a>
<a href="index.html#15237">[ thread ]</a>
<a href="subject.html#15237">[ subject ]</a>
<a href="author.html#15237">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
