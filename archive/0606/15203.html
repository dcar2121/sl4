<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Two draft papers: AI and existential risk; heuristics and biases</title>
<meta name="Author" content="Peter C. McCluskey (pcm@rahul.net)">
<meta name="Subject" content="Re: Two draft papers: AI and existential risk; heuristics and biases">
<meta name="Date" content="2006-06-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Two draft papers: AI and existential risk; heuristics and biases</h1>
<!-- received="Wed Jun  7 16:06:02 2006" -->
<!-- isoreceived="20060607220602" -->
<!-- sent="Wed,  7 Jun 2006 15:05:57 -0700 (PDT)" -->
<!-- isosent="20060607220557" -->
<!-- name="Peter C. McCluskey" -->
<!-- email="pcm@rahul.net" -->
<!-- subject="Re: Two draft papers: AI and existential risk; heuristics and biases" -->
<!-- id="20060607220557.2078D2BA21@mauve.rahul.net" -->
<!-- inreplyto="44830B56.3080003@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Peter C. McCluskey (<a href="mailto:pcm@rahul.net?Subject=Re:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases"><em>pcm@rahul.net</em></a>)<br>
<strong>Date:</strong> Wed Jun 07 2006 - 16:05:57 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15204.html">Charles D Hixson: "Re: Singularity awareness"</a>
<li><strong>Previous message:</strong> <a href="15202.html">Robin Lee Powell: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<li><strong>In reply to:</strong> <a href="15102.html">Eliezer S. Yudkowsky: "Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15237.html">Christopher Healey: "RE: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15203">[ date ]</a>
<a href="index.html#15203">[ thread ]</a>
<a href="subject.html#15203">[ subject ]</a>
<a href="author.html#15203">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;Your AI Risks paper is a good deal better than your previous papers.
<br>
<p>&nbsp;Your argument for a hard takeoff is unconvincing, but at least it looks
<br>
like a serious attempt at an argument (in contrast to your prior assertions
<br>
which looked like pure handwaving).
<br>
&nbsp;Your nuclear fission analogy seems to provide a strong hint as to why
<br>
you expect a faster takeoff than I do.
<br>
&nbsp;I doubt that that analogy is appropriate because it describes a regime
<br>
change from exponential decrease to one of exponential increase. Whereas
<br>
with intelligence we've had exponential increase for quite some time.
<br>
We've even had some changes in the techniques used to drive the increase
<br>
in intelligence. We probably had a switch from random selection in most
<br>
apes to sexual selection that used proto-human brains to guide breeding
<br>
efforts. Then we developed cultural aids that enable some of our abilities
<br>
to improve in a Lamarckian fashion. These produced accelerations that were
<br>
too fast for bonobos and neanderthals to cope with, but not too fast that
<br>
they couldn't have observed and reacted to it if they had had the wisdom to
<br>
evaluate the risks and the resources to respond with.
<br>
&nbsp;Do you have an argument for why the fission analogy is likely to be more
<br>
apt than past changes in the way that increases in intelligence were driven?
<br>
<p>&nbsp;On page 22, you say:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;No system is secure that searches for ways to defeat its own security.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;If the AI would harm humanity in any context, you must be doing
<br>
&nbsp;&nbsp;&nbsp;&nbsp;something wrong on a very deep level, laying your foundations awry.
<br>
I get the impression that you think these two sentences say roughly the
<br>
same thing, but I agree with the first but find the second overly broad
<br>
in a way that you are still biasing your description of the problem toward
<br>
the a solution similar to the one you thought you had in CFAI. The risk
<br>
that an AI would want to harm humanity if its creator told it to do so is
<br>
not the same kind of risk as the risk of an AI being designed with a goal
<br>
that would imply harming humanity. You don't appear to have presented an
<br>
argument why it is essential to solve the broader set of risks in the AI
<br>
software.
<br>
<p>&nbsp;On page 27, you sound unreasonably confident of an AI's ability to
<br>
build nanotech devices. AI access to a sophisticated quantum computer
<br>
would probably be sufficient for the approach you describe to work on
<br>
the first attempt. But with more conservative assumptions about the
<br>
cpu power available to it, everything I know about molecular modelling
<br>
suggests that the AI would not be able to produce good enough models
<br>
to produce a working device on it's first few attempts.
<br>
&nbsp;Your argument is not very sensitive to the probability that this scenario
<br>
will work the first time. But the way you phrase your argument suggests
<br>
that you tend to seriously overestimate your ability to forecast big
<br>
innovations and are biased toward underestimating their difficulty,
<br>
which reduces my confidence in your ability to analyze AI risks.
<br>
<p>&nbsp;On page 29 you use the word majoritarian in ways that seem confusing.
<br>
Are you switching between referring to a majority of humans and a majority
<br>
of AIs? If so, you need to be a little clearer that you're making that
<br>
switch.
<br>
<p>&nbsp;On page 33 you say: &quot;Furthermore, to scan a human brain neuron by neuron,
<br>
we need sophisticated nanotechnology.&quot; That seems inconsistent with
<br>
Merkle's analysis of how it could be done with technology available in
<br>
1989 in this paper: <a href="http://www.merkle.com/merkleDir/brainAnalysis.html">http://www.merkle.com/merkleDir/brainAnalysis.html</a>.
<br>
The difficulty of understanding human minds well enough to improve them
<br>
rapidly seems to be the main problem with the human enhancement scenario,
<br>
so the effort need for uploading seems unimportant to your arguments, and
<br>
you could remove that paragraph without affecting the rest of the paper.
<br>
<pre>
-- 
------------------------------------------------------------------------------
Peter McCluskey         | Science is the belief in the ignorance of experts.
www.bayesianinvestor.com| - Richard Feynman
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15204.html">Charles D Hixson: "Re: Singularity awareness"</a>
<li><strong>Previous message:</strong> <a href="15202.html">Robin Lee Powell: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<li><strong>In reply to:</strong> <a href="15102.html">Eliezer S. Yudkowsky: "Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15237.html">Christopher Healey: "RE: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15203">[ date ]</a>
<a href="index.html#15203">[ thread ]</a>
<a href="subject.html#15203">[ subject ]</a>
<a href="author.html#15203">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
