<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [agi] Two draft papers: AI and existential risk; heuristics and biases</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="Re: [agi] Two draft papers: AI and existential risk; heuristics and biases">
<meta name="Date" content="2006-06-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [agi] Two draft papers: AI and existential risk; heuristics and biases</h1>
<!-- received="Wed Jun  7 01:24:26 2006" -->
<!-- isoreceived="20060607072426" -->
<!-- sent="Wed, 7 Jun 2006 00:24:03 -0700" -->
<!-- isosent="20060607072403" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="Re: [agi] Two draft papers: AI and existential risk; heuristics and biases" -->
<!-- id="638d4e150606070024q1e09c138taf31d9c492c9d209@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="448661F5.9010601@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=Re:%20[agi]%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Wed Jun 07 2006 - 01:24:03 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15178.html">Ben Goertzel: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Previous message:</strong> <a href="15176.html">Michael Vassar: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>In reply to:</strong> <a href="15174.html">Eliezer S. Yudkowsky: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15180.html">BillK: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15177">[ date ]</a>
<a href="index.html#15177">[ thread ]</a>
<a href="subject.html#15177">[ subject ]</a>
<a href="author.html#15177">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi Eli,
<br>
<p><em>&gt; First, as discussed in the chapter, there's a major context change
</em><br>
<em>&gt; between the AI's prehuman stage and the AI's posthuman stage.  I can
</em><br>
<em>&gt; think of *many* failure modes such that the AI appears to behave well as
</em><br>
<em>&gt; a prehuman, then wipes out humanity as a posthuman.  What I fear from
</em><br>
<em>&gt; this scientific-sounding business of &quot;experimental investigation&quot; is
</em><br>
<em>&gt; that the results of your investigation will be observed good behavior,
</em><br>
<em>&gt; and you will conclude that the AI &quot;is good&quot; and will stay good under
</em><br>
<em>&gt; extreme context changes.  This is not, in fact, a licensable conclusion.
</em><br>
<p>You are making incorrect assumptions about the goals of the
<br>
experimentation that I want to do.
<br>
<p>Of course, just because a young, prehuman or young-child-human AI
<br>
system acts like a good puppy in its simulation world, doesn't mean it
<br>
will continue to act like a good puppy once it becomes vastly smarter
<br>
and vastly different...
<br>
<p>However, right now I have very little understanding of the dynamics of
<br>
AI goal systems under conditions where the AI can modify its own goal
<br>
hierarchy based on its experience (not revising its supergoal, but
<br>
certainly revising its *understanding* of its supergoal as its own
<br>
general understanding advances).  I feel that this kind of
<br>
understanding is necessary in order to do a detailed design of a
<br>
Friendly AI or of any likely-to-be-successful strongly-self-modifying
<br>
AI.  I could attempt to achieve this kind of understanding through
<br>
pure theory right now, but I have not yet succeeded at doing so, and I
<br>
have a feeling that this kind of theory will be easier to come by once
<br>
there are more empirical examples of this kind of dynamics to look at.
<br>
<p>You may conjecture that this kind of dynamics will be totally
<br>
different in pre-adult-human-level than in adult-human-level AGIs (to
<br>
use a very crude classification system).  I admit this is possible,
<br>
but I am not at all sure that you're right.  If you are right then I
<br>
will discover this via observing that the goal system revision
<br>
dynamics in a pre-adult-human-level Novamente are too simplistic to be
<br>
useful in guiding advanced theory on this topic.
<br>
<p>Anyway, &quot;goal system dynamics&quot; is just one example (albeit a very
<br>
important one) of an AGI-theoretic topic that I believe will be
<br>
explorable more tractably once more understanding of the topic is
<br>
obtained via experimenting with adult-human-level AGIs.
<br>
<p>So, I don't want to be misinterpreted....  It is not the case that I
<br>
think we can understand general AGI ethics by playing with ethics in
<br>
toddler-level AGIs in a simulation world.  I agree that we should have
<br>
a reasonably solid theoretical understanding of AGI before launching
<br>
an AGI-driven Singularity -- unless the world is in a really dire
<br>
situation at that point and all the other alternatives seem even
<br>
riskier.  Rather, it is the case that I think we can better formulate
<br>
theories about AGI after having some experience with toddler and
<br>
young-child level AIs and studying their internal dynamics under
<br>
various conditions.  These theories will then guide us on the path to
<br>
creating more advanced AIs in a responsible and effective way.
<br>
<p>You want to come up with the theory before building even the toddlers.
<br>
&nbsp;I think coming up with the theory will be both easier and more
<br>
effective in the context of playing with the AI toddlers.  Of course
<br>
there will be a bunch of nontrivial extrapolation in using
<br>
observations of AI toddlers to formulate theories about AI adults ----
<br>
but still, this is not as hard as extrapolating from the current level
<br>
of knowledge (empirical knowledge about humans plus pure math, with no
<br>
empirical knowledge about AGIs at all) to formulate theories about AI
<br>
adults...
<br>
<p><p><em>&gt; Second, there's an *enormous* amount of experimentation and observation
</em><br>
<em>&gt; that's already been done in the cognitive sciences.  I feed off this
</em><br>
<em>&gt; body of pre-existing work in a dozen fields, and it gives me more
</em><br>
<em>&gt; concentrated evidence than I could assemble for myself in a hundred
</em><br>
<em>&gt; lifetimes.  And all that I have studied is not the thousandth part of
</em><br>
<em>&gt; the whole.  But where the processor is inefficient, no amount of
</em><br>
<em>&gt; evidence may suffice.  If there's already a thousand times as much
</em><br>
<em>&gt; evidence as you could review in your lifetime, what makes you think that
</em><br>
<em>&gt; what's needed is one more experiment - rather than an insight that we
</em><br>
<em>&gt; already have more than enough evidence to see, but we aren't looking at
</em><br>
<em>&gt; the right way?
</em><br>
<p>The data we now have from cognitive science is only rather indirectly
<br>
pertinent to the detailed design of nonhumanlike AI systems.
<br>
<p>The kind of evidence I want is the kind that can only be gotten by
<br>
studying the *internal dynamics and structures* inside an AI system as
<br>
it carries out various sorts of tasks and behaviors.
<br>
<p>For example: Under what conditions will an AGI implicitly reinterpret
<br>
its supergoal so dramatically that in effect it is no longer the same
<br>
supergoal as it started out with?  What kinds of goal systems are in
<br>
practice &quot;attractors&quot; so that nearby goal systems will tend to drift
<br>
into them via natural cognitive dynamics upon interaction with the
<br>
world ... how big are the basins of attraction?
<br>
<p>I can't explore questions like this with human minds because I can't
<br>
launch an ensemble of humans with slightly different goal systems and
<br>
let them learn and adapt and then see where each one ends up.  And I
<br>
can't explore questions like this using current mathematics because I
<br>
just wind up with a bunch of equations that no one has any clue how to
<br>
solve.  Yet in my view this is the kind of question that needs to be
<br>
investigated if we are to really understand AGI or FAI.
<br>
<p><em>&gt; Of course this objection has a special poignancy for me because, as far
</em><br>
<em>&gt; as I can tell, yes, we already have all the evidence we need, far more
</em><br>
<em>&gt; than enough, and the only problem is understanding the implications of
</em><br>
<em>&gt; what we already know.  Pity that humans aren't logically omniscient.
</em><br>
<p>This might be true, but even if so, it's irrelevant.  We have to work
<br>
around the limitations of our cognitive systems, and sometimes
<br>
gathering more information than is logically necessary is the best way
<br>
to do that.
<br>
<p><em>&gt; But just which experiments do you propose to perform, and what do you
</em><br>
<em>&gt; expect them to tell you?
</em><br>
<p>There is a long list of experiments I would like to perform, and
<br>
describing them all in a comprehensible way would take many dozens of
<br>
pages and would require the reader to have detailed background
<br>
knowledge about Novamente.  The above-loosely-described ones on goal
<br>
systems have been thought through in much more detail than was
<br>
described above, and these constitute only one small example of the
<br>
types of experiments worth performing....
<br>
<p><p><em>&gt; Now in practice, I admit that there have been cases where the
</em><br>
<em>&gt; experimental observations told us which hypotheses we needed to test;
</em><br>
<em>&gt; nearly all revolutionary science, as opposed to routine science, happens
</em><br>
<em>&gt; this way.  But it is also true in practice that you have to know what
</em><br>
<em>&gt; you're seeing.  When it comes to interpreting what the behavior of an
</em><br>
<em>&gt; AGI can tell us about its internal workings, I think you may need to
</em><br>
<em>&gt; solve most of the problem in order to know what you're seeing.
</em><br>
<p>I disagree with the latter sentence.  I believe that in the case of a
<br>
Novamente system, it will be quite possible for us to draw connections
<br>
between the system's behaviors and its internal workings.  Remember
<br>
that Novamente is designed with this in mind.  One of the design
<br>
principles of the system has been that the system's internal
<br>
structures and dynamics should be as transparent to us as possible.
<br>
Drawing this sort of connection with regard to human brains behaviors
<br>
and internal workings may be much more difficult as brains were not
<br>
designed with this kind of transparency in mind.
<br>
<p><em>&gt; Third, last time I checked, you were still attempting to come up with
</em><br>
<em>&gt; reasons why the AI you planned to experiment with could not possibly
</em><br>
<em>&gt; undergo hard takeoff, rather than building in controlled ascent /
</em><br>
<em>&gt; emergency shutdown features in at every step as a simple matter of
</em><br>
<em>&gt; course.
</em><br>
<p>No, we are not attempting to come up with reasons why the AI we plan
<br>
to experiment with cannot possibly undergo hard takeoff.  We have
<br>
engineered our system in such a way that we know that a toddler-level
<br>
version of Novamente will not be able to undergo hard takeoff.  No
<br>
&quot;attempt to come up with reasons&quot; is necessary or has been undertaken.
<br>
<p><em>&gt;I recall that you once said that the chance of the current
</em><br>
<em>&gt; version of Novamente undergoing an unexpected hard takeoff was &quot;a
</em><br>
<em>&gt; million to one&quot;.  If you've read the heuristics and biases chapter, you
</em><br>
<em>&gt; now know why a statement like this is sufficient to make me say to
</em><br>
<em>&gt; myself, &quot;This is the way the world ends.&quot;
</em><br>
<p>I read your chapter and some of the references in it, and I like your
<br>
chapter very much ... but I find your argument in the above paragraph
<br>
quite specious.  You are IMO abusing the &quot;heuristics and biases&quot;
<br>
results in the above paragraph.
<br>
<p>The chance of the current version of Novamente undergoing an
<br>
unexpected hard takeoff is EFFECTIVELY ZERO.  It is not just a million
<br>
to one.  It is effectively the same as the chance of my cellphone or
<br>
the wart on my uncle's nose or the instance of Firefox on my laptop
<br>
undergoing a hard takeoff.
<br>
<p>In spite of the heuristics and biases results, I am still going to
<br>
maintain that the odds of my daughter transmogrifying into a stockpile
<br>
of radioactive waste tomorrow morning are effectively zero.   I am not
<br>
going to refuse to sleep in the same house as her just because of a
<br>
fear that I am underestimating the risk that she will transmogrify
<br>
into a stockpile of radioactive waste....
<br>
<p><em>&gt; It's not thought, but action, that counts.  I'd have a very different
</em><br>
<em>&gt; opinion of this verbal advice to &quot;devise an experimental strategy for
</em><br>
<em>&gt; FAI&quot; if you posted a webpage containing a list of which FAI-related
</em><br>
<em>&gt; experiments you wanted to do, what you thought you might learn from them
</em><br>
<em>&gt; that you couldn't read off of existing science, and which observations
</em><br>
<em>&gt; you felt would license you to make which conclusions about the rules of
</em><br>
<em>&gt; Friendly AI.
</em><br>
<p>That webpage will not be posted, not because the experiments are
<br>
unknown, but because describing them in any detail would require that
<br>
the reader had detailed knowledge of the Novamente architecture, the
<br>
details of which are proprietary.
<br>
<p><em>&gt; But in terms of how you spend your work-hours, which code you write,
</em><br>
<em>&gt; your development plans, how you allocate your limited reading time to
</em><br>
<em>&gt; particular fields, then this business of &quot;First experiment with AGI&quot; has
</em><br>
<em>&gt; the fascinating and not-very-coincidental-looking property of having
</em><br>
<em>&gt; given rise to a plan that looks exactly like the plan one would pursue
</em><br>
<em>&gt; if Friendly AI were not, in fact, an issue.
</em><br>
<p>At this stage, we are doing basically the same work as we would do if
<br>
Friendliness were not an issue.
<br>
<p>However, once we reach the toddler-level stage, then the types of
<br>
experiments we do with the system will be affected by the fact that
<br>
Friendliness is an issue.  And, the work we do to bring the system
<br>
beyond the toddler-level stage will be strongly affected by
<br>
Friendliness concerns.
<br>
<p><em>&gt; There are deeper theoretical
</em><br>
<em>&gt; reasons (I'm working on a paper about this) why you could not possibly
</em><br>
<em>&gt; expect an AI to be Friendly unless you had enough evidence to *know* it
</em><br>
<em>&gt; was Friendly; roughly, you could not expect *any* complex behavior that
</em><br>
<em>&gt; was a small point in the space of possibilities, unless you had enough
</em><br>
<em>&gt; evidence to single out that small point in the large space.
</em><br>
<p>You do not know what percentage of AI systems comprehensible and
<br>
engineerable by humans, and taught by humans to be Friendly, and
<br>
engineered to be internally transparent and judged by knowledgeable
<br>
humans to have internal structures/dynamics consistent with
<br>
Friendliness, are going to be Friendly.
<br>
<p>So, you don't know how small are the odds of Friendliness within the
<br>
relevant subspace of the set of all possible AI systems.  The odds of
<br>
Friendliness within the space of all AI systems is irrelevant.
<br>
<p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15178.html">Ben Goertzel: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Previous message:</strong> <a href="15176.html">Michael Vassar: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>In reply to:</strong> <a href="15174.html">Eliezer S. Yudkowsky: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15180.html">BillK: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15177">[ date ]</a>
<a href="index.html#15177">[ thread ]</a>
<a href="subject.html#15177">[ subject ]</a>
<a href="author.html#15177">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
