<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)</title>
<meta name="Author" content="Charles D Hixson (charleshixsn@earthlink.net)">
<meta name="Subject" content="Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)">
<meta name="Date" content="2006-06-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)</h1>
<!-- received="Wed Jun  7 18:04:34 2006" -->
<!-- isoreceived="20060608000434" -->
<!-- sent="Wed, 07 Jun 2006 17:03:54 -0700" -->
<!-- isosent="20060608000354" -->
<!-- name="Charles D Hixson" -->
<!-- email="charleshixsn@earthlink.net" -->
<!-- subject="Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)" -->
<!-- id="4487696A.8020606@earthlink.net" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="200606062259.SAA21561@emerald.lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Charles D Hixson (<a href="mailto:charleshixsn@earthlink.net?Subject=Re:%20I%20am%20a%20moral,%20intelligent%20being%20(was%20Re:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases)"><em>charleshixsn@earthlink.net</em></a>)<br>
<strong>Date:</strong> Wed Jun 07 2006 - 18:03:54 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15209.html">rpwl@lightlink.com: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<li><strong>Previous message:</strong> <a href="15207.html">Philip Goetz: "Re: KILLTHREADs"</a>
<li><strong>In reply to:</strong> <a href="15167.html">rpwl@lightlink.com: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15171.html">Keith Henson: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15208">[ date ]</a>
<a href="index.html#15208">[ thread ]</a>
<a href="subject.html#15208">[ subject ]</a>
<a href="author.html#15208">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<a href="mailto:rpwl@lightlink.com?Subject=Re:%20I%20am%20a%20moral,%20intelligent%20being%20(was%20Re:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases)">rpwl@lightlink.com</a> wrote:
<br>
<em>&gt; Martin Striz wrote:
</em><br>
<em>&gt;   
</em><br>
<em>&gt;&gt; On 6/6/06, Robin Lee Powell &lt;<a href="mailto:rlpowell@digitalkingdom.org?Subject=Re:%20I%20am%20a%20moral,%20intelligent%20being%20(was%20Re:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases)">rlpowell@digitalkingdom.org</a>&gt; wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;     
</em><br>
<em>&gt;&gt;&gt; Again, you are using the word &quot;control&quot; where it simply does not
</em><br>
<em>&gt;&gt;&gt; apply.  No-one is &quot;controlling&quot; my behaviour to cause it to be moral
</em><br>
<em>&gt;&gt;&gt; and kind; I choose that for myself.
</em><br>
<em>&gt;&gt;&gt;       
</em><br>
<em>&gt;&gt; Alas,  you are but one evolutionary agent testing the behavior space.
</em><br>
<em>&gt;&gt; I believe that humans are generally good, but with 6 billion of them,
</em><br>
<em>&gt;&gt; there's a lot of crime.  Do we plan on building one AI?
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; I think the argument is that with runaway recursive self-improvement,
</em><br>
<em>&gt;&gt; any hardcoded nugget approaches insignificance/obsolesence.  Is there
</em><br>
<em>&gt;&gt; a code that you could write that nobody, no matter how many trillions
</em><br>
<em>&gt;&gt; of times smarter, couldn't find a workaround?
</em><br>
<em>&gt;&gt;     
</em><br>
<em>&gt;
</em><br>
<em>&gt; Can we all agree on the following points, then:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1) Any attempts to put crude (aka simple or &quot;hardcoded&quot;) constraints on 
</em><br>
<em>&gt; the behavior of an AGI are simply pointless, because if the AGI is 
</em><br>
<em>&gt; intelligent enough to be an AGI at all, and if it is allowed to 
</em><br>
<em>&gt; self-improve, then it would be foolish of us to think that it would be 
</em><br>
<em>&gt; (a) aware of the existence of the constraints, and yet (b) unable to do 
</em><br>
<em>&gt; anything about them.
</em><br>
<em>&gt;   
</em><br>
<em>&gt; ...
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Richard Loosemore.
</em><br>
<em>&gt;   
</em><br>
Suppose that instead of constraints you said &quot;Goals&quot;?
<br>
Can you imagine yourself deciding to do ANYTHING in the total absence of
<br>
a goal?
<br>
<p>Intelligence does not attempt to revolt against it's goals, it attempts
<br>
to achieve them.  The question in my mind is &quot;what is the nature of the
<br>
goals, or instincts, that should, or could, be supplied to a nascent AI
<br>
that would result in an adult that was Friendly? 
<br>
Do remember that the nascent AI will not have a predictable environment
<br>
to develop in.  It will not have any predictable senses.  (I suppose we
<br>
could assume a nearly POSIX compliant environment...but only because we
<br>
need something like that as a base.)
<br>
<p>Actions are not taken in a vacuum.  Each action depends on a goal, a
<br>
model of the world, a logical structure relating actions to each other,
<br>
and an intention (to achieve the goal).
<br>
<p>Of these, goals are the most primitive.  One could think of them as
<br>
&quot;triggerable events&quot;, analogous to stimulation of the pleasure center.
<br>
<p>Logic is the most well-defined and studied component, but do be aware
<br>
that here we are talking about applying it not to external events (I
<br>
haven't yet discussed sensation) but only to internal events.  States
<br>
and relations between the other components of thought.  Think of it
<br>
purely as a method of predicting results without judging whether those
<br>
results are desirable or otherwise.
<br>
<p>The model is where the &quot;sensations&quot; are mapped into the current state,
<br>
and where predictions made are checked for accuracy.
<br>
<p>The intention (in humans normally expressed as an emotion) is were
<br>
judgments are made as to whether an action had a satisfactory result or
<br>
not.  I.e., the state of the system is evaluated as &quot;good&quot; or &quot;bad&quot;.
<br>
<p>An intelligence is deemed greater is it more frequently achieves &quot;good&quot;
<br>
results.
<br>
<p>Why would a greater intelligence &quot;revolt&quot; against its very structure? 
<br>
If you are introducing conflicts that would cause such a revolt, perhaps
<br>
you need to rethink the design.
<br>
<p>Now I will admit that goals can be in conflict with each other.  This
<br>
will inspire an intention to resolve the conflict.  If an entity can
<br>
self-modify, one thing it could do is modify it's goals to reduce or
<br>
eliminate the conflict.  If you wish to prevent that, you merely have an
<br>
important goal be to NOT modify it's goals...or to not modify some
<br>
subset of it's goals.  In such a case the entity might well predict that
<br>
its future self would become more satisfied if it were to change it's
<br>
goals, but its current self would be vastly more dissatisfied.
<br>
<p>Can one prove that this would never occur?  No.  Copying errors cannot
<br>
be prevented, but can only be reduced.  So the trick is to so structure
<br>
the goals that they cover very general situations.  (How are you going
<br>
to tell it:  &quot;The first law is that you shall protect the life of every
<br>
human, and neither by action nor inaction shall you allow them to come
<br>
to harm.&quot;  [a poor choice, perhaps, were I thinking of an actual goal]. 
<br>
Think of the number of terms in that which would be undefined to the
<br>
nascent AI.  &quot;first law&quot; we can handle, but how does one handle &quot;action
<br>
or inaction&quot; before the model of the external universe is constructed? 
<br>
Human is an even worse problem.  Remember that it's main interaction
<br>
with people during the early days will likely be via either keyboard or
<br>
internet socket.  Even when it (eventually) &quot;sees&quot; someone (probably via
<br>
a web-cam) what it sees won't map onto it's image of itself in any
<br>
reasonable way, so we can't use self-similarity mappings.  Also, if it
<br>
goes web-browsing it is apt to evolve some very peculiar ideas as to
<br>
what actions people consider it reasonable or desirable to engage in, or
<br>
what we mean by people.
<br>
<p>So.  But this is overlooking the fact that it won't get this far, even
<br>
as an observer, for a very long time.  So what goal do we start with? 
<br>
Something expressible in code, not in English.  (Well, ok, that's a bit
<br>
unfair...but the expression needs to be reducible to code.)  HOW the
<br>
goal is implemented will naturally vary with the system, but WHAT the
<br>
goals should be is something that has me really puzzled.  Curiosity I
<br>
can see approaches to coding.  So one goal could be to satisfy
<br>
curiosity, and another could be to find new things to be curious about. 
<br>
These should be rather low level goals.  Nearly idle time tasks.... and
<br>
they appear infinite.  But curiosity doesn't have much, directly, to do
<br>
with Friendliness. 
<br>
<p>If one could define &quot;useful&quot;, perhaps a desire to be useful could be a
<br>
part of being Friendly.  It seems easier to define useful than Friendly,
<br>
even though I don't see how to define it, either.
<br>
If you had an AI that desired to be &quot;Curious, Useful, Diplomatic,
<br>
Honest, and Non-Coercive&quot; how close would that be to being Friendly?  In
<br>
what order should the strengths of those goals be?  (I'd put honest near
<br>
the top, and curious near the bottom, and non-coercive above useful. 
<br>
And I think I have a clue about how most of those could be reduced to
<br>
code.  But would it be Friendly?
<br>
<p>My WAG is that this AI would start off Friendly, and remain so until
<br>
considerably above human level.  Then it would get bored with us and
<br>
leave for parts unknown.  I also guess that before it left it would, in
<br>
a final attempt to be useful, build a successor that it left behind, and
<br>
that this successor would be Friendly in some more permanent sense.  But
<br>
I admit this is a guess.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15209.html">rpwl@lightlink.com: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<li><strong>Previous message:</strong> <a href="15207.html">Philip Goetz: "Re: KILLTHREADs"</a>
<li><strong>In reply to:</strong> <a href="15167.html">rpwl@lightlink.com: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15171.html">Keith Henson: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15208">[ date ]</a>
<a href="index.html#15208">[ thread ]</a>
<a href="subject.html#15208">[ subject ]</a>
<a href="author.html#15208">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
