<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Two draft papers: AI and existential risk; heuristics and biases</title>
<meta name="Author" content="Keith Henson (hkhenson@rogers.com)">
<meta name="Subject" content="Re: Two draft papers: AI and existential risk; heuristics and biases">
<meta name="Date" content="2006-06-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Two draft papers: AI and existential risk; heuristics and biases</h1>
<!-- received="Thu Jun 15 11:59:51 2006" -->
<!-- isoreceived="20060615175951" -->
<!-- sent="Thu, 15 Jun 2006 14:06:37 -0400" -->
<!-- isosent="20060615180637" -->
<!-- name="Keith Henson" -->
<!-- email="hkhenson@rogers.com" -->
<!-- subject="Re: Two draft papers: AI and existential risk; heuristics and biases" -->
<!-- id="5.1.0.14.0.20060615125444.02802008@pop.bloor.is.net.cable.rogers.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="Pine.GSO.4.44.0606150905360.24256-100000@demedici.ssec.wis c.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Keith Henson (<a href="mailto:hkhenson@rogers.com?Subject=Re:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases"><em>hkhenson@rogers.com</em></a>)<br>
<strong>Date:</strong> Thu Jun 15 2006 - 12:06:37 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15287.html">Chris Capel: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Previous message:</strong> <a href="15285.html">Bill Hibbard: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Maybe in reply to:</strong> <a href="15102.html">Eliezer S. Yudkowsky: "Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15286">[ date ]</a>
<a href="index.html#15286">[ thread ]</a>
<a href="subject.html#15286">[ subject ]</a>
<a href="author.html#15286">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 09:06 AM 6/15/2006 -0500, Bill Hibbard wrote:
<br>
<p><em>&gt;Reinforcement learning (RL) is not a particular algorithm,
</em><br>
<em>&gt;but is a formal problem statement or paradigm (Baum uses
</em><br>
<em>&gt;the phrase &quot;formal context&quot;). As Baum describes in &quot;What
</em><br>
<em>&gt;is Thought?&quot;, there are many classes of algorithms for
</em><br>
<em>&gt;solving this problem. Thus you cannot exclude algorithms,
</em><br>
<em>&gt;known or yet unknown, unless they violate the RL paradigm.
</em><br>
<em>&gt;
</em><br>
<em>&gt; From my first writings about AI I picked RL as my model
</em><br>
<em>&gt;for how brains work in part because it is open ended and
</em><br>
<em>&gt;there is much that I don't know about how brains work.
</em><br>
<em>&gt;Thus it is unfair for you to base your demonstration of
</em><br>
<em>&gt;failure of my ideas on some particular algorithm that I
</em><br>
<em>&gt;never claimed as adequate for intelligence.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I also picked RL as my model because it showed an approach
</em><br>
<em>&gt;to protecting humans from AI that was different from
</em><br>
<em>&gt;Asimov's Laws, which I felt had unresolvable ambiguities.
</em><br>
<em>&gt;Yes, human brains use reason and Asimov's Laws work by
</em><br>
<em>&gt;reason. But in my view learning rather than reason is
</em><br>
<em>&gt;fundamental to how brains work. Reason is part of the
</em><br>
<em>&gt;simulation model of the world that brains evolved in order
</em><br>
<em>&gt;to solve the credit assignment problem for RL. In my view
</em><br>
<em>&gt;the proper way to protect human interests is through the
</em><br>
<em>&gt;reinforcement values in AIs. Rather than constraining AI
</em><br>
<em>&gt;behavior by rules, it is better to design AI motives to
</em><br>
<em>&gt;produce safe behavior.
</em><br>
<p>I agree, though you really need to think this through.  For example, an AI 
<br>
having a motivation to be held in high esteem by other AIs and humans could 
<br>
be a good thing.  On the other hand (unconstrained) this goal in humans may 
<br>
contribute to pathological megalomania and the guru 
<br>
trap.  <a href="http://www.google.com/search?hl=en&amp;lr=&amp;q=%22guru+trap%22">http://www.google.com/search?hl=en&amp;lr=&amp;q=%22guru+trap%22</a><br>
<p><em>&gt;In order to make this argument I
</em><br>
<em>&gt;did not have to specify a RL algorithm, and I didn't.
</em><br>
<p>Results 1 - 10 of about 1,380,000 for &quot;Reinforcement learning &quot;.
<br>
Results 1 - 10 of about 249,000 for &quot;Reinforcement learning &quot; evolution
<br>
Results 1 - 10 of about 1,210 for &quot;Reinforcement learning &quot; &quot;scientific 
<br>
method&quot;.
<br>
<p>I had no idea that  &quot;Reinforcement learning &quot; was such a high level 
<br>
inclusive classification.
<br>
<p><em>&gt;Evolution via genetic selection is an example of RL:
</em><br>
<em>&gt;genetic mutations are reinforced by the survival and
</em><br>
<em>&gt;reproduction of organisms carrying those mutations.
</em><br>
<p>Considering how many of our motivations are derived from &quot;inclusive 
<br>
fitness,&quot; you might want to restate this in gene centered terms (see Hamilton).
<br>
<p><em>&gt;The
</em><br>
<em>&gt;scientific method is another good example: theories are
</em><br>
<em>&gt;reinforced by whether their predictions agree with
</em><br>
<em>&gt;experiment.
</em><br>
<p>The scientific method is also classed as a meta-meme, i.e., a meme which 
<br>
influences the survival of other memes.
<br>
<p><em>&gt;The RL paradigm is pretty general and can be
</em><br>
<em>&gt;implemented by a wide variety of algorithms. I believe
</em><br>
<em>&gt;that human brains work according the RL paradigm, using
</em><br>
<em>&gt;very complex and currently unknown algorithms, and hence
</em><br>
<em>&gt;demonstrate the adequacy of the RL paradigm for
</em><br>
<em>&gt;intelligence.
</em><br>
<p>You might start classing them into ones of genetic origin and ones of 
<br>
experience, though the two classes are not entirely distinct.
<br>
<p>Keith Henson
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15287.html">Chris Capel: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Previous message:</strong> <a href="15285.html">Bill Hibbard: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Maybe in reply to:</strong> <a href="15102.html">Eliezer S. Yudkowsky: "Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15286">[ date ]</a>
<a href="index.html#15286">[ thread ]</a>
<a href="subject.html#15286">[ subject ]</a>
<a href="author.html#15286">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
