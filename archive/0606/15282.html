<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Two draft papers: AI and existential risk; heuristics and biases</title>
<meta name="Author" content="Bill Hibbard (test@demedici.ssec.wisc.edu)">
<meta name="Subject" content="Re: Two draft papers: AI and existential risk; heuristics and biases">
<meta name="Date" content="2006-06-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Two draft papers: AI and existential risk; heuristics and biases</h1>
<!-- received="Thu Jun 15 08:16:36 2006" -->
<!-- isoreceived="20060615141636" -->
<!-- sent="Thu, 15 Jun 2006 09:06:08 -0500 (CDT)" -->
<!-- isosent="20060615140608" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@demedici.ssec.wisc.edu" -->
<!-- subject="Re: Two draft papers: AI and existential risk; heuristics and biases" -->
<!-- id="Pine.GSO.4.44.0606150905360.24256-100000@demedici.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="448E0801.1010805@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases"><em>test@demedici.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Thu Jun 15 2006 - 08:06:08 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15283.html">Robin Lee Powell: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Previous message:</strong> <a href="15281.html">Keith Henson: "RE: Human motivations was Two draft papers:"</a>
<li><strong>In reply to:</strong> <a href="15268.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15283.html">Robin Lee Powell: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15283.html">Robin Lee Powell: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15282">[ date ]</a>
<a href="index.html#15282">[ thread ]</a>
<a href="subject.html#15282">[ subject ]</a>
<a href="author.html#15282">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer Yudkowsky wrote:
<br>
<em>&gt; Bill Hibbard wrote:
</em><br>
<em>&gt; &gt; Eliezer,
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&gt;I don't think it
</em><br>
<em>&gt; &gt;&gt;inappropriate to cite a problem that is general to supervised learning
</em><br>
<em>&gt; &gt;&gt;and reinforcement, when your proposal is to, in general, use supervised
</em><br>
<em>&gt; &gt;&gt;learning and reinforcement.  You can always appeal to a &quot;different
</em><br>
<em>&gt; &gt;&gt;algorithm&quot; or a &quot;different implementation&quot; that, in some unspecified
</em><br>
<em>&gt; &gt;&gt;way, doesn't have a problem.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; But you are not demonstrating a general problem. You are
</em><br>
<em>&gt; &gt; instead relying on specific examples (primitive neural
</em><br>
<em>&gt; &gt; networks and systems that cannot distingish a human from
</em><br>
<em>&gt; &gt; a smiley) that fail trivially. You should be clear whether
</em><br>
<em>&gt; &gt; you claim that reinforcement learning (RL) must inevitably
</em><br>
<em>&gt; &gt; lead to:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;   1. A failure of intelligence.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; or:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;   2. A failure of friendliness.
</em><br>
<em>&gt;
</em><br>
<em>&gt; As it happens, my model of intelligence says that what I would call
</em><br>
<em>&gt; &quot;reinforcement learning&quot; is not, in fact, adequate to intelligence.
</em><br>
<em>&gt; However, the fact that you believe &quot;reinforcement learning&quot; is adequate
</em><br>
<em>&gt; to intelligence, suggests that you would take any possible factor that I
</em><br>
<em>&gt; thought was additionally necessary, and claim that it was part of the
</em><br>
<em>&gt; framework you regarded as &quot;reinforcement learning&quot;.
</em><br>
<p>Reinforcement learning (RL) is not a particular algorithm,
<br>
but is a formal problem statement or paradigm (Baum uses
<br>
the phrase &quot;formal context&quot;). As Baum describes in &quot;What
<br>
is Thought?&quot;, there are many classes of algorithms for
<br>
solving this problem. Thus you cannot exclude algorithms,
<br>
known or yet unknown, unless they violate the RL paradigm.
<br>
<p><em>&gt;From my first writings about AI I picked RL as my model
</em><br>
for how brains work in part because it is open ended and
<br>
there is much that I don't know about how brains work.
<br>
Thus it is unfair for you to base your demonstration of
<br>
failure of my ideas on some particular algorithm that I
<br>
never claimed as adequate for intelligence.
<br>
<p>I also picked RL as my model because it showed an approach
<br>
to protecting humans from AI that was different from
<br>
Asimov's Laws, which I felt had unresolvable ambiguities.
<br>
Yes, human brains use reason and Asimov's Laws work by
<br>
reason. But in my view learning rather than reason is
<br>
fundamental to how brains work. Reason is part of the
<br>
simulation model of the world that brains evolved in order
<br>
to solve the credit assignment problem for RL. In my view
<br>
the proper way to protect human interests is through the
<br>
reinforcement values in AIs. Rather than constraining AI
<br>
behavior by rules, it is better to design AI motives to
<br>
produce safe behavior.  n order to make this argument I
<br>
did not have to specify a RL algorithm, and I didn't.
<br>
<p>Evolution via genetic selection is an example of RL:
<br>
genetic mutations are reinforced by the survival and
<br>
reproduction of organisms carrying those mutations. The
<br>
scientific method is another good example: theories are
<br>
reinforced by whether their predictions agree with
<br>
experiment. The RL paradigm is pretty general and can be
<br>
implemented by a wide variety of algorithms. I believe
<br>
that human brains work according the RL paradigm, using
<br>
very complex and currently unknown algorithms, and hence
<br>
demonstrate the adequacy of the RL paradigm for
<br>
intelligence.
<br>
<p><em>&gt; What I am presently discussing is failure of friendliness.  However, the
</em><br>
<em>&gt; fact that we use different models of intelligence is also responsible
</em><br>
<em>&gt; for our disagreement about this second point.  Explaining a model of
</em><br>
<em>&gt; intelligence tends to be very difficult, and so, from my perspective,
</em><br>
<em>&gt; the main important thing is that you should understand that I have a
</em><br>
<em>&gt; legitimate (that is, honestly meant) disagreement with you about what
</em><br>
<em>&gt; reinforcement systems do and what happens in practice when you use them.
</em><br>
<p>I never doubt that you honestly mean what you write.
<br>
<p>Since you think the RL paradigm is inadequate for
<br>
intelligence, then you should see friendliness as a moot
<br>
issue for RL. If it isn't intelligent, it isn't a threat.
<br>
<p>Your scenario of a system that is adequate for intelligence
<br>
in its ability to rule the world, but absurdly inadequate
<br>
for intelligence in its inability to distinguish a smiley
<br>
face from a human, is inconsistent. Inconsistent assumptions
<br>
can be used to demonstrate anything. If you think that RL is
<br>
inadequate for intelligence, you should argue for that rather
<br>
than using inconsistent assumptions to turn it into an
<br>
argument about friendliness.
<br>
<p>Based on your writings, I think you probably do have a
<br>
model of intelligence that does not fit the RL paradigm.
<br>
For example, your desire to *prove* that your design for
<br>
intelligence will *never* violate certain invariants
<br>
seems difficult to reconcile with the RL paradigm, since
<br>
effective RL algorithms tend to employ outside-the-box
<br>
mechanisms like genetic mutations and inspired, crazy
<br>
ideas.
<br>
<p><em>&gt; By the way, I've got some other tasks to take on in the near future, and
</em><br>
<em>&gt; I may not be able to discuss the actual technical disagreement at
</em><br>
<em>&gt; length.  As said, I will include a footnote pointing to your
</em><br>
<em>&gt; disagreement, and to my response.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Your example of the US Army's primitive neural network
</em><br>
<em>&gt; &gt; experiments is a failure of intelligence. Your statement
</em><br>
<em>&gt; &gt; about smiley faces assumes a general success at intelligence
</em><br>
<em>&gt; &gt; by the system, but an absurd failure of intelligence in the
</em><br>
<em>&gt; &gt; part of the system that recognizes humans and their emotions,
</em><br>
<em>&gt; &gt; leading to a failure of friendliness.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Let me try to analyze the model of intelligence behind your statement.
</em><br>
<em>&gt; You're thinking something along the lines of:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Supervised algorithms&quot; (sort of like those in the most advanced
</em><br>
<em>&gt; artificial neural networks) give rise to &quot;reinforcement learning&quot;;
</em><br>
<p>I'm not sure what this means, but doubt that I agree.
<br>
I used the phrase &quot;supervised learning&quot; in my 2001
<br>
paper to indicate RL (algorithm unspecified, because
<br>
the RL algorithms necessary for intelligence were
<br>
unknown in 2001, and still unknown) with reinforcements
<br>
coming from external trainers rather than some internal
<br>
encoding. I used &quot;supervised&quot; to indicate supervision
<br>
by an external agent, and certainly not to indicate
<br>
artificial neural networks.
<br>
<p><em>&gt; &quot;Reinforcement learning&quot; gives rise to &quot;intelligence&quot;;
</em><br>
<p>The RL paradigm , with currently unknown algorithms,
<br>
gives rise to intelligence.
<br>
<p><em>&gt; &quot;Intelligence&quot; is what lets an AI shape the world, and also what tells
</em><br>
<em>&gt; it that tiny molecular smiley faces are bad examples of happiness, while
</em><br>
<em>&gt; an actual human smiling is a good example of happiness.
</em><br>
<em>&gt;
</em><br>
<em>&gt; In your journal paper from 2004, you seem to propose using a two-layer
</em><br>
<em>&gt; system of reinforcement, with the first layer being observed agreement
</em><br>
<em>&gt; from humans as a reinforcer of its definition of happiness, and the
</em><br>
<em>&gt; second layer being reinforcement of behaviors that lead to &quot;happiness&quot;
</em><br>
<em>&gt; as thus defined.  So in this case, we substitute:  &quot;'Intelligence' is
</em><br>
<em>&gt; what tells an AI that tiny molecular speakers chirping &quot;Yes! Good job!&quot;
</em><br>
<em>&gt; are bad examples of agreement with its definition of happiness, while an
</em><br>
<em>&gt; actual human saying &quot;Yes! Good job!&quot; is a good example.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; After all, it sure seems stupid to confuse human smiles with tiny
</em><br>
<em>&gt; molecular smiley faces!  How silly of the army tank classifier, not to
</em><br>
<em>&gt; realize that it was supposed to detect tanks, instead of detecting
</em><br>
<em>&gt; cloudy days!
</em><br>
<em>&gt;
</em><br>
<em>&gt; But a neural network the size of a planet, given the same examples,
</em><br>
<em>&gt; would have failed in the same way.
</em><br>
<p>But I certainly never said that neural networks were the
<br>
proper RL algorithm for intelligence. Of course, it
<br>
depends on what you mean by the phrase &quot;neural networks&quot;.
<br>
Its general use among computer scientists is for a network
<br>
of formalized neurons without feedback, and these are
<br>
certainly inadequte for intelligence. On the other hand,
<br>
human brains are networks of real neurons (and other types
<br>
of cells) that do demonstrate intelligence in an easily
<br>
portable size.
<br>
<p><em>&gt; You previously said:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; When it is feasible to build a super-intelligence, it will
</em><br>
<em>&gt; &gt; be feasible to build hard-wired recognition of &quot;human facial
</em><br>
<em>&gt; &gt; expressions, human voices and human body language&quot; (to use
</em><br>
<em>&gt; &gt; the words of mine that you quote) that exceed the recognition
</em><br>
<em>&gt; &gt; accuracy of current humans such as you and me, and will
</em><br>
<em>&gt; &gt; certainly not be fooled by &quot;tiny molecular pictures of
</em><br>
<em>&gt; &gt; smiley-faces.&quot; You should not assume such a poor
</em><br>
<em>&gt; &gt; implementation of my idea that it cannot make
</em><br>
<em>&gt; &gt; discriminations that are trivial to current humans.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It's trivial to discriminate between a photo of a picture with a
</em><br>
<em>&gt; camouflaged tank, and a photo of an empty forest.  They're different
</em><br>
<em>&gt; pixel maps.  If you transform them into strings of 1s and 0s, they're
</em><br>
<em>&gt; different strings.  Discriminating between them is as simple as testing
</em><br>
<em>&gt; them for equality.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But there's an exponentially vast space of functions that classify all
</em><br>
<em>&gt; possible pixel-maps of a fixed size into &quot;plus&quot; and &quot;minus&quot; spaces.  If
</em><br>
<em>&gt; you talk about the space of all possible computations that implement
</em><br>
<em>&gt; these classification functions, the space is trivially infinite and
</em><br>
<em>&gt; trivially undecidable.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Of course a super-AI, or an ordinary neural network, can trivially
</em><br>
<em>&gt; discriminate between a tiny molecular picture of a smiley face, or a
</em><br>
<em>&gt; smiling human, or between two pictures of the same smiling human from a
</em><br>
<em>&gt; slightly different angle.  The issue is whether the AI will *classify*
</em><br>
<em>&gt; these trivially discriminable stimuli into &quot;plus&quot; and &quot;minus&quot; spaces the
</em><br>
<em>&gt; way *you* hope it will.
</em><br>
<p>But there is a broad consensus among humans about
<br>
classifications. I assume that the RL paradigm is
<br>
adequate for intelligence in humans and in a super-AI,
<br>
and hence can conclude that a super-AI classifies
<br>
within the gamut of how the general human consensus
<br>
would classify.
<br>
<p>To make your case you must demonstrate that the RL
<br>
paradigm is inadequate for intelligence, and without
<br>
assuming some particular class of RL algorithms.
<br>
<p><em>&gt; If you look at the actual pixel-map that shows a camouflaged tank,
</em><br>
<em>&gt; there's not a little XML tag in the picture itself that says &quot;Hey,
</em><br>
<em>&gt; network, classify this picture as a good example!&quot;  The classification
</em><br>
<em>&gt; is not a property of the picture alone.  Thinking as though the
</em><br>
<em>&gt; classification is a property of the picture is an instance of Mind
</em><br>
<em>&gt; Projection Fallacy, as mentioned in my AI chapter.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Maybe you actually *wanted* the neural network to discriminate sunny
</em><br>
<em>&gt; days from cloudy days.  So you fed it exactly the same data instances,
</em><br>
<em>&gt; with exactly the same supervision, and used a slightly different
</em><br>
<em>&gt; learning algorithm - and found to your dismay that the network was so
</em><br>
<em>&gt; stupid, it learned to detect tanks instead of cloudy days.  But a really
</em><br>
<em>&gt; smart intelligence would not be so stupid that it couldn't tell the
</em><br>
<em>&gt; difference between cloudy days and sunny days.
</em><br>
<em>&gt;
</em><br>
<em>&gt; There are many possible ways to *classify* different data instances, and
</em><br>
<em>&gt; the classification involves information that is not directly present in
</em><br>
<em>&gt; the instances.  In contrast, finding that two instances are not
</em><br>
<em>&gt; identical uses only information present in the data instances
</em><br>
<em>&gt; themselves.  Saying that a superintelligence could discriminate between
</em><br>
<em>&gt; tiny molecular smiley faces and human smiles is, I would say, correct.
</em><br>
<em>&gt; But it is not correct to say that any sufficiently intelligent mind will
</em><br>
<em>&gt; automatically *classify* the instances the way you want them to.
</em><br>
<p>But humans constantly depend on agreement with other
<br>
humans on classifications. This consensus on
<br>
classifications is adequate so that humans know what
<br>
each other mean. A sufficiently intelligent mind will
<br>
learn to classify in agreement with the human consensus
<br>
if its reinforcement value for learning classifications
<br>
is to agree with humans.
<br>
<p><em>&gt; Let's say that the AI's training data is:
</em><br>
<em>&gt;
</em><br>
<em>&gt; Dataset 1:
</em><br>
<em>&gt;
</em><br>
<em>&gt; Plus:  {Smile_1, Smile_2, Smile_3}
</em><br>
<em>&gt; Minus: {Dog_1, Cat_1, Dog_2, Dog_3, Cat_2, Dog_4, Boat_1, Car_1, Dog_5,
</em><br>
<em>&gt; Cat_3, Boat_2, Dog_6}
</em><br>
<em>&gt;
</em><br>
<em>&gt; Now the AI grows up into a superintelligence, and encounters this data:
</em><br>
<em>&gt;
</em><br>
<em>&gt; Dataset 2:  {Dog_7, Cat_4, Galaxy_1, Dog_8, Nanofactory_1, Smiley_1,
</em><br>
<em>&gt; Dog_9, Cat_5, Smiley_2, Smile_4, Boat_3, Galaxy_2, Nanofactory_2,
</em><br>
<em>&gt; Smiley_3, Cat_6, Boat_4, Smile_5, Galaxy_3}
</em><br>
<em>&gt;
</em><br>
<em>&gt; It is not a property *of dataset 2* that the classification *you want* is:
</em><br>
<em>&gt;
</em><br>
<em>&gt; Plus:  {Smile_4, Smile_5}
</em><br>
<em>&gt; Minus: {Dog_7, Cat_4, Galaxy_1, Dog_8, Nanofactory_1, Smiley_1, Dog_9,
</em><br>
<em>&gt; Cat_5, Smiley_2, Boat_3, Galaxy_2, Nanofactory_2, Smiley_3, Cat_6,
</em><br>
<em>&gt; Boat_4, Galaxy_3}
</em><br>
<em>&gt;
</em><br>
<em>&gt; Rather than:
</em><br>
<em>&gt;
</em><br>
<em>&gt; Plus:  {Smiley_1, Smiley_2, Smile_4, Smiley_3, Smile_5}
</em><br>
<em>&gt; Minus: {Dog_7, Cat_4, Galaxy_1, Dog_8, Nanofactory_1, Dog_9, Cat_5,
</em><br>
<em>&gt; Boat_3, Galaxy_2, Nanofactory_2, Cat_6, Boat_4, Galaxy_3}
</em><br>
<em>&gt;
</em><br>
<em>&gt; If you want the top classification rather than the bottom one, you must
</em><br>
<em>&gt; infuse into the *prior state* of the AI some *additional information*,
</em><br>
<em>&gt; not present in dataset 2.  That, of course, is the point of giving the
</em><br>
<em>&gt; AI dataset 1.  But if you do not understand *how* the AI is classifying
</em><br>
<em>&gt; dataset 1, and then the AI enters a drastically different context, there
</em><br>
<em>&gt; is the danger that the AI is classifying dataset 1 using a very
</em><br>
<em>&gt; different method from the one *you yourself originally used* to classify
</em><br>
<em>&gt; dataset 1, and that the AI will, as a result, classify dataset 2 in ways
</em><br>
<em>&gt; different from how you yourself would have classified dataset 2.  (This
</em><br>
<em>&gt; line of reasoning leads to &quot;Coherent Extrapolated Volition&quot;, if I go on
</em><br>
<em>&gt; to ask what happens if I would have wanted to classify dataset 1 itself
</em><br>
<em>&gt; a bit differently if I had more empirical knowledge, or thought faster.)
</em><br>
<em>&gt;
</em><br>
<em>&gt; You cannot throw computing power at this problem.  Brute force, or even
</em><br>
<em>&gt; brute intelligence, is not the issue here.
</em><br>
<p>Of course there are ambiguous cases and minor
<br>
disagreements between humans, and so no AI will
<br>
match every human's classification. But the AI will
<br>
agree with the general human consensus for those
<br>
cases where consensus exists. The general human
<br>
consensus is very clear that a tiny molecular smiley
<br>
face is not a human. There is pretty unanimous
<br>
consensus on the classification &quot;human&quot;.
<br>
<p><em>&gt; &gt; If your claim is that RL can succeed at intelligence but must
</em><br>
<em>&gt; &gt; lead to a failure of friendliness, then it is reasonable to
</em><br>
<em>&gt; &gt; cite and quote me. But please use my 2004 AAAI paper . . .
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&gt;If you are genuinely repudiating your old ideas ...
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; . . . use my 2004 AAAI paper because I do repudiate the
</em><br>
<em>&gt; &gt; statement in my 2001 paper that recognition of humans and
</em><br>
<em>&gt; &gt; their emotions should be hard-wired (i.e., static). That
</em><br>
<em>&gt; &gt; is just the section of my 2001 paper that you quoted.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I will include, in the footnote, a statement that your 2004 paper
</em><br>
<em>&gt; proposes a two-layer system.  But this is not at all germane to the
</em><br>
<em>&gt; point I was making - though the footnote will serve to notify readers
</em><br>
<em>&gt; that your ideas have not remained static.  Please remember that my
</em><br>
<em>&gt; purpose is not to present Bill Hibbard's current ideas, but to use, as
</em><br>
<em>&gt; an example of failure, an idea that you published in a peer-reviewed
</em><br>
<em>&gt; journal in 2001.  If you have taken alarm at the notion of hardwiring
</em><br>
<em>&gt; happiness as reinforcement, then you ought to say something like:
</em><br>
<em>&gt; &quot;Though it makes me uncomfortable, I can't ethically argue that you
</em><br>
<em>&gt; should not publish my old mistake as a warning to others who might
</em><br>
<em>&gt; otherwise follow in my footsteps; but you must include a footnote saying
</em><br>
<em>&gt; that I now also agree it's a terrible idea.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Most importantly, your 2004 paper simply does not contain any paragraph
</em><br>
<em>&gt; that serves the introductory and expository role of the paragraph I
</em><br>
<em>&gt; quoted from your 2001 paper.  There's nothing I can quote from 2004 that
</em><br>
<em>&gt; will make as much sense to the reader.  If I were praising your 2001
</em><br>
<em>&gt; paper, rather than criticizing it, would you have the same objection?
</em><br>
<p>Sorry to hear this. I think my 2004 paper presents a much
<br>
clearer and better thought out description of intelligence
<br>
and human safety. But I appreciate your offer to include a
<br>
footnote.
<br>
<p><em>&gt; &gt; Not that I am sure that hard-wired recognition of humans and
</em><br>
<em>&gt; &gt; their emotions inevitably leads to a failure of friendliness,
</em><br>
<em>&gt;
</em><br>
<em>&gt; Okay, now it looks like you *haven't* taken alarm at this.
</em><br>
<p>I think my newer ideas are more likely to produce
<br>
friendly AI, and since I don't think a proof of
<br>
perpetual friendliness is possible, &quot;more likely&quot; is
<br>
about as good as it gets. I would intervene if someone
<br>
wanted to use my 2001 paper as the basis for actually
<br>
building a SI, but there's no chance of that.
<br>
<p><em>&gt; &gt; since the super-intelligence (SI) may understand that humans
</em><br>
<em>&gt; &gt; would be happier if they could evolve to other physical forms
</em><br>
<em>&gt; &gt; but still be recognized by the SI as humans, and decide to
</em><br>
<em>&gt; &gt; modify itself (or build an improved replacement). But if this
</em><br>
<em>&gt; &gt; is my scenario, then why not design continuing learning of
</em><br>
<em>&gt; &gt; recognition of humans and their emotions into the system in
</em><br>
<em>&gt; &gt; the first place. Hence my change of views.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think at this point you're just putting yourself into the SI's shoes,
</em><br>
<em>&gt; empathically, using your own brain to make predictions about what the SI
</em><br>
<em>&gt; will do.  Not, reasoning about the technical difficulties associated
</em><br>
<em>&gt; with infusing certain information into the SI.
</em><br>
<p>As I said above, I think the SI can learn classifications
<br>
that are within the gamut of general human consensus,
<br>
including classification of long term life satisfaction.
<br>
The SI will also have the ability to make (imperfect)
<br>
predictions about individual humans just as we can make
<br>
such predictions about each other. Based on this, and the
<br>
SI's values, I can make some predictions about its behavior.
<br>
This all comes down to our disagreement about whether the RL
<br>
paradigm is adequate for intelligence, and you should make
<br>
your argument in those terms.
<br>
<p><em>&gt; &gt; I am sure you have not repudiated everything in CFAI,
</em><br>
<em>&gt;
</em><br>
<em>&gt; I can't think offhand of any particular positive proposal I would say
</em><br>
<em>&gt; was correct.  (Maybe the section in which I rederived the Bayesian value
</em><br>
<em>&gt; of information, but that's standard.)
</em><br>
<em>&gt;
</em><br>
<em>&gt; Some negative criticisms of other possible methods and their failures,
</em><br>
<em>&gt; as presented in CFAI, continue to hold.  It is far easier to say what is
</em><br>
<em>&gt; wrong than what is right.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; and I
</em><br>
<em>&gt; &gt; have not repudiated everything in my earlier publications.
</em><br>
<em>&gt; &gt; I continue to believe that RL is critical to acheiving
</em><br>
<em>&gt; &gt; intelligence with a feasible amount of computing resources,
</em><br>
<em>&gt; &gt; and I continue to believe that collective long-term human
</em><br>
<em>&gt; &gt; happiness should be the basic reinforcement value for SI.
</em><br>
<em>&gt; &gt; But I now think that a SI should continue to learn recognition
</em><br>
<em>&gt; &gt; of humans and their emotions via reinforcement, rather than
</em><br>
<em>&gt; &gt; these recognitions being hard-wired as the result of supervised
</em><br>
<em>&gt; &gt; learning. My recent writings have also refined my views about
</em><br>
<em>&gt; &gt; how human happiness should be defined, and how the happiness of
</em><br>
<em>&gt; &gt; many people should be combined into an overall reinforcement
</em><br>
<em>&gt; &gt; value.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It is not my present purpose to criticize these new ideas of yours at
</em><br>
<em>&gt; length, only the technical problem with using reinforcement learning to
</em><br>
<em>&gt; do pretty much anything.
</em><br>
<p>There are much better known advocates of RL as the basis
<br>
for intelligence than me. But I am an advocate and happy
<br>
to be named as such. I am not happy to be named as an
<br>
advocate of artificial neural networks as adequate for
<br>
intelligence, or an advocate of systems that cannot
<br>
distinguish a smiley face from a human.
<br>
<p><em>&gt; &gt;&gt;I see no relevant difference between these two proposals, except that
</em><br>
<em>&gt; &gt;&gt;the paragraph you cite (presumably as a potential replacement) is much
</em><br>
<em>&gt; &gt;&gt;less clear to the outside academic reader.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; If you see no difference between my earlier and later ideas,
</em><br>
<em>&gt; &gt; then please use a scenario based on my later papers. That will
</em><br>
<em>&gt; &gt; be a better demonstration of the strength of your arguments,
</em><br>
<em>&gt; &gt; and be fairer to me.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If you had a paragraph serving an equivalent introductory purpose in a
</em><br>
<em>&gt; later peer-reviewed paper, I would use it.  But the paragraphs from your
</em><br>
<em>&gt; later papers are much less clear to the outside academic reader, and it
</em><br>
<em>&gt; would not be clear what I am criticizing, even though it is the same
</em><br>
<em>&gt; problem in both cases.  That's the sticking point from my perspective.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Of course, it would be best to demonstrate your claim (either
</em><br>
<em>&gt; &gt; that RL must lead to a failure of intelligence, or can succeed
</em><br>
<em>&gt; &gt; at intelligence but must lead to a failure of friendliness) in
</em><br>
<em>&gt; &gt; general. But if you cannot do that and must rely on a specific
</em><br>
<em>&gt; &gt; example, then at least do not pick an example that fails for
</em><br>
<em>&gt; &gt; trivial reasons.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The reasons are not trivial; they are general.  I know it seems &quot;stupid&quot;
</em><br>
<em>&gt; and &quot;trivial&quot; to you, but getting rid of the stupidness and triviality
</em><br>
<em>&gt; is a humongous nontrivial challenge that cannot be solved by throwing
</em><br>
<em>&gt; brute intelligence at the problem.
</em><br>
<p>Huh? Is stupidity a problem that cannot be solved by
<br>
throwing brute intelligence at it? But I think you
<br>
meant something else and just used bad wording.
<br>
<p>Brute intelligence can produce classifications that
<br>
agree with the general human consensus, and hence
<br>
&quot;know what we mean&quot; in the same way that we know what
<br>
each other mean.
<br>
<p><em>&gt; You do not need to agree with my criticism before I can publish a paper
</em><br>
<em>&gt; critical of your ideas; all the more so if I include a URL to your
</em><br>
<em>&gt; rebuttal.  Let the reader judge.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; As I wrote above, if you think RL must fail at intelligence,
</em><br>
<em>&gt; &gt; you would be best to quote Eric Baum.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Eric Baum's thesis is not reinforcement learning, it is Occam's Razor.
</em><br>
<em>&gt; Frankly I think you are too hung up on reinforcement learning.  But that
</em><br>
<em>&gt; is a separate issue.
</em><br>
<p>On page 29 of &quot;What is Thought?&quot;, Baum wrote:
<br>
<p>&nbsp;&nbsp;Evolution thus leads to creatures that are essentially
<br>
&nbsp;&nbsp;reinforcement learners with an innate, programmed-in
<br>
&nbsp;&nbsp;reward system: avoid pain, eat when hungry but not when
<br>
&nbsp;&nbsp;full, desire parental approval, and react to stop whatever
<br>
&nbsp;&nbsp;causes your child to cry.
<br>
<p>This is a clear statement that humans and other animals
<br>
are reinforcement learners.
<br>
<p>Chapter 7 is entitled &quot;Reinforcement Learning&quot;, Chapter
<br>
10 is devoted to his experiments relating economic
<br>
principles to the solution of the very difficult credit
<br>
assignment problem of RL, and other chapters include
<br>
numerous insights into how brains learn by reinforcement.
<br>
<p>The book does include extensive discussion of Occam's
<br>
Razor. On pages 12 and 13 Occam's Razor is used to chose
<br>
between different classes of RL algorithms, and this is
<br>
extensively elaborated throughout the book.
<br>
<p>Any time I write that RL is the basis of intelligence, I
<br>
cite Baum's &quot;What is Thought?&quot; He is a widely respected
<br>
RL researcher and a more eloquent advocate than I.
<br>
<p><em>&gt; &gt; If you think RL can succeed at intelligence but must fail at
</em><br>
<em>&gt; &gt; friendliness, but just want to demonstrate it for a specific
</em><br>
<em>&gt; &gt; example, then use a scenario in which:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;   1. The SI recognizes humans and their emotions as accurately
</em><br>
<em>&gt; &gt;   as any human, and continually relearns that recognition as
</em><br>
<em>&gt; &gt;   humans evolve (for example, to become SIs themselves).
</em><br>
<em>&gt;
</em><br>
<em>&gt; You say &quot;recognize as accurately as any human&quot;, implying it is a feature
</em><br>
<em>&gt; of the data.  Better to say &quot;classify in the same way humans do&quot;.
</em><br>
<p>I agree, your wording is better. Or &quot;within the gamut
<br>
of general human consensus.&quot;
<br>
<p><em>&gt; &gt;   2. The SI values people after death at the maximally unhappy
</em><br>
<em>&gt; &gt;   value, in order to avoid motivating the SI to kill unhappy
</em><br>
<em>&gt; &gt;   people.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;   3. The SI combines the happiness of many people in a way (such
</em><br>
<em>&gt; &gt;   as by averaging) that does not motivate a simple numerical
</em><br>
<em>&gt; &gt;   increase (or decrease) in the number of people.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;   4. The SI weights unhappiness stronger than happiness, so that
</em><br>
<em>&gt; &gt;   it focuses it efforts on helping unhappy people.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;   5. The SI develops models of all humans and what produces
</em><br>
<em>&gt; &gt;   long-term happiness in each of them.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;   6. The SI develops models of the interactions among humans
</em><br>
<em>&gt; &gt;   and how these interactions affect the happiness of each.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Rearranging deck chairs on the Titanic; in my view this goes down
</em><br>
<em>&gt; completely the wrong pathway for how to solve the problem, and it is not
</em><br>
<em>&gt; germane to the specific criticism I leveled.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; I do not pretend to have all the answers. Clearly, making RL work
</em><br>
<em>&gt; &gt; will require solution to a number of currently unsolved problems.
</em><br>
<em>&gt;
</em><br>
<em>&gt; RL is not the true Way.  But it is not my purpose to discuss that now.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; I appreciate your offer to include my URL in your article,
</em><br>
<em>&gt; &gt; where I can give my response. Please use this (please proof
</em><br>
<em>&gt; &gt; read carefully for typos in the final galleys):
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;   <a href="http://www.ssec.wisc.edu/~billh/g/AIRisk_Reply.html">http://www.ssec.wisc.edu/~billh/g/AIRisk_Reply.html</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; After I send you the revised draft, it would be helpful if I could see
</em><br>
<em>&gt; at least some reply in that URL before final galleys, so that I know I'm
</em><br>
<em>&gt; not directing my readers toward a blank page.
</em><br>
<p><em>&gt;From the time I sent that URL it contained a statement
</em><br>
that I am waiting to see the final version of AIRisk.pdf.
<br>
Now I have added a record of our email exchange, which
<br>
is a good explanation of the issues. I look forward to
<br>
seeing a revised draft.
<br>
<p><em>&gt; &gt; If you take my suggestion, by elevating your discussion to a
</em><br>
<em>&gt; &gt; general explanation of why RL systems must fail or at least using
</em><br>
<em>&gt; &gt; a strong scenario, that will make my response more friendly since
</em><br>
<em>&gt; &gt; I am happier to be named as an advocate of RL than to be
</em><br>
<em>&gt; &gt; conflated with trivial failure.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I will probably give a URL to my own reply, which might well just be a
</em><br>
<em>&gt; link to this email message.  This email does - at least by my lights -
</em><br>
<em>&gt; explain what I think the general problem is, and why the example given
</em><br>
<em>&gt; is not due to a trivial lack of computing power or failure to read
</em><br>
<em>&gt; information directly present in the data itself.
</em><br>
<em>&gt;
</em><br>
<em>&gt;  &gt; I would prefer that you not use
</em><br>
<em>&gt;  &gt; the quote you were using from my 2001 paper, as I repudiate
</em><br>
<em>&gt;  &gt; supervised learning of hard-wired values. Please use some quote
</em><br>
<em>&gt;  &gt; from and cite my 2004 AAAI paper, since there is nothing in it
</em><br>
<em>&gt;  &gt; that I repudiate yet (but you will find more refined views in my
</em><br>
<em>&gt;  &gt; 2005 on-line paper).
</em><br>
<em>&gt;
</em><br>
<em>&gt; I am sorry and I do sympathize, but there simply isn't any introductory
</em><br>
<em>&gt; paragraph in your 2004 paper that would make as much sense to the
</em><br>
<em>&gt; reader.  My current plan is for the footnote to say that your proposal
</em><br>
<em>&gt; has changed to a two-layer system, and cite the 2004 paper.  From my
</em><br>
<em>&gt; perspective they are not different in any important sense.
</em><br>
<p>I appreciate your offer of a footnote and a citation to my
<br>
2004 paper, along with your willingness to provide my URL.
<br>
<p><em>&gt; I hope this satisfies you; I do need to move on.
</em><br>
<p>I will be dissatisfied if you make your case by assuming
<br>
some algorithm, such as artificial neural networks, that I
<br>
never claimed was adequate for intelligence, or if you use
<br>
an example of a system that is adequate for intelligence
<br>
in its ability to rule the world but inadequate for
<br>
intelligence in its inability to distinguish a smiley face
<br>
from a human.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15283.html">Robin Lee Powell: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Previous message:</strong> <a href="15281.html">Keith Henson: "RE: Human motivations was Two draft papers:"</a>
<li><strong>In reply to:</strong> <a href="15268.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15283.html">Robin Lee Powell: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15283.html">Robin Lee Powell: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15282">[ date ]</a>
<a href="index.html#15282">[ thread ]</a>
<a href="subject.html#15282">[ subject ]</a>
<a href="author.html#15282">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
