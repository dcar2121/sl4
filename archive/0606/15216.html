<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [agi] Two draft papers: AI and existential risk; heuristics and biases</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="Re: [agi] Two draft papers: AI and existential risk; heuristics and biases">
<meta name="Date" content="2006-06-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [agi] Two draft papers: AI and existential risk; heuristics and biases</h1>
<!-- received="Thu Jun  8 09:26:27 2006" -->
<!-- isoreceived="20060608152627" -->
<!-- sent="Thu, 8 Jun 2006 08:25:43 -0700" -->
<!-- isosent="20060608152543" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="Re: [agi] Two draft papers: AI and existential risk; heuristics and biases" -->
<!-- id="638d4e150606080825k6095da72i1d3f54abde89320e@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="1149713497.23244.75.camel@localhost.localdomain" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=Re:%20[agi]%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Thu Jun 08 2006 - 09:25:43 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15217.html">Ben Goertzel: "Re: Two draft papers: AI and existential risk; heuristics and biases."</a>
<li><strong>Previous message:</strong> <a href="15215.html">Indriunas, Mindaugas: "Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence"</a>
<li><strong>In reply to:</strong> <a href="15198.html">Peter de Blanc: "Re: [agi] Two draft papers: AI and existential risk;	heuristics	and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15219.html">Michael Vassar: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15219.html">Michael Vassar: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15233.html">Peter de Blanc: "Re: [agi] Two draft papers: AI and existential risk; heuristics	and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15216">[ date ]</a>
<a href="index.html#15216">[ thread ]</a>
<a href="subject.html#15216">[ subject ]</a>
<a href="author.html#15216">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi,
<br>
<p>Peter: Your message was crossposted to AGI and SL4, but I am replying
<br>
only to SL4 as the topic seemed to have little directly to do with
<br>
AGI, in the form it's evolved into...
<br>
<p><em>&gt; Consider that the theory of evolution is not part of the world's
</em><br>
<em>&gt; consensus.
</em><br>
<p>Indeed, this is pathetic....  But, something like 80% of the world's
<br>
population believes in reincarnation, so this is hardly surprising...
<br>
<p>Building consensus among the world's population about anything of
<br>
significance is not very realistic.
<br>
<p>However, building consensus among the population of individuals who
<br>
share a common a-superstitious, rationalist, Singularitarian
<br>
perspective is not so obviously a vain goal...
<br>
<p><em>&gt;Consider that the Bayes' Theorem is not part of the
</em><br>
<em>&gt; scientific consensus. It isn't even part of this list's consensus!
</em><br>
<p>Hmmm... IMO, the latter statement is pretty silly...
<br>
<p>I have never met a scientist who did not accept Bayes Theorem as a
<br>
piece of mathematics.
<br>
<p>The argument between Bayesian and other approaches to uncertain
<br>
inference and statistical modeling is not about whether Bayes Theorem
<br>
is true or not, but rather about which heuristic assumptions one
<br>
should make when applying this and other probabilistic mathematics to
<br>
the real world.  Classicial statistics makes one set of heuristic
<br>
assumptions, conventional Bayesian statistics makes another ...
<br>
alternate approaches like Walley's imprecise probabilities or NARS
<br>
make yet other assumptions....  I happen to find some of these
<br>
assumption-sets preferable to others, but that's another story..
<br>
<p>-- Ben
<br>
<p>These
<br>
<em>&gt; are ancient ideas - way older than us. The consensus lags *centuries*
</em><br>
<em>&gt; behind people who think.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; It IS my contention that there is a relatively simple,
</em><br>
<em>&gt; &gt; inductively-robust (in a mathematical proof sense) formulation of
</em><br>
<em>&gt; &gt; friendliness that will guarantee that there won't be effects that *I*
</em><br>
<em>&gt; &gt; consider undesirable, horrible, or immoral.  It will, of course/however,
</em><br>
<em>&gt; &gt; produce a number of effects that others will decry as undesirable, horrible,
</em><br>
<em>&gt; &gt; or immoral -- like allowing abortion and assisted suicide in a reasonable
</em><br>
<em>&gt; &gt; number of cases, NOT allowing the killing of infidels, allowing almost any
</em><br>
<em>&gt; &gt; personal modifications (with  truly informed consent) that are non-harmful
</em><br>
<em>&gt; &gt; to others, NOT allowing the imposition of personal modifications whether
</em><br>
<em>&gt; &gt; they be physical, mental, or spiritual, etc.
</em><br>
<em>&gt;
</em><br>
<em>&gt; How relatively simple? Evolution doesn't do simple. I doubt that any
</em><br>
<em>&gt; human goal system has a simple mathematical formalization.
</em><br>
<em>&gt;
</em><br>
<em>&gt; -------
</em><br>
<em>&gt; To unsubscribe, change your address, or temporarily deactivate your subscription,
</em><br>
<em>&gt; please go to <a href="http://v2.listbox.com/member/?listname=<a href="mailto:agi@v2.listbox.com?Subject=Re:%20[agi]%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases">agi@v2.listbox.com</a>">http://v2.listbox.com/member/?listname=<a href="mailto:agi@v2.listbox.com?Subject=Re:%20[agi]%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases">agi@v2.listbox.com</a></a>
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15217.html">Ben Goertzel: "Re: Two draft papers: AI and existential risk; heuristics and biases."</a>
<li><strong>Previous message:</strong> <a href="15215.html">Indriunas, Mindaugas: "Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence"</a>
<li><strong>In reply to:</strong> <a href="15198.html">Peter de Blanc: "Re: [agi] Two draft papers: AI and existential risk;	heuristics	and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15219.html">Michael Vassar: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15219.html">Michael Vassar: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15233.html">Peter de Blanc: "Re: [agi] Two draft papers: AI and existential risk; heuristics	and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15216">[ date ]</a>
<a href="index.html#15216">[ thread ]</a>
<a href="subject.html#15216">[ subject ]</a>
<a href="author.html#15216">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
