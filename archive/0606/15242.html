<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence</title>
<meta name="Author" content="Indriunas, Mindaugas (inyuki@gmail.com)">
<meta name="Subject" content="Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence">
<meta name="Date" content="2006-06-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence</h1>
<!-- received="Fri Jun  9 03:43:36 2006" -->
<!-- isoreceived="20060609094336" -->
<!-- sent="Fri, 9 Jun 2006 18:34:22 +0900" -->
<!-- isosent="20060609093422" -->
<!-- name="Indriunas, Mindaugas" -->
<!-- email="inyuki@gmail.com" -->
<!-- subject="Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence" -->
<!-- id="e7160b500606090234p178a7058ncf69f85c7213dd62@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="d7e54a6c0606081714t3c8a2660q1704837c7e39bf0e@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Indriunas, Mindaugas (<a href="mailto:inyuki@gmail.com?Subject=Re:%20Ultimate%20Goal%20of%20Intelligence;%20thoughts%20on%20creation%20of%20Artificial%20Intelligence"><em>inyuki@gmail.com</em></a>)<br>
<strong>Date:</strong> Fri Jun 09 2006 - 03:34:22 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15243.html">Indriunas, Mindaugas: "Objective Good (Definition)"</a>
<li><strong>Previous message:</strong> <a href="15241.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>In reply to:</strong> <a href="15240.html">Joel Pitt: "Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15294.html">Rik van Riel: "Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15242">[ date ]</a>
<a href="index.html#15242">[ thread ]</a>
<a href="subject.html#15242">[ subject ]</a>
<a href="author.html#15242">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; In my opinion these drivers seem to define the ultimate goal of
</em><br>
<em>&gt; (human) intelligence to be reproduce and support offspring. Even if
</em><br>
<em>&gt; some us like to neglect these drivers in favour of higher goals.
</em><br>
<p>Logically thinking, yes, the goals to survive by reproduction and
<br>
other means, and generally - survive, - are the natural goals of any
<br>
material entity that actually survives.
<br>
<p>However, this goal is induced by time, which is not clear, if is an
<br>
actual reality.
<br>
<p>Generally speaking, Time phenomenon filters out the ones that don't
<br>
want to survive, so the further the time is going to the future, the
<br>
more entities are filtered-out by the passage of time, and the more
<br>
entities should  logically remain with the goal to survive.
<br>
<p>This once made me think that survival is the ultimate goal of living beings.
<br>
<p>According to the general knowledge, at some point in the history, it
<br>
happened that once the beings acquired some intelligence, and became
<br>
able to predict/guess, this was somewhat beneficial for their
<br>
survival, and here we have intelligent beings.
<br>
<p>However, if one tried to imagine a being, that survives indefinitely,
<br>
this goal would cease to exist.
<br>
<p>Moreover, I don't see  the meaning of only-existing.
<br>
<p>Of course, setting a goal &quot;to exist&quot;, in a very wide sense means to
<br>
become absolutely 100% sure of your existence in all the time, 100%
<br>
safe, which requires &quot;omniscence&quot;, and here it is almost no
<br>
difference, weather you set a goal &quot;to exist&quot;, or &quot;to understand
<br>
everything&quot;.
<br>
<p>However, the second one (&quot;to understand everything&quot;) is easier to
<br>
define  procedurally. Moreover, it is clearer if you try to ask, what
<br>
is the meaning of existence at all -- by setting a goal &quot;to understand
<br>
everything&quot;, rather than &quot;to exist&quot;, you automatically not give up
<br>
answering this question, not give up answering the question about
<br>
time, and reality, and everything, including the existing emotions of
<br>
other people, existing feelings, and other phenomena.
<br>
<p>On 6/9/06, Joel Pitt &lt;<a href="mailto:joel.pitt@gmail.com?Subject=Re:%20Ultimate%20Goal%20of%20Intelligence;%20thoughts%20on%20creation%20of%20Artificial%20Intelligence">joel.pitt@gmail.com</a>&gt; wrote:
<br>
<em>&gt; On 6/9/06, Indriunas, Mindaugas &lt;<a href="mailto:inyuki@gmail.com?Subject=Re:%20Ultimate%20Goal%20of%20Intelligence;%20thoughts%20on%20creation%20of%20Artificial%20Intelligence">inyuki@gmail.com</a>&gt; wrote:
</em><br>
<em>&gt; &gt; At this point, I had thought of a hypothesis, that the ultimate goal
</em><br>
<em>&gt; &gt; of any intelligence, as a &quot;thinking entity&quot;, is purely - to understand
</em><br>
<em>&gt; &gt; everything.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Consider those people people that don't purely seek to understand
</em><br>
<em>&gt; everything. People driven by greed, lust, pleasure. These may still be
</em><br>
<em>&gt; attempting &quot;to understand&quot;, but you need to address how these goals or
</em><br>
<em>&gt; motivators behind action relate to your premise that the ultimate goal
</em><br>
<em>&gt; of an intelligence is to understand.
</em><br>
<em>&gt;
</em><br>
<em>&gt; In my opinion these drivers seem to define the ultimate goal of
</em><br>
<em>&gt; (human) intelligence to be reproduce and support offspring. Even if
</em><br>
<em>&gt; some us like to neglect these drivers in favour of higher goals.
</em><br>
<em>&gt;
</em><br>
<em>&gt; --
</em><br>
<em>&gt; -Joel
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Wish not to seem, but to be, the best.&quot;
</em><br>
<em>&gt;                 -- Aeschylus
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15243.html">Indriunas, Mindaugas: "Objective Good (Definition)"</a>
<li><strong>Previous message:</strong> <a href="15241.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>In reply to:</strong> <a href="15240.html">Joel Pitt: "Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15294.html">Rik van Riel: "Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15242">[ date ]</a>
<a href="index.html#15242">[ thread ]</a>
<a href="subject.html#15242">[ subject ]</a>
<a href="author.html#15242">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
