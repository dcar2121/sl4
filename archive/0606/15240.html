<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence</title>
<meta name="Author" content="Joel Pitt (joel.pitt@gmail.com)">
<meta name="Subject" content="Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence">
<meta name="Date" content="2006-06-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence</h1>
<!-- received="Thu Jun  8 20:07:40 2006" -->
<!-- isoreceived="20060609020740" -->
<!-- sent="Fri, 9 Jun 2006 12:14:56 +1200" -->
<!-- isosent="20060609001456" -->
<!-- name="Joel Pitt" -->
<!-- email="joel.pitt@gmail.com" -->
<!-- subject="Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence" -->
<!-- id="d7e54a6c0606081714t3c8a2660q1704837c7e39bf0e@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="e7160b500606080602yca8b256lfb9e7a0175cc5848@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Joel Pitt (<a href="mailto:joel.pitt@gmail.com?Subject=Re:%20Ultimate%20Goal%20of%20Intelligence;%20thoughts%20on%20creation%20of%20Artificial%20Intelligence"><em>joel.pitt@gmail.com</em></a>)<br>
<strong>Date:</strong> Thu Jun 08 2006 - 18:14:56 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15241.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Previous message:</strong> <a href="15239.html">Jef Allbright: "Re: [agi] Four axioms"</a>
<li><strong>In reply to:</strong> <a href="15215.html">Indriunas, Mindaugas: "Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15242.html">Indriunas, Mindaugas: "Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence"</a>
<li><strong>Reply:</strong> <a href="15242.html">Indriunas, Mindaugas: "Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence"</a>
<li><strong>Reply:</strong> <a href="15294.html">Rik van Riel: "Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15240">[ date ]</a>
<a href="index.html#15240">[ thread ]</a>
<a href="subject.html#15240">[ subject ]</a>
<a href="author.html#15240">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 6/9/06, Indriunas, Mindaugas &lt;<a href="mailto:inyuki@gmail.com?Subject=Re:%20Ultimate%20Goal%20of%20Intelligence;%20thoughts%20on%20creation%20of%20Artificial%20Intelligence">inyuki@gmail.com</a>&gt; wrote:
<br>
<em>&gt; At this point, I had thought of a hypothesis, that the ultimate goal
</em><br>
<em>&gt; of any intelligence, as a &quot;thinking entity&quot;, is purely - to understand
</em><br>
<em>&gt; everything.
</em><br>
<p>Consider those people people that don't purely seek to understand
<br>
everything. People driven by greed, lust, pleasure. These may still be
<br>
attempting &quot;to understand&quot;, but you need to address how these goals or
<br>
motivators behind action relate to your premise that the ultimate goal
<br>
of an intelligence is to understand.
<br>
<p>In my opinion these drivers seem to define the ultimate goal of
<br>
(human) intelligence to be reproduce and support offspring. Even if
<br>
some us like to neglect these drivers in favour of higher goals.
<br>
<p><pre>
-- 
-Joel
&quot;Wish not to seem, but to be, the best.&quot;
                -- Aeschylus
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15241.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Previous message:</strong> <a href="15239.html">Jef Allbright: "Re: [agi] Four axioms"</a>
<li><strong>In reply to:</strong> <a href="15215.html">Indriunas, Mindaugas: "Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15242.html">Indriunas, Mindaugas: "Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence"</a>
<li><strong>Reply:</strong> <a href="15242.html">Indriunas, Mindaugas: "Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence"</a>
<li><strong>Reply:</strong> <a href="15294.html">Rik van Riel: "Re: Ultimate Goal of Intelligence; thoughts on creation of Artificial Intelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15240">[ date ]</a>
<a href="index.html#15240">[ thread ]</a>
<a href="subject.html#15240">[ subject ]</a>
<a href="author.html#15240">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
