<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Two draft papers: AI and existential risk; heuristics and biases</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Two draft papers: AI and existential risk; heuristics and biases">
<meta name="Date" content="2006-06-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Two draft papers: AI and existential risk; heuristics and biases</h1>
<!-- received="Mon Jun 12 18:34:09 2006" -->
<!-- isoreceived="20060613003409" -->
<!-- sent="Mon, 12 Jun 2006 17:34:09 -0700" -->
<!-- isosent="20060613003409" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Two draft papers: AI and existential risk; heuristics and biases" -->
<!-- id="448E0801.1010805@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="Pine.GSO.4.44.0606071216390.18285-100000@demedici.ssec.wisc.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Jun 12 2006 - 18:34:09 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15269.html">Eliezer S. Yudkowsky: "Re: Fwd: [ai-philosophy] Robotic evolution and ethics"</a>
<li><strong>Previous message:</strong> <a href="15267.html">Jeff Medina: "Re: Fwd: [ai-philosophy] Robotic evolution and ethics"</a>
<li><strong>In reply to:</strong> <a href="15189.html">Bill Hibbard: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15282.html">Bill Hibbard: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15282.html">Bill Hibbard: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15268">[ date ]</a>
<a href="index.html#15268">[ thread ]</a>
<a href="subject.html#15268">[ subject ]</a>
<a href="author.html#15268">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Bill Hibbard wrote:
<br>
<em>&gt; Eliezer,
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;I don't think it
</em><br>
<em>&gt;&gt;inappropriate to cite a problem that is general to supervised learning
</em><br>
<em>&gt;&gt;and reinforcement, when your proposal is to, in general, use supervised
</em><br>
<em>&gt;&gt;learning and reinforcement.  You can always appeal to a &quot;different
</em><br>
<em>&gt;&gt;algorithm&quot; or a &quot;different implementation&quot; that, in some unspecified
</em><br>
<em>&gt;&gt;way, doesn't have a problem.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But you are not demonstrating a general problem. You are
</em><br>
<em>&gt; instead relying on specific examples (primitive neural
</em><br>
<em>&gt; networks and systems that cannot distingish a human from
</em><br>
<em>&gt; a smiley) that fail trivially. You should be clear whether
</em><br>
<em>&gt; you claim that reinforcement learning (RL) must inevitably
</em><br>
<em>&gt; lead to:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;   1. A failure of intelligence.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; or:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;   2. A failure of friendliness.
</em><br>
<p>As it happens, my model of intelligence says that what I would call 
<br>
&quot;reinforcement learning&quot; is not, in fact, adequate to intelligence. 
<br>
However, the fact that you believe &quot;reinforcement learning&quot; is adequate 
<br>
to intelligence, suggests that you would take any possible factor that I 
<br>
thought was additionally necessary, and claim that it was part of the 
<br>
framework you regarded as &quot;reinforcement learning&quot;.
<br>
<p>What I am presently discussing is failure of friendliness.  However, the 
<br>
fact that we use different models of intelligence is also responsible 
<br>
for our disagreement about this second point.  Explaining a model of 
<br>
intelligence tends to be very difficult, and so, from my perspective, 
<br>
the main important thing is that you should understand that I have a 
<br>
legitimate (that is, honestly meant) disagreement with you about what 
<br>
reinforcement systems do and what happens in practice when you use them.
<br>
<p>By the way, I've got some other tasks to take on in the near future, and 
<br>
I may not be able to discuss the actual technical disagreement at 
<br>
length.  As said, I will include a footnote pointing to your 
<br>
disagreement, and to my response.
<br>
<p><em>&gt; Your example of the US Army's primitive neural network
</em><br>
<em>&gt; experiments is a failure of intelligence. Your statement
</em><br>
<em>&gt; about smiley faces assumes a general success at intelligence
</em><br>
<em>&gt; by the system, but an absurd failure of intelligence in the
</em><br>
<em>&gt; part of the system that recognizes humans and their emotions,
</em><br>
<em>&gt; leading to a failure of friendliness.
</em><br>
<p>Let me try to analyze the model of intelligence behind your statement. 
<br>
You're thinking something along the lines of:
<br>
<p>&quot;Supervised algorithms&quot; (sort of like those in the most advanced 
<br>
artificial neural networks) give rise to &quot;reinforcement learning&quot;;
<br>
<p>&quot;Reinforcement learning&quot; gives rise to &quot;intelligence&quot;;
<br>
<p>&quot;Intelligence&quot; is what lets an AI shape the world, and also what tells 
<br>
it that tiny molecular smiley faces are bad examples of happiness, while 
<br>
an actual human smiling is a good example of happiness.
<br>
<p>In your journal paper from 2004, you seem to propose using a two-layer 
<br>
system of reinforcement, with the first layer being observed agreement 
<br>
from humans as a reinforcer of its definition of happiness, and the 
<br>
second layer being reinforcement of behaviors that lead to &quot;happiness&quot; 
<br>
as thus defined.  So in this case, we substitute:  &quot;'Intelligence' is 
<br>
what tells an AI that tiny molecular speakers chirping &quot;Yes! Good job!&quot; 
<br>
are bad examples of agreement with its definition of happiness, while an 
<br>
actual human saying &quot;Yes! Good job!&quot; is a good example.&quot;
<br>
<p>After all, it sure seems stupid to confuse human smiles with tiny 
<br>
molecular smiley faces!  How silly of the army tank classifier, not to 
<br>
realize that it was supposed to detect tanks, instead of detecting 
<br>
cloudy days!
<br>
<p>But a neural network the size of a planet, given the same examples, 
<br>
would have failed in the same way.
<br>
<p>You previously said:
<br>
<p><em>&gt; When it is feasible to build a super-intelligence, it will
</em><br>
<em>&gt; be feasible to build hard-wired recognition of &quot;human facial
</em><br>
<em>&gt; expressions, human voices and human body language&quot; (to use
</em><br>
<em>&gt; the words of mine that you quote) that exceed the recognition
</em><br>
<em>&gt; accuracy of current humans such as you and me, and will
</em><br>
<em>&gt; certainly not be fooled by &quot;tiny molecular pictures of
</em><br>
<em>&gt; smiley-faces.&quot; You should not assume such a poor
</em><br>
<em>&gt; implementation of my idea that it cannot make
</em><br>
<em>&gt; discriminations that are trivial to current humans.
</em><br>
<p>It's trivial to discriminate between a photo of a picture with a 
<br>
camouflaged tank, and a photo of an empty forest.  They're different 
<br>
pixel maps.  If you transform them into strings of 1s and 0s, they're 
<br>
different strings.  Discriminating between them is as simple as testing 
<br>
them for equality.
<br>
<p>But there's an exponentially vast space of functions that classify all 
<br>
possible pixel-maps of a fixed size into &quot;plus&quot; and &quot;minus&quot; spaces.  If 
<br>
you talk about the space of all possible computations that implement 
<br>
these classification functions, the space is trivially infinite and 
<br>
trivially undecidable.
<br>
<p>Of course a super-AI, or an ordinary neural network, can trivially 
<br>
discriminate between a tiny molecular picture of a smiley face, or a 
<br>
smiling human, or between two pictures of the same smiling human from a 
<br>
slightly different angle.  The issue is whether the AI will *classify* 
<br>
these trivially discriminable stimuli into &quot;plus&quot; and &quot;minus&quot; spaces the 
<br>
way *you* hope it will.
<br>
<p>If you look at the actual pixel-map that shows a camouflaged tank, 
<br>
there's not a little XML tag in the picture itself that says &quot;Hey, 
<br>
network, classify this picture as a good example!&quot;  The classification 
<br>
is not a property of the picture alone.  Thinking as though the 
<br>
classification is a property of the picture is an instance of Mind 
<br>
Projection Fallacy, as mentioned in my AI chapter.
<br>
<p>Maybe you actually *wanted* the neural network to discriminate sunny 
<br>
days from cloudy days.  So you fed it exactly the same data instances, 
<br>
with exactly the same supervision, and used a slightly different 
<br>
learning algorithm - and found to your dismay that the network was so 
<br>
stupid, it learned to detect tanks instead of cloudy days.  But a really 
<br>
smart intelligence would not be so stupid that it couldn't tell the 
<br>
difference between cloudy days and sunny days.
<br>
<p>There are many possible ways to *classify* different data instances, and 
<br>
the classification involves information that is not directly present in 
<br>
the instances.  In contrast, finding that two instances are not 
<br>
identical uses only information present in the data instances 
<br>
themselves.  Saying that a superintelligence could discriminate between 
<br>
tiny molecular smiley faces and human smiles is, I would say, correct. 
<br>
But it is not correct to say that any sufficiently intelligent mind will 
<br>
automatically *classify* the instances the way you want them to.
<br>
<p>Let's say that the AI's training data is:
<br>
<p>Dataset 1:
<br>
<p>Plus:  {Smile_1, Smile_2, Smile_3}
<br>
Minus: {Dog_1, Cat_1, Dog_2, Dog_3, Cat_2, Dog_4, Boat_1, Car_1, Dog_5, 
<br>
Cat_3, Boat_2, Dog_6}
<br>
<p>Now the AI grows up into a superintelligence, and encounters this data:
<br>
<p>Dataset 2:  {Dog_7, Cat_4, Galaxy_1, Dog_8, Nanofactory_1, Smiley_1, 
<br>
Dog_9, Cat_5, Smiley_2, Smile_4, Boat_3, Galaxy_2, Nanofactory_2, 
<br>
Smiley_3, Cat_6, Boat_4, Smile_5, Galaxy_3}
<br>
<p>It is not a property *of dataset 2* that the classification *you want* is:
<br>
<p>Plus:  {Smile_4, Smile_5}
<br>
Minus: {Dog_7, Cat_4, Galaxy_1, Dog_8, Nanofactory_1, Smiley_1, Dog_9, 
<br>
Cat_5, Smiley_2, Boat_3, Galaxy_2, Nanofactory_2, Smiley_3, Cat_6, 
<br>
Boat_4, Galaxy_3}
<br>
<p>Rather than:
<br>
<p>Plus:  {Smiley_1, Smiley_2, Smile_4, Smiley_3, Smile_5}
<br>
Minus: {Dog_7, Cat_4, Galaxy_1, Dog_8, Nanofactory_1, Dog_9, Cat_5, 
<br>
Boat_3, Galaxy_2, Nanofactory_2, Cat_6, Boat_4, Galaxy_3}
<br>
<p>If you want the top classification rather than the bottom one, you must 
<br>
infuse into the *prior state* of the AI some *additional information*, 
<br>
not present in dataset 2.  That, of course, is the point of giving the 
<br>
AI dataset 1.  But if you do not understand *how* the AI is classifying 
<br>
dataset 1, and then the AI enters a drastically different context, there 
<br>
is the danger that the AI is classifying dataset 1 using a very 
<br>
different method from the one *you yourself originally used* to classify 
<br>
dataset 1, and that the AI will, as a result, classify dataset 2 in ways 
<br>
different from how you yourself would have classified dataset 2.  (This 
<br>
line of reasoning leads to &quot;Coherent Extrapolated Volition&quot;, if I go on 
<br>
to ask what happens if I would have wanted to classify dataset 1 itself 
<br>
a bit differently if I had more empirical knowledge, or thought faster.)
<br>
<p>You cannot throw computing power at this problem.  Brute force, or even 
<br>
brute intelligence, is not the issue here.
<br>
<p><em>&gt; If your claim is that RL can succeed at intelligence but must
</em><br>
<em>&gt; lead to a failure of friendliness, then it is reasonable to
</em><br>
<em>&gt; cite and quote me. But please use my 2004 AAAI paper . . .
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;If you are genuinely repudiating your old ideas ...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; . . . use my 2004 AAAI paper because I do repudiate the
</em><br>
<em>&gt; statement in my 2001 paper that recognition of humans and
</em><br>
<em>&gt; their emotions should be hard-wired (i.e., static). That
</em><br>
<em>&gt; is just the section of my 2001 paper that you quoted.
</em><br>
<p>I will include, in the footnote, a statement that your 2004 paper 
<br>
proposes a two-layer system.  But this is not at all germane to the 
<br>
point I was making - though the footnote will serve to notify readers 
<br>
that your ideas have not remained static.  Please remember that my 
<br>
purpose is not to present Bill Hibbard's current ideas, but to use, as 
<br>
an example of failure, an idea that you published in a peer-reviewed 
<br>
journal in 2001.  If you have taken alarm at the notion of hardwiring 
<br>
happiness as reinforcement, then you ought to say something like: 
<br>
&quot;Though it makes me uncomfortable, I can't ethically argue that you 
<br>
should not publish my old mistake as a warning to others who might 
<br>
otherwise follow in my footsteps; but you must include a footnote saying 
<br>
that I now also agree it's a terrible idea.&quot;
<br>
<p>Most importantly, your 2004 paper simply does not contain any paragraph 
<br>
that serves the introductory and expository role of the paragraph I 
<br>
quoted from your 2001 paper.  There's nothing I can quote from 2004 that 
<br>
will make as much sense to the reader.  If I were praising your 2001 
<br>
paper, rather than criticizing it, would you have the same objection?
<br>
<p><em>&gt; Not that I am sure that hard-wired recognition of humans and
</em><br>
<em>&gt; their emotions inevitably leads to a failure of friendliness,
</em><br>
<p>Okay, now it looks like you *haven't* taken alarm at this.
<br>
<p><em>&gt; since the super-intelligence (SI) may understand that humans
</em><br>
<em>&gt; would be happier if they could evolve to other physical forms
</em><br>
<em>&gt; but still be recognized by the SI as humans, and decide to
</em><br>
<em>&gt; modify itself (or build an improved replacement). But if this
</em><br>
<em>&gt; is my scenario, then why not design continuing learning of
</em><br>
<em>&gt; recognition of humans and their emotions into the system in
</em><br>
<em>&gt; the first place. Hence my change of views.
</em><br>
<p>I think at this point you're just putting yourself into the SI's shoes, 
<br>
empathically, using your own brain to make predictions about what the SI 
<br>
will do.  Not, reasoning about the technical difficulties associated 
<br>
with infusing certain information into the SI.
<br>
<p><em>&gt; I am sure you have not repudiated everything in CFAI,
</em><br>
<p>I can't think offhand of any particular positive proposal I would say 
<br>
was correct.  (Maybe the section in which I rederived the Bayesian value 
<br>
of information, but that's standard.)
<br>
<p>Some negative criticisms of other possible methods and their failures, 
<br>
as presented in CFAI, continue to hold.  It is far easier to say what is 
<br>
wrong than what is right.
<br>
<p><em>&gt; and I
</em><br>
<em>&gt; have not repudiated everything in my earlier publications.
</em><br>
<em>&gt; I continue to believe that RL is critical to acheiving
</em><br>
<em>&gt; intelligence with a feasible amount of computing resources,
</em><br>
<em>&gt; and I continue to believe that collective long-term human
</em><br>
<em>&gt; happiness should be the basic reinforcement value for SI.
</em><br>
<em>&gt; But I now think that a SI should continue to learn recognition
</em><br>
<em>&gt; of humans and their emotions via reinforcement, rather than
</em><br>
<em>&gt; these recognitions being hard-wired as the result of supervised
</em><br>
<em>&gt; learning. My recent writings have also refined my views about
</em><br>
<em>&gt; how human happiness should be defined, and how the happiness of
</em><br>
<em>&gt; many people should be combined into an overall reinforcement
</em><br>
<em>&gt; value.
</em><br>
<p>It is not my present purpose to criticize these new ideas of yours at 
<br>
length, only the technical problem with using reinforcement learning to 
<br>
do pretty much anything.
<br>
<p><em>&gt;&gt;I see no relevant difference between these two proposals, except that
</em><br>
<em>&gt;&gt;the paragraph you cite (presumably as a potential replacement) is much
</em><br>
<em>&gt;&gt;less clear to the outside academic reader.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If you see no difference between my earlier and later ideas,
</em><br>
<em>&gt; then please use a scenario based on my later papers. That will
</em><br>
<em>&gt; be a better demonstration of the strength of your arguments,
</em><br>
<em>&gt; and be fairer to me.
</em><br>
<p>If you had a paragraph serving an equivalent introductory purpose in a 
<br>
later peer-reviewed paper, I would use it.  But the paragraphs from your 
<br>
later papers are much less clear to the outside academic reader, and it 
<br>
would not be clear what I am criticizing, even though it is the same 
<br>
problem in both cases.  That's the sticking point from my perspective.
<br>
<p><em>&gt; Of course, it would be best to demonstrate your claim (either
</em><br>
<em>&gt; that RL must lead to a failure of intelligence, or can succeed
</em><br>
<em>&gt; at intelligence but must lead to a failure of friendliness) in
</em><br>
<em>&gt; general. But if you cannot do that and must rely on a specific
</em><br>
<em>&gt; example, then at least do not pick an example that fails for
</em><br>
<em>&gt; trivial reasons.
</em><br>
<p>The reasons are not trivial; they are general.  I know it seems &quot;stupid&quot; 
<br>
and &quot;trivial&quot; to you, but getting rid of the stupidness and triviality 
<br>
is a humongous nontrivial challenge that cannot be solved by throwing 
<br>
brute intelligence at the problem.
<br>
<p>You do not need to agree with my criticism before I can publish a paper 
<br>
critical of your ideas; all the more so if I include a URL to your 
<br>
rebuttal.  Let the reader judge.
<br>
<p><em>&gt; As I wrote above, if you think RL must fail at intelligence,
</em><br>
<em>&gt; you would be best to quote Eric Baum.
</em><br>
<p>Eric Baum's thesis is not reinforcement learning, it is Occam's Razor. 
<br>
Frankly I think you are too hung up on reinforcement learning.  But that 
<br>
is a separate issue.
<br>
<p><em>&gt; If you think RL can succeed at intelligence but must fail at
</em><br>
<em>&gt; friendliness, but just want to demonstrate it for a specific
</em><br>
<em>&gt; example, then use a scenario in which:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;   1. The SI recognizes humans and their emotions as accurately
</em><br>
<em>&gt;   as any human, and continually relearns that recognition as
</em><br>
<em>&gt;   humans evolve (for example, to become SIs themselves).
</em><br>
<p>You say &quot;recognize as accurately as any human&quot;, implying it is a feature 
<br>
of the data.  Better to say &quot;classify in the same way humans do&quot;.
<br>
<p><em>&gt;   2. The SI values people after death at the maximally unhappy
</em><br>
<em>&gt;   value, in order to avoid motivating the SI to kill unhappy
</em><br>
<em>&gt;   people.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;   3. The SI combines the happiness of many people in a way (such
</em><br>
<em>&gt;   as by averaging) that does not motivate a simple numerical
</em><br>
<em>&gt;   increase (or decrease) in the number of people.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;   4. The SI weights unhappiness stronger than happiness, so that
</em><br>
<em>&gt;   it focuses it efforts on helping unhappy people.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;   5. The SI develops models of all humans and what produces
</em><br>
<em>&gt;   long-term happiness in each of them.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;   6. The SI develops models of the interactions among humans
</em><br>
<em>&gt;   and how these interactions affect the happiness of each.
</em><br>
<p>Rearranging deck chairs on the Titanic; in my view this goes down 
<br>
completely the wrong pathway for how to solve the problem, and it is not 
<br>
germane to the specific criticism I leveled.
<br>
<p><em>&gt; I do not pretend to have all the answers. Clearly, making RL work
</em><br>
<em>&gt; will require solution to a number of currently unsolved problems.
</em><br>
<p>RL is not the true Way.  But it is not my purpose to discuss that now.
<br>
<p><em>&gt; I appreciate your offer to include my URL in your article,
</em><br>
<em>&gt; where I can give my response. Please use this (please proof
</em><br>
<em>&gt; read carefully for typos in the final galleys):
</em><br>
<em>&gt; 
</em><br>
<em>&gt;   <a href="http://www.ssec.wisc.edu/~billh/g/AIRisk_Reply.html">http://www.ssec.wisc.edu/~billh/g/AIRisk_Reply.html</a>
</em><br>
<p>After I send you the revised draft, it would be helpful if I could see 
<br>
at least some reply in that URL before final galleys, so that I know I'm 
<br>
not directing my readers toward a blank page.
<br>
<p><em>&gt; If you take my suggestion, by elevating your discussion to a
</em><br>
<em>&gt; general explanation of why RL systems must fail or at least using
</em><br>
<em>&gt; a strong scenario, that will make my response more friendly since
</em><br>
<em>&gt; I am happier to be named as an advocate of RL than to be
</em><br>
<em>&gt; conflated with trivial failure.
</em><br>
<p>I will probably give a URL to my own reply, which might well just be a 
<br>
link to this email message.  This email does - at least by my lights - 
<br>
explain what I think the general problem is, and why the example given 
<br>
is not due to a trivial lack of computing power or failure to read 
<br>
information directly present in the data itself.
<br>
<p><em> &gt; I would prefer that you not use
</em><br>
<em> &gt; the quote you were using from my 2001 paper, as I repudiate
</em><br>
<em> &gt; supervised learning of hard-wired values. Please use some quote
</em><br>
<em> &gt; from and cite my 2004 AAAI paper, since there is nothing in it
</em><br>
<em> &gt; that I repudiate yet (but you will find more refined views in my
</em><br>
<em> &gt; 2005 on-line paper).
</em><br>
<p>I am sorry and I do sympathize, but there simply isn't any introductory 
<br>
paragraph in your 2004 paper that would make as much sense to the 
<br>
reader.  My current plan is for the footnote to say that your proposal 
<br>
has changed to a two-layer system, and cite the 2004 paper.  From my 
<br>
perspective they are not different in any important sense.
<br>
<p>I hope this satisfies you; I do need to move on.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15269.html">Eliezer S. Yudkowsky: "Re: Fwd: [ai-philosophy] Robotic evolution and ethics"</a>
<li><strong>Previous message:</strong> <a href="15267.html">Jeff Medina: "Re: Fwd: [ai-philosophy] Robotic evolution and ethics"</a>
<li><strong>In reply to:</strong> <a href="15189.html">Bill Hibbard: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15282.html">Bill Hibbard: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15282.html">Bill Hibbard: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15268">[ date ]</a>
<a href="index.html#15268">[ thread ]</a>
<a href="subject.html#15268">[ subject ]</a>
<a href="author.html#15268">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
