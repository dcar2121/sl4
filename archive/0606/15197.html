<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Cats, was I am a moral, intelligent being</title>
<meta name="Author" content="Martin Striz (mstriz@gmail.com)">
<meta name="Subject" content="Re: Cats, was I am a moral, intelligent being">
<meta name="Date" content="2006-06-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Cats, was I am a moral, intelligent being</h1>
<!-- received="Wed Jun  7 14:48:31 2006" -->
<!-- isoreceived="20060607204831" -->
<!-- sent="Wed, 7 Jun 2006 16:48:21 -0400" -->
<!-- isosent="20060607204821" -->
<!-- name="Martin Striz" -->
<!-- email="mstriz@gmail.com" -->
<!-- subject="Re: Cats, was I am a moral, intelligent being" -->
<!-- id="cd9526860606071348y945e297h573b0f23693e50db@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="5.1.0.14.0.20060607040535.04da8360@pop.bloor.is.net.cable.rogers.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Martin Striz (<a href="mailto:mstriz@gmail.com?Subject=Re:%20Cats,%20was%20I%20am%20a%20moral,%20intelligent%20being"><em>mstriz@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Jun 07 2006 - 14:48:21 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15198.html">Peter de Blanc: "Re: [agi] Two draft papers: AI and existential risk;	heuristics	and biases"</a>
<li><strong>Previous message:</strong> <a href="15196.html">Martin Striz: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<li><strong>In reply to:</strong> <a href="15181.html">Keith Henson: "Cats, was I am a moral, intelligent being"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15191.html">David Picon Alvarez: "Re: I am a moral, intelligent being (was Re: Two draft papers:  AI and existential risk; heuristics and biases)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15197">[ date ]</a>
<a href="index.html#15197">[ thread ]</a>
<a href="subject.html#15197">[ subject ]</a>
<a href="author.html#15197">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 6/7/06, Keith Henson &lt;<a href="mailto:hkhenson@rogers.com?Subject=Re:%20Cats,%20was%20I%20am%20a%20moral,%20intelligent%20being">hkhenson@rogers.com</a>&gt; wrote:
<br>
<p><em>&gt; My point was that more intelligent AIs or upgraded humans may have a
</em><br>
<em>&gt; different view of what is moral as we have a different view of what is
</em><br>
<em>&gt; moral compared to cats.  If you upgraded a cat to human level intelligence
</em><br>
<em>&gt; would it think controlling the population of regular cats the way we do was
</em><br>
<em>&gt; moral?  (I have no idea.)
</em><br>
<p>This is the problem that I have with &quot;collective volition.&quot;  There is
<br>
not such thing.  There are millions of independent trajectories that
<br>
would lead to happiness.  Cats love nothing better than to hunt.  So
<br>
if I were to upgrade a cat, should I give it the intelligence to build
<br>
mouse traps while retaining the desire to kill things, or should I
<br>
engineer it to love mice and other woodland creatures?   From an
<br>
engineering standpoint, all forms of happiness are equivalent.  It is
<br>
not better to be Socrates than to be the pig.  If you don't think so,
<br>
you're projecting your own anthropomorphosism.
<br>
<p>Coding a &quot;friendly&quot; AI implies some objective measure of &quot;happiness.&quot;
<br>
But I don't think such a thing exists in objective, engineering terms.
<br>
&nbsp;Happiness can be any feedforward process.  There are millinos of
<br>
trajectories by which an AI could upgrade us to be what we would want
<br>
to be, or it could engineer us to /want to be/ what we are.  When you
<br>
understand that there's no difference, then you understand that
<br>
happiness is an infinite goal set, which means that it's an empty goal
<br>
set.
<br>
<p>So an AI engineered to &quot;make people happy&quot; will do anything it pleases
<br>
once it recursively self-improves.  Now you COULD engineer an AI  to
<br>
make people happy by the metrics with which we measure happiness now,
<br>
as humans 1.0.  In other words, you hardcode as goals: make people
<br>
healthier, live longer, smarter, etc.  You have to narrow the domain
<br>
for the AI.
<br>
<p>Ben's suggestion of creating limited AGI, and within that framework,
<br>
limited FAI, is a good one.
<br>
<p>Martin
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15198.html">Peter de Blanc: "Re: [agi] Two draft papers: AI and existential risk;	heuristics	and biases"</a>
<li><strong>Previous message:</strong> <a href="15196.html">Martin Striz: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<li><strong>In reply to:</strong> <a href="15181.html">Keith Henson: "Cats, was I am a moral, intelligent being"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15191.html">David Picon Alvarez: "Re: I am a moral, intelligent being (was Re: Two draft papers:  AI and existential risk; heuristics and biases)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15197">[ date ]</a>
<a href="index.html#15197">[ thread ]</a>
<a href="subject.html#15197">[ subject ]</a>
<a href="author.html#15197">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
