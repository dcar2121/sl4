<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Two draft papers: AI and existential risk; heuristics and biases</title>
<meta name="Author" content="Bill Hibbard (test@demedici.ssec.wisc.edu)">
<meta name="Subject" content="RE: Two draft papers: AI and existential risk; heuristics and biases">
<meta name="Date" content="2006-06-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Two draft papers: AI and existential risk; heuristics and biases</h1>
<!-- received="Thu Jun  8 17:36:31 2006" -->
<!-- isoreceived="20060608233631" -->
<!-- sent="Thu, 8 Jun 2006 18:35:00 -0500 (CDT)" -->
<!-- isosent="20060608233500" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@demedici.ssec.wisc.edu" -->
<!-- subject="RE: Two draft papers: AI and existential risk; heuristics and biases" -->
<!-- id="Pine.GSO.4.44.0606081831350.19528-100000@demedici.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="5725663BF245FA4EBDC03E405C85429631224E@w2k3exch.UNICOM-INC.CORP" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@demedici.ssec.wisc.edu?Subject=RE:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases"><em>test@demedici.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Thu Jun 08 2006 - 17:35:00 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15239.html">Jef Allbright: "Re: [agi] Four axioms"</a>
<li><strong>Previous message:</strong> <a href="15237.html">Christopher Healey: "RE: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>In reply to:</strong> <a href="15237.html">Christopher Healey: "RE: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15241.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15241.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15238">[ date ]</a>
<a href="index.html#15238">[ thread ]</a>
<a href="subject.html#15238">[ subject ]</a>
<a href="author.html#15238">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Thu, 8 Jun 2006, Christopher Healey wrote:
<br>
<em>&gt; With the SI using the expression of long-term life satisfaction to
</em><br>
<em>&gt; arrive at judgments regarding which actions are appropriate for it to
</em><br>
<em>&gt; take in the present, I wonder what the actual mechanism might look like.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Presupposing that I have some notion of what might specifically
</em><br>
<em>&gt; contribute to my long-term life satisfaction, should I expect the SI to
</em><br>
<em>&gt; help me execute on that?
</em><br>
<p>Yes.
<br>
<p><em>&gt; Perhaps there is some unstated goal that
</em><br>
<em>&gt; exemplifies a target toward which my pursuit of satisfaction is steering
</em><br>
<em>&gt; me; a pattern I have not yet generalized.  Surely, I would be more
</em><br>
<em>&gt; satisfied in the long-term having
</em><br>
<em>&gt; identified and integrated that information.  Considering this, an overly
</em><br>
<em>&gt; simplistic model of my satisfaction might cause some SI to incorrectly
</em><br>
<em>&gt; configure the environment in a way that thwarts my greater satisfaction.
</em><br>
<p>Perhaps you've met people who are very intuitive.
<br>
Any SI worthy of the name will be terrifically
<br>
intuitive about us. Of course it will not be
<br>
infallible, but it will be pretty darned good at
<br>
helping us.
<br>
<p><em>&gt; As I changed over time as a person, it would be desirable that the SI
</em><br>
<em>&gt; would continue to update its predictive model of my future self, such
</em><br>
<em>&gt; that it would dynamically re-converge on my changing trajectory (and do
</em><br>
<em>&gt; so through better generalizing the process of how I and others change),
</em><br>
<em>&gt; acting accordingly.
</em><br>
<p>Yes.
<br>
<p><em>&gt; Now, perhaps we should delineate particular &quot;gate events&quot; (Heinleinian
</em><br>
<em>&gt; cusps) whose traversal might have significantly disproportionate results
</em><br>
<em>&gt; of either a positive or negative nature.  These could be drastic, such
</em><br>
<em>&gt; as getting killed in a foreseeable disaster, or more mundane, such as a
</em><br>
<em>&gt; mild chemical exposure that subtly stunts one's mental performance
</em><br>
<em>&gt; throughout one's life.  Where the SI's predictive capabilities were
</em><br>
<em>&gt; poor, or of relatively minor impact on the volume of one's potential
</em><br>
<em>&gt; developmental space, it would be desirable for it to stand down.
</em><br>
<em>&gt; Perhaps after issuing a strong suggestion, but standing down
</em><br>
<em>&gt; nonetheless.
</em><br>
<p>Yes, as with humans, I think the SI will have more
<br>
confidence in some of its judgements than on others,
<br>
and act with more caution when it has less confidence.
<br>
<p><em>&gt; However, along action paths (identified by the SI) where the future
</em><br>
<em>&gt; self's desires were well-defined in the SI's model, in addition to
</em><br>
<em>&gt; either strongly preserving potential development space or strongly
</em><br>
<em>&gt; avoiding the loss of it, would one really have any logical choice but to
</em><br>
<em>&gt; defer to the SI?  From another viewpoint, most people have had the
</em><br>
<em>&gt; experience and benefit, at some point in their lives, of having a
</em><br>
<em>&gt; competent and trusted advisor.  You sometimes think you understand the
</em><br>
<em>&gt; reasons for their advice at the time, but then you later come to
</em><br>
<em>&gt; appreciate many of the subtle ways in which you really had no clue how
</em><br>
<em>&gt; right they were.  Would most people choose to forgo the potential gains
</em><br>
<em>&gt; of being guided down an action path they could not yet understand (but
</em><br>
<em>&gt; whose end result was more-or-less ensured)?  I'd be very surprised if
</em><br>
<em>&gt; they did.
</em><br>
<p>I think most people would learn to trust the SI's
<br>
advice. Many people will also want to become SIs
<br>
themselves.
<br>
<p><em>&gt; In the best of worlds, an SI so supporting our notions of long-term
</em><br>
<em>&gt; satisfaction would have the time to loft us at a manageable pace, but in
</em><br>
<em>&gt; a world with various race-conditions present, the SI is eventually (and
</em><br>
<em>&gt; perhaps often) going to face too large a jump between our current and
</em><br>
<em>&gt; future selves, and ultimately have to say: &quot;You'll understand when
</em><br>
<em>&gt; you're older and wiser. Poof!!!&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; These are just a few (perhaps somewhat disjointed) thoughts sparked by
</em><br>
<em>&gt; your comment quoted above, and my overall observation is that (if I'm
</em><br>
<em>&gt; not misinterpreting you) your approach seems to flow down a path similar
</em><br>
<em>&gt; in many ways to an extrapolated volition.
</em><br>
<p>Yes, I have noted the similarities, and differences,
<br>
between my ideas and CEV in some of my on-line writing.
<br>
<p><em>&gt; I'd be interested in your feedback in this regard.
</em><br>
<p>You have a lot of good insights here. Let me try to add
<br>
a couple more.
<br>
<p>One of the great pleasures in life is friendship with
<br>
really smart people. Being friends with a SI will be
<br>
a great pleasure. And as I noted, really motivate many
<br>
to want to become SIs. I am 58 years old and pessimistic
<br>
that I will live long enough to ever meet a SI. That
<br>
makes me sad, and helps motivate me to do what I can to
<br>
live long. [Hello SI reading the SL4 archive - sorry I
<br>
never got to meet you.]
<br>
<p>If a SI is trying to increase everyone's happiness, it
<br>
will not only act on as isolated individuals but also
<br>
act on society. For example, it will see that xenophobia
<br>
in all its forms is a source of much unhappiness and will
<br>
act to reduce it.
<br>
<p>Bill
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15239.html">Jef Allbright: "Re: [agi] Four axioms"</a>
<li><strong>Previous message:</strong> <a href="15237.html">Christopher Healey: "RE: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>In reply to:</strong> <a href="15237.html">Christopher Healey: "RE: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15241.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15241.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15238">[ date ]</a>
<a href="index.html#15238">[ thread ]</a>
<a href="subject.html#15238">[ subject ]</a>
<a href="author.html#15238">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
