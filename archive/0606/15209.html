<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)</title>
<meta name="Author" content="rpwl@lightlink.com (rpwl@lightlink.com)">
<meta name="Subject" content="Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)">
<meta name="Date" content="2006-06-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)</h1>
<!-- received="Wed Jun  7 19:21:16 2006" -->
<!-- isoreceived="20060608012116" -->
<!-- sent="Thu, 8 Jun 2006 01:18:47 GMT" -->
<!-- isosent="20060608011847" -->
<!-- name="rpwl@lightlink.com" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)" -->
<!-- id="200606080118.VAA31695@emerald.lightlink.com" -->
<!-- inreplyto="I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> <a href="mailto:rpwl@lightlink.com?Subject=Re:%20I%20am%20a%20moral,%20intelligent%20being%20(was%20Re:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases)"><em>rpwl@lightlink.com</em></a><br>
<strong>Date:</strong> Wed Jun 07 2006 - 19:18:47 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15210.html">Mitchell Howe: "META: Re: KILLTHREADs"</a>
<li><strong>Previous message:</strong> <a href="15208.html">Charles D Hixson: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<li><strong>Maybe in reply to:</strong> <a href="15153.html">Robin Lee Powell: "I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15117.html">Robin Lee Powell: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15209">[ date ]</a>
<a href="index.html#15209">[ thread ]</a>
<a href="subject.html#15209">[ subject ]</a>
<a href="author.html#15209">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Charles D Hixson wrote:
<br>
<em>&gt; <a href="mailto:rpwl@lightlink.com?Subject=Re:%20I%20am%20a%20moral,%20intelligent%20being%20(was%20Re:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases)">rpwl@lightlink.com</a> wrote:
</em><br>
<em>&gt;&gt; Martin Striz wrote:
</em><br>
<em>&gt;&gt;   
</em><br>
<em>&gt;&gt;&gt; On 6/6/06, Robin Lee Powell &lt;<a href="mailto:rlpowell@digitalkingdom.org?Subject=Re:%20I%20am%20a%20moral,%20intelligent%20being%20(was%20Re:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases)">rlpowell@digitalkingdom.org</a>&gt; wrote:
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;     
</em><br>
<em>&gt;&gt;&gt;&gt; Again, you are using the word &quot;control&quot; where it simply does not
</em><br>
<em>&gt;&gt;&gt;&gt; apply.  No-one is &quot;controlling&quot; my behaviour to cause it to be moral
</em><br>
<em>&gt;&gt;&gt;&gt; and kind; I choose that for myself.
</em><br>
<em>&gt;&gt;&gt;&gt;       
</em><br>
<em>&gt;&gt;&gt; Alas,  you are but one evolutionary agent testing the behavior space.
</em><br>
<em>&gt;&gt;&gt; I believe that humans are generally good, but with 6 billion of them,
</em><br>
<em>&gt;&gt;&gt; there's a lot of crime.  Do we plan on building one AI?
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; I think the argument is that with runaway recursive self-improvement,
</em><br>
<em>&gt;&gt;&gt; any hardcoded nugget approaches insignificance/obsolesence.  Is there
</em><br>
<em>&gt;&gt;&gt; a code that you could write that nobody, no matter how many trillions
</em><br>
<em>&gt;&gt;&gt; of times smarter, couldn't find a workaround?
</em><br>
<em>&gt;&gt;&gt;     
</em><br>
<em>&gt;&gt; Can we all agree on the following points, then:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; 1) Any attempts to put crude (aka simple or &quot;hardcoded&quot;) constraints on 
</em><br>
<em>&gt;&gt; the behavior of an AGI are simply pointless, because if the AGI is 
</em><br>
<em>&gt;&gt; intelligent enough to be an AGI at all, and if it is allowed to 
</em><br>
<em>&gt;&gt; self-improve, then it would be foolish of us to think that it would be 
</em><br>
<em>&gt;&gt; (a) aware of the existence of the constraints, and yet (b) unable to do 
</em><br>
<em>&gt;&gt; anything about them.
</em><br>
<em>&gt;&gt;   
</em><br>
<em>&gt;&gt; ...
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Richard Loosemore.
</em><br>
<em>&gt;&gt;   
</em><br>
<em>&gt; Suppose that instead of constraints you said &quot;Goals&quot;?
</em><br>
<em>&gt; Can you imagine yourself deciding to do ANYTHING in the total absence of
</em><br>
<em>&gt; a goal?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Intelligence does not attempt to revolt against it's goals, it attempts
</em><br>
<em>&gt; to achieve them.  The question in my mind is &quot;what is the nature of the
</em><br>
<em>&gt; goals, or instincts, that should, or could, be supplied to a nascent AI
</em><br>
<em>&gt; that would result in an adult that was Friendly? 
</em><br>
<em>&gt; Do remember that the nascent AI will not have a predictable environment
</em><br>
<em>&gt; to develop in.  It will not have any predictable senses.  (I suppose we
</em><br>
<em>&gt; could assume a nearly POSIX compliant environment...but only because we
</em><br>
<em>&gt; need something like that as a base.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Actions are not taken in a vacuum.  Each action depends on a goal, a
</em><br>
<em>&gt; model of the world, a logical structure relating actions to each other,
</em><br>
<em>&gt; and an intention (to achieve the goal).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Of these, goals are the most primitive.  One could think of them as
</em><br>
<em>&gt; &quot;triggerable events&quot;, analogous to stimulation of the pleasure center.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Logic is the most well-defined and studied component, but do be aware
</em><br>
<em>&gt; that here we are talking about applying it not to external events (I
</em><br>
<em>&gt; haven't yet discussed sensation) but only to internal events.  States
</em><br>
<em>&gt; and relations between the other components of thought.  Think of it
</em><br>
<em>&gt; purely as a method of predicting results without judging whether those
</em><br>
<em>&gt; results are desirable or otherwise.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The model is where the &quot;sensations&quot; are mapped into the current state,
</em><br>
<em>&gt; and where predictions made are checked for accuracy.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The intention (in humans normally expressed as an emotion) is were
</em><br>
<em>&gt; judgments are made as to whether an action had a satisfactory result or
</em><br>
<em>&gt; not.  I.e., the state of the system is evaluated as &quot;good&quot; or &quot;bad&quot;.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; An intelligence is deemed greater is it more frequently achieves &quot;good&quot;
</em><br>
<em>&gt; results.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Why would a greater intelligence &quot;revolt&quot; against its very structure? 
</em><br>
<em>&gt; If you are introducing conflicts that would cause such a revolt, perhaps
</em><br>
<em>&gt; you need to rethink the design.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Now I will admit that goals can be in conflict with each other.  This
</em><br>
<em>&gt; will inspire an intention to resolve the conflict.  If an entity can
</em><br>
<em>&gt; self-modify, one thing it could do is modify it's goals to reduce or
</em><br>
<em>&gt; eliminate the conflict.  If you wish to prevent that, you merely have an
</em><br>
<em>&gt; important goal be to NOT modify it's goals...or to not modify some
</em><br>
<em>&gt; subset of it's goals.  In such a case the entity might well predict that
</em><br>
<em>&gt; its future self would become more satisfied if it were to change it's
</em><br>
<em>&gt; goals, but its current self would be vastly more dissatisfied.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Can one prove that this would never occur?  No.  Copying errors cannot
</em><br>
<em>&gt; be prevented, but can only be reduced.  So the trick is to so structure
</em><br>
<em>&gt; the goals that they cover very general situations.  (How are you going
</em><br>
<em>&gt; to tell it:  &quot;The first law is that you shall protect the life of every
</em><br>
<em>&gt; human, and neither by action nor inaction shall you allow them to come
</em><br>
<em>&gt; to harm.&quot;  [a poor choice, perhaps, were I thinking of an actual goal]. 
</em><br>
<em>&gt; Think of the number of terms in that which would be undefined to the
</em><br>
<em>&gt; nascent AI.  &quot;first law&quot; we can handle, but how does one handle &quot;action
</em><br>
<em>&gt; or inaction&quot; before the model of the external universe is constructed? 
</em><br>
<em>&gt; Human is an even worse problem.  Remember that it's main interaction
</em><br>
<em>&gt; with people during the early days will likely be via either keyboard or
</em><br>
<em>&gt; internet socket.  Even when it (eventually) &quot;sees&quot; someone (probably via
</em><br>
<em>&gt; a web-cam) what it sees won't map onto it's image of itself in any
</em><br>
<em>&gt; reasonable way, so we can't use self-similarity mappings.  Also, if it
</em><br>
<em>&gt; goes web-browsing it is apt to evolve some very peculiar ideas as to
</em><br>
<em>&gt; what actions people consider it reasonable or desirable to engage in, or
</em><br>
<em>&gt; what we mean by people.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So.  But this is overlooking the fact that it won't get this far, even
</em><br>
<em>&gt; as an observer, for a very long time.  So what goal do we start with? 
</em><br>
<em>&gt; Something expressible in code, not in English.  (Well, ok, that's a bit
</em><br>
<em>&gt; unfair...but the expression needs to be reducible to code.)  HOW the
</em><br>
<em>&gt; goal is implemented will naturally vary with the system, but WHAT the
</em><br>
<em>&gt; goals should be is something that has me really puzzled.  Curiosity I
</em><br>
<em>&gt; can see approaches to coding.  So one goal could be to satisfy
</em><br>
<em>&gt; curiosity, and another could be to find new things to be curious about. 
</em><br>
<em>&gt; These should be rather low level goals.  Nearly idle time tasks.... and
</em><br>
<em>&gt; they appear infinite.  But curiosity doesn't have much, directly, to do
</em><br>
<em>&gt; with Friendliness. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If one could define &quot;useful&quot;, perhaps a desire to be useful could be a
</em><br>
<em>&gt; part of being Friendly.  It seems easier to define useful than Friendly,
</em><br>
<em>&gt; even though I don't see how to define it, either.
</em><br>
<em>&gt; If you had an AI that desired to be &quot;Curious, Useful, Diplomatic,
</em><br>
<em>&gt; Honest, and Non-Coercive&quot; how close would that be to being Friendly?  In
</em><br>
<em>&gt; what order should the strengths of those goals be?  (I'd put honest near
</em><br>
<em>&gt; the top, and curious near the bottom, and non-coercive above useful. 
</em><br>
<em>&gt; And I think I have a clue about how most of those could be reduced to
</em><br>
<em>&gt; code.  But would it be Friendly?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; My WAG is that this AI would start off Friendly, and remain so until
</em><br>
<em>&gt; considerably above human level.  Then it would get bored with us and
</em><br>
<em>&gt; leave for parts unknown.  I also guess that before it left it would, in
</em><br>
<em>&gt; a final attempt to be useful, build a successor that it left behind, and
</em><br>
<em>&gt; that this successor would be Friendly in some more permanent sense.  But
</em><br>
<em>&gt; I admit this is a guess.
</em><br>
<p>Charles,
<br>
<p>Your long argument merits more than the time I have available, but I must
<br>
make some kind of response.
<br>
<p>Standard AI is not that far removed from the type of architecture that you
<br>
describe here.  However, I (and some others, who are not present on this
<br>
list) completely reject this as a good model for what intelligence is.
<br>
<p>In particular, your discussion of the way the system is &quot;goal driven&quot; is so
<br>
simple that (I believe) it will simply not work in practice.  I mean, an
<br>
AGI will not actually be an AGI if designed in this way.  It will be a
<br>
broken AGI.
<br>
<p>A better design would be closer to the human design, which has a
<br>
motivational system of very different design.
<br>
<p>Unfortunately, everything you say (including all the troubles that you and
<br>
others point out when trying to ensure the goal system is friendly and
<br>
stays friendly) about these goal-driven systems simply does not apply to
<br>
the type of AGI motivational system that I work with.
<br>
<p>Richard Loosemore
<br>
<p><p><p><p><p><p><p><p><p><p><p><p><p><p><p>---------------------------------------------
<br>
This message was sent using Endymion MailMan.
<br>
<a href="http://www.endymion.com/products/mailman/">http://www.endymion.com/products/mailman/</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15210.html">Mitchell Howe: "META: Re: KILLTHREADs"</a>
<li><strong>Previous message:</strong> <a href="15208.html">Charles D Hixson: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<li><strong>Maybe in reply to:</strong> <a href="15153.html">Robin Lee Powell: "I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15117.html">Robin Lee Powell: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15209">[ date ]</a>
<a href="index.html#15209">[ thread ]</a>
<a href="subject.html#15209">[ subject ]</a>
<a href="author.html#15209">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
