<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Two draft papers: AI and existential risk; heuristics and biases</title>
<meta name="Author" content="Bill Hibbard (test@demedici.ssec.wisc.edu)">
<meta name="Subject" content="Re: Two draft papers: AI and existential risk; heuristics and biases">
<meta name="Date" content="2006-06-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Two draft papers: AI and existential risk; heuristics and biases</h1>
<!-- received="Wed Jun  7 11:26:30 2006" -->
<!-- isoreceived="20060607172630" -->
<!-- sent="Wed, 7 Jun 2006 12:24:55 -0500 (CDT)" -->
<!-- isosent="20060607172455" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@demedici.ssec.wisc.edu" -->
<!-- subject="Re: Two draft papers: AI and existential risk; heuristics and biases" -->
<!-- id="Pine.GSO.4.44.0606071216390.18285-100000@demedici.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="4485D42E.5080809@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases"><em>test@demedici.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Wed Jun 07 2006 - 11:24:55 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15190.html">Peter de Blanc: "Re: [agi] Two draft papers: AI and existential risk; heuristics	and biases"</a>
<li><strong>Previous message:</strong> <a href="15188.html">Robin Lee Powell: "Re: Let's resolve it with a thought experiment"</a>
<li><strong>In reply to:</strong> <a href="15161.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15212.html">Robin Lee Powell: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15212.html">Robin Lee Powell: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15268.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15189">[ date ]</a>
<a href="index.html#15189">[ thread ]</a>
<a href="subject.html#15189">[ subject ]</a>
<a href="author.html#15189">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer,
<br>
<p><em>&gt; I don't think it
</em><br>
<em>&gt; inappropriate to cite a problem that is general to supervised learning
</em><br>
<em>&gt; and reinforcement, when your proposal is to, in general, use supervised
</em><br>
<em>&gt; learning and reinforcement.  You can always appeal to a &quot;different
</em><br>
<em>&gt; algorithm&quot; or a &quot;different implementation&quot; that, in some unspecified
</em><br>
<em>&gt; way, doesn't have a problem.
</em><br>
<p>But you are not demonstrating a general problem. You are
<br>
instead relying on specific examples (primitive neural
<br>
networks and systems that cannot distingish a human from
<br>
a smiley) that fail trivially. You should be clear whether
<br>
you claim that reinforcement learning (RL) must inevitably
<br>
lead to:
<br>
<p>&nbsp;&nbsp;1. A failure of intelligence.
<br>
<p>or:
<br>
<p>&nbsp;&nbsp;2. A failure of friendliness.
<br>
<p>Your example of the US Army's primitive neural network
<br>
experiments is a failure of intelligence. Your statement
<br>
about smiley faces assumes a general success at intelligence
<br>
by the system, but an absurd failure of intelligence in the
<br>
part of the system that recognizes humans and their emotions,
<br>
leading to a failure of friendliness.
<br>
<p>If your claim is that RL must lead to a failure of
<br>
intelligence, then you should cite and quote from Eric Baum's
<br>
What is Thought? (in my opinion, Baum deserves the Nobel
<br>
Prize in Economics for his experiments linking economic
<br>
principles with effective RL in multi-agent learning systems).
<br>
<p>If your claim is that RL can succeed at intelligence but must
<br>
lead to a failure of friendliness, then it is reasonable to
<br>
cite and quote me. But please use my 2004 AAAI paper . . .
<br>
<p><em>&gt; If you are genuinely repudiating your old ideas ...
</em><br>
<p>. . . use my 2004 AAAI paper because I do repudiate the
<br>
statement in my 2001 paper that recognition of humans and
<br>
their emotions should be hard-wired (i.e., static). That
<br>
is just the section of my 2001 paper that you quoted.
<br>
<p>Not that I am sure that hard-wired recognition of humans and
<br>
their emotions inevitably leads to a failure of friendliness,
<br>
since the super-intelligence (SI) may understand that humans
<br>
would be happier if they could evolve to other physical forms
<br>
but still be recognized by the SI as humans, and decide to
<br>
modify itself (or build an improved replacement). But if this
<br>
is my scenario, then why not design continuing learning of
<br>
recognition of humans and their emotions into the system in
<br>
the first place. Hence my change of views.
<br>
<p>I am sure you have not repudiated everything in CFAI, and I
<br>
have not repudiated everything in my earlier publications.
<br>
I continue to believe that RL is critical to acheiving
<br>
intelligence with a feasible amount of computing resources,
<br>
and I continue to believe that collective long-term human
<br>
happiness should be the basic reinforcement value for SI.
<br>
But I now think that a SI should continue to learn recognition
<br>
of humans and their emotions via reinforcement, rather than
<br>
these recognitions being hard-wired as the result of supervised
<br>
learning. My recent writings have also refined my views about
<br>
how human happiness should be defined, and how the happiness of
<br>
many people should be combined into an overall reinforcement
<br>
value.
<br>
<p><em>&gt; I see no relevant difference between these two proposals, except that
</em><br>
<em>&gt; the paragraph you cite (presumably as a potential replacement) is much
</em><br>
<em>&gt; less clear to the outside academic reader.
</em><br>
<p>If you see no difference between my earlier and later ideas,
<br>
then please use a scenario based on my later papers. That will
<br>
be a better demonstration of the strength of your arguments,
<br>
and be fairer to me.
<br>
<p>Of course, it would be best to demonstrate your claim (either
<br>
that RL must lead to a failure of intelligence, or can succeed
<br>
at intelligence but must lead to a failure of friendliness) in
<br>
general. But if you cannot do that and must rely on a specific
<br>
example, then at least do not pick an example that fails for
<br>
trivial reasons.
<br>
<p>As I wrote above, if you think RL must fail at intelligence,
<br>
you would be best to quote Eric Baum.
<br>
<p>If you think RL can succeed at intelligence but must fail at
<br>
friendliness, but just want to demonstrate it for a specific
<br>
example, then use a scenario in which:
<br>
<p>&nbsp;&nbsp;1. The SI recognizes humans and their emotions as accurately
<br>
&nbsp;&nbsp;as any human, and continually relearns that recognition as
<br>
&nbsp;&nbsp;humans evolve (for example, to become SIs themselves).
<br>
<p>&nbsp;&nbsp;2. The SI values people after death at the maximally unhappy
<br>
&nbsp;&nbsp;value, in order to avoid motivating the SI to kill unhappy
<br>
&nbsp;&nbsp;people.
<br>
<p>&nbsp;&nbsp;3. The SI combines the happiness of many people in a way (such
<br>
&nbsp;&nbsp;as by averaging) that does not motivate a simple numerical
<br>
&nbsp;&nbsp;increase (or decrease) in the number of people.
<br>
<p>&nbsp;&nbsp;4. The SI weights unhappiness stronger than happiness, so that
<br>
&nbsp;&nbsp;it focuses it efforts on helping unhappy people.
<br>
<p>&nbsp;&nbsp;5. The SI develops models of all humans and what produces
<br>
&nbsp;&nbsp;long-term happiness in each of them.
<br>
<p>&nbsp;&nbsp;6. The SI develops models of the interactions among humans
<br>
&nbsp;&nbsp;and how these interactions affect the happiness of each.
<br>
<p>If you demonstrate a failure of friendliness against a weaker
<br>
scenario, all that really demonstrates is that you needed the
<br>
weak scenario in order to make your case. And it is unfair to
<br>
me. As I said, best would be a general demonstration, but if
<br>
you must pick an example, at least pick a strong example.
<br>
<p>I do not pretend to have all the answers. Clearly, making RL work
<br>
will require solution to a number of currently unsolved problems.
<br>
Jeff Hawkins' work on hierarchical temporal memory (HTM) is
<br>
interesting in this respect, given the interactions within the
<br>
human brain between the cortex (modeled by HTM) and lower brain
<br>
areas where RL has been observed (in my view RL is in a lower area
<br>
because it is fundamental, and the higher areas evolved to create
<br>
the simulation model of the world necessary to solve the credit
<br>
assignment problem for RL). Clearly RL is not the whole answer,
<br>
but I think Eric Baum has it right that it is critical to
<br>
intelligence.
<br>
<p>I appreciate your offer to include my URL in your article,
<br>
where I can give my response. Please use this (please proof
<br>
read carefully for typos in the final galleys):
<br>
<p>&nbsp;&nbsp;<a href="http://www.ssec.wisc.edu/~billh/g/AIRisk_Reply.html">http://www.ssec.wisc.edu/~billh/g/AIRisk_Reply.html</a>
<br>
<p>If you take my suggestion, by elevating your discussion to a
<br>
general explanation of why RL systems must fail or at least using
<br>
a strong scenario, that will make my response more friendly since
<br>
I am happier to be named as an advocate of RL than to be
<br>
conflated with trivial failure. I would prefer that you not use
<br>
the quote you were using from my 2001 paper, as I repudiate
<br>
supervised learning of hard-wired values. Please use some quote
<br>
from and cite my 2004 AAAI paper, since there is nothing in it
<br>
that I repudiate yet (but you will find more refined views in my
<br>
2005 on-line paper).
<br>
<p>Bill
<br>
<p>p.s., Although I receive digest messages from extropy-chat,
<br>
for some reason my recent posts to it have all bounced. Could
<br>
someone please forward this message to extropy-chat?
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15190.html">Peter de Blanc: "Re: [agi] Two draft papers: AI and existential risk; heuristics	and biases"</a>
<li><strong>Previous message:</strong> <a href="15188.html">Robin Lee Powell: "Re: Let's resolve it with a thought experiment"</a>
<li><strong>In reply to:</strong> <a href="15161.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15212.html">Robin Lee Powell: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15212.html">Robin Lee Powell: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15268.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15189">[ date ]</a>
<a href="index.html#15189">[ thread ]</a>
<a href="subject.html#15189">[ subject ]</a>
<a href="author.html#15189">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
