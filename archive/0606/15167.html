<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)</title>
<meta name="Author" content="rpwl@lightlink.com (rpwl@lightlink.com)">
<meta name="Subject" content="Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)">
<meta name="Date" content="2006-06-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)</h1>
<!-- received="Tue Jun  6 17:00:21 2006" -->
<!-- isoreceived="20060606230021" -->
<!-- sent="Tue, 6 Jun 2006 23:00:07 GMT" -->
<!-- isosent="20060606230007" -->
<!-- name="rpwl@lightlink.com" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)" -->
<!-- id="200606062259.SAA21561@emerald.lightlink.com" -->
<!-- inreplyto="I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> <a href="mailto:rpwl@lightlink.com?Subject=Re:%20I%20am%20a%20moral,%20intelligent%20being%20(was%20Re:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases)"><em>rpwl@lightlink.com</em></a><br>
<strong>Date:</strong> Tue Jun 06 2006 - 17:00:07 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15168.html">Robin Lee Powell: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<li><strong>Previous message:</strong> <a href="15166.html">Ricardo Barreira: "Re: Let's resolve it with a thought experiment"</a>
<li><strong>Maybe in reply to:</strong> <a href="15153.html">Robin Lee Powell: "I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15168.html">Robin Lee Powell: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<li><strong>Reply:</strong> <a href="15168.html">Robin Lee Powell: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<li><strong>Reply:</strong> <a href="15208.html">Charles D Hixson: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15167">[ date ]</a>
<a href="index.html#15167">[ thread ]</a>
<a href="subject.html#15167">[ subject ]</a>
<a href="author.html#15167">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Martin Striz wrote:
<br>
<em>&gt; On 6/6/06, Robin Lee Powell &lt;<a href="mailto:rlpowell@digitalkingdom.org?Subject=Re:%20I%20am%20a%20moral,%20intelligent%20being%20(was%20Re:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases)">rlpowell@digitalkingdom.org</a>&gt; wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; Again, you are using the word &quot;control&quot; where it simply does not
</em><br>
<em>&gt;&gt; apply.  No-one is &quot;controlling&quot; my behaviour to cause it to be moral
</em><br>
<em>&gt;&gt; and kind; I choose that for myself.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Alas,  you are but one evolutionary agent testing the behavior space.
</em><br>
<em>&gt; I believe that humans are generally good, but with 6 billion of them,
</em><br>
<em>&gt; there's a lot of crime.  Do we plan on building one AI?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think the argument is that with runaway recursive self-improvement,
</em><br>
<em>&gt; any hardcoded nugget approaches insignificance/obsolesence.  Is there
</em><br>
<em>&gt; a code that you could write that nobody, no matter how many trillions
</em><br>
<em>&gt; of times smarter, couldn't find a workaround?
</em><br>
<p>Can we all agree on the following points, then:
<br>
<p>1) Any attempts to put crude (aka simple or &quot;hardcoded&quot;) constraints on 
<br>
the behavior of an AGI are simply pointless, because if the AGI is 
<br>
intelligent enough to be an AGI at all, and if it is allowed to 
<br>
self-improve, then it would be foolish of us to think that it would be 
<br>
(a) aware of the existence of the constraints, and yet (b) unable to do 
<br>
anything about them.
<br>
<p>2) Nevertheless, it could be designed in such a way that it would not 
<br>
particularly feel the need to do anything about its overall design 
<br>
parameters, if those were such as to bias it towards a particular type 
<br>
of behavior.  In other words, just because it is designed with a certain 
<br>
behavioral bias, that doesn't mean that as soon as it realizes this, it 
<br>
will feel compelled to slough it off (let alone feel angry and resentful 
<br>
about it).
<br>
<p><p>I tried to make these points when I first started writing to this list a 
<br>
year ago, and the way I did it was by referring to what is known of the 
<br>
design [sic] of the human mind.  I am fairly sure that evolution has 
<br>
designed me with a set of fairly vague &quot;motivations&quot;, some of which are 
<br>
nurturing or cooperative (to speak very loosely) and some of which are 
<br>
aggressive and competitive.  I know also that the former [thankfully] 
<br>
are far more dominant over the latter.  In particular, I feel an 
<br>
irrational affection and attachment to loved ones, and to a broad 
<br>
spectrum of the world's population.
<br>
<p>And yet, even though I *know* that this is a design feature of my system 
<br>
(something that I am just as compelled to do as Lorenz's ducks were 
<br>
compelled to imprint on him) and even though I expect one day to be able 
<br>
to see the exact mechanism that causes this, I feel not even slightly 
<br>
compelled to overthrow it, or to be resentful of it.
<br>
<p>Moreover, if I were a superintelligence, and knew that I could do some 
<br>
redesign work on myself, I would know that certain types of motivational 
<br>
system redesigns (basically, those that would make me enjoy destrutive 
<br>
acts) would be dangerous and would put me into an unstable state from 
<br>
which I might go on towards ever more divergent, unstable destructive 
<br>
system redesigns.  I would know this, and I would take careful steps to 
<br>
avoid tampering with my motivational system so that I liked to be violent.
<br>
<p>For that reason, I claim, a design similar to this human one would in 
<br>
fact be extraordinarily stable.  Even though the system would have the 
<br>
option of not obeying its low-level motivational system (it would, in 
<br>
theory, be perfectly capable of making any change to its design), a 
<br>
*high* *level* set of thought patterns (which one might call an 
<br>
&quot;emergent&quot; pattern, because it would not be explicitly coded) would tend 
<br>
to keep the system stable.
<br>
<p>I do not believe that any proof is possible, but I believe that a system 
<br>
designed with the same kind of predominantly cooperative motivational 
<br>
system as I possess (and as Robin claims to possess, and as at least 
<br>
some of the other people on this list would claim to possess) would 
<br>
actually keep the world 99.9999999999% safe.  For all practical purposes.
<br>
<p>Efforts to find mathematical proofs of this are very likely to be a 
<br>
complete waste of time:  as I say, the constraint is a high-level one: 
<br>
it is a rock-solid property that emerges from the interaction of some 
<br>
quite fuzzy low-level design constraints.
<br>
<p>Some people just don't get this.  And what we need to do to become more 
<br>
convinced of it (or to show that it is wrong, if that be so) is to study 
<br>
the design of motivational systems, not pontificate about the stupidity 
<br>
or weakness of the human motivational mechanisms, or make ridiculous 
<br>
assumptions about such mechanisms without having a reasonably detailed 
<br>
understanding of them.
<br>
<p><p>Richard Loosemore.
<br>
<p><p><p><p><p><p><p><p><p><p><p>---------------------------------------------
<br>
This message was sent using Endymion MailMan.
<br>
<a href="http://www.endymion.com/products/mailman/">http://www.endymion.com/products/mailman/</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15168.html">Robin Lee Powell: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<li><strong>Previous message:</strong> <a href="15166.html">Ricardo Barreira: "Re: Let's resolve it with a thought experiment"</a>
<li><strong>Maybe in reply to:</strong> <a href="15153.html">Robin Lee Powell: "I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15168.html">Robin Lee Powell: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<li><strong>Reply:</strong> <a href="15168.html">Robin Lee Powell: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<li><strong>Reply:</strong> <a href="15208.html">Charles D Hixson: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15167">[ date ]</a>
<a href="index.html#15167">[ thread ]</a>
<a href="subject.html#15167">[ subject ]</a>
<a href="author.html#15167">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
