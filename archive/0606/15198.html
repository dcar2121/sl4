<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [agi] Two draft papers: AI and existential risk;	heuristics	and biases</title>
<meta name="Author" content="Peter de Blanc (peter.deblanc@verizon.net)">
<meta name="Subject" content="Re: [agi] Two draft papers: AI and existential risk;	heuristics	and biases">
<meta name="Date" content="2006-06-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [agi] Two draft papers: AI and existential risk;	heuristics	and biases</h1>
<!-- received="Wed Jun  7 15:00:41 2006" -->
<!-- isoreceived="20060607210041" -->
<!-- sent="Wed, 07 Jun 2006 16:51:37 -0400" -->
<!-- isosent="20060607205137" -->
<!-- name="Peter de Blanc" -->
<!-- email="peter.deblanc@verizon.net" -->
<!-- subject="Re: [agi] Two draft papers: AI and existential risk;	heuristics	and biases" -->
<!-- id="1149713497.23244.75.camel@localhost.localdomain" -->
<!-- inreplyto="027901c68a6e$d0a94e30$d912a8c0@MWASER01" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Peter de Blanc (<a href="mailto:peter.deblanc@verizon.net?Subject=Re:%20[agi]%20Two%20draft%20papers:%20AI%20and%20existential%20risk;	heuristics	and%20biases"><em>peter.deblanc@verizon.net</em></a>)<br>
<strong>Date:</strong> Wed Jun 07 2006 - 14:51:37 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15199.html">Eliezer S. Yudkowsky: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<li><strong>Previous message:</strong> <a href="15197.html">Martin Striz: "Re: Cats, was I am a moral, intelligent being"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15216.html">Ben Goertzel: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15216.html">Ben Goertzel: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15198">[ date ]</a>
<a href="index.html#15198">[ thread ]</a>
<a href="subject.html#15198">[ subject ]</a>
<a href="author.html#15198">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Wed, 2006-06-07 at 16:13 -0400, Mark Waser wrote:
<br>
<em>&gt;     I'm pretty sure that I've got the science and math that I need and, as I 
</em><br>
<p>Okay. I supposed the opposite, not because of anything you said, but
<br>
because the base rate is so low.
<br>
<p><em>&gt; said above, I don't feel compelled to listen to everyone.  However, if I 
</em><br>
<em>&gt; can't get a decent consensus out of a pretty bright, educated group (or at 
</em><br>
<em>&gt; least, the open-minded, bright, and educated members of a group like this), 
</em><br>
<em>&gt; then it's a pretty good sign that my ideas aren't where they should be.
</em><br>
<p>I think it's just hopeless. You'll never get a consensus out of a pre-
<br>
selected population this size on a new idea.
<br>
<p>Consider that the theory of evolution is not part of the world's
<br>
consensus. Consider that the Bayes' Theorem is not part of the
<br>
scientific consensus. It isn't even part of this list's consensus! These
<br>
are ancient ideas - way older than us. The consensus lags *centuries*
<br>
behind people who think.
<br>
<p><em>&gt; It IS my contention that there is a relatively simple, 
</em><br>
<em>&gt; inductively-robust (in a mathematical proof sense) formulation of 
</em><br>
<em>&gt; friendliness that will guarantee that there won't be effects that *I* 
</em><br>
<em>&gt; consider undesirable, horrible, or immoral.  It will, of course/however, 
</em><br>
<em>&gt; produce a number of effects that others will decry as undesirable, horrible, 
</em><br>
<em>&gt; or immoral -- like allowing abortion and assisted suicide in a reasonable 
</em><br>
<em>&gt; number of cases, NOT allowing the killing of infidels, allowing almost any 
</em><br>
<em>&gt; personal modifications (with  truly informed consent) that are non-harmful 
</em><br>
<em>&gt; to others, NOT allowing the imposition of personal modifications whether 
</em><br>
<em>&gt; they be physical, mental, or spiritual, etc.
</em><br>
<p>How relatively simple? Evolution doesn't do simple. I doubt that any
<br>
human goal system has a simple mathematical formalization.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15199.html">Eliezer S. Yudkowsky: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existential risk; heuristics and biases)"</a>
<li><strong>Previous message:</strong> <a href="15197.html">Martin Striz: "Re: Cats, was I am a moral, intelligent being"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15216.html">Ben Goertzel: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15216.html">Ben Goertzel: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15198">[ date ]</a>
<a href="index.html#15198">[ thread ]</a>
<a href="subject.html#15198">[ subject ]</a>
<a href="author.html#15198">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
