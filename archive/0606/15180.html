<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [agi] Two draft papers: AI and existential risk; heuristics and biases</title>
<meta name="Author" content="BillK (pharos@gmail.com)">
<meta name="Subject" content="Re: [agi] Two draft papers: AI and existential risk; heuristics and biases">
<meta name="Date" content="2006-06-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [agi] Two draft papers: AI and existential risk; heuristics and biases</h1>
<!-- received="Wed Jun  7 02:08:18 2006" -->
<!-- isoreceived="20060607080818" -->
<!-- sent="Wed, 7 Jun 2006 16:08:08 +0800" -->
<!-- isosent="20060607080808" -->
<!-- name="BillK" -->
<!-- email="pharos@gmail.com" -->
<!-- subject="Re: [agi] Two draft papers: AI and existential risk; heuristics and biases" -->
<!-- id="ee50357e0606070108t7bfc1f71q6297edcb6d48b45c@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="448661F5.9010601@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> BillK (<a href="mailto:pharos@gmail.com?Subject=Re:%20[agi]%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases"><em>pharos@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Jun 07 2006 - 02:08:08 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15181.html">Keith Henson: "Cats, was I am a moral, intelligent being"</a>
<li><strong>Previous message:</strong> <a href="15179.html">Michael Vassar: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>In reply to:</strong> <a href="15174.html">Eliezer S. Yudkowsky: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15186.html">Jef Allbright: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15180">[ date ]</a>
<a href="index.html#15180">[ thread ]</a>
<a href="subject.html#15180">[ subject ]</a>
<a href="author.html#15180">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 6/7/06, Eliezer S. Yudkowsky wrote:
<br>
&lt;snip&gt;
<br>
<em>&gt;
</em><br>
<em>&gt; Because this is a young field, how much mileage you get out will be
</em><br>
<em>&gt; determined in large part by how much sweat you put in.  That's the
</em><br>
<em>&gt; simple practical truth.  The reasons why you do X are irrelevant given
</em><br>
<em>&gt; that you do X; they're &quot;screened off&quot;, in Pearl's terminology.  It
</em><br>
<em>&gt; doesn't matter how good your excuse is for putting off work on Friendly
</em><br>
<em>&gt; AI, or for not building emergency shutdown features, given that that's
</em><br>
<em>&gt; what you actually do.  And this is the complaint of IT security
</em><br>
<em>&gt; professionals the world over; that people would rather not think about
</em><br>
<em>&gt; IT security, that they would rather do the minimum possible and just get
</em><br>
<em>&gt; it over with and go back to their day jobs.  Who can blame them for such
</em><br>
<em>&gt; human frailty?  But the result is poor IT security.
</em><br>
<em>&gt;
</em><br>
<p><p>This is the real world that you have to deal with.
<br>
You cannot get the funding, or the time, to do the job properly,
<br>
because there is always pressure to be the first to market.
<br>
<p>AGI is so tricky a problem that just getting it to work at all is
<br>
regarded as a minor miracle.  (Like the early days of computers and
<br>
the internet).
<br>
Implement first, we can always patch it afterwards.
<br>
<p>A much more worrying consideration, of course, is that the people with
<br>
the most resources, DARPA (and the Chinese) want an AGI to help them
<br>
kill their enemies. For defensive reasons only, naturally.
<br>
<p>When AGI is being developed as a weapon by massive government
<br>
resources, AGI ethics and being Friendly doesn't even get into the
<br>
specification.
<br>
Following orders from the human owners does.
<br>
<p><p>BillK
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15181.html">Keith Henson: "Cats, was I am a moral, intelligent being"</a>
<li><strong>Previous message:</strong> <a href="15179.html">Michael Vassar: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>In reply to:</strong> <a href="15174.html">Eliezer S. Yudkowsky: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15186.html">Jef Allbright: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15180">[ date ]</a>
<a href="index.html#15180">[ thread ]</a>
<a href="subject.html#15180">[ subject ]</a>
<a href="author.html#15180">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
