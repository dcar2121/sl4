<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [agi] Two draft papers: AI and existential risk; heuristics and biases</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: [agi] Two draft papers: AI and existential risk; heuristics and biases">
<meta name="Date" content="2006-06-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [agi] Two draft papers: AI and existential risk; heuristics and biases</h1>
<!-- received="Tue Jun  6 23:25:03 2006" -->
<!-- isoreceived="20060607052503" -->
<!-- sent="Tue, 06 Jun 2006 22:19:49 -0700" -->
<!-- isosent="20060607051949" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: [agi] Two draft papers: AI and existential risk; heuristics and biases" -->
<!-- id="448661F5.9010601@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="638d4e150606060809qdbb0840t525e5acc174960bb@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20[agi]%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Jun 06 2006 - 23:19:49 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15175.html">Michael Vassar: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existentia"</a>
<li><strong>Previous message:</strong> <a href="15173.html">Eliezer S. Yudkowsky: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>In reply to:</strong> <a href="15150.html">Ben Goertzel: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15176.html">Michael Vassar: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15176.html">Michael Vassar: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15177.html">Ben Goertzel: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15180.html">BillK: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15186.html">Jef Allbright: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15174">[ date ]</a>
<a href="index.html#15174">[ thread ]</a>
<a href="subject.html#15174">[ subject ]</a>
<a href="author.html#15174">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em> &gt;
</em><br>
<em> &gt; This brings us back to my feeling that some experimentation with AGI
</em><br>
<em> &gt; systems is going to be necessary before FAI can be understood
</em><br>
<em> &gt; reasonably well on a theoretical level.  Basically, in my view, one
</em><br>
<em> &gt; way these things may unfold is
</em><br>
<em> &gt;
</em><br>
<em> &gt; * Experimentation with simplistic AGI systems ... leads to
</em><br>
<em> &gt; * Theoretical understanding of AGI under limited resources ... which
</em><br>
<em> &gt; leads to...
</em><br>
<em> &gt; * The capability of theoretically understanding FAI ... which leads 
</em><br>
to ...
<br>
<em> &gt; * Building FAI
</em><br>
<em> &gt;
</em><br>
<em> &gt; Now, this building of FAI *may* take the form of creating a whole new
</em><br>
<em> &gt; AGI architecture from scratch, *or* it may take the form of minorly
</em><br>
<em> &gt; modifying an existing AGI ... or it may be understood why some
</em><br>
<em> &gt; existing AGI design is adequate and there is not really any
</em><br>
<em> &gt; Friendliness problem with it.  We don't know which of these
</em><br>
<em> &gt; eventualities will occur because we don't have the theory of FAI
</em><br>
<em> &gt; yet...
</em><br>
<em> &gt;
</em><br>
<em>&gt; Your excellent article AIGR, in my view, does not do a good job of
</em><br>
<em>&gt; arguing against this sort of perspective that I'm advocating here.  I
</em><br>
<em>&gt; understand that this is not its job, though: it is mostly devoted to
</em><br>
<em>&gt; making more basic points, which are not sufficiently widely
</em><br>
<em>&gt; appreciated and with which I mainly agree enthusiastically.
</em><br>
<p>I am highly skeptical of calls for &quot;AGI experimentation&quot; as an answer to 
<br>
Friendly AI concerns, for several reasons.
<br>
<p><p>First, as discussed in the chapter, there's a major context change 
<br>
between the AI's prehuman stage and the AI's posthuman stage.  I can 
<br>
think of *many* failure modes such that the AI appears to behave well as 
<br>
a prehuman, then wipes out humanity as a posthuman.  What I fear from 
<br>
this scientific-sounding business of &quot;experimental investigation&quot; is 
<br>
that the results of your investigation will be observed good behavior, 
<br>
and you will conclude that the AI &quot;is good&quot; and will stay good under 
<br>
extreme context changes.  This is not, in fact, a licensable conclusion.
<br>
<p>Trial and error is all well and good in science, unless you happen to be 
<br>
dealing with an existential risk.  Richard Loosemore, at the AGI 
<br>
conference, said, &quot;We need to go out and do some alchemy.&quot;  Alchemy 
<br>
killed a hell of a lot of alchemists; and in later days, so too Madam 
<br>
Curie, who investigated radiation before anyone realized it was 
<br>
dangerous...  And yes, they were martyrs to science; we are better off, 
<br>
even counting the casualties, than if they had never tried; the 
<br>
overwhelming majority of humans survived, and eventually profited from 
<br>
the knowledge gained by these early pioneers of what *not* to do.  But - 
<br>
as I said in the chapter conclusion - imagine how careful you would have 
<br>
to be if you wanted to survive as an *individual*; and that is how 
<br>
careful humanity must be to survive existential risks.  I espouse the 
<br>
Proactionary Principle for everything *except* existential risks.  When 
<br>
you have to get it right the first time, alchemy is not an option.  So 
<br>
let's dispense with praising ourselves on looking scientific if we 
<br>
propose just doing the experiment and seeing what happens; because it is 
<br>
not unheard-of for the experimental result to be &quot;You've made an amazing 
<br>
discovery!  By the way, you're dead.&quot;  We can't afford a species-scale 
<br>
version of that.
<br>
<p><p>Second, there's an *enormous* amount of experimentation and observation 
<br>
that's already been done in the cognitive sciences.  I feed off this 
<br>
body of pre-existing work in a dozen fields, and it gives me more 
<br>
concentrated evidence than I could assemble for myself in a hundred 
<br>
lifetimes.  And all that I have studied is not the thousandth part of 
<br>
the whole.  But where the processor is inefficient, no amount of 
<br>
evidence may suffice.  If there's already a thousand times as much 
<br>
evidence as you could review in your lifetime, what makes you think that 
<br>
what's needed is one more experiment - rather than an insight that we 
<br>
already have more than enough evidence to see, but we aren't looking at 
<br>
the right way?
<br>
<p>Of course this objection has a special poignancy for me because, as far 
<br>
as I can tell, yes, we already have all the evidence we need, far more 
<br>
than enough, and the only problem is understanding the implications of 
<br>
what we already know.  Pity that humans aren't logically omniscient.
<br>
<p>But just which experiments do you propose to perform, and what do you 
<br>
expect them to tell you?  If you don't know what you're looking for, 
<br>
what is this one experiment such that spending a year to perform it has 
<br>
a higher probability of yielding an unexpected, unguessable insight, 
<br>
than spending the same year studying a thousand existing experiments 
<br>
from twenty fields?  Especially if your experiment constitutes an 
<br>
existential risk?
<br>
<p>As a Bayesian, I ought to be skeptical because, in theory, how you 
<br>
interpret evidence depends on which hypotheses you are testing.  You 
<br>
might see good behavior and elaborate moral arguments from a simple test 
<br>
AI and say, &quot;Ooh, it all just emerged!  I'm glad; I thought that was 
<br>
going to be really difficult.&quot;  But one could equally say, &quot;The 
<br>
probability of this specific complexity 'just emerging' is 
<br>
infinitesimal; but what is not an infinitesimal probability is that the 
<br>
AI's odd internal utility function has given rise to the behavioral 
<br>
strategy of trying to fool you.&quot;  Which hypothesis has greater prior 
<br>
probability?  Which hypothesis has greater likelihood density in the 
<br>
range of the observed evidence?
<br>
<p>Now in practice, I admit that there have been cases where the 
<br>
experimental observations told us which hypotheses we needed to test; 
<br>
nearly all revolutionary science, as opposed to routine science, happens 
<br>
this way.  But it is also true in practice that you have to know what 
<br>
you're seeing.  When it comes to interpreting what the behavior of an 
<br>
AGI can tell us about its internal workings, I think you may need to 
<br>
solve most of the problem in order to know what you're seeing.  In 
<br>
matters of Friendly AI, I think that the same evidence might be 
<br>
interpreted in highly different ways by a naive observer (especially 
<br>
someone who wanted to believe the AI was friendly) and someone who'd 
<br>
gone deep into theory before performing the experiment.  In ordinary 
<br>
science, you can exclude the hypothesis that Nature is actively trying 
<br>
to deceive you.
<br>
<p><p>Third, last time I checked, you were still attempting to come up with 
<br>
reasons why the AI you planned to experiment with could not possibly 
<br>
undergo hard takeoff, rather than building in controlled ascent / 
<br>
emergency shutdown features in at every step as a simple matter of 
<br>
course.  I recall that you once said that the chance of the current 
<br>
version of Novamente undergoing an unexpected hard takeoff was &quot;a 
<br>
million to one&quot;.  If you've read the heuristics and biases chapter, you 
<br>
now know why a statement like this is sufficient to make me say to 
<br>
myself, &quot;This is the way the world ends.&quot;  Whether or not the Novamente 
<br>
project is responsible (I regard that as quite a small probability), 
<br>
this is the way the world ends.  It is quite possible that whosoever 
<br>
destroys the world will do so using an AI that they believed &quot;couldn't 
<br>
possibly&quot; be a threat, so putting emergency features into AIs that 
<br>
&quot;can't possibly&quot; be a threat is a highly valuable heuristic.  Even 
<br>
natural selection, the trivial case of design with intelligence equal to 
<br>
zero, output an AGI.  For me to trust that someone genuinely did have a 
<br>
legitimate reason for staking Earth's existence on their desire to know 
<br>
some AGI result - and this would be the case even with every safety 
<br>
precaution imaginable - I'd have to see them taking safety precautions 
<br>
as a matter of course, not trying to guess whether they were &quot;necessary&quot; 
<br>
or not.  Guessing this is a bad habit.  Sooner or later you'll guess wrong.
<br>
<p>There's similarly the case chronicled by Nicholas Nassim Taleb, which I 
<br>
didn't manage to put into the _Cognitive biases_ chapter, of someone 
<br>
who, as his stocks continually dropped, continually kept asserting that 
<br>
they were bound to go up again; and Taleb notes that he did not plan in 
<br>
advance what to do in case his stocks dropped, or plan in advance what 
<br>
that contingency would mean, or set any hard point where he would take 
<br>
alarm and sell; and thus he was wiped out.  So you don't plan to 
<br>
implement emergency shutdown features today, and what's much more 
<br>
alarming, you haven't exposed for comment your schedule of exactly when 
<br>
they will become necessary.  In fact, you haven't even given us any 
<br>
reason to believe that, if you got unexpectedly powerful results out of 
<br>
an AGI, you wouldn't just send out for champagne, pushing your brilliant 
<br>
new discovery as far and as fast as you could, coming up with reasons 
<br>
why you were safe or expending the minimal effort needed to purchase a 
<br>
warm glow of satisfaction, and proceed thus until, unnoticed, you pass 
<br>
the unmarked and unremarkable spot that was your very last chance to 
<br>
turn back.
<br>
<p><p>But the most important reason I am highly skeptical is the same reason 
<br>
my current self would be highly skeptical of section 4 of CFAI, which is 
<br>
that the actual strategy, the actual day-to-day actions, look exactly 
<br>
like they would look if the thought of FAI had never entered your/my 
<br>
mind.  The purpose of rational thought is to direct actions.  If you're 
<br>
not going to change your actions, there's really no point in wasting all 
<br>
that effort on thinking.
<br>
<p>It's not thought, but action, that counts.  I'd have a very different 
<br>
opinion of this verbal advice to &quot;devise an experimental strategy for 
<br>
FAI&quot; if you posted a webpage containing a list of which FAI-related 
<br>
experiments you wanted to do, what you thought you might learn from them 
<br>
that you couldn't read off of existing science, and which observations 
<br>
you felt would license you to make which conclusions about the rules of 
<br>
Friendly AI.  Why would I feel better?  Because I don't expect someone's 
<br>
first forays into Friendly AI to turn out well.  So what matters is 
<br>
whether the particular mistakes someone makes force them to learn more 
<br>
about the subject, hold their ideas to high standards, work out detailed 
<br>
consequences, expose themselves to criticism, witness other people doing 
<br>
things that they think will fail and so beginning to appreciate the 
<br>
danger of the problem...  What counts in the long run is whether your 
<br>
initial mistaken approach forces you to go out and learn many fields, 
<br>
work out consequences in detail, appreciate the scope of the problem, 
<br>
hold yourself to high standards, fear mental errors, and, above all, 
<br>
keep working on the problem.
<br>
<p>The mistake I made in 1996, when I thought any SI would automatically be 
<br>
Friendly, was a very bad mistake because it meant I didn't have to 
<br>
devote further thought to the problem.  The mistake I made in 2001 was 
<br>
just as much a mistake, but it was a mistake that got me to spend much 
<br>
more time thinking about the problem and study related fields and hold 
<br>
myself to a higher standard.
<br>
<p>If I saw a detailed experimental plan, expressed mathematically, that 
<br>
drew on concepts from four different fields or whatever... I wouldn't 
<br>
credit you with succeeding, but I'd see that you were actually changing 
<br>
your strategy, making Friendliness a first-line requirement in the sense 
<br>
of devoting actual work-hours to it, showing willingness to shoulder 
<br>
safety considerations even when they seem bothersome and inconvenient, 
<br>
etc. etc.  It would count as *trying*.  It might get you to the point 
<br>
where you said, &quot;Whoa!  Now that I've studied just this one specific 
<br>
problem for a couple of years, I realize that what I had previously 
<br>
planned to do won't work!&quot;
<br>
<p>But in terms of how you spend your work-hours, which code you write, 
<br>
your development plans, how you allocate your limited reading time to 
<br>
particular fields, then this business of &quot;First experiment with AGI&quot; has 
<br>
the fascinating and not-very-coincidental-looking property of having 
<br>
given rise to a plan that looks exactly like the plan one would pursue 
<br>
if Friendly AI were not, in fact, an issue.
<br>
<p>Because this is a young field, how much mileage you get out will be 
<br>
determined in large part by how much sweat you put in.  That's the 
<br>
simple practical truth.  The reasons why you do X are irrelevant given 
<br>
that you do X; they're &quot;screened off&quot;, in Pearl's terminology.  It 
<br>
doesn't matter how good your excuse is for putting off work on Friendly 
<br>
AI, or for not building emergency shutdown features, given that that's 
<br>
what you actually do.  And this is the complaint of IT security 
<br>
professionals the world over; that people would rather not think about 
<br>
IT security, that they would rather do the minimum possible and just get 
<br>
it over with and go back to their day jobs.  Who can blame them for such 
<br>
human frailty?  But the result is poor IT security.
<br>
<p>What kind of project might be able to make me believe that they had a 
<br>
serious chance of achieving Friendly AI?  They would have to show that, 
<br>
rather than needing to be *argued into* spending effort and taking 
<br>
safety precautions, they enthusiastically ran out and did as much work 
<br>
on safety as they could.  That would not be sufficient but it would 
<br>
certainly be necessary.  They would have to show that they did *not* 
<br>
give in to the natural human tendency to put things off until tomorrow, 
<br>
nor come up with clever excuses why inconvenient things do not need to 
<br>
be done today, nor invent reasons why they are almost certainly safe for 
<br>
the moment.  For whoso does this today will in all probability do the 
<br>
same tomorrow and tomorrow and tomorrow.
<br>
<p>I do think there's a place for experiment in Friendly AI development 
<br>
work, which is as follows:  One is attempting to make an experimental 
<br>
prediction of posthuman friendliness and staking the world on this 
<br>
prediction; there is no chance for trial and error; so, as you're 
<br>
building the AI, you make experimental predictions about what it should 
<br>
do.  You check those predictions, by observation.  And if an 
<br>
experimental prediction is wrong, you halt, melt, catch fire, and start 
<br>
over, either with a better theory, or holding yourself to a stricter 
<br>
standard for what you dare to predict.  Maybe that is one way an 
<br>
adolescent could confront an adult task.  There are deeper theoretical 
<br>
reasons (I'm working on a paper about this) why you could not possibly 
<br>
expect an AI to be Friendly unless you had enough evidence to *know* it 
<br>
was Friendly; roughly, you could not expect *any* complex behavior that 
<br>
was a small point in the space of possibilities, unless you had enough 
<br>
evidence to single out that small point in the large space.  So, 
<br>
although it sounds absurd, you *should* be able to know in advance what 
<br>
you can and can't predict, and test those predictions you dare make, and 
<br>
use that same strict standard to predict the AI will be Friendly.  You 
<br>
should be able to win in that way if you can win at all, which is the 
<br>
point of the requirement.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15175.html">Michael Vassar: "Re: I am a moral, intelligent being (was Re: Two draft papers: AI and existentia"</a>
<li><strong>Previous message:</strong> <a href="15173.html">Eliezer S. Yudkowsky: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>In reply to:</strong> <a href="15150.html">Ben Goertzel: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15176.html">Michael Vassar: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15176.html">Michael Vassar: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15177.html">Ben Goertzel: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15180.html">BillK: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15186.html">Jef Allbright: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15174">[ date ]</a>
<a href="index.html#15174">[ thread ]</a>
<a href="subject.html#15174">[ subject ]</a>
<a href="author.html#15174">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
