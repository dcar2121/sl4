<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [agi] Two draft papers: AI and existential risk; heuristics and biases</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: [agi] Two draft papers: AI and existential risk; heuristics and biases">
<meta name="Date" content="2006-06-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [agi] Two draft papers: AI and existential risk; heuristics and biases</h1>
<!-- received="Tue Jun  6 23:23:14 2006" -->
<!-- isoreceived="20060607052314" -->
<!-- sent="Tue, 06 Jun 2006 22:20:43 -0700" -->
<!-- isosent="20060607052043" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: [agi] Two draft papers: AI and existential risk; heuristics and biases" -->
<!-- id="4486622B.4030208@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="638d4e150606060809qdbb0840t525e5acc174960bb@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20[agi]%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Jun 06 2006 - 23:20:43 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15174.html">Eliezer S. Yudkowsky: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Previous message:</strong> <a href="15172.html">James MacAulay: "Re: Let's resolve it with a thought experiment"</a>
<li><strong>In reply to:</strong> <a href="15150.html">Ben Goertzel: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15174.html">Eliezer S. Yudkowsky: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15173">[ date ]</a>
<a href="index.html#15173">[ thread ]</a>
<a href="subject.html#15173">[ subject ]</a>
<a href="author.html#15173">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; CFAI tried, and ultimately didn't succeed, to articulate an approach
</em><br>
<em>&gt; to solving the problem of Friendly AI.  Or at least, that is the
</em><br>
<em>&gt; impression it made on me....
</em><br>
<em>&gt; 
</em><br>
<em>&gt; On the other hand, AIGR basically just outlines the problem of
</em><br>
<em>&gt; Friendly AI and explains why it's important and why it's hard.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In this sense, it seems to be a retreat....
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I suppose the subtext is that your attempts to take the intuitions
</em><br>
<em>&gt; underlying CFAI and turn them into a more rigorous and defensible
</em><br>
<em>&gt; theory did not succeed.
</em><br>
<p>The subtext is:
<br>
<p>1)  Do not propose any solutions before discussing the problem as 
<br>
thoroughly as possible without proposing any.  This is most important 
<br>
when dealing with extremely difficult problems, as that is when people 
<br>
are most apt to propose solutions immediately.  See the associated book 
<br>
chapter on heuristics and biases.
<br>
<p>2)  This is a book chapter for general academic readers interested in 
<br>
how AI fits into the big picture of global catastrophic risks.  It was 
<br>
sharply constrained by space, as you may have noticed, and there simply 
<br>
wasn't time to go into any AI-design details.  That would have been a 
<br>
book not a chapter.
<br>
<p>3)  I am still working on a rigorous theory and have made what I count 
<br>
as progress.  Over the next year or so, I hope to work on this nearly 
<br>
full-time, and am refusing to take on other commitments (such as book 
<br>
chapters) to make sure my time stays free.
<br>
<p>CFAI proposed a solution too quickly, and worse, claimed it was a 
<br>
workable approach in itself.  Before a complete solution necessarily 
<br>
comes a partial solution, where you know how to solve M out of N 
<br>
problems with M &lt; N.  This is where I am now, but at least I know it. 
<br>
At the time of CFAI, I (Eliezer-2001) had difficulty admitting I didn't 
<br>
have a workable solution in hand, because that would have meant that I'd 
<br>
have to work more on FAI theory instead of doing what I wanted to do, 
<br>
what I thought would make me look more respectable, and plunging 
<br>
straight into AI as soon as I had the funding to hire more 
<br>
programmers...  Actually, it would be more accurate to say the reason I 
<br>
wanted to believe I had a workable solution in hand, was because this 
<br>
let me preserve all the existing plans for AGI development that I had 
<br>
made before I realized that FAI was an issue.  People try to preserve as 
<br>
much of their existing plans as possible, when unexpected news arrives; 
<br>
in this case, what I needed to do, and did not do, was rethink all my 
<br>
plans from scratch.  But that was a much younger Eliezer...  Needless to 
<br>
say, I think that you, Ben, are now making my old mistake.
<br>
<p><em>&gt; I also note that your Coherent Extrapolated Volition ideas were not
</em><br>
<em>&gt; focused on in AIGR, which I think is corrrect because I consider CEV a
</em><br>
<em>&gt; fascinating science-fictional speculation without much likelihood of
</em><br>
<em>&gt; ever being practically relevant.
</em><br>
<p>That is because CEV is merely my proposed *solution*, and AIGR doesn't 
<br>
even get far enough into discussing the problem; it is nowhere near the 
<br>
point where it would become wise to propose a solution.  Did you read 
<br>
the chapter on heuristics and biases?  If not, please stop here, and 
<br>
read that chapter.
<br>
<p><em>&gt; I agree with you that taking a more rigorous mathematical approach is
</em><br>
<em>&gt; going to be the way -- if any -- to a theory of FAI.  However, I am
</em><br>
<em>&gt; more optimistic that this approach will lead to a theory of FAI
</em><br>
<em>&gt; **assuming monstrously great computational resources** than to a
</em><br>
<em>&gt; theory of pragmatic FAI.  This would be expected since thanks to
</em><br>
<em>&gt; Schmidhuber, Hutter and crew we now have the beginnings of a theory of
</em><br>
<em>&gt; AGI itself assuming monstrously great computational resources, but
</em><br>
<em>&gt; nothing approaching a theory of AGI assuming realistic computational
</em><br>
<em>&gt; resources...
</em><br>
<p>As previously disscussed on AGI, I think that Schmidhuber, Hutter et. 
<br>
al. left key dimensions out of their AI, such as its ability to conceive 
<br>
of what happens when it drops an anvil on its own head.  That is, what 
<br>
happens when an environmental process penetrates the intrinsic Cartesian 
<br>
boundary on which their formalism is based.
<br>
<p><em>&gt; It would seem to me that FIRST we should try to create a theoretical
</em><br>
<em>&gt; framework useful for analyzing and describing AGIs that operate with
</em><br>
<em>&gt; realistic computational resources.
</em><br>
<p>This is more or less what I'm doing right now.
<br>
<p>And lo, I only started making progress on the problem by holding it to 
<br>
Friendly AI standards of determinism and knowability.  Otherwise, you 
<br>
end up sweeping all the interesting parts of the problem under the rug, 
<br>
forgiving your own ignorance, hoping for good results without proof, and 
<br>
generally holding yourself to much too low a standard to come up with an 
<br>
interesting theory.  I've made a lot more progress on AGI than in the 
<br>
CFAI/LOGI era, and the difference was holding myself to the standard of 
<br>
proof-of-Friendliness.
<br>
<p>AGI understanding will always run ahead of FAI understanding; I have 
<br>
previously remarked on this point - it is what makes the problem of 
<br>
Earth's survival *difficult*.  Surely it is not possible to be able to 
<br>
build FAI, and not be able to build AGI.  But you can develop 
<br>
sophisticated AGI techniques that are not even theoretically usable for 
<br>
AGI, that *cannot* be reshaped to safety.  Thinking about AGI doesn't 
<br>
put you on an incremental path.  I've *been* there, Ben.  I wrote LOGI 
<br>
while thinking about AGI, and then I had to throw LOGI away and start 
<br>
over from scratch because it wasn't even on an incremental path to FAI. 
<br>
&nbsp;&nbsp;Neither is CFAI, for that matter.
<br>
<p><em> &gt; Then Friendly AI should be
</em><br>
<em> &gt; approached, theoretically, within this framework.
</em><br>
<p>You won't be able to approach FAI &quot;within&quot; an AGI framework that was 
<br>
designed without thinking about FAI.  You *will*, always, be able to 
<br>
approach a certain type of AGI within a framework that was designed for 
<br>
thinking about FAI.  *That* is where the inequality comes from - that's 
<br>
why you'll always know more about AGI than FAI at any given point.
<br>
<p>What you need is a frame of mind in which there are no &quot;AGI&quot; problems. 
<br>
There is, simply, the goal of building a Friendly AI, and you do what is 
<br>
required for that in whatever order seems best, including devising a 
<br>
theory of optimization with limited computational resources.  That 
<br>
someone else could call that &quot;AGI&quot; is of no consequence, except insofar 
<br>
as there is existential risk from incomplete theories.
<br>
<p><em>&gt; I can see the viability of also proceeding in a more specialized way,
</em><br>
<em>&gt; and trying to get a theory of FAI under limited resources in the
</em><br>
<em>&gt; absence of an understanding of other sorts of AGIs under limited
</em><br>
<em>&gt; resources.  But my intuition is that the best way to approach &quot;FAI
</em><br>
<em>&gt; under limited resources&quot; is to first get an understanding of &quot;AGI
</em><br>
<em>&gt; under limited resources.&quot;
</em><br>
<p>The vast majority of AGI techniques are intrinsically unsuited to FAI 
<br>
and are not on an incremental pathway to FAI.  So why am I, right now, 
<br>
working mostly on &quot;AGI&quot;-ish questions, rather than CEV-ish questions? 
<br>
Because, in the course of solving those problems which are naturally 
<br>
encountered on the road to an FAI theory, one finds that the simplest 
<br>
questions of FAI, which must be answered first before moving on to more 
<br>
complex questions, happen to be questions *about a certain type of AGI*. 
<br>
&nbsp;&nbsp;This does *not* mean you can answer them if you conceive of what you 
<br>
are doing as &quot;trying to build AGI&quot; rather than &quot;trying to build FAI&quot;.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15174.html">Eliezer S. Yudkowsky: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Previous message:</strong> <a href="15172.html">James MacAulay: "Re: Let's resolve it with a thought experiment"</a>
<li><strong>In reply to:</strong> <a href="15150.html">Ben Goertzel: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15174.html">Eliezer S. Yudkowsky: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15173">[ date ]</a>
<a href="index.html#15173">[ thread ]</a>
<a href="subject.html#15173">[ subject ]</a>
<a href="author.html#15173">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
