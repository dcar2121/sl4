<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Two draft papers: AI and existential  risk;	heuristics and biases</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Two draft papers: AI and existential  risk;	heuristics and biases">
<meta name="Date" content="2006-06-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Two draft papers: AI and existential  risk;	heuristics and biases</h1>
<!-- received="Tue Jun 13 00:35:20 2006" -->
<!-- isoreceived="20060613063520" -->
<!-- sent="Mon, 12 Jun 2006 23:34:22 -0700" -->
<!-- isosent="20060613063422" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Two draft papers: AI and existential  risk;	heuristics and biases" -->
<!-- id="448E5C6E.9080507@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="7.0.1.0.2.20060609224449.02383310@gmu.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Two%20draft%20papers:%20AI%20and%20existential%20%20risk;	heuristics%20and%20biases"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Jun 13 2006 - 00:34:22 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15272.html">Jef Allbright: "Re: [extropy-chat] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Previous message:</strong> <a href="15270.html">George Dvorsky: "Re: Fwd: [ai-philosophy] Robotic evolution and ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15272.html">Jef Allbright: "Re: [extropy-chat] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15272.html">Jef Allbright: "Re: [extropy-chat] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15278.html">Keith Henson: "Human motivations was Two draft papers:"</a>
<li><strong>Reply:</strong> <a href="15287.html">Chris Capel: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15271">[ date ]</a>
<a href="index.html#15271">[ thread ]</a>
<a href="subject.html#15271">[ subject ]</a>
<a href="author.html#15271">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Robin Hanson wrote:
<br>
<em>&gt; At 12:33 PM 6/4/2006, Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;These are drafts of my chapters for Nick Bostrom's forthcoming edited
</em><br>
<em>&gt;&gt;volume _Global Catastrophic Risks_.
</em><br>
<em>&gt;&gt;_Cognitive biases potentially affecting judgment of global risks_
</em><br>
<em>&gt;&gt;   <a href="http://intelligence.org/Biases.pdf">http://intelligence.org/Biases.pdf</a>
</em><br>
<em>&gt;&gt;An introduction to the field of heuristics and biases ...
</em><br>
<em>&gt;&gt;_Artificial Intelligence and Global Risk_
</em><br>
<em>&gt;&gt;   <a href="http://intelligence.org/AIRisk.pdf">http://intelligence.org/AIRisk.pdf</a>
</em><br>
<em>&gt;&gt;The new standard introductory material on Friendly AI.
</em><br>
<p>Robin,
<br>
<p>It turns out that I've got more stuff coming up (moving to a new 
<br>
apartment within Silicon Valley) so I may not be able to carry on this 
<br>
conversation in as much detail as I'd like.  I did want to respond to at 
<br>
least what you've said so far.  If you write a long response to this, be 
<br>
forewarned - I may not be able to respond back.
<br>
<p><em>&gt; The chapter on cognitive biases was excellent.   Regarding the other 
</em><br>
<em>&gt; chapter, while you seem to have thought lots about many related 
</em><br>
<em>&gt; issues over the years, you don't seem to have worked much on the 
</em><br>
<em>&gt; issue I get stuck on: the idea that a single relatively isolated AI 
</em><br>
<em>&gt; system could suddenly change from negligible to overwhelmingly powerful.
</em><br>
<p>As you may recall, that's where I got started on this &quot;seed AI&quot; business 
<br>
in 1998 - talking about recursive self-improvement.
<br>
<p>But while writing the chapter, I made a conscious decision to talk more 
<br>
about Friendly AI, and less about seed AI.  Because, among other 
<br>
reasons, Friendly AI is both harder to explain and more important to 
<br>
explain.  People &quot;get&quot; the concept of seed AI relatively easily, though 
<br>
they may or may not agree with it.
<br>
<p><em>&gt; You warn repeatedly about how easy is is to fool oneself into 
</em><br>
<em>&gt; thinking one understands AI, and you want readers to apply this to 
</em><br>
<em>&gt; their intuitions about the goals an AI may have.  
</em><br>
<p>The danger is anthropomorphic thinking, in general.  The case of goals 
<br>
is an extreme case where we have specific, hardwired, wrong intuitions. 
<br>
&nbsp;&nbsp;But more generally, all your experience is in a human world, and it 
<br>
distorts your thinking.  Perception is the perception of differences. 
<br>
When something doesn't vary in our experience, we stop even perceiving 
<br>
it; it becomes as invisible as the oxygen in the air.  The most 
<br>
insidious biases, as we both know, are the ones that people don't see.
<br>
<p>You expect surface effects to work like they do in your human 
<br>
experience, even when the fundamental causes of those surface effects 
<br>
change.  You expect assertions to be justified in terms of their 
<br>
perceived departure from what seems normal to you, but your norms are 
<br>
human norms.  For example:
<br>
<p><em>&gt; But you seem to be 
</em><br>
<em>&gt; relying almost entirely on unarticulated intuitions when you conclude 
</em><br>
<em>&gt; that very large and rapid improvement of isolated AIs is likely.
</em><br>
<p>Here you measure &quot;rapid&quot; on a human scale.  There is nothing in the laws 
<br>
of physics which says that one thought per 10^45 Planck intervals is 
<br>
&quot;normal&quot;, one thought per 10^55 Planck intervals is &quot;slow&quot;, and one 
<br>
thought per 10^35 Planck intervals is &quot;fast&quot;.
<br>
<p>Pretend that a politically correct review committee is going to go over 
<br>
all your work looking for signs of humanocentrism.
<br>
<p><em> &gt; A standard abstraction seems useful to me:  when knowledge
</em><br>
<em> &gt; accumulates in many small compatible representations, growth is in
</em><br>
<em> &gt; the largest system that can share such representations.
</em><br>
<p>Presuming that information can be shared more cheaply than it can be 
<br>
initially produced; i.e. that the cost of bandwidth is less than the 
<br>
cost of local production.
<br>
<p><em> &gt; Since DNA
</em><br>
<em> &gt; is sharable mainly within a species, the improvements that any one
</em><br>
<em> &gt; small family of members can produce are usually small compared to the
</em><br>
<em> &gt; improvements transferred by sex within the species.
</em><br>
<p>Here you analogize to evolution.  This is something to be wary of 
<br>
because evolution is an extremely unusual special case of an 
<br>
optimization process.  I use all sorts of evolutionary arguments, but 
<br>
only to illustrate *how different* an optimization process can be from 
<br>
human intelligence - never to say that something *must* be like evolution.
<br>
<p>When knowledge accumulates in small modular representations, growth is 
<br>
in the largest system that *does* share such representations - not the 
<br>
largest system that *can*.  In principle, species could develop means of 
<br>
swapping adaptations among themselves.  Wouldn't you like gills?  But 
<br>
that's not how it works with multicellular organisms.  There's a very 
<br>
clear evolutionary logic for this - it's not a mystery.  But if a human 
<br>
were in charge of the system, if we were running the show, we'd 
<br>
plagiarize the heck out of everything and export adaptations wholesale 
<br>
between species.
<br>
<p>So in fact, ecology contradicts the generalization you brought it to 
<br>
support - that growth is within the largest pool where knowledge *can* 
<br>
be shared, as a human onlooker thinks of opportunity.  Growth is within 
<br>
the pool where knowledge *is* shared.
<br>
<p>The ecological world is like one in which every two human cultures that 
<br>
became sufficiently different, *completely stopped* communicating with 
<br>
each other.
<br>
<p>We'd never do that.  Even if we hated their guts, we'd steal their guns.
<br>
<p>In spirit, if not in letter, this may seem like an argument in your 
<br>
direction.  Evolution is dumber than a brain, and as we moved in the 
<br>
direction of increasing intelligence, we seemed to move toward 
<br>
perceiving more opportunities for communication.  Or at least more 
<br>
opportunities for theft.  Humans plagiarized flight from birds, but I 
<br>
haven't seen much capability-transfer going the other way.
<br>
<p>But:
<br>
<p>There's a wider universe out there;
<br>
It doesn't work like you do;
<br>
You can't trust your intuitions;
<br>
Evolutionary analogies have dangers both subtle and gross;
<br>
Just because something *could* happen doesn't mean that it will.
<br>
<p>This also struck me about your &quot;Dreams of Autarky&quot;; you said:
<br>
<p><em>&gt; The cells in our bodies are largely-autonomous devices and manufacturing plants, producing most of what they need internally. Our biological bodies are as wholes even more autonomous, requiring only water, air, food, and minimal heat to maintain and reproduce themselves under a wide variety of circumstances. Furthermore, our distant human ancestors acquired tools that made them even more general, i.e., able to survive and thrive in an unusually diverse range of environments. And the minds our ancestors acquired were built to function largely autonomously, with only minor inputs from other minds.
</em><br>
<p>And from this you read:  There is a trend toward greater interdependency 
<br>
over recent time (~10 Ky), and you expect this trend to continue.
<br>
<p>An alternate reading would be:  Modern human culture is a bizarre 
<br>
special case in a universe that doesn't usually work that way.  I 
<br>
discuss this in more detail below.
<br>
<p><em> &gt; Since humans
</em><br>
<em> &gt; share their knowledge via language and copying practices, the
</em><br>
<em> &gt; improvements that a small group of people can make are small compared
</em><br>
<em> &gt; to the improvements transferred from others, and made available by
</em><br>
<em> &gt; trading with those others.
</em><br>
<p>And this is an example of what I mean by anchoring on human norms.  In 
<br>
your everyday experience, an economy is made up of humans trading 
<br>
*artifacts* and *knowledge*.  You don't even think to question this, 
<br>
because it's so universal.
<br>
<p>Humans don't trade brains.  They don't open up their skulls and trade 
<br>
visual cortex.  They don't trade adaptations.  They don't even trade 
<br>
procedural knowledge.  No matter how much someone offers to pay me, I 
<br>
cannot sell them my command of English or my ability to write 
<br>
entertaining nonfiction - not that I would ever sell the original.  I'm 
<br>
not sure I would sell a copy.  But the point is that I have no choice. 
<br>
I *can't* sell, whether I want to or not.
<br>
<p>We can trade the products of our minds, but not the means of production. 
<br>
&nbsp;&nbsp;This is an IMPORTANT ASSUMPTION in human affairs.
<br>
<p>John K Clark once said:  &quot;It mystifies me why anyone would even try to 
<br>
move large quantities of matter around the universe at close to the 
<br>
speed of light.  It's as silly as sending ice cubes to the south pole by 
<br>
Federal Express.  There's already plenty of matter in the Virgo Galactic 
<br>
Cluster 2 billion light years away and it's every bit as good as the 
<br>
matter we have here.&quot;
<br>
<p>As it becomes more economical to ship the factory, it becomes less 
<br>
economical to ship the products of the factory.  This is 
<br>
double-bonus-true of cognition.  A compact description of the underlying 
<br>
rules of arithmetic (e.g. the axioms of addition) can give rise to a 
<br>
vast variety of surface facts (e.g. that 953,188 + 12,152 = 965,340). 
<br>
Trying to capture the surface behaviors, rather than the underlying 
<br>
generator, rapidly runs into the problem of needing to capture an 
<br>
infinite number of facts. AI people who run into this problem and don't 
<br>
understand where it comes from refer to it as the &quot;common-sense problem&quot; 
<br>
or &quot;frame problem&quot;, and think that the solution is to build an AI that 
<br>
can understand English so it can download all the arithmetical facts it 
<br>
needs from the Internet.
<br>
<p>In our modern world, everything focuses around shipping around 
<br>
declarative verbal sentences, because this is what human beings evolved 
<br>
to trade.  We can't trade procedural knowledge, except by extremely 
<br>
laborious, expensive, failure-prone processes - such as multi-year 
<br>
apprenticeships in school.  And neural circuitry we cannot trade at all.
<br>
<p>When you reach down into the generators, you find more power than when 
<br>
you only play with surface phenomena.  You amplify leverage by moving 
<br>
closer to the start of the causal chain.  Like moving the pebbles at the 
<br>
top of the mountain where they start avalanches.  You cannot build Deep 
<br>
Blue (the famous program that beat Garry Kasparov for the world chess 
<br>
championship) by programming in a good chess move for every possible 
<br>
chess position.  First of all, it is impossible to build a chess player 
<br>
this way, because you don't know exactly which positions it will 
<br>
encounter.  And second, even if you did this, the resulting program 
<br>
would not play chess any better than you do.  Deep Blue's programmers 
<br>
didn't just capture their own chess-move generator.  If they'd captured 
<br>
their own chess-move generator, they could have avoided the problem of 
<br>
programming an infinite number of chess positions - but they couldn't 
<br>
have beat Garry Kasparov; they couldn't have built a program that played 
<br>
better chess than any human in the world.  The programmers built a 
<br>
*better* move generator.  This is something they couldn't even do on the 
<br>
level of organization of trading surface moves.
<br>
<p>At Goertzel's recent AGI conference, I said:  &quot;The only thing I know of 
<br>
more difficult than building a Friendly AI is creating a child.&quot;  And 
<br>
someone inevitably said:  &quot;Creating a child is easy, anyone can do it.&quot; 
<br>
&nbsp;&nbsp;And I said:  &quot;That is like putting quarters into a Coke machine, and 
<br>
saying, 'Look, I made a Coke!'&quot;
<br>
<p>Humans who spark the process of embryogenesis possess none of the 
<br>
knowledge they would need to design children in their own right; they 
<br>
are just pulling the lever that starts an incredibly complex machine 
<br>
that they don't understand and couldn't build themselves.
<br>
<p>People sometimes try to build AIs from &quot;semantic networks&quot;, with data 
<br>
like is(cat, animal) or cuts(lawnmower, grass), and then they're 
<br>
surprised when the AI doesn't do anything.  This is because a verbal 
<br>
sentence - the units of knowledge most commonly traded among humans - 
<br>
are like levers for starting a machine.  That's all we need to trade 
<br>
among ourselves, because we all have the machine.  But people don't 
<br>
realize this - the machine is universal, and therefore it's invisible; 
<br>
perception is the perception of differences.  So someone who programs 
<br>
these tiny, lifeless LISP tokens into an AI is surprised when the AI 
<br>
does absolutely nothing interesting, because as far as they can see, the 
<br>
AI has everything it needs.  But the levers have no mechanisms to 
<br>
trigger, the instruction set has no CPU.  When you see the word &quot;cat&quot; it 
<br>
paints a complex picture in your visual cortex - the mere ASCII string 
<br>
carries none of that information, it is just a lever that triggers a 
<br>
machine you already have.
<br>
<p>We are like people who refine gasoline, and trade gasoline, and 
<br>
understand the concept of &quot;running out of gas&quot;, but who never think 
<br>
about cars.  So you don't focus on the question of whether there might 
<br>
be more efficient cars.
<br>
<p>And yet there are these things called &quot;chimps&quot; that can't use any of the 
<br>
knowledge you're so playfully batting about.  You don't even think to 
<br>
ask why chimps are excluded from the knowledge economy - though they're 
<br>
incredibly close to us evolutionarily.  You don't encounter chimps in 
<br>
your everyday life; they don't participate in your economy... and yet 
<br>
what separates humans from chimps is the very last layer of icing on a 
<br>
brain-cake that's almost entirely shared between us.
<br>
<p>A comparative handful of improvements to underlying *generators*, 
<br>
underlying *brain circuitry*, are enough to entirely exclude chimps from 
<br>
our knowledge economy; they cannot absorb the knowledge we are trading 
<br>
around, and can do nothing with it.  Ricardo's Law of Comparative 
<br>
Advantage does not extend to chimps.  And chimps are our closest 
<br>
cousins!  What about mice?  What about lizards?  *That* is the power of 
<br>
between-species intelligence differences - underlying generators that 
<br>
differ by the presence of entire complex adaptations.
<br>
<p>Humans don't ship around brain circuitry and complex adaptations because 
<br>
we can't.  We don't even realize how powerful they are, because 
<br>
differences of brain circuitry are so hugely powerful as to drop our 
<br>
closest competitors out of the economy and out of sight.  Anything that 
<br>
doesn't have *all* your brain circuitry and all your complex adaptations 
<br>
is so powerless, compared to you, that it doesn't occur to you to look 
<br>
in that direction - even though a chimp has 95% of your genomic complexity.
<br>
<p>This is what I mean by saying that humans are an unusual special case of 
<br>
non-autarky.  Ordinarily, when an optimization process builds something, 
<br>
it builds things that, by comparison to an interdependent human economy, 
<br>
look like autarkic monoliths.  Humans are extremely unusual because we 
<br>
gained the ability to transfer units of knowledge (lever-pulling 
<br>
instructions) between ourselves, but we could not reach down to the 
<br>
level on which evolution built us to begin with.  Thus we could *not* 
<br>
encapsulate the accumulating complexity into our own system designs.  We 
<br>
could *not* give our children the accumulated knowledge of our science, 
<br>
we could *not* build into their bodies the accumulated power of our 
<br>
technology.  Evolution, in contrast, usually builds into each member of 
<br>
a species all the adaptive complexity it manages to accumulate.  Why 
<br>
shouldn't it, since it can?
<br>
<p><em> &gt; The obvious question about a single AI is why its improvements could
</em><br>
<em> &gt; not with the usual ease be transferred to other AIs or humans, or
</em><br>
<em> &gt; made available via trades with those others.
</em><br>
<p>Transferring to other AIs is one issue, but that you ask about 
<br>
transferring to humans indicates pretty clearly that you're thinking 
<br>
about declarative knowledge rather than brain circuitry.
<br>
<p>Insert here the usual lecture about the brain being a mess of spaghetti 
<br>
code that is not modular, cannot easily be read out or written to, runs 
<br>
at slow serial speeds, was never designed to be improved, and is not 
<br>
end-user-modifiable.  (It's easier to build a 747 from scratch; than to 
<br>
inflate an existing bird to the size of a 747, that actually flies, as 
<br>
fast as a 747, without killing the bird or making it very uncomfortable. 
<br>
&nbsp;&nbsp;I'm not saying it could never, ever be done; but if it happens at all, 
<br>
it will be because the bird built a seed that grew into a 747 that 
<br>
upgraded the bird.  (And at this point the metaphor bursts into flames 
<br>
and dies.))
<br>
<p>You could imagine drawing a circle around all the AIs in the world, and 
<br>
suppose that growth is on the level of their knowledge economy.  WHICH 
<br>
CONSISTS OF TRADING AROUND BRAINWARE AND COMPLEX ADAPTATIONS.  The stuff 
<br>
that's so powerful that chimps who merely have 95% of what you have 
<br>
might as well not exist from your economic viewpoint.
<br>
<p>What goes on inside that circle is just as much a hard takeoff from the 
<br>
perspective of an outside human.
<br>
<p>Not that I think we'll see a knowledge economy among different AIs 
<br>
undergo hard takeoff, because...
<br>
<p><em> &gt; Today a single human can share the ideas within his own
</em><br>
<em> &gt; head far easier than he can share those ideas with others -
</em><br>
<em> &gt; communication with other people is far more expensive and
</em><br>
<em> &gt; error-prone.   Yet the rate at which a single human can innovate is
</em><br>
<em> &gt; so small relative to the larger economy that most innovation comes
</em><br>
<em> &gt; from ideas shared across people.
</em><br>
<p>Again, anchoring on the human way of doing things.  You do not have the 
<br>
capability to solve a problem by throwing ONE BIG human at it, so you 
<br>
think in terms of throwing lots of individual minds.
<br>
<p>But which is more effective - one human, six chimps, or a hundred 
<br>
squirrels?  All else being equal, it will generally be far more 
<br>
efficient to build a coherent individual out of the same amount of 
<br>
computing power, rather than divide that individual into pieces. 
<br>
Otherwise the human brain would have naturally evolved to consist of a 
<br>
hundred compartmentalized communicating squirrels.  (If this reminds you 
<br>
of anyone you know, it is pure coincidence.)
<br>
<p>Having individual minds is like having economies with separate 
<br>
currencies, fortified borders, heavily protectionist trade barriers, and 
<br>
wide seas separating their wooden ships.  It's more efficient to take 
<br>
down the trade barriers and adopt the same currency, in which case you 
<br>
soon end up with a single economy.
<br>
<p>Now, maybe France *wants* to preserve its French identity within the 
<br>
European Union, as a matter of intrinsic utilities; but that is a 
<br>
separate matter from maximizing efficiency.
<br>
<p>And even more importantly...
<br>
<p><em> &gt; If so, this single AI
</em><br>
<em> &gt; would just be part of our larger system of self-improvement.   The
</em><br>
<em> &gt; scenario of rapid isolated self-improvement would seem to be where
</em><br>
<em> &gt; the AI found a new system of self-improvement, where knowledge
</em><br>
<em> &gt; production was far more effective, *and* where internal sharing of
</em><br>
<em> &gt; knowledge was vastly easier than external sharing.
</em><br>
<p>You seem to be visualizing a world in which, at the time the *first* AI 
<br>
approaches the threshold of recursive self-improvement,
<br>
<p>(1) There are already lots of AIs around that fall short of strong 
<br>
recursivity.
<br>
<p>And these AIs:
<br>
<p>(2) Have ability to trade meaningful, important units between themselves.
<br>
<p>You think of knowledge of the kind humans evolved to share with each 
<br>
other.  I think of underlying brain circuitry of the kind that differs 
<br>
between species and is the ultimate generator of all human culture.  The 
<br>
latter is harder to trade - though, obviously, far more valuable.  How 
<br>
much would you pay for another 20 IQ points?  (And that's not even a 
<br>
difference of the interspecies kind, just the froth of individual 
<br>
variation.)
<br>
<p>Furthermore, the AIs can:
<br>
<p>(3) Gain significant economic benefits by reciprocally trading their 
<br>
software to each other.
<br>
<p>And they must also have:
<br>
<p>(4) Compatible motives in the long run.
<br>
<p>When I look over the present AGI landscape, and imagine what would 
<br>
happen if an AGI reached the threshold of strong recursivity in the next 
<br>
&nbsp;&nbsp;decade, I find myself thinking that:
<br>
<p>(1) There are so few AGI projects around at all, let alone projects with 
<br>
a clue, that at the time the first AGI reaches the critical threshold, 
<br>
there will be no other AGIs in the near vicinity of power.
<br>
<p>(2) Current AGI projects use such wildly differing theories that it 
<br>
would be a matter of serious difficulty for AGIs of less than superhuman 
<br>
ability to trade modules with each other.  (Albeit far less difficult 
<br>
than trading with humans.)  Or look at it this way - it takes a lot more 
<br>
programming ability to rewrite *another* AI's code than to rewrite your 
<br>
*own* code.  Brains predate language; internal bandwidth predates 
<br>
external bandwidth.  So the hard takeoff, when it starts, starts inside 
<br>
one AI.
<br>
<p>(3) Different AGIs, having been produced by different designers on 
<br>
different AGI projects, will not be like humans who are all the same 
<br>
make and model of car and interact economically as equals.  More like 
<br>
different species.  The top AGI will have as little to gain from trading 
<br>
with the next runner-up as we have to gain from trading with 
<br>
chimpanzees.  Or less; chimpanzees are 95% similar to us.  Even 
<br>
Ricardo's Law falls off the edge of the interspecies abyss.  If the AI 
<br>
wants twice as much brainpower on the problem, it'll absorb twice as 
<br>
much processing power into itself.
<br>
<p>(4) I'm not sure whether AIs of different motives would be willing to 
<br>
cooperate, even among the very rare Friendly AIs.  If it is *possible* 
<br>
to proceed strictly by internal self-improvement, there is a 
<br>
*tremendous* expected utility bonus to doing so, if it avoids having to 
<br>
share power later.
<br>
<p>With respect to (4), I am admittedly not visualizing a large group of 
<br>
individuals interacting as rough equals.  *Those* would have a motive to 
<br>
form coalitions for fear of being beaten by other coalitions.  (Whether 
<br>
humans would be worth including into any coalition, on grounds of pure 
<br>
efficiency, is a separate issue.)  But if you *automatically* visualize 
<br>
a large group of individuals interacting as rough equals, you need to 
<br>
put more effort into questioning your anchoring on human norms.  The 
<br>
psychic unity of humankind *mandates* that healthy humans do not differ 
<br>
by the presence of entire complex adaptations.
<br>
<p>*Of course* the economies you know run on entities who are all 
<br>
approximate equals - anyone who's not an approximate equal, like your 
<br>
chimp cousins, falls off the edge of vision.  Of course there are lots 
<br>
of similar individuals in a your economy - evolution doesn't produce 
<br>
unique prototypes, and human brains don't agglomerate into unitary 
<br>
megaminds.
<br>
<p><em> &gt; You say that humans today and natural selection do not self-improve
</em><br>
<em> &gt; in the &quot;strong sense&quot; because humans &quot;haven't rewritten the human
</em><br>
<em> &gt; brain,&quot; &quot;its limbic core, its cerebral cortex, its prefrontal
</em><br>
<em> &gt; self-models&quot; and natural selection has not &quot;rearchitected&quot; &quot;the
</em><br>
<em> &gt; process of mutation and recombination and selection,&quot; with &quot;its focus
</em><br>
<em> &gt; on allele frequencies&quot; while an AI &quot;could rewrite its code from
</em><br>
<em> &gt; scratch.&quot;
</em><br>
<em> &gt;
</em><br>
<em> &gt; The code of an AI is
</em><br>
<em> &gt; just one part of a larger system that would allow an AI to
</em><br>
<em> &gt; self-improve, just as the genetic code is a self-modifiable part of
</em><br>
<em> &gt; the larger system of natural selection, and human culture and beliefs
</em><br>
<em> &gt; are a self-modifiable part of human improvement today.
</em><br>
<p>Not &quot;self-modifiable&quot;.  The genome (as Hofstadter emphasized at the 
<br>
Singularity Summit, the genetic code means the ATCG coding system) is 
<br>
modified by the logic of natural selection.  To discover a case in which 
<br>
gene-optimizing logic was embedded in the genome itself would be a 
<br>
stunning, Lamarckian revolution in biology.
<br>
<p>The genome carries out processes, such as randomized sexual 
<br>
recombination, which are not of themselves optimizing, but which 
<br>
contribute to the logic of natural selection.  The logic of evolution is 
<br>
quite simple.  Sexual recombination is the only major example I can 
<br>
think of where the logic of evolution was significantly modified by 
<br>
genomic content.  Perhaps the original invention of DNA would count as 
<br>
replicators modifying the logic of evolution - though I'm not even sure 
<br>
I'd count that.
<br>
<p>Neither random mutation, nor random recombination, actually implement 
<br>
the optimizing part of the process - the part that produces information 
<br>
in the genome.  That part comes from nonrandom environmental selection. 
<br>
&nbsp;&nbsp;As far as I can think, the only genes which implement organismal-level 
<br>
optimization logics are those responsible for sexual selection within a 
<br>
species - and even they don't write directly to DNA.
<br>
<p>It is a lot easier to understand how evolution works than to understand 
<br>
how the brain works.  Evolution is a small handful of tricks - point 
<br>
mutation, random recombination, natural selection, sexual selection. 
<br>
They play out in very complex ways, but the optimization logic is 
<br>
simple.  The human brain is a *much bigger* set of tricks and is 
<br>
correspondingly more efficient.  And yet the brain does not write to DNA.
<br>
<p>Human culture and human beliefs are not a &quot;self-modifiable&quot; part of 
<br>
human improvement.  They are modified by human brains, but cannot freely 
<br>
rewrite the optimization logic of human brains.  One might argue that 
<br>
writing and science are analogous to the invention of DNA and sex 
<br>
respectively, significantly changing the rules of the game.  Even so 
<br>
there's an underlayer we can't reach.  If you think that the human brain 
<br>
isn't doing the important work of intelligence, only rules handed down 
<br>
culturally, then just try and program those cultural rules into a 
<br>
computer - if you can share them between humans, surely they're explicit 
<br>
enough to program...  What you'll find, after your AI project fails, is 
<br>
that your database of cultural knowledge consists of rules for how to 
<br>
pull levers on a complex machine you don't understand.  If you don't 
<br>
have the complex machine, the lever-pulling rules are useless.  If you 
<br>
don't believe me, just try to build a scientist using your declarative 
<br>
knowledge of how to be a good scientist.  It's harder than it looks.
<br>
<p>We ain't got strong recursivity.
<br>
<p><em> &gt; This argument seems to me to need a whole lot of elaboration and
</em><br>
<em> &gt; clarification to be persuasive, if it is to go beyond the mere
</em><br>
<em> &gt; logical possibility of rapid self-improvement.
</em><br>
<p>The game here is follow-the-work-of-optimization, which is similar to 
<br>
follow-the-entropy in thermodynamics or follow-the-evidence in 
<br>
probability theory.
<br>
<p>I can't do an analytic calculation of the RSI curve.  So why do I expect 
<br>
it to be &quot;fast&quot; as humans measure quickness?  Largely, it is an 
<br>
(admittedly imprecise) perception of lots of low-hanging fruit (with 
<br>
clear, obvious reasons why evolution or human engineering has not 
<br>
already plucked those fruit).  The most blatant case is the opportunity 
<br>
for fast serial speeds, and the second most blatant is the ability to 
<br>
absorb vast amounts of new hardware, but there's software issues too. 
<br>
Our &quot;fast and frugal&quot; heuristics are impressive for doing so much with 
<br>
so little, but humans not noticing the direction of correlations smaller 
<br>
than .6 probably throws away a *lot* of information.
<br>
<p>The intuition of fast takeoff comes from realizing just *how much* room 
<br>
there is for improvement.  As in, orders and orders of magnitude.  In 
<br>
the case of hardware, this is readily visible; software is harder to 
<br>
understand and therefore there is a publication bias against it, but 
<br>
there is no reason in principle to expect evolved software to be closer 
<br>
to optimality than evolved hardware.  What I'm seeing (albeit 
<br>
imprecisely) is that human software is, like the hardware, orders and 
<br>
orders of magnitude short of optimality.  Think of it as an anthropic 
<br>
argument:  The software we use for general intelligence is the smallest 
<br>
possible incremental modification of a chimpanzee that lets the chimp 
<br>
build a computer, because if there were any way to do it with less, we'd 
<br>
be having this conversation about RSI at that level of intelligence instead.
<br>
<p>Admittedly, this intuition is hard to convey.  If only there were some 
<br>
way of transferring procedural skills and intuitions!  Alas, we don't. 
<br>
Looks like we humans have a lot of room for improvement!
<br>
<p><em> &gt; So a modest advantage for the AI's
</em><br>
<em> &gt; internal sharing would not be enough - the advantage would have to be
</em><br>
<em> &gt; enormous.
</em><br>
<p>I think it will be.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15272.html">Jef Allbright: "Re: [extropy-chat] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Previous message:</strong> <a href="15270.html">George Dvorsky: "Re: Fwd: [ai-philosophy] Robotic evolution and ethics"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15272.html">Jef Allbright: "Re: [extropy-chat] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15272.html">Jef Allbright: "Re: [extropy-chat] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15278.html">Keith Henson: "Human motivations was Two draft papers:"</a>
<li><strong>Reply:</strong> <a href="15287.html">Chris Capel: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15271">[ date ]</a>
<a href="index.html#15271">[ thread ]</a>
<a href="subject.html#15271">[ subject ]</a>
<a href="author.html#15271">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
