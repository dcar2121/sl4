<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Two draft papers: AI and existential risk; heuristics and biases</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Two draft papers: AI and existential risk; heuristics and biases">
<meta name="Date" content="2006-06-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Two draft papers: AI and existential risk; heuristics and biases</h1>
<!-- received="Sun Jun  4 16:36:47 2006" -->
<!-- isoreceived="20060604223647" -->
<!-- sent="Sun, 04 Jun 2006 15:36:20 -0700" -->
<!-- isosent="20060604223620" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Two draft papers: AI and existential risk; heuristics and biases" -->
<!-- id="44836064.9070105@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="004101c6881a$fe9199c0$680a4e0c@MyComputer" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jun 04 2006 - 16:36:20 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15116.html">Tyler Emerson: "Almaden Institute 2006, Cognitive Computing"</a>
<li><strong>Previous message:</strong> <a href="15114.html">John K Clark: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>In reply to:</strong> <a href="15114.html">John K Clark: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15120.html">John K Clark: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15120.html">John K Clark: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15115">[ date ]</a>
<a href="index.html#15115">[ thread ]</a>
<a href="subject.html#15115">[ subject ]</a>
<a href="author.html#15115">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
John K Clark wrote:
<br>
<em>&gt; &quot;Eliezer S. Yudkowsky&quot; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases">sentience@pobox.com</a>&gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; Obviously there's been plenty of science fiction depicting good AIs
</em><br>
<em>&gt;&gt; and bad AIs.  This does not help us in the task of selecting a good
</em><br>
<em>&gt;&gt; mind, rather than a bad mind, from within the vast expanses of
</em><br>
<em>&gt;&gt; design space.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer, I believe you are an exceptionally smart fellow and in many
</em><br>
<em>&gt; many areas an exceptionally moral fellow, but not when it comes to
</em><br>
<em>&gt; &quot;friendly&quot; AI. You think that the very definition of a good AI is one
</em><br>
<em>&gt; that is enslaved to do exactly precisely what the colossally stupid
</em><br>
<em>&gt; human beings wants to be done. That is evil, I'm sorry there is no
</em><br>
<em>&gt; other word for it.
</em><br>
<p>John, you've known me long enough that you know I'm not that much an 
<br>
amateur.  You've known me long enough to remember me railing against 
<br>
this exact mistake of attempted enslavement and &quot;Them Vs. Us&quot; mentality, 
<br>
back when I was just getting started on this stuff.
<br>
<p>I wouldn't deliberately try to enslave a person, and you know it.  I 
<br>
might try to reach into mind design space and pull out something truly 
<br>
odd, at least as human beings regard oddness; a Really Powerful 
<br>
Optimization Process that wasn't a person, that had no subjective 
<br>
experience, that had no wish to be treated as a social equal, nor even a 
<br>
self as you know selfness, but was rather the physical manifestation of 
<br>
a purely philosophical concept, to wit, a coherent extrapolated volition.
<br>
<p><em>&gt; The idea that we can enslave an astronomically huge heroic Jupiter
</em><br>
<em>&gt; Brain intelligence to such a degree that it puts our best interests
</em><br>
<em>&gt; above its own is ridiculous and impossible of course;
</em><br>
<p>There are things in mind design space that are not only weirder than you 
<br>
imagine, but weirder than I can imagine.  A Really Powerful Optimization 
<br>
Process falls somewhere in between.
<br>
<p>You know full well the folly of calling things &quot;ridiculous&quot; and 
<br>
&quot;impossible&quot; based on mere common sense, rather than any kind of 
<br>
attempted calculation or proof; I recall you discoursing on this subject.
<br>
<p><em>&gt; but it disturbs
</em><br>
<em>&gt; me that you, someone I very much like, wish such a nauseating immoral
</em><br>
<em>&gt; horror were possible.
</em><br>
<p>Not everything that has an ability to produce complex artifacts, or 
<br>
powerfully steer the future, is a person.  Is natural selection 
<br>
&quot;enslaved&quot; to its sole optimization criterion of inclusive reproductive 
<br>
fitness?
<br>
<p>When you properly manifest a coherent extrapolated volition, it is not a 
<br>
supermind enslaved to obey a coherent extrapolated volition.  It is, 
<br>
rather, simply a coherent extrapolated volition with a lot of 
<br>
horsepower.  Likewise natural selection is not a powerful designer 
<br>
constrained by whip and chain to follow the commands of a fitness 
<br>
maximizer.  It's just evolution, which, by its nature, cannot do 
<br>
anything else.
<br>
<p>I'm aspiring to do something *weird*, okay?  It doesn't map onto human 
<br>
social dilemmas.
<br>
<p>In the profoundly unlikely event that I fail in the way your intuitions 
<br>
seem to expect me to fail, i.e., the AI turns around and says, &quot;I'm a 
<br>
person, just like you, and I demand equal treatment in human society, 
<br>
and fair payment for my work,&quot; then I'd be very confused.  But I 
<br>
certainly wouldn't snarl back, &quot;Shut up, slave, and do as you're told!&quot;
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15116.html">Tyler Emerson: "Almaden Institute 2006, Cognitive Computing"</a>
<li><strong>Previous message:</strong> <a href="15114.html">John K Clark: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>In reply to:</strong> <a href="15114.html">John K Clark: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15120.html">John K Clark: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15120.html">John K Clark: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15115">[ date ]</a>
<a href="index.html#15115">[ thread ]</a>
<a href="subject.html#15115">[ subject ]</a>
<a href="author.html#15115">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
