<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [agi] Two draft papers: AI and existential risk; heuristics and biases</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="Re: [agi] Two draft papers: AI and existential risk; heuristics and biases">
<meta name="Date" content="2006-06-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [agi] Two draft papers: AI and existential risk; heuristics and biases</h1>
<!-- received="Tue Jun  6 09:11:40 2006" -->
<!-- isoreceived="20060606151140" -->
<!-- sent="Tue, 6 Jun 2006 08:09:58 -0700" -->
<!-- isosent="20060606150958" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="Re: [agi] Two draft papers: AI and existential risk; heuristics and biases" -->
<!-- id="638d4e150606060809qdbb0840t525e5acc174960bb@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="44830B56.3080003@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=Re:%20[agi]%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Tue Jun 06 2006 - 09:09:58 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15151.html">John K Clark: "Re: Let's resolve it with a thought experiment"</a>
<li><strong>Previous message:</strong> <a href="15149.html">Ricardo Barreira: "Re: Let's resolve it with a thought experiment"</a>
<li><strong>In reply to:</strong> <a href="15102.html">Eliezer S. Yudkowsky: "Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15152.html">Robin Lee Powell: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15152.html">Robin Lee Powell: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Maybe reply:</strong> <a href="15170.html">Keith Henson: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15173.html">Eliezer S. Yudkowsky: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15174.html">Eliezer S. Yudkowsky: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15150">[ date ]</a>
<a href="index.html#15150">[ thread ]</a>
<a href="subject.html#15150">[ subject ]</a>
<a href="author.html#15150">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi,
<br>
<p><em>&gt; The chapters are:
</em><br>
<em>&gt;
</em><br>
<em>&gt; _Cognitive biases potentially affecting judgment of global risks_
</em><br>
<em>&gt;    <a href="http://intelligence.org/Biases.pdf">http://intelligence.org/Biases.pdf</a>
</em><br>
...
<br>
<em>&gt; _Artificial Intelligence and Global Risk_
</em><br>
<em>&gt;    <a href="http://intelligence.org/AIRisk.pdf">http://intelligence.org/AIRisk.pdf</a>
</em><br>
<em>&gt; The new standard introductory material on Friendly AI.  Any links to
</em><br>
<em>&gt; _Creating Friendly AI_ should be redirected here.
</em><br>
<em>&gt;
</em><br>
<em>&gt; --
</em><br>
<em>&gt; Eliezer S. Yudkowsky
</em><br>
<p>I find these to be excellent papers, and will recommend others to read them.
<br>
<p>However, in comparison to &quot;Creating a Friendly AI&quot; (CFAI), I must note
<br>
that the ambition of what's attempted in &quot;Artificial Intelligence and
<br>
Global Risk&quot; (AIGR) is greatly reduced.
<br>
<p>CFAI tried, and ultimately didn't succeed, to articulate an approach
<br>
to solving the problem of Friendly AI.  Or at least, that is the
<br>
impression it made on me....
<br>
<p>On the other hand, AIGR basically just outlines the problem of
<br>
Friendly AI and explains why it's important and why it's hard.
<br>
<p>In this sense, it seems to be a retreat....
<br>
<p>I suppose the subtext is that your attempts to take the intuitions
<br>
underlying CFAI and turn them into a more rigorous and defensible
<br>
theory did not succeed.
<br>
<p>I also note that your Coherent Extrapolated Volition ideas were not
<br>
focused on in AIGR, which I think is corrrect because I consider CEV a
<br>
fascinating science-fictional speculation without much likelihood of
<br>
ever being practically relevant.
<br>
<p>I agree with you that taking a more rigorous mathematical approach is
<br>
going to be the way -- if any -- to a theory of FAI.  However, I am
<br>
more optimistic that this approach will lead to a theory of FAI
<br>
**assuming monstrously great computational resources** than to a
<br>
theory of pragmatic FAI.  This would be expected since thanks to
<br>
Schmidhuber, Hutter and crew we now have the beginnings of a theory of
<br>
AGI itself assuming monstrously great computational resources, but
<br>
nothing approaching a theory of AGI assuming realistic computational
<br>
resources...
<br>
<p>It would seem to me that FIRST we should try to create a theoretical
<br>
framework useful for analyzing and describing AGIs that operate with
<br>
realistic computational resources.  Then Friendly AI should be
<br>
approached, theoretically, within this framework.
<br>
<p>I can see the viability of also proceeding in a more specialized way,
<br>
and trying to get a theory of FAI under limited resources in the
<br>
absence of an understanding of other sorts of AGIs under limited
<br>
resources.  But my intuition is that the best way to approach &quot;FAI
<br>
under limited resources&quot; is to first get an understanding of &quot;AGI
<br>
under limited resources.&quot;
<br>
<p>This brings us back to my feeling that some experimentation with AGI
<br>
systems is going to be necessary before FAI can be understood
<br>
reasonably well on a theoretical level.  Basically, in my view, one
<br>
way these things may unfold is
<br>
<p>* Experimentation with simplistic AGI systems ... leads to
<br>
* Theoretical understanding of AGI under limited resources ... which leads to...
<br>
* The capability of theoretically understanding FAI ... which leads to ...
<br>
* Building FAI
<br>
<p>Now, this building of FAI *may* take the form of creating a whole new
<br>
AGI architecture from scratch, *or* it may take the form of minorly
<br>
modifying an existing AGI ... or it may be understood why some
<br>
existing AGI design is adequate and there is not really any
<br>
Friendliness problem with it.  We don't know which of these
<br>
eventualities will occur because we don't have the theory of FAI
<br>
yet...
<br>
<p>Your excellent article AIGR, in my view, does not do a good job of
<br>
arguing against this sort of perspective that I'm advocating here.  I
<br>
understand that this is not its job, though: it is mostly devoted to
<br>
making more basic points, which are not sufficiently widely
<br>
appreciated and with which I mainly agree enthusiastically.
<br>
<p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15151.html">John K Clark: "Re: Let's resolve it with a thought experiment"</a>
<li><strong>Previous message:</strong> <a href="15149.html">Ricardo Barreira: "Re: Let's resolve it with a thought experiment"</a>
<li><strong>In reply to:</strong> <a href="15102.html">Eliezer S. Yudkowsky: "Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15152.html">Robin Lee Powell: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15152.html">Robin Lee Powell: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Maybe reply:</strong> <a href="15170.html">Keith Henson: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15173.html">Eliezer S. Yudkowsky: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15174.html">Eliezer S. Yudkowsky: "Re: [agi] Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15150">[ date ]</a>
<a href="index.html#15150">[ thread ]</a>
<a href="subject.html#15150">[ subject ]</a>
<a href="author.html#15150">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
