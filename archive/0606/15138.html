<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Two draft papers: AI and existential risk; heuristics and biases</title>
<meta name="Author" content="Bill Hibbard (test@demedici.ssec.wisc.edu)">
<meta name="Subject" content="Re: Two draft papers: AI and existential risk; heuristics and biases">
<meta name="Date" content="2006-06-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Two draft papers: AI and existential risk; heuristics and biases</h1>
<!-- received="Mon Jun  5 12:10:01 2006" -->
<!-- isoreceived="20060605181001" -->
<!-- sent="Mon, 5 Jun 2006 13:01:05 -0500 (CDT)" -->
<!-- isosent="20060605180105" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@demedici.ssec.wisc.edu" -->
<!-- subject="Re: Two draft papers: AI and existential risk; heuristics and biases" -->
<!-- id="Pine.GSO.4.44.0606051258010.16849-100000@demedici.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="Two draft papers: AI and existential risk; heuristics and biases" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20Two%20draft%20papers:%20AI%20and%20existential%20risk;%20heuristics%20and%20biases"><em>test@demedici.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Mon Jun 05 2006 - 12:01:05 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15139.html">John K Clark: "Re: Let's resolve it with a thought experiment"</a>
<li><strong>Previous message:</strong> <a href="15137.html">Keith Henson: "Self improvement example with humans in the loop"</a>
<li><strong>Maybe in reply to:</strong> <a href="15102.html">Eliezer S. Yudkowsky: "Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15161.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15161.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15138">[ date ]</a>
<a href="index.html#15138">[ thread ]</a>
<a href="subject.html#15138">[ subject ]</a>
<a href="author.html#15138">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer,
<br>
<p><em>&gt; These are drafts of my chapters for Nick Bostrom's forthcoming edited
</em><br>
<em>&gt; volume _Global Catastrophic Risks_.  I may not have much time for
</em><br>
<em>&gt; further editing, but if anyone discovers any gross mistakes, then
</em><br>
<em>&gt; there's still time for me to submit changes.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The chapters are:
</em><br>
<em>&gt;  . . .
</em><br>
<em>&gt; _Artificial Intelligence and Global Risk_
</em><br>
<em>&gt;    <a href="http://intelligence.org/AIRisk.pdf">http://intelligence.org/AIRisk.pdf</a>
</em><br>
<em>&gt; The new standard introductory material on Friendly AI.  Any links to
</em><br>
<em>&gt; _Creating Friendly AI_ should be redirected here.
</em><br>
<p>In Section 6.2 you quote my ideas written in 2001 for
<br>
hard-wiring recognition of expressions of human happiness
<br>
as values for super-intelligent machines. I have three
<br>
problems with your critique:
<br>
<p>1. Immediately after my quote you discuss problems with
<br>
neural network experiments by the US Army. But I never said
<br>
hard-wired learning of recognition of expressions of human
<br>
happiness should be done using neural networks like those
<br>
used by the army. You are conflating my idea with another,
<br>
and then explaining how the other failed.
<br>
<p>2. In your section 6.2 you write:
<br>
<p>&nbsp;&nbsp;If an AI &quot;hard-wired&quot; to such code possessed the power - and
<br>
&nbsp;&nbsp;[Hibbard, B. 2001. Super-intelligent machines. ACM SIGGRAPH
<br>
&nbsp;&nbsp;Computer Graphics, 35(1).] spoke of superintelligence - would
<br>
&nbsp;&nbsp;the galaxy end up tiled with tiny molecular pictures of
<br>
&nbsp;&nbsp;smiley-faces?
<br>
<p>When it is feasible to build a super-intelligence, it will
<br>
be feasible to build hard-wired recognition of &quot;human facial
<br>
expressions, human voices and human body language&quot; (to use
<br>
the words of mine that you quote) that exceed the recognition
<br>
accuracy of current humans such as you and me, and will
<br>
certainly not be fooled by &quot;tiny molecular pictures of
<br>
smiley-faces.&quot; You should not assume such a poor
<br>
implementation of my idea that it cannot make
<br>
discriminations that are trivial to current humans.
<br>
<p>3. I have moved beyond my idea for hard-wired recognition of
<br>
expressions of human emotions, and you should critique my
<br>
recent ideas where they supercede my earlier ideas. In my
<br>
2004 paper:
<br>
<p>&nbsp;&nbsp;Reinforcement Learning as a Context for Integrating AI Research,
<br>
&nbsp;&nbsp;Bill Hibbard, 2004 AAAI Fall Symposium on Achieving Human-Level
<br>
&nbsp;&nbsp;Intelligence through Integrated Systems and Research
<br>
&nbsp;&nbsp;<a href="http://www.ssec.wisc.edu/~billh/g/FS104HibbardB.pdf">http://www.ssec.wisc.edu/~billh/g/FS104HibbardB.pdf</a>
<br>
<p>I say:
<br>
<p>&nbsp;&nbsp;Valuing human happiness requires abilities to recognize
<br>
&nbsp;&nbsp;humans and to recognize their happiness and unhappiness.
<br>
&nbsp;&nbsp;Static versions of these abilities could be created by
<br>
&nbsp;&nbsp;supervised learning. But given the changing nature of our
<br>
&nbsp;&nbsp;world, especially under the influence of machine
<br>
&nbsp;&nbsp;intelligence, it would be safer to make these abilities
<br>
&nbsp;&nbsp;dynamic. This suggests a design of interacting learning
<br>
&nbsp;&nbsp;processes. One set of processes would learn to recognize
<br>
&nbsp;&nbsp;humans and their happiness, reinforced by agreement from
<br>
&nbsp;&nbsp;the currently recognized set of humans. Another set of
<br>
&nbsp;&nbsp;processes would learn external behaviors, reinforced by
<br>
&nbsp;&nbsp;human happiness according to the recognition criteria
<br>
&nbsp;&nbsp;learned by the first set of processes. This is analogous
<br>
&nbsp;&nbsp;to humans, whose reinforcement values depend on
<br>
&nbsp;&nbsp;expressions of other humans, where the recognition of
<br>
&nbsp;&nbsp;those humans and their expressions is continuously
<br>
&nbsp;&nbsp;learned and updated.
<br>
<p>And I further clarify and update my ideas in a 2005
<br>
on-line paper:
<br>
<p>&nbsp;&nbsp;The Ethics and Politics of Super-Intelligent Machines
<br>
&nbsp;&nbsp;<a href="http://www.ssec.wisc.edu/~billh/g/SI_ethics_politics.doc">http://www.ssec.wisc.edu/~billh/g/SI_ethics_politics.doc</a>
<br>
<p><p>Please adjust your discussion of my ideas to:
<br>
<p>&nbsp;&nbsp;1. Not conflate my ideas with others.
<br>
&nbsp;&nbsp;2. Not assume a poor implementation of my ideas.
<br>
&nbsp;&nbsp;3. Not critique my old ideas when they have been
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;replaced by newer ideas in my publications.
<br>
<p>Thank you,
<br>
Bill
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15139.html">John K Clark: "Re: Let's resolve it with a thought experiment"</a>
<li><strong>Previous message:</strong> <a href="15137.html">Keith Henson: "Self improvement example with humans in the loop"</a>
<li><strong>Maybe in reply to:</strong> <a href="15102.html">Eliezer S. Yudkowsky: "Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15161.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<li><strong>Reply:</strong> <a href="15161.html">Eliezer S. Yudkowsky: "Re: Two draft papers: AI and existential risk; heuristics and biases"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15138">[ date ]</a>
<a href="index.html#15138">[ thread ]</a>
<a href="subject.html#15138">[ subject ]</a>
<a href="author.html#15138">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
