<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Loosemore's Collected Writings on SL4 - Part 4</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Loosemore's Collected Writings on SL4 - Part 4">
<meta name="Date" content="2006-08-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Loosemore's Collected Writings on SL4 - Part 4</h1>
<!-- received="Sat Aug 26 20:39:21 2006" -->
<!-- isoreceived="20060827023921" -->
<!-- sent="Sat, 26 Aug 2006 22:36:14 -0400" -->
<!-- isosent="20060827023614" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Loosemore's Collected Writings on SL4 - Part 4" -->
<!-- id="44F1051E.2060906@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Loosemore's%20Collected%20Writings%20on%20SL4%20-%20Part%204"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Sat Aug 26 2006 - 20:36:14 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15817.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 7"</a>
<li><strong>Previous message:</strong> <a href="15815.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 5"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15816">[ date ]</a>
<a href="index.html#15816">[ thread ]</a>
<a href="subject.html#15816">[ subject ]</a>
<a href="author.html#15816">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
[begin part 4]
<br>
<p>************************************************************
<br>
*                                                          *
<br>
* The Complex Systems Critique (Again)                     *
<br>
*                                                          *
<br>
************************************************************
<br>
Ben Goertzel wrote:
<br>
<p><em>&gt; Richard,
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think you have made a reasonably convincing argument why
</em><br>
<em>&gt; AGI should proceed via allowing a proto-AGI system to
</em><br>
<em>&gt; engage with a world via sensors and actuators, and
</em><br>
<em>&gt; construct its own symbols to represent the world
</em><br>
<em>&gt; around it.  I agree that these symbols will generally
</em><br>
<em>&gt; not be simple, and also that sophisticated learning
</em><br>
<em>&gt; mechanisms will be required to learn them.
</em><br>
<em>&gt;
</em><br>
<em>&gt; What is not clear to me is why the &quot;complexity&quot; of
</em><br>
<em>&gt; these symbols and learning  mechanisms necessarily
</em><br>
<em>&gt; has to entail &quot;complexity&quot; in the sense of
</em><br>
<em>&gt; complex systems theory (as opposed to just
</em><br>
<em>&gt; &quot;complexity&quot; in the sense of complicatedness
</em><br>
<em>&gt; and sophistication).
</em><br>
<p>Your question makes me fear that I muddied the waters when I talked
<br>
about the complicatedness of symbols.  And I was trying so hard, too!
<br>
<p>The answer to your question was in the text, but alas it may have gotten
<br>
buried. When I talked about the &quot;complicatedness&quot; of the symbols, I was
<br>
summarizing something that I then (later on) tried to spell out in more
<br>
detail: I was postulating that when we try to get serious amounts of
<br>
high-level learning into an AGI (analogy-making and so on), we are
<br>
forced to put in precisely the kinds of tangled, reflexive,
<br>
self-modifying mechanisms that leads to complexity in the system as a
<br>
whole, and in complicatedness in the symbols.
<br>
<p>Even without any other arguments, I was claiming, we should be deeply
<br>
worried that AGI programmers tend to shy away from trying to build those
<br>
very high-level learning mechanisms. Is it fair of me to say this? Do
<br>
they really shy away from them? I think they do. Many AGI folks talk
<br>
about such stuff as if it is next on the list after they get the basic
<br>
mechanisms sorted out - but it might also be that the real reason people
<br>
avoid them is that nobody has much idea how to build them *without*
<br>
straying into the domain of complex, self-modifying, tangled, recursive
<br>
(etc) systems. Let me put the same point, but coming from the other
<br>
direction: do you see any research groups throwing themselves full-tilt
<br>
into the problem of understanding those high-level learning (aka
<br>
concept-building or structure-finding) mechanisms? Oh boy, yes! For just
<br>
one example, look at the FARG group at Indiana U
<br>
(<a href="http://www.cogsci.indiana.edu/index.html">http://www.cogsci.indiana.edu/index.html</a>). But these folks take the
<br>
complex-systems approach. They eat, drink and breathe complexity.
<br>
<p>So it is not that the complicatedness of symbols means anything by
<br>
itself (I apologize for misleading you there) - a symbol, after all, is
<br>
a local mechanism, and the point of a complex system is that the
<br>
complexity is in the system as a whole, not in the local units. No, what
<br>
I meant was: the symbols have to be complicated precisely because we
<br>
need to make them develop by themselves using powerful (tangled,
<br>
complex) learning mechanisms. [The complicatedness of the symbols raises
<br>
a slightly different issue that I started to discuss, but for clarity I
<br>
will leave it aside here and come back to it if you wish].
<br>
<p>I submit that most AGI people assume that they are going to be able to
<br>
crack the learning problem later, *without* having to resort to tangled
<br>
complexity. They believe that they will be able to invent all the
<br>
learning mechanisms required in an AGI without having to give those
<br>
learning mechanisms the power to trasnform the system as a whole into a
<br>
complex system.
<br>
<p>I further submit that they believe the format for the symbols that they
<br>
are using now (relatively uncomplicated and interpretable, in my
<br>
terminology) will not be substantially affected by the later
<br>
introduction of those learning mechanisms. When I talked about the
<br>
symbols becoming more complicated, I was referring to this assumption,
<br>
saying that I believe that the later introduction of proper learning
<br>
mechanisms will actually affect the format of the symbols, and that the
<br>
change may be so huge that we may discover that the only way to get the
<br>
symbols to develop by themselves (supplied only with real world I/O and
<br>
no programmer intervention) is to to give them so much freedom to
<br>
develop that all the apparatus we put into in the symbols, that we
<br>
thought was so important, turns out to be redundant.
<br>
<p>So all of this is about observing a process within the AI community.
<br>
Specifically, these observations. (a) I see people (over the course of
<br>
at least four decades now) concentrating on the mechanisms-of-thought in
<br>
non-grounded systems, and postponing the problem of building the kind of
<br>
powerful learning mechanisms that could generate the symbols that are
<br>
used in those mechanisms-of-thought. (b) I see a few people embracing
<br>
the problem of those powerful learning mechanisms, but those people take
<br>
a complex systems approach because all the indications are that the kind
<br>
of reflexive, self-modifying characteristics needed in such learning
<br>
mechanisms will lead to complex systems. Now, why do the latter group go
<br>
straight for complex systems? We need to be careful not to trivialise
<br>
their reasons for doing so: they don't do it just because it's fun; they
<br>
don't do it because they don't know any better; they don't do it because
<br>
they are mathematically naive wimps who have no faith in the power of
<br>
mathematics to grow until it can describe things that people previously
<br>
dismissed as to difficult to describe .... they do it because they have
<br>
an extremely broad range of knowledge, have come at the problem from a
<br>
number of angles, and have decided that there is a very profound message
<br>
coming from all the studies that have been done on different sorts of
<br>
complex system. And the message, as far as they are concerned, is that
<br>
*if* you are going to build a mechanism that captures what appears to be
<br>
an extremely reflexive, self-modifying and tangled ability as the kinds
<br>
of learning and concept building that are important to them, *then* you
<br>
had jolly well better get used to the idea that the mechanism is going
<br>
to be complex, because in the thousands upon thousands of other examples
<br>
of systems with that kind of tangledness, we always observe an element
<br>
of complexity.
<br>
<p>So, from these two observations, I take away this conclusion. The people
<br>
who insist that we will be able to build powerful learning mechanisms in
<br>
an AGI *without* recourse to complexity, are precisely those people who
<br>
have not tried to build such mechanisms. They offer a firm conviction
<br>
that they will be able to do so, but they can offer nothing except their
<br>
blind faith in the future. [And where they do attempt to build learning
<br>
mechanisms, they only try relatively simple kinds of learning (concept
<br>
building) and they have never demonstrated that their mechanisms are
<br>
powerful enough to ground a broad-based intelligence]. The only people
<br>
who have ventured into this domain have accepted the evidence that
<br>
complexity is unavoidable, and they give reasons why they think so.
<br>
<p>The standard response to this argument (at least from some quarters in
<br>
this list), is that I have not given a demonstration why an AGI *cannot*
<br>
be built without complexity, or a demonstration of why complexity *must*
<br>
be necessary. This is a completely nonsensical demand: a rigorous proof
<br>
or demonstration is not possible: that is the whole point of my
<br>
argument! If I could give a rigorous mathematical or logical proof why
<br>
you cannot build an AGI without complexity, the very rigor of that proof
<br>
would invalidate my argument!
<br>
<p>I most certainly have given a demonstration: it is an empirical one.
<br>
Look at all the examples of learning systems that work; look at the way
<br>
the non-complex AGI researchers run away from powerful learning
<br>
mechanisms and never demonstrate any convincing reasons to believe their
<br>
systems can be grounded; look at all the evidence from natural systems
<br>
that are intelligent, but which do not use crystalline, non-complex
<br>
thinking and learning mechanisms; look at all the systems which have
<br>
large numbers of interacting, self-modifying, adaptive components that
<br>
interact with the world (not just intelligent systems, but all the
<br>
others) and ask yourself why it is that we cannot find any examples
<br>
where someone could start by observing the global behavior of the system
<br>
and then reason back to the local mechanisms that must have given rise
<br>
to that behavior.
<br>
<p>And at the end of all this observing, ask yourself what reason an AGI
<br>
researcher has to denigrate the human mind as a lousy design (even
<br>
thoughthey don't understand the design!) and say that they can do it
<br>
better without introducing complexity, without offering any examples of
<br>
working (grounded) systems to back up their claim, and without offering
<br>
any mathematical proofs or demonstrations that their approach will one
<br>
day work, when they get the learning and grounding mechanisms fully
<br>
worked out.
<br>
<p>The boot, I submit, is on the other foot. The rest of the community is
<br>
asking the non-complex AGI folks [apologies for the awkward term: I am
<br>
not sure what to call people who eschew complexity, except perhaps the
<br>
Old Guard] why *we* should go along with what looks like their blind
<br>
faith in being able to build a fully capable, grounded AGI without
<br>
resorting to complexity.
<br>
<p>*****
<br>
<p>I have concentrated on one aspect of the learning mechanisms (the
<br>
expected tangledness, if you like) because this is the most obvious
<br>
thing that would lead to complexity. However, this is not the only plank
<br>
of the argument. I have brought up some of these other issues elsewhere,
<br>
but it might be better for me to organize them more systematically,
<br>
rather than throw them into the pot right now.
<br>
<p>In closing, let me say that I look very negative when presenting this
<br>
argument, even though I actually do have concrete suggestions for what
<br>
we should do instead. Some people have jumped to false conclusions about
<br>
what I would recommend us doing, if the above were true: the fact is, I
<br>
have barely even mentioned what I think we should be doing. Right now,
<br>
my goal is to suggest that here we have an issue of truly enormous
<br>
importance, and that we should first of all accept that it really is an
<br>
issue, then go on to talk about what can be done about it. But I want to
<br>
get to first base first and get people to agree that there is an issue.
<br>
<p>Richard Loosemore.
<br>
<p>[end part 4]
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15817.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 7"</a>
<li><strong>Previous message:</strong> <a href="15815.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 5"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15816">[ date ]</a>
<a href="index.html#15816">[ thread ]</a>
<a href="subject.html#15816">[ subject ]</a>
<a href="author.html#15816">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:57 MDT
</em></small></p>
</body>
</html>
