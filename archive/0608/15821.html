<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Loosemore's Collected Writings on SL4 - Part 8</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Loosemore's Collected Writings on SL4 - Part 8">
<meta name="Date" content="2006-08-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Loosemore's Collected Writings on SL4 - Part 8</h1>
<!-- received="Sat Aug 26 20:41:38 2006" -->
<!-- isoreceived="20060827024138" -->
<!-- sent="Sat, 26 Aug 2006 22:38:33 -0400" -->
<!-- isosent="20060827023833" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Loosemore's Collected Writings on SL4 - Part 8" -->
<!-- id="44F105A9.4050001@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Loosemore's%20Collected%20Writings%20on%20SL4%20-%20Part%208"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Sat Aug 26 2006 - 20:38:33 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15822.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 11 - end"</a>
<li><strong>Previous message:</strong> <a href="15820.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 10"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15821">[ date ]</a>
<a href="index.html#15821">[ thread ]</a>
<a href="subject.html#15821">[ subject ]</a>
<a href="author.html#15821">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
[begin part 8]
<br>
<p>************************************************************
<br>
*                                                          *
<br>
* The Complex Systems Critique (Again)                     *
<br>
*                                                          *
<br>
************************************************************
<br>
<p>Another way to say what I have been trying to say:
<br>
<p>The question is: how does the *design* of a cognitive system's learning
<br>
mechanisms interact with the *design* of its &quot;thinking and reasoning and
<br>
knowledge representation&quot; mechanism?
<br>
<p>Can you, for example, sort out the thinking/reasoning/knowledge
<br>
representation mechanism first, then go back and find some good learning
<br>
mechanisms that will fill that mechanism with the right sort of data,
<br>
using only real world interaction and (virtually) no hand-holding from
<br>
the experimenter?
<br>
<p>Or is it the case that you can pick a thinking/reasoning/knowledge
<br>
representation mechanism of your choice, and then discover to your
<br>
horror that there is not ever going to be a learning mechanism that
<br>
feeds that mechanism properly?  Could it be that the two are so
<br>
interrelated that a wrong choice of one precludes ANY choice of the other?
<br>
<p>Now, complex adaptive systems theory would seem to indicate that if the
<br>
learning mechanisms are powerful enough to make the system Class IV
<br>
(i.e. complex-adaptive), the global behavior of those learning
<br>
mechanisms is going to be disconnected from the local behavior .... you
<br>
can't pick a global behavior first and then pick a local mechanism that
<br>
generates that behavior. That is the disconnect.
<br>
<p>If this were the case with cognitive systems, we would get the situation
<br>
we have now. And one way out would be to build the kind of development
<br>
environement and adopt the kind of research strategy I have talked about.
<br>
<p>Richard Loosemore.
<br>
<p><p><p>************************************************************
<br>
*                                                          *
<br>
* Goals Systems and Semantics of Goal Statements           *
<br>
*                                                          *
<br>
************************************************************
<br>
<p>Michael Vassar wrote:
<br>
<p><em>&gt; Some posters seem to be very seriously unaware of what
</em><br>
<em>&gt; was said in CAFAI, but having read and understood it
</em><br>
<em>&gt; should be a prerequisite to posting here.
</em><br>
<em>&gt; My complaints
</em><br>
<em>&gt; Friendly AIs are explicitly NOT prevented from messing
</em><br>
<em>&gt; with their source-code or with their goal systems.
</em><br>
<em>&gt; However, they act according to decision theory.  ....
</em><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;^^^^^^^^^^^^^^^
<br>
I have to go on record here as saying that I (and others who are poorly
<br>
represented on this list) fundamentally disagree with this statement. I
<br>
would not want readers of these posts to get the idea that this is THE
<br>
universally agreed way to build an artificial intelligence. Moreover,
<br>
many of the recent debates on this list are utterly dependent on the
<br>
assumption that you state above, so to people like me these debates are
<br>
just wheel-spinning built on nonsensical premises.
<br>
<p>Here is why.
<br>
<p>Friendly AIs built on decision theory have goal systems that specify
<br>
their goals: but in what form are the goals represented, and how are
<br>
they interpreted? Here is a nice example of a goal:
<br>
<p>&nbsp;&nbsp;&quot;Put the blue block on top of the red block&quot;
<br>
<p>In a Blocks World, the semantics of this goal - its &quot;meaning&quot; - are not
<br>
at all difficult. All fine and good: standard 1970's-issue artificial
<br>
intelligence, etc.
<br>
<p>But what happens when the goals become more abstract:
<br>
<p>&nbsp;&nbsp;&quot;Maximize the utility function, where the utility function specifies
<br>
that thinking is good&quot;
<br>
<p>I've deliberately chosen a silly UF (thinking is good) because people on
<br>
this list frequently talk as if a goal like that has a meaning that is
<br>
just as transparent as the meaning of &quot;put the blue block on top of the
<br>
red block&quot;. The semantics of &quot;thinking is good&quot; is clearly not trivial,
<br>
and in fact it is by no means obvious that the phrase can be given a
<br>
clear enough semantics to enable it to be used as a sensible input to a
<br>
decision-theory-driven AGI.
<br>
<p>The behavior of an AGI with such a goal would depend crucially on what
<br>
mechanisms it used to interpret the meaning of &quot;thinking is good&quot;. So
<br>
much so, in fact, that it becomes stupid to talk of the system as being
<br>
governed by the decision theory component: it is not, it is governed by
<br>
whatever mechanisms you can cobble together to interpret that vague goal
<br>
statement. What initially looked like the dog's tail (the mechanisms
<br>
that govern the interpretation of goals) starts to wag the dog (the
<br>
decision-theory-based goal engine).
<br>
<p>The standard response to this criticism is that while the semantics are
<br>
not obvious, the whole point of modern AI research is to build systems
<br>
that do rigorously interpret the semantics in some kind of compositional
<br>
way, even in the cases of abstract goals like &quot;thinking is good&quot;. In
<br>
other words, the claim is that I am seeing a fundamental problem where
<br>
others only see a bunch of complex implementation details.
<br>
<p>This is infuriating nonsense: there are many people out there who
<br>
utterly disagree with this position, and who have solid reasons for
<br>
doing so. I am one of them.
<br>
<p>So when you say &quot;Friendly AIs [...] act according to decision theory.&quot;
<br>
you mean &quot;The particular interpretation of how to build a Friendly AI
<br>
that is common on this list, acts according to decision theory.&quot;
<br>
<p>And, as I say, much of the recent discussion about passive AI and goal
<br>
systems is just content-free speculation, from my point of view.
<br>
<p>Richard Loosemore
<br>
<p><p><p>[end part 8]
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15822.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 11 - end"</a>
<li><strong>Previous message:</strong> <a href="15820.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 10"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15821">[ date ]</a>
<a href="index.html#15821">[ thread ]</a>
<a href="subject.html#15821">[ subject ]</a>
<a href="author.html#15821">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:57 MDT
</em></small></p>
</body>
</html>
