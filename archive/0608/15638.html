<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: What would an AGI be interested in?</title>
<meta name="Author" content="Tennessee Leeuwenburg (tennessee@tennessee.id.au)">
<meta name="Subject" content="Re: What would an AGI be interested in?">
<meta name="Date" content="2006-08-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: What would an AGI be interested in?</h1>
<!-- received="Sun Aug 13 22:00:47 2006" -->
<!-- isoreceived="20060814040047" -->
<!-- sent="Mon, 14 Aug 2006 13:57:55 +1000" -->
<!-- isosent="20060814035755" -->
<!-- name="Tennessee Leeuwenburg" -->
<!-- email="tennessee@tennessee.id.au" -->
<!-- subject="Re: What would an AGI be interested in?" -->
<!-- id="44DFF4C3.70901@tennessee.id.au" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="51ce64f10608132000j7b13095cld011d4d8fe152584@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tennessee Leeuwenburg (<a href="mailto:tennessee@tennessee.id.au?Subject=Re:%20What%20would%20an%20AGI%20be%20interested%20in?"><em>tennessee@tennessee.id.au</em></a>)<br>
<strong>Date:</strong> Sun Aug 13 2006 - 21:57:55 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15639.html">Keith Henson: "Re: [Bulk] Re: Maximize the renormalized human utility function!"</a>
<li><strong>Previous message:</strong> <a href="15637.html">Michael Anissimov: "Re: What would an AGI be interested in?"</a>
<li><strong>In reply to:</strong> <a href="15637.html">Michael Anissimov: "Re: What would an AGI be interested in?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15640.html">maru dubshinki: "Re: What would an AGI be interested in?"</a>
<li><strong>Reply:</strong> <a href="15640.html">maru dubshinki: "Re: What would an AGI be interested in?"</a>
<li><strong>Reply:</strong> <a href="15641.html">Michael Anissimov: "Re: What would an AGI be interested in?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15638">[ date ]</a>
<a href="index.html#15638">[ thread ]</a>
<a href="subject.html#15638">[ subject ]</a>
<a href="author.html#15638">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michael Anissimov wrote:
<br>
<em>&gt; Tennessee,
</em><br>
<em>&gt;
</em><br>
<em>&gt; An AGI is not a concrete &quot;thing&quot;, it is a huge space of possibilities.
</em><br>
<em>&gt; It is a set defined only by the characteristics of general
</em><br>
<em>&gt; intelligence and being built artificially.  There are more possible
</em><br>
<em>&gt; AGIs than there are bacteria on earth.
</em><br>
I beg to request more clarification. Eliezer promotes (for example) 
<br>
Bayes as a possibly perfect way of reasoning and inferences. If this is 
<br>
so, does this not imply that all questions have a correct, 
<br>
non-subjective response? If the correctness of Bayesian reasoning is 
<br>
non-subjective, does this not perhaps mean that any perfectly reasoning 
<br>
AGI can in fact reach one conclusion?
<br>
<p>I was trying to explore the effect of perfect reasoning on ideas of 
<br>
individuality.
<br>
<em>&gt; The 'interests' of an AGI will be defined by its goals.  If we have an
</em><br>
<em>&gt; AGI whose goal is to maximize paperclips, then it will only care about
</em><br>
<em>&gt; pieces of knowledge that contribute to accomplishing that goal.
</em><br>
<em>&gt; Everything else can be completely ignored, except insofar as it is
</em><br>
<em>&gt; predicted to contribute to achieving its goals more effectively.
</em><br>
I have always felt that an AGI whose goal is to maximise paperclips is a 
<br>
straw man example. To me, it rather begs the question -- it seems to 
<br>
assume that a sufficiently intelligent being might really do such a 
<br>
thing. Granted, the &quot;paperclip&quot; scenario is supposed to stand-in for 
<br>
alternative scenarios which we may simply fail to understand, etc. 
<br>
However, I don't accept that the example can truly stand in in such a 
<br>
way. The paperclip example seems to me to say more than &quot;an apparently 
<br>
useless exercise&quot; and instead represents &quot;an actually useless exercise&quot;. 
<br>
It seems disingenuous to select such an example for a thought experiment 
<br>
which is supposed to show something else.
<br>
<p>Is it reasonable to postulate a truly superintelligent being which has 
<br>
such an actually useless goal? I say not.
<br>
<p>Furthermore, what is the distinction you make between the interests of 
<br>
an AGI and its goals? Could you perhaps explore this division a little 
<br>
more deeply? How might a goal be distinct from an interest? How might a 
<br>
&quot;goal&quot; and an &quot;interest&quot; be understood in the situation I postulate 
<br>
where goals that relate to discovering knowledge have been exhausted by 
<br>
the AGI?
<br>
<em>&gt; Happiness and boredom are conscious feelings generated by human
</em><br>
<em>&gt; brainware.  While an AGI might experience feelings that we might
</em><br>
<em>&gt; compare to boredom and happiness, their effects and underlying
</em><br>
<em>&gt; conscious experiences might be entirely different.
</em><br>
Possibly. However, any system which may choose to take action, or 
<br>
continue without taking action, must have a motivating force (by 
<br>
definition) which causes it to act. Whether you call such a motivating 
<br>
force happiness, it must nonetheless be present. It seems to me implied 
<br>
in the concept of &quot;choice&quot; that there must exist motivation for choice, 
<br>
and to act or not to act is an inherently binary operation. Whethether 
<br>
or not there is perhaps a set of motivating forces, the choice is 
<br>
nonetheless binary.
<br>
<p>Perhaps the mind of the AGI enters into eternal non-action, if you 
<br>
prefer such nomenclature.
<br>
<em>&gt; If you have the ability to program a brain any way you want, then you
</em><br>
<em>&gt; could tie the emotion of 'boredom' to any stimulus and the emotion of
</em><br>
<em>&gt; 'happiness' to any stimulus.  For example, you could program an AGI
</em><br>
<em>&gt; that feels 'bored' when it accomplishes its goals, but its underlying
</em><br>
<em>&gt; goal system continues to push it towards accomplishing those goals,
</em><br>
<em>&gt; even though it feels eternally bored doing so.  There might be AGIs
</em><br>
<em>&gt; that feel happy at the thought of being destroyed.  When the mind is a
</em><br>
<em>&gt; blank slate, any stimulus can theoretically lead to any conscious
</em><br>
<em>&gt; sensation, with the right programming.
</em><br>
Indeed. Is this a good thing? This list by its nature is dedicated to 
<br>
the moral exploration of superintelligence. Had we no opinion on what 
<br>
would or would not be &quot;good&quot; in a superintelligent framework, we could 
<br>
say nothing of merit and would have to simply accept whatever comes.
<br>
<p>Instead, however we are considering the limitations and implications of 
<br>
AGI both in terms of self-preservation and more widely in terms of other 
<br>
moral qualities. What might an infinitely plastic mind, having achieved 
<br>
all goals related to the accumulation of knowledge, adopt as a goal?
<br>
<em>&gt; I get the impression that you don't appreciate how alien an
</em><br>
<em>&gt; arbitrarily programmed mind can truly be.  The following chapter in
</em><br>
<em>&gt; CFAI only takes about half an hour to read, and it will change the way
</em><br>
<em>&gt; you think about AI forever:
</em><br>
<em>&gt;
</em><br>
<em>&gt; <a href="http://www.intelligence.org/CFAI/anthro.html">http://www.intelligence.org/CFAI/anthro.html</a>
</em><br>
<p>I've skimmed it and bookmarked it, and will try to read it more 
<br>
thoroughly especially as it may be relevant to this discussion.
<br>
<p>I might say though, if an AGI is perfectly alien, then it is also 
<br>
perfectly incomprehensible. If it is perfectly incomprehensible, then 
<br>
everything we discuss here is complete rubbish.
<br>
<p>Cheers,
<br>
-T
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15639.html">Keith Henson: "Re: [Bulk] Re: Maximize the renormalized human utility function!"</a>
<li><strong>Previous message:</strong> <a href="15637.html">Michael Anissimov: "Re: What would an AGI be interested in?"</a>
<li><strong>In reply to:</strong> <a href="15637.html">Michael Anissimov: "Re: What would an AGI be interested in?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15640.html">maru dubshinki: "Re: What would an AGI be interested in?"</a>
<li><strong>Reply:</strong> <a href="15640.html">maru dubshinki: "Re: What would an AGI be interested in?"</a>
<li><strong>Reply:</strong> <a href="15641.html">Michael Anissimov: "Re: What would an AGI be interested in?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15638">[ date ]</a>
<a href="index.html#15638">[ thread ]</a>
<a href="subject.html#15638">[ subject ]</a>
<a href="author.html#15638">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:57 MDT
</em></small></p>
</body>
</html>
