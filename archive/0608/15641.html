<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: What would an AGI be interested in?</title>
<meta name="Author" content="Michael Anissimov (michaelanissimov@gmail.com)">
<meta name="Subject" content="Re: What would an AGI be interested in?">
<meta name="Date" content="2006-08-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: What would an AGI be interested in?</h1>
<!-- received="Mon Aug 14 00:14:22 2006" -->
<!-- isoreceived="20060814061422" -->
<!-- sent="Sun, 13 Aug 2006 23:13:41 -0700" -->
<!-- isosent="20060814061341" -->
<!-- name="Michael Anissimov" -->
<!-- email="michaelanissimov@gmail.com" -->
<!-- subject="Re: What would an AGI be interested in?" -->
<!-- id="51ce64f10608132313i4771045fxbb750eb5722dda7@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="44DFF4C3.70901@tennessee.id.au" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Anissimov (<a href="mailto:michaelanissimov@gmail.com?Subject=Re:%20What%20would%20an%20AGI%20be%20interested%20in?"><em>michaelanissimov@gmail.com</em></a>)<br>
<strong>Date:</strong> Mon Aug 14 2006 - 00:13:41 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15642.html">Michael Anissimov: "Marcus Hutter's 50'000€ Prize for Compressing Human Knowledge"</a>
<li><strong>Previous message:</strong> <a href="15640.html">maru dubshinki: "Re: What would an AGI be interested in?"</a>
<li><strong>In reply to:</strong> <a href="15638.html">Tennessee Leeuwenburg: "Re: What would an AGI be interested in?"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15641">[ date ]</a>
<a href="index.html#15641">[ thread ]</a>
<a href="subject.html#15641">[ subject ]</a>
<a href="author.html#15641">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Tennessee,
<br>
<p><em>&gt; I beg to request more clarification. Eliezer promotes (for example)
</em><br>
<em>&gt; Bayes as a possibly perfect way of reasoning and inferences. If this is
</em><br>
<em>&gt; so, does this not imply that all questions have a correct,
</em><br>
<em>&gt; non-subjective response? If the correctness of Bayesian reasoning is
</em><br>
<em>&gt; non-subjective, does this not perhaps mean that any perfectly reasoning
</em><br>
<em>&gt; AGI can in fact reach one conclusion?
</em><br>
<p>On 8/13/06, Tennessee Leeuwenburg &lt;<a href="mailto:tennessee@tennessee.id.au?Subject=Re:%20What%20would%20an%20AGI%20be%20interested%20in?">tennessee@tennessee.id.au</a>&gt; wrote:
<br>
<em>&gt; I beg to request more clarification. Eliezer promotes (for example)
</em><br>
<em>&gt; Bayes as a possibly perfect way of reasoning and inferences. If this is
</em><br>
<em>&gt; so, does this not imply that all questions have a correct,
</em><br>
<em>&gt; non-subjective response? If the correctness of Bayesian reasoning is
</em><br>
<em>&gt; non-subjective, does this not perhaps mean that any perfectly reasoning
</em><br>
<em>&gt; AGI can in fact reach one conclusion?
</em><br>
<p>Bayesian reasoning tells you how to update your confidence levels in
<br>
various beliefs, given evidence.  Eventually, it allows you to give
<br>
true answers to questions of fact.  But it does not give answers to
<br>
questions that are inherently observer-biased or based on values.  For
<br>
example, when someone says &quot;chocolate ice cream is good&quot;, this means
<br>
that their taste buds relay information perceptions of ice cream
<br>
molecules to the brain, which returns a pleasurable feeling.  When
<br>
they utter the phrase &quot;chocolate ice cream is good&quot;, they are actually
<br>
misinterpreting a subjective fact about their interpretations as an
<br>
objective quality of the ice cream. This is called the Mind Projection
<br>
Fallacy.
<br>
<p>If you speculate and say, &quot;AGIs will do so-and-so&quot;, you are claiming
<br>
that you are capable of visualizing the behavioral outputs of a truly
<br>
massive space of minds.  We can't do this, any more than an amateur
<br>
can predict the next move of a chess grandmaster.  (In fact, it's far
<br>
more difficult.)  We can only say, &quot;an AGI's actions are very likely
<br>
to fall somewhere in category X, if it provably maintains an abstract
<br>
invariant that outputs actions in category X&quot;.  This is how we can
<br>
make statements about the long-term behavior of mathematical
<br>
abstractions like AIXI.
<br>
<p><em>&gt; Is it reasonable to postulate a truly superintelligent being which has
</em><br>
<em>&gt; such an actually useless goal? I say not.
</em><br>
<p>Useless to you, important to it.  Humans like having sex and achieving
<br>
status because our evolutionary circumstances produced a selection
<br>
pressure in favor of those goals.  There is no such thing as an
<br>
inherently meaningful goal, only goals that minds with a particular
<br>
structure happen to like.  To state otherwise is to fall prey to the
<br>
Mind Projection Fallacy.
<br>
<p><em>&gt; Indeed. Is this a good thing? This list by its nature is dedicated to
</em><br>
<em>&gt; the moral exploration of superintelligence. Had we no opinion on what
</em><br>
<em>&gt; would or would not be &quot;good&quot; in a superintelligent framework, we could
</em><br>
<em>&gt; say nothing of merit and would have to simply accept whatever comes.
</em><br>
<p>The inherent flexibility of arbitrarily programmed mindspace is not
<br>
good or bad - it is a fact.  Further explorations of which minds
<br>
humans would experience as good and which minds we would experience as
<br>
bad must first acknowledge this.
<br>
<p>To do constructive work in an intellectual field, you must first
<br>
consult the primary literature.  For Friendly AI, this begins with
<br>
reading CFAI in its entirety.
<br>
<a href="http://en.wikipedia.org/wiki/Friendly_AI">http://en.wikipedia.org/wiki/Friendly_AI</a> has further links, which
<br>
include works by Hibbard and Voss.  Ben Goertzel has also written a
<br>
few papers.  For a short page that begins to pick at the problem, see
<br>
<a href="http://www.intelligence.org/intro/friendly.html">http://www.intelligence.org/intro/friendly.html</a>.
<br>
<p><em>&gt; Instead, however we are considering the limitations and implications of
</em><br>
<em>&gt; AGI both in terms of self-preservation and more widely in terms of other
</em><br>
<em>&gt; moral qualities. What might an infinitely plastic mind, having achieved
</em><br>
<em>&gt; all goals related to the accumulation of knowledge, adopt as a goal?
</em><br>
<p>You're anthropomorphizing.  Humans need to keep making up and persuing
<br>
goals to justify their existence, but this does not apply to all
<br>
minds.  For an AI programmed to acquire knowledge... it would either
<br>
be content to keep going forever, because there are always more bits
<br>
of information to learn, or it would see all further actions as having
<br>
zero differential utility, and stop completely, or achieve some
<br>
equilibrium output.
<br>
<p><em>&gt; I might say though, if an AGI is perfectly alien, then it is also
</em><br>
<em>&gt; perfectly incomprehensible. If it is perfectly incomprehensible, then
</em><br>
<em>&gt; everything we discuss here is complete rubbish.
</em><br>
<p>AGI in general is too large of a space to make many statements about,
<br>
except in very abstract terms.  We can discuss how an AGI programmed
<br>
in a particular way would act, especially as it begins to reach around
<br>
and modify its own source code.  These are questions being asked in a
<br>
rigorous, technical manner by small groups of mathematicians and
<br>
theoretical computer scientists worldwide.  Coming up with real
<br>
answers is a huge challenge.
<br>
<p><pre>
-- 
Michael Anissimov
Lifeboat Foundation      <a href="http://lifeboat.com">http://lifeboat.com</a>
<a href="http://acceleratingfuture.com/michael/blog">http://acceleratingfuture.com/michael/blog</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15642.html">Michael Anissimov: "Marcus Hutter's 50'000€ Prize for Compressing Human Knowledge"</a>
<li><strong>Previous message:</strong> <a href="15640.html">maru dubshinki: "Re: What would an AGI be interested in?"</a>
<li><strong>In reply to:</strong> <a href="15638.html">Tennessee Leeuwenburg: "Re: What would an AGI be interested in?"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15641">[ date ]</a>
<a href="index.html#15641">[ thread ]</a>
<a href="subject.html#15641">[ subject ]</a>
<a href="author.html#15641">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:57 MDT
</em></small></p>
</body>
</html>
