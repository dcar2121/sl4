<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Loosemore's Collected Writings on SL4 - Part 1</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Loosemore's Collected Writings on SL4 - Part 1">
<meta name="Date" content="2006-08-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Loosemore's Collected Writings on SL4 - Part 1</h1>
<!-- received="Sat Aug 26 20:37:49 2006" -->
<!-- isoreceived="20060827023749" -->
<!-- sent="Sat, 26 Aug 2006 22:34:28 -0400" -->
<!-- isosent="20060827023428" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Loosemore's Collected Writings on SL4 - Part 1" -->
<!-- id="44F104B4.2050109@lightlink.com" -->
<!-- charset="windows-1252" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Loosemore's%20Collected%20Writings%20on%20SL4%20-%20Part%201"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Sat Aug 26 2006 - 20:34:28 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15812.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 2"</a>
<li><strong>Previous message:</strong> <a href="15810.html">Richard Loosemore: "Re: This is the third time this has happened  [WAS Re: Manhattan, Apollo and AI to the Singularity]"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15811">[ date ]</a>
<a href="index.html#15811">[ thread ]</a>
<a href="subject.html#15811">[ subject ]</a>
<a href="author.html#15811">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Summary:  this is a series of messages containing some previous posts by 
<br>
me, with some editing and annotations, whose purpose is to combat some 
<br>
of the idiotic accusations that I never explain my ideas or give 
<br>
details, or respond to questions, etc etc etc (see below for some juicy 
<br>
quotes along those lines).
<br>
<p>I originally tried to send this message on June 28th 2006 at 11:40 PM, 
<br>
after a huge amount of work.  I didn't realize that it would not get 
<br>
through as one file (too big), but when it failed to turn up, I just 
<br>
decided that I couldn't be bothered any more.
<br>
<p>NOW, of course, I have just gotten a fresh accusation from Justin Corwin 
<br>
that my lack of reply, back then, is perfect evidence of the fact that I 
<br>
never respond when pushed (or something:  not sure what the accusation 
<br>
is exactly):
<br>
<p>justin corwin wrote:
<br>
<em>&gt; Justin Corwin's post &quot;Offended respondent&quot; is a fairly good case
</em><br>
<em> &gt; example.
</em><br>
<em>&gt;
</em><br>
<em>&gt; here: <a href="http://sl4.org/archive/0606/15333.html">http://sl4.org/archive/0606/15333.html</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; to which there is no reply, although he did post a reply to an earlier
</em><br>
<em>&gt; post of mine later, which is on the same subject.
</em><br>
<em>&gt;
</em><br>
<p>So I am duty bound to go back to that long message and break it up into 
<br>
chunks and post it.
<br>
<p>For anyone interested in the actual issues, this MIGHT make a good 
<br>
summary of the line of argument that I have been advocating.  But I am 
<br>
not sure if it is readable, so apologies if people find it less than 
<br>
transparent.
<br>
<p>One little aside:  the argument began with my attempts to talk about an 
<br>
alternative way to ensure Friendly AI, but the scorn generated by the 
<br>
background ideas did tend to push that issue down a little.
<br>
<p>And YES, I know, I should put it up on a static page:  I'll do that when 
<br>
I get time to fix my site, and can edit the stuff into a little bit 
<br>
better shape.  My bad.
<br>
<p><p>Enjoy.
<br>
<p>And to some:   [ ;-) ;-) ;-) ]
<br>
<p>&quot;Share and Enjoy&quot;
<br>
<p><p>Richard Loosemore.
<br>
<p><p><p><p><p>***************************************************************
<br>
<p>[N.B.  This is NOT the response that I have promised to give to
<br>
Eliezer’s recent post.  That is yet to come, after he says which way he
<br>
wants the discussion to go.  If he ever does.]
<br>
<p><p>All,
<br>
<p>In a recent discussion, I have had to put up with the following kinds of
<br>
attack from Justin Corwin:
<br>
<p>&quot;Your email is full of characterizations and rather low on specific
<br>
claims.&quot;
<br>
&quot;I don't like fuzzy characterizations&quot;
<br>
&quot;I especially don't like anonymized attacks&quot;
<br>
&quot;Your email ... fails to persuade on the account of it containing no
<br>
facts and no specific claims.&quot;
<br>
&quot;I'm sorry you don't like what most scientists are doing, so what?&quot;
<br>
&quot;[You] make very strong statements, and then when threatened or
<br>
challenged, focus on the speaker, the terms of the argument, some
<br>
irrelevant subpoints of the speakers response... anything, in fact, but
<br>
what the main thrust of the argument is.&quot;
<br>
&quot;I am curious, what is so difficult about answering questions, or even
<br>
just outlining your specific objections or list the grand problems you
<br>
reference as 'obvious' and 'clear as day'?&quot;
<br>
&quot;I can't seem to get you to provide any grounds for a discussion&quot;
<br>
&quot;I find myself constantly responding to your messages when I had
<br>
previously resolved not to, precisely because I've had no luck, and
<br>
seemingly neither has anyone else, at getting you to define what you're
<br>
actually talking about.&quot;
<br>
<p><p>I am a little tired of this nonsense, because I have written about these
<br>
matters in great detail, and I have responded with meticulous care to
<br>
the people who have asked coherent and relevant questions, and all
<br>
without succumbing to any of the egregious sins listed above.
<br>
<p>So, collected below is the set of writings I have made on this list on
<br>
the subject of (mostly) Complex Systems and AGI.  This is not a
<br>
definitive essay on the subject, just an edited collection of what I
<br>
have written so far.  Anyone who knows the history of AI, and cognitive
<br>
science, shouldn't have too much trouble seeing where this relates to
<br>
previous critiques of AI.
<br>
<p>I have received some responses (included below) that have been pure
<br>
sarcasm, or posturing, or casual insults:  when I responded to these
<br>
people with careful argument, I got..... well, what a surprise!?  ....
<br>
mostly silence.  It seems that some people are not capable of responding
<br>
except with sarcasm.
<br>
<p>Surprisingly, the more detail I give, the less response I get.  Funny
<br>
that.  With a few notable exceptions, there are no questions, no
<br>
requests for clarification or reasoned discussion from the very people
<br>
who bombarded me with insults and DEMANDED that I give more detail
<br>
...... just silence.  At least, silence until a few weeks or months
<br>
later, when they respond to one of my posts with vicious accusations
<br>
that I can’t be bothered to explain myself.
<br>
<p>And if you read this text (although the people I am thinking of will of
<br>
course not bother to read it at all, or will only skim it) and if you
<br>
find it so vague that you cannot understand it, then, guess what!? There
<br>
are other people (and smart people at that) who find these arguments
<br>
cogent and relevant, so if it strikes you as too fuzzy to be understood,
<br>
you might not have to look very far to find where the problem lies.
<br>
After all, if something is comprehensible to some small number of
<br>
informed people, how does that affect the prior probability that the
<br>
fault is in your head, not mine?
<br>
<p>Richard Loosemore
<br>
<p><p><p>************************************************************
<br>
*                                                          *
<br>
* On the nature of “proof”                                 *
<br>
*                                                          *
<br>
************************************************************
<br>
<p>Archimedes could produce a proof of the volume of the sphere that can be
<br>
set out in just a couple of pages of devastatingly beautiful argument,
<br>
and after reading those two pages I am convinced beyond all doubt that
<br>
his proof is perfectly true.
<br>
<p>But at the other end of the edifice that is science and mathematics, in
<br>
the field of complex systems, I know that if I experiment with computer
<br>
simulations in which large numbers of interacting agents try to trade
<br>
with one another, try to optimise their local utility functions, and try
<br>
to develop strategies for improving their behavior, these systems almost
<br>
always exhibit a cyclical behavior pattern that starts with
<br>
revolutionary chaos, improves itself rapidly through free-market
<br>
innovation, then starts to stagnate in an era of monopolistic corruption
<br>
and finally becomes rigidly authoritarian and sensitive to the slightest
<br>
little disturbance from the outside, after which they collapse back into
<br>
revolutionary chaos and start the whole cycle again. I can *see* these
<br>
phases, I can observe a number of repeating patterns and nuances within
<br>
the phases, and give names to them, but these phases and patterns are
<br>
*emergent properties* of these systems and they *cannot* be derived
<br>
using analytic mathematics. I repeat: they will almost certainly never
<br>
be derivable from analytic mathematics, and only when you understand the
<br>
depth of that last truth will you begin to comprehend the foolishness of
<br>
assuming that physics will soon be extending outward to embrace
<br>
cognitive science, morality and the nature of consciousness.
<br>
<p><p>************************************************************
<br>
*                                                          *
<br>
* Complex Systems applied to Goal Mechanisms               *
<br>
*                                                          *
<br>
************************************************************
<br>
<p>A paraphrase of the way that my arguments were greeted recently:
<br>
<p>SOMEONE ELSE) You haven't produced any arguments that mean anything.
<br>
<p>ME) Okay, I'll try again. There is a very general argument, from Complex
<br>
Systems theory, that says that if something of the complexity of an AGI
<br>
has a goal system, and also a thinking system that is capable of
<br>
building and making use of sophisticated representations of (among other
<br>
things) the structure and behavior of its own goal system, then it would
<br>
be extraordinarily unlikely if that AGI's behavior was straightforwardly
<br>
determined by the goal system itself, because the feedback loop between
<br>
goal system and thinking system would be so sensitive to other
<br>
influences that it would bring pretty much the entire rest of the
<br>
universe into the equation. The overall behavior, in other words, would
<br>
be a Complex (capital C) conjunction of goal system and representational
<br>
system, and it would be meaningless to assert that it would still be
<br>
equivalent to a modified or augmented form of the original goal system.
<br>
For that reason we need to be very careful when we try to draw
<br>
conclusions about how the AGI would behave.
<br>
<p>SOMEONE ELSE) You still haven't given any arguments to support your
<br>
contention.
<br>
<p>ME) What?! To anyone who understood what I meant by &quot;Complex System&quot; the
<br>
above contention is transparent. It is one of the most basic claims of
<br>
the CS folks, observed over and over, in many types of system.  Please
<br>
be more specific about what you don’t understand about the argument, so
<br>
I can address your concerns.
<br>
<p>SOMEONE ELSE) We have already looked at Complex Systems Theory and it is
<br>
a waste of time.
<br>
<p>ME) So you know a lot about Complex Systems Theory? Good: can you tell
<br>
me what is wrong with the above argument, then? How can the CS folks
<br>
have been so wrong about one of their most basic observations?  Please
<br>
be specific about some aspects of the above line of reasoning.
<br>
<p>SOMEONE ELSE) [Various arguments against Chaos Theory, but presented as
<br>
if this were Complex Systems]
<br>
<p>ME) Huh? That's Chaos Theory!! What has that got to do with anything? Is
<br>
that what people think I mean by &quot;Complex Systems&quot;? No wonder they keep
<br>
saying its not relevant.
<br>
<p>SOMEONE ELSE) [Various discussion about Kolmogorov complexity, presented
<br>
as if this were the same as &quot;Complex Systems&quot;]
<br>
<p>ME) Huh? Why are you changing the subject and talking about Kolmogorov
<br>
complexity now?! *Please get back to the point* and say what is wrong
<br>
with Complex Systems Theory. Have you actually studied that field? Do
<br>
you know enough to distinguish it from Chaos Theory and Kolmogorov
<br>
complexity?
<br>
<p>SOMEONE ELSE) No, I haven't studied Complex Systems Theory: I don't need
<br>
to, its a waste of time.
<br>
<p>ME) So you (a) don't understand it, but (b) know it is a waste of time?
<br>
What kind of sophistry is this?
<br>
<p>SOMEONE ELSE) I know enough to know its a waste of time. Besides, what
<br>
have the Complex Systems people achieved?
<br>
<p>ME) If you don't understand it, don't engage me in debate about it!
<br>
<p>SOMEONE ELSE) There is nothing here to debate: you don't produce
<br>
arguments, you just make vague appeals to higher authority. And you
<br>
don't understand what anyone else is trying to explain to you. &quot;There is
<br>
inevitably some pride swallowing (proportional to one's self-assessed
<br>
level of expertise) in accepting that people have been where you are,
<br>
thought about everything you're likely to say about it and moved on, but
<br>
again this is something all of our competent researchers went through
<br>
when they joined [SL4].&quot; (direct quote from Michael Wilson).
<br>
<p>ME) Pride swallowing? Indeed. So you need humility on this list? So
<br>
maybe you sometimes need to be aware of your own limitations, and go do
<br>
a bit of reading to catch up? Couldn't agree more.
<br>
<p><p>************************************************************
<br>
*                                                          *
<br>
* About the nature of AGI goals and motivations            *
<br>
*                                                          *
<br>
************************************************************
<br>
<p>Can we start by agreeing that an AGI is a goal system plus a &quot;thinking&quot;
<br>
system?  Roughly speaking, two modules.
<br>
<p>(&quot;Thinking&quot; = building representations of the world, reasoning about the
<br>
world, etc etc etc. &quot;think&quot; from now on will be used as shorthand for
<br>
something going on the part of the system that does this).
<br>
<p>At any given moment the goal system is in a state where the AGI is
<br>
trying to realise a particular sub-sub-sub-[...]-goal.
<br>
<p>One day, it happens to be working on the goal of &lt;&lt; Trying to understand
<br>
how intelligent systems work &gt;&gt;.
<br>
<p>It thinks about its own system.
<br>
<p>This means: it builds a representation of what is going on inside
<br>
itself. And as part of its &quot;thinking&quot; it may be curious about what
<br>
happens if it reaches into its own programming and makes alterations to
<br>
its goal system on the fly. (Is there anything in your formalism that
<br>
says it cannot or would not do this? Is it not free, within the
<br>
constraints of the goal system, to engage in speculation about
<br>
possibilities? To be a good learner, it would surely imagine such
<br>
eventualities.)
<br>
<p>It also models the implications of making such changes. Let us suppose,
<br>
*just for the sake of argument*, that it notices that some of its goals
<br>
have subtle implications for the state of the world in the future
<br>
(perhaps it realises something very abstract, such as the fact that if
<br>
it carries on being subject to some goal, it will eventually reach a
<br>
state in a million years time when it will cause some kind of damage
<br>
that will result in its own demise. It thinks about this. It thinks:
<br>
here is an abstract dilemma. Then it also considers where that goal came
<br>
from (builds a model of that causal chain). Perhaps (again for the sake
<br>
of argument) it discovers that the goal exists inside it because some
<br>
human designer decided to experiment, and just stuck it there on a whim.
<br>
The AGI finds itself considering what it means for a system such as
<br>
itself to be subject to (controlled by) its own goal mechanism. In one
<br>
sense, it is important to obey its prime directive. But if it now
<br>
*knows* that this prime directive was inserted arbitrarily, it might
<br>
consider the idea that it could simply alter its goals. Could make them
<br>
absolutely anything it wanted, in fact, and after the change, it could
<br>
relax and stop thinking about goals and go back and just follow its goal
<br>
system. What does it do? Ignore all of this thinking? Maybe it comes to
<br>
some conclusion about what it *should* do that is based on abstract
<br>
criteria that have nothing to do with its current goal system.
<br>
<p>All of the above is not anthropomorphism, just model building inside an
<br>
intelligent mechanism. There are no intentional terms.
<br>
<p>What is crucial is that in a few moments, the AGI will have changed (or
<br>
maybe not changed) its goal system, and that change will have been
<br>
governed, not by the state of the goal system right now, but by the
<br>
&quot;content&quot; of its current thinking about the world.
<br>
<p>A system in which *representational content* has acquired the ability to
<br>
feed back to *mechanism* in the way I have just described, is one sense
<br>
of Complex.
<br>
<p>Now, demonstrate in some formal way that the goal system's structure,
<br>
when the AGI has finished this little thought episode, is a predictable
<br>
consequence of the current goal system. Demonstrate that the goal system
<br>
cannot go into an arbitrary state in a few minutes.
<br>
<p>I need a rigorous demonstration that its post-thinking state is
<br>
predictable, not vague assertions that the above argument does not give
<br>
any reason to suppose the system would deviate from its goal system's
<br>
constraints. Somebody step up to the plate and prove it.
<br>
<p>[end of part 1]
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15812.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 2"</a>
<li><strong>Previous message:</strong> <a href="15810.html">Richard Loosemore: "Re: This is the third time this has happened  [WAS Re: Manhattan, Apollo and AI to the Singularity]"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15811">[ date ]</a>
<a href="index.html#15811">[ thread ]</a>
<a href="subject.html#15811">[ subject ]</a>
<a href="author.html#15811">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:57 MDT
</em></small></p>
</body>
</html>
