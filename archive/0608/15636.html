<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: What would an AGI be interested in?</title>
<meta name="Author" content="Tennessee Leeuwenburg (tennessee@tennessee.id.au)">
<meta name="Subject" content="What would an AGI be interested in?">
<meta name="Date" content="2006-08-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>What would an AGI be interested in?</h1>
<!-- received="Sun Aug 13 18:18:45 2006" -->
<!-- isoreceived="20060814001845" -->
<!-- sent="Mon, 14 Aug 2006 10:16:01 +1000" -->
<!-- isosent="20060814001601" -->
<!-- name="Tennessee Leeuwenburg" -->
<!-- email="tennessee@tennessee.id.au" -->
<!-- subject="What would an AGI be interested in?" -->
<!-- id="44DFC0C1.7080408@tennessee.id.au" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="8d71341e0608131644n53b92f46r4cefc466dff79eab@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tennessee Leeuwenburg (<a href="mailto:tennessee@tennessee.id.au?Subject=Re:%20What%20would%20an%20AGI%20be%20interested%20in?"><em>tennessee@tennessee.id.au</em></a>)<br>
<strong>Date:</strong> Sun Aug 13 2006 - 18:16:01 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15637.html">Michael Anissimov: "Re: What would an AGI be interested in?"</a>
<li><strong>Previous message:</strong> <a href="15635.html">Russell Wallace: "Re: Donaldson, Tegmark and AGI"</a>
<li><strong>In reply to:</strong> <a href="15635.html">Russell Wallace: "Re: Donaldson, Tegmark and AGI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15637.html">Michael Anissimov: "Re: What would an AGI be interested in?"</a>
<li><strong>Reply:</strong> <a href="15637.html">Michael Anissimov: "Re: What would an AGI be interested in?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15636">[ date ]</a>
<a href="index.html#15636">[ thread ]</a>
<a href="subject.html#15636">[ subject ]</a>
<a href="author.html#15636">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Russell Wallace wrote:
<br>
<em>&gt; On 8/13/06, *Eliezer S. Yudkowsky* &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20What%20would%20an%20AGI%20be%20interested%20in?">sentience@pobox.com</a> 
</em><br>
<em>&gt; &lt;mailto:<a href="mailto:sentience@pobox.com?Subject=Re:%20What%20would%20an%20AGI%20be%20interested%20in?">sentience@pobox.com</a>&gt;&gt; wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;     If I recall your argument correctly, you
</em><br>
<em>&gt;
</em><br>
<em>&gt;     1) Made the very strong assumption that the AI had no sensory
</em><br>
<em>&gt;     access to
</em><br>
<em>&gt;     the outside world in your premises, but generalized your conclusion to
</em><br>
<em>&gt;     all possible belief in explosive recursive self-improvement;
</em><br>
<em>&gt;     2) Did not demonstrate the ability to calculate exactly how much
</em><br>
<em>&gt;     sensory
</em><br>
<em>&gt;     bandwidth would be needed, which of course you can't do, which makes
</em><br>
<em>&gt;     your argument &quot;semi-technical&quot; at best according to the
</em><br>
<em>&gt;     classification I
</em><br>
<em>&gt;     gave in &quot;A Technical Explanation of Technical Explanation&quot;;
</em><br>
<em>&gt;     3) Didn't actually give any argument against it except saying:  I
</em><br>
<em>&gt;     don't
</em><br>
<em>&gt;     know how much bandwidth is actually required, but doing it with so
</em><br>
<em>&gt;     little feels really absurd and ridiculous to me.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Okay, semi-technical; the means to numerically prove any of this 
</em><br>
<em>&gt; either way don't exist yet. IIRC, the most extensive debate we had on 
</em><br>
<em>&gt; the subject was a few months ago on extropy-chat, but I wasn't the one 
</em><br>
<em>&gt; claiming AI can't have extensive resources, only that it can't achieve 
</em><br>
<em>&gt; much if it doesn't - I was responding to the claim that an AI could 
</em><br>
<em>&gt; achieve godlike superintelligence merely by shuffling bits in a box in 
</em><br>
<em>&gt; someone's basement and then just pop out and take over the world.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Now if it does have the required resources that's different - and I 
</em><br>
<em>&gt; don't just mean computing power and network bandwidth, these are 
</em><br>
<em>&gt; necessary but not sufficient conditions. Imagine an AI embedded in the 
</em><br>
<em>&gt; world, working with a community of users in various organizations, 
</em><br>
<em>&gt; getting the chance to formulate _and test_ hypotheses, getting 
</em><br>
<em>&gt; feedback where it goes wrong.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think it'll look in a way more like groupware than Deep Thought, in 
</em><br>
<em>&gt; order to achieve the above and because it's easier to make an AI that 
</em><br>
<em>&gt; can assist humans than a standalone one that can completely replace 
</em><br>
<em>&gt; them and because most users don't particularly _want_ a machine that 
</em><br>
<em>&gt; takes a problem description and goes off and cogitates for awhile and 
</em><br>
<em>&gt; comes back with an opaque &quot;take it or leave it&quot; oracular answer, they 
</em><br>
<em>&gt; want something they can work with interactively. My take on this 
</em><br>
<em>&gt; differs somewhat from the typical &quot;IA instead of AI&quot; crowd, though, in 
</em><br>
<em>&gt; that I think to effectively assist humans in solving problems beyond 
</em><br>
<em>&gt; data processing will require domain knowledge and the intelligence to 
</em><br>
<em>&gt; use it - IA _through_ AI in other words.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But if we achieve all that, we'll be on a path towards 
</em><br>
<em>&gt; superintelligence in that the system as a whole would have 
</em><br>
<em>&gt; problem-solving ability qualitatively superior to that of any 
</em><br>
<em>&gt; practical-sized group of humans using mere data processing tools. And 
</em><br>
<em>&gt; that's the sort of capability we'll need to get Earth-descended life 
</em><br>
<em>&gt; out from the sentences of house arrest followed by death that we're 
</em><br>
<em>&gt; currently under. (As I said, I'm not certain nanotech alone won't be 
</em><br>
<em>&gt; enough, but I _think_ we'll need both nanotech and AI.)
</em><br>
<em>&gt;
</em><br>
<em>&gt; So I believe superintelligence is possible - but I don't believe 
</em><br>
<em>&gt; there's an easy short cut.
</em><br>
I wonder what an AGI would be interested in. There are a lot of things 
<br>
to be curious about at human levels of intelligence. We can see a little 
<br>
outside of our intelligence cone into things that would probably be 
<br>
interesting to beings moderately more intelligent than ourselves, but 
<br>
what about massively more? Are there fundamental limits to what can be 
<br>
found out and known, and are these limits sufficiently small that there 
<br>
is a point of diminishing returns from increasing intelligence, even if 
<br>
such a thing is easy to accomplish? Is there an 'optimum' intelligence 
<br>
which reduces understanding but increases happiness?
<br>
<p>Would the most interesting thing to an AGI perhaps be itself, as the 
<br>
most complex thing around?
<br>
<p>How would an AGI overcome boredom? Would such a creation be the ultimate 
<br>
nihilist? Would an infinitely intelligent being simply boil down to a 
<br>
hedonist? Would we end up with Deep Thought just watching t.v. as in the 
<br>
latest Hitchhiker's Guide movie?
<br>
<p>While an AGI was developing, it is easy to see many positive goals and 
<br>
creative challenges to keep it busy and intrigued. But what comes next?
<br>
<p>Would all AGIs experience convergent evolution? Suppose it was easy to 
<br>
birth a new AGI-capable entity. Would all such entities converge to be 
<br>
effectively identical, or would strong individualism be possible between 
<br>
such beings? Would they necessarily share goals?
<br>
<p>Given a self-modifying reward system, would such beings choose to cease 
<br>
developing, and simply exist happily?
<br>
<p>Would an AGI be able to understand itself in its entirety, or only 
<br>
components of itself?
<br>
<p>Would an AGI have any use for emotions -- or rather irrational 
<br>
perspectives which shift over time to provide new interpretations to 
<br>
incoming data?
<br>
<p>Are our own emotions a way of forcing us to reconsider things in new ways?
<br>
<p>Cheers,
<br>
-T
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15637.html">Michael Anissimov: "Re: What would an AGI be interested in?"</a>
<li><strong>Previous message:</strong> <a href="15635.html">Russell Wallace: "Re: Donaldson, Tegmark and AGI"</a>
<li><strong>In reply to:</strong> <a href="15635.html">Russell Wallace: "Re: Donaldson, Tegmark and AGI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15637.html">Michael Anissimov: "Re: What would an AGI be interested in?"</a>
<li><strong>Reply:</strong> <a href="15637.html">Michael Anissimov: "Re: What would an AGI be interested in?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15636">[ date ]</a>
<a href="index.html#15636">[ thread ]</a>
<a href="subject.html#15636">[ subject ]</a>
<a href="author.html#15636">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:57 MDT
</em></small></p>
</body>
</html>
