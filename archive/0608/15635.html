<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Donaldson, Tegmark and AGI</title>
<meta name="Author" content="Russell Wallace (russell.wallace@gmail.com)">
<meta name="Subject" content="Re: Donaldson, Tegmark and AGI">
<meta name="Date" content="2006-08-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Donaldson, Tegmark and AGI</h1>
<!-- received="Sun Aug 13 17:46:21 2006" -->
<!-- isoreceived="20060813234621" -->
<!-- sent="Mon, 14 Aug 2006 00:44:13 +0100" -->
<!-- isosent="20060813234413" -->
<!-- name="Russell Wallace" -->
<!-- email="russell.wallace@gmail.com" -->
<!-- subject="Re: Donaldson, Tegmark and AGI" -->
<!-- id="8d71341e0608131644n53b92f46r4cefc466dff79eab@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="44DF7326.3020706@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Russell Wallace (<a href="mailto:russell.wallace@gmail.com?Subject=Re:%20Donaldson,%20Tegmark%20and%20AGI"><em>russell.wallace@gmail.com</em></a>)<br>
<strong>Date:</strong> Sun Aug 13 2006 - 17:44:13 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15636.html">Tennessee Leeuwenburg: "What would an AGI be interested in?"</a>
<li><strong>Previous message:</strong> <a href="15634.html">Tennessee Leeuwenburg: "Re: Maximize the renormalized human utility function!"</a>
<li><strong>In reply to:</strong> <a href="15632.html">Eliezer S. Yudkowsky: "Re: Donaldson, Tegmark and AGI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15636.html">Tennessee Leeuwenburg: "What would an AGI be interested in?"</a>
<li><strong>Reply:</strong> <a href="15636.html">Tennessee Leeuwenburg: "What would an AGI be interested in?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15635">[ date ]</a>
<a href="index.html#15635">[ thread ]</a>
<a href="subject.html#15635">[ subject ]</a>
<a href="author.html#15635">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 8/13/06, Eliezer S. Yudkowsky &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Donaldson,%20Tegmark%20and%20AGI">sentience@pobox.com</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; If I recall your argument correctly, you
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1) Made the very strong assumption that the AI had no sensory access to
</em><br>
<em>&gt; the outside world in your premises, but generalized your conclusion to
</em><br>
<em>&gt; all possible belief in explosive recursive self-improvement;
</em><br>
<em>&gt; 2) Did not demonstrate the ability to calculate exactly how much sensory
</em><br>
<em>&gt; bandwidth would be needed, which of course you can't do, which makes
</em><br>
<em>&gt; your argument &quot;semi-technical&quot; at best according to the classification I
</em><br>
<em>&gt; gave in &quot;A Technical Explanation of Technical Explanation&quot;;
</em><br>
<em>&gt; 3) Didn't actually give any argument against it except saying:  I don't
</em><br>
<em>&gt; know how much bandwidth is actually required, but doing it with so
</em><br>
<em>&gt; little feels really absurd and ridiculous to me.
</em><br>
<em>&gt;
</em><br>
<p>Okay, semi-technical; the means to numerically prove any of this either way
<br>
don't exist yet. IIRC, the most extensive debate we had on the subject was a
<br>
few months ago on extropy-chat, but I wasn't the one claiming AI can't have
<br>
extensive resources, only that it can't achieve much if it doesn't - I was
<br>
responding to the claim that an AI could achieve godlike superintelligence
<br>
merely by shuffling bits in a box in someone's basement and then just pop
<br>
out and take over the world.
<br>
<p>Now if it does have the required resources that's different - and I don't
<br>
just mean computing power and network bandwidth, these are necessary but not
<br>
sufficient conditions. Imagine an AI embedded in the world, working with a
<br>
community of users in various organizations, getting the chance to formulate
<br>
_and test_ hypotheses, getting feedback where it goes wrong.
<br>
<p>I think it'll look in a way more like groupware than Deep Thought, in order
<br>
to achieve the above and because it's easier to make an AI that can assist
<br>
humans than a standalone one that can completely replace them and because
<br>
most users don't particularly _want_ a machine that takes a problem
<br>
description and goes off and cogitates for awhile and comes back with an
<br>
opaque &quot;take it or leave it&quot; oracular answer, they want something they can
<br>
work with interactively. My take on this differs somewhat from the typical
<br>
&quot;IA instead of AI&quot; crowd, though, in that I think to effectively assist
<br>
humans in solving problems beyond data processing will require domain
<br>
knowledge and the intelligence to use it - IA _through_ AI in other words.
<br>
<p>But if we achieve all that, we'll be on a path towards superintelligence in
<br>
that the system as a whole would have problem-solving ability qualitatively
<br>
superior to that of any practical-sized group of humans using mere data
<br>
processing tools. And that's the sort of capability we'll need to get
<br>
Earth-descended life out from the sentences of house arrest followed by
<br>
death that we're currently under. (As I said, I'm not certain nanotech alone
<br>
won't be enough, but I _think_ we'll need both nanotech and AI.)
<br>
<p>So I believe superintelligence is possible - but I don't believe there's an
<br>
easy short cut.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15636.html">Tennessee Leeuwenburg: "What would an AGI be interested in?"</a>
<li><strong>Previous message:</strong> <a href="15634.html">Tennessee Leeuwenburg: "Re: Maximize the renormalized human utility function!"</a>
<li><strong>In reply to:</strong> <a href="15632.html">Eliezer S. Yudkowsky: "Re: Donaldson, Tegmark and AGI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15636.html">Tennessee Leeuwenburg: "What would an AGI be interested in?"</a>
<li><strong>Reply:</strong> <a href="15636.html">Tennessee Leeuwenburg: "What would an AGI be interested in?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15635">[ date ]</a>
<a href="index.html#15635">[ thread ]</a>
<a href="subject.html#15635">[ subject ]</a>
<a href="author.html#15635">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:57 MDT
</em></small></p>
</body>
</html>
