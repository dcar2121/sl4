<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Loosemore's Collected Writings on SL4 - Part 5</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Loosemore's Collected Writings on SL4 - Part 5">
<meta name="Date" content="2006-08-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Loosemore's Collected Writings on SL4 - Part 5</h1>
<!-- received="Sat Aug 26 20:39:46 2006" -->
<!-- isoreceived="20060827023946" -->
<!-- sent="Sat, 26 Aug 2006 22:36:42 -0400" -->
<!-- isosent="20060827023642" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Loosemore's Collected Writings on SL4 - Part 5" -->
<!-- id="44F1053A.8050801@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Loosemore's%20Collected%20Writings%20on%20SL4%20-%20Part%205"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Sat Aug 26 2006 - 20:36:42 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15816.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 4"</a>
<li><strong>Previous message:</strong> <a href="15814.html">Eliezer S. Yudkowsky: "Re: The Conjunction Fallacy Fallacy  [WAS Re: Anti-singularity spam.]"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15815">[ date ]</a>
<a href="index.html#15815">[ thread ]</a>
<a href="subject.html#15815">[ subject ]</a>
<a href="author.html#15815">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
[begin part 5]
<br>
<p><p>************************************************************
<br>
*                                                          *
<br>
* The Complex Systems Critique (Again)                     *
<br>
*                                                          *
<br>
************************************************************
<br>
<em>&gt; Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; Richard Loosemore wrote:
</em><br>
<em>&gt;&gt; As far as I am concerned, the widespread (is
</em><br>
<em>&gt;&gt; it really widespread?) SL4 assumption that
</em><br>
<em>&gt;&gt; &quot;strictly humanoid intelligence would not
</em><br>
<em>&gt;&gt; likely be Friendly ...[etc.]&quot; is based on a
</em><br>
<em>&gt;&gt; puerile understanding of, and contempt of,
</em><br>
<em>&gt;&gt; the mechanics of human intelligence.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Untrue.  I spent my first six years from 1996 to 2002
</em><br>
<em>&gt; studying the mechanics of human intelligence, until I
</em><br>
<em>&gt; understood it well enough to see why it wouldn't work. I
</em><br>
<em>&gt; suppose that in your lexicon, &quot;Complex Systems Theory&quot;
</em><br>
<em>&gt; and &quot;mechanics of human intelligence&quot; are synonyms.
</em><br>
<em>&gt; In my vocabulary, they are not synonyms, and studying
</em><br>
<em>&gt; such mere matters as neuroscience and cognitive
</em><br>
<em>&gt; psychology counts as trying to understand the mechanics
</em><br>
<em>&gt; of human intelligence, whatever my regard for
</em><br>
<em>&gt; &quot;Complex Systems Theory&quot; as a source of useful,
</em><br>
<em>&gt; predictive, engineering-helpful hypotheses about
</em><br>
<em>&gt; human intelligence. Disdain for your private theory of
</em><br>
<em>&gt; human intelligence is not the same as disdain for
</em><br>
<em>&gt; understanding the mechanics of human intelligence.
</em><br>
<p>Once again, you demonstrate my point for me....
<br>
<p>1) You deliver irrelevant insults:
<br>
<p><em>&gt; I suppose that in your lexicon, &quot;Complex Systems Theory&quot;
</em><br>
<em>&gt; and &quot;mechanics of human intelligence&quot; are synonyms.
</em><br>
<p>I don't need to respond to this (it is too obviously silly).
<br>
<p>&nbsp;&nbsp;2) You make assertions about things you know nothing about:
<br>
<p><em>&gt; Disdain for your private theory of human intelligence
</em><br>
<em>&gt; is not the same as disdain for understanding the
</em><br>
<em>&gt; mechanics of human intelligence.
</em><br>
<p>I have never discussed my &quot;private theory of human intelligence&quot; on this
<br>
list, so how could you possibly disdain it? I have discussed one issue
<br>
only. You responded to that issue with personal insults, red herrings
<br>
and numerous comments that showed you did not know the difference
<br>
between complex systems theory and chaos theory (a confusion that
<br>
rendered most of your other statements worthless).
<br>
<p>3) And you demonstrate the most amazing contempt for the subject and all
<br>
the other people who study it:
<br>
<p><em>&gt; I spent my first six years from 1996 to 2002 studying the
</em><br>
<em>&gt; mechanics of human intelligence, until I understood it well enough to 
</em><br>
<em>&gt; see why it wouldn't work.
</em><br>
<p>To be able to justify your assertion that &quot;it&quot; wouldn't work, you would
<br>
have to do experiments (simulations) and/or produce some theoretical
<br>
ideas, and *most important of all* you would also have to make coherent
<br>
replies to those who find fault with your position. I have read your
<br>
writings on the subject: I can find nothing but rambling,
<br>
stream-of-consciousness speculations. Where are your experiments? Where
<br>
are your coherent arguments in support of this claim? Where are your
<br>
coherent replies to your critics?
<br>
<p>I was one of those critics. I produced arguments based on a vast body of
<br>
empirical data. You made no coherent response to those arguments, never
<br>
demonstrating that you even comprehended what the arguments actually
<br>
were about In fact, looking back over the sum total of words you wrote
<br>
against the position that I elaborated earlier, I can find nothing that
<br>
is not either irrelevant posturing, a dismissal of an entire body of
<br>
research on the grounds that you consider it worthless (this being
<br>
exactly the contempt that I referred to above), an ad hominem attack on
<br>
either my credentials or those of hundreds of other researchers (whose
<br>
work you confuse with work going on in another field), or a blatant
<br>
non-sequiteur.
<br>
<p>&nbsp;&nbsp;The specific point I made in my post above was a small one: I was
<br>
referring to the way that some people here tend to assert that the
<br>
structure of the human mind is clearly sub-optimal, or clearly flawed,
<br>
or clearly not the best way to design an AGI, or clearly bad from the
<br>
point of view of guaranteeing Friendliness. This is an entirely
<br>
debatable point of view, but when challenged, a vocal subset of SL4
<br>
likes to respond not with arguments, but with the kind of invective that
<br>
you just delivered.
<br>
<p><p><p>************************************************************
<br>
*                                                          *
<br>
* Building an AGI using Systematic Experimentation         *
<br>
*                                                          *
<br>
************************************************************
<br>
<p>1) &quot;Prove&quot; that an AGI will be friendly? Proofs are for mathematicians.
<br>
I consider the use of the word &quot;proof,&quot; about the behavior of an AGI, as
<br>
on the same level of validity as the use of the word &quot;proof&quot; in
<br>
statements about evolutionary proclivities, for example &quot;Prove that no
<br>
tree could ever evolve, naturally, in such a way that it had a red
<br>
smiley face depicted on every leaf.&quot; Talking about proofs of
<br>
friendliness would be a fundamental misunderstanding of the role of the
<br>
word &quot;proof&quot;. We have enough problems with creationists and intelligent
<br>
design freaks abusing the word, without us getting confused about it too.
<br>
<p>If anyone disagrees with this, it is important to answer certain
<br>
objections. Do not simply assert that proof is possible, give some
<br>
reason why we should believe it to be so. In order to do this, you have
<br>
to give some coherent response to the arguments I previously set out (in
<br>
which the Complex Systems community asked you to explain why AGI systems
<br>
would be exempt from the empirical regularities they have observed).
<br>
<p>2) Since proof is impossible, the next best thing is a solid set of
<br>
reasons to believe in friendliness of a particular design. I will
<br>
quickly sketch how I think this will come about.
<br>
<p>First, many people have talked as if building a &quot;human-like&quot; AGI would
<br>
be very difficult. I think that this is a mistake, for the following
<br>
reasons.
<br>
<p>I think that what has been going on in the AI community for the last
<br>
couple of decades is a prolonged bark up the wrong tree, and that this
<br>
has made our lives more difficult than it should be.
<br>
<p>Specifically, I think that we (the early AI researchers) started from
<br>
the observation of certain *high-level* reasoning mechanisms that are
<br>
observable in the human mind, and generalized to the idea that these
<br>
mechanisms could be the foundational mechanisms of a thinking system.
<br>
The problem is that when we (as practitioners of philosophical logic)
<br>
get into discussions about the amazing way in which &quot;All Men Are Mortal&quot;
<br>
can be combined with &quot;Socrates is a Man&quot; to yield the conclusion
<br>
&quot;Socrates is Mortal&quot;, we are completely oblivious to the fact that a
<br>
huge piece of cognitive apparatus is sitting there, under the surface,
<br>
allowing us to relate words like &quot;all&quot; and &quot;mortal&quot; and &quot;Socrates&quot; and
<br>
&quot;men&quot; to things in the world, and to one another, and we are also
<br>
missing the fact that there are vast numbers of other conclusions that
<br>
this cognitive apparatus arrives at, on a moment by moment basis, that
<br>
are extremely difficult to squeeze into the shape of a syllogism. In
<br>
other words, you have this enormous cognitive mechanism, coming to
<br>
conclusions about the world all the time, and then it occasionally comes
<br>
to conclusions using just *one*, particularly clean, little subcomponent
<br>
of its array of available mechanisms, and we naively seize upon this
<br>
subcomponent and think that *that* is how the whole thing operates.
<br>
<p>By itself, this argument against the &quot;logical&quot; approach to AI might only
<br>
be a feeling, so we would then have to divide into two camps and each
<br>
pursue our own vision of AI until one of us succeeded.
<br>
<p>However, the people on my side of the divide have made our arguments
<br>
concrete enough that we can now be more specific about the problem, as
<br>
follows.
<br>
<p>What we say is this. The logic approach is bad because it starts with
<br>
presumptions about the local mechanisms of the system and then tries to
<br>
extend that basic design out until the system can build its own new
<br>
knowledge, and relate its fundamental concepts to the sensorimotor
<br>
signals that connect it to the outside world.... and from our experience
<br>
with complex systems we know that that kind of backwards design approach
<br>
will usually mean that the systems you design will partially work but
<br>
always get into trouble the further out you try to extend them. Because
<br>
of the complex-systems disconnect between local and global, each time
<br>
you start with preconceived notions about the local, you will find that
<br>
the global behavior never quite matches up with what you want it to be.
<br>
<p>So in other words, our criticism is *not* that you should be looking for
<br>
nebulous or woolly-headed &quot;emergent&quot; properties that explain cognition
<br>
-- that kind of &quot;emergence&quot; is a red herring -- instead, you should be
<br>
noticing that the hardest part of your implementation is always the
<br>
learning and grounding aspect of the system. Everything looks good on a
<br>
small, local scale (especially if you make your formalism extremely
<br>
elaborate, to deal with all the nasty little issues that arise) but it
<br>
never scales properly. In fact, some who take the logical approach will
<br>
confess that they still haven't thought much about exactly how learning
<br>
happens ... they have postponed that one.
<br>
<p>This is exactly what has been happening in AI research. And it has been
<br>
going on for, what, 20 years now? Plenty of theoretical analysis. Lots
<br>
of systems that do little jobs a little tiny bit better than before. A
<br>
few systems that are designed to appear, to a naive consumer, as though
<br>
they are intelligent (all the stuff coming out of Japan). But overall,
<br>
stagnation.
<br>
<p>So now, if this analysis is correct, what should be done?
<br>
<p>The alternative is to do something that has never been tried.
<br>
<p>Build a development environment that allowed rapid construction of large
<br>
numbers of different systems, so we can start to empirically study the
<br>
effects of changing the local mechanisms. We should try
<br>
cognitively-inspired mechanisms at the local level, but adapt them
<br>
according to what makes them globally stable. The point is not to
<br>
presuppose what the local mechanisms are, but to use what we know of
<br>
human cognition to get mechanisms that are in the right ballpark, then
<br>
experimentally adjust them to find out under what conditions they are
<br>
both stable and doing the things we want them to.
<br>
<p>I have been working on a set of candidate mechanisms for years. And also
<br>
working on the characteristics of a software development environment
<br>
that would allow this rapid construction of systems. There is no hiding
<br>
the fact that this would be a big project, but I believe it would
<br>
produce a software tool that all researchers could use to quickly create
<br>
systems that they could study, and by having a large number of people
<br>
attacking it from different angles, progress would be rapid.
<br>
<p>What I think would happen if we tried this approach is that we would
<br>
find ourselves not needing enormous complexity after all. This is just a
<br>
hunch, I agree, but I offer it as no more than that: we cannot possibly
<br>
know, until we try such an approach, if we find a quagmire or an easy
<br>
sail to the finish.
<br>
<p>But I can tell you this: we have never tried such an approach before,
<br>
and the one thing that we do know from the complex systems research (you
<br>
can argue with everything else, but you cannot argue with this) is that
<br>
we won't know the outcome until we try.
<br>
<p>(Notice that the availability of such a development environment would
<br>
not in any way preclude the kind of logic-based AI that is now the
<br>
favorite. You could just as easily build such models. The problem is
<br>
that people who did so would be embarrassed into showing how their
<br>
mechanisms interacted with real sensory and motor systems, and how they
<br>
acquired their higher level knowledge from primitives.... and that might
<br>
be a problem because in a side by side comparison I think it would be
<br>
finally obvious that the approach just simply did not work. Again
<br>
though, just by hunch. I want the development environment to become
<br>
available so we can do such comparisons, and stop philosophizing about it.)
<br>
<p>Finally, on the subject that we started with: motivations of an AGI. The
<br>
class of system I am proposing would have a motivational/emotional
<br>
system that is distinct from the immediate goal stack. Related, but not
<br>
be confused.
<br>
<p>I think we could build small scale examples of cognitive systems, insert
<br>
different kinds of M/E systems in them, and allow them to interact with
<br>
one another in simple virtual worlds. We could study the stability of
<br>
the systems, their cooperative behavior towards one another, their
<br>
response to situations in which they faced threats, etc. I think we
<br>
could look for telltale signs of breakdown, and perhaps even track their
<br>
&quot;thoughts&quot; to see what their view of the world was, and how that
<br>
interacted with their motivations.
<br>
<p>And what we might well discover is that the disconnect between M/E
<br>
system and intellect is just as it appears to be in humans: humans are
<br>
intellectual systems with aggressive M/E systems tacked on underneath.
<br>
They don't need the aggression (it was just useful during evolution),
<br>
and without it they become immensely stable.
<br>
<p>I think that we could also understand the nature of the &quot;attachment&quot;
<br>
mechanisms that make human beings have irrational fondness for one
<br>
another, and for a species as a whole, and incorporate that in a design.
<br>
I think we could stud the effects of that mechanism, and come to be sure
<br>
of its stability.
<br>
<p>And, at the end of the day, I think we will come to understand the
<br>
nature of M/E systems so well that we will be able to say with a fair
<br>
degree of certainty that the more knowledge an AGI has, the more it
<br>
tends to understand the need for cooperation. I think we might (just
<br>
might) discover that we could trust such systems.
<br>
<p>But we have to experiment to find out, and experiment in a way that
<br>
nobody has ever done before.
<br>
<p>&nbsp;&nbsp;Richard Loosemore.
<br>
<p><p>[end part 5]
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15816.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 4"</a>
<li><strong>Previous message:</strong> <a href="15814.html">Eliezer S. Yudkowsky: "Re: The Conjunction Fallacy Fallacy  [WAS Re: Anti-singularity spam.]"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15815">[ date ]</a>
<a href="index.html#15815">[ thread ]</a>
<a href="subject.html#15815">[ subject ]</a>
<a href="author.html#15815">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:57 MDT
</em></small></p>
</body>
</html>
