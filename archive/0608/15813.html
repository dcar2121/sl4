<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Loosemore's Collected Writings on SL4 - Part 3</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Loosemore's Collected Writings on SL4 - Part 3">
<meta name="Date" content="2006-08-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Loosemore's Collected Writings on SL4 - Part 3</h1>
<!-- received="Sat Aug 26 20:38:44 2006" -->
<!-- isoreceived="20060827023844" -->
<!-- sent="Sat, 26 Aug 2006 22:35:37 -0400" -->
<!-- isosent="20060827023537" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Loosemore's Collected Writings on SL4 - Part 3" -->
<!-- id="44F104F9.5030308@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Loosemore's%20Collected%20Writings%20on%20SL4%20-%20Part%203"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Sat Aug 26 2006 - 20:35:37 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15814.html">Eliezer S. Yudkowsky: "Re: The Conjunction Fallacy Fallacy  [WAS Re: Anti-singularity spam.]"</a>
<li><strong>Previous message:</strong> <a href="15812.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 2"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15813">[ date ]</a>
<a href="index.html#15813">[ thread ]</a>
<a href="subject.html#15813">[ subject ]</a>
<a href="author.html#15813">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
[begin part 3]
<br>
<p><p>************************************************************
<br>
*                                                          *
<br>
* What is (and is not) a Complex System                    *
<br>
*                                                          *
<br>
************************************************************
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Eliezer S. Yudkowsky wrote:] My car, alas, is not
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a Carnot Engine. There are all these practical
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;complications in the real world, doncha know,
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;which makes thermodynamics irrelevant if you're
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;trying to build *efficient* systems. My car is a
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Complex System, too: I looked under the hood and
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;there were all these interacting doohickeys whose
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;local behavior was completely different from the
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;globally emergent car. That carburetor thingy
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;doesn't even have wheels, much less an engine!
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As a result I find myself completely unable to
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;predict my car's behavior. Why, one time I
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;floored the gas pedal, and my car moved fifty
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;miles backward and refilled its own gas tank!
<br>
<p>Your car, alas, is also not a Complex System.
<br>
<p>I mean, this was supposed to be a joke, right: you didn't really think
<br>
it was a complex system? I assume you are still thinking about a serious
<br>
reply ... one that doesn't involve any confusion about what a Complex
<br>
system or a Chaotic system really is?
<br>
<p>This looks like either sarcasm, ignorance or both.
<br>
<p><p>Richard Loosemore
<br>
<p>************************************************************
<br>
*                                                          *
<br>
* The Complex Systems Critique (Again)                     *
<br>
*                                                          *
<br>
************************************************************
<br>
<p><p>Ben Goertzel wrote:
<br>
<em>&gt; Richard,
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; [Richard Loosemore wrote:]
</em><br>
<em>&gt;&gt; The answers that others offer to your questions are,
</em><br>
<em>&gt;&gt; pretty much:  no
</em><br>
<em>&gt;&gt; you cannot really avoid complex systems, and mathematical
</em><br>
<em>&gt;&gt; verification of their friendliness is the very last thing
</em><br>
<em>&gt;&gt; you would be able to do.
</em><br>
<em>&gt;&gt; The main defining characteristic of complex systems is that such
</em><br>
<em>&gt;&gt; mathematical verification is out of reach.
</em><br>
<em>&gt;
</em><br>
<em>&gt; There is no question that human mind/brain is a complex system which
</em><br>
<em>&gt; achieves its intelligence via emergence, self-organization, strange
</em><br>
<em>&gt; attractors, terminal attractors, and all that great stuff....
</em><br>
<em>&gt;
</em><br>
<em>&gt; And there is little question that emergence-based intelligence is
</em><br>
<em>&gt; intrinsically very difficult to predict and control with a high
</em><br>
<em>&gt; degree of reliability, thus rendering verified Friendliness an
</em><br>
<em>&gt; unlikely outcome.
</em><br>
<em>&gt;
</em><br>
<em>&gt; However, these observations don't tell you much about whether it's
</em><br>
<em>&gt; possible to use digital computers to create an intelligence that
</em><br>
<em>&gt; DOESN'T rely critically on the emergent phenomena typically
</em><br>
<em>&gt; associated with biological complex dynamical systems.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Semi-similarly, the human mind/brain probably uses complex emergent
</em><br>
<em>&gt; phenomena to add 2+2 and get 4, but, a calculator doesn't, and a
</em><br>
<em>&gt; calculator does a better job of arithmetic anyway.
</em><br>
<em>&gt;
</em><br>
<em>&gt; One may argue that flexible, creative intelligence is fundamentally
</em><br>
<em>&gt; different than arithmetic, and is not achievable within limited
</em><br>
<em>&gt; computational resources except via complex, unpredictable emergent
</em><br>
<em>&gt; dynamics.  In fact I strongly SUSPECT this is true, but I
</em><br>
<em>&gt; haven't SHOWN that it's true in a convincing way, and I'm not
</em><br>
<em>&gt; 100% convinced it's true.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If you have a strong argument why this contention is true, I'd be
</em><br>
<em>&gt; very eager to hear it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; On the other hand, some others seems pretty sure that the
</em><br>
<em>&gt; opposite is true, and that it IS possible to achieve powerful
</em><br>
<em>&gt; intelligence under limited resources without requiring
</em><br>
<em>&gt; unpredictable emergent phenomena.
</em><br>
<em>&gt;
</em><br>
<em>&gt; However, I haven't seen any strong arguments in this
</em><br>
<em>&gt; direction either.
</em><br>
<em>&gt;
</em><br>
<em>&gt; -- Ben Goertzel
</em><br>
<em>&gt;
</em><br>
<p>Ben,
<br>
<p>In reply to your question, I'll see if I can outline my argument in more
<br>
detail than previously.
<br>
<p>[I am targeting this argument at people who actually understand what a
<br>
complex system is: I am beyond the point of trying to educate people who
<br>
not only do not understand, but are scornful of even making the effort
<br>
to understand, and who repeatedly produce false arguments based on
<br>
caricatures of what I and the complex systems people have claimed.]
<br>
<p>Preliminary Remark 1: The best I am going to be able to do is offer
<br>
convincing empirical reasons why the thesis is true; certain proof is
<br>
beyond reach, alas. So it will always be something of a judgement call
<br>
whether one accepts these arguments or not.
<br>
<p>*Overview*
<br>
<p>The overall direction of the argument is going to be this: that in all
<br>
the work on AGI to date, there has been relatively little emphasis on
<br>
getting the &quot;symbols&quot; [please interpret the word loosely: these are just
<br>
the basic representational units that encode the smallest chunks of
<br>
knowledge about the world] to be constructed entirely without programmer
<br>
intervention. In other words, we tend not to let our systems develop
<br>
their symbols entirely as a result of the interaction of a learning
<br>
mechanism with a stream of environmental input. Rather, we tend to put
<br>
&quot;ungrounded&quot; symbols in, which we interpret. The argument is going to be
<br>
that there are indications that this facet of an AGI (whatever apparatus
<br>
allows the symbols to be fully grounded) is going to be more important
<br>
than we suppose, and that it will introduce a great deal of complexity,
<br>
and that that, in turn, will be impossible to avoid.
<br>
<p>*Detailed Version of this Argument*
<br>
<p>We all know that in the Good Old Days, a lot of AI folks would build
<br>
systems in which they inserted simple tokens, labelled them &quot;arch&quot;
<br>
&quot;hand&quot; &quot;table&quot; &quot;red&quot; and so on, then wrapped a mechanism around those
<br>
symbols so the system could manipulate the symbols as a representation
<br>
of a world, and as a result dispaly some intelligent behavior (like, be
<br>
able to manipulate, and answer questions about, the placement of blocks
<br>
on a table).
<br>
<p>What we now know is that these kinds of systems had problems, not the
<br>
least of which was the fact that the symbols were not grounded: the
<br>
system never created the symbols itself, so it was up to the programmer
<br>
to interpret them manually (so to speak).
<br>
<p>What was the solution? Clearly, one part of the solution had to be to
<br>
give the system a learning mechanism, so it could build new symbols out
<br>
of simpler ones. Or rather, so it could build symbols from scratch, out
<br>
of real-world raw input of some kind. Or rather... well, you can see
<br>
that it isn't entirely clear, so let's unwrap this in a bit more detail.
<br>
<p>Question: Do we have to get the system to build all its symbols ab
<br>
initio, from a completely blank slate? Can't we give it some starter
<br>
symbols and ask it to develop from there? Surely it would be reasonable
<br>
if the system had some innate knowledge of the world, rather than none
<br>
at all? The consensus view on this questtion is that it should not be
<br>
necessary to go all the way back to signals coming from raw nerve
<br>
endings in eyes, ears, hands, etc, but that we should be able to put in
<br>
some primitive symbols and get the rest of them to be generated by the
<br>
system. (Aside: the connectionists were pretty much defined to be the
<br>
group that broke ranks at this point and insisted that we go down to
<br>
much deeper levels ... we will avoid talking about them for a moment,
<br>
however).
<br>
<p>So learning mechanisms should start with some simple symbols and create
<br>
more complex (more abstract) ones as a result of observing and
<br>
interacting with the world. Where do we draw the line, though? Which
<br>
primitives are acceptable, and which do we think are too high-level?
<br>
People disagree. Many who work in the machine learning field do not
<br>
really accept any constraints on the high-levelness of their primitive
<br>
symbols, and are happy to get any symbols to develop into any others,
<br>
not caring if the primitives look low-level enough that we can believe
<br>
they escape the grounding problem.
<br>
<p>We, however (from the perspective of this essay) care about the
<br>
grounding problem and see a learning mechanism as a way to justify our
<br>
usage of symbols: we believe that if we find a plausible learning
<br>
mechanism (or set of mechanisms) it would be capable of going all the
<br>
way from very primitive sensorimotor signals, or very primitive innate
<br>
symbols, all the way up to the most abstract symbols the system could
<br>
ever use. If we found something as plausible as that, we would believe
<br>
we had escaped the grounding problem.
<br>
<p>Next step. We notice that, whatever learning mechanism we put our money
<br>
on, it is going to complicate our symbols.
<br>
<p>[Terminology Note: I will use &quot;complicate&quot; to mean just what it seems,
<br>
and *nothing whatsoever* to do with &quot;Complex&quot; as in Complex Systems.
<br>
Hopefully this will avoid confusion. If I use &quot;complex&quot; it will mean &quot;as
<br>
in 'complex systems'&quot;].
<br>
<p>What do I mean by complicating our symbols? Only that if they are going
<br>
to develop, they need more stuff inside them. They might become &quot;frames&quot;
<br>
or &quot;scripts&quot; or they might be clusters of features, or they might have
<br>
prototypes stored in them.... whatever the details, there seems to be a
<br>
need to put more apparatus in a symbol if it is going to develop. Or, if
<br>
not passive data inside the symbol (the way I have implied so far), then
<br>
more mechanism inside it. It seems quite hard, for a variety of subtle
<br>
reasons, to build a good learning mechanism that involves utterly
<br>
simple, passive symbols (just a token, an activation level and links to
<br>
other tokens) and a completely separate learning mechanism that is
<br>
outside the symbols.
<br>
<p>Now, I might be being unfair here by implying that the reason our
<br>
symbols became more complicated was because, and only because, we needed
<br>
learning mechanisms. I don't think I really mean to put it that strongly
<br>
(and it might be interesting only to future historians of AI anyhow). We
<br>
had other reasons for making them more complicated, on of them being
<br>
that we wanted non-serial models of cognition in which less power was
<br>
centralized in a single learning mechanism and more power distributed
<br>
out to the (now active, not passive) symbols.
<br>
<p>So perhaps the best way to summarize is this: we found, for a variety of
<br>
reasons, that we were pushed toward more complicated mechanisms (and/or
<br>
data structures) inside our symbols, in order to get them to do more
<br>
interesting things, or in order to get over problems that they clearly had.
<br>
<p>This is a very subtle point, so although a lot of people reading this
<br>
will be right along with me, accepting all of this as obvious, I know
<br>
there are going to be some voices that dispute it. For that reason, I am
<br>
going to dwell on it for just a moment more.
<br>
<p>Sometimes you may think that you do not need complicated, active
<br>
symbols, and that in fact you can get away with quite simple structures,
<br>
allied with a sophisticated learning mechanism that builds new symbols
<br>
and connects them to other symbols in just the right, subtle way that
<br>
allows the system as a whole to be generally intelligent. In response to
<br>
this position, I will say that there is a trap here: you can always
<br>
rearrange one of my complicated-symbol systems to make it look as if the
<br>
symbols are actually simple (and maybe passive also), at the cost of
<br>
making the learning and thinking mechanism more complicated. You know
<br>
the kind of thing I mean: someone proposes that symbols should be active
<br>
neuron-like things, and then some anti-neural-net contrarian insists
<br>
that they can do the same thing with a centralised mechanism acting on a
<br>
matrix of passive data values. We have all seen these kinds of disputes,
<br>
so let's just cut through all the nonsense and point out that you can
<br>
always reformulate anything to look like anything else, and that some
<br>
types of formulation look more natural, more efficient and (generally)
<br>
more parsimonious. So when I argue that there is a tendency towards more
<br>
complicated symbols, I mean that the consensus intuition of the
<br>
community is that the simplest, most parsimonious AGI systems tend to
<br>
work with symbols that have more complicated apparatus inside them than
<br>
simply a token plus a couple other bits.
<br>
<p>So I want to wrap up all of this and put it in a Hypothesis:
<br>
<p>*Complicated Symbol Hypothesis*
<br>
<p>&nbsp;&nbsp;&quot;To the extent that AGI researchers acknowledge the need to capture
<br>
sophisticted learning capabilities in their systems, they discover that
<br>
they need their symbols to be more complicated.&quot;
<br>
<p>Corollary:
<br>
<p>&nbsp;&nbsp;&quot;The only people who believe that symbols do not need to be
<br>
complicated are the ones who are in denial about the need for learning
<br>
mechanisms.&quot;
<br>
<p>*** Please note the status of this claim. It is not a provable
<br>
contention: it is an observation about the way things have been going.
<br>
It is based on much thought and intuition about the different kinds of
<br>
AI systems and their faults and strengths. The term &quot;complicated&quot; is
<br>
open to quite wide interpretation, and it does not mean that there is an
<br>
linear growth on complicatedness as sophistication of learning mechanism
<br>
goes up, only that we seem to need relatively complicated symbols (as
<br>
compared with simple passive tokens with activation levels and simple
<br>
links) in order to capture realistic amounts of learning. ***
<br>
<p>But now, what of it? What does it matter that we need more stuff in the
<br>
symbols? How is this relevant to the original question about complexity
<br>
in AGI design?
<br>
<p>To answer this, I am going to try to unpack that idea of &quot;complicated
<br>
symbols&quot; to get at some of the details.
<br>
<p>When we observe humans going through the process of learning the
<br>
knowledge that they learn, we notice that they have some extremely
<br>
powerful mechanisms in there. I mean &quot;powerful&quot; in the sense of being
<br>
clever and subtle. They seem to use analogy a lot, for example. To get
<br>
some idea of the subtlety, just go back and look at the stuff Hofstadter
<br>
comes up with in GEB and in Metamagical Themas. When I talk about
<br>
&quot;learning&quot; I don't mean the restricted, narrow sense in which AI folks
<br>
usually talk about learning systems, I mean the whole shebang: the
<br>
full-up, flexible kind of learning that people engage in, where jumping
<br>
up a level of representation and pulling analogies around seems to be
<br>
the almost the norm, rather than the exception.
<br>
<p>Getting this kind of learning capability into an AGI is the goal, as far
<br>
as the present discussion is concerned. Anything less is not good
<br>
enough. I think we can all agree that there is a big gap between current
<br>
machine capabilities and the awesome generality of the human system.
<br>
<p>But how to do it? How do we close that gap?
<br>
<p>At this point, everyone has a different philosophy. I look at language
<br>
learning in a 2-to-6 year old child and I think I observe that when I
<br>
talk to that child, I can define pretty much anything in the whole world
<br>
by referring to loose examples and analogies, and the chold does an
<br>
astonishing job of getting what I am saying. I even define grammatical
<br>
subtleties that way, when teaching how to talk. But to someone like
<br>
Chomsky, Fodor or Pinker, this entire process may be governed by a
<br>
massive amount of innate machinery [and, yes, Fodor at least seems to
<br>
believe that innateness does not just apply to the grammatical machinery
<br>
but to most of the conceptual apparatus as well, with just a little
<br>
content filling in to be done during maturation].
<br>
<p>Then there are folks who don't go for the Chomskian innateness idea, but
<br>
who do insist that there is nothing wrong with our current ideas about
<br>
the basic format (data and mechanisms) inside symbols, all we need to do
<br>
is build a system that is big enough and fast enough, and connect it up
<br>
to sufficiently much real world input (and realistic motor output
<br>
systems) and it will develop the same rich repertoire of knowledge that
<br>
we see in humans. These people believe that all we need to do is take
<br>
symbols the way they are currently conceived, add some richer, improved,
<br>
to-be-determined learning mechanisms that browse on those symbols, and
<br>
all will eventually be well.
<br>
<p>So what do I think? I disagree with the position taken in that last
<br>
paragraph. Now I am now going to try to focus in on exactly why I disagree.
<br>
<p>Imagine a hypothetical AGI researcher who first decides what the format
<br>
of a symbol should be and then tries to hunt for a learning mechanism
<br>
that will allow symbols of that sort to develop as a result of
<br>
interaction with the world. Just for the sake of argument (which means
<br>
don't ask me to defend this!) let's be really naive and throw down some
<br>
example symbol structure: maybe each symbol is a token with activation
<br>
level, truth value, labelled connections to some other symbols (with the
<br>
labels coming from a fixed set of twenty possible labels) and maybe an
<br>
instance number. Who knows, something like that.
<br>
<p>First the researcher convinces themself that the proposed system can
<br>
work if given ungrounded, programmer-interpreted symbols. They knock up
<br>
a system for reasoning about a little knowledge domain and show that
<br>
given a stream of predigested, interpreted information, the system can
<br>
come to some interesting conclusions within the domain. Maybe the system
<br>
gets loaded on a spacecraft bound for the outer solar system and it can
<br>
&quot;think&quot; about some of the ship's technical troubles and come up with
<br>
strategies for resolving them. And it does a reasonable job, we'll suppose.
<br>
<p>So far, not much learning. It didn't learn about spaceraft repair from a
<br>
textbook, or from getting out a wrenc and trying to build spacecraft, or
<br>
from long conversations with the engineers, it was just preloaded with
<br>
symbols and information.
<br>
<p>But the researcher is hopeful, so they start adding learning mechanisms
<br>
in an attempt to get the system to augment itself. The idea is not just
<br>
to get it to add to its existing knowledge, but to start with less
<br>
knowledge and get to where it is now, by doing its own learning. We are,
<br>
after all, on a quest to eliminate most of that preloaded knowledge
<br>
because we want to ground the system in the real world.
<br>
<p>But as the researcher tries to devise more and more powerful kinds of
<br>
learning mechanisms, they discover a trend. Those more powerful
<br>
mechanisms need more complicated stuff inside the symbols. Let's suppose
<br>
that they are trying to get analogy to happen: they find that the
<br>
existing symbol structure is too limited and they need to add on a bunch
<br>
of extra doohickeys that represent..... well, they don't represent
<br>
anything that easily has a name at the symbol level, but they are needed
<br>
anyhow to get the system to do flexible, tangled kinds of stuff that
<br>
leads to the building of new symbols out of old.
<br>
<p>When and if the researcher tries to avoid this - tries to keep the
<br>
symbols nice and clean, like they were before - they discover something
<br>
rather annoying: the only way they can do this is to put more stuff in
<br>
the learning mechanisms (outside of the symbols) instead. Keep the
<br>
symbols clean, but make the (non-symbol-internal) learning mechanisms a
<br>
lot more complex. And then there is even more trouble, because it turns
<br>
out that the learning mechanism itself starts to need, not just more
<br>
machinery, but its own knowledge content! Now there are two places where
<br>
knowledge is being acquired: symbol system and learning engine. And they
<br>
don't talk, these two systems. In the process of trying to keep the
<br>
symbols clean and simple, the learning system had to invent new
<br>
strategies for learning, and (this is the real cause of the trouble)
<br>
some of those new learning mechanisms really seemed to be dependent on
<br>
the content of the world knowledge.
<br>
<p>Something difficult-to-explain is happening here. It is because learning
<br>
(knowledge acauisition and refinement) is apparently so flexible and
<br>
reflexive and tangled in humans, that we have reason to believe that the
<br>
(human) learning mechanism that is the generator of this behavior must
<br>
itself involve some quite tangled mechanisms.
<br>
<p>What does this seem to imply for the design of an AGI? It seems to
<br>
indicate that if we want a natural, parsimonious design, we are going to
<br>
inevitably head towards a type of system in which the symbols are
<br>
allowed to grow in a tangled way right from the outset, with knowledge
<br>
_about_ the world and knowledge about _how to understand the world_
<br>
being inextricably intertwined. And then, when we try to get those
<br>
systems to actually work with real world I/O, we will discover that we
<br>
have to add tweaks and mechanisms inside the symbols, and in the
<br>
surrounding architecture, to keep the system stable. And sooner or later
<br>
we discover that we have a system that seems to learn new concepts from
<br>
[almost] scratch quite well, but we have lost our ability to exactly
<br>
interpret what is the meaning of the apparatus inside and around the
<br>
symbols. We might find that there is no such thing as a symbol that
<br>
represents &quot;cup&quot;, there is only a cluster of units and operators that
<br>
can be used to stand for the cup concept wherever it is needed, and the
<br>
cluster manifests in different ways depending on whether we are picking
<br>
up a cup, describing a cup, trying to defining a cup, trying to catch a
<br>
falling cup, trying to design an artistic looking cup, and so on.
<br>
<p>In short, we may end up discovering, in the process of trying to get a
<br>
realistic set of human-like, powerul, recursive learning mechanisms to
<br>
actually work, that all the original apparatus we put in those symbols
<br>
becomes completely redundant! All the reasons the system now functions
<br>
might actually be enshrined in the extra apparatus we had to introduce
<br>
to (a) get it to learn powerfully and (b) get it to be stable in spite
<br>
of the tangledness.
<br>
<p>But wait, that sounds like a presumption on my part: why jump to the
<br>
conclusion that &quot;all the original apparatus we put in those symbols
<br>
becomes completely redundant&quot;? Why on earth should we believe this would
<br>
happen? Isn't this just a random piece of guesswork?
<br>
<p>The reason it is not a wild guess is that when the complicated learning
<br>
mechanisms were introduced, they were so tangled and recursive (with
<br>
data and mechanism being intertwined) that they forced the system away
<br>
from the &quot;simple system&quot; regime and smack bang in the middle of the
<br>
&quot;complex system&quot; regime. In other words, when you put that kind of
<br>
reflexivity and adaptiveness in such a system, it is quite likely that
<br>
the low level mechanisms needed to make its stable will look different
<br>
from the high-level behavior. We *want* something that approximates our
<br>
conventional understanding of symbols to appear in the top level
<br>
behavior - that is our design goal - and we want enormously powerful
<br>
adaptive mechanisms. The experience of teh complex systems community is
<br>
that you can't start with design goals and easily get mechanisms that do
<br>
that. And you especially cannot start with low level mechanisms that
<br>
look like the desired high-level ones, add a soupcon of adpativeness,
<br>
and then expect the high and low levels to still be the same.
<br>
<p>Now, the diehard AGI researcher listens to me say this and replies: &quot;But
<br>
I am not *trying* to emulate the messy human design: I believe that a
<br>
completely different design can succeed, involving fairly clean symbols
<br>
and a very limited amount of tangling. Just because humans do it that
<br>
way, doesn't mean that a machine has to.&quot;
<br>
<p>The reply is this. As you put more powerful learning mechanisms in your
<br>
designs, I see your symbols getting more complicated. I see an enormous
<br>
gap, still, between the power of human learning mechanisms and the power
<br>
of existing AGI mechanisms. There is a serious possibility that you can
<br>
only keep a clean, non-complex AGI design by steering clear of extremely
<br>
tangled, recursive, powerful knowledge acquisition mechanisms. And if
<br>
you steer clear of them, you may find that you never get the AGI to
<br>
actually work.
<br>
<p>To the complex systems person, observing the only known example of an
<br>
[non-A]GI (the human mind), it appears to be a matter of faith that real
<br>
learning can happen in a clean, non-complex AGI design. Their point of
<br>
view (my point of view) is: go straight for the jugular please, and
<br>
produce mechanisms of awesome, human level learning power, but *without*
<br>
sending the system into Complex territory, and I will have some reason
<br>
to believe it possible.
<br>
<p>At the moment, I see no compelling reason to believe, in the face of the
<br>
complexity I see in the human design.
<br>
<p>I am sure I could articulate this argument better. But that is my best
<br>
shot for the moment.
<br>
<p>Richard Loosemore.
<br>
<p>[end part 3]
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15814.html">Eliezer S. Yudkowsky: "Re: The Conjunction Fallacy Fallacy  [WAS Re: Anti-singularity spam.]"</a>
<li><strong>Previous message:</strong> <a href="15812.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 2"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15813">[ date ]</a>
<a href="index.html#15813">[ thread ]</a>
<a href="subject.html#15813">[ subject ]</a>
<a href="author.html#15813">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:57 MDT
</em></small></p>
</body>
</html>
