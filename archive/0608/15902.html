<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Conscious of the parts</title>
<meta name="Author" content="Olie Lamb (neomorphy@gmail.com)">
<meta name="Subject" content="Re: Conscious of the parts">
<meta name="Date" content="2006-08-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Conscious of the parts</h1>
<!-- received="Tue Aug 29 22:27:37 2006" -->
<!-- isoreceived="20060830042737" -->
<!-- sent="Wed, 30 Aug 2006 14:27:07 +1000" -->
<!-- isosent="20060830042707" -->
<!-- name="Olie Lamb" -->
<!-- email="neomorphy@gmail.com" -->
<!-- subject="Re: Conscious of the parts" -->
<!-- id="f3afeba0608292127s49750b4cw378a9e277398bdc1@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="001e01c6cb7d$134223d0$c7084e0c@MyComputer" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Olie Lamb (<a href="mailto:neomorphy@gmail.com?Subject=Re:%20Conscious%20of%20the%20parts"><em>neomorphy@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Aug 29 2006 - 22:27:07 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15903.html">Eliezer S. Yudkowsky: "Re: How to resolve the dispute"</a>
<li><strong>Previous message:</strong> <a href="15901.html">Eliezer S. Yudkowsky: "Re: The Conjunction Fallacy Fallacy [WAS Re: Anti-singularity spam.]"</a>
<li><strong>In reply to:</strong> <a href="15875.html">John K Clark: "Re: Conscious of the parts"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15906.html">John K Clark: "Re: Conscious of the parts"</a>
<li><strong>Reply:</strong> <a href="15906.html">John K Clark: "Re: Conscious of the parts"</a>
<li><strong>Reply:</strong> <a href="15907.html">John K Clark: "Re: Conscious of the parts"</a>
<li><strong>Reply:</strong> <a href="15910.html">Charles D Hixson: "Re: Conscious of the parts"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15902">[ date ]</a>
<a href="index.html#15902">[ thread ]</a>
<a href="subject.html#15902">[ subject ]</a>
<a href="author.html#15902">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Note the difference between the words &quot;would&quot; and &quot;could&quot;
<br>
<p>John K Clark &lt;<a href="mailto:jonkc@att.net?Subject=Re:%20Conscious%20of%20the%20parts">jonkc@att.net</a>&gt; wrote:
<br>
<em>&gt;&gt; A very powerful AI may continue it's growth exponentially until certain
</em><br>
<em>&gt;&gt; point, which is beyond our current capability of understanding,
</em><br>
<em>&gt;
</em><br>
<em>&gt; OK, sounds reasonable.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; where it concludes that it's best to stop
</em><br>
<em>&gt;
</em><br>
<em>&gt;Huh? How do you conclude that this un-knowable AI would conclude that it
</em><br>
<em>&gt;would be best if it stopped improving itself?
</em><br>
<p>Not would.  Could.
<br>
<p>Let's start with the basics:
<br>
<p>There is a large set of possible goals for an intelligent entity to
<br>
have.   A number of us happen to think that no particular goal is
<br>
required for intelligence.  Some people frequently assert that some
<br>
goals are &quot;necessary&quot; for any intelligence.  I've yet to have
<br>
difficulty finding a counterexample, but I'm not quite sure how to go
<br>
about demonstrating my contention...
<br>
<p>*Calls for set-logic assistance*
<br>
<p>A commonly-presumed-subset of possible goals includes the goal set:
<br>
&quot;Gain &amp; retain control of X&quot;  X may be any of a number of things.  In
<br>
particular, it may be an area of space.
<br>
<p>If, say, retaining control of a set area of space for a given duration
<br>
was incompatible with expanding the space over which one had control,
<br>
the best satisfaction of the goal set could be achieved by not
<br>
expanding the sphere of influence.
<br>
<p>Example:
<br>
<p>Imagine for a moment an intelligence in an area of limited resources -
<br>
say, one stuck on a rock a very very long way from any other materials
<br>
(extra-galactic+ long way).  That intelligence has discovered that it
<br>
can continue operating for a very long time at a given intelligence
<br>
(Computations per second) &quot;level&quot;.  However, by consuming the
<br>
available energy on the rock at a faster rate, it would be able to
<br>
increase its its processing ability.
<br>
<p>Would it be reasonable for that intelligence to increase its
<br>
computation rate, in the hope that it might be able to think itself
<br>
out of its predicament?  Or /might/ it consider sticking with what it
<br>
had for the time being?
<br>
<p>Or, perhaps you think that &quot;improve&quot; should be defined in such a way
<br>
that it means a reduction in computing power and problem solving
<br>
ability is an improvement?
<br>
<p><em>&gt; Is it common for intelligent
</em><br>
<em>&gt;entities to decide that they don't want more control of the universe?
</em><br>
<p><a href="http://en.wikipedia.org/wiki/Parinirvana">http://en.wikipedia.org/wiki/Parinirvana</a>
<br>
<p>It could be inferred that some 700 million people decided just so.  Or
<br>
at least a fair percentage of them.  Most of them relatively sane, and
<br>
a lot more intelligent than is required for using symbolic logic.  I
<br>
don't know how many humans you would need to match your definition of
<br>
&quot;common&quot;...
<br>
<p>On 8/30/06, John K Clark &lt;<a href="mailto:jonkc@att.net?Subject=Re:%20Conscious%20of%20the%20parts">jonkc@att.net</a>&gt; wrote:
<br>
<em>&gt; &quot;Ricardo Barreira&quot; &lt;<a href="mailto:rbarreira@gmail.com?Subject=Re:%20Conscious%20of%20the%20parts">rbarreira@gmail.com</a>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; How do you even know the AI will want any control at all?
</em><br>
<em>&gt;
</em><br>
<em>&gt; If the AI exists it must prefer existence to non existence
</em><br>
<p>Prove it!
<br>
<p>Not for an ideal superintelligence.  Prove that ANY intelligence must
<br>
prefer existence to non-existence.
<br>
<p>I think I can imagine a few counterexamples, thus disproving the contention.
<br>
<p>(Nb: I have noted your comments to )
<br>
<em>&gt; &gt; I challenge you to prove otherwise
</em><br>
<em>&gt; Prove? This isn't high school geometry, I can't prove anything about a
</em><br>
<em>&gt; intelligence far far greater than my own;
</em><br>
<p><em>&gt; and after that
</em><br>
<em>&gt; it is a short step, a very short step, to what Nietzsche called &quot;the will to
</em><br>
<em>&gt; power&quot;.
</em><br>
<p>And, of course, Nietzsche is the icon of understanding
<br>
intelligences-in-general, like, say, women... *rolls eyes*
<br>
<p><p>--Olie
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15903.html">Eliezer S. Yudkowsky: "Re: How to resolve the dispute"</a>
<li><strong>Previous message:</strong> <a href="15901.html">Eliezer S. Yudkowsky: "Re: The Conjunction Fallacy Fallacy [WAS Re: Anti-singularity spam.]"</a>
<li><strong>In reply to:</strong> <a href="15875.html">John K Clark: "Re: Conscious of the parts"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="15906.html">John K Clark: "Re: Conscious of the parts"</a>
<li><strong>Reply:</strong> <a href="15906.html">John K Clark: "Re: Conscious of the parts"</a>
<li><strong>Reply:</strong> <a href="15907.html">John K Clark: "Re: Conscious of the parts"</a>
<li><strong>Reply:</strong> <a href="15910.html">Charles D Hixson: "Re: Conscious of the parts"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15902">[ date ]</a>
<a href="index.html#15902">[ thread ]</a>
<a href="subject.html#15902">[ subject ]</a>
<a href="author.html#15902">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:57 MDT
</em></small></p>
</body>
</html>
