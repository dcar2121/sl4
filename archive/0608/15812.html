<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Loosemore's Collected Writings on SL4 - Part 2</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Loosemore's Collected Writings on SL4 - Part 2">
<meta name="Date" content="2006-08-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Loosemore's Collected Writings on SL4 - Part 2</h1>
<!-- received="Sat Aug 26 20:38:10 2006" -->
<!-- isoreceived="20060827023810" -->
<!-- sent="Sat, 26 Aug 2006 22:35:06 -0400" -->
<!-- isosent="20060827023506" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Loosemore's Collected Writings on SL4 - Part 2" -->
<!-- id="44F104DA.9030204@lightlink.com" -->
<!-- charset="windows-1252" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Loosemore's%20Collected%20Writings%20on%20SL4%20-%20Part%202"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Sat Aug 26 2006 - 20:35:06 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="15813.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 3"</a>
<li><strong>Previous message:</strong> <a href="15811.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 1"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15812">[ date ]</a>
<a href="index.html#15812">[ thread ]</a>
<a href="subject.html#15812">[ subject ]</a>
<a href="author.html#15812">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
[begin part 2]
<br>
<p>************************************************************
<br>
*                                                          *
<br>
* The Complex Systems Critique of AI                       *
<br>
*                                                          *
<br>
************************************************************
<br>
<p>*Complex Adaptive Systems* (aka &quot;Complex Systems&quot;)
<br>
<p>If one builds systems that are composed of many (more or less identical)
<br>
elements, each of which is relatively simple, but able to do some
<br>
moderately interesting amount of computation (with messages being
<br>
exchanged between them, some influences coming in from outside and
<br>
adaptation going on), then one observes that such systems sometimes
<br>
exhibit interesting behaviors, as follows.
<br>
<p>Sometimes they evolve in chaotic ways. In fact, *usually* they evolve in
<br>
chaotic ways. Not interesting.
<br>
<p>Sometimes they head straight toward a latchup state after being switched
<br>
on, and stay there. Not chaos, just boring.
<br>
<p>An interesting subset (those sometimes referred to as being &quot;on the edge
<br>
of chaos&quot;) can show very ordered behavior. These are Complex Systems.
<br>
Capital &quot;C&quot;, notice, to distinguish them from &quot;complex&quot; in the sense of
<br>
merely complicated.
<br>
<p>What is interesting about these is that they often show global
<br>
regularities that do not appear to be derivable (using any form of
<br>
analytic mathematics) from the local rules that govern the unit
<br>
behaviors. This is what a CAS (&quot;Complex Adaptive Systems&quot;) person would
<br>
refer to as &quot;emergent&quot; behaviors. More than that, some of these global
<br>
regularities appear to be common to many types of CAS. In other words,
<br>
you can build alll sorts of systems with enormously different local
<br>
rules and global architectures, and the same patterns of global behavior
<br>
seem to crop up time and again.
<br>
<p>What to conclude from this?
<br>
<p>First, that bit about &quot;do not appear to be derivable (using any form of
<br>
analytic mathematics)&quot; is something that a lot of people have thought
<br>
deeply about .... this is no mere statement of inability, but a profound
<br>
realization about what it means to do math. Namely: if you look at
<br>
Mathematics as a whole you can see that the space of soluble, analytic,
<br>
tractable problems is and always has been a pitiably small corner of the
<br>
space of all possible systems. It is trivially easy to write down
<br>
equations (or systems, speaking more generally) that are completely
<br>
intractable. The default assumption made by some people is that
<br>
Mathematics as a domain of inquiry is gradually pushing back the
<br>
frontiers and that in an infinite universe there may come a time when
<br>
all possible problems (equations/systems) become tractable (i.e.
<br>
analytically solvable) BUT there is a substantial body of thought,
<br>
especially post-Godel, that believes that those systems are not just
<br>
difficult to solve, but actually impossible. When I talk about &quot;the
<br>
limitations of mathematics&quot; I mean precisely that point of view.
<br>
<p>All that the CAS people did was to come up with some fabulously
<br>
interesting types of regularity (the emergent properties of Complex
<br>
Adaptive Systems), and then point out that the tractability of the
<br>
problem of accounting for these regularities is way, way, way beyond
<br>
anything else. They allude to a philosophical/methodological position in
<br>
the math community, not to mere &quot;difficulty&quot;. Heck, if there are
<br>
nonlinear DEs that the math folks declare to be &quot;ridiculously hard and
<br>
probably impossible to solve&quot;, then what are these Complex Systems,
<br>
which are a gazillion times more complex?
<br>
<p>Take the regularities observed in one of the most trivial systems that
<br>
we can think about, Conway's Life. Can we find a set of equations that
<br>
will generate the &quot;regular&quot; forms that emerge in that game? All of the
<br>
regular forms, not just some. We should plug in the algorithm that
<br>
defines the game, and out the other end should come descriptions of the
<br>
glider guns etc. Maybe there are optimists who think this is possible.
<br>
There are many people, I submit, who consider this kind of solution to
<br>
be impossible. The function that generates regularities given local
<br>
rules, in the Comway system, is *never* going to be found. It does not
<br>
exist.
<br>
<p>What is the relevance for AI?
<br>
<p>When people try to cook up formalisms that are supposed to be the core
<br>
of an intelligence, they often refer to systems of interacting parts in
<br>
which they (the designers) think they know (a) what the parts look like
<br>
and (b) how the parts interact and (c) what the system architecture and
<br>
environmental input/output connection amounts to. A CAS person looks at
<br>
these systems and says &quot;Wait, that's a recipe for Complexity&quot;. And what
<br>
they mean is that the designer may *think* that a system can be built
<br>
with (e.g.) bayesian local rules etc., but until they actually build a
<br>
complete working version that grows up whilst interacting with a real
<br>
environment, it is by no means certain that what they will get globally
<br>
is what they thought they were going to get when they invented the local
<br>
aspects of the design. In practice, it just never works that way. The
<br>
connection between local and global is not usually very simple.
<br>
<p>So you may find that if a few well-structured pieces of knowledge are
<br>
set up in the AGI system by the programmer, the Bayesian-inspired local
<br>
mechanism can allow the system to hustle along quite comfortably for a
<br>
while .... until it gradually seizes up. To bring in an analogy here,
<br>
the Complex Systems person would say this is like trying to tile a
<br>
gently curved noneuclidean space .... it looks euclidean on a local
<br>
scale, but it would be a mistake to think you can tile it with a
<br>
euclidean pattern.
<br>
<p>(This is a more general version of what was previously called the
<br>
Grounding Problem, of course).
<br>
<p>So this is the lesson that the CAS folks are trying to bring to the
<br>
table. (1) They know that most of the time when someone puts together a
<br>
real system of interacting, adaptive units, there can be global
<br>
regularities that are not identical to the local mechanisms. (2) They
<br>
see AGI people coming up with proposals regarding the mechanisms of
<br>
thought, but those ideas are inspired by certain aspects of what the
<br>
high-level behavior *ought* to be (e.g. Bayesian reasoning), and the AGI
<br>
people often talk as if it is obvious that these are also the underlying
<br>
local mechanisms...... but this jump from local to global is simply not
<br>
warranted!
<br>
<p>I want to conclude by quoting one extract from your [Yudkowsky’s]
<br>
message that sums up the whole argument:
<br>
<p>[Richard Loosemore wrote:] “Like Behaviorists and Ptolemaic Astronomers,
<br>
they mistake a formalism that approximately describes a system for the
<br>
mechanism that is actually inside the system. They can carry on like
<br>
this for centuries, adding epicycles onto their models in order to
<br>
refine them. When Bayesian Inference does not seem to cut it, they
<br>
assert that *in principle* a sufficiently complex Bayesian Inference
<br>
system really would be able to cut it ... but they are not able to
<br>
understand that the &quot;in principle&quot; bit of their argument depends on
<br>
subtleties that they don't think much about.”
<br>
<p>[Eliezer Yudkowsky wrote:] “There are subtleties to real-world
<br>
intelligence that don't appear in standard Bayesian decision theory (he
<br>
said controversially), but Bayesian decision theory can describe a hell
<br>
of a lot more than naive students think. I bet that if you name three
<br>
subtleties, I can describe how Bayes plus expected utility plus
<br>
Solomonoff (= AIXI) would do it given infinite computing power.”
<br>
<p>You make my point for me. The Ptolemaic astronomers would have used
<br>
exactly the same argument that you do: &quot;Name some subtle ways in which
<br>
the heavenly bodies do not move according to the standard set of
<br>
epicycles, and I can describe how an infinite number of epicycles would
<br>
do it....&quot; Yes, yes yes! But they were wrong, because the *real*
<br>
mechanism for planetary movement was not actually governed by epicycles,
<br>
it was governed by something completely different, and all the Ptolemaic
<br>
folks were barking up the wrong tree when though their system was in
<br>
principle capable of covering the data.
<br>
<p>I have not said exactly how to proceed from here on out (although I do
<br>
have many thoughts to share with people about how, given the above
<br>
situation, we should really try to do AI), because at the moment all I
<br>
am trying to establish is that there is a big, serious problem, coming
<br>
in from the Complex Systems community, that says that this Bayesian kind
<br>
of approach (along with many others) to building an AGI is based on
<br>
faith and wishful thinking.
<br>
<p>And a vital corollary to the above arguments about how to build an AGI
<br>
is the fact that _absolutely guaranteeing_ a Friendly AI is impossible
<br>
the way you are trying to do it. If AGI systems that actually work are
<br>
Complex (and all the indications are that they are indeed Complex), then
<br>
guarantees are impossible. It's a waste of time to look for absolute
<br>
guarantees. (Other indications of Friendliness .... now that's a
<br>
different matter).
<br>
<p>These points are so crucial to the issues being discussed on this list,
<br>
that at the very least they need to be taken seriously, rather than
<br>
dismissed out of hand by people who are unbelievably scornful of the
<br>
Complex Systems community. That was the reason that I originally sent
<br>
the &quot;Retrenchment&quot; post.
<br>
<p>Richard Loosemore
<br>
<p>P.S.
<br>
<p>I am not trying to be definitive and say that a full Bayesian AGI *must*
<br>
be complex, and therefore have all the problems outlined earlier. I only
<br>
mean that there are overwhelming indications, at the moment, that it
<br>
would be complex, etc. etc. I am trying to get us all to agree that
<br>
there is an issue of enormous importance here, and I am trying to state
<br>
the issue as clearly as I can, so we can discuss it further. All my
<br>
energy at the moment is being used to combat denials or
<br>
misunderstandings of the very existence of the issue.
<br>
<p>Hey, at the end of the day, it might not be the case. That would be
<br>
extraordinarily interesting.
<br>
<p>RL.
<br>
<p>************************************************************
<br>
*                                                          *
<br>
* Complex Systems vs. [the Bayesian Approach to] AI        *
<br>
*                                                          *
<br>
************************************************************
<br>
The attack I am making is that there is an array of subtle faults that
<br>
the Bayesian approach cannot get out of *unless* it makes recourse to
<br>
e.g. infinite computing power, or postulates that a Bayesian AGI would
<br>
have a certain freakish characteristic (outlined below).
<br>
<p>This argument that I am presenting is at the paradigm level.
<br>
<p>So, one of those subtleties is that the Complex Systems folks are out
<br>
there saying, in effect:
<br>
<p>******* Quote from a hypothetical CAS theorist *******
<br>
<p>&quot;Wait: have you tried to put together a complete Bayesian system that
<br>
understands and reasons about the world, *and* which can acquire its own
<br>
knowledge, ground its own concepts and interact with the world? The
<br>
reason we ask is that we have studied vast numbers of systems that are
<br>
adaptive, and we have noticed a trend: their global behavior tends to be
<br>
very different from their local mechanisms. (Indeed, the global seems to
<br>
be impossible to derive from the local.) The way this applies to your
<br>
specific case is that you want your AGI system to have certain global
<br>
features (like the ability to understand the world, learn new knowledge,
<br>
etc.) but you are trying to build it using local mechanisms that look
<br>
very much like something that you are hoping to get at the global level
<br>
(the system as a whole is supposed to have a global reasoning capacity
<br>
that is Bayesian in character). If you succeed in doing this - having
<br>
Bayesian global behavior *and* Bayesian local mechanisms - then the
<br>
folks in the CAS community will want to know about it, because you will
<br>
have produced a system that is utterly unique: it will have the same
<br>
behavior at both global and local level. In all our experience, we have
<br>
never seen such a thing: complex systems just don't do that!
<br>
<p>&quot;P.S. Please don't send us any more proofs or demonstrations or
<br>
arguments that your strict Bayesian core ideas have no Complexity in
<br>
them; that they are just like some conventional, complicated computer
<br>
program of the sort that exist today. It might well be true that your
<br>
core Bayesian ideas are that predictable, but that means nothing until
<br>
you say exactly how a real computing system (not some fantasy with
<br>
infinite computing power!) would implement a full AGI, one that also
<br>
includes enough apparatus to establish a mapping from internal
<br>
representation to external referents as a result of its learning system
<br>
and its particular sensorimotor system. It is precisely in this extended
<br>
system that all the Complexity is likely to be, because it is here that
<br>
the system has its most reflexive, adaptive components, and whenever we
<br>
have watched people insert those kinds of adaptive mechanisms in a
<br>
system, it either becomes (a) chaotic, (b) dead, or (c) complex.
<br>
<p>&quot;P.P.S. If you don't accept that an extended version of your Bayesian
<br>
AGI, in the sense just described, would actually need any Complex
<br>
Adaptive low-level mechanisms in the extended portion, then show us such
<br>
a system that actually works. Show us some really juicy, believable
<br>
creation of new, more and more abstract &quot;concepts&quot; by the system, in
<br>
domains of knowledge far removed from its original programming, where
<br>
those concepts arise from interaction with a non-trivial environment ...
<br>
and do all of this without inserting any code that would render it
<br>
liable to get Complex. Don't just claim that it will work, show it
<br>
actually happening in a real system. Don't just claim that it is obvious
<br>
that you could do it: prove it.&quot;
<br>
<p>******* end hypothetical quote *******
<br>
<p>Here, then, is the Complex Systems equivalent of the &quot;simple and
<br>
elegant&quot; appeal that Copernicus made: Your fully implemented Bayesian
<br>
AGI, if it could be built, would be a Complex System (in the technical
<br>
sense), but it would also have a direct relationship between its global
<br>
behavior and its local mechanisms, and there is a massive body of
<br>
evidence that such a system would be an utter freak of a complex system.
<br>
We don't believe in freaks: Occam's Razor, plain and simple. If you
<br>
believe, today, that you can build such a freakish thing, you are going
<br>
on blind faith.
<br>
<p>This argument is at the paradigm level. It is not that there is some
<br>
specific system or equation, devised by someone working down at Santa
<br>
Fe, that can beat out your Bayesian formalism, it's that everything
<br>
going on at Santa Fe indicates that the paradigm on which the Bayesian
<br>
approach rests is flawed because of a &quot;religious&quot; belief that a Bayesian
<br>
AGI would be exempt from the observed characteristics of all other
<br>
compex systems.
<br>
<p>&nbsp;&nbsp;*Afterword*
<br>
<p>Am I trying to say that an AGI cannot be built at all? NO! I believe
<br>
there are other approaches that will let us do it, but I refuse to be
<br>
drawn into that discussion until such time as the above argument is
<br>
actually understood. There are some very interesting and deep
<br>
discussions that we could all be having about what to do next under such
<br>
circumstances (I want to have those discussions!), but I have to
<br>
communicate the above argument fully, and get it out of the way, before
<br>
there is any hope of going on to the next step.
<br>
<p><p>[end part 2]
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="15813.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 3"</a>
<li><strong>Previous message:</strong> <a href="15811.html">Richard Loosemore: "Loosemore's Collected Writings on SL4 - Part 1"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#15812">[ date ]</a>
<a href="index.html#15812">[ thread ]</a>
<a href="subject.html#15812">[ subject ]</a>
<a href="author.html#15812">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:57 MDT
</em></small></p>
</body>
</html>
