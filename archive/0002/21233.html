<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [SL4] Sinking the Boat</title>
<meta name="Author" content="DaleJohnstone@email.com (DaleJohnstone@email.com)">
<meta name="Subject" content="Re: [SL4] Sinking the Boat">
<meta name="Date" content="2000-02-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [SL4] Sinking the Boat</h1>
<!-- received="Wed Feb  9 23:04:21 2000" -->
<!-- isoreceived="20000210060421" -->
<!-- sent="Thu, 10 Feb 2000 04:33:05 GMT" -->
<!-- isosent="20000210043305" -->
<!-- name="DaleJohnstone@email.com" -->
<!-- email="DaleJohnstone@email.com" -->
<!-- subject="Re: [SL4] Sinking the Boat" -->
<!-- id="38a61830.28514899@smtp.screaming.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="yam8074.2952.21628424@relay.f9.net.uk" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> <a href="mailto:DaleJohnstone@email.com?Subject=Re:%20[SL4]%20Sinking%20the%20Boat"><em>DaleJohnstone@email.com</em></a><br>
<strong>Date:</strong> Wed Feb 09 2000 - 21:33:05 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="21234.html">Marc Forrester: "[SL4] Abandon Ship"</a>
<li><strong>Previous message:</strong> <a href="21232.html">Marc Forrester: "[SL4] Sinking the Boat"</a>
<li><strong>In reply to:</strong> <a href="21232.html">Marc Forrester: "[SL4] Sinking the Boat"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21234.html">Marc Forrester: "[SL4] Abandon Ship"</a>
<li><strong>Reply:</strong> <a href="21234.html">Marc Forrester: "[SL4] Abandon Ship"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21233">[ date ]</a>
<a href="index.html#21233">[ thread ]</a>
<a href="subject.html#21233">[ subject ]</a>
<a href="author.html#21233">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
From: <a href="mailto:DaleJohnstone@email.com?Subject=Re:%20[SL4]%20Sinking%20the%20Boat">DaleJohnstone@email.com</a>
<br>
<p><p><em>&gt;&gt; I don't think it's safe to assume that our 'moral' behaviour is
</em><br>
<em>&gt;&gt; the most optimal and that anything else puts a limit on potential.
</em><br>
<em>&gt;&gt; Probably a highly selfish, shoot first mentality would be.  Although
</em><br>
<em>&gt;&gt; a society of such creatures wouldn't flourish, but the military
</em><br>
<em>&gt;&gt; certainly wouldn't care about that.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Precisely so.  Whereas we do.  We want to produce Minds that -can-
</em><br>
<em>&gt;flourish in a society, and that can be a part of our society in order
</em><br>
<em>&gt;to bootstrap them into their own.  Intelligence -needs- a society,
</em><br>
<em>&gt;because society is one of the most complex and extelligent worlds a
</em><br>
<em>&gt;developing mind can play in.  How smart would Einstein have grown to
</em><br>
<em>&gt;be if he was 'raised' as a lab-rat by some alien species?
</em><br>
<p>What we want and what the military want are two different things. That
<br>
doesn't mean they're mutually exclusive. We can't design them to be
<br>
'good', or rather we can, but they could easily be changed.
<br>
<p>I agree intelligence would do better in a richer environment. I
<br>
wouldn't go so far as to say it can't happen without it.
<br>
<p>I'm sure as lab-rat of an alien species Einstein would have a most
<br>
stimulating time :)
<br>
<p><em>&gt;&gt; As for redesigning itself, you're assuming this isn't a fundamental
</em><br>
<em>&gt;&gt; part of it's intelligent design in the first place. My money would be
</em><br>
<em>&gt;&gt; on some form of self-modification at some level to enable intelligent
</em><br>
<em>&gt;&gt; behaviour.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Some form, yes, but free to completely rebuild itself from the most
</em><br>
<em>&gt;fundamental drives upward?  Not a goal of smart weapon research.
</em><br>
<em>&gt;You don't want your warplanes getting -too- smart and asking
</em><br>
<em>&gt;dangerous questions like &quot;What's in it for me?&quot;
</em><br>
<p>Human level intelligence doesn't automatically imply some sense of
<br>
self-preservation. Without evolution's constraints you could make even
<br>
stranger monsters. They would most certainly try to avoid
<br>
uncontrollable AIs but the issue of how intelligent is separate.
<br>
<p><em>&gt;&gt; Again I don't think you can design in any safeguards again 'irrational
</em><br>
<em>&gt;&gt; drives'.  Asimovs Laws wouldn't work, and even if they did they could
</em><br>
<em>&gt;&gt; be changed.  Once you understand how to build minds you can bias them
</em><br>
<em>&gt;&gt; quite easily.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I think the safeguards are built into the universe.  If one group
</em><br>
<em>&gt;create a mind full of irrational and contradictory instincts for use
</em><br>
<em>&gt;as a weapon, and another build a saner mind as a sibling, friend,
</em><br>
<em>&gt;and child, and encourage it to grow in all ways, which mind is
</em><br>
<em>&gt;going to be the smartest, going by the universal intelligence
</em><br>
<em>&gt;test of survival ability?
</em><br>
<p>If any I'd say there's only one safeguard built into the universe and
<br>
that's Existence. What's good at existing exists longer. What tries to
<br>
exist will be more likely to exist.
<br>
This can work on many levels. If you promote the existence of others
<br>
(who also promote existence) then you're all likely to be better off,
<br>
because of richer interaction possibilities that improve the group.
<br>
I'm basically an optimist about non-human intelligence.
<br>
<p>Things get bitchy when there's limited resources. Most wars are about
<br>
resource squabbles, or triggered by the evolved behaviour attached to
<br>
that (tribalism, racism, nationalism, xenophobia etc).
<br>
<p>It's my sincere hope that nanotechnology will make these behaviours
<br>
redundant with enough resources for all. Humans have the capability to
<br>
be unbelievably stupid though (religion for example). Let's hope
<br>
nanotech and/or AI can fix the human condition before we really screw
<br>
up.
<br>
<p><p><em>&gt;Quite so.  An open source Singularity project would not be hugely
</em><br>
<em>&gt;useful to them, the primary concern is that they must not be allowed
</em><br>
<em>&gt;to stop it.  Certainly, they have resources.  The critical difference
</em><br>
<em>&gt;between us and them, I think, is that they won't encourage their 'smart'
</em><br>
<em>&gt;missiles to read and play.  They feel no kinship.
</em><br>
<p>My argument was that the project *would* suddenly become useful to
<br>
them if it succeeded, or was about to. They also have the fastest
<br>
machines. They could catch up and overtake us while simultaneously
<br>
closing us down (or making it bloody difficult), all in the name of
<br>
national security.
<br>
<p>What I'm most unclear about is the period between a human-level AI
<br>
being built (or trained), and the runaway singularity process. 
<br>
<p>Assuming a non-nanotech world, this isn't a instant process.
<br>
You could mass produce clones of your first working AI and replace
<br>
most human jobs, including ours. Then the feedback loop is complete.
<br>
Whoever has the fastest machines will determine who first sees the
<br>
fruits of the first iteration. That's most likely to be some well
<br>
equipped agency like DARPA, and the fruits will most likely be
<br>
nanotech. They wouldn't want a foreign power getting there first
<br>
(everyone has AIs remember).
<br>
So then we have a situation where because of their fast computers
<br>
they're first with nanotech and they know Iraq, North Korea, and China
<br>
etc will have it soon (maybe in an hour, maybe in a month, who knows
<br>
how much CPU power they each have).
<br>
This strikes me as a f**king dangerous scenario.
<br>
<p>The nanotech before AI scenario is starting to look more appealing to
<br>
me. Nanobots will be designed in a simulated environment which is safe
<br>
to screw up. Lessons can be learnt from virtual mistakes.
<br>
<p>Yet I suppose the side that gets nanotech first also has the most time
<br>
to research and build defenses. However that might be considerably
<br>
more difficult than a basic weapon.
<br>
<p>Urgh... this is all very depressing stuff. There has to be a safe path
<br>
through this mess or we're all screwed.
<br>
<p><em>&gt;&gt; I'm not sure I like the idea of changing the rules from under people.
</em><br>
<em>&gt;&gt; That sounds very destabilizing.  You preferably want to keep the
</em><br>
<em>&gt;&gt; balance of power level and not rock the boat so much it sinks.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Ah, no, not really.  The driving force behind Singularitarianism,
</em><br>
<em>&gt;(Which is far too long a word, BTW :) is that the boat is already
</em><br>
<em>&gt;sinking, the world is full of hatred, stupidity, destitution and
</em><br>
<em>&gt;agonies, the 'civilised' nations are rapidly changing from the
</em><br>
<em>&gt;imperfect Republics that they were into de-facto Feudal
</em><br>
<em>&gt;Aristocracies that laughingly call themselves Democracy,
</em><br>
<em>&gt;and physical science is charging forwards far ahead of
</em><br>
<em>&gt;human maturity.  And we thought the Cold War was scary.
</em><br>
<p>People can be stupid, the boat may already be sinking, things have to
<br>
change, but changing the rules from under people sounds like
<br>
extremism, and that's what we're trying to avoid.
<br>
<p><em>&gt;&gt; A stable increase in the intelligence of AIs would be great,
</em><br>
<em>&gt;&gt; but I think it'll happen as a breakthrough.  Hopefully the hardware
</em><br>
<em>&gt;&gt; limitations will cushion the blow so people can see the singularity
</em><br>
<em>&gt;&gt; growing and prepare for it, instead of crapping themselves and
</em><br>
<em>&gt;&gt; doing something stupid.
</em><br>
<em>&gt;
</em><br>
<em>&gt;A breakthrough, and then a whole series of ever-faster breakthroughs,
</em><br>
<em>&gt;researched by the AI Minds themselves.  Singularity is essentially a
</em><br>
<em>&gt;sudden and irrevocable boat-sinking change in the rules that will
</em><br>
<em>&gt;inevitably occur as soon as intelligent minds arise with the ability
</em><br>
<em>&gt;to design their own upgrades.  It doesn't actually have to be AI,
</em><br>
<em>&gt;with nanotech, transhumans will do it to themselves, but nanotech
</em><br>
<em>&gt;is too dangerous without posthuman intelligence, and AI is something
</em><br>
<em>&gt;we can start serious, effective work on right now.
</em><br>
<p>We don't want boat-sinking change. That doesn't help anyone.
<br>
Boat-sinking means we've lost and lots of people die.
<br>
We need better plans than to offload it onto some future entity.
<br>
<p><em>&gt;People panicking doing something stupid, now, there is the greatest
</em><br>
<em>&gt;danger.  An open, distributed project is one powerful defense against
</em><br>
<em>&gt;such things.  Likeable, human (Or transhuman, or posthuman) AI Minds
</em><br>
<em>&gt;with a sense of humour and a pleasant speaking voice would be another.
</em><br>
<em>&gt;Films like the Bicentennial Man also, (As opposed to the Matrix..)
</em><br>
<em>&gt;meaningless and deathist though that ultimately was, and characters
</em><br>
<em>&gt;like Data and #5.  It's not going to be an easy time, though.
</em><br>
<em>&gt;The monotheistic religions are going to be the biggest problem.
</em><br>
<p>It's ironic that the evolved behaviours designed to keep us alive
<br>
could pose the biggest threat. Fear of the unknown.
<br>
<p><em>&gt;
</em><br>
<em>&gt;&gt; (I wouldn't mind seeing an open source group
</em><br>
<em>&gt;&gt;  beat a 2 billion dollar agency though :)
</em><br>
<em>&gt;
</em><br>
<em>&gt;It can happen.  Open networks are smarter than primate heirarchies,
</em><br>
<em>&gt;and free thinking futurists of all kinds are smarter than military
</em><br>
<em>&gt;scientists.  We will also take every opportunity to improve ourselves,
</em><br>
<em>&gt;where their priority is simply to do their jobs and obey orders.
</em><br>
<p>hehehe, I think you underestimate military scientists. I'll wave the
<br>
flag for the freedom &amp; progress tribe though :)
<br>
<p>/me does a tarzan yell
<br>
<p><p><p>--------------------------- ONElist Sponsor ----------------------------
<br>
<p>Get what you deserve with NextCard Visa. ZERO. Rates as low as 0.0 
<br>
percent Intro APR, online balance transfers, Rewards Points, no hidden 
<br>
fees, and much more. Get NextCard today and get the credit you deserve.
<br>
Apply now! Get your NextCard Visa at
<br>
&lt;a href=&quot; <a href="http://clickme.onelist.com/ad/NextcardCreative6">http://clickme.onelist.com/ad/NextcardCreative6</a> &quot;&gt;Click Here&lt;/a&gt;
<br>
<p>------------------------------------------------------------------------
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="21234.html">Marc Forrester: "[SL4] Abandon Ship"</a>
<li><strong>Previous message:</strong> <a href="21232.html">Marc Forrester: "[SL4] Sinking the Boat"</a>
<li><strong>In reply to:</strong> <a href="21232.html">Marc Forrester: "[SL4] Sinking the Boat"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21234.html">Marc Forrester: "[SL4] Abandon Ship"</a>
<li><strong>Reply:</strong> <a href="21234.html">Marc Forrester: "[SL4] Abandon Ship"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21233">[ date ]</a>
<a href="index.html#21233">[ thread ]</a>
<a href="subject.html#21233">[ subject ]</a>
<a href="author.html#21233">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:06 MDT
</em></small></p>
</body>
</html>
