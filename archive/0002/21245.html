<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [SL4] AI Ethics &amp; Banning the Future.</title>
<meta name="Author" content="Marc Forrester (A1200@mharr.f9.co.uk)">
<meta name="Subject" content="Re: [SL4] AI Ethics &amp; Banning the Future.">
<meta name="Date" content="2000-02-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [SL4] AI Ethics &amp; Banning the Future.</h1>
<!-- received="Fri Feb 18 07:58:42 2000" -->
<!-- isoreceived="20000218145842" -->
<!-- sent="Fri, 18 Feb 2000 02:46:47 +0000" -->
<!-- isosent="20000218024647" -->
<!-- name="Marc Forrester" -->
<!-- email="A1200@mharr.f9.co.uk" -->
<!-- subject="Re: [SL4] AI Ethics &amp; Banning the Future." -->
<!-- id="yam8083.2380.1077612256@relay.f9.net.uk" -->
<!-- inreplyto="LOBBLDGHBFLPJDFBIPFEAELFELAA.pmcculler@kia.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marc Forrester (<a href="mailto:A1200@mharr.f9.co.uk?Subject=Re:%20[SL4]%20AI%20Ethics%20&amp;%20Banning%20the%20Future."><em>A1200@mharr.f9.co.uk</em></a>)<br>
<strong>Date:</strong> Thu Feb 17 2000 - 19:46:47 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="21246.html">Marc Forrester: "Re: [SL4] AI Ethics &amp; Banning the Future."</a>
<li><strong>Previous message:</strong> <a href="21244.html">Patrick McCuller: "RE: [SL4] AI Ethics &amp; Banning the Future."</a>
<li><strong>In reply to:</strong> <a href="21244.html">Patrick McCuller: "RE: [SL4] AI Ethics &amp; Banning the Future."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21247.html">Patrick McCuller: "RE: [SL4] AI Ethics &amp; Banning the Future."</a>
<li><strong>Reply:</strong> <a href="21247.html">Patrick McCuller: "RE: [SL4] AI Ethics &amp; Banning the Future."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21245">[ date ]</a>
<a href="index.html#21245">[ thread ]</a>
<a href="subject.html#21245">[ subject ]</a>
<a href="author.html#21245">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
From: Marc Forrester &lt;<a href="mailto:A1200@mharr.f9.co.uk?Subject=Re:%20[SL4]%20AI%20Ethics%20&amp;%20Banning%20the%20Future.">A1200@mharr.f9.co.uk</a>&gt;
<br>
<p>Patrick McCuller: Thursday 17-Feb-00
<br>
<em>&gt;&gt; &quot;Tortured Norns&quot;
</em><br>
<em>&gt;&gt; <a href="http://www.geocities.com/SiliconValley/Park/2495/">http://www.geocities.com/SiliconValley/Park/2495/</a>
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The more I think about this, the less happy I am.
</em><br>
<p>Yup.  SL4 must be the most uplifting place on the Net. :L
<br>
<p><em>&gt; Humans aren't terribly moral to begin with; in a consequence-
</em><br>
<em>&gt; free environment, there is an opportunity for great evil.
</em><br>
<p>For this reason, I find projects like the Roboneko to be very promising.
<br>
Give the serious AI some kind of body or other physical ability and senses
<br>
in the real world.  It means less to simulate on the computer, so more CPUs
<br>
for the brain, and the relationships we develop with these creatures will be
<br>
so much better if they can bite us when necessary.  (And where necessary :)
<br>
<p>Hmm.  Creatures.  A word soon to be used accurately for the first time ever.
<br>
<p><em>&gt; I tend to think that the first strong AI will have in the neighborhood of
</em><br>
<em>&gt; 10^9 lines of code, and require significant parallel processing. With any
</em><br>
<em>&gt; luck, most people won't have access to enough hardware to be able to
</em><br>
torture
<br>
<em>&gt; strong AIs. This doesn't solve the fundamental problem though...
</em><br>
<p>Not a safe bet, either.  Integrated arrays of thousands of parallel
<br>
processor+memory modules on one chip are likely in the near future.
<br>
<p><em>&gt; Presumably we could apply this moral gradient to software. I don't think I
</em><br>
<em>&gt; could torture Visual Cafe (it sure tortures me), but integrated weak AIs
</em><br>
<em>&gt; emulating, for instance, kittens, would count.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Though I gather 'emulating' isn't exactly the right word.
</em><br>
<p>That would be upload, wouldn't it?  How about 'imitating'?
<br>
<p>I'm already working by that sort of ethical scale, extending it to cover
<br>
software
<br>
doesn't seem like much of a stretch, but then, I see myself as a kind of
<br>
software.
<br>
I don't think most people are open to that sort of thinking.  Humanists are
<br>
a
<br>
good place to start, though.  If some transhuman thoughts and writings can
<br>
filter into the wider Net via Humanism, the next generation should be able
<br>
to pick up the meme and run with it.
<br>
<p><em>&gt; In biological organisms, sufficient damage will cause death, and death is
</em><br>
<em>&gt; usually permanent. In software, nobody can hear you scream.  Over and
</em><br>
over.
<br>
<p>In parallel, even.  Ng.  And every second you were looking the wrong way,
<br>
an hour, a year, an eternity to them.  I can only hope that such treatment
<br>
would destroy a mind's consiousness, or lead them to rise above suffering
<br>
like a buddhist master.
<br>
<p>However..  Need we design any pain into AI?  I can't see a convincing
<br>
argument
<br>
for it, if their bodies are robust and easily repaired, just as ours should
<br>
be
<br>
fifty years from now.  Sure, Norns 'scream' and 'bleed' if you 'cut' them,
<br>
but assuming they have some basic proto-awareness, is it necessarily
<br>
something
<br>
that they find unpleasant?  It's not like a hundred million of their
<br>
ancestors
<br>
have been the ones who survived because of a visceral response to injury..
<br>
<p><em>&gt; I think most humans could eventually be convinced not to hurt machine
</em><br>
<em>&gt; intelligences, but some humans you might have to torture to death a
</em><br>
<em>&gt; few times before it sinks in.
</em><br>
<p>I'm in favour of the Culture's solution to dangerously maladjusted persons,
<br>
just have someone follow them around and stop them doing whatever it is.
<br>
Any kind of posthuman would be able to take the role without excessive
<br>
inconvenience, living their private lives during the human's sleeping.
<br>
If only other humans are available, we'd need to operate in a rota.
<br>
<p><p>--------------------------- ONElist Sponsor ----------------------------
<br>
<p>@Backup-The Easiest Way to Protect and Access your files.
<br>
Automatic backups and off-site storage of your critical data.  Install 
<br>
your FREE trial today and have a chance to WIN a digital camera.
<br>
&lt;a href=&quot; <a href="http://clickme.onelist.com/ad/AtBackup2">http://clickme.onelist.com/ad/AtBackup2</a> &quot;&gt;Click Here&lt;/a&gt;
<br>
<p>------------------------------------------------------------------------
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="21246.html">Marc Forrester: "Re: [SL4] AI Ethics &amp; Banning the Future."</a>
<li><strong>Previous message:</strong> <a href="21244.html">Patrick McCuller: "RE: [SL4] AI Ethics &amp; Banning the Future."</a>
<li><strong>In reply to:</strong> <a href="21244.html">Patrick McCuller: "RE: [SL4] AI Ethics &amp; Banning the Future."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21247.html">Patrick McCuller: "RE: [SL4] AI Ethics &amp; Banning the Future."</a>
<li><strong>Reply:</strong> <a href="21247.html">Patrick McCuller: "RE: [SL4] AI Ethics &amp; Banning the Future."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21245">[ date ]</a>
<a href="index.html#21245">[ thread ]</a>
<a href="subject.html#21245">[ subject ]</a>
<a href="author.html#21245">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:06 MDT
</em></small></p>
</body>
</html>
