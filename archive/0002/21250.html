<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: [SL4] AI Ethics &amp; Banning the Future.</title>
<meta name="Author" content="Patrick McCuller (patrick@kia.net)">
<meta name="Subject" content="RE: [SL4] AI Ethics &amp; Banning the Future.">
<meta name="Date" content="2000-02-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: [SL4] AI Ethics &amp; Banning the Future.</h1>
<!-- received="Tue Feb 22 18:51:33 2000" -->
<!-- isoreceived="20000223015133" -->
<!-- sent="Tue, 22 Feb 2000 18:46:00 -0500" -->
<!-- isosent="20000222234600" -->
<!-- name="Patrick McCuller" -->
<!-- email="patrick@kia.net" -->
<!-- subject="RE: [SL4] AI Ethics &amp; Banning the Future." -->
<!-- id="LOBBLDGHBFLPJDFBIPFECEABEMAA.pmcculler@kia.net" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="yam8087.10.1077546232@relay.f9.net.uk" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Patrick McCuller (<a href="mailto:patrick@kia.net?Subject=RE:%20[SL4]%20AI%20Ethics%20&amp;%20Banning%20the%20Future."><em>patrick@kia.net</em></a>)<br>
<strong>Date:</strong> Tue Feb 22 2000 - 16:46:00 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="21251.html">Marc Forrester: "Re: [SL4] AI Ethics &amp; Banning the Future."</a>
<li><strong>Previous message:</strong> <a href="21249.html">Marc Forrester: "Re: [SL4] AI Ethics &amp; Banning the Future."</a>
<li><strong>In reply to:</strong> <a href="21249.html">Marc Forrester: "Re: [SL4] AI Ethics &amp; Banning the Future."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21251.html">Marc Forrester: "Re: [SL4] AI Ethics &amp; Banning the Future."</a>
<li><strong>Reply:</strong> <a href="21251.html">Marc Forrester: "Re: [SL4] AI Ethics &amp; Banning the Future."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21250">[ date ]</a>
<a href="index.html#21250">[ thread ]</a>
<a href="subject.html#21250">[ subject ]</a>
<a href="author.html#21250">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
From: &quot;Patrick McCuller&quot; &lt;<a href="mailto:patrick@kia.net?Subject=RE:%20[SL4]%20AI%20Ethics%20&amp;%20Banning%20the%20Future.">patrick@kia.net</a>&gt;
<br>
<p>I'm nitpicking today.
<br>
<p><em>&gt; From: Marc Forrester &lt;<a href="mailto:SL4@mharr.force9.co.uk?Subject=RE:%20[SL4]%20AI%20Ethics%20&amp;%20Banning%20the%20Future.">SL4@mharr.force9.co.uk</a>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Moore's law is the rate at which computers advance in the absence of major
</em><br>
<em>&gt; paradigm shifts, such as replacing valves with transistors, or integrating
</em><br>
<em>&gt; transistors into microcircuit chips.  The next shift will be from single
</em><br>
<em>&gt; central processing units to massive parallel-processing on one chip,
</em><br>
<em>&gt; the question is whether this happens before the arrival of strong AI,
</em><br>
<em>&gt; or after it.  I think the global entertainment industry has the
</em><br>
<em>&gt; resources to make it before.  Call it a 50/50 chance.
</em><br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In his book 'The Age of Spiritual Machines', Kurzweil makes a pretty good
<br>
case that Moore's law describes a curve that describes the amount of
<br>
computational power available for fixed money at any given time. He fits
<br>
Moore's law onto a graph going back a century, to human computers and so on.
<br>
There are - I don't have the book to reference - at least three paradigm
<br>
shifts. One heartening thing is that the graph is actually better than
<br>
Moore's law, though not by much.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We could call it Kurzweil's law.
<br>
<p><em>&gt; &gt; Who's to say an AI won't have a hundred
</em><br>
<em>&gt; &gt; million generations of descendants?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Well, therein lies the singularity stuff.  For that to happen, the
</em><br>
<em>&gt; generations need a world millions of times faster than our physical
</em><br>
<em>&gt; one to grow up in, and our technology is nowhere near being able to
</em><br>
<em>&gt; provide such things.  Theirs could be, but if it is they hardly
</em><br>
<em>&gt; need worry about oppression from the likes of us. :)
</em><br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;They don't really need to be millions of times faster. There's plenty of
<br>
time left in the universe even at our rate. I'm sorry, this is a nitpick.
<br>
<p><em>&gt;
</em><br>
<em>&gt; Pain is any sensory qualia that induces a feeling of suffering.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Suffering is something else entirely, and it is quite separate.
</em><br>
<em>&gt; Minds can suffer without pain, (Although human minds tend to
</em><br>
<em>&gt; experience hallucinatory pain as a result of such suffering,
</em><br>
<em>&gt; another result of our long and messy evolution) and they can
</em><br>
<em>&gt; experience pain without suffering.
</em><br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This leaves open the possibility that suffering is itself qualia.
<br>
<em>&gt;
</em><br>
<em>&gt; Pain, as the mechanism through which a creature suffers as a
</em><br>
<em>&gt; direct result of injury, is not necessary, and may be discarded,
</em><br>
<em>&gt; if physical injury is nothing more than an inconvenience.
</em><br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I do indeed concur. Though I feel that the possibility of physical
<br>
destruction, no matter how remote, can never be eliminated. A supernova
<br>
could take out anything. Falling into a deep enough gravity well could take
<br>
out anything (actually, I'm trying hard to remember where I read a theory of
<br>
definite information loss at event horizons; you can pull mass out of a
<br>
black hole but you can't get 'ordered' matter out.) I probably can't
<br>
accurately predict what would be lethal to a post-singularity being, but I'm
<br>
certain lethal events would exist. Therefore an awareness of injury and
<br>
physical danger will be a part of any long-lived intelligent being, whatever
<br>
the qualia involved.
<br>
<p><p><em>&gt; It seems that this god-play may lead us to find answers to the
</em><br>
<em>&gt; ancient theological question of suffering. (Not that we claim to
</em><br>
<em>&gt; be omnibenevolent, of course.)  It may well prove to be necessary
</em><br>
<em>&gt; for an intelligent mind to suffer, in order that it might have
</em><br>
<em>&gt; desires.  Or it may prove possible to create minds that never
</em><br>
<em>&gt; suffer, but are simply more euphoric at some times than others.
</em><br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I think what you're describing is called the soul making theodicy, and
<br>
though I hate to keep quoting books at you, Martin's 'Atheism: A
<br>
Philosophical Justification' makes the case that suffering isn't necessary
<br>
to build character or to motivate desires. He postulates that even in a
<br>
world free from pain, people can still be generous with time or love. And
<br>
even if there is no suffering, there must always be a happiness gradient -
<br>
you could always be happier. The world could always be better.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In application to AI, conditions will never be perfect. An AI will take
<br>
actions and attempt to solve problems, and sometime fail. Eliezer has an
<br>
elegant demonstration of goal formation in a state in which there are no
<br>
pressing problems to attend to.
<br>
<p><a href="http://singularity.posthuman.com/tmol-faq/logic.html">http://singularity.posthuman.com/tmol-faq/logic.html</a>
<br>
<p><p><em>&gt; &gt; Philosophy aside, intentionally damaging an AI's
</em><br>
<em>&gt; &gt; cognitive processes would constitute torture.
</em><br>
<em>&gt;
</em><br>
<em>&gt; That doesn't sound right..  If someone opened up my brain and
</em><br>
<em>&gt; intentionally damaged my cognitive processes I'd certainly call
</em><br>
<em>&gt; that grievous assault, but torture?  That would apply more to
</em><br>
<em>&gt; the physical process of sawing my head open.  Then again, if I
</em><br>
<em>&gt; was aware of the whole thing I doubt I'd be comfortable with
</em><br>
<em>&gt; the experience whatever the physical pain involved.
</em><br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Messing with cognitive processes can be torture. Imagine total paranoia.
<br>
Imagine having panphobia or even panophobia! Imagine having Capgras'
<br>
syndrome or even Cotard's syndrome.  (People with Capgras' syndrome often
<br>
believe that their loved ones have been replaced by exact duplicates:
<br>
robots, aliens, clones, whatever. This is a result of brain damage that
<br>
affects the association of visual perception with emotional response. People
<br>
with Cotard's syndrome, which I gather is immensely rare, lose their ability
<br>
to associate or remember associations of emotional response with *anything*,
<br>
even themselves. They usually believe that they are dead. Apparently some
<br>
even hallucinate their own flesh rotting away.)
<br>
<p><em>&gt;
</em><br>
<em>&gt; Perhaps torture is the act of forcing an experience on a creature
</em><br>
<em>&gt; against its will, with the intent to cause suffering.  That way it
</em><br>
<em>&gt; doesn't matter what pain and suffering actually mean from the
</em><br>
<em>&gt; victim's perspective, it is the intent to cause it that makes
</em><br>
<em>&gt; the act one of torture.
</em><br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The victim's perspective has to be a part of it, otherwise torture can be a
<br>
victimless crime, which I don't believe in for all sorts of moral reasons.
<br>
(The victim's perspective can't be the entire matter either.) We could
<br>
postulate that torture isn't necessarily a crime; but unless it is
<br>
self-inflicted, that doesn't make a lot of sense either.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Though even arguing this opens up huge avenues. For instance, it may be
<br>
possible to kill someone without their experiencing pain - or even anything
<br>
at all. Sudden enough death would preclude any kind of realization. This
<br>
wouldn't be torture, but it would still be wrong.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I have a feeling the whole thing is semantics.
<br>
<p><p>Patrick McCuller
<br>
<p><p>------------------------------------------------------------------------
<br>
Get what you deserve with NextCard Visa! Rates as low as 2.9% 
<br>
Intro or 9.9% Fixed APR, online balance transfers, Rewards Points, 
<br>
no hidden fees, and much more! Get NextCard today and get the 
<br>
credit you deserve! Apply now! Get your NextCard Visa at:
<br>
<a href="http://click.egroups.com/1/913/3/_/_/_/951262900/">http://click.egroups.com/1/913/3/_/_/_/951262900/</a>
<br>
------------------------------------------------------------------------
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="21251.html">Marc Forrester: "Re: [SL4] AI Ethics &amp; Banning the Future."</a>
<li><strong>Previous message:</strong> <a href="21249.html">Marc Forrester: "Re: [SL4] AI Ethics &amp; Banning the Future."</a>
<li><strong>In reply to:</strong> <a href="21249.html">Marc Forrester: "Re: [SL4] AI Ethics &amp; Banning the Future."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21251.html">Marc Forrester: "Re: [SL4] AI Ethics &amp; Banning the Future."</a>
<li><strong>Reply:</strong> <a href="21251.html">Marc Forrester: "Re: [SL4] AI Ethics &amp; Banning the Future."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21250">[ date ]</a>
<a href="index.html#21250">[ thread ]</a>
<a href="subject.html#21250">[ subject ]</a>
<a href="author.html#21250">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:06 MDT
</em></small></p>
</body>
</html>
