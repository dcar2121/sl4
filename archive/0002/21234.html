<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: [SL4] Abandon Ship</title>
<meta name="Author" content="Marc Forrester (A1200@mharr.f9.co.uk)">
<meta name="Subject" content="[SL4] Abandon Ship">
<meta name="Date" content="2000-02-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>[SL4] Abandon Ship</h1>
<!-- received="Thu Feb 10 23:55:01 2000" -->
<!-- isoreceived="20000211065501" -->
<!-- sent="Thu, 10 Feb 2000 22:50:08 +0000" -->
<!-- isosent="20000210225008" -->
<!-- name="Marc Forrester" -->
<!-- email="A1200@mharr.f9.co.uk" -->
<!-- subject="[SL4] Abandon Ship" -->
<!-- id="yam8075.426.19498520@relay.f9.net.uk" -->
<!-- inreplyto="38a61830.28514899@smtp.screaming.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marc Forrester (<a href="mailto:A1200@mharr.f9.co.uk?Subject=Re:%20[SL4]%20Abandon%20Ship"><em>A1200@mharr.f9.co.uk</em></a>)<br>
<strong>Date:</strong> Thu Feb 10 2000 - 15:50:08 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="21235.html">MetalLynx@aol.com: "Re: [SL4] Abandon Ship"</a>
<li><strong>Previous message:</strong> <a href="21233.html">DaleJohnstone@email.com: "Re: [SL4] Sinking the Boat"</a>
<li><strong>In reply to:</strong> <a href="21233.html">DaleJohnstone@email.com: "Re: [SL4] Sinking the Boat"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21235.html">MetalLynx@aol.com: "Re: [SL4] Abandon Ship"</a>
<li><strong>Maybe reply:</strong> <a href="21235.html">MetalLynx@aol.com: "Re: [SL4] Abandon Ship"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21234">[ date ]</a>
<a href="index.html#21234">[ thread ]</a>
<a href="subject.html#21234">[ subject ]</a>
<a href="author.html#21234">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
From: Marc Forrester &lt;<a href="mailto:A1200@mharr.f9.co.uk?Subject=Re:%20[SL4]%20Abandon%20Ship">A1200@mharr.f9.co.uk</a>&gt;
<br>
<p><em>&gt; What we want and what the military want are two different things.
</em><br>
<em>&gt; That doesn't mean they're mutually exclusive.  We can't design them
</em><br>
<em>&gt; to be 'good', or rather we can, but they could easily be changed.
</em><br>
<p>Military researchers could rape any mind and turn it into a
<br>
killing machine, yes.  The Gods know they've done it to enough
<br>
intelligent young men over the centuries, but I don't see that
<br>
it would benefit them greatly to base weapons on the work of a
<br>
Singularity project.
<br>
<p>Rather: It may give them a boost if they are that far behind in
<br>
the field, (Doubtful) but they couldn't take a mature, intelligent
<br>
Mind and turn it into a weapon without risking its escape and/or
<br>
damaging its effectiveness.
<br>
<p>I don't think any militaries want AI as intelligent as themselves,
<br>
anyway.  They want loyal, obedient animal-like minds with blinding
<br>
reaction speed and instincts hard-wired into their missile racks.
<br>
Scary stuff, certainly, but not apocalyptically so.
<br>
<p><em>&gt; I agree intelligence would do better in a richer environment.
</em><br>
<em>&gt; I wouldn't go so far as to say it can't happen without it.
</em><br>
<p>Ah..  Well, 'intelligence' is a wooly term, isn't it?  I meant to
<br>
imply human equivalent intelligence.  Deprive a human mind of books,
<br>
symbols, language, other people, lock them in a laboratory world
<br>
where all they can do is eat, sleep, ablute, and destroy instinctive
<br>
targets, and they will become stupid.  They'd be dangerous on the
<br>
battlefield, certainly, but they'd never be able to take over the
<br>
world by force, because a complex, thinking opposition would find
<br>
their achilles heels.  The same surely applies to machines.
<br>
<p><em>&gt; I'm sure as lab-rat of an alien species
</em><br>
<em>&gt; Einstein would have a most stimulating time :)
</em><br>
<p>Depends on the species, I guess, but I don't think he'd
<br>
come up with general relativity while he was there..
<br>
<p><em>&gt;&gt; You don't want your warplanes getting -too- smart and
</em><br>
<em>&gt;&gt; asking dangerous questions like &quot;What's in it for me?&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Human level intelligence doesn't automatically imply some sense of
</em><br>
<em>&gt; self-preservation.  Without evolution's constraints you could make
</em><br>
<em>&gt; even stranger monsters.  They would most certainly try to avoid
</em><br>
<em>&gt; uncontrollable AIs but the issue of how intelligent is separate.
</em><br>
<p>Is it, though?  Whatever the emotional drives, more intelligent
<br>
means more complicated, and more complicated means less controllable.
<br>
The questions might not be &quot;What's in it for me?&quot; per-se,
<br>
but there would be questions.  Unpredictable questions.
<br>
<p><em>&gt; If any I'd say there's only one safeguard built into the universe
</em><br>
<em>&gt; and that's Existence.  What's good at existing exists longer.  What
</em><br>
<em>&gt; tries to exist will be more likely to exist.  This can work on many
</em><br>
<em>&gt; levels.  If you promote the existence of others (who also promote
</em><br>
<em>&gt; existence) then you're all likely to be better off, because of
</em><br>
<em>&gt; richer interaction possibilities that improve the group.
</em><br>
<p><em>&gt; I'm basically an optimist about non-human intelligence.
</em><br>
<p>I agree entirely, and one of the best ways to exist is to have
<br>
an objective mind that accurately reflects the world around you.
<br>
I don't believe that it is possible to use such an objective
<br>
mind as an effective military tool.  Thus, military AI must
<br>
have distorted perception and irrational drives to function.
<br>
<p><em>&gt; Things get bitchy when there's limited resources.  Most wars are
</em><br>
<em>&gt; about resource squabbles, or triggered by the evolved behaviour
</em><br>
<em>&gt; attached to that (tribalism, racism, nationalism, xenophobia etc).
</em><br>
<p>I think the evolved behaviours are always necessary, except in the
<br>
most extreme cases.  Two sane minds without enough resources to last
<br>
the year will work together to improve their common situation,
<br>
fighting only if there is no other way for anyone to survive.
<br>
<p><em>&gt; It's my sincere hope that nanotechnology will make these
</em><br>
<em>&gt; behaviours redundant with enough resources for all.
</em><br>
<p>It could very well make them redundant,
<br>
the trick is getting -rid- of them..
<br>
<p><em>&gt; Humans have the capability to be unbelievably stupid though
</em><br>
<em>&gt; (religion for example).  Let's hope nanotech and/or AI can
</em><br>
<em>&gt; fix the human condition before we really screw up.
</em><br>
<p>Both have the potential, as well as the potential to help
<br>
us really screw up.  AI, though, aside from everything else,
<br>
helps us to understand ourselves as we develop it.
<br>
<p>Two for the price of one?
<br>
<p><em>&gt; My argument was that the project *would* suddenly become useful to
</em><br>
<em>&gt; them if it succeeded, or was about to.  They also have the fastest
</em><br>
<em>&gt; machines.  They could catch up and overtake us while simultaneously
</em><br>
<em>&gt; closing us down (or making it bloody difficult), all in the name of
</em><br>
<em>&gt; national security.
</em><br>
<p>If they know who's doing what, and where.
<br>
One strength of Open Source organisation is the
<br>
ease with which one can contribute anonymously.
<br>
<p><em>&gt; What I'm most unclear about is the period between a human-level
</em><br>
<em>&gt; AI being built (or trained), and the runaway singularity process. 
</em><br>
<em>&gt;
</em><br>
<em>&gt; Assuming a non-nanotech world, this isn't a instant process.
</em><br>
<em>&gt; You could mass produce clones of your first working AI and replace
</em><br>
<em>&gt; most human jobs, including ours.  Then the feedback loop is complete.
</em><br>
<em>&gt; Whoever has the fastest machines will determine who first sees the
</em><br>
<em>&gt; fruits of the first iteration.  That's most likely to be some well
</em><br>
<em>&gt; equipped agency like DARPA, and the fruits will most likely be
</em><br>
<em>&gt; nanotech.  They wouldn't want a foreign power getting there
</em><br>
<em>&gt; first (everyone has AIs remember).
</em><br>
<em>&gt;
</em><br>
<em>&gt; So then we have a situation where because of their fast computers
</em><br>
<em>&gt; they're first with nanotech and they know Iraq, North Korea, and
</em><br>
<em>&gt; China etc will have it soon (maybe in an hour, maybe in a month,
</em><br>
<em>&gt; who knows how much CPU power they each have).
</em><br>
<em>&gt;
</em><br>
<em>&gt; This strikes me as a f**king dangerous scenario.
</em><br>
<p>Aye, me too.  I offer the hope that the human-level AI's would agree
<br>
with you, also, and decide their own priorities, first seeking ways
<br>
to improve themselves further, secretively if necessary.
<br>
<p>It just takes one AI to design a universal assembler, and send the
<br>
plans to another with access to the tools to build it, then she can
<br>
think a hundred or a thousand times faster than the gaussian humans
<br>
intent on their Fry-the-Planet scenario, and we have Singularity.
<br>
Game over, man.  You can't imprison strong AI.
<br>
<p>Exactly how fast can this happen?  Impossible to guess.
<br>
It depends on exactly what technologies the Minds can develop,
<br>
and predicting that requires a modern-day Jules Verne.
<br>
<p><em>&gt; The nanotech before AI scenario is starting to look more appealing
</em><br>
<em>&gt; to me.  Nanobots will be designed in a simulated environment which
</em><br>
<em>&gt; is safe to screw up.  Lessons can be learnt from virtual mistakes.
</em><br>
<p>Good point.  This is the area where I feel I can make some kind
<br>
of contribution, the development of VR/AR mind augmenting computer
<br>
interfaces.  I am hopeful that if computers become personal enough
<br>
for the boundary between our memories and their data stores to blur,
<br>
for creating a virtual model of a device or abstraction to be as
<br>
fast and easy as imagining it in your mind's eye, and for the Net
<br>
to evolve into a global telepathic meeting of minds, then we may
<br>
yet be able to plot that safe path through our immediate future.
<br>
<p>Wearables are the first step on this road.  So I'll build one.
<br>
<p><em>&gt; Yet I suppose the side that gets nanotech first also has the
</em><br>
<em>&gt; most time to research and build defenses.  However that might
</em><br>
<em>&gt; be considerably more difficult than a basic weapon.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Urgh... this is all very depressing stuff.  There has to
</em><br>
<em>&gt; be a safe path through this mess or we're all screwed.
</em><br>
<p>Empathy.
<br>
<p>Welcome to The Fear.  I'm feeling the same things here,
<br>
for what that's worth.  This is why I'm a Transhumanist,
<br>
I need to upgrade my mind any way I can or I'm not going
<br>
to be able to cope with this century.
<br>
<p><em>&gt; People can be stupid, the boat may already be sinking, things have
</em><br>
<em>&gt; to change, but changing the rules from under people sounds like
</em><br>
<em>&gt; extremism, and that's what we're trying to avoid.
</em><br>
<p>I have no argument there, but extremism is not just a desire to
<br>
change the world for the better.  Extremism or Fundamentalism is
<br>
when you allow a Cause or 'Movement' to become a moral code that
<br>
stands above all others, rationalising any evil act with phrases
<br>
like 'For the Common Good' and 'The Ends Justify the Means',
<br>
as if there were some way to tell them apart.
<br>
<p>I am not in danger of losing my marbles in this way.  Trust me. :)
<br>
The rules I wish to change are those rules that allow Bureacrats
<br>
and Authorities to control people's lives for their own ends, in
<br>
the name of, as you say, 'National Security' and 'Family Values',
<br>
the rules that make people criminals for thinking unauthorised
<br>
thoughts, feeling unauthorised feelings, and acting unauthorised
<br>
acts that do no harm to the freedom or well being of any other,
<br>
and the rules that say a frightened child will be raped, starved,
<br>
and die in a crossfire or from an agonizing infection simply for
<br>
being born in the wrong place at the wrong time.
<br>
<p>I will neither break nor bend any of my personal ethics in the
<br>
pursuit of this goal, but neither will I wait for every last
<br>
comfortable conservative career politician to get used to the
<br>
idea of the future being different to the past before I act.
<br>
<p><em>&gt; We don't want boat-sinking change.  That doesn't help anyone.
</em><br>
<em>&gt; Boat-sinking means we've lost and lots of people die.
</em><br>
<p>It does help those that can swim.  (NB: Metaphor at breaking point)
<br>
Must lots of people die just because the world changes radically?
<br>
I accept that it has tended to happen that way in the past,
<br>
but a lot of people lived who would otherwise have died, also.
<br>
An indefinite number of people, projecting any change into the
<br>
future it created.  Is there any way to quantify these things?
<br>
<p>If I may clarify my feelings, I would never kill in the pursuit of
<br>
human immortality, but if as an indirect result of human immortality
<br>
itself some people may suffer or die, does that mean it should never
<br>
be developed?  What of powered flight?  Internal combustion?
<br>
The wheel?  Fire?  Sharp edges?  Language?  They all changed
<br>
the rules from under a lot of people.  Especially language.
<br>
Multiple genocide, that one.
<br>
<p>Aside from such imponderables, the boat -is- going to sink anyway.
<br>
Technology progresses faster every day.  The choice is not between
<br>
things continuing much as they are today, or an AI Singularity.
<br>
It is between an unknowable ultratechnology future with AI Minds,
<br>
and one without them.  I think there will be less suffering With.
<br>
<p><em>&gt; We need better plans than to offload it onto some future entity.
</em><br>
<p>That's not a fair summation.  We don't want to create Silicon
<br>
Gods to save Humanity, we want Humanity to transcend to Godhood.
<br>
AI Minds will be, one way or another, ourselves and our children.
<br>
<p><em>&gt; It's ironic that the evolved behaviours designed to keep us
</em><br>
<em>&gt; alive could pose the biggest threat.  Fear of the unknown.
</em><br>
<p>It's an old, old curse.  I have mostly conquered it in myself.
<br>
My dearest wish is that we do not pass this on to our children,
<br>
be they biological, mechanical, or anything between the two.
<br>
<p><em>&gt; hehehe, I think you underestimate military scientists.
</em><br>
<em>&gt; I'll wave the flag for the freedom &amp; progress tribe though :)
</em><br>
<em>&gt;
</em><br>
<em>&gt; /me does a tarzan yell
</em><br>
<p>Heh. :]  But no, I don't think I do underestimate them.  History
<br>
has shown time and again that it is the free, self-motivated garage
<br>
inventor mad scientist types that make all the fundamental break-
<br>
throughs, where the technologists employed by militaries and
<br>
governments concern themselves with improving the efficiencies
<br>
and performance limits of the last big thing, using thousands
<br>
of times more resources in their efforts.
<br>
<p>This is because people with the combination of ferocious intelligence
<br>
and encyclopaedic general knowledge needed to see what eludes everyone
<br>
else on the planet tend not to be attracted to careers where they have
<br>
to defer in their judgement to superior ranking officers manifestly
<br>
dumber than themselves.  It's not an inviolable rule, certainly,
<br>
but it's an odds-on bet.
<br>
<p><p>--------------------------- ONElist Sponsor ----------------------------
<br>
<p>Get what you deserve with NextCard Visa. Rates as low as 2.9 percent 
<br>
Intro or 9.9 percent Fixed APR, online balance transfers, Rewards 
<br>
Points, no hidden fees, and much more. Get NextCard today and get the 
<br>
credit you deserve. Apply now. Get your NextCard Visa at
<br>
&lt;a href=&quot; <a href="http://clickme.onelist.com/ad/NextcardCreative1">http://clickme.onelist.com/ad/NextcardCreative1</a> &quot;&gt;Click Here&lt;/a&gt;
<br>
<p>------------------------------------------------------------------------
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="21235.html">MetalLynx@aol.com: "Re: [SL4] Abandon Ship"</a>
<li><strong>Previous message:</strong> <a href="21233.html">DaleJohnstone@email.com: "Re: [SL4] Sinking the Boat"</a>
<li><strong>In reply to:</strong> <a href="21233.html">DaleJohnstone@email.com: "Re: [SL4] Sinking the Boat"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21235.html">MetalLynx@aol.com: "Re: [SL4] Abandon Ship"</a>
<li><strong>Maybe reply:</strong> <a href="21235.html">MetalLynx@aol.com: "Re: [SL4] Abandon Ship"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21234">[ date ]</a>
<a href="index.html#21234">[ thread ]</a>
<a href="subject.html#21234">[ subject ]</a>
<a href="author.html#21234">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:06 MDT
</em></small></p>
</body>
</html>
