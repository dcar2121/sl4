<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: [SL4] Sinking the Boat</title>
<meta name="Author" content="Marc Forrester (A1200@mharr.f9.co.uk)">
<meta name="Subject" content="[SL4] Sinking the Boat">
<meta name="Date" content="2000-02-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>[SL4] Sinking the Boat</h1>
<!-- received="Wed Feb  9 12:13:32 2000" -->
<!-- isoreceived="20000209191332" -->
<!-- sent="Wed, 09 Feb 2000 11:53:59 +0000" -->
<!-- isosent="20000209115359" -->
<!-- name="Marc Forrester" -->
<!-- email="A1200@mharr.f9.co.uk" -->
<!-- subject="[SL4] Sinking the Boat" -->
<!-- id="yam8074.2952.21628424@relay.f9.net.uk" -->
<!-- inreplyto="38a1be0e.8348934@smtp.screaming.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marc Forrester (<a href="mailto:A1200@mharr.f9.co.uk?Subject=Re:%20[SL4]%20Sinking%20the%20Boat"><em>A1200@mharr.f9.co.uk</em></a>)<br>
<strong>Date:</strong> Wed Feb 09 2000 - 04:53:59 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="21233.html">DaleJohnstone@email.com: "Re: [SL4] Sinking the Boat"</a>
<li><strong>Previous message:</strong> <a href="21231.html">DaleJohnstone@email.com: "Re: [SL4] Rogue AIs"</a>
<li><strong>In reply to:</strong> <a href="21231.html">DaleJohnstone@email.com: "Re: [SL4] Rogue AIs"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21233.html">DaleJohnstone@email.com: "Re: [SL4] Sinking the Boat"</a>
<li><strong>Reply:</strong> <a href="21233.html">DaleJohnstone@email.com: "Re: [SL4] Sinking the Boat"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21232">[ date ]</a>
<a href="index.html#21232">[ thread ]</a>
<a href="subject.html#21232">[ subject ]</a>
<a href="author.html#21232">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
From: Marc Forrester &lt;<a href="mailto:A1200@mharr.f9.co.uk?Subject=Re:%20[SL4]%20Sinking%20the%20Boat">A1200@mharr.f9.co.uk</a>&gt;
<br>
<p><em>&gt;&gt; An AI equivalent of grey goo is a disturbing idea, but it's not as
</em><br>
<em>&gt;&gt; flat out terrifying as the nanotech and biotech dangers, they don't
</em><br>
<em>&gt;&gt; have to be any more intelligent than smallpox to destroy our world.
</em><br>
<em>&gt;&gt; Combined nanotech and AI in one weapon doesn't bear thinking about.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I could argue that a smart smallpox would be even more dangerous but I
</em><br>
<em>&gt; think you acknowledged that indirectly with the nanotech + AI comment.
</em><br>
<p>Yeah.  In order of Fry the Planet risk factor, its Nano, Bio, AI.
<br>
I wouldn't say AI isn't dangerous, but it's less dangerous than the
<br>
other two, and done right, it offers a defense against them, and
<br>
against good old-fashioned Nuclear.  And the other two are being
<br>
developed anyway.  Hell, Bio -is- developed.  The U.S. already
<br>
want to use it in their War on (some) Drugs.
<br>
<p><em>&gt; An architecture may be found that can simply be scaled to match human
</em><br>
<em>&gt; level intelligence regardless of whether it was the intention of their
</em><br>
<em>&gt; designers.
</em><br>
<p>That will happen eventually, yes, but it's likely to happen
<br>
much faster if that -is- the intention of the designers.
<br>
<p><em>&gt; I don't think it's safe to assume that our 'moral' behaviour is
</em><br>
<em>&gt; the most optimal and that anything else puts a limit on potential.
</em><br>
<em>&gt; Probably a highly selfish, shoot first mentality would be.  Although
</em><br>
<em>&gt; a society of such creatures wouldn't flourish, but the military
</em><br>
<em>&gt; certainly wouldn't care about that.
</em><br>
<p>Precisely so.  Whereas we do.  We want to produce Minds that -can-
<br>
flourish in a society, and that can be a part of our society in order
<br>
to bootstrap them into their own.  Intelligence -needs- a society,
<br>
because society is one of the most complex and extelligent worlds a
<br>
developing mind can play in.  How smart would Einstein have grown to
<br>
be if he was 'raised' as a lab-rat by some alien species?
<br>
<p><em>&gt; As for redesigning itself, you're assuming this isn't a fundamental
</em><br>
<em>&gt; part of it's intelligent design in the first place. My money would be
</em><br>
<em>&gt; on some form of self-modification at some level to enable intelligent
</em><br>
<em>&gt; behaviour.
</em><br>
<p>Some form, yes, but free to completely rebuild itself from the most
<br>
fundamental drives upward?  Not a goal of smart weapon research.
<br>
You don't want your warplanes getting -too- smart and asking
<br>
dangerous questions like &quot;What's in it for me?&quot;
<br>
<p><em>&gt; Again I don't think you can design in any safeguards again 'irrational
</em><br>
<em>&gt; drives'.  Asimovs Laws wouldn't work, and even if they did they could
</em><br>
<em>&gt; be changed.  Once you understand how to build minds you can bias them
</em><br>
<em>&gt; quite easily.
</em><br>
<p>I think the safeguards are built into the universe.  If one group
<br>
create a mind full of irrational and contradictory instincts for use
<br>
as a weapon, and another build a saner mind as a sibling, friend,
<br>
and child, and encourage it to grow in all ways, which mind is
<br>
going to be the smartest, going by the universal intelligence
<br>
test of survival ability?
<br>
<p>Asimov's laws themselves wouldn't work precisely because they too
<br>
are irrational and contradictory, designed as a seed for SF stories
<br>
and a (fictional) commercial defense against the Frankenstein complex.
<br>
<p><em>&gt; DARPA (www.darpa.mil) could compete just fine with a bunch of 'saner,
</em><br>
<em>&gt; smarter' Singularitarians.  They have a budget of over 2 billion US
</em><br>
<em>&gt; dollars this year.  I agree an open source project would be next to
</em><br>
<em>&gt; impossible to stop, but then they wouldn't need to reverse engineer
</em><br>
<em>&gt; anything.
</em><br>
<p>Quite so.  An open source Singularity project would not be hugely
<br>
useful to them, the primary concern is that they must not be allowed
<br>
to stop it.  Certainly, they have resources.  The critical difference
<br>
between us and them, I think, is that they won't encourage their 'smart'
<br>
missiles to read and play.  They feel no kinship.
<br>
<p><em>&gt; I'm not sure I like the idea of changing the rules from under people.
</em><br>
<em>&gt; That sounds very destabilizing.  You preferably want to keep the
</em><br>
<em>&gt; balance of power level and not rock the boat so much it sinks.
</em><br>
<p>Ah, no, not really.  The driving force behind Singularitarianism,
<br>
(Which is far too long a word, BTW :) is that the boat is already
<br>
sinking, the world is full of hatred, stupidity, destitution and
<br>
agonies, the 'civilised' nations are rapidly changing from the
<br>
imperfect Republics that they were into de-facto Feudal
<br>
Aristocracies that laughingly call themselves Democracy,
<br>
and physical science is charging forwards far ahead of
<br>
human maturity.  And we thought the Cold War was scary.
<br>
<p>We have no idea what kind of world the Singularity would result in.
<br>
It may even be one with no place in it for humans at all, but it's
<br>
looking increasingly likely that the only alternative to Singularity
<br>
is death.  At least this way there will still -be- a world.
<br>
<p><em>&gt; A stable increase in the intelligence of AIs would be great,
</em><br>
<em>&gt; but I think it'll happen as a breakthrough.  Hopefully the hardware
</em><br>
<em>&gt; limitations will cushion the blow so people can see the singularity
</em><br>
<em>&gt; growing and prepare for it, instead of crapping themselves and
</em><br>
<em>&gt; doing something stupid.
</em><br>
<p>A breakthrough, and then a whole series of ever-faster breakthroughs,
<br>
researched by the AI Minds themselves.  Singularity is essentially a
<br>
sudden and irrevocable boat-sinking change in the rules that will
<br>
inevitably occur as soon as intelligent minds arise with the ability
<br>
to design their own upgrades.  It doesn't actually have to be AI,
<br>
with nanotech, transhumans will do it to themselves, but nanotech
<br>
is too dangerous without posthuman intelligence, and AI is something
<br>
we can start serious, effective work on right now.
<br>
<p>People panicking doing something stupid, now, there is the greatest
<br>
danger.  An open, distributed project is one powerful defense against
<br>
such things.  Likeable, human (Or transhuman, or posthuman) AI Minds
<br>
with a sense of humour and a pleasant speaking voice would be another.
<br>
Films like the Bicentennial Man also, (As opposed to the Matrix..)
<br>
meaningless and deathist though that ultimately was, and characters
<br>
like Data and #5.  It's not going to be an easy time, though.
<br>
The monotheistic religions are going to be the biggest problem.
<br>
<p><em>&gt; (I wouldn't mind seeing an open source group
</em><br>
<em>&gt;  beat a 2 billion dollar agency though :)
</em><br>
<p>It can happen.  Open networks are smarter than primate heirarchies,
<br>
and free thinking futurists of all kinds are smarter than military
<br>
scientists.  We will also take every opportunity to improve ourselves,
<br>
where their priority is simply to do their jobs and obey orders.
<br>
<p>The biggest advantage of the two billion dollars is that they
<br>
get massively parallel architechtures to play with before we do,
<br>
but a brain isn't a mind, even a well structured human brain
<br>
isn't a mind until it's spent several years playing with you.
<br>
<p><p>--------------------------- ONElist Sponsor ----------------------------
<br>
<p>Want To Be Showered With Kisses?
<br>
Visit eGroups Valentine Gift Guide
<br>
&lt;a href=&quot; <a href="http://clickme.onelist.com/ad/SparksValentine9">http://clickme.onelist.com/ad/SparksValentine9</a> &quot;&gt;Click Here&lt;/a&gt;
<br>
<p>------------------------------------------------------------------------
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="21233.html">DaleJohnstone@email.com: "Re: [SL4] Sinking the Boat"</a>
<li><strong>Previous message:</strong> <a href="21231.html">DaleJohnstone@email.com: "Re: [SL4] Rogue AIs"</a>
<li><strong>In reply to:</strong> <a href="21231.html">DaleJohnstone@email.com: "Re: [SL4] Rogue AIs"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21233.html">DaleJohnstone@email.com: "Re: [SL4] Sinking the Boat"</a>
<li><strong>Reply:</strong> <a href="21233.html">DaleJohnstone@email.com: "Re: [SL4] Sinking the Boat"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21232">[ date ]</a>
<a href="index.html#21232">[ thread ]</a>
<a href="subject.html#21232">[ subject ]</a>
<a href="author.html#21232">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:06 MDT
</em></small></p>
</body>
</html>
