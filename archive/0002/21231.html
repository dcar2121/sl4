<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [SL4] Rogue AIs</title>
<meta name="Author" content="DaleJohnstone@email.com (DaleJohnstone@email.com)">
<meta name="Subject" content="Re: [SL4] Rogue AIs">
<meta name="Date" content="2000-02-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [SL4] Rogue AIs</h1>
<!-- received="Tue Feb  8 21:03:46 2000" -->
<!-- isoreceived="20000209040346" -->
<!-- sent="Wed, 09 Feb 2000 01:09:37 GMT" -->
<!-- isosent="20000209010937" -->
<!-- name="DaleJohnstone@email.com" -->
<!-- email="DaleJohnstone@email.com" -->
<!-- subject="Re: [SL4] Rogue AIs" -->
<!-- id="38a1be0e.8348934@smtp.screaming.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="yam8073.1183.20570424@relay.f9.net.uk" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> <a href="mailto:DaleJohnstone@email.com?Subject=Re:%20[SL4]%20Rogue%20AIs"><em>DaleJohnstone@email.com</em></a><br>
<strong>Date:</strong> Tue Feb 08 2000 - 18:09:37 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="21232.html">Marc Forrester: "[SL4] Sinking the Boat"</a>
<li><strong>Previous message:</strong> <a href="21230.html">Marc Forrester: "Re: [SL4] JOIN: Dale Johnstone"</a>
<li><strong>In reply to:</strong> <a href="21230.html">Marc Forrester: "Re: [SL4] JOIN: Dale Johnstone"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21232.html">Marc Forrester: "[SL4] Sinking the Boat"</a>
<li><strong>Reply:</strong> <a href="21232.html">Marc Forrester: "[SL4] Sinking the Boat"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21231">[ date ]</a>
<a href="index.html#21231">[ thread ]</a>
<a href="subject.html#21231">[ subject ]</a>
<a href="author.html#21231">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
From: <a href="mailto:DaleJohnstone@email.com?Subject=Re:%20[SL4]%20Rogue%20AIs">DaleJohnstone@email.com</a>
<br>
<p><p><em>&gt;Greets.  Marc Forrester, Transhumanist, and as yet inexperienced
</em><br>
<em>&gt;information technician.  (Database setup, rescuing files from obscure
</em><br>
<em>&gt;and inappropriate formats, straight programming where necessary.)
</em><br>
<p>Hiya Marc. :)
<br>
<p><em>&gt;An AI equivalent of grey goo is a disturbing idea, but it's not as
</em><br>
<em>&gt;flat out terrifying as the nanotech and biotech dangers, they don't
</em><br>
<em>&gt;have to be any more intelligent than smallpox to destroy our world.
</em><br>
<em>&gt;Combined nanotech and AI in one weapon doesn't bear thinking about.
</em><br>
<p>I could argue that a smart smallpox would be even more dangerous but I
<br>
think you acknowledged that indirectly with the nanotech + AI comment.
<br>
<p><em>&gt;AI developed by military 'thinkers' will likely not be developed to
</em><br>
<em>&gt;match human intelligence, let alone exceed it.  It will probably have
</em><br>
<em>&gt;irrational drives that limit its potential, and it will certainly not
</em><br>
<em>&gt;be intentionally given the ability to redesign itself at will.
</em><br>
<p>An architecture may be found that can simply be scaled to match human
<br>
level intelligence regardless of whether it was the intention of their
<br>
designers.
<br>
I don't think it's safe to assume that our 'moral' behaviour is the
<br>
most optimal and that anything else puts a limit on potential.
<br>
Probably a highly selfish, shoot first mentality would be. Although a
<br>
society of such creatures wouldn't flourish, but the military
<br>
certainly wouldn't care about that.
<br>
As for redesigning itself, you're assuming this isn't a fundamental
<br>
part of it's intelligent design in the first place. My money would be
<br>
on some form of self-modification at some level to enable intelligent
<br>
behaviour.
<br>
<p>I don't have a fundamental problem with an AI that can redesign
<br>
itself. It the human factor I don't trust.
<br>
<p><em>&gt;AI developed by Singularitarians will be entirely the opposite,
</em><br>
<em>&gt;and so would not be easy to cripple for use as a military or state
</em><br>
<em>&gt;slave machine.  Some 'authority' may very well seize the project,
</em><br>
<em>&gt;but the big advantage of open source is that they can't destroy
</em><br>
<em>&gt;the original, so they'd be competing with free Singularitarians
</em><br>
<em>&gt;elsewhere in the world, trying to reverse engineer a project
</em><br>
<em>&gt;designed by saner, smarter people than themselves and pervert
</em><br>
<em>&gt;it to the antithesis of its original design.
</em><br>
<p>Again I don't think you can design in any safeguards again 'irrational
<br>
drives'. Asimovs Laws wouldn't work, and even if they did they could
<br>
be changed. Once you understand how to build minds you can bias them
<br>
quite easily.
<br>
DARPA (www.darpa.mil) could compete just fine with a bunch of 'saner,
<br>
smarter' Singularitarians. They have a budget of over 2 billion US
<br>
dollars this year. I agree an open source project would be next to
<br>
impossible to stop, but then they wouldn't need to reverse engineer
<br>
anything.
<br>
<p><em>&gt;Chances of success?
</em><br>
<em>&gt;
</em><br>
<em>&gt;I think there is far more danger in not acting, or acting in a
</em><br>
<em>&gt;slow and secretive manner, than there is in the old forces of the
</em><br>
<em>&gt;dark ages hitching a ride in our slipstream.  If we act fast now,
</em><br>
<em>&gt;we can change the rules from under them.  They will not adapt.
</em><br>
<p>I'm beginning to agree that if a working AI was built it would be best
<br>
if it was also open.
<br>
I'm not sure I like the idea of changing the rules from under people.
<br>
That sounds very destabilizing. You preferably want to keep the
<br>
balance of power level and not rock the boat so much it sinks.
<br>
<p>A stable increase in the intelligence of AIs would be great, but I
<br>
think it'll happen as a breakthrough. Hopefully the hardware
<br>
limitations will cushion the blow so people can see the singularity
<br>
growing and prepare for it, instead of crapping themselves and doing
<br>
something stupid.
<br>
<p>(I wouldn't mind seeing an open source group beat a 2 billion dollar
<br>
agency though :)
<br>
<p><p><p>--------------------------- ONElist Sponsor ----------------------------
<br>
<p>Valentine's Day Shopping Made Simple. 
<br>
&lt;a href=&quot; <a href="http://clickme.onelist.com/ad/SparksValentine7">http://clickme.onelist.com/ad/SparksValentine7</a> &quot;&gt;Click Here&lt;/a&gt;
<br>
<p>------------------------------------------------------------------------
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="21232.html">Marc Forrester: "[SL4] Sinking the Boat"</a>
<li><strong>Previous message:</strong> <a href="21230.html">Marc Forrester: "Re: [SL4] JOIN: Dale Johnstone"</a>
<li><strong>In reply to:</strong> <a href="21230.html">Marc Forrester: "Re: [SL4] JOIN: Dale Johnstone"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21232.html">Marc Forrester: "[SL4] Sinking the Boat"</a>
<li><strong>Reply:</strong> <a href="21232.html">Marc Forrester: "[SL4] Sinking the Boat"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21231">[ date ]</a>
<a href="index.html#21231">[ thread ]</a>
<a href="subject.html#21231">[ subject ]</a>
<a href="author.html#21231">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:06 MDT
</em></small></p>
</body>
</html>
