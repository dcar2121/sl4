<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?</title>
<meta name="Author" content="Matt Mahoney (matmahoney@yahoo.com)">
<meta name="Subject" content="Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?">
<meta name="Date" content="2008-04-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?</h1>
<!-- received="Sat Apr 12 20:03:00 2008" -->
<!-- isoreceived="20080413020300" -->
<!-- sent="Sat, 12 Apr 2008 18:58:45 -0700 (PDT)" -->
<!-- isosent="20080413015845" -->
<!-- name="Matt Mahoney" -->
<!-- email="matmahoney@yahoo.com" -->
<!-- subject="Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?" -->
<!-- id="656253.44668.qm@web51909.mail.re2.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="79ecaa350804120811k31e05967v1948ef949b2f4175@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Matt Mahoney (<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20What%20are%20&quot;AGI-first'ers&quot;%20expecting%20AGI%20will%20teach%20us%20about%20FAI?"><em>matmahoney@yahoo.com</em></a>)<br>
<strong>Date:</strong> Sat Apr 12 2008 - 19:58:45 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="18452.html">Vladimir Nesov: "Re: Bounded population (was Re: Bounded utility)"</a>
<li><strong>Previous message:</strong> <a href="18450.html">Matt Mahoney: "Re: The role of consciousness (Re: The GLUT and functionalism)"</a>
<li><strong>In reply to:</strong> <a href="18445.html">Rolf Nelson: "What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18453.html">Daniel Burfoot: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18451">[ date ]</a>
<a href="index.html#18451">[ thread ]</a>
<a href="subject.html#18451">[ subject ]</a>
<a href="author.html#18451">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
--- Rolf Nelson &lt;<a href="mailto:rolf.h.d.nelson@gmail.com?Subject=Re:%20What%20are%20&quot;AGI-first'ers&quot;%20expecting%20AGI%20will%20teach%20us%20about%20FAI?">rolf.h.d.nelson@gmail.com</a>&gt; wrote:
<br>
<p><em>&gt; On Fri, Feb 29, 2008 at 6:02 PM, Ben Goertzel &lt;<a href="mailto:ben@goertzel.org?Subject=Re:%20What%20are%20&quot;AGI-first'ers&quot;%20expecting%20AGI%20will%20teach%20us%20about%20FAI?">ben@goertzel.org</a>&gt; wrote:
</em><br>
<em>&gt; &gt;  The fact that AGi ethics is incredibly badly understood right now, and
</em><br>
<em>&gt; &gt;  the only clear route to understand it better is to make more empirical
</em><br>
<em>&gt; &gt;  progress toward AGI.   I find it unlikely that dramatic advances in AGI
</em><br>
<em>&gt; &gt;  ethical theory are going to be made in a vacuum, separate from
</em><br>
<em>&gt; &gt;  coupled advances in AGI practice.  I know some others disagree on
</em><br>
<em>&gt; &gt;  this.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; For any of the many people who agree with Ben's sentiment:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Large numbers of people have made various AI advances in the past. In
</em><br>
<em>&gt; none of these cases, to my knowledge, have FAI people said, &quot;a-ha,
</em><br>
<em>&gt; that's one of the pieces of data I was waiting for, this advances FAI
</em><br>
<em>&gt; theory.&quot; Why would we expect this to change in the future? At the very
</em><br>
<em>&gt; least, doesn't this show that even if FAI advances require AGI
</em><br>
<em>&gt; advances, the &quot;bottleneck&quot; is that there are too few people working on
</em><br>
<em>&gt; deriving FAI from existing AGI, rather than too few people working on
</em><br>
<em>&gt; existing AGI?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Are there specific facts about AGI that you're waiting to find out,
</em><br>
<em>&gt; such that if the result of a pending experiment is A, then successful
</em><br>
<em>&gt; FAI theory lies in one direction, but if the result is B, then
</em><br>
<em>&gt; successful FAI theory lies in a different direction? If so, what are
</em><br>
<em>&gt; such facts?
</em><br>
<p>I agree with Ben.  Discussions of friendliness seem to break down into widely
<br>
divergent views in the absence of even the most fundamental agreement on what
<br>
AGI will look like.
<br>
<p>I believe it is easier to analyze threats in the context of specific
<br>
proposals.  My proposal for competitive message distribution at
<br>
<a href="http://www.mattmahoney.net/agi.html">http://www.mattmahoney.net/agi.html</a> is quite different from the usual attempts
<br>
to build something resembling a human mind.  I believe that AGI will emerge in
<br>
the form of a large collection of narrow domain experts and an infrastructure
<br>
that routes messages to the right experts.  I estimate that it will cost USD
<br>
$1 quadrillion and take 30 years to reach parity with carbon-based
<br>
intelligence.  I argue that it will be built because AGI is worth that much
<br>
and the system provides economic incentives for people to contribute. 
<br>
Information has negative value on average, so peers that are the most helpful
<br>
to humans by identifying and routing the most useful information and filtering
<br>
the rest will be the winners in a market where peers compete for reputation
<br>
and computing resources.
<br>
<p>In my proposal I have identified several types of attacks, such as spam and
<br>
forged messages.  I am sure I have overlooked something.  Two that I did not
<br>
mention:
<br>
<p>1. Intelligent worms.  Security tools are double-edged.  There is practically
<br>
no tool used to defend information systems that isn't also useful to an
<br>
attacker.  (For example, systems that probe for vulnerabilities, test for weak
<br>
passwords, check files against a suite of virus detectors, etc).  A big source
<br>
of vulnerability is software bugs.  We would like for AI to automatically
<br>
analyze software and test it for vulnerabilities, which currently only humans
<br>
can do.  If this technology was available, a worm could use it to discover and
<br>
immediately exploit thousands of vulnerabilities and quickly take over nearly
<br>
every computer on the internet.
<br>
<p>2. Getting what you want.  Distributed AGI grants immediate wishes, not our
<br>
extrapolated volition, because that is what the economic system rewards. 
<br>
Humans evolved in a world where we can't have everything we want.  What
<br>
happens to the human race when we can all have eternal bliss (wireheading) or
<br>
a magic genie (in a simulated world)?
<br>
<p>I believe my proposal is friendly only in the near term.  I don't have
<br>
solutions for the long term problems.
<br>
<p><em>&gt; At what point will you know that AGI has advanced enough that FAI can
</em><br>
<em>&gt; proceed?
</em><br>
<p>I am pessimistic.  The early designers of the internet (TCP/IP, HTTP, HTML)
<br>
could not have anticipated today's security problems, and could not have run a
<br>
simulation to test for them.  I don't believe we can anticipate all of the
<br>
problems that will arise from AGI until we actually build it, and then it will
<br>
be too late because we won't know what our computers are doing any more.
<br>
<p><p>-- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20What%20are%20&quot;AGI-first'ers&quot;%20expecting%20AGI%20will%20teach%20us%20about%20FAI?">matmahoney@yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18452.html">Vladimir Nesov: "Re: Bounded population (was Re: Bounded utility)"</a>
<li><strong>Previous message:</strong> <a href="18450.html">Matt Mahoney: "Re: The role of consciousness (Re: The GLUT and functionalism)"</a>
<li><strong>In reply to:</strong> <a href="18445.html">Rolf Nelson: "What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18453.html">Daniel Burfoot: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18451">[ date ]</a>
<a href="index.html#18451">[ thread ]</a>
<a href="subject.html#18451">[ subject ]</a>
<a href="author.html#18451">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
