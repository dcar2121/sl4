<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Agreements (was Re: Property rights)</title>
<meta name="Author" content="Tim Freeman (tim@fungible.com)">
<meta name="Subject" content="Agreements (was Re: Property rights)">
<meta name="Date" content="2008-04-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Agreements (was Re: Property rights)</h1>
<!-- received="Sun Apr 27 10:43:06 2008" -->
<!-- isoreceived="20080427164306" -->
<!-- sent="Sun, 27 Apr 2008 07:21:22 -0700" -->
<!-- isosent="20080427142122" -->
<!-- name="Tim Freeman" -->
<!-- email="tim@fungible.com" -->
<!-- subject="Agreements (was Re: Property rights)" -->
<!-- id="20080427164045.38BA6D269B@fungible.com" -->
<!-- inreplyto="eeec289b0804262004n58bb09ecjab7ee4129ddfc09b@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tim Freeman (<a href="mailto:tim@fungible.com?Subject=Re:%20Agreements%20(was%20Re:%20Property%20rights)"><em>tim@fungible.com</em></a>)<br>
<strong>Date:</strong> Sun Apr 27 2008 - 08:21:22 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="18721.html">Thomas McCabe: "Thomas McCabe wants to chat"</a>
<li><strong>Previous message:</strong> <a href="18719.html">Rolf Nelson: "Friendliness nomenclature"</a>
<li><strong>In reply to:</strong> <a href="18713.html">Byrne Hobart: "Re: Property rights (was Re: Can't afford to rescue cows)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18728.html">Stuart Armstrong: "Re: Agreements (was Re: Property rights)"</a>
<li><strong>Reply:</strong> <a href="18728.html">Stuart Armstrong: "Re: Agreements (was Re: Property rights)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18720">[ date ]</a>
<a href="index.html#18720">[ thread ]</a>
<a href="subject.html#18720">[ subject ]</a>
<a href="author.html#18720">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
(I'm skipping lots of sensible text I agree with.)
<br>
<p>From: &quot;Byrne Hobart&quot; &lt;<a href="mailto:bhobart@gmail.com?Subject=Re:%20Agreements%20(was%20Re:%20Property%20rights)">bhobart@gmail.com</a>&gt;
<br>
<em>&gt;'Future choice' refers to a choice in the future.
</em><br>
<p>Reading more context than I quoted above, it's apparent that a &quot;future
<br>
choice&quot; is a decision made now to do something later.  Thanks, that
<br>
helps.
<br>
<p><em>&gt;If you're incapable of making commitments regarding the future, it
</em><br>
<em>&gt;would be hard to have property rights, though.
</em><br>
<p>The basic problem is that my scheme [1] is incapable of specifying
<br>
anything about commitments.  I'm not up to saying anything about
<br>
verbal behavior in general because I don't know how.  The AI might
<br>
figure out verbal behavior, but only as part of its general ability to
<br>
learn.  &quot;Commitment&quot; implies some sort of connection between the
<br>
statement made and some future decision-to-do, and in my present
<br>
scheme there is no such connection because in my present scheme,
<br>
statements are completely absent.
<br>
<p>Apparently CEV [2] and the GodAI scheme [3] don't talk about keeping
<br>
commitments either.  I didn't get around to understanding Practical
<br>
Benevolence [4] yet.  Does anyone know of a FAI proposal that aspires
<br>
to get the FAI to keep its agreements for some reason beyond
<br>
expedience?
<br>
<p>If the FAI takes over, then expedience won't constrain it.  You can
<br>
also expect it to be a much better lawyer than you, so I don't know
<br>
how to safely use words with it even if we find some way to make it
<br>
want to keep promises.  This is a standard problem. [5]
<br>
<p><em>&gt;So it all compiles down to mutual agreements; the abstraction just
</em><br>
<em>&gt;makes it easy to talk about.
</em><br>
<p>I agree that it's sufficient to talk about agreements, and that
<br>
enumerating the agreements that make up a property right is clumsy.
<br>
Clumsy is an option for these specifications, but not for a real
<br>
implementation.
<br>
<p><em>&gt;Of course, if you don't believe in such agreements, we can't have property
</em><br>
<em>&gt;rights as I think of them. But it seems like a pretty elementary part of
</em><br>
<em>&gt;human behavior to make promises about the future. If we can't do that, we
</em><br>
<em>&gt;can't have governments, and most forms of anarchism won't work, either. I
</em><br>
<em>&gt;guess we'd be stuck with Stirner.
</em><br>
<p>After a little googling, I'm guessing you mean Max Stirner.  His main
<br>
work seems to be &quot;The Ego and its Own&quot;, which I haven't read yet.  Do
<br>
you recommend starting there?
<br>
<p>To a first approximation, the &quot;Ego&quot; of this proposed AI that doesn't
<br>
understand agreements is basically act utilitarianism, that is, the
<br>
greatest good for the greatest number of humans.  (There are minor
<br>
caveats: it could be configured to care about other sets of entities,
<br>
it could weight them unequally, and then there's respect, which is
<br>
essentially conflict-aversion.)  Rules are verbal behavior, so I can't
<br>
specify rules and therefore rule utilitarianism is not an option.
<br>
<p><em>&gt;I'm starting the AIXI paper right now.
</em><br>
<p>The hardest part in reading it is that there's no glossary of
<br>
notation, and there is a lot of notation that's introduced and then
<br>
not used for a few pages and then relied on later so you can't just
<br>
flip back a page or two to figure out what he meant.  At the end his
<br>
main point slightly abuses his own notation and I had to have my whole
<br>
hand-built glossary there in front of me to guess what he meant.  If
<br>
you have access to a copy of Hutter's subsequent book &quot;Universal
<br>
Artificial Intelligence&quot;, there's a glossary of notation on page xvii,
<br>
and he does seem to cover the same material.  It might be easier to
<br>
start there, if the glossary is accurate.  I haven't checked.
<br>
<p><em>&gt;But while I do that, I have to ask what sort of model you're using for
</em><br>
<em>&gt;an intelligence if it cannot commit in advance to preferring a given
</em><br>
<em>&gt;outcome or choice, especially in the context of being rewarded or
</em><br>
<em>&gt;punished for choices.
</em><br>
<p>Talking about the same AI as earlier (citation [1]), verbal behavior
<br>
is learned.  It is not part of the specification.  It might learn to
<br>
make and usually (or conceivably always) keep commitments, or perhaps
<br>
the environment and circumstances don't require that and it wouldn't
<br>
learn that.  When the time to keep the commitment comes, the
<br>
commitment is part of the past and the only incentive to keep the
<br>
commitment are the relative consequences of keeping it verses the
<br>
alternatives.
<br>
<p><em>&gt;I would be really interested in how you would formalize your views. 
</em><br>
<p>I think you're really interested in a formalization of the AI's point
<br>
of view.  That's in reference [1].
<br>
<p>You might be asking for a formalization of my own point of view about
<br>
how things should be.  To a first approximation, I don't think I have
<br>
one, since I don't think it makes sense to use the word &quot;should&quot; in
<br>
that context.
<br>
<p><em>&gt;What principles do you start with to derive not-property-rights from
</em><br>
<em>&gt;the very small set of pretty fundamental attributes I argued would
</em><br>
<em>&gt;automatically lead to such rights?
</em><br>
<p>The difference in conclusion is primarily caused by the lack of
<br>
ability to specify verbal behavior.  Thus the AI can't presently
<br>
describe its future choice, and therefore commitments and agreements
<br>
and so forth cannot be directly specified.
<br>
<p>If the AI learns verbal behavior, and the AI's planning horizon is
<br>
long enough so the long-term consequences of keeping agreements
<br>
matter, then maybe it's actual behavior would be better than something
<br>
you could easily infer from the spec.  Also, if it has been configured
<br>
to be Friendly toward the people it's making commitments to, and those
<br>
people are relying on it keeping its commitments, then the compassion
<br>
and respect it has for those people are likely to cause it to keep the
<br>
commitments.  
<br>
<p>Or maybe it would feed the hungry methamphetamine addict, if that
<br>
seemed more important.  Hmm, if the methamphetamine addict traded his
<br>
food money for drugs, that would tend to lead the AI to conclude that
<br>
he doesn't care much about eating, which would tend to lead to the AI
<br>
not doing anything extraordinary to feed him.
<br>
<p>Things might get weird toward the end of the planning horizon.  At
<br>
that point, the AI is trying to put the world soon into a state
<br>
desired by the humans, and the humans have a longer planning horizon
<br>
than the AI, so the important long term planning is happening in the
<br>
AI's model of the humans.  I *think* it would still behave reasonably,
<br>
but I'm not sure and devoting some effort to looking for
<br>
counterexamples is probably worthwhile.  So far as I can tell, it's
<br>
important to have a finite planning horizon to avoid indefinitely
<br>
delayed gratification [6].
<br>
<p>If someone can devise an example of using anything resembling AIXI to
<br>
specify verbal behavior, I'd have a lot more choices here.  I tried
<br>
for a while and got nowhere.
<br>
<pre>
-- 
Tim Freeman               <a href="http://www.fungible.com">http://www.fungible.com</a>           <a href="mailto:tim@fungible.com?Subject=Re:%20Agreements%20(was%20Re:%20Property%20rights)">tim@fungible.com</a>
[1] <a href="http://www.fungible.com/respect/paper.html">http://www.fungible.com/respect/paper.html</a>
[2] <a href="http://www.sl4.org/wiki/CoherentExtrapolatedVolition">http://www.sl4.org/wiki/CoherentExtrapolatedVolition</a>
[3] <a href="http://www.neweuropeancentury.org/GodAI.pdf">http://www.neweuropeancentury.org/GodAI.pdf</a>
[4] <a href="http://rationalmorality.info/wp-content/uploads/2007/12/practical-benevolence-2007-12-06_iostemp.pdf">http://rationalmorality.info/wp-content/uploads/2007/12/practical-benevolence-2007-12-06_iostemp.pdf</a>
[5] <a href="http://www.overcomingbias.com/2007/11/complex-wishes.html">http://www.overcomingbias.com/2007/11/complex-wishes.html</a>
[6] <a href="http://www.fungible.com/respect/paper.html#deferred-gratification">http://www.fungible.com/respect/paper.html#deferred-gratification</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18721.html">Thomas McCabe: "Thomas McCabe wants to chat"</a>
<li><strong>Previous message:</strong> <a href="18719.html">Rolf Nelson: "Friendliness nomenclature"</a>
<li><strong>In reply to:</strong> <a href="18713.html">Byrne Hobart: "Re: Property rights (was Re: Can't afford to rescue cows)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18728.html">Stuart Armstrong: "Re: Agreements (was Re: Property rights)"</a>
<li><strong>Reply:</strong> <a href="18728.html">Stuart Armstrong: "Re: Agreements (was Re: Property rights)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18720">[ date ]</a>
<a href="index.html#18720">[ thread ]</a>
<a href="subject.html#18720">[ subject ]</a>
<a href="author.html#18720">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
