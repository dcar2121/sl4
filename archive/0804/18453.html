<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?</title>
<meta name="Author" content="Daniel Burfoot (daniel.burfoot@gmail.com)">
<meta name="Subject" content="Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?">
<meta name="Date" content="2008-04-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?</h1>
<!-- received="Sat Apr 12 20:56:54 2008" -->
<!-- isoreceived="20080413025654" -->
<!-- sent="Sun, 13 Apr 2008 11:53:26 +0900" -->
<!-- isosent="20080413025326" -->
<!-- name="Daniel Burfoot" -->
<!-- email="daniel.burfoot@gmail.com" -->
<!-- subject="Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?" -->
<!-- id="eafe728f0804121953j428a761cx5aeb734c37f79e25@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="79ecaa350804120811k31e05967v1948ef949b2f4175@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Daniel Burfoot (<a href="mailto:daniel.burfoot@gmail.com?Subject=Re:%20What%20are%20&quot;AGI-first'ers&quot;%20expecting%20AGI%20will%20teach%20us%20about%20FAI?"><em>daniel.burfoot@gmail.com</em></a>)<br>
<strong>Date:</strong> Sat Apr 12 2008 - 20:53:26 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="18454.html">Nick Tarleton: "What should we do (Re: The role of consciousness)"</a>
<li><strong>Previous message:</strong> <a href="18452.html">Vladimir Nesov: "Re: Bounded population (was Re: Bounded utility)"</a>
<li><strong>In reply to:</strong> <a href="18445.html">Rolf Nelson: "What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18462.html">Rolf Nelson: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<li><strong>Reply:</strong> <a href="18462.html">Rolf Nelson: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18453">[ date ]</a>
<a href="index.html#18453">[ thread ]</a>
<a href="subject.html#18453">[ subject ]</a>
<a href="author.html#18453">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Sun, Apr 13, 2008 at 12:11 AM, Rolf Nelson &lt;<a href="mailto:rolf.h.d.nelson@gmail.com?Subject=Re:%20What%20are%20&quot;AGI-first'ers&quot;%20expecting%20AGI%20will%20teach%20us%20about%20FAI?">rolf.h.d.nelson@gmail.com</a>&gt;
<br>
wrote:
<br>
<p><em>&gt; Large numbers of people have made various AI advances in the past.
</em><br>
<p><p>It's not clear to me that this statement is true, in the following sense: I
<br>
don't necessarily believe that any particular piece of current AI theory
<br>
(whatever that is) will ultimately be useful for building an AGI. On the
<br>
other hand, many advances were useful in the sense of explicating problems
<br>
and exploring why certain methods aren't as powerful as we might think.
<br>
<p>Of course, this depends on the granularity with which you define &quot;advance&quot;.
<br>
I think reinforcement learning is an advance, but only if defined in the
<br>
broadest possible terms (an agent pursuing reward in an uncertain world). I
<br>
don't necessarily believe any current RL algorithm will help with AGI - the
<br>
formalism is just too limiting. I would make a similar statement for neural
<br>
networks and statistical learning theory.
<br>
<p><p><em>&gt; At what point will you know that AGI has advanced enough that FAI can
</em><br>
<em>&gt; proceed?
</em><br>
<p><p>This is an interesting question. I would say AGI is nearly ready if one
<br>
could define a general purpose algorithm that provides the solution, or a
<br>
core element of the solution, to a wide variety of tasks like face
<br>
recogition, speech recognition, computer vision, and motion control; all
<br>
without being specifically designed for those purposes.
<br>
<p>Regarding the question of how AGI will help for FAI, I consider it
<br>
reasonable to believe that if an AGI can learn abstractions, as it must in
<br>
order to become intelligent, then it can also learn the abstraction &quot;good&quot;,
<br>
if seeded with an appropriately large amount of knowledge about human
<br>
culture. This relates to Plato's notion &quot;forms&quot; and in particular the &quot;Form
<br>
of the Good&quot;.
<br>
<p>Dan
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18454.html">Nick Tarleton: "What should we do (Re: The role of consciousness)"</a>
<li><strong>Previous message:</strong> <a href="18452.html">Vladimir Nesov: "Re: Bounded population (was Re: Bounded utility)"</a>
<li><strong>In reply to:</strong> <a href="18445.html">Rolf Nelson: "What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18462.html">Rolf Nelson: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<li><strong>Reply:</strong> <a href="18462.html">Rolf Nelson: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18453">[ date ]</a>
<a href="index.html#18453">[ thread ]</a>
<a href="subject.html#18453">[ subject ]</a>
<a href="author.html#18453">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
