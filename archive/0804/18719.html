<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Friendliness nomenclature</title>
<meta name="Author" content="Rolf Nelson (rolf.h.d.nelson@gmail.com)">
<meta name="Subject" content="Friendliness nomenclature">
<meta name="Date" content="2008-04-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Friendliness nomenclature</h1>
<!-- received="Sun Apr 27 10:18:55 2008" -->
<!-- isoreceived="20080427161855" -->
<!-- sent="Sun, 27 Apr 2008 12:16:41 -0400" -->
<!-- isosent="20080427161641" -->
<!-- name="Rolf Nelson" -->
<!-- email="rolf.h.d.nelson@gmail.com" -->
<!-- subject="Friendliness nomenclature" -->
<!-- id="79ecaa350804270916o1dbd20c9n5e68a7030e8fc796@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Rolf Nelson (<a href="mailto:rolf.h.d.nelson@gmail.com?Subject=Re:%20Friendliness%20nomenclature"><em>rolf.h.d.nelson@gmail.com</em></a>)<br>
<strong>Date:</strong> Sun Apr 27 2008 - 10:16:41 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="18720.html">Tim Freeman: "Agreements (was Re: Property rights)"</a>
<li><strong>Previous message:</strong> <a href="18718.html">Byrne Hobart: "Re: Property rights (was Re: Can't afford to rescue cows)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18719">[ date ]</a>
<a href="index.html#18719">[ thread ]</a>
<a href="subject.html#18719">[ subject ]</a>
<a href="author.html#18719">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Here's my proposal for terminology.
<br>
<p>A &quot;Friendly AI&quot; is an *AGI* that has a *deliberately* positive, rather than
<br>
a negative, impact on humanity.
<br>
<p>1. A narrow AI that diagnoses cancer is not a &quot;Friendly AI&quot; because it is
<br>
not an AGI; it is trivially controllable, and its intelligence is not
<br>
powerful nor general enough to deliberately cause harm against the wishes of
<br>
its creators and operators.
<br>
<p>2. Suppose a rogue, but boxed, AGI almost destroys the world, but we use
<br>
that AGI to learn from our mistakes, and in the end we realize it was an odd
<br>
stroke of luck that we had that particular boxed AGI to learn from. The AGI
<br>
made the world a better place, but only accidentally (from the AGI's point
<br>
of view); it was not part of the AGI's goal structure to do so in that way.
<br>
Because its positive impact was not a *deliberate* part of its goal
<br>
structure, the AGI is not &quot;Friendly&quot; per se.
<br>
<p>The semantic &quot;prototype&quot; is a deliberately-built AGI that safely ushers
<br>
humanity through the Singularity. Weaker semantic examples of the category:
<br>
<p>1. An AGI that *would* be a Friendly AI in most Possible Worlds, but fails
<br>
in ours, for example because Jason Vorhees is watching and stabs everyone
<br>
when we try to turn the Friendly AI on.
<br>
<p>2. An AGI that is Friendly towards Joe Smith of 1243 Maple Drive, but that
<br>
kills the rest of us at Joe's bidding.
<br>
<p>3. A powerful AGI that gives everyone a free popsicle, and then chooses to
<br>
shut itself off.
<br>
<p>Extremely weak semantic examples:
<br>
<p>4. A human has (non-artificial) General Intelligence, so an extremely broad
<br>
model of Friendliness might characterize humans as (capital-F) Friendly, by
<br>
analogy with a Friendly AI.
<br>
<p>5. A model may view Friendliness as a chain of systems: FAI-1 builds FAI-2,
<br>
FAI-2 builds FAI-3, etc. An AGI is Friendly in such a model if it is
<br>
&quot;terminally&quot; Friendly, or if it builds an AGI that is friendly. By
<br>
induction, a human who builds FAI-1 might be considered Friendly by such a
<br>
model. (However, evolution would not be considered &quot;Friendly&quot; even though it
<br>
built the human, because it fails the &quot;deliberateness&quot; requirement.)
<br>
<p>A Friendly AI project cleaves naturally into two challenges:
<br>
<p>1. Friendliness Theory: How can an AGI remain Friendly through successive
<br>
rounds of modifications? How can you get an AGI to want what you &quot;really&quot;
<br>
want, and what do we mean by &quot;what you really want&quot;?
<br>
<p>2. AGI Ethics: If the Friendliness Theory problem were solved for an AGI,
<br>
what would it be ethical for us to do with that AGI?
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18720.html">Tim Freeman: "Agreements (was Re: Property rights)"</a>
<li><strong>Previous message:</strong> <a href="18718.html">Byrne Hobart: "Re: Property rights (was Re: Can't afford to rescue cows)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18719">[ date ]</a>
<a href="index.html#18719">[ thread ]</a>
<a href="subject.html#18719">[ subject ]</a>
<a href="author.html#18719">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
