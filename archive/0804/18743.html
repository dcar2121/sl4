<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Defining friendliness (was Re: Can't afford to resuce cows)</title>
<meta name="Author" content="Matt Mahoney (matmahoney@yahoo.com)">
<meta name="Subject" content="Defining friendliness (was Re: Can't afford to resuce cows)">
<meta name="Date" content="2008-04-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Defining friendliness (was Re: Can't afford to resuce cows)</h1>
<!-- received="Mon Apr 28 16:58:11 2008" -->
<!-- isoreceived="20080428225811" -->
<!-- sent="Mon, 28 Apr 2008 15:56:02 -0700 (PDT)" -->
<!-- isosent="20080428225602" -->
<!-- name="Matt Mahoney" -->
<!-- email="matmahoney@yahoo.com" -->
<!-- subject="Defining friendliness (was Re: Can't afford to resuce cows)" -->
<!-- id="584306.44087.qm@web51911.mail.re2.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="38f493f10804280128w764a82a4h762f5ee1fe5609cf@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Matt Mahoney (<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Defining%20friendliness%20(was%20Re:%20Can't%20afford%20to%20resuce%20cows)"><em>matmahoney@yahoo.com</em></a>)<br>
<strong>Date:</strong> Mon Apr 28 2008 - 16:56:02 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="18744.html">Thomas McCabe: "Re: CEV specifies who the AI cares about (was Re: Can't afford to rescue cows)"</a>
<li><strong>Previous message:</strong> <a href="18742.html">Nick Tarleton: "Re: CEV specifies who the AI cares about (was Re: Can't afford to rescue cows)"</a>
<li><strong>In reply to:</strong> <a href="18725.html">Stuart Armstrong: "Re: Can't afford to resuce cows (was Re: Arbitrarily decide who benefits)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18687.html">Samantha Atkins: "Re: Arbitrarily decide who benefits (was Re: Bounded population)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18743">[ date ]</a>
<a href="index.html#18743">[ thread ]</a>
<a href="subject.html#18743">[ subject ]</a>
<a href="author.html#18743">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
--- Stuart Armstrong &lt;<a href="mailto:dragondreaming@googlemail.com?Subject=Re:%20Defining%20friendliness%20(was%20Re:%20Can't%20afford%20to%20resuce%20cows)">dragondreaming@googlemail.com</a>&gt; wrote:
<br>
<p><em>&gt; But we still have to solve the friendliness problem long before we
</em><br>
<em>&gt; begin to worry about the details of that uploaded world...
</em><br>
<p>Before we can solve it, we have to define it.
<br>
<p>I am aware that CEV is a definition.  However, it is human-centered. 
<br>
Should AI grant the extrapolated wishes of animals?  If so, which
<br>
species?  What about embryos?  What about convicts?
<br>
<p>In the future we will have to ask much harder question like this about
<br>
machines that are &quot;sort of&quot; human.  What about the &quot;original&quot; you when
<br>
you step into a teleportation booth?  What about robot slaves who look
<br>
and act human except that they only want to serve us?  Does it matter
<br>
if  the robot has a copy of the memories of one individual, or a blend
<br>
of many people?  Should AI &quot;free&quot; the robot by reprogramming its
<br>
motivational system because afterwards it will be &quot;happier&quot; that it
<br>
did?
<br>
<p>I am not looking for answers to these questions, because there are
<br>
thousands more like it, and it gets tedious.  I know this has been
<br>
discussed before.  The answer is that the AI will be thousands of times
<br>
smarter than us so it will just figure out all the right answers.  I
<br>
don't buy it.  The trend is in the opposite direction.  In the ancient
<br>
past there was no dispute about abortion.  It did not exist.  Now we
<br>
dispute stem cell research and cloning.  In the ancient past there was
<br>
no dispute about animal rights.  They had none.  Now we have PETA.  In
<br>
the present there is no dispute about machine rights.  They have none. 
<br>
Do you really expect future machines to agree?
<br>
<p>Friendliness is defined in the context of ethical beliefs, for example,
<br>
the &quot;average ethical beliefs of all humans currently alive&quot;.  This is
<br>
reasonable in our current world where there is a sharp line between
<br>
human and nonhuman.  As the line gets fuzzier, those on the inside will
<br>
make decisions about whom to include or exclude.  Do we include
<br>
uploads?  Do multiple copies of uploads have more rights than a single
<br>
copy?  How is this decision making process stable against growing to
<br>
include insects and nanobots or shrinking to include just a single
<br>
godlike AI?
<br>
<p>How is volition defined when goals are programmable?  I know that an AI
<br>
should not want to modify its own goals, because if it could it would
<br>
program itself to be a happy idiot.  Evolution has eliminated this
<br>
capability in ourselves.  But our ethics allow us to program the goals
<br>
of others.  We want our children to not want to cheat, steal, or lie. 
<br>
We want drug addicts to not want drugs.  So what's wrong with an AI
<br>
reprogramming you to be a happy idiot?
<br>
<p>I know about apotheosis.
<br>
<a href="http://sysopmind.com/sing/principles.html#apotheosis">http://sysopmind.com/sing/principles.html#apotheosis</a>
<br>
<p>I don't buy it.  We feel fear, pain, and suffering because our
<br>
ancestors who didn't did not pass on their DNA.  Happiness is
<br>
increasing utility, dU(x)/dt.  It is mathematically bounded over finite
<br>
x, sorry.  We could eliminate the source of our fears and suffering but
<br>
then we would not appreciate how much better the world is.  Do you
<br>
really think you would be happier if you could have everything you
<br>
want?  A simulated world with a magic genie?  In the real world you are
<br>
just a happy idiot, just a program with no I/O.
<br>
<p><p>-- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20Defining%20friendliness%20(was%20Re:%20Can't%20afford%20to%20resuce%20cows)">matmahoney@yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18744.html">Thomas McCabe: "Re: CEV specifies who the AI cares about (was Re: Can't afford to rescue cows)"</a>
<li><strong>Previous message:</strong> <a href="18742.html">Nick Tarleton: "Re: CEV specifies who the AI cares about (was Re: Can't afford to rescue cows)"</a>
<li><strong>In reply to:</strong> <a href="18725.html">Stuart Armstrong: "Re: Can't afford to resuce cows (was Re: Arbitrarily decide who benefits)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18687.html">Samantha Atkins: "Re: Arbitrarily decide who benefits (was Re: Bounded population)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18743">[ date ]</a>
<a href="index.html#18743">[ thread ]</a>
<a href="subject.html#18743">[ subject ]</a>
<a href="author.html#18743">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
