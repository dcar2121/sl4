<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?</title>
<meta name="Author" content="Vladimir Nesov (robotact@gmail.com)">
<meta name="Subject" content="Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?">
<meta name="Date" content="2008-04-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?</h1>
<!-- received="Mon Apr 28 10:58:06 2008" -->
<!-- isoreceived="20080428165806" -->
<!-- sent="Mon, 28 Apr 2008 20:56:03 +0400" -->
<!-- isosent="20080428165603" -->
<!-- name="Vladimir Nesov" -->
<!-- email="robotact@gmail.com" -->
<!-- subject="Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?" -->
<!-- id="b54769d90804280956g40f1ed6ane1d55012576ab739@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="4812E3F9.6000500@gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Vladimir Nesov (<a href="mailto:robotact@gmail.com?Subject=Re:%20What%20are%20&quot;AGI-first'ers&quot;%20expecting%20AGI%20will%20teach%20us%20about%20FAI?"><em>robotact@gmail.com</em></a>)<br>
<strong>Date:</strong> Mon Apr 28 2008 - 10:56:03 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="18737.html">Tim Freeman: "What if there's an expiration date? (was Re: Arbitrarily decide who benefits)"</a>
<li><strong>Previous message:</strong> <a href="18735.html">Samantha Atkins: "Re: Arbitrarily decide who benefits (was Re: Bounded population)"</a>
<li><strong>In reply to:</strong> <a href="18683.html">Samantha Atkins: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18736">[ date ]</a>
<a href="index.html#18736">[ thread ]</a>
<a href="subject.html#18736">[ subject ]</a>
<a href="author.html#18736">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hello Samantha, thanks for the feedback.
<br>
<p>On Sat, Apr 26, 2008 at 12:12 PM, Samantha Atkins &lt;<a href="mailto:sjatkins@gmail.com?Subject=Re:%20What%20are%20&quot;AGI-first'ers&quot;%20expecting%20AGI%20will%20teach%20us%20about%20FAI?">sjatkins@gmail.com</a>&gt; wrote:
<br>
<em>&gt; Vladimir Nesov wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; I think that the right kind of investigation of Friendliness should in
</em><br>
<em>&gt; &gt; fact help in AGI, not the other way around. It should be able to
</em><br>
<em>&gt; &gt; formulate the problem that actually needs solving, in a form that can
</em><br>
<em>&gt; &gt; be addressed. The main question of Friendliness theory is how to build
</em><br>
<em>&gt; &gt; a system that significantly helps us (which results in great changes
</em><br>
<em>&gt; &gt; to us and the world), while at the same time (with the goal of)
</em><br>
<em>&gt; &gt; preserving and developing things that we care about.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; Since we are quite aware of how limited our intelligence is and how tangled
</em><br>
<em>&gt; and suspect the roots of our values are, I am not able to be sure that
</em><br>
<em>&gt; &quot;things that we care about&quot; are the best a superior mind can come up with or
</em><br>
<em>&gt; should unduly limit it.  I am not at all sure that even a super efficient
</em><br>
<em>&gt; direct extrapolation from the kinds of beings we are leads to the very best
</em><br>
<em>&gt; that can be for us much less to the best state for the local universe.
</em><br>
<em>&gt;
</em><br>
<p>I apparently used &quot;things we care about&quot; in a deeper sense than
<br>
&quot;results of the poll&quot;. You say &quot;direct extrapolation&quot; -- what do you
<br>
mean by that? Whatever path you *choose*, it's some kind of
<br>
extrapolation, and what I'm addressing in this message is a general
<br>
idea of how it could be expressed. How to decide which path to take?
<br>
If you ignore humans as a core of such extrapolation, what do you base
<br>
extrapolation on? Not that human-based Friendly modification of the
<br>
kind I talk about must stick with particular apish properties, it
<br>
might as well  develop something much more &quot;pure&quot;, who knows... I
<br>
think it most certainly will.
<br>
<p>Friendly modification is supposed to act as a kind of extension to
<br>
*your* intelligence, even if it's implemented separately. It's not
<br>
supposed to have will of its own, but to guess your intentions and
<br>
help to further them without contaminating them with its own bias,
<br>
developing technologies, providing advice. And do this at the
<br>
fundamental levels of organization, not in a &quot;genie mode&quot;. It's a
<br>
direct analogy to what your own intelligence does for you. Is your
<br>
mind always Friendly to you? How do you develop a system that is at
<br>
least as Friendly to you as your own mind? Or even more Friendly? What
<br>
is the measure of how much a mind is Friendly to itself? What kind of
<br>
system is Friendly in this sense to some blob of physical matter?
<br>
<p><p><em>&gt; &gt; This role looks very much like what intelligence should do in general.
</em><br>
<em>&gt; &gt; Currently, intelligence enables simple drives inbuilt in our biology
</em><br>
<em>&gt; &gt; to have their way in situations that they can never comprehend and
</em><br>
<em>&gt; &gt; which were not around at the time they were programmed in by
</em><br>
<em>&gt; &gt; evolution. Intelligence empowers these drives, allows them to deal
</em><br>
<em>&gt; &gt; with many novel situations and solve problems which they can't on
</em><br>
<em>&gt; &gt; their own, while carrying on what their intention was.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; Intelligence to some degree goes beyond those drives, sees the limits of
</em><br>
<em>&gt; their utility and what may be better.  If we are to 'become as gods' then we
</em><br>
<em>&gt; must at some point somehow go beyond our evolutionary psychology.    A
</em><br>
<em>&gt; psychological ape will not enjoy being an upload except in a carefully
</em><br>
<em>&gt; crafted virtual monkey house.   A psychological ape will not even enjoy an
</em><br>
<em>&gt; indefinitely long life of apish pleasures in countless variations.  At some
</em><br>
<em>&gt; point we are more and more not as our EP says.
</em><br>
<em>&gt;
</em><br>
<p>Yes, we move further away from our initial human nature, but in which
<br>
direction? E.g. randomly jumping to become a being of eternal pure
<br>
suffering looks like a bad choice. There are many bad choices out
<br>
there, and precise understanding of our current nature should be a
<br>
better guide on this road than random decrees issued by committee of
<br>
moral philosophers.
<br>
<p><p><em>&gt; &gt; This process is not perfect, so in modern environment some of purposes
</em><br>
<em>&gt; &gt; don't play out. People eat wrong foods and become ill, or decide not
</em><br>
<em>&gt; &gt; to have many children. Propagation of DNA is no longer a very
</em><br>
<em>&gt; &gt; significant goal for humans. This is an example of subtly Unfriendly
</em><br>
<em>&gt; &gt; AI, the kind that Friendliness-blind AGI development can end up
</em><br>
<em>&gt; &gt; supplying: it works great at first and *seems* to follow its intended
</em><br>
<em>&gt; &gt; goals very reliably, but in the end it has all the control and starts
</em><br>
<em>&gt; &gt; to ignore initial purpose.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; Are you saying that good AGI must keep us being happy little breeders?
</em><br>
<em>&gt; Purpose evolves or it is a dead endless circle.
</em><br>
<em>&gt;
</em><br>
<p>We are not as single-minded as idealized DNA replicators. Humans have
<br>
enough moral anarchy to explore all kinds of possibilities, but as we
<br>
are seemingly constrained by physical limitations, we'd have to
<br>
prioritize based on something.
<br>
<p><p><em>&gt; &gt; Grasping the principles by which modification to a system results in a
</em><br>
<em>&gt; &gt; different dynamics that can be said to preserve intention of initial
</em><br>
<em>&gt; &gt; dynamics, while obviously altering the way it operates, can, I think,
</em><br>
<em>&gt; &gt; be a key to general intelligence. If this intention-preserving
</em><br>
<em>&gt; &gt; modification process is expressed on low level, it doesn't need to
</em><br>
<em>&gt; &gt; have higher-level anthropic concepts engraved on its circuits, it even
</em><br>
<em>&gt; &gt; doesn't need to know about humans. It can be a *simple* statistical
</em><br>
<em>&gt; &gt; creature. All it needs is to extrapolate the development of our corner
</em><br>
<em>&gt; &gt; of the universe, where humans are the main statistical anomaly. It
</em><br>
<em>&gt; &gt; will automatically figure out what does it mean to be Friendly, if
</em><br>
<em>&gt; &gt; such is its nature.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; I don't for a moment believe that the wise guiding of the development and
</em><br>
<em>&gt; evolution of the human species will or can be achieved by some automated
</em><br>
<em>&gt; statistical process.    To me that is much more dangerously un-sane a notion
</em><br>
<em>&gt; that simply developing actual AGI as quickly as possible because we need the
</em><br>
<em>&gt; intelligence NOW.
</em><br>
<em>&gt;
</em><br>
<p>Any AGI is some kind of &quot;automated statistical process&quot;. Likely I'm
<br>
wrong about the part where it's unnecessary to even tell this AGI
<br>
about humans, but the main point of my argument is in shifting the
<br>
focus from AGI that needs to be told what to do, to AGI that by its
<br>
nature does what we want, without explicit Friendliness content or
<br>
careless demands. It doesn't base its Friendliness on the idea that
<br>
was taught by human programmers on initial stages of development and
<br>
later refined, but on its study of physical makeup of humans or
<br>
civilization as a whole (directly performed on much-higher-than quarks
<br>
levels of organisation, of course).
<br>
<p><p>I think this line can probably help in developing actual AGI of any
<br>
kind: a smarter AGI can be obtained by stumping a Friendly
<br>
modification device on top of a stupider one, so figuring out Friendly
<br>
modification should provide a specification for scalable AGI system
<br>
(and vice versa). I'm currently further down the road on the AGI side,
<br>
but adding another view from the side of Friendliness enriches the
<br>
perspective, which helps as another sanity check.
<br>
<p><pre>
-- 
Vladimir Nesov
<a href="mailto:robotact@gmail.com?Subject=Re:%20What%20are%20&quot;AGI-first'ers&quot;%20expecting%20AGI%20will%20teach%20us%20about%20FAI?">robotact@gmail.com</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18737.html">Tim Freeman: "What if there's an expiration date? (was Re: Arbitrarily decide who benefits)"</a>
<li><strong>Previous message:</strong> <a href="18735.html">Samantha Atkins: "Re: Arbitrarily decide who benefits (was Re: Bounded population)"</a>
<li><strong>In reply to:</strong> <a href="18683.html">Samantha Atkins: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18736">[ date ]</a>
<a href="index.html#18736">[ thread ]</a>
<a href="subject.html#18736">[ subject ]</a>
<a href="author.html#18736">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
