<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?</title>
<meta name="Author" content="Samantha Atkins (sjatkins@gmail.com)">
<meta name="Subject" content="Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?">
<meta name="Date" content="2008-04-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?</h1>
<!-- received="Sat Apr 26 02:15:00 2008" -->
<!-- isoreceived="20080426081500" -->
<!-- sent="Sat, 26 Apr 2008 01:12:41 -0700" -->
<!-- isosent="20080426081241" -->
<!-- name="Samantha Atkins" -->
<!-- email="sjatkins@gmail.com" -->
<!-- subject="Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?" -->
<!-- id="4812E3F9.6000500@gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="b54769d90804130847q28810762g6d4483c2c4546b7c@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:sjatkins@gmail.com?Subject=Re:%20What%20are%20&quot;AGI-first'ers&quot;%20expecting%20AGI%20will%20teach%20us%20about%20FAI?"><em>sjatkins@gmail.com</em></a>)<br>
<strong>Date:</strong> Sat Apr 26 2008 - 02:12:41 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="18684.html">Samantha Atkins: "Re: AGI investment (Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?)"</a>
<li><strong>Previous message:</strong> <a href="18682.html">Lee Corbin: "Re: Shock level 4 (was Re: META SL4)"</a>
<li><strong>In reply to:</strong> <a href="18464.html">Vladimir Nesov: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18736.html">Vladimir Nesov: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<li><strong>Reply:</strong> <a href="18736.html">Vladimir Nesov: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18683">[ date ]</a>
<a href="index.html#18683">[ thread ]</a>
<a href="subject.html#18683">[ subject ]</a>
<a href="author.html#18683">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Vladimir Nesov wrote:
<br>
<em>&gt; I think that the right kind of investigation of Friendliness should in
</em><br>
<em>&gt; fact help in AGI, not the other way around. It should be able to
</em><br>
<em>&gt; formulate the problem that actually needs solving, in a form that can
</em><br>
<em>&gt; be addressed. The main question of Friendliness theory is how to build
</em><br>
<em>&gt; a system that significantly helps us (which results in great changes
</em><br>
<em>&gt; to us and the world), while at the same time (with the goal of)
</em><br>
<em>&gt; preserving and developing things that we care about.
</em><br>
<em>&gt;   
</em><br>
<p>Since we are quite aware of how limited our intelligence is and how 
<br>
tangled and suspect the roots of our values are, I am not able to be 
<br>
sure that &quot;things that we care about&quot; are the best a superior mind can 
<br>
come up with or should unduly limit it.  I am not at all sure that even 
<br>
a super efficient direct extrapolation from the kinds of beings we are 
<br>
leads to the very best that can be for us much less to the best state 
<br>
for the local universe.  
<br>
<p><em>&gt; This role looks very much like what intelligence should do in general.
</em><br>
<em>&gt; Currently, intelligence enables simple drives inbuilt in our biology
</em><br>
<em>&gt; to have their way in situations that they can never comprehend and
</em><br>
<em>&gt; which were not around at the time they were programmed in by
</em><br>
<em>&gt; evolution. Intelligence empowers these drives, allows them to deal
</em><br>
<em>&gt; with many novel situations and solve problems which they can't on
</em><br>
<em>&gt; their own, while carrying on what their intention was.
</em><br>
<em>&gt;   
</em><br>
Intelligence to some degree goes beyond those drives, sees the limits of 
<br>
their utility and what may be better.  If we are to 'become as gods' 
<br>
then we must at some point somehow go beyond our evolutionary 
<br>
psychology.    A psychological ape will not enjoy being an upload except 
<br>
in a carefully crafted virtual monkey house.   A psychological ape will 
<br>
not even enjoy an indefinitely long life of apish pleasures in countless 
<br>
variations.  At some point we are more and more not as our EP says.  
<br>
<em>&gt; This process is not perfect, so in modern environment some of purposes
</em><br>
<em>&gt; don't play out. People eat wrong foods and become ill, or decide not
</em><br>
<em>&gt; to have many children. Propagation of DNA is no longer a very
</em><br>
<em>&gt; significant goal for humans. This is an example of subtly Unfriendly
</em><br>
<em>&gt; AI, the kind that Friendliness-blind AGI development can end up
</em><br>
<em>&gt; supplying: it works great at first and *seems* to follow its intended
</em><br>
<em>&gt; goals very reliably, but in the end it has all the control and starts
</em><br>
<em>&gt; to ignore initial purpose.
</em><br>
<em>&gt;   
</em><br>
Are you saying that good AGI must keep us being happy little breeders?  
<br>
Purpose evolves or it is a dead endless circle.
<br>
<p><em>&gt; Grasping the principles by which modification to a system results in a
</em><br>
<em>&gt; different dynamics that can be said to preserve intention of initial
</em><br>
<em>&gt; dynamics, while obviously altering the way it operates, can, I think,
</em><br>
<em>&gt; be a key to general intelligence. If this intention-preserving
</em><br>
<em>&gt; modification process is expressed on low level, it doesn't need to
</em><br>
<em>&gt; have higher-level anthropic concepts engraved on its circuits, it even
</em><br>
<em>&gt; doesn't need to know about humans. It can be a *simple* statistical
</em><br>
<em>&gt; creature. All it needs is to extrapolate the development of our corner
</em><br>
<em>&gt; of the universe, where humans are the main statistical anomaly. It
</em><br>
<em>&gt; will automatically figure out what does it mean to be Friendly, if
</em><br>
<em>&gt; such is its nature.
</em><br>
<em>&gt;
</em><br>
<em>&gt;   
</em><br>
I don't for a moment believe that the wise guiding of the development 
<br>
and evolution of the human species will or can be achieved by some 
<br>
automated statistical process.    To me that is much more dangerously 
<br>
un-sane a notion that simply developing actual AGI as quickly as 
<br>
possible because we need the intelligence NOW. 
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18684.html">Samantha Atkins: "Re: AGI investment (Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?)"</a>
<li><strong>Previous message:</strong> <a href="18682.html">Lee Corbin: "Re: Shock level 4 (was Re: META SL4)"</a>
<li><strong>In reply to:</strong> <a href="18464.html">Vladimir Nesov: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18736.html">Vladimir Nesov: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<li><strong>Reply:</strong> <a href="18736.html">Vladimir Nesov: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18683">[ date ]</a>
<a href="index.html#18683">[ thread ]</a>
<a href="subject.html#18683">[ subject ]</a>
<a href="author.html#18683">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
