<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?</title>
<meta name="Author" content="Daniel Burfoot (daniel.burfoot@gmail.com)">
<meta name="Subject" content="Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?">
<meta name="Date" content="2008-04-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?</h1>
<!-- received="Mon Apr 14 06:30:03 2008" -->
<!-- isoreceived="20080414123003" -->
<!-- sent="Mon, 14 Apr 2008 21:26:31 +0900" -->
<!-- isosent="20080414122631" -->
<!-- name="Daniel Burfoot" -->
<!-- email="daniel.burfoot@gmail.com" -->
<!-- subject="Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?" -->
<!-- id="eafe728f0804140526k1b013c0dv560b264dff336135@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="79ecaa350804130641j3f281ca6v1593c7eec03382eb@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Daniel Burfoot (<a href="mailto:daniel.burfoot@gmail.com?Subject=Re:%20What%20are%20&quot;AGI-first'ers&quot;%20expecting%20AGI%20will%20teach%20us%20about%20FAI?"><em>daniel.burfoot@gmail.com</em></a>)<br>
<strong>Date:</strong> Mon Apr 14 2008 - 06:26:31 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="18473.html">Tim Freeman: "Arbitrarily decide who benefits (was Re: Bounded population)"</a>
<li><strong>Previous message:</strong> <a href="18471.html">Vladimir Nesov: "Re: Bounded population (was Re: Bounded utility)"</a>
<li><strong>In reply to:</strong> <a href="18462.html">Rolf Nelson: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18459.html">Samantha Atkins: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18472">[ date ]</a>
<a href="index.html#18472">[ thread ]</a>
<a href="subject.html#18472">[ subject ]</a>
<a href="author.html#18472">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Sun, Apr 13, 2008 at 10:41 PM, Rolf Nelson &lt;<a href="mailto:rolf.h.d.nelson@gmail.com?Subject=Re:%20What%20are%20&quot;AGI-first'ers&quot;%20expecting%20AGI%20will%20teach%20us%20about%20FAI?">rolf.h.d.nelson@gmail.com</a>&gt;
<br>
wrote:
<br>
<p><em>&gt; On Sat, Apr 12, 2008 at 10:53 PM, Daniel Burfoot
</em><br>
<em>&gt; &lt;<a href="mailto:daniel.burfoot@gmail.com?Subject=Re:%20What%20are%20&quot;AGI-first'ers&quot;%20expecting%20AGI%20will%20teach%20us%20about%20FAI?">daniel.burfoot@gmail.com</a>&gt; wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; This is an interesting question. I would say AGI is nearly ready if one
</em><br>
<em>&gt; &gt; could define a general purpose algorithm that provides the solution, or
</em><br>
<em>&gt; a
</em><br>
<em>&gt; &gt; core element of the solution, to a wide variety of tasks like face
</em><br>
<em>&gt; &gt; recogition, speech recognition, computer vision, and motion control; all
</em><br>
<em>&gt; &gt; without being specifically designed for those purposes.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Call this the Burfoot Date for now.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1. How confident are you that AGI wouldn't have taken over by the
</em><br>
<em>&gt; Burfoot Date?
</em><br>
<p><p>I would say that taking over the world is strictly more difficult than face
<br>
recognition etc. I don't consider this to be an obvious statement, however
<br>
(I can imagine, but deem unlikely, an AI that could take over the world
<br>
without being able to recognize faces). I expect the AI would have to
<br>
perform significantly more learning in order to obtain intelligence
<br>
sufficient to &quot;take over&quot;.
<br>
<p>2. Assuming AGI hasn't taken over by the Burfoot Date, how much time
<br>
<em>&gt; would remain between the Burfoot Date and when the AGI takes over?
</em><br>
<p><p>I imagine the appropriate time scale would be on the order of years.
<br>
<p>I also don't consider it inevitable that the AGI would take over, given the
<br>
above mentioned abilities. Humans were at about our current level of
<br>
intelligence for a long time before modern civilization came about. Thus, an
<br>
agent can have intelligence but for whatever reason not do anything with it.
<br>
<p><p>3. How will you proceed when the Burfoot Date comes up? How do you
<br>
<em>&gt; believe others will proceed after the Burfoot Date?
</em><br>
<p><p>It's far enough away that I haven't yet worried too much about it. However,
<br>
I would consider various safeguards appropriate:
<br>
<p>1) limiting the amount of computing power available to the AI
<br>
2) limiting the amount of energy available to the AI
<br>
3) advocating government oversight of further research
<br>
4) limiting the AI to passive observation of the world
<br>
5) limiting the types of goal functions that are given to the AI
<br>
<p>Of course, I don't believe that these safeguards are perfect. Regarding
<br>
others, I'm not sure what they'll do and that causes me some lack of sleep,
<br>
but not too much, as the Burfoot Date is still quite far away, I think.
<br>
<p>As an amusing aside, Avogadro did not know the value of his number even to
<br>
within an order of magnitude.
<br>
<p>Dan
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18473.html">Tim Freeman: "Arbitrarily decide who benefits (was Re: Bounded population)"</a>
<li><strong>Previous message:</strong> <a href="18471.html">Vladimir Nesov: "Re: Bounded population (was Re: Bounded utility)"</a>
<li><strong>In reply to:</strong> <a href="18462.html">Rolf Nelson: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18459.html">Samantha Atkins: "Re: What are &quot;AGI-first'ers&quot; expecting AGI will teach us about FAI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18472">[ date ]</a>
<a href="index.html#18472">[ thread ]</a>
<a href="subject.html#18472">[ subject ]</a>
<a href="author.html#18472">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
