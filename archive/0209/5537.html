<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: continuity of self</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: continuity of self">
<meta name="Date" content="2002-09-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: continuity of self</h1>
<!-- received="Tue Sep 17 13:52:10 2002" -->
<!-- isoreceived="20020917195210" -->
<!-- sent="Tue, 17 Sep 2002 13:28:15 -0400" -->
<!-- isosent="20020917172815" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: continuity of self" -->
<!-- id="3D87662F.1090307@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJOEOHDJAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20continuity%20of%20self"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Sep 17 2002 - 11:28:15 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5538.html">Ben Goertzel: "RE: continuity of self"</a>
<li><strong>Previous message:</strong> <a href="5536.html">Ben Goertzel: "RE: Rationality and altered states of consciousness"</a>
<li><strong>In reply to:</strong> <a href="5532.html">Ben Goertzel: "RE: continuity of self  [ was META: Sept. 11 and  Singularity]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5538.html">Ben Goertzel: "RE: continuity of self"</a>
<li><strong>Reply:</strong> <a href="5538.html">Ben Goertzel: "RE: continuity of self"</a>
<li><strong>Reply:</strong> <a href="5541.html">Cliff Stabbert: "Re: continuity of self"</a>
<li><strong>Reply:</strong> <a href="5553.html">Samantha Atkins: "Re: continuity of self"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5537">[ date ]</a>
<a href="index.html#5537">[ thread ]</a>
<a href="subject.html#5537">[ subject ]</a>
<a href="author.html#5537">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em> &gt;
</em><br>
<em> &gt;&gt; Also under the &quot;historical perspective&quot; department, the most
</em><br>
<em> &gt;&gt; important forms of poverty are not monetary poverty but intelligence
</em><br>
<em> &gt;&gt; poverty, lifespan poverty, and the lack of other resources which are
</em><br>
<em> &gt;&gt; currently so hard to obtain that people tend not to think of their
</em><br>
<em> &gt;&gt; absence as &quot;poverty&quot; but simply &quot;the human condition&quot;.
</em><br>
<em> &gt;&gt;
</em><br>
<em> &gt;&gt; -- Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em> &gt;&gt;
</em><br>
<em> &gt;
</em><br>
<em> &gt; These assessments are highly subjective.
</em><br>
<em> &gt;
</em><br>
<em> &gt; From the point of view of many people in the world, material poverty is
</em><br>
<em> &gt; a much bigger problem than lifespan poverty or intelligence poverty.
</em><br>

<br>
Immediately apparent subjective priorities according to hacked-up 
<br>
chimpanzee priority-assignment hardware are not always the same as 
<br>
rational priorities given the same goals.
<br>

<br>
<em> &gt; And I can't help suspecting that you, Eliezer, might consider material
</em><br>
<em> &gt; poverty a little more serious if you were experiencing it!  Even given
</em><br>
<em> &gt; your generally nonmaterialistic world-view and personality.
</em><br>

<br>
Why is it, Ben, that you chide me for failing to appreciate diversity, yet 
<br>
you seem to have so much trouble accepting that this one person, Eliezer, 
<br>
could have an outlook that is really seriously different than your own, 
<br>
rather than some transient whim?  I don't have any trouble appreciating 
<br>
that others are different from me, even though I may judge those 
<br>
differences as better or worse.  You, on the other hand, seem to have 
<br>
difficulty believing that there is any difference at all between you and 
<br>
someone you are immediately talking to, regardless of what theoretical 
<br>
differences you might claim to believe in or respect.
<br>

<br>
Suppose that I did tend to focus more on material poverty if I were 
<br>
experiencing it.  That supervention of my wired-in chimpanzee priorities 
<br>
is not necessarily more correct.  I might as well say to some Third 
<br>
Worlder &quot;You might consider material poverty less serious if you lived 
<br>
here.&quot;  For that matter, I could also be tortured until I considered 
<br>
ending the pain to be the most important thing in the universe.  So what? 
<br>
&nbsp;&nbsp;What does this have to do with the price of tea in China, or to be more 
<br>
precise, the Bayesian Probability Theorem?  How do any of these things 
<br>
change the facts?  In what way are they &quot;evidence&quot; about the issue at 
<br>
hand?  I run on vulnerable hardware with known flaws, such that there are 
<br>
certain environmental stimuli that would produce very-high-priority 
<br>
signals capable of disrupting more rational means of aligning subgoals 
<br>
with supergoals; environmental stimuli may even result in negative or 
<br>
positive reinforcement in sufficient amounts to overwrite the current goal 
<br>
system.  Again, so what?  That's just a broken Eliezer, not an enlightened 
<br>
Eliezer.
<br>

<br>
I regard myself as an imperfect approximation to morality and rationality. 
<br>
&nbsp;&nbsp;Note that I do not say &quot;I regard myself as an imperfect approximation to 
<br>
what I define as morality and rationality&quot;, because if that were the 
<br>
explicit definition, then all kinds of future conditions would count as 
<br>
&quot;fulfilling&quot; this definition which actually just corrupted the future 
<br>
Eliezer's definition.   It's a fact that my definition is stored on 
<br>
Eliezer's brainware.  The definition nonetheless does not make *explict* 
<br>
mention of Eliezer.  Like, *this* is the map, and *this* is the territory, 
<br>
and you can't fold up the territory and put in your glove compartment, see?
<br>

<br>
I regard myself as an imperfect approximation to morality and rationality. 
<br>
&nbsp;&nbsp;If you place me under environmental conditions that disrupt rationality, 
<br>
that makes me a less accurate approximation.  If you put my brain through 
<br>
changes that destroy my definition of the target, I cease to be an 
<br>
approximation to rationality/ethics/morality in any real sense.  Again, so 
<br>
what?  I don't model that set of hypothetical conditions as actually 
<br>
changing rationality or morality, so why should I care?  Forget about what 
<br>
you could theoretically torture Eliezer into caring about; that's a broken 
<br>
Eliezer, or even a non-Eliezer, and whatever brainwashed mantras this 
<br>
hypothetical entity would output has no relevance to the current 
<br>
functioning Eliezer's attempt to determine what constitutes rationality or 
<br>
morality, since those definitions, regardless of whether they are *stored* 
<br>
in Eliezer's memory, make no actual internal *mention* of Eliezer as 
<br>
either a present or future determinant of morality.
<br>

<br>
It doesn't matter what you can do to my brain by imposing various 
<br>
environmental conditions unless that hypothetical scenario provides real 
<br>
information about rationality or morality.  It automatically matters to 
<br>
*you* because the definition of morality stored in Ben's memory makes 
<br>
*explicit internal mention* of Ben's subjective opinion as an important 
<br>
determinant of morality, so if you imagine future conditions that would 
<br>
change your subjective opinion by direct supervention of chimp brainware, 
<br>
it looks to you like it's morally relevant.  Actually, this is 
<br>
overcomplicating things; it matters to you because you directly process 
<br>
the anticipation of subjective pleasure and subjective pain.
<br>

<br>
Well, I see things differently.  It's not because I have different 
<br>
brainware.  It's because I have a different way of thinking deliberatively 
<br>
about morality.  DIFFERENT!  Yes, different ways of thinking about 
<br>
morality than yours do exist!  Now your way may be right, and my way may 
<br>
be wrong - I don't think it'd be antisocial or unfriendly of you to raise 
<br>
that possibility, and in fact I rather wish you would - but we do think 
<br>
about morality differently.  You've certainly proved yourself capable of 
<br>
uttering the sentence &quot;Oh, but Eliezer, different ways of thinking about 
<br>
morality may exist&quot;, but it seems to me that you then go on to refuse to 
<br>
actually model any kind of moral thinking different from yours.  I 
<br>
understand that you think differently about morality than I do.  I think 
<br>
you're wrong, but I accept you're definitely different.  I even try to 
<br>
model the causes and effects involved; yeah, sure, I might be getting it 
<br>
completely wrong, but at least I'm *trying*.  I can tell there's a 
<br>
difference in what we think &quot;morality&quot; *is*, and I'm trying to understand 
<br>
it, not dismiss it as a shallow surface disagreement.
<br>

<br>
<em> &gt; While the human condition in itself is profoundly flawed, there is no
</em><br>
<em> &gt; doubt that some humans live in vastly more flawed conditions than
</em><br>
<em> &gt; others.
</em><br>

<br>
&quot;Vastly&quot;?  I think that word reflects your different perspective (at least 
<br>
one of us must be wrong) on the total variance within the human cluster 
<br>
versus the variance between the entire human cluster and a posthuman 
<br>
standard of living.  I think that the most you could say is that some 
<br>
humans live in very slightly less flawed conditions than others.  Maybe 
<br>
not even that.
<br>

<br>
<em> &gt; As a person of great material privilege, you are inclined to
</em><br>
<em> &gt; focus primarily on the limitations and problems we all share.
</em><br>

<br>
As a student of minds-in-general, I define humanity by looking at the 
<br>
features of human psychology and existence that are panhuman and reflect 
<br>
the accumulated deep pool of complex functional adaptation, rather than 
<br>
the present surface froth of variations between cultures and individuals.
<br>

<br>
If I am a person of &quot;great material privilege&quot;, by the way, I would very 
<br>
much like to have my own nanocomputer.  What?  I can't buy that?  And 
<br>
neither can Bill Gates?  Guess we're both poor.
<br>

<br>
<em> &gt; Of course, I agree with you that creating a superhuman AGI can be a
</em><br>
<em> &gt; great way to end material poverty as well as to overcome the many
</em><br>
<em> &gt; self-defeating characteristics of human nature.
</em><br>

<br>
It's a way to rewrite almost every aspect of life as we know it.  You can 
<br>
take all the force of that tremendous impact and try to turn it to pure 
<br>
light.  You can even hypothesize that this tremendous impact, expressed as 
<br>
pure light, would have effects that include the ending of fleeting 
<br>
present-day problems like material poverty.  But it is unwise in the 
<br>
extreme to imagine that the Singularity is a tool which can be channeled 
<br>
into things like &quot;ending material poverty&quot; because some computer 
<br>
programmer wants that specifically.
<br>

<br>
-- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>

<br>

<br>

<br>

<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5538.html">Ben Goertzel: "RE: continuity of self"</a>
<li><strong>Previous message:</strong> <a href="5536.html">Ben Goertzel: "RE: Rationality and altered states of consciousness"</a>
<li><strong>In reply to:</strong> <a href="5532.html">Ben Goertzel: "RE: continuity of self  [ was META: Sept. 11 and  Singularity]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5538.html">Ben Goertzel: "RE: continuity of self"</a>
<li><strong>Reply:</strong> <a href="5538.html">Ben Goertzel: "RE: continuity of self"</a>
<li><strong>Reply:</strong> <a href="5541.html">Cliff Stabbert: "Re: continuity of self"</a>
<li><strong>Reply:</strong> <a href="5553.html">Samantha Atkins: "Re: continuity of self"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5537">[ date ]</a>
<a href="index.html#5537">[ thread ]</a>
<a href="subject.html#5537">[ subject ]</a>
<a href="author.html#5537">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
