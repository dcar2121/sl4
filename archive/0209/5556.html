<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Rationality and altered states of consciousness</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Rationality and altered states of consciousness">
<meta name="Date" content="2002-09-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Rationality and altered states of consciousness</h1>
<!-- received="Wed Sep 18 11:13:03 2002" -->
<!-- isoreceived="20020918171303" -->
<!-- sent="Wed, 18 Sep 2002 08:17:07 -0600" -->
<!-- isosent="20020918141707" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Rationality and altered states of consciousness" -->
<!-- id="LAEGJLOGJIOELPNIOOAJIEPNDJAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="352F6024-CAF9-11D6-BCE5-000A27B4DEFC@rbisland.cx" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Rationality%20and%20altered%20states%20of%20consciousness"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Wed Sep 18 2002 - 08:17:07 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5557.html">Ben Goertzel: "multiple possible universes"</a>
<li><strong>Previous message:</strong> <a href="5555.html">Gordon Worley: "Re: Rationality and altered states of consciousness"</a>
<li><strong>In reply to:</strong> <a href="5555.html">Gordon Worley: "Re: Rationality and altered states of consciousness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5558.html">Eliezer S. Yudkowsky: "Rationality, intelligence, evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5556">[ date ]</a>
<a href="index.html#5556">[ thread ]</a>
<a href="subject.html#5556">[ subject ]</a>
<a href="author.html#5556">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>

<br>
<em>&gt; &gt; I now see a lot more harmony between our differing definitions of these
</em><br>
<em>&gt; &gt; things than I did a month ago.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't.  Intelligence is not rationality.  Intelligence is not a force
</em><br>
<em>&gt; that processes can draw upon.  Intelligence refers to goal-oriented
</em><br>
<em>&gt; processes (NOTE:  intelligence != rational intelligence) that draws on
</em><br>
<em>&gt; rationality.  The human mind has explicit goals, evolution does not.
</em><br>

<br>
Well, I absolutely do not agree that rationality is more &quot;force-like&quot; than
<br>
intelligence is.  Both rationality and intelligence are descriptors that may
<br>
be applied to various systems in the universe.
<br>

<br>
I can't escape the feeling that your notion of rationality is not entirely
<br>
rational.  It seems to have some of the dogmatic aspect of a religious
<br>
belief.  But maybe I'm just not understanding it fully...
<br>

<br>
I'll make one more attempt to state my perspective on rationality.
<br>

<br>
What I'd call &quot;explicitly rational thought&quot; is one particular strategy that
<br>
intelligent minds can use to achieve their goals.  Explicitly rational
<br>
thought involves the use of formal logic and probability theory.  It doesn't
<br>
require that the mind have explicit expressions of the appropriate
<br>
mathematical rules inside itself.  But it does require that the mind take
<br>
*incremental reasoning steps* that are close to the action of individual
<br>
mathematical rules from logic or probability theory.  This is
<br>
&quot;ratiocination&quot;, it's close to what Eliezer calls &quot;deliberation&quot; perhaps
<br>
(though I don't fully understand his notion of deliberation).
<br>

<br>
[ I even think that my recent work on probabilistic term logic and
<br>
second-order probability has made a significant contribution to the
<br>
understanding of how rational thought works (we'll see what others think on
<br>
this when the Novamente book is published -- Eliezer, this is largely new
<br>
stuff that wasn't in the rough draft book you read).]
<br>

<br>
[Why you guys obsess on Bayes' Theorem, which is just one among many useful
<br>
results in logic and probability theory, I don't know.  It seems to me a lot
<br>
better just to talk about &quot;probabilistic logical inference&quot; than to place
<br>
Bayes' Theorem at the fore as you habitually do.]
<br>

<br>
Next, what I'd call &quot;implicitly rational thought&quot; is when a system has a
<br>
goal, and its actions approximate the actions that would be taken by an
<br>
explicitly rational agent in the same situation with the same goal -- even
<br>
though the system itself is not carrying out incremental operations that are
<br>
explicitly rational in nature.
<br>

<br>
It seems that the maximum implicit rationality, given finite resource
<br>
constraints, is often NOT achieved through explicit rationality.  This is a
<br>
deep cognitive science idea (not original with me, though this phrasing is
<br>
my own), which some future mathematics of the mind may allow us to present
<br>
as a theorem, who knows.
<br>

<br>
In other words, it seems that minds can sometimes maximize their implicit
<br>
rationality by doing things that at the micro-level don't appear rational.
<br>

<br>
In fact, something stronger appears to be the case: Often explicit
<br>
rationality is an EXTREMELY BAD strategy for achieving maximal
<br>
rationality....  In many cases, if one wants to do the
<br>
probabilistically-logically best thing, obeying probabilistic logic in one's
<br>
incremental mental steps is a TERRIBLE way to do it....
<br>

<br>
Lack of understanding of this principle is largely responsible for the
<br>
excesses of symbolic AI and logic-based cognitive science.
<br>

<br>
This principle means that it is VERY DIFFICULT to assess the (implicit)
<br>
rationality or otherwise of another system.  Not impossible, just
<br>
difficult -- because what looks like irrationality may sometimes be &quot;the
<br>
execution of heuristics that are implicitly rational, although in their
<br>
incremental performance very different from explicit rationality.&quot;
<br>

<br>
I define intelligence as the ability of a system to achieve complex goals in
<br>
complex environments.  You may define intelligence differently; intelligence
<br>
is a natural-language concept which is intrinsically fuzzy, and my
<br>
definition may capture only part of it.
<br>

<br>
Given my definition of intelligence, it follows that a system's intelligence
<br>
should be closely tied to its degree of *implicit rationality*.
<br>

<br>
Specifically: According to my definitions it seems to follow that: Given
<br>
fixed resource constraints and a fixed set of goals, a system that has more
<br>
implicit rationality is going to have more intelligence.
<br>

<br>
What I've just described seems to me a very pragmatic treatment of
<br>
rationality in the mind.  I don't see why one needs to posit a cosmic force
<br>
of rationality, nor do I see why one wants to single out Bayes' Theorem as
<br>
being more important than all the other rules of logic and probability
<br>
theory.
<br>

<br>
It is also true that one can use logic and probability theory to model the
<br>
behavior of non-intelligent systems.  They are universal modeling tools --
<br>
not always useful, but almost always applicable (though in cases of very
<br>
small sample spaces, probability theory is not reliable).  I don't see why
<br>
this fact should be blown up into proclamations like &quot;You must learn to see
<br>
BPT coursing through the veins, capillaries and arteries of the cosmos&quot;, or
<br>
however Eliezer phrased it.  Arithmetic is also a universal and commonly
<br>
useful modeling tool; so are differential and integral calculus (if one
<br>
includes their discrete analogues).  We are clever to have invented these
<br>
very general tools for understanding the universe!  But one shouldn't
<br>
confuse the universe itself with our tools for understanding and analyzing
<br>
it.
<br>

<br>

<br>
-- Ben
<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>
I should clarify one thing however: According to my definition of
<br>
intelligence as currently stated, *explicit* goals aren't required....  I
<br>
think that requiring explicitness leads one down a very difficult path.  Try
<br>
to formally define &quot;explicit&quot;!  [This comes up in the theory of AI quite a
<br>
lot -- it gets at the question of symbolic AI versus subsymbolic AI... when
<br>
does one say that a neural net contains an explicit rather than an implicit
<br>
representation of something?]
<br>

<br>
-- Ben
<br>

<br>

<br>

<br>

<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5557.html">Ben Goertzel: "multiple possible universes"</a>
<li><strong>Previous message:</strong> <a href="5555.html">Gordon Worley: "Re: Rationality and altered states of consciousness"</a>
<li><strong>In reply to:</strong> <a href="5555.html">Gordon Worley: "Re: Rationality and altered states of consciousness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5558.html">Eliezer S. Yudkowsky: "Rationality, intelligence, evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5556">[ date ]</a>
<a href="index.html#5556">[ thread ]</a>
<a href="subject.html#5556">[ subject ]</a>
<a href="author.html#5556">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
