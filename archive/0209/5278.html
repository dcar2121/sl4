<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Activism vs. Futurism</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Activism vs. Futurism">
<meta name="Date" content="2002-09-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Activism vs. Futurism</h1>
<!-- received="Sat Sep 07 03:35:50 2002" -->
<!-- isoreceived="20020907093550" -->
<!-- sent="Sat, 07 Sep 2002 02:47:38 -0400" -->
<!-- isosent="20020907064738" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Activism vs. Futurism" -->
<!-- id="3D79A10A.3030001@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJMECBDJAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Activism%20vs.%20Futurism"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Sep 07 2002 - 00:47:38 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5279.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<li><strong>Previous message:</strong> <a href="5277.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<li><strong>In reply to:</strong> <a href="5276.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5279.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<li><strong>Reply:</strong> <a href="5279.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5278">[ date ]</a>
<a href="index.html#5278">[ thread ]</a>
<a href="subject.html#5278">[ subject ]</a>
<a href="author.html#5278">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em> &gt; Eliezer says:
</em><br>
<em> &gt;
</em><br>
<em> &gt;&gt; Ben, there is a difference between working at a full-time job that
</em><br>
<em> &gt;&gt; happens to (in belief or in fact) benefit the Singularity; and
</em><br>
<em> &gt;&gt; explicitly beginning from the Singularity as a starting point and
</em><br>
<em> &gt;&gt; choosing your actions accordingly, before the fact rather than
</em><br>
<em> &gt;&gt; afterward.
</em><br>
<em> &gt;
</em><br>
<em> &gt; I guess there is a clear *psychological* difference there, but not a
</em><br>
<em> &gt; clear *pragmatic* one.
</em><br>
<p>Psychological differences *make* pragmatic differences.  If your life was 
<br>
more directed by abstract reasoning and less by fleeting subjective 
<br>
impressions you'd have more experience with the way that philosophical 
<br>
differences can propagate down to huge differences in action and strategy.
<br>
<p><em> &gt; Consider the case of someone who is working on a project for a while,
</em><br>
<em> &gt; and later realizes that it has the potential to help with the
</em><br>
<em> &gt; Singularity. Suppose they then continue their project with even greater
</em><br>
<em> &gt; enthusiasm because they now see it's broader implications in terms of
</em><br>
<em> &gt; the Singularity. To me, this person is working toward the Singularity
</em><br>
<em> &gt; just as validly as if they had started their project with the
</em><br>
<em> &gt; Singularity in mind.
</em><br>
<p>Yes, well, again, that's because you haven't accumulated any experience 
<br>
with the intricacies of Singularity strategy and hence have the to-me 
<br>
bizarre belief that you can take a project invented for other reasons and 
<br>
nudge it in the direction of a few specific aspects of the Singularity and 
<br>
end up with something that's as strong and coherent as a project created 
<br>
from scratch to serve the Singularity.  It is possible that you will turn 
<br>
out to be correct about this - it could be a fact - but from my 
<br>
perspective it leads to what I see as blatant searing errors on your part.
<br>
<p>Of course, if I couldn't be friends with people who make what I see as 
<br>
blatant searing errors, I wouldn't have any friends.
<br>
<p><em> &gt; The reason I chose to work on AI instead of time travel, unification of
</em><br>
<em> &gt;  fundamental physics, genetics or pure mathematics, is that I felt it
</em><br>
<em> &gt; had the potential to bring about this transcendent order of being
</em><br>
<em> &gt; faster.
</em><br>
<p>I knew about the possibilities of recursively self-improving AI and 
<br>
enormously faster-than-human AI since age 11.  It didn't make much 
<br>
impression on me except to convince me that the future would be cool in 
<br>
some vague unspecified way, and influence me to think in terms of a career 
<br>
in nanotechnology.  What changed my life was Vinge's idea of 
<br>
smarter-than-human intelligence causing a breakdown in our model of the 
<br>
future, not any of the previous speculations about recursive 
<br>
self-improvement or faster-than-human thinking, which is why I think that 
<br>
Vinge hit the nail exactly on the head the first time (impressive, that) 
<br>
and that Kurzweil, Smart, and others who extrapolate Moore's Law are 
<br>
missing the whole point.
<br>
<p>Again, there's a difference between being *influenced* by a picture of the 
<br>
future, and making activist choices based on, and solely on, an explicit 
<br>
ethics and futuristic strategy.  This &quot;psychological difference&quot; is 
<br>
reflected in more complex strategies, the ability to rule out courses of 
<br>
action that would otherwise be rationalized, a perception of fine 
<br>
differences... all the things that humans use their intelligence for.
<br>
<p><em> &gt; You may feel that someone who is explicitly working toward the
</em><br>
<em> &gt; Singularity as the *prime supergoal* of all their actions, can be
</em><br>
<em> &gt; trusted more thoroughly to make decisions pertinent toward the
</em><br>
<em> &gt; Singularity.
</em><br>
<p>It's not just a question of trust - although you're right, I don't trust 
<br>
you to make correct choices about when Novamente needs which Friendly AI 
<br>
features unless the sole and only point of Novamente is as a Singularity 
<br>
seed.  It's a question of professional competence as a Singularity 
<br>
strategist; a whole area of thought that I don't think you've explored.
<br>
<p><em> &gt; But I am not so sure of this at all.  The problem is that
</em><br>
<em> &gt; this kind of extremism in devotion to a cause, throughout human
</em><br>
<em> &gt; history, has often been correlated with poor judgment.  This is not to
</em><br>
<em> &gt; say that I *mistrust* someone particularly if they have the Singularity
</em><br>
<em> &gt; (or anything else) as a prime supergoal of their actions, only to say
</em><br>
<em> &gt; that I don't value their judgment particularly because of their extreme
</em><br>
<em> &gt; single-mindedness.
</em><br>
<em> &gt;
</em><br>
<em> &gt; Personally, although the Singularity (in my own sense of the term) has
</em><br>
<em> &gt; been the main guiding motivation behind my research work and my whole
</em><br>
<em> &gt; career, it is NOT the entire motivation behind my life.  It is not the
</em><br>
<em> &gt; supergoal of ALL my actions, only of a majority of my actions.  I don't
</em><br>
<em> &gt; improvise at the piano because it is helpful for the Singularity, I do
</em><br>
<em> &gt; it because I enjoy it. I could make a rationalization and say that I
</em><br>
<em> &gt; need to play piano sometimes because it clears my mind and makes me
</em><br>
<em> &gt; more able to work on AI afterwards, but I don't bother to make that
</em><br>
<em> &gt; rationalization.  I accept that I have a goal heterarchy in my mind,
</em><br>
<em> &gt; not a goal hierarchy, and that working toward the Singularity is a very
</em><br>
<em> &gt; important goal of the human organism that is me, but not the *prime
</em><br>
<em> &gt; supergoal*.
</em><br>
<p>Your goal heterarchy has the strange property that one of the goals in it 
<br>
affects six billion lives, the fate of Earth-originating intelligent life, 
<br>
and the entire future, while the others do not.  Your bizarre attempt to 
<br>
consider these goals as coequal is the reason that I think you're using 
<br>
fleeting subjective impressions of importance rather than conscious 
<br>
consideration of predicted impacts.
<br>
<p>I realize that many people on Earth get along just fine using their 
<br>
built-in subjective impressions to assign relative importance to their 
<br>
goals, despite the flaws and the inconsistencies and the blatant searing 
<br>
errors from a normative standpoint; but for someone involved in the 
<br>
Singularity it is dangerous, and for a would-be constructor of real AI it 
<br>
is absurd.
<br>
<p><em> &gt; In your definition of whether someone is &quot;working full time on the
</em><br>
<em> &gt; Singularity,&quot; you are judging people based on their psychological
</em><br>
<em> &gt; motivations rather than their actions.  If you like to categorize
</em><br>
<em> &gt; people in this way, you're welcome to.  One problem with this, however,
</em><br>
<em> &gt; is that your own insight into other peoples' psychological motivations
</em><br>
<em> &gt; is rather limited.
</em><br>
<p>This from the person who seems unwilling to believe that real altruists 
<br>
exist?  It's not like I'm the only one, or even a very exceptional one if 
<br>
we're just going to measure strength of commitment.  Go watch the film 
<br>
&quot;Gandhi&quot; some time and ask yourself about the thousands of people who 
<br>
followed Gandhi into the line of fire *without* even Gandhi's protection 
<br>
of celebrity.  Now why wouldn't you expect people like that to get 
<br>
involved with the Singularity?  Where *else* would they go?
<br>
<p><em> &gt; I prefer to judge whether someone is working toward the Singularity or
</em><br>
<em> &gt; not by looking at *what they're actually doing*.
</em><br>
<em> &gt;
</em><br>
<em> &gt; And I *certainly* don't think it's reasonable to implicitly assume that
</em><br>
<em> &gt; only people working for SIAI are truly devoted to the Singularity!
</em><br>
<p>I don't.  But it seems reasonable to assume that only people whose day 
<br>
jobs are explicitly and solely about the Singularity have day jobs 
<br>
explicitly and solely about the Singularity.  There are already people 
<br>
beyond me and Chris Rovner whose *lives* are explictly and solely about 
<br>
the Singularity; it's having a second person freed up to *work full-time* 
<br>
on it that's the exciting part.  At least if you're me.
<br>
<p><em> &gt; SIAI does not look to me like a generic Singularity-promoting
</em><br>
<em> &gt; organization. By all appearances it is an organization devoted to your
</em><br>
<em> &gt; particular approach to AI and Friendly AI.  Thus, if someone is devoted
</em><br>
<em> &gt; to the Singularity but thinks your approaches to these problems are not
</em><br>
<em> &gt; correct, they are probably not going to join SIAI.  Thus, your
</em><br>
<em> &gt; implication that only people involved with SIAI are truly full-time
</em><br>
<em> &gt; devoted to the Singularity, sounds a lot like an implication that only
</em><br>
<em> &gt; people who agree with your ideas are truly devoted to the Singularity.
</em><br>
<em> &gt; I'm sure that's not exactly what you meant to say, but it sure comes
</em><br>
<em> &gt; across that way sometimes.
</em><br>
<p>Well, Ben, this is because there are two groups of people who know damn 
<br>
well that SIAI is devoted solely, firstly, and only to the Singularity, 
<br>
and unfortunately you belong to neither.
<br>
<p>The first group is composed of people who've been around since I first 
<br>
showed up on the Extropians list in 1996, and who can attest that I 
<br>
initially started out by talking about a wide range of possible 
<br>
Singularity strategies, from human intelligence enhancement via 
<br>
neurohacking, to collaborative filtering as a means of increasing 
<br>
civilizationwide intelligence, to AI, before gradually settling on seed AI 
<br>
as a strategy as it became more and more apparent that this was probably 
<br>
the fastest way, with the Singularity Institute being founded only after 
<br>
the writing of a long document that explicitly considered possible paths 
<br>
to the Singularity and the differential impacts of different Singularity 
<br>
strategies.
<br>
<p>The second group is composed of Singularity rationalists to whom working 
<br>
toward the Singularity as the ab-initio strategic goal is 
<br>
straightforwardly and obviously rational; they have no reason to believe 
<br>
I'm not capable of doing the same.  If I believe that a certain strategy 
<br>
constitutes the best path toward the Singularity (and there has to be one, 
<br>
unless you want to spend the rest of your life in wishy-washy armwaving) 
<br>
then - as this group knows - this is a quite sufficient explanation for my 
<br>
focusing on that strategy; other explanations for my psychology, such as a 
<br>
subjective personal attachment to a particular theory of AI, are possible 
<br>
hypotheses to be considered, but certainly not necessary to the 
<br>
explanation, since all it takes is supposing that I can follow a simple 
<br>
chain of logic to its conclusion.
<br>
<p>For all that you try to chide me on it, Ben, sometimes it seems to me that 
<br>
you're the one who has difficulty imagining psychologies unlike his own. 
<br>
I know that a lot of people don't think the way I do.  It so happens 
<br>
there's a *reason* I think this particular way, and that I think those 
<br>
other people are *wrong*, but I know that a lot of wrong ways of thinking 
<br>
exist.  Now you can certainly go ahead and tell me that the way I think is 
<br>
wrong, but I wish you'd accept that the way I think is *different*.  I 
<br>
have no trouble accepting that your way of thinking is different from mine 
<br>
and no compunctions about labeling it as wrong.  You try to avoid labeling 
<br>
different ways of thinking as &quot;wrong&quot; but the price of doing so appears to 
<br>
have been that you can no longer really appreciate that wrong ways of 
<br>
thinking exist.  I'm a rational altruist working solely for the 
<br>
Singularity and that involves major real differences from your way of 
<br>
thinking.  Get over it.  If you think my psychology is wrong, say so, but 
<br>
accept that my mind works differently than yours.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5279.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<li><strong>Previous message:</strong> <a href="5277.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<li><strong>In reply to:</strong> <a href="5276.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5279.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<li><strong>Reply:</strong> <a href="5279.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5278">[ date ]</a>
<a href="index.html#5278">[ thread ]</a>
<a href="subject.html#5278">[ subject ]</a>
<a href="author.html#5278">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
