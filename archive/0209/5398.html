<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Friendly AI communications issues (was: Consciousness)</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Friendly AI communications issues (was: Consciousness)">
<meta name="Date" content="2002-09-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Friendly AI communications issues (was: Consciousness)</h1>
<!-- received="Tue Sep 10 11:29:03 2002" -->
<!-- isoreceived="20020910172903" -->
<!-- sent="Tue, 10 Sep 2002 09:46:27 -0400" -->
<!-- isosent="20020910134627" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Friendly AI communications issues (was: Consciousness)" -->
<!-- id="3D7DF7B3.10305@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="009601c2587e$186fd120$6d6c0141@ci52026b" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Friendly%20AI%20communications%20issues%20(was:%20Consciousness)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Sep 10 2002 - 07:46:27 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5399.html">David Cake: "Re: This is SL4.  This is SL4 on drugs.  Any questions?"</a>
<li><strong>Previous message:</strong> <a href="5397.html">Brian Phillips: "Re: Consciousness was Re: This is SL4. This is SL4 on drugs. Any questions?"</a>
<li><strong>In reply to:</strong> <a href="5397.html">Brian Phillips: "Re: Consciousness was Re: This is SL4. This is SL4 on drugs. Any questions?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5303.html">Cliff Stabbert: "Re[2]: This is SL4.  This is SL4 on drugs.  Any questions?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5398">[ date ]</a>
<a href="index.html#5398">[ thread ]</a>
<a href="subject.html#5398">[ subject ]</a>
<a href="author.html#5398">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Brian Phillips wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I suspect that understanding the mechanisms whereby a biological frame
</em><br>
<em>&gt; produces sentience is only the first step in getting a *useful*
</em><br>
<em>&gt; machine intelligence.
</em><br>
<em>&gt; I have raised this point before, with Annissimov and others.
</em><br>
<em>&gt; &quot;How do you get something that can or will talk to you?&quot;
</em><br>
<em>&gt; Apparently people have a real problem with discerning the difference
</em><br>
<em>&gt; between a critique of Friendliness and a healthy appreciation of
</em><br>
<em>&gt; the differences between *purely human states of mental function
</em><br>
<em>&gt; within the SAME BRAIN* (normal vs. tripping), to say nothing
</em><br>
<em>&gt; of what an AI's &quot;thought processesses&quot; would be like.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't critique Friendliness, it sounds like a grand idea (you
</em><br>
<em>&gt; want to give the AI something like a rationally derived
</em><br>
<em>&gt; Buddha-nature, this sounds like a good thing for a functionally
</em><br>
<em>&gt; omnipotent entity to have....). I do think the communication
</em><br>
<em>&gt; issues are non-trivial (and boy are they EVER non-trivial).
</em><br>

<br>
In a word... yeah.  Except that I don't think a human on drugs is a good 
<br>
metaphor for the alienness of an AI.  Admittedly I've never been on drugs 
<br>
or even talked in realtime to anyone on drugs, but y'know, I'm fairly 
<br>
confident of this anyway.  Drugs don't
<br>

<br>
The way I think of the question is &quot;How do you explain Friendliness to an 
<br>
AI that understands nothing except billiard balls?&quot;, or, at a more 
<br>
advanced level, an AI that understands itself and billiard balls.
<br>

<br>
Let's say that's *all*.
<br>

<br>
Now... how do you explain Friendliness?
<br>

<br>
The interesting thing, though, is that this question has a real answer. 
<br>
Yes, there are difficulties in AI communication that are unlike anything 
<br>
you'd experience in talking to a human, on drugs or otherwise.  But there 
<br>
are also - as I have very recently started to realize - things that the AI 
<br>
can do that have absolutely no analogue in human thinking.  If there are 
<br>
new problems, there are new solutions as well.  And sometimes the new 
<br>
solutions to the new problems are shockingly more powerful than the human 
<br>
way of doing things.  And I mean that in a good way.
<br>

<br>
It's like the way you start out by saying &quot;Intelligence is so amazingly 
<br>
hard&quot; but then, once you can visualize &quot;recursive self-improvement&quot; in 
<br>
enough detail that it's not just a phrase anymore, you realize that if you 
<br>
*can* build an intelligent AI it's soon going to be *way* more intelligent 
<br>
than a human.
<br>

<br>
It's like the way you start out by saying &quot;Friendliness / altruism / love 
<br>
/ kindness is amazingly hard&quot; but, once you learn enough evolutionary 
<br>
psychology that you can see human emotions as something real rather than a 
<br>
mysterious property of humans, you can mentally model the idea of a mind 
<br>
that is *far more* friendly than a modern-day human, a mind to which we've 
<br>
passed on love and caring without passing on hate, tribalistic thinking, 
<br>
the urge to dominate, the tendency to rationalize amassing personal power. 
<br>
&nbsp;&nbsp;This doesn't speak to the issue of whether an AI or a human upload is 
<br>
more likely to reliably converge to this point, but it's something where 
<br>
if you haven't spent some time mentally modeling FAI you aren't likely to 
<br>
even *see* the goal.
<br>

<br>
There are things like that for the communication problem too, which this 
<br>
margin is unfortunately too small to contain, plus they're so alien Greg 
<br>
Egan would spit out his coffee... I *really* have to write this up at some 
<br>
point.
<br>

<br>
I think there may even be a valid analogy here to the reason that 
<br>
technophilia usually turns out to be a better strategy than technophobia; 
<br>
it's easier to see automobiles putting the saddle industry out of 
<br>
business, but harder to visualize in advance all the jobs created by the 
<br>
present-day auto industry.  You have to visualize the solutions along with 
<br>
the problems, and - usually, but not always - the solutions turn out to be 
<br>
even more powerful than the problems.  That's why technological advance 
<br>
almost always turns out to be a good thing despite all fears; it's what 
<br>
technophobes fail to take into account in their predictions and it's why a 
<br>
certain measure of courage in confronting the future has been historically 
<br>
useful.
<br>

<br>
It could be that when we actually start building AI, we'll find out that 
<br>
the inhumanly hard problems are real but the inhumanly good solutions are 
<br>
chimerical.  In which case SingInst would drop the whole AI strategy like 
<br>
a radioactive weasel and try like hell to figure out another strategy that 
<br>
has a snowball's chance of working.  And you'll also note that I'm trying 
<br>
hard to make sure I know what the inhumanly good solutions are *in 
<br>
advance* - or at least know that *a* inhumanly good solution exists, 
<br>
regardless of whether it's the one we end up using - rather than relying 
<br>
on the historically reliable empirical regularity that we'll find the 
<br>
solutions as we go along.  You can't *know* in advance that empirical 
<br>
regularities like Moore's law will continue, no matter how reliably they 
<br>
have worked in the past, unless you have an underlying causal model.
<br>

<br>
<em>&gt; (The worst part is  people tell  me I am &quot;anthropomorphizing
</em><br>
<em>&gt; AI&quot;.. feyh!)
</em><br>
<em>&gt; This is a wildly anthropomorphic example but it should serve
</em><br>
<em>&gt; to illustrate the point.  (Just as a thought experiment)
</em><br>
<em>&gt;   Upload a lungfish. Give it an IQ of 130 or so. From the point
</em><br>
<em>&gt; you turn the computer on make sure the lungfish is experiencing
</em><br>
<em>&gt; something rather analogous to the effects of 600 micrograms
</em><br>
<em>&gt; of LSD in a healthy 90 kg adult human. The rise-plateau-fall
</em><br>
<em>&gt; pattern of intensity interaction is, rather than linear, wildly chaotic,
</em><br>
<em>&gt; or perhaps rythmic in some virutally undetectable pattern (for
</em><br>
<em>&gt; the lungfish anyway). Give the lungfish the ability to moderate the
</em><br>
<em>&gt; dissolving effects of the drug, with one difference.. the lungfish
</em><br>
<em>&gt; doesn't know which way is *down* (i.e. even if it knew
</em><br>
<em>&gt; what normal was like it can't apply that knowledge to the
</em><br>
<em>&gt; present state). Give this lungfish access to a file in which you
</em><br>
<em>&gt; have placed an electronic version of the human Universal
</em><br>
<em>&gt; Grammer. Talk to the lungfish.
</em><br>
<em>&gt;   Was what you just did to the lungfish a nice thing to do?
</em><br>
<em>&gt; Would you do it to a child? A pet? a seed AI?
</em><br>
<em>&gt; Hmmm......
</em><br>

<br>
I confess I don't see what you're trying to illustrate.  Also a lungfish 
<br>
doesn't have nearly enough complexity to unambiguously specify what a 
<br>
&quot;lungfish of IQ 130&quot; would be like, so we're talking about a pretty wide 
<br>
range of possible minds-in-general here.  For example, the lungfish of IQ 
<br>
130 could have a far stronger mind than a human of IQ 130, so that a 
<br>
perturbation of the order caused by the LSD dosage would be simply 
<br>
shrugged off.
<br>

<br>
-- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>

<br>

<br>

<br>

<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5399.html">David Cake: "Re: This is SL4.  This is SL4 on drugs.  Any questions?"</a>
<li><strong>Previous message:</strong> <a href="5397.html">Brian Phillips: "Re: Consciousness was Re: This is SL4. This is SL4 on drugs. Any questions?"</a>
<li><strong>In reply to:</strong> <a href="5397.html">Brian Phillips: "Re: Consciousness was Re: This is SL4. This is SL4 on drugs. Any questions?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5303.html">Cliff Stabbert: "Re[2]: This is SL4.  This is SL4 on drugs.  Any questions?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5398">[ date ]</a>
<a href="index.html#5398">[ thread ]</a>
<a href="subject.html#5398">[ subject ]</a>
<a href="author.html#5398">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
