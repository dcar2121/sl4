<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Miller's The Mating Mind</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Miller's The Mating Mind">
<meta name="Date" content="2002-09-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Miller's The Mating Mind</h1>
<!-- received="Sun Sep 29 21:37:13 2002" -->
<!-- isoreceived="20020930033713" -->
<!-- sent="Sun, 29 Sep 2002 21:41:41 -0600" -->
<!-- isosent="20020930034141" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Miller's The Mating Mind" -->
<!-- id="LAEGJLOGJIOELPNIOOAJIEAFDLAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="OE34FHh9lanc9apUZq3000030af@hotmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Miller's%20The%20Mating%20Mind"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sun Sep 29 2002 - 21:41:41 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5388.html">Damien Broderick: "RE: Miller's The Mating Mind"</a>
<li><strong>Previous message:</strong> <a href="5386.html">Eliezer S. Yudkowsky: "Incrementalism in venture capital"</a>
<li><strong>In reply to:</strong> <a href="5383.html">hook: "Re: Miller's The Mating Mind"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5388.html">Damien Broderick: "RE: Miller's The Mating Mind"</a>
<li><strong>Reply:</strong> <a href="5388.html">Damien Broderick: "RE: Miller's The Mating Mind"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5387">[ date ]</a>
<a href="index.html#5387">[ thread ]</a>
<a href="subject.html#5387">[ subject ]</a>
<a href="author.html#5387">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer wrote:
<br>
<em>&gt; The potential donors are interesting; the potential capitalists
</em><br>
<em>&gt; are not, due to their mundane methods of calculating ROI.  Their
</em><br>
<em>&gt; idea of high-tech stuff is sl&lt;4.
</em><br>
<p>Actually, hi-tech investors are not always as naive as you seem to think.
<br>
<p>Admittedly, some investors are ABSOLUTE IDIOTS.
<br>
<p>However, others are extremely broad-minded and future-savvy.
<br>
<p>I know quite a few wealthy people who
<br>
<p>1) understand SL4 ideas reasonably well
<br>
<p>2) do not strongly believe that donating money to pure research would make a
<br>
more significant impact on the Singularity, than investing money in
<br>
commercial technology develoment.
<br>
<p>Consistently with this, they are investing their MONEY with a view toward
<br>
MAKING MORE MONEY while PROMOTING HI-TECH DEVELOPMENT generally.  They are
<br>
being businesspeople, but also using their money to push technology forward.
<br>
<p>Their view is different from mine, but it is not an idiotic one.  After all
<br>
the bulk of progress toward the Singularity is clearly being made by
<br>
commercial efforts.  These faster and faster computers we see each year are
<br>
not produced using money from donors motivated by rational altruism.
<br>
<p>A lot of these investors believe that the best way to get to SL4 technology,
<br>
is to first fund SL2 technology, then SL3, then SL4.  This kind of
<br>
incremental approach is the way things are normally done in business.
<br>
<p><em>&gt; To Ben the Singularity appears to be icing on the cake (what the
</em><br>
<em>&gt; cake itself is will remain a mystery, though I suspect it is simply
</em><br>
<em>&gt; financial gain).
</em><br>
<em>&gt; The thing is Ben doesn't grok seed AI, which is essential to
</em><br>
<em>&gt; getting anything transhuman within a timeframe to possibly beat
</em><br>
<em>&gt; nanotech.
</em><br>
<p>I wonder where you cook up these assertions about me, Eliezer.  Surely not
<br>
using Bayes' Theorem?  If so I think you need to adjust your prior....  I'd
<br>
suggest the Solomonoff-Leven universal prior distribution....
<br>
<p>No, it is not true that I'm more interested in financial gain than in the
<br>
Singularity.  I am a philosopher-scientist-engineer above all -- and
<br>
transcending death and obsoleting reality as we know it are vastly more
<br>
important to me than making money.  If you look at what I've achieved
<br>
intellectually in  my life, and then look at my pathetic bank balance, I
<br>
think you'll find this statement well validated ;-&gt;
<br>
<p>I have made the choice to pursue commercial software development based on
<br>
&quot;narrow AI&quot; as a route to funding AGI research, and as a way of feeding
<br>
myself and my family.  Having made this choice, I take my narrow AI work
<br>
(e.g. in bioinformatics) very seriously.  But that doesn't change the fact
<br>
that my primary life goal is to participate in obsoleting death and
<br>
transforming mind &amp; reality through AGI (and potentially other scientific
<br>
research).
<br>
<p>Unlike you, I and my Novamente colleagues do not have a patron to take care
<br>
of us.  We do not have the option to spend 100% of our time working directly
<br>
on AGI.  We're pleased to be in the position of spending some of our time on
<br>
AGI, and some of our time on AGI-related narrow-AI work.  It sure beats
<br>
flipping burgers!!
<br>
<p><em>&gt; His &quot;sort of&quot; interest has a lot to do with this
</em><br>
<em>&gt; incomprehension.
</em><br>
<p>It is really very silly of you to repeatedly insist that I am only &quot;sort of&quot;
<br>
interested in the stuff I've been writing about and working on 60+ hours per
<br>
week for the last 15 years or so.
<br>
<p><em>&gt; He doesn't get Friendship Programming esp. structure, which is
</em><br>
<em>&gt; also essential.
</em><br>
<p>By which you mean: I have a different theory of how to create ethically
<br>
positive AGI's than you do.
<br>
<p>I really believe I *understand* what you're saying about Friendship
<br>
Programming.  I just don't agree with you.  In my view, you have never
<br>
adequately addressed the issue of &quot;concept and goal drift through repeated
<br>
self-modifications.&quot;
<br>
<p>It is not as though you've proved a mathematical theorem about Friendly AI,
<br>
or made a solid empirical discovery about Friendly AI.  You have not
<br>
formulated anything regarding Friendly AI about which it can be said &quot;any
<br>
reasonable, educated human should be expected to accept this upon reading
<br>
it.&quot;  You have simply formulated some interesting, plausible conceptual
<br>
arguments.  They are thought-provoking and well-thought-out, and I think
<br>
you're an excellent theorist.  But they're just plausible conceptual
<br>
arguments -- which you find more plausible than I do ... not surprisingly,
<br>
since you're the one who formulated them....
<br>
<p><em>&gt;  Of course it's plausible that one might get CFAI and Seed AI and
</em><br>
<em>&gt; why they matter and still have a cursory interest, owing to apathy
</em><br>
<em>&gt; or antipathy deeper than I can address here.
</em><br>
<p>&quot;Seed AI&quot; is a very broadly accepted concept.  That an AI when intelligent
<br>
enough will be able to modify its own code and architecture, thus
<br>
exponentially making itself more and more intelligent -- not many AI
<br>
researchers doubt this, actually.  The big open questions here have to do
<br>
with the rate of the exponential increase, and the difficulty of getting to
<br>
the stage where intelligent goal-directed self-modification can begin.  You
<br>
have a higher estimate of the rate of exponential increase than almost
<br>
anyone else.  However, I have a higher estimate of this rate than almost
<br>
anyone else except you ;-&gt;
<br>
<p>CFAI, to me, is a much more speculative thing than seed AI.  I think we'll
<br>
have a much better idea about how to guide the development of a
<br>
superintelligent AI through the Singularity, after we have some
<br>
near-human-level AGI systems to study.  I think that time spent on such
<br>
issues now is largely wasted time.  I think it's more useful right now to
<br>
work on creating near-human-level AGI's that we can test and learn from,
<br>
than to work on creating plausible speculative theories of how we'll make
<br>
our human-level AGI's ethically positive.
<br>
<p><p><em>&gt; As for me, I'm only
</em><br>
<em>&gt; interested in the matter of saving the solar system from total
</em><br>
<em>&gt; sterility.
</em><br>
<p>Your dedication is admirable, and your creative thinking on the future of
<br>
AI, technology, mind and reality is often excellent.
<br>
<p>However, your attitude toward me and many other human beings, really gets on
<br>
my nerves sometimes.
<br>
<p>-- Ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5388.html">Damien Broderick: "RE: Miller's The Mating Mind"</a>
<li><strong>Previous message:</strong> <a href="5386.html">Eliezer S. Yudkowsky: "Incrementalism in venture capital"</a>
<li><strong>In reply to:</strong> <a href="5383.html">hook: "Re: Miller's The Mating Mind"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5388.html">Damien Broderick: "RE: Miller's The Mating Mind"</a>
<li><strong>Reply:</strong> <a href="5388.html">Damien Broderick: "RE: Miller's The Mating Mind"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5387">[ date ]</a>
<a href="index.html#5387">[ thread ]</a>
<a href="subject.html#5387">[ subject ]</a>
<a href="author.html#5387">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
