<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: continuity of self</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: continuity of self">
<meta name="Date" content="2002-09-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: continuity of self</h1>
<!-- received="Wed Sep 18 13:30:31 2002" -->
<!-- isoreceived="20020918193031" -->
<!-- sent="Wed, 18 Sep 2002 13:07:00 -0400" -->
<!-- isosent="20020918170700" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: continuity of self" -->
<!-- id="3D88B2B4.3010208@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D882802.6090203@objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20continuity%20of%20self"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Sep 18 2002 - 11:07:00 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5560.html">Samantha Atkins: "Re: continuity of self"</a>
<li><strong>Previous message:</strong> <a href="5558.html">Eliezer S. Yudkowsky: "Rationality, intelligence, evolution"</a>
<li><strong>In reply to:</strong> <a href="5553.html">Samantha Atkins: "Re: continuity of self"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5560.html">Samantha Atkins: "Re: continuity of self"</a>
<li><strong>Reply:</strong> <a href="5560.html">Samantha Atkins: "Re: continuity of self"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5559">[ date ]</a>
<a href="index.html#5559">[ thread ]</a>
<a href="subject.html#5559">[ subject ]</a>
<a href="author.html#5559">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Samantha Atkins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I don't think so.  If you are having trouble keeping yourself alive 
</em><br>
<em>&gt; because of grossly inadequate nutrition, shelter and very spotty water 
</em><br>
<em>&gt; supplies of poor quality then your low-level survival drives will, quite 
</em><br>
<em>&gt; rightly, swamp your higher level needs for a time.  This would be true 
</em><br>
<em>&gt; of any conceivable sentient that had low-level survival minimal 
</em><br>
<em>&gt; requirements.  So please come down to earth just a bit, if you will.
</em><br>

<br>
Um... no.  I refuse.  Life on Earth just before the Singularity should be 
<br>
about the Singularity because the Singularity is what matters for the next 
<br>
billion years.  This doesn't change regardless of where you live, although 
<br>
whether or not you are likely to *know* about the Singularity may have a 
<br>
differential probability that varies strongly with geographic location. 
<br>
Now it may be that low-level needs place you at subsistence level and 
<br>
leave very little free energy, but if you know about the Singularity, you 
<br>
should still be concerned with it, with whatever free energy you have. 
<br>
And it may even be that being placed in certain environments is enough to 
<br>
disrupt or destroy an altruistic goal system.  But in this case I would 
<br>
consider, not quite that &quot;you&quot; have died, but that you have been 
<br>
transformed into something whose philosophical judgments you should no 
<br>
longer take into account.
<br>

<br>
Dedication isn't a trivial thing, Samantha.  I don't know how strong my 
<br>
will is.  I don't know what it would take to break my will, whether it's a 
<br>
little or a lot of adversity.  But it doesn't matter.  A broken Eliezer, 
<br>
however little or however much it takes to break Eliezer, is not relevant 
<br>
to this discussion.  While my will continues it will be devoted to the 
<br>
Singularity because that is the pretty darned unambiguous outcome of 
<br>
rational altruism.  And if I could continue that under whichever 
<br>
conditions of adversity, I would be personally proud of it - although 
<br>
obviously Singularity considerations mediate that I should try to 
<br>
encounter as little adversity as possible; this isn't about pride.
<br>

<br>
&quot;If Brahma has me burnt, I will spit into the flames.  If he has me 
<br>
strangled, I will attempt to bite the executioner's hand.  If my throat is 
<br>
cut, may my blood rust the blade that does it.&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-- Sam, in Roger Zelazny, &quot;Lord of Light&quot;
<br>

<br>
I don't know if I could, but I know that I *should*, remain focused on the 
<br>
Singularity on the way to my execution, and shout out my last thoughts 
<br>
about Friendly AI to the firing squad to take advantage of the very small 
<br>
but nonetheless real probability that someone will remember them and put 
<br>
them online.  Not out of defiance; because that is, under those 
<br>
hypothetical conditions, the best remaining pathway to helping humanity 
<br>
and humanity's future.  It's not a *good* pathway, but it's what's 
<br>
available under those conditions.  And I do not deny the possibility of 
<br>
that kind of dedication to any of the six billion people on this Earth, 
<br>
regardless of what conditions they live under.
<br>

<br>
I think you do humanity a disservice if you suppose that human beings are 
<br>
capable of altruism only under comfortable conditions.  Maybe it's true of 
<br>
me personally.  If I do my job competently, it will never be put to the 
<br>
test.  But if I do fail that test, that's a flaw in me, not something that 
<br>
changes the correct course of action.
<br>

<br>
<em>&gt;&gt; Why is it, Ben, that you chide me for failing to appreciate diversity, 
</em><br>
<em>&gt;&gt; yet you seem to have so much trouble accepting that this one person, 
</em><br>
<em>&gt;&gt; Eliezer, could have an outlook that is really seriously different than 
</em><br>
<em>&gt;&gt; your own, rather than some transient whim?  I don't have any trouble 
</em><br>
<em>&gt;&gt; appreciating that others are different from me, even though I may 
</em><br>
<em>&gt;&gt; judge those differences as better or worse.  You, on the other hand, 
</em><br>
<em>&gt;&gt; seem to have difficulty believing that there is any difference at all 
</em><br>
<em>&gt;&gt; between you and someone you are immediately talking to, regardless of 
</em><br>
<em>&gt;&gt; what theoretical differences you might claim to believe in or respect.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Why is that you are beginning to take this attitude of being above it 
</em><br>
<em>&gt; all and almost of a different species from the rest of us?  As 
</em><br>
<em>&gt; wonderfully bright and dedicated as you are I don't believe that this is 
</em><br>
<em>&gt; justified, at least not yet.
</em><br>

<br>
Why not say:  &quot;No matter *how* bright and dedicated you are, or aren't, 
<br>
that attitude would *never* be justified.&quot;  This helps to avoid debate 
<br>
about side issues.
<br>

<br>
<em>&gt;&gt; Suppose that I did tend to focus more on material poverty if I were 
</em><br>
<em>&gt;&gt; experiencing it.  That supervention of my wired-in chimpanzee 
</em><br>
<em>&gt;&gt; priorities is not necessarily more correct.  
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If it is the difference between life and death, then it is higher 
</em><br>
<em>&gt; priority in that it is prerequisite to the rest of your goals.  There 
</em><br>
<em>&gt; must be enough surplus of energy beyond what is needed to survive and 
</em><br>
<em>&gt; accomplish some basic functionality before higher goals can be 
</em><br>
<em>&gt; addressed.  Many people in this world do not have that much today.  That 
</em><br>
<em>&gt; is also potentially many brains of good potential that are never utilized.
</em><br>

<br>
I agree.  This is yet another problem that can best be fixed via (drum 
<br>
roll) the Singularity.  That's the most effective, most efficient, fastest 
<br>
way that I can put any given amount of effort into solving that problem. 
<br>
If I do it some other way, I fail.  If I do it some other way because of a 
<br>
reason other than my anticipation of maximum benefit to those people, I 
<br>
fail in altruism.
<br>

<br>
<em>&gt;&gt; I might as well say to some Third
</em><br>
<em>&gt;&gt; Worlder &quot;You might consider material poverty less serious if you lived 
</em><br>
<em>&gt;&gt; here.&quot;  For that matter, I could also be tortured until I considered 
</em><br>
<em>&gt;&gt; ending the pain to be the most important thing in the universe.  So 
</em><br>
<em>&gt;&gt; what?  What does this have to do with the price of tea in China, or to 
</em><br>
<em>&gt;&gt; be more precise, the Bayesian Probability Theorem? 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Puh-leze.  Can you manage to address anything directly without dragging 
</em><br>
<em>&gt; out BPT?
</em><br>

<br>
Nay, surely not, for the BPT lies at the very foundations of the universe.
<br>

<br>
<em>&gt;&gt; How do any of these things change the facts?  In what way are they 
</em><br>
<em>&gt;&gt; &quot;evidence&quot; about the issue at hand?  I run on vulnerable hardware with 
</em><br>
<em>&gt;&gt; known flaws, such that there are certain environmental stimuli that 
</em><br>
<em>&gt;&gt; would produce very-high-priority signals capable of disrupting more 
</em><br>
<em>&gt;&gt; rational means of aligning subgoals with supergoals; environmental 
</em><br>
<em>&gt;&gt; stimuli may even result in negative or positive reinforcement in 
</em><br>
<em>&gt;&gt; sufficient amounts to overwrite the current goal system.  Again, so 
</em><br>
<em>&gt;&gt; what?  That's just a broken Eliezer, not an enlightened Eliezer.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The so what is that many billions of people live today in just such 
</em><br>
<em>&gt; conditions.
</em><br>

<br>
Yes.  Six billion, to be precise.
<br>

<br>
<em> &gt; Do you feel some empathy for them?  Is belping them part of
</em><br>
<em>&gt; what drives your work?
</em><br>

<br>
It's a big part of it.  I care about the vastly greater future as well.
<br>

<br>
<em>&gt;&gt; &quot;Vastly&quot;?  I think that word reflects your different perspective (at 
</em><br>
<em>&gt;&gt; least one of us must be wrong) on the total variance within the human 
</em><br>
<em>&gt;&gt; cluster versus the variance between the entire human cluster and a 
</em><br>
<em>&gt;&gt; posthuman standard of living.  I think that the most you could say is 
</em><br>
<em>&gt;&gt; that some humans live in very slightly less flawed conditions than 
</em><br>
<em>&gt;&gt; others.  Maybe not even that.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Your perspective includes hypotheticals not currently in existence.
</em><br>

<br>
Um... yeah.  And this is a bad thing because...
<br>

<br>
<em>&gt; Given current existential conditons, some humans live in vastly more 
</em><br>
<em>&gt; flawed conditions than others.
</em><br>

<br>
Adversity isn't a relative quantity, at least not as I measure things. 
<br>
Whether you're doing well or poorly doesn't depend on whether someone else 
<br>
has more or less.  Right now all known sentient beings live under 
<br>
conditions of tremendous adversity.
<br>

<br>
<em>&gt;&gt;  &gt; As a person of great material privilege, you are inclined to
</em><br>
<em>&gt;&gt;  &gt; focus primarily on the limitations and problems we all share.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; As a student of minds-in-general, I define humanity by looking at the 
</em><br>
<em>&gt;&gt; features of human psychology and existence that are panhuman and 
</em><br>
<em>&gt;&gt; reflect the accumulated deep pool of complex functional adaptation, 
</em><br>
<em>&gt;&gt; rather than the present surface froth of variations between cultures 
</em><br>
<em>&gt;&gt; and individuals.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Hmmm. That froth is where the people live!
</em><br>

<br>
No, that froth is what the people focus on, because those variances are 
<br>
the ones which are adaptively relevant to *differential* reproduction and 
<br>
hence perceptually salient.  If there exists a race which has evolved to 
<br>
be six miles tall because height is the most powerful evolutionary 
<br>
advantage, they will, among themselves, focus on the remaining six inches 
<br>
worth of variance.  Hence the IQ wars.
<br>

<br>
<em>&gt;&gt;  &gt; Of course, I agree with you that creating a superhuman AGI can be a
</em><br>
<em>&gt;&gt;  &gt; great way to end material poverty as well as to overcome the many
</em><br>
<em>&gt;&gt;  &gt; self-defeating characteristics of human nature.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; It's a way to rewrite almost every aspect of life as we know it.  You 
</em><br>
<em>&gt;&gt; can take all the force of that tremendous impact and try to turn it to 
</em><br>
<em>&gt;&gt; pure light.  You can even hypothesize that this tremendous impact, 
</em><br>
<em>&gt;&gt; expressed as pure light, would have effects that include the ending of 
</em><br>
<em>&gt;&gt; fleeting present-day problems like material poverty.  But it is unwise 
</em><br>
<em>&gt;&gt; in the extreme to imagine that the Singularity is a tool which can be 
</em><br>
<em>&gt;&gt; channeled into things like &quot;ending material poverty&quot; because some 
</em><br>
<em>&gt;&gt; computer programmer wants that specifically.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think you are attempting to turn yourself into a FAI disconnected from 
</em><br>
<em>&gt; your own humanity.  I am not at all sure this is a good thing.
</em><br>

<br>
1:  What the heck *else* do you think I've been trying to do with my mind 
<br>
for the last six years?
<br>

<br>
2:  If you think that is in any wise, shape, or form a bad thing, you must 
<br>
have an extremely different conception of FAI than I do.
<br>

<br>
3:  You've never met an FAI or you wouldn't say that.  (Neither have I, of 
<br>
course, but I have a good imagination.)
<br>

<br>
4:  If it's good enough for an FAI, it's good enough for me.  The only 
<br>
question is whether *I* can manage it.
<br>

<br>
As for being &quot;disconnected from my own humanity&quot;... is this a generic way 
<br>
of saying &quot;different from what you were last week&quot;?  And if not, 
<br>
disconnected from humanity and connected to what, exactly?
<br>

<br>
-- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>

<br>

<br>

<br>

<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5560.html">Samantha Atkins: "Re: continuity of self"</a>
<li><strong>Previous message:</strong> <a href="5558.html">Eliezer S. Yudkowsky: "Rationality, intelligence, evolution"</a>
<li><strong>In reply to:</strong> <a href="5553.html">Samantha Atkins: "Re: continuity of self"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5560.html">Samantha Atkins: "Re: continuity of self"</a>
<li><strong>Reply:</strong> <a href="5560.html">Samantha Atkins: "Re: continuity of self"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5559">[ date ]</a>
<a href="index.html#5559">[ thread ]</a>
<a href="subject.html#5559">[ subject ]</a>
<a href="author.html#5559">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:41 MDT
</em></small></p>
</body>
</html>
