<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Activism vs. Futurism</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Activism vs. Futurism">
<meta name="Date" content="2002-09-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Activism vs. Futurism</h1>
<!-- received="Sat Sep 07 11:51:52 2002" -->
<!-- isoreceived="20020907175152" -->
<!-- sent="Sat, 7 Sep 2002 09:20:59 -0600" -->
<!-- isosent="20020907152059" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Activism vs. Futurism" -->
<!-- id="LAEGJLOGJIOELPNIOOAJIECGDJAA.ben@goertzel.org" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3D79A10A.3030001@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Activism%20vs.%20Futurism"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sat Sep 07 2002 - 09:20:59 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5280.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<li><strong>Previous message:</strong> <a href="5278.html">Eliezer S. Yudkowsky: "Re: Activism vs. Futurism"</a>
<li><strong>In reply to:</strong> <a href="5278.html">Eliezer S. Yudkowsky: "Re: Activism vs. Futurism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5280.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<li><strong>Reply:</strong> <a href="5280.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<li><strong>Reply:</strong> <a href="5281.html">Aaron McBride: "Meta: RE: Activism vs. Futurism"</a>
<li><strong>Reply:</strong> <a href="5290.html">Brian Atkins: "Re: Activism vs. Futurism"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5279">[ date ]</a>
<a href="index.html#5279">[ thread ]</a>
<a href="subject.html#5279">[ subject ]</a>
<a href="author.html#5279">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
hi,
<br>
<p><em>&gt;  &gt; Eliezer says:
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt;&gt; Ben, there is a difference between working at a full-time job that
</em><br>
<em>&gt;  &gt;&gt; happens to (in belief or in fact) benefit the Singularity; and
</em><br>
<em>&gt;  &gt;&gt; explicitly beginning from the Singularity as a starting point and
</em><br>
<em>&gt;  &gt;&gt; choosing your actions accordingly, before the fact rather than
</em><br>
<em>&gt;  &gt;&gt; afterward.
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; I guess there is a clear *psychological* difference there, but not a
</em><br>
<em>&gt;  &gt; clear *pragmatic* one.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Psychological differences *make* pragmatic differences.  If your life was
</em><br>
<em>&gt; more directed by abstract reasoning and less by fleeting subjective
</em><br>
<em>&gt; impressions
</em><br>
<p>You're quite presumptive, and quite incorrect, about my own life and
<br>
psychology, Eliezer!
<br>
<p>I don't know where you got the idea that my life is substantially driven by
<br>
&quot;fleeting subjective impressions&quot; ????
<br>
<p>That seems like a very strange claim to me, and would probably seem strange
<br>
to anyone who knew me well.
<br>
<p>For one thing, I've been going in basically the same direction with my life
<br>
since age 15 or so -- so whatever has been governing my life has hardly been
<br>
&quot;fleeting&quot; on the time scale of a human life (as opposed to, say, the
<br>
geological or astronomical time scales, on which ALL our lives are fleeting
<br>
;)
<br>
<p><em>&gt; you'd have more experience with the way that philosophical
</em><br>
<em>&gt; differences can propagate down to huge differences in action and strategy.
</em><br>
<p>Of course philosophical differences imply differences in actions and
<br>
strategies.  But they do so in VERY complex ways, not in obvious and simple
<br>
ways.
<br>
<p>For example, among my father's friends in an earlier stage of his life, I
<br>
knew many hard-line Marxists who sincerely believed the US government was
<br>
evil and had to be overthrown.   This came out of deeply-held philosophy on
<br>
their part.  Some of these people actually joined revolutionary efforts in
<br>
other countries; others stayed home and wrote papers.  One guy shot himself
<br>
partly because the new US revolution seemed so far off.  The same philosophy
<br>
led to many different actions.
<br>
<p><em>&gt;  &gt; Consider the case of someone who is working on a project for a while,
</em><br>
<em>&gt;  &gt; and later realizes that it has the potential to help with the
</em><br>
<em>&gt;  &gt; Singularity. Suppose they then continue their project with even greater
</em><br>
<em>&gt;  &gt; enthusiasm because they now see it's broader implications in terms of
</em><br>
<em>&gt;  &gt; the Singularity. To me, this person is working toward the Singularity
</em><br>
<em>&gt;  &gt; just as validly as if they had started their project with the
</em><br>
<em>&gt;  &gt; Singularity in mind.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes, well, again, that's because you haven't accumulated any experience
</em><br>
<em>&gt; with the intricacies of Singularity strategy
</em><br>
<p>You seem to believe that only you, and those who agree with you, have any
<br>
understanding of &quot;Singularity strategy&quot;
<br>
<p>To you, it seems, Kurzweil has lost the Singularity... I just don't get it
<br>
either... only a tiny handful of people see the light (i.e. think about the
<br>
Singularity close enough to the exact same way you do)
<br>
<p>It seems to me that there are many different ways of looking at the
<br>
Singularity and working toward it, and that with the current state of
<br>
knowledge, we really don't know whose view is correct.
<br>
<p>How do you explain the fact that
<br>
<p>a) you have written your views on the Singularity down
<br>
b) Kurzweil and I both are highly intelligent and know a lot about the
<br>
Singularity and are aware of your views [I don't know about Ray; I've read
<br>
them in detail]
<br>
c) Neither of us agrees with you in detail
<br>
<p>Do you explain it by
<br>
<p>1) saying that we're being irrational and you're being rational?
<br>
<p>Or
<br>
<p>2)admitting that you aren't able to make a convincing argument due to the
<br>
limited knowledge that your ideas are based upon, and the fact they they're
<br>
fundamentally based on some intuitive leaps.
<br>
<p><p>If 2), then how can you say with such confidence that only people who agree
<br>
closely with you have any understanding of Singularity strategy?
<br>
<p>If 1), then I think you're deluding yourself, of course...
<br>
<p><p><em>&gt; and hence have the to-me
</em><br>
<em>&gt; bizarre belief that you can take a project invented for other reasons and
</em><br>
<em>&gt; nudge it in the direction of a few specific aspects of the
</em><br>
<em>&gt; Singularity and
</em><br>
<em>&gt; end up with something that's as strong and coherent as a project created
</em><br>
<em>&gt; from scratch to serve the Singularity.
</em><br>
<p>Frankly, my own feeling is that my own project is significantly &quot;stronger&quot;
<br>
than the SIAI project.  However, I realize that I have a bias here, and that
<br>
my judgment here is based partly on intuitions; so I don't believe others
<br>
are irrational if they disagree with me!
<br>
<p>I don't really believe your approach to Friendly AI will work (based on what
<br>
I've read about it so far, for reasons that have been much discussed on this
<br>
list), and I haven't seen any algorithm-level details about your approach to
<br>
AI in general.  So I have no reason, at this stage, to consider SIAI's work
<br>
&quot;strong.&quot;
<br>
<p>I agree that SIAI's (i.e. your) writings are *coherent*, in the sense that
<br>
they all present a common and reasonably comprehensible point of view.  But
<br>
it's reasonably easy to do that in conceptual and even semi-technical
<br>
writings.  Marxist political writings and libertarian political writings are
<br>
each very coherent, in themselves, yet they can't both be correct!
<br>
<p><p><em>&gt; Of course, if I couldn't be friends with people who make what I see as
</em><br>
<em>&gt; blatant searing errors, I wouldn't have any friends.
</em><br>
<p>Hmmmm... I'm not going to follow this one up ;)
<br>
<p><p><em>&gt; What changed my life was Vinge's idea of
</em><br>
<em>&gt; smarter-than-human intelligence causing a breakdown in our model of the
</em><br>
<em>&gt; future, not any of the previous speculations about recursive
</em><br>
<em>&gt; self-improvement or faster-than-human thinking, which is why I think that
</em><br>
<em>&gt; Vinge hit the nail exactly on the head the first time (impressive, that)
</em><br>
<em>&gt; and that Kurzweil, Smart, and others who extrapolate Moore's Law are
</em><br>
<em>&gt; missing the whole point.
</em><br>
<p>I don't think it's fair to say that Kurzweil, Smart and others are &quot;missing
<br>
the whole point.&quot;
<br>
<p>I think they are seeing a slightly different point than you are (or I am),
<br>
and I think it's reasonably possible one of them will turn out to be righter
<br>
than you (or me).
<br>
<p>I disagree with them, but they're smart people and I can't convince them I'm
<br>
right, because our disagreements are based on intuitions rather than
<br>
demonstrated facts.
<br>
<p>Kurzweil believes that real AI will almost surely come about only thru
<br>
simulation of human brains, and that increase in intelligence of these
<br>
simulated human brains will be moderately but not extremely fast.
<br>
<p>I disagree with him on these points, but he's not &quot;missing the whole point&quot;
<br>
about the Singularity.
<br>
<p>George W. Bush, for example, is missing the whole point about the
<br>
Singularity!
<br>
<p>Although I believe AGI is achievable within this decade, and that
<br>
intelligence acceleration will be fact after human-level AGI is reached, I
<br>
don't believe these conclusions are *obvious*, and I don't expect to be able
<br>
to convince people with *differently-oriented intuitions* of these things
<br>
until after the AGI is achieved.  Fortunately I have found some others with
<br>
similarly-oriented intuitions to work on the project with me.
<br>
<p><em>&gt; Again, there's a difference between being *influenced* by a
</em><br>
<em>&gt; picture of the
</em><br>
<em>&gt; future, and making activist choices based on, and solely on, an explicit
</em><br>
<em>&gt; ethics and futuristic strategy.  This &quot;psychological difference&quot; is
</em><br>
<em>&gt; reflected in more complex strategies, the ability to rule out courses of
</em><br>
<em>&gt; action that would otherwise be rationalized, a perception of fine
</em><br>
<em>&gt; differences... all the things that humans use their intelligence for.
</em><br>
<p>Perhaps so....  However, I don't see this incredibly superior complexity of
<br>
strategy and fineness of perception in your writings or SIAI's actions so
<br>
far.  I'll be waiting with bated breath ;)
<br>
<p><em>&gt;  &gt; You may feel that someone who is explicitly working toward the
</em><br>
<em>&gt;  &gt; Singularity as the *prime supergoal* of all their actions, can be
</em><br>
<em>&gt;  &gt; trusted more thoroughly to make decisions pertinent toward the
</em><br>
<em>&gt;  &gt; Singularity.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It's not just a question of trust - although you're right, I don't trust
</em><br>
<em>&gt; you to make correct choices about when Novamente needs which Friendly AI
</em><br>
<em>&gt; features unless the sole and only point of Novamente is as a Singularity
</em><br>
<em>&gt; seed.
</em><br>
<p>Of course the sole and only *long term* point of Novamente is as a
<br>
Singularity seed.
<br>
<p>We are using interim versions of Novamente for commercial purposes (e.g.
<br>
bioinformatics), but under licensing agreements that leave us (the core team
<br>
of developers) full ownership of the codebase and full rights to use it for
<br>
research purposes.
<br>
<p>You may find this impure, but hey, we don't have a patron like SIAI does.
<br>
<p>We are intentionally structuring our commercial pursuits in such a way that
<br>
will not interfere with our long term humanitarian/transhumanitarian goals
<br>
for the system.  This is actually a pain in terms of legal paperwork, but is
<br>
a necessity because we ARE developing the system with these long term goals
<br>
in mind.
<br>
<p><p><em>&gt; It's a question of professional competence as a Singularity
</em><br>
<em>&gt; strategist; a whole area of thought that I don't think you've explored.
</em><br>
<p>You don't think I have explored it ... because I have made the choice not to
<br>
spend my time writing about it.
<br>
<p>Actually, many people have thought a great deal about Singularity strategy
<br>
with out taking their time to write a lot about it, for various reasons --
<br>
lack of enthusiasm for writing, or else (as in my case) not seeing a purpose
<br>
in doing writing on the topic at present.
<br>
<p>The time for me to write down my view on Singularity strategy will be when
<br>
Novamente approaches human-level AGI.  Furthermore, my thoughts will be more
<br>
valuable at that time due to the added insight obtained from experimenting
<br>
with near human level AI's.
<br>
<p><em>&gt; Your goal heterarchy has the strange property that one of the goals in it
</em><br>
<em>&gt; affects six billion lives, the fate of Earth-originating
</em><br>
<em>&gt; intelligent life,
</em><br>
<em>&gt; and the entire future, while the others do not.  Your bizarre attempt to
</em><br>
<em>&gt; consider these goals as coequal is the reason that I think you're using
</em><br>
<em>&gt; fleeting subjective impressions of importance rather than conscious
</em><br>
<em>&gt; consideration of predicted impacts.
</em><br>
<p>I tend look at things from more than one perspective.
<br>
<p><em>&gt;From a larger perspective, of course, working toward the Singularity is
</em><br>
tremendously more important than the goal of entertaining myself or taking
<br>
care of my kids....
<br>
<p><em>&gt;From my own perspective as an individual human, all these things are
</em><br>
important.  That's just the way it is.  So I make a balance.
<br>
<p>You also ignore the fact that there are differing degrees of certainty
<br>
attached to these different goals.
<br>
<p>I.e., whether my work will affect the Singularity is uncertain -- and it's
<br>
also possible that my (or your) work will affect it *badly* even though I
<br>
think it will affect it well...
<br>
<p>Whereas my shorter-term goals are rather more tangible, concrete, and easier
<br>
to estimate about...
<br>
<p><em>&gt; I realize that many people on Earth get along just fine using their
</em><br>
<em>&gt; built-in subjective impressions to assign relative importance to their
</em><br>
<em>&gt; goals, despite the flaws and the inconsistencies and the blatant searing
</em><br>
<em>&gt; errors from a normative standpoint; but for someone involved in the
</em><br>
<em>&gt; Singularity it is dangerous, and for a would-be constructor of real AI it
</em><br>
<em>&gt; is absurd.
</em><br>
<p>It's not really absurd.  I'm a human being, and working toward creating real
<br>
AI is one aspect of my life.  It's a very, very important aspect, but still
<br>
it's not my ENTIRE life.
<br>
<p>This afternoon, I'm not deciding whether to go outside and play with my kids
<br>
this afternoon, or stay here at the computer, based on a rational
<br>
calculation.  I'm deciding it based on my mixed-up human judgment, which
<br>
incorporates the fact that it'll be fun to go outside and play, that it's
<br>
good for the kids to get time with their dad, etc.
<br>
<p>I don't think I need to govern the details of my day to day life based on
<br>
rational calculation in order to be rational about designing AGI.
<br>
<p><em>&gt; This from the person who seems unwilling to believe that real altruists
</em><br>
<em>&gt; exist?  It's not like I'm the only one, or even a very exceptional one if
</em><br>
<em>&gt; we're just going to measure strength of commitment.  Go watch the film
</em><br>
<em>&gt; &quot;Gandhi&quot; some time and ask yourself about the thousands of people who
</em><br>
<em>&gt; followed Gandhi into the line of fire *without* even Gandhi's protection
</em><br>
<em>&gt; of celebrity.  Now why wouldn't you expect people like that to get
</em><br>
<em>&gt; involved with the Singularity?  Where *else* would they go?
</em><br>
<p>I'm not sure what you mean exactly by comparing yourself to Gandhi?
<br>
<p>He was a great man, but not a perfect man -- he made some bad errors in
<br>
judgment, and it's clear from his biography that he was motivated by plenty
<br>
of his own psychological demons as well as by altruistic feelings.
<br>
<p>Of course many people will follow inspirational leaders who hold extreme
<br>
points of view.  That doesn't mean that I will agree these leaders have good
<br>
judgment!
<br>
<p><em>&gt; Well, Ben, this is because there are two groups of people who know damn
</em><br>
<em>&gt; well that SIAI is devoted solely, firstly, and only to the Singularity,
</em><br>
<em>&gt; and unfortunately you belong to neither.
</em><br>
<p>I understand that the *idea* of SIAI is to promote the Singularity
<br>
generically.
<br>
<p>However, the *practice* of SIAI, so far, seems very narrowly tied to your
<br>
own perspectives on all Singularity-related issues.
<br>
<p>May I ask, what does SIAI plan to do to promote alternatives to your
<br>
approach to AGI?  If it gets a lot of funds, will it split the money among
<br>
different AGI projects, or will it put them all into your own AGI project?
<br>
<p>What does it plan to do to promote alternatives to your own speculative
<br>
theory on Friendly AI?  (And I think all theories on Friendly AI are
<br>
speculative at this point, not just yours.)
<br>
<p>When I see the SIAI website posting some of the views on Friendly AI that
<br>
explicitly contradict your own, I'll start to feel more like it's a generic
<br>
Singularity-promoting organization.
<br>
<p>When I see the SIAI funding people whose views explicitly contradict your
<br>
own, then I'll really believe SIAI is what you say it is.
<br>
<p>I must note that my own organizations are NOT generic in nature.  My &quot;Real
<br>
AI Institute&quot; is devoted specifically to developing the Novamente AI Engine
<br>
for research purposes, and makes no pretensions of genericity.
<br>
<p>I do think there is a role for a generic Singularity-promoting organization,
<br>
but I think it would be best if this organization were not tied to anyone's
<br>
particular views on AI or Friendliness or other specific topics -- not mine,
<br>
not yours, not Kurzweil's, etc.
<br>
<p><p><em>&gt;  You try to avoid
</em><br>
<em>&gt; labeling
</em><br>
<em>&gt; different ways of thinking as &quot;wrong&quot; but the price of doing so
</em><br>
<em>&gt; appears to
</em><br>
<em>&gt; have been that you can no longer really appreciate that wrong ways of
</em><br>
<em>&gt; thinking exist.  I'm a rational altruist working solely for the
</em><br>
<em>&gt; Singularity and that involves major real differences from your way of
</em><br>
<em>&gt; thinking.  Get over it.  If you think my psychology is wrong, say so, but
</em><br>
<em>&gt; accept that my mind works differently than yours.
</em><br>
<p>I do think wrong ways of thinking exist.  I think that Hitler had a wrong
<br>
way of thinking, and I think that George W. Bush has a wrong way of
<br>
thinking, and I think that nearly all religious people have wrong ways of
<br>
thinking.
<br>
<p>I don't know if you have a wrong way of thinking or not.  However, I worry
<br>
sometimes that you may have a psychologically unhealthy way of thinking.  I
<br>
hope not...
<br>
<p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5280.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<li><strong>Previous message:</strong> <a href="5278.html">Eliezer S. Yudkowsky: "Re: Activism vs. Futurism"</a>
<li><strong>In reply to:</strong> <a href="5278.html">Eliezer S. Yudkowsky: "Re: Activism vs. Futurism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5280.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<li><strong>Reply:</strong> <a href="5280.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<li><strong>Reply:</strong> <a href="5281.html">Aaron McBride: "Meta: RE: Activism vs. Futurism"</a>
<li><strong>Reply:</strong> <a href="5290.html">Brian Atkins: "Re: Activism vs. Futurism"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5279">[ date ]</a>
<a href="index.html#5279">[ thread ]</a>
<a href="subject.html#5279">[ subject ]</a>
<a href="author.html#5279">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
