<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Activism vs. Futurism</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Activism vs. Futurism">
<meta name="Date" content="2002-09-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Activism vs. Futurism</h1>
<!-- received="Sat Sep 07 11:51:53 2002" -->
<!-- isoreceived="20020907175153" -->
<!-- sent="Sat, 7 Sep 2002 09:31:57 -0600" -->
<!-- isosent="20020907153157" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Activism vs. Futurism" -->
<!-- id="LAEGJLOGJIOELPNIOOAJOECGDJAA.ben@goertzel.org" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJIECGDJAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Activism%20vs.%20Futurism"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sat Sep 07 2002 - 09:31:57 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5281.html">Aaron McBride: "Meta: RE: Activism vs. Futurism"</a>
<li><strong>Previous message:</strong> <a href="5279.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<li><strong>In reply to:</strong> <a href="5279.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5281.html">Aaron McBride: "Meta: RE: Activism vs. Futurism"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5280">[ date ]</a>
<a href="index.html#5280">[ thread ]</a>
<a href="subject.html#5280">[ subject ]</a>
<a href="author.html#5280">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Please remember, Eli, all this yakking started because I was taken aback by
<br>
your statement that you and your new colleague at SIAI are the *only ones in
<br>
the world* working on the Singularity full-time.
<br>
<p>All the quibbling on various related points aside, I still think this
<br>
statement on your part is wrong, and reflects a possibly dangerous
<br>
egocentricity.
<br>
<p>-- Ben G
<br>
<p><p><p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: Ben Goertzel [mailto:<a href="mailto:ben@goertzel.org?Subject=RE:%20Activism%20vs.%20Futurism">ben@goertzel.org</a>]
</em><br>
<em>&gt; Sent: Saturday, September 07, 2002 9:21 AM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sysopmind.com?Subject=RE:%20Activism%20vs.%20Futurism">sl4@sysopmind.com</a>
</em><br>
<em>&gt; Subject: RE: Activism vs. Futurism
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; hi,
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;  &gt; Eliezer says:
</em><br>
<em>&gt; &gt;  &gt;
</em><br>
<em>&gt; &gt;  &gt;&gt; Ben, there is a difference between working at a full-time job that
</em><br>
<em>&gt; &gt;  &gt;&gt; happens to (in belief or in fact) benefit the Singularity; and
</em><br>
<em>&gt; &gt;  &gt;&gt; explicitly beginning from the Singularity as a starting point and
</em><br>
<em>&gt; &gt;  &gt;&gt; choosing your actions accordingly, before the fact rather than
</em><br>
<em>&gt; &gt;  &gt;&gt; afterward.
</em><br>
<em>&gt; &gt;  &gt;
</em><br>
<em>&gt; &gt;  &gt; I guess there is a clear *psychological* difference there, but not a
</em><br>
<em>&gt; &gt;  &gt; clear *pragmatic* one.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Psychological differences *make* pragmatic differences.  If
</em><br>
<em>&gt; your life was
</em><br>
<em>&gt; &gt; more directed by abstract reasoning and less by fleeting subjective
</em><br>
<em>&gt; &gt; impressions
</em><br>
<em>&gt;
</em><br>
<em>&gt; You're quite presumptive, and quite incorrect, about my own life
</em><br>
<em>&gt; and psychology, Eliezer!
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't know where you got the idea that my life is substantially
</em><br>
<em>&gt; driven by &quot;fleeting subjective impressions&quot; ????
</em><br>
<em>&gt;
</em><br>
<em>&gt; That seems like a very strange claim to me, and would probably
</em><br>
<em>&gt; seem strange to anyone who knew me well.
</em><br>
<em>&gt;
</em><br>
<em>&gt; For one thing, I've been going in basically the same direction
</em><br>
<em>&gt; with my life since age 15 or so -- so whatever has been governing
</em><br>
<em>&gt; my life has hardly been &quot;fleeting&quot; on the time scale of a human
</em><br>
<em>&gt; life (as opposed to, say, the geological or astronomical time
</em><br>
<em>&gt; scales, on which ALL our lives are fleeting ;)
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; you'd have more experience with the way that philosophical
</em><br>
<em>&gt; &gt; differences can propagate down to huge differences in action
</em><br>
<em>&gt; and strategy.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Of course philosophical differences imply differences in actions
</em><br>
<em>&gt; and strategies.  But they do so in VERY complex ways, not in
</em><br>
<em>&gt; obvious and simple ways.
</em><br>
<em>&gt;
</em><br>
<em>&gt; For example, among my father's friends in an earlier stage of his
</em><br>
<em>&gt; life, I knew many hard-line Marxists who sincerely believed the
</em><br>
<em>&gt; US government was evil and had to be overthrown.   This came out
</em><br>
<em>&gt; of deeply-held philosophy on their part.  Some of these people
</em><br>
<em>&gt; actually joined revolutionary efforts in other countries; others
</em><br>
<em>&gt; stayed home and wrote papers.  One guy shot himself partly
</em><br>
<em>&gt; because the new US revolution seemed so far off.  The same
</em><br>
<em>&gt; philosophy led to many different actions.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;  &gt; Consider the case of someone who is working on a project for a while,
</em><br>
<em>&gt; &gt;  &gt; and later realizes that it has the potential to help with the
</em><br>
<em>&gt; &gt;  &gt; Singularity. Suppose they then continue their project with
</em><br>
<em>&gt; even greater
</em><br>
<em>&gt; &gt;  &gt; enthusiasm because they now see it's broader implications in terms of
</em><br>
<em>&gt; &gt;  &gt; the Singularity. To me, this person is working toward the Singularity
</em><br>
<em>&gt; &gt;  &gt; just as validly as if they had started their project with the
</em><br>
<em>&gt; &gt;  &gt; Singularity in mind.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Yes, well, again, that's because you haven't accumulated any experience
</em><br>
<em>&gt; &gt; with the intricacies of Singularity strategy
</em><br>
<em>&gt;
</em><br>
<em>&gt; You seem to believe that only you, and those who agree with you,
</em><br>
<em>&gt; have any understanding of &quot;Singularity strategy&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; To you, it seems, Kurzweil has lost the Singularity... I just
</em><br>
<em>&gt; don't get it either... only a tiny handful of people see the
</em><br>
<em>&gt; light (i.e. think about the Singularity close enough to the exact
</em><br>
<em>&gt; same way you do)
</em><br>
<em>&gt;
</em><br>
<em>&gt; It seems to me that there are many different ways of looking at
</em><br>
<em>&gt; the Singularity and working toward it, and that with the current
</em><br>
<em>&gt; state of knowledge, we really don't know whose view is correct.
</em><br>
<em>&gt;
</em><br>
<em>&gt; How do you explain the fact that
</em><br>
<em>&gt;
</em><br>
<em>&gt; a) you have written your views on the Singularity down
</em><br>
<em>&gt; b) Kurzweil and I both are highly intelligent and know a lot
</em><br>
<em>&gt; about the Singularity and are aware of your views [I don't know
</em><br>
<em>&gt; about Ray; I've read them in detail]
</em><br>
<em>&gt; c) Neither of us agrees with you in detail
</em><br>
<em>&gt;
</em><br>
<em>&gt; Do you explain it by
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1) saying that we're being irrational and you're being rational?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Or
</em><br>
<em>&gt;
</em><br>
<em>&gt; 2)admitting that you aren't able to make a convincing argument
</em><br>
<em>&gt; due to the limited knowledge that your ideas are based upon, and
</em><br>
<em>&gt; the fact they they're fundamentally based on some intuitive leaps.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; If 2), then how can you say with such confidence that only people
</em><br>
<em>&gt; who agree closely with you have any understanding of Singularity strategy?
</em><br>
<em>&gt;
</em><br>
<em>&gt; If 1), then I think you're deluding yourself, of course...
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; and hence have the to-me
</em><br>
<em>&gt; &gt; bizarre belief that you can take a project invented for other
</em><br>
<em>&gt; reasons and
</em><br>
<em>&gt; &gt; nudge it in the direction of a few specific aspects of the
</em><br>
<em>&gt; &gt; Singularity and
</em><br>
<em>&gt; &gt; end up with something that's as strong and coherent as a
</em><br>
<em>&gt; project created
</em><br>
<em>&gt; &gt; from scratch to serve the Singularity.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Frankly, my own feeling is that my own project is significantly
</em><br>
<em>&gt; &quot;stronger&quot; than the SIAI project.  However, I realize that I have
</em><br>
<em>&gt; a bias here, and that my judgment here is based partly on
</em><br>
<em>&gt; intuitions; so I don't believe others are irrational if they
</em><br>
<em>&gt; disagree with me!
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't really believe your approach to Friendly AI will work
</em><br>
<em>&gt; (based on what I've read about it so far, for reasons that have
</em><br>
<em>&gt; been much discussed on this list), and I haven't seen any
</em><br>
<em>&gt; algorithm-level details about your approach to AI in general.  So
</em><br>
<em>&gt; I have no reason, at this stage, to consider SIAI's work &quot;strong.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I agree that SIAI's (i.e. your) writings are *coherent*, in the
</em><br>
<em>&gt; sense that they all present a common and reasonably
</em><br>
<em>&gt; comprehensible point of view.  But it's reasonably easy to do
</em><br>
<em>&gt; that in conceptual and even semi-technical writings.  Marxist
</em><br>
<em>&gt; political writings and libertarian political writings are each
</em><br>
<em>&gt; very coherent, in themselves, yet they can't both be correct!
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Of course, if I couldn't be friends with people who make what I see as
</em><br>
<em>&gt; &gt; blatant searing errors, I wouldn't have any friends.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Hmmmm... I'm not going to follow this one up ;)
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; What changed my life was Vinge's idea of
</em><br>
<em>&gt; &gt; smarter-than-human intelligence causing a breakdown in our model of the
</em><br>
<em>&gt; &gt; future, not any of the previous speculations about recursive
</em><br>
<em>&gt; &gt; self-improvement or faster-than-human thinking, which is why I
</em><br>
<em>&gt; think that
</em><br>
<em>&gt; &gt; Vinge hit the nail exactly on the head the first time
</em><br>
<em>&gt; (impressive, that)
</em><br>
<em>&gt; &gt; and that Kurzweil, Smart, and others who extrapolate Moore's Law are
</em><br>
<em>&gt; &gt; missing the whole point.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't think it's fair to say that Kurzweil, Smart and others
</em><br>
<em>&gt; are &quot;missing the whole point.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think they are seeing a slightly different point than you are
</em><br>
<em>&gt; (or I am), and I think it's reasonably possible one of them will
</em><br>
<em>&gt; turn out to be righter than you (or me).
</em><br>
<em>&gt;
</em><br>
<em>&gt; I disagree with them, but they're smart people and I can't
</em><br>
<em>&gt; convince them I'm right, because our disagreements are based on
</em><br>
<em>&gt; intuitions rather than demonstrated facts.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Kurzweil believes that real AI will almost surely come about only
</em><br>
<em>&gt; thru simulation of human brains, and that increase in
</em><br>
<em>&gt; intelligence of these simulated human brains will be moderately
</em><br>
<em>&gt; but not extremely fast.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I disagree with him on these points, but he's not &quot;missing the
</em><br>
<em>&gt; whole point&quot; about the Singularity.
</em><br>
<em>&gt;
</em><br>
<em>&gt; George W. Bush, for example, is missing the whole point about the
</em><br>
<em>&gt; Singularity!
</em><br>
<em>&gt;
</em><br>
<em>&gt; Although I believe AGI is achievable within this decade, and that
</em><br>
<em>&gt; intelligence acceleration will be fact after human-level AGI is
</em><br>
<em>&gt; reached, I don't believe these conclusions are *obvious*, and I
</em><br>
<em>&gt; don't expect to be able to convince people with
</em><br>
<em>&gt; *differently-oriented intuitions* of these things until after the
</em><br>
<em>&gt; AGI is achieved.  Fortunately I have found some others with
</em><br>
<em>&gt; similarly-oriented intuitions to work on the project with me.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Again, there's a difference between being *influenced* by a
</em><br>
<em>&gt; &gt; picture of the
</em><br>
<em>&gt; &gt; future, and making activist choices based on, and solely on, an
</em><br>
<em>&gt; explicit
</em><br>
<em>&gt; &gt; ethics and futuristic strategy.  This &quot;psychological difference&quot; is
</em><br>
<em>&gt; &gt; reflected in more complex strategies, the ability to rule out
</em><br>
<em>&gt; courses of
</em><br>
<em>&gt; &gt; action that would otherwise be rationalized, a perception of fine
</em><br>
<em>&gt; &gt; differences... all the things that humans use their intelligence for.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Perhaps so....  However, I don't see this incredibly superior
</em><br>
<em>&gt; complexity of strategy and fineness of perception in your
</em><br>
<em>&gt; writings or SIAI's actions so far.  I'll be waiting with bated breath ;)
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;  &gt; You may feel that someone who is explicitly working toward the
</em><br>
<em>&gt; &gt;  &gt; Singularity as the *prime supergoal* of all their actions, can be
</em><br>
<em>&gt; &gt;  &gt; trusted more thoroughly to make decisions pertinent toward the
</em><br>
<em>&gt; &gt;  &gt; Singularity.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; It's not just a question of trust - although you're right, I
</em><br>
<em>&gt; don't trust
</em><br>
<em>&gt; &gt; you to make correct choices about when Novamente needs which
</em><br>
<em>&gt; Friendly AI
</em><br>
<em>&gt; &gt; features unless the sole and only point of Novamente is as a
</em><br>
<em>&gt; Singularity
</em><br>
<em>&gt; &gt; seed.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Of course the sole and only *long term* point of Novamente is as
</em><br>
<em>&gt; a Singularity seed.
</em><br>
<em>&gt;
</em><br>
<em>&gt; We are using interim versions of Novamente for commercial
</em><br>
<em>&gt; purposes (e.g. bioinformatics), but under licensing agreements
</em><br>
<em>&gt; that leave us (the core team of developers) full ownership of the
</em><br>
<em>&gt; codebase and full rights to use it for research purposes.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You may find this impure, but hey, we don't have a patron like
</em><br>
<em>&gt; SIAI does.
</em><br>
<em>&gt;
</em><br>
<em>&gt; We are intentionally structuring our commercial pursuits in such
</em><br>
<em>&gt; a way that will not interfere with our long term
</em><br>
<em>&gt; humanitarian/transhumanitarian goals for the system.  This is
</em><br>
<em>&gt; actually a pain in terms of legal paperwork, but is a necessity
</em><br>
<em>&gt; because we ARE developing the system with these long term goals in mind.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; It's a question of professional competence as a Singularity
</em><br>
<em>&gt; &gt; strategist; a whole area of thought that I don't think you've explored.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You don't think I have explored it ... because I have made the
</em><br>
<em>&gt; choice not to spend my time writing about it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Actually, many people have thought a great deal about Singularity
</em><br>
<em>&gt; strategy with out taking their time to write a lot about it, for
</em><br>
<em>&gt; various reasons -- lack of enthusiasm for writing, or else (as in
</em><br>
<em>&gt; my case) not seeing a purpose in doing writing on the topic at present.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The time for me to write down my view on Singularity strategy
</em><br>
<em>&gt; will be when Novamente approaches human-level AGI.  Furthermore,
</em><br>
<em>&gt; my thoughts will be more valuable at that time due to the added
</em><br>
<em>&gt; insight obtained from experimenting with near human level AI's.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Your goal heterarchy has the strange property that one of the
</em><br>
<em>&gt; goals in it
</em><br>
<em>&gt; &gt; affects six billion lives, the fate of Earth-originating
</em><br>
<em>&gt; &gt; intelligent life,
</em><br>
<em>&gt; &gt; and the entire future, while the others do not.  Your bizarre
</em><br>
<em>&gt; attempt to
</em><br>
<em>&gt; &gt; consider these goals as coequal is the reason that I think you're using
</em><br>
<em>&gt; &gt; fleeting subjective impressions of importance rather than conscious
</em><br>
<em>&gt; &gt; consideration of predicted impacts.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I tend look at things from more than one perspective.
</em><br>
<em>&gt;
</em><br>
<em>&gt; From a larger perspective, of course, working toward the
</em><br>
<em>&gt; Singularity is tremendously more important than the goal of
</em><br>
<em>&gt; entertaining myself or taking care of my kids....
</em><br>
<em>&gt;
</em><br>
<em>&gt; From my own perspective as an individual human, all these things
</em><br>
<em>&gt; are important.  That's just the way it is.  So I make a balance.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You also ignore the fact that there are differing degrees of
</em><br>
<em>&gt; certainty attached to these different goals.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I.e., whether my work will affect the Singularity is uncertain --
</em><br>
<em>&gt; and it's also possible that my (or your) work will affect it
</em><br>
<em>&gt; *badly* even though I think it will affect it well...
</em><br>
<em>&gt;
</em><br>
<em>&gt; Whereas my shorter-term goals are rather more tangible, concrete,
</em><br>
<em>&gt; and easier to estimate about...
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; I realize that many people on Earth get along just fine using their
</em><br>
<em>&gt; &gt; built-in subjective impressions to assign relative importance to their
</em><br>
<em>&gt; &gt; goals, despite the flaws and the inconsistencies and the
</em><br>
<em>&gt; blatant searing
</em><br>
<em>&gt; &gt; errors from a normative standpoint; but for someone involved in the
</em><br>
<em>&gt; &gt; Singularity it is dangerous, and for a would-be constructor of
</em><br>
<em>&gt; real AI it
</em><br>
<em>&gt; &gt; is absurd.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It's not really absurd.  I'm a human being, and working toward
</em><br>
<em>&gt; creating real AI is one aspect of my life.  It's a very, very
</em><br>
<em>&gt; important aspect, but still it's not my ENTIRE life.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This afternoon, I'm not deciding whether to go outside and play
</em><br>
<em>&gt; with my kids this afternoon, or stay here at the computer, based
</em><br>
<em>&gt; on a rational calculation.  I'm deciding it based on my mixed-up
</em><br>
<em>&gt; human judgment, which incorporates the fact that it'll be fun to
</em><br>
<em>&gt; go outside and play, that it's good for the kids to get time with
</em><br>
<em>&gt; their dad, etc.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't think I need to govern the details of my day to day life
</em><br>
<em>&gt; based on rational calculation in order to be rational about designing AGI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; This from the person who seems unwilling to believe that real altruists
</em><br>
<em>&gt; &gt; exist?  It's not like I'm the only one, or even a very
</em><br>
<em>&gt; exceptional one if
</em><br>
<em>&gt; &gt; we're just going to measure strength of commitment.  Go watch the film
</em><br>
<em>&gt; &gt; &quot;Gandhi&quot; some time and ask yourself about the thousands of people who
</em><br>
<em>&gt; &gt; followed Gandhi into the line of fire *without* even Gandhi's
</em><br>
<em>&gt; protection
</em><br>
<em>&gt; &gt; of celebrity.  Now why wouldn't you expect people like that to get
</em><br>
<em>&gt; &gt; involved with the Singularity?  Where *else* would they go?
</em><br>
<em>&gt;
</em><br>
<em>&gt; I'm not sure what you mean exactly by comparing yourself to Gandhi?
</em><br>
<em>&gt;
</em><br>
<em>&gt; He was a great man, but not a perfect man -- he made some bad
</em><br>
<em>&gt; errors in judgment, and it's clear from his biography that he was
</em><br>
<em>&gt; motivated by plenty of his own psychological demons as well as by
</em><br>
<em>&gt; altruistic feelings.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Of course many people will follow inspirational leaders who hold
</em><br>
<em>&gt; extreme points of view.  That doesn't mean that I will agree
</em><br>
<em>&gt; these leaders have good judgment!
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Well, Ben, this is because there are two groups of people who know damn
</em><br>
<em>&gt; &gt; well that SIAI is devoted solely, firstly, and only to the Singularity,
</em><br>
<em>&gt; &gt; and unfortunately you belong to neither.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I understand that the *idea* of SIAI is to promote the
</em><br>
<em>&gt; Singularity generically.
</em><br>
<em>&gt;
</em><br>
<em>&gt; However, the *practice* of SIAI, so far, seems very narrowly tied
</em><br>
<em>&gt; to your own perspectives on all Singularity-related issues.
</em><br>
<em>&gt;
</em><br>
<em>&gt; May I ask, what does SIAI plan to do to promote alternatives to
</em><br>
<em>&gt; your approach to AGI?  If it gets a lot of funds, will it split
</em><br>
<em>&gt; the money among different AGI projects, or will it put them all
</em><br>
<em>&gt; into your own AGI project?
</em><br>
<em>&gt;
</em><br>
<em>&gt; What does it plan to do to promote alternatives to your own
</em><br>
<em>&gt; speculative theory on Friendly AI?  (And I think all theories on
</em><br>
<em>&gt; Friendly AI are speculative at this point, not just yours.)
</em><br>
<em>&gt;
</em><br>
<em>&gt; When I see the SIAI website posting some of the views on Friendly
</em><br>
<em>&gt; AI that explicitly contradict your own, I'll start to feel more
</em><br>
<em>&gt; like it's a generic Singularity-promoting organization.
</em><br>
<em>&gt;
</em><br>
<em>&gt; When I see the SIAI funding people whose views explicitly
</em><br>
<em>&gt; contradict your own, then I'll really believe SIAI is what you say it is.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I must note that my own organizations are NOT generic in nature.
</em><br>
<em>&gt; My &quot;Real AI Institute&quot; is devoted specifically to developing the
</em><br>
<em>&gt; Novamente AI Engine for research purposes, and makes no
</em><br>
<em>&gt; pretensions of genericity.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I do think there is a role for a generic Singularity-promoting
</em><br>
<em>&gt; organization, but I think it would be best if this organization
</em><br>
<em>&gt; were not tied to anyone's particular views on AI or Friendliness
</em><br>
<em>&gt; or other specific topics -- not mine, not yours, not Kurzweil's, etc.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;  You try to avoid
</em><br>
<em>&gt; &gt; labeling
</em><br>
<em>&gt; &gt; different ways of thinking as &quot;wrong&quot; but the price of doing so
</em><br>
<em>&gt; &gt; appears to
</em><br>
<em>&gt; &gt; have been that you can no longer really appreciate that wrong ways of
</em><br>
<em>&gt; &gt; thinking exist.  I'm a rational altruist working solely for the
</em><br>
<em>&gt; &gt; Singularity and that involves major real differences from your way of
</em><br>
<em>&gt; &gt; thinking.  Get over it.  If you think my psychology is wrong,
</em><br>
<em>&gt; say so, but
</em><br>
<em>&gt; &gt; accept that my mind works differently than yours.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I do think wrong ways of thinking exist.  I think that Hitler had
</em><br>
<em>&gt; a wrong way of thinking, and I think that George W. Bush has a
</em><br>
<em>&gt; wrong way of thinking, and I think that nearly all religious
</em><br>
<em>&gt; people have wrong ways of thinking.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't know if you have a wrong way of thinking or not.
</em><br>
<em>&gt; However, I worry sometimes that you may have a psychologically
</em><br>
<em>&gt; unhealthy way of thinking.  I hope not...
</em><br>
<em>&gt;
</em><br>
<em>&gt; -- Ben G
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5281.html">Aaron McBride: "Meta: RE: Activism vs. Futurism"</a>
<li><strong>Previous message:</strong> <a href="5279.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<li><strong>In reply to:</strong> <a href="5279.html">Ben Goertzel: "RE: Activism vs. Futurism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5281.html">Aaron McBride: "Meta: RE: Activism vs. Futurism"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5280">[ date ]</a>
<a href="index.html#5280">[ thread ]</a>
<a href="subject.html#5280">[ subject ]</a>
<a href="author.html#5280">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
