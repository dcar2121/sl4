<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: the contingency of value systems in an AI</title>
<meta name="Author" content="Mitchell J Porter (mjporter@U.Arizona.EDU)">
<meta name="Subject" content="the contingency of value systems in an AI">
<meta name="Date" content="2001-06-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>the contingency of value systems in an AI</h1>
<!-- received="Mon Jun 04 12:13:02 2001" -->
<!-- isoreceived="20010604181302" -->
<!-- sent="Mon, 4 Jun 2001 01:28:20 -0700 (MST)" -->
<!-- isosent="20010604082820" -->
<!-- name="Mitchell J Porter" -->
<!-- email="mjporter@U.Arizona.EDU" -->
<!-- subject="the contingency of value systems in an AI" -->
<!-- id="Pine.A41.4.21.0106040125430.42664-100000@f1n8.u.arizona.edu" -->
<!-- charset="US-ASCII" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Mitchell J Porter (<a href="mailto:mjporter@U.Arizona.EDU?Subject=Re:%20the%20contingency%20of%20value%20systems%20in%20an%20AI"><em>mjporter@U.Arizona.EDU</em></a>)<br>
<strong>Date:</strong> Mon Jun 04 2001 - 02:28:20 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1437.html">Eliezer S. Yudkowsky: "Re: the contingency of value systems in an AI"</a>
<li><strong>Previous message:</strong> <a href="1435.html">do_not_reply_to_this_address: "Re: superintelligence and ethical egoism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1437.html">Eliezer S. Yudkowsky: "Re: the contingency of value systems in an AI"</a>
<li><strong>Reply:</strong> <a href="1437.html">Eliezer S. Yudkowsky: "Re: the contingency of value systems in an AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1436">[ date ]</a>
<a href="index.html#1436">[ thread ]</a>
<a href="subject.html#1436">[ subject ]</a>
<a href="author.html#1436">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I don't think people realize how much of a blank slate an AI would be. 
<br>
For a cognitive architecture along the lines of: 
<br>
<p>main () {
<br>
&nbsp;&nbsp;until (goalCondition(worldState) == TRUE) do {
<br>
&nbsp;&nbsp;&nbsp;&nbsp;chooseActionMostLikelyToRealizeGoalCondition(); 
<br>
&nbsp;&nbsp;&nbsp;&nbsp;performAction();
<br>
&nbsp;&nbsp;&nbsp;&nbsp;updateWorldState();
<br>
&nbsp;&nbsp;} ;
<br>
}
<br>
<p>- *anything* can serve as a goal condition. If chooseAction...()
<br>
is smart enough, such a program deserves to be regarded as
<br>
superintelligent. One might say that the superintelligence in
<br>
such an entity is confined to the means rather than the ends. 
<br>
But frankly I have trouble seeing how the *ends* (the goal condition, 
<br>
goal system, value system, method of ranking possible futures,...)
<br>
can ever be regarded as intrinsically intelligent or intrinsically
<br>
stupid. For such an architecture, intelligence is a property of
<br>
the methods used to achieve the goals, not of the goals themselves. 
<br>
<p>This is not to say that there's no relationship between intelligence
<br>
and goals. For example, a certain degree of 'representational
<br>
intelligence' is necessary just to describe a goal state. And
<br>
a self-modifying AI which started with a particular supergoal might
<br>
well decide to change its goals, *if* the new goal system led to
<br>
the achievement of the original supergoal more effectively than
<br>
a goal system in which that original goal remains explicitly at 
<br>
the apex. (Actually, I'm not sure that *supergoal* change would
<br>
ever be called for except in very anomalous circumstances. If your
<br>
supergoal is X, and you find that there is a higher power which
<br>
destroys anything whose supergoal is X, then you should change
<br>
your supergoal to a Y which has X as a subgoal, since that will
<br>
increase the chances of condition X being realized. But in a
<br>
generic social context of competitive self-interested entities, 
<br>
it should be enough to make self-preservation, peaceful coexistence,
<br>
etc., prominent subgoals, in which case they will remain capable
<br>
of being overridden by a supergoal.) 
<br>
<p>Ben Houston said
<br>
<p><em>&gt; Do you really think they would be that uncaring?
</em><br>
<p>My point is just that they could be. 
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1437.html">Eliezer S. Yudkowsky: "Re: the contingency of value systems in an AI"</a>
<li><strong>Previous message:</strong> <a href="1435.html">do_not_reply_to_this_address: "Re: superintelligence and ethical egoism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1437.html">Eliezer S. Yudkowsky: "Re: the contingency of value systems in an AI"</a>
<li><strong>Reply:</strong> <a href="1437.html">Eliezer S. Yudkowsky: "Re: the contingency of value systems in an AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1436">[ date ]</a>
<a href="index.html#1436">[ thread ]</a>
<a href="subject.html#1436">[ subject ]</a>
<a href="author.html#1436">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
