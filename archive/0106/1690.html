<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Putting super-intelligence in a body</title>
<meta name="Author" content="Brian Phillips (deepbluehalo@earthlink.net)">
<meta name="Subject" content="Re: Putting super-intelligence in a body">
<meta name="Date" content="2001-06-30">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Putting super-intelligence in a body</h1>
<!-- received="Sat Jun 30 21:14:55 2001" -->
<!-- isoreceived="20010701031455" -->
<!-- sent="Sat, 30 Jun 2001 20:16:39 -0400" -->
<!-- isosent="20010701001639" -->
<!-- name="Brian Phillips" -->
<!-- email="deepbluehalo@earthlink.net" -->
<!-- subject="Re: Putting super-intelligence in a body" -->
<!-- id="003101c101c3$1a77e4c0$3d5af7a5@computer" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="3B3E51BF.15D28E0E@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brian Phillips (<a href="mailto:deepbluehalo@earthlink.net?Subject=Re:%20Putting%20super-intelligence%20in%20a%20body"><em>deepbluehalo@earthlink.net</em></a>)<br>
<strong>Date:</strong> Sat Jun 30 2001 - 18:16:39 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1691.html">gabriel C: "Re: SI Jail"</a>
<li><strong>Previous message:</strong> <a href="1689.html">gabriel C: "Re: SI Jail"</a>
<li><strong>In reply to:</strong> <a href="1687.html">Eliezer S. Yudkowsky: "Re: Putting super-intelligence in a body"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1692.html">Brian Phillips: "Re: Putting super-intelligence in a body"</a>
<li><strong>Reply:</strong> <a href="1692.html">Brian Phillips: "Re: Putting super-intelligence in a body"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1690">[ date ]</a>
<a href="index.html#1690">[ thread ]</a>
<a href="subject.html#1690">[ subject ]</a>
<a href="author.html#1690">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
----- Original Message -----
<br>
From: Eliezer S. Yudkowsky &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Putting%20super-intelligence%20in%20a%20body">sentience@pobox.com</a>&gt;
<br>
To: &lt;<a href="mailto:sl4@sysopmind.com?Subject=Re:%20Putting%20super-intelligence%20in%20a%20body">sl4@sysopmind.com</a>&gt;
<br>
Sent: Saturday, June 30, 2001 6:25 PM
<br>
Subject: Re: Putting super-intelligence in a body
<br>
<p><p><em>&gt; Jack Richardson wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I'm not sure we can assume that an enhanced computer necessarily will be
</em><br>
the
<br>
<em>&gt; &gt; mechanism through which a transhuman AI arises. On the other hand, a
</em><br>
<em>&gt; &gt; friendly AI might well be needed to protect us from ourselves. It may be
</em><br>
<em>&gt; &gt; that an advanced AI can be developed in the next ten years, but soon
</em><br>
after
<br>
<em>&gt; &gt; that, rapidly enhancing humans will begin to catch up.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Jack, you're being far too conservative.  You are, as Kurzweil will
</em><br>
<em>&gt; probably be saying five years from now, thinking exponentially instead of
</em><br>
<em>&gt; asymptotically.  Anyway, I don't know whether you've read &quot;Staring into
</em><br>
<em>&gt; the Singularity&quot; (sysopmind.com) or &quot;What is Seed AI?&quot; (intelligence.org) but
</em><br>
<em>&gt; in either case, the moral of the story is that transhumanity very very
</em><br>
<em>&gt; rapidly enhances itself up to superintelligence.  If a transhuman AI is
</em><br>
<em>&gt; developed in the next ten years, then ultraintelligent AI is developed in
</em><br>
<em>&gt; the next ten years.
</em><br>
&nbsp;Big If. Always that IF crops in.  :)  The problem is software. That's
<br>
why we should learn as much as possible about the only working
<br>
&quot;intelligence&quot; we have access to. It's perfectly possible that we will
<br>
have to reverse-engineer sentience after a fashion. The mad hardware
<br>
may make the process of Understanding our own software doable..
<br>
and the Human Genome Project gets followed up (quickly) by the
<br>
Human Brain Project...which is then (using the hardware of the next
<br>
2-3 decades) followed even more quickly by the Human Mind Project.
<br>
At which point the models and the original templates and the revised
<br>
improved versions achieve Singularity. En masse.  Until someone
<br>
can demonstrate code of even near infrahuman AI, reverse-engineering
<br>
has my confidence. Feel free to Future Shock me Eli!
<br>
<em>&gt; Humans do not even remotely begin to catch up unless
</em><br>
<em>&gt; the ultraintelligent AI wants them to catch up, in which case Friendly AI
</em><br>
<em>&gt; has been achieved and a basically healthy Singularity has been initiated
</em><br>
<em>&gt; successfully.  If you live next door to a Friendly superintelligence there
</em><br>
<em>&gt; is no reason to mess around with half-measures like genetic engineering or
</em><br>
<em>&gt; neurosurgery; you are who you choose to be, whether that's a biological
</em><br>
<em>&gt; human or an uploaded humanborn SI.
</em><br>
Would you like to be a biological human or an uploaded humanborn SI?
<br>
Answer: Yes. I would also like to be a non-human born SI. All at the same
<br>
time. :)
<br>
&nbsp;&nbsp;Which is why I say neurosurgery (including microsurgical techniques and
<br>
advances in neuroradiology, neuropathology and basic neuroscience) is
<br>
a good place to be.  Certainly a surer (if such a word is even appropriate
<br>
to
<br>
this discussion) path to understanding of &quot;a&quot; set of working code.  Which
<br>
can of course be used to duplicate and improve.
<br>
&nbsp;&nbsp;Eli, what is the difference between an adept Expert System specializing in
<br>
investigation and correlation of the nature of human sentience (and then
<br>
using that understanding to recode itself) and your conception of
<br>
a seed AI? Beyond the Friendly issue. Wouldn't they be very similar in
<br>
eventual outcomes if both worked?
<br>
<p><p>regards,
<br>
Brian
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1691.html">gabriel C: "Re: SI Jail"</a>
<li><strong>Previous message:</strong> <a href="1689.html">gabriel C: "Re: SI Jail"</a>
<li><strong>In reply to:</strong> <a href="1687.html">Eliezer S. Yudkowsky: "Re: Putting super-intelligence in a body"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1692.html">Brian Phillips: "Re: Putting super-intelligence in a body"</a>
<li><strong>Reply:</strong> <a href="1692.html">Brian Phillips: "Re: Putting super-intelligence in a body"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1690">[ date ]</a>
<a href="index.html#1690">[ thread ]</a>
<a href="subject.html#1690">[ subject ]</a>
<a href="author.html#1690">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
