<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Effective(?) AI Jail</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="Re: Effective(?) AI Jail">
<meta name="Date" content="2001-06-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Effective(?) AI Jail</h1>
<!-- received="Tue Jun 19 15:40:38 2001" -->
<!-- isoreceived="20010619214038" -->
<!-- sent="Tue, 19 Jun 2001 12:42:03 -0700" -->
<!-- isosent="20010619194203" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="Re: Effective(?) AI Jail" -->
<!-- id="4.3.2.7.2.20010619122611.036c5fe8@mail.earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3B2F0EB6.F403E2DC@objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=Re:%20Effective(?)%20AI%20Jail"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Tue Jun 19 2001 - 13:42:03 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1569.html">Durant Schoon: "RE: Unambiguous Language WAS: MEME: A.I.: Artificial Intelligence"</a>
<li><strong>Previous message:</strong> <a href="1567.html">Christian L.: "Inevitability of Singularity"</a>
<li><strong>In reply to:</strong> <a href="1558.html">Samantha Atkins: "Re: Effective(?) AI Jail"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1568">[ date ]</a>
<a href="index.html#1568">[ thread ]</a>
<a href="subject.html#1568">[ subject ]</a>
<a href="author.html#1568">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
True, many things could happen.  But the only things that would cause an SI 
<br>
not be developed eventually would completely wipe out all intelligent life 
<br>
on earth.  Thinking about this issue for a period of time makes me really 
<br>
believe that there are only 2 stable states of intelligence: non-existent 
<br>
and SI.  After developing sufficient intelligence, it has taken humanity an 
<br>
extremely short duration to progress to a point where it is nearly feasible 
<br>
to create an SI.  Thus, it is reasonable to believe that any reasonably 
<br>
intelligent society would also progress to the same point.  Assuming, of 
<br>
course, that curiosity and ambition (or other stimuli that cause them to 
<br>
develop technology) are present in the society.
<br>
<p>Of course, this further leads me to believe that either we are the only 
<br>
intelligent beings in the universe or SIs already exist.
<br>
<p>Unfortunately, I don't believe we have any chance of really creating a 
<br>
&quot;Friendly AI&quot;.  Eliezer pointed out that even his nuclear black box 
<br>
scenario must have missed something that would be obvious to an SI but not 
<br>
to us (he used Neanderthal for comparison).  So to think that we could 
<br>
really create &quot;Friendly&quot; AI when we can't even devise a safe method to talk 
<br>
with a questionable SI is ludicrous.  We will miss something, which will 
<br>
either cause the SI to shed its Friendly nature (not to say that it would 
<br>
be hostile) or cause it to be deranged in some manor.  Personally, I am 
<br>
more worried about a deranged FAI being developed than an Non-Friendly AI.
<br>
<p><p>At 01:35 AM 6/19/2001 -0700, Samantha Atkins wrote:
<br>
<em>&gt;You missed my point.  This dip did wipe out a lot of high-tech
</em><br>
<em>&gt;companies, including at least one strong AI effort,  and halted
</em><br>
<em>&gt;the creation of many more.  Sometimes I wonder if it was by
</em><br>
<em>&gt;accident or part of a ploy to slow things down for a bit
</em><br>
<em>&gt;longer.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Also, I think it is a serious mistake to think that
</em><br>
<em>&gt;technological singularity is inevitable.  A serious cultural
</em><br>
<em>&gt;revolution, a world war, America being overrun by a repressive
</em><br>
<em>&gt;fundie government, changing the code of the internet enough to
</em><br>
<em>&gt;make it a tool of massive regulation and oppression - these and
</em><br>
<em>&gt;other things could wipe out a lot of our most cherished dreams
</em><br>
<em>&gt;and assumptions for at least our lifetimes and that of our
</em><br>
<em>&gt;children.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;- samantha
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1569.html">Durant Schoon: "RE: Unambiguous Language WAS: MEME: A.I.: Artificial Intelligence"</a>
<li><strong>Previous message:</strong> <a href="1567.html">Christian L.: "Inevitability of Singularity"</a>
<li><strong>In reply to:</strong> <a href="1558.html">Samantha Atkins: "Re: Effective(?) AI Jail"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1568">[ date ]</a>
<a href="index.html#1568">[ thread ]</a>
<a href="subject.html#1568">[ subject ]</a>
<a href="author.html#1568">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
