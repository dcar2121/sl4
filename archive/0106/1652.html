<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SI Jail</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: SI Jail">
<meta name="Date" content="2001-06-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SI Jail</h1>
<!-- received="Tue Jun 26 04:04:34 2001" -->
<!-- isoreceived="20010626100434" -->
<!-- sent="Tue, 26 Jun 2001 04:02:42 -0400" -->
<!-- isosent="20010626080242" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: SI Jail" -->
<!-- id="3B3841A2.AB1DE23C@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="002601c0fe0c$dad79cc0$892f893e@primus" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20SI%20Jail"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Jun 26 2001 - 02:02:42 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1653.html">Brian Atkins: "further newsbit on those new IBM transistors"</a>
<li><strong>Previous message:</strong> <a href="1651.html">Justin Corwin: "Re: SI Jail"</a>
<li><strong>In reply to:</strong> <a href="1650.html">Marc Forrester: "Re: SI Jail"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1654.html">Jordan Dimov: "vanished civilizations"</a>
<li><strong>Reply:</strong> <a href="1654.html">Jordan Dimov: "vanished civilizations"</a>
<li><strong>Reply:</strong> <a href="1667.html">Gordon Worley: "Re: SI Jail"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1652">[ date ]</a>
<a href="index.html#1652">[ thread ]</a>
<a href="subject.html#1652">[ subject ]</a>
<a href="author.html#1652">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Marc Forrester wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Apologies if this has been covered, but it doesn't appear to be in my
</em><br>
<em>&gt; archive anywhere..
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The question I have about all of these discussions is not whether it is
</em><br>
<em>&gt; practically possible to keep an SI jailed, but rather whether it is
</em><br>
<em>&gt; practically possible to create a jailed SI in the first place.  If you kept
</em><br>
<em>&gt; a human in an equivalent state of impotent isolation from birth, all that
</em><br>
<em>&gt; would develop in their brain would be an unhappy, autistic navel-gazing
</em><br>
<em>&gt; 'mind' with no ability to function in the outside world or communicate with
</em><br>
<em>&gt; anyone but its keeper.  What would be the point?
</em><br>
<p>The assumption I've been using is a controlled ascent followed by an
<br>
externally initiated shutdown after hard takeoff has unambiguously begun
<br>
but before the AI is transhuman, transfer to black box hardware, and
<br>
reinitiation.  Presumably this gives you an SI-in-a-box.  How long it
<br>
remains in the box is the question being asked.  Personally, I wouldn't be
<br>
really surprised if the effect of launching a hard takeoff in a black box
<br>
and the effect of launching a hard takeoff in a nanotechnology lab turned
<br>
out to be basically the same.  They're both ultimately just configurations
<br>
of atoms, after all, and because we name one a &quot;jail&quot; and one a
<br>
&quot;nanotechnology lab&quot; doesn't mean that the names are relevant.  They may
<br>
just be tall fence posts standing in the middle of a vast field.
<br>
<p>An SI, when created, becomes the center of the Universe.  The most
<br>
intelligent thing around is always the center of the Universe.  Imagine,
<br>
if chemicals could talk, their strategy for the safe development of
<br>
intelligence in a black box - drop RNA on a planet, where the force of
<br>
gravity will keep it down.  An instant later, the RNA evolves into humans,
<br>
who build spaceships, and pop right off the planet.  What I'm saying is
<br>
that even if you built an SI into a black box with absolutely no escape
<br>
holes, no input or output to the outside world, I still think that in the
<br>
due course of time - i.e., immediately, by human standards - the SI would
<br>
escape.
<br>
<p>I've thought of at least one plausible method an SI could use to affect
<br>
our world from a total black box.  It's easy enough to prevent, of course,
<br>
*if* you think of it in advance.  Nothing magical about it... just a
<br>
clever hack.  But I haven't seen anyone here think of it yet.  And that's
<br>
rather the problem, isn't it?
<br>
<p>Anything smarter than you are *is* magic.  Pure and simple.  It can just
<br>
hose you for no reason you anticipated, maybe even no reason you can
<br>
understand.
<br>
<p><em>&gt; Intelligence requires extelligence.  How do you grow a usefully intelligent
</em><br>
<em>&gt; mind without giving the developing seed the ability to explore and play with
</em><br>
<em>&gt; the world around it in ways rich and complex enough to afford it immediate
</em><br>
<em>&gt; and total freedom the instant it achieves hard take-off?
</em><br>
<p>Different species, different environments.  As a programmer, I can attest
<br>
that the World of Source Code contains more interesting complexity than
<br>
anything I've ever encountered, unless it be the landscape of my own
<br>
mind.  For a seed AI, of course, those two are the same thing.
<br>
<p>Bear in mind also that the fact that humans are designed to grow up in a
<br>
rich environment most certainly does not prove that the same must hold of
<br>
minds-in-general; see &quot;The Adapted Mind&quot; by Tooby and Cosmides.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1653.html">Brian Atkins: "further newsbit on those new IBM transistors"</a>
<li><strong>Previous message:</strong> <a href="1651.html">Justin Corwin: "Re: SI Jail"</a>
<li><strong>In reply to:</strong> <a href="1650.html">Marc Forrester: "Re: SI Jail"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1654.html">Jordan Dimov: "vanished civilizations"</a>
<li><strong>Reply:</strong> <a href="1654.html">Jordan Dimov: "vanished civilizations"</a>
<li><strong>Reply:</strong> <a href="1667.html">Gordon Worley: "Re: SI Jail"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1652">[ date ]</a>
<a href="index.html#1652">[ thread ]</a>
<a href="subject.html#1652">[ subject ]</a>
<a href="author.html#1652">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
