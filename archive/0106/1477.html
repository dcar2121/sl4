<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Coercive Transhuman Memes and Exponential Cults</title>
<meta name="Author" content="Durant Schoon (durant@ilm.com)">
<meta name="Subject" content="Re: Coercive Transhuman Memes and Exponential Cults">
<meta name="Date" content="2001-06-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Coercive Transhuman Memes and Exponential Cults</h1>
<!-- received="Tue Jun 12 20:34:14 2001" -->
<!-- isoreceived="20010613023414" -->
<!-- sent="Tue, 12 Jun 2001 14:01:11 -0700 (PDT)" -->
<!-- isosent="20010612210111" -->
<!-- name="Durant Schoon" -->
<!-- email="durant@ilm.com" -->
<!-- subject="Re: Coercive Transhuman Memes and Exponential Cults" -->
<!-- id="durant-1010612140110.A04159100@sleeper" -->
<!-- inreplyto="3B25539A.72100EA4@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Durant Schoon (<a href="mailto:durant@ilm.com?Subject=Re:%20Coercive%20Transhuman%20Memes%20and%20Exponential%20Cults"><em>durant@ilm.com</em></a>)<br>
<strong>Date:</strong> Tue Jun 12 2001 - 15:01:11 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1478.html">Durant Schoon: "Re: law of requisite variety (RE: Coercive Transhuman Memes...)"</a>
<li><strong>Previous message:</strong> <a href="1476.html">John Stick: "Re: Coercive Transhuman Memes and Exponential Cults"</a>
<li><strong>In reply to:</strong> <a href="1459.html">Eliezer S. Yudkowsky: "Re: Coercive Transhuman Memes and Exponential Cults"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1462.html">Ben Houston: "RE: Coercive Transhuman Memes and Exponential Cults"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1477">[ date ]</a>
<a href="index.html#1477">[ thread ]</a>
<a href="subject.html#1477">[ subject ]</a>
<a href="author.html#1477">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; From: &quot;Eliezer S. Yudkowsky&quot; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Coercive%20Transhuman%20Memes%20and%20Exponential%20Cults">sentience@pobox.com</a>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Durant Schoon wrote:
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; &gt; Charlie has to be smarter than the *Sysop* to *covertly* take
</em><br>
<em>&gt; &gt; &gt; over Bill, at least if Bill has a normal relationship with the Sysop.
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; Not smarter. Just smart enough. Charlie merely needs to be smart enough to
</em><br>
<em>&gt; &gt; persuade Bill to incrementally change vis volition, without violating any
</em><br>
<em>&gt; &gt; rules of the Sysop.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But what are the Sysop's rules?  They are Bill's rules!  Rather than, as I
</em><br>
<em>&gt; think you may be visualizing, some a-priori definition of which messages
</em><br>
<em>&gt; are allowed to pass between two entities with a given level of
</em><br>
<em>&gt; intelligence.  
</em><br>
<p>Ok, they are Bill's rules.
<br>
<p>Let's suppose Bill could be happy choosing any number of directions
<br>
for in his life. Maybe a new question I'm raising is: Is it ethical
<br>
for other entities to influence Bill toward particular (happy)
<br>
targets? There may be targets which are equalling appealing to Bill
<br>
but which have various and varied benefits for others. Will there be
<br>
limits placed on more powerful entities competing for any side
<br>
benefits for Bill's decisions? 
<br>
<p>Concrete Example: Whether I buy Guess jeans or Levi's, my happiness as
<br>
a result of my actions might be pretty much the same. To the two
<br>
corporations Guess Corp. and Levi's Corp., however, my decision could
<br>
make all the difference in the world*. 
<br>
<p>Your agument may be that from Bill's point of view, this does not
<br>
matter. But I'm wondering if this becomes more complicated when the
<br>
possibility that Bill would be happy in an exponential cult is
<br>
non-zero and/or he's able to rewire how much he'd like being in one.
<br>
If Bill is naturally prone to cult membership but just hasn't found
<br>
the right cult yet, then I suppose the Sysop would help him find True
<br>
Happiness and that would be the right thing to do.
<br>
<p>Sysop: &quot;No really Brother Bill, you would be very happy here. I have
<br>
complete knowledge of your psychological subnetworks and, although you
<br>
may not realize it yet, this is a good fit for you. Your wife Barbara
<br>
will probably leave you, but your kids will still visit you and you'll
<br>
prefer the trade off after time. As always the choice is yours...&quot;
<br>
Ok, I admit this example is a little unfair &lt;evil smirk&gt;.
<br>
<p>If Bill is not naturally prone to cults, should there be protection
<br>
against aggressive/effective strains (the kind that make you want to
<br>
join)? Maybe this will just be part of the standard filter. It also
<br>
implies that exponential cults are bad things to be avoided.
<br>
<p>* Guess Corp. may not be allowed to hold my dog hostage until I buy
<br>
&nbsp;&nbsp;their jeans, but they are allowed to clutter my view with billboards
<br>
&nbsp;&nbsp;of pretty people.
<br>
<p><p><em>&gt; It is possible, probably even likely, that almost all of
</em><br>
<em>&gt; the message-filtering rules will converge to the same basic standard.  
</em><br>
<p>I'm assuming that people will explicitly set (or have an internal
<br>
setting for already) a personal tolerance for the amount of change
<br>
they are willing to undergo. Once the standardized message-filtering
<br>
is applied, further settings based on one's acceptable-mutation-rate
<br>
may come into play. 
<br>
<p>I do like the idea that generic, reasonable rules seem possible and
<br>
further that they could be tailored to individuals. What I think is
<br>
difficult is knowing *which* rules to choose before it is too
<br>
late. But that's a job for the Sysop, as you say. I have seen some
<br>
pretty disgusting images on the web that I wished I hadn't seen
<br>
(www.rotten.com **) and I can't un-see them now. I knew I probably
<br>
didn't want to look at them, but I went ahead and did it anyway. I
<br>
suppose life will be very different when the Sysop can whisper in my
<br>
ear: &quot;Durant, you'll like this but not that.&quot; I'll also, presumebably,
<br>
be able un-see things (ie. edit my memories...or better hide them
<br>
indeterminantly). 
<br>
<p>Perhaps this will all be covered in the welcome video: &quot;Hello, if
<br>
you're watching this video, you've made the first step and just
<br>
increased your intelligence three orders of magnitude from human
<br>
average. Before you complete the process and accept full write
<br>
permission to your volitional centers, we strongly suggest you apply
<br>
the standard filter to avoid automatically opening Viral Brain Script
<br>
files. These files have a tendency to induce a cultlike following and
<br>
a herd mentality when choosing certain consumer products.&quot;
<br>
<p>** WARNING: www.rotten.com has images which could be construed as
<br>
&nbsp;&nbsp;&nbsp;extremely grotesque (ok, it's just plain disturbing). There's also
<br>
&nbsp;&nbsp;&nbsp;www.goatse.cx which I found in someone's sig w/o a warning and
<br>
&nbsp;&nbsp;&nbsp;decided to view it - also extremely graphic.
<br>
<p><em>&gt; My
</em><br>
<em>&gt; candidate for this standard would be &quot;It is unethical to convince people
</em><br>
<em>&gt; of things, even true things, by a method so powerful that it could be used
</em><br>
<em>&gt; to convince them of false things.&quot;  
</em><br>
<p>Ah yes, good point, but I don't think I could argue with it ;)
<br>
<p><em>&gt; But the point is that the ultimate
</em><br>
<em>&gt; decision is up to Bill.
</em><br>
<p>When I was a young boy, I didn't like tomatoes or or onions. At some
<br>
point tried them individually and changed my mind. Now I really love
<br>
them. It seems like it boils down to how much we want to change in the
<br>
future. I don't, for example, think I'll ever enjoy a manure sandwich,
<br>
no matter how good it tastes...not even smothered in sauteed onions
<br>
and topped with vine ripened, garden sweet tomatoes. In fact I
<br>
probably wouldn't want to modify myself to like them, even if there
<br>
were no health concerns. But can I really say that?...maybe I'd be
<br>
missing something...
<br>
<p><em>&gt; So Bill can simply say, &quot;Disallow all messages that are intended to
</em><br>
<em>&gt; convert me to a cult, or that might as well be so intended.&quot;  And that'd
</em><br>
<em>&gt; be that.  If that *still* doesn't work then Bill can adopt a &quot;prohibited
</em><br>
<em>&gt; unless allowed&quot; rule, and totally block off all communication from smarter
</em><br>
<em>&gt; entities except Friendly AIs, known total altruists, and messages where
</em><br>
<em>&gt; the Sysop appends a note saying &quot;Bill, I'm damn sure you need and want to
</em><br>
<em>&gt; read this.&quot;  
</em><br>
<p>That gives me an idea! I'll start The Cult of Known Altruists...oh
<br>
wait, you beat me to it ;-)
<br>
<p><em>&gt; And if that doesn't work, I guess that Bill basically has the
</em><br>
<em>&gt; option of either entirely silencing the Spaces Beyond or doing a
</em><br>
<em>&gt; fast-as-possible transcendence personally.  
</em><br>
<p>Resistance is not futile ... but only for ultra-paranoid hermits :)
<br>
<p><em>&gt; There might be a threshold
</em><br>
<em>&gt; level of superintelligence beyond which not even a Power can fool you.
</em><br>
<p>That would be interesting, but I'm not sure why that would be. With
<br>
limits on other entities, maybe you could get close to that.
<br>
<p><em>&gt; &gt; Charlie might also do this completely openly. In fact,
</em><br>
<em>&gt; &gt; if Charlie does not do this, then transhuman Cindy probably will, ie.
</em><br>
<em>&gt; &gt; someone would do it after enough time passes. And you know transhuman Cindy
</em><br>
<em>&gt; &gt; has a way with words. She makes everything so clear and understandable.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Sure.  If you expose a human to a superintelligence, even through a VT100
</em><br>
<em>&gt; terminal, then the human's sole safeguard from total mental takeover is an
</em><br>
<em>&gt; ethical superintelligence.  I'm pretty confident of this.  With what I
</em><br>
<em>&gt; know of intelligence so far, it looks to me like a being that had a list
</em><br>
<em>&gt; of all the emotional and intuitive sequiturs, and that could keep track of
</em><br>
<em>&gt; a hundred different chunks in short-term memory, could chat with a human
</em><br>
<em>&gt; for a bit and then navigate her like a chess search tree.  We simply are
</em><br>
<em>&gt; not that complicated except by our own wimpy standards.
</em><br>
<p>yup.
<br>
<p><em>&gt; Hence the folly of &quot;containment&quot;.
</em><br>
<p>yup.
<br>
<p><em>&gt; In fact, if I were the &quot;created AI&quot; and I could chat only through a VT100
</em><br>
<em>&gt; terminal, I could probably also convince you to let me out, using only
</em><br>
<em>&gt; truthful arguments, while obeying my own ethical constraints, as long as
</em><br>
<em>&gt; the person on the other end was fairly rational.  An irrational jailkeeper
</em><br>
<em>&gt; would probably require a transhuman jailbreak, though.
</em><br>
<p>Hmm, it might depend on how &quot;irrational&quot;, but ok.
<br>
<p><em>&gt; &gt; Smart people can be convinced to make incorrect conclusions if there is
</em><br>
<em>&gt; &gt; enough spin and doubt created or if an idea is &quot;irresistably appealing&quot;.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &quot;Smart&quot; being relative, of course this is true.  
</em><br>
<p>Yes, intelligence is relative and context dependent. I should have
<br>
been less categorical. There are no stupid people, only stupid
<br>
questions :)
<br>
<p><em>&gt; But in this case the
</em><br>
<em>&gt; first thing that smart people do is ask the Sysop to filter their
</em><br>
<em>&gt; messages, or better yet, blaze up to superintelligence themselves.
</em><br>
<p>With filtering we have the equally-appealing-to-Bill problem above.
<br>
<p>With blazing up, we get an arms race...maybe not a bad thing, maybe
<br>
unavoidable.
<br>
<p><em>&gt; &gt; For some category of non-dangerous manipulation, the sysop won't intervene.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The Sysop intervenes when you ask the Sysop to intervene; when you define
</em><br>
<em>&gt; intervention as desirable.  If you define intervention as desirable for a
</em><br>
<em>&gt; transhuman-originated message intended to cause you to arrive at an
</em><br>
<em>&gt; incorrect conclusion - I sure would - then the Sysop will intervene.
</em><br>
<p>Ah, but it doesn't have to lead to incorrect reasoning, ie. outright
<br>
deception (the equally-appealing thing again). Maybe I can help myself
<br>
out of this conundrum, though, simply by asking the Sysop for all
<br>
ulterior motives for messages directed to me. If that works, maybe I'd
<br>
feel more in control. Maybe I could avoid things my current self wants
<br>
to avoid for my future self.
<br>
<p><em>&gt; &gt; fnord
</em><br>
<em>&gt; 
</em><br>
<em>&gt; all your base are belong to us
</em><br>
<p>:)
<br>
<p><em>&gt; &gt; I suppose this leads to another question about property rights and volition.
</em><br>
<em>&gt; &gt; If there is a land grab and I get out to the other matter in the universe first,
</em><br>
<em>&gt; &gt; claim it and convert it to computronium populated with sentients who follow my
</em><br>
<em>&gt; &gt; cult am I violating anyone's volition in an unfair manner?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; My guess is that if matter is a limited resource, then the Sysop expands
</em><br>
<em>&gt; outward at lightspeed, and incoming matter is distributed according to
</em><br>
<em>&gt; some algorithm that I'm not really sure I can guess, except that the most
</em><br>
<em>&gt; obvious one is to distribute it evenly among all current citizens.
</em><br>
<p>I suppose that would work. Now I'm tempted to speculate what a person
<br>
should do, once an SI is created (new thread). Eli's answer would
<br>
probably be: Ask the SI. Now I wonder what the first thing an SI would
<br>
do...Collect lots of money and spew PR secretively to allow a graceful
<br>
announcement? Hmm, maybe there's a good story there somewhere about
<br>
The Grand Debutante Ball...
<br>
<p>Limited resources also makes me wonder about the old &quot;Sentients as
<br>
Temporary Variables&quot; thread, ie. infinitely forking new sentients, but
<br>
that doesn't seem like an intractable problem, really. Maybe just a
<br>
difficult one to get right.
<br>
<p><em>&gt; I think that truthful ideas, and to a lesser extent ideas that are not
</em><br>
<em>&gt; totally objective but that are valid for almost all humans, will spread
</em><br>
<em>&gt; exponentially from human to human; or, even more likely, emerge instantly
</em><br>
<em>&gt; as a result of people asking the Sysop.  Why is this a bad thing?
</em><br>
<p>It is perhaps hopeful that we humans are able to place reason over
<br>
non-reason (ok it could be said we are joining the Cult of Reason). I
<br>
do believe that some mechinisms should arise to control the degree to
<br>
which we are manipulated in the future, though. Hopefully our natural
<br>
aversion to outside manipulation will factor largely in our
<br>
ascendance.  I'm still curious about how much entities will be allowed
<br>
to &quot;direct me&quot; indirectly, where the outcomes don't matter that much
<br>
to me.
<br>
<p><pre>
--
Durant Schoon
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1478.html">Durant Schoon: "Re: law of requisite variety (RE: Coercive Transhuman Memes...)"</a>
<li><strong>Previous message:</strong> <a href="1476.html">John Stick: "Re: Coercive Transhuman Memes and Exponential Cults"</a>
<li><strong>In reply to:</strong> <a href="1459.html">Eliezer S. Yudkowsky: "Re: Coercive Transhuman Memes and Exponential Cults"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1462.html">Ben Houston: "RE: Coercive Transhuman Memes and Exponential Cults"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1477">[ date ]</a>
<a href="index.html#1477">[ thread ]</a>
<a href="subject.html#1477">[ subject ]</a>
<a href="author.html#1477">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
