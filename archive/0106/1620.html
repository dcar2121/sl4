<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SI as puppet master</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="Re: SI as puppet master">
<meta name="Date" content="2001-06-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SI as puppet master</h1>
<!-- received="Fri Jun 22 22:28:49 2001" -->
<!-- isoreceived="20010623042849" -->
<!-- sent="Fri, 22 Jun 2001 18:51:30 -0700" -->
<!-- isosent="20010623015130" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="Re: SI as puppet master" -->
<!-- id="4.3.2.7.2.20010622182628.022243f8@mail.earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3B334511.7249E8EF@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=Re:%20SI%20as%20puppet%20master"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Fri Jun 22 2001 - 19:51:30 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1621.html">James Higgins: "Re: SI as puppet master"</a>
<li><strong>Previous message:</strong> <a href="1619.html">Ben Houston: "sleep and communal dreaming (was RE: Unambiguous Language...)"</a>
<li><strong>In reply to:</strong> <a href="1611.html">Eliezer S. Yudkowsky: "Re: SI as puppet master"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1609.html">James Higgins: "Re: HUMOR: Singularity adventure"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1620">[ date ]</a>
<a href="index.html#1620">[ thread ]</a>
<a href="subject.html#1620">[ subject ]</a>
<a href="author.html#1620">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
First, thanks for the compliment.
<br>
<p>Second, a little clarification on my intent.  I realize that the quality of 
<br>
thought is very important, but we are not in a position to understand what 
<br>
better thought is like.  However, we can comprehend what faster thought is 
<br>
like so I used that for my example.  Further, any statement that an SI 
<br>
could take over a human via VT100 using some kind of &quot;magic&quot; is hard to 
<br>
swallow.  It may be possible, but it is unlikely that we will really know 
<br>
until it is too late.  My goal was to demonstrate a straight forward method 
<br>
by which an SI could feasibly control a human that was comprehensible and 
<br>
believable.  Hopefully I accomplished what I set out to do.
<br>
<p>Lastly, on the issue of speed.  I believe that slowing down an SI 
<br>
sufficiently would have a serious effect on its ability to persuade us.  If 
<br>
it is only able to converse at the rate of approximately 1 sentence per 
<br>
half hour, I would imagine that all timing attacks are out of the 
<br>
question.  Further, better thought may not be able to overcome faster 
<br>
thought in many situations.  Lets look at an example.  You have a very well 
<br>
trained dog named Rover who can fetch the newspaper and is very compliant 
<br>
when given an instruction.  Then, lets say that you are suddenly slowed 
<br>
down to approximately 1/1000ths your current speed.  You are still very 
<br>
much smarter than Rover, but it becomes incredibly hard to get rover to do 
<br>
anything by command.  Even if your output came out in realtime (buffered), 
<br>
you would actually start issuing the &quot;Fetch the paper&quot; command 30 minutes 
<br>
before it was delivered.  By that time Rover could be in a different room 
<br>
or chewing his favorite bone and thus not paying attention to you.  It 
<br>
would be more luck than anything if you did manage to get Rover to fetch 
<br>
the bone.
<br>
<p>Also, I have a question.  Does anyone plan to give an unknown SI detailed 
<br>
information about human anatomy, brain function and/or psychology?  Somehow 
<br>
I doubt that an SI will have access to this type of information and I find 
<br>
it incredibly hard to believe that it could pull off any sort of timing or 
<br>
low-level attack without this data.
<br>
<p><p>At 09:16 AM 6/22/2001 -0400, Eliezer wrote:
<br>
<em>&gt;James Higgins wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I think that sums up a reasonable argument on how an SI could accomplish
</em><br>
<em>&gt; &gt; this.  So, how do we prevent this from happening?  The answer is quite
</em><br>
<em>&gt; &gt; simple actually: We slow it down.  Any SI that is not fully trusted should
</em><br>
<em>&gt; &gt; be resource starved.  So that once it actually hits SI the performance is
</em><br>
<em>&gt; &gt; such that it would take 20+ minutes (our time) for it to respond to a
</em><br>
<em>&gt; &gt; single question.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Wow.  The SL4 mailing list is working.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I should start by saying that I don't actually agree that this suggestion
</em><br>
<em>&gt;will work; it's not the *speed* of thought but the *quality* of thought, I
</em><br>
<em>&gt;think, that lets an unfriendly SI take over a human.  Remember that Deep
</em><br>
<em>&gt;Blue had to examine literally billions of chess positions using an
</em><br>
<em>&gt;unintelligent algorithm to achieve rough parity with a human examining a
</em><br>
<em>&gt;couple of moves per second.  I like to say that Deep Blue - unlike any
</em><br>
<em>&gt;human being - actually played *chess*; that is, actually navigated a
</em><br>
<em>&gt;search tree composed of actual, raw chess positions.  Kasparov, and all
</em><br>
<em>&gt;other human beings including myself, navigate a much smaller search tree
</em><br>
<em>&gt;composed of the *regularities* in the actual Game of Chess.  And we can
</em><br>
<em>&gt;play the game of Regularities in Chess for the same reason that Deep Blue
</em><br>
<em>&gt;can play Actual Chess; we are significantly more complex and intricate
</em><br>
<em>&gt;than the structure of regularities in chess, just as Deep Blue is
</em><br>
<em>&gt;significantly more complex than Actual Chess (although nowhere near
</em><br>
<em>&gt;complex enough to understand the Regularities of Chess.)
</em><br>
<em>&gt;
</em><br>
<em>&gt;A human cannot play the Game of Human Regularities because humans are
</em><br>
<em>&gt;roughly on a par with each other, complexity-wise.  Rather, we must
</em><br>
<em>&gt;persuade; we must operate through the use of personal analogy and
</em><br>
<em>&gt;empathy.  A transhuman can play the Game of Human Regularities and take
</em><br>
<em>&gt;over a human through a VT100 terminal, albeit this would require mental
</em><br>
<em>&gt;effort and creativity such as we expend to play chess.  A
</em><br>
<em>&gt;superintelligence has the theoretical computing capacity to play the Game
</em><br>
<em>&gt;of Actual Humans - i.e., model us on a level low enough to be syntax
</em><br>
<em>&gt;rather than semantics - and thereby, in some sense, obviate the need to
</em><br>
<em>&gt;understand some of the regularities, allowing a superintelligence to
</em><br>
<em>&gt;emulate a human using raw computing power rather than raw smartness.  It
</em><br>
<em>&gt;is difficult to see why a superintelligence would want to do this.  The
</em><br>
<em>&gt;argument serves mainly as a gedankenexperiment in the VT100 Takeover
</em><br>
<em>&gt;debate.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Alternatively, it may be that modeling of the Game of Actual Humans, at
</em><br>
<em>&gt;least for some circumstances, could enable a superintelligence to pull off
</em><br>
<em>&gt;stunts such as Deep Blue going pawn-hunting while Kasparov was after its
</em><br>
<em>&gt;king, a feat later compared to a general walking through a battlefield and
</em><br>
<em>&gt;keeping track of each bullet in flight.  The analogy for a VT100 Takeover
</em><br>
<em>&gt;might be predicting and exploiting low-level anomalies in the target brain
</em><br>
<em>&gt;to assist high-level effects.  For example, timing a statement with some
</em><br>
<em>&gt;split-second regularity so as to take advantage of precise coordination
</em><br>
<em>&gt;with a previously triggered memory retrieval or insight so that the human
</em><br>
<em>&gt;fails to notice, or has a particular response to, the statement made.  I'm
</em><br>
<em>&gt;not sure you can slow down an SI by starving it of processing power, but
</em><br>
<em>&gt;if you did, I think it'd lose some VT100 Takeover capacities, but not all
</em><br>
<em>&gt;of them, and not the *primary* one, which is that the SI is fundamentally
</em><br>
<em>&gt;smarter than we are and is therefore - literally - more persuasive than
</em><br>
<em>&gt;any human who ever lived, even if playing the game on our level; and, even
</em><br>
<em>&gt;more importantly, is smarter than us and may therefore be playing a
</em><br>
<em>&gt;different game than we think we are playing.
</em><br>
<em>&gt;
</em><br>
<em>&gt;That said, James Higgins's suggestion of &quot;slow the SI down&quot; does not, I
</em><br>
<em>&gt;think, actually work on an SI - but it is nonetheless brilliant, creative,
</em><br>
<em>&gt;and completely original.  So is Jimmy Wales's suggestion that a locked-up
</em><br>
<em>&gt;SI should only be allowed to communicate with humans through binary code
</em><br>
<em>&gt;(with my appended note that the yes-or-no signal needs deterministic
</em><br>
<em>&gt;timing).  We are exploring genuinely new territory here that, AFAIK,
</em><br>
<em>&gt;neither science fiction nor the Extropian mailing list have touched.
</em><br>
<em>&gt;
</em><br>
<em>&gt;--              --              --              --              --
</em><br>
<em>&gt;Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt;Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1621.html">James Higgins: "Re: SI as puppet master"</a>
<li><strong>Previous message:</strong> <a href="1619.html">Ben Houston: "sleep and communal dreaming (was RE: Unambiguous Language...)"</a>
<li><strong>In reply to:</strong> <a href="1611.html">Eliezer S. Yudkowsky: "Re: SI as puppet master"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1609.html">James Higgins: "Re: HUMOR: Singularity adventure"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1620">[ date ]</a>
<a href="index.html#1620">[ thread ]</a>
<a href="subject.html#1620">[ subject ]</a>
<a href="author.html#1620">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
