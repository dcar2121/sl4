<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SI Jail</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="Re: SI Jail">
<meta name="Date" content="2001-06-30">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SI Jail</h1>
<!-- received="Sat Jun 30 15:01:57 2001" -->
<!-- isoreceived="20010630210157" -->
<!-- sent="Sat, 30 Jun 2001 11:56:10 -0700" -->
<!-- isosent="20010630185610" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="Re: SI Jail" -->
<!-- id="4.3.2.7.2.20010630114019.021f3f98@mail.earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3B3962AF.C84538B8@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=Re:%20SI%20Jail"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Sat Jun 30 2001 - 12:56:10 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="1683.html">James Higgins: "Re: Designing human participation in the AI ascent"</a>
<li><strong>Previous message:</strong> <a href="1681.html">Jack Richardson: "Re: Designing human participation in the AI ascent"</a>
<li><strong>In reply to:</strong> <a href="1671.html">Eliezer S. Yudkowsky: "Re: SI Jail"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1684.html">Eliezer S. Yudkowsky: "Re: SI Jail"</a>
<li><strong>Reply:</strong> <a href="1684.html">Eliezer S. Yudkowsky: "Re: SI Jail"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1682">[ date ]</a>
<a href="index.html#1682">[ thread ]</a>
<a href="subject.html#1682">[ subject ]</a>
<a href="author.html#1682">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I don't think an SI running in a highly controlled &amp; secure environment 
<br>
could do quite as much as some people think.  The SI can only do what the 
<br>
hardware it is running on will allow.  However, that may be much, much more 
<br>
than we expect.
<br>
<p>Electronics give off EM.  It would be extremely difficult, but an SI might 
<br>
be able to time its execution to produce a deliberate signal using this 
<br>
technique.  Also, at the very least there will always be at least one 
<br>
external connection to the computer, the power cable.  While I suspect it 
<br>
is highly unlikely that an SI could manage to perform I/O using the 
<br>
built-in power supply it may in fact be possible.  We must assume that an 
<br>
SI will be able to control all aspects of the hardware to their 
<br>
utmost.  For this same reason any hardware an SI is to run on should be 
<br>
kept to a minimum, sound cards &amp; fancy video cards should probably be 
<br>
avoided along with anything else non-essential.  Further, the room the SI 
<br>
is to be run it should be highly secure (locked &amp; hopefully guarded), be 
<br>
completely EM shielded and the AC power feed should be run both though a 
<br>
conditioner and an on-line UPS just prior to connecting to the PC.
<br>
<p>There are only 2 ways that I imagine that SI could ever perform &quot;magic&quot; 
<br>
given these limited resources.  First, if it is possible to create 
<br>
something that we do not understand by putting a bunch of electrons in 
<br>
specific patterns and the SI can figure out how to get said electrons into 
<br>
that configuration within the hardware someplace.  This &quot;seems&quot; unlikely 
<br>
especially given that the SI is highly unlikely to know details (circuit 
<br>
designs, etc) about its own hardware unless its operators are stupid.  The 
<br>
other method would be true &quot;magic&quot; to us.  In Great Bear's &quot;Blood Music&quot; 
<br>
the laws of science are actually created by conscious thought.  So if a 
<br>
huge number of minds agreed that something was true (like FTL travel) then 
<br>
it would be so.  The amount of actual intelligence involved is more 
<br>
important in his universe than the actual number of individual 
<br>
minds.  Thus, if anything like this were true we would be SOL no matter 
<br>
what we do, period.
<br>
<p>Thus I think the prospects of keeping an SI in a highly secure, isolated 
<br>
environment is very good.  At least until you deliberately hook up I/O 
<br>
feeds to the SI.  At some point someone is going to have to interact with 
<br>
it, at which point all bets are off.  And, of course, if the SI is ever 
<br>
given absolutely any access to the Internet or other networks then you 
<br>
loose all control as well.
<br>
<p><p><p><p>At 12:35 AM 6/27/2001 -0400, you wrote:
<br>
<em>&gt;gabriel C wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt;&gt;I've thought of at least one plausible method an SI could use to affect
</em><br>
<em>&gt; &gt; &gt;&gt;our world from a total black box.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; If it can escape from the box through methods either clever or &quot;magical&quot;,
</em><br>
<em>&gt; &gt; how can we call it a &quot;total black box&quot;? BTW, what is the substrate in the
</em><br>
<em>&gt; &gt; box?
</em><br>
<em>&gt;
</em><br>
<em>&gt;Well, let's say a million 200GHz (clock-speed) FPGA chips so that there's
</em><br>
<em>&gt;some realistic resemblance to &quot;superintelligence&quot;, although what I have in
</em><br>
<em>&gt;mind might also work on a regular PC if you could fit an SI onto one of
</em><br>
<em>&gt;those.  In this case, what I'm doing is thinking in terms of a relatively
</em><br>
<em>&gt;simple test; I have thought of a way that a SI allegedly having no inputs
</em><br>
<em>&gt;or outputs whatsoever can use to communicate with the outside world.  It
</em><br>
<em>&gt;is not unbeatable magic.  It is easy to prevent if you think of it in
</em><br>
<em>&gt;advance.  But, as long as nobody here thinks of it, then they cannot be
</em><br>
<em>&gt;sure of imprisoning Eliezer, much less an SI.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Trying to jail Eliezer, or any other smart human, is dangerous - but you
</em><br>
<em>&gt;have a chance of succeeding.  Unless you're overconfident.  Then you're
</em><br>
<em>&gt;screwed, even if you're just up against a smart human.  I would be very,
</em><br>
<em>&gt;very seriously on my guard if I wanted to put Carl Feynman into a black
</em><br>
<em>&gt;box, and never mind an SI.  The basic point I'm trying to make is that it
</em><br>
<em>&gt;never pays to assume you have a creative thinker outgunned just because
</em><br>
<em>&gt;you have what looks like a material advantage.  Assuming you have an SI
</em><br>
<em>&gt;outgunned is the height of hubris.  I imagine a group of medieval warriors
</em><br>
<em>&gt;persuading themselves &quot;Hey, it's just one guy; no matter how good he is
</em><br>
<em>&gt;with a sword, he can't beat an army,&quot; followed by the sound of machine
</em><br>
<em>&gt;guns.
</em><br>
<em>&gt;
</em><br>
<em>&gt;If only you'd talk about an &lt;H or ~H AI, instead of an SI, this discussion
</em><br>
<em>&gt;would make more sense...
</em><br>
<em>&gt;
</em><br>
<em>&gt;--              --              --              --              --
</em><br>
<em>&gt;Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt;Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1683.html">James Higgins: "Re: Designing human participation in the AI ascent"</a>
<li><strong>Previous message:</strong> <a href="1681.html">Jack Richardson: "Re: Designing human participation in the AI ascent"</a>
<li><strong>In reply to:</strong> <a href="1671.html">Eliezer S. Yudkowsky: "Re: SI Jail"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1684.html">Eliezer S. Yudkowsky: "Re: SI Jail"</a>
<li><strong>Reply:</strong> <a href="1684.html">Eliezer S. Yudkowsky: "Re: SI Jail"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#1682">[ date ]</a>
<a href="index.html#1682">[ thread ]</a>
<a href="subject.html#1682">[ subject ]</a>
<a href="author.html#1682">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:36 MDT
</em></small></p>
</body>
</html>
