<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: [sl4] AI's behaving badly (subtitle: There's more to me that utility - why there's society, and possibility too)</title>
<meta name="Author" content="Stuart Armstrong (dragondreaming@googlemail.com)">
<meta name="Subject" content="[sl4] AI's behaving badly (subtitle: There's more to me that utility - why there's society, and possibility too)">
<meta name="Date" content="2008-12-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>[sl4] AI's behaving badly (subtitle: There's more to me that utility - why there's society, and possibility too)</h1>
<!-- received="Sun Dec  7 03:55:44 2008" -->
<!-- isoreceived="20081207105544" -->
<!-- sent="Sun, 7 Dec 2008 10:55:40 +0000" -->
<!-- isosent="20081207105540" -->
<!-- name="Stuart Armstrong" -->
<!-- email="dragondreaming@googlemail.com" -->
<!-- subject="[sl4] AI's behaving badly (subtitle: There's more to me that utility - why there's society, and possibility too)" -->
<!-- id="38f493f10812070255k3ac767f0p5f7a1a1471f195ea@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Stuart Armstrong (<a href="mailto:dragondreaming@googlemail.com?Subject=Re:%20[sl4]%20AI's%20behaving%20badly%20(subtitle:%20There's%20more%20to%20me%20that%20utility%20-%20why%20there's%20society,%20and%20possibility%20too)"><em>dragondreaming@googlemail.com</em></a>)<br>
<strong>Date:</strong> Sun Dec 07 2008 - 03:55:40 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19579.html">Bryan Bishop: "Re: [sl4] JOIN: Hello, etc."</a>
<li><strong>Previous message:</strong> <a href="19577.html">Stuart Armstrong: "Re: [sl4] JOIN: Hello, etc."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19593.html">Petter Wingren-Rasmussen: "Re: [sl4] AI's behaving badly (subtitle: There's more to me that utility - why there's society, and possibility too)"</a>
<li><strong>Reply:</strong> <a href="19593.html">Petter Wingren-Rasmussen: "Re: [sl4] AI's behaving badly (subtitle: There's more to me that utility - why there's society, and possibility too)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19578">[ date ]</a>
<a href="index.html#19578">[ thread ]</a>
<a href="subject.html#19578">[ subject ]</a>
<a href="author.html#19578">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Dear Tim,
<br>
<p>Sorry for the delay; now, a slightly more detailed critique of your approach.
<br>
The first problem is that you try and define your terms (compassion,
<br>
respect, short-term, long-term) in terms that an AI would understand
<br>
(utility function), but think about them in ways that are familiar to
<br>
you (see <a href="http://en.wikipedia.org/wiki/Intuition_pump">http://en.wikipedia.org/wiki/Intuition_pump</a>).
<br>
<p>Use David Hilbert's axiomatic approach here - rewrite your paper but
<br>
instead of compassion write &quot;glasses of beer&quot;; instead of respect,
<br>
write &quot;hate&quot;; short and long term should be replaced with &quot;thin
<br>
buildings&quot; and &quot;fat buildings&quot; respectively. If your terms are
<br>
properly defined (and they need to be, to construct an AI), then your
<br>
paper should still be just as convincing as before. If not, then you
<br>
are allowing your intuitions to override your understanding.
<br>
<p>But the main problem is that you are then aware that problems may
<br>
emerge from your set up, and then go around &quot;patching&quot; them. For
<br>
instance, fixing the way the AI updates its vision of your utility,
<br>
etc... The problem is that you are trying to imagine, in advance, all
<br>
that could go wrong - and that is something we are spectacularly poor
<br>
at. It is basically a work of literature, or imagination - what do I
<br>
feel could go wrong here, is there something obvious, is there
<br>
something I've read that might be relevant, what are the dystopias
<br>
I've seen recently.
<br>
<p>But the world can fail in more ways than we can possibly imagine. To
<br>
make your approach work, you have to do the reverse: define clearly
<br>
what the correct outcome is (it can be a class of outcomes, and each
<br>
one need not be defined, but the class has to be), prove that this
<br>
outcome (or outcome class) will always be a &quot;good&quot; outcome, and then
<br>
demonstrate that your approach will lead into this class. Not patching
<br>
what could go wrong, but showing that the situation will inevitably be
<br>
right. And to do that, it needs mathematical models, with intelligent
<br>
agent, utility function, compassion, respect, long term, short term,
<br>
and all those mathematically defined, and experiments or proofs to
<br>
demonstrate what class of behaviours the system will end up in.
<br>
<p>(Note: it might seem hypocritical that myself, as a firm advocate of
<br>
the messy patching process:
<br>
<a href="http://www.neweuropeancentury.org/GodAI.pdf">http://www.neweuropeancentury.org/GodAI.pdf</a> , would criticise your
<br>
method. But there are a pair of small but crucial differences; I do
<br>
not need that every error be caught in advance of starting up the AI,
<br>
and my class of good outcomes is defined: it is the class of future
<br>
events that, when described to us in full and exhaustive details,
<br>
would seem good to us now).
<br>
<p>In the meantime, here is another problem to patch (and explains the subtitle):
<br>
For most people, our short term utilities have much greater salliance
<br>
and meaning to us than our long term utilities. This does not actually
<br>
mean that we are short term self-centered bastards; it's just that we
<br>
live in a world and a society that are designed to ensure that most of
<br>
our short term desires fail. If I know I could never sleep with the
<br>
boss, it is safer to dream and fantasise and want that. If it could
<br>
really happen... then I reign in my desires, and start really looking
<br>
at the whole situation.
<br>
<p>Therefore it is safe for us to have these short term, unlikely
<br>
desires, and to spend so much time on them. Our long term desires, on
<br>
the other hand, tend to occupy just a small fragment of our daily
<br>
thoughts. Fortunately, a lot of our long term desires do not need much
<br>
more than this; a career choice, a few donations to charities or
<br>
political movements, the choice of friends to hang out with, an
<br>
occasional purchase, maybe the theme for a speech (if we are into
<br>
giving speeches). Even for the best of us, the long term desires
<br>
simply set up the framework for our short term desires (ie a
<br>
speech-writer for the singularity institute and a speech-writer for
<br>
the Nazi party will be following similar procedures most days;
<br>
worrying about phraseology, considering emotional impact, considering
<br>
the audience, wondering how best to get the point accross - there will
<br>
be some differences, mainly in rationality, but not enough to say,
<br>
just based on most everyday thoughts, that the first is part of a
<br>
movement that might save humanity and the other is evil beyond words).
<br>
The daily lives of soldiers in similar conditions is virtually
<br>
indistinguishable, whatever side they are on etc...
<br>
<p>So now the AI comes along, and looks at these utilities. Then, it will
<br>
overestimate short terms utility, because that is what we do. And it
<br>
will try and grant us our short term desires, unaware that it is
<br>
destroying the balance between long and short term utilities. So it
<br>
may turn us into spoiled brats; more importantly, if asked &quot;if this
<br>
human was put in a room with a button that said &quot;Press this button a
<br>
get a ice cream, but ten people will be killed in ten years time&quot; &quot;,
<br>
then the AI will make the wrong judgement. And things will start to go
<br>
horribly wrong from there on.
<br>
<p>Summary: our short term desires are held in check by the possibilities
<br>
in the world, so we overemphasise them. Our job can tell more about us
<br>
than our everyday utility function. An AI observing us from our
<br>
behaviour, would construct a utility function that overemphasises the
<br>
short term even more, and completely denigrates the importance of our
<br>
job (or our &quot;position in society&quot;). It would then make the wrong
<br>
decisions. And, with our short term desires granted, we may change
<br>
into beings we wouldn't want to become - because the AI will not
<br>
manage the transition skillfully, because that is not its role, nor
<br>
does it understand this transition in the way we do.
<br>
<p>Anyway, I'm not advocating you patch this problem as well (I'm sure
<br>
that I could come up with other holes, and the Eleizer could, and even
<br>
after we run out of ideas, be sure there are still holes we haven't
<br>
caught), be rethink the approach.
<br>
<p>Sorry for the over-long email,
<br>
<p>Stuart
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19579.html">Bryan Bishop: "Re: [sl4] JOIN: Hello, etc."</a>
<li><strong>Previous message:</strong> <a href="19577.html">Stuart Armstrong: "Re: [sl4] JOIN: Hello, etc."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19593.html">Petter Wingren-Rasmussen: "Re: [sl4] AI's behaving badly (subtitle: There's more to me that utility - why there's society, and possibility too)"</a>
<li><strong>Reply:</strong> <a href="19593.html">Petter Wingren-Rasmussen: "Re: [sl4] AI's behaving badly (subtitle: There's more to me that utility - why there's society, and possibility too)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19578">[ date ]</a>
<a href="index.html#19578">[ thread ]</a>
<a href="subject.html#19578">[ subject ]</a>
<a href="author.html#19578">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
