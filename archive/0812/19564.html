<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] AI's behaving badly</title>
<meta name="Author" content="Tim Freeman (tim@fungible.com)">
<meta name="Subject" content="Re: [sl4] AI's behaving badly">
<meta name="Date" content="2008-12-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] AI's behaving badly</h1>
<!-- received="Tue Dec  2 22:26:40 2008" -->
<!-- isoreceived="20081203052640" -->
<!-- sent="Tue,  2 Dec 2008 19:31:24 -0700" -->
<!-- isosent="20081203023124" -->
<!-- name="Tim Freeman" -->
<!-- email="tim@fungible.com" -->
<!-- subject="Re: [sl4] AI's behaving badly" -->
<!-- id="20081203052639.EEB73D26FD@fungible.com" -->
<!-- inreplyto="38f493f10812021257i6e06aa37qec05ed659551d368@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tim Freeman (<a href="mailto:tim@fungible.com?Subject=Re:%20[sl4]%20AI's%20behaving%20badly"><em>tim@fungible.com</em></a>)<br>
<strong>Date:</strong> Tue Dec 02 2008 - 19:31:24 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19565.html">Petter Wingren-Rasmussen: "[sl4] How to imprint rules (re:AI's behaving badly)"</a>
<li><strong>Previous message:</strong> <a href="19563.html">Peter de Blanc: "Re: [sl4] AI's behaving badly"</a>
<li><strong>In reply to:</strong> <a href="19562.html">Stuart Armstrong: "[sl4] AI's behaving badly"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19564">[ date ]</a>
<a href="index.html#19564">[ thread ]</a>
<a href="subject.html#19564">[ subject ]</a>
<a href="author.html#19564">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
From: &quot;Stuart Armstrong&quot; &lt;<a href="mailto:dragondreaming@googlemail.com?Subject=Re:%20[sl4]%20AI's%20behaving%20badly">dragondreaming@googlemail.com</a>&gt;
<br>
<em>&gt;Two situations: the first one is where the AI is excessively short
</em><br>
<em>&gt;term. Then the disaster is rather clear, as the AI runs through all
</em><br>
<em>&gt;available ressources (and neglects trying to accumulate extra
</em><br>
<em>&gt;ressources) trying to please humans, leading to an
</em><br>
<em>&gt;environmental/social/economic collapse (if the AI is short term, then
</em><br>
<em>&gt;it MUST privilege short term goals over any other considerations;
</em><br>
<em>&gt;using up irreplaceable ressources immediately, in a way that will fill
</em><br>
<em>&gt;the atmosphere with poisonous gas, is something the AI is compelled to
</em><br>
<em>&gt;do, if it results in a better experience for people today).
</em><br>
<p>It's trying to give the humans what they want.  If the humans are
<br>
long-term, then making a short term decision to give them what they
<br>
want stil has good long term consequences because in the short term
<br>
the humans want desirable long term consequences.  If a short-term AI
<br>
is catering to short-term humans, then I agree we might find ourselves
<br>
in a short-term universe, which would be an undesirable outcome for
<br>
the long-term minority.
<br>
<p>If humans in aggregate are short-term and you don't want the AI to be
<br>
short-term, then you want to implement an AI that does more of what
<br>
you want than what the short-term majority wants.  In this case there
<br>
will be conflict and it would only be luck that leaves reasonable
<br>
people in control of the thing.  This problem seems inherent in the
<br>
situation.
<br>
<p><em>&gt;The second situation is where the AI can think longer term.
</em><br>
<p>Ya, sorry to mislead you there.  The paper is stale.  Right now I
<br>
think the AI should only think about giving humans what they want in
<br>
the short term, and leave it to the humans to include long term
<br>
concerns in their wants or not.  If the AI does long term planning
<br>
then there are strange artifacts that arise when the end of the AI's
<br>
planning horizon approaches
<br>
(<a href="http://www.fungible.com/respect/paper.html#grocery-shopping">http://www.fungible.com/respect/paper.html#grocery-shopping</a>), and
<br>
other strange things happen if we continuously push the AI's planning
<br>
horizon into the future as time passes
<br>
(<a href="http://www.fungible.com/respect/paper.html#deferred-gratification">http://www.fungible.com/respect/paper.html#deferred-gratification</a>).
<br>
I hope that people naturally become more concerned about the long term
<br>
as their short term desires are satisfied.
<br>
<p><em>&gt;This is much more dangerous. What is the ideal world for an AI? A
</em><br>
<em>&gt;world where it can maximise human's utilities, without having to
</em><br>
<em>&gt;reduce anyone's.  As before, brains-in-an-armoured-jar, drug-regressed
</em><br>
<em>&gt;to six months, boredom removed and with repeated simple stimuli and a
</em><br>
<em>&gt;cocaine high, is the ideal world for this.
</em><br>
<p>This isn't specific to a long-term AI.  The AI could do this in the
<br>
short term.
<br>
<p>This is the deception scenario, described at
<br>
<a href="http://www.fungible.com/respect/paper.html#deception">http://www.fungible.com/respect/paper.html#deception</a>, or maybe it's
<br>
the aggressive neurosurgery test case immediately after that; they're
<br>
essentially the same.  The fix proposed there is to ensure that it
<br>
makes sense to apply the humans' utility function to the AI's
<br>
world-model, and to maximize the humans' utility applied to the AI's
<br>
world model.  This way, the AI perceives no utility gain from
<br>
deceiving the humans, since such deception only changes the human's
<br>
world model.
<br>
<p><em>&gt;PS: fixing the AI to obeying people's current utilities won't help
</em><br>
<em>&gt;much - it will result in the AI giving us gifts we don't want any
</em><br>
<em>&gt;more, bound only by respect considerations we no longer have. And the
</em><br>
<em>&gt;next generation will be moulded by the AI into the situation above.
</em><br>
<p>The plan proposed in the paper is to reevaluate people's utilities
<br>
continuously.  Assuming time is measured in the AI's timesteps, the
<br>
plan is to take action at time X so that what people get at time X+1
<br>
is what they wanted at time X, as much as possible, and then increment
<br>
X and continue.  Everyone seems to misunderstand what I wrote there,
<br>
so it must be poorly explained.
<br>
<p>I don't know how to cope with the next generation.  The AI should
<br>
probably not have compassion for young children directly; instead it
<br>
should have compassion for their parents and promote the wellbeing of
<br>
the children because the parents want it to.  I don't see a way to
<br>
deal with a child becoming an adult that does not feel like an
<br>
arbitrary decision.
<br>
<pre>
-- 
Tim Freeman               <a href="http://www.fungible.com">http://www.fungible.com</a>           <a href="mailto:tim@fungible.com?Subject=Re:%20[sl4]%20AI's%20behaving%20badly">tim@fungible.com</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19565.html">Petter Wingren-Rasmussen: "[sl4] How to imprint rules (re:AI's behaving badly)"</a>
<li><strong>Previous message:</strong> <a href="19563.html">Peter de Blanc: "Re: [sl4] AI's behaving badly"</a>
<li><strong>In reply to:</strong> <a href="19562.html">Stuart Armstrong: "[sl4] AI's behaving badly"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19564">[ date ]</a>
<a href="index.html#19564">[ thread ]</a>
<a href="subject.html#19564">[ subject ]</a>
<a href="author.html#19564">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
