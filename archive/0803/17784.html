<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Understanding the problem of friendliness</title>
<meta name="Author" content="Vladimir Nesov (robotact@gmail.com)">
<meta name="Subject" content="Understanding the problem of friendliness">
<meta name="Date" content="2008-03-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Understanding the problem of friendliness</h1>
<!-- received="Thu Mar  6 11:41:30 2008" -->
<!-- isoreceived="20080306184130" -->
<!-- sent="Thu, 6 Mar 2008 21:38:42 +0300" -->
<!-- isosent="20080306183842" -->
<!-- name="Vladimir Nesov" -->
<!-- email="robotact@gmail.com" -->
<!-- subject="Understanding the problem of friendliness" -->
<!-- id="b54769d90803061038y3506449aocf166f0b6a44b8df@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Vladimir Nesov (<a href="mailto:robotact@gmail.com?Subject=Re:%20Understanding%20the%20problem%20of%20friendliness"><em>robotact@gmail.com</em></a>)<br>
<strong>Date:</strong> Thu Mar 06 2008 - 11:38:42 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17785.html">Krekoski Ross: "Re: Mindless Thought Experiments"</a>
<li><strong>Previous message:</strong> <a href="17783.html">Matt Mahoney: "Re: Mindless Thought Experiments"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17784">[ date ]</a>
<a href="index.html#17784">[ thread ]</a>
<a href="subject.html#17784">[ subject ]</a>
<a href="author.html#17784">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Tue, Feb 12, 2008 at 1:52 AM, Eliezer S. Yudkowsky
<br>
&lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Understanding%20the%20problem%20of%20friendliness">sentience@pobox.com</a>&gt; wrote:
<br>
<em>&gt; <a href="http://en.wikipedia.org/wiki/AI-complete">http://en.wikipedia.org/wiki/AI-complete</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt;  Why go to all the trouble of building an AI?  Why not just build a
</em><br>
<em>&gt;  natural-language-understander that compiles English requests to
</em><br>
<em>&gt;  programs, and then type into the prompt, &quot;Please make an AI&quot;?
</em><br>
<em>&gt;
</em><br>
<em>&gt;  The English-to-program-compiler is hence AI-complete, meaning that if
</em><br>
<em>&gt;  you can build it, you can build an AI - hence you shouldn't expect it
</em><br>
<em>&gt;  to be any easier than AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt;  Similarly, building an AI that knows what you &quot;really mean&quot; by
</em><br>
<em>&gt;  &quot;Friendly&quot; when you type &quot;Please make a Friendly AI&quot; at the prompt, is
</em><br>
<em>&gt;  FAI-complete, and not any easier than building a Friendly AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt;  (I find that conversations of this sort have more the shade of someone
</em><br>
<em>&gt;  trying to figure out how to game the Dungeons and Dragons rules for
</em><br>
<em>&gt;  the wish spell, than AI science... remember, nothing ever runs on
</em><br>
<em>&gt;  English rules; even your brain doesn't run on English rules.)
</em><br>
<em>&gt;
</em><br>
<p>OK, I think got that now. I couldn't see how can there be AGIs that
<br>
can't get what you mean by e.g. 'friendly AI', but are still
<br>
dangerous, so I jumped to conclusion that problem with making AGI
<br>
friendly must lie in it having a rigid (Plato-style), unreliable or
<br>
diverging goal system (thing everybody is talking about), which my
<br>
proposal seems to solve, but in retrospect this procedure should be
<br>
obvious, so it's beside the point.
<br>
<p>Unfriendly AGI is a problem of idiot savant. It has enough ability to
<br>
interface with real world, it can do certain things much better (or
<br>
faster, cheaper) then humans, and in many areas it can become a
<br>
serious, potentially runaway power. It won't be able to understand
<br>
subtle enough issues, such as what you mean by 'friendly', but will be
<br>
able to solve some real-world puzzles that are more straightforward or
<br>
roughly follow from few first principles that it embodies.
<br>
<p>The problem of friendly AI is a problem of making an AGI that listens
<br>
to the world as opposed to blindly rewriting it.
<br>
<p><pre>
-- 
Vladimir Nesov
<a href="mailto:robotact@gmail.com?Subject=Re:%20Understanding%20the%20problem%20of%20friendliness">robotact@gmail.com</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17785.html">Krekoski Ross: "Re: Mindless Thought Experiments"</a>
<li><strong>Previous message:</strong> <a href="17783.html">Matt Mahoney: "Re: Mindless Thought Experiments"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17784">[ date ]</a>
<a href="index.html#17784">[ thread ]</a>
<a href="subject.html#17784">[ subject ]</a>
<a href="author.html#17784">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
