<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Chaining God</title>
<meta name="Author" content="Stuart Armstrong (dragondreaming@googlemail.com)">
<meta name="Subject" content="Re: Chaining God">
<meta name="Date" content="2008-03-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Chaining God</h1>
<!-- received="Wed Mar 12 08:14:37 2008" -->
<!-- isoreceived="20080312141437" -->
<!-- sent="Wed, 12 Mar 2008 15:11:49 +0100" -->
<!-- isosent="20080312141149" -->
<!-- name="Stuart Armstrong" -->
<!-- email="dragondreaming@googlemail.com" -->
<!-- subject="Re: Chaining God" -->
<!-- id="38f493f10803120711n239618a8q7a1144c71a39aafe@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="79ecaa350803111908u1754d5adj745eb34446d7175e@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Stuart Armstrong (<a href="mailto:dragondreaming@googlemail.com?Subject=Re:%20Chaining%20God"><em>dragondreaming@googlemail.com</em></a>)<br>
<strong>Date:</strong> Wed Mar 12 2008 - 08:11:49 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17972.html">Lee Corbin: "Re: Objective Meaning Must Exhibit Isomorphism"</a>
<li><strong>Previous message:</strong> <a href="17970.html">Stuart Armstrong: "Re: The GLUT and functionalism"</a>
<li><strong>In reply to:</strong> <a href="17955.html">Rolf Nelson: "Re: Chaining God"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17971">[ date ]</a>
<a href="index.html#17971">[ thread ]</a>
<a href="subject.html#17971">[ subject ]</a>
<a href="author.html#17971">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Dear Rolf,
<br>
<p>Thanks for your comments!
<br>
<p><em>&gt; I can think of definitions of trustworthy that are
</em><br>
<em>&gt; useful-to-have-implemented (like &quot;won't kick off a line of descendants that
</em><br>
<em>&gt; will eventually kill me&quot;) and definitions of trustworthy that are
</em><br>
<em>&gt; practical-to-measure (like &quot;won't stab me in the next 30 seconds&quot;), which do
</em><br>
<em>&gt; you mean when you use the word &quot;trustworthy&quot; in the paper?
</em><br>
<p>The ambiguity in &quot;trustworthy&quot; is part of the problem. I tend to use
<br>
it in the sense of &quot;telling the truth&quot;, and acting on what it says it
<br>
will. My position is that the words used - &quot;honest&quot;, &quot;trustworthy&quot;,
<br>
&quot;safe&quot; - mean different things in different circumstances. My friend
<br>
down the pub may be honest, trustworthy and safe - but make him
<br>
president of the US, without changing anything about him, and he will
<br>
remain pretty honest, but become un-trustworthy (since trustworthiness
<br>
in a politician is something different than it is for a friend) and
<br>
probably very unsafe.
<br>
<p><em>&gt; The chaining system looks isomorphic to a subset of self-improving systems,
</em><br>
<em>&gt; where the step
</em><br>
<em>&gt;
</em><br>
<em>&gt; A -&gt; A + A' (AI A creates AI A' and continues running)
</em><br>
<em>&gt;
</em><br>
<em>&gt; maps to
</em><br>
<em>&gt;
</em><br>
<em>&gt; A -&gt; [C + A + A'] (AI rewrites its code to become a different AI, which is
</em><br>
<em>&gt; itself a composite of A, A', and a control module that gives A veto power
</em><br>
<em>&gt; over A'). Framed this way, is there a short explanation of why this
</em><br>
<em>&gt; limitation (on how an AGI can modify itself) is helpful? If this limitation
</em><br>
<em>&gt; is required to keep some kind of invariant, it might be better to specify
</em><br>
<em>&gt; the invariant directly,
</em><br>
<p>The answer to this is that I do not believe that this invariant can be
<br>
specified in advance (&quot;honesty&quot;, maybe; &quot;safeness&quot; (or friendliness)
<br>
definetly not). At different levels of power and understanding, these
<br>
terms need to be redefined. The closest analogy is the difference
<br>
between how adults and children would define them. Or, take an upload
<br>
of the most moral individual you can find; if you were to multiply his
<br>
intelligence by a million, would you still be sure of how &quot;safe&quot; or
<br>
moral he was?
<br>
<p>The procedure is:
<br>
A figures out an invariant I, with some human help, which encompasses
<br>
its best (and our best) understanding of what &quot;safe&quot;, &quot;honest&quot; and
<br>
&quot;trustworthy&quot; mean. It then implements I within itself. It then
<br>
constructs the next AI, called A', and verifies whether this AI has
<br>
the invariant I (to within very narrow bounds). If not, it vetoes it;
<br>
if so, it allows A' to proceed. A' then constructs its own invariant
<br>
I', checks it with us and with the lower AI's, and the procedure
<br>
repeats itself.
<br>
<p><em>&gt; In other words, why do you believe the proposed
</em><br>
<em>&gt; system wouldn't take a random walk away from &quot;humanity lives&quot; scenarios and
</em><br>
<em>&gt; towards &quot;humanity dies&quot; scenarios?
</em><br>
<p>It's worst than that: I feel that keeping everything constant, the
<br>
system will walk from &quot;humanity lives&quot; to &quot;humanity dies&quot; quite
<br>
naturally. This has to be corrected for at each stage. My reasons for
<br>
thiking this is possible is
<br>
1) A lot of the AI's ressources are devoted to ensuring such a drift
<br>
doesn't happen.
<br>
<p>2) The more advanced AI's will know more, not less, about what
<br>
circumstances would cause humanity to end. They will be able to design
<br>
an AI that is safer than they are themselves, definitely safer than a
<br>
lower level AI is.
<br>
<p>3) They constantly &quot;touch base&quot; with us, and will not proceed without
<br>
our approval. This means that the drift to &quot;humanity dies&quot; has to
<br>
happen in such a way we don't realise it. This would be easy to do if
<br>
the AI is out to get us; but since it isn't, and since it will be
<br>
searching the realm of our future possibilities with great precision,
<br>
we would see such a drift before it was complete (and, hopefully, way
<br>
before the AI is out of control).
<br>
<p>Again, thanks for your comments,
<br>
<p>Stuart
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17972.html">Lee Corbin: "Re: Objective Meaning Must Exhibit Isomorphism"</a>
<li><strong>Previous message:</strong> <a href="17970.html">Stuart Armstrong: "Re: The GLUT and functionalism"</a>
<li><strong>In reply to:</strong> <a href="17955.html">Rolf Nelson: "Re: Chaining God"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17971">[ date ]</a>
<a href="index.html#17971">[ thread ]</a>
<a href="subject.html#17971">[ subject ]</a>
<a href="author.html#17971">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
