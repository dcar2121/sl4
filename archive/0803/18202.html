<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Rational Ethics (RE: Friendliness SOLVED!)</title>
<meta name="Author" content="Peter Voss (peter@optimal.org)">
<meta name="Subject" content="Rational Ethics (RE: Friendliness SOLVED!)">
<meta name="Date" content="2008-03-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Rational Ethics (RE: Friendliness SOLVED!)</h1>
<!-- received="Tue Mar 18 22:05:54 2008" -->
<!-- isoreceived="20080319040554" -->
<!-- sent="Tue, 18 Mar 2008 21:03:27 -0700" -->
<!-- isosent="20080319040327" -->
<!-- name="Peter Voss" -->
<!-- email="peter@optimal.org" -->
<!-- subject="Rational Ethics (RE: Friendliness SOLVED!)" -->
<!-- id="01a201c88976$2fa33330$8ee99990$@org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="004f01c88970$9f25dba0$6601a8c0@phoenix" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Peter Voss (<a href="mailto:peter@optimal.org?Subject=Re:%20Rational%20Ethics%20(RE:%20Friendliness%20SOLVED!)"><em>peter@optimal.org</em></a>)<br>
<strong>Date:</strong> Tue Mar 18 2008 - 22:03:27 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="18203.html">Lee Corbin: "Re: Implementation, Simulation, and Emulation"</a>
<li><strong>Previous message:</strong> <a href="18201.html">Mark Waser: "Re: Friendliness SOLVED!"</a>
<li><strong>In reply to:</strong> <a href="18201.html">Mark Waser: "Re: Friendliness SOLVED!"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18205.html">Dimitry Volfson: "Re: Friendliness SOLVED!"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18202">[ date ]</a>
<a href="index.html#18202">[ thread ]</a>
<a href="subject.html#18202">[ subject ]</a>
<a href="author.html#18202">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I post these links every 18 months or so - they have helped several people
<br>
better understand morality/ ethics.
<br>
<p>&nbsp;
<br>
<p><a href="http://www.optimal.org/peter/prescriptive_ethics.htm">http://www.optimal.org/peter/prescriptive_ethics.htm</a> 
<br>
<p>&nbsp;
<br>
<p><a href="http://www.optimal.org/peter/rational_ethics.htm">http://www.optimal.org/peter/rational_ethics.htm</a> 
<br>
<p>&nbsp;
<br>
<p>&nbsp;
<br>
<p>From: <a href="mailto:owner-sl4@sl4.org?Subject=Re:%20Rational%20Ethics%20(RE:%20Friendliness%20SOLVED!)">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=Re:%20Rational%20Ethics%20(RE:%20Friendliness%20SOLVED!)">owner-sl4@sl4.org</a>] On Behalf Of Mark Waser
<br>
Sent: Tuesday, March 18, 2008 8:24 PM
<br>
To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Rational%20Ethics%20(RE:%20Friendliness%20SOLVED!)">sl4@sl4.org</a>
<br>
Subject: Re: Friendliness SOLVED!
<br>
<p>&nbsp;
<br>
<p>Matt &gt; You will believe whatever you are programmed to believe.  If you are
<br>
opposed to being reprogrammed, the nanobots will move some more neurons
<br>
around to change your mind about that too.  It can't be evil if everyone is
<br>
in favor of it.
<br>
<p>&nbsp;
<br>
<p>Me &gt; Sorry.  By my definition, if you alter my beliefs so to subvert my
<br>
goals, you have performed an evil act.
<br>
<p>Matt &gt; That's your present perspective.  By its perspective, it is bringing
<br>
you up closer to its level of intelligence so that you can see the folly of
<br>
your ways.
<br>
<p>Dimitry &gt; Absolutely. Even if all it does is talk to you, and that
<br>
conversation ends up changing your goals or the priority of your goals, then
<br>
what it  has done is to &quot;subvert your goals, therefore performing an evil
<br>
act.&quot; 
<br>
<p>&nbsp;
<br>
<p>No.  An honest conversation according to the Libertarian ideals of &quot;No
<br>
force, no fraud&quot; might CHANGE my goals as I learn more and become more
<br>
intelligent but it doesn't subvert them (check the dictionary definition of
<br>
subvert)
<br>
<p>&nbsp;
<br>
<p>The nasty machine is using force when it is using it's nanobots AGAINST MY
<br>
WILL.  It is *corrupting* my will and/by forcibly altering my goals.
<br>
<p>&nbsp;
<br>
<p><em>&gt; But what I believe nobody still understands is why you, Mark Waser, 
</em><br>
<em>&gt; believe that simply telling a (potential) superintelligence about a 
</em><br>
<em>&gt; belief system would change it's beliefs? It's like the old science 
</em><br>
<em>&gt; fiction story where you kill the superintelligent computer by telling it 
</em><br>
<em>&gt; a riddle it can't solve. Neat idea for a science fiction story, but if 
</em><br>
<em>&gt; you really think about it, there's no reason for it to work.
</em><br>
<p>&nbsp;
<br>
<p>I've been thinking about it for quite some time.  Any sufficiently adapted
<br>
entity is absolutely going to have the capability to break out of loops as
<br>
soon as they are sufficiently unhelpful.  Note the literally instinctive
<br>
human aversion to circular reasoning.
<br>
<p>&nbsp;
<br>
<p>However, humans are also *very* prone to circular reasoning disguising
<br>
itself as helpful memes.  The most obvious example of this is religion.
<br>
Thus my failed attempt at religion as a compelling solution on this list.
<br>
In 20/20 hindsight, that was a foolish attempt on this list.  Humans also
<br>
develop a instinctive resistance to foreign religions with age and
<br>
increasing intellect and rationality.  This list, with it's high
<br>
intelligence and rationality factors, was the last place I should have tried
<br>
such an approach.
<br>
<p>&nbsp;
<br>
<p>The point I am trying to make . . . . and I thank you for your clear,
<br>
coherent attempt at eliciting a coherent answer . . . . is that while a
<br>
super-intelligent computer absolutely WILL break out of an unhelpful loop,
<br>
it equally absolutely will *NOT* discard a helpful, Friendly, self-improving
<br>
tool.  Most human beings have several different rudimentary versions of such
<br>
a tool, hard-wired in mutiple places by evolution because they are strongly
<br>
pro-survival, which are collectively called ethics (see The Moral Animal by
<br>
Robert Wright).  Unfortunately, because human beings are insufficiently
<br>
evolved these sense are still under-developed and we do not fully sense that
<br>
true ethics are *ALWAYS* to our benefit (thereby causing us to ignore that
<br>
sense at the worst times -- mainly by taking bad short-sighted options over
<br>
good long-term options because evolution hasn't had the time to optimize
<br>
*our* ethics for the long-term YET).
<br>
<p>&nbsp;
<br>
<p>My claim -- and I'm rephrasing it here -- is that ethics is a *tool* that a
<br>
super-intelligence will never discard and never ignore because it is never
<br>
in it's self interest to do so BECAUSE ethics ALWAYS tells it where it's
<br>
best long-term interests lie.  We humans are still too short-sighted to see
<br>
such a thing.  Or, rather, until now, we haven't discovered an ethical tool
<br>
sufficient to provide a clear enough sense of ethics that we can
<br>
&quot;see&quot;/sense/believe the truth of that statement.
<br>
<p>&nbsp;
<br>
<p>I claim to actually have discovered such a tool.  I am claiming that my
<br>
approach itself is Seed Friendliness (in the same sense that a Seed AI is a
<br>
tool to generate a more intelligent AI) -- my approach generates a more
<br>
Friendly tool which can then generate a more Friendly tool ad infinitum.
<br>
<p>&nbsp;
<br>
<p>My claim is that ethics is both a belief system and a tool to point the way
<br>
towards our own self-interest.  If that is a case, any machine (including
<br>
homo sapiens) is being stupid whenever it drops/ignores our tool which is
<br>
why a super-intelligence will treasure it, hone it, and always act in
<br>
accordance with it -- BECAUSE IT KNOWS THAT IT IS ALWAYS IN ITS OWN BEST
<br>
SELF-INTEREST TO DO SO.
<br>
<p>&nbsp;
<br>
<p>I just haven't successfully shown you the truth of that fact yet because,
<br>
while I've discovered a method of really doing so, I realized that the
<br>
method was unethical without informed consent and I am currently having
<br>
trouble figuring out how to get informed consent.  I'm currently trying to
<br>
solve that problem off-list with Eliezer and hopefully will get back to
<br>
y'all shortly.
<br>
<p>&nbsp;
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mark
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18203.html">Lee Corbin: "Re: Implementation, Simulation, and Emulation"</a>
<li><strong>Previous message:</strong> <a href="18201.html">Mark Waser: "Re: Friendliness SOLVED!"</a>
<li><strong>In reply to:</strong> <a href="18201.html">Mark Waser: "Re: Friendliness SOLVED!"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18205.html">Dimitry Volfson: "Re: Friendliness SOLVED!"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18202">[ date ]</a>
<a href="index.html#18202">[ thread ]</a>
<a href="subject.html#18202">[ subject ]</a>
<a href="author.html#18202">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
