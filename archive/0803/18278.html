<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Hiding AI research from Bad People was Re: OpenCog Concerns</title>
<meta name="Author" content="William Pearson (wil.pearson@gmail.com)">
<meta name="Subject" content="Hiding AI research from Bad People was Re: OpenCog Concerns">
<meta name="Date" content="2008-03-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Hiding AI research from Bad People was Re: OpenCog Concerns</h1>
<!-- received="Sun Mar 23 17:29:15 2008" -->
<!-- isoreceived="20080323232915" -->
<!-- sent="Sun, 23 Mar 2008 23:26:55 +0000" -->
<!-- isosent="20080323232655" -->
<!-- name="William Pearson" -->
<!-- email="wil.pearson@gmail.com" -->
<!-- subject="Hiding AI research from Bad People was Re: OpenCog Concerns" -->
<!-- id="ab5bcc90803231626o52a63db9j987775e07e678b2a@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> William Pearson (<a href="mailto:wil.pearson@gmail.com?Subject=Re:%20Hiding%20AI%20research%20from%20Bad%20People%20was%20Re:%20OpenCog%20Concerns"><em>wil.pearson@gmail.com</em></a>)<br>
<strong>Date:</strong> Sun Mar 23 2008 - 17:26:55 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="18279.html">Stathis Papaioannou: "Re: Bekenstein bound (Re: A model of consciousness)"</a>
<li><strong>Previous message:</strong> <a href="18277.html">Edward Miller: "Re: OpenCog Concerns"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18280.html">Daniel Burfoot: "Re: Hiding AI research from Bad People was Re: OpenCog Concerns"</a>
<li><strong>Reply:</strong> <a href="18280.html">Daniel Burfoot: "Re: Hiding AI research from Bad People was Re: OpenCog Concerns"</a>
<li><strong>Reply:</strong> <a href="18282.html">J. Andrew Rogers: "Re: Hiding AI research from Bad People was Re: OpenCog Concerns"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18278">[ date ]</a>
<a href="index.html#18278">[ thread ]</a>
<a href="subject.html#18278">[ subject ]</a>
<a href="author.html#18278">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 23/03/2008, Daniel Burfoot &lt;<a href="mailto:daniel.burfoot@gmail.com?Subject=Re:%20Hiding%20AI%20research%20from%20Bad%20People%20was%20Re:%20OpenCog%20Concerns">daniel.burfoot@gmail.com</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; On Sun, Mar 23, 2008 at 5:19 PM, Edward Miller &lt;<a href="mailto:progressive_1987@yahoo.com?Subject=Re:%20Hiding%20AI%20research%20from%20Bad%20People%20was%20Re:%20OpenCog%20Concerns">progressive_1987@yahoo.com</a>&gt;
</em><br>
<em>&gt; wrote:
</em><br>
<em>&gt; &gt; I am wondering if it is possible to specify no
</em><br>
<em>&gt; &gt; military use in OpenCog's license. Certainly DARPA
</em><br>
<em>&gt; &gt; would love to get their hands on it if it is useful,
</em><br>
<em>&gt; &gt; and maybe we ought not to let them.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think this is an excellent idea. The issue is deep and difficult, of
</em><br>
<em>&gt; course, but that doesn't mean we shouldn't at least try to do something
</em><br>
<em>&gt; about it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I imagine it would encourage greater participation in the project as well.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I would like these issues to be more widely discussed. I wonder if one could
</em><br>
<em>&gt; achieve a critical mass of scientists who would sign an agreement to refuse
</em><br>
<em>&gt; DARPA funding, refuse to collaborate with DARPA-funded researchers, and to
</em><br>
<em>&gt; refuse to cite papers supported by DARPA (and also DARPA's counterparts in
</em><br>
<em>&gt; other countries).
</em><br>
<em>&gt;
</em><br>
<p>Sorry to put a downer on this idea, but it smacks of naivety somewhat.
<br>
The military  of any country (and DARPA is not the only one that you
<br>
might be worried about) are not going to respect copyright if it harms
<br>
their perceived defensive capability. These are the people that have
<br>
secrets upon secrets. They will just use your code, secretly.
<br>
<p>The two ways you can go are is hope that friendly singletons can get
<br>
from human to hyperhuman intelligent quickly (one big genie), or
<br>
spread the secret of singleton resisting AI as far and as fast as
<br>
possible (many small genies with many competing goals). The
<br>
non-singleton model relies on vast intelligence magnification not
<br>
being very likely. Eliezer has written voluminously on the singleton
<br>
subject, and opencog is Not The Way To Go, for this path.
<br>
<p>Singleton resistant AI would have to be similar to human in that one
<br>
process within it couldn't access all its code and its code (analogous
<br>
to rewiring during neural plasticity) would change over time. Making
<br>
it as unlikely to explode as humans, in fact as soon as it understood
<br>
some of its code, its code would change as understanding would be of a
<br>
procedural (literally!) nature.
<br>
<p>Each AI would have a different goal made by a different person, some
<br>
might be friendly, other likely not (militarily developed), most might
<br>
be slaved to humans (or as prefer to think of them as exogenous brain
<br>
prostheses). You would have to try and steer the maximum amount of
<br>
compute power so that the end result is somewhat palatable.
<br>
<p>Far and fast is my argument so that any one persons mistake is
<br>
unlikely to stomp on the rest of the people. Democracy and the markets
<br>
seem to be the least bad types of power structures we have made.
<br>
<p>So I'd argue for less time spent philosophizing about the license,
<br>
more time spent actually making the damn thing work, and getting the
<br>
word out to spread the tech to people you want to have it. Oh yeah and
<br>
making sure the design cannot be easily botnetted by a big, bad
<br>
non-morally constrained one.
<br>
<p>&nbsp;&nbsp;Will Pearson
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18279.html">Stathis Papaioannou: "Re: Bekenstein bound (Re: A model of consciousness)"</a>
<li><strong>Previous message:</strong> <a href="18277.html">Edward Miller: "Re: OpenCog Concerns"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18280.html">Daniel Burfoot: "Re: Hiding AI research from Bad People was Re: OpenCog Concerns"</a>
<li><strong>Reply:</strong> <a href="18280.html">Daniel Burfoot: "Re: Hiding AI research from Bad People was Re: OpenCog Concerns"</a>
<li><strong>Reply:</strong> <a href="18282.html">J. Andrew Rogers: "Re: Hiding AI research from Bad People was Re: OpenCog Concerns"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18278">[ date ]</a>
<a href="index.html#18278">[ thread ]</a>
<a href="subject.html#18278">[ subject ]</a>
<a href="author.html#18278">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
