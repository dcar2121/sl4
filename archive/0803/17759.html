<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: OpenCog Concerns</title>
<meta name="Author" content="Matt Mahoney (matmahoney@yahoo.com)">
<meta name="Subject" content="Re: OpenCog Concerns">
<meta name="Date" content="2008-03-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: OpenCog Concerns</h1>
<!-- received="Tue Mar  4 20:42:02 2008" -->
<!-- isoreceived="20080305034202" -->
<!-- sent="Tue, 4 Mar 2008 19:39:52 -0800 (PST)" -->
<!-- isosent="20080305033952" -->
<!-- name="Matt Mahoney" -->
<!-- email="matmahoney@yahoo.com" -->
<!-- subject="Re: OpenCog Concerns" -->
<!-- id="436977.48481.qm@web51909.mail.re2.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="252626.31756.qm@web53005.mail.re2.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Matt Mahoney (<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20OpenCog%20Concerns"><em>matmahoney@yahoo.com</em></a>)<br>
<strong>Date:</strong> Tue Mar 04 2008 - 20:39:52 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17760.html">Lee Corbin: "Re: Objective Meaning Must Exhibit Isomorphism"</a>
<li><strong>Previous message:</strong> <a href="17758.html">Matt Mahoney: "Re: Mindless Thought Experiments"</a>
<li><strong>In reply to:</strong> <a href="17744.html">Jeff Herrlich: "Re: OpenCog Concerns"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17767.html">Jeff Herrlich: "Re: OpenCog Concerns"</a>
<li><strong>Reply:</strong> <a href="17767.html">Jeff Herrlich: "Re: OpenCog Concerns"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17759">[ date ]</a>
<a href="index.html#17759">[ thread ]</a>
<a href="subject.html#17759">[ subject ]</a>
<a href="author.html#17759">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
--- Jeff Herrlich &lt;<a href="mailto:jeff_herrlich@yahoo.com?Subject=Re:%20OpenCog%20Concerns">jeff_herrlich@yahoo.com</a>&gt; wrote:
<br>
<p><em>&gt; &quot;I don't buy it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Friendliness has nothing to do with keeping AI out of the hands of &quot;bad
</em><br>
<em>&gt; guys&quot;.
</em><br>
<em>&gt; Nobody, good or bad, has yet solved the friendliness problem.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Right, I meant &quot;good guys&quot; as a very general term that refers to programmers
</em><br>
<em>&gt; who both understand the safety issues, and who are committed to building a
</em><br>
<em>&gt; safe, universally beneficial AGI.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There is a danger from programmers/teams who aren't even aware of safety
</em><br>
<em>&gt; issues, and another (possibly smaller) danger from programmers/teams who
</em><br>
<em>&gt; understand the safety issues but who might seek to use the AGI for selfish
</em><br>
<em>&gt; benefits instead of universal benefits (eg. rogue governments, etc). It's
</em><br>
<em>&gt; easy to label as science fiction, but it's also not an impossibility.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think that as proto-AGIs develop we will gain a better practical
</em><br>
<em>&gt; understanding of AGI safety.
</em><br>
<p>My question is about the safety of distributed AI that emerges from a network
<br>
of narrowly specialized experts that talk to each other.  You can consider my
<br>
proposal at <a href="http://www.mattmahoney.net/agi.html">http://www.mattmahoney.net/agi.html</a> or more generally any
<br>
environment where peers compete for resources and reputation with no
<br>
centralized control.
<br>
<p>I think I am at least aware of some of the risks of runaway recursive self
<br>
improvement.  I believe that distributed AI controlled by billions of humans
<br>
and whose primary source of knowledge is also from humans will at least
<br>
reflect a consensus view of ethics and friendliness.  Peers will compete for
<br>
reputation and audience in a hostile environment, so we should expect them to
<br>
respond to questions with useful and correct answers, including questions
<br>
about human goals and what is the right thing to do in a wide variety of
<br>
circumstances.
<br>
<p>Distributed AI has special risks.  As computing power gets cheaper and peers
<br>
become more intelligent, humans will no longer be the primary source of
<br>
knowledge and become less relevant.  The language between peers could evolve
<br>
from natural language to something too complex for humans to understand. 
<br>
Shortly afterwards there would be a singularity.
<br>
<p>Another risk for distributed AI is that when intelligence develops to the
<br>
point where the system can rewrite its own software, it will also become
<br>
possible to develop intelligent worms that can discover and exploit new
<br>
security holes faster than humans can patch them.  Conventional security such
<br>
as virus scanners, firewalls, and intrusion detection systems would be no
<br>
protection, because the attacks would be unknown to them.  It is quite
<br>
possible that peers will expend the majority of their CPU cycles fending off
<br>
attacks and filtering spam, while at the same time trying to defeat the
<br>
defenses of other peers.
<br>
<p>Of course there are risks of AI in general that depend on philosophical
<br>
questions that can't be answered.  Is the AI friendly if we ask to be put in
<br>
an eternal state of bliss and it obeys?  Is it a good outcome if humans are
<br>
extinct but our memories are preserved by superhuman AI?  Such questions are
<br>
fun to discuss but they only seem to waste our time without leading to any
<br>
progress.
<br>
<p><p>-- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20OpenCog%20Concerns">matmahoney@yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17760.html">Lee Corbin: "Re: Objective Meaning Must Exhibit Isomorphism"</a>
<li><strong>Previous message:</strong> <a href="17758.html">Matt Mahoney: "Re: Mindless Thought Experiments"</a>
<li><strong>In reply to:</strong> <a href="17744.html">Jeff Herrlich: "Re: OpenCog Concerns"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17767.html">Jeff Herrlich: "Re: OpenCog Concerns"</a>
<li><strong>Reply:</strong> <a href="17767.html">Jeff Herrlich: "Re: OpenCog Concerns"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17759">[ date ]</a>
<a href="index.html#17759">[ thread ]</a>
<a href="subject.html#17759">[ subject ]</a>
<a href="author.html#17759">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:02 MDT
</em></small></p>
</body>
</html>
