<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Edge.org: Jaron Lanier</title>
<meta name="Author" content="Colin (chales1@bigpond.net.au)">
<meta name="Subject" content="RE: Edge.org: Jaron Lanier">
<meta name="Date" content="2003-11-30">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Edge.org: Jaron Lanier</h1>
<!-- received="Sun Nov 30 04:14:17 2003" -->
<!-- isoreceived="20031130111417" -->
<!-- sent="Sun, 30 Nov 2003 22:13:47 +1100" -->
<!-- isosent="20031130111347" -->
<!-- name="Colin" -->
<!-- email="chales1@bigpond.net.au" -->
<!-- subject="RE: Edge.org: Jaron Lanier" -->
<!-- id="001301c3b733$09f78f60$0100a8c0@pc2" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="87n0aexuse.fsf@snark.piermont.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Colin (<a href="mailto:chales1@bigpond.net.au?Subject=RE:%20Edge.org:%20Jaron%20Lanier"><em>chales1@bigpond.net.au</em></a>)<br>
<strong>Date:</strong> Sun Nov 30 2003 - 04:13:47 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7309.html">Perry E.Metzger: "Re: Edge.org: Jaron Lanier"</a>
<li><strong>Previous message:</strong> <a href="7307.html">Randall Randall: "Re: Edge.org: Jaron Lanier"</a>
<li><strong>In reply to:</strong> <a href="7306.html">Perry E.Metzger: "Re: Edge.org: Jaron Lanier"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7309.html">Perry E.Metzger: "Re: Edge.org: Jaron Lanier"</a>
<li><strong>Reply:</strong> <a href="7309.html">Perry E.Metzger: "Re: Edge.org: Jaron Lanier"</a>
<li><strong>Reply:</strong> <a href="7310.html">Randall Randall: "Re: Edge.org: Jaron Lanier"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7308">[ date ]</a>
<a href="index.html#7308">[ thread ]</a>
<a href="subject.html#7308">[ subject ]</a>
<a href="author.html#7308">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Perry E.Metzger
<br>
<em>&gt; 
</em><br>
<em>&gt; &quot;Colin&quot; &lt;<a href="mailto:chales1@bigpond.net.au?Subject=RE:%20Edge.org:%20Jaron%20Lanier">chales1@bigpond.net.au</a>&gt; writes:
</em><br>
<em>&gt; &gt; A model of a thing is not a thing!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Is a thought of a unicorn a real thought?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Or, to throw the spear straight at Mr. Searle, is a 
</em><br>
<em>&gt; simulation of an addition somehow different from &quot;really&quot; 
</em><br>
<em>&gt; adding two numbers?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; (Presumably a Chinese room made up of neurons in a small 
</em><br>
<em>&gt; dense volume following a deterministic program can't be 
</em><br>
<em>&gt; conscious because none of those neurons, when interviewed, 
</em><br>
<em>&gt; experience qualia individually. :)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Sorry to single out one sentence among many for assault -- I 
</em><br>
<em>&gt; just see red when Searle's bizarroid argument gets mentioned 
</em><br>
<em>&gt; even indirectly.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; As for myself, I don't believe we'll solve the problem of 
</em><br>
<em>&gt; consciousness -- and we won't care. The problem of producing 
</em><br>
<em>&gt; a synthetic construct that passes (or more to the ultimate point,
</em><br>
<em>&gt; surpasses) the Turing Test is not a problem of producing a 
</em><br>
<em>&gt; consciousness -- it is a problem of producing a black box 
</em><br>
<em>&gt; that has a particularly observable external behavior.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; (Indeed, one might easily argue that, from the point of view 
</em><br>
<em>&gt; of the Friendly AI people, it is unnecessary that the god 
</em><br>
<em>&gt; they wish to create be conscious so long as it acts as though 
</em><br>
<em>&gt; it has a conscience, whether it is &quot;aware&quot; of it or not.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Perry
</em><br>
<em>&gt; 
</em><br>
<p><p><em>&gt; Is a thought of a unicorn a real thought?
</em><br>
<p>Yes. If you mean the subjective visual imagery associated with
<br>
reflective thinking of a unicorn.
<br>
Yes. If you mean the audio subjective experience of the phonemes of the
<br>
word unicorn (that you just had when reading the word).
<br>
Yes. If you mean the subjectively void associative processes that
<br>
generate behaviour in respect of unicorns such as the muscle triggers
<br>
used in speech generation when talking about unicorns.
<br>
<p>Brain matter does all of these things. They are all real even if the
<br>
unicorn is not. It's time to dump the simplistic linguistic traps from
<br>
philosophers and move on.
<br>
<p><em>&gt; Or, to throw the spear straight at Mr. Searle, is a 
</em><br>
<em>&gt; simulation of an addition somehow different from &quot;really&quot; 
</em><br>
<em>&gt; adding two numbers?
</em><br>
<p>Do you mean the role the subjective experience of the phonetics in
<br>
learning the addition? Or the subjective experience of the visual
<br>
representation as it relates to the abstractions of quantity and
<br>
operators? Or the relatively experienceless process of habituated
<br>
addition? 
<br>
<p>Again too simplistic. Not a basis for taking any position one way or
<br>
another.
<br>
<p>You fall into the trap that I spent so much time delineating in the
<br>
previous post - Discipline blindess - You go anti Searle without being
<br>
able to prove conclusively that the subjective experiences are
<br>
unimportant in intelligence. How can you do that? Nobody has done that
<br>
yet. Has anyone written anything on the proposal that the quale is/is
<br>
not the brain's solution to the symbol grounding problem?  No. Has
<br>
computer science, pumping squillions into AI on the tacit assumption
<br>
that the answer is nay, proven it one way or another? No. Has computer
<br>
science proven that creating algorithmic models of measurements of
<br>
'thing' captures and produces a subjective experience of 'thingness'
<br>
and/or that this 'thingness' experience is/is not optional in relation
<br>
to understanding objects with the property of 'thingness'? No!
<br>
<p>The solution to the conundrum I delineated in my previous email is to
<br>
find the place where the viewpoints of these disparit disciplines will
<br>
be found to be both true and false in some way when viewed in retrospect
<br>
from a position of knowledge of the final solution. To find the solution
<br>
you have to look at the real evidence and say after me:
<br>
<p>&quot;I/We/They am/are _BOTH_ right and wrong in some way not yet understood.
<br>
I must cease alliances with bandwagons, drop dogma and question every
<br>
assumption, every convention, every expression. Despite all my attempts
<br>
my view of brain matter (maybe all matter) is missing an important
<br>
ingredient and this ingredient's importance in what I am doing is not
<br>
known&quot;. 
<br>
<p>Nobody can possibly take a totally provable stance for or against searle
<br>
or any one else! 
<br>
<p><em>&gt; a synthetic construct that passes (or more to the ultimate point,
</em><br>
<em>&gt; surpasses) the Turing Test is not a problem of producing a 
</em><br>
<em>&gt; consciousness -- it is a problem of producing a black box 
</em><br>
<em>&gt; that has a particularly observable external behavior.
</em><br>
<p><em>&gt; (Indeed, one might easily argue that, from the point of view 
</em><br>
<em>&gt; of the Friendly AI people, it is unnecessary that the god 
</em><br>
<em>&gt; they wish to create be conscious so long as it acts as though 
</em><br>
<em>&gt; it has a conscience, whether it is &quot;aware&quot; of it or not.)
</em><br>
<p>OK. Again...
<br>
<p>That the functionalist/computationalist approach is the one true path to
<br>
this 'god' is the tacit assumption by all computer science.
<br>
But..........
<br>
<p>Where is the proof it will/will not 'understand' what it is like to be
<br>
us? 
<br>
Where is the proof it will/will not have a conception of 'friendly'?
<br>
Where is the proof it it will even know it is there?
<br>
Where is the proof this approach is/is not just a sophisticated version
<br>
of ascription of the style kids have when playing with dolls. For that
<br>
is what the turing test is/is not as the case may be.
<br>
<p>These proofs do not exist. Yet somehow you bow to the great Turing test
<br>
as if it was what is needed to be completely satisfied that real
<br>
understanding exists in an artefact. Why would anyone attempt to create
<br>
such a creature on the basis of such a level of ignorance? How can
<br>
anyone take any one position  as proven to an investor? I know you have
<br>
to start somewhere. But that somewhere is sitting on axiomatic clouds,
<br>
and in 2003 is clearly not producing results and yet advocates cling to
<br>
it like a life buoy.
<br>
<p>Computer science/AI always assumes, again tacitly, that the humble quale
<br>
is optional and/or emergent from representational complexity in any
<br>
form. This is blinkered thinking and I hope squadrons of Jaron Laniers
<br>
line up to poke the whole of computer science and any other form of
<br>
discipline blindness in the eye to get them to wake up see there is a
<br>
problem.
<br>
<p>If I remember correctly Searle has backed off from the 'biology only'
<br>
stance (chinese room era) to a  non-biological matter-as-computation
<br>
stance. Brain matter is only proven sufficient, not necessary. This is a
<br>
reasonable position when surrounded by so much evidence that there is a
<br>
subjective experience and that it comes from brain matter in a fashion
<br>
without any explanation (and an even less explanation for models of
<br>
brain matter run on virtual machines in manufactured silicon rocks). 
<br>
<p>Is &quot;computer science&quot; even science? The way it acts leaves me with
<br>
doubts. It has hallmarks of religion born of a desperate need to
<br>
abstract and virtualise away from the real world. This will work really
<br>
well until the real world becomes a mandatory component. Something that
<br>
an industry based on abstraction is likely to sail by in blissful
<br>
ignorance.
<br>
<p>The Jaron Laniers of the world are welcome signs of a rising tide of
<br>
necessary questioning and we need to wake up and listen. Put models and
<br>
Turing tests and virtual machines aside for a moment and really consider
<br>
the act of being in the universe and what that tells you about matter.
<br>
Clarity will emerge.
<br>
<p>Before then any position taken is just dogma. Hume and Kant showed us
<br>
how to throw off the shackles of dogmatism centuries ago. What the hell
<br>
is wrong with us? Have we learned nothing?
<br>
<p>Colin Hales
<br>
*ok I'm done :-) *
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7309.html">Perry E.Metzger: "Re: Edge.org: Jaron Lanier"</a>
<li><strong>Previous message:</strong> <a href="7307.html">Randall Randall: "Re: Edge.org: Jaron Lanier"</a>
<li><strong>In reply to:</strong> <a href="7306.html">Perry E.Metzger: "Re: Edge.org: Jaron Lanier"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7309.html">Perry E.Metzger: "Re: Edge.org: Jaron Lanier"</a>
<li><strong>Reply:</strong> <a href="7309.html">Perry E.Metzger: "Re: Edge.org: Jaron Lanier"</a>
<li><strong>Reply:</strong> <a href="7310.html">Randall Randall: "Re: Edge.org: Jaron Lanier"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7308">[ date ]</a>
<a href="index.html#7308">[ thread ]</a>
<a href="subject.html#7308">[ subject ]</a>
<a href="author.html#7308">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:43 MDT
</em></small></p>
</body>
</html>
