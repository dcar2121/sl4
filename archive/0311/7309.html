<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Edge.org: Jaron Lanier</title>
<meta name="Author" content="Perry E.Metzger (perry@piermont.com)">
<meta name="Subject" content="Re: Edge.org: Jaron Lanier">
<meta name="Date" content="2003-11-30">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Edge.org: Jaron Lanier</h1>
<!-- received="Sun Nov 30 08:49:48 2003" -->
<!-- isoreceived="20031130154948" -->
<!-- sent="Sun, 30 Nov 2003 10:49:45 -0500" -->
<!-- isosent="20031130154945" -->
<!-- name="Perry E.Metzger" -->
<!-- email="perry@piermont.com" -->
<!-- subject="Re: Edge.org: Jaron Lanier" -->
<!-- id="87brqtyhye.fsf@snark.piermont.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="001301c3b733$09f78f60$0100a8c0@pc2" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Perry E.Metzger (<a href="mailto:perry@piermont.com?Subject=Re:%20Edge.org:%20Jaron%20Lanier"><em>perry@piermont.com</em></a>)<br>
<strong>Date:</strong> Sun Nov 30 2003 - 08:49:45 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7310.html">Randall Randall: "Re: Edge.org: Jaron Lanier"</a>
<li><strong>Previous message:</strong> <a href="7308.html">Colin: "RE: Edge.org: Jaron Lanier"</a>
<li><strong>In reply to:</strong> <a href="7308.html">Colin: "RE: Edge.org: Jaron Lanier"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7310.html">Randall Randall: "Re: Edge.org: Jaron Lanier"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7309">[ date ]</a>
<a href="index.html#7309">[ thread ]</a>
<a href="subject.html#7309">[ subject ]</a>
<a href="author.html#7309">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&quot;Colin&quot; &lt;<a href="mailto:chales1@bigpond.net.au?Subject=Re:%20Edge.org:%20Jaron%20Lanier">chales1@bigpond.net.au</a>&gt; writes:
<br>
<em>&gt;&gt; Or, to throw the spear straight at Mr. Searle, is a 
</em><br>
<em>&gt;&gt; simulation of an addition somehow different from &quot;really&quot; 
</em><br>
<em>&gt;&gt; adding two numbers?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Do you mean the role the subjective experience of the phonetics in
</em><br>
<em>&gt; learning the addition? Or the subjective experience of the visual
</em><br>
<em>&gt; representation as it relates to the abstractions of quantity and
</em><br>
<em>&gt; operators? Or the relatively experienceless process of habituated
</em><br>
<em>&gt; addition? 
</em><br>
<p>No. I mean if I simulate the addition of two numbers, am I really
<br>
adding them?
<br>
<p>Searle keeps saying things like &quot;a simulated hurricane doesn't blow
<br>
anything over&quot;. Fair enough. But if I simulate adding two things (that
<br>
is, have a non-human construct follow some algorithms humans use for
<br>
adding and add them), is the output just &quot;simulated&quot; addition, or
<br>
actual addition?
<br>
<p>(If you think I'm being disingenuous in asking this, of course I am.)
<br>
<p>I note that you cut out my other comment. I'll bring it back.
<br>
<p>&nbsp;&nbsp;&nbsp;(Presumably a Chinese room made up of neurons in a small dense volume
<br>
&nbsp;&nbsp;&nbsp;following a deterministic program can't be conscious because none of
<br>
&nbsp;&nbsp;&nbsp;those neurons, when interviewed, experience qualia individually. :)
<br>
<p>All of Searle's arguments add up (pardon the expression -- I know I
<br>
can't really add, merely simulate addition) to a big question he keeps
<br>
begging but is never willing to face. If he's right, what makes us
<br>
think people are conscious, either? After all, if the man in the
<br>
Chinese room (and his pencil and paper and rulebook) following
<br>
instructions blindly doesn't have the &quot;Subjective Experience&quot; of
<br>
knowing Chinese, why is it any different for our neurons? Hell, it is
<br>
even worse -- the man in the Chinese room presumably is conscious
<br>
himself, but I strongly suspect none of my neurons are individually
<br>
conscious at all.
<br>
<p>So why is it that people are conscious, either, if Searle is right?
<br>
<p>(In some sense, of course, Dennett takes up this very argument in
<br>
&quot;Consciousness Explained&quot;...)
<br>
<p><em>&gt; You fall into the trap that I spent so much time delineating in the
</em><br>
<em>&gt; previous post - Discipline blindess - You go anti Searle without being
</em><br>
<em>&gt; able to prove conclusively that the subjective experiences are
</em><br>
<em>&gt; unimportant in intelligence.
</em><br>
<p>Hardly. I make no claims at all. I don't even claim you are
<br>
conscious. In fact, I defy you to prove that you are.
<br>
<p>But as I'm discipline blind, perhaps you, as a person who has
<br>
discipline sight, would care to guide a poor misguided wanderer into
<br>
the direction of a more objective understanding of consciousness?
<br>
<p><em>&gt; &quot;I/We/They am/are _BOTH_ right and wrong in some way not yet understood.
</em><br>
<em>&gt; I must cease alliances with bandwagons, drop dogma and question every
</em><br>
<em>&gt; assumption, every convention, every expression. Despite all my attempts
</em><br>
<em>&gt; my view of brain matter (maybe all matter) is missing an important
</em><br>
<em>&gt; ingredient and this ingredient's importance in what I am doing is not
</em><br>
<em>&gt; known&quot;. 
</em><br>
<em>&gt;
</em><br>
<em>&gt; Nobody can possibly take a totally provable stance for or against searle
</em><br>
<em>&gt; or any one else! 
</em><br>
<p>Indeed, I would agree with you that there is no way to prove or
<br>
disprove Searle's stance. It is what some people call
<br>
&quot;non-falsifiable&quot;. Are you familiar with what that implies?
<br>
<p><em>&gt;&gt; a synthetic construct that passes (or more to the ultimate point,
</em><br>
<em>&gt;&gt; surpasses) the Turing Test is not a problem of producing a 
</em><br>
<em>&gt;&gt; consciousness -- it is a problem of producing a black box 
</em><br>
<em>&gt;&gt; that has a particularly observable external behavior.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; (Indeed, one might easily argue that, from the point of view 
</em><br>
<em>&gt;&gt; of the Friendly AI people, it is unnecessary that the god 
</em><br>
<em>&gt;&gt; they wish to create be conscious so long as it acts as though 
</em><br>
<em>&gt;&gt; it has a conscience, whether it is &quot;aware&quot; of it or not.)
</em><br>
<em>&gt;
</em><br>
<em>&gt; OK. Again...
</em><br>
<em>&gt;
</em><br>
<em>&gt; That the functionalist/computationalist approach is the one true path to
</em><br>
<em>&gt; this 'god' is the tacit assumption by all computer science.
</em><br>
<em>&gt; But..........
</em><br>
<p>I said nothing of the sort. I merely said that the Friendly AI folks
<br>
seek function in their construct. Whether that function requires
<br>
certain things (like consciousness in the Friendly AI) is irrelevant
<br>
to their goal, which is not to construct a conscious god but to
<br>
construct a god.
<br>
<p><em>&gt; Where is the proof it will/will not 'understand' what it is like to be
</em><br>
<em>&gt; us? 
</em><br>
<em>&gt; Where is the proof it will/will not have a conception of 'friendly'?
</em><br>
<em>&gt; Where is the proof it it will even know it is there?
</em><br>
<p>Who said it would or would not? I merely noted that it would be
<br>
irrelevant to the goal if it did or did not.
<br>
<p><em>&gt; These proofs do not exist. Yet somehow you bow to the great Turing test
</em><br>
<em>&gt; as if it was what is needed to be completely satisfied that real
</em><br>
<em>&gt; understanding exists in an artefact.
</em><br>
<p>I think you miss my point completely. I did not say (in this
<br>
discussion) that a Turing test proves anything about internal
<br>
experience at all -- merely that if something passes a Turing test it
<br>
is externally/functionally behaving as though it were intelligent. I
<br>
leave alone the question of the construct's internal experience
<br>
entirely.
<br>
<p>Have you ever read &quot;The Unfortunate Dualist&quot; by Smullyan, by the way?
<br>
<p><em>&gt; Computer science/AI always assumes, again tacitly, that the humble quale
</em><br>
<em>&gt; is optional and/or emergent from representational complexity in any
</em><br>
<em>&gt; form.
</em><br>
<p>I don't believe any such assumption is &quot;always&quot; made. Perhaps you
<br>
could provide us with a proof of this?
<br>
<p><em>&gt; If I remember correctly Searle has backed off from the 'biology only'
</em><br>
<em>&gt; stance (chinese room era) to a  non-biological matter-as-computation
</em><br>
<em>&gt; stance. Brain matter is only proven sufficient, not necessary.
</em><br>
<p>But he's proven so well that nothing is sufficient already, hasn't he?
<br>
I mean, there is no way that you could possibly be conscious, either,
<br>
if he's correct.
<br>
<p><em>&gt; Is &quot;computer science&quot; even science?
</em><br>
<p>Can one write haiku about toothbrushes?
<br>
<p><p>Perry
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7310.html">Randall Randall: "Re: Edge.org: Jaron Lanier"</a>
<li><strong>Previous message:</strong> <a href="7308.html">Colin: "RE: Edge.org: Jaron Lanier"</a>
<li><strong>In reply to:</strong> <a href="7308.html">Colin: "RE: Edge.org: Jaron Lanier"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7310.html">Randall Randall: "Re: Edge.org: Jaron Lanier"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7309">[ date ]</a>
<a href="index.html#7309">[ thread ]</a>
<a href="subject.html#7309">[ subject ]</a>
<a href="author.html#7309">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:43 MDT
</em></small></p>
</body>
</html>
