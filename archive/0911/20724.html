<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Value of a machine that acts like a human brain (was Re: [sl4] The Jaguar Supercomputer)</title>
<meta name="Author" content="Tim Freeman (tim@fungible.com)">
<meta name="Subject" content="Value of a machine that acts like a human brain (was Re: [sl4] The Jaguar Supercomputer)">
<meta name="Date" content="2009-11-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Value of a machine that acts like a human brain (was Re: [sl4] The Jaguar Supercomputer)</h1>
<!-- received="Thu Nov 26 10:34:18 2009" -->
<!-- isoreceived="20091126173418" -->
<!-- sent="Thu, 26 Nov 2009 09:03:38 -0700" -->
<!-- isosent="20091126160338" -->
<!-- name="Tim Freeman" -->
<!-- email="tim@fungible.com" -->
<!-- subject="Value of a machine that acts like a human brain (was Re: [sl4] The Jaguar Supercomputer)" -->
<!-- id="20091126173417.21750D2973@fungible.com" -->
<!-- inreplyto="44D3E1F0-3CEE-4CCA-98E5-BBE6763B4046@gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tim Freeman (<a href="mailto:tim@fungible.com?Subject=Re:%20Value%20of%20a%20machine%20that%20acts%20like%20a%20human%20brain%20(was%20Re:%20[sl4]%20The%20Jaguar%20Supercomputer)"><em>tim@fungible.com</em></a>)<br>
<strong>Date:</strong> Thu Nov 26 2009 - 09:03:38 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="20725.html">Charles Hixson: "Re: [sl4] Re: goals of AI... non physical components of mind != soul"</a>
<li><strong>Previous message:</strong> <a href="20723.html">Toby Weston: "Re: [sl4] Re: goals of AI... non physical components of mind != soul"</a>
<li><strong>In reply to:</strong> <a href="20672.html">Matt Paul: "Re: [sl4] The Jaguar Supercomputer"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20639.html">Bradley Thomas: "[sl4] Scary article about the effect of supposedly &quot;inert&quot; nanoparticles on the human body"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20724">[ date ]</a>
<a href="index.html#20724">[ thread ]</a>
<a href="subject.html#20724">[ subject ]</a>
<a href="author.html#20724">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
From: Matt Paul &lt;<a href="mailto:lizardblue@gmail.com?Subject=Re:%20Value%20of%20a%20machine%20that%20acts%20like%20a%20human%20brain%20(was%20Re:%20[sl4]%20The%20Jaguar%20Supercomputer)">lizardblue@gmail.com</a>&gt;
<br>
<em>&gt;...what exactly the perceived value of the AI you  
</em><br>
<em>&gt;guys discuss is beyond normal scientific desire to understand. I don't  
</em><br>
<em>&gt;see the practical and prudent value of a machine that acts like a  
</em><br>
<em>&gt;human brain.  Fascinating and cool certainly, but I don't see the  
</em><br>
<em>&gt;actual benefits to mankind. I do see many potential problems for  
</em><br>
<em>&gt;mankind though...
</em><br>
<p>Well, a machine that acts like a human brain might be a good thing to
<br>
live in, especially if your own brain stopped working.  Copy the state
<br>
information and then carry on with your life after the failure of the
<br>
organic version.  Some people have philosophical issues with that, but
<br>
they are unlikely to be with us for all that long, so I'll just wait
<br>
them out.  Reality has a way of resolving philosophical disputes.  I
<br>
have had friends who had philosophical issues with cryonics die and
<br>
get buried, so I have seen this principle at work, even though I don't
<br>
like the outcome.  I should have argued more with the guy.  Philosophy
<br>
is an important game, but it drains enthusiasm to play it with people
<br>
who don't take it seriously.
<br>
<p>On the other hand, maybe you're talking about a machine that acts like
<br>
a human brain but is radically different somehow (more intelligent or
<br>
maybe just faster).  I agree that that poses many potential problems
<br>
for mankind.  Groups of humans tend to go nuts and attempt genocide
<br>
every generation or two, and it's especially easy to choose to attack
<br>
a group of humans that are different from you and your group.  The
<br>
ordinary humans are going to be different from the better-or-faster
<br>
uploads, so we will probably lose if things go that way.
<br>
<p>Another option is a machine that is intelligent, but doesn't work like
<br>
a human brain.  Business competition will result in something like
<br>
that being built.  If we get it right, one that likes humans will win,
<br>
and if we get it wrong, one that likes specific corporations will win,
<br>
or maybe one that is insane will win.  Corporations generally have a
<br>
fiduciary responsibility to deliver value to their shareholders, which
<br>
may be other corporations.  If corporations stay in control of society
<br>
and eventually they don't need human employees any more we'll all be
<br>
marginalized and eventually recycled.  It seems to me we'd better make
<br>
sure that a nonhuman AI that likes humans wins.
<br>
<p>Some people imagine a world where there's a stable community of AI's
<br>
with opposing goals, much like we presently have a relatively stable
<br>
world with a bunch of humans with opposing goals.  This is different
<br>
from the winner-takes-all scenarios described in the previous
<br>
paragraph.  Unfortunately, I think it is a winner-take-all game.  If
<br>
two AI's have opposing goals and neither can overpower the other, it
<br>
seems technically feasible and mutually beneficial for them to replace
<br>
themselves with one AI that has a compromise set of goals.  (For the
<br>
purposes of this argument, I count two separate computational nodes
<br>
that have the same utility function as one AI with multiple pieces,
<br>
not as two AI's.)  Humans can't do that so extrapolating from human
<br>
behavior gets the wrong result here, IMO.
<br>
<pre>
-- 
Tim Freeman               <a href="http://www.fungible.com">http://www.fungible.com</a>           <a href="mailto:tim@fungible.com?Subject=Re:%20Value%20of%20a%20machine%20that%20acts%20like%20a%20human%20brain%20(was%20Re:%20[sl4]%20The%20Jaguar%20Supercomputer)">tim@fungible.com</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="20725.html">Charles Hixson: "Re: [sl4] Re: goals of AI... non physical components of mind != soul"</a>
<li><strong>Previous message:</strong> <a href="20723.html">Toby Weston: "Re: [sl4] Re: goals of AI... non physical components of mind != soul"</a>
<li><strong>In reply to:</strong> <a href="20672.html">Matt Paul: "Re: [sl4] The Jaguar Supercomputer"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="20639.html">Bradley Thomas: "[sl4] Scary article about the effect of supposedly &quot;inert&quot; nanoparticles on the human body"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#20724">[ date ]</a>
<a href="index.html#20724">[ thread ]</a>
<a href="subject.html#20724">[ subject ]</a>
<a href="author.html#20724">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
