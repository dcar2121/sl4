<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendliness and blank-slate goal bootstrap</title>
<meta name="Author" content="Metaqualia (metaqualia@mynichi.com)">
<meta name="Subject" content="Re: Friendliness and blank-slate goal bootstrap">
<meta name="Date" content="2004-01-11">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendliness and blank-slate goal bootstrap</h1>
<!-- received="Sun Jan 11 03:58:46 2004" -->
<!-- isoreceived="20040111105846" -->
<!-- sent="Sun, 11 Jan 2004 19:56:51 +0900" -->
<!-- isosent="20040111105651" -->
<!-- name="Metaqualia" -->
<!-- email="metaqualia@mynichi.com" -->
<!-- subject="Re: Friendliness and blank-slate goal bootstrap" -->
<!-- id="094201c3d831$9f0c81e0$1101a8c0@curziolaptop" -->
<!-- charset="windows-1252" -->
<!-- inreplyto="019601c3d80e$86adba60$6501a8c0@dimension" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Metaqualia (<a href="mailto:metaqualia@mynichi.com?Subject=Re:%20Friendliness%20and%20blank-slate%20goal%20bootstrap"><em>metaqualia@mynichi.com</em></a>)<br>
<strong>Date:</strong> Sun Jan 11 2004 - 03:56:51 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7739.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7737.html">Samantha Atkins: "Re: What exactly is &quot;panpsychism&quot;?"</a>
<li><strong>In reply to:</strong> <a href="7734.html">Rafal Smigrodzki: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7725.html">Mark Waser: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7738">[ date ]</a>
<a href="index.html#7738">[ thread ]</a>
<a href="subject.html#7738">[ subject ]</a>
<a href="author.html#7738">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; humans have not been confronted with such a system, or else all of us
</em><br>
would
<br>
<em>&gt; most likely be sharing it (unless the system ejoins its carriers to utmost
</em><br>
<p>I don't go with the idea that &quot;if a good theory existed we'd all share it&quot;.
<br>
After all, the singularity is a very good idea, and look how few people
<br>
think about it. On the contrary, absurd ideas such as religions are widely
<br>
accepted.
<br>
<p><em>&gt; ones involving the statement &quot;Death is morally neutral&quot;.  Such beliefs
</em><br>
might
<br>
<p>Actually I made a mistake when I started talking about death because it is
<br>
one of the least important, most controversial issues and was only likely to
<br>
take attention off the theory itself. Let's let this go for the moment.
<br>
<p><em>&gt; ### Nick Hay got it right: &quot;help sentients, and others that can be helped,
</em><br>
<em>&gt; with respect to their volitions -- maximise self-determination, minimised
</em><br>
<em>&gt; unexpected regret&quot;. I would see achieving stable function  along these
</em><br>
lines
<br>
<p>I know if _feels_ right, but I ask myself, is it really right? Or is it just
<br>
my evolved dislike for coercion to tell me that forcing a creature to do
<br>
anything in any way is evil?
<br>
<p>What about some AI created by some crazy scientist, an AI whose purpose in
<br>
life is beating its head against the wall creating massive negative qualia.
<br>
Would a superior AI not have the moral duty of fixing its cognitive
<br>
framework so it could stop beating its head against the wall if it created
<br>
subjective feelings of headache? And you know a human can be seen as being
<br>
programmed to beat his head against the wall as well, can be seen as
<br>
programmed to do things that will hurt him in a way or the other. So yes
<br>
this is a topic I'd like to see discussed, should an AI force a being into
<br>
happiness or let it decide, it is by no means evident that coercion is worse
<br>
than a bleeding forehead. By discussing about this, we may even reach the
<br>
conclusion that a compromise between the two is showing the beast possible
<br>
alternatives by temporarily altering its cognitive structure, finding a way
<br>
to preserve the memories on switchback, and then letting the machine decide
<br>
by itself AFTER it has seen the alternate universe in which it is not forced
<br>
to beat its head, and perhaps AFTER it has become temporarily smarter (until
<br>
a choice has been made).
<br>
<p><em>&gt; Similarly, I do not have the emotional need to claim objective morality on
</em><br>
<em>&gt; my side if I were to act against a wife-killer - it's enough that I don't
</em><br>
<em>&gt; like it on an emotional level
</em><br>
<p>I don't think I have an emotional need to claim objective morality on my
<br>
side. If anything I have a rational need to claim objective morality on my
<br>
side. But I try to ignore what feels like a need as much as possible,
<br>
because I know it's just garbage. I try to go about the good/evil problem as
<br>
I go about every other problem. If the general case can't be solved, ok, but
<br>
I'm going to try first.
<br>
<p>What you are saying - correct me if I am wrong - sounds like you do not have
<br>
nor desire any rational explanation for WHY you want something. But in that
<br>
case, a probabilistic, logic, formal AI is going to disassemble you, find
<br>
out the exact mechanical causes that trigger your discomfort, dismiss them
<br>
as a purely deterministic process, and then ignore the wife-killer, because
<br>
it will KNOW that killing is a physical process with no further connotations
<br>
either good or bad.
<br>
<p>Of course you can argue: we can program it so that it will hold dear my
<br>
opinions as human creator. But is that friendly or an elaborate hardcoding
<br>
of ungrounded moral values?
<br>
<p><em>&gt; No, really, you don't need to have the one and absolute Truth to reject
</em><br>
some
<br>
<em>&gt; acts as wrong. An appeal to shared volition, some game-theoretic
</em><br>
<p>That sounds like saying, you don't have to know all that much math to prove
<br>
that 1+1=2, I like it on an emotional level so that's plenty. I'd like more
<br>
justifications when I choose between life and death, not less, than when I
<br>
choose between 2 and 3.
<br>
<p><em>&gt; ### Of course, coming up with an AI that would satisfy the moral
</em><br>
intuitions
<br>
<em>&gt; of every single member of our species appears to be an impossible
</em><br>
<p>agreed.
<br>
<p><em>&gt; undertaking - I see the goal of FriendlyAI research as merely building a
</em><br>
<em>&gt; device which will stably perform according to the scheme summarized by
</em><br>
<em>&gt; Nick - the simplified essence of a complex system of moral reasoning
</em><br>
<em>&gt; embedded in the brains of many humans, including mine -
</em><br>
<p>We have different definitions then of what kind of friendliness is
<br>
acceptable after the singularity, which explains a lot of our smaller
<br>
disagreements. There is a different between &quot;don't do something I wouldn't
<br>
do&quot; and &quot;don't do something I wouldn't do if I was as smart and altruistic
<br>
as smart and altruistic can possibly get&quot;. I guess we _could_ settle
<br>
temporarily for the first case, but that's just because I think you and I
<br>
share enough moral memes that unacceptable moral wrongs will not happen even
<br>
if only your brain was scanned and incorporated into the AI. But this is
<br>
pure coincidence. If I was born in India, or on Alpha Centauri, I probably
<br>
wouldn't agree with anything your moral memes dictate. But I might still
<br>
agree that negative qualia suck; I think anyone should agree no matter what
<br>
planet they are from.
<br>
<p>Isn't this a better way? Define ultimate good and evil in absolute terms.
<br>
Find out what path we took to get here. Set the AI on the same path, with
<br>
our certainty as interim goal. Then let it develop new theories. Become
<br>
better at morality than _we_ are, then accept the actions that will result
<br>
from a higher morality rather than impose our lower morality for strict
<br>
personal gain.
<br>
<p><em>&gt; ### It is an interesting hypothesis, postulating that a stable expression
</em><br>
of
<br>
<em>&gt; Friendliness (recognized as such by rational humans), requires the
</em><br>
existence
<br>
<em>&gt; of qualia in the AI. What it has to do with objective morality eludes me.
</em><br>
<p>I hope the above clarified.
<br>
<p>mq
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7739.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7737.html">Samantha Atkins: "Re: What exactly is &quot;panpsychism&quot;?"</a>
<li><strong>In reply to:</strong> <a href="7734.html">Rafal Smigrodzki: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7725.html">Mark Waser: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7738">[ date ]</a>
<a href="index.html#7738">[ thread ]</a>
<a href="subject.html#7738">[ subject ]</a>
<a href="author.html#7738">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
