<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: An essay I just wrote on the Singularity.</title>
<meta name="Author" content="Perry E. Metzger (perry@piermont.com)">
<meta name="Subject" content="Re: An essay I just wrote on the Singularity.">
<meta name="Date" content="2004-01-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: An essay I just wrote on the Singularity.</h1>
<!-- received="Fri Jan  2 13:43:20 2004" -->
<!-- isoreceived="20040102204320" -->
<!-- sent="Fri, 02 Jan 2004 15:43:18 -0500" -->
<!-- isosent="20040102204318" -->
<!-- name="Perry E. Metzger" -->
<!-- email="perry@piermont.com" -->
<!-- subject="Re: An essay I just wrote on the Singularity." -->
<!-- id="87oetmrsih.fsf@snark.piermont.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20040102014612.68113.qmail@web11706.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Perry E. Metzger (<a href="mailto:perry@piermont.com?Subject=Re:%20An%20essay%20I%20just%20wrote%20on%20the%20Singularity."><em>perry@piermont.com</em></a>)<br>
<strong>Date:</strong> Fri Jan 02 2004 - 13:43:18 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7483.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7481.html">Lawrence Foard: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>In reply to:</strong> <a href="7451.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7515.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="7515.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7482">[ date ]</a>
<a href="index.html#7482">[ thread ]</a>
<a href="subject.html#7482">[ subject ]</a>
<a href="author.html#7482">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Tommy McCabe &lt;<a href="mailto:rocketjet314@yahoo.com?Subject=Re:%20An%20essay%20I%20just%20wrote%20on%20the%20Singularity.">rocketjet314@yahoo.com</a>&gt; writes:
<br>
<em>&gt;&gt; So one can have AI. I don't dispute that. What I'm
</em><br>
<em>&gt;&gt; talking about is
</em><br>
<em>&gt;&gt; &quot;Friendly&quot; AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Humans can, obviously not must, but can be altruistic,
</em><br>
<em>&gt; the human equivalent of Friendliness. And if one can
</em><br>
<em>&gt; do it in DNA, one can do it in code.
</em><br>
<p>Can they? I am not sure that they can, in the sense of leaving behind
<br>
large numbers of copies of their genes if they behave in a
<br>
consistently altruistic manner.
<br>
<p>BTW, I am not referring to aiding siblings or such -- Dawkins has
<br>
excellent arguments for why that isn't really altruism -- and I'm not
<br>
talking about casual giving to charity or such activities.
<br>
<p>What I'm talking about is consistently aiding strangers in deference
<br>
to your own children, or other such activities. I'm far from sure that
<br>
such behavior isn't powerfully selected against, and one sees very
<br>
little of it in our society, so I'm not sure that it hasn't in fact
<br>
been evolved out.
<br>
<p><em>&gt;&gt; &gt; And if you have Friendly human-equivalent AI,
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt; You've taken a leap. Step back. Just because we know
</em><br>
<em>&gt;&gt; we can build AI
</em><br>
<em>&gt;&gt; doesn't mean we know we can build &quot;Friendly&quot; AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Again, there are humans which are very friendly, and
</em><br>
<em>&gt; humans weren't built with friendliness in mind at all.
</em><br>
<p>I don't think there are many humans who are friendly the way &quot;Friendly
<br>
AI&quot; has to be. A &quot;Friendly AI&quot; has to favor the preservation of other
<br>
creatures who are not members of its line (humans and their
<br>
descendents) over its own.
<br>
<p><em>&gt;&gt; &gt;&gt; There are several problems here, including the fact that there
</em><br>
<em>&gt;&gt; &gt;&gt; is no absolute morality (and thus no way to universally
</em><br>
<em>&gt;&gt; &gt;&gt; determine &quot;the good&quot;),
</em><br>
<em>&gt;&gt; &gt;
</em><br>
<em>&gt;&gt; &gt; This is the postition of subjective morality, which is far from
</em><br>
<em>&gt;&gt; &gt; proven. It's not a 'fact', it is a possibility.
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt; It is unproven, for the exact same reason that the non-existence of
</em><br>
<em>&gt;&gt; God is unproven and indeed unprovable -- I can come up with all
</em><br>
<em>&gt;&gt; sorts of non-falsifiable scenarios in which a God could
</em><br>
<em>&gt;&gt; exist. However, an absolute morality requires a bunch of
</em><br>
<em>&gt;&gt; assumptions -- again, non-falsifiable assumptions. As a good
</em><br>
<em>&gt;&gt; Popperian, depending on things that are non-falsifiable rings alarm
</em><br>
<em>&gt;&gt; bells in my head.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Perhaps I should clarify that: subjective morality is
</em><br>
<em>&gt; not only unproven, it is nowhere near certain. Neither
</em><br>
<em>&gt; is objective morality. The matter is still up for
</em><br>
<em>&gt; debate. A good Friendliness design should be
</em><br>
<em>&gt; compatible with either.
</em><br>
<p>1) I argue quite strongly that there is no objective morality. You
<br>
cannot find answers to all &quot;moral questions&quot; by making inquiry to
<br>
some sort of moral oracle algorithm. Indeed, the very notion of
<br>
&quot;morality&quot; is disputed -- you will find plenty of people who don't
<br>
think they have any moral obligations at all!
<br>
&nbsp;&nbsp;&nbsp;
<br>
Taking a step past that, though, it is trivially seen that the bulk of
<br>
the population does not share a common moral code, and that even those
<br>
portions which they claim to hold to they don't generally follow. Even
<br>
the people here on this mailing list will substantially agree on major
<br>
points of &quot;morality&quot;.
<br>
<p>There is, on top of that, no known way to establish what is 'morally
<br>
correct'. You and I can easily ascertain the &quot;correct&quot; speed of light
<br>
in a vacuum (to within a small error boun) with straightforward
<br>
scientific tools. There is, however, no experiment we can conduct to
<br>
determine the &quot;correct&quot; behavior in the face of a moral dilemma.
<br>
<p>By the way, one shudders at what would happen if one could actually
<br>
build superhuman entities to try to *enforce* morality. See Greg
<br>
Bear's &quot;Strength of Stones&quot; for one scenario about where that
<br>
foolishness might lead.
<br>
<p><em>&gt;&gt; &gt;&gt; that it is not clear that a construct like this would be able to
</em><br>
<em>&gt;&gt; &gt;&gt; battle it out effectively against other constructs from
</em><br>
<em>&gt;&gt; &gt;&gt; societies that do not construct Friendly AIs (or indeed that the
</em><br>
<em>&gt;&gt; &gt;&gt; winner in the universe won't be the societies that produce the
</em><br>
<em>&gt;&gt; &gt;&gt; meanest, baddest-assed intelligences rather than the friendliest
</em><br>
<em>&gt;&gt; &gt;&gt; -- see evolution on earth), etc.
</em><br>
<em>&gt;&gt; &gt;
</em><br>
<em>&gt;&gt; &gt; Battle it out? The 'winner'? The 'winner' in this case is the AI
</em><br>
<em>&gt;&gt; &gt; who makes it to superintelligence first.
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt; How do we know that this hasn't already happened elsewhere in the
</em><br>
<em>&gt;&gt; universe? We don't. We just assume (probably correctly) that it
</em><br>
<em>&gt;&gt; hasn't happened on our planet -- but there are all sorts of other
</em><br>
<em>&gt;&gt; planets out there. The Universe is Big. You don't want to build
</em><br>
<em>&gt;&gt; something that will have trouble with Outside Context Problems (as
</em><br>
<em>&gt;&gt; Ian Banks dubbed them).
</em><br>
<em>&gt;
</em><br>
<em>&gt; Another rephrasing: the first superintellgence that
</em><br>
<em>&gt; knows about us.
</em><br>
<p>It doesn't matter who knows about us first. What matters is what
<br>
happens when the other hyperintelligence from the other side of the
<br>
galaxy sends over its probes and it turns out that it isn't nearly as
<br>
&quot;friendly&quot; and has far more resources. Such an intelligence may be the
<br>
*last* thing we encounter in our history.
<br>
<p><em>&gt;&gt; &gt;&gt; Anyway, I find it interesting to speculate on possible constructs
</em><br>
<em>&gt;&gt; &gt;&gt; like The Friendly AI, but not safe to assume that they're going to
</em><br>
<em>&gt;&gt; &gt;&gt; be in one's future.
</em><br>
<em>&gt;&gt; &gt;
</em><br>
<em>&gt;&gt; &gt; Of course you can't assume that there will be a
</em><br>
<em>&gt;&gt; &gt; Singularity caused by a Friendly AI, but I'm pretty
</em><br>
<em>&gt;&gt; &gt; darn sure I want it to happen!
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt; I want roses to grow unbidden from the wood of my
</em><br>
<em>&gt;&gt; writing desk.
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt; Don't speak of desire. Speak of realistic
</em><br>
<em>&gt;&gt; possibility.
</em><br>
<em>&gt;  
</em><br>
<em>&gt; I consider that a realistic possibility. And the
</em><br>
<em>&gt; probability of that happening can be influenced by us.
</em><br>
<p>I'm well aware that you consider the possibility realistic. I
<br>
don't. Chacun a son gout. However, I'm happy to continue explaining
<br>
why I think it would be difficult to guarantee that an AI would be
<br>
&quot;Friendly&quot;.
<br>
<p><em>&gt;&gt; &gt;&gt; The prudent transhumanist considers survival in wide variety of
</em><br>
<em>&gt;&gt; &gt;&gt; scenarios.
</em><br>
<em>&gt;&gt; &gt;
</em><br>
<em>&gt;&gt; &gt; Survival? If the first transhuman is Friendly,
</em><br>
<em>&gt;&gt; &gt; survival is a given,
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt; No, it is not, because it isn't even clear that there will be any
</em><br>
<em>&gt;&gt; way to define &quot;Friendly&quot; well enough. See &quot;no absolute morality&quot;,
</em><br>
<em>&gt;&gt; above.
</em><br>
<em>&gt;
</em><br>
<em>&gt; That is the problem of Friendliness definition, which
</em><br>
<em>&gt; Eli knows a lot better than I do. A hard problem, I admit.
</em><br>
<p>I suspect that for many reasons such a definition is impossible, or at
<br>
least beyond the reach of man.
<br>
<p>Among other problems, we have the fact that no one has ever come up
<br>
with a universally acceptable morality, and we have Rice's Theorem
<br>
staring down the barrel at us too.
<br>
<p><pre>
-- 
Perry E. Metzger		<a href="mailto:perry@piermont.com?Subject=Re:%20An%20essay%20I%20just%20wrote%20on%20the%20Singularity.">perry@piermont.com</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7483.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7481.html">Lawrence Foard: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>In reply to:</strong> <a href="7451.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7515.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="7515.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7482">[ date ]</a>
<a href="index.html#7482">[ thread ]</a>
<a href="subject.html#7482">[ subject ]</a>
<a href="author.html#7482">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:43 MDT
</em></small></p>
</body>
</html>
