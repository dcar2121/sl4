<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Darwinian dynamics unlikely to apply to superintelligence</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Darwinian dynamics unlikely to apply to superintelligence">
<meta name="Date" content="2004-01-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Darwinian dynamics unlikely to apply to superintelligence</h1>
<!-- received="Fri Jan  2 20:31:45 2004" -->
<!-- isoreceived="20040103033145" -->
<!-- sent="Fri, 02 Jan 2004 22:31:32 -0500" -->
<!-- isosent="20040103033132" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Darwinian dynamics unlikely to apply to superintelligence" -->
<!-- id="3FF63794.1090004@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20040102212718.B16063@weidai.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Darwinian%20dynamics%20unlikely%20to%20apply%20to%20superintelligence"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri Jan 02 2004 - 20:31:32 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7528.html">Lawrence Foard: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7526.html">Wei Dai: "Re: qualia-oriented SI (was: &quot;friendly&quot; humans?)"</a>
<li><strong>In reply to:</strong> <a href="7520.html">Wei Dai: "Re: Darwinian dynamics unlikely to apply to superintelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7533.html">Wei Dai: "Re: Darwinian dynamics unlikely to apply to superintelligence"</a>
<li><strong>Reply:</strong> <a href="7533.html">Wei Dai: "Re: Darwinian dynamics unlikely to apply to superintelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7527">[ date ]</a>
<a href="index.html#7527">[ thread ]</a>
<a href="subject.html#7527">[ subject ]</a>
<a href="author.html#7527">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Wei Dai wrote:
<br>
<em>&gt; On Fri, Jan 02, 2004 at 06:07:54PM -0500, Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; This last point is particularly important in understanding why
</em><br>
<em>&gt;&gt; replicator dynamics are unlikely to apply to SIs.  At most, we are
</em><br>
<em>&gt;&gt; likely to see one initial filter in which SIs that halt or fence
</em><br>
<em>&gt;&gt; themselves off in tiny spheres are removed from the cosmic
</em><br>
<em>&gt;&gt; observables.  Almost any utility function I have ever heard proposed
</em><br>
<em>&gt;&gt; will choose to spread across the cosmos and transform matter into
</em><br>
<em>&gt;&gt; either (1) *maximally high-fidelity copies* of the optimization
</em><br>
<em>&gt;&gt; control structure or (2) configurations that fulfill intrinsic
</em><br>
<em>&gt;&gt; utilities.  If the optimization control structure is copied at
</em><br>
<em>&gt;&gt; extremely high fidelity, there are no important heritable differences
</em><br>
<em>&gt;&gt; for natural selection to act on.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; How do you ensure that the optimization control structures *remain* 
</em><br>
<em>&gt; high-fidelity copies over billions of years?
</em><br>
<p>Hm, this problem looks oddly familiar...
<br>
<p>Well, to give it a human answer - just by way of arguing possibility - you 
<br>
could encrypt the original instructions in such a way that a single error 
<br>
scrambles them, store the instructions in multiple places and compare 
<br>
them, keep them in a central redundant 'queen' and transmit them to 
<br>
workers without ever locally storing the instructions in unencrypted 
<br>
form... where have I seen this before?  Oh, yes, the Foresight Guidelines.
<br>
<p>To the Guidelines I will add my own observation that, while it is not 
<br>
possible with expected utility optimizers that work on 'standard' utility 
<br>
functions, once you start adding in Friendliness structure it should be 
<br>
possible to build a utilitarian optimizer that can compare its utility 
<br>
function with those of nearby neighbors, and eliminate errors by 
<br>
redundancy checking, or probabilistically compromise for smooth falloff, 
<br>
rather than, say, fighting a war.  Vide external reference semantics; if 
<br>
an error in the utility function is a serious probability then a 
<br>
probabilistic utility functions can deal with it, and it would be very 
<br>
much in the interest of the original optimizer to create that structure in 
<br>
any regions of matter it optimizes.
<br>
<p>If you only need to do something a quadrillion quadrillion times, no more, 
<br>
then it does not seem too physically expensive to drive the expected error 
<br>
rate down to effectively zero; vide the 10^-64 expected error rates on 
<br>
primitive operations in Nanosystems, with exponentially better expected 
<br>
fidelity achievable by raising the potential energy barrier.
<br>
<p><em>&gt; Each control structure
</em><br>
<em>&gt; will have to process environmental data as well as communications with 
</em><br>
<em>&gt; neighboring control structures and the so called &quot;configurations that 
</em><br>
<em>&gt; fulfill intrinsic utilities&quot; which I presume may be intelligent beings.
</em><br>
<em>&gt; All of these will have local variations. Somehow the SI has to ensure
</em><br>
<em>&gt; that processing these variations does not cause heritable differences.
</em><br>
<p>Or at least, not heritable variations of a kind that we regard as viruses, 
<br>
rather than, say, acceptable personality quirks, or desirable diversity. 
<br>
But even in human terms, what's wrong with encrypting the control block?
<br>
<p><em>&gt; This may or may not be possible, but as I suggested earlier the only
</em><br>
<em>&gt; way it can succeed is if the SI imposes a strict limit on the kinds of
</em><br>
<em>&gt; communications that can occur between neighboring control structures
</em><br>
<em>&gt; and between control structures and the configurations that fulfill
</em><br>
<em>&gt; intrinsic utilities if they are intelligent beings to prevent memes
</em><br>
<em>&gt; from being transmitted.
</em><br>
<p>Humans are hardly top-of-the-line in the cognitive security department. 
<br>
You can hack into us easily enough.  But how do you hack an SI 
<br>
optimization process?  What kind of &quot;memes&quot; are we talking about here? 
<br>
How do they replicate, what do they do; why, if they are destructive and 
<br>
foreseeable, is it impossible to prevent them?  We are not talking about a 
<br>
Windows network.
<br>
<p><em>&gt;&gt; If there were heritable differences, they are not likely to covary
</em><br>
<em>&gt;&gt; with large differences in reproductive fitness, insofar as all the
</em><br>
<em>&gt;&gt; optimization control structures will choose equally to transform
</em><br>
<em>&gt;&gt; nearby matter.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I disagree with this. Whatever limitations the SI imposes on the
</em><br>
<em>&gt; control structures in order to maximize their long term fidelity will
</em><br>
<em>&gt; have a negative effect on their reproductive fitness when in
</em><br>
<em>&gt; competition with replicators that do not have such limits, and
</em><br>
<em>&gt; certainly the removal of these limits is a heritable difference.
</em><br>
<p>Okay, so possibly a Friendly SI expands spherically as (.98T)^3 and an 
<br>
unfriendly SI expands spherically as (.99T)^3, though I don't see why the 
<br>
UFSI would not need to expend an equal amount of effort in ensuring its 
<br>
own fidelity.  Even so, under that assumption it would work out to a 
<br>
constant factor of UFSIs being 3% larger; or a likelihood ratio of 1.03 in 
<br>
favor of observing UFSI (given some prior probability of emergence); or in 
<br>
terms of natural selection, essentially zero selection pressure - and you 
<br>
can't even call it that, because it's not being iterated.  I say again 
<br>
that natural selection is a quantitative pressure that can be calculated 
<br>
given various scenarios, not something that goes from zero to one given 
<br>
the presence of &quot;heritable difference&quot; and so on.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7528.html">Lawrence Foard: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7526.html">Wei Dai: "Re: qualia-oriented SI (was: &quot;friendly&quot; humans?)"</a>
<li><strong>In reply to:</strong> <a href="7520.html">Wei Dai: "Re: Darwinian dynamics unlikely to apply to superintelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7533.html">Wei Dai: "Re: Darwinian dynamics unlikely to apply to superintelligence"</a>
<li><strong>Reply:</strong> <a href="7533.html">Wei Dai: "Re: Darwinian dynamics unlikely to apply to superintelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7527">[ date ]</a>
<a href="index.html#7527">[ thread ]</a>
<a href="subject.html#7527">[ subject ]</a>
<a href="author.html#7527">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:43 MDT
</em></small></p>
</body>
</html>
