<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=Windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendliness and blank-slate goal bootstrap</title>
<meta name="Author" content="Rafal Smigrodzki (rafal@smigrodzki.org)">
<meta name="Subject" content="Re: Friendliness and blank-slate goal bootstrap">
<meta name="Date" content="2004-01-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendliness and blank-slate goal bootstrap</h1>
<!-- received="Sat Jan 10 23:45:31 2004" -->
<!-- isoreceived="20040111064531" -->
<!-- sent="Sun, 11 Jan 2004 01:45:37 -0500" -->
<!-- isosent="20040111064537" -->
<!-- name="Rafal Smigrodzki" -->
<!-- email="rafal@smigrodzki.org" -->
<!-- subject="Re: Friendliness and blank-slate goal bootstrap" -->
<!-- id="019601c3d80e$86adba60$6501a8c0@dimension" -->
<!-- charset="Windows-1252" -->
<!-- inreplyto="08cc01c3d7fd$cc959a40$1101a8c0@curziolaptop" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Rafal Smigrodzki (<a href="mailto:rafal@smigrodzki.org?Subject=Re:%20Friendliness%20and%20blank-slate%20goal%20bootstrap"><em>rafal@smigrodzki.org</em></a>)<br>
<strong>Date:</strong> Sat Jan 10 2004 - 23:45:37 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7735.html">Samantha Atkins: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7733.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>In reply to:</strong> <a href="7733.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7738.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7738.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7734">[ date ]</a>
<a href="index.html#7734">[ thread ]</a>
<a href="subject.html#7734">[ subject ]</a>
<a href="author.html#7734">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
----- Original Message ----- 
<br>
From: &quot;Metaqualia&quot; &lt;<a href="mailto:metaqualia@mynichi.com?Subject=Re:%20Friendliness%20and%20blank-slate%20goal%20bootstrap">metaqualia@mynichi.com</a>&gt;
<br>
<p><p><em>&gt; &gt; the belief in an objective morality to which you are privy, a belief
</em><br>
<em>&gt; &gt; infrequent at the sl4-level.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If space and time can be relative, then morality can be absolute, no? :)
</em><br>
<em>&gt;
</em><br>
<em>&gt; You have not explained why an objective morality is impossible to
</em><br>
formulate.
<br>
<em>&gt; You have just said that entertaining such thoughts is 'unworthy' just like
</em><br>
<em>&gt; heliocentrism was &quot;unworthy of the light of day&quot;.
</em><br>
<p>### I do not know if an objective morality (i.e. a system of moral reasoning
<br>
which forces itself on any rational, intelligent mind by its sheer
<br>
convincing inevitability, regardless of the mind's previous built-in
<br>
judgments) can or cannot be found. At the present level of our intelligence,
<br>
humans have not been confronted with such a system, or else all of us would
<br>
most likely be sharing it (unless the system ejoins its carriers to utmost
<br>
secrecy). Whether cognitively enhanced beings will discover such a system is
<br>
not likely to be legitimately predicted by us, either. However, while
<br>
descriptions of purportedly objective moral systems abound in some social
<br>
strata, to my knowledge they are distinctly unpopular on sl4, especially
<br>
ones involving the statement &quot;Death is morally neutral&quot;.  Such beliefs might
<br>
be worthy of sl4 discussion if supported by a formidable array of
<br>
sophisticated arguments, but so far such arguments have not been presented.
<br>
<p>-----------------------------------------
<br>
<p><em>&gt; It's ok to choose creating a morally-reasoning AI versus creating an AI
</em><br>
with
<br>
<em>&gt; a preset moral. But at this point, we do not even have an arrow. Go to the
</em><br>
<em>&gt; east, they will tell you that not killing your wife when she commits
</em><br>
<em>&gt; adultery is immoral. They will tell you that killing cows is more immoral
</em><br>
<em>&gt; than killing monkeys. What is an AI gonna make of all this? Average out
</em><br>
all
<br>
<em>&gt; moral/immoral statements? Would that make barbie dolls slightly more
</em><br>
immoral
<br>
<em>&gt; than killing your political enemy? Can't make a saint when you maintain
</em><br>
<em>&gt; right and wrong are relative!
</em><br>
<p>### Nick Hay got it right: &quot;help sentients, and others that can be helped,
<br>
with respect to their volitions -- maximise self-determination, minimised
<br>
unexpected regret&quot;. I would see achieving stable function  along these lines
<br>
as the engineering challenge before Ben, Peter and Eliezer, since this
<br>
formulation feels (&quot;feels&quot; -  I am waving the red-flag word of subjectivity
<br>
here) congenial to me, and whether someone could call it objectively right
<br>
or dismiss as objectively wrong carries little weight with me (unless such
<br>
pronouncement came from someone whose intellect I trust more than my own).
<br>
Similarly, I do not have the emotional need to claim objective morality on
<br>
my side if I were to act against a wife-killer - it's enough that I don't
<br>
like it on an emotional level and that I know (intellectually) that allowing
<br>
such deeds reduces the likelihood of my wishes coming true.
<br>
<p>No, really, you don't need to have the one and absolute Truth to reject some
<br>
acts as wrong. An appeal to shared volition, some game-theoretic
<br>
simulations, and it is possible to come up with working solutions to moral
<br>
dilemmas, including a principled rejection of cultural moral relativism.
<br>
<p>-------------------------------------------------
<br>
<em>&gt;
</em><br>
<em>&gt; There is confusion; as a species, we know nothing. How can we expect to
</em><br>
<em>&gt; create an AI who reasons about morality when we, the creators, can't agree
</em><br>
<em>&gt; on what this morality is?
</em><br>
<p>### Of course, coming up with an AI that would satisfy the moral intuitions
<br>
of every single member of our species appears to be an impossible
<br>
undertaking - I see the goal of FriendlyAI research as merely building a
<br>
device which will stably perform according to the scheme summarized by
<br>
Nick - the simplified essence of a complex system of moral reasoning
<br>
embedded in the brains of many humans, including mine - which is why I
<br>
enormously like the idea of Friendly AI, as expounded by Eliezer. No doubt
<br>
some will claim Ben is Satan himself, when he unveils his BabyAI in a few
<br>
years - I don't feel much of a need to beg them for forgiveness (although
<br>
some bullet-proofing might be a good idea).
<br>
<p>----------------------------
<br>
<p><em>&gt; I have already given an example of something that we may have completely
</em><br>
<em>&gt; missed  because objective morality has not been talked about: &quot;the AI may
</em><br>
<em>&gt; not reach any kind of moral belief after all, unless it can experience
</em><br>
<em>&gt; qualia&quot;. Without experiencing qualia, a rigorously logical system will
</em><br>
come
<br>
<em>&gt; to hard determinism and interpret your screams as a physical reaction of
</em><br>
<em>&gt; neuron activation and air flow from your mouth. A morally neutral physical
</em><br>
<em>&gt; process. Do we know better? Then let's find some _valid_ justifications
</em><br>
for
<br>
<em>&gt; it.
</em><br>
<p>### It is an interesting hypothesis, postulating that a stable expression of
<br>
Friendliness (recognized as such by rational humans), requires the existence
<br>
of qualia in the AI. What it has to do with objective morality eludes me.
<br>
Would you mind telling me what do you mean by objective morality or &quot;real&quot;
<br>
values?
<br>
<p>Rafal
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7735.html">Samantha Atkins: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7733.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>In reply to:</strong> <a href="7733.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7738.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7738.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7734">[ date ]</a>
<a href="index.html#7734">[ thread ]</a>
<a href="subject.html#7734">[ subject ]</a>
<a href="author.html#7734">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
