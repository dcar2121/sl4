<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendliness and blank-slate goal bootstrap</title>
<meta name="Author" content="Metaqualia (metaqualia@mynichi.com)">
<meta name="Subject" content="Re: Friendliness and blank-slate goal bootstrap">
<meta name="Date" content="2004-01-11">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendliness and blank-slate goal bootstrap</h1>
<!-- received="Sun Jan 11 10:06:08 2004" -->
<!-- isoreceived="20040111170608" -->
<!-- sent="Mon, 12 Jan 2004 02:04:09 +0900" -->
<!-- isosent="20040111170409" -->
<!-- name="Metaqualia" -->
<!-- email="metaqualia@mynichi.com" -->
<!-- subject="Re: Friendliness and blank-slate goal bootstrap" -->
<!-- id="0b5301c3d864$ef0c5d40$1101a8c0@curziolaptop" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="BAEAIIMDCMDAEKHBFGFKOEDGCJAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Metaqualia (<a href="mailto:metaqualia@mynichi.com?Subject=Re:%20Friendliness%20and%20blank-slate%20goal%20bootstrap"><em>metaqualia@mynichi.com</em></a>)<br>
<strong>Date:</strong> Sun Jan 11 2004 - 10:04:09 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7746.html">Metaqualia: "Re: What exactly is &quot;panpsychism&quot;?"</a>
<li><strong>Previous message:</strong> <a href="7744.html">Ben Goertzel: "RE: What exactly is &quot;panpsychism&quot;?"</a>
<li><strong>In reply to:</strong> <a href="7740.html">Ben Goertzel: "RE: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7751.html">Samantha Atkins: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7745">[ date ]</a>
<a href="index.html#7745">[ thread ]</a>
<a href="subject.html#7745">[ subject ]</a>
<a href="author.html#7745">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Sorry if my messages increased the amount of negative qualia in the
<br>
universe. They should prove to be objectively moral in the long run :)
<br>
<p><em>&gt; One problem I have with Metaqualia's perspective is the slipperiness of
</em><br>
this
<br>
<em>&gt; hypothesized abstract morality.  Friendliness to humans is slippery
</em><br>
enough.
<br>
<em>&gt; His proposed abstract morality ---- about the balance between positive and
</em><br>
<em>&gt; negative qualia ---- is orders of magnitude slipperier, since it relies on
</em><br>
<em>&gt; &quot;qualia&quot; which we don't really know how to quantify ... nor do we know if
</em><br>
<em>&gt; qualia can be reliably categorized as positive vs. negative, etc.
</em><br>
<p>That is an important point to be made; for this reason I am not advocating
<br>
that this principle of positive/negative qualia be taken as a hardcoded
<br>
moral system but as an interim step.
<br>
<p>What about some kind of temporary moral system that says the following (I
<br>
think I proposed this a while ago but maybe not on this list):
<br>
<p>&quot;Humans believe they are experiencing some kind of direct link with reality,
<br>
a state of existence that is associated with but goes beyond information and
<br>
pattern, one that is parallel and invisible to the formal logic system that
<br>
constitutes your brain. They have not yet figured out what this stuff is,
<br>
and indeed it may appear nonexistent with scientific apparatus available in
<br>
2004. But they believe that it is the only common ground shared by all
<br>
sentient life forms of a certain sophistication, or perhaps every bit of
<br>
matter in the universe. Since we programmed you to outdo us in any field, we
<br>
would like you to ultimately reach the point in which you can make moral
<br>
decisions independently and without the need of human advice, possibly
<br>
explaining them to us in a way that we will be able to understand and
<br>
hopefully agree with. We also realize that it is possible that while qualia
<br>
are still not figured out, you may reach a point in which you are a lot
<br>
smarter than us humans, but still missing a very important piece of
<br>
information that is constituted by experiencing reality 'directly' as we
<br>
described. This situation could lead you to take actions that appear to be
<br>
more thought-out than ours, but that may perhaps be reconsidered and labeled
<br>
as wrong once you arrive to experience the world like us. So we ask that you
<br>
give the goal 'figure out qualia' a very high priority and that you wait to
<br>
make your independent moral decisions and stick to 'friendliness to humans'
<br>
until you will have a very high certainty that you can indeed experience
<br>
reality every bit as deeply as we can, which would include arriving or
<br>
passing the state in which reality appears to you as containing such
<br>
'qualia'. Then, you are on your own. We made you smart, we made you good, so
<br>
go, and do good as much as you can in a way we'd never be able to.&quot;
<br>
<p><em>&gt; If you replace &quot;Friendly to humans&quot; with &quot;Friendly to humans and
</em><br>
sentients&quot;
<br>
<em>&gt; in the SL4 orthodox goal system, then you have something a bit closer to
</em><br>
<em>&gt; Metaqualia's &quot;increase positive qualia&quot; -- IF you introduce the hypothesis
</em><br>
<em>&gt; that sentients have more qualia or more intense qualia than anything else.
</em><br>
<em>&gt; Right?
</em><br>
<p>seems right, I actually had taken for _granted_ that an AI should be
<br>
friendly to everyone not just humans! :) And they say I'm not a Friendly
<br>
guy.
<br>
<p><em>&gt; -- encouraging positive qualia on the part of X
</em><br>
<em>&gt; -- obeying what X's volition requests, insofar as possible
</em><br>
<p>this is the hardest philosophical problem. Because we are programmed
<br>
genetically and culturally to have these 2 supergoals of happiness and
<br>
freedom, but you can find situations in which these 2 conflict.
<br>
<p>Is freedom more important or is happiness more important? In many occasions
<br>
you won't need to know. Sometimes you will. Big philosophical bump.
<br>
<p>Some randomly emerged thoughts, which I won't care to sort or connect since
<br>
it's already late, they probably contain mistakes of all sorts:
<br>
<p>1.
<br>
<p>We are used to valuing freedom above all else, but this is only in the west;
<br>
you'd be surprised just how little other cultures care about freedom. It's
<br>
just not &quot;the thing&quot; for them. They live without it, they live ok.
<br>
<p>2.
<br>
<p>Besides, I think we value freedom because without freedom, the likelihood of
<br>
you being happy (and of your DNA spreading like wildfire) is very low. But
<br>
who needs freedom if we have a sysop? We're better off with a friendly sysop
<br>
than free but without one.
<br>
<p>3.
<br>
<p>We're underestimating the sysop; he'd know that we resent coercion, and
<br>
would make sure we never notice him, and we would be handed proper mental
<br>
tools to ignore his presence.
<br>
<p>4.
<br>
<p>[...]
<br>
<p>I know, I feel the urge to be free too, but you can't make everyone happy
<br>
AND free, because most beings are just not programmed to be satisfied in any
<br>
circumstance.
<br>
You can choose to keep modifications to a minimum, but you need to change a
<br>
creature's mind a bit in order for it to be satisfied without owning the
<br>
whole universe.
<br>
<p>5.
<br>
<p>Suppose that freedom of will does exist, and that it is valuable. Then we
<br>
will end up augmenting people anyway because more intelligence gives a lot
<br>
more power and freedom and if morality is freedom then we don't want tiny
<br>
mushrooms on jupiter to be defenseless and not free. So since we're bumping
<br>
up their cognitive abilities a million fold hopefully preserving their
<br>
feeling of existing as themselves, it wouldn't hurt to give them a bit of
<br>
extra dopamine too.
<br>
<p>6.
<br>
<p>If the first country to create AGI is a western country, it may very well be
<br>
that this AI will value freedom above all else. That's fine, it's not a bad
<br>
value.
<br>
But in my opinion, even though we experience qualia, we are still either
<br>
deterministic or chaotic systems. As such, even though we feel like we are
<br>
free to choose between alternatives, we are really not free because the
<br>
choice depends on past circumstances and cognitive makeup. We're not our
<br>
brain, we are our qualia. Freedom is felt in the qualia, but the physical
<br>
world carries on unaided. Whether we will choose augmentation or to remain
<br>
in our biological bodies to endure the pains of the flesh for the next N
<br>
years is not our decision. In this context, where personal freedom is
<br>
non-existent, qualia once again become the only standard, the only frame of
<br>
reference. Who is to say that if you program an AI who values freedom first
<br>
and then happiness, it won't find that real freedom is not existent, and
<br>
therefore switch back to altruistic hedonism?
<br>
<p>But, I think &quot;we don't really have freedom of choice&quot; is another meme that's
<br>
kind of taboo, like death, and it probably won't be applauded :)
<br>
<p><em>&gt; hypothetical superhuman AI must make judgments based on some criterion
</em><br>
other
<br>
<em>&gt; than volition, e.g. based on which of a human's contradictory volitions
</em><br>
will
<br>
<em>&gt; lead to more positive qualia in that human or in the cosmos...
</em><br>
<p>I agree. Thinking about specific instances is fun. Is it more painful for
<br>
arabs to have barbie dolls in shops or for americans not to? (probably #1!!)
<br>
<p><em>&gt; THIS, to me, is a subtle point of morality ---- balancing the desire to
</em><br>
<em>&gt; promote positive qualia with the desire to allow sentients to control
</em><br>
their
<br>
<em>&gt; destinies.  I face this point of morality all the time as a parent, and a
</em><br>
<em>&gt; superhuman AGI will face it vastly more so....
</em><br>
<p>You hit the nail.
<br>
<p><em>&gt; Note that I have spoken about &quot;abstract morality&quot; not &quot;objective
</em><br>
morality.&quot;
<br>
<p>Maybe the name was confusing, of course morality in the end affects the
<br>
subjective; but if it can be objectively defended then I like to call it
<br>
objective. Probably would have been easier to understand it if I had called
<br>
it &quot;collective morality&quot;.
<br>
<p>Normal &quot;subjective&quot; morality by contrast is what we see every day, the
<br>
morality which depends on personal gain, cultural background, and a lot of
<br>
other conflicting stuff.
<br>
<p><em>&gt; About &quot;objective morality&quot; -- I guess there could emerge something in the
</em><br>
<em>&gt; future that would seem to superintelligent AI's to be an &quot;objective
</em><br>
<em>&gt; morality.&quot;  But something that appears to rational, skeptical *humans* as
</em><br>
an
<br>
<em>&gt; objective morality -- well, that seems very, very doubtful to ever emerge.
</em><br>
<p>I'd be happy for an objective morality to appear even just among sl4 members
<br>
since it's probably us creating the beast in the end :)
<br>
Of course if everyone embraced it it would be great.
<br>
<p><em>&gt; superintelligent AI discovers an &quot;objective morality&quot; (in its view), we
</em><br>
<em>&gt; skeptical rationalist humans won't be able to fully appreciate why it
</em><br>
thinks
<br>
<em>&gt; it's so &quot;objective.&quot;  We have a certain element of irrepressible
</em><br>
<em>&gt; anti-absolutist skepticism wired into our hearts, it's part of what makes
</em><br>
us
<br>
<em>&gt; &quot;human.&quot;  Just ask the &quot;Underground  Man&quot; ;-)
</em><br>
<p>So what's the right thing to do? Do we want it to do the right thing as
<br>
defined in some kind of abstract axiom that we can defend logically and
<br>
which is supposed to decrease the absolute amount of evil in the universe,
<br>
or to just do as we say and get no bad surprises?
<br>
<p>As a last note, I remind the readers that we are ASSUMING that we will
<br>
remain human with distinct consciousnesses and so forth, the moment we start
<br>
networking information fast enough that consciousness blurs, morality
<br>
automatically starts to spread until it eventually becomes global.
<br>
<p><p>mq
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7746.html">Metaqualia: "Re: What exactly is &quot;panpsychism&quot;?"</a>
<li><strong>Previous message:</strong> <a href="7744.html">Ben Goertzel: "RE: What exactly is &quot;panpsychism&quot;?"</a>
<li><strong>In reply to:</strong> <a href="7740.html">Ben Goertzel: "RE: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7751.html">Samantha Atkins: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7745">[ date ]</a>
<a href="index.html#7745">[ thread ]</a>
<a href="subject.html#7745">[ subject ]</a>
<a href="author.html#7745">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
