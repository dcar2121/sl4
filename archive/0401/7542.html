<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: An essay I just wrote on the Singularity.</title>
<meta name="Author" content="Perry E. Metzger (perry@piermont.com)">
<meta name="Subject" content="Re: An essay I just wrote on the Singularity.">
<meta name="Date" content="2004-01-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: An essay I just wrote on the Singularity.</h1>
<!-- received="Sat Jan  3 00:20:32 2004" -->
<!-- isoreceived="20040103072032" -->
<!-- sent="Sat, 03 Jan 2004 02:20:29 -0500" -->
<!-- isosent="20040103072029" -->
<!-- name="Perry E. Metzger" -->
<!-- email="perry@piermont.com" -->
<!-- subject="Re: An essay I just wrote on the Singularity." -->
<!-- id="87y8splcqq.fsf@snark.piermont.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20040102184113.6ab4aef6.samantha@objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Perry E. Metzger (<a href="mailto:perry@piermont.com?Subject=Re:%20An%20essay%20I%20just%20wrote%20on%20the%20Singularity."><em>perry@piermont.com</em></a>)<br>
<strong>Date:</strong> Sat Jan 03 2004 - 00:20:29 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7543.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7541.html">Lawrence Foard: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>In reply to:</strong> <a href="7522.html">Samantha Atkins: "Re: An essay I just wrote on the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0312/7390.html">Ben Goertzel: "RE: An essay I just wrote on the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7542">[ date ]</a>
<a href="index.html#7542">[ thread ]</a>
<a href="subject.html#7542">[ subject ]</a>
<a href="author.html#7542">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Samantha Atkins &lt;<a href="mailto:samantha@objectent.com?Subject=Re:%20An%20essay%20I%20just%20wrote%20on%20the%20Singularity.">samantha@objectent.com</a>&gt; writes:
<br>
<em>&gt; On Fri, 02 Jan 2004 17:22:40 -0500
</em><br>
<em>&gt; &quot;Perry E. Metzger&quot; &lt;<a href="mailto:perry@piermont.com?Subject=Re:%20An%20essay%20I%20just%20wrote%20on%20the%20Singularity.">perry@piermont.com</a>&gt; wrote:
</em><br>
<em>&gt;&gt; &gt; Dah.  But the point I was attempting to explore is that a definition
</em><br>
<em>&gt;&gt; &gt; of Friendliness that covered the present and immediately foreseeable
</em><br>
<em>&gt;&gt; &gt; situation (friendliness toward humans) might be sufficient to speak
</em><br>
<em>&gt;&gt; &gt; of Friendly AI.  A pan-sentient definition might also be possible
</em><br>
<em>&gt;&gt; &gt; and even natural but may not be required in the first attempt.    So
</em><br>
<em>&gt;&gt; &gt; it is not clear to me that &quot;absolute&quot; or &quot;universal&quot; morality, or
</em><br>
<em>&gt;&gt; &gt; even universal definitions of friendliness are required in order to
</em><br>
<em>&gt;&gt; &gt; meaningfully proceed.
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt; I will grant that it is possible that someone will come up with a
</em><br>
<em>&gt;&gt; working definition of &quot;Friendly&quot; that is good enough, and a way to
</em><br>
<em>&gt;&gt; inculcate it into an AI they are building so deeply that it won't
</em><br>
<em>&gt;&gt; slip. I similarly grant that it is possible some talented person will
</em><br>
<em>&gt;&gt; come up with an algorithm that solves the traveling salesman in
</em><br>
<em>&gt;&gt; polynomial time. I'm not holding my breath, though.
</em><br>
<em>&gt;
</em><br>
<em>&gt; As a Friendly AI may be necessary to our survival, it presumably has
</em><br>
<em>&gt; a higher priority than the traveling salesman problem.  While
</em><br>
<em>&gt; holding one's breath is not advisable, putting energy into the
</em><br>
<em>&gt; problem is.
</em><br>
<p>I'm not convinced. First, I'm unsure that there is any evidence that
<br>
our survival requires such a thing -- the existence of humans has not
<br>
eliminated the existence of ants. Second of all, I'm not sure other
<br>
strategies might not be much more fruitful in that they might have a
<br>
shot at working. For example, with good enough intelligence
<br>
amplification, I could achieve a lot of things I'm interested in,
<br>
including, likely, personal survival. Third, it is far from clear to
<br>
me that I trust any of the people involved in this not to produce
<br>
something far worse than the disease they're trying to cure.
<br>
<p><em>&gt;&gt; I put it that way because this is not a new argument. The argument
</em><br>
<em>&gt;&gt; over the nature of &quot;the good&quot; goes back thousands of years. I could
</em><br>
<em>&gt;&gt; easily hand anyone who liked 50 fine books produced over the last
</em><br>
<em>&gt;&gt; 2500 years -- from The Republic through stuff written in the last year
</em><br>
<em>&gt;&gt; or two -- exploring the question of how to make decisions about what
</em><br>
<em>&gt;&gt; is and isn't &quot;moral&quot; or &quot;good&quot;, and no one has made much progress to
</em><br>
<em>&gt;&gt; the goal, though they've explored lots of interesting territory.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes indeed.  A lot of the arguments fall apart and some seem
</em><br>
<em>&gt; promising.  Perhaps they are all lacking.  Perhaps we humans aren't
</em><br>
<em>&gt; even smart enough to ask the question cleanly enough or to fully
</em><br>
<em>&gt; support a &quot;good-enough&quot; answer even if we stumbled upon it.   But
</em><br>
<em>&gt; this does not mean the question is fundamentally and forever
</em><br>
<em>&gt; unanswerable.
</em><br>
<p>Many questions men ask are meaningless. For example, for much of our
<br>
history we've asked &quot;what is the meaning of life&quot; -- as though the
<br>
question has an objective answer.
<br>
<p>The reasonable evidence from the hunt for an objective moral system is
<br>
that there isn't one, any more than there is phlogiston or the
<br>
ether. I think the evidence against objective morality is very very
<br>
strong. Better, then, to simply live as one can without it.
<br>
<p><em>&gt;&gt; Absent a way to determine if, say, eating a cow is immoral, there will
</em><br>
<em>&gt;&gt; be no way for The Friendly AI to determine if it should be protecting
</em><br>
<em>&gt;&gt; cows from being eaten -- doubtless the PETA types would argue that it
</em><br>
<em>&gt;&gt; is fundamental that they should not be, and the folks at Ruth's Chris
</em><br>
<em>&gt;&gt; would argue otherwise, and perhaps they would both petition The
</em><br>
<em>&gt;&gt; Friendly AI for resolution, only for none to be achieved.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I would guess that the AI would point out that eating cows is not at
</em><br>
<em>&gt; all necessary post-singularity and to forbid the behavior toward a
</em><br>
<em>&gt; possibly upliftable sentient.  Not sure we would even have cows.
</em><br>
<em>&gt; But I take your point.
</em><br>
<p>Indeed. The cows are merely an example.
<br>
<p><em>&gt;&gt; &gt; I was not asking you to predict what you think would happen but to
</em><br>
<em>&gt;&gt; &gt; express what it is you would like to happen and believe worthwhile
</em><br>
<em>&gt;&gt; &gt; to work toward bringing into being.
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt; I would like to see strong nanotechnology and IA technologies, because
</em><br>
<em>&gt;&gt; I could apply them to my own personal survival, but beyond that, I
</em><br>
<em>&gt;&gt; don't know what the spectrum of things that could happen are, or how I
</em><br>
<em>&gt;&gt; might choose among them meaningfully.
</em><br>
<em>&gt;
</em><br>
<em>&gt; So, do you have anything you care about beyond your own personal
</em><br>
<em>&gt; survival?
</em><br>
<p>I'm a small 'o' objectivist -- beyond survival, I choose my own
<br>
personal pleasure and happiness, and I sincerely hope others do the
<br>
same.
<br>
<p><em>&gt; Any preferences for the type of world you live in
</em><br>
<p>I have no idea what the choices available will be -- and no one else
<br>
really does, either. Hard to meaningfully choose under those
<br>
circumstances.
<br>
<p><em>&gt; or the kind of company that may or may not be around, for instance?
</em><br>
<p>Well, I rather like my friends and loved ones, but I have no idea if
<br>
most of them would even want to become something beyond what they are
<br>
now. Many of them have expressed a strong disinterest in achieving any
<br>
sort of personal transformation from human acorn into posthuman oak --
<br>
indeed, a violent distaste even for the idea of extreme personal
<br>
longevity. I see no reason I'd want to impose it upon them. Perhaps
<br>
they will change their minds. Perhaps I'd want for new
<br>
friends. Perhaps I'd make them among other posthuman intelligences, or
<br>
perhaps I'd make them more literally by building them, or perhaps I'd
<br>
edit out my desire for companionship. Likely the nature of my social
<br>
relationships, if any, will be utterly incomprehensible to me as I am
<br>
now.  Ants speculate poorly, I suspect, upon the social relationships
<br>
of higher primates.  I have no idea what things might be like -- and
<br>
neither does anyone else, really.
<br>
<p><em>&gt;&gt; I don't pretend I have the foresight to be able to guide history into
</em><br>
<em>&gt;&gt; a direction I would like -- I don't even pretend to be able to guide a
</em><br>
<em>&gt;&gt; small company with any certainty and I have at least operated those
</em><br>
<em>&gt;&gt; enough to have understanding of the problem and feel like I can do a
</em><br>
<em>&gt;&gt; reasonable job at it. The variables involved with an entire society on
</em><br>
<em>&gt;&gt; the scale of the one we have are beyond my comprehension. That's why
</em><br>
<em>&gt;&gt; I'm a libertarian, not a central planning freak.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You present a false dichotomy between inability and having to have
</em><br>
<em>&gt; near godlike foresight to make much difference;
</em><br>
<p>Oh, one can indeed make a lot of difference -- but it is nearly
<br>
impossible to do so deliberately. Sir Tim Berners-Lee didn't set out
<br>
to change the world, and neither did Georg Cantor, and neither did
<br>
most of the folks who've made a big difference. I'm not aware of many
<br>
who really deliberately set out to change the world and succeeded --
<br>
mostly it happens through serendipity. The work I've been most known
<br>
for over the years has often been trivial stuff I've done in a couple
<br>
of hours while hung over one weekend. If I ever alter the future in a
<br>
big way, it will probably be in some similar manner.
<br>
<p>Anyway, in a world like that, one tries above all to do as little harm
<br>
as one can, and to try to have a great deal of fun. If along the way
<br>
you happen to invent something nifty, well, great. One also tries to
<br>
take a reasonably conservative position with respect to personal
<br>
survival -- but not so conservative as to ruin one's fun.
<br>
<p><em>&gt; between being a libertarian and being a &quot;central planning freak&quot;.  I
</em><br>
<em>&gt; think it is in each person's range of responsibility to consider to
</em><br>
<em>&gt; the extent of their abilities what kind of world they wish to
</em><br>
<em>&gt; inhabit and to do what they can to acheive it.  It doesn't take any
</em><br>
<em>&gt; pretense or super-ability to do what one can guided by one's best
</em><br>
<em>&gt; knowledge and values to the extent of one's abilities.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If we don't work at least in part at the level of envisioning what
</em><br>
<em>&gt; we want then how in the hell do we expect to have any chance of
</em><br>
<em>&gt; getting there?
</em><br>
<p>We don't have any chance of getting to any future we can envision
<br>
right now. The forces at work are beyond any single individual's
<br>
control -- and I'm glad for that, because most individuals would
<br>
impose terribly myopic desires, and indeed things that would end up
<br>
more dystopiac than utopiac.
<br>
<p><p><pre>
-- 
Perry E. Metzger		<a href="mailto:perry@piermont.com?Subject=Re:%20An%20essay%20I%20just%20wrote%20on%20the%20Singularity.">perry@piermont.com</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7543.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7541.html">Lawrence Foard: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>In reply to:</strong> <a href="7522.html">Samantha Atkins: "Re: An essay I just wrote on the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0312/7390.html">Ben Goertzel: "RE: An essay I just wrote on the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7542">[ date ]</a>
<a href="index.html#7542">[ thread ]</a>
<a href="subject.html#7542">[ subject ]</a>
<a href="author.html#7542">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:43 MDT
</em></small></p>
</body>
</html>
