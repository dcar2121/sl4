<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendliness and blank-slate goal bootstrap</title>
<meta name="Author" content="Nick Hay (nickjhay@hotmail.com)">
<meta name="Subject" content="Re: Friendliness and blank-slate goal bootstrap">
<meta name="Date" content="2004-01-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendliness and blank-slate goal bootstrap</h1>
<!-- received="Sat Jan 10 16:54:21 2004" -->
<!-- isoreceived="20040110235421" -->
<!-- sent="Sun, 11 Jan 2004 12:49:38 +1300" -->
<!-- isosent="20040110234938" -->
<!-- name="Nick Hay" -->
<!-- email="nickjhay@hotmail.com" -->
<!-- subject="Re: Friendliness and blank-slate goal bootstrap" -->
<!-- id="20040110234938.GA371@nickh" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="3FFF69BD.608@earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Nick Hay (<a href="mailto:nickjhay@hotmail.com?Subject=Re:%20Friendliness%20and%20blank-slate%20goal%20bootstrap"><em>nickjhay@hotmail.com</em></a>)<br>
<strong>Date:</strong> Sat Jan 10 2004 - 16:49:38 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7732.html">Charles Hixson: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7730.html">Rafal Smigrodzki: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>In reply to:</strong> <a href="7710.html">Charles Hixson: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7732.html">Charles Hixson: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7732.html">Charles Hixson: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7731">[ date ]</a>
<a href="index.html#7731">[ thread ]</a>
<a href="subject.html#7731">[ subject ]</a>
<a href="author.html#7731">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 10/01/04 15:55:57, Charles Hixson wrote:
<br>
<em>&gt; Nick Hay wrote:
</em><br>
<em>&gt; &gt; Metaqualia wrote:
</em><br>
<em>&gt; &gt; &gt; ...
</em><br>
<em>&gt; &gt; ...
</em><br>
<em>&gt; &gt; You could go with &quot;reduce undesirable qualia, increase desirable
</em><br>
<em>&gt; &gt; ones&quot; if you liked.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Be very careful here! The easiest way to reduce undesirable qualia is   
</em><br>
<em>&gt; to kill off everyone who has the potential for experiencing them.
</em><br>
<p>This rule does have lots of problems. What may not be clear from the  
<br>
context is this is my summary of Metaqualia's view. I don't think it  
<br>
works, nor is it sufficent. I had:
<br>
<p>Nick Hay wrote:
<br>
<em>&gt; Personally, [my best guess at what's really right ie. the best  
</em><br>
<em>&gt; morality for an altruist is] along the lines of &quot;help sentients, and  
</em><br>
<em>&gt; others that can be helped, with respect to their volitions --  
</em><br>
<em>&gt; maximise self-determination, minimised unexpected regret&quot;. Focusing  
</em><br>
<em>&gt; on the aspect of helpfulness and implementing others' volitions.
</em><br>
<p>This, of course, hides a huge amount of complexity -- even if I were  
<br>
trying to transfer this to a human, rather than an AI.
<br>
<p><em>&gt;&gt; Instead of thinking about what kind of morality an AI should start   
</em><br>
<em>&gt;&gt; with have, and then transferring it over, why not jump back a step?  
</em><br>
<em>&gt;&gt; Transfer  your ability to think &quot;what kind of morality should an AI  
</em><br>
<em>&gt;&gt; start with?&quot; so  the AI itself can make sure you got it right? It  
</em><br>
<em>&gt;&gt; seems like you're forced  into giving very simple inadequate verbal  
</em><br>
<em>&gt;&gt; phrases (hiding huge amounts of conceputal complexity) to describe  
</em><br>
<em>&gt;&gt; morality. You're losing too much  in the compression.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It seems to me that a person's method for determining the desireable   
</em><br>
<em>&gt; morality is based partially on instincts, partially on training, and   
</em><br>
<em>&gt; partially on self-interest. I've seen remarkable transformations in   
</em><br>
<em>&gt; what  was considered moral occur when the self-interest of the  
</em><br>
<em>&gt; decider has  shifted. Similarly, the morality that is desired seems  
</em><br>
<em>&gt; to be very  subject to what other people insist is the right thing to  
</em><br>
<em>&gt; do. War  propaganda is a very current example here. I include this   
</em><br>
<em>&gt; modification  under the category of instinctual. People ARE herd  
</em><br>
<em>&gt; animals...to an  extent.
</em><br>
<p>Sure, and since we're human this is our problem too. The human &quot;moral  
<br>
reasoning methods&quot; leave much to be desired. That is, typically when  
<br>
humans examine their moral reasoning methods with their moral reasoning  
<br>
methods, they find much to be desired: the methods are not self- 
<br>
consistent, nor stable under reflection. We find some factors influence  
<br>
our moralities when they shouldn't, or in ways they shouldn't. We find  
<br>
other factors we think should, or thought did, influence things which  
<br>
don't. This means you shouldn't &quot;hard code&quot; these methods as some fixed  
<br>
ideal. This doesn't mean you can ignore them and start with a blank  
<br>
slate, but it does mean you have to be careful and understand *exactly*  
<br>
what you're doing.
<br>
<p>Of course these instincts are, in some ways, probably like our  
<br>
instincts for, say, vision: exceedingly complex, with a fair deal of  
<br>
nontrivial structure. This is easy to see with the visual system since  
<br>
we can &quot;simply&quot; trace back from the eyes through various parts of the  
<br>
brain. The complexities underlying our moral reasoning process, and our  
<br>
sense of desirabilty, are less scientifically accessible (however, I  
<br>
haven't examined the literature, so I don't know what we know).
<br>
<p><p><em>&gt; So, when you are saying that the AI should use the process that you   
</em><br>
<em>&gt; do ... are you sure about that? Just how heavily do you want the AI  
</em><br>
<em>&gt; to  weigh it's self interest? Do you want it to be able to justify   
</em><br>
<em>&gt; intentionally killing off the entire human race? Or even not   
</em><br>
<em>&gt; considering  that danger very important when calculating risks? (Note  
</em><br>
<em>&gt; that people  frequently don't. I consider this one of our major  
</em><br>
<em>&gt; failings as a  species.)
</em><br>
<p>No, I did not mean to imply a pattern-copy of the human moral reasoning  
<br>
process. A closer approximation would be how humans *think* they  
<br>
reason, or how they'd like to reason (if they had the ability to  
<br>
choose), this would fix the errors you have suggested.
<br>
<p>I do agree that ignoring existential risks, or not paying them enough  
<br>
of the right attention, is a pretty major flaw. Clearly, however, it's  
<br>
not impossible to take them into consideration ie. some do.
<br>
<p>So I'm not saying that the AI should use the exact reasoning process we  
<br>
use, but that we *do* have to create some kind of reasoning process  
<br>
inspired by ours. We can't leave a blank slate, or simply stick some  
<br>
moral conclusions eg. &quot;don't kill children&quot; onto it. I'm not saying  
<br>
&quot;this is the right way to create a Friendly AI&quot; (I don't know how to  
<br>
create FAI), except in broad and uncertain terms, but more &quot;this is not  
<br>
the right way to create a Friendly AI, you are missing, at least,  
<br>
&lt;these&gt; important things&quot;.
<br>
<p>However, even an pattern-copy of the human moral reasoning process  
<br>
(pattern-copying a human is conceptually simple as a goal, but in  
<br>
practice pretty difficult) is better than pattern-copying the output of  
<br>
one (ie. a humanly devised morality). The former contains, most  
<br>
importantly, the structure *behind* the conclusions, the ability to  
<br>
rederive conclusions upon gaining new skills, understanding, or upon  
<br>
noticing mistakes. Unrealistic (for a FAI), but illustrative, examples:
<br>
<p>eg. &quot;Wow, this whole philosophy was inspired by selfishness; I didn't  
<br>
know that, nor did I want it. Ok, what does it look like *without*  
<br>
selfishness...&quot;
<br>
eg. &quot;Wow, this plan could destroy the human species! Why didn't I  
<br>
notice that before? Hmmm, seems like my reasoning process doesn't take  
<br>
into account extinction with the appropriate seriousness. Let's see if  
<br>
I can fix this...&quot;
<br>
<p>None of these examples would be possible if we only transferred our  
<br>
conclusions ontop a blank slate. If we transferred our conclusions, and  
<br>
the reasons for those conclusions, and the mechanisms we used to  
<br>
generate those reasons and conclusions, in the right way, this kind of  
<br>
reasoning (maybe) becomes possible.
<br>
<p><p><em>&gt;&gt; Remember, friendliness isn't Friendliness. The former would involve   
</em><br>
<em>&gt;&gt; something like making an AI friend, the latter is nothing like it.  
</em><br>
<em>&gt;&gt; Where he  says &quot;Friendliness should be the supergoal&quot; it means  
</em><br>
<em>&gt;&gt; something more like  &quot;Whatever is really right should be the  
</em><br>
<em>&gt;&gt; supergoal&quot;. Friendliness is an  external
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is assuming that &quot;right&quot; has some absolute meaning, but this is   
</em><br>
<em>&gt; only true in the context of a certain set of axioms (call them   
</em><br>
<em>&gt; instincts). And also from a particular point of view. Friendliness   
</em><br>
<em>&gt; can  be friendliness, if that's what the instincts say is right.
</em><br>
<p>Perhaps, but this is why I said &quot;something more like&quot; (I guess, can't  
<br>
actually remember the specific thoughts). The supergoal (in so much as  
<br>
we use a goal architecture in this way) shouldn't be some fixed moral  
<br>
conclusion, such as &quot;be like a human friend&quot;. It should be more open- 
<br>
ended than that, to allow the FAI to grow.
<br>
<p><em>&gt; Remember  that your basic goals can't be choosen, but they must be  
</em><br>
<em>&gt; describable  in  very simple terms. And nearly all formulations are  
</em><br>
<em>&gt; subject to failure  modes. E.g., Eliza would be much more willing to  
</em><br>
<em>&gt; talk to it than any  person would, so having &quot;someone to talk to&quot;  
</em><br>
<em>&gt; would be a bad choice of  supergoal. If you make it friendly to those  
</em><br>
<em>&gt; with some particular DNA  characteristics (say something that  
</em><br>
<em>&gt; specifies a particular protein  unique to humans), then it won't  
</em><br>
<em>&gt; necessarily be friendly to uploads.  Etc. This is one of the more  
</em><br>
<em>&gt; difficult things to get right. And  getting  it right is crucial.  
</em><br>
<em>&gt; Even better would be being able to *know* that  you  got it right.
</em><br>
<p>Yes, this is one problem with giving (or, trying to give) an AI some  
<br>
fixed moral conclusion you thought up: you might, later on, think up  
<br>
reasons why that morality was wrong. If you didn't transfer the  
<br>
cognitive mechanisms you used to generate, and are now invoking to  
<br>
correct, the morality, the AI will say something like &quot;yes, but if I  
<br>
change *that* then I won't be as friendly to those with human DNA!&quot; ie.  
<br>
the supergoal is fixed. You can't change it through the goal system,  
<br>
you have to fiddle with the AI's internals *against* its goal system  
<br>
(perhaps possible for an infant-level AI, but not for a smarter one).
<br>
<p>It seems like you're thinking in terms of &quot;picking the best morality  
<br>
for an AI&quot; (an AI assumed to have the structure humans use to implement  
<br>
moralities) rather than, say, &quot;engineering an AI that can understand  
<br>
morality, along with the arguments and philosophies we use to decide  
<br>
which actions we should or shouldn't take&quot;.
<br>
<p>- Nick Hay
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7732.html">Charles Hixson: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7730.html">Rafal Smigrodzki: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>In reply to:</strong> <a href="7710.html">Charles Hixson: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7732.html">Charles Hixson: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7732.html">Charles Hixson: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7731">[ date ]</a>
<a href="index.html#7731">[ thread ]</a>
<a href="subject.html#7731">[ subject ]</a>
<a href="author.html#7731">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:45 MDT
</em></small></p>
</body>
</html>
