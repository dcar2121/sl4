<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Darwinian dynamics unlikely to apply to superintelligence</title>
<meta name="Author" content="Wei Dai (weidai@weidai.com)">
<meta name="Subject" content="Re: Darwinian dynamics unlikely to apply to superintelligence">
<meta name="Date" content="2004-01-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Darwinian dynamics unlikely to apply to superintelligence</h1>
<!-- received="Fri Jan  2 22:18:26 2004" -->
<!-- isoreceived="20040103051826" -->
<!-- sent="Sat, 3 Jan 2004 00:18:19 -0500" -->
<!-- isosent="20040103051819" -->
<!-- name="Wei Dai" -->
<!-- email="weidai@weidai.com" -->
<!-- subject="Re: Darwinian dynamics unlikely to apply to superintelligence" -->
<!-- id="20040103001819.A7220@weidai.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3FF63794.1090004@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Wei Dai (<a href="mailto:weidai@weidai.com?Subject=Re:%20Darwinian%20dynamics%20unlikely%20to%20apply%20to%20superintelligence"><em>weidai@weidai.com</em></a>)<br>
<strong>Date:</strong> Fri Jan 02 2004 - 22:18:19 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7534.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7532.html">Lawrence Foard: "Re: &quot;friendly&quot; humans?"</a>
<li><strong>In reply to:</strong> <a href="7527.html">Eliezer S. Yudkowsky: "Re: Darwinian dynamics unlikely to apply to superintelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7536.html">Eliezer S. Yudkowsky: "Re: Darwinian dynamics unlikely to apply to superintelligence"</a>
<li><strong>Reply:</strong> <a href="7536.html">Eliezer S. Yudkowsky: "Re: Darwinian dynamics unlikely to apply to superintelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7533">[ date ]</a>
<a href="index.html#7533">[ thread ]</a>
<a href="subject.html#7533">[ subject ]</a>
<a href="author.html#7533">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Fri, Jan 02, 2004 at 10:31:32PM -0500, Eliezer S. Yudkowsky wrote:
<br>
<em>&gt; Or at least, not heritable variations of a kind that we regard as viruses, 
</em><br>
<em>&gt; rather than, say, acceptable personality quirks, or desirable diversity. 
</em><br>
<em>&gt; But even in human terms, what's wrong with encrypting the control block?
</em><br>
<p>The behavior of a machine depends on both code and data, in other words on
<br>
its control algorithms and on its state information. You can protect the
<br>
code but the data is going to change and cause differences in behavior. 
<br>
<p><em>&gt; Humans are hardly top-of-the-line in the cognitive security department. 
</em><br>
<em>&gt; You can hack into us easily enough.  But how do you hack an SI 
</em><br>
<em>&gt; optimization process?  What kind of &quot;memes&quot; are we talking about here? 
</em><br>
<em>&gt; How do they replicate, what do they do; why, if they are destructive and 
</em><br>
<em>&gt; foreseeable, is it impossible to prevent them?  We are not talking about a 
</em><br>
<em>&gt; Windows network.
</em><br>
<p>I don't know what kind of memes SI components would find it useful to
<br>
exchange with each other, but perhaps precomputed chunks of data,
<br>
algorithmic shortcuts, scientific theories, information about other SIs
<br>
encountered, philosophical musings, etc. This so called &quot;SI optimization
<br>
process&quot; is of course actually a complex intellect. How can you know that
<br>
no meme will arise in ten billion years anywhere in the visible universe
<br>
that could cause it to change its mind about what kind of optimization it
<br>
should undertake?
<br>
<p>Your question &quot;why, if they are destructive and foreseeable, is it 
<br>
impossible to prevent them&quot; makes you sound like you've never thought 
<br>
about security problems before. It's kind of like asking &quot;why, if the Al 
<br>
Queda are destructive and foreseeable, is it impossible to prevent them?&quot; 
<br>
Well, it may not be impossible, but doing so will certainly impose a cost.
<br>
<p><em>&gt; Okay, so possibly a Friendly SI expands spherically as (.98T)^3 and an 
</em><br>
<em>&gt; unfriendly SI expands spherically as (.99T)^3, though I don't see why the 
</em><br>
<em>&gt; UFSI would not need to expend an equal amount of effort in ensuring its 
</em><br>
<em>&gt; own fidelity.  
</em><br>
<p>Because the UFSI has a bigger threat to deal with, namely the FSI. And the
<br>
FSI, once it notices the UFSI, also has a bigger threat to deal with and
<br>
would be forced to lower its own efforts at ensuring fidelity.
<br>
<p><em>&gt; Even so, under that assumption it would work out to a 
</em><br>
<em>&gt; constant factor of UFSIs being 3% larger; or a likelihood ratio of 1.03 in 
</em><br>
<em>&gt; favor of observing UFSI (given some prior probability of emergence); or in 
</em><br>
<em>&gt; terms of natural selection, essentially zero selection pressure - and you 
</em><br>
<em>&gt; can't even call it that, because it's not being iterated.  I say again 
</em><br>
<em>&gt; that natural selection is a quantitative pressure that can be calculated 
</em><br>
<em>&gt; given various scenarios, not something that goes from zero to one given 
</em><br>
<em>&gt; the presence of &quot;heritable difference&quot; and so on.
</em><br>
<p>This analysis makes no sense. If you have two spheres expanding at
<br>
different rates, one of them is eventually going to completely enclose the
<br>
other, and in this case cutting off all growth of the Friendly SI. And
<br>
that doesn't even take into consideration the possibility that the UFSI
<br>
could just eat the FSI.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7534.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7532.html">Lawrence Foard: "Re: &quot;friendly&quot; humans?"</a>
<li><strong>In reply to:</strong> <a href="7527.html">Eliezer S. Yudkowsky: "Re: Darwinian dynamics unlikely to apply to superintelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7536.html">Eliezer S. Yudkowsky: "Re: Darwinian dynamics unlikely to apply to superintelligence"</a>
<li><strong>Reply:</strong> <a href="7536.html">Eliezer S. Yudkowsky: "Re: Darwinian dynamics unlikely to apply to superintelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7533">[ date ]</a>
<a href="index.html#7533">[ thread ]</a>
<a href="subject.html#7533">[ subject ]</a>
<a href="author.html#7533">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:43 MDT
</em></small></p>
</body>
</html>
