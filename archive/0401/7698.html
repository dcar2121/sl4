<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Dispersing AIs throughout the universe</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Dispersing AIs throughout the universe">
<meta name="Date" content="2004-01-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Dispersing AIs throughout the universe</h1>
<!-- received="Thu Jan  8 19:55:13 2004" -->
<!-- isoreceived="20040109025513" -->
<!-- sent="Thu, 08 Jan 2004 21:55:08 -0500" -->
<!-- isosent="20040109025508" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Dispersing AIs throughout the universe" -->
<!-- id="3FFE180C.9080202@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="BAEAIIMDCMDAEKHBFGFKCELKCIAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Dispersing%20AIs%20throughout%20the%20universe"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu Jan 08 2004 - 19:55:08 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7699.html">Mitchell Porter: "Re: Recursive self-improvement curves"</a>
<li><strong>Previous message:</strong> <a href="7697.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<li><strong>In reply to:</strong> <a href="7693.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7701.html">Metaqualia: "Re: Dispersing AIs throughout the universe"</a>
<li><strong>Reply:</strong> <a href="7701.html">Metaqualia: "Re: Dispersing AIs throughout the universe"</a>
<li><strong>Reply:</strong> <a href="7704.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7698">[ date ]</a>
<a href="index.html#7698">[ thread ]</a>
<a href="subject.html#7698">[ subject ]</a>
<a href="author.html#7698">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It might be significantly easier to engineer an AI with a 20% or 1%
</em><br>
<em>&gt; (say) chance of being Friendly, than to engineer one with a 99.99%
</em><br>
<em>&gt; chance of being Friendly.  If this is the case, then the
</em><br>
<em>&gt; broad-physical-dispersal approach that I suggested makes sense.
</em><br>
<p>1)  I doubt that it is &quot;significantly easier&quot;.  To get a 1% chance you 
<br>
must solve 99% of the problem, as 'twere.  It is no different from trying 
<br>
to build a machine with a 1% chance of being an internal combustion 
<br>
engine, a program with a 1% chance of being a spreadsheet, or a document 
<br>
with a 1% chance of being well-formed XML.
<br>
<p>2)  Ignoring (1), and supposing someone built an AI with a 1% real chance 
<br>
of being Friendly, I exceedingly doubt its maker would have the skill to 
<br>
calculate that as a quantitative probability.  To go over a program 
<br>
specification and correctly, quantitatively calculate the probability that 
<br>
it meets a certain criterion requires knowing exactly what that criterion 
<br>
is, which variables the process flow critically depends on, and how those 
<br>
variables contribute to the final probability, and so on.  To correctly 
<br>
calculate that a poorly assembled program (one representing the limit of 
<br>
its maker's skill) has a 1% chance of being Friendly - even to within an 
<br>
order of magnitude! - requires a skill level considerably, no, enormously 
<br>
higher than that required to build a program with a 99.99% chance of being 
<br>
Friendly; you must have reduced the entire problem to math, know the exact 
<br>
criterion of success, tracked the dependency of success on every one of 
<br>
the variables, and be capable of performing this calculation as 
<br>
quantitative math for programs poorly assembled.  NASA successfully 
<br>
designed, built, and launched multiple space shuttles on multiple 
<br>
successful missions, but their attempt to calculate a quantitative 
<br>
probability of mission failure was statistically laughable and 
<br>
demonstrably incorrect.  If someone were to build, at the limit of their 
<br>
skill, a program with a 1% real chance of being Friendly, and an observer 
<br>
correctly calculated this probability, the observer would have to be a god.
<br>
<p>3)  So we are not talking about a quantitative calculation that a program 
<br>
will be Friendly, but rather an application of the Principle of 
<br>
Indifference to surface outcomes.  The maker just doesn't really know 
<br>
whether the program will be Friendly or not, and so pulls a probability 
<br>
out of his ass.  This reminds me of the story about when Australia was 
<br>
starting its national lottery, and the television crews interviewed a man 
<br>
in the street, asking him what he thought his chances were of winning. 
<br>
&quot;50/50&quot;, he said, &quot;either I win or I don't.&quot;
<br>
<p>4)  Extremely extensive research shows that &quot;probabilities&quot; which people 
<br>
pull out of their asses (as opposed to being able to calculate them 
<br>
quantitatively) are not calibrated, that is, they bear essentially no 
<br>
relation to reality.  People use the term &quot;99% probable&quot; as a sort of 
<br>
emotional ejaculation indicating that they believe in something really 
<br>
strongly.  There is no relation to actual probabilities.  On empirical 
<br>
tests, the range of surprises for 98% confidence intervals (where the 
<br>
person gives an upper limit of 99% confidence and a lower limit of 99% 
<br>
confidence) ranges between 30% and 60%, and the most common number I have 
<br>
seen is 45%.  This incredible overconfidence and enormously poor 
<br>
calibration gets worse as the difficulty of the problem increases.  The 
<br>
problem is, of course, is that &quot;probabilities&quot; that people pull out of 
<br>
their asses are just not related to reality in any way, nor do most people 
<br>
realize that probabilities need to be calibrated.  For example, someone 
<br>
who says that the chance of an AI undergoing a hard takeoff is &quot;a million 
<br>
to one&quot; is implying that he could be asked a million questions of equal 
<br>
difficulty and expect to get at most one of them wrong.  In reality, if 
<br>
you cannot do the calculation and get a quantitative probability, you DO 
<br>
NOT KNOW the probability and that is all there is to it.  Research also 
<br>
shows that this is one of the &quot;resistant delusions&quot; - most people, 
<br>
confronted with the research that shows that making up probabilities 
<br>
doesn't work, go on making up probabilities; being told about the research 
<br>
fails to have the emotional impact that would be needed to overcome the 
<br>
fun of making up probabilities.  This is why I am trying to nag everyone 
<br>
in the transhumanist community into reading Tversky and Kahneman's 
<br>
&quot;Judgment under uncertainty.&quot;
<br>
<p>5)  Plausibility is not the same as frequency.  If you evaluate the 
<br>
evidence (hopefully Bayesian evidence) back and forth and wind up by 
<br>
estimating a 95% probability that &quot;2 + 2 = 4&quot;, it doesn't mean you think 
<br>
&quot;2 + 2 = 4&quot; on 19 out of 20 occasions.
<br>
<p>6)  And finally, of course, the probabilities are not independent!  If the 
<br>
best AI you can make isn't good enough, a million copies of it don't have 
<br>
independent chances of success.
<br>
<p>So to sum up, what we have is something like this.  A person - let us 
<br>
suppose for the sake of tradition that it is an Australian man - buys a 
<br>
lottery ticket.  On the surface, the odds seem like they should be 50/50, 
<br>
since either he wins or he doesn't.  It seems, though, that a lot of 
<br>
people think it's really unlikely that he'll win the lottery, and so he 
<br>
concedes that the probability might be a little lower.  Checking how 
<br>
strongly he feels about it, he finds that he wants to believe but is 
<br>
afraid of being proven wrong, a state of mind that he describes with the 
<br>
phrase &quot;20% probability&quot;, which allows a satisfying hope of winning, while 
<br>
still being low enough to ward off most disappointment in the event of 
<br>
failure.  However, the person comes up with a clever plan.  Suppose that 
<br>
he puts the lottery ticket under a hat.  Then, once he knows the winning 
<br>
number, he'll pull the hat off the lottery ticket.  If the ticket doesn't 
<br>
win the first time, he'll put the hat back on, and pull it off again. 
<br>
Without knowing the winning numbers (at the present time), he calculates 
<br>
in advance that on each occasion his chance of seeing the winning number 
<br>
under the hat is 20%.  So if he does this just 20 times, his chance of 
<br>
winning is 99%!
<br>
<p>What's wrong with this picture:
<br>
<p>a)  Confusing plausibility with frequency;
<br>
b)  Assigning something called a &quot;probability&quot; in the absence of a theory 
<br>
powerful enough to calculate it quantitatively;
<br>
c)  Treating highly correlated probabilities as independent;
<br>
d)  Applying the Principle of Indifference to surface outcomes rather than 
<br>
elementary interchangeable events; and
<br>
e)  Attempting to trade off not knowing how to solve a problem for 
<br>
confessing a &quot;1%&quot; probability of success.
<br>
<p>And if you're wondering why I'm so down on this, it's because it seems to 
<br>
me like yet another excuse for not knowing how to build a Friendly AI.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7699.html">Mitchell Porter: "Re: Recursive self-improvement curves"</a>
<li><strong>Previous message:</strong> <a href="7697.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<li><strong>In reply to:</strong> <a href="7693.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7701.html">Metaqualia: "Re: Dispersing AIs throughout the universe"</a>
<li><strong>Reply:</strong> <a href="7701.html">Metaqualia: "Re: Dispersing AIs throughout the universe"</a>
<li><strong>Reply:</strong> <a href="7704.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7698">[ date ]</a>
<a href="index.html#7698">[ thread ]</a>
<a href="subject.html#7698">[ subject ]</a>
<a href="author.html#7698">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:43 MDT
</em></small></p>
</body>
</html>
