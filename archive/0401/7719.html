<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Dispersing AIs throughout the universe</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Dispersing AIs throughout the universe">
<meta name="Date" content="2004-01-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Dispersing AIs throughout the universe</h1>
<!-- received="Sat Jan 10 03:38:20 2004" -->
<!-- isoreceived="20040110103820" -->
<!-- sent="Sat, 10 Jan 2004 05:38:13 -0500" -->
<!-- isosent="20040110103813" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Dispersing AIs throughout the universe" -->
<!-- id="3FFFD615.6080102@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="BAEAIIMDCMDAEKHBFGFKKEMMCIAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Dispersing%20AIs%20throughout%20the%20universe"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Jan 10 2004 - 03:38:13 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7720.html">Mitchell Porter: "Re: What exactly is &quot;panpsychism&quot;?"</a>
<li><strong>Previous message:</strong> <a href="7718.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>In reply to:</strong> <a href="7704.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7722.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<li><strong>Reply:</strong> <a href="7722.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<li><strong>Reply:</strong> <a href="7723.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7719">[ date ]</a>
<a href="index.html#7719">[ thread ]</a>
<a href="subject.html#7719">[ subject ]</a>
<a href="author.html#7719">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt;&gt; Ben Goertzel wrote:
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt;&gt; Eliezer,
</em><br>
<em>&gt;&gt;&gt; 
</em><br>
<em>&gt;&gt;&gt; It might be significantly easier to engineer an AI with a 20% or 1%
</em><br>
<em>&gt;&gt;&gt;  (say) chance of being Friendly, than to engineer one with a 99.99%
</em><br>
<em>&gt;&gt;&gt;  chance of being Friendly.  If this is the case, then the 
</em><br>
<em>&gt;&gt;&gt; broad-physical-dispersal approach that I suggested makes sense.
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt; 1)  I doubt that it is &quot;significantly easier&quot;.  To get a 1% chance
</em><br>
<em>&gt;&gt; you must solve 99% of the problem, as 'twere.  It is no different
</em><br>
<em>&gt;&gt; from trying to build a machine with a 1% chance of being an internal
</em><br>
<em>&gt;&gt; combustion engine, a program with a 1% chance of being a spreadsheet,
</em><br>
<em>&gt;&gt; or a document with a 1% chance of being well-formed XML.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Actually, the analogies you're making are quite poor, because internal 
</em><br>
<em>&gt; combustion engines and spreadsheets and XML documents are not complex 
</em><br>
<em>&gt; self-organizing systems. 
</em><br>
<p>An internal combustion engine is a complex dynamic system.  The gasoline 
<br>
flows, each molecule bumping into other molecules, its fluidity determined 
<br>
by details of the electromagnetic interaction between molecules relative 
<br>
to the prevailing temperature, yet the fluid dynamics as a whole can be 
<br>
excellently simplified to equations that describe quite different 
<br>
quantities; when electricity flows through the spark plugs it obeys 
<br>
Maxwell's Equations, and when the gas mixed with oxygen explodes the 
<br>
explosion ripples out in obedience to differential equations.
<br>
<p>Having seen, from other fields beyond AI, what it means to &quot;understand&quot; a 
<br>
system, and having recently finally understood a thing or two about AI for 
<br>
the first time, I now realize that &quot;complex dynamic system&quot; means &quot;I do 
<br>
not understand which particular dynamics are involved&quot;.
<br>
<p>&quot;Emergence&quot; is sort of like the way that fluid dynamics can be usefully 
<br>
simplified to high-level equations that are unlike the deep kinetic and 
<br>
electromagnetic equations, except that in Artificial Intelligence the word 
<br>
&quot;emergent&quot; means &quot;without understanding either level&quot;.
<br>
<p>&quot;Self-organizing&quot;?  Well, now there's a wonderful magic wand of a word, to 
<br>
be used whenever something mysterious happens, and indeed 
<br>
&quot;self-organizing&quot; seems to make a fairly good synonym for &quot;mysterious&quot;. 
<br>
If I were ever to use the word, while still aspiring to my usual standards 
<br>
for well-definedness, I would pick some more rigorous criterion of usage, 
<br>
like, say, &quot;self-organizing&quot; referring to a multiplicity of locally 
<br>
centered optimization pressures interacting to create an optimization 
<br>
pressure on some higher-level property of the system, c.f. &quot;emergence&quot; 
<br>
above.  Or perhaps &quot;self-organizing&quot; could also be extended to cases where 
<br>
an external optimization pressure, like natural selection, builds a system 
<br>
where distributed local properties and their interactions create the 
<br>
systemic behaviors that were subject to the external optimization 
<br>
pressure.  But here I am holding myself to much higher standards than most 
<br>
people do when they use so marvelous and poetic a word as 
<br>
&quot;self-organizing&quot;.  Mostly, IMO, it is used much like &quot;phlogiston&quot; in an 
<br>
earlier era.  Where does the organization come from?  It's self-organizing!
<br>
<p><em>&gt; With Friendly AI, we're talking about
</em><br>
<em>&gt; creating an initial condition, letting some dynamics run (interactively
</em><br>
<em>&gt; with the environment, which includes us), and then continually nudging
</em><br>
<em>&gt; the dynamics to keep them running in a Friendly direction.  This is a
</em><br>
<em>&gt; very different --- and plausibly, much less deterministic -- process
</em><br>
<em>&gt; than building a relatively static machine like a car engine or a
</em><br>
<em>&gt; spreadsheet.
</em><br>
<p>It sounds to me like a description that applies quite well to a car 
<br>
engine.  A car engine has initial conditions, check.  Dynamics, check. 
<br>
Nudging, check.  The difference is not in determinism, the difference is 
<br>
whether the designer is mystified by what is allegedly going on.  A car 
<br>
engine is nothing to sneer at; it is a work of art built by people who 
<br>
understood the dynamics.
<br>
<p><em>&gt;&gt; 2)  Ignoring (1), and supposing someone built an AI with a 1% real
</em><br>
<em>&gt;&gt; chance of being Friendly, I exceedingly doubt its maker would have
</em><br>
<em>&gt;&gt; the skill to calculate that as a quantitative probability.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Of course, that's true.  But it's also true that, if someone built an
</em><br>
<em>&gt; AI with a 99.999% chance of being Friendly, it's maker is not likely to
</em><br>
<em>&gt; have the skill to calculate that as a quantitative probability.
</em><br>
<p>Yes, this is covered in the part where I said:
<br>
<p><em>&gt;&gt; To correctly calculate that a poorly assembled program (one 
</em><br>
<em>&gt;&gt; representing the limit of its maker's skill) has a 1% chance of being 
</em><br>
<em>&gt;&gt; Friendly - even to within an order of magnitude! - requires a skill 
</em><br>
<em>&gt;&gt; level considerably, no, enormously higher than that required to build a 
</em><br>
<em>&gt;&gt; program with a 99.99% chance of being Friendly.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Making quantitative predictions about this kind of system is next to
</em><br>
<em>&gt; impossible, because the dynamic evolution of the system is going to
</em><br>
<em>&gt; depend on its environment -- on human interactions with it, and so
</em><br>
<em>&gt; forth.  So to make a rigorous probability estimate you'd have to set
</em><br>
<em>&gt; various quantitative bounds on various environmental conditions, human
</em><br>
<em>&gt; behaviors, etc.  Very tricky ... not just a math problem, for sure...
</em><br>
<em>&gt; (and the math problems involved are formidable enough even without
</em><br>
<em>&gt; these environmental-modeling considerations!)
</em><br>
<p>Yes, I said as much in my post.
<br>
<p><em>&gt;&gt; 3)  So we are not talking about a quantitative calculation that a
</em><br>
<em>&gt;&gt; program will be Friendly, but rather an application of the Principle
</em><br>
<em>&gt;&gt; of Indifference to surface outcomes.  The maker just doesn't really
</em><br>
<em>&gt;&gt; know whether the program will be Friendly or not, and so pulls a
</em><br>
<em>&gt;&gt; probability out of his ass.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There are a lot of intermediate cases between a fully rigorous
</em><br>
<em>&gt; quantitative calculation, and a purely nonrigorous &quot;ass number.&quot;
</em><br>
<p>Not as many as one might think.  See below.
<br>
<p><em>&gt; After all, should you ever come up with a design that you think will
</em><br>
<em>&gt; ensure Friendliness, you're not likely to have a fully rigorous
</em><br>
<em>&gt; mathematical proof that it will do so ... there will be a certain
</em><br>
<em>&gt; amount of informal reasoning required to follow your arguments.
</em><br>
<p>One of the subtle damages done by our humanly inherent political frame of 
<br>
mind is that, as do reporters or media, we tend to categorize things as 
<br>
&quot;provable&quot; or &quot;not provable&quot;.  Since nothing is &quot;provable&quot; enough to 
<br>
satisfy Leon Kass and the Precautionary Principle - since the political 
<br>
battle is unwinnable, or at least unwinnable through any amount of proof - 
<br>
well, why bother trying to prove things at all?  But if one is to clear 
<br>
away all political mud, it becomes apparent that there are many possible 
<br>
standards of rational evidence to apply.  In Friendly AI work I intend to 
<br>
apply a certain rule which says that I need a particular kind of strict 
<br>
specific expectation of a positive result before I, as a programmer, take 
<br>
any action - with any piece of code, initial conditions and rules for 
<br>
&quot;complex emergent dynamics&quot;, and so on, being considered as a special case 
<br>
of programmer action.  The strictness to which I refer, the required 
<br>
grounds for holding the expectation of success, is something that runs 
<br>
skew to the impossible standard of the rhetorical trick known as the 
<br>
Precautionary Principle, so Leon Kass would not be satisfied.  But so 
<br>
what?  Just because Leon Kass cannot be satisfied by any standard of 
<br>
proof, does not mean that I will commit suicide by relaxing my standard of 
<br>
justified expectation.
<br>
<p>There's a difference between an engineering expectation that a 
<br>
well-defined system accomplishes a well-defined result, which has not yet 
<br>
been qualified as a mathematical proof; and holding an excited expectation 
<br>
of success without even a clear criterion of success.  These are quite 
<br>
different levels of &quot;informality&quot;.
<br>
<p><em>&gt;&gt; 4)  Extremely extensive research shows that &quot;probabilities&quot; which
</em><br>
<em>&gt;&gt; people pull out of their asses (as opposed to being able to calculate
</em><br>
<em>&gt;&gt; them quantitatively) are not calibrated, that is, they bear
</em><br>
<em>&gt;&gt; essentially no relation to reality.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Sure, but your statement doesn't hold when applied to teams of
</em><br>
<em>&gt; scientists making careful estimates of probabilities events based on a
</em><br>
<em>&gt; combination of science, math and intuition.  In this case, estimates
</em><br>
<em>&gt; are still imperfect -- and errors happen -- but things are not as bad
</em><br>
<em>&gt; as you're alleging.
</em><br>
<p>Unless you are familiar with the literature here, my reply is:  &quot;Sorry, 
<br>
Ben, yours is a widespread opinion and in no sense an obviously stupid 
<br>
one, but it has been shown to be wrong.&quot;
<br>
<p>The most famous work in this area is with interviewers predicting student 
<br>
success and doctors making probabilistic clinical judgments, but these are 
<br>
just the most famous examples; the work has been applied to many domains. 
<br>
&nbsp;&nbsp;(I, alas, am only acquainted with the most famous studies, since this is 
<br>
not my primary field, but I know it's a robust result.)  Note that in the 
<br>
case of doctors, physicians not only have access to medical statistics and 
<br>
well-understood underlying models, but also extensive clinical experience 
<br>
with frequent feedback on cases much like the one being immediately 
<br>
considered.  So are doctors making clinical judgments of probability 
<br>
well-calibrated?  No, not at all.  See Robyn Dawes on &quot;The robust beauty 
<br>
of improper linear models&quot;, in &quot;Judgment under uncertainty&quot;, as well as a 
<br>
good many other papers nearby, but Dawes goes into the greatest depth on 
<br>
how tenaciously people hold on to their illusions of predictability 
<br>
(though this too is a robust result).
<br>
<p>In particular, Dawes explains how, even after supposed experts are shown 
<br>
that (a) the variables of interest are far less predictable than they 
<br>
thought and that (b) experts can be defeated by an improper linear model 
<br>
using *randomly chosen weights* with the correct sign, people still hold 
<br>
out endless hopes of the superiority of &quot;clinical judgment&quot;.  You can show 
<br>
people the evidence and they still won't believe.  They want to believe 
<br>
the variables are predictable.  They want to believe the experts can 
<br>
predict them.  They presume endlessly that something must have been wrong 
<br>
with this study and that study, even as the studies keep piling up. 
<br>
Dawes, in one of his books, narrates how after describing one study of 
<br>
clinical judgment, someone in the audience said, &quot;Well, you should have 
<br>
tested the judgment of [Famous Dr. X]&quot;, where, although Dawes couldn't say 
<br>
so at the time, Dr. X was in fact one of the experts tested.
<br>
<p>So, Ben, I'll believe that scientists making careful guesses of 
<br>
probabilities based on their equivalent of clinical judgment - in any case 
<br>
where &quot;intuition&quot; is part of the mix along with science and math - are in 
<br>
fact well-calibrated, when I see a study proving it.  Because it goes 
<br>
against everything I have heard from any study of human judgment.
<br>
<p>Humans fail at the task of probabilistic clinical judgment, performing 
<br>
more poorly than simple linear models, even in cases where the 
<br>
fundamentals are known and the expert has years of experience dealing with 
<br>
the same problem as the one presently at hand.  Not experience with 
<br>
&quot;simpler cases&quot; or &quot;test problems&quot;, experience with immediate feedback on 
<br>
many samples in the same context as the present problem.  Calibration gets 
<br>
worse, and overconfidence gets worse, as the difficulty of the problem 
<br>
increases.  So now the field of AI, with no grasp on the fundamentals, and 
<br>
no sample of cases from the same context, will offer a &quot;clinical judgment&quot; 
<br>
of the probability that an AI will be Friendly?  In a word:  No.
<br>
<p>This doesn't mean that building a Friendly AI is impossible.  It means 
<br>
that if it can be done, it will be done when someone looks over the design 
<br>
and says:  &quot;I believe that I understand every single thing that this 
<br>
design is supposed to accomplish; I believe that there is not one place 
<br>
where I am closing my eyes and hoping; I believe that I am no longer 
<br>
confused.  This design has massive overkill directed toward solving the 
<br>
problem, on a level I couldn't even have imagined when I was a neophyte. 
<br>
I know that availability is not the same as probability, but if I were 
<br>
naive enough to evaluate the availability, then I have difficulty in 
<br>
imagining how this design could fail at all, let alone how it could fail 
<br>
catastrophically.  And I know that emotional certainty is not the same as 
<br>
calibrated confidence, but if I were naive enough to see the problem 
<br>
emotionally, then I would say that what I see would inspire most people to 
<br>
have the emotional reaction they name '99.9% confidence'.&quot;
<br>
<p>If someone looks at an AI, and they think they understand all the 
<br>
fundamentals, and the AI is such as to inspire the emotional reaction that 
<br>
people usually name &quot;70% confidence&quot;, then the AI has maybe 1 chance in 
<br>
100 of working.  And if someone looks at the AI and they don't really know 
<br>
whether it will work, but they're sort of hoping, then they're committing 
<br>
elaborate suicide.
<br>
<p>Now you can believe that you understand a system and yet be wrong, as Leon 
<br>
Kass is bound to point out.  But so what?  How strong the statement &quot;I 
<br>
believe that I finally understand&quot; is as evidence depends on how lightly 
<br>
the person utters it, how given they are to wishful thinking, whether the 
<br>
person understands what it means to understand something.  It can be 
<br>
wrong, yes.  But to go ahead when you have such an infinitesimal 
<br>
understanding that even you, as an overconfident human, do not perceive 
<br>
one hell of an impressive understanding; well, that is suicide.
<br>
<p>P(actual-understanding|impressed-with-own-understanding) may be low.  But
<br>
p(impressed|actual-understanding) is damned high, when you consider how 
<br>
impressive an actual understanding would be.
<br>
<p><em>&gt;&gt; 6)  And finally, of course, the probabilities are not independent!
</em><br>
<em>&gt;&gt; If the best AI you can make isn't good enough, a million copies of it
</em><br>
<em>&gt;&gt; don't have independent chances of success.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This depends a great deal on the nature of the AI.
</em><br>
<p>Not really.  My rejoinder to this would sound a lot more impressive if we 
<br>
agreed on more details in the theory of optimization processes, but in 
<br>
English, my rejoinder reads:
<br>
<p>If anything kills you it should be an unknown unknown.  There is 
<br>
absolutely no excuse for being killed by a known unknown internal to the AI.
<br>
<p>Suppose you have a Russian roulette gun with a &quot;randomized&quot; (omitting 
<br>
Jaynes's cogent objections to the notion of randomization; perhaps it was 
<br>
quantum split) barrel containing one bullet.  You put the gun to your head 
<br>
and fire.  If you were to split yourself into a thousand (local, not 
<br>
Everett) copies, and play Russian roulette twenty times in succession, 
<br>
around twenty-six of you would be expected to survive.  But why are you 
<br>
playing Russian roulette to begin with?  If you are walking alone in the 
<br>
desert, and you see a *premanufactured* gun that delivers a million 
<br>
dollars into your lap on an empty barrel, or kills you on one chance out 
<br>
of six, then - it depends on your utilities - you might decide to risk 
<br>
your life on a play.  But if you are *building* the gun, you have no 
<br>
excuse.  You should design the gun not to do *anything* if it comes up on 
<br>
a loaded chamber.  You have that privilege, the privilege of not killing 
<br>
yourself, whenever you deal with a visible known unknown.  You do not have 
<br>
to build an AI which, depending on the value of the known unknown, is 
<br>
either Friendly or unFriendly, because you can instead build an AI which 
<br>
is either Friendly or stops.  You can always stop, and in fact, whenever 
<br>
you are *uncertain* you can halt temporarily, you don't need to wait for 
<br>
it to become a catastrophe, and this is why I've switched from the concept 
<br>
of &quot;measure utilities and always pick the most desirable&quot; to &quot;pick the 
<br>
most desirable action, unless the entropy in the decision system is too 
<br>
high, in which case the optimization process itself suspends&quot;.  You don't 
<br>
need to shoot yourself; this is a basic precept of FAI development.  You 
<br>
are creating something, designing a process flow, and if you ever see a 
<br>
branch of the process flow that depends on a known unknown and goes into 
<br>
either &quot;Friendly&quot; or &quot;unFriendly&quot;, you black out the part of the flowchart 
<br>
that reads &quot;unFriendly&quot; and put in &quot;process halts&quot;.  One of the stunning 
<br>
realizations I've had recently is that you can always do this.
<br>
<p>There is no excuse for being killed by a known unknown.  If you are going 
<br>
to die with even the slightest dignity, so that your epitaph reads 
<br>
&quot;stupidity&quot; rather than &quot;suicide&quot;, you should be killed by an unknown 
<br>
unknown, a basic design flaw, a misunderstanding of the underlying 
<br>
philosophy.  There is no excuse for being killed by a flowchart that does 
<br>
what you thought it would.  If you knew, you should have made a different 
<br>
flowchart.  Now why would a basic design flaw be an independent 
<br>
probability for a million copies of the best AI you can make?
<br>
<p><em>&gt; If you're creating an AI that is a dynamical system, and you're
</em><br>
<em>&gt; building an initial condition and letting it evolve in a way that is
</em><br>
<em>&gt; driven partially by environmental influences, then if you run many
</em><br>
<em>&gt; copies of it independently
</em><br>
<em>&gt; 
</em><br>
<em>&gt; a) of course, the dynamics of the different instances are not 
</em><br>
<em>&gt; probabilistically independent
</em><br>
<em>&gt; 
</em><br>
<em>&gt; b) nevertheless, they may be VERY far from identical, and may come to a
</em><br>
<em>&gt; wide variety of different outcomes
</em><br>
<em>&gt; 
</em><br>
<em>&gt; My dreamed idea (which wasn't so serious, by the way!) did not rely
</em><br>
<em>&gt; upon the assumption of complete probabilistic independence between
</em><br>
<em>&gt; multiple evolving instances of the AI.  It did rely upon the idea of a
</em><br>
<em>&gt; self-modifying AI as a pragmatically-nondeterministic,
</em><br>
<em>&gt; environmentally-coupled system rather than a strongly-deterministic
</em><br>
<em>&gt; system like an auto engine.
</em><br>
<p>An auto engine contains much entropy and so is about as &quot;nondeterministic&quot; 
<br>
as any other physical system of roughly the same temperature (depending on 
<br>
molecular degrees of freedom, but, whatever).  What makes an auto engine 
<br>
&quot;strongly deterministic&quot;, it seems, is that we understand how an auto 
<br>
engine works and so it reliably performs the function that we wish of it, 
<br>
which is all that we actually care about, the molecular degrees of freedom 
<br>
dropping out of the equation as irrelevant to our goal processes.  It's 
<br>
not how much entropy is physically in the system, it's how much entropy we 
<br>
care about, and how much entropy there is in that one all-important bit, 
<br>
&quot;success or failure&quot;; what makes an auto engine &quot;strongly deterministic&quot;, 
<br>
is not being maintained at zero kelvin, but that, from a functional 
<br>
perspective, it works reliably because it was designed with understanding.
<br>
<p><em>&gt;&gt; What's wrong with this picture:
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt; a)  Confusing plausibility with frequency; b)  Assigning something
</em><br>
<em>&gt;&gt; called a &quot;probability&quot; in the absence of a theory powerful enough to
</em><br>
<em>&gt;&gt; calculate it quantitatively; c)  Treating highly correlated
</em><br>
<em>&gt;&gt; probabilities as independent; d)  Applying the Principle of
</em><br>
<em>&gt;&gt; Indifference to surface outcomes rather than elementary
</em><br>
<em>&gt;&gt; interchangeable events; and e)  Attempting to trade off not knowing
</em><br>
<em>&gt;&gt; how to solve a problem for confessing a &quot;1%&quot; probability of success.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Sorry, I am definitely not guilty of errors a, c or d
</em><br>
<em>&gt; 
</em><br>
<em>&gt; As for error b, I don't think it's bad to call an unknown quantity a 
</em><br>
<em>&gt; probability just because I currently lack the evidence to calculate the
</em><br>
<em>&gt;  value of the quantity.  b is not an error.
</em><br>
<p>I think it's okay to use the word &quot;probability&quot;, as in, &quot;this variable has 
<br>
a probability but I don't know what it is&quot;, but not to use words like &quot;20% 
<br>
probability&quot; if they are based on subjective impressions rather than 
<br>
quantitative calculation or samples from a defined context.  &quot;20% sure&quot; 
<br>
would be better, but the number &quot;20%&quot; in this case measures not 
<br>
probability but psychological support, and it cannot be manipulated 
<br>
mathematically like a probability.
<br>
<p><em>&gt; As for e, there may be some problems for which there are no 
</em><br>
<em>&gt; guaranteed-successful solutions, only solutions that have a reasonable 
</em><br>
<em>&gt; probability of success.  You seem highly certain that Friendly AI does
</em><br>
<em>&gt; not lie in this class, but you have given no evidence in favor of your 
</em><br>
<em>&gt; assertion.
</em><br>
<p>If the FAI runs on reliable hardware, there should be no legitimate source 
<br>
of uncertainty from quantum branching or thermal vibrations, which are the 
<br>
legitimate sources of irreducible randomness in physical processes. 
<br>
Another alternative is that we are not dealing with irreducible randomness 
<br>
but with confusion on the part of the designer, who is 20% sure that the 
<br>
AI will work right, which is not at all the same as an AI with a 20% 
<br>
probability of working.  There is no excuse for being killed by a known 
<br>
unknown internal to the AI.
<br>
<p>I can think offhand of three classes of known unknows which are calculably 
<br>
probabilistic but not accessible to the AI, black boxes with definite 
<br>
contents, but I am not going to say what they are; if you want to make a 
<br>
case for the Friendliness or unFriendliness of the AI depending on such a 
<br>
variable you will have to make the case for yourself.  It sounds too 
<br>
exotic to me.  The only reason the scenario *has any intuitive appeal* 
<br>
(despite the conditions for fulfillment being so exotic) is that people 
<br>
are 20% sure their system will work, so it seems plausible that the system 
<br>
might have a 20% probability of working, and yet these are fundamentally 
<br>
different and inconvertible quantities.
<br>
<p><em>&gt;&gt; And if you're wondering why I'm so down on this, it's because it
</em><br>
<em>&gt;&gt; seems to me like yet another excuse for not knowing how to build a
</em><br>
<em>&gt;&gt; Friendly AI.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Actually, I *do* know how to build a Friendly AI.... ;-)
</em><br>
<p>Heh.  This statement caused much excitement on the #sl4 IRC channel.  I 
<br>
had to explain that this meant that you now thought you knew how to build 
<br>
an AI, and that the word &quot;Friendly&quot; was there for luck.
<br>
<p>Personally, I do *not* yet know how to build a Friendly process.  I rather 
<br>
doubt that you could define Friendliness.  Last time I checked you were 
<br>
still claiming that you knew it to be impossible to resolve the basic 
<br>
theoretical confusions without a chimp-level mind to experiment on.
<br>
<p>Please don't use the word &quot;Friendly&quot; so lightly.  There are plenty of 
<br>
available alternatives, like &quot;moral AI&quot; or for that matter &quot;friendly AI&quot;. 
<br>
&nbsp;&nbsp;Part of the reason I went to the trouble of capitalizing it was so that 
<br>
people who mean, i.e., &quot;I want to build an AI and I hope it will be a nice 
<br>
person&quot; could say &quot;I want to build a friendly AI&quot;.  &quot;I know how to build a 
<br>
Friendly AI&quot; is one hell of a strong claim over and above the weaker claim 
<br>
that you merely know how to build a living mind from scratch.
<br>
<p><em>&gt; [I wasn't so sure a year ago, but recent simplifications in the
</em><br>
<em>&gt; Novamente design (removing some of the harder-to-control components and
</em><br>
<em>&gt; replacing them with probabilistic-inference-based alternatives) have
</em><br>
<em>&gt; made me more confident.]
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But I can't *guarantee* this AI will be Friendly no matter what; all I
</em><br>
<em>&gt; can give are reasonable intuitive arguments why it will be Friendly.
</em><br>
<em>&gt; The probability of the Friendliness outcome is not easy to calculate,
</em><br>
<em>&gt; as you've pointed out so loquaciously.
</em><br>
<p>I am not asking for a guarantee.  But what you call &quot;intuitive argument&quot; 
<br>
seems to me philosophicalish thinking, driven by wishes and hopes, not 
<br>
binding on Nature.  Last time I checked, you said that you didn't think 
<br>
the fundamental problems of FAI were solvable without a chimp-level mind 
<br>
to experiment on (very clever excuse, that), so obviously you have not 
<br>
already solved the fundamental problems.  The standard to which I hold is 
<br>
not a mathematical proof, but a technical argument - not an intuitive 
<br>
philosophicalish argument, a technical argument - which includes technical 
<br>
definitions of everything that you wish to achieve, and a walkthrough of 
<br>
the processes showing that they produce behaviors which achieve those 
<br>
definitions; the sort of technical argument that would be given by someone 
<br>
who had solved all the foundational problems and taken the entire problem 
<br>
out of the domain of philosophy.  When I have that, I will then be willing 
<br>
to say at last:  &quot;I do know how to build a Friendly AI.&quot;  And perhaps Leon 
<br>
Kass will not consider it proof, but at least I will no longer consider it 
<br>
suicide.
<br>
<p><em>&gt; And then I wonder whether that my &quot;reasonable intuitive arguments&quot; ----
</em><br>
<em>&gt; and *all* human arguments, whether we consider them rigorous or not;
</em><br>
<em>&gt; even our best mathematical proofs --- kinda fall apart and reveal
</em><br>
<em>&gt; limitations when we get into the domain of vastly transhuman
</em><br>
<em>&gt; intelligence.
</em><br>
<p>&quot;Reasonable intuitive arguments&quot; have been exhaustively demonstrated to 
<br>
fall apart on guessing why Aunt Mabel is coughing, let alone in the domain 
<br>
of vastly transhuman intelligence.
<br>
<p>There is a huge spectrum between a mathematical proof and a &quot;reasonable 
<br>
intuitive argument&quot;.  (I assume a &quot;reasonable intuitive argument&quot; applied 
<br>
to AI is somewhere between a doctor's &quot;informed clinical judgment&quot; minus 
<br>
the well-understood fundamentals and any past experience, and the wishful 
<br>
thinking of Greek philosophers.)  What is needed is something at the high 
<br>
end of the reliability spectrum, a specific, purely technical definition 
<br>
of all desirable properties of the outcome and a purely technical 
<br>
walkthrough showing that the employed processes work to this end.  Note 
<br>
that I say &quot;walkthrough showing that&quot;, not &quot;reasons why&quot; or &quot;arguments 
<br>
for&quot;.  History shows that reasons why and arguments for are not binding on 
<br>
Nature, but once a problem has been understood, an engineering walkthrough 
<br>
sometimes works.
<br>
<p><em>&gt; So I tend to apply (a mix of rigorous and intuitive)
</em><br>
<em>&gt; probabilistic thinking when thinking about transhuman AI's that are
</em><br>
<em>&gt; just a bit smarter than humans ... and then rely on ignorance-based
</em><br>
<em>&gt; Principle of Indifference type thinking, when thinking about transhuman
</em><br>
<em>&gt; AI's that are vastly, vastly smarter than any of us.
</em><br>
<p>Is there even one example from the history of science where Nature has not 
<br>
replied to this sort of argument with, &quot;So what?&quot;  Einstein, maybe, but he 
<br>
had math in which to phrase his intuitions.
<br>
<p>Here is the lesson of my own experience in this area:  Ignorance does not 
<br>
work!  If you do not know, you are screwed no matter how clever you are 
<br>
about working around your ignorance.  When you have fundamental problems 
<br>
you must solve them.  If you do not solve them it is like asking a Greek 
<br>
philosopher to guess that the nature of matter is quantum physics, you're 
<br>
just screwed.  It doesn't matter how clever you are, you're just screwed.
<br>
<p>I do not know everything about Friendly AI - there are N fundamental 
<br>
problems of which I have only solved M - but whenever I solve a problem X, 
<br>
it immediately becomes apparent that had I tried to build a Friendly AI 
<br>
without first solving problem X, I would have just been screwed.  Repeat 
<br>
this experience a few times and you start to see that the only virtue of 
<br>
ignorance is that, being ignorant of how to solve the problem, one is also 
<br>
ignorant of the impossibility of success without knowledge.
<br>
<p>I am speaking of &quot;virtue&quot; from a political standpoint, of course; 
<br>
ignorance makes a very convenient argument, especially when speaking to an 
<br>
audience that can be counted to nod along sympathetically when you confess 
<br>
to not knowing.  The humble confession of ignorance fuzzes out every 
<br>
counterargument; why, you may be certain the Earth goes around the Sun, 
<br>
good sir, but how can any of us really know anything?  I am willing to 
<br>
concede it is possible, perhaps, but who really knows?  Your inexplicable 
<br>
confidence in heliocentrism, dear sir, can only be the mark of 
<br>
irrationality; reasonable folk are willing to admit when they do not know...
<br>
<p>It is much easier to convince people that no one really knows, and hence, 
<br>
why not hope? than to propound the implausible, arrogant, elitist 
<br>
proposition that YOU have understood what others do not.  And yet assuming 
<br>
success and looking backward, anyone who builds a Friendly AI will need to 
<br>
have understood one hell of a lot of stuff.  One who humbly admits to a 
<br>
lack of understanding may, perhaps, deserve our thanks for their honesty, 
<br>
time and funding to think and learn and grow their understanding further; 
<br>
but they will not build a Friendly AI, honest ignorance is just not good 
<br>
enough for that.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7720.html">Mitchell Porter: "Re: What exactly is &quot;panpsychism&quot;?"</a>
<li><strong>Previous message:</strong> <a href="7718.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>In reply to:</strong> <a href="7704.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7722.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<li><strong>Reply:</strong> <a href="7722.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<li><strong>Reply:</strong> <a href="7723.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7719">[ date ]</a>
<a href="index.html#7719">[ thread ]</a>
<a href="subject.html#7719">[ subject ]</a>
<a href="author.html#7719">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:43 MDT
</em></small></p>
</body>
</html>
