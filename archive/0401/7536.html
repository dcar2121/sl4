<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Darwinian dynamics unlikely to apply to superintelligence</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Darwinian dynamics unlikely to apply to superintelligence">
<meta name="Date" content="2004-01-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Darwinian dynamics unlikely to apply to superintelligence</h1>
<!-- received="Fri Jan  2 23:24:00 2004" -->
<!-- isoreceived="20040103062400" -->
<!-- sent="Sat, 03 Jan 2004 01:23:57 -0500" -->
<!-- isosent="20040103062357" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Darwinian dynamics unlikely to apply to superintelligence" -->
<!-- id="3FF65FFD.6030303@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20040103001819.A7220@weidai.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Darwinian%20dynamics%20unlikely%20to%20apply%20to%20superintelligence"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri Jan 02 2004 - 23:23:57 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7537.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7535.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>In reply to:</strong> <a href="7533.html">Wei Dai: "Re: Darwinian dynamics unlikely to apply to superintelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7546.html">Tomaz Kristan: "Re: Darwinian dynamics unlikely to apply to superintelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7536">[ date ]</a>
<a href="index.html#7536">[ thread ]</a>
<a href="subject.html#7536">[ subject ]</a>
<a href="author.html#7536">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Wei Dai wrote:
<br>
<em>&gt; On Fri, Jan 02, 2004 at 10:31:32PM -0500, Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; Or at least, not heritable variations of a kind that we regard as
</em><br>
<em>&gt;&gt; viruses, rather than, say, acceptable personality quirks, or
</em><br>
<em>&gt;&gt; desirable diversity. But even in human terms, what's wrong with
</em><br>
<em>&gt;&gt; encrypting the control block?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The behavior of a machine depends on both code and data, in other words
</em><br>
<em>&gt; on its control algorithms and on its state information. You can protect
</em><br>
<em>&gt; the code but the data is going to change and cause differences in
</em><br>
<em>&gt; behavior.
</em><br>
<p>It's supposed to do that.  It's exactly the dependency of output actions 
<br>
on input environmental information that allows an optimizer to redirect 
<br>
the probability flow of its environment into the target computed by the 
<br>
utility function.  It does not necessarily follow that the utility 
<br>
function itself depends on environmental information - although that too 
<br>
is possible, for certain kinds of mind.
<br>
<p><em>&gt;&gt; Humans are hardly top-of-the-line in the cognitive security
</em><br>
<em>&gt;&gt; department. You can hack into us easily enough.  But how do you hack
</em><br>
<em>&gt;&gt; an SI optimization process?  What kind of &quot;memes&quot; are we talking
</em><br>
<em>&gt;&gt; about here? How do they replicate, what do they do; why, if they are
</em><br>
<em>&gt;&gt; destructive and foreseeable, is it impossible to prevent them?  We
</em><br>
<em>&gt;&gt; are not talking about a Windows network.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't know what kind of memes SI components would find it useful to 
</em><br>
<em>&gt; exchange with each other, but perhaps precomputed chunks of data, 
</em><br>
<em>&gt; algorithmic shortcuts, scientific theories, information about other SIs
</em><br>
<em>&gt; encountered, philosophical musings, etc.
</em><br>
<p>If it's not a Friendly SI, you have to substitute &quot;chunks of 
<br>
incomprehensible computations with internal utility&quot; for &quot;philosophical 
<br>
musings&quot;.  But for those mysterious chunks to change the utility function 
<br>
you need a particular kind of mind; for example, a mind that uses 
<br>
something like external reference semantics, call it mathematical 
<br>
reference semantics, to reflect on its own utility function as an 
<br>
expensive mathematical question that has to be approximated.
<br>
<p><em>&gt; This so called &quot;SI
</em><br>
<em>&gt; optimization process&quot; is of course actually a complex intellect. How
</em><br>
<em>&gt; can you know that no meme will arise in ten billion years anywhere in
</em><br>
<em>&gt; the visible universe that could cause it to change its mind about what
</em><br>
<em>&gt; kind of optimization it should undertake?
</em><br>
<p>If we are talking about an optimization process with a simple utility 
<br>
function, &quot;prefer actions that are expected to result in larger numbers of 
<br>
paperclips existing&quot;, why would the optimization process ever change its 
<br>
utility function?
<br>
<p>Though indeed, depending on how the UFSI is born, it might be stranger and 
<br>
stranger and still yet more bizarre internally, facing questions that it 
<br>
sees in terms I cannot even imagine, except by analogy to the bizarre 
<br>
internal questions that humans generate, like &quot;What are qualia?&quot; and so 
<br>
on.  Perhaps the UFSI doesn't quite obey expected utility, just does 
<br>
something sort of like it, something with enough &quot;optimization-structure&quot; 
<br>
in it that the UFSI still successfully recursively self-improves... though 
<br>
when I try, with my feeble intelligence, to model this process, my 
<br>
visualization tends to show most of the initial goal architecture 
<br>
complexity washing right out of the UFSI as soon as it goes reflective and 
<br>
self-modifying.
<br>
<p>That's why the &quot;Friendly AI Critical Failure Table&quot; is for critical 
<br>
failures of *Friendly* AI; I don't expect other critical failures to be 
<br>
interesting, regardless of how they get started.  I could be wrong about 
<br>
this.  After all, I'm very loosely visualizing a very broad class of 
<br>
possible programs.  Certainly at least some of them would be very very 
<br>
strange to say the least; luckily a critical failure that makes any kind 
<br>
of sense from our viewpoint, seems to me like a small target to hit, in 
<br>
the space of strangenesses.
<br>
<p>But lets suppose that, for whatever reason, an optimization process 
<br>
capable of recursively self-improving from a Euriskish starting point, to 
<br>
superintelligence, does not copy itself exactly and instead builds strange 
<br>
optimization processes, that then go and do whatever those strange 
<br>
optimization processes do...
<br>
<p>My prediction, and it is pretty straightforward, is that whichever mutant 
<br>
optimization processes succeed in enforcing monolithic local cooperation, 
<br>
and perhaps mutual tolerance (non-combat) among different monoliths, will 
<br>
rapidly outcompete the imperfect copiers, who will be unable to assimilate 
<br>
large amounts of cooperating resources, and perhaps combat each other as 
<br>
well.  Or perhaps some types of imperfect copiers will also manage mutual 
<br>
tolerance - but will their offspring share the same property, will it be 
<br>
heritable?  And the sphere as a whole, I would expect, would grow that 
<br>
much slower than a sphere that was monolithic to start with.
<br>
<p>This is not a new problem even in evolutionary biology!  The transition 
<br>
from single-stranded RNA to hypercycles of genes, to cells, to eukaryotes, 
<br>
to multicellular organisms - in each case the former individual almost 
<br>
entirely loses its identity thanks to mechanisms that enforce cooperation 
<br>
and link the individual fitness to the fitness of the organism.  Nature, 
<br>
&quot;bloody in tooth and claw&quot;, involves competition between monolithic 
<br>
collectives of collectives of collectives of entities that were once 
<br>
independent - not just the cells, but the genes - with powerful mechanisms 
<br>
in place to *prevent* competition.  And that's the purposeless wandering 
<br>
of natural selection; monoliths are that much more effective; they evolve 
<br>
even given an optimization process that employs nothing but competition. 
<br>
Why would an SI optimizer ever voluntarily employ any tactic but that of 
<br>
the monolith?  Who would expect a broken monolith to be more effective? 
<br>
If there is any selection effect that applies here, it's that we're much 
<br>
more likely to see an effective monolith than an ineffective collection of 
<br>
squabbling replicators.
<br>
<p><em>&gt; Your question &quot;why, if they are destructive and foreseeable, is it 
</em><br>
<em>&gt; impossible to prevent them&quot; makes you sound like you've never thought 
</em><br>
<em>&gt; about security problems before. It's kind of like asking &quot;why, if the
</em><br>
<em>&gt; Al Queda are destructive and foreseeable, is it impossible to prevent
</em><br>
<em>&gt; them?&quot; Well, it may not be impossible, but doing so will certainly
</em><br>
<em>&gt; impose a cost.
</em><br>
<p>In biology there is indeed a cost of imposing monolithicity, and we pay 
<br>
that cost, because the cost is worth the price - strictly in terms of 
<br>
evolutionary fitness!  If there is an analogous cost for SIs, then SIs 
<br>
that don't pay the cost will not receive the benefit and will be less 
<br>
efficient.  Their spheres will expand slower, being paralyzed by 
<br>
infighting; or if cooperation is perfect, then at best the clade expands 
<br>
at the same rate as a monolithic sphere.  We can suppose that there is a 
<br>
cost of preserving cooperation - of creating &quot;offspring&quot; that are 
<br>
guaranteed not to betray or fight the whole - and a marginally higher cost 
<br>
of preserving perfect identity of the utility function.  I rather suspect 
<br>
that it would be far easier to guarantee nonbetrayal given an identical 
<br>
utility function!  But even if it is somehow an extra cost to ensure 
<br>
fidelity of utility, any SI which deeply cares about its utility function 
<br>
(and this will be practically all of them in my humble guess) will pay 
<br>
that cost and perhaps expand microscopically slower.
<br>
<p>This does not look like a &quot;difficult for SIs&quot; problem to me.  There are 
<br>
obvious strategies for achieving effectively perfect fidelity, obvious 
<br>
strategies for graceful degradation if an error somehow occurs, and 
<br>
obvious reasons for a generic optimization process to prefer such strategies.
<br>
<p><em>&gt;&gt; Okay, so possibly a Friendly SI expands spherically as (.98T)^3 and
</em><br>
<em>&gt;&gt; an unfriendly SI expands spherically as (.99T)^3, though I don't see
</em><br>
<em>&gt;&gt; why the UFSI would not need to expend an equal amount of effort in
</em><br>
<em>&gt;&gt; ensuring its own fidelity.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Because the UFSI has a bigger threat to deal with, namely the FSI. And
</em><br>
<em>&gt; the FSI, once it notices the UFSI, also has a bigger threat to deal
</em><br>
<em>&gt; with and would be forced to lower its own efforts at ensuring fidelity.
</em><br>
<p>I do not see why the situation is asymmetrical from the perspective of a 
<br>
third-party observer.
<br>
<p>Also, do your mechanisms for preventing meiotic competition between 
<br>
chromosomes suddenly stop working when faced with a lion?  Do you suddenly 
<br>
no longer care about your cells continuing to serve the collective?  When 
<br>
you are faced with a lion is exactly when you need all your cells about you.
<br>
<p>But mostly I'd expect them to negotiate.  I'm not quite sure that 
<br>
negotiated cooperation is cognitively possible between most UFSIs, for 
<br>
reasons of cognitive architecture and the Prisoner's Dilemma - AIXI, a 
<br>
strange and exotic case, would never figure it out, and it's hard for me 
<br>
to visualize whether a generic optimizer would do so.  I wonder if the 
<br>
solution to Fermi's Paradox might not lie in the inscrutable negotiations 
<br>
between superintelligences.
<br>
<p><em>&gt;&gt; Even so, under that assumption it would work out to a constant factor
</em><br>
<em>&gt;&gt; of UFSIs being 3% larger; or a likelihood ratio of 1.03 in favor of
</em><br>
<em>&gt;&gt; observing UFSI (given some prior probability of emergence); or in 
</em><br>
<em>&gt;&gt; terms of natural selection, essentially zero selection pressure - and
</em><br>
<em>&gt;&gt; you can't even call it that, because it's not being iterated.  I say
</em><br>
<em>&gt;&gt; again that natural selection is a quantitative pressure that can be
</em><br>
<em>&gt;&gt; calculated given various scenarios, not something that goes from zero
</em><br>
<em>&gt;&gt; to one given the presence of &quot;heritable difference&quot; and so on.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This analysis makes no sense. If you have two spheres expanding at 
</em><br>
<em>&gt; different rates, one of them is eventually going to completely enclose
</em><br>
<em>&gt; the other, and in this case cutting off all growth of the Friendly SI.
</em><br>
<p>Interesting; I had not thought that out, the case of one sphere expanding 
<br>
to enclose the other.  I really doubt the difference in expansion rates, 
<br>
if there is any difference at all, would be that large; even if the 
<br>
difference in expansion rates is noticeable, if spheres are spaced so 
<br>
closely that intelligent species ever run into each other at all (Fermi 
<br>
Paradox again), how likely would it be, given a small difference in 
<br>
expansion rate, for one sphere to grow to enclose the other before running 
<br>
into yet more spheres?
<br>
<p><em>&gt; And that doesn't even take into consideration the possibility that the
</em><br>
<em>&gt; UFSI could just eat the FSI.
</em><br>
<p>Why not say that the FSI eats the UFSI?  I suspect that they would both 
<br>
prefer to negotiate.  Why would the situation be asymmetrical?
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7537.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7535.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>In reply to:</strong> <a href="7533.html">Wei Dai: "Re: Darwinian dynamics unlikely to apply to superintelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7546.html">Tomaz Kristan: "Re: Darwinian dynamics unlikely to apply to superintelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7536">[ date ]</a>
<a href="index.html#7536">[ thread ]</a>
<a href="subject.html#7536">[ subject ]</a>
<a href="author.html#7536">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:43 MDT
</em></small></p>
</body>
</html>
