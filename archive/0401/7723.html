<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Dispersing AIs throughout the universe</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Dispersing AIs throughout the universe">
<meta name="Date" content="2004-01-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Dispersing AIs throughout the universe</h1>
<!-- received="Sat Jan 10 06:38:57 2004" -->
<!-- isoreceived="20040110133857" -->
<!-- sent="Sat, 10 Jan 2004 08:40:37 -0500" -->
<!-- isosent="20040110134037" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Dispersing AIs throughout the universe" -->
<!-- id="BAEAIIMDCMDAEKHBFGFKAEAJCJAA.ben@goertzel.org" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3FFFD615.6080102@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Dispersing%20AIs%20throughout%20the%20universe"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sat Jan 10 2004 - 06:40:37 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7724.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<li><strong>Previous message:</strong> <a href="7722.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<li><strong>In reply to:</strong> <a href="7719.html">Eliezer S. Yudkowsky: "Re: Dispersing AIs throughout the universe"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7723">[ date ]</a>
<a href="index.html#7723">[ thread ]</a>
<a href="subject.html#7723">[ subject ]</a>
<a href="author.html#7723">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer,
<br>
<p>This is a response to your comments on the accuracy of human inference.
<br>
<p>First, about the accuracies of the probability judgments made by scientists.
<br>
I have read a lot of papers on this topic, mostly in the late 1990's, when I
<br>
briefly worked with a behavioral psychologist who was studying the uncanny
<br>
accuracy with which pigeons can carry out probabilistic inference, in some
<br>
contexts.
<br>
<p>Clearly this is a subtle topic, and humans (or human groups) are much more
<br>
accurate in some contexts than in others.
<br>
<p>The most disturbing thing that I recall from reading that literature was how
<br>
greatly human inference accuracy varies depending on the context.  For
<br>
example, expert poker players are damn good at making intuitive probability
<br>
assessments in a poker context, but if you give them formally similar
<br>
problems in a domain other than poker, their ability to make probability
<br>
assessments doesn't carry over nearly as easily as one would like.
<br>
<p>While humans are much better than current AI systems at generalizing
<br>
abilities from one context to another, we're not all THAT good at it.  The
<br>
abilities we develop are, in many cases, peculiarly over-tied to the
<br>
specific domain in which they're developed.
<br>
<p>This relates to the reason why I think playing with chimp-level AGI's will
<br>
be very useful in order to teach us how to think about, and theorize about,
<br>
and teach, human-level AGI's.  Not only will we derive new scientific and
<br>
mathematical theories from playing with the chimp-level AGI's, we will also
<br>
build our own domain-specific inferential intuitions regarding AGI and
<br>
Friendly AI.
<br>
<p>The results you cite about doctors are familiar to me, and did not surprise
<br>
me at all when I learned them.  The cognitive processes going on in clinical
<br>
physicians are not so similar to those going on in research scientists or
<br>
engineers.  The culture is also very, very different.  Human inference
<br>
quality is highly context-dependent, and some contexts lend themselves more
<br>
to accurate inference than others.  There are a lot of reasons why clinical
<br>
medicine would NOT be expected to correlate with high-quality inference.
<br>
These are related (though not identical) to the reasons why  medical expert
<br>
systems have been systematically rejected throughout the medical
<br>
establishment, even though they demonstrably give better diagnostic accuracy
<br>
than human doctors.  Clearly, this is not a community that values
<br>
inferential accuracy so much, as compared to other things!
<br>
<p>-- Ben G
<br>
<p><p><p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20Dispersing%20AIs%20throughout%20the%20universe">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20Dispersing%20AIs%20throughout%20the%20universe">owner-sl4@sl4.org</a>]On Behalf Of Eliezer
</em><br>
<em>&gt; S. Yudkowsky
</em><br>
<em>&gt; Sent: Saturday, January 10, 2004 5:38 AM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=RE:%20Dispersing%20AIs%20throughout%20the%20universe">sl4@sl4.org</a>
</em><br>
<em>&gt; Subject: Re: Dispersing AIs throughout the universe
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ben Goertzel wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&gt; Ben Goertzel wrote:
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt;&gt; Eliezer,
</em><br>
<em>&gt; &gt;&gt;&gt;
</em><br>
<em>&gt; &gt;&gt;&gt; It might be significantly easier to engineer an AI with a 20% or 1%
</em><br>
<em>&gt; &gt;&gt;&gt;  (say) chance of being Friendly, than to engineer one with a 99.99%
</em><br>
<em>&gt; &gt;&gt;&gt;  chance of being Friendly.  If this is the case, then the
</em><br>
<em>&gt; &gt;&gt;&gt; broad-physical-dispersal approach that I suggested makes sense.
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt; 1)  I doubt that it is &quot;significantly easier&quot;.  To get a 1% chance
</em><br>
<em>&gt; &gt;&gt; you must solve 99% of the problem, as 'twere.  It is no different
</em><br>
<em>&gt; &gt;&gt; from trying to build a machine with a 1% chance of being an internal
</em><br>
<em>&gt; &gt;&gt; combustion engine, a program with a 1% chance of being a spreadsheet,
</em><br>
<em>&gt; &gt;&gt; or a document with a 1% chance of being well-formed XML.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Actually, the analogies you're making are quite poor, because internal
</em><br>
<em>&gt; &gt; combustion engines and spreadsheets and XML documents are not complex
</em><br>
<em>&gt; &gt; self-organizing systems.
</em><br>
<em>&gt;
</em><br>
<em>&gt; An internal combustion engine is a complex dynamic system.  The gasoline
</em><br>
<em>&gt; flows, each molecule bumping into other molecules, its fluidity
</em><br>
<em>&gt; determined
</em><br>
<em>&gt; by details of the electromagnetic interaction between molecules relative
</em><br>
<em>&gt; to the prevailing temperature, yet the fluid dynamics as a whole can be
</em><br>
<em>&gt; excellently simplified to equations that describe quite different
</em><br>
<em>&gt; quantities; when electricity flows through the spark plugs it obeys
</em><br>
<em>&gt; Maxwell's Equations, and when the gas mixed with oxygen explodes the
</em><br>
<em>&gt; explosion ripples out in obedience to differential equations.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Having seen, from other fields beyond AI, what it means to &quot;understand&quot; a
</em><br>
<em>&gt; system, and having recently finally understood a thing or two
</em><br>
<em>&gt; about AI for
</em><br>
<em>&gt; the first time, I now realize that &quot;complex dynamic system&quot; means &quot;I do
</em><br>
<em>&gt; not understand which particular dynamics are involved&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Emergence&quot; is sort of like the way that fluid dynamics can be usefully
</em><br>
<em>&gt; simplified to high-level equations that are unlike the deep kinetic and
</em><br>
<em>&gt; electromagnetic equations, except that in Artificial Intelligence
</em><br>
<em>&gt; the word
</em><br>
<em>&gt; &quot;emergent&quot; means &quot;without understanding either level&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Self-organizing&quot;?  Well, now there's a wonderful magic wand of a
</em><br>
<em>&gt; word, to
</em><br>
<em>&gt; be used whenever something mysterious happens, and indeed
</em><br>
<em>&gt; &quot;self-organizing&quot; seems to make a fairly good synonym for &quot;mysterious&quot;.
</em><br>
<em>&gt; If I were ever to use the word, while still aspiring to my usual
</em><br>
<em>&gt; standards
</em><br>
<em>&gt; for well-definedness, I would pick some more rigorous criterion of usage,
</em><br>
<em>&gt; like, say, &quot;self-organizing&quot; referring to a multiplicity of locally
</em><br>
<em>&gt; centered optimization pressures interacting to create an optimization
</em><br>
<em>&gt; pressure on some higher-level property of the system, c.f. &quot;emergence&quot;
</em><br>
<em>&gt; above.  Or perhaps &quot;self-organizing&quot; could also be extended to
</em><br>
<em>&gt; cases where
</em><br>
<em>&gt; an external optimization pressure, like natural selection, builds
</em><br>
<em>&gt; a system
</em><br>
<em>&gt; where distributed local properties and their interactions create the
</em><br>
<em>&gt; systemic behaviors that were subject to the external optimization
</em><br>
<em>&gt; pressure.  But here I am holding myself to much higher standards
</em><br>
<em>&gt; than most
</em><br>
<em>&gt; people do when they use so marvelous and poetic a word as
</em><br>
<em>&gt; &quot;self-organizing&quot;.  Mostly, IMO, it is used much like &quot;phlogiston&quot; in an
</em><br>
<em>&gt; earlier era.  Where does the organization come from?  It's
</em><br>
<em>&gt; self-organizing!
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; With Friendly AI, we're talking about
</em><br>
<em>&gt; &gt; creating an initial condition, letting some dynamics run (interactively
</em><br>
<em>&gt; &gt; with the environment, which includes us), and then continually nudging
</em><br>
<em>&gt; &gt; the dynamics to keep them running in a Friendly direction.  This is a
</em><br>
<em>&gt; &gt; very different --- and plausibly, much less deterministic -- process
</em><br>
<em>&gt; &gt; than building a relatively static machine like a car engine or a
</em><br>
<em>&gt; &gt; spreadsheet.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It sounds to me like a description that applies quite well to a car
</em><br>
<em>&gt; engine.  A car engine has initial conditions, check.  Dynamics, check.
</em><br>
<em>&gt; Nudging, check.  The difference is not in determinism, the difference is
</em><br>
<em>&gt; whether the designer is mystified by what is allegedly going on.  A car
</em><br>
<em>&gt; engine is nothing to sneer at; it is a work of art built by people who
</em><br>
<em>&gt; understood the dynamics.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt; 2)  Ignoring (1), and supposing someone built an AI with a 1% real
</em><br>
<em>&gt; &gt;&gt; chance of being Friendly, I exceedingly doubt its maker would have
</em><br>
<em>&gt; &gt;&gt; the skill to calculate that as a quantitative probability.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Of course, that's true.  But it's also true that, if someone built an
</em><br>
<em>&gt; &gt; AI with a 99.999% chance of being Friendly, it's maker is not likely to
</em><br>
<em>&gt; &gt; have the skill to calculate that as a quantitative probability.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes, this is covered in the part where I said:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt; To correctly calculate that a poorly assembled program (one
</em><br>
<em>&gt; &gt;&gt; representing the limit of its maker's skill) has a 1% chance of being
</em><br>
<em>&gt; &gt;&gt; Friendly - even to within an order of magnitude! - requires a skill
</em><br>
<em>&gt; &gt;&gt; level considerably, no, enormously higher than that required
</em><br>
<em>&gt; to build a
</em><br>
<em>&gt; &gt;&gt; program with a 99.99% chance of being Friendly.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Making quantitative predictions about this kind of system is next to
</em><br>
<em>&gt; &gt; impossible, because the dynamic evolution of the system is going to
</em><br>
<em>&gt; &gt; depend on its environment -- on human interactions with it, and so
</em><br>
<em>&gt; &gt; forth.  So to make a rigorous probability estimate you'd have to set
</em><br>
<em>&gt; &gt; various quantitative bounds on various environmental conditions, human
</em><br>
<em>&gt; &gt; behaviors, etc.  Very tricky ... not just a math problem, for sure...
</em><br>
<em>&gt; &gt; (and the math problems involved are formidable enough even without
</em><br>
<em>&gt; &gt; these environmental-modeling considerations!)
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes, I said as much in my post.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt; 3)  So we are not talking about a quantitative calculation that a
</em><br>
<em>&gt; &gt;&gt; program will be Friendly, but rather an application of the Principle
</em><br>
<em>&gt; &gt;&gt; of Indifference to surface outcomes.  The maker just doesn't really
</em><br>
<em>&gt; &gt;&gt; know whether the program will be Friendly or not, and so pulls a
</em><br>
<em>&gt; &gt;&gt; probability out of his ass.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; There are a lot of intermediate cases between a fully rigorous
</em><br>
<em>&gt; &gt; quantitative calculation, and a purely nonrigorous &quot;ass number.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Not as many as one might think.  See below.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; After all, should you ever come up with a design that you think will
</em><br>
<em>&gt; &gt; ensure Friendliness, you're not likely to have a fully rigorous
</em><br>
<em>&gt; &gt; mathematical proof that it will do so ... there will be a certain
</em><br>
<em>&gt; &gt; amount of informal reasoning required to follow your arguments.
</em><br>
<em>&gt;
</em><br>
<em>&gt; One of the subtle damages done by our humanly inherent political frame of
</em><br>
<em>&gt; mind is that, as do reporters or media, we tend to categorize things as
</em><br>
<em>&gt; &quot;provable&quot; or &quot;not provable&quot;.  Since nothing is &quot;provable&quot; enough to
</em><br>
<em>&gt; satisfy Leon Kass and the Precautionary Principle - since the political
</em><br>
<em>&gt; battle is unwinnable, or at least unwinnable through any amount
</em><br>
<em>&gt; of proof -
</em><br>
<em>&gt; well, why bother trying to prove things at all?  But if one is to clear
</em><br>
<em>&gt; away all political mud, it becomes apparent that there are many possible
</em><br>
<em>&gt; standards of rational evidence to apply.  In Friendly AI work I intend to
</em><br>
<em>&gt; apply a certain rule which says that I need a particular kind of strict
</em><br>
<em>&gt; specific expectation of a positive result before I, as a programmer, take
</em><br>
<em>&gt; any action - with any piece of code, initial conditions and rules for
</em><br>
<em>&gt; &quot;complex emergent dynamics&quot;, and so on, being considered as a
</em><br>
<em>&gt; special case
</em><br>
<em>&gt; of programmer action.  The strictness to which I refer, the required
</em><br>
<em>&gt; grounds for holding the expectation of success, is something that runs
</em><br>
<em>&gt; skew to the impossible standard of the rhetorical trick known as the
</em><br>
<em>&gt; Precautionary Principle, so Leon Kass would not be satisfied.  But so
</em><br>
<em>&gt; what?  Just because Leon Kass cannot be satisfied by any standard of
</em><br>
<em>&gt; proof, does not mean that I will commit suicide by relaxing my
</em><br>
<em>&gt; standard of
</em><br>
<em>&gt; justified expectation.
</em><br>
<em>&gt;
</em><br>
<em>&gt; There's a difference between an engineering expectation that a
</em><br>
<em>&gt; well-defined system accomplishes a well-defined result, which has not yet
</em><br>
<em>&gt; been qualified as a mathematical proof; and holding an excited
</em><br>
<em>&gt; expectation
</em><br>
<em>&gt; of success without even a clear criterion of success.  These are quite
</em><br>
<em>&gt; different levels of &quot;informality&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt; 4)  Extremely extensive research shows that &quot;probabilities&quot; which
</em><br>
<em>&gt; &gt;&gt; people pull out of their asses (as opposed to being able to calculate
</em><br>
<em>&gt; &gt;&gt; them quantitatively) are not calibrated, that is, they bear
</em><br>
<em>&gt; &gt;&gt; essentially no relation to reality.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Sure, but your statement doesn't hold when applied to teams of
</em><br>
<em>&gt; &gt; scientists making careful estimates of probabilities events based on a
</em><br>
<em>&gt; &gt; combination of science, math and intuition.  In this case, estimates
</em><br>
<em>&gt; &gt; are still imperfect -- and errors happen -- but things are not as bad
</em><br>
<em>&gt; &gt; as you're alleging.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Unless you are familiar with the literature here, my reply is:  &quot;Sorry,
</em><br>
<em>&gt; Ben, yours is a widespread opinion and in no sense an obviously stupid
</em><br>
<em>&gt; one, but it has been shown to be wrong.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; The most famous work in this area is with interviewers predicting student
</em><br>
<em>&gt; success and doctors making probabilistic clinical judgments, but
</em><br>
<em>&gt; these are
</em><br>
<em>&gt; just the most famous examples; the work has been applied to many domains.
</em><br>
<em>&gt;   (I, alas, am only acquainted with the most famous studies,
</em><br>
<em>&gt; since this is
</em><br>
<em>&gt; not my primary field, but I know it's a robust result.)  Note that in the
</em><br>
<em>&gt; case of doctors, physicians not only have access to medical
</em><br>
<em>&gt; statistics and
</em><br>
<em>&gt; well-understood underlying models, but also extensive clinical experience
</em><br>
<em>&gt; with frequent feedback on cases much like the one being immediately
</em><br>
<em>&gt; considered.  So are doctors making clinical judgments of probability
</em><br>
<em>&gt; well-calibrated?  No, not at all.  See Robyn Dawes on &quot;The robust beauty
</em><br>
<em>&gt; of improper linear models&quot;, in &quot;Judgment under uncertainty&quot;, as well as a
</em><br>
<em>&gt; good many other papers nearby, but Dawes goes into the greatest depth on
</em><br>
<em>&gt; how tenaciously people hold on to their illusions of predictability
</em><br>
<em>&gt; (though this too is a robust result).
</em><br>
<em>&gt;
</em><br>
<em>&gt; In particular, Dawes explains how, even after supposed experts are shown
</em><br>
<em>&gt; that (a) the variables of interest are far less predictable than they
</em><br>
<em>&gt; thought and that (b) experts can be defeated by an improper linear model
</em><br>
<em>&gt; using *randomly chosen weights* with the correct sign, people still hold
</em><br>
<em>&gt; out endless hopes of the superiority of &quot;clinical judgment&quot;.  You
</em><br>
<em>&gt; can show
</em><br>
<em>&gt; people the evidence and they still won't believe.  They want to believe
</em><br>
<em>&gt; the variables are predictable.  They want to believe the experts can
</em><br>
<em>&gt; predict them.  They presume endlessly that something must have been wrong
</em><br>
<em>&gt; with this study and that study, even as the studies keep piling up.
</em><br>
<em>&gt; Dawes, in one of his books, narrates how after describing one study of
</em><br>
<em>&gt; clinical judgment, someone in the audience said, &quot;Well, you should have
</em><br>
<em>&gt; tested the judgment of [Famous Dr. X]&quot;, where, although Dawes
</em><br>
<em>&gt; couldn't say
</em><br>
<em>&gt; so at the time, Dr. X was in fact one of the experts tested.
</em><br>
<em>&gt;
</em><br>
<em>&gt; So, Ben, I'll believe that scientists making careful guesses of
</em><br>
<em>&gt; probabilities based on their equivalent of clinical judgment - in
</em><br>
<em>&gt; any case
</em><br>
<em>&gt; where &quot;intuition&quot; is part of the mix along with science and math - are in
</em><br>
<em>&gt; fact well-calibrated, when I see a study proving it.  Because it goes
</em><br>
<em>&gt; against everything I have heard from any study of human judgment.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Humans fail at the task of probabilistic clinical judgment, performing
</em><br>
<em>&gt; more poorly than simple linear models, even in cases where the
</em><br>
<em>&gt; fundamentals are known and the expert has years of experience
</em><br>
<em>&gt; dealing with
</em><br>
<em>&gt; the same problem as the one presently at hand.  Not experience with
</em><br>
<em>&gt; &quot;simpler cases&quot; or &quot;test problems&quot;, experience with immediate feedback on
</em><br>
<em>&gt; many samples in the same context as the present problem.
</em><br>
<em>&gt; Calibration gets
</em><br>
<em>&gt; worse, and overconfidence gets worse, as the difficulty of the problem
</em><br>
<em>&gt; increases.  So now the field of AI, with no grasp on the
</em><br>
<em>&gt; fundamentals, and
</em><br>
<em>&gt; no sample of cases from the same context, will offer a &quot;clinical
</em><br>
<em>&gt; judgment&quot;
</em><br>
<em>&gt; of the probability that an AI will be Friendly?  In a word:  No.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This doesn't mean that building a Friendly AI is impossible.  It means
</em><br>
<em>&gt; that if it can be done, it will be done when someone looks over
</em><br>
<em>&gt; the design
</em><br>
<em>&gt; and says:  &quot;I believe that I understand every single thing that this
</em><br>
<em>&gt; design is supposed to accomplish; I believe that there is not one place
</em><br>
<em>&gt; where I am closing my eyes and hoping; I believe that I am no longer
</em><br>
<em>&gt; confused.  This design has massive overkill directed toward solving the
</em><br>
<em>&gt; problem, on a level I couldn't even have imagined when I was a neophyte.
</em><br>
<em>&gt; I know that availability is not the same as probability, but if I were
</em><br>
<em>&gt; naive enough to evaluate the availability, then I have difficulty in
</em><br>
<em>&gt; imagining how this design could fail at all, let alone how it could fail
</em><br>
<em>&gt; catastrophically.  And I know that emotional certainty is not the same as
</em><br>
<em>&gt; calibrated confidence, but if I were naive enough to see the problem
</em><br>
<em>&gt; emotionally, then I would say that what I see would inspire most
</em><br>
<em>&gt; people to
</em><br>
<em>&gt; have the emotional reaction they name '99.9% confidence'.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; If someone looks at an AI, and they think they understand all the
</em><br>
<em>&gt; fundamentals, and the AI is such as to inspire the emotional
</em><br>
<em>&gt; reaction that
</em><br>
<em>&gt; people usually name &quot;70% confidence&quot;, then the AI has maybe 1 chance in
</em><br>
<em>&gt; 100 of working.  And if someone looks at the AI and they don't
</em><br>
<em>&gt; really know
</em><br>
<em>&gt; whether it will work, but they're sort of hoping, then they're committing
</em><br>
<em>&gt; elaborate suicide.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Now you can believe that you understand a system and yet be
</em><br>
<em>&gt; wrong, as Leon
</em><br>
<em>&gt; Kass is bound to point out.  But so what?  How strong the statement &quot;I
</em><br>
<em>&gt; believe that I finally understand&quot; is as evidence depends on how lightly
</em><br>
<em>&gt; the person utters it, how given they are to wishful thinking, whether the
</em><br>
<em>&gt; person understands what it means to understand something.  It can be
</em><br>
<em>&gt; wrong, yes.  But to go ahead when you have such an infinitesimal
</em><br>
<em>&gt; understanding that even you, as an overconfident human, do not perceive
</em><br>
<em>&gt; one hell of an impressive understanding; well, that is suicide.
</em><br>
<em>&gt;
</em><br>
<em>&gt; P(actual-understanding|impressed-with-own-understanding) may be low.  But
</em><br>
<em>&gt; p(impressed|actual-understanding) is damned high, when you consider how
</em><br>
<em>&gt; impressive an actual understanding would be.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt; 6)  And finally, of course, the probabilities are not independent!
</em><br>
<em>&gt; &gt;&gt; If the best AI you can make isn't good enough, a million copies of it
</em><br>
<em>&gt; &gt;&gt; don't have independent chances of success.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; This depends a great deal on the nature of the AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Not really.  My rejoinder to this would sound a lot more impressive if we
</em><br>
<em>&gt; agreed on more details in the theory of optimization processes, but in
</em><br>
<em>&gt; English, my rejoinder reads:
</em><br>
<em>&gt;
</em><br>
<em>&gt; If anything kills you it should be an unknown unknown.  There is
</em><br>
<em>&gt; absolutely no excuse for being killed by a known unknown internal
</em><br>
<em>&gt; to the AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Suppose you have a Russian roulette gun with a &quot;randomized&quot; (omitting
</em><br>
<em>&gt; Jaynes's cogent objections to the notion of randomization; perhaps it was
</em><br>
<em>&gt; quantum split) barrel containing one bullet.  You put the gun to
</em><br>
<em>&gt; your head
</em><br>
<em>&gt; and fire.  If you were to split yourself into a thousand (local, not
</em><br>
<em>&gt; Everett) copies, and play Russian roulette twenty times in succession,
</em><br>
<em>&gt; around twenty-six of you would be expected to survive.  But why are you
</em><br>
<em>&gt; playing Russian roulette to begin with?  If you are walking alone in the
</em><br>
<em>&gt; desert, and you see a *premanufactured* gun that delivers a million
</em><br>
<em>&gt; dollars into your lap on an empty barrel, or kills you on one chance out
</em><br>
<em>&gt; of six, then - it depends on your utilities - you might decide to risk
</em><br>
<em>&gt; your life on a play.  But if you are *building* the gun, you have no
</em><br>
<em>&gt; excuse.  You should design the gun not to do *anything* if it comes up on
</em><br>
<em>&gt; a loaded chamber.  You have that privilege, the privilege of not killing
</em><br>
<em>&gt; yourself, whenever you deal with a visible known unknown.  You do
</em><br>
<em>&gt; not have
</em><br>
<em>&gt; to build an AI which, depending on the value of the known unknown, is
</em><br>
<em>&gt; either Friendly or unFriendly, because you can instead build an AI which
</em><br>
<em>&gt; is either Friendly or stops.  You can always stop, and in fact, whenever
</em><br>
<em>&gt; you are *uncertain* you can halt temporarily, you don't need to wait for
</em><br>
<em>&gt; it to become a catastrophe, and this is why I've switched from
</em><br>
<em>&gt; the concept
</em><br>
<em>&gt; of &quot;measure utilities and always pick the most desirable&quot; to &quot;pick the
</em><br>
<em>&gt; most desirable action, unless the entropy in the decision system is too
</em><br>
<em>&gt; high, in which case the optimization process itself suspends&quot;.  You don't
</em><br>
<em>&gt; need to shoot yourself; this is a basic precept of FAI development.  You
</em><br>
<em>&gt; are creating something, designing a process flow, and if you ever see a
</em><br>
<em>&gt; branch of the process flow that depends on a known unknown and goes into
</em><br>
<em>&gt; either &quot;Friendly&quot; or &quot;unFriendly&quot;, you black out the part of the
</em><br>
<em>&gt; flowchart
</em><br>
<em>&gt; that reads &quot;unFriendly&quot; and put in &quot;process halts&quot;.  One of the stunning
</em><br>
<em>&gt; realizations I've had recently is that you can always do this.
</em><br>
<em>&gt;
</em><br>
<em>&gt; There is no excuse for being killed by a known unknown.  If you are going
</em><br>
<em>&gt; to die with even the slightest dignity, so that your epitaph reads
</em><br>
<em>&gt; &quot;stupidity&quot; rather than &quot;suicide&quot;, you should be killed by an unknown
</em><br>
<em>&gt; unknown, a basic design flaw, a misunderstanding of the underlying
</em><br>
<em>&gt; philosophy.  There is no excuse for being killed by a flowchart that does
</em><br>
<em>&gt; what you thought it would.  If you knew, you should have made a different
</em><br>
<em>&gt; flowchart.  Now why would a basic design flaw be an independent
</em><br>
<em>&gt; probability for a million copies of the best AI you can make?
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; If you're creating an AI that is a dynamical system, and you're
</em><br>
<em>&gt; &gt; building an initial condition and letting it evolve in a way that is
</em><br>
<em>&gt; &gt; driven partially by environmental influences, then if you run many
</em><br>
<em>&gt; &gt; copies of it independently
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; a) of course, the dynamics of the different instances are not
</em><br>
<em>&gt; &gt; probabilistically independent
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; b) nevertheless, they may be VERY far from identical, and may come to a
</em><br>
<em>&gt; &gt; wide variety of different outcomes
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; My dreamed idea (which wasn't so serious, by the way!) did not rely
</em><br>
<em>&gt; &gt; upon the assumption of complete probabilistic independence between
</em><br>
<em>&gt; &gt; multiple evolving instances of the AI.  It did rely upon the idea of a
</em><br>
<em>&gt; &gt; self-modifying AI as a pragmatically-nondeterministic,
</em><br>
<em>&gt; &gt; environmentally-coupled system rather than a strongly-deterministic
</em><br>
<em>&gt; &gt; system like an auto engine.
</em><br>
<em>&gt;
</em><br>
<em>&gt; An auto engine contains much entropy and so is about as
</em><br>
<em>&gt; &quot;nondeterministic&quot;
</em><br>
<em>&gt; as any other physical system of roughly the same temperature
</em><br>
<em>&gt; (depending on
</em><br>
<em>&gt; molecular degrees of freedom, but, whatever).  What makes an auto engine
</em><br>
<em>&gt; &quot;strongly deterministic&quot;, it seems, is that we understand how an auto
</em><br>
<em>&gt; engine works and so it reliably performs the function that we wish of it,
</em><br>
<em>&gt; which is all that we actually care about, the molecular degrees
</em><br>
<em>&gt; of freedom
</em><br>
<em>&gt; dropping out of the equation as irrelevant to our goal processes.  It's
</em><br>
<em>&gt; not how much entropy is physically in the system, it's how much
</em><br>
<em>&gt; entropy we
</em><br>
<em>&gt; care about, and how much entropy there is in that one all-important bit,
</em><br>
<em>&gt; &quot;success or failure&quot;; what makes an auto engine &quot;strongly deterministic&quot;,
</em><br>
<em>&gt; is not being maintained at zero kelvin, but that, from a functional
</em><br>
<em>&gt; perspective, it works reliably because it was designed with understanding.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt; What's wrong with this picture:
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt; a)  Confusing plausibility with frequency; b)  Assigning something
</em><br>
<em>&gt; &gt;&gt; called a &quot;probability&quot; in the absence of a theory powerful enough to
</em><br>
<em>&gt; &gt;&gt; calculate it quantitatively; c)  Treating highly correlated
</em><br>
<em>&gt; &gt;&gt; probabilities as independent; d)  Applying the Principle of
</em><br>
<em>&gt; &gt;&gt; Indifference to surface outcomes rather than elementary
</em><br>
<em>&gt; &gt;&gt; interchangeable events; and e)  Attempting to trade off not knowing
</em><br>
<em>&gt; &gt;&gt; how to solve a problem for confessing a &quot;1%&quot; probability of success.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Sorry, I am definitely not guilty of errors a, c or d
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; As for error b, I don't think it's bad to call an unknown quantity a
</em><br>
<em>&gt; &gt; probability just because I currently lack the evidence to calculate the
</em><br>
<em>&gt; &gt;  value of the quantity.  b is not an error.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think it's okay to use the word &quot;probability&quot;, as in, &quot;this
</em><br>
<em>&gt; variable has
</em><br>
<em>&gt; a probability but I don't know what it is&quot;, but not to use words
</em><br>
<em>&gt; like &quot;20%
</em><br>
<em>&gt; probability&quot; if they are based on subjective impressions rather than
</em><br>
<em>&gt; quantitative calculation or samples from a defined context.  &quot;20% sure&quot;
</em><br>
<em>&gt; would be better, but the number &quot;20%&quot; in this case measures not
</em><br>
<em>&gt; probability but psychological support, and it cannot be manipulated
</em><br>
<em>&gt; mathematically like a probability.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; As for e, there may be some problems for which there are no
</em><br>
<em>&gt; &gt; guaranteed-successful solutions, only solutions that have a reasonable
</em><br>
<em>&gt; &gt; probability of success.  You seem highly certain that Friendly AI does
</em><br>
<em>&gt; &gt; not lie in this class, but you have given no evidence in favor of your
</em><br>
<em>&gt; &gt; assertion.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If the FAI runs on reliable hardware, there should be no
</em><br>
<em>&gt; legitimate source
</em><br>
<em>&gt; of uncertainty from quantum branching or thermal vibrations,
</em><br>
<em>&gt; which are the
</em><br>
<em>&gt; legitimate sources of irreducible randomness in physical processes.
</em><br>
<em>&gt; Another alternative is that we are not dealing with irreducible
</em><br>
<em>&gt; randomness
</em><br>
<em>&gt; but with confusion on the part of the designer, who is 20% sure that the
</em><br>
<em>&gt; AI will work right, which is not at all the same as an AI with a 20%
</em><br>
<em>&gt; probability of working.  There is no excuse for being killed by a known
</em><br>
<em>&gt; unknown internal to the AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I can think offhand of three classes of known unknows which are
</em><br>
<em>&gt; calculably
</em><br>
<em>&gt; probabilistic but not accessible to the AI, black boxes with definite
</em><br>
<em>&gt; contents, but I am not going to say what they are; if you want to make a
</em><br>
<em>&gt; case for the Friendliness or unFriendliness of the AI depending on such a
</em><br>
<em>&gt; variable you will have to make the case for yourself.  It sounds too
</em><br>
<em>&gt; exotic to me.  The only reason the scenario *has any intuitive appeal*
</em><br>
<em>&gt; (despite the conditions for fulfillment being so exotic) is that people
</em><br>
<em>&gt; are 20% sure their system will work, so it seems plausible that
</em><br>
<em>&gt; the system
</em><br>
<em>&gt; might have a 20% probability of working, and yet these are fundamentally
</em><br>
<em>&gt; different and inconvertible quantities.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;&gt; And if you're wondering why I'm so down on this, it's because it
</em><br>
<em>&gt; &gt;&gt; seems to me like yet another excuse for not knowing how to build a
</em><br>
<em>&gt; &gt;&gt; Friendly AI.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Actually, I *do* know how to build a Friendly AI.... ;-)
</em><br>
<em>&gt;
</em><br>
<em>&gt; Heh.  This statement caused much excitement on the #sl4 IRC channel.  I
</em><br>
<em>&gt; had to explain that this meant that you now thought you knew how to build
</em><br>
<em>&gt; an AI, and that the word &quot;Friendly&quot; was there for luck.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Personally, I do *not* yet know how to build a Friendly process.
</em><br>
<em>&gt; I rather
</em><br>
<em>&gt; doubt that you could define Friendliness.  Last time I checked you were
</em><br>
<em>&gt; still claiming that you knew it to be impossible to resolve the basic
</em><br>
<em>&gt; theoretical confusions without a chimp-level mind to experiment on.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Please don't use the word &quot;Friendly&quot; so lightly.  There are plenty of
</em><br>
<em>&gt; available alternatives, like &quot;moral AI&quot; or for that matter &quot;friendly AI&quot;.
</em><br>
<em>&gt;   Part of the reason I went to the trouble of capitalizing it was so that
</em><br>
<em>&gt; people who mean, i.e., &quot;I want to build an AI and I hope it will
</em><br>
<em>&gt; be a nice
</em><br>
<em>&gt; person&quot; could say &quot;I want to build a friendly AI&quot;.  &quot;I know how
</em><br>
<em>&gt; to build a
</em><br>
<em>&gt; Friendly AI&quot; is one hell of a strong claim over and above the
</em><br>
<em>&gt; weaker claim
</em><br>
<em>&gt; that you merely know how to build a living mind from scratch.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; [I wasn't so sure a year ago, but recent simplifications in the
</em><br>
<em>&gt; &gt; Novamente design (removing some of the harder-to-control components and
</em><br>
<em>&gt; &gt; replacing them with probabilistic-inference-based alternatives) have
</em><br>
<em>&gt; &gt; made me more confident.]
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; But I can't *guarantee* this AI will be Friendly no matter what; all I
</em><br>
<em>&gt; &gt; can give are reasonable intuitive arguments why it will be Friendly.
</em><br>
<em>&gt; &gt; The probability of the Friendliness outcome is not easy to calculate,
</em><br>
<em>&gt; &gt; as you've pointed out so loquaciously.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I am not asking for a guarantee.  But what you call &quot;intuitive argument&quot;
</em><br>
<em>&gt; seems to me philosophicalish thinking, driven by wishes and hopes, not
</em><br>
<em>&gt; binding on Nature.  Last time I checked, you said that you didn't think
</em><br>
<em>&gt; the fundamental problems of FAI were solvable without a chimp-level mind
</em><br>
<em>&gt; to experiment on (very clever excuse, that), so obviously you have not
</em><br>
<em>&gt; already solved the fundamental problems.  The standard to which I hold is
</em><br>
<em>&gt; not a mathematical proof, but a technical argument - not an intuitive
</em><br>
<em>&gt; philosophicalish argument, a technical argument - which includes
</em><br>
<em>&gt; technical
</em><br>
<em>&gt; definitions of everything that you wish to achieve, and a walkthrough of
</em><br>
<em>&gt; the processes showing that they produce behaviors which achieve those
</em><br>
<em>&gt; definitions; the sort of technical argument that would be given
</em><br>
<em>&gt; by someone
</em><br>
<em>&gt; who had solved all the foundational problems and taken the entire problem
</em><br>
<em>&gt; out of the domain of philosophy.  When I have that, I will then
</em><br>
<em>&gt; be willing
</em><br>
<em>&gt; to say at last:  &quot;I do know how to build a Friendly AI.&quot;  And
</em><br>
<em>&gt; perhaps Leon
</em><br>
<em>&gt; Kass will not consider it proof, but at least I will no longer
</em><br>
<em>&gt; consider it
</em><br>
<em>&gt; suicide.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; And then I wonder whether that my &quot;reasonable intuitive arguments&quot; ----
</em><br>
<em>&gt; &gt; and *all* human arguments, whether we consider them rigorous or not;
</em><br>
<em>&gt; &gt; even our best mathematical proofs --- kinda fall apart and reveal
</em><br>
<em>&gt; &gt; limitations when we get into the domain of vastly transhuman
</em><br>
<em>&gt; &gt; intelligence.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Reasonable intuitive arguments&quot; have been exhaustively demonstrated to
</em><br>
<em>&gt; fall apart on guessing why Aunt Mabel is coughing, let alone in
</em><br>
<em>&gt; the domain
</em><br>
<em>&gt; of vastly transhuman intelligence.
</em><br>
<em>&gt;
</em><br>
<em>&gt; There is a huge spectrum between a mathematical proof and a &quot;reasonable
</em><br>
<em>&gt; intuitive argument&quot;.  (I assume a &quot;reasonable intuitive argument&quot; applied
</em><br>
<em>&gt; to AI is somewhere between a doctor's &quot;informed clinical judgment&quot; minus
</em><br>
<em>&gt; the well-understood fundamentals and any past experience, and the wishful
</em><br>
<em>&gt; thinking of Greek philosophers.)  What is needed is something at the high
</em><br>
<em>&gt; end of the reliability spectrum, a specific, purely technical definition
</em><br>
<em>&gt; of all desirable properties of the outcome and a purely technical
</em><br>
<em>&gt; walkthrough showing that the employed processes work to this end.  Note
</em><br>
<em>&gt; that I say &quot;walkthrough showing that&quot;, not &quot;reasons why&quot; or &quot;arguments
</em><br>
<em>&gt; for&quot;.  History shows that reasons why and arguments for are not
</em><br>
<em>&gt; binding on
</em><br>
<em>&gt; Nature, but once a problem has been understood, an engineering
</em><br>
<em>&gt; walkthrough
</em><br>
<em>&gt; sometimes works.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; So I tend to apply (a mix of rigorous and intuitive)
</em><br>
<em>&gt; &gt; probabilistic thinking when thinking about transhuman AI's that are
</em><br>
<em>&gt; &gt; just a bit smarter than humans ... and then rely on ignorance-based
</em><br>
<em>&gt; &gt; Principle of Indifference type thinking, when thinking about transhuman
</em><br>
<em>&gt; &gt; AI's that are vastly, vastly smarter than any of us.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Is there even one example from the history of science where
</em><br>
<em>&gt; Nature has not
</em><br>
<em>&gt; replied to this sort of argument with, &quot;So what?&quot;  Einstein,
</em><br>
<em>&gt; maybe, but he
</em><br>
<em>&gt; had math in which to phrase his intuitions.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Here is the lesson of my own experience in this area:  Ignorance does not
</em><br>
<em>&gt; work!  If you do not know, you are screwed no matter how clever you are
</em><br>
<em>&gt; about working around your ignorance.  When you have fundamental problems
</em><br>
<em>&gt; you must solve them.  If you do not solve them it is like asking a Greek
</em><br>
<em>&gt; philosopher to guess that the nature of matter is quantum physics, you're
</em><br>
<em>&gt; just screwed.  It doesn't matter how clever you are, you're just screwed.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I do not know everything about Friendly AI - there are N fundamental
</em><br>
<em>&gt; problems of which I have only solved M - but whenever I solve a
</em><br>
<em>&gt; problem X,
</em><br>
<em>&gt; it immediately becomes apparent that had I tried to build a Friendly AI
</em><br>
<em>&gt; without first solving problem X, I would have just been screwed.  Repeat
</em><br>
<em>&gt; this experience a few times and you start to see that the only virtue of
</em><br>
<em>&gt; ignorance is that, being ignorant of how to solve the problem,
</em><br>
<em>&gt; one is also
</em><br>
<em>&gt; ignorant of the impossibility of success without knowledge.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I am speaking of &quot;virtue&quot; from a political standpoint, of course;
</em><br>
<em>&gt; ignorance makes a very convenient argument, especially when
</em><br>
<em>&gt; speaking to an
</em><br>
<em>&gt; audience that can be counted to nod along sympathetically when
</em><br>
<em>&gt; you confess
</em><br>
<em>&gt; to not knowing.  The humble confession of ignorance fuzzes out every
</em><br>
<em>&gt; counterargument; why, you may be certain the Earth goes around the Sun,
</em><br>
<em>&gt; good sir, but how can any of us really know anything?  I am willing to
</em><br>
<em>&gt; concede it is possible, perhaps, but who really knows?  Your inexplicable
</em><br>
<em>&gt; confidence in heliocentrism, dear sir, can only be the mark of
</em><br>
<em>&gt; irrationality; reasonable folk are willing to admit when they do
</em><br>
<em>&gt; not know...
</em><br>
<em>&gt;
</em><br>
<em>&gt; It is much easier to convince people that no one really knows, and hence,
</em><br>
<em>&gt; why not hope? than to propound the implausible, arrogant, elitist
</em><br>
<em>&gt; proposition that YOU have understood what others do not.  And yet
</em><br>
<em>&gt; assuming
</em><br>
<em>&gt; success and looking backward, anyone who builds a Friendly AI
</em><br>
<em>&gt; will need to
</em><br>
<em>&gt; have understood one hell of a lot of stuff.  One who humbly admits to a
</em><br>
<em>&gt; lack of understanding may, perhaps, deserve our thanks for their honesty,
</em><br>
<em>&gt; time and funding to think and learn and grow their understanding further;
</em><br>
<em>&gt; but they will not build a Friendly AI, honest ignorance is just not good
</em><br>
<em>&gt; enough for that.
</em><br>
<em>&gt;
</em><br>
<em>&gt; --
</em><br>
<em>&gt; Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt; Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7724.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<li><strong>Previous message:</strong> <a href="7722.html">Ben Goertzel: "RE: Dispersing AIs throughout the universe"</a>
<li><strong>In reply to:</strong> <a href="7719.html">Eliezer S. Yudkowsky: "Re: Dispersing AIs throughout the universe"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7723">[ date ]</a>
<a href="index.html#7723">[ thread ]</a>
<a href="subject.html#7723">[ subject ]</a>
<a href="author.html#7723">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:43 MDT
</em></small></p>
</body>
</html>
