<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: An essay I just wrote on the Singularity.</title>
<meta name="Author" content="Tommy McCabe (rocketjet314@yahoo.com)">
<meta name="Subject" content="Re: An essay I just wrote on the Singularity.">
<meta name="Date" content="2004-01-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: An essay I just wrote on the Singularity.</h1>
<!-- received="Fri Jan  2 18:58:20 2004" -->
<!-- isoreceived="20040103015820" -->
<!-- sent="Fri, 2 Jan 2004 17:58:18 -0800 (PST)" -->
<!-- isosent="20040103015818" -->
<!-- name="Tommy McCabe" -->
<!-- email="rocketjet314@yahoo.com" -->
<!-- subject="Re: An essay I just wrote on the Singularity." -->
<!-- id="20040103015818.27877.qmail@web11707.mail.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="87oetmrsih.fsf@snark.piermont.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tommy McCabe (<a href="mailto:rocketjet314@yahoo.com?Subject=Re:%20An%20essay%20I%20just%20wrote%20on%20the%20Singularity."><em>rocketjet314@yahoo.com</em></a>)<br>
<strong>Date:</strong> Fri Jan 02 2004 - 18:58:18 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7516.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7514.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>In reply to:</strong> <a href="7482.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7537.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="7537.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7515">[ date ]</a>
<a href="index.html#7515">[ thread ]</a>
<a href="subject.html#7515">[ subject ]</a>
<a href="author.html#7515">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
--- &quot;Perry E. Metzger&quot; &lt;<a href="mailto:perry@piermont.com?Subject=Re:%20An%20essay%20I%20just%20wrote%20on%20the%20Singularity.">perry@piermont.com</a>&gt; wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Tommy McCabe &lt;<a href="mailto:rocketjet314@yahoo.com?Subject=Re:%20An%20essay%20I%20just%20wrote%20on%20the%20Singularity.">rocketjet314@yahoo.com</a>&gt; writes:
</em><br>
<em>&gt; &gt;&gt; So one can have AI. I don't dispute that. What
</em><br>
<em>&gt; I'm
</em><br>
<em>&gt; &gt;&gt; talking about is
</em><br>
<em>&gt; &gt;&gt; &quot;Friendly&quot; AI.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Humans can, obviously not must, but can be
</em><br>
<em>&gt; altruistic,
</em><br>
<em>&gt; &gt; the human equivalent of Friendliness. And if one
</em><br>
<em>&gt; can
</em><br>
<em>&gt; &gt; do it in DNA, one can do it in code.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Can they? I am not sure that they can, in the sense
</em><br>
<em>&gt; of leaving behind
</em><br>
<em>&gt; large numbers of copies of their genes if they
</em><br>
<em>&gt; behave in a
</em><br>
<em>&gt; consistently altruistic manner.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; BTW, I am not referring to aiding siblings or such
</em><br>
<em>&gt; -- Dawkins has
</em><br>
<em>&gt; excellent arguments for why that isn't really
</em><br>
<em>&gt; altruism -- and I'm not
</em><br>
<em>&gt; talking about casual giving to charity or such
</em><br>
<em>&gt; activities.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What I'm talking about is consistently aiding
</em><br>
<em>&gt; strangers in deference
</em><br>
<em>&gt; to your own children, or other such activities. I'm
</em><br>
<em>&gt; far from sure that
</em><br>
<em>&gt; such behavior isn't powerfully selected against, and
</em><br>
<em>&gt; one sees very
</em><br>
<em>&gt; little of it in our society, so I'm not sure that it
</em><br>
<em>&gt; hasn't in fact
</em><br>
<em>&gt; been evolved out.
</em><br>
<p>For the ten thousandth time, Darwinian evolution and
<br>
its associated complex functional adaptations do not
<br>
apply to transhumans. A Friendly transhuman would be
<br>
concerned both about vis offspring and sentients ve's
<br>
never heard of. And you're telling me Ghandi isn't
<br>
altruistic? Never mind if altruism is an accident- I'm
<br>
not a professor in evolution - but it is possible.
<br>
<p><em>&gt; &gt;&gt; &gt; And if you have Friendly human-equivalent AI,
</em><br>
<em>&gt; &gt;&gt; 
</em><br>
<em>&gt; &gt;&gt; You've taken a leap. Step back. Just because we
</em><br>
<em>&gt; know
</em><br>
<em>&gt; &gt;&gt; we can build AI
</em><br>
<em>&gt; &gt;&gt; doesn't mean we know we can build &quot;Friendly&quot; AI.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Again, there are humans which are very friendly,
</em><br>
<em>&gt; and
</em><br>
<em>&gt; &gt; humans weren't built with friendliness in mind at
</em><br>
<em>&gt; all.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't think there are many humans who are friendly
</em><br>
<em>&gt; the way &quot;Friendly
</em><br>
<em>&gt; AI&quot; has to be. A &quot;Friendly AI&quot; has to favor the
</em><br>
<em>&gt; preservation of other
</em><br>
<em>&gt; creatures who are not members of its line (humans
</em><br>
<em>&gt; and their
</em><br>
<em>&gt; descendents) over its own.
</em><br>
<p>Read CFAI extermely thoroughly, (literally), then take
<br>
a hammer, and pound CFAI into your head
<br>
(metaphorically, please!). The concept of a goal
<br>
system that centers around the observer is an
<br>
evolutionary complex functional adaptation, and those
<br>
don't appear in AIs. See MistakesOfClassicalAI on the
<br>
SL4 Wiki. The behaviours of favoring yourself at all
<br>
and sentients you are related to is anthropomorphic,
<br>
not because it isn't possible in an AI, but because
<br>
it's built into our heads and it's entierly optional
<br>
in an AI. To an AI that hasn't been built with an
<br>
observer-centered goal system, the self/not self
<br>
distinction is exactly the same type of thing as the
<br>
memory/hard drive distinction.
<br>
<p><em>&gt; &gt;&gt; &gt;&gt; There are several problems here, including the
</em><br>
<em>&gt; fact that there
</em><br>
<em>&gt; &gt;&gt; &gt;&gt; is no absolute morality (and thus no way to
</em><br>
<em>&gt; universally
</em><br>
<em>&gt; &gt;&gt; &gt;&gt; determine &quot;the good&quot;),
</em><br>
<em>&gt; &gt;&gt; &gt;
</em><br>
<em>&gt; &gt;&gt; &gt; This is the postition of subjective morality,
</em><br>
<em>&gt; which is far from
</em><br>
<em>&gt; &gt;&gt; &gt; proven. It's not a 'fact', it is a possibility.
</em><br>
<em>&gt; &gt;&gt; 
</em><br>
<em>&gt; &gt;&gt; It is unproven, for the exact same reason that
</em><br>
<em>&gt; the non-existence of
</em><br>
<em>&gt; &gt;&gt; God is unproven and indeed unprovable -- I can
</em><br>
<em>&gt; come up with all
</em><br>
<em>&gt; &gt;&gt; sorts of non-falsifiable scenarios in which a God
</em><br>
<em>&gt; could
</em><br>
<em>&gt; &gt;&gt; exist. However, an absolute morality requires a
</em><br>
<em>&gt; bunch of
</em><br>
<em>&gt; &gt;&gt; assumptions -- again, non-falsifiable
</em><br>
<em>&gt; assumptions. As a good
</em><br>
<em>&gt; &gt;&gt; Popperian, depending on things that are
</em><br>
<em>&gt; non-falsifiable rings alarm
</em><br>
<em>&gt; &gt;&gt; bells in my head.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Perhaps I should clarify that: subjective morality
</em><br>
<em>&gt; is
</em><br>
<em>&gt; &gt; not only unproven, it is nowhere near certain.
</em><br>
<em>&gt; Neither
</em><br>
<em>&gt; &gt; is objective morality. The matter is still up for
</em><br>
<em>&gt; &gt; debate. A good Friendliness design should be
</em><br>
<em>&gt; &gt; compatible with either.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1) I argue quite strongly that there is no objective
</em><br>
<em>&gt; morality. You
</em><br>
<em>&gt; cannot find answers to all &quot;moral questions&quot; by
</em><br>
<em>&gt; making inquiry to
</em><br>
<em>&gt; some sort of moral oracle algorithm. Indeed, the
</em><br>
<em>&gt; very notion of
</em><br>
<em>&gt; &quot;morality&quot; is disputed -- you will find plenty of
</em><br>
<em>&gt; people who don't
</em><br>
<em>&gt; think they have any moral obligations at all!
</em><br>
<p>And you can find people who think that Zeta aliens are
<br>
communicating with us about the giant planet that is
<br>
going to come careening through the solar system
<br>
stopping the Earth's rotation in May 2003 (yes, May
<br>
2003, the month that has already passed).
<br>
<p><em>&gt; Taking a step past that, though, it is trivially
</em><br>
<em>&gt; seen that the bulk of
</em><br>
<em>&gt; the population does not share a common moral code,
</em><br>
<em>&gt; and that even those
</em><br>
<em>&gt; portions which they claim to hold to they don't
</em><br>
<em>&gt; generally follow. Even
</em><br>
<em>&gt; the people here on this mailing list will
</em><br>
<em>&gt; substantially agree on major
</em><br>
<em>&gt; points of &quot;morality&quot;.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There is, on top of that, no known way to establish
</em><br>
<em>&gt; what is 'morally
</em><br>
<em>&gt; correct'. You and I can easily ascertain the
</em><br>
<em>&gt; &quot;correct&quot; speed of light
</em><br>
<em>&gt; in a vacuum (to within a small error boun) with
</em><br>
<em>&gt; straightforward
</em><br>
<em>&gt; scientific tools. There is, however, no experiment
</em><br>
<em>&gt; we can conduct to
</em><br>
<em>&gt; determine the &quot;correct&quot; behavior in the face of a
</em><br>
<em>&gt; moral dilemma.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; By the way, one shudders at what would happen if one
</em><br>
<em>&gt; could actually
</em><br>
<em>&gt; build superhuman entities to try to *enforce*
</em><br>
<em>&gt; morality. See Greg
</em><br>
<em>&gt; Bear's &quot;Strength of Stones&quot; for one scenario about
</em><br>
<em>&gt; where that
</em><br>
<em>&gt; foolishness might lead.
</em><br>
<p>I don't have the book. Please elaborate on that.
<br>
<p><em>&gt; &gt;&gt; &gt;&gt; that it is not clear that a construct like
</em><br>
<em>&gt; this would be able to
</em><br>
<em>&gt; &gt;&gt; &gt;&gt; battle it out effectively against other
</em><br>
<em>&gt; constructs from
</em><br>
<em>&gt; &gt;&gt; &gt;&gt; societies that do not construct Friendly AIs
</em><br>
<em>&gt; (or indeed that the
</em><br>
<em>&gt; &gt;&gt; &gt;&gt; winner in the universe won't be the societies
</em><br>
<em>&gt; that produce the
</em><br>
<em>&gt; &gt;&gt; &gt;&gt; meanest, baddest-assed intelligences rather
</em><br>
<em>&gt; than the friendliest
</em><br>
<em>&gt; &gt;&gt; &gt;&gt; -- see evolution on earth), etc.
</em><br>
<em>&gt; &gt;&gt; &gt;
</em><br>
<em>&gt; &gt;&gt; &gt; Battle it out? The 'winner'? The 'winner' in
</em><br>
<em>&gt; this case is the AI
</em><br>
<em>&gt; &gt;&gt; &gt; who makes it to superintelligence first.
</em><br>
<em>&gt; &gt;&gt; 
</em><br>
<em>&gt; &gt;&gt; How do we know that this hasn't already happened
</em><br>
<em>&gt; elsewhere in the
</em><br>
<em>&gt; &gt;&gt; universe? We don't. We just assume (probably
</em><br>
<em>&gt; correctly) that it
</em><br>
<em>&gt; &gt;&gt; hasn't happened on our planet -- but there are
</em><br>
<em>&gt; all sorts of other
</em><br>
<em>&gt; &gt;&gt; planets out there. The Universe is Big. You don't
</em><br>
<em>&gt; want to build
</em><br>
<em>&gt; &gt;&gt; something that will have trouble with Outside
</em><br>
<em>&gt; Context Problems (as
</em><br>
<em>&gt; &gt;&gt; Ian Banks dubbed them).
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Another rephrasing: the first superintellgence
</em><br>
<em>&gt; that
</em><br>
<em>&gt; &gt; knows about us.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It doesn't matter who knows about us first. What
</em><br>
<em>&gt; matters is what
</em><br>
<em>&gt; happens when the other hyperintelligence from the
</em><br>
<em>&gt; other side of the
</em><br>
<em>&gt; galaxy sends over its probes and it turns out that
</em><br>
<em>&gt; it isn't nearly as
</em><br>
<em>&gt; &quot;friendly&quot; and has far more resources. Such an
</em><br>
<em>&gt; intelligence may be the
</em><br>
<em>&gt; *last* thing we encounter in our history.
</em><br>
<p>I don't know what happens when superintelligences come
<br>
into conflict. Really. I can pretty much guarantee,
<br>
however, that the first superintellgence will be the
<br>
last one before the Singularity (in the solar system).
<br>
<p><em>&gt; &gt;&gt; &gt;&gt; Anyway, I find it interesting to speculate on
</em><br>
<em>&gt; possible constructs
</em><br>
<em>&gt; &gt;&gt; &gt;&gt; like The Friendly AI, but not safe to assume
</em><br>
<em>&gt; that they're going to
</em><br>
<em>&gt; &gt;&gt; &gt;&gt; be in one's future.
</em><br>
<em>&gt; &gt;&gt; &gt;
</em><br>
<em>&gt; &gt;&gt; &gt; Of course you can't assume that there will be a
</em><br>
<em>&gt; &gt;&gt; &gt; Singularity caused by a Friendly AI, but I'm
</em><br>
<em>&gt; pretty
</em><br>
<em>&gt; &gt;&gt; &gt; darn sure I want it to happen!
</em><br>
<em>&gt; &gt;&gt; 
</em><br>
<em>&gt; &gt;&gt; I want roses to grow unbidden from the wood of my
</em><br>
<em>&gt; &gt;&gt; writing desk.
</em><br>
<em>&gt; &gt;&gt; 
</em><br>
<em>&gt; &gt;&gt; Don't speak of desire. Speak of realistic
</em><br>
<em>&gt; &gt;&gt; possibility.
</em><br>
<em>&gt; &gt;  
</em><br>
<em>&gt; &gt; I consider that a realistic possibility. And the
</em><br>
<em>&gt; &gt; probability of that happening can be influenced by
</em><br>
<em>&gt; us.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'm well aware that you consider the possibility
</em><br>
<em>&gt; realistic. I
</em><br>
<em>&gt; don't. Chacun a son gout. However, I'm happy to
</em><br>
<em>&gt; continue explaining
</em><br>
<em>&gt; why I think it would be difficult to guarantee that
</em><br>
<em>&gt; an AI would be
</em><br>
<em>&gt; &quot;Friendly&quot;.
</em><br>
<p>Please do.
<br>
<p><em>&gt; &gt;&gt; &gt;&gt; The prudent transhumanist considers survival
</em><br>
<em>&gt; in wide variety of
</em><br>
<em>&gt; &gt;&gt; &gt;&gt; scenarios.
</em><br>
<em>&gt; 
</em><br>
=== message truncated ===
<br>
<p>__________________________________
<br>
Do you Yahoo!?
<br>
Find out what made the Top Yahoo! Searches of 2003
<br>
<a href="http://search.yahoo.com/top2003">http://search.yahoo.com/top2003</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7516.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7514.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>In reply to:</strong> <a href="7482.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7537.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="7537.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7515">[ date ]</a>
<a href="index.html#7515">[ thread ]</a>
<a href="subject.html#7515">[ subject ]</a>
<a href="author.html#7515">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:43 MDT
</em></small></p>
</body>
</html>
