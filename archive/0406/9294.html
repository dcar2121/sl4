<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Fundamentals - was RE: Visualizing muddled volitions</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Fundamentals - was RE: Visualizing muddled volitions">
<meta name="Date" content="2004-06-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Fundamentals - was RE: Visualizing muddled volitions</h1>
<!-- received="Wed Jun 16 14:41:15 2004" -->
<!-- isoreceived="20040616204115" -->
<!-- sent="Wed, 16 Jun 2004 16:41:11 -0400" -->
<!-- isosent="20040616204111" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Fundamentals - was RE: Visualizing muddled volitions" -->
<!-- id="40D0B067.6020203@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="C02025521F687E43B93A11680A71FAD4246958@sherwood.Avatar.local" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Fundamentals%20-%20was%20RE:%20Visualizing%20muddled%20volitions"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Jun 16 2004 - 14:41:11 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9295.html">Samantha Atkins: "Re: Collective Volition: Wanting vs Doing."</a>
<li><strong>Previous message:</strong> <a href="9293.html">Brent Thomas: "RE: Fundamentals - was RE: Visualizing muddled volitions"</a>
<li><strong>In reply to:</strong> <a href="9293.html">Brent Thomas: "RE: Fundamentals - was RE: Visualizing muddled volitions"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9301.html">Metaqualia: "Re: Fundamentals - was RE: Visualizing muddled volitions"</a>
<li><strong>Reply:</strong> <a href="9301.html">Metaqualia: "Re: Fundamentals - was RE: Visualizing muddled volitions"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9294">[ date ]</a>
<a href="index.html#9294">[ thread ]</a>
<a href="subject.html#9294">[ subject ]</a>
<a href="author.html#9294">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Brent Thomas wrote:
<br>
<p><em>&gt; Answers below as appropriate -- indicated by !!!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Summary: As far as the system itself, most likely self developed and 
</em><br>
<em>&gt; capable of 'changing the world', my only fundamental need/desire/request
</em><br>
<em>&gt; is that it allow me (the current me, not some calculated approximation)
</em><br>
<em>&gt; to act as a 'final judge' and accept or reject any modifications it
</em><br>
<em>&gt; would perform on my person. Change the environment to whatever the
</em><br>
<em>&gt; collective derives as fitting our volition, change pretty much anything
</em><br>
<em>&gt; everyone can agree on...but don't change any sentient without presenting
</em><br>
<em>&gt; the choice and ensuring the sentient is comfortable (to the limit of
</em><br>
<em>&gt; their ability to understand) the choice.
</em><br>
<p>Brent, if it was me designing my own Nice Place To Live, that sort of thing 
<br>
would always happen as the result of a deliberate action by the sentient, 
<br>
and there wouldn't *be* any drastically self-modifying actions available 
<br>
until you got your driver's license for your own source code.
<br>
<p>The problem is that once you start designing a Nice Place To Live, you lose 
<br>
the whole bootstrapping effect of the initial dynamic - you have to get 
<br>
everything exactly right on your first try, not *approximately* right, 
<br>
exactly right.
<br>
<p><em>&gt; Its not a genie bottle because
</em><br>
<em>&gt; the 'system' works as you have envisioned...
</em><br>
<p>But systems *don't* work as envisioned.  It will be hard enough to get 
<br>
collective volition to work as envisioned.  You're imagining a very 
<br>
detailed and specific system, meant to serve a particular end, and you're 
<br>
only imagining the consequences of the system that you see with a day's 
<br>
work and human-level intelligence.  You aren't imagining that the system 
<br>
runs into an unforeseen circumstance you didn't think of when writing down 
<br>
the initial design, and then it's unable to adapt.
<br>
<p><em>&gt; only the fundamental
</em><br>
<em>&gt; difference is that no action can be taken to a sentient without their
</em><br>
<em>&gt; consent.
</em><br>
<p>Define me &quot;action&quot;.  Explaining things to a sentient is an action that 
<br>
modifies the sentient.
<br>
<p>Define me &quot;consent&quot;.  Define me &quot;sentient&quot;.  Define them down to the level 
<br>
of physics.  Give me a well-specified predicate that applies to the AI's 
<br>
internal representation of reality in this and all successor 
<br>
implementations.  Get it all exactly right on the first try, for you will 
<br>
not let me turn the questions over to a bootstrapping initial dynamic.
<br>
<p><em>&gt; (if they were violent, or otherwise inclined not to be involved
</em><br>
<em>&gt; that is the point of the enclaves and the responsibility of the system
</em><br>
<em>&gt; to provide such space for them to exist as they choose...and imho this
</em><br>
<em>&gt; is no bother or hardship for the system...by the point it can alter the
</em><br>
<em>&gt; environment/bodies/selves of sentients it can also protect them)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I DO think your collective volition is 'right on' in how the system 
</em><br>
<em>&gt; should model and improve itself and the environment...i just must insist
</em><br>
<em>&gt; on the rights of a 'last judge' be in MY hands when it comes to my 
</em><br>
<em>&gt; body/self/intellect. And truly, for the capabilities I envision this 
</em><br>
<em>&gt; system will develop in a short period, maintaining enclaves and 
</em><br>
<em>&gt; providing explanations to whatever level of detail a being requests will
</em><br>
<em>&gt; probably take .0000000001% (or less!) of the systems capability.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Whats the rush, or the need to impose?
</em><br>
<p>*Probably* none.  Don't trust &quot;probably&quot;!  I can very easily see 
<br>
circumstances where the CV-RPOP poofs into existence and correctly, 
<br>
humanely decides that it wants to get the hell out of Dodge, *now*, upload 
<br>
everyone and run for it before... I don't know what, but I can see it 
<br>
happening.
<br>
<p>There are too many unforeseen consequences of this wish of yours!  Don't 
<br>
imagine everything working exactly as planned.  Imagine everyone cursing 
<br>
the name of Brent Thomas for the next hundred generations, or the next 
<br>
hundred billion years, because he didn't think of that one important 
<br>
consequence of his wish.  What if I hadn't asked you about the human 
<br>
infants?  Do you think you would have thought of it on your own?
<br>
<p><em>&gt; Protect the FUNDAMENTAL condition
</em><br>
<em>&gt; where a sentient is not to be affected unless they choose to be 
</em><br>
<em>&gt; affected...
</em><br>
<p>And everyone curses your name for the next hundred billion years because 
<br>
the System spends fifteen hours out of every day asking everyone whether 
<br>
it's okay to rotate the planet another degree of arc.  Plus, talking to a 
<br>
sentient affects them.
<br>
<p><em>&gt; if you consider this deeply enough I'm confident that you 
</em><br>
<em>&gt; will agree that you would wish the ability to refuse outside change.
</em><br>
<p>Of course I do.  I wish for a lot of things, and I don't trust my native 
<br>
wishing abilities.
<br>
<p><em>&gt; I do think that most will embrace the change, and the change will be 
</em><br>
<em>&gt; better, smarter, faster etc...but retain the ability to choose.
</em><br>
<p>I agree this is a good idea, and I dare not write it into the code.
<br>
<p><em>&gt; Would you like to name nine other things that are so fundamental to 
</em><br>
<em>&gt; having an acceptable process that it should be a basic condition?  If
</em><br>
<em>&gt; you can't, I'm sure nine other people would be happy to do so.  Al-Qaeda
</em><br>
<em>&gt; thinks that basing the AI on the Koran is so fundamental to having an
</em><br>
<em>&gt; acceptable process that it should be a basic condition.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; !!! NO - there is only one fundamental thing...ASK before modification 
</em><br>
<em>&gt; and respect the answer. There is no need for other fundamentals in a
</em><br>
<em>&gt; friendly system operating from our collective volition.
</em><br>
<p>Yeah, but different people seem to have widely different opinions for the 
<br>
One Fundamental Thing.
<br>
<p><em>&gt; Including human infants, I assume.  I'll expect you to deliver the 
</em><br>
<em>&gt; exact, eternal, unalterable specification of what constitutes a
</em><br>
<em>&gt; &quot;sentient&quot; by Thursday.  Whatever happened to keeping things simple?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; !!! I'll deliver it today...any being that the system can communicate 
</em><br>
<em>&gt; with and that is capable of responding.
</em><br>
<p>That's not well-specified.  Specifying it well would be an independent 
<br>
project of scope comparable to well-specifying the set of transforms and 
<br>
associated order of evaluation associated with &quot;knew more, thought faster&quot;.
<br>
<p>As stated, your definition applies to ELIZA.
<br>
<p><em>&gt; The system should be able to
</em><br>
<em>&gt; communicate with any human (in any modality), and (when!) we encounter
</em><br>
<em>&gt; alien sentients they should not be 'modified' before we are capable of
</em><br>
<em>&gt; communicating with them ;-) By responding I mean that the system must
</em><br>
<em>&gt; explain what modification it is intending to make and allow an informed
</em><br>
<em>&gt; choice.
</em><br>
<p>&quot;Explaining&quot; intrinsically implies taking actions that modify the sentient 
<br>
toward a goal state defined as understanding a given set of beliefs.  If 
<br>
you didn't bootstrap this from an initial dynamic, and got it wrong when 
<br>
you wrote it down as an unalterable invariant, you could easily overwrite 
<br>
everyone in the solar system with minds that deeply understood the 
<br>
consequences of getting a glass of water.
<br>
<p>What you want to do is *dangerous*.  Extremely dangerous.  It doesn't 
<br>
matter if you think it's a great idea morally, it doesn't get any less 
<br>
extremely dangerous.
<br>
<p><em>&gt; If the system is unable to clearly (to that target) explain why
</em><br>
<em>&gt; the modification is necessary it should not perform it.
</em><br>
<p>Define me &quot;clearly&quot;.  Define it down to the level of a predicate that 
<br>
operates over arbitrary atomic configurations of matter.  Get it exactly 
<br>
right on the first try.
<br>
<p>By restraining the initial dynamic with an external system, you're losing 
<br>
the bootstrapping capability of the initial dynamic with respect to that 
<br>
external system.  It's not an unsolvable problem, but I'd have to solve it 
<br>
with something resembling a Brent Thomas volition-extrapolating system that 
<br>
asks how you would define &quot;clearly&quot; if you knew more, thought faster etc. 
<br>
And at that point, why should I turn it over to you, instead of the planet? 
<br>
&nbsp;&nbsp;Why not ask the same volition whether the whole thing is a good idea in 
<br>
the first place?  Do I ask your volition to produce a definition of 
<br>
&quot;clearly&quot; even if your volition thinks the whole thing is an awful idea? 
<br>
What if your volition returns blank code?
<br>
<p><em>&gt; If the system
</em><br>
<em>&gt; needs (for some reason determined by the collective volition) to modify
</em><br>
<em>&gt; a sentient and the system cannot communicate with it then the sentient
</em><br>
<em>&gt; should be 'enclaved'
</em><br>
<p>Define &quot;enclaved&quot;, down to the level etc.
<br>
<p><em>&gt; if necessary until the system is able to
</em><br>
<em>&gt; explain...dont modify without permission anything capable of
</em><br>
<em>&gt; giving/rejecting permission. Pretty simple.
</em><br>
<p>The light now leaving the Simple constellation will not shine on this 
<br>
project proposal for millions of years.
<br>
<p><em>&gt; For this particular example human infants should generally not need to 
</em><br>
<em>&gt; be modified by the system unless their parent wishes them to be
</em><br>
<p>Good heavens.
<br>
<p>Define me &quot;infant&quot;.  Define me &quot;parent&quot;.  Is a retarded adult a child? 
<br>
What if the original father is an unknown bum in Ohio and the child was 
<br>
adopted?  What if the entire human species ends up fitting your definition 
<br>
of children?  How is this unalterable rule going to work a billion years 
<br>
from now?
<br>
<p><em>&gt; (and I 
</em><br>
<em>&gt; do think we GIVE the RIGHT to modify infants to human parents 
</em><br>
<em>&gt; today...nothing really new here).
</em><br>
<p>Socially, no.  But oh the can of worms, if the FAI programmer has to 
<br>
personally write the definitions.
<br>
<p><em>&gt; In this instance the infants are only 
</em><br>
<em>&gt; potential sentients as they are not capable of responding.
</em><br>
<p>You're applying common sense, and that's a good thing.  But your common 
<br>
sense won't go into code so easily, nor apply to the next million years. 
<br>
There is a way to amplify common sense so that it can apply with that kind 
<br>
of precision; it is called &quot;extrapolating a volition&quot;, your common sense if 
<br>
you knew more, thought faster, saw more consequences, could more precisely 
<br>
specify what you wanted.  But why should I extrapolate just this idea of 
<br>
Brent Thomas's?  Why not turn over the question to a collective volition?
<br>
<p><em>&gt; The system 
</em><br>
<em>&gt; truly isn't a genie because the collective volition (so I believe) will 
</em><br>
<em>&gt; not see the need to modify infants (whats so urgent they need to be 
</em><br>
<em>&gt; modified anyway? I don't forsee any condition where the system could not
</em><br>
<em>&gt; enclave them until they develop enough to communicate)
</em><br>
<p>Enclave them away from their parents, away from anyone else who might 
<br>
modify them?  Enclave them with all the mass murderers who opted out, or 
<br>
force their parents to stay with them?  You're rattling off bright ideas 
<br>
with unintended consequences one after the other, and you might be able to 
<br>
patch the proposal to where it's not obviously wrong to me, but you can't 
<br>
patch the proposal to where it's really genuinely right.
<br>
<p>All of this is showing up in your proposal after I questioned it; I didn't 
<br>
hear this originally.  You cannot hardcode this sort of thing!
<br>
<p><em>&gt; Could you please elaborate further on all the independent details you 
</em><br>
<em>&gt; would like to code into eternal, unalterable invariants?  If you add
</em><br>
<em>&gt; enough of
</em><br>
<em>&gt; them we can drive the probability of them all working as expected down 
</em><br>
<em>&gt; to effectively zero.  Three should be sufficient, but redundancy is
</em><br>
<em>&gt; always a good thing.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; !!! Sure, just one detail --- don't modify a sentient without 
</em><br>
<em>&gt; permission, when modification is projected according to the collective 
</em><br>
<em>&gt; volition explain process until sentient grasps concept and only proceed 
</em><br>
<em>&gt; if accepted. Pretty straight forward.
</em><br>
<p>Straightforward like the design of the human brain is straightforward.
<br>
<p><em>&gt;&gt; Do this and I think the vision of the coming singularity will be more 
</em><br>
<em>&gt;&gt; palatable for all humanity.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It's not about public relations, it's about living with the actual 
</em><br>
<em>&gt; result for the next ten billion years if that wonderful PR invariant
</em><br>
<em>&gt; turns out to be a bad idea.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; !!! First it is about public relations (initially) or else your efforts 
</em><br>
<em>&gt; a FAI may be stomped by the establishment and foom! Some non F ai will 
</em><br>
<em>&gt; be developed...into the razor blades blindly...it behoves us to make the
</em><br>
<em>&gt; approach palatable to humanity.
</em><br>
<p>Not by hacking the Collective Volition so that it goes wrong.  If this was 
<br>
about PR, I would never have raised the issue of Friendliness in the first 
<br>
place - once you raise the issue, people argue with you about it.  It would 
<br>
be much more clever, if PR were the problem, to avoid discussing the issue, 
<br>
so no one would take it seriously.
<br>
<p>And furthermore, even if the system ends up being temporary, our first days 
<br>
will affect the next billion years.  I flatly refuse to lend myself to 
<br>
playing PR games with the actual code.  If people have just concerns I will 
<br>
try to address them.  Aside from that, forget it.
<br>
<p><em>&gt; Not under your system, no.  I would like to allow your grownup self 
</em><br>
<em>&gt; and/or your volition to object effectively.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; !!! Sorry...the decision is MINE...and there is no rush...even if the 
</em><br>
<em>&gt; projected volition is correct and my future self will have wanted that I
</em><br>
<em>&gt; still require the CHOICE - maybe it would be better if I were to follow
</em><br>
<em>&gt; the recommendation but life is a journey and I don't want to skip 
</em><br>
<em>&gt; ahead...The system should present the option and respect the decision.
</em><br>
<p>Maybe there's a better way to keep the decision yours.  Maybe it is ruled 
<br>
out by having the system explain everything to you in advance.  Maybe the 
<br>
process of explanation causes you to argue elaborately and thereby depart 
<br>
from the path you would have taken.  Maybe knowing in advance takes all the 
<br>
fun out of it.  If so, the mistake is *hardcoded* - there is no way to 
<br>
revoke it, even if it ends up being awkward and unnecessary, even if it 
<br>
destroys you.
<br>
<p><em>&gt; I suppose that if that is the sort of solution you would come up with 
</em><br>
<em>&gt; after thinking about it for a few years, it might be the secondary
</em><br>
<em>&gt; dynamic. For myself I would argue against that, because it sounds like
</em><br>
<em>&gt; individuals have been handed genie bottles with warning labels, and I
</em><br>
<em>&gt; don't think that's a good thing.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; !!!but that's exactly the point...if you don't think it's a good thing, 
</em><br>
<em>&gt; well that doesn't matter to me... I am the one who has to choose. And
</em><br>
<em>&gt; remember this is only in respect to modifications the system DECIDES to
</em><br>
<em>&gt; MAKE to me...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; How it reacts to 'wishes' (ala genie) is a whole nother 
</em><br>
<em>&gt; discussion...this fundamental application of CHOICE is only to things
</em><br>
<em>&gt; the system decides it needs to do and in the process must CHANGE me...at
</em><br>
<em>&gt; that point I get to choose.
</em><br>
<p>It's a powerful argument.  And I admit that some of my opposition is simply 
<br>
because I once thought that individual choice should be absolutely 
<br>
sovereign, and then I learned more, thought longer, and realized that there 
<br>
are things I value more than the absolute sovereignty of the individual. 
<br>
That I want infants to grow into humans, if nothing else, would break 
<br>
autonomy as an absolute principle.
<br>
<p>But that ends up being irrelevant, because the other problems are enough to 
<br>
torpedo the proposal.
<br>
<p><em>&gt; !!! Again...there is only one fundamental thing here
</em><br>
<p>There's about twenty fundamental things here.
<br>
<p><em>&gt; ...insofar as the 
</em><br>
<em>&gt; system decides it needs to modify me it must first obtain
</em><br>
<em>&gt; permission...thats it. Pretty basic and no trade off required...remember
</em><br>
<em>&gt; this applies only to modifications to my person/self/intellect that the
</em><br>
<em>&gt; system deems necessary. This control (if I have any say about it and
</em><br>
<em>&gt; that's the basic point isnt it?) must not be surrendered. And there are
</em><br>
<em>&gt; no circumstances that I can forsee (with my limited 2004 intellect
</em><br>
<em>&gt; yes...but that is the sentient being asked to make a choice) that cannot
</em><br>
<em>&gt; wait, be fully explained, and abide by my choice.
</em><br>
<p>Good heavens, how hard have you tried to foresee?  Want to bet twenty 
<br>
dollars that two years from today, you will disagree with at least one 
<br>
confident assertion in your emails?
<br>
<p><em>&gt; The collective
</em><br>
<em>&gt; volition guides the systems of the universe as it should...I just
</em><br>
<em>&gt; reserve the right to say 'no' as it regards my
</em><br>
<em>&gt; self/intellect/personality.
</em><br>
<p>So now you're identifying your *intellect* or *personality* as the critical 
<br>
thing to safeguard?  So it'd be okay to transport you to an alternate 
<br>
dimension based on a hentai anime, as long as the essential *you* wasn't 
<br>
altered?  What kind of consequences do you need to understand, in how much 
<br>
detail, before you can give informed consent?  Does the system spend ten 
<br>
hours explaining to you the exact neurological damage every time you drink 
<br>
a glass of alcohol?  If you say that this doesn't need to happen because it 
<br>
happens as the result of your own actions, is it okay to leave a cake 
<br>
labeled &quot;eat me&quot; at your front door that oh incidentally raises your IQ 20 
<br>
points?  Endless can of worms, here.
<br>
<p>The whole point of having an *initial dynamic* is that it compresses down 
<br>
the problem to where it can consist of a small number of identifiable 
<br>
technical problems that can be satisfactorily solved to bootstrap the 
<br>
process.  By trying to add a Bill of Inalienable Rights to the initial 
<br>
dynamic, you're losing that, and presumably ruling out large numbers of 
<br>
scenarios where we want something entirely different to which a Bill of 
<br>
Inalienable Rights is orthogonal.  It constitutes taking over the world.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9295.html">Samantha Atkins: "Re: Collective Volition: Wanting vs Doing."</a>
<li><strong>Previous message:</strong> <a href="9293.html">Brent Thomas: "RE: Fundamentals - was RE: Visualizing muddled volitions"</a>
<li><strong>In reply to:</strong> <a href="9293.html">Brent Thomas: "RE: Fundamentals - was RE: Visualizing muddled volitions"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9301.html">Metaqualia: "Re: Fundamentals - was RE: Visualizing muddled volitions"</a>
<li><strong>Reply:</strong> <a href="9301.html">Metaqualia: "Re: Fundamentals - was RE: Visualizing muddled volitions"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9294">[ date ]</a>
<a href="index.html#9294">[ thread ]</a>
<a href="subject.html#9294">[ subject ]</a>
<a href="author.html#9294">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
