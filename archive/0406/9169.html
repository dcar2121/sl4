<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Collective Volition: Wanting vs Doing.</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Collective Volition: Wanting vs Doing.">
<meta name="Date" content="2004-06-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Collective Volition: Wanting vs Doing.</h1>
<!-- received="Sat Jun 12 16:12:25 2004" -->
<!-- isoreceived="20040612221225" -->
<!-- sent="Sat, 12 Jun 2004 18:12:11 -0400" -->
<!-- isosent="20040612221211" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Collective Volition: Wanting vs Doing." -->
<!-- id="40CB7FBB.80604@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="006b01c450c4$55eeec30$6401a8c0@mra02" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Collective%20Volition:%20Wanting%20vs%20Doing."><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Jun 12 2004 - 16:12:11 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9170.html">Richard Kowalski: "Bayesian meditation workshops ; was O.K, weeks up.  Let's get back to bashing Sing Inst ;"</a>
<li><strong>Previous message:</strong> <a href="9168.html">Eliezer Yudkowsky: "Bayesian monasteries"</a>
<li><strong>In reply to:</strong> <a href="9167.html">Michael Roy Ames: "Collective Volition: Wanting vs Doing."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9173.html">Michael Roy Ames: "Re: Collective Volition: Wanting vs Doing."</a>
<li><strong>Reply:</strong> <a href="9173.html">Michael Roy Ames: "Re: Collective Volition: Wanting vs Doing."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9169">[ date ]</a>
<a href="index.html#9169">[ thread ]</a>
<a href="subject.html#9169">[ subject ]</a>
<a href="author.html#9169">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michael Roy Ames wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; It is confusing to talk about CV *doing* something.  CV is data used by FAI
</em><br>
<em>&gt; to help figure out what humans *want* to do.  The 'optimization process'
</em><br>
<em>&gt; then has to figure out just how to get from the existing situation to a
</em><br>
<em>&gt; desired future situation, without 'breaking any eggs'.
</em><br>
<p>I agree that my original wording is confusing - but I was describing a 
<br>
confusing thing.  The CV *is* doing something; the CV is *not* data used by 
<br>
FAI.  The FAI is an optimization process that defines itself as an 
<br>
approximation to collective volition.
<br>
<p>Humans aren't expected utility maximizers.  Descriptively speaking, we 
<br>
don't run on expected utility at all.  There are decisions humans make that 
<br>
are hard to describe in terms of expected utility, such as, for example, 
<br>
the choice between satisficing and maximizing, or the choice between an 
<br>
expected utility formalism that represents infinite utilities and an 
<br>
expected utility formalism that does not (an allusion to a recent Nick 
<br>
Bostrom paper).
<br>
<p>Naively, we might phrase the problem:  &quot;Humans are expected utility 
<br>
maximizers, FAIs are expected utility maximizers, let's transfer the 
<br>
utility function from the human to the FAI.&quot;  Now note that even naively we 
<br>
do not speak of human utility functions as *data* for a constant FAI 
<br>
utility function that tries to maximally &quot;satisfy human utility functions&quot;. 
<br>
&nbsp;&nbsp;We speak of transferring over the utility functions themselves. 
<br>
Otherwise, for example, the FAI would wirehead on altering humans to have 
<br>
easily satisfied utility functions.
<br>
<p>But this naive description is not good enough, I think; humans are not 
<br>
expected utility maximizers.  So the next step up in abstraction is to view 
<br>
a human as a decision process, a dynamic-that-outputs-preferences, and to 
<br>
define &quot;expected utility maximization&quot; as a breakable abstraction from the 
<br>
decision process.  I am not sure how to formally define this business of 
<br>
creating abstractions from decision processes, but it is where I think FAI 
<br>
theory must go.  An example of an abstraction is looking at a human 
<br>
outputting decisions, and deducing expected utility - but that is only one 
<br>
kind of abstraction one might employ.
<br>
<p>The distinction is that a Collective Volition does not ask &quot;What would 
<br>
extrapolated humankind want?&quot; but &quot;What would extrapolated humankind 
<br>
decide?&quot;  There's a major difference, in formal terms; the former is a 
<br>
special case of the latter.  Similarly, the optimization process that views 
<br>
itself as an approximation to collective volition does not have a fixed 
<br>
utility function that says &quot;Do what human utility functions say,&quot; nor a 
<br>
variable utility function bound to the human equivalent of utility 
<br>
functions, nor a constant utility function that says &quot;Do what extrapolated 
<br>
humankind would decide.&quot;  Rather, the FAI views its own decision process as 
<br>
an approximation to what extrapolated humankind would decide.
<br>
<p>This is a step toward handling the kind of problem Mitchell Porter is 
<br>
concerned about, choosing between different systems for representing 
<br>
expected utility and the like.  Faced with such a dilemma one should not 
<br>
ask &quot;What would a human want?&quot;, but &quot;What would a human decide?&quot; which is 
<br>
the more general form of the question.
<br>
<p>It may help to know, at this point, that the original motivation for 
<br>
expected utility was that any set of *decisions* which obeyed certain 
<br>
consistency axioms could be summarized using expected utility.  In the 
<br>
original math, wanting is deduced from deciding, not the other way around.
<br>
<p>*If the FAI works correctly*, then the existence of an FAI is transparent; 
<br>
the act of setting the FAI in motion is the act of manifesting an 
<br>
approximation of the collective volition of extrapolated humankind, which 
<br>
may choose to change its dynamic, or choose to write some code other than 
<br>
an FAI.  The collective volition is not an external thing that an 
<br>
independent FAI tries to satisfy.  The collective volition would be the 
<br>
same function that makes decisions about the FAI's internal code.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9170.html">Richard Kowalski: "Bayesian meditation workshops ; was O.K, weeks up.  Let's get back to bashing Sing Inst ;"</a>
<li><strong>Previous message:</strong> <a href="9168.html">Eliezer Yudkowsky: "Bayesian monasteries"</a>
<li><strong>In reply to:</strong> <a href="9167.html">Michael Roy Ames: "Collective Volition: Wanting vs Doing."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9173.html">Michael Roy Ames: "Re: Collective Volition: Wanting vs Doing."</a>
<li><strong>Reply:</strong> <a href="9173.html">Michael Roy Ames: "Re: Collective Volition: Wanting vs Doing."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9169">[ date ]</a>
<a href="index.html#9169">[ thread ]</a>
<a href="subject.html#9169">[ subject ]</a>
<a href="author.html#9169">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
