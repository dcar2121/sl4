<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: SIAI has become slightly amusing</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: SIAI has become slightly amusing">
<meta name="Date" content="2004-06-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: SIAI has become slightly amusing</h1>
<!-- received="Fri Jun  4 12:22:28 2004" -->
<!-- isoreceived="20040604182228" -->
<!-- sent="Fri, 4 Jun 2004 14:22:14 -0400" -->
<!-- isosent="20040604182214" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: SIAI has become slightly amusing" -->
<!-- id="00e401c44a60$dcc6d2f0$6401a8c0@ZOMBIETHUSTRA" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="40C0B7A1.8040204@intelligence.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20SIAI%20has%20become%20slightly%20amusing"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Fri Jun 04 2004 - 12:22:14 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9085.html">Eliezer Yudkowsky: "Re: SIAI has become slightly amusing"</a>
<li><strong>Previous message:</strong> <a href="9083.html">Michael Anissimov: "Re: SIAI has become slightly amusing"</a>
<li><strong>In reply to:</strong> <a href="9083.html">Michael Anissimov: "Re: SIAI has become slightly amusing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9088.html">Eliezer Yudkowsky: "Re: SIAI has become slightly amusing"</a>
<li><strong>Reply:</strong> <a href="9088.html">Eliezer Yudkowsky: "Re: SIAI has become slightly amusing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9084">[ date ]</a>
<a href="index.html#9084">[ thread ]</a>
<a href="subject.html#9084">[ subject ]</a>
<a href="author.html#9084">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi Michael,
<br>
<p><em>&gt; Also, it seems like your judgement of their 
</em><br>
<em>&gt; overconfidence is 
</em><br>
<em>&gt; heavily based on exchanges between *you* and them - 
</em><br>
<p>No, it's also based on statements such as (paraphrases, not quotes)
<br>
<p>&quot;I understand both AI and the Singularity much better than anyone else
<br>
in the world&quot; 
<br>
&quot;I don't need to ever study in a university, because I understand what's
<br>
important better than all the professors anyway.&quot;
<br>
Etc. etc.
<br>
(Eli)
<br>
<p>&quot;A scientist who doesn;t accept SIAI's theory is not a good scientist&quot;
<br>
&quot;SIAI's theories are on a par with Einstein's General Relativity Theory&quot;
<br>
&quot;I hesitate to call the SL4 list a peanut gallery.&quot; [meaning: nothing
<br>
anyone else says on the list is of value]
<br>
(Michael Wilson)
<br>
<p><em>&gt; are they equally 
</em><br>
<em>&gt; dismissive towards *all* AGI designers?  
</em><br>
<p>Basically: Yes, they are.  
<br>
<p>Eli seems to have a liking for James Rogers' approach, but has been
<br>
extremely dismissive toward every other AI approach I've seen him talk
<br>
about.  
<br>
<p>However, most AI folks, when obnoxiously dismissed by him, don't bother
<br>
to argue (since they have better things to do with their time, and since
<br>
he's not an &quot;AI professional&quot; whose opinions &quot;matter&quot; in academia or
<br>
industry.)
<br>
<p><em>&gt; The word &quot;imposing&quot; suggests something out of line with your 
</em><br>
<em>&gt; volition.  
</em><br>
<em>&gt; But the whole point of any FAI is to carry out your volition.  If the 
</em><br>
<em>&gt; volition it is carrying out is unfavorable and foreign to 
</em><br>
<em>&gt; you, then that 
</em><br>
<em>&gt; would constitute a failure on the part of the programmers.  
</em><br>
<p>The notion of your &quot;volition&quot; as Eliezer now proposes it is NOT
<br>
necessarily aligned with your desires or your will at this moment.
<br>
<p>Rather, it's an AI's estimate of what you WOULD want at this moment, if
<br>
you were a better person according to your own standards of goodness.
<br>
<p>Tricky notion, no?
<br>
<p><em>&gt; But we can't very well have an SI carrying out everyone's requests 
</em><br>
<em>&gt; without considering the interactions between the consequences 
</em><br>
<em>&gt; of these 
</em><br>
<em>&gt; requests, right?  CollectiveVolition is a sophisticated way of doing 
</em><br>
<em>&gt; that.  
</em><br>
<p>Yes, but there's a difference btw collective volition and collective
<br>
*will*. 
<br>
<p>Collective will, collective choice -- that's commonly known as
<br>
&quot;democracy.&quot;  Yes, it's not easy to implement, but it's a
<br>
well-understood social technology.
<br>
<p>Collective volition is different from this: more interesting but
<br>
scarier.
<br>
<p>You don't seem to be confronting this difference...
<br>
<p><em>&gt; Well, the idea is to not deny you the chance to grow and change and 
</em><br>
<em>&gt; learn to the extent that that would bother you.
</em><br>
<p>The idea, it seems, is to allow me to grow and change and learn to the
<br>
extent that the AI estimates I will, in future, want my past self to be
<br>
allowed to do.
<br>
<p>In other words, the AI is supposed to treat me like a child, and
<br>
estimate what the adult-me is eventually going to want the child-me to
<br>
have been allowed to do.  
<br>
&nbsp;
<br>
In raising my kids, I use this method sometimes, but more often I let
<br>
the children do what they presently desire rather than what I think
<br>
their future selves will want their past selves to have done.
<br>
<p>I think that, as a first principle, sentient beings should be allowed
<br>
their free choice, and issues of &quot;collective volition&quot; and such should
<br>
only enter into the picture in order to resolve conflicts between
<br>
different sentience's free choices.
<br>
<p><em>&gt; Good question!  The ultimate answer will be for the FAI to 
</em><br>
<em>&gt; decide, and 
</em><br>
<em>&gt; we want to seed the FAI with the moral complexity necessary 
</em><br>
<em>&gt; to make that 
</em><br>
<em>&gt; decision with transhuman wisdom and compassion.  
</em><br>
<p>I much prefer to embody an AI with &quot;respect choices of sentient beings
<br>
whenever possible&quot; as a core value.
<br>
<p>Concrete choice, not estimated volition.
<br>
<p>This is a major ethical choice, on which Eliezer and I appear to
<br>
currently significantly differ.
<br>
<p><em>&gt; On SL4, this mostly manifests itself with respect to you. 
</em><br>
<p>This is only because no one else bothers to take the time to challenge
<br>
Eliezer in a serious way, because either
<br>
<p>* they lack the technical chops and/or breadth of knowledge to do so,
<br>
and/or
<br>
<p>* they lack the inclination (they don't find it entertaining, they don't
<br>
take Eliezer seriously enough to bother arguing with him, etc.)
<br>
<p><em>&gt; Eliezer has published many hundreds of pages on his FAI theory, 
</em><br>
<em>&gt; certainly more detail than I've seen from any other AGI 
</em><br>
<em>&gt; designer.  What 
</em><br>
<em>&gt; makes you think Eliezer/Michael have a &quot;dislike for sharing 
</em><br>
<em>&gt; their more detailed ideas&quot;?
</em><br>
<p>The fact that Eliezer has said so to me, in the past.  He said he didn't
<br>
want to share the details of his ideas on AI because I or others might
<br>
use them in an unsafe way.
<br>
<p>More recently, Michael Wilson has made comments to me in a similar vein.
<br>
<p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9085.html">Eliezer Yudkowsky: "Re: SIAI has become slightly amusing"</a>
<li><strong>Previous message:</strong> <a href="9083.html">Michael Anissimov: "Re: SIAI has become slightly amusing"</a>
<li><strong>In reply to:</strong> <a href="9083.html">Michael Anissimov: "Re: SIAI has become slightly amusing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9088.html">Eliezer Yudkowsky: "Re: SIAI has become slightly amusing"</a>
<li><strong>Reply:</strong> <a href="9088.html">Eliezer Yudkowsky: "Re: SIAI has become slightly amusing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9084">[ date ]</a>
<a href="index.html#9084">[ thread ]</a>
<a href="subject.html#9084">[ subject ]</a>
<a href="author.html#9084">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
