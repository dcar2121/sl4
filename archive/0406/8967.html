<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: FAI: Collective Volition</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: FAI: Collective Volition">
<meta name="Date" content="2004-06-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: FAI: Collective Volition</h1>
<!-- received="Wed Jun  2 10:10:10 2004" -->
<!-- isoreceived="20040602161010" -->
<!-- sent="Wed, 02 Jun 2004 12:09:58 -0400" -->
<!-- isosent="20040602160958" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: FAI: Collective Volition" -->
<!-- id="40BDFBD6.3010105@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20040602105200.A1508@weidai.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20FAI:%20Collective%20Volition"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Jun 02 2004 - 10:09:58 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8968.html">Ben Goertzel: "RE: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8966.html">tmazanec1@juno.com: "Can I stop getting email and still read archives?"</a>
<li><strong>In reply to:</strong> <a href="8953.html">Wei Dai: "Re: FAI: Collective Volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9025.html">Wei Dai: "Re: FAI: Collective Volition"</a>
<li><strong>Reply:</strong> <a href="9025.html">Wei Dai: "Re: FAI: Collective Volition"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8967">[ date ]</a>
<a href="index.html#8967">[ thread ]</a>
<a href="subject.html#8967">[ subject ]</a>
<a href="author.html#8967">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Wei Dai wrote:
<br>
<p><em>&gt; On Wed, Jun 02, 2004 at 05:32:38AM -0400, Eliezer Yudkowsky wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; The last question strikes me as irrelevant;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It's relevant to your fifth goal:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 5. Avoid creating a motive for modern-day humans to fight over the 
</em><br>
<em>&gt; initial dynamic.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If you can't convince modern-day humans that collective volition 
</em><br>
<em>&gt; represents them then naturally they'll want to fight. For example, if 
</em><br>
<em>&gt; al-Qaeda programmers wrote an AI, &quot;knew more&quot; would mean knowing that
</em><br>
<em>&gt; only Allah exists, and they would fight anyone who suggests that &quot;knew
</em><br>
<em>&gt; more&quot; should mean a Bayesian probability distribution over a wide range
</em><br>
<em>&gt; of gods.
</em><br>
<p>The point of the analogy is to postulate al-Qaeda programmers smart enough 
<br>
to actually build an AI.  Perhaps a better phrase in (5) would be, &quot;avoid 
<br>
policies which would create conflicts of interest if multiple parties 
<br>
followed them&quot;.  Categorical Imperative sort of thing.  I am *not* going to 
<br>
&quot;program&quot; my AI with the instruction that Allah does not exist, just as I 
<br>
do not want the al-Qaeda programmers programming their AI with the 
<br>
instruction that Allah does exist.  Let the Bayesian Thingy find the map 
<br>
that reflects the territory.  So the al-Qaeda programmers would advise me, 
<br>
for they know I will not listen if they mention Allah in their advice.
<br>
<p>No matter what any AI project does, some modern-day human will be annoyed. 
<br>
&nbsp;&nbsp;At some point one is left with Aristotle indignantly objecting to the use 
<br>
of a collective volition model that assumes that thought resides in the 
<br>
brain; the brain is an organ for cooling the blood, and Aristotle wishes 
<br>
his volition extrapolated on this basis.  I draw the line at objections 
<br>
which, if they were true, would cause the project to fail harmlessly.  For 
<br>
on that premise the speaker has nothing to fear from me, and should not 
<br>
even be paying attention.
<br>
<p>I am trying not to be a jerk, but you can always hypothesize someone who 
<br>
calls me a jerk regardless.  In such cases I will wait for some real person 
<br>
to show up who is offended.  Hypothetical complainers too often are 
<br>
partial-people, constructed from imagination as stereotypes, bearing all 
<br>
the bad qualities and none of the good.
<br>
<p><em>&gt;&gt; Let me toss your question back to you:  What do you think a devout 
</em><br>
<em>&gt;&gt; Christian should be said to *want*, conditional upon Christianity
</em><br>
<em>&gt;&gt; being false?  Fred wants box A conditional upon box A containing the
</em><br>
<em>&gt;&gt; dimaond; Fred wants box B conditional upon box B containing the
</em><br>
<em>&gt;&gt; diamond.  What may a devout Christian be said to want, conditional
</em><br>
<em>&gt;&gt; upon Christianity being false?  I can think of several approaches.
</em><br>
<em>&gt;&gt; The human approach would be to *tell* the devout Christian that
</em><br>
<em>&gt;&gt; Christianity was false, then accept what they said in reply; but that
</em><br>
<em>&gt;&gt; is the Christian's reaction on being *told* that Christianity is
</em><br>
<em>&gt;&gt; false, it is not what the Christian &quot;would want&quot; conditional upon
</em><br>
<em>&gt;&gt; Christianity being false.  If the Christian is capable of seriously
</em><br>
<em>&gt;&gt; thinking about the possibility, the problem is straightforward enough.
</em><br>
<em>&gt;&gt; If not, how would one extract an answer for the conditional question?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The thing is, I'm not sure that's the right question to ask, and the 
</em><br>
<em>&gt; example I choose was meant to show the apparent absurdity of asking it. 
</em><br>
<em>&gt; So I don't understand what point you're making by tossing the question 
</em><br>
<em>&gt; back to me.
</em><br>
<p>I don't think it's an absurd question to ask, and I can see several 
<br>
possible ways to ask it, such as:
<br>
<p>1)  How would former Christians who still identify with and sympathize with 
<br>
their past selves, prefer those past selves to be treated?
<br>
2)  What policy would a loving Christian (one who aspires to treat their 
<br>
neighbors, even their non-Christian neighbors, as they treat themselves) 
<br>
suggest as a general policy for &quot;people with wrong religious beliefs&quot;?
<br>
3)  What policy does the Christian suggest about the general case of deeply 
<br>
held but wrong beliefs, without specifying that it is about religion?
<br>
4)  Suppose it's possible to modularly extrapolate someone with their 
<br>
ideological block against *considering the possibility* removed - the same 
<br>
person, but without 'realizing' that their religion forbids them to 
<br>
seriously think about the possibility.  If this can be done, what would 
<br>
they say of the hypothetical?
<br>
5)  If the predictable result of growing up farther together is to lose 
<br>
faith in a particular way, what would the self of that outcome say?
<br>
<p>And, as you point out:
<br>
<p>6)  Suppose we extrapolated the person with the knowledge forcibly 
<br>
inserted, and modeled the resulting catastrophic breakdown of faith, what 
<br>
would that person say of their present self?
<br>
<p><em>&gt; Have you looked at any of the existing literature on preference 
</em><br>
<em>&gt; aggregation? For example this paper: &quot;Utilitarian Aggregation of Beliefs
</em><br>
<em>&gt;  and Tastes&quot;, available at 
</em><br>
<em>&gt; <a href="http://www.tau.ac.il/~schmeid/PDF/Gil_Sam_Schmeid_Utilitarian_Bayesian.pdf">http://www.tau.ac.il/~schmeid/PDF/Gil_Sam_Schmeid_Utilitarian_Bayesian.pdf</a>.
</em><br>
<em>&gt;  I think it might be worth taking a look if you haven't already.
</em><br>
<p>Reading... read.  Relevant stuff, thanks.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8968.html">Ben Goertzel: "RE: Volitional Morality and Action Judgement"</a>
<li><strong>Previous message:</strong> <a href="8966.html">tmazanec1@juno.com: "Can I stop getting email and still read archives?"</a>
<li><strong>In reply to:</strong> <a href="8953.html">Wei Dai: "Re: FAI: Collective Volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9025.html">Wei Dai: "Re: FAI: Collective Volition"</a>
<li><strong>Reply:</strong> <a href="9025.html">Wei Dai: "Re: FAI: Collective Volition"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8967">[ date ]</a>
<a href="index.html#8967">[ thread ]</a>
<a href="subject.html#8967">[ subject ]</a>
<a href="author.html#8967">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
