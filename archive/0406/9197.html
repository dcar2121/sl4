<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Collective Volition: Wanting vs Doing.</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Collective Volition: Wanting vs Doing.">
<meta name="Date" content="2004-06-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Collective Volition: Wanting vs Doing.</h1>
<!-- received="Sun Jun 13 15:58:04 2004" -->
<!-- isoreceived="20040613215804" -->
<!-- sent="Sun, 13 Jun 2004 17:58:04 -0400" -->
<!-- isosent="20040613215804" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Collective Volition: Wanting vs Doing." -->
<!-- id="40CCCDEC.5010108@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="C6033C2E-BD80-11D8-85E5-000A95B1AFDE@objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Collective%20Volition:%20Wanting%20vs%20Doing."><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jun 13 2004 - 15:58:04 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9198.html">Ben Goertzel: "RE: About &quot;safe&quot; AGI architecture"</a>
<li><strong>Previous message:</strong> <a href="9196.html">Samantha Atkins: "Re: About &quot;safe&quot; AGI architecture"</a>
<li><strong>In reply to:</strong> <a href="9195.html">Samantha Atkins: "Re: Collective Volition: Wanting vs Doing."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9205.html">Keith Henson: "Re: Collective Volition: Wanting vs Doing."</a>
<li><strong>Reply:</strong> <a href="9205.html">Keith Henson: "Re: Collective Volition: Wanting vs Doing."</a>
<li><strong>Reply:</strong> <a href="9206.html">Samantha Atkins: "Re: Collective Volition: Wanting vs Doing."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9197">[ date ]</a>
<a href="index.html#9197">[ thread ]</a>
<a href="subject.html#9197">[ subject ]</a>
<a href="author.html#9197">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Samantha Atkins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; On Jun 13, 2004, at 7:56 AM, Eliezer Yudkowsky wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Samantha, you write that you might have a badly warped view of what 
</em><br>
<em>&gt;&gt; kind of person you would like to be.  &quot;Badly warped&quot; by what criterion 
</em><br>
<em>&gt;&gt; that I feed to the FAI?  Your criterion?  Someone else's?  Where am I 
</em><br>
<em>&gt;&gt; supposed to get this information, if not, somehow, from you?  When you 
</em><br>
<em>&gt;&gt; write down exactly how the information is supposed to get from point A 
</em><br>
<em>&gt;&gt; (you) to point B (the FAI), and what the FAI does with the information 
</em><br>
<em>&gt;&gt; once it's there, you'll have something that looks like - surprise! - a 
</em><br>
<em>&gt;&gt; volition-extrapolating dynamic.  It's not a coincidence.  That's where 
</em><br>
<em>&gt;&gt; the idea of a volition-extrapolating dynamic *originally comes from*.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; That is my point.  The information is not necessarily available from the 
</em><br>
<em>&gt; person[s] in sufficient quality to make wise decisions that actually 
</em><br>
<em>&gt; work for the good of humanity.
</em><br>
<p>Okay... how do you know this?  Also, where do I get the information?  Like, 
<br>
the judgment criterion for &quot;wise decisions&quot; or &quot;good of humanity&quot;.  Please 
<br>
note that I mean that as a serious question, not a rhetorical one.  You're 
<br>
getting the information from somewhere, and it exists in your brain; there 
<br>
must be a way for me to suck it out of your skull.
<br>
<p><em>&gt;&gt; It's more an order-of-evaluation question than anything else.  I 
</em><br>
<em>&gt;&gt; currently guess that one needs to evaluate some &quot;knew more&quot; and 
</em><br>
<em>&gt;&gt; &quot;thought faster&quot; before evaluating &quot;more the people we wished we 
</em><br>
<em>&gt;&gt; were&quot;.  Mostly because &quot;knew more&quot; and &quot;thought faster&quot; starting from 
</em><br>
<em>&gt;&gt; a modern-day human who makes fluffy bunny errors doesn't have quite 
</em><br>
<em>&gt;&gt; the same opportunity to go open-endedly recursively wrong as &quot;more the 
</em><br>
<em>&gt;&gt; people we wished we were&quot; evaluated on a FBer.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, we all can make whatever assumptions we wish for what &quot;knew more&quot; 
</em><br>
<em>&gt; and &quot;thought faster&quot; would and would not remove or add to in our bag of 
</em><br>
<em>&gt; human characteristics.
</em><br>
<p>I realize it's currently a subjective judgment call, Samantha, but it looks 
<br>
to me like &quot;knew more&quot; and &quot;thought faster&quot; tend to preserve underlying 
<br>
invariants in a way that &quot;more the people we wished we were&quot; does not 
<br>
*necessarily* do (or at least, the necessity is subject to decision). 
<br>
Like... if I'd had access to, and been foolish enough to use, &quot;more the 
<br>
people we wished we were&quot;-class transformations, early in my career, I 
<br>
would have screwed myself up irrevocably.  While thinking longer and 
<br>
learning more cleared up at least some of my confusion, I hope.  That's why 
<br>
I would tend to put &quot;knew more&quot; and &quot;thought faster&quot; earlier in the order 
<br>
of evaluation.
<br>
<p><em>&gt;&gt; Right.  AI augmented self-improvement of humankind with the explicit 
</em><br>
<em>&gt;&gt; notation that the chicken-and-egg part of this problem is that 
</em><br>
<em>&gt;&gt; modern-day humans aren't smart enough to self-improve without stomping 
</em><br>
<em>&gt;&gt; all over their own minds with unintended consequences, aren't even 
</em><br>
<em>&gt;&gt; smart enough to evaluate the question &quot;What kind of person do you want 
</em><br>
<em>&gt;&gt; to be?&quot; over its real experiential consequences rather than a small 
</em><br>
<em>&gt;&gt; subset of human verbal descriptions of humanly expected consequences.  
</em><br>
<em>&gt;&gt; So rather than creating a *separate* self-improving humane thing, one 
</em><br>
<em>&gt;&gt; does something philosophically more complex and profound (but perhaps 
</em><br>
<em>&gt;&gt; not more difficult from the standpoint of FAI theory, although it 
</em><br>
<em>&gt;&gt; *sounds* a lot harder).  One binds a transparent optimization process 
</em><br>
<em>&gt;&gt; to predict what the grownup selves of modern-day humans would say if 
</em><br>
<em>&gt;&gt; modern humans grew up together with the ability to self-improve 
</em><br>
<em>&gt;&gt; knowing the consequences.  The decision function of the extrapolated 
</em><br>
<em>&gt;&gt; adult humanity includes the ability of the collective volition to 
</em><br>
<em>&gt;&gt; restrain its own power or rewrite the optimization function to 
</em><br>
<em>&gt;&gt; something else; the collective volition extrapolates its awareness 
</em><br>
<em>&gt;&gt; that it is just an extrapolation and not our actual decisions.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Excellent.  I get it!
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; In other words, one handles *only* and *exactly* the chicken-and-egg 
</em><br>
<em>&gt;&gt; part of the problem - that modern-day humans aren't smart enough to 
</em><br>
<em>&gt;&gt; self-improve to an adult humanity, and that modern-day society isn't 
</em><br>
<em>&gt;&gt; smart enough to render emergency first aid to itself - by writing an 
</em><br>
<em>&gt;&gt; AI that extrapolates over *exactly those* gaps to arrive at a picture 
</em><br>
<em>&gt;&gt; of future humankind if those problems were solved.  Then the 
</em><br>
<em>&gt;&gt; extrapolated superposed possible future humankinds, the collective 
</em><br>
<em>&gt;&gt; volition, hopefully decides to act in our time to boost us over the 
</em><br>
<em>&gt;&gt; chicken-and-egg recursion; doing enough to solve the hard part of the 
</em><br>
<em>&gt;&gt; problem, but not annoying us or taking over our lives, since that's 
</em><br>
<em>&gt;&gt; not what we want (I think; at least it's not what I want).  Or maybe 
</em><br>
<em>&gt;&gt; the collective volition does something else.  I may have phrased the 
</em><br>
<em>&gt;&gt; problem wrong.  But for I as an FAI programmer to employ some other 
</em><br>
<em>&gt;&gt; solution, such as creating a new species of humane intelligence, would 
</em><br>
<em>&gt;&gt; be inelegant; it doesn't solve exactly and only the difficult part of 
</em><br>
<em>&gt;&gt; the problem.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; I may end up needing to be inelegant, but first I want to try really 
</em><br>
<em>&gt;&gt; hard to find a way to do the Right Thing.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Thanks.  That clears up a lot.
</em><br>
<p>You're welcome!
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9198.html">Ben Goertzel: "RE: About &quot;safe&quot; AGI architecture"</a>
<li><strong>Previous message:</strong> <a href="9196.html">Samantha Atkins: "Re: About &quot;safe&quot; AGI architecture"</a>
<li><strong>In reply to:</strong> <a href="9195.html">Samantha Atkins: "Re: Collective Volition: Wanting vs Doing."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9205.html">Keith Henson: "Re: Collective Volition: Wanting vs Doing."</a>
<li><strong>Reply:</strong> <a href="9205.html">Keith Henson: "Re: Collective Volition: Wanting vs Doing."</a>
<li><strong>Reply:</strong> <a href="9206.html">Samantha Atkins: "Re: Collective Volition: Wanting vs Doing."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9197">[ date ]</a>
<a href="index.html#9197">[ thread ]</a>
<a href="subject.html#9197">[ subject ]</a>
<a href="author.html#9197">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
