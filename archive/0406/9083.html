<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI has become slightly amusing</title>
<meta name="Author" content="Michael Anissimov (anissimov@intelligence.org)">
<meta name="Subject" content="Re: SIAI has become slightly amusing">
<meta name="Date" content="2004-06-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI has become slightly amusing</h1>
<!-- received="Fri Jun  4 11:55:49 2004" -->
<!-- isoreceived="20040604175549" -->
<!-- sent="Fri, 04 Jun 2004 10:55:45 -0700" -->
<!-- isosent="20040604175545" -->
<!-- name="Michael Anissimov" -->
<!-- email="anissimov@intelligence.org" -->
<!-- subject="Re: SIAI has become slightly amusing" -->
<!-- id="40C0B7A1.8040204@intelligence.org" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="009f01c44a40$a900c9a0$6401a8c0@ZOMBIETHUSTRA" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Anissimov (<a href="mailto:anissimov@intelligence.org?Subject=Re:%20SIAI%20has%20become%20slightly%20amusing"><em>anissimov@intelligence.org</em></a>)<br>
<strong>Date:</strong> Fri Jun 04 2004 - 11:55:45 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9084.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<li><strong>Previous message:</strong> <a href="9082.html">Ben Goertzel: "RE: All AGI projects are scary"</a>
<li><strong>In reply to:</strong> <a href="9073.html">Ben Goertzel: "SIAI has become slightly scary"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9084.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<li><strong>Reply:</strong> <a href="9084.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<li><strong>Reply:</strong> <a href="9085.html">Eliezer Yudkowsky: "Re: SIAI has become slightly amusing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9083">[ date ]</a>
<a href="index.html#9083">[ thread ]</a>
<a href="subject.html#9083">[ subject ]</a>
<a href="author.html#9083">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<p><em>&gt;One of my problems is what seems to be a nearly insane degree of
</em><br>
<em>&gt;self-confidence on the part of both of you.  So much self-confidence, in
</em><br>
<em>&gt;a way which leads to dismissiveness of the opinions of others, seems to
</em><br>
<em>&gt;me not to generally be correlated with good judgment.
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<p>Everyone dismisses some opinions and respects others.  Which opinions 
<br>
you dismiss and which you respect will be based on your evaluation of 
<br>
the opinion itself and the track record of the person giving the 
<br>
opinion.  Michael and Eliezer may be relatively dismissive towards many 
<br>
of your ideas, but they often explain why.  (Eliezer especially.)  If 
<br>
Eliezer dismisses your opinions so quickly, then why are there literally 
<br>
hundreds of pages in the SL4 archives from occasions he engaged you in 
<br>
earnest?  Also, it seems like your judgement of their overconfidence is 
<br>
heavily based on exchanges between *you* and them - are they equally 
<br>
dismissive towards *all* AGI designers?  Clearly they 
<br>
(probabilistically) respect the opinions of the the authors of the 
<br>
literature they read, at the very least, so it seems like there are a 
<br>
lot of people they *do* respect.
<br>
<p><em>&gt;I don't want some AI program, created by you guys or anyone else,
</em><br>
<em>&gt;imposing its inference of my &quot;volition&quot; upon me.  
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<p>The word &quot;imposing&quot; suggests something out of line with your volition.  
<br>
But the whole point of any FAI is to carry out your volition.  If the 
<br>
volition it is carrying out is unfavorable and foreign to you, then that 
<br>
would constitute a failure on the part of the programmers.  The point is 
<br>
to carry out your orders in such a way that the *intent* takes 
<br>
precedence over the *letter* of your requests.  Imagine a continuum of 
<br>
AIs, one extreme paying attention to nothing but the letter of your 
<br>
requests, the other extreme carrying your intent too far to the point 
<br>
where you disapprove.  The task of the FAI programmer is to create an 
<br>
initial dynamic that rests appropriately between these two extremes.
<br>
<p><em>&gt;When I enounced the three values of Joy, Growth and Choice in a recent
</em><br>
<em>&gt;essay, I really meant *choice* -- i.e., I meant *what I choose, now, me
</em><br>
<em>&gt;being who I am*.  I didn't mean *what I would choose if I were what I
</em><br>
<em>&gt;think I'd like to be*, which is my understanding of Eliezer's current
</em><br>
<em>&gt;notion of &quot;volition.&quot;
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<p>But we can't very well have an SI carrying out everyone's requests 
<br>
without considering the interactions between the consequences of these 
<br>
requests, right?  CollectiveVolition is a sophisticated way of doing 
<br>
that.  You can say &quot;I want the future SI derived from my seed to respect 
<br>
human choice&quot;, but that still leaves open a massive space of alternative 
<br>
AI designs, many of them UnFriendly.  A successful FAI will be a subset 
<br>
of that class, but do you know which constraints will be necessary to 
<br>
create such a FAI?  Eliezer has tossed around many ideas for such 
<br>
constraints in his writings, which will function as urgently needed 
<br>
theoretical feedstock for any technical implementation.  He isn't 
<br>
finished, but he's taken some major steps towards the goal, IMO.
<br>
<p><em>&gt;To have some AI program extrapolate from my brain what it estimates I'd
</em><br>
<em>&gt;like to be, and then modify the universe according to the choices this
</em><br>
<em>&gt;estimated Ben's-ideal-of-Ben would make (along with the estimated
</em><br>
<em>&gt;choices of others) --- this denies me the right to be human, to grow and
</em><br>
<em>&gt;change and learn.  According to my personal value system, this is not a
</em><br>
<em>&gt;good thing at all.  
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<p>Well, the idea is to not deny you the chance to grow and change and 
<br>
learn to the extent that that would bother you.
<br>
<p><em>&gt;I'm reminded of Eliezer's statement that, while he loves humanity in
</em><br>
<em>&gt;general in an altruistic way, he often feels each individual human is
</em><br>
<em>&gt;pretty worthless (&quot;would be more useful as ballast on a balloon&quot; or
</em><br>
<em>&gt;something like that, was the phrasing used).  It now seems that what
</em><br>
<em>&gt;Eliezer wants to maintain is not actual humanity, but some abstraction
</em><br>
<em>&gt;of &quot;what humanity would want if it were what it wanted to be.&quot;  
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<p>Much of present-day humanity likes to kill, harass, and torture people a 
<br>
lot.  Can you really blame him for wanting to create an initial dynamic 
<br>
that respects &quot;our wish if we knew more, thought faster, were more the 
<br>
people we wished we were, had grown up farther together; where the 
<br>
extrapolation converges rather than diverges&quot;, rather than simply 
<br>
following our volitional requests to the letter?  Again, this is a 
<br>
continuum problem, with two unfavorable extremes and a desirable 
<br>
compromise in the middle.
<br>
<p><em>&gt;Eventually this series might converge, or it might not.  Suppose the
</em><br>
<em>&gt;series doesn't converge, then which point in the iteration does the AI
</em><br>
<em>&gt;choose as &quot;Ben's volition&quot;?  Does it average over all the terms in the
</em><br>
<em>&gt;series?  Egads again.
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<p>Good question!  The ultimate answer will be for the FAI to decide, and 
<br>
we want to seed the FAI with the moral complexity necessary to make that 
<br>
decision with transhuman wisdom and compassion.  Eliezer and Co. won't 
<br>
be specifying the answer in the code.
<br>
<p><em>&gt;So what SIAI seems to be right now is: A group of people with 
</em><br>
<em>&gt;
</em><br>
<em>&gt;-- nearly-insane self-confidence
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<p>On SL4, this mostly manifests itself with respect to you.  I would also 
<br>
distinguish between overconfidence in mailing list discussions (which is 
<br>
partially theatrical, ref Eliezer's recent post on being SIAI's mad 
<br>
scientist in the basement), versus overconfidence in FAI implementation 
<br>
decisions.  Overconfidence with respect to the latter is a cardinal sin; 
<br>
with respect to the former, it is not.
<br>
<p><em>&gt;-- a dislike for sharing their more detailed ideas with the
</em><br>
<em>&gt;peanut-brained remainder of the world (presumably because *we* might do
</em><br>
<em>&gt;something dangerous with their brilliant insights?!)
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<p>Eliezer has published many hundreds of pages on his FAI theory, 
<br>
certainly more detail than I've seen from any other AGI designer.  What 
<br>
makes you think Eliezer/Michael have a &quot;dislike for sharing their more 
<br>
detailed ideas&quot;?
<br>
<p><em>&gt;Yes, I know SIAI isn't just Eliezer.  There's Tyler and Mike Anissimov.
</em><br>
<em>&gt;So far as I know, those guys aren't scary in any way.  I have plenty
</em><br>
<em>&gt;respect for both of them.  
</em><br>
<em>&gt;
</em><br>
<p>Thanks!  Unlike Eliezer, I *am* trying to be like Belldandy, sweetness 
<br>
and light.  I'm sad that Eliezer couldn't overcome his own condescending 
<br>
tendencies, but if it makes him happy to behave the way he does, then I 
<br>
think we should all respect that.  Feel free to view me as a younger 
<br>
Eliezer in the Everett branch where he retained his commitment to 
<br>
extreme niceness.
<br>
<p><pre>
-- 
Michael Anissimov                           <a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
Advocacy Director, Singularity Institute for Artificial Intelligence
--
Subscribe to our free eBulletin for research and community news:
<a href="http://www.intelligence.org/news/subscribe.html">http://www.intelligence.org/news/subscribe.html</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9084.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<li><strong>Previous message:</strong> <a href="9082.html">Ben Goertzel: "RE: All AGI projects are scary"</a>
<li><strong>In reply to:</strong> <a href="9073.html">Ben Goertzel: "SIAI has become slightly scary"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9084.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<li><strong>Reply:</strong> <a href="9084.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<li><strong>Reply:</strong> <a href="9085.html">Eliezer Yudkowsky: "Re: SIAI has become slightly amusing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9083">[ date ]</a>
<a href="index.html#9083">[ thread ]</a>
<a href="subject.html#9083">[ subject ]</a>
<a href="author.html#9083">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
