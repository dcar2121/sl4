<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Summary of current FAI thought</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Summary of current FAI thought">
<meta name="Date" content="2004-06-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Summary of current FAI thought</h1>
<!-- received="Fri Jun  4 23:03:52 2004" -->
<!-- isoreceived="20040605050352" -->
<!-- sent="Fri, 4 Jun 2004 22:03:48 -0700" -->
<!-- isosent="20040605050348" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Summary of current FAI thought" -->
<!-- id="BA731979-B6AD-11D8-86D4-000A95B1AFDE@objectent.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="40BCE60E.9050302@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Summary%20of%20current%20FAI%20thought"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Fri Jun 04 2004 - 23:03:48 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9119.html">Samantha Atkins: "Re: FAI: Collective Volition"</a>
<li><strong>Previous message:</strong> <a href="9117.html">J. Andrew Rogers: "Re: SIAI will self-destruct"</a>
<li><strong>In reply to:</strong> <a href="8916.html">Eliezer Yudkowsky: "Re: Summary of current FAI thought"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9121.html">Robin Lee Powell: "Re: Summary of current FAI thought"</a>
<li><strong>Reply:</strong> <a href="9121.html">Robin Lee Powell: "Re: Summary of current FAI thought"</a>
<li><strong>Reply:</strong> <a href="9132.html">Eliezer Yudkowsky: "Re: Summary of current FAI thought"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9118">[ date ]</a>
<a href="index.html#9118">[ thread ]</a>
<a href="subject.html#9118">[ subject ]</a>
<a href="author.html#9118">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Jun 1, 2004, at 1:24 PM, Eliezer Yudkowsky wrote:
<br>
<p><em>&gt;&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; For our purposes (pragmatics, not AI theory) FAI is a special case of 
</em><br>
<em>&gt; seed AGI.  Seed AGI without having solved the Friendliness problem 
</em><br>
<em>&gt; seems to me a huge risk, i.e., seed AGI is the thing most likely to 
</em><br>
<em>&gt; kill off humanity if FAI doesn't come first.  If a non-F seed AGI goes 
</em><br>
<em>&gt; foom, that's it, game over.
</em><br>
<p>I have heard you say this many times.   However, it is not certain that 
<br>
a non-F seed AGI going &quot;foom&quot; would kill off humanity.   At least it 
<br>
isn't to me.
<br>
<p><em>&gt;   I think we should be building in takeoff safeguards into essentially 
</em><br>
<em>&gt; everything &quot;AGI&quot; or &quot;seed&quot;, because it's too dangerous to guess when 
</em><br>
<em>&gt; safeguards start becoming necessary.  I can't do the math to calculate 
</em><br>
<em>&gt; critical mass, and there are too many surprises when one can't do the 
</em><br>
<em>&gt; math.
</em><br>
<p>You are attempting to rigorously shape something more complex than any 
<br>
evolved life-form and capable of self-change at a much greater rate 
<br>
with little real limits.   I can't help feeling like you are attempting 
<br>
the impossible.   Compared to such a task totally understanding and 
<br>
predicting even the most complicated software systems currently in 
<br>
existence is simple.   Honestly, I don't believe it is doable.
<br>
<p><em>&gt;
</em><br>
<em>&gt;&gt; 2) You propose building a non-AGI, non-sentient system with the goal 
</em><br>
<em>&gt;&gt; and
</em><br>
<em>&gt;&gt; ability to:
</em><br>
<em>&gt;
</em><br>
<em>&gt; A Really Powerful Optimization Process is an AGI but non-sentient, if 
</em><br>
<em>&gt; I can figure out how to guarantee nonsentience for the hypotheses it 
</em><br>
<em>&gt; develops to model sentient beings in external reality.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;    a) extract a coherent view from humanity of what humans would want 
</em><br>
<em>&gt;&gt; if
</em><br>
<em>&gt;&gt; they were more 'grown up' - ie. were more rational, used more/better
</em><br>
<em>&gt;&gt; knowledge in their decisions, had overcome many of their prejudices 
</em><br>
<em>&gt;&gt; and
</em><br>
<em>&gt;&gt; emotional limitations.
</em><br>
<em>&gt;
</em><br>
<em>&gt; ...more or less.
</em><br>
<em>&gt;
</em><br>
<p>To do this implies the FRPOP needs to model/manipulate/understand 
<br>
humans to a depth beyond the understanding of perhaps any human ever.   
<br>
And do this preferably without even being sentient?   This doesn't look 
<br>
at all workable.
<br>
<p>Presumably it only has current knowledge of the subjects involved and 
<br>
this without full sentient being referents.   The knowledge to date and 
<br>
any likely extrapolation of such knowledge in say the next decade is 
<br>
likely to not be up to the task.  Extrapolations from the FRPOP without 
<br>
it understanding anything about qualia or knowing sentience itself are 
<br>
likely to be extremely dangerous to human well-being.
<br>
<p><em>&gt;&gt;    b) recurse this analysis, assuming that these human desires had 
</em><br>
<em>&gt;&gt; largely
</em><br>
<em>&gt;&gt; been achieved.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Recursing the analysis gets you a longer-distance extrapolation, but 
</em><br>
<em>&gt; presumably more grownup people.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;    c) Continue to improve this process until these computed human 
</em><br>
<em>&gt;&gt; wants
</em><br>
<em>&gt;&gt; converge/ cohere 'sufficiently'
</em><br>
<em>&gt;
</em><br>
<em>&gt; Or fail to converge, in which I'd have to look for some other 
</em><br>
<em>&gt; reasonably nice solution (or more likely move to a backup plan that 
</em><br>
<em>&gt; had already been developed).
</em><br>
<em>&gt;
</em><br>
<p>You will certainly need that backup plan.  Please describe it when you 
<br>
get the chance.
<br>
<p><em>&gt;&gt; -- only then implement strategies that
</em><br>
<em>&gt;&gt; ensure that these wishes are not thwarted by natural events or human 
</em><br>
<em>&gt;&gt; action.
</em><br>
<em>&gt;
</em><br>
<em>&gt; That's phrased too negatively; the volition could render first aid, 
</em><br>
<em>&gt; not only protect - do whatever the coherent wishes said was worth 
</em><br>
<em>&gt; doing, bearing in mind that the coherent wish may be to limit the work 
</em><br>
<em>&gt; done by the collective volition.
</em><br>
<em>&gt;
</em><br>
<p>But wouldn't the FRPOP only honor a coherent wish that allowed it to 
<br>
fulfill its primary directive of protecting/serving humans?   If the 
<br>
collective coherent wish was for the FRPOP to bug out, would it do so?
<br>
<p><p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9119.html">Samantha Atkins: "Re: FAI: Collective Volition"</a>
<li><strong>Previous message:</strong> <a href="9117.html">J. Andrew Rogers: "Re: SIAI will self-destruct"</a>
<li><strong>In reply to:</strong> <a href="8916.html">Eliezer Yudkowsky: "Re: Summary of current FAI thought"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9121.html">Robin Lee Powell: "Re: Summary of current FAI thought"</a>
<li><strong>Reply:</strong> <a href="9121.html">Robin Lee Powell: "Re: Summary of current FAI thought"</a>
<li><strong>Reply:</strong> <a href="9132.html">Eliezer Yudkowsky: "Re: Summary of current FAI thought"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9118">[ date ]</a>
<a href="index.html#9118">[ thread ]</a>
<a href="subject.html#9118">[ subject ]</a>
<a href="author.html#9118">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
