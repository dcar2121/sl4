<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Collective Volition: Wanting vs Doing.</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Collective Volition: Wanting vs Doing.">
<meta name="Date" content="2004-06-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Collective Volition: Wanting vs Doing.</h1>
<!-- received="Sun Jun 13 08:56:49 2004" -->
<!-- isoreceived="20040613145649" -->
<!-- sent="Sun, 13 Jun 2004 10:56:48 -0400" -->
<!-- isosent="20040613145648" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Collective Volition: Wanting vs Doing." -->
<!-- id="40CC6B30.7000201@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="2BCBB02B-BD08-11D8-85E5-000A95B1AFDE@objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Collective%20Volition:%20Wanting%20vs%20Doing."><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jun 13 2004 - 08:56:48 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9181.html">Ben Goertzel: "RE: About &quot;safe&quot; AGI architecture"</a>
<li><strong>Previous message:</strong> <a href="9179.html">Samantha Atkins: "Re: About &quot;safe&quot; AGI architecture"</a>
<li><strong>In reply to:</strong> <a href="9178.html">Samantha Atkins: "Re: Collective Volition: Wanting vs Doing."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9182.html">Philip Sutton: "Re: Where does friendliness come from before Collective Volition"</a>
<li><strong>Reply:</strong> <a href="9182.html">Philip Sutton: "Re: Where does friendliness come from before Collective Volition"</a>
<li><strong>Reply:</strong> <a href="9195.html">Samantha Atkins: "Re: Collective Volition: Wanting vs Doing."</a>
<li><strong>Reply:</strong> <a href="9204.html">Mike: "RE: Collective Volition: Wanting vs Doing."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9180">[ date ]</a>
<a href="index.html#9180">[ thread ]</a>
<a href="subject.html#9180">[ subject ]</a>
<a href="author.html#9180">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Samantha Atkins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; On Jun 12, 2004, at 6:57 PM, Eliezer Yudkowsky wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; This question does appear to keep popping up.  Roughly, a collective 
</em><br>
<em>&gt;&gt; volition is what I get when:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; (a) I step back and ask the meta-question of how I decided an earlier 
</em><br>
<em>&gt;&gt; Eliezer's view of &quot;Friendliness&quot; was &quot;mistaken&quot;.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; (b) I apply the same meta-question to everyone else on the planet.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Whatever it is that you use, mentally, to consider any alternative to 
</em><br>
<em>&gt;&gt; collective volition, anything that would be of itself friendlier - 
</em><br>
<em>&gt;&gt; that's you, a human, making the decision; so now imagine that we take 
</em><br>
<em>&gt;&gt; you and extrapolate you re-making that decision at a higher level of 
</em><br>
<em>&gt;&gt; intelligence, knew more, thought faster, more the person etc.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes, I get that and it is enticing.  But precisely how will the FRPOP 
</em><br>
<em>&gt; gets its bearings as to what is the &quot;direction&quot; of &quot;more the person&quot;?  
</em><br>
<em>&gt; Some of the others are a bit problematic too.  But this one seems the 
</em><br>
<em>&gt; best and central trick.  More the person I would like to be?  I, with 
</em><br>
<em>&gt; all my warts?  Wouldn't I have a perhaps badly warped view of what kind 
</em><br>
<em>&gt; of person I would like to be?  Would the person I would like to be make 
</em><br>
<em>&gt; indeed better choices?   How will the AI know of this person or model 
</em><br>
<em>&gt; this person?
</em><br>
<p>Samantha, you write that you might have a badly warped view of what kind of 
<br>
person you would like to be.  &quot;Badly warped&quot; by what criterion that I feed 
<br>
to the FAI?  Your criterion?  Someone else's?  Where am I supposed to get 
<br>
this information, if not, somehow, from you?  When you write down exactly 
<br>
how the information is supposed to get from point A (you) to point B (the 
<br>
FAI), and what the FAI does with the information once it's there, you'll 
<br>
have something that looks like - surprise! - a volition-extrapolating 
<br>
dynamic.  It's not a coincidence.  That's where the idea of a 
<br>
volition-extrapolating dynamic *originally comes from*.
<br>
<p><em>&gt;&gt; The benefit of CV is that (a) we aren't stuck with your decision about 
</em><br>
<em>&gt;&gt; Friendliness forever (b) you don't have to make the decision using 
</em><br>
<em>&gt;&gt; human-level intelligence.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, we don't make the decision at all it seems to me.  The AI does 
</em><br>
<em>&gt; based on its extrapolation of our idealized selves.
</em><br>
<p>Not precisely, but it's closer than saying that we would make the decision 
<br>
using (EEK!) human-level intelligence.
<br>
<p><em>&gt; I am not sure 
</em><br>
<em>&gt; exactly what our inputs would be.  What do you have in mind?
</em><br>
<p>Our inputs would be our current selves, from which the decision of a future 
<br>
humankind might be predicted.
<br>
<p><em>&gt;&gt; It's easy to see that all those other darned humans can't be trusted, 
</em><br>
<em>&gt;&gt; but what if we can't trust ourselves either?  If you can employ an 
</em><br>
<em>&gt;&gt; extrapolation powerful enough to leap out of your own fundamental 
</em><br>
<em>&gt;&gt; errors, you should be able to employ it on all those other darned 
</em><br>
<em>&gt;&gt; humans too.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well yes, but it is an &quot;if&quot; isn't it?  
</em><br>
<p>Yep.  Big if.
<br>
<p><em>&gt; It is actually a fairly old 
</em><br>
<em>&gt; spiritual exercise to invoke one's most idealized self and listen to its 
</em><br>
<em>&gt; advise or let it decide.   It takes many forms, some of which put the 
</em><br>
<em>&gt; idealized self as other than one's self, but the gist is not that 
</em><br>
<em>&gt; different.
</em><br>
<p>In the spiritual exercise your idealized self will never be any smarter 
<br>
than you are, never know anything you don't.  It will say things that sound 
<br>
wise to your current self - things a village elder might say - things you 
<br>
might severely disagree with if you did know more, think smarter.  Can you 
<br>
ask your idealized spiritual self to build a nanobot?  I don't trust the 
<br>
notion of *spiritual* extrapolation for grounding; I think that's the wrong 
<br>
direction.  The word &quot;spirituality&quot; makes people people go warm and fuzzy, 
<br>
and yes we need the warm fuzzies, but I think that if we took people and 
<br>
filtered out everything that a member of the Fluffy Bunny Coven would call 
<br>
&quot;unspiritual&quot;, we'd end up with unhumans.
<br>
<p>(<a href="http://tftb.com/deify/fluffybunny.htm">http://tftb.com/deify/fluffybunny.htm</a>)
<br>
<p>It's more an order-of-evaluation question than anything else.  I currently 
<br>
guess that one needs to evaluate some &quot;knew more&quot; and &quot;thought faster&quot; 
<br>
before evaluating &quot;more the people we wished we were&quot;.  Mostly because 
<br>
&quot;knew more&quot; and &quot;thought faster&quot; starting from a modern-day human who makes 
<br>
fluffy bunny errors doesn't have quite the same opportunity to go 
<br>
open-endedly recursively wrong as &quot;more the people we wished we were&quot; 
<br>
evaluated on a FBer.
<br>
<p>One obvious rule for order-of-evaluation would be to define a metric of 
<br>
distance (difficulty of explanation to current self) and carry out 
<br>
shorter-distance extrapolations before longer-distance extrapolations.
<br>
<p><em>&gt;&gt; Maybe a better metaphor for collective volition would be that it 
</em><br>
<em>&gt;&gt; refers questions to an extrapolated adult humankind, or to a 
</em><br>
<em>&gt;&gt; superposition of the adult humanities we might become.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So the AI becomes an adjunct and amplifier of a specialized form of 
</em><br>
<em>&gt; introspective spiritual exercise?  Wild!  AI augmented self-improvement 
</em><br>
<em>&gt; of humankind.
</em><br>
<p>Right.  AI augmented self-improvement of humankind with the explicit 
<br>
notation that the chicken-and-egg part of this problem is that modern-day 
<br>
humans aren't smart enough to self-improve without stomping all over their 
<br>
own minds with unintended consequences, aren't even smart enough to 
<br>
evaluate the question &quot;What kind of person do you want to be?&quot; over its 
<br>
real experiential consequences rather than a small subset of human verbal 
<br>
descriptions of humanly expected consequences.  So rather than creating a 
<br>
*separate* self-improving humane thing, one does something philosophically 
<br>
more complex and profound (but perhaps not more difficult from the 
<br>
standpoint of FAI theory, although it *sounds* a lot harder).  One binds a 
<br>
transparent optimization process to predict what the grownup selves of 
<br>
modern-day humans would say if modern humans grew up together with the 
<br>
ability to self-improve knowing the consequences.  The decision function of 
<br>
the extrapolated adult humanity includes the ability of the collective 
<br>
volition to restrain its own power or rewrite the optimization function to 
<br>
something else; the collective volition extrapolates its awareness that it 
<br>
is just an extrapolation and not our actual decisions.
<br>
<p>In other words, one handles *only* and *exactly* the chicken-and-egg part 
<br>
of the problem - that modern-day humans aren't smart enough to self-improve 
<br>
to an adult humanity, and that modern-day society isn't smart enough to 
<br>
render emergency first aid to itself - by writing an AI that extrapolates 
<br>
over *exactly those* gaps to arrive at a picture of future humankind if 
<br>
those problems were solved.  Then the extrapolated superposed possible 
<br>
future humankinds, the collective volition, hopefully decides to act in our 
<br>
time to boost us over the chicken-and-egg recursion; doing enough to solve 
<br>
the hard part of the problem, but not annoying us or taking over our lives, 
<br>
since that's not what we want (I think; at least it's not what I want).  Or 
<br>
maybe the collective volition does something else.  I may have phrased the 
<br>
problem wrong.  But for I as an FAI programmer to employ some other 
<br>
solution, such as creating a new species of humane intelligence, would be 
<br>
inelegant; it doesn't solve exactly and only the difficult part of the problem.
<br>
<p>I may end up needing to be inelegant, but first I want to try really hard 
<br>
to find a way to do the Right Thing.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9181.html">Ben Goertzel: "RE: About &quot;safe&quot; AGI architecture"</a>
<li><strong>Previous message:</strong> <a href="9179.html">Samantha Atkins: "Re: About &quot;safe&quot; AGI architecture"</a>
<li><strong>In reply to:</strong> <a href="9178.html">Samantha Atkins: "Re: Collective Volition: Wanting vs Doing."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9182.html">Philip Sutton: "Re: Where does friendliness come from before Collective Volition"</a>
<li><strong>Reply:</strong> <a href="9182.html">Philip Sutton: "Re: Where does friendliness come from before Collective Volition"</a>
<li><strong>Reply:</strong> <a href="9195.html">Samantha Atkins: "Re: Collective Volition: Wanting vs Doing."</a>
<li><strong>Reply:</strong> <a href="9204.html">Mike: "RE: Collective Volition: Wanting vs Doing."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9180">[ date ]</a>
<a href="index.html#9180">[ thread ]</a>
<a href="subject.html#9180">[ subject ]</a>
<a href="author.html#9180">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
