<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: FAI: Collective Volition</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: FAI: Collective Volition">
<meta name="Date" content="2004-06-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: FAI: Collective Volition</h1>
<!-- received="Wed Jun  2 07:25:41 2004" -->
<!-- isoreceived="20040602132541" -->
<!-- sent="Wed, 02 Jun 2004 09:25:38 -0400" -->
<!-- isosent="20040602132538" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: FAI: Collective Volition" -->
<!-- id="40BDD552.5050202@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="009d01c4489d$43137330$6401a8c0@ZOMBIETHUSTRA" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20FAI:%20Collective%20Volition"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Jun 02 2004 - 07:25:38 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8938.html">Ben Goertzel: "RE: Sentient vs non-sentient super general AI"</a>
<li><strong>Previous message:</strong> <a href="8936.html">Philip Sutton: "RE: Sentient vs non-sentient super general AI"</a>
<li><strong>In reply to:</strong> <a href="8932.html">Ben Goertzel: "RE: FAI: Collective Volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8943.html">Ben Goertzel: "Sentience"</a>
<li><strong>Reply:</strong> <a href="8943.html">Ben Goertzel: "Sentience"</a>
<li><strong>Reply:</strong> <a href="8945.html">Ben Goertzel: "RE: FAI: Collective Volition"</a>
<li><strong>Reply:</strong> <a href="9026.html">fudley: "Sentiencee  [Was  FAI: Collective Volition]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8937">[ date ]</a>
<a href="index.html#8937">[ thread ]</a>
<a href="subject.html#8937">[ subject ]</a>
<a href="author.html#8937">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<p><em>&gt; Eliezer,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; About this idea of creating a non-sentient optimization process, to 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; A) predict possible futures for the universe
</em><br>
<em>&gt; B) analyze the global human psyche and figure out the &quot;collective
</em><br>
<em>&gt; volition&quot; of humanity
</em><br>
<em>&gt; 
</em><br>
<em>&gt; instead of creating a superhuman mind....
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I can't say it's impossible that this would work.   It goes against my
</em><br>
<em>&gt; scientific intuition, which says that sentience of some sort would
</em><br>
<em>&gt; almost surely be needed to achieve these things, but my scientific
</em><br>
<em>&gt; intuition could be wrong.  Also, my notion of &quot;sentience of some sort&quot;
</em><br>
<em>&gt; may grow and become more flexible as more AGI and AGI-ish systems become
</em><br>
<em>&gt; available for interaction!
</em><br>
<p>I have not the vaguest idea of what you mean by &quot;sentience&quot;.  I am still 
<br>
proposing reflectivity - superhuman reflectivity, in fact.  Why are you so 
<br>
horrified at my proposing to omit something if you do not know what you 
<br>
mean by the term, let alone what I mean by it?
<br>
<p><em>&gt; However, it does seem to me that either problem A or B above is
</em><br>
<em>&gt; significantly more difficult than creating a self-modifying AGI system.
</em><br>
<em>&gt; Again, I could be wrong on this, but ... Sheesh.  
</em><br>
<p>Yes, saving the world is significantly more difficult than blowing it up. 
<br>
I would rise to the challenge, raise the level of my game sufficiently to 
<br>
change the default destiny of a seed AI researcher, rather than walking 
<br>
into the whirling razor blades of which I was once ignorant.  I understand 
<br>
if you decide that the challenge is beyond you, for yes, it is difficult. 
<br>
But it is harder to understand why you are still on the playing field, 
<br>
endangering yourself and others.
<br>
<p><em>&gt; To create a self-modifying AGI system, at very worst one has to
</em><br>
<em>&gt; understand the way the human brain works, and then emulate something
</em><br>
<em>&gt; like it in a more mutably self-modifiable medium such as computer
</em><br>
<em>&gt; software.  This is NOT the approach I'm taking with Novamente; I'm just
</em><br>
<em>&gt; pointing it out to place a bound on the difficulty of creating a
</em><br>
<em>&gt; self-modifying AGI system.  The biggest &quot;in principle&quot; obstacle here is
</em><br>
<em>&gt; that it could conceivably require insanely much computational power --
</em><br>
<em>&gt; or quantum computing, quantum gravity computing, etc. -- to get AGI to
</em><br>
<em>&gt; work at the human level (for example, if the microtubule hypothesis is
</em><br>
<em>&gt; right).  Even so, then we just have the engineering problem of creating
</em><br>
<em>&gt; a more mutable substrate than human brain tissue, and reimplementing
</em><br>
<em>&gt; human brain algorithms within it.  
</em><br>
<em>&gt; 
</em><br>
<em>&gt; On the other hand, the task of creating a non-sentient optimization
</em><br>
<em>&gt; process of the sort you describe is a lot more nebulous (due to the lack
</em><br>
<em>&gt; of even partially relevant examples to work from).  Yeah, in principle
</em><br>
<em>&gt; it's easy to create optimization processes of arbitrary power -- so long
</em><br>
<em>&gt; as one isn't concerned about memory or processor usage.  But
</em><br>
<em>&gt; contemporary science tells us basically NOTHING about how to make
</em><br>
<em>&gt; uber-optimization-processes like the one you're envisioning.  The ONLY
</em><br>
<em>&gt; guidance it gives us in this direction, pertains to &quot;how to build a
</em><br>
<em>&gt; sentience that can act as a very powerful optimization process.&quot;
</em><br>
<p>Again, I've got no clue what you mean by 'non-sentient'.  I was still 
<br>
planning on using recursive self-improvement, self-modification, total 
<br>
self-access or &quot;autopotence&quot; in Nick Bostrom's phrase, full reflectivity, 
<br>
et cetera.
<br>
<p><em>&gt; So, it seems to me that you're banking on creating a whole new branch of
</em><br>
<em>&gt; science basically from nothing,
</em><br>
<p>Not from nothing.  Plenty of precedents, even if they are not widely known.
<br>
<p><em>&gt; whereas to create AGI one MAY not need
</em><br>
<em>&gt; to do that, one may only need to &quot;fix&quot; the existing sciences of
</em><br>
<em>&gt; &quot;cognitive science&quot; and AI.  
</em><br>
<p>Does that mean that you'll create something without understanding how it 
<br>
works?  Whirling razor blades, here we come.
<br>
<p><em>&gt; It seems to me that, even if what you're suggesting is possible (which I
</em><br>
<em>&gt; really doubt), you're almost certain to be beaten in the race by someone
</em><br>
<em>&gt; working to build a sentient AGI.
</em><br>
<p>By &quot;sentient AGI&quot; you mean recursive self-improvement thrown together at 
<br>
random, which of course will be sentient whatever that means, because 
<br>
humans are sentient therefore so must be an AGI?  Or is sentience just this 
<br>
holy and mysterious power that you don't know how it works, but you think 
<br>
it is important, so I'm committing blasphemy by suggesting that I not 
<br>
include it, whatever it is?
<br>
<p>Seriously, I don't see how anyone can make this huge fuss over sentience in 
<br>
AGI when you don't know how it works and you can't give me a walkthrough of 
<br>
how it produces useful outputs.  I have a few small ideas about 
<br>
half-understood architectural quirks that give humans the belief they are 
<br>
conscious, architectural quirks to which I applied the term &quot;sentience&quot;. 
<br>
Evidently this was a huge mistake.  I hereby announce my intent to build 
<br>
non-floogly AI.
<br>
<p><em>&gt; Therefore, to succeed with this new plan, you'll probably need to create
</em><br>
<em>&gt; some kind of fascist state in which working on AGI is illegal and
</em><br>
<em>&gt; punishable by death, imprisonment or lobotomy.
</em><br>
<p>Maybe that's what you'd do in my shoes.  My brilliant new notion is to 
<br>
understand what I am doing, rather than randomly guessing, and see if that 
<br>
lets me finish my work before the meddling dabblers blow up the world by 
<br>
accident.  Though in your case it does begin to border on being on purpose.
<br>
<p><p>&quot;Sentient&quot;, &quot;non-sentient&quot;, this is phlogiston, Greek philosophy.  You 
<br>
cannot argue about something you do not understand, mystical substances and 
<br>
mystical properties of mystical systems.  I intend to unravel the sacred 
<br>
mystery of flooginess, which I have already come far enough to declare as a 
<br>
non-mysterious target.  Then, having unravelled it, I will either know how 
<br>
to build a non-floogly optimization process, or I will know in detail why 
<br>
floogling is necessary.  I am only announcing my moral preference in favor 
<br>
of non-floogly optimization processes.  I won't definitely say it's 
<br>
possible or impossible until I know in more detail where the present 
<br>
confusion comes from, and I certainly don't intend to make up campfire 
<br>
stories about the mystical powers of floogly systems until I can do a 
<br>
walkthrough of how they work.  Do a walkthrough, not tell stories about 
<br>
them.  As far as I can guess, the whole thing will turn out to be a 
<br>
confusion, probably a very interesting confusion, but a confusion nonetheless.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8938.html">Ben Goertzel: "RE: Sentient vs non-sentient super general AI"</a>
<li><strong>Previous message:</strong> <a href="8936.html">Philip Sutton: "RE: Sentient vs non-sentient super general AI"</a>
<li><strong>In reply to:</strong> <a href="8932.html">Ben Goertzel: "RE: FAI: Collective Volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8943.html">Ben Goertzel: "Sentience"</a>
<li><strong>Reply:</strong> <a href="8943.html">Ben Goertzel: "Sentience"</a>
<li><strong>Reply:</strong> <a href="8945.html">Ben Goertzel: "RE: FAI: Collective Volition"</a>
<li><strong>Reply:</strong> <a href="9026.html">fudley: "Sentiencee  [Was  FAI: Collective Volition]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8937">[ date ]</a>
<a href="index.html#8937">[ thread ]</a>
<a href="subject.html#8937">[ subject ]</a>
<a href="author.html#8937">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
