<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: SIAI has become slightly scary</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="SIAI has become slightly scary">
<meta name="Date" content="2004-06-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>SIAI has become slightly scary</h1>
<!-- received="Fri Jun  4 08:31:57 2004" -->
<!-- isoreceived="20040604143157" -->
<!-- sent="Fri, 4 Jun 2004 10:31:44 -0400" -->
<!-- isosent="20040604143144" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="SIAI has become slightly scary" -->
<!-- id="009f01c44a40$a900c9a0$6401a8c0@ZOMBIETHUSTRA" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="SEA2-DAV71oJkAIH0Bj00029fcd@hotmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=Re:%20SIAI%20has%20become%20slightly%20scary"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Fri Jun 04 2004 - 08:31:44 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9074.html">Jef Allbright: "Re: [extropy-chat] Eugen Leitl on AI design"</a>
<li><strong>Previous message:</strong> <a href="9072.html">Thomson Comer: "Re: ESSAY: 'Debunking Hippy-Dippy moral philosophy'"</a>
<li><strong>In reply to:</strong> <a href="9070.html">Slawomir Paliwoda: "Re: Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9076.html">Eliezer Yudkowsky: "Re: SIAI has become slightly scary"</a>
<li><strong>Reply:</strong> <a href="9076.html">Eliezer Yudkowsky: "Re: SIAI has become slightly scary"</a>
<li><strong>Reply:</strong> <a href="9083.html">Michael Anissimov: "Re: SIAI has become slightly amusing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9073">[ date ]</a>
<a href="index.html#9073">[ thread ]</a>
<a href="subject.html#9073">[ subject ]</a>
<a href="author.html#9073">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer and Michael Wilson,
<br>
<p>Quite frankly, the two of you are starting to scare me a little bit.
<br>
<p>Just a *little* bit, because I consider it pretty unlikely that you guys
<br>
will be able to create anything near a detailed enough AGI theory to go
<br>
about really building a successful AGI program (or large-scale
<br>
uber-optimization-process, or whatever you want to call it).
<br>
<p>But in the, say, 2% likely scenario that these guys really will be able
<br>
to create the right kind of theory ... I'm pretty scared by the outcome.
<br>
<p>One of my problems is what seems to be a nearly insane degree of
<br>
self-confidence on the part of both of you.  So much self-confidence, in
<br>
a way which leads to dismissiveness of the opinions of others, seems to
<br>
me not to generally be correlated with good judgment.
<br>
<p>[I'm not one to dismiss &quot;insanity&quot; generally -- boring and narrowminded
<br>
people have called me &quot;insane&quot; many times, and I have a lot of respect
<br>
for the creative power of socially marginal or unacceptable frames of
<br>
mind.  But some types of &quot;insanity&quot; are scarier than others.  Note that
<br>
I'm not accusing you guys of actually being clinically whacko -- I know
<br>
Eli well enough to know that's not true of him, anyway ;-)  However, the
<br>
degree of self-confidence displayed by both of you lately seems to me to
<br>
come disturbingly close to the &quot;delusions of grandeur&quot; level.  Comparing
<br>
SIAI's theory of FAI to General Relativity Theory?  Calling everyone
<br>
who's not part of the SIAI club a &quot;meddler and dabbler&quot; who's part of
<br>
the &quot;peanut gallery&quot; and just needs to be &quot;shown the path&quot;.  Egads!!
<br>
Yeah, it's all in good fun, it's humorous, etc.  But it's not too hard
<br>
to see through the humor to the actual attitudes, in this case.]
<br>
<p>Another problem I have is this notion of &quot;volition&quot; (related to
<br>
Eliezer's notion of &quot;humane&quot;-ness).  This line of thinking is intriguing
<br>
philosophically, but scary pragmatically.  
<br>
<p>I don't want some AI program, created by you guys or anyone else,
<br>
imposing its inference of my &quot;volition&quot; upon me.  
<br>
<p>When I enounced the three values of Joy, Growth and Choice in a recent
<br>
essay, I really meant *choice* -- i.e., I meant *what I choose, now, me
<br>
being who I am*.  I didn't mean *what I would choose if I were what I
<br>
think I'd like to be*, which is my understanding of Eliezer's current
<br>
notion of &quot;volition.&quot;
<br>
<p>To have some AI program extrapolate from my brain what it estimates I'd
<br>
like to be, and then modify the universe according to the choices this
<br>
estimated Ben's-ideal-of-Ben would make (along with the estimated
<br>
choices of others) --- this denies me the right to be human, to grow and
<br>
change and learn.  According to my personal value system, this is not a
<br>
good thing at all.  
<br>
&nbsp;&nbsp;
<br>
I'm reminded of Eliezer's statement that, while he loves humanity in
<br>
general in an altruistic way, he often feels each individual human is
<br>
pretty worthless (&quot;would be more useful as ballast on a balloon&quot; or
<br>
something like that, was the phrasing used).  It now seems that what
<br>
Eliezer wants to maintain is not actual humanity, but some abstraction
<br>
of &quot;what humanity would want if it were what it wanted to be.&quot;  
<br>
<p>As a side point, this notion of &quot;what humanity wants to be&quot; or &quot;what Ben
<br>
wants to be&quot; is a confusing and conflicted one, in itself.  Eli has
<br>
realized this, but chooses not to focus on it, as he believes he'll
<br>
resolve the problems in future; I'm not so sure.  As Eli pointed out
<br>
somewhere, there's a iteration:
<br>
<p>0. What Ben is
<br>
1. What Ben wants to be
<br>
2. What Ben would want to be, once he had become what he wants to be
<br>
3.  Etc. ...
<br>
...
<br>
<p>Eventually this series might converge, or it might not.  Suppose the
<br>
series doesn't converge, then which point in the iteration does the AI
<br>
choose as &quot;Ben's volition&quot;?  Does it average over all the terms in the
<br>
series?  Egads again.
<br>
<p>So what SIAI seems to be right now is: A group of people with 
<br>
<p>-- nearly-insane self-confidence
<br>
-- a dislike for sharing their more detailed ideas with the
<br>
peanut-brained remainder of the world (presumably because *we* might do
<br>
something dangerous with their brilliant insights?!)
<br>
-- a desire to give us humans, not what we want, but what their AI
<br>
program estimates we would want if we were what we wanted to be
<br>
<p>The only way this isn't extremely scary is if SIAI lacks the ideas
<br>
required to create this volition-implementing AI program.  Fortunately,
<br>
I think it's very likely that they DO lack the ideas required.
<br>
<p>Yes, I know SIAI isn't just Eliezer.  There's Tyler and Mike Anissimov.
<br>
So far as I know, those guys aren't scary in any way.  I have plenty
<br>
respect for both of them.  But what distinguishes SIAI from just being a
<br>
Singularity-and-AGI advocacy group is Eliezer's particular vision of how
<br>
to use AI to bring the Singularity about.  So I think it's fair to judge
<br>
SIAI by Eliezer's vision, though not by his personality quirks.
<br>
<p>On the bright side, I've noticed that Eliezer changes his views every
<br>
year or so, as his understanding of the relevant issues deepens.  I'm
<br>
hoping that the next shift is in a less disturbing direction!
<br>
<p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9074.html">Jef Allbright: "Re: [extropy-chat] Eugen Leitl on AI design"</a>
<li><strong>Previous message:</strong> <a href="9072.html">Thomson Comer: "Re: ESSAY: 'Debunking Hippy-Dippy moral philosophy'"</a>
<li><strong>In reply to:</strong> <a href="9070.html">Slawomir Paliwoda: "Re: Volitional Morality and Action Judgement"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9076.html">Eliezer Yudkowsky: "Re: SIAI has become slightly scary"</a>
<li><strong>Reply:</strong> <a href="9076.html">Eliezer Yudkowsky: "Re: SIAI has become slightly scary"</a>
<li><strong>Reply:</strong> <a href="9083.html">Michael Anissimov: "Re: SIAI has become slightly amusing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9073">[ date ]</a>
<a href="index.html#9073">[ thread ]</a>
<a href="subject.html#9073">[ subject ]</a>
<a href="author.html#9073">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
