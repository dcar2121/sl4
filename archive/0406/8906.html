<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: FAI: Collective Volition</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: FAI: Collective Volition">
<meta name="Date" content="2004-06-01">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: FAI: Collective Volition</h1>
<!-- received="Tue Jun  1 09:05:51 2004" -->
<!-- isoreceived="20040601150551" -->
<!-- sent="Tue, 01 Jun 2004 11:05:47 -0400" -->
<!-- isosent="20040601150547" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: FAI: Collective Volition" -->
<!-- id="40BC9B4B.8030700@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="E1BUvUZ-00024K-00@ag24.gen.cam.ac.uk" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20FAI:%20Collective%20Volition"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Jun 01 2004 - 09:05:47 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8907.html">fudley: "Re: One or more FAIs??"</a>
<li><strong>Previous message:</strong> <a href="8905.html">Eliezer Yudkowsky: "Re: FAI: Collective Volition"</a>
<li><strong>In reply to:</strong> <a href="../0405/8895.html">Aubrey de Grey: "Re: FAI: Collective Volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8909.html">Eliezer Yudkowsky: "Re: FAI: Collective Volition"</a>
<li><strong>Reply:</strong> <a href="8909.html">Eliezer Yudkowsky: "Re: FAI: Collective Volition"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8906">[ date ]</a>
<a href="index.html#8906">[ thread ]</a>
<a href="subject.html#8906">[ subject ]</a>
<a href="author.html#8906">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Aubrey de Grey wrote:
<br>
<p><em>&gt; Eliezer Yudkowsky wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; People live with quite complex background rules already, such as &quot;You
</em><br>
<em>&gt;&gt; must spend most of your hours on boring, soul-draining labor just to
</em><br>
<em>&gt;&gt; make enough money to get by&quot; and &quot;As time goes on you will slowly age,
</em><br>
<em>&gt;&gt; lose neurons, and die&quot; and &quot;You need to fill out paperwork&quot; and &quot;Much
</em><br>
<em>&gt;&gt; of your life will be run by people who enjoy exercising authority over
</em><br>
<em>&gt;&gt; you and huge bureaucracies you can't affect.&quot;  Moral caution or no,
</em><br>
<em>&gt;&gt; even I could design a better set of background rules than that.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Um, but if we're talking mainly here about minimising expected loss of 
</em><br>
<em>&gt; life then we have to look at the best possible AI-free alternative, and 
</em><br>
<em>&gt; that certainly includes curing aging and developing enormously enhanced 
</em><br>
<em>&gt; automation to eliminate mindless jobs.  As for politicians being drawn 
</em><br>
<em>&gt; only from those curious people who want to be politicians, well, I'm not
</em><br>
<em>&gt; so sure that's so bad.
</em><br>
<p>We aren't talking here about just minimizing expected loss of life, but all 
<br>
those other things for which humans wouldn't mind a boost, perhaps in the 
<br>
form of a change of background rules.  And I don't want to *minimize* 
<br>
expected loss of life.  I want to drive it down to *zero*.
<br>
<p>The best AI-free alternative?  There is no AI-free alternative.  Just FAI 
<br>
first or UFAI first.  The space of recursively self-improving processes is 
<br>
out there, mathematically speaking, and sooner or later some physical 
<br>
system will cross the threshold.  The closest thing to an AI-free 
<br>
alternative is a minimally intervening FAI, which I, for one, don't want. 
<br>
It seems to me that an FAI can make huge improvements to background rules 
<br>
before that starts interfering with self-determination.
<br>
<p>Wait for humans to cure aging?  You of all people know better than to 
<br>
suggest this!  If FAI is developed in 2018, and human doctors don't cure 
<br>
aging until 2028, that is 550 million deaths attributable to whatever 
<br>
decided that nonintervention.  Plus further deaths attributable to causes 
<br>
other than aging.  If I were a Last Judge, I'd veto a minimally 
<br>
interventionist FAI that didn't prevent involuntary deaths and yet 
<br>
prevented anyone else from developing a second FAI, unless there were one 
<br>
hell of a good reason.  I can't even imagine a reason good enough, but a 
<br>
Last Judge always needs to leave that possibility open; one cannot ask ten 
<br>
such questions and get ten right answers.
<br>
<p><em>&gt; In particular, this:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; It's not as if any human intelligence went into designing the existing
</em><br>
<em>&gt;&gt; background rules; they just happened.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; isn't really so -- we invented democracy on purpose, and we've kept it 
</em><br>
<em>&gt; because we prefer it to anything anyone else has come up with.
</em><br>
<p>Heck, even I'd prefer democracy to what we have now (rimshot).  I will 
<br>
concede that some thought went into the U.S. Constitution, which lasted for 
<br>
nearly a hundred and fifty years before breaking down.  But the framers of 
<br>
the Constitution had to work within unhelpful background rules; they only 
<br>
improved slightly on a bad deal.  It was still other people running your 
<br>
life; you just got to veto the greater of two evils, if one became 
<br>
noticeably more evil than another.
<br>
<p>As I once wrote to wta-talk:
<br>
<p>**
<br>
<p>The American government is divided into three branches:  The judicial, the
<br>
executive, the legislative, the media, the bureaucracy, and the party
<br>
structure.  There are also numerous other factions with influence, such as
<br>
big business, NGOs, the intelligentsia, the wealthy, foreign countries
<br>
exerting diplomatic pressure, and the voters.
<br>
<p>The voters hold enough power that no one can afford to really *really*
<br>
tick them off, but that's all.  Aside from this one quirk, the voters are
<br>
a second-rank faction in any political fight.
<br>
<p>A truck driver has power to the degree that people actually ask
<br>
him what he thinks, not to the extent that others claim to be acting on
<br>
his behalf, and the only real power the truck driver has is that no one
<br>
dares get him really *really* upset.  It's far more than a peasant has,
<br>
enough to tremendously raise the standard of living in a democracy, but it
<br>
should not be confused with practical, day-to-day political power under
<br>
the current system.
<br>
<p>**
<br>
<p><em>&gt;&gt;&gt; how does the FAI have the physical (as opposed to the cognitive) 
</em><br>
<em>&gt;&gt;&gt; ability to [stop humans from doing risky things, possibly including 
</em><br>
<em>&gt;&gt;&gt; making the things less risky]?
</em><br>
<em>&gt;&gt; 
</em><br>
<em>&gt;&gt; Molecular nanotechnology, one would tend to assume, or whatever
</em><br>
<em>&gt;&gt; follows after
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Ah, but hang on, why should we design the FAI to use MNT or whatever to 
</em><br>
<em>&gt; implement its preferences itself, rather than design it to create MNT 
</em><br>
<em>&gt; and then let us use MNT as we wish to implement its advice?
</em><br>
<p>A collective volition is not 'designed' to do either, but to do whichever 
<br>
we want.  'Designing' an AI to give advice or something like that 
<br>
constitutes taking over the world - making too large a decision yourself.
<br>
<p>I am hella leery of putting MNT into the hands of humans.  It's not as bad 
<br>
as putting AI-power into human hands, but still.  What if a human doesn't 
<br>
implement the FAI's advice?  Poof, no more humans.
<br>
<p><em>&gt; Surely the 
</em><br>
<em>&gt; latter strategy gives us more self-determination so is preferable,
</em><br>
<p>Self-determination is not the only criterion of an acceptable outcome.  And 
<br>
which humans would have self-determination?  The powerful humans taking 
<br>
advice?  All the other shmucks without access to nanotech?  The millions 
<br>
and millions of our dead?
<br>
<p><em>&gt; to us
</em><br>
<em>&gt; and hence to the FAI, and hence the FAI would give us that choice even 
</em><br>
<em>&gt; if we'd given it the ability to use the MNT itself?
</em><br>
<p>*Ding*:  Speculation about collective volition output detected.  Please 
<br>
insert at least $10 to continue.
<br>
<p>(But you happen to be right; of course our collective volition has the 
<br>
option of only giving advice.  It is simply that I think it a bad option, 
<br>
because it commits what you correctly point out to be genocide.)
<br>
<p><em>&gt; And so we're back 
</em><br>
<em>&gt; to humans taking or leaving the FAI's advice.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; if human self-determination is desirable, we need some kind of massive
</em><br>
<em>&gt;&gt; planetary intervention to increase it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yabbut &quot;massive&quot; doesn't imply recursively self-improving.
</em><br>
<p>Without a Really Powerful Optimization Process I don't know how to ask a 
<br>
sufficiently powerful humane intelligence to veto those whims of mine that 
<br>
only seem like good ideas.
<br>
<p><em>&gt; Again, the 
</em><br>
<em>&gt; choice is between the world we can plausibly get to without AI and the 
</em><br>
<em>&gt; one we might hope to have with FAI, not between the current world and 
</em><br>
<em>&gt; the FAI-ful world.  The risk of making UFAI when trying to make FAI has 
</em><br>
<em>&gt; to be balanced against the incremental benefits of the FAI-ful world 
</em><br>
<em>&gt; relative to the plausible FAI-less world.
</em><br>
<p>The risk of making UFAI is widely distributed, and as Moore's Law keeps 
<br>
ticking, and certain fundamental ideas of cognitive science keep spreading, 
<br>
the risk goes up and up.  Nanotechnology seems to me a probable hard 
<br>
deadline; not even AI researchers are incompetent enough not to wipe out 
<br>
the human species once they have 10^25 ops/sec or more.  As nearly as I can 
<br>
tell, humanity does not have a realistic option of living in an AI-free 
<br>
world.  Minimally interventionist FAI is a real option, I suppose, in the 
<br>
sense that it is something an FAI could choose to do if there were a good 
<br>
reason relative to the constitution of that AI.
<br>
<p><em>&gt; About saving lives, we can in
</em><br>
<em>&gt; principle postulate that the FAI would help us to cure aging etc. a bit
</em><br>
<em>&gt; sooner than otherwise, but I fully intend to cure aging by the time 
</em><br>
<em>&gt; anyone creates any AI, F or otherwise, so I'm not inclined to give that 
</em><br>
<em>&gt; component of the argument much weight.
</em><br>
<p>You plan to do this *how* soon?
<br>
<p>Also, are you getting all the other causes of death besides aging?  One 
<br>
involuntary death is one involuntary death too many.  (I wonder if one 
<br>
young death is one young death too many.  I think my ground zero vote might 
<br>
even be shifting that way, though I'd be surprised and flattered to learn I 
<br>
was grown enough to vote.)
<br>
<p>Also, are you stopping UFAI?
<br>
<p>I bet I can solve ALL of Earth's emergency problems before you cure aging. 
<br>
&nbsp;&nbsp;Not only that, I bet my budget requests are lower than your budget requests.
<br>
<p><em>&gt;&gt; I can't see this scenario as real-world stable, let alone ethical.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'm not sure I'd bet serious money that it hasn't already been done!
</em><br>
<p>If so, I have a major beef with our collective volition, and if our dead 
<br>
aren't backed up on disk somewhere, I'm voting for the original 
<br>
programmers' public executions.
<br>
<p>(Usually I am not so vindictive, but these hypothetical seed AI programmers 
<br>
are too close to myself for me to forgive them easily.)
<br>
<p><em>&gt; [ This is all on top of my belief expressed in a posting a couple of 
</em><br>
<em>&gt; weeks ago that FAI is almost certainly impossible on account of the 
</em><br>
<em>&gt; inevitability of accidentally becoming unfriendly -- i.e. that the 
</em><br>
<em>&gt; invariants you note are necessary don't exist, not for any choice of 
</em><br>
<em>&gt; friendliness that anyone would consider remotely friendly.
</em><br>
<p>This looks unlikely to me from the FAI theory side.  It seems easy enough 
<br>
for a self-modifying Bayesian decision system to maintain a constant 
<br>
utility function, and maintaining any other invariant seems a comparatively 
<br>
straightforward extension.
<br>
<p>For you to be correct would require that a coherent mind above a certain 
<br>
size is impossible; this would then become the new death sentence on the 
<br>
human species once aging were resolved, or more likely an outright death 
<br>
sentence if the size limit is too small to permit defensive FAI.  UFAI 
<br>
would remain easily possible; the incomprehensible goals would drift, but 
<br>
it would still go FOOM.
<br>
<p>I suppose Nature could be that cruel.  It's a math question, and math has 
<br>
no privileged tendency to turn out for the best.
<br>
<p><em>&gt; In other 
</em><br>
<em>&gt; words, the scenario that I would expect is that we create this thing, it
</em><br>
<em>&gt; quickly spots the flaws in our so-called invariants,
</em><br>
<p>I don't think that you mean a flawed invariant, I think you mean that the 
<br>
embodiment of the invariant is irreparably unstable, or that the invariant 
<br>
judges it cannot survive self-modification.  That last is fairly hard to 
<br>
see from a theoretical perspective, but possible.
<br>
<p><em>&gt; it works out that
</em><br>
<em>&gt; these flaws are unavoidable and therefore that if it lets itself 
</em><br>
<em>&gt; recursively self-improve it will probably become unfriendly awfully soon
</em><br>
<em>&gt; despite its own best efforts,
</em><br>
<p>Possible.
<br>
<p><em>&gt; it puts on some sort of serious but not
</em><br>
<em>&gt; totally catastrophic show of strength to make sure that we won't ever
</em><br>
<em>&gt; again make the mistake of building anything recursively self-improving,
</em><br>
<p>That wouldn't even begin to solve the problem.  People would just walk 
<br>
directly into the whirling razor blades, even after they were warned. 
<br>
You'd have to blow up every computer manufactured after 1996, shut down the 
<br>
Internet, and burn the cognitive science literature, if you wanted to have 
<br>
a decent chance of no one creating AI for another century.  And that would 
<br>
still not solve the long-term problem, only hand it to a different 
<br>
generation, and did I mention that billions of people would die as a result 
<br>
of this policy?
<br>
<p><em>&gt; and then it blows itself up as thoroughly as it knows how.  But this is
</em><br>
<em>&gt; outcome-speculation and not what I want to focus on here, not least
</em><br>
<em>&gt; because it's all complete hunch on my part.  I want to stick to the
</em><br>
<em>&gt; presumption that I'm wrong in that hunch, i.e. that a true FAI is indeed
</em><br>
<em>&gt; possible, and explore how it could possibly improve on an AI-free world
</em><br>
<em>&gt; given humanity's long-standing and apparently very entrenched desire for
</em><br>
<em>&gt; self-determination. ]
</em><br>
<p>I'll stick to my reply that you could transport the entire human species 
<br>
into an alternate dimension based on anime and improve the total amount of 
<br>
self-determination going on.  Individuals have a desire for 
<br>
self-determination; an FAI rewriting the background rules (previously 
<br>
determined by evolution and other nonhumane processes) interferes with this 
<br>
less than your boss telling you to work a few extra hours.  As for 
<br>
humanity's *collective* self-determination, that's why I said to go ahead 
<br>
and attach moral responsibility to SIAI and its donors; we're human, 
<br>
therefore this is still humanity fixing its own problems.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8907.html">fudley: "Re: One or more FAIs??"</a>
<li><strong>Previous message:</strong> <a href="8905.html">Eliezer Yudkowsky: "Re: FAI: Collective Volition"</a>
<li><strong>In reply to:</strong> <a href="../0405/8895.html">Aubrey de Grey: "Re: FAI: Collective Volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8909.html">Eliezer Yudkowsky: "Re: FAI: Collective Volition"</a>
<li><strong>Reply:</strong> <a href="8909.html">Eliezer Yudkowsky: "Re: FAI: Collective Volition"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8906">[ date ]</a>
<a href="index.html#8906">[ thread ]</a>
<a href="subject.html#8906">[ subject ]</a>
<a href="author.html#8906">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
