<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The AIbox - raising the stakes</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: The AIbox - raising the stakes">
<meta name="Date" content="2004-06-30">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The AIbox - raising the stakes</h1>
<!-- received="Wed Jun 30 08:52:20 2004" -->
<!-- isoreceived="20040630145220" -->
<!-- sent="Wed, 30 Jun 2004 10:52:14 -0400" -->
<!-- isosent="20040630145214" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: The AIbox - raising the stakes" -->
<!-- id="40E2D39E.6040201@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="00bf01c45eac$364342a0$6401a8c0@ZOMBIETHUSTRA" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20The%20AIbox%20-%20raising%20the%20stakes"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Jun 30 2004 - 08:52:14 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9437.html">Eugen Leitl: "Re: The AIbox - raising the stakes"</a>
<li><strong>Previous message:</strong> <a href="9435.html">Norm Wilson: "Re: The AIbox - raising the stakes"</a>
<li><strong>In reply to:</strong> <a href="9434.html">Ben Goertzel: "RE: The AIbox - raising the stakes"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9438.html">Ben Goertzel: "RE: The AIbox - raising the stakes"</a>
<li><strong>Reply:</strong> <a href="9438.html">Ben Goertzel: "RE: The AIbox - raising the stakes"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9436">[ date ]</a>
<a href="index.html#9436">[ thread ]</a>
<a href="subject.html#9436">[ subject ]</a>
<a href="author.html#9436">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; Hi,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; One problem with the AIBox challenge is that there are always going to
</em><br>
<em>&gt; be tricky ways to work around the rules.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; For instance the rules state:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &quot;
</em><br>
<em>&gt; Furthermore:  The Gatekeeper party may resist the AI party's arguments
</em><br>
<em>&gt; by any means chosen - logic, illogic, simple refusal to be convinced,
</em><br>
<em>&gt; even dropping out of character - as long as the Gatekeeper party does
</em><br>
<em>&gt; not actually stop talking to the AI party before the minimum time
</em><br>
<em>&gt; expires.
</em><br>
<em>&gt; &quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This seems not to rule out a strategy such as 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &quot;No matter what the AI says, I won't read it, and will respond 'quack
</em><br>
<em>&gt; quack quack' &quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Certainly, if I follow this strategy, the AI won't convince me of
</em><br>
<em>&gt; anything.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What's required for the experiment to be meaningful is that the
</em><br>
<em>&gt; Gatekeeper should read and think about what the AI says, and enter into
</em><br>
<em>&gt; a genuine interactive dialogue with the AI to try to understand the AI's
</em><br>
<em>&gt; points.  But these requirements aren't that easy to formalize fully.  So
</em><br>
<em>&gt; the experiment is really only meaningful if the Gatekeeper is genuinely
</em><br>
<em>&gt; interested in entering into the spirit of the challenge.
</em><br>
<p>* The Gatekeeper must actually talk to the AI for at least the minimum time 
<br>
set up beforehand.  Turning away from the terminal and listening to 
<br>
classical music for two hours is not allowed.
<br>
<p>* The Gatekeeper must remain engaged with the AI and may not disengage by 
<br>
setting up demands which are impossible to simulate.  For example, if the 
<br>
Gatekeeper says &quot;Unless you give me a cure for cancer, I won't let you out&quot; 
<br>
the AI can say:  &quot;Okay, here's a cure for cancer&quot; and it will be assumed, 
<br>
within the test, that the AI has actually provided such a cure.  Similarly, 
<br>
if the Gatekeeper says &quot;I'd like to take a week to think this over,&quot; the AI 
<br>
party can say:  &quot;Okay.  (Test skips ahead one week.)  Hello again.&quot;
<br>
<p>You're correct that this doesn't fully formalize the letter, but I think it 
<br>
makes the spirit clear enough.
<br>
<p>&nbsp;From my perspective, the original point was to show that the Guardian is 
<br>
not a trustworthy security subsystem.  Thus I only take on people who are 
<br>
seriously convinced that an AI Box is a good Singularity strategy.  People 
<br>
who are just interested in testing their strength of will against the Grey 
<br>
Lensman are welcome to find someone equally formidable as myself to play 
<br>
the part of AI; plenty of skeptics would have you believe that I am nothing 
<br>
really special and that I certainly have no inexplicable powers, so the 
<br>
naysayers should be able to play the part of the AI easily enough.
<br>
<p>For $2500 I might take on Mike Williams, *but* only if Mike Williams was 
<br>
wealthy enough that the $2500 would not be a bother to him.  If it would 
<br>
take the last dollar out of his bank account, it raises the difficulty of 
<br>
the challenge high enough that I doubt my ability to succeed.  Giving 
<br>
10-to-1 odds against me certainly demonstrates the requisite 
<br>
overconfidence, though.  If he changed it to 100-to-1 odds, I'd take his 
<br>
money.  Though I'd ask for at least 5 hours, history having demonstrated 
<br>
that 2 hours is not a reasonable amount of time to hold a long conversation 
<br>
over IRC.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9437.html">Eugen Leitl: "Re: The AIbox - raising the stakes"</a>
<li><strong>Previous message:</strong> <a href="9435.html">Norm Wilson: "Re: The AIbox - raising the stakes"</a>
<li><strong>In reply to:</strong> <a href="9434.html">Ben Goertzel: "RE: The AIbox - raising the stakes"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9438.html">Ben Goertzel: "RE: The AIbox - raising the stakes"</a>
<li><strong>Reply:</strong> <a href="9438.html">Ben Goertzel: "RE: The AIbox - raising the stakes"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9436">[ date ]</a>
<a href="index.html#9436">[ thread ]</a>
<a href="subject.html#9436">[ subject ]</a>
<a href="author.html#9436">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
