<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: FAI: Collective Volition</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: FAI: Collective Volition">
<meta name="Date" content="2004-06-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: FAI: Collective Volition</h1>
<!-- received="Wed Jun  2 06:29:45 2004" -->
<!-- isoreceived="20040602122945" -->
<!-- sent="Wed, 2 Jun 2004 08:29:33 -0400" -->
<!-- isosent="20040602122933" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: FAI: Collective Volition" -->
<!-- id="009d01c4489d$43137330$6401a8c0@ZOMBIETHUSTRA" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="016501c44864$8e0e7c60$c6b36044@amd" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20FAI:%20Collective%20Volition"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Wed Jun 02 2004 - 06:29:33 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8933.html">Michael Wilson: "RE: SIAI seeking seed AI programmer candidates"</a>
<li><strong>Previous message:</strong> <a href="8931.html">Metaqualia: "Re: Dangers of human self-modification"</a>
<li><strong>In reply to:</strong> <a href="8921.html">Mike: "RE: FAI: Collective Volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8936.html">Philip Sutton: "RE: Sentient vs non-sentient super general AI"</a>
<li><strong>Reply:</strong> <a href="8936.html">Philip Sutton: "RE: Sentient vs non-sentient super general AI"</a>
<li><strong>Reply:</strong> <a href="8937.html">Eliezer Yudkowsky: "Re: FAI: Collective Volition"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8932">[ date ]</a>
<a href="index.html#8932">[ thread ]</a>
<a href="subject.html#8932">[ subject ]</a>
<a href="author.html#8932">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer,
<br>
<p>About this idea of creating a non-sentient optimization process, to 
<br>
<p>A) predict possible futures for the universe
<br>
B) analyze the global human psyche and figure out the &quot;collective
<br>
volition&quot; of humanity
<br>
<p>instead of creating a superhuman mind....
<br>
<p>I can't say it's impossible that this would work.   It goes against my
<br>
scientific intuition, which says that sentience of some sort would
<br>
almost surely be needed to achieve these things, but my scientific
<br>
intuition could be wrong.  Also, my notion of &quot;sentience of some sort&quot;
<br>
may grow and become more flexible as more AGI and AGI-ish systems become
<br>
available for interaction!
<br>
<p>However, it does seem to me that either problem A or B above is
<br>
significantly more difficult than creating a self-modifying AGI system.
<br>
Again, I could be wrong on this, but ... Sheesh.  
<br>
<p>To create a self-modifying AGI system, at very worst one has to
<br>
understand the way the human brain works, and then emulate something
<br>
like it in a more mutably self-modifiable medium such as computer
<br>
software.  This is NOT the approach I'm taking with Novamente; I'm just
<br>
pointing it out to place a bound on the difficulty of creating a
<br>
self-modifying AGI system.  The biggest &quot;in principle&quot; obstacle here is
<br>
that it could conceivably require insanely much computational power --
<br>
or quantum computing, quantum gravity computing, etc. -- to get AGI to
<br>
work at the human level (for example, if the microtubule hypothesis is
<br>
right).  Even so, then we just have the engineering problem of creating
<br>
a more mutable substrate than human brain tissue, and reimplementing
<br>
human brain algorithms within it.  
<br>
<p>On the other hand, the task of creating a non-sentient optimization
<br>
process of the sort you describe is a lot more nebulous (due to the lack
<br>
of even partially relevant examples to work from).  Yeah, in principle
<br>
it's easy to create optimization processes of arbitrary power -- so long
<br>
as one isn't concerned about memory or processor usage.  But
<br>
contemporary science tells us basically NOTHING about how to make
<br>
uber-optimization-processes like the one you're envisioning.  The ONLY
<br>
guidance it gives us in this direction, pertains to &quot;how to build a
<br>
sentience that can act as a very powerful optimization process.&quot;
<br>
<p>So, it seems to me that you're banking on creating a whole new branch of
<br>
science basically from nothing, whereas to create AGI one MAY not need
<br>
to do that, one may only need to &quot;fix&quot; the existing sciences of
<br>
&quot;cognitive science&quot; and AI.  
<br>
<p>It seems to me that, even if what you're suggesting is possible (which I
<br>
really doubt), you're almost certain to be beaten in the race by someone
<br>
working to build a sentient AGI.
<br>
<p>Therefore, to succeed with this new plan, you'll probably need to create
<br>
some kind of fascist state in which working on AGI is illegal and
<br>
punishable by death, imprisonment or lobotomy.
<br>
<p>But I'd suggest you hold off on taking power just yet, as you may
<br>
radically change your theoretical perspective again next year ;-)
<br>
<p>--  Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8933.html">Michael Wilson: "RE: SIAI seeking seed AI programmer candidates"</a>
<li><strong>Previous message:</strong> <a href="8931.html">Metaqualia: "Re: Dangers of human self-modification"</a>
<li><strong>In reply to:</strong> <a href="8921.html">Mike: "RE: FAI: Collective Volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8936.html">Philip Sutton: "RE: Sentient vs non-sentient super general AI"</a>
<li><strong>Reply:</strong> <a href="8936.html">Philip Sutton: "RE: Sentient vs non-sentient super general AI"</a>
<li><strong>Reply:</strong> <a href="8937.html">Eliezer Yudkowsky: "Re: FAI: Collective Volition"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8932">[ date ]</a>
<a href="index.html#8932">[ thread ]</a>
<a href="subject.html#8932">[ subject ]</a>
<a href="author.html#8932">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
