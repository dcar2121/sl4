<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI has become slightly amusing</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: SIAI has become slightly amusing">
<meta name="Date" content="2004-06-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI has become slightly amusing</h1>
<!-- received="Fri Jun  4 12:46:44 2004" -->
<!-- isoreceived="20040604184644" -->
<!-- sent="Fri, 04 Jun 2004 14:46:41 -0400" -->
<!-- isosent="20040604184641" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: SIAI has become slightly amusing" -->
<!-- id="40C0C391.5020902@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="00e401c44a60$dcc6d2f0$6401a8c0@ZOMBIETHUSTRA" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20SIAI%20has%20become%20slightly%20amusing"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri Jun 04 2004 - 12:46:41 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9089.html">Eliezer Yudkowsky: "Re: SIAI has become slightly amusing"</a>
<li><strong>Previous message:</strong> <a href="9087.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<li><strong>In reply to:</strong> <a href="9084.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9093.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<li><strong>Reply:</strong> <a href="9093.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9088">[ date ]</a>
<a href="index.html#9088">[ thread ]</a>
<a href="subject.html#9088">[ subject ]</a>
<a href="author.html#9088">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; No, it's also based on statements such as (paraphrases, not quotes)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &quot;I understand both AI and the Singularity much better than anyone else
</em><br>
<em>&gt; in the world&quot; 
</em><br>
<p>No, just much better than anyone whose opinions on the subject have been 
<br>
written up and made their way in front of my eyeballs.
<br>
<p><em>&gt; &quot;I don't need to ever study in a university, because I understand what's
</em><br>
<em>&gt; important better than all the professors anyway.&quot;
</em><br>
<p>False.
<br>
<p><em>&gt; &quot;A scientist who doesn;t accept SIAI's theory is not a good scientist&quot;
</em><br>
<p>I repudiate this.
<br>
<p><em>&gt; &quot;SIAI's theories are on a par with Einstein's General Relativity Theory&quot;
</em><br>
<p>I repudiate this.
<br>
<p><em>&gt; Eli seems to have a liking for James Rogers' approach, but has been
</em><br>
<em>&gt; extremely dismissive toward every other AI approach I've seen him talk
</em><br>
<em>&gt; about.  
</em><br>
<p>There are a hundred mistakes for every correct answer.  I'm very fond of 
<br>
many scientific fields, but not many popular contemporary approaches to AI. 
<br>
&nbsp;&nbsp;I like Marcus Hutter's work, and I have a sentimental fondness for 
<br>
Douglas Lenat's old work with Eurisko.  But mostly I reserve my fondnesses 
<br>
for fields other than AI.  I don't think much of many *AGI* approaches out 
<br>
there; specific tools such as neural networks, Bayesian belief networks, 
<br>
and so on, may work just fine.
<br>
<p><em>&gt; The notion of your &quot;volition&quot; as Eliezer now proposes it is NOT
</em><br>
<em>&gt; necessarily aligned with your desires or your will at this moment.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Rather, it's an AI's estimate of what you WOULD want at this moment, if
</em><br>
<em>&gt; you were a better person according to your own standards of goodness.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Tricky notion, no?
</em><br>
<p>Yep, that it is.
<br>
<p><em>&gt; The idea, it seems, is to allow me to grow and change and learn to the
</em><br>
<em>&gt; extent that the AI estimates I will, in future, want my past self to be
</em><br>
<em>&gt; allowed to do.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In other words, the AI is supposed to treat me like a child, and
</em><br>
<em>&gt; estimate what the adult-me is eventually going to want the child-me to
</em><br>
<em>&gt; have been allowed to do.  
</em><br>
<p>A powerful analogy, but a dangerous one.  Human children are designed by 
<br>
natural selection to have parents; human adults are not.
<br>
<p><em>&gt; In raising my kids, I use this method sometimes, but more often I let
</em><br>
<em>&gt; the children do what they presently desire rather than what I think
</em><br>
<em>&gt; their future selves will want their past selves to have done.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think that, as a first principle, sentient beings should be allowed
</em><br>
<em>&gt; their free choice, and issues of &quot;collective volition&quot; and such should
</em><br>
<em>&gt; only enter into the picture in order to resolve conflicts between
</em><br>
<em>&gt; different sentience's free choices.
</em><br>
<p>But what if you are horrified by the consequences of this choice in 30 
<br>
years?  How much thought did you put into this before deciding to make it 
<br>
the eternal law of the human species?  Are you so confident in the power of 
<br>
your moral reasoning, oh modest Ben?
<br>
<p><em>&gt; I much prefer to embody an AI with &quot;respect choices of sentient beings
</em><br>
<em>&gt; whenever possible&quot; as a core value.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Concrete choice, not estimated volition.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is a major ethical choice, on which Eliezer and I appear to
</em><br>
<em>&gt; currently significantly differ.
</em><br>
<p>Actually, I think the key point of our difference is in how to make the 
<br>
meta-decision between concrete choice and extrapolated volition.  I don't 
<br>
dare make the concrete choice myself, so I turn it over to extrapolated 
<br>
volition.  (I arrogantly try to reduce my probability of massively screwing 
<br>
up, rather than humbly submitting to the sacred unknown.)  I certainly hope 
<br>
that our extrapolated volition is to respect concrete choice, and if I were 
<br>
a Last Judge and I peeked and I saw concrete choice extensively violated 
<br>
and there wasn't a good reason, I'd veto.
<br>
<p><em>&gt; The fact that Eliezer has said so to me, in the past.  He said he didn't
</em><br>
<em>&gt; want to share the details of his ideas on AI because I or others might
</em><br>
<em>&gt; use them in an unsafe way.
</em><br>
<p>I think I may have said something along the lines of, &quot;There is no sane 
<br>
reason to discuss AGI with you until I have finished discussing Friendly 
<br>
AI, since you need to know the FAI stuff anyway, and if you can't get FAI 
<br>
you probably can't get the AGI either.&quot;
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9089.html">Eliezer Yudkowsky: "Re: SIAI has become slightly amusing"</a>
<li><strong>Previous message:</strong> <a href="9087.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<li><strong>In reply to:</strong> <a href="9084.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9093.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<li><strong>Reply:</strong> <a href="9093.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9088">[ date ]</a>
<a href="index.html#9088">[ thread ]</a>
<a href="subject.html#9088">[ subject ]</a>
<a href="author.html#9088">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
