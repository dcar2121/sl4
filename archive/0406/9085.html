<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI has become slightly amusing</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: SIAI has become slightly amusing">
<meta name="Date" content="2004-06-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI has become slightly amusing</h1>
<!-- received="Fri Jun  4 12:25:33 2004" -->
<!-- isoreceived="20040604182533" -->
<!-- sent="Fri, 04 Jun 2004 14:25:29 -0400" -->
<!-- isosent="20040604182529" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: SIAI has become slightly amusing" -->
<!-- id="40C0BE99.1040709@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="40C0B7A1.8040204@intelligence.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20SIAI%20has%20become%20slightly%20amusing"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri Jun 04 2004 - 12:25:29 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9086.html">Randall Randall: "Re: Sentience  [Was  FAI: Collective Volition]"</a>
<li><strong>Previous message:</strong> <a href="9084.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<li><strong>In reply to:</strong> <a href="9083.html">Michael Anissimov: "Re: SIAI has become slightly amusing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9087.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<li><strong>Reply:</strong> <a href="9087.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9085">[ date ]</a>
<a href="index.html#9085">[ thread ]</a>
<a href="subject.html#9085">[ subject ]</a>
<a href="author.html#9085">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michael Anissimov wrote:
<br>
<p><em>&gt; Ben Goertzel wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; I don't want some AI program, created by you guys or anyone else,
</em><br>
<em>&gt;&gt; imposing its inference of my &quot;volition&quot; upon me.   
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The word &quot;imposing&quot; suggests something out of line with your volition.  
</em><br>
<em>&gt; But the whole point of any FAI is to carry out your volition.  If the 
</em><br>
<em>&gt; volition it is carrying out is unfavorable and foreign to you, then that 
</em><br>
<em>&gt; would constitute a failure on the part of the programmers.  The point is 
</em><br>
<em>&gt; to carry out your orders in such a way that the *intent* takes 
</em><br>
<em>&gt; precedence over the *letter* of your requests.  Imagine a continuum of 
</em><br>
<em>&gt; AIs, one extreme paying attention to nothing but the letter of your 
</em><br>
<em>&gt; requests, the other extreme carrying your intent too far to the point 
</em><br>
<em>&gt; where you disapprove.  The task of the FAI programmer is to create an 
</em><br>
<em>&gt; initial dynamic that rests appropriately between these two extremes.
</em><br>
<p>No, Ben's got a legitimate concern there.  Remember that the purpose of the 
<br>
initial dynamic is to choose a dynamic, based on a majority vote of 
<br>
extrapolated humanity, and that this majority vote could conceivably do 
<br>
something to Ben he doesn't like.  I gave extensive reasons in Collective 
<br>
Volition for why this can't be pre-emptively ruled out by the programmers 
<br>
(i.e., it also rules out infants growing up into humans rather than 
<br>
super-infants, the programmers must decide For All Time what constitutes a 
<br>
sentient being, the initial dynamic doesn't have full freedom to rewrite 
<br>
itself, and so on).  It's under the heading of why the initial dynamic 
<br>
can't include a programmer-decided Bill of Rights.  Ultimately it boils 
<br>
down to moral caution:  If you pick ten Rights, three will be Wrong.
<br>
<p><em>&gt;&gt; When I enounced the three values of Joy, Growth and Choice in a recent
</em><br>
<em>&gt;&gt; essay, I really meant *choice* -- i.e., I meant *what I choose, now, me
</em><br>
<em>&gt;&gt; being who I am*.  I didn't mean *what I would choose if I were what I
</em><br>
<em>&gt;&gt; think I'd like to be*, which is my understanding of Eliezer's current
</em><br>
<em>&gt;&gt; notion of &quot;volition.&quot;
</em><br>
<p>Yes, I understand.  But I don't trust people's unvarnished choices, to 
<br>
accomplish what they wish, even to not kill them outright.  People aren't 
<br>
cautious enough.  I called this &quot;Murder by genie bottle&quot; and I meant it.
<br>
<p>The power to tear apart a god like tinfoil is too much power.  The power 
<br>
must be kept out of the hands of corporations, governments, the original 
<br>
programmers, the human species itself until it has a chance to grow up. 
<br>
Otherwise we're gonna die, murder by genie bottle.  And yet we need SI, to 
<br>
protect us from UFAI, to render emergency first aid and perhaps do other 
<br>
things we can't comprehend.  Collective Volition is my current proposal for 
<br>
cutting the knot.  Yes, it is scary.  And yes, there is a possibility that 
<br>
what you regard as your individual rights will be violated; I can't rule 
<br>
that out with personally exercising more control over the future than I 
<br>
dare.  All I can do is plan verification processes, ways to make sure that 
<br>
we end up in a Nice Place to Live.
<br>
<p><em>&gt;&gt; To have some AI program extrapolate from my brain what it estimates I'd
</em><br>
<em>&gt;&gt; like to be, and then modify the universe according to the choices this
</em><br>
<em>&gt;&gt; estimated Ben's-ideal-of-Ben would make (along with the estimated
</em><br>
<em>&gt;&gt; choices of others) --- this denies me the right to be human, to grow and
</em><br>
<em>&gt;&gt; change and learn.  According to my personal value system, this is not a
</em><br>
<em>&gt;&gt; good thing at all.   
</em><br>
<p>For humanity to grow up together, choose our work and do it ourselves?  Let 
<br>
not our collective volition deprive us of our destiny?  That is also my 
<br>
wish, in this passing moment, and the dynamics of Collective Volition are 
<br>
designed to take exactly that sort of wish into account.
<br>
<p><em>&gt;&gt; Eventually this series might converge, or it might not.  Suppose the
</em><br>
<em>&gt;&gt; series doesn't converge, then which point in the iteration does the AI
</em><br>
<em>&gt;&gt; choose as &quot;Ben's volition&quot;?  Does it average over all the terms in the
</em><br>
<em>&gt;&gt; series?  Egads again.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Good question!  The ultimate answer will be for the FAI to decide, and 
</em><br>
<em>&gt; we want to seed the FAI with the moral complexity necessary to make that 
</em><br>
<em>&gt; decision with transhuman wisdom and compassion.  Eliezer and Co. won't 
</em><br>
<em>&gt; be specifying the answer in the code.
</em><br>
<p>No, that's exactly the sort of thing that has to be specified in an initial 
<br>
dynamic.  The initial dynamic isn't permanent, but it needs an *initial* 
<br>
specification.  This is discussed in Collective Volition, albeit only 
<br>
peripherally, because the details of this process are exactly the sort of 
<br>
thing that I'm likely to change with improving technical understanding. 
<br>
But there's enough discussion in CV to show the problem exists and that I 
<br>
know it exists.
<br>
<p>In answer to Ben's question, the extrapolation doesn't 'pick one term' or 
<br>
'average over' terms, it superposes terms and calculates the distance and 
<br>
chaos.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9086.html">Randall Randall: "Re: Sentience  [Was  FAI: Collective Volition]"</a>
<li><strong>Previous message:</strong> <a href="9084.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<li><strong>In reply to:</strong> <a href="9083.html">Michael Anissimov: "Re: SIAI has become slightly amusing"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9087.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<li><strong>Reply:</strong> <a href="9087.html">Ben Goertzel: "RE: SIAI has become slightly amusing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9085">[ date ]</a>
<a href="index.html#9085">[ thread ]</a>
<a href="subject.html#9085">[ subject ]</a>
<a href="author.html#9085">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
