<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: We Can't Fool the Super Intelligence</title>
<meta name="Author" content="Mike (mikew12345@cox.net)">
<meta name="Subject" content="RE: We Can't Fool the Super Intelligence">
<meta name="Date" content="2004-06-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: We Can't Fool the Super Intelligence</h1>
<!-- received="Sat Jun 26 00:37:55 2004" -->
<!-- isoreceived="20040626063755" -->
<!-- sent="Fri, 25 Jun 2004 23:39:18 -0700" -->
<!-- isosent="20040626063918" -->
<!-- name="Mike" -->
<!-- email="mikew12345@cox.net" -->
<!-- subject="RE: We Can't Fool the Super Intelligence" -->
<!-- id="003201c45b48$4ef616e0$c6b36044@amd" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20040626050817.6457.qmail@web60001.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Mike (<a href="mailto:mikew12345@cox.net?Subject=RE:%20We%20Can't%20Fool%20the%20Super%20Intelligence"><em>mikew12345@cox.net</em></a>)<br>
<strong>Date:</strong> Sat Jun 26 2004 - 00:39:18 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9411.html">Marc Geddes: "Re: My last thoughts on Collective Volition for now"</a>
<li><strong>Previous message:</strong> <a href="9409.html">Thomas Buckner: "RE: We Can't Fool the Super Intelligence"</a>
<li><strong>In reply to:</strong> <a href="9409.html">Thomas Buckner: "RE: We Can't Fool the Super Intelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9412.html">Simon Gordon: "RE: We Can't Fool the Super Intelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9410">[ date ]</a>
<a href="index.html#9410">[ thread ]</a>
<a href="subject.html#9410">[ subject ]</a>
<a href="author.html#9410">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20We%20Can't%20Fool%20the%20Super%20Intelligence">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20We%20Can't%20Fool%20the%20Super%20Intelligence">owner-sl4@sl4.org</a>] On Behalf 
</em><br>
<em>&gt; Of Thomas Buckner
</em><br>
<em>&gt; Sent: Friday, June 25, 2004 10:08 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=RE:%20We%20Can't%20Fool%20the%20Super%20Intelligence">sl4@sl4.org</a>
</em><br>
<em>&gt; Subject: RE: We Can't Fool the Super Intelligence
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; --- Simon Gordon &lt;<a href="mailto:sim_dizzy@yahoo.com?Subject=RE:%20We%20Can't%20Fool%20the%20Super%20Intelligence">sim_dizzy@yahoo.com</a>&gt; wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; &gt;The universe is more interesting with us in it.
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; LOL. Really?  Well you could be right, but given that
</em><br>
<em>&gt; &gt; at some point the superAI would be capable of bringing
</em><br>
<em>&gt; &gt; into existence any number of an infinite variety of
</em><br>
<em>&gt; &gt; &quot;designer intelligences&quot;, i tend to think that we
</em><br>
<em>&gt; &gt; would only really be interesting from a historical 
</em><br>
<em>&gt; perspective...and 
</em><br>
<em>&gt; &gt; is history all that interesting? Maybe to some humans it is but i 
</em><br>
<em>&gt; &gt; doubt whether higher intelligences would be at all bothered about 
</em><br>
<em>&gt; &gt; anything like that.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes, really. I think you deem that the superAI will be &quot;vast, 
</em><br>
<em>&gt; cool, and unsympathetic&quot; to the degree that ve has no concept 
</em><br>
<em>&gt; of how humorous our follies and farces are. Even our 
</em><br>
<em>&gt; stupidity is interesting. Strange, but true. And yes, I think 
</em><br>
<em>&gt; even resurrecting the dead by recreating all possible 
</em><br>
<em>&gt; iterations of intelligence (including ourselves) is 
</em><br>
<em>&gt; reasonable if the processing capacity is available. To do 
</em><br>
<em>&gt; otherwise is (to me) a bit like reproducing half the 
</em><br>
<em>&gt; Mandelbrot set while deciding that the other half is somehow 
</em><br>
<em>&gt; inferior. I myself have strongly come to suspect that human 
</em><br>
<em>&gt; qualia are not intrinsically better than, say, bonobo qualia 
</em><br>
<em>&gt; on a good day, or dolphin qualia. Why would a superior 
</em><br>
<em>&gt; intelligence deny itself access to other modes if ve had the 
</em><br>
<em>&gt; choice? I assert that this falls into the class of things 
</em><br>
<em>&gt; people think a SAI might do that in fact ve would not do, 
</em><br>
<em>&gt; because ve would know better. There might be useful learnings 
</em><br>
<em>&gt; or experiences ve could derive from 'seeing through our 
</em><br>
<em>&gt; eyes', so that if ve got rid of us before the resource was 
</em><br>
<em>&gt; exhausted, ve would be doing something dumb. I once mentioned 
</em><br>
<em>&gt; to a woman acquaintance a bit of data that I gleaned from an 
</em><br>
<em>&gt; article in Esquire (a men's magazine). She replied, &quot;I don't 
</em><br>
<em>&gt; read men's magazines.&quot; I told her that this was unenlightened 
</em><br>
<em>&gt; because I have a rule: Never limit your sources of 
</em><br>
<em>&gt; information. Now, if I am smart enough to understand that, so 
</em><br>
<em>&gt; is any SAI worth ver salt. Ve might run out of uses for us 
</em><br>
<em>&gt; eventually, but we can't predict if or when, and initially, 
</em><br>
<em>&gt; neither can the SAI. Don't sell us short the way we sell 
</em><br>
<em>&gt; other life short, and don't sell SAI short the way you sell 
</em><br>
<em>&gt; us short. We are able to destroy other life and intelligence 
</em><br>
<em>&gt; because we are smarter, but we do it because we are not smart enough.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Tom Buckner
</em><br>
<em>&gt; 
</em><br>
<em>&gt; =====
</em><br>
<p>Put this on a much larger scale and see if it still makes as much sense.
<br>
How much intelligence is there in a petri dish culture?  Does your
<br>
argument still apply?  &quot;Don't destroy that specimen in the dish, because
<br>
it might contain a unique source of information.&quot;
<br>
<p>Once the study of the virus is done, the usual thing to do is to destroy
<br>
the sample.  If the AI is truly thousands of times more intelligent than
<br>
us, it may not take it too long to decide that it's learned all that's
<br>
useful from us.  
<br>
<p>Additionally, the AI will undoubtedly have many new and interesting
<br>
things to occupy its time; things that we can't begin to imagine.  Human
<br>
existence may be as interesting to the AI as the daily routine of a sea
<br>
slug is to the average human today.  
<br>
<p>Unless the AI perceives a real *need* for humans to exist, (a need as
<br>
basic as the hardware it runs on), the need to remove humans may win
<br>
out.  We consume more than our share of resources, we foul up the
<br>
planet, we fight amongst ourselves, etc.  We may have value as another
<br>
source of information, but is that enough to balance the equation?
<br>
Especially when the AI can run all the simulations it wants, in all the
<br>
multiverse permutations imaginable, without the associated impact on the
<br>
planet?  A nuclear holocaust has value as a source of information, but
<br>
that doesn't mean we should have one.
<br>
<p>I think our future is shaky unless:
<br>
1)  humans are essential for the continued existence of the AI, and more
<br>
importantly,
<br>
2)  the AI accepts #1 as true.
<br>
<p>Mike W.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9411.html">Marc Geddes: "Re: My last thoughts on Collective Volition for now"</a>
<li><strong>Previous message:</strong> <a href="9409.html">Thomas Buckner: "RE: We Can't Fool the Super Intelligence"</a>
<li><strong>In reply to:</strong> <a href="9409.html">Thomas Buckner: "RE: We Can't Fool the Super Intelligence"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9412.html">Simon Gordon: "RE: We Can't Fool the Super Intelligence"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9410">[ date ]</a>
<a href="index.html#9410">[ thread ]</a>
<a href="subject.html#9410">[ subject ]</a>
<a href="author.html#9410">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
