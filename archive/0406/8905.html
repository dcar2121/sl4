<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: FAI: Collective Volition</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: FAI: Collective Volition">
<meta name="Date" content="2004-06-01">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: FAI: Collective Volition</h1>
<!-- received="Tue Jun  1 06:01:11 2004" -->
<!-- isoreceived="20040601120111" -->
<!-- sent="Tue, 01 Jun 2004 08:01:06 -0400" -->
<!-- isosent="20040601120106" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: FAI: Collective Volition" -->
<!-- id="40BC7002.4030504@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="D478897F-B39D-11D8-86D4-000A95B1AFDE@objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20FAI:%20Collective%20Volition"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Jun 01 2004 - 06:01:06 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8906.html">Eliezer Yudkowsky: "Re: FAI: Collective Volition"</a>
<li><strong>Previous message:</strong> <a href="8904.html">Metaqualia: "Re: FAI: Collective Volition"</a>
<li><strong>In reply to:</strong> <a href="8901.html">Samantha Atkins: "Re: FAI: Collective Volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8915.html">Peter Voss: "Summary of current FAI thought"</a>
<li><strong>Reply:</strong> <a href="8915.html">Peter Voss: "Summary of current FAI thought"</a>
<li><strong>Reply:</strong> <a href="8921.html">Mike: "RE: FAI: Collective Volition"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8905">[ date ]</a>
<a href="index.html#8905">[ thread ]</a>
<a href="subject.html#8905">[ subject ]</a>
<a href="author.html#8905">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Samantha Atkins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Thanks for writing this Eliezer.
</em><br>
<p>Thanks for commenting!
<br>
<p><em>&gt; My first level comments follow.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; How can we possibly get an SAI, excuse me, a &quot;Friendly Really Powerful 
</em><br>
<em>&gt; Optimization Process&quot;. to successfully extrapolate the full collective 
</em><br>
<em>&gt; volition of humanity?  At this point in the  game we can't even master 
</em><br>
<em>&gt; simple DWIM applications.   We do not have AIs that are capable of 
</em><br>
<em>&gt; understanding immediate volition much less the full extended volition.
</em><br>
<p>Huge progress in AI is required to do this?  Yes, no kidding.  Leave that 
<br>
part to SIAI.  For the moment we speak only of the ends to which the means 
<br>
are directed.
<br>
<p><em>&gt; So how can any device/AI/optimization process claiming to do so possibly
</em><br>
<em>&gt; seem other than completely arbitrary and un-Friendly?
</em><br>
<p>It is not about &quot;claiming&quot;.  If one has not verified the Friendly Thingy to 
<br>
work as claimed, one does not set it in motion.
<br>
<p><em>&gt; Extrapolation of volition based on what we would want if we were very 
</em><br>
<em>&gt; different beings than we are is even more likely to go far off the mark.
</em><br>
<p>Hence the measurement of distance and spread.
<br>
<p><em>&gt; How can this possibly not diverge wildly into whatever the FAI (or
</em><br>
<em>&gt; whatever) forces to converge into simply what it believes would be 
</em><br>
<em>&gt; &quot;best&quot;?   This is unlikely to bear a lot of resemblance to what any 
</em><br>
<em>&gt; actual humans want at any depth.
</em><br>
<p>I have added PAQ 9 to address this:
<br>
<p>**
<br>
<p>Q9. How does the dynamic force individual volitions to cohere? (Frequently 
<br>
Asked)
<br>
<p>A9. The dynamic doesn't force anything. The engineering goal is to ask what 
<br>
humankind &quot;wants&quot;, or rather would decide if we knew more, thought faster, 
<br>
were more the people we wished we were, had grown up farther together, etc. 
<br>
&quot;There is nothing which humanity can be said to 'want' in this sense&quot; is a 
<br>
possible answer to this question. Meaning, you took your best shot at 
<br>
asking what humanity wanted, and humanity didn't want anything coherent. 
<br>
But you cannot force the collective volition to cohere by computing some 
<br>
other question than &quot;What does humanity want?&quot; That is fighting alligators 
<br>
instead of draining the swamp - solving an engineering subproblem at the 
<br>
expense of what you meant the project to accomplish.
<br>
<p>There are nonobvious reasons our volitions would cohere. In the Middle East 
<br>
the Israelis hate the Palestinians and the Palestinians hate the Israelis; 
<br>
in Belfast the Catholics hate the Protestants and the Protestants hate the 
<br>
Catholics; India hates Pakistan and Pakistan hates India. But Gandhi loves 
<br>
everyone, and Martin Luther King loves everyone, and so their wishes add up 
<br>
instead of cancelling out, coherent like the photons in a laser. One might 
<br>
say that love obeys Bose-Einstein statistics while hatred obeys Fermi-Dirac 
<br>
statistics.
<br>
<p>Similarly, disagreements may be predicated on:
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Different guesses to simple questions of fact. (&quot;Knew more&quot; 
<br>
increases coherence.)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Poor solutions to cognitive problems. (&quot;Think faster&quot; increases 
<br>
coherence.)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Impulses that would play lesser relative roles in the people we 
<br>
wished we were.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Decisions we would not want our extrapolated volitions to include.
<br>
<p>Suppose that's not enough to produce coherence? Collective volition, as a 
<br>
moral solution, doesn't require some exact particular set of rules for the 
<br>
initial dynamic. You can't take leave of asking what humanity &quot;wants&quot;, but 
<br>
it's all right, albeit dangerous, to try more than one plausible definition 
<br>
of &quot;want&quot;. I don't think it's gerrymandering to probe the space of 
<br>
&quot;dynamics that plausibly ask what humanity wants&quot; to find a dynamic that 
<br>
produces a coherent output, provided:
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The meta-dynamic looks only for coherence, no other constraints.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The meta-dynamic searches a small search space.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The meta-dynamic satisfices rather than maximizes.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The dynamic itself doesn't force coherence where none exists.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* A Last Judge peeks at the actual answer and checks it makes sense.
<br>
<p>I would not object to an initial dynamic that contained a meta-rule along 
<br>
the lines of: &quot;Extrapolate far enough that our medium-distance wishes 
<br>
cohere with each other and don't interfere with long-distance vetoes. If 
<br>
that doesn't work, try this different order of evaluation. If that doesn't 
<br>
work then fail, because it looks like humankind doesn't want anything.&quot;
<br>
<p>Note that this meta-dynamic tests one quantitative variable (how far to 
<br>
extrapolate) and one binary variable (two possible orders of evaluation), 
<br>
and not, say, all possible orders of evaluation.
<br>
<p>Forcing a confident answer is a huge no-no in FAI theory. If an FAI 
<br>
discovers that an answering procedure is inadequate, you do not force it to 
<br>
produce an answer anyway.
<br>
<p>**
<br>
<p><em>&gt; The task you describe for the FRPOP is the tasks myths would have a 
</em><br>
<em>&gt; fully enlightened, god-level, super-wise being attempt and only then 
</em><br>
<em>&gt; with a lot of cautions.  IMHO, attempting to do this with a un-sentient
</em><br>
<em>&gt; recursively self-improving process is the height of folly.  It seems 
</em><br>
<em>&gt; even more hubristic and difficult than the creation of a &gt;human 
</em><br>
<em>&gt; intelligent sentience.  I don't see why you believe yourself incapable
</em><br>
<em>&gt; of the latter but capable of the former.
</em><br>
<p>I don't know that I am incapable of creating a child.  What I am presently 
<br>
incapable of is understanding the implications, guessing what future 
<br>
humankind will think of the act, whether they will call it child abuse, the 
<br>
significance it would bear in history.  Why am I worried about child abuse? 
<br>
&nbsp;&nbsp;If I have so little knowledge that I cannot even deliberately *not* 
<br>
create a sentient being, then all I can do is probe around blindly. 
<br>
Experience tells me that one who probes around blindly is likely to 
<br>
massively screw up, which, if I succeeded, would be child abuse.
<br>
<p><em>&gt; Now you do back away from such implications somewhat by having volition
</em><br>
<em>&gt; extrapolation be only a first level working solution until &quot;that 
</em><br>
<em>&gt; voliton&quot; evolves and/or builds something better.   But what do you mean
</em><br>
<em>&gt; by &quot;that volition&quot; here?  Is it the FRPOP, what the FRPOP becomes, the
</em><br>
<em>&gt; FRPOP plus humanity plus the extrapolated vision to date or what?  It 
</em><br>
<em>&gt; isn't very clear to me.
</em><br>
<p>Very roughly speaking, it's the FRPOP refracting through humanity to 
<br>
extrapolate what our decision would be in N years, for medium-distance 
<br>
values of N, plus a long-distance veto.
<br>
<p><em>&gt; If the ruleset is scrutable and able to be understood by some, many, 
</em><br>
<em>&gt; most humans then why couldn't humans come up with it?
</em><br>
<p>We're not talking about legal regulations but apparent behaviors of Nature 
<br>
(fixed in the original paper).
<br>
<p><em>&gt; I am not at all sure that our &quot;collective volition&quot; is superior to the 
</em><br>
<em>&gt; very best of our relatively individual volition.
</em><br>
<p>Best under what or whose standard of bestness?  Everyone used to nag me 
<br>
about this question, and while it was annoying that certain people smugly 
<br>
thought it unanswerable, it still needed an answer.  My professional answer 
<br>
is, &quot;the coherent output of an extrapolated collective volition&quot;.  What's 
<br>
your answer?
<br>
<p><em>&gt; Saying it is 
</em><br>
<em>&gt; collective may make it sound more egalitarian, democratic and so on but
</em><br>
<em>&gt; may not have much to do with it actually being best able to guarantee 
</em><br>
<em>&gt; human survival and well-being.  It looks like you were getting toward 
</em><br>
<em>&gt; the same thing in your 2+ days partial recanting/adjustment-wishes.   I
</em><br>
<em>&gt; don't see how &quot;referring the problem back to humanity&quot; is all that 
</em><br>
<em>&gt; likely to solve the problem.   It might however be the best that can be
</em><br>
<em>&gt; done.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think I see that you are attempting to extrapolate beyond the present
</em><br>
<em>&gt; average state of humans and their self-knowledge/stated 
</em><br>
<em>&gt; wishes/prejudices and so on to what they really, really want in their 
</em><br>
<em>&gt; most whole-beyond-idealization core.
</em><br>
<p>I wouldn't call it so much &quot;really, really want&quot; as &quot;would want if we were 
<br>
more grown up&quot; - it's that ephemeral-seeming &quot;grown up&quot; part I'm trying to 
<br>
give a well-specified definition.  Strength of emotion is not sufficient, 
<br>
though strength of emotion counts in the extrapolation.
<br>
<p><em> &gt; I just find it unlikely to near
</em><br>
<em> &gt; absurdity to believe any un-sentient optimizing process, no matter how
</em><br>
<em> &gt; recursively self-improving, will ever arrive there.
</em><br>
<p>Unless the extrapolation is blocked by the requirement that the simulations 
<br>
not be themselves sentient (PAQ 5), I disagree; we are matter and what 
<br>
matter computes, other matter can approximate.  The moral question is &quot;What 
<br>
are we trying to compute?&quot;  As for actually computing it, that is the 
<br>
technical part.
<br>
<p><em>&gt; Where are sections on enforcement of conditions that keep humanity from
</em><br>
<em>&gt; destroying itself?
</em><br>
<p>How am I supposed to know what a collective volition would do in that 
<br>
department?  Do I look like a collective volition?  I can think of what 
<br>
seem like good solutions, but so what?
<br>
<p><em>&gt; What if the collective volition leads to 
</em><br>
<em>&gt; self-destruction or the destruction of other sentient beings?
</em><br>
<p>Because that's what we want?  Because that's what those other rotten 
<br>
bastards want?  Because the programmers screwed up the initial dynamic?
<br>
<p>What if super-Gandhi starts letting the air out of car's tires?  What if 
<br>
toaster ovens grow wings?  I need you to give me a reason for the failure; 
<br>
it will always be possible to postulate the blank fact of failure without 
<br>
giving a reason, and then shake one's finger at the evil collective 
<br>
volition in that hypothetical scenario.
<br>
<p><em>&gt; But more
</em><br>
<em>&gt; importantly, what does the FAI protect us from and how is it intended
</em><br>
<em>&gt; to do so?
</em><br>
<p>What's all this business about protecting?  Is that what we &quot;want&quot;?
<br>
<p><em>&gt; Section 6 is very useful.  You do not want to build a god but you want
</em><br>
<em>&gt; to enshrine the &quot;true&quot; vox populi, vox Dei.  It is interesting in that
</em><br>
<em>&gt; the vox populi is the extrapolation of the volition of the people and
</em><br>
<em>&gt; in that manner a reaching for the highest within human desires and 
</em><br>
<em>&gt; potentials.   This is almost certainly the best that can be done by and
</em><br>
<em>&gt; for humans including those building an FAI.   But the question is 
</em><br>
<em>&gt; whether that is good enough to create that which to some (yet to be 
</em><br>
<em>&gt; specified extent) enforces or nudges powerfully toward that collective 
</em><br>
<em>&gt; volition.   Is there another choice?  Perhaps not.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; A problem is whether the most desirable volition is part of the 
</em><br>
<em>&gt; collective volition or relatively rare.  A rare individual or group of 
</em><br>
<em>&gt; individuals' vision may be a much better goal and may perhaps even be 
</em><br>
<em>&gt; what most humans eventually have as their volition when they get wise 
</em><br>
<em>&gt; enough, smart enough and so on.   If so then collective volition is not
</em><br>
<em>&gt; sufficient.  Judgment of the best volition must be made to get the best
</em><br>
<em>&gt; result.  Especially if the collective volition at this time is adverse
</em><br>
<em>&gt; to the best volition and if the enforcement of the collective volition,
</em><br>
<em>&gt; no matter how gentle, might even preclude the better volition.   Just 
</em><br>
<em>&gt; because being able to judge is hard and fraught with danger doesn't mean
</em><br>
<em>&gt; it is necessarily better not to do so.
</em><br>
<p>Now added as PAQ 8:
<br>
<p>**
<br>
<p>Q8. A problem is whether the most desirable volition is part of the 
<br>
collective volition or relatively rare. A rare individual or group of 
<br>
individuals' vision may be a much better goal and may perhaps even be what 
<br>
most humans eventually have as their volition when they get wise enough, 
<br>
smart enough and so on. (SamanthaAtkins)
<br>
<p>A8. &quot;Wise enough, smart enough&quot; - this is just what I'm trying to describe 
<br>
specifically how to extrapolate! &quot;What most humans eventually have as their 
<br>
decision when they get wise enough, smart enough, and so on&quot; is exactly 
<br>
what their collective volition is supposed to guess.
<br>
<p>It is you who speaks these words, &quot;most desirable volition&quot;, &quot;much better 
<br>
goal&quot;. How does the dynamic know what is the &quot;most desirable volition&quot; or 
<br>
what is a &quot;much better goal&quot;, if not by looking at you to find the initial 
<br>
direction of extrapolation?
<br>
<p>How does the dynamic know what is &quot;wiser&quot;, if not by looking at your 
<br>
judgment of wisdom? The order of evaluation cannot be recursive, although 
<br>
the order of evaluation can iterate into a steady state. You cannot ask 
<br>
what definition of flongy you would choose if you were flongier. You can 
<br>
ask what definition of wisdom you would choose if you knew more, thought 
<br>
faster. And then you can ask what definition of wisdom-2 your wiser-1 self 
<br>
would choose on the next iteration (keeping track of the increasing 
<br>
distance). But the extrapolation must climb the mountain of your volition 
<br>
from the foothills of your current self; your volition cannot reach down 
<br>
like a skyhook and lift up the extrapolation.
<br>
<p>It is a widespread human perception that some individuals are wiser and 
<br>
kinder than others. Suppose our collective volition does decide to weight 
<br>
volitions by wisdom and kindness - a suggestion I strongly dislike, for it 
<br>
smacks of disenfranchisement. It would still take a majority vote of 
<br>
extrapolated volitions for the initial dynamic to decide how to judge 
<br>
&quot;wisdom&quot; and &quot;kindness&quot;. I don't think it wise to tell the initial dynamic 
<br>
to look to whichever humans judge themselves as wiser and kinder. And if 
<br>
the programmers define their own criteria of &quot;wisdom&quot; and &quot;kindness&quot; into a 
<br>
dynamic's search for leaders, that is taking over the world by proxy. (You 
<br>
wouldn't want the al-Qaeda programmers doing that, right? Though it might 
<br>
work out all right in the end, so long as the terrorists told the initial 
<br>
dynamic to extrapolate their selected wise men.)
<br>
<p>If we know that we are not the best, then let us extrapolate our volitions 
<br>
in the direction of becoming more the people we wish we were, not 
<br>
concentrate Earth's destiny into the hands of our &quot;best&quot;.  What if our best 
<br>
are not good enough?  We should need to extrapolate them farther.  If so, 
<br>
why not extrapolate everyone?
<br>
<p>**
<br>
<p><em>&gt; The earth is NOT scheduled to go to &quot;vanish in a puff of smiley faces&quot;.
</em><br>
<em>&gt; I very much do not agree that that is the only alternative to FAI.
</em><br>
<p>I don't know the future, but I know the technical fact that it looks easier 
<br>
than I would have hoped to build things that go FOOM, and harder than I 
<br>
would have hoped to make something sorta nice, and most people aren't even 
<br>
interested in rising to the challenge.  Maybe the Earth isn't scheduled to 
<br>
vanish in a puff of smiley faces, but it's my best guess.
<br>
<p><em>&gt; Q1's answer is no real answer.  The answer today is we have no friggin'
</em><br>
<em>&gt; idea how to do this.
</em><br>
<p>Speak for yourself, albeit today I only have specific ideas as to how to do 
<br>
very simple operations of this kind.  (Q1:  How do you &quot;extrapolate&quot; what 
<br>
we would think if we knew more, thought faster/smarter?)  Not knowing how 
<br>
to do something is a temporary state of mind that, in my experience, tends 
<br>
to go away over time.  Having no clue what you *want* to do is a much 
<br>
severer problem, and so I call this progress.
<br>
<p><em>&gt; I am not into blaming SIAI and find it amazing you would toss this in.
</em><br>
<p>The meaning of &quot;blame&quot; in that case was meant as deliberate irony.  I have 
<br>
changed and expanded PAQ 2; it now reads:
<br>
<p>**
<br>
<p>Q2. Removing the ability of humanity to do itself in and giving it a much 
<br>
better chance of surviving Singularity is of course a wonderful goal. But 
<br>
even if you call the FAI &quot;optimizing processes&quot; or some such it will still 
<br>
be a solution outside of humanity rather than humanity growing into being 
<br>
enough to take care of its problems. Whether the FAI is a &quot;parent&quot; or not 
<br>
it will be an alien &quot;gift&quot; to fix what humanity cannot. Why not have 
<br>
humanity itself recursively self-improve? (SamanthaAtkins)
<br>
<p>A2. For myself, the best solution I can imagine at this time is to make 
<br>
collective volition our Nice Place to Live, not forever, but to give 
<br>
humanity a breathing space to grow up. Perhaps there is a better way, but 
<br>
this one still seems pretty good. As for it being a solution outside of 
<br>
humanity, or humanity being unable to fix its own problems... on this one 
<br>
occasion I say, go ahead and assign the moral responsibility for the fix to 
<br>
the Singularity Institute and its donors.
<br>
<p>Moral responsibility for specific choices of a collective volition is hard 
<br>
to track down, in the era before direct voting. No individual human may 
<br>
have formulated such an intention and acted with intent to carry it out. 
<br>
But as for the general fact that a bunch of stuff gets fixed, the 
<br>
programming team and SIAI's donors are human and it was their intention 
<br>
that a bunch of stuff get fixed. I should call this a case of humanity 
<br>
solving its own problems, if on a highly abstract level.
<br>
<p>**
<br>
<p><em>&gt; Q4's answer disturbs me.  if there are &quot;inalienable rights&quot; it is not 
</em><br>
<em>&gt; because someone or other has the opinion that such rights exists.
</em><br>
<p>Precisely; inalienable rights are just those rights of which our opinion is 
<br>
that the rights exist independently of our opinions.
<br>
<p>Confusing, yes.  It took my mighty intellect six years (1996-2002) to sort 
<br>
out the tangle.
<br>
<p><em>&gt; It is because the nature of human beings is not utterly mutable
</em><br>
<p>That's a moral judgment.  Not a physical impossibility.
<br>
<p><em>&gt; and this
</em><br>
<em>&gt; fixed nature leads to the conclusion that some things are required for
</em><br>
<em>&gt; human well-functioning.
</em><br>
<p>Required by what standard?  Well-functioning by what standard?  Why is 
<br>
&quot;required for human well-functioning&quot; the source of inalienable rights?  I 
<br>
am not smugly calling the questions unanswerable, I am asking where to pick 
<br>
up the information and the exact specifications.
<br>
<p><em>&gt; These things that are required by the nature
</em><br>
<em>&gt; of humans are &quot;inalienable&quot; in that they are not the made-up opinions of
</em><br>
<em>&gt; anyone or some favor of some governmental body or other.
</em><br>
<p>Yes, that is why the *successor dynamic* might include a Bill of Rights 
<br>
independent of the majority opinion, because the majority *felt* - not 
<br>
decided, but felt - that these rights existed independently of the majority 
<br>
opinion.
<br>
<p><em>&gt; As such 
</em><br>
<em>&gt; these true inalienable rights should grow straight out of Friendliness 
</em><br>
<em>&gt; towards humanity.
</em><br>
<p>Who defines Friendliness?
<br>
<p>Okay, now I'm just being gratuitously evil.  It's just that people have 
<br>
been torturing me with that question for years, and when I saw the 
<br>
opportunity to strike, I just had to do it; I'm sure you understand. 
<br>
&quot;Where does the information of Friendliness come from?&quot; is the non-evil 
<br>
form of the question.
<br>
<p>Under the information flow of collective volition, an inalienable right 
<br>
would arise from a majority perception like yours, that the right existed 
<br>
independently of anyone's opinion.  Hast thou a better suggestion?
<br>
<p><em>&gt; Your answer also mixes freely current opinions of the majority of human
</em><br>
<em>&gt; kind and actual collective volition.  It seems rather sloppy.
</em><br>
<p>Can you point out where the mixing occurs?
<br>
<p><em>&gt; That's all for now.  I am taking a break from computers for the next 4 
</em><br>
<em>&gt; days or so.   So I'll catch up next weekend.
</em><br>
<p>See ya.  Thanks for dialoguing!
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8906.html">Eliezer Yudkowsky: "Re: FAI: Collective Volition"</a>
<li><strong>Previous message:</strong> <a href="8904.html">Metaqualia: "Re: FAI: Collective Volition"</a>
<li><strong>In reply to:</strong> <a href="8901.html">Samantha Atkins: "Re: FAI: Collective Volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8915.html">Peter Voss: "Summary of current FAI thought"</a>
<li><strong>Reply:</strong> <a href="8915.html">Peter Voss: "Summary of current FAI thought"</a>
<li><strong>Reply:</strong> <a href="8921.html">Mike: "RE: FAI: Collective Volition"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8905">[ date ]</a>
<a href="index.html#8905">[ thread ]</a>
<a href="subject.html#8905">[ subject ]</a>
<a href="author.html#8905">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
