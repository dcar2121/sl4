<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Collective Volition: Wanting vs Doing.</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Collective Volition: Wanting vs Doing.">
<meta name="Date" content="2004-06-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Collective Volition: Wanting vs Doing.</h1>
<!-- received="Sun Jun 13 15:29:46 2004" -->
<!-- isoreceived="20040613212946" -->
<!-- sent="Sun, 13 Jun 2004 14:29:38 -0700" -->
<!-- isosent="20040613212938" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Collective Volition: Wanting vs Doing." -->
<!-- id="C6033C2E-BD80-11D8-85E5-000A95B1AFDE@objectent.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="40CC6B30.7000201@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Collective%20Volition:%20Wanting%20vs%20Doing."><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Sun Jun 13 2004 - 15:29:38 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="9196.html">Samantha Atkins: "Re: About &quot;safe&quot; AGI architecture"</a>
<li><strong>Previous message:</strong> <a href="9194.html">mike99: "Ev Psych &amp; AI (was RE: META: One-week cool-off period)"</a>
<li><strong>In reply to:</strong> <a href="9180.html">Eliezer Yudkowsky: "Re: Collective Volition: Wanting vs Doing."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9197.html">Eliezer Yudkowsky: "Re: Collective Volition: Wanting vs Doing."</a>
<li><strong>Reply:</strong> <a href="9197.html">Eliezer Yudkowsky: "Re: Collective Volition: Wanting vs Doing."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9195">[ date ]</a>
<a href="index.html#9195">[ thread ]</a>
<a href="subject.html#9195">[ subject ]</a>
<a href="author.html#9195">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Jun 13, 2004, at 7:56 AM, Eliezer Yudkowsky wrote:
<br>
<p><em>&gt; Samantha Atkins wrote:
</em><br>
<em>&gt;&gt; On Jun 12, 2004, at 6:57 PM, Eliezer Yudkowsky wrote:
</em><br>
<em>&gt;&gt;&gt; This question does appear to keep popping up.  Roughly, a collective 
</em><br>
<em>&gt;&gt;&gt; volition is what I get when:
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; (a) I step back and ask the meta-question of how I decided an 
</em><br>
<em>&gt;&gt;&gt; earlier Eliezer's view of &quot;Friendliness&quot; was &quot;mistaken&quot;.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; (b) I apply the same meta-question to everyone else on the planet.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; Whatever it is that you use, mentally, to consider any alternative 
</em><br>
<em>&gt;&gt;&gt; to collective volition, anything that would be of itself friendlier 
</em><br>
<em>&gt;&gt;&gt; - that's you, a human, making the decision; so now imagine that we 
</em><br>
<em>&gt;&gt;&gt; take you and extrapolate you re-making that decision at a higher 
</em><br>
<em>&gt;&gt;&gt; level of intelligence, knew more, thought faster, more the person 
</em><br>
<em>&gt;&gt;&gt; etc.
</em><br>
<em>&gt;&gt; Yes, I get that and it is enticing.  But precisely how will the FRPOP 
</em><br>
<em>&gt;&gt; gets its bearings as to what is the &quot;direction&quot; of &quot;more the person&quot;? 
</em><br>
<em>&gt;&gt;  Some of the others are a bit problematic too.  But this one seems 
</em><br>
<em>&gt;&gt; the best and central trick.  More the person I would like to be?  I, 
</em><br>
<em>&gt;&gt; with all my warts?  Wouldn't I have a perhaps badly warped view of 
</em><br>
<em>&gt;&gt; what kind of person I would like to be?  Would the person I would 
</em><br>
<em>&gt;&gt; like to be make indeed better choices?   How will the AI know of this 
</em><br>
<em>&gt;&gt; person or model this person?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Samantha, you write that you might have a badly warped view of what 
</em><br>
<em>&gt; kind of person you would like to be.  &quot;Badly warped&quot; by what criterion 
</em><br>
<em>&gt; that I feed to the FAI?  Your criterion?  Someone else's?  Where am I 
</em><br>
<em>&gt; supposed to get this information, if not, somehow, from you?  When you 
</em><br>
<em>&gt; write down exactly how the information is supposed to get from point A 
</em><br>
<em>&gt; (you) to point B (the FAI), and what the FAI does with the information 
</em><br>
<em>&gt; once it's there, you'll have something that looks like - surprise! - a 
</em><br>
<em>&gt; volition-extrapolating dynamic.  It's not a coincidence.  That's where 
</em><br>
<em>&gt; the idea of a volition-extrapolating dynamic *originally comes from*.
</em><br>
<em>&gt;
</em><br>
<p>That is my point.  The information is not necessarily available from 
<br>
the person[s] in sufficient quality to make wise decisions that 
<br>
actually work for the good of humanity.
<br>
<p><em>&gt;&gt;&gt; The benefit of CV is that (a) we aren't stuck with your decision 
</em><br>
<em>&gt;&gt;&gt; about Friendliness forever (b) you don't have to make the decision 
</em><br>
<em>&gt;&gt;&gt; using human-level intelligence.
</em><br>
<em>&gt;&gt; Well, we don't make the decision at all it seems to me.  The AI does 
</em><br>
<em>&gt;&gt; based on its extrapolation of our idealized selves.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Not precisely, but it's closer than saying that we would make the 
</em><br>
<em>&gt; decision using (EEK!) human-level intelligence.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; I am not sure exactly what our inputs would be.  What do you have in 
</em><br>
<em>&gt;&gt; mind?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Our inputs would be our current selves, from which the decision of a 
</em><br>
<em>&gt; future humankind might be predicted.
</em><br>
<p>I hear you but I am doubtful it can both be done and generate human 
<br>
well-being.
<br>
<p><em>&gt;
</em><br>
<em>&gt; In the spiritual exercise your idealized self will never be any 
</em><br>
<em>&gt; smarter than you are, never know anything you don't.
</em><br>
<p>That is not helpfully so.  Taking the time to invoke the &quot;higher self&quot; 
<br>
and see through its eyes is a lot smarter than when we don't take the 
<br>
time.
<br>
<p><em>&gt;  It will say things that sound wise to your current self - things a 
</em><br>
<em>&gt; village elder might say - things you might severely disagree with if 
</em><br>
<em>&gt; you did know more, think smarter.  Can you ask your idealized 
</em><br>
<em>&gt; spiritual self to build a nanobot?
</em><br>
<p>Yes and no.   The idealized self and invoking others to see from their 
<br>
idealized self is far more likely to devote the energies necessary to 
<br>
build a nanobot and not misuse the technology than otherwise.
<br>
<p><em>&gt;   I don't trust the notion of *spiritual* extrapolation for grounding; 
</em><br>
<em>&gt; I think that's the wrong direction.
</em><br>
<p>Well, ok.  But you do a fine job of something that looks very much like 
<br>
it.
<br>
<p><em>&gt; The word &quot;spirituality&quot; makes people people go warm and fuzzy, and yes 
</em><br>
<em>&gt; we need the warm fuzzies, but I think that if we took people and 
</em><br>
<em>&gt; filtered out everything that a member of the Fluffy Bunny Coven would 
</em><br>
<em>&gt; call &quot;unspiritual&quot;, we'd end up with unhumans.
</em><br>
<p>Do not mistake the ineffectual type of bliss-babies (or more often 
<br>
would-be bliss babies) for the extend of spirituality and what it 
<br>
offers.  Some of the most spiritual people I know are also some of the 
<br>
most hard-headed and realistic.
<br>
<p><p><em>&gt; It's more an order-of-evaluation question than anything else.  I 
</em><br>
<em>&gt; currently guess that one needs to evaluate some &quot;knew more&quot; and 
</em><br>
<em>&gt; &quot;thought faster&quot; before evaluating &quot;more the people we wished we 
</em><br>
<em>&gt; were&quot;.  Mostly because &quot;knew more&quot; and &quot;thought faster&quot; starting from 
</em><br>
<em>&gt; a modern-day human who makes fluffy bunny errors doesn't have quite 
</em><br>
<em>&gt; the same opportunity to go open-endedly recursively wrong as &quot;more the 
</em><br>
<em>&gt; people we wished we were&quot; evaluated on a FBer.
</em><br>
<p>Well, we all can make whatever assumptions we wish for what &quot;knew more&quot; 
<br>
and &quot;thought faster&quot; would and would not remove or add to in our bag of 
<br>
human characteristics.
<br>
<p><em>&gt;
</em><br>
<em>&gt; One obvious rule for order-of-evaluation would be to define a metric 
</em><br>
<em>&gt; of distance (difficulty of explanation to current self) and carry out 
</em><br>
<em>&gt; shorter-distance extrapolations before longer-distance extrapolations.
</em><br>
<p>Yes, this is always a good way to proceed.
<br>
<p><em>&gt;
</em><br>
<em>&gt;&gt;&gt; Maybe a better metaphor for collective volition would be that it 
</em><br>
<em>&gt;&gt;&gt; refers questions to an extrapolated adult humankind, or to a 
</em><br>
<em>&gt;&gt;&gt; superposition of the adult humanities we might become.
</em><br>
<em>&gt;&gt; So the AI becomes an adjunct and amplifier of a specialized form of 
</em><br>
<em>&gt;&gt; introspective spiritual exercise?  Wild!  AI augmented 
</em><br>
<em>&gt;&gt; self-improvement of humankind.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Right.  AI augmented self-improvement of humankind with the explicit 
</em><br>
<em>&gt; notation that the chicken-and-egg part of this problem is that 
</em><br>
<em>&gt; modern-day humans aren't smart enough to self-improve without stomping 
</em><br>
<em>&gt; all over their own minds with unintended consequences, aren't even 
</em><br>
<em>&gt; smart enough to evaluate the question &quot;What kind of person do you want 
</em><br>
<em>&gt; to be?&quot; over its real experiential consequences rather than a small 
</em><br>
<em>&gt; subset of human verbal descriptions of humanly expected consequences.  
</em><br>
<em>&gt; So rather than creating a *separate* self-improving humane thing, one 
</em><br>
<em>&gt; does something philosophically more complex and profound (but perhaps 
</em><br>
<em>&gt; not more difficult from the standpoint of FAI theory, although it 
</em><br>
<em>&gt; *sounds* a lot harder).  One binds a transparent optimization process 
</em><br>
<em>&gt; to predict what the grownup selves of modern-day humans would say if 
</em><br>
<em>&gt; modern humans grew up together with the ability to self-improve 
</em><br>
<em>&gt; knowing the consequences.  The decision function of the extrapolated 
</em><br>
<em>&gt; adult humanity includes the ability of the collective volition to 
</em><br>
<em>&gt; restrain its own power or rewrite the optimization function to 
</em><br>
<em>&gt; something else; the collective volition extrapolates its awareness 
</em><br>
<em>&gt; that it is just an extrapolation and not our actual decisions.
</em><br>
<em>&gt;
</em><br>
<p>Excellent.  I get it!
<br>
<p><em>&gt; In other words, one handles *only* and *exactly* the chicken-and-egg 
</em><br>
<em>&gt; part of the problem - that modern-day humans aren't smart enough to 
</em><br>
<em>&gt; self-improve to an adult humanity, and that modern-day society isn't 
</em><br>
<em>&gt; smart enough to render emergency first aid to itself - by writing an 
</em><br>
<em>&gt; AI that extrapolates over *exactly those* gaps to arrive at a picture 
</em><br>
<em>&gt; of future humankind if those problems were solved.  Then the 
</em><br>
<em>&gt; extrapolated superposed possible future humankinds, the collective 
</em><br>
<em>&gt; volition, hopefully decides to act in our time to boost us over the 
</em><br>
<em>&gt; chicken-and-egg recursion; doing enough to solve the hard part of the 
</em><br>
<em>&gt; problem, but not annoying us or taking over our lives, since that's 
</em><br>
<em>&gt; not what we want (I think; at least it's not what I want).  Or maybe 
</em><br>
<em>&gt; the collective volition does something else.  I may have phrased the 
</em><br>
<em>&gt; problem wrong.  But for I as an FAI programmer to employ some other 
</em><br>
<em>&gt; solution, such as creating a new species of humane intelligence, would 
</em><br>
<em>&gt; be inelegant; it doesn't solve exactly and only the difficult part of 
</em><br>
<em>&gt; the problem.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I may end up needing to be inelegant, but first I want to try really 
</em><br>
<em>&gt; hard to find a way to do the Right Thing.
</em><br>
<p>Thanks.  That clears up a lot.
<br>
<p><p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="9196.html">Samantha Atkins: "Re: About &quot;safe&quot; AGI architecture"</a>
<li><strong>Previous message:</strong> <a href="9194.html">mike99: "Ev Psych &amp; AI (was RE: META: One-week cool-off period)"</a>
<li><strong>In reply to:</strong> <a href="9180.html">Eliezer Yudkowsky: "Re: Collective Volition: Wanting vs Doing."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="9197.html">Eliezer Yudkowsky: "Re: Collective Volition: Wanting vs Doing."</a>
<li><strong>Reply:</strong> <a href="9197.html">Eliezer Yudkowsky: "Re: Collective Volition: Wanting vs Doing."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#9195">[ date ]</a>
<a href="index.html#9195">[ thread ]</a>
<a href="subject.html#9195">[ subject ]</a>
<a href="author.html#9195">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
