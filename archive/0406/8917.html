<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: FAI: Collective Volition</title>
<meta name="Author" content="Eliezer Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: FAI: Collective Volition">
<meta name="Date" content="2004-06-01">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: FAI: Collective Volition</h1>
<!-- received="Tue Jun  1 14:31:06 2004" -->
<!-- isoreceived="20040601203106" -->
<!-- sent="Tue, 01 Jun 2004 16:31:07 -0400" -->
<!-- isosent="20040601203107" -->
<!-- name="Eliezer Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: FAI: Collective Volition" -->
<!-- id="40BCE78B.2050608@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="6.0.3.0.0.20040601134052.01c46ec0@pop-server.satx.rr.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20FAI:%20Collective%20Volition"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Jun 01 2004 - 14:31:07 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="8918.html">Damien Broderick: "Re: FAI: Collective Volition"</a>
<li><strong>Previous message:</strong> <a href="8916.html">Eliezer Yudkowsky: "Re: Summary of current FAI thought"</a>
<li><strong>In reply to:</strong> <a href="8913.html">Damien Broderick: "Re: FAI: Collective Volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8918.html">Damien Broderick: "Re: FAI: Collective Volition"</a>
<li><strong>Reply:</strong> <a href="8918.html">Damien Broderick: "Re: FAI: Collective Volition"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8917">[ date ]</a>
<a href="index.html#8917">[ thread ]</a>
<a href="subject.html#8917">[ subject ]</a>
<a href="author.html#8917">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Damien Broderick wrote:
<br>
<p><em>&gt; Interesting if somewhat baggy paper. Thanks for posting it!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; At 01:26 PM 6/1/2004 -0400, Eliezer wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; The point is that it's rewritable moral content if the moral content 
</em><br>
<em>&gt;&gt; is not what we want, which I view as an important moral point; that it 
</em><br>
<em>&gt;&gt; gives humanity a vote rather than just me, which is another important 
</em><br>
<em>&gt;&gt; moral point to me personally; and so on.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; My sense was that it gives the system's [mysteriously arrived at] 
</em><br>
<em>&gt; estimate of humanity's [mysteriously arrived at] optimal vote. This, as 
</em><br>
<em>&gt; Aubrey pointed out, is very different, and critical.
</em><br>
<p>True.  I don't trust modern-day humanity any more than I trust modern-day 
<br>
me.  Less, actually, for they don't seem to realize when they need to be 
<br>
scared.  That includes transhumanists.  &quot;Yay, my own source code!  [Modify] 
<br>
aaargh [dies].&quot;
<br>
<p>Incidentally, I suspect that optimal meta-rules call for satisficing votes, 
<br>
and even the meta-rules may end up as satisficing rather than optimal.  I'm 
<br>
trying to figure out how to attach a decent explanation of this to 
<br>
Collective Volition, because the problem would be hugely intractable if you 
<br>
had to extrapolate *the* decision of each individual, rather than get a 
<br>
spread of possible opinions that lets the dynamic estimate *a satisficing 
<br>
decision* for the superposed population.  Another good reason to forge 
<br>
one's own philosophies rather than paying any attention to what that silly 
<br>
collective volition does (if it ends up as our Nice Place to Live).
<br>
<p><em>&gt; By the way, two fragments of the paper stung my attention:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &lt; The dynamics will be choices of mathematical viewpoint, computer 
</em><br>
<em>&gt; programs, optimization targets, reinforcement criteria, and AI training 
</em><br>
<em>&gt; games with teams of agents manipulating billiard balls. &gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'd like to see some.
</em><br>
<p>No can do; I'm still in the middle of working out exactly what I want. 
<br>
Right now I don't even know how to do simple things, let alone something as 
<br>
complicated and dangerous as collective volition.  If I tried I'd go up in 
<br>
a puff of paperclips.  There are N necessary problems and I've solved M 
<br>
problems, M &lt; N.
<br>
<p><em>&gt; &lt; The technical side of Friendly AI is not discussed here. The technical 
</em><br>
<em>&gt; side of Friendly AI is hard and requires, like, actual math and stuff. 
</em><br>
<em>&gt; (Not as much math as I'd like, but yes, there is now math involved.)  &gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'd still like to see some.
</em><br>
<p>Okay, here's one of my prides and joys, of which I've got no clue whether 
<br>
it's original.  Let U(x) be a utility function mapping over final states X. 
<br>
&nbsp;&nbsp;Utility functions produce invariant outcomes when weighted by any 
<br>
constant multiplicative factor, so the total utility of an outcome, or the 
<br>
expected utility of an action, is not a good measure of the strength of an 
<br>
optimization process (although it might be a good measure of the strength 
<br>
of one optimization process relative to another).  Let V(u) be a measure of 
<br>
the volume of states in X with U(x) &gt; u.  As u increases, V(u) decreases, 
<br>
reflecting the smaller volume of states with U(x) &gt; u.  Take the logarithm 
<br>
of the volume v = V(u) measured as a fraction of the total configuration 
<br>
space V, log(v/V).  Take the negation of this quantity, -log(v/V), and call 
<br>
it the information.  We now have a relatively objective measurement of the 
<br>
power of an optimization process, which is the information it produces in 
<br>
outcomes.  The smaller the target it can hit, the more powerful the 
<br>
optimization process.
<br>
<p>(Mitchell Porter helped point out a major and silly error in my original 
<br>
math for this.)
<br>
<p>This is how you'd go about translating something like &quot;self-determination&quot; 
<br>
into &quot;don't optimize individual lives [too heavily]&quot;.  If an AI is 
<br>
extrapolating your possible futures and *maximizing* its expected utility, 
<br>
or its estimate of your volition's utility, then you've got the entire 
<br>
phase space compressed down to a point; a huge amount of information that 
<br>
leaves no room for personal decision except as an expected event in the 
<br>
AI's game plan.
<br>
<p>(This is why FAI theory no longer runs on expected utility maximization, 
<br>
aside from many good reasons to regard maximization as insanely dangerous 
<br>
during an AI's youth.  Actually, I realized while writing &quot;Collective 
<br>
Volition&quot; that I would need to junk expected utility entirely and develop a 
<br>
theory of abstractions from decision systems, but that's a separate story.)
<br>
<p>A satisficing decision system would raise U(x) above some threshold level, 
<br>
but not *maximize* it, leaving the remainder of your life open to your own 
<br>
planning.  Even this might interfere with self-determination too much, 
<br>
which is why I suggested the notion about collective background rules.  You 
<br>
can give people arbitrarily fun superpowers, and so long as the people are 
<br>
still wielding the superpowers themselves, you aren't optimizing their 
<br>
individual lives (by extrapolating their futures and selecting particular 
<br>
outcomes from the set).  You're *improving* their lives, which is what we 
<br>
might call a transform that predictably increases utility by some rough 
<br>
amount.  The heuristics and biases folks show that the important thing for 
<br>
life satisfaction is to make sure that things keep improving.
<br>
<p>Satisficing bindings, improving pressures, preferring biases... there's 
<br>
plenty of options open besides *maximizing* expected utility.  But for FAI 
<br>
to make sense, you have to stop thinking of &quot;utility&quot; as a mysterious 
<br>
valuable substance inherent in objects, and start thinking of optimization 
<br>
processes that steer the future, with a utility function acting as the 
<br>
steering control.
<br>
<p>This is an example of what I mean by a choice of mathematical viewpoint: 
<br>
karma is measured in bits.  For example, someone with four bits of good 
<br>
karma would end up with their space of possible futures compressed to the 
<br>
best sixteenth of the pie, or more subtly end up with their spectrum of 
<br>
possible futures conditional on some action A compressed to the best 
<br>
sixteenth of actions A, in terms of expected utility of A - perhaps leaving 
<br>
all the original futures open, but with a better weighting on the 
<br>
probabilities.  Perhaps humanity will legislate the equivalent of the 
<br>
second law of thermodynamics for karma; you can't end up with four bits of 
<br>
karma without at least four bits of alerted human choice going into 
<br>
creating it.  I offer up this wild speculation as a way of showing that 
<br>
things like &quot;self-determination&quot; can be translated into terms explicable to 
<br>
even a very young AI.  Though as Norm Wilson points out, self-determination 
<br>
is not part of the initial dynamic in collective volition.
<br>
<p>It's not a very intimidating example, I know.  Mostly, the math involved is 
<br>
still plain old-fashioned standard Bayesian decision theory, of which it is 
<br>
simple enough to learn the math as equations, but one must learn to see the 
<br>
math immanent in all things FAI.  It's not much math, but it's enough math 
<br>
to keep out anyone who hasn't received their Bayesian enlightenment; so 
<br>
there, it's a technical discipline, no mortals allowed, yay, I'm a big 
<br>
expert look at my business cards.  Seriously, most of the questions now 
<br>
being tossed around about the implementation mechanism behind collective 
<br>
volition, all the &quot;But that's impossible&quot; and &quot;That's too hard even for a 
<br>
superintelligence&quot; and &quot;Wouldn't the AI do XYZ instead?&quot; would go away if 
<br>
people received their Bayesian enlightenment.  That's why I emphasized that 
<br>
the Collective Volition paper is just about what I want to do, and take my 
<br>
word for it that it looks theoretically possible (except for that one part 
<br>
about preventing sentient simulations which I don't yet understand clearly).
<br>
<p>Regarding optimization processes as compressing outcomes into narrow 
<br>
regions of configuration space gets rid of a lot of the dreck about 
<br>
&quot;intentionality&quot; and all the wonderful properties that people keep wanting 
<br>
to attribute to &quot;intelligence&quot;.  Natural selection produces information in 
<br>
DNA by compressing the sequences into regions of DNA-space with high 
<br>
fitness.  Humans produce tools from narrow regions of tool-space, plans 
<br>
from narrow regions of plan-space.  Paperclip maximizers steer reality into 
<br>
regions containing the maximum possible number of paperclips.  Yes, you can 
<br>
specify a paperclip maximizer mathematically, in fact it's a lot easier 
<br>
than specifying anything nice, and yes, a paperclip maximizer is what you 
<br>
end up with if you don't solve all N problems in FAI.
<br>
<p>I'm sort of sleepy (it nears the end of my current sleep cycle) and I don't 
<br>
know if any of that made sense.  Guess I just wanted to prove I wasn't 
<br>
bluffing before I went to bed.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8918.html">Damien Broderick: "Re: FAI: Collective Volition"</a>
<li><strong>Previous message:</strong> <a href="8916.html">Eliezer Yudkowsky: "Re: Summary of current FAI thought"</a>
<li><strong>In reply to:</strong> <a href="8913.html">Damien Broderick: "Re: FAI: Collective Volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8918.html">Damien Broderick: "Re: FAI: Collective Volition"</a>
<li><strong>Reply:</strong> <a href="8918.html">Damien Broderick: "Re: FAI: Collective Volition"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#8917">[ date ]</a>
<a href="index.html#8917">[ thread ]</a>
<a href="subject.html#8917">[ subject ]</a>
<a href="author.html#8917">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:47 MDT
</em></small></p>
</body>
</html>
