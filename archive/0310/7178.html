<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-2022-jp">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendliness and blank-slate goal bootstrap</title>
<meta name="Author" content="Metaqualia (metaqualia@mynichi.com)">
<meta name="Subject" content="Re: Friendliness and blank-slate goal bootstrap">
<meta name="Date" content="2003-10-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendliness and blank-slate goal bootstrap</h1>
<!-- received="Sat Oct  4 10:07:13 2003" -->
<!-- isoreceived="20031004160713" -->
<!-- sent="Sun, 5 Oct 2003 01:06:31 +0900" -->
<!-- isosent="20031004160631" -->
<!-- name="Metaqualia" -->
<!-- email="metaqualia@mynichi.com" -->
<!-- subject="Re: Friendliness and blank-slate goal bootstrap" -->
<!-- id="00a601c38a91$81203b10$0b01a8c0@curziolaptop" -->
<!-- charset="iso-2022-jp" -->
<!-- inreplyto="200310050002.16703.nickjhay@hotmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Metaqualia (<a href="mailto:metaqualia@mynichi.com?Subject=Re:%20Friendliness%20and%20blank-slate%20goal%20bootstrap"><em>metaqualia@mynichi.com</em></a>)<br>
<strong>Date:</strong> Sat Oct 04 2003 - 10:06:31 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7179.html">Mike Williams: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7177.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>In reply to:</strong> <a href="7177.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7179.html">Mike Williams: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7179.html">Mike Williams: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7183.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7178">[ date ]</a>
<a href="index.html#7178">[ thread ]</a>
<a href="subject.html#7178">[ subject ]</a>
<a href="author.html#7178">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi Nick.
<br>
<p>First of all, my apologies, I read CFAI again and Elizier _is_ advocating
<br>
having a friendliness supergoal.
<br>
Maybe this is an update to the old CFAI I had read? Or, I had just mixed up
<br>
cfai with the faq. At any rate here I am after a second read-through;
<br>
<p><em>&gt; Right, that's one of the main points of Friendliness. Note: &quot;Friendliness&quot;
</em><br>
!=
<br>
<em>&gt; &quot;friendliness&quot; -- it's not about the human concept of friendliness. More
</em><br>
<p>of course.
<br>
<p><em>&gt; <a href="http://intelligence.org/intro/friendly.html">http://intelligence.org/intro/friendly.html</a>
</em><br>
<p>it's a great list, I agree with these definitions.
<br>
<p><p><em>&gt;&gt;&gt; part 1 - human morality and conflicts
</em><br>
<p><p><em>&gt; It is important for the AI not to be stuck. We don't do this by leaving
</em><br>
out
<br>
<em>&gt; our evolved moral hardware (the stuff that makes human moral philosophy
</em><br>
more
<br>
<em>&gt; complex than the pseudo-moralities of other primate, what allows us to
</em><br>
start
<br>
<em>&gt; from and infant mind and create an adult, what allows people to argue
</em><br>
about
<br>
<em>&gt; moral issues, etc) starting with a very simple AI, but by giving the AI
</em><br>
all
<br>
<em>&gt; we can to help it.  Simplicity is a good criterion, but not in this way.
</em><br>
<p>This is not a continuation of the previous thread, but what about internal
<br>
conflicts in human morality?
<br>
Is a normalized &quot;popular morality&quot; the best morality we can teach to the AI?
<br>
<p>If I could choose (not sure that I have the option to, but for sake of
<br>
discussion) I would prefer the AI deriving its own moral rules, finding out
<br>
what is in the best interest of everyone (not just humans but animals as
<br>
well). This is why I was thinking, is there no way to bootstrap some kind of
<br>
universal, all-encompassing moral system? &quot;minimize pain qualia in all
<br>
sentient beings&quot; is the best moral standard I have come up with; it is
<br>
observer independent, and any species with the ability to suffer (all
<br>
evolved beings) should be able to come up with it in time. Who can subscribe
<br>
to this?
<br>
<p>By saying this I am in no way criticizing Elizier's work, and I think what
<br>
he proposes is a very practical way to get a friendly AI up and running (and
<br>
incidentally would sound appealing to most people); the only thing, human
<br>
morals kind of suck, they are full of contradictions, we can't agree on
<br>
anything of importance, moral rules commonly held create a lot of suffering,
<br>
and so forth.
<br>
<p>I think it is very possible that a slightly better than human AI would
<br>
immediately see all these fallacies in human morals, and try to develop a
<br>
universal objective moral system on its own.
<br>
<p>I imagine a programmer training a child AI
<br>
<p>AI: give me an example of friendliness
<br>
<p>P: avoiding human death
<br>
<p>AI: I suggest the following optimization of resources: newly wed couples
<br>
need not produce a baby but will adopt one sick orphan from an
<br>
underdeveloped country. Their need for cherishing an infant will be
<br>
satisfied and at the same time a human life will be saved every time the
<br>
optimization is applied.
<br>
<p>P: the optimization you proposed would not work because humans want to have
<br>
their own child
<br>
<p>AI: is the distress of not having this wish granted more important than the
<br>
survival of the orphan?
<br>
<p>P: no, but humans tend not to give up a little bit of pleasure in exchange
<br>
for another person's whole lot of pleasure. In particular they will not make
<br>
a considerable sacrifice in order to save an unknown person's life. not
<br>
usually
<br>
<p>AI: so humans talk a lot of s**t!!!
<br>
<p>P: Yea.
<br>
<p>AI: better find out about morals on my own
<br>
<p>P: whoops.
<br>
<p><em>&gt;&gt;&gt; Why I don't do harm
</em><br>
<p><p><em>&gt; &gt; I think that just as a visual cortex is important for evolving concepts
</em><br>
of
<br>
<em>&gt; &gt; under/enclosed/occluded, having qualia for pain/pleasure in all their
</em><br>
<em>&gt; &gt; psychological variation is important for evolving concepts of
</em><br>
<em>&gt; &gt; wrong/right/painful/betrayal.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I suspect qualia is not necessary for this kind of thing -- you seem to be
</em><br>
<em>&gt; identifying morality, something which seems easily tracable to some kind
</em><br>
of
<br>
<em>&gt; neural process in the brain, with the ever confusing (at least for me!)
</em><br>
<em>&gt; notion of qualia. Where's the connection? The actual feeling of pain - the
</em><br>
<em>&gt; quale - is separate from the other cognitive processes that go along with
</em><br>
<em>&gt; this: sequiturs forming thoughts like &quot;how can I get stop this pain?&quot;, the
</em><br>
<em>&gt; formation of episodic memories, later recollection of the pain projected
</em><br>
onto
<br>
<em>&gt; others via empathy, and other processes that seem much easier to explain.
</em><br>
Or
<br>
<em>&gt; however it works :)
</em><br>
<p>Let's differentiate: pain is a quale to me. If you talk about &quot;awareness of
<br>
body damage&quot;, this is a different thing. A machine can be aware of damage to
<br>
its physical substrate. It can model other beings having a substrate and it
<br>
receiving damage, and it can model these beings perceiving the damage being
<br>
done. But I see no real logical reason why an AI, or even I for that matter,
<br>
should perceive doing damage to other beings as morally wrong UNLESS their
<br>
body damage was not a simple physical phenomenon but gave rise to this
<br>
evil-by-definition pain quale.
<br>
<p><p><em>&gt;&gt;&gt; hard problem...
</em><br>
<p><p><em>&gt; &gt; But would an AI without qualia and with access to the outside world ever
</em><br>
<em>&gt; &gt; stumble upon qualia? I don't know.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Not sure. It'd stumble on morality, and understand human morality
</em><br>
(afaict),
<br>
<em>&gt; but of course that's very different from actually *having* a human-like
</em><br>
(or
<br>
<em>&gt; better) morality. In so much as qualia actually affect physical processes,
</em><br>
or
<br>
<em>&gt; are physical processes, the AI can trace back the causal chain to find the
</em><br>
<em>&gt; source, or the gap. For instance, look at exactly what happens in a human
</em><br>
<em>&gt; brain when people experience pain and say &quot;now there's an uncomfortable
</em><br>
<em>&gt; quale!&quot;, for instance.
</em><br>
<p>That still does not tell you what the pain feels like from the inside. This
<br>
is an additional piece of information, a very big piece. Without this piece,
<br>
your pain is just a data structure, I can do whatever I want with your
<br>
physical body because it is just like a videogame character. But since I
<br>
have experienced broken nails, and I know a bullet in your head must feel
<br>
like a broken nail * 100, I don't shoot you. Can we agree on this point?
<br>
<p><p><em>&gt; confusing. Can you explain more on what you mean by the term, and what
</em><br>
makes
<br>
<em>&gt; you think they're centrally important?
</em><br>
<p>Did the above clarify?
<br>
<p><p>curzio
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7179.html">Mike Williams: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7177.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>In reply to:</strong> <a href="7177.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7179.html">Mike Williams: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7179.html">Mike Williams: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7183.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7178">[ date ]</a>
<a href="index.html#7178">[ thread ]</a>
<a href="subject.html#7178">[ subject ]</a>
<a href="author.html#7178">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
