<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-2022-jp">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendliness and blank-slate goal bootstrap</title>
<meta name="Author" content="Nick Hay (nickjhay@hotmail.com)">
<meta name="Subject" content="Re: Friendliness and blank-slate goal bootstrap">
<meta name="Date" content="2003-10-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendliness and blank-slate goal bootstrap</h1>
<!-- received="Mon Oct  6 00:00:37 2003" -->
<!-- isoreceived="20031006060037" -->
<!-- sent="Mon, 6 Oct 2003 18:56:32 +1300" -->
<!-- isosent="20031006055632" -->
<!-- name="Nick Hay" -->
<!-- email="nickjhay@hotmail.com" -->
<!-- subject="Re: Friendliness and blank-slate goal bootstrap" -->
<!-- id="200310061856.35460.nickjhay@hotmail.com" -->
<!-- charset="iso-2022-jp" -->
<!-- inreplyto="014701c38b0b$51710b90$0b01a8c0@curziolaptop" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Nick Hay (<a href="mailto:nickjhay@hotmail.com?Subject=Re:%20Friendliness%20and%20blank-slate%20goal%20bootstrap"><em>nickjhay@hotmail.com</em></a>)<br>
<strong>Date:</strong> Sun Oct 05 2003 - 23:56:32 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7190.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7188.html">Aaron McBride: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>In reply to:</strong> <a href="7185.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0401/7710.html">Charles Hixson: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7189">[ date ]</a>
<a href="index.html#7189">[ thread ]</a>
<a href="subject.html#7189">[ subject ]</a>
<a href="author.html#7189">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Metaqualia wrote:
<br>
<em>&gt; Can you give me some examples of metamoral guidelines that most people can
</em><br>
<em>&gt; relate to?
</em><br>
<em>&gt; &quot;Reduce undesirable qualia, increase desirable ones&quot; is the only really
</em><br>
<em>&gt; universal metamoral rule that I can think of. Formulating it seems to
</em><br>
<em>&gt; require qualia.
</em><br>
<p>Hm, that's more of a moral rule than a metamoral rule. Metamoral forces are 
<br>
more like empathy, observer symmetry, simplicity. Hmm, I might be mixing this 
<br>
up with simple moral principles too. Have a look at shaper/anchor semantics 
<br>
in CFAI.
<br>
<p><em>&gt; &gt; Instead of thinking about what kind of morality an AI should start with
</em><br>
<em>&gt; &gt; have,
</em><br>
<em>&gt; &gt; and then transferring it over, why not jump back a step? Transfer your
</em><br>
<em>&gt; &gt; ability to think &quot;what kind of morality should an AI start with?&quot; so the
</em><br>
<em>&gt; &gt; AI
</em><br>
<em>&gt; &gt; itself can make sure you got it right? It seems like you're forced into
</em><br>
<em>&gt; &gt; giving very simple inadequate verbal phrases (hiding huge amounts of
</em><br>
<em>&gt; &gt; conceputal complexity) to describe morality. You're losing too much in
</em><br>
<em>&gt; &gt; the compression.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I know what you are saying, and I agree completely. The AI should have the
</em><br>
<em>&gt; tools to build its own ideas on everything, it should not follow what we
</em><br>
<em>&gt; say to the letter, both because that would not enable it to go beyond what
</em><br>
<em>&gt; we have thought up and also because what we tell it is likely to be wrong
</em><br>
<em>&gt; at least in some respects.
</em><br>
<em>&gt;
</em><br>
<em>&gt; What I am asking myself is: &quot;is qualia-enabled empathy a necessary
</em><br>
<em>&gt; ingredient to developing a desire for a universal morality?&quot;
</em><br>
<p>It might be necessary for the finer details, I don't understand qualia, but it 
<br>
seems unnecessary for simple empathy. But then I don't understand empathy 
<br>
well enough to code it, so... :)
<br>
<p><em>&gt; I think it is, but I'll be glad to be proved wrong, since I have no clue
</em><br>
<em>&gt; how to qualia-enable an AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; AI: So perhaps I should focus more on the moralities people think they or
</em><br>
<em>&gt; &gt; wish
</em><br>
<em>&gt; &gt; they had, the principles they proclaim (fairness, equality), rather than
</em><br>
<em>&gt; &gt; the
</em><br>
<em>&gt; &gt; actual actions they take?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Seems like a fair counter-argument to my imaginary dialogue.
</em><br>
<em>&gt; What about conflicting principles? 
</em><br>
<p>Well, in this case you do what humans do - either find a way around the 
<br>
contradiction (high technology is a great way to do that), balance the two 
<br>
conflicting principles, or something else. 
<br>
<p><em>&gt; The solution would of course be inventing nanotechnology, abolishing
</em><br>
<em>&gt; private property and creating a world where money is not needed; revising
</em><br>
<em>&gt; human mating strategies so that betrayal no longer happens; revise sexual
</em><br>
<em>&gt; reproduction so that children and parents can never have conflicting
</em><br>
<em>&gt; interests. Or more simply, upload everyone, merge consciousness, and in one
</em><br>
<em>&gt; stroke erase all human dilemmas.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But you see that while these extraordinary changes would benefit everyone,
</em><br>
<em>&gt; if proposed, would suffer lots of antagonism, since people view such
</em><br>
<em>&gt; dramatic changes brought about by an external force as a threat to their
</em><br>
<em>&gt; free will.
</em><br>
<p>Then these changes should be made less dramatic, where possible. It's 
<br>
important for people to understand their world.
<br>
<p><em>&gt; I could go on forever finding cases in which human values conflict and the
</em><br>
<em>&gt; only way to make them not conflict would be a deep restructuring of the
</em><br>
<em>&gt; world. What are your thoughts on this? I think that whatever restructuring
</em><br>
<em>&gt; the AI deems valuable, should be done. After all the greatest benefit in
</em><br>
<em>&gt; having a smarter friend is not having him help you DO what you want (hold
</em><br>
<em>&gt; the wall while I bang my head); but having him TELL you what you really
</em><br>
<em>&gt; want (stop it).
</em><br>
<p>Both are subcases of the AI doing what you really want, it seems. The only way 
<br>
obvious is deep restructuring of the world, perhaps, but it may be possible 
<br>
to do this in a way that doesn't disturb anyone. Maybe. This is, clearly, not 
<br>
a problem specifically of AI, but interesting none-the-less.
<br>
<p><em>&gt; &gt; Or, they had a complex moral system which negatively valued pain. Then
</em><br>
<em>&gt; &gt; the system could argue about how pain is bad isn't nice &quot;other sentients
</em><br>
<em>&gt; &gt; very much dislike experiencing pain. well, most of them&quot;, and could take
</em><br>
<em>&gt; &gt; actions
</em><br>
<em>&gt; &gt; to reduce it. This is indepedent of it &quot;really experiencing&quot; pain, or
</em><br>
<em>&gt; &gt; even reacting to pain in the way humans do (when you're in a lot of pain
</em><br>
<em>&gt; &gt; your mind
</em><br>
<em>&gt; &gt; is kind of crippled -- I don't think this is either a necessary or
</em><br>
<em>&gt; &gt; desirable property of minds).
</em><br>
<em>&gt;
</em><br>
<em>&gt; Good points. My first reactions:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1. how could a strictly logical system reason that pain isn't nice? After
</em><br>
<em>&gt; all, a software bug isn't nice, as it represent the failure of an
</em><br>
<em>&gt; information processing system to reach its goal. Waves hitting the shore
</em><br>
<em>&gt; are the failure of the system [water+wind] to carry out an intrinsic
</em><br>
<em>&gt; program (move water in the direction of the wind) which can be seen as a
</em><br>
<em>&gt; perfectly valid goal, so having a shore somewhere is not nice. What makes
</em><br>
<em>&gt; our pain unique is that it feels like something isn't it? [and just in case
</em><br>
<em>&gt; waves suffer when they hit the shore, luckily we'll never know about it,
</em><br>
<em>&gt; because if it were so then the destruction of the universe would be the
</em><br>
<em>&gt; only way of reducing pain and not doing it would be morally unjustifiable]
</em><br>
<p>An AI is not a strictly logical system. I don't think you can reason that 
<br>
&quot;pain is nice&quot; from a strictly logical vacuum. 
<br>
<p>Most general information processing systems don't have goals except as we 
<br>
anthropomorphise them. A software bug is only bad from our point of view, the 
<br>
system doesn't reflectively examine itself and see a bug as detracting from 
<br>
its goals. You need a particular kind of system to have goals anything like 
<br>
the purpose we imagine the system to have.
<br>
<p>Here's an example of a system that (I think) roughly has some interesting 
<br>
kinds of goal-oriented behaviour. Suppose we have a reliable way to identify 
<br>
the overall pain in a given universe, and we have a prediction system that 
<br>
accurately models the universe (give it infinite computing power, or so). 
<br>
Then, given that model of the universe, it runs a prediction about the 
<br>
consequences of all possible actions it could take, runs the pain-locator on 
<br>
each state, and picks the action which leads to the least pain. This system 
<br>
should, given enough computational power and an accurate model of pain, lead 
<br>
the universe into painfree zones. If this physical system used computations 
<br>
would could identify with reasoning, and had some kind of reflectivity, and a 
<br>
bunch of other things, it'd see it as 'obvious' that pain should be reduced.
<br>
<p>(Of course this is a highly dangerous and physically unrealistic model, so 
<br>
don't try this.)
<br>
<p><em>&gt; 2. Of course I am not suggesting that we implement an AI that constantly
</em><br>
<em>&gt; gets depressed, has back aches and so forth, but if (and it is only an if
</em><br>
<em>&gt; for now) understanding pain subjectively is a prerequisite for wanting to
</em><br>
<em>&gt; prevent others from feeling pain (and the only way to develop a desire for
</em><br>
<em>&gt; a universal moral system) then the AI should have at least one painful
</em><br>
<em>&gt; experience, and store it somewhere for recall. Or wait until it does before
</em><br>
<em>&gt; it decides on moral/metamoral matters.
</em><br>
<p>Sure, if it's necessary. If the AI doesn't seem to understand things without 
<br>
it, you might have to engineer a pain/pleasure architecture for empathy's 
<br>
sake.
<br>
<p><em>&gt; So this AI would help beings who want to stop feeling pain but not beings
</em><br>
<em>&gt; who do not want to stop feeling pain (for ideological reasons for example).
</em><br>
<em>&gt; Seems good at least for normal beings who are not too depressed, too stupid
</em><br>
<em>&gt; or too ideologically involved in something for wanting to help themselves.
</em><br>
<em>&gt; What about these exceptions? It's not like pain does not feel bad to them.
</em><br>
<em>&gt; But they have the further handicap of not being able to work toward a
</em><br>
<em>&gt; solution. Should the AI help them too? 
</em><br>
<p>Yes, help should be given to all. It is very tricky to determine what's 
<br>
helpful, and what a particular mind really wants, especially in unusual cases 
<br>
like that.
<br>
<p><em>&gt; What is the absolute evil, not being
</em><br>
<em>&gt; able to realize one's wishes, or the subjective agony that the laws of
</em><br>
<em>&gt; physics can create? I vote for the second, and think that a transhuman AI
</em><br>
<em>&gt; should enforce compulsory heaven :) After all, nobody would want to go back
</em><br>
<em>&gt; after they were &quot;given a hand&quot;. I realize this sounds like a violation of
</em><br>
<em>&gt; one's freedom, or some crackhead futuristic utopian scenario, but if
</em><br>
<em>&gt; banging your head against the wall is freedom I prefer compulsory aspirin
</em><br>
<em>&gt; :)
</em><br>
<p>I tend to lean towards the former, but naturally the AI should be able to 
<br>
handle either, just like humans can. If a person really does want to feel 
<br>
pain, as opposed to just thinking they do (a person's volition is not 
<br>
equivalent to the implementation of it in their head), then I'd let them. 
<br>
<p><em>&gt; Also, what about beings whose goal system conflicts with other beings'?
</em><br>
<em>&gt; Should the AI help out by granting the wish (no way to make everyone happy)
</em><br>
<em>&gt; or by erasing wishes in order to remove the conflict and allow universal
</em><br>
<em>&gt; satisfaction? Or deducing from the original wish, a better scenario for
</em><br>
<em>&gt; both beings and enforce that scenario, even though both beings may
</em><br>
<em>&gt; momentarily be opposed to it?
</em><br>
<p>Volitional conflicts are also tricky. Although giving a sentient volitional 
<br>
priority over itself seems like a good heuristic that'd solve a lot of 
<br>
problems we have today.
<br>
<p><em>&gt; [I am not asking myself these questions so I can program an AI to do what I
</em><br>
<em>&gt; want it to do, but I need to have my own moral system straight if I am
</em><br>
<em>&gt; going to think about how to evolve a straight moral system... of course
</em><br>
<em>&gt; whatever conclusion I arrive to, the goal is duplicating the reasoning
</em><br>
<em>&gt; inside the machine so that it may do its own philosophy, and the question
</em><br>
<em>&gt; is, does it need qualia to think what I am thinking here]
</em><br>
<p>Pretty much.
<br>
<p><em>&gt; &gt; You seem to be suggesting the only way a mind can understand another's
</em><br>
<em>&gt; pain
</em><br>
<em>&gt; &gt; (this is an arbitary mind, not a human) is by empathy ie. &quot;because I
</em><br>
<em>&gt; &gt; don't
</em><br>
<em>&gt;
</em><br>
<em>&gt; Understanding another's pain can be a completely intellectual endeavour (I
</em><br>
<em>&gt; see neurotransmitters, don't look too good, therefore it's what humans call
</em><br>
<em>&gt; pain). But labeling this pain as evil and undesireable from an objective
</em><br>
<em>&gt; perspective, this seems to require empathy. What other path is there to
</em><br>
<em>&gt; take us from &quot;cognitive system recognized a situation requiring immediate
</em><br>
<em>&gt; attention, situation which could compromise structural integrity&quot; to &quot;evil
</em><br>
<em>&gt; is occurring - stop it now&quot; ?
</em><br>
<p>I don't think its necessary for an AI to see damage to *itself* as 
<br>
intrinsically evil. It's undesirable in so much as it gets in the way of the 
<br>
AI achieving good.
<br>
<p>One way for an AI to see a state as undesirable, is given in my example of a 
<br>
pain reducing system -- we build the decision system such that it choose 
<br>
actions (including internal actions like &quot;what should I think next&quot;) to 
<br>
minimise pain. Now this is a simplistic model, it lacks all the complexity of 
<br>
human morality. To transfer a more accurate understanding of the 
<br>
undesirability of pain-states (in many cases) you do more complex things :)
<br>
<p><em>&gt; &gt; like it, and take actions to reduce my pain, I should take actions to
</em><br>
<em>&gt; reduce
</em><br>
<em>&gt; &gt; others' pain&quot; (note this is a big leap, and it's non-trivial to have an
</em><br>
<em>&gt; &gt; AI see this as a valid moral argument).
</em><br>
<em>&gt;
</em><br>
<em>&gt; The AI would do the following math,
</em><br>
<em>&gt;
</em><br>
<em>&gt; IF pain feels to others as it feels to me, THEN I have the following
</em><br>
<em>&gt; options.
</em><br>
<em>&gt;
</em><br>
<em>&gt; help them &gt; decrease pain in the universe
</em><br>
<em>&gt; don't help them &gt; pain stays the same
</em><br>
<p>You've implicitly invoked empathy and some degree of altruism here. Without it 
<br>
you have something more like:
<br>
<p>help them &gt; I use some resource, and I feel no less pain
<br>
don't help them &gt; I save some resource, maybe this can be used to decease my 
<br>
expected pain
<br>
<p>So the AI doesn't help them.
<br>
<p><em>&gt; &gt; Even here, I don't think it's necessary. Pain is not just a data
</em><br>
<em>&gt; structure,
</em><br>
<em>&gt; &gt; because (to over simplify) pain states are negatively valued by the goal
</em><br>
<em>&gt; &gt; system. When a pain state is noticed, you can get thoughts like &quot;how can
</em><br>
<em>&gt; &gt; I reduce this pain?&quot;, you can actions taken to remove that pain.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The idea that I am proposing is that pain (physical, mental, whatever) is
</em><br>
<em>&gt; evil not because it is valued negatively by the goal system, but because it
</em><br>
<em>&gt; feels bad to experience it. Missing the train is valued negatively by your
</em><br>
<em>&gt; goal system, of course it puts you in some kind of distress, but it has
</em><br>
<em>&gt; nothing to do with having your skin burned. The latter is much more evil,
</em><br>
<em>&gt; even if you had a way to completely reconstruct the skin afterward, and the
</em><br>
<em>&gt; momentary burning did not influence the probability of achieving your
</em><br>
<em>&gt; supergoals, it would still be more evil than missing the train (which makes
</em><br>
<em>&gt; you arrive late somewhere and MAY prevent you from achieving an important
</em><br>
<em>&gt; goal)
</em><br>
<p>Here we run into the problem of humans messy 'goal system'. When I say goal 
<br>
system I'm covering everything that could lead to a decision. I'm thinking 
<br>
more of my anti-pain AI -- the goal system is the source of all actions.
<br>
<p><em>&gt; &gt; my knowledge of Friendliness development. But I don't see hurting a mind
</em><br>
<em>&gt; as
</em><br>
<em>&gt; &gt; necessary to explain why hurt is bad, and why hurt is something to work
</em><br>
<em>&gt; &gt; towards removing.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I tend to believe that experiencing at least moderate pain at least once is
</em><br>
<em>&gt; necessary to understand why pain is worth removing. Otherwise pain is just
</em><br>
<em>&gt; another information processing type, morally neutral.
</em><br>
<p>Unless there are other ways to transfer morality. Indeed, you need to at least 
<br>
transfer empathy and altruism for an AI 'feeling pain' to have the effect you 
<br>
want.
<br>
<p><em>&gt; PS: a good reply to all I have written above is &quot;the AI should do this
</em><br>
<em>&gt; exact thinking and come up with its solution&quot;. But remember, I have qualia,
</em><br>
<em>&gt; and the AI probably won't (at least in the beginning), so it may not be
</em><br>
<em>&gt; able to formulate sentences like &quot;are qualia needed to develop a universal
</em><br>
<em>&gt; moral framework?&quot;, no matter how smart it got.
</em><br>
<p>You'd certainly want the AI to notice this, if that were the case. 
<br>
<p><em>&gt; PPS: &quot;so what is your solution?&quot; &gt;&gt;&gt; ok I will try. What about convincing
</em><br>
<em>&gt; the AI that there is something humans know that they cannot teach, that is
</em><br>
<em>&gt; fundamental to developing morals and metamorals, something that we don't
</em><br>
<em>&gt; know how to define because we don't know the physics behind it. Convince
</em><br>
<em>&gt; the AI to create the subgoal &quot;figure out qualia&quot; in order to satisfy the
</em><br>
<em>&gt; goal &quot;elaborate a universal morality&quot; in order to satisfy the supergoal
</em><br>
<em>&gt; &quot;friendliness&quot;.
</em><br>
<p>Figuring out qualia seems to be a pretty good subgoal in any case. The AI 
<br>
should be able to unknown causes behind morality, qualia being one example.
<br>
<p>- Nick
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7190.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7188.html">Aaron McBride: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>In reply to:</strong> <a href="7185.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0401/7710.html">Charles Hixson: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7189">[ date ]</a>
<a href="index.html#7189">[ thread ]</a>
<a href="subject.html#7189">[ subject ]</a>
<a href="author.html#7189">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
