<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-2022-jp">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendliness and blank-slate goal bootstrap</title>
<meta name="Author" content="Nick Hay (nickjhay@hotmail.com)">
<meta name="Subject" content="Re: Friendliness and blank-slate goal bootstrap">
<meta name="Date" content="2003-10-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendliness and blank-slate goal bootstrap</h1>
<!-- received="Sat Oct  4 16:58:10 2003" -->
<!-- isoreceived="20031004225810" -->
<!-- sent="Sun, 5 Oct 2003 10:54:19 +1200" -->
<!-- isosent="20031004225419" -->
<!-- name="Nick Hay" -->
<!-- email="nickjhay@hotmail.com" -->
<!-- subject="Re: Friendliness and blank-slate goal bootstrap" -->
<!-- id="200310051154.20796.nickjhay@hotmail.com" -->
<!-- charset="iso-2022-jp" -->
<!-- inreplyto="00a601c38a91$81203b10$0b01a8c0@curziolaptop" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Nick Hay (<a href="mailto:nickjhay@hotmail.com?Subject=Re:%20Friendliness%20and%20blank-slate%20goal%20bootstrap"><em>nickjhay@hotmail.com</em></a>)<br>
<strong>Date:</strong> Sat Oct 04 2003 - 16:54:19 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7184.html">Gordon Worley: "Re: Pattern recognition"</a>
<li><strong>Previous message:</strong> <a href="7182.html">Mike Williams: "RE:  Pattern recognition"</a>
<li><strong>In reply to:</strong> <a href="7178.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7185.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7185.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="../0401/7710.html">Charles Hixson: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7183">[ date ]</a>
<a href="index.html#7183">[ thread ]</a>
<a href="subject.html#7183">[ subject ]</a>
<a href="author.html#7183">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Metaqualia wrote:
<br>
<em>&gt; This is not a continuation of the previous thread, but what about internal
</em><br>
<em>&gt; conflicts in human morality?
</em><br>
<em>&gt; Is a normalized &quot;popular morality&quot; the best morality we can teach to the
</em><br>
<em>&gt; AI?
</em><br>
<p>&quot;popular morality&quot;? The morality we do teach the FAI is less important than 
<br>
the metamorality (ie. the human universal adaptations you and I use to argue, 
<br>
develop and choose moralities) we transfer. The FAI can go over any morality 
<br>
we did transfer and see if it were really right and fair. Or it could start 
<br>
again from stratch. Or both. 
<br>
<p>However the morality we do transfer, as an interim approximation, won't be 
<br>
some popular kind of morality, or some sum of all present-day human 
<br>
moralities, but our best guess at what is the really right thing to do. 
<br>
Personally, I'd go with thinking along the lines of &quot;help sentients, and 
<br>
others that can be helped, with respect to their volitions -- maximise 
<br>
self-determination, minimised unexpected regret&quot;. Focusing on the aspect of 
<br>
helpfulness and implementing others' volitions.
<br>
<p>You could go with &quot;reduce undesirable qualia, increase desirable ones&quot; if you 
<br>
liked.
<br>
<p><em>&gt; If I could choose (not sure that I have the option to, but for sake of
</em><br>
<em>&gt; discussion) I would prefer the AI deriving its own moral rules, finding out
</em><br>
<em>&gt; what is in the best interest of everyone (not just humans but animals as
</em><br>
<em>&gt; well). This is why I was thinking, is there no way to bootstrap some kind
</em><br>
<em>&gt; of universal, all-encompassing moral system? &quot;minimize pain qualia in all
</em><br>
<em>&gt; sentient beings&quot; is the best moral standard I have come up with; it is
</em><br>
<em>&gt; observer independent, and any species with the ability to suffer (all
</em><br>
<em>&gt; evolved beings) should be able to come up with it in time. Who can
</em><br>
<em>&gt; subscribe to this?
</em><br>
<p>There appears to be way to bootstrap this, and that's part of what CFAI is 
<br>
about (although it may not look like it at first and CFAI focuses more on the 
<br>
final state rather than the development process). Part of the idea is to 
<br>
create an AI with the ability to reason and develop moralities at least as 
<br>
good as any human or group there of (eg. governments, civilisations, etc). 
<br>
Then, as the AI increases its intelligence past the human level, it can 
<br>
develop its morality along with it -- it's not forced to be some chimera of 
<br>
human-level morality stuck into a superintelligence. The departure from the 
<br>
blank-slate goal system is the realisation that it takes a lot of work to 
<br>
make a truly fair AI, to make an AI as independent of its programmers as 
<br>
possible, that it's not best accomplished by leaving out as much of 
<br>
everything as possible, but by building in the skills *we* use to identify 
<br>
and implement fairness (and to decide what things we should leave out/include 
<br>
in the AI). Our moral adaptations (and more!).
<br>
<p>I think the universal aspect of your morality is desirable. Just because the 
<br>
FAI has a human-like moral system (ie. we give it our own moral hardware to 
<br>
start with, and some interim morality for it to think about) doesn't mean 
<br>
it's biased towards humans. You have a human moral system, with all the 
<br>
hardware flaws, with a pretty poor level of self-awareness and 
<br>
self-modification ability (compared to a mature seed AI), and yet you can 
<br>
understand that moralities should be as universal and fair as possible. You 
<br>
understand that maybe other people are important, maybe animals are 
<br>
important. A FAI is designed to be able to do likewise.
<br>
<p>Instead of thinking about what kind of morality an AI should start with have, 
<br>
and then transferring it over, why not jump back a step? Transfer your 
<br>
ability to think &quot;what kind of morality should an AI start with?&quot; so the AI 
<br>
itself can make sure you got it right? It seems like you're forced into 
<br>
giving very simple inadequate verbal phrases (hiding huge amounts of 
<br>
conceputal complexity) to describe morality. You're losing too much in the 
<br>
compression.
<br>
<p><em>&gt; By saying this I am in no way criticizing Elizier's work, and I think what
</em><br>
<em>&gt; he proposes is a very practical way to get a friendly AI up and running
</em><br>
<em>&gt; (and incidentally would sound appealing to most people); the only thing,
</em><br>
<em>&gt; human morals kind of suck, they are full of contradictions, we can't agree
</em><br>
<em>&gt; on anything of importance, moral rules commonly held create a lot of
</em><br>
<em>&gt; suffering, and so forth.
</em><br>
<p>Remember, friendliness isn't Friendliness. The former would involve something 
<br>
like making an AI friend, the latter is nothing like it. Where he says 
<br>
&quot;Friendliness should be the supergoal&quot; it means something more like &quot;Whatever 
<br>
is really right should be the supergoal&quot;. Friendliness is an external 
<br>
reference (ie. referencing something outside the AI so it can realise what 
<br>
the morality it has now is an interim approximation) to whatever is right, 
<br>
with things like &quot;help people have what they really want&quot; or &quot;reduce 
<br>
undesirable qualia&quot; being a tentative hypotheses about it. 
<br>
<p>By &quot;human morality&quot; I mean a morality a human could have, not the set of all 
<br>
moral conclusions people actually agree upon. Humans do have a tendency to 
<br>
disagree about things and generally be mistaken. 
<br>
<p><em>&gt; I think it is very possible that a slightly better than human AI would
</em><br>
<em>&gt; immediately see all these fallacies in human morals, and try to develop a
</em><br>
<em>&gt; universal objective moral system on its own.
</em><br>
<p>This is exactly the situation we're planning for. It turns out this isn't a 
<br>
simple ability, obvious to even the simplest kinds of minds, but a complex 
<br>
ability we have to analyse and develop moralities. 
<br>
<p><em>&gt; I imagine a programmer training a child AI
</em><br>
<em>&gt;
</em><br>
<em>&gt; AI: give me an example of friendliness
</em><br>
<em>&gt;
</em><br>
<em>&gt; P: avoiding human death
</em><br>
<em>&gt;
</em><br>
<em>&gt; AI: I suggest the following optimization of resources: newly wed couples
</em><br>
<em>&gt; need not produce a baby but will adopt one sick orphan from an
</em><br>
<em>&gt; underdeveloped country. Their need for cherishing an infant will be
</em><br>
<em>&gt; satisfied and at the same time a human life will be saved every time the
</em><br>
<em>&gt; optimization is applied.
</em><br>
<em>&gt;
</em><br>
<em>&gt; P: the optimization you proposed would not work because humans want to have
</em><br>
<em>&gt; their own child
</em><br>
<em>&gt;
</em><br>
<em>&gt; AI: is the distress of not having this wish granted more important than the
</em><br>
<em>&gt; survival of the orphan?
</em><br>
<em>&gt;
</em><br>
<em>&gt; P: no, but humans tend not to give up a little bit of pleasure in exchange
</em><br>
<em>&gt; for another person's whole lot of pleasure. In particular they will not
</em><br>
<em>&gt; make a considerable sacrifice in order to save an unknown person's life.
</em><br>
<em>&gt; not usually
</em><br>
<p>[Conversation redirection:]
<br>
<p>AI: So perhaps I should focus more on the moralities people think they or wish 
<br>
they had, the principles they proclaim (fairness, equality), rather than the 
<br>
actual actions they take?
<br>
<p>P: Yes. 
<br>
<p>(of course these conversations are far too human, they would be nothing like 
<br>
this.)
<br>
<p><em>&gt; Let's differentiate: pain is a quale to me. If you talk about &quot;awareness of
</em><br>
<em>&gt; body damage&quot;, this is a different thing. A machine can be aware of damage
</em><br>
<em>&gt; to its physical substrate. It can model other beings having a substrate and
</em><br>
<em>&gt; it receiving damage, and it can model these beings perceiving the damage
</em><br>
<em>&gt; being done. But I see no real logical reason why an AI, or even I for that
</em><br>
<em>&gt; matter, should perceive doing damage to other beings as morally wrong
</em><br>
<em>&gt; UNLESS their body damage was not a simple physical phenomenon but gave rise
</em><br>
<em>&gt; to this evil-by-definition pain quale.
</em><br>
<p>Or, they had a complex moral system which negatively valued pain. Then the 
<br>
system could argue about how pain is bad isn't nice &quot;other sentients very 
<br>
much dislike experiencing pain. well, most of them&quot;, and could take actions 
<br>
to reduce it. This is indepedent of it &quot;really experiencing&quot; pain, or even 
<br>
reacting to pain in the way humans do (when you're in a lot of pain your mind 
<br>
is kind of crippled -- I don't think this is either a necessary or desirable 
<br>
property of minds).
<br>
<p>One way this could work is by helpfulness. If you were an AI looking on this 
<br>
pain-experiencing-sentient, you can ask &quot;what does this sentient want? does 
<br>
it enjoy the state it's in?&quot;. To a first approximation, you can notice each 
<br>
time a sentient is in pain, it goes to great measures to remove the pain. 
<br>
You, in your desire to be helpful, decide to help it remove the pain, and 
<br>
make sure you yourself never induce that kind of thing. Now there are loads 
<br>
of gaps in that, but it's a step towards human helpfulness.
<br>
<p><em>&gt; That still does not tell you what the pain feels like from the inside. This
</em><br>
<em>&gt; is an additional piece of information, a very big piece. Without this
</em><br>
<em>&gt; piece, your pain is just a data structure, I can do whatever I want with
</em><br>
<em>&gt; your physical body because it is just like a videogame character. But since
</em><br>
<em>&gt; I have experienced broken nails, and I know a bullet in your head must feel
</em><br>
<em>&gt; like a broken nail * 100, I don't shoot you. Can we agree on this point?
</em><br>
<p>You seem to be suggesting the only way a mind can understand another's pain 
<br>
(this is an arbitary mind, not a human) is by empathy ie. &quot;because I don't 
<br>
like it, and take actions to reduce my pain, I should take actions to reduce 
<br>
others' pain&quot; (note this is a big leap, and it's non-trivial to have an AI 
<br>
see this as a valid moral argument). I suspect a mind with a powerful empathy 
<br>
ability could use any source of undesirability (eg. having your goals 
<br>
frustrated) as a source of empathy for &quot;that's really not nice&quot;.
<br>
<p>Even here, I don't think it's necessary. Pain is not just a data structure, 
<br>
because (to over simplify) pain states are negatively valued by the goal 
<br>
system. When a pain state is noticed, you can get thoughts like &quot;how can I 
<br>
reduce this pain?&quot;, you can actions taken to remove that pain.
<br>
<p>But how can we teach the AI that pain *should* be &quot;negatively valued&quot; (in the 
<br>
right way!) in the first place? To this I have no good answer. I've overrun 
<br>
my knowledge of Friendliness development. But I don't see hurting a mind as 
<br>
necessary to explain why hurt is bad, and why hurt is something to work 
<br>
towards removing.
<br>
<p><em>&gt; Did the above clarify?
</em><br>
<p>Sort of :) 
<br>
<p>- Nick
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7184.html">Gordon Worley: "Re: Pattern recognition"</a>
<li><strong>Previous message:</strong> <a href="7182.html">Mike Williams: "RE:  Pattern recognition"</a>
<li><strong>In reply to:</strong> <a href="7178.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7185.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7185.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="../0401/7710.html">Charles Hixson: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7183">[ date ]</a>
<a href="index.html#7183">[ thread ]</a>
<a href="subject.html#7183">[ subject ]</a>
<a href="author.html#7183">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
