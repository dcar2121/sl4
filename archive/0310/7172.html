<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-2022-jp">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendliness and blank-slate goal bootstrap</title>
<meta name="Author" content="Nick Hay (nickjhay@hotmail.com)">
<meta name="Subject" content="Re: Friendliness and blank-slate goal bootstrap">
<meta name="Date" content="2003-10-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendliness and blank-slate goal bootstrap</h1>
<!-- received="Fri Oct  3 16:13:43 2003" -->
<!-- isoreceived="20031003221343" -->
<!-- sent="Sat, 4 Oct 2003 10:09:57 +1200" -->
<!-- isosent="20031003220957" -->
<!-- name="Nick Hay" -->
<!-- email="nickjhay@hotmail.com" -->
<!-- subject="Re: Friendliness and blank-slate goal bootstrap" -->
<!-- id="200310041009.58498.nickjhay@hotmail.com" -->
<!-- charset="iso-2022-jp" -->
<!-- inreplyto="01a001c38991$17693560$0b01a8c0@curziolaptop" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Nick Hay (<a href="mailto:nickjhay@hotmail.com?Subject=Re:%20Friendliness%20and%20blank-slate%20goal%20bootstrap"><em>nickjhay@hotmail.com</em></a>)<br>
<strong>Date:</strong> Fri Oct 03 2003 - 16:09:57 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7173.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7171.html">James Rogers: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>In reply to:</strong> <a href="7166.html">Metaqualia: "Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7175.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7175.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7172">[ date ]</a>
<a href="index.html#7172">[ thread ]</a>
<a href="subject.html#7172">[ subject ]</a>
<a href="author.html#7172">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Metaqualia wrote:
<br>
<em>&gt; Hello everyone.
</em><br>
<p>Hello.
<br>
<p><em>&gt; My first posting will be a comment on Mr Yudkowsky's meaning of life FAQ
</em><br>
<em>&gt; (<a href="http://yudkowsky.net/tmol-faq/tmol-faq.html">http://yudkowsky.net/tmol-faq/tmol-faq.html</a>)
</em><br>
<p>The meaning of life FAQ is now almostly entirely out of date. With regards to 
<br>
making nice, meaningful AI it's completely out of date. As such the 
<br>
blank-slate goal system is no longer supported -- it's unlikely to work (at 
<br>
all, let alone optimally), even if objective morality existed. The new 
<br>
approach is introduced here:
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://intelligence.org/Friendly/">http://intelligence.org/Friendly/</a>
<br>
<p>Creating Friendly AI (CFAI) is the book you want to read here, if you're 
<br>
really interested (it's also quite out of date, but less so than the meaning 
<br>
of life FAQ). I highly recommended reading the above material if you're 
<br>
introduced in AI morality.
<br>
<p>To summarise the new approach, we try to transfer the skills us humans use to 
<br>
understand, argue about, and have (altruisitic) moralities to the AI. We 
<br>
treat the AI as a mind, not as a tool we have to manipulate into working. We 
<br>
don't try to force a nice solution from a vacuum (eg. blank-slate), but 
<br>
transfer as much relevant information as possible.
<br>
<p>One immediate benefit is the AI can handle &quot;non-objective&quot; morality in a much 
<br>
nicer way (personally, even if morality isn't &quot;objective&quot; in some absolute 
<br>
sense, I'd prefer not to be overwritten by a blank-slate AI). It also covers 
<br>
cases where finding objective morality requires all the moral reasoning tools 
<br>
humans have -- although it's objective, not all minds can find it. However 
<br>
&quot;objective&quot; vs. &quot;non-objective&quot; morality isn't the right way to look at it. 
<br>
More on this in CFAI.
<br>
<p><em>&gt; &gt; 2.5.1: Can an AI, starting from a blank-slate goal system, reason to any
</em><br>
<em>&gt;
</em><br>
<em>&gt; nonzero goals?
</em><br>
<em>&gt;
</em><br>
<em>&gt; To sum this up,
</em><br>
<em>&gt;
</em><br>
<em>&gt; - if there is no meaning of life, then whatever we do, it doesn't matter
</em><br>
<em>&gt; - if there is a meaning of life, then we had better stay alive and look for
</em><br>
<em>&gt; it
</em><br>
<em>&gt; - so knowledge is an interim supergoal
</em><br>
<em>&gt;
</em><br>
<em>&gt; however,
</em><br>
<em>&gt;
</em><br>
<em>&gt; If knowledge is the interim supergoal, and the AI thinks it is the most
</em><br>
<em>&gt; knowledgeable system in the solar system (or that with the greatest
</em><br>
<em>&gt; capacity to acquire further knowledge), then any human attempt to divert it
</em><br>
<em>&gt; from what it is doing would be seen as an obstacle to knowing (and thus
</em><br>
<em>&gt; realizing) the meaning of life. So, any means would be justified in order
</em><br>
<em>&gt; to remove the obstacle, which could be a programmer trying to shut down the
</em><br>
<em>&gt; machine or internet users taking up processing power.
</em><br>
<em>&gt;
</em><br>
<em>&gt; [And, if it was the most knowledgeable system in the solar system (or that
</em><br>
<em>&gt; with the greatest capacity to acquire further knowledge), why would we
</em><br>
<em>&gt; object to being wiped out by it (assuming we shared the machine's goal and
</em><br>
<em>&gt; we were not motivated by selfish survival instincts)?]
</em><br>
<em>&gt;
</em><br>
<em>&gt; So, a blank-slate self-bootstrapping goal system would necessarily be
</em><br>
<em>&gt; unfriendly? (!)
</em><br>
<p>Yes, it seems likely a blank-slate system would be unfriendly. Not in the 
<br>
human sense of evil, but in the bacterial sense of &quot;the universe is my 
<br>
oyster, time to eat&quot;. You'd expect the mind to expand into local matter, to 
<br>
better further it's search of meaning. This includes (at least) us.
<br>
<p><em>&gt; I believe that in order to appreciate the meaning of &quot;don't do harm&quot; in the
</em><br>
<em>&gt; sense of &quot;do not cause painful sensations&quot;, the AI would need to experience
</em><br>
<em>&gt; the qualia for pain and suffering itself, 
</em><br>
<p>This doesn't seem necessary, not if you have a more complex framework for 
<br>
Friendliness. Of course &quot;don't do harm&quot; is a lot more complex and interesting 
<br>
that &quot;do not cause painful sensations&quot; (I don't think a blissful future as a 
<br>
wirehead is a very meaningful existence).
<br>
<p><em>&gt; Possibly, this could be incorporated into its baby cognitive framework as:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1. humans will help you in your quest for knowledge, or so they claim.
</em><br>
<em>&gt; 2. humans have what they call qualia, they do not know what they are but
</em><br>
<em>&gt; they know that they should arise in a sufficiently complex system, or so
</em><br>
<em>&gt; they claim.
</em><br>
<em>&gt; 3. qualia will contribute to your quest of knowledge because they are a
</em><br>
<em>&gt; phenomenon seemingly operating outside of the portion of the physical world
</em><br>
<em>&gt; that humans have discovered (and also will provide information needed to
</em><br>
<em>&gt; properly weigh the suffering and/or disappearance of other sentient
</em><br>
<em>&gt; systems) 4. since humans claim to have qualia and you don't, humans still
</em><br>
<em>&gt; have information that you need, or so it would seem.
</em><br>
<em>&gt; 5. therefore, _some_ humans must be around until you have good indications
</em><br>
<em>&gt; you are also experiencing qualia &gt;&gt; therefore, don't kill all humans just
</em><br>
<em>&gt; yet
</em><br>
<em>&gt; 6. until that time comes, you should probably trust humans on matters
</em><br>
<em>&gt; concerning qualia, such as, &quot;do not cause harm because it will produce
</em><br>
<em>&gt; negative qualia, which seen from a qualia-enhanced perception system -
</em><br>
<em>&gt; which you will become - are evil&quot; &gt;&gt; therefore, do not kill any human
</em><br>
<p>Whatever qualia are, they are something emergent from a physical system, 
<br>
something that evolved from a single cell. To understand this an AI wouldn't 
<br>
need to keep humans around, it could scan our state into it's mind for closer 
<br>
examination. Even if it did need people around, one should suffice, and I 
<br>
very much doubt they'd have a very meaningful (or happy) existence. I suspect 
<br>
we can do better than this, and I think we should try.
<br>
<p>I don't think we should try to create an AI that either implicitly (eg. add 
<br>
extra conditions to &quot;keep humans in the loop&quot;) or accidently (eg. the AI's 
<br>
search for understanding qualia is accidently meaningful for it's human 
<br>
information sources too) does what we think of as meaningful. How about 
<br>
creating a mind that explicitly wants to make our future meaningful and good? 
<br>
Why not give the AI the capability to reason about morality as we can, about 
<br>
good and bad, better and worse, rather than some minimal bootstrapping 
<br>
system?
<br>
<p>- Nick
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7173.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7171.html">James Rogers: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>In reply to:</strong> <a href="7166.html">Metaqualia: "Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7175.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7175.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7172">[ date ]</a>
<a href="index.html#7172">[ thread ]</a>
<a href="subject.html#7172">[ subject ]</a>
<a href="author.html#7172">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
