<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-2022-jp">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendliness and blank-slate goal bootstrap</title>
<meta name="Author" content="Metaqualia (metaqualia@mynichi.com)">
<meta name="Subject" content="Re: Friendliness and blank-slate goal bootstrap">
<meta name="Date" content="2003-10-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendliness and blank-slate goal bootstrap</h1>
<!-- received="Sun Oct  5 00:39:14 2003" -->
<!-- isoreceived="20031005063914" -->
<!-- sent="Sun, 5 Oct 2003 15:38:35 +0900" -->
<!-- isosent="20031005063835" -->
<!-- name="Metaqualia" -->
<!-- email="metaqualia@mynichi.com" -->
<!-- subject="Re: Friendliness and blank-slate goal bootstrap" -->
<!-- id="014701c38b0b$51710b90$0b01a8c0@curziolaptop" -->
<!-- charset="iso-2022-jp" -->
<!-- inreplyto="200310051154.20796.nickjhay@hotmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Metaqualia (<a href="mailto:metaqualia@mynichi.com?Subject=Re:%20Friendliness%20and%20blank-slate%20goal%20bootstrap"><em>metaqualia@mynichi.com</em></a>)<br>
<strong>Date:</strong> Sun Oct 05 2003 - 00:38:35 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7186.html">Robin Lee Powell: "Re: HUMOR: Friendly AI Critical Failure Table"</a>
<li><strong>Previous message:</strong> <a href="7184.html">Gordon Worley: "Re: Pattern recognition"</a>
<li><strong>In reply to:</strong> <a href="7183.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7187.html">Metaqualia: "Feasibility: 100% Bayesian systems"</a>
<li><strong>Reply:</strong> <a href="7187.html">Metaqualia: "Feasibility: 100% Bayesian systems"</a>
<li><strong>Reply:</strong> <a href="7188.html">Aaron McBride: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7189.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7185">[ date ]</a>
<a href="index.html#7185">[ thread ]</a>
<a href="subject.html#7185">[ subject ]</a>
<a href="author.html#7185">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi Nick,
<br>
<p><em>&gt; &quot;popular morality&quot;? The morality we do teach the FAI is less important
</em><br>
than
<br>
<em>&gt; the metamorality (ie. the human universal adaptations you and I use to
</em><br>
argue,
<br>
<em>&gt; develop and choose moralities) we transfer. The FAI can go over any
</em><br>
morality
<br>
<em>&gt; we did transfer and see if it were really right and fair. Or it could
</em><br>
start
<br>
<em>&gt; again from stratch. Or both.
</em><br>
<em>&gt; You could go with &quot;reduce undesirable qualia, increase desirable ones&quot; if
</em><br>
you
<br>
<em>&gt; liked.
</em><br>
<p>Can you give me some examples of metamoral guidelines that most people can
<br>
relate to?
<br>
&quot;Reduce undesirable qualia, increase desirable ones&quot; is the only really
<br>
universal metamoral rule that I can think of. Formulating it seems to
<br>
require qualia.
<br>
<p><em>&gt; Instead of thinking about what kind of morality an AI should start with
</em><br>
have,
<br>
<em>&gt; and then transferring it over, why not jump back a step? Transfer your
</em><br>
<em>&gt; ability to think &quot;what kind of morality should an AI start with?&quot; so the
</em><br>
AI
<br>
<em>&gt; itself can make sure you got it right? It seems like you're forced into
</em><br>
<em>&gt; giving very simple inadequate verbal phrases (hiding huge amounts of
</em><br>
<em>&gt; conceputal complexity) to describe morality. You're losing too much in the
</em><br>
<em>&gt; compression.
</em><br>
<p>I know what you are saying, and I agree completely. The AI should have the
<br>
tools to build its own ideas on everything, it should not follow what we say
<br>
to the letter, both because that would not enable it to go beyond what we
<br>
have thought up and also because what we tell it is likely to be wrong at
<br>
least in some respects.
<br>
<p>What I am asking myself is: &quot;is qualia-enabled empathy a necessary
<br>
ingredient to developing a desire for a universal morality?&quot;
<br>
<p>I think it is, but I'll be glad to be proved wrong, since I have no clue how
<br>
to qualia-enable an AI.
<br>
<p><em>&gt; AI: So perhaps I should focus more on the moralities people think they or
</em><br>
wish
<br>
<em>&gt; they had, the principles they proclaim (fairness, equality), rather than
</em><br>
the
<br>
<em>&gt; actual actions they take?
</em><br>
<p>Seems like a fair counter-argument to my imaginary dialogue.
<br>
What about conflicting principles? For example, humans value freedom and see
<br>
a restriction of their freedom as evil. They also value equality, and see
<br>
inequality as evil (at least in the US). These two values are in conflict
<br>
when we allow rich fathers to pass on wealth to their children. This is not
<br>
fair, so every human being should be made to start with the same assets; but
<br>
this limits your freedom to leave your belongings to whoever you want. Just
<br>
a silly example, but to point out the fact that even the basic principles we
<br>
proclaim are sometimes in conflict. (AND let's not even go into conflicts
<br>
arising between moralities in different cultures: the chinese believe
<br>
deference and unconditional respect for parents is essential, some middle
<br>
eastern males believe it is moral to kill your woman (and her family if you
<br>
are man enough) if she betrays you... ).
<br>
<p>The solution would of course be inventing nanotechnology, abolishing private
<br>
property and creating a world where money is not needed; revising human
<br>
mating strategies so that betrayal no longer happens; revise sexual
<br>
reproduction so that children and parents can never have conflicting
<br>
interests. Or more simply, upload everyone, merge consciousness, and in one
<br>
stroke erase all human dilemmas.
<br>
<p>But you see that while these extraordinary changes would benefit everyone,
<br>
if proposed, would suffer lots of antagonism, since people view such
<br>
dramatic changes brought about by an external force as a threat to their
<br>
free will.
<br>
<p>I could go on forever finding cases in which human values conflict and the
<br>
only way to make them not conflict would be a deep restructuring of the
<br>
world. What are your thoughts on this? I think that whatever restructuring
<br>
the AI deems valuable, should be done. After all the greatest benefit in
<br>
having a smarter friend is not having him help you DO what you want (hold
<br>
the wall while I bang my head); but having him TELL you what you really want
<br>
(stop it).
<br>
<p><em>&gt; (of course these conversations are far too human, they would be nothing
</em><br>
like
<br>
<em>&gt; this.)
</em><br>
<p>of course :)
<br>
<p><em>&gt; Or, they had a complex moral system which negatively valued pain. Then the
</em><br>
<em>&gt; system could argue about how pain is bad isn't nice &quot;other sentients very
</em><br>
<em>&gt; much dislike experiencing pain. well, most of them&quot;, and could take
</em><br>
actions
<br>
<em>&gt; to reduce it. This is indepedent of it &quot;really experiencing&quot; pain, or even
</em><br>
<em>&gt; reacting to pain in the way humans do (when you're in a lot of pain your
</em><br>
mind
<br>
<em>&gt; is kind of crippled -- I don't think this is either a necessary or
</em><br>
desirable
<br>
<em>&gt; property of minds).
</em><br>
<p>Good points. My first reactions:
<br>
<p>1. how could a strictly logical system reason that pain isn't nice? After
<br>
all, a software bug isn't nice, as it represent the failure of an
<br>
information processing system to reach its goal. Waves hitting the shore are
<br>
the failure of the system [water+wind] to carry out an intrinsic program
<br>
(move water in the direction of the wind) which can be seen as a perfectly
<br>
valid goal, so having a shore somewhere is not nice. What makes our pain
<br>
unique is that it feels like something isn't it? [and just in case waves
<br>
suffer when they hit the shore, luckily we'll never know about it, because
<br>
if it were so then the destruction of the universe would be the only way of
<br>
reducing pain and not doing it would be morally unjustifiable]
<br>
<p>2. Of course I am not suggesting that we implement an AI that constantly
<br>
gets depressed, has back aches and so forth, but if (and it is only an if
<br>
for now) understanding pain subjectively is a prerequisite for wanting to
<br>
prevent others from feeling pain (and the only way to develop a desire for a
<br>
universal moral system) then the AI should have at least one painful
<br>
experience, and store it somewhere for recall. Or wait until it does before
<br>
it decides on moral/metamoral matters.
<br>
<p><em>&gt; One way this could work is by helpfulness. If you were an AI looking on
</em><br>
this
<br>
<em>&gt; pain-experiencing-sentient, you can ask &quot;what does this sentient want?
</em><br>
does
<br>
<em>&gt; it enjoy the state it's in?&quot;. To a first approximation, you can notice
</em><br>
each
<br>
<em>&gt; time a sentient is in pain, it goes to great measures to remove the pain.
</em><br>
<em>&gt; You, in your desire to be helpful, decide to help it remove the pain, and
</em><br>
<em>&gt; make sure you yourself never induce that kind of thing. Now there are
</em><br>
loads
<br>
<em>&gt; of gaps in that, but it's a step towards human helpfulness.
</em><br>
<p>So this AI would help beings who want to stop feeling pain but not beings
<br>
who do not want to stop feeling pain (for ideological reasons for example).
<br>
Seems good at least for normal beings who are not too depressed, too stupid
<br>
or too ideologically involved in something for wanting to help themselves.
<br>
What about these exceptions? It's not like pain does not feel bad to them.
<br>
But they have the further handicap of not being able to work toward a
<br>
solution. Should the AI help them too? What is the absolute evil, not being
<br>
able to realize one's wishes, or the subjective agony that the laws of
<br>
physics can create? I vote for the second, and think that a transhuman AI
<br>
should enforce compulsory heaven :) After all, nobody would want to go back
<br>
after they were &quot;given a hand&quot;. I realize this sounds like a violation of
<br>
one's freedom, or some crackhead futuristic utopian scenario, but if banging
<br>
your head against the wall is freedom I prefer compulsory aspirin :)
<br>
<p>Also, what about beings whose goal system conflicts with other beings'?
<br>
Should the AI help out by granting the wish (no way to make everyone happy)
<br>
or by erasing wishes in order to remove the conflict and allow universal
<br>
satisfaction? Or deducing from the original wish, a better scenario for both
<br>
beings and enforce that scenario, even though both beings may momentarily be
<br>
opposed to it?
<br>
<p>[I am not asking myself these questions so I can program an AI to do what I
<br>
want it to do, but I need to have my own moral system straight if I am going
<br>
to think about how to evolve a straight moral system... of course whatever
<br>
conclusion I arrive to, the goal is duplicating the reasoning inside the
<br>
machine so that it may do its own philosophy, and the question is, does it
<br>
need qualia to think what I am thinking here]
<br>
<p><em>&gt; You seem to be suggesting the only way a mind can understand another's
</em><br>
pain
<br>
<em>&gt; (this is an arbitary mind, not a human) is by empathy ie. &quot;because I don't
</em><br>
<p>Understanding another's pain can be a completely intellectual endeavour (I
<br>
see neurotransmitters, don't look too good, therefore it's what humans call
<br>
pain). But labeling this pain as evil and undesireable from an objective
<br>
perspective, this seems to require empathy. What other path is there to take
<br>
us from &quot;cognitive system recognized a situation requiring immediate
<br>
attention, situation which could compromise structural integrity&quot; to &quot;evil
<br>
is occurring - stop it now&quot; ?
<br>
<p><em>&gt; like it, and take actions to reduce my pain, I should take actions to
</em><br>
reduce
<br>
<em>&gt; others' pain&quot; (note this is a big leap, and it's non-trivial to have an AI
</em><br>
<em>&gt; see this as a valid moral argument).
</em><br>
<p>The AI would do the following math,
<br>
<p>IF pain feels to others as it feels to me, THEN I have the following
<br>
options.
<br>
<p>help them &gt; decrease pain in the universe
<br>
don't help them &gt; pain stays the same
<br>
<p>IF it doesn't feel to others as it feels to me or if I am the only conscious
<br>
being in the universe and others are zombies, THEN
<br>
<p>it doesn't matter what I do
<br>
<p>therefore the AI would go and help the being.
<br>
<p><em>&gt;I suspect a mind with a powerful empathy
</em><br>
<em>&gt; ability could use any source of undesirability (eg. having your goals
</em><br>
<em>&gt; frustrated) as a source of empathy for &quot;that's really not nice&quot;.
</em><br>
<p>Exactly my point, there is a difference between some weird cognitive system
<br>
interpreting a bunch of data as negative (waves hitting the coast line, or a
<br>
computer game decreasing player stamina) and pain.
<br>
<p><em>&gt; Even here, I don't think it's necessary. Pain is not just a data
</em><br>
structure,
<br>
<em>&gt; because (to over simplify) pain states are negatively valued by the goal
</em><br>
<em>&gt; system. When a pain state is noticed, you can get thoughts like &quot;how can I
</em><br>
<em>&gt; reduce this pain?&quot;, you can actions taken to remove that pain.
</em><br>
<p>The idea that I am proposing is that pain (physical, mental, whatever) is
<br>
evil not because it is valued negatively by the goal system, but because it
<br>
feels bad to experience it. Missing the train is valued negatively by your
<br>
goal system, of course it puts you in some kind of distress, but it has
<br>
nothing to do with having your skin burned. The latter is much more evil,
<br>
even if you had a way to completely reconstruct the skin afterward, and the
<br>
momentary burning did not influence the probability of achieving your
<br>
supergoals, it would still be more evil than missing the train (which makes
<br>
you arrive late somewhere and MAY prevent you from achieving an important
<br>
goal)
<br>
<p><em>&gt; But how can we teach the AI that pain *should* be &quot;negatively valued&quot; (in
</em><br>
the
<br>
<em>&gt; right way!) in the first place? To this I have no good answer. I've
</em><br>
overrun
<br>
<p>Programmer enters the room with a baseball bat, says &quot;today I am going to
<br>
teach you a thing or two...&quot; heheheh
<br>
<p><em>&gt; my knowledge of Friendliness development. But I don't see hurting a mind
</em><br>
as
<br>
<em>&gt; necessary to explain why hurt is bad, and why hurt is something to work
</em><br>
<em>&gt; towards removing.
</em><br>
<p>I tend to believe that experiencing at least moderate pain at least once is
<br>
necessary to understand why pain is worth removing. Otherwise pain is just
<br>
another information processing type, morally neutral.
<br>
<p>Does anyone else have an opinion?
<br>
<p>PS: a good reply to all I have written above is &quot;the AI should do this exact
<br>
thinking and come up with its solution&quot;. But remember, I have qualia, and
<br>
the AI probably won't (at least in the beginning), so it may not be able to
<br>
formulate sentences like &quot;are qualia needed to develop a universal moral
<br>
framework?&quot;, no matter how smart it got.
<br>
<p>PPS: &quot;so what is your solution?&quot; &gt;&gt;&gt; ok I will try. What about convincing
<br>
the AI that there is something humans know that they cannot teach, that is
<br>
fundamental to developing morals and metamorals, something that we don't
<br>
know how to define because we don't know the physics behind it. Convince the
<br>
AI to create the subgoal &quot;figure out qualia&quot; in order to satisfy the goal
<br>
&quot;elaborate a universal morality&quot; in order to satisfy the supergoal
<br>
&quot;friendliness&quot;.
<br>
<p><p>curzio
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7186.html">Robin Lee Powell: "Re: HUMOR: Friendly AI Critical Failure Table"</a>
<li><strong>Previous message:</strong> <a href="7184.html">Gordon Worley: "Re: Pattern recognition"</a>
<li><strong>In reply to:</strong> <a href="7183.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7187.html">Metaqualia: "Feasibility: 100% Bayesian systems"</a>
<li><strong>Reply:</strong> <a href="7187.html">Metaqualia: "Feasibility: 100% Bayesian systems"</a>
<li><strong>Reply:</strong> <a href="7188.html">Aaron McBride: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7189.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7185">[ date ]</a>
<a href="index.html#7185">[ thread ]</a>
<a href="subject.html#7185">[ subject ]</a>
<a href="author.html#7185">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
