<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-2022-JP">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Friendliness and blank-slate goal bootstrap</title>
<meta name="Author" content="Durant Schoon (durant@escfx.com)">
<meta name="Subject" content="RE: Friendliness and blank-slate goal bootstrap">
<meta name="Date" content="2003-10-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Friendliness and blank-slate goal bootstrap</h1>
<!-- received="Fri Oct  3 12:08:36 2003" -->
<!-- isoreceived="20031003180836" -->
<!-- sent="Fri, 3 Oct 2003 11:08:31 -0700" -->
<!-- isosent="20031003180831" -->
<!-- name="Durant Schoon" -->
<!-- email="durant@escfx.com" -->
<!-- subject="RE: Friendliness and blank-slate goal bootstrap" -->
<!-- id="71C4D6CB0CFA0E4F909198B0AB8A571FC3D6CB@exchange.escfx.com" -->
<!-- charset="ISO-2022-JP" -->
<!-- inreplyto="Friendliness and blank-slate goal bootstrap" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Durant Schoon (<a href="mailto:durant@escfx.com?Subject=RE:%20Friendliness%20and%20blank-slate%20goal%20bootstrap"><em>durant@escfx.com</em></a>)<br>
<strong>Date:</strong> Fri Oct 03 2003 - 12:08:31 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7169.html">Martin Striz: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7167.html">Ben Goertzel: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Maybe in reply to:</strong> <a href="7166.html">Metaqualia: "Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7169.html">Martin Striz: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7169.html">Martin Striz: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7173.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7168">[ date ]</a>
<a href="index.html#7168">[ thread ]</a>
<a href="subject.html#7168">[ subject ]</a>
<a href="author.html#7168">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
(preface: so, after 3 months working 7 days a week, ~75 hours/week,
<br>
I'm done with my FX work for Matrix Revolutions!)
<br>
<p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: Metaqualia [mailto:<a href="mailto:metaqualia@mynichi.com?Subject=RE:%20Friendliness%20and%20blank-slate%20goal%20bootstrap">metaqualia@mynichi.com</a>]
</em><br>
<em>&gt; Sent: Friday, October 03, 2003 2:31 AM
</em><br>
<p><em>&gt; Hello everyone.
</em><br>
<p>Hello.
<br>
<p><em>&gt; My first posting will be a comment on Mr Yudkowsky's meaning 
</em><br>
<em>&gt; of life FAQ
</em><br>
<em>&gt; (<a href="http://yudkowsky.net/tmol-faq/tmol-faq.html">http://yudkowsky.net/tmol-faq/tmol-faq.html</a>)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; 2.5.1: Can an AI, starting from a blank-slate goal system, 
</em><br>
<em>&gt; reason to any
</em><br>
<em>&gt; nonzero goals?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; To sum this up,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; - if there is no meaning of life, then whatever we do, it 
</em><br>
<em>&gt; doesn't matter
</em><br>
<em>&gt; - if there is a meaning of life, then we had better stay 
</em><br>
<em>&gt; alive and look for
</em><br>
<em>&gt; it
</em><br>
<em>&gt; - so knowledge is an interim supergoal
</em><br>
<em>&gt; 
</em><br>
<em>&gt; however,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If knowledge is the interim supergoal, and the AI thinks it 
</em><br>
<em>&gt; is the most
</em><br>
<em>&gt; knowledgeable system in the solar system (or that with the 
</em><br>
<em>&gt; greatest capacity
</em><br>
<em>&gt; to acquire further knowledge), then any human attempt to 
</em><br>
<em>&gt; divert it from what
</em><br>
<em>&gt; it is doing would be seen as an obstacle to knowing (and thus 
</em><br>
<em>&gt; realizing) the
</em><br>
<em>&gt; meaning of life. So, any means would be justified in order to 
</em><br>
<em>&gt; remove the
</em><br>
<em>&gt; obstacle, which could be a programmer trying to shut down the 
</em><br>
<em>&gt; machine or
</em><br>
<em>&gt; internet users taking up processing power.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; [And, if it was the most knowledgeable system in the solar 
</em><br>
<em>&gt; system (or that
</em><br>
<em>&gt; with the greatest capacity to acquire further knowledge), why would we
</em><br>
<em>&gt; object to being wiped out by it (assuming we shared the 
</em><br>
<em>&gt; machine's goal and
</em><br>
<em>&gt; we were not motivated by selfish survival instincts)?]
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So, a blank-slate self-bootstrapping goal system would necessarily be
</em><br>
<em>&gt; unfriendly? (!)
</em><br>
<p>Not necessarily. It is still possible for a self-bootstraping goal
<br>
system to become Friendly. If you consider the history of life on
<br>
Earth as such a self-bootstrapping system and each of us sentients
<br>
as a leaf node in this forward branching process and that one
<br>
individual or group of us can produce Friendly AI, then that 
<br>
possibility is still open. 
<br>
<p>When thinking about such possibilities, it is also useful to
<br>
consider the vast number of afriendly systems, ie. those that
<br>
are neither friendly nor unfriendly. A blank-slate self-
<br>
bootstrapping goal system, might tend to one of those as well.
<br>
Or if you don't think so, maybe you can offer some reasons why.
<br>
<p><em>&gt; It would be really nice if we found that the most basic 
</em><br>
<em>&gt; morality system (do
</em><br>
<em>&gt; not make beings feel pain, make them feel pleasure when you 
</em><br>
<em>&gt; can) can also be
</em><br>
<em>&gt; bootstrapped.
</em><br>
<p>Basic morality would be nice. Knowing how to boostrap a satisfactory
<br>
transhuman morality would be nicer :)
<br>
<p><em>&gt; I believe that in order to appreciate the meaning of &quot;don't 
</em><br>
<em>&gt; do harm&quot; in the
</em><br>
<em>&gt; sense of &quot;do not cause painful sensations&quot;, the AI would need 
</em><br>
<em>&gt; to experience
</em><br>
<em>&gt; the qualia for pain and suffering itself, so that it could 
</em><br>
<em>&gt; deduce that this
</em><br>
<em>&gt; unexplained phenomenon is in the most direct sense evil and 
</em><br>
<em>&gt; must be avoided
</em><br>
<em>&gt; in any observer.... but we do not know how qualia work or 
</em><br>
<em>&gt; what they are, let
</em><br>
<em>&gt; alone how to program a machine to 'have them'. So, is there a 
</em><br>
<em>&gt; way to tell
</em><br>
<em>&gt; the machine that it has to &quot;wait until it has qualia&quot; or &quot;try 
</em><br>
<em>&gt; to achieve
</em><br>
<em>&gt; qualia&quot; before it makes moral decision on the fate of other beings?
</em><br>
<p>I don't agree. I do not personally know the pain of being thrown
<br>
in a wheat thresher. But I can tell you with certainty that I want
<br>
to avoid it.
<br>
<p>If an AI knows what we want to avoid, don't you think the AI might be
<br>
able to behave exactly the same as if &quot;the AI has it's own qualia and 
<br>
is more directly sympathetic to humanity&quot;?
<br>
<p>Besides, the AIs I like to conceive of experience all the qualia we 
<br>
do, plus qualia we cannot yet imagine ;-)
<br>
<p><em>&gt; Possibly, this could be incorporated into its baby cognitive 
</em><br>
<em>&gt; framework as:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 4. since humans claim to have qualia and you don't, humans still have
</em><br>
<em>&gt; information that you need, or so it would seem.
</em><br>
<em>&gt; 5. therefore, _some_ humans must be around until you have 
</em><br>
<em>&gt; good indications
</em><br>
<em>&gt; you are also experiencing qualia &gt;&gt; therefore, don't kill all 
</em><br>
<em>&gt; humans just
</em><br>
<em>&gt; yet
</em><br>
<p>That seems like a thin shield of protection. Hopefully we can come up 
<br>
with something safer than that.
<br>
<p><em>&gt; [bonus. if you are already superintelligent and still are not 
</em><br>
<em>&gt; experiencing
</em><br>
<em>&gt; qualia it would be a good idea to get some humans to 
</em><br>
<em>&gt; transcend and see if a
</em><br>
<em>&gt; qualia-augmented superintelligence is better at finding out 
</em><br>
<em>&gt; what qualia are
</em><br>
<em>&gt; than you qualia-less superintelligence are.]
</em><br>
<p>Ok, one femto second passes. Done. What happens next?
<br>
<p><em>&gt; PS: The termination of the AI would still be an ultimate 
</em><br>
<em>&gt; evil, and death of
</em><br>
<em>&gt; all humans will be preferred in an extreme situation in which 
</em><br>
<em>&gt; it's either
</em><br>
<em>&gt; one way or the other, since the AI is better off looking for 
</em><br>
<em>&gt; the meaning of
</em><br>
<em>&gt; life without humans than dead
</em><br>
<p>Ah, but this is where you probably want to read CFAI to get a 
<br>
good sense of how a Friendly AI might want to arrange it's 
<br>
goals. The Ultimate Evil, and any class of evil hopefully, 
<br>
would be avoided by setting Friendliness as the supergoal.
<br>
<p><a href="http://www.intelligence.org/friendly/index.html">http://www.intelligence.org/friendly/index.html</a>
<br>
<p><em>&gt; PPPS: about harm, and suffering:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think that killing and doing harm to a being are two very different
</em><br>
<em>&gt; things. Killing swiftly without the victim being able to 
</em><br>
<em>&gt; notice and process
</em><br>
<em>&gt; what is going on is a clean termination, it does not comport negative
</em><br>
<em>&gt; feedback loops, or pain qualia. Making the being suffer, on 
</em><br>
<em>&gt; the other hand,
</em><br>
<em>&gt; creates a subjective sensation of agony. While I am quite 
</em><br>
<em>&gt; confident that an
</em><br>
<em>&gt; intelligence experiencing suffering would label suffering as 
</em><br>
<em>&gt; negative (many
</em><br>
<em>&gt; humans do, and if it weren't for more powerful selfish 
</em><br>
<em>&gt; instincts most humans
</em><br>
<em>&gt; would probably avoid suffering to others, given the 
</em><br>
<em>&gt; opportunity), I am not
</em><br>
<em>&gt; confident that it would label clean termination as negative. On the
</em><br>
<em>&gt; contrary, beings that are programmed to suffer such as human 
</em><br>
<em>&gt; beings would
</em><br>
<em>&gt; probably be likely targets for moral massacres (kill them, so 
</em><br>
<em>&gt; they will not
</em><br>
<em>&gt; suffer) [possible happier scenario: make their brain unable to process
</em><br>
<em>&gt; pain... but what if only the first option was currently feasible?]
</em><br>
<p>I try to avoid focusing on the issue of qualia with regard to Super
<br>
Intelligence, other than for pure recreational thinking. Qualia are only
<br>
interesting to me in the sense that they are part of my own personal
<br>
goal system. Most likely, once I am consciously in charge of all that
<br>
(if Friendly AI succeeds, I will be), I will feel great! I'm probably 
<br>
going to want to experience more-wonderful-things(tm), but eventually, 
<br>
the real question then turns into: What do I want to become and what do 
<br>
I have to do to get there?
<br>
<p>(The answer to this partially involves what others want to become as 
<br>
well).
<br>
<p>Are qualia important for designing a Friendly AI? If so, then they are
<br>
imporant. Otherwise, I'd rather think about something else.
<br>
<p>Cheers,
<br>
Durant.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7169.html">Martin Striz: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7167.html">Ben Goertzel: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Maybe in reply to:</strong> <a href="7166.html">Metaqualia: "Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7169.html">Martin Striz: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7169.html">Martin Striz: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7173.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7168">[ date ]</a>
<a href="index.html#7168">[ thread ]</a>
<a href="subject.html#7168">[ subject ]</a>
<a href="author.html#7168">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
