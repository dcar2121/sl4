<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-2022-jp">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendliness and blank-slate goal bootstrap</title>
<meta name="Author" content="Metaqualia (metaqualia@mynichi.com)">
<meta name="Subject" content="Re: Friendliness and blank-slate goal bootstrap">
<meta name="Date" content="2003-10-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendliness and blank-slate goal bootstrap</h1>
<!-- received="Fri Oct  3 23:59:26 2003" -->
<!-- isoreceived="20031004055926" -->
<!-- sent="Sat, 4 Oct 2003 14:59:01 +0900" -->
<!-- isosent="20031004055901" -->
<!-- name="Metaqualia" -->
<!-- email="metaqualia@mynichi.com" -->
<!-- subject="Re: Friendliness and blank-slate goal bootstrap" -->
<!-- id="001401c38a3c$9d24a080$0b01a8c0@curziolaptop" -->
<!-- charset="iso-2022-jp" -->
<!-- inreplyto="71C4D6CB0CFA0E4F909198B0AB8A571FC3D6CB@exchange.escfx.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Metaqualia (<a href="mailto:metaqualia@mynichi.com?Subject=Re:%20Friendliness%20and%20blank-slate%20goal%20bootstrap"><em>metaqualia@mynichi.com</em></a>)<br>
<strong>Date:</strong> Fri Oct 03 2003 - 23:59:01 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7174.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7172.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>In reply to:</strong> <a href="7168.html">Durant Schoon: "RE: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7172.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7173">[ date ]</a>
<a href="index.html#7173">[ thread ]</a>
<a href="subject.html#7173">[ subject ]</a>
<a href="author.html#7173">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; Not necessarily. It is still possible for a self-bootstraping goal
</em><br>
<em>&gt; system to become Friendly.
</em><br>
<p>I am not excluding this possibility, but not being sure whether the AI will
<br>
become a friendly AI is equivalent to having failed in creating the
<br>
framework for friendly AI.
<br>
<p><em>&gt; If you consider the history of life on
</em><br>
<em>&gt; Earth as such a self-bootstrapping system and each of us sentients
</em><br>
<em>&gt; as a leaf node in this forward branching process and that one
</em><br>
<em>&gt; individual or group of us can produce Friendly AI, then that
</em><br>
<em>&gt; possibility is still open.
</em><br>
<p>The fact that humans have evolved the concept of friendliness in my view has
<br>
nothing to do with whether a seed AI will ultimately be able to
<br>
self-bootstrap itself into friendliness. We are imperfectly deceptive
<br>
systems, social morality arises because if we kick someone's butt they will
<br>
kick ours, and this will be encoded genetically sooner or later.
<br>
<p>Aside from the genetically encoded &quot;feeling bad about doing harm&quot;, the
<br>
reason _I_ decide to be friendly (not kill slain or massacre people) is not
<br>
because I have this evolved instinct to sometimes be nice (I choose to
<br>
ignore much stronger instincts at times) but because I take the leap of
<br>
faith of saying, they contain a subjective consciousness just as mine, so
<br>
they must experience pain as I do, so I won't crack their skull open. If I
<br>
had absolutely no experience of pain, and saw it as a physical process
<br>
happening in someone's brain, I would probably think differently. Just like
<br>
I do not mind killing insects that annoy me or cutting tree branches when
<br>
they are in the way, because I do not know about _their_ subjectivity (well
<br>
I do have moments of hesitation, but just because I am nice :)
<br>
<p><em>&gt; When thinking about such possibilities, it is also useful to
</em><br>
<em>&gt; consider the vast number of afriendly systems, ie. those that
</em><br>
<em>&gt; are neither friendly nor unfriendly. A blank-slate self-
</em><br>
<em>&gt; bootstrapping goal system, might tend to one of those as well.
</em><br>
<em>&gt; Or if you don't think so, maybe you can offer some reasons why.
</em><br>
<p>Don't think a system can be neutral. Friendly or unfriendly. The AI can mind
<br>
its own business until someone comes along and tries to mess with it, at
<br>
that point if it's friendly it will go out of its way not to
<br>
cripple/erase/disintegrate the person, otherwise it will take the shortest
<br>
path to eliminating the obstacle.
<br>
<p><em>&gt; &gt; It would be really nice if we found that the most basic
</em><br>
<em>&gt; &gt; morality system (do
</em><br>
<em>&gt; &gt; not make beings feel pain, make them feel pleasure when you
</em><br>
<em>&gt; &gt; can) can also be
</em><br>
<em>&gt; &gt; bootstrapped.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Basic morality would be nice. Knowing how to boostrap a satisfactory
</em><br>
<em>&gt; transhuman morality would be nicer :)
</em><br>
<p>I was not referring to basic morality as human morality, but as the most
<br>
basic set of moral rules that can be seen as potentially universal. &quot;Do not
<br>
create subjective pain&quot; is basically what I intend for &quot;basic morality&quot;,
<br>
actually I shouldn't have used the word &quot;basic&quot;, let's call it &quot;physical
<br>
morality&quot; or &quot;universal morality&quot; or &quot;objective morality&quot;.
<br>
<p><em>&gt; I don't agree. I do not personally know the pain of being thrown
</em><br>
<em>&gt; in a wheat thresher. But I can tell you with certainty that I want
</em><br>
<em>&gt; to avoid it.
</em><br>
<p>That is because you have experienced disruption to your body tissues before,
<br>
it felt painful, so you want to avoid repeating the experience. If you had
<br>
never ever experienced pain, what would prevent you from looking at millions
<br>
of creatures burning in agony and interpreting their screams as purely
<br>
physical phenomena?
<br>
<p><em>&gt; Besides, the AIs I like to conceive of experience all the qualia we
</em><br>
<em>&gt; do, plus qualia we cannot yet imagine ;-)
</em><br>
<p>hence my nickname. But it is not clear whether even a transhuman AI will
<br>
have qualia. Being intelligent is not equivalent to being sensitive. When I
<br>
do extreme mental exercise that does not make me taste strawberries or hear
<br>
bach.
<br>
<p><em>&gt; &gt; Possibly, this could be incorporated into its baby cognitive
</em><br>
<em>&gt; &gt; framework as:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; 4. since humans claim to have qualia and you don't, humans still have
</em><br>
<em>&gt; &gt; information that you need, or so it would seem.
</em><br>
<em>&gt; &gt; 5. therefore, _some_ humans must be around until you have
</em><br>
<em>&gt; &gt; good indications
</em><br>
<em>&gt; &gt; you are also experiencing qualia &gt;&gt; therefore, don't kill all
</em><br>
<em>&gt; &gt; humans just
</em><br>
<em>&gt; &gt; yet
</em><br>
<em>&gt;
</em><br>
<em>&gt; That seems like a thin shield of protection. Hopefully we can come up
</em><br>
<em>&gt; with something safer than that.
</em><br>
<p>if you can come up with it, let me know. remember that it must be something
<br>
arising from simple logic, not a simple &quot;block&quot; put somewhere to restrict
<br>
the AI from accomplishing an otherwise desirable goal.
<br>
<p><em>&gt; &gt; [bonus. if you are already superintelligent and still are not
</em><br>
<em>&gt; &gt; experiencing
</em><br>
<em>&gt; &gt; qualia it would be a good idea to get some humans to
</em><br>
<em>&gt; &gt; transcend and see if a
</em><br>
<em>&gt; &gt; qualia-augmented superintelligence is better at finding out
</em><br>
<em>&gt; &gt; what qualia are
</em><br>
<em>&gt; &gt; than you qualia-less superintelligence are.]
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ok, one femto second passes. Done. What happens next?
</em><br>
<p>My augmented self will think about what's next.
<br>
<p><em>&gt; &gt; PS: The termination of the AI would still be an ultimate
</em><br>
<em>&gt; &gt; evil, and death of
</em><br>
<em>&gt; &gt; all humans will be preferred in an extreme situation in which
</em><br>
<em>&gt; &gt; it's either
</em><br>
<em>&gt; &gt; one way or the other, since the AI is better off looking for
</em><br>
<em>&gt; &gt; the meaning of
</em><br>
<em>&gt; &gt; life without humans than dead
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ah, but this is where you probably want to read CFAI to get a
</em><br>
<em>&gt; good sense of how a Friendly AI might want to arrange it's
</em><br>
<em>&gt; goals. The Ultimate Evil, and any class of evil hopefully,
</em><br>
<em>&gt; would be avoided by setting Friendliness as the supergoal.
</em><br>
<p>I read CFAI, but I disagree about it saying that you must set friendliness
<br>
as the supergoal. One of the basic lessons i took home from CFAI is that you
<br>
can't just put a supergoal of &quot;be friendly&quot; and expect the machine to be
<br>
friendly as it improves itself, you must find a way to let it discover
<br>
friendliness on its own. Besides, &quot;be friendly&quot; is not a clear rule at all;
<br>
I can think of conflicting scenarios in which friendliness cannot be
<br>
achieved period (disarming a human trying to kill another necessarily means
<br>
going against the will of the first person and possibly creating distress) -
<br>
better to let the AI come to a set of morals on its own and figure out the
<br>
specific cases from a superior moral standpoint.
<br>
<p><em>&gt; I try to avoid focusing on the issue of qualia with regard to Super
</em><br>
<em>&gt; Intelligence, other than for pure recreational thinking. Qualia are only
</em><br>
<em>&gt; interesting to me in the sense that they are part of my own personal
</em><br>
<em>&gt; goal system. Most likely, once I am consciously in charge of all that
</em><br>
<p>Qualia have nothing to do with goal systems, the two can exist separately.
<br>
<p><em>&gt; Are qualia important for designing a Friendly AI? If so, then they are
</em><br>
<em>&gt; imporant. Otherwise, I'd rather think about something else.
</em><br>
<p>This was my point, is experiencing pain is necessary to elaborating a
<br>
self-bootstrapping moral framework in which pain is evil?
<br>
<p><p>curzio
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7174.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7172.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>In reply to:</strong> <a href="7168.html">Durant Schoon: "RE: Friendliness and blank-slate goal bootstrap"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7172.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7173">[ date ]</a>
<a href="index.html#7173">[ thread ]</a>
<a href="subject.html#7173">[ subject ]</a>
<a href="author.html#7173">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
