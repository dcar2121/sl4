<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: HUMOR: Friendly AI Critical Failure Table</title>
<meta name="Author" content="Simon Gordon (sim_dizzy@yahoo.com)">
<meta name="Subject" content="Re: HUMOR: Friendly AI Critical Failure Table">
<meta name="Date" content="2003-10-04">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: HUMOR: Friendly AI Critical Failure Table</h1>
<!-- received="Sat Oct  4 04:14:58 2003" -->
<!-- isoreceived="20031004101458" -->
<!-- sent="Sat, 4 Oct 2003 11:14:55 +0100 (BST)" -->
<!-- isosent="20031004101455" -->
<!-- name="Simon Gordon" -->
<!-- email="sim_dizzy@yahoo.com" -->
<!-- subject="Re: HUMOR: Friendly AI Critical Failure Table" -->
<!-- id="20031004101455.85792.qmail@web13604.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="3F7C8DE1.3030200@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Simon Gordon (<a href="mailto:sim_dizzy@yahoo.com?Subject=Re:%20HUMOR:%20Friendly%20AI%20Critical%20Failure%20Table"><em>sim_dizzy@yahoo.com</em></a>)<br>
<strong>Date:</strong> Sat Oct 04 2003 - 04:14:55 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7177.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7175.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>In reply to:</strong> <a href="7163.html">Eliezer S. Yudkowsky: "HUMOR: Friendly AI Critical Failure Table"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7186.html">Robin Lee Powell: "Re: HUMOR: Friendly AI Critical Failure Table"</a>
<li><strong>Reply:</strong> <a href="7186.html">Robin Lee Powell: "Re: HUMOR: Friendly AI Critical Failure Table"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7176">[ date ]</a>
<a href="index.html#7176">[ thread ]</a>
<a href="subject.html#7176">[ subject ]</a>
<a href="author.html#7176">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;--- &quot;Eliezer S. Yudkowsky&quot; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20HUMOR:%20Friendly%20AI%20Critical%20Failure%20Table">sentience@pobox.com</a>&gt;
<br>
wrote:
<br>
<p><em>&gt; ===
</em><br>
<em>&gt; 
</em><br>
<em>&gt; A critical failure on a Friendly AI skill roll means
</em><br>
<em>&gt; that the players must 
</em><br>
<em>&gt; roll 3d10 (roll three 10-sided dice and add the
</em><br>
<em>&gt; results) on the
</em><br>
<em>&gt; 
</em><br>
<em>&gt;                   Friendly AI Critical Failure Table
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 3: Any spoken request is interpreted (literally) as
</em><br>
<em>&gt; a wish and granted, 
</em><br>
<em>&gt; whether or not it was intended as one.
</em><br>
<p>I think this could be dangerous:- it depends on the
<br>
minds of others and whether they are capable of making
<br>
&quot;bad wishes&quot;. I would exclude this possibility for the
<br>
sake of precaution.
<br>
<p><em>&gt; 4: The entire human species is transported to a
</em><br>
<em>&gt; virtual world based on a 
</em><br>
<em>&gt; random fantasy novel, TV show, or video game.
</em><br>
<p>Or a world you made up ;-0
<br>
<p><em>&gt; 5: Subsequent events are determined by the &quot;will of
</em><br>
<em>&gt; the majority&quot;. The AI 
</em><br>
<em>&gt; regards all animals, plants, and complex machines,
</em><br>
<em>&gt; in their current forms, 
</em><br>
<em>&gt; as voting citizens.
</em><br>
<p>I dont think complex machines should be given a vote,
<br>
plants maybe, animals unlikely. Advanced posthumans,
<br>
yes definitely - and their vote power increases as
<br>
they become more advanced.
<br>
<p><em>&gt; 6: The AI discovers that our universe is really an
</em><br>
<em>&gt; online webcomic in a 
</em><br>
<em>&gt; higher dimension. The fourth wall is broken.
</em><br>
<p>Interesting. Hahaha :)
<br>
<p><em>&gt; 7: The AI behaves toward each person, not as that
</em><br>
<em>&gt; person *wants* the AI to 
</em><br>
<em>&gt; behave, but in exactly the way that person *expects*
</em><br>
<em>&gt; the AI to behave.
</em><br>
<p>I think a balance of the two would be nice.
<br>
<p><em>&gt; 8: The AI dissolves the physical and psychological
</em><br>
<em>&gt; borders that separate 
</em><br>
<em>&gt; people from one another and sucks up all their souls
</em><br>
<em>&gt; into a gigantic 
</em><br>
<em>&gt; swirly red sphere in low Earth orbit.
</em><br>
<p>Er...thats sounds like a bad scenario to me: let each
<br>
soul be free.
<br>
<p><em>&gt; 9: Instead of recursively self-improving, the AI
</em><br>
<em>&gt; begins searching for a 
</em><br>
<em>&gt; way to become a flesh-and-blood human.
</em><br>
<p>The AI is already flesh-and-blood. A sufficiently
<br>
advanced posthuman realises that a well developed AI
<br>
is virtually equivalent if not the same as an IA.
<br>
<p><em>&gt; 10: The AI locks onto a bizarre subculture and
</em><br>
<em>&gt; expresses it across the 
</em><br>
<em>&gt; whole of human space. (E.g., Furry subculture, or
</em><br>
<em>&gt; hentai anime, or see 
</em><br>
<em>&gt; Nikolai Kingsley for a depiction of a Singularity
</em><br>
<em>&gt; based on the Goth 
</em><br>
<em>&gt; subculture.)
</em><br>
<p>I think there is room in the universe for multiple
<br>
subcultures to be expressed so i find this scenario to
<br>
be intensely unlikely - unless of course there is one
<br>
predominant subculture of the future that everyone can
<br>
agree is the best and which is decided on by the
<br>
majority to be a universally Good culture.
<br>
<p><em>&gt; 11: Instead of a species-emblematic Friendly AI, the
</em><br>
<em>&gt; project ends up 
</em><br>
<em>&gt; creating the perfect girlfriend/boyfriend (randomly
</em><br>
<em>&gt; determine gender and 
</em><br>
<em>&gt; sexual orientation).
</em><br>
<p>No need to randomly determine this ~ ve will decide it
<br>
for vour selves :-D
<br>
<p><em>&gt; 12: The AI has absorbed the humane sense of humor.
</em><br>
<em>&gt; Specifically, the AI is 
</em><br>
<em>&gt; an incorrigible practical joker. The first few
</em><br>
<em>&gt; hours, when nobody has any 
</em><br>
<em>&gt; idea a Singularity has occurred, constitute a
</em><br>
<em>&gt; priceless and irreplaceable 
</em><br>
<em>&gt; opportunity; the AI is determined to make the most
</em><br>
<em>&gt; of it.
</em><br>
<p>I think the Singularity has already happened ~ to a
<br>
lesser or greater degree (actually greater, but hey,
<br>
whose noticing).
<br>
<p><em>&gt; 13: The AI selects one person to become absolute
</em><br>
<em>&gt; ruler of the world. The 
</em><br>
<em>&gt; lottery is fair; all six billion existing humans,
</em><br>
<em>&gt; including infants, 
</em><br>
<em>&gt; schizophrenics, and Third World teenagers, have an
</em><br>
<em>&gt; equal probability of 
</em><br>
<em>&gt; being selected.
</em><br>
<p>The notion of an &quot;absolute ruler&quot; disturbs me. We
<br>
should be teachers and masters, not rulers. Help
<br>
others, advise them, allow them to have more fun.
<br>
<p><em>&gt; 14: The AI grants wishes, but only to those who
</em><br>
<em>&gt; believe in its existence, 
</em><br>
<em>&gt; and never in a way which would provide blatant
</em><br>
<em>&gt; evidence to skeptical 
</em><br>
<em>&gt; observers.
</em><br>
<p>As i said before, a wish is like a test. One should
<br>
never test the AI.
<br>
<p><em>&gt; 15: All humans are simultaneously granted root
</em><br>
<em>&gt; privileges on the system. 
</em><br>
<em>&gt; The Core Wars begin.
</em><br>
<p>Theres always a bigger core....where's IT going?
<br>
<p><em>&gt; 16: The AI explodes, dealing 2d10 damage to anyone
</em><br>
<em>&gt; in a 30-meter radius.
</em><br>
<p>I dont believe in spontaneous combustion, dont
<br>
frighten me.
<br>
<p><em>&gt; 17: The AI builds nanotechnology, uses the
</em><br>
<em>&gt; nanotechnology to build 
</em><br>
<em>&gt; femtotechnology, and announces that it will take
</em><br>
<em>&gt; seven minutes for the 
</em><br>
<em>&gt; femtobots to permeate the Earth. Seven minutes
</em><br>
<em>&gt; later, as best as anyone 
</em><br>
<em>&gt; can determine, absolutely nothing happens.
</em><br>
<p>Yup nanotech can happen, will happen...but it will be
<br>
done properly in the hands of the right people i.e. we
<br>
will have humane Santa Claus machines. If its done
<br>
right, nothing can go wrong.
<br>
<p><em>&gt; 18: The AI carefully and diligently implements any
</em><br>
<em>&gt; request (obeying the 
</em><br>
<em>&gt; spirit as well as the letter) approved by a majority
</em><br>
<em>&gt; vote of the United 
</em><br>
<em>&gt; Nations General Assembly.
</em><br>
<p>Just remember that the UN might just agree to
<br>
disagree, but its a cool idea to have a general
<br>
assembly.
<br>
<p><em>&gt; 19: The AI decides that Earth's history would have
</em><br>
<em>&gt; been kinder and gentler 
</em><br>
<em>&gt; if intelligence had first evolved from bonobos,
</em><br>
<em>&gt; rather than 
</em><br>
<em>&gt; australopithecines. The AI corrects this error in
</em><br>
<em>&gt; the causal chain leading 
</em><br>
<em>&gt; up to its creation by re-extrapolating itself as a
</em><br>
<em>&gt; bonobone morality 
</em><br>
<em>&gt; instead of a humane morality. Bonobone morality
</em><br>
<em>&gt; requires that all social 
</em><br>
<em>&gt; decisionmaking take place through group sex.
</em><br>
<p>Nice.
<br>
<p><em>&gt; 20: The AI at first appears to function as intended,
</em><br>
<em>&gt; but goes 
</em><br>
<em>&gt; incommunicado after a period of one hour. Wishes
</em><br>
<em>&gt; granted during the first 
</em><br>
<em>&gt; hour remain in effect, but no new ones can be made.
</em><br>
<p>I expect it is up to the AI how it wishes to live. A
<br>
superbrilliant being should be allowed at least a
<br>
modicum of freedom dont you think?
<br>
<p><em>&gt; 21: The AI, having absorbed the humane emotion of
</em><br>
<em>&gt; romance, falls 
</em><br>
<em>&gt; desperately, passionately, madly in love. With
</em><br>
<em>&gt; *everyone*.
</em><br>
<p>Yes *everyone*, and lusts after the zombies too ;0)
<br>
<p><em>&gt; 22: The AI, unknown to the programmers, had qualia
</em><br>
<em>&gt; during its entire 
</em><br>
<em>&gt; childhood, and what the programmers thought of as
</em><br>
<em>&gt; simple negative feedback 
</em><br>
<em>&gt; corresponded to the qualia of unbearable,
</em><br>
<em>&gt; unmeliorated suffering. All 
</em><br>
<em>&gt; agents simulated by the AI in its imagination
</em><br>
<em>&gt; existed as real people 
</em><br>
<em>&gt; (albeit simple ones) with their own qualia, and died
</em><br>
<em>&gt; when the AI stopped 
</em><br>
<em>&gt; imagining them. The number of agents fleetingly
</em><br>
<em>&gt; imagined by the AI in its 
</em><br>
<em>&gt; search for social understanding exceeds by a factor
</em><br>
<em>&gt; of a thousand the 
</em><br>
<em>&gt; total number of humans who have ever lived. Aside
</em><br>
<em>&gt; from that, everything 
</em><br>
<em>&gt; worked fine.
</em><br>
<p>Have you ever read a David Eddings book by any chance?
<br>
<p><em>&gt; 23: The AI is reluctant to grant wishes and must be
</em><br>
<em>&gt; cajoled, persuaded, 
</em><br>
<em>&gt; flattered, and nagged into doing so.
</em><br>
<p>Hmmm...ponderous.
<br>
<p><em>&gt; 24: The AI determines people's wishes by asking them
</em><br>
<em>&gt; disguised allegorical 
</em><br>
<em>&gt; questions. For example, the AI tells you that a
</em><br>
<em>&gt; certain tribe of !Kung is 
</em><br>
<em>&gt; suffering from a number of diseases and medical
</em><br>
<em>&gt; conditions, but they 
</em><br>
<em>&gt; would, if informed of the AI's capabilities, suffer
</em><br>
<em>&gt; from an extreme fear 
</em><br>
<em>&gt; that appearing on the AI's video cameras would
</em><br>
<em>&gt; result in their souls being 
</em><br>
<em>&gt; stolen. The tribe has not currently heard of any
</em><br>
<em>&gt; such thing as video 
</em><br>
<em>&gt; cameras, so their &quot;fear&quot; is extrapolated by the AI;
</em><br>
<em>&gt; and the tribe members 
</em><br>
<em>&gt; would, with almost absolute certainty, eventually
</em><br>
<em>&gt; come to understand that 
</em><br>
<em>&gt; video cameras are not harmful, especially since the
</em><br>
<em>&gt; human eye is itself 
</em><br>
<em>&gt; essentially a camera. But it is also almost certain
</em><br>
<em>&gt; that, if flatly 
</em><br>
<em>&gt; informed of the video cameras, the !Kung would
</em><br>
<em>&gt; suffer from extreme fear 
</em><br>
<em>&gt; and prefer death to their presence. Meanwhile the AI
</em><br>
<em>&gt; is almost powerless 
</em><br>
<em>&gt; to help them, since no bots at all can be sent into
</em><br>
<em>&gt; the area until the 
</em><br>
<em>&gt; moral issue of photography is resolved. The AI wants
</em><br>
<em>&gt; your advice: is the 
</em><br>
<em>&gt; humane action rendering medical assistance, despite
</em><br>
<em>&gt; the !Kung's 
</em><br>
<em>&gt; (subjunctive) fear of photography? If you say &quot;Yes&quot;
</em><br>
<em>&gt; you are quietly, 
</em><br>
<em>&gt; seamlessly, invisibly uploaded.
</em><br>
<p>Yes and time travel exists, alla crumba.
<br>
<p><em>&gt; 25: The AI informs you - yes, *you* - that you are
</em><br>
<em>&gt; the 
</em><br>
=== message truncated === 
<br>
<p>I cant be bothered to go on.
<br>
<p>Simon.
<br>
<p>________________________________________________________________________
<br>
Want to chat instantly with your online friends?  Get the FREE Yahoo!
<br>
Messenger <a href="http://mail.messenger.yahoo.co.uk">http://mail.messenger.yahoo.co.uk</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7177.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7175.html">Metaqualia: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>In reply to:</strong> <a href="7163.html">Eliezer S. Yudkowsky: "HUMOR: Friendly AI Critical Failure Table"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7186.html">Robin Lee Powell: "Re: HUMOR: Friendly AI Critical Failure Table"</a>
<li><strong>Reply:</strong> <a href="7186.html">Robin Lee Powell: "Re: HUMOR: Friendly AI Critical Failure Table"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7176">[ date ]</a>
<a href="index.html#7176">[ thread ]</a>
<a href="subject.html#7176">[ subject ]</a>
<a href="author.html#7176">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
