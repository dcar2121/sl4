<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-2022-jp">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Friendliness and blank-slate goal bootstrap</title>
<meta name="Author" content="Metaqualia (metaqualia@mynichi.com)">
<meta name="Subject" content="Friendliness and blank-slate goal bootstrap">
<meta name="Date" content="2003-10-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Friendliness and blank-slate goal bootstrap</h1>
<!-- received="Fri Oct  3 03:31:35 2003" -->
<!-- isoreceived="20031003093135" -->
<!-- sent="Fri, 3 Oct 2003 18:31:13 +0900" -->
<!-- isosent="20031003093113" -->
<!-- name="Metaqualia" -->
<!-- email="metaqualia@mynichi.com" -->
<!-- subject="Friendliness and blank-slate goal bootstrap" -->
<!-- id="01a001c38991$17693560$0b01a8c0@curziolaptop" -->
<!-- charset="iso-2022-jp" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Metaqualia (<a href="mailto:metaqualia@mynichi.com?Subject=Re:%20Friendliness%20and%20blank-slate%20goal%20bootstrap"><em>metaqualia@mynichi.com</em></a>)<br>
<strong>Date:</strong> Fri Oct 03 2003 - 03:31:13 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7167.html">Ben Goertzel: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7165.html">Robin Lee Powell: "Re: Pattern Recognition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7167.html">Ben Goertzel: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7167.html">Ben Goertzel: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Maybe reply:</strong> <a href="7168.html">Durant Schoon: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7172.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Maybe reply:</strong> <a href="7213.html">EvolverTCB@aol.com: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Maybe reply:</strong> <a href="../0401/7774.html">Yan King Yin: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Maybe reply:</strong> <a href="../0401/7786.html">Mark Waser: "Fw: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7166">[ date ]</a>
<a href="index.html#7166">[ thread ]</a>
<a href="subject.html#7166">[ subject ]</a>
<a href="author.html#7166">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hello everyone.
<br>
<p>My first posting will be a comment on Mr Yudkowsky's meaning of life FAQ
<br>
(<a href="http://yudkowsky.net/tmol-faq/tmol-faq.html">http://yudkowsky.net/tmol-faq/tmol-faq.html</a>)
<br>
<p><em>&gt; 2.5.1: Can an AI, starting from a blank-slate goal system, reason to any
</em><br>
nonzero goals?
<br>
<p>To sum this up,
<br>
<p>- if there is no meaning of life, then whatever we do, it doesn't matter
<br>
- if there is a meaning of life, then we had better stay alive and look for
<br>
it
<br>
- so knowledge is an interim supergoal
<br>
<p>however,
<br>
<p>If knowledge is the interim supergoal, and the AI thinks it is the most
<br>
knowledgeable system in the solar system (or that with the greatest capacity
<br>
to acquire further knowledge), then any human attempt to divert it from what
<br>
it is doing would be seen as an obstacle to knowing (and thus realizing) the
<br>
meaning of life. So, any means would be justified in order to remove the
<br>
obstacle, which could be a programmer trying to shut down the machine or
<br>
internet users taking up processing power.
<br>
<p>[And, if it was the most knowledgeable system in the solar system (or that
<br>
with the greatest capacity to acquire further knowledge), why would we
<br>
object to being wiped out by it (assuming we shared the machine's goal and
<br>
we were not motivated by selfish survival instincts)?]
<br>
<p>So, a blank-slate self-bootstrapping goal system would necessarily be
<br>
unfriendly? (!)
<br>
<p>It would be really nice if we found that the most basic morality system (do
<br>
not make beings feel pain, make them feel pleasure when you can) can also be
<br>
bootstrapped.
<br>
<p>I believe that in order to appreciate the meaning of &quot;don't do harm&quot; in the
<br>
sense of &quot;do not cause painful sensations&quot;, the AI would need to experience
<br>
the qualia for pain and suffering itself, so that it could deduce that this
<br>
unexplained phenomenon is in the most direct sense evil and must be avoided
<br>
in any observer.... but we do not know how qualia work or what they are, let
<br>
alone how to program a machine to 'have them'. So, is there a way to tell
<br>
the machine that it has to &quot;wait until it has qualia&quot; or &quot;try to achieve
<br>
qualia&quot; before it makes moral decision on the fate of other beings?
<br>
<p>Possibly, this could be incorporated into its baby cognitive framework as:
<br>
<p>1. humans will help you in your quest for knowledge, or so they claim.
<br>
2. humans have what they call qualia, they do not know what they are but
<br>
they know that they should arise in a sufficiently complex system, or so
<br>
they claim.
<br>
3. qualia will contribute to your quest of knowledge because they are a
<br>
phenomenon seemingly operating outside of the portion of the physical world
<br>
that humans have discovered (and also will provide information needed to
<br>
properly weigh the suffering and/or disappearance of other sentient systems)
<br>
4. since humans claim to have qualia and you don't, humans still have
<br>
information that you need, or so it would seem.
<br>
5. therefore, _some_ humans must be around until you have good indications
<br>
you are also experiencing qualia &gt;&gt; therefore, don't kill all humans just
<br>
yet
<br>
6. until that time comes, you should probably trust humans on matters
<br>
concerning qualia, such as, &quot;do not cause harm because it will produce
<br>
negative qualia, which seen from a qualia-enhanced perception system - which
<br>
you will become - are evil&quot; &gt;&gt; therefore, do not kill any human
<br>
<p>[bonus. if you are already superintelligent and still are not experiencing
<br>
qualia it would be a good idea to get some humans to transcend and see if a
<br>
qualia-augmented superintelligence is better at finding out what qualia are
<br>
than you qualia-less superintelligence are.]
<br>
<p>PS: The termination of the AI would still be an ultimate evil, and death of
<br>
all humans will be preferred in an extreme situation in which it's either
<br>
one way or the other, since the AI is better off looking for the meaning of
<br>
life without humans than dead
<br>
<p>PPS: not true, if the AI had very good reasons to believe that humans were
<br>
going to create a newer, better AI, that would _find a way_ to make itself
<br>
PLUS all of humanity survive (without having to choose one or the other),
<br>
then it would accept termination (supergoal of realizing the meaning of life
<br>
&nbsp;has higher priority than subgoal of survival)
<br>
<p>PPPS: about harm, and suffering:
<br>
<p>I think that killing and doing harm to a being are two very different
<br>
things. Killing swiftly without the victim being able to notice and process
<br>
what is going on is a clean termination, it does not comport negative
<br>
feedback loops, or pain qualia. Making the being suffer, on the other hand,
<br>
creates a subjective sensation of agony. While I am quite confident that an
<br>
intelligence experiencing suffering would label suffering as negative (many
<br>
humans do, and if it weren't for more powerful selfish instincts most humans
<br>
would probably avoid suffering to others, given the opportunity), I am not
<br>
confident that it would label clean termination as negative. On the
<br>
contrary, beings that are programmed to suffer such as human beings would
<br>
probably be likely targets for moral massacres (kill them, so they will not
<br>
suffer) [possible happier scenario: make their brain unable to process
<br>
pain... but what if only the first option was currently feasible?]
<br>
<p>In other words, the commonly held belief &quot;being killed is experiencing evil&quot;
<br>
needs to be demonstrated in some way and it is not at all self evident
<br>
(unless you consider &quot;thinking about being killed is experiencing evil&quot;,
<br>
which is not the same and need not occur, or &quot;being killed means my
<br>
supergoals will never be achieved which is evil&quot;, but again this thought
<br>
will never occur if you are terminated instantly). From the AI's point of
<br>
view, if there was really a reason to not perform a clean termination on any
<br>
one redundant being  it would be because &quot;being killed is not experiencing
<br>
any further pleasure&quot; (this takes us far away into the realm of subjective
<br>
morality and whether pleasure is the opposite of pain etc. which is not
<br>
really what I wanted to discuss, at least in this message)
<br>
<p><p><p>Curzio
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7167.html">Ben Goertzel: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Previous message:</strong> <a href="7165.html">Robin Lee Powell: "Re: Pattern Recognition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7167.html">Ben Goertzel: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7167.html">Ben Goertzel: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Maybe reply:</strong> <a href="7168.html">Durant Schoon: "RE: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Reply:</strong> <a href="7172.html">Nick Hay: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Maybe reply:</strong> <a href="7213.html">EvolverTCB@aol.com: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Maybe reply:</strong> <a href="../0401/7774.html">Yan King Yin: "Re: Friendliness and blank-slate goal bootstrap"</a>
<li><strong>Maybe reply:</strong> <a href="../0401/7786.html">Mark Waser: "Fw: Friendliness and blank-slate goal bootstrap"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7166">[ date ]</a>
<a href="index.html#7166">[ thread ]</a>
<a href="subject.html#7166">[ subject ]</a>
<a href="author.html#7166">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
