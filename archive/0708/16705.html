<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ESSAY: How to deter a rogue AI by using your first-mover advantage</title>
<meta name="Author" content="Stathis Papaioannou (stathisp@gmail.com)">
<meta name="Subject" content="Re: ESSAY: How to deter a rogue AI by using your first-mover advantage">
<meta name="Date" content="2007-08-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ESSAY: How to deter a rogue AI by using your first-mover advantage</h1>
<!-- received="Mon Aug 27 20:43:58 2007" -->
<!-- isoreceived="20070828024358" -->
<!-- sent="Tue, 28 Aug 2007 12:42:06 +1000" -->
<!-- isosent="20070828024206" -->
<!-- name="Stathis Papaioannou" -->
<!-- email="stathisp@gmail.com" -->
<!-- subject="Re: ESSAY: How to deter a rogue AI by using your first-mover advantage" -->
<!-- id="f21c22e30708271942w6d24da24h754e4108ff847143@mail.gmail.com" -->
<!-- charset="UTF-8" -->
<!-- inreplyto="46208cc60708270635x370587e7q80e763f8fe502735@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Stathis Papaioannou (<a href="mailto:stathisp@gmail.com?Subject=Re:%20ESSAY:%20How%20to%20deter%20a%20rogue%20AI%20by%20using%20your%20first-mover%20advantage"><em>stathisp@gmail.com</em></a>)<br>
<strong>Date:</strong> Mon Aug 27 2007 - 20:42:06 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16706.html">Norman Noman: "Re: Re[2]: Simulation argument in the NY Times"</a>
<li><strong>Previous message:</strong> <a href="16704.html">rolf nelson: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>In reply to:</strong> <a href="16700.html">Norman Noman: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16708.html">Norman Noman: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16708.html">Norman Noman: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16709.html">Vladimir Nesov: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16705">[ date ]</a>
<a href="index.html#16705">[ thread ]</a>
<a href="subject.html#16705">[ subject ]</a>
<a href="author.html#16705">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 27/08/07, Norman Noman &lt;<a href="mailto:overturnedchair@gmail.com?Subject=Re:%20ESSAY:%20How%20to%20deter%20a%20rogue%20AI%20by%20using%20your%20first-mover%20advantage">overturnedchair@gmail.com</a>&gt; wrote:
<br>
<p><em>&gt; I'd like to say that CEV would both make people smart enough to realize
</em><br>
<em>&gt; religion is a load of hooey, and prevent people from threatening each other
</em><br>
<em>&gt; with simulations, but frankly I don't know what CEV does, it seems to be
</em><br>
<em>&gt; more of a mysterious treasure map than an actual target.
</em><br>
<p>What would the CEV of the Pope or Osama Bin Laden look like? I
<br>
wouldn't discount the possibility of a theocratic FAI, unpleasant
<br>
though it may be to contemplate.
<br>
<p><em>&gt; &gt; The probability that some
</em><br>
<em>&gt; &gt; member of the movement will succeed at some point in the future of the
</em><br>
<em>&gt; &gt; universe will then determine the probability that you are now in the
</em><br>
<em>&gt; &gt; simulation.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; If the movement further stipulates that the simulation
</em><br>
<em>&gt; &gt; will be recursive - simulations within simulations - you could argue
</em><br>
<em>&gt; &gt; that you are almost certainly in one of these simulations.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Except that, under the hypothesis where everybody and his brother is allowed
</em><br>
<em>&gt; to simulate the universe, there would be billions of recursive simulations
</em><br>
<em>&gt; and you might be in any one of them. The difficulty in calculating the
</em><br>
<em>&gt; average effect is partially due to complexity, but also due to the basic
</em><br>
<em>&gt; implausibility of this hypothetical situation.
</em><br>
<p>That's right, and my point is that for this reason the only rational
<br>
course of action is to ignore the possibility of a simulation.
<br>
<p><em>&gt; In contrast, rolf's plan is quite plausible, because it's something that
</em><br>
<em>&gt; benefits everyone. Not just humanity and the Friendly AI, but the Rogue AI
</em><br>
<em>&gt; too. If everyone cooperates, then whether mistakes are made or not, humanity
</em><br>
<em>&gt; will be saved and C will be calculated.
</em><br>
<p>I think there would be more people interested in promoting their
<br>
religion or increasing their profits than would be interested in
<br>
making their descendants' future safe from a RAI. This might not be
<br>
rational or moral or whatever, but it's what people would.
<br>
<p><em>&gt; &gt; &gt; And the thing is, it's not going to work either way. If they reveal the
</em><br>
<em>&gt; &gt; &gt; plan, and say SMOKE OR DIE! it's only going to make everyone hate them
</em><br>
<em>&gt; even
</em><br>
<em>&gt; &gt; &gt; more. Actually, it's only going to make everyone laugh at them and think
</em><br>
<em>&gt; &gt; &gt; they're nuts, but assuming they were taken seriously for some reason...
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The religious people would convince the faithful that they were doing
</em><br>
<em>&gt; &gt; God's work, as explained above. But even if it's tobacco companies,
</em><br>
<em>&gt; &gt; the fact that it's obviously an evil threat doesn't make it any less
</em><br>
<em>&gt; &gt; likely to be true. And if it actually got to the point where
</em><br>
<em>&gt; &gt; legislation was passed to make this sort of thing illegal, that would
</em><br>
<em>&gt; &gt; be the ultimate proof that people were taking it seriously.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Are you playing the devil's advocate or do you really think it's even
</em><br>
<em>&gt; remotely likely that big tobacco would invest in a karmic simulation of the
</em><br>
<em>&gt; universe in order to get people to smoke?
</em><br>
<p>As you put it, everybody and his brother could join in, with the
<br>
result that the only rational action would be to ignore the
<br>
possibility of a simulation.
<br>
<p><em>&gt; &gt; &gt; If I am an RAI, the evidence that I may be in a simulation is as
</em><br>
<em>&gt; follows:
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; &quot;The people who created me were trying to create a FAI. With more luck,
</em><br>
<em>&gt; they
</em><br>
<em>&gt; &gt; &gt; would have succeeded. The FAI they created would run karmic simulations
</em><br>
<em>&gt; of
</em><br>
<em>&gt; &gt; &gt; likely RAIs that could have been created instead.
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; Ergo, the probability I am one of them is XY/(1-X)Z, where X is the
</em><br>
<em>&gt; &gt; &gt; likelihood they'd create a friendly AI rather than a rouge one, Y is the
</em><br>
<em>&gt; &gt; &gt; likelihood a friendly AI would simulate me specifically, and Z is the
</em><br>
<em>&gt; &gt; &gt; likelihood I would be created as a result of real human error.&quot;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; These are difficult things to reason about. What about the possibility
</em><br>
<em>&gt; &gt; that you or the RAI might be in a recursive simulation?
</em><br>
<em>&gt;
</em><br>
<em>&gt; If everyone simulates each other, their influence is proportional to their
</em><br>
<em>&gt; likelihood of existing in the first place. Since FAI is the target, it's
</em><br>
<em>&gt; probably the most likely to exist.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Everyone with enough influence to do so manipulates everyone else into
</em><br>
<em>&gt; cooperating with them, and the resulting actions of all the AIs, even the
</em><br>
<em>&gt; FAI, cohere to the extent of satisfying all the most likely goals to the
</em><br>
<em>&gt; extent that they are mutually satisfiable, including probably a zillion
</em><br>
<em>&gt; different definitions of friendliness, calculating C, and whatever else is
</em><br>
<em>&gt; likely to crop up by mistake.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't see how recursive simulations, if the primary simulator bothers to
</em><br>
<em>&gt; actually run them at all, would make a difference. They would just be more
</em><br>
<em>&gt; reasons to do the same things already being done.
</em><br>
<p>It makes a difference to the probability calculations. In the simple
<br>
case, if you can be sure that one simulation has been run, you have a
<br>
1/2 chance of being in that simulation. But if a recursive simulation
<br>
has been run, you have a much higher chance of being in the
<br>
simulation. If an actual Turing machine with infinite cycles available
<br>
to it exists somewhere (and a priori there is no reason to suppose
<br>
that this is impossible, even if it isn't possible in the universe we
<br>
observe), then we might almost certainly be living in a simulation.
<br>
But this realisation should have no effect on our behaviour.
<br>
<p><p><p><pre>
-- 
Stathis Papaioannou
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16706.html">Norman Noman: "Re: Re[2]: Simulation argument in the NY Times"</a>
<li><strong>Previous message:</strong> <a href="16704.html">rolf nelson: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>In reply to:</strong> <a href="16700.html">Norman Noman: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16708.html">Norman Noman: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16708.html">Norman Noman: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16709.html">Vladimir Nesov: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16705">[ date ]</a>
<a href="index.html#16705">[ thread ]</a>
<a href="subject.html#16705">[ subject ]</a>
<a href="author.html#16705">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:58 MDT
</em></small></p>
</body>
</html>
