<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=koi8-r">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI: Why We Exist and Our Short-Term Research Program</title>
<meta name="Author" content="Алексей Турчин (avturchin@mail.ru)">
<meta name="Subject" content="Re: SIAI: Why We Exist and Our Short-Term Research Program">
<meta name="Date" content="2007-08-01">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI: Why We Exist and Our Short-Term Research Program</h1>
<!-- received="Wed Aug  1 07:32:28 2007" -->
<!-- isoreceived="20070801133228" -->
<!-- sent="Wed, 01 Aug 2007 17:28:41 +0400" -->
<!-- isosent="20070801132841" -->
<!-- name="Алексей Турчин" -->
<!-- email="avturchin@mail.ru" -->
<!-- subject="Re: SIAI: Why We Exist and Our Short-Term Research Program" -->
<!-- id="E1IGEFl-000Jwo-00.avturchin-mail-ru@f106.mail.ru" -->
<!-- charset="koi8-r" -->
<!-- inreplyto="632d2cda0707311721w5f03c969le2821c0123979f8f@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Алексей Турчин (<a href="mailto:avturchin@mail.ru?Subject=Re:%20SIAI:%20Why%20We%20Exist%20and%20Our%20Short-Term%20Research%20Program"><em>avturchin@mail.ru</em></a>)<br>
<strong>Date:</strong> Wed Aug 01 2007 - 07:28:41 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16432.html">R. W.: "Re: SIAI: Why We Exist and Our Short-Term Research Program"</a>
<li><strong>Previous message:</strong> <a href="../0707/16430.html">Tyler Emerson: "SIAI: Why We Exist and Our Short-Term Research Program"</a>
<li><strong>In reply to:</strong> <a href="../0707/16430.html">Tyler Emerson: "SIAI: Why We Exist and Our Short-Term Research Program"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16432.html">R. W.: "Re: SIAI: Why We Exist and Our Short-Term Research Program"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16431">[ date ]</a>
<a href="index.html#16431">[ thread ]</a>
<a href="subject.html#16431">[ subject ]</a>
<a href="author.html#16431">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I am really inspired by work of SIAI
<br>
I could suggest you that I will represent you institute in Russian Federation. I have already translated (with permission of Eliezer) several his works on Russian and widely publish them in the Net. More 1000 people read them. We have here several GAI projects, but unfortunately they do not understand the importance of Friendliness. I contacted with some projects and tell them the issue.
<br>
<p>Here are Russian translations:
<br>
<p>Э. Юдковский
<br>
Систематические ошибки в рассуждениях, потенциально влияющие на оценку глобальных рисков.
<br>
<a href="http://www.proza.ru/texts/2007/03/08-62.html">http://www.proza.ru/texts/2007/03/08-62.html</a>
<br>
<p>Э.Юдковский
<br>
Искусственный интеллект как позитивный и негативный фактор глобального риска.
<br>
<a href="http://www.proza.ru/texts/2007/03/22-285.html">http://www.proza.ru/texts/2007/03/22-285.html</a>
<br>
Элиезер Юдковский
<br>
Вглядываясь в Сингулярность.
<br>
<a href="http://www.proza.ru/texts/2007/07/08-42.html">http://www.proza.ru/texts/2007/07/08-42.html</a>
<br>
Э.Юдковский.
<br>
Таблица критических ошибок Дружественного ИИ.
<br>
<a href="http://www.proza.ru/texts/2007/07/09-228.html">http://www.proza.ru/texts/2007/07/09-228.html</a>
<br>
<p>Siai рекомендации по созданию дружественного ИИ.
<br>
<a href="http://www.proza.ru/texts/2007/07/13-272.html">http://www.proza.ru/texts/2007/07/13-272.html</a>
<br>
<p>-----Original Message-----
<br>
From: &quot;Tyler Emerson&quot; &lt;<a href="mailto:emerson@intelligence.org?Subject=Re:%20SIAI:%20Why%20We%20Exist%20and%20Our%20Short-Term%20Research%20Program">emerson@intelligence.org</a>&gt;
<br>
To: <a href="mailto:sl4@sl4.org?Subject=Re:%20SIAI:%20Why%20We%20Exist%20and%20Our%20Short-Term%20Research%20Program">sl4@sl4.org</a>, <a href="mailto:wta-talk@transhumanism.org?Subject=Re:%20SIAI:%20Why%20We%20Exist%20and%20Our%20Short-Term%20Research%20Program">wta-talk@transhumanism.org</a>, <a href="mailto:extropy-chat@lists.extropy.org?Subject=Re:%20SIAI:%20Why%20We%20Exist%20and%20Our%20Short-Term%20Research%20Program">extropy-chat@lists.extropy.org</a>
<br>
Date: Tue, 31 Jul 2007 17:21:28 -0700
<br>
Subject: SIAI: Why We Exist and Our Short-Term Research Program
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Dear all:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Here is a new overview of SIAI, focusing on why we think our mission
</em><br>
<em>&gt; is an important one, and where we're looking to focus research efforts
</em><br>
<em>&gt; in the short-term.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; <a href="http://www.intelligence.org/blog/2007/07/31/siai-why-we-exist-and-our-short-term-research-program/">http://www.intelligence.org/blog/2007/07/31/siai-why-we-exist-and-our-short-term-research-program/</a>
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Let me know what you think: emerson@intelligence.org. I look forward to
</em><br>
<em>&gt; any thoughts you have.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I hope you enjoy it!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Best regards,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; --
</em><br>
<em>&gt; Tyler Emerson
</em><br>
<em>&gt; Executive Director
</em><br>
<em>&gt; Singularity Institute for Artificial Intelligence
</em><br>
<em>&gt; P.O . Box 50182, Palo Alto, CA 94303 USA
</em><br>
<em>&gt; 650-353-6063 | <a href="mailto:emerson@intelligence.org?Subject=Re:%20SIAI:%20Why%20We%20Exist%20and%20Our%20Short-Term%20Research%20Program">emerson@intelligence.org</a> |  singinst.org
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ***
</em><br>
<em>&gt; 
</em><br>
<em>&gt; SIAI: Why We Exist and Our Short-Term Research Program
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Why SIAI Exists
</em><br>
<em>&gt; 
</em><br>
<em>&gt; As the 21st century progresses, an increasing number of
</em><br>
<em>&gt; forward-thinking scientists and technologists are coming to the
</em><br>
<em>&gt; conclusion that this will be the century of AI: the century when human
</em><br>
<em>&gt; inventions exceed human beings in general intelligence. When exactly
</em><br>
<em>&gt; this will happen, no one knows for sure; Ray Kurzweil, for example,
</em><br>
<em>&gt; has estimated 2029.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Of course, where the future is concerned, nothing is certain except
</em><br>
<em>&gt; surprise; but the mere fact that so many knowledgeable people (such as
</em><br>
<em>&gt; Stephen Hawking, Douglas Hofstadter, Bill Joy, and Martin Rees) take
</em><br>
<em>&gt; the near advent of advanced AI as a plausible possibility, should
</em><br>
<em>&gt; serve as a &quot;wake-up call&quot; to anyone seriously concerned about the
</em><br>
<em>&gt; future of humanity.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The potential of advanced AI, for good or evil, has been amply
</em><br>
<em>&gt; explored in science fiction literature and cinema. In the early 90's,
</em><br>
<em>&gt; Vernor Vinge coined the term &quot;technological singularity&quot; to refer to
</em><br>
<em>&gt; the difficulty of predicting or understanding what will happen after
</em><br>
<em>&gt; the point at which humans are no longer the most intelligent and
</em><br>
<em>&gt; capable minds on Earth.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It's easy to be passive about this issue. Technology is advancing, and
</em><br>
<em>&gt; none of us have the power to stop it. There are also plenty of more
</em><br>
<em>&gt; pressing issues around us, so there may seem no clear need to worry
</em><br>
<em>&gt; about something that may happen in 2029, or 2020, or 2050.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Everyone involved with SIAI, however, believes that this kind of
</em><br>
<em>&gt; passivity is both shortsighted and dangerous. As a starting point,
</em><br>
<em>&gt; futuristic predictions are not always overoptimistic   sometimes they
</em><br>
<em>&gt; wind up overpessimistic instead. Jetsons-style spacecraft aren't here
</em><br>
<em>&gt; yet, but the Internet is, and hardly anyone foresaw that until it came
</em><br>
<em>&gt; about. It's important to also note that the 22 years until Kurzweil's
</em><br>
<em>&gt; 2029 prediction is not very long at all. Advanced AI is a big thing to
</em><br>
<em>&gt; understand, and it's also something that can be done either safely or
</em><br>
<em>&gt; unsafely. The time to start thinking very, very hard about how to do
</em><br>
<em>&gt; it safely is this year, not next year, or five years from now. The
</em><br>
<em>&gt; potential dangers of creating advanced AI the wrong way are very
</em><br>
<em>&gt; severe; and the potential rewards of creating it the right way are at
</em><br>
<em>&gt; least equally tremendous.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Our core, long-term mission at the Singularity Institute is to figure
</em><br>
<em>&gt; out how to develop advanced AI safely to help bring about a world in
</em><br>
<em>&gt; which the vast potential benefits of this technology can be enjoyed by
</em><br>
<em>&gt; all of humanity. We want to create a rigorous scientific,
</em><br>
<em>&gt; mathematical, and engineering framework to guide the development of
</em><br>
<em>&gt; safe advanced AI.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In our view, this is the most critical issue facing humanity. We are
</em><br>
<em>&gt; on the verge of creating minds exceeding our own. Unfortunately, the
</em><br>
<em>&gt; amount of societal resources presently going into figuring out how to
</em><br>
<em>&gt; do this right is absurdly tiny. SIAI is the only organization on the
</em><br>
<em>&gt; planet right now that's squarely focused on this incredibly important
</em><br>
<em>&gt; problem. By reading this, you are among the .01% who have even heard
</em><br>
<em>&gt; about this issue; and that estimate may be high.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The Most Important Question Facing Humanity
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There are many ways to work toward figuring out how to develop
</em><br>
<em>&gt; advanced AI. Engineering specific AI systems is valuable, as it helps
</em><br>
<em>&gt; us gain experimental knowledge of semi-advanced AI systems, while
</em><br>
<em>&gt; they're still at an infra-human level. Studying human brain and
</em><br>
<em>&gt; cognition is valuable, since after all, at the present time, the human
</em><br>
<em>&gt; mind is the only highly generally intelligent system we have at our
</em><br>
<em>&gt; disposal to study. Other disciplines like ethical philosophy and
</em><br>
<em>&gt; mathematical decisions theory also have a lot to contribute.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; However, there is one question we feel is absolutely critical to the
</em><br>
<em>&gt; goal of figuring out how to develop advanced AI the right way, which
</em><br>
<em>&gt; remains essentially unexplored within academia and industry. SIAI's
</em><br>
<em>&gt; short-term research mission is to resolve this one question as
</em><br>
<em>&gt; thoroughly as possible. Compactly stated, the question is this:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; How can one make an AI system that modifies and improves itself, yet
</em><br>
<em>&gt; does not lose track of the top-level goals with which it was
</em><br>
<em>&gt; originally supplied?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This question is simple to state but devilishly difficult to resolve  
</em><br>
<em>&gt; it's not even an easy thing to formalize in the language of modern
</em><br>
<em>&gt; mathematics and AI.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; To understand the significance of this question, think about this:
</em><br>
<em>&gt; What is the most likely way for humans to create an AI system that's a
</em><br>
<em>&gt; lot smarter than humans? The answer is: To create an AI system that's
</em><br>
<em>&gt; a little smarter than humans   and ask it to figure out how to make
</em><br>
<em>&gt; itself a little bit smarter; and so on, and so on.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is not an original idea, it's been around since at least the
</em><br>
<em>&gt; 1930's, in various forms. However, we are approaching a time when it
</em><br>
<em>&gt; can actually happen. The pressing question is, then: If we embody the
</em><br>
<em>&gt; initial &quot;a little smarter than humans&quot; AI system with some nice goals
</em><br>
<em>&gt; (including helping humans rather than harming them), how do we know
</em><br>
<em>&gt; the subsequent systems it creates, and the ones its creations create,
</em><br>
<em>&gt; etc., will still embody these goals?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The current focus of SIAI's Research Program is to move toward a
</em><br>
<em>&gt; rigorous understanding and hopefully a clear resolution of this
</em><br>
<em>&gt; question.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; SIAI's Short-Term Research Program
</em><br>
<em>&gt; 
</em><br>
<em>&gt; We aim to resolve this crucial question by simultaneously proceeding
</em><br>
<em>&gt; on two fronts:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1. Experimentation with practical, contemporary AI systems that modify
</em><br>
<em>&gt; and improve their own source code.
</em><br>
<em>&gt;  2. Extension and refinement of mathematical tools to enable rigorous
</em><br>
<em>&gt; formal analysis of advanced self-improving AI's.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; These directions are not disjoint; they have great potential to
</em><br>
<em>&gt; cross-pollinate each other, just as theoretical and empirical science
</em><br>
<em>&gt; have done throughout the ages. On a technical level, part of the
</em><br>
<em>&gt; cross-pollination will occur because both our experimental and our
</em><br>
<em>&gt; theoretical work is grounded in probability theory: probabilistic AI
</em><br>
<em>&gt; and probabilistic mathematics.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; A Practical Project in Self-Modifying AI
</em><br>
<em>&gt; 
</em><br>
<em>&gt; For the practical aspect of the SIAI Research Program, we intend to
</em><br>
<em>&gt; take the MOSES probabilistic evolutionary learning system, which
</em><br>
<em>&gt; exists in the public domain and was developed by Dr. Moshe Looks in
</em><br>
<em>&gt; his PhD work at Washington University in 2006, and deploy it
</em><br>
<em>&gt; self-referentially, in a manner that allows MOSES to improve its own
</em><br>
<em>&gt; learning methodology.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; MOSES is currently implemented in C++, and is configured to learn
</em><br>
<em>&gt; software programs that are expressed in a simple language called
</em><br>
<em>&gt; Combo. Deploying MOSES self-referentially will require the
</em><br>
<em>&gt; re-implementation of MOSES in Combo, and then the improvement of
</em><br>
<em>&gt; several aspects of MOSES's internal learning algorithms.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Hitherto MOSES has proved useful for data mining, biological data
</em><br>
<em>&gt; analysis, and the control of simple embodied agents in virtual worlds.
</em><br>
<em>&gt; In a current project, Novamente LLC and Electric Sheep Company are
</em><br>
<em>&gt; using it to control a simple virtual agent acting in Second Life.
</em><br>
<em>&gt; Learning to improve MOSES will be the most difficult task yet posed to
</em><br>
<em>&gt; MOSES, but also the most interesting.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Applying MOSES self-referentially will give us a fascinating concrete
</em><br>
<em>&gt; example of self-modifying AI software   far short of human-level
</em><br>
<em>&gt; general intelligence initially, but nevertheless with many lessons to
</em><br>
<em>&gt; teach us about the more ambitious self-modifying AI's that may be
</em><br>
<em>&gt; possible.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Toward a Rigorous Theory of Self-Modifying AI
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Studying self-modification in the context of a particular contemporary
</em><br>
<em>&gt; AI algorithm such as MOSEs is important, but ultimately it only takes
</em><br>
<em>&gt; you so far. One of the values of mathematics is that it lets you
</em><br>
<em>&gt; explore important issues in advance of actually observing them
</em><br>
<em>&gt; empirically. For instance, using mathematics, Einstein understood the
</em><br>
<em>&gt; nature of black holes long before they were ever empirically observed.
</em><br>
<em>&gt; Similarly, we may use mathematics to understand things about advanced
</em><br>
<em>&gt; self-modifying probabilistic AI systems, even before we have worked
</em><br>
<em>&gt; out the details of how to create them (and before we have sufficient
</em><br>
<em>&gt; hardware to run them).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Theoretical computer scientists such as Marcus Hutter and Juergen
</em><br>
<em>&gt; Schmidhuber, in recent years, have developed a rigorous mathematical
</em><br>
<em>&gt; theory of artificial general intelligence (AGI). While this work is
</em><br>
<em>&gt; revolutionary, it has its limitations. Most of its conclusions apply
</em><br>
<em>&gt; only to AI systems that use a truly massive amount of computational
</em><br>
<em>&gt; resources   more than we could ever assemble in physical reality.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What needs to be done, in order to create a mathematical theory that
</em><br>
<em>&gt; is useful for studying the self-modifying AI systems we will build in
</em><br>
<em>&gt; the future, is to scale Hutter and Schmidhuber's theory down to deal
</em><br>
<em>&gt; with AI systems involving more plausible amounts of computational
</em><br>
<em>&gt; resources. This is far from an easy task, but it is a concrete
</em><br>
<em>&gt; mathematical task, and we have specific conjectures regarding how to
</em><br>
<em>&gt; approach it. The self-referential MOSES implementation, mentioned
</em><br>
<em>&gt; above, may serve as an important test case here: if a scaled-down
</em><br>
<em>&gt; mathematical theory of AGI is any good, it should be able to tell us
</em><br>
<em>&gt; something about self-referential MOSES.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This sort of work is difficult, and the time required for success is
</em><br>
<em>&gt; hard to predict. However, we feel very strongly that this sort of
</em><br>
<em>&gt; foundational work   inspired by close collaboration with computational
</em><br>
<em>&gt; experiment   is the most likely route to achieving true understanding
</em><br>
<em>&gt; of the fundamental question posed above: How can one make an AI system
</em><br>
<em>&gt; that modifies and improves itself, yet does not lose track of the
</em><br>
<em>&gt; top-level goals with which it was originally supplied?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Hiring Plan
</em><br>
<em>&gt; 
</em><br>
<em>&gt; SIAI is currently a small organization, with one full-time Research
</em><br>
<em>&gt; Fellow (Eliezer Yudkowsky) and part-time involvement by a number of AI
</em><br>
<em>&gt; researchers, including Director of Research Dr. Ben Goertzel. We are
</em><br>
<em>&gt; seeking additional funding so as to enable, initially, the hiring of
</em><br>
<em>&gt; two doctoral or post-doctoral Research Fellows to focus on the above
</em><br>
<em>&gt; two areas (practical and theoretical exploration of self-modifying
</em><br>
<em>&gt; AI).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; These two Fellows would work under the supervision of Dr. Ben
</em><br>
<em>&gt; Goertzel; and in collaboration with Eliezer Yudkowsky as well. They
</em><br>
<em>&gt; would also benefit from interaction with the group of AI luminaries
</em><br>
<em>&gt; who are involved with SIAI, including SIAI Director Ray Kurzweil and
</em><br>
<em>&gt; SIAI Advisors Neil Jacobstein and Dr. Stephen Omohundro.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Two Research Fellows, of course, represent a rather small allocation
</em><br>
<em>&gt; of society's overall resources   one could argue that, in fact, a
</em><br>
<em>&gt; substantial percentage of our collective resources should be allocated
</em><br>
<em>&gt; to exploring issues such as those that concern SIAI, given their
</em><br>
<em>&gt; potentially extreme importance to the future of humankind. But many
</em><br>
<em>&gt; great things start from small initiatives, and we believe that the
</em><br>
<em>&gt; right two researchers, focused squarely on these issues, can make a
</em><br>
<em>&gt; huge difference in advancing knowledge and better directing AI R&amp;D in
</em><br>
<em>&gt; the right direction.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Part of our goal is to make progress on these issues ourselves,
</em><br>
<em>&gt; in-house within SIAI; and part of our goal is to, by demonstrating
</em><br>
<em>&gt; this progress, interest the wider AI R&amp;D community in these
</em><br>
<em>&gt; foundational issues. Either way: the goal is to move toward a deeper
</em><br>
<em>&gt; understanding of these incredibly important issues.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Toward a Positive Singularity
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Advanced self-modifying AI is almost sure to happen in this century  
</em><br>
<em>&gt; as Ray Kurzweil, Bill Joy, and others have foreseen. The big question
</em><br>
<em>&gt; is whether we succeed in creating it with rigor, care, and foresight.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; SIAI doesn't claim to have the answers   not yet, anyway. What we do
</em><br>
<em>&gt; have is a systematic, well-defined research program, aimed at focusing
</em><br>
<em>&gt; on the most essential questions. With sustained effort, maybe a little
</em><br>
<em>&gt; brilliance and luck, and a lot of help, we may well create an
</em><br>
<em>&gt; understanding that will help the human race navigate its way in the
</em><br>
<em>&gt; coming decades to a positive Singularity. If you are aligned with this
</em><br>
<em>&gt; vision, we hope you will help us.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Why is it advantageous to invest in SIAI now rather later? There's a
</em><br>
<em>&gt; clear, rational answer to this question: If you invest now, you will
</em><br>
<em>&gt; increase the probability that we can scale SIAI and its community of
</em><br>
<em>&gt; friends and supporters to a level where there's a sufficiently-sized
</em><br>
<em>&gt; body of capable researchers who can work full-time on these critical
</em><br>
<em>&gt; issues. SIAI is the only organization focused on these problems right
</em><br>
<em>&gt; now, thus we are a nucleus around which a certain amount of talent has
</em><br>
<em>&gt; already accrued, and around which additional talent can be accrued
</em><br>
<em>&gt; over time. If you invest later, you will likely have reduced the
</em><br>
<em>&gt; probability that SIAI will be able to reach a sufficient critical mass
</em><br>
<em>&gt; to effectively confront these issues before it's way too late. SIAI
</em><br>
<em>&gt; must boot-strap into existence a scientific field and research
</em><br>
<em>&gt; community for the study of safe, recursively self-improving systems;
</em><br>
<em>&gt; this field and community doesn't exist yet. This is going to be hard;
</em><br>
<em>&gt; it's going to take time, but the sooner SIAI can grow, the greater the
</em><br>
<em>&gt; chance we'll have of being able to catalyze a critical mass in-time to
</em><br>
<em>&gt; deal with these problems before we're in a nose-dive situation that we
</em><br>
<em>&gt; can't reverse.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; One of the best ways to support SIAI is by contributing to the
</em><br>
<em>&gt; Singularity Challenge, which will allow us to grow the organization.
</em><br>
<em>&gt; If you donate or email us a pledge by August 6th, we can ensure your
</em><br>
<em>&gt; gift is matched. We hope many of you reading will do this; and thank
</em><br>
<em>&gt; you!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; <a href="http://www.intelligence.org/challenge/">http://www.intelligence.org/challenge/</a>
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If you want to get involved with SIAI, or if you have resources to
</em><br>
<em>&gt; share (such as expertise, talent, promotion, or contacts), then please
</em><br>
<em>&gt; email us: institute@intelligence.org.
</em><br>
<em>&gt; 
</em><br>
<p>Посетите мой Живой Журнал www.livejournal.com/users/turchin - и узнайте то, что я думаю прямо сейчас - и ещё то, что хотел сказать вам, но не успел :)
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16432.html">R. W.: "Re: SIAI: Why We Exist and Our Short-Term Research Program"</a>
<li><strong>Previous message:</strong> <a href="../0707/16430.html">Tyler Emerson: "SIAI: Why We Exist and Our Short-Term Research Program"</a>
<li><strong>In reply to:</strong> <a href="../0707/16430.html">Tyler Emerson: "SIAI: Why We Exist and Our Short-Term Research Program"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16432.html">R. W.: "Re: SIAI: Why We Exist and Our Short-Term Research Program"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16431">[ date ]</a>
<a href="index.html#16431">[ thread ]</a>
<a href="subject.html#16431">[ subject ]</a>
<a href="author.html#16431">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:58 MDT
</em></small></p>
</body>
</html>
