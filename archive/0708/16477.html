<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Continuing Evolution in Humans (was: A very surreal day)</title>
<meta name="Author" content="Daniel (kopacetic101@gmail.com)">
<meta name="Subject" content="Re: Continuing Evolution in Humans (was: A very surreal day)">
<meta name="Date" content="2007-08-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Continuing Evolution in Humans (was: A very surreal day)</h1>
<!-- received="Tue Aug 14 09:20:21 2007" -->
<!-- isoreceived="20070814152021" -->
<!-- sent="Tue, 14 Aug 2007 16:17:18 +0100" -->
<!-- isosent="20070814151718" -->
<!-- name="Daniel" -->
<!-- email="kopacetic101@gmail.com" -->
<!-- subject="Re: Continuing Evolution in Humans (was: A very surreal day)" -->
<!-- id="873f8b0c0708140817o48e5af47td9135eb717433c9a@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="fb04b6260708140734nce8cd23t2759cecc5cc81562@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Daniel (<a href="mailto:kopacetic101@gmail.com?Subject=Re:%20Continuing%20Evolution%20in%20Humans%20(was:%20A%20very%20surreal%20day)"><em>kopacetic101@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Aug 14 2007 - 09:17:18 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16478.html">Eric Rauch: "Simulation argument in the NY Times"</a>
<li><strong>Previous message:</strong> <a href="16476.html">Daniel: "Re: Continuing Evolution in Humans (was: A very surreal day)"</a>
<li><strong>In reply to:</strong> <a href="16475.html">Byrne Hobart: "Re: Continuing Evolution in Humans (was: A very surreal day)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16483.html">Diego Navarro: "Re: Continuing Evolution in Humans (was: A very surreal day)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16477">[ date ]</a>
<a href="index.html#16477">[ thread ]</a>
<a href="subject.html#16477">[ subject ]</a>
<a href="author.html#16477">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
-&quot;All else being equal, of course it does! If nothing else, it means they
<br>
spend more time thinking about hard problems because they spend less time
<br>
looking for their calculator. But in a more serious sense, 1) if there isn't
<br>
a direct correlation, being able to solve certain problems easily will
<br>
affect which problems you consider solvable -- and, again all else being
<br>
equal, the people who make the most progress are the ones who think we can
<br>
make the most progress. 2) there is a correlation, because the variable IQ
<br>
measures is predictive across numerous fields (
<br>
<a href="http://www.gnxp.com/blog/2007/03/g-precis.php">http://www.gnxp.com/blog/2007/03/g-precis.php</a> ).&quot;-
<br>
<p>We need to steer away from the term IQ. Intelligence and an IQ score are
<br>
very different as the current tests measure pattern recognition and
<br>
mathematical ability as well as verbal reasoning *indicating*, perhaps
<br>
intelligence. Yet intelligence without wisdom is a Ferrari without gas.
<br>
<p>-&quot;AIs thrive in an environment that values intelligence. AIs will also be
<br>
powerful enough to change their environment. When we reach an equilibrium, I
<br>
think it'll devalue many human traits (interpersonal skills, for example,
<br>
won't mean much if most emotional responses are an avoidable cost). &quot;-
<br>
<p>Then perhaps AI programmers will be revered? *Picks up LISP book*
<br>
<p>-&quot;We'll always be constrained by somehow -- I'd probably give up a large
<br>
fraction of my political and economic liberty in exchange for faster thought
<br>
and statistical immortality, and that's probably the deal we're going to
<br>
get. The AI's pitch will be, roughly, &quot;I can allocate resources more
<br>
effectively than you, and I'm constantly getting better. But given guidance
<br>
from me, you're worth more than it costs to keep you around. So I'll keep
<br>
you around, as long as you do what I say -- and you'll live a longer,
<br>
happier life for it.&quot; The AI isn't a politician or a cult leader; it's not
<br>
making zero-sum allocations like everyone else who makes that kind of
<br>
promise. &quot;-
<br>
<p>I would be more than happy to give up my political/economic freedoms for the
<br>
above.
<br>
<p>-&quot;The first step on the road to the Singularity: the invention of beer.
<br>
<p>And this is a good point: between coffee and pot, we've all gotten a lot
<br>
better at using chemicals to regulate our mood so we're productive when we
<br>
have to be and happy when we want to be. Unfortunately, I don't think
<br>
there's a huge amount of progress possible through psychotropics. If we get
<br>
to the point where mood is always aligned with need, how much more
<br>
productive will we be? It probably varies from one field to another, but
<br>
there are limits (and that's ignoring the long-term effect of these
<br>
chemicals; jury's still out, except where it's in and the verdict is bad).
<br>
&quot;-
<br>
<p>Having seen first hand the effects of various long term usage of chemicals
<br>
on a variety of society's upper and lower echelons I would go with anything
<br>
not in moderation is bad, very bad, losing memory, cognition, bad.. I don't
<br>
drink coffee, never felt the need. Though I do agree that everyday
<br>
substances can vastly affect our mood/state of mind and there are people who
<br>
require them.
<br>
<p>On 8/14/07, Byrne Hobart &lt;<a href="mailto:sometimesfunnyalwaysright@gmail.com?Subject=Re:%20Continuing%20Evolution%20in%20Humans%20(was:%20A%20very%20surreal%20day)">sometimesfunnyalwaysright@gmail.com</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;  The general principle behind that affirmation also implies that
</em><br>
<em>&gt; &gt; someone's aptitude for doing fast linear algebra and Fourier
</em><br>
<em>&gt; &gt; transforms in their heads (basically autistic savants (I'm not bashing
</em><br>
<em>&gt; &gt; autistics, there's a chance I'm one)) correlates to their aptitude for
</em><br>
<em>&gt; &gt; scientific (numerical) programming. Do you really want to uphold a
</em><br>
<em>&gt; &gt; theory that implies that?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; All else being equal, of course it does! If nothing else, it means they
</em><br>
<em>&gt; spend more time thinking about hard problems because they spend less time
</em><br>
<em>&gt; looking for their calculator. But in a more serious sense, 1) if there isn't
</em><br>
<em>&gt; a direct correlation, being able to solve certain problems easily will
</em><br>
<em>&gt; affect which problems you consider solvable -- and, again all else being
</em><br>
<em>&gt; equal, the people who make the most progress are the ones who think we can
</em><br>
<em>&gt; make the most progress. 2) there is a correlation, because the variable IQ
</em><br>
<em>&gt; measures is predictive across numerous fields (
</em><br>
<em>&gt; <a href="http://www.gnxp.com/blog/2007/03/g-precis.php">http://www.gnxp.com/blog/2007/03/g-precis.php</a> ).
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Oh well. I think I'm killing a fly with a cannonball by now. But
</em><br>
<em>&gt; &gt; really, the discussion on /what/ will differentiate humans in their
</em><br>
<em>&gt; &gt; ability to relate to post-singularity Powers or whatever (I'm not keen
</em><br>
<em>&gt; &gt; at sci-fi words; the singularity /is/ an event horizon) can't be based
</em><br>
<em>&gt; &gt; on their current similarity to what the Powers would work like. I'd
</em><br>
<em>&gt; &gt; venture it's the opposite -- what matters is how COMPLEMENTARY to the
</em><br>
<em>&gt; &gt; Powers a person is.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I'm not sure. When Northern Europe emerged from the Dark Ages, literacy
</em><br>
<em>&gt; was an increasingly valuable skill because it allowed people to interface
</em><br>
<em>&gt; with wealthy and knowledgeable elites. A skill complementary to that of
</em><br>
<em>&gt; effete, wealthy scientist/statesmen like Newton would be the ability to bash
</em><br>
<em>&gt; in skulls using blunt objects, but the demand for skull-bashers was not, as
</em><br>
<em>&gt; far as I know, rising during the Renaissance.
</em><br>
<em>&gt;
</em><br>
<em>&gt; AIs thrive in an environment that values intelligence. AIs will also be
</em><br>
<em>&gt; powerful enough to change their environment. When we reach an equilibrium, I
</em><br>
<em>&gt; think it'll devalue many human traits (interpersonal skills, for example,
</em><br>
<em>&gt; won't mean much if most emotional responses are an avoidable cost).
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; This all, of course, is based on the rather unpleasant scenario of an
</em><br>
<em>&gt; &gt; &quot;unfriendly&quot; singulatiy where we're all reduced to serving a godlike
</em><br>
<em>&gt; &gt; Power's needs.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; We'll always be constrained by somehow -- I'd probably give up a large
</em><br>
<em>&gt; fraction of my political and economic liberty in exchange for faster thought
</em><br>
<em>&gt; and statistical immortality, and that's probably the deal we're going to
</em><br>
<em>&gt; get. The AI's pitch will be, roughly, &quot;I can allocate resources more
</em><br>
<em>&gt; effectively than you, and I'm constantly getting better. But given guidance
</em><br>
<em>&gt; from me, you're worth more than it costs to keep you around. So I'll keep
</em><br>
<em>&gt; you around, as long as you do what I say -- and you'll live a longer,
</em><br>
<em>&gt; happier life for it.&quot; The AI isn't a politician or a cult leader; it's not
</em><br>
<em>&gt; making zero-sum allocations like everyone else who makes that kind of
</em><br>
<em>&gt; promise.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; In a rather tangential sidenote, isn't the current research in the
</em><br>
<em>&gt; &gt; mood mechanisms of the human brain transhumanist at some level? I know
</em><br>
<em>&gt; &gt; I feel augmented by the psychotropic medication I've been taking, the
</em><br>
<em>&gt; &gt; way (though not in the same magnitude) I venture I'd feel if I had
</em><br>
<em>&gt; &gt; coprocessor chips implanted.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; The first step on the road to the Singularity: the invention of beer.
</em><br>
<em>&gt;
</em><br>
<em>&gt; And this is a good point: between coffee and pot, we've all gotten a lot
</em><br>
<em>&gt; better at using chemicals to regulate our mood so we're productive when we
</em><br>
<em>&gt; have to be and happy when we want to be. Unfortunately, I don't think
</em><br>
<em>&gt; there's a huge amount of progress possible through psychotropics. If we get
</em><br>
<em>&gt; to the point where mood is always aligned with need, how much more
</em><br>
<em>&gt; productive will we be? It probably varies from one field to another, but
</em><br>
<em>&gt; there are limits (and that's ignoring the long-term effect of these
</em><br>
<em>&gt; chemicals; jury's still out, except where it's in and the verdict is bad).
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16478.html">Eric Rauch: "Simulation argument in the NY Times"</a>
<li><strong>Previous message:</strong> <a href="16476.html">Daniel: "Re: Continuing Evolution in Humans (was: A very surreal day)"</a>
<li><strong>In reply to:</strong> <a href="16475.html">Byrne Hobart: "Re: Continuing Evolution in Humans (was: A very surreal day)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16483.html">Diego Navarro: "Re: Continuing Evolution in Humans (was: A very surreal day)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16477">[ date ]</a>
<a href="index.html#16477">[ thread ]</a>
<a href="subject.html#16477">[ subject ]</a>
<a href="author.html#16477">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:58 MDT
</em></small></p>
</body>
</html>
