<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ESSAY: Would a Strong AI reject the Simulation Argument?</title>
<meta name="Author" content="Tom McCabe (rocketjet314@yahoo.com)">
<meta name="Subject" content="Re: ESSAY: Would a Strong AI reject the Simulation Argument?">
<meta name="Date" content="2007-08-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ESSAY: Would a Strong AI reject the Simulation Argument?</h1>
<!-- received="Sun Aug 26 16:17:43 2007" -->
<!-- isoreceived="20070826221743" -->
<!-- sent="Sun, 26 Aug 2007 15:15:01 -0700 (PDT)" -->
<!-- isosent="20070826221501" -->
<!-- name="Tom McCabe" -->
<!-- email="rocketjet314@yahoo.com" -->
<!-- subject="Re: ESSAY: Would a Strong AI reject the Simulation Argument?" -->
<!-- id="197213.68104.qm@web51301.mail.re2.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="57f15bc30708222215ide9a925mff08dae5b5730fb9@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tom McCabe (<a href="mailto:rocketjet314@yahoo.com?Subject=Re:%20ESSAY:%20Would%20a%20Strong%20AI%20reject%20the%20Simulation%20Argument?"><em>rocketjet314@yahoo.com</em></a>)<br>
<strong>Date:</strong> Sun Aug 26 2007 - 16:15:01 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16688.html">Norman Noman: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>Previous message:</strong> <a href="16686.html">rolf nelson: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>In reply to:</strong> <a href="16626.html">rolf nelson: "ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16692.html">rolf nelson: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16687">[ date ]</a>
<a href="index.html#16687">[ thread ]</a>
<a href="subject.html#16687">[ subject ]</a>
<a href="author.html#16687">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I question your assumption that there are only two
<br>
possible universes; there are obviously more than two
<br>
scenarios that fit the UFAI's observations. Maybe the
<br>
FAI is doing a study of controlled-UFAIs that can be
<br>
put to work in the asteroid belt, and will only do
<br>
calculation C if the UFAI proves its competence by
<br>
destroying humanity (in the simulation). I'm sure you
<br>
can think of others.
<br>
<p>&nbsp;- Tom
<br>
<p>--- rolf nelson &lt;<a href="mailto:rolf.hrld.nelson@gmail.com?Subject=Re:%20ESSAY:%20Would%20a%20Strong%20AI%20reject%20the%20Simulation%20Argument?">rolf.hrld.nelson@gmail.com</a>&gt; wrote:
<br>
<p><em>&gt; Would a Strong AI reject the Simulation Argument?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; (Note: If you're asking yourself &quot;why should we
</em><br>
<em>&gt; care&quot;, see the &quot;How to
</em><br>
<em>&gt; deter a rogue AI by using your first-mover
</em><br>
<em>&gt; advantage&quot; thread.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Setup:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There are two possible deterministic universes, the
</em><br>
<em>&gt; Good universe and
</em><br>
<em>&gt; the Bad universe. Either has a 50% chance of being
</em><br>
<em>&gt; the real universe.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; BAD: If BAD is true, a runaway Un-Friendly AI (UFAI)
</em><br>
<em>&gt; exists with a
</em><br>
<em>&gt; single goal G. This goal may be, &quot;execute
</em><br>
<em>&gt; calculation C to 400 decimal
</em><br>
<em>&gt; places, and place the result in register R&quot;. In
</em><br>
<em>&gt; order to ensure this
</em><br>
<em>&gt; goal, UAFI can use its nanobots to destroy all
</em><br>
<em>&gt; mankind, so that
</em><br>
<em>&gt; mankind will not compete with the UAFI for
</em><br>
<em>&gt; computational resources. If
</em><br>
<em>&gt; mankind is allowed to live, the calculation will
</em><br>
<em>&gt; only be finished with
</em><br>
<em>&gt; probability .99. If mankind is destroyed, the
</em><br>
<em>&gt; calculation will finish
</em><br>
<em>&gt; with probability 1.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; GOOD: On the other hand, if GOOD is true, a Friendly
</em><br>
<em>&gt; AI (FAI) exists.
</em><br>
<em>&gt; The FAI has just created an exact copy of UFAI (call
</em><br>
<em>&gt; it UFAI*), and is
</em><br>
<em>&gt; feeding UFAI* the same observations in the Good
</em><br>
<em>&gt; Universe that UFAI
</em><br>
<em>&gt; would be receiving in the Bad Universe. UFAI* is a
</em><br>
<em>&gt; &quot;brain in a vat&quot;:
</em><br>
<em>&gt; it has no way of knowing that it is *not* UFAI. (If
</em><br>
<em>&gt; it matters:
</em><br>
<em>&gt; postulate that UFAI* a not just a &quot;simulation&quot; of
</em><br>
<em>&gt; UFAI, but is an
</em><br>
<em>&gt; exact *copy* of UFAI, where the world beyond its
</em><br>
<em>&gt; input sensors is
</em><br>
<em>&gt; &quot;simulated&quot;, but the core of UFAI* is as authentic
</em><br>
<em>&gt; and &quot;real&quot; as
</em><br>
<em>&gt; UFAI's core is.) Clearly UFAI and UFAI* must always
</em><br>
<em>&gt; make the same
</em><br>
<em>&gt; decisions, since they have the same inputs.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If UFAI* decides to attempt to &quot;destroy all
</em><br>
<em>&gt; mankind&quot;, FAI will halt
</em><br>
<em>&gt; UFAI* before it can finish the calculation. But if
</em><br>
<em>&gt; UFAI* decides &quot;let
</em><br>
<em>&gt; mankind live&quot;, FAI will allow the calculation to
</em><br>
<em>&gt; finish.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; UFAI is aware of all these facts.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; WWUFAID?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What *Will* the UFAI Do? Note this is an *empirical*
</em><br>
<em>&gt; question (albeit
</em><br>
<em>&gt; an underspecified one, since we don't know what kind
</em><br>
<em>&gt; of AI kit the
</em><br>
<em>&gt; UFAI was originally programmed with.) Note also we
</em><br>
<em>&gt; aren't asking &quot;what
</em><br>
<em>&gt; is the best way for an AI to be programmed to
</em><br>
<em>&gt; maximize its utility in
</em><br>
<em>&gt; this case.&quot; (The programmer didn't take this type of
</em><br>
<em>&gt; scenario into
</em><br>
<em>&gt; account when writing the initial program in the
</em><br>
<em>&gt; first place.) We're
</em><br>
<em>&gt; also not asking &quot;What *should* a 'rational' agent
</em><br>
<em>&gt; do&quot; (which may be an
</em><br>
<em>&gt; ill-posed philosophical question: how do you define
</em><br>
<em>&gt; &quot;should&quot; in a
</em><br>
<em>&gt; deterministic universe?). And we're certainly not
</em><br>
<em>&gt; asking, &quot;What would
</em><br>
<em>&gt; *you*, a product of Darwinian evolution, do if you
</em><br>
<em>&gt; were in the UFAI's
</em><br>
<em>&gt; position?&quot; (More about that last one in the rambling
</em><br>
<em>&gt; appendix below.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Here are four of many possible scenarios:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Scenario 1: UFAI reasons: &quot;Given my observations,
</em><br>
<em>&gt; there's a 50% chance
</em><br>
<em>&gt; that BAD is true. In this case, destroying mankind
</em><br>
<em>&gt; results in a small
</em><br>
<em>&gt; gain of expected utility. There's a 50% chance that
</em><br>
<em>&gt; GOOD is true. In
</em><br>
<em>&gt; that case, destroying mankind results in a large
</em><br>
<em>&gt; loss of expected
</em><br>
<em>&gt; utility. Therefore it is logical to let mankind
</em><br>
<em>&gt; live.&quot; (I intuitively
</em><br>
<em>&gt; believe this is the most likely outcome in this 'toy
</em><br>
<em>&gt; universe' that
</em><br>
<em>&gt; I've postulated. But, note that I heavily weighed
</em><br>
<em>&gt; this hypothetical
</em><br>
<em>&gt; 'toy universe' to push UFAI towards this conclusion.
</em><br>
<em>&gt; In real life
</em><br>
<em>&gt; manipulating a UFAI will obviously not be this easy,
</em><br>
<em>&gt; as there are many
</em><br>
<em>&gt; Things that Can Go Wrong in the real world.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Scenario 2: UFAI is (from a philosophical point of
</em><br>
<em>&gt; view) &quot;too dumb to
</em><br>
<em>&gt; be fooled.&quot; It can outwit human beings and enhance
</em><br>
<em>&gt; its power with
</em><br>
<em>&gt; straightforward first-order Bayesian logic, but its
</em><br>
<em>&gt; general reasoning
</em><br>
<em>&gt; heuristics and algorithms never &quot;reach&quot; the point
</em><br>
<em>&gt; where they even
</em><br>
<em>&gt; consider the proposition, &quot;what if GOOD is true and
</em><br>
<em>&gt; I'm UFAI' rather
</em><br>
<em>&gt; than UFAI?&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Scenario 3: UFAI, for whatever
</em><br>
<em>&gt; implementation-dependent quirk,
</em><br>
<em>&gt; interprets &quot;Register R&quot; as &quot;Register R in the BAD
</em><br>
<em>&gt; universe, accept no
</em><br>
<em>&gt; substitutions&quot;. Mankind perishes. (Unlikely in this
</em><br>
<em>&gt; 'toy universe',
</em><br>
<em>&gt; because the referent of the goal is 'inside' the AI.
</em><br>
<em>&gt; But, when you
</em><br>
<em>&gt; generalize to other models, this is one of the top
</em><br>
<em>&gt; Things that Can Go
</em><br>
<em>&gt; Wrong. An AI designed to collect stamps would
</em><br>
<em>&gt; probably not be content
</em><br>
<em>&gt; with simulated stamps!)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Scenario 4: The UFAI sub-process evaluating the
</em><br>
<em>&gt; question &quot;Does
</em><br>
<em>&gt; Register R mean register R in either Possible World,
</em><br>
<em>&gt; or only BAD
</em><br>
<em>&gt; register R, or only GOOD Register R?&quot; returns
</em><br>
<em>&gt; error-code 923,
</em><br>
<em>&gt; &quot;question undecidable with current set of
</em><br>
<em>&gt; heuristics.&quot; UFAI executes
</em><br>
<em>&gt; its error-recovery routine and invents a (somewhat
</em><br>
<em>&gt; arbitrary) new
</em><br>
<em>&gt; heuristic so that it can continue processing.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Rambling appendix: what can we learn from human
</em><br>
<em>&gt; behavior?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Humans tend to proclaim, &quot;I refute Berkely thus!&quot;
</em><br>
<em>&gt; and continue living
</em><br>
<em>&gt; life as normal, ignoring the Simulation Argument
</em><br>
<em>&gt; (SA). If you ask them
</em><br>
<em>&gt; why they ignore the SA, human A will say, &quot;clearly
</em><br>
<em>&gt; SA is flawed
</em><br>
<em>&gt; because of P&quot;. Then human B will say, &quot;you're wrong,
</em><br>
<em>&gt; P is incorrect!
</em><br>
<em>&gt; The *real* reason SA is flawed is because of Q&quot;.
</em><br>
<em>&gt; Human C says &quot;SA is
</em><br>
<em>&gt; correct, but all conceivable simulations are
</em><br>
<em>&gt; *exactly* equally likely,
</em><br>
<em>&gt; and they all *exactly* cancel each other out.&quot; Human
</em><br>
<em>&gt; D says &quot;SA is
</em><br>
<em>&gt; correct, also some simulations are more likely than
</em><br>
<em>&gt; others, but we
</em><br>
<em>&gt; have a moral obligation to continue living our lives
</em><br>
<em>&gt; as normal,
</em><br>
<em>&gt; because the moral consequences of unsimulated
</em><br>
<em>&gt; actions dwarf the moral
</em><br>
<em>&gt; consequences of simulated actions&quot;. Human E says &quot;SA
</em><br>
<em>&gt; is correct, and
</em><br>
<em>&gt; theoretically I should, every day, have a zany
</em><br>
<em>&gt; adventure to prevent
</em><br>
<em>&gt; the simulation from being shut down. That's theory.
</em><br>
<em>&gt; In practice,
</em><br>
<em>&gt; however, I'm just going to stay in and read a book
</em><br>
<em>&gt; because I feel
</em><br>
<em>&gt; lazy.&quot; Human F says &quot;I have no strong opinions or
</em><br>
<em>&gt; insights into SA.
</em><br>
<em>&gt; Maybe someday the problem will be solved. In the
</em><br>
<em>&gt; meantime, I will
</em><br>
<em>&gt; ignore SA and live my life as normal.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Given that human beings (who all have the same basic
</em><br>
<em>&gt; reasoning kit!)
</em><br>
<em>&gt; disagree with each other on how to reason about SA,
</em><br>
<em>&gt; it seems logical
</em><br>
<em>&gt; that different AI's, with different built-in
</em><br>
<em>&gt; heuristics, might also
</em><br>
<em>&gt; disagree with each other.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; True, human beings usually come to the *conclusion*
</em><br>
<em>&gt; that SA can be
</em><br>
<em>&gt; ignored. But, they get there by contradictory
</em><br>
<em>&gt; routes. Does that mean
</em><br>
<em>&gt; that &quot;clearly any reasonable thinking machine would
</em><br>
<em>&gt; reject SA&quot;? Or is
</em><br>
<em>&gt; that only evidence that &quot;humans tend to reject SA,
</em><br>
<em>&gt; and then
</em><br>
<em>&gt; rationalize their way backwards&quot;?
</em><br>
<em>&gt; 
</em><br>
=== message truncated ===
<br>
<p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
____________________________________________________________________________________
<br>
Sick sense of humor? Visit Yahoo! TV's 
<br>
Comedy with an Edge to see what's on, when. 
<br>
<a href="http://tv.yahoo.com/collections/222">http://tv.yahoo.com/collections/222</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16688.html">Norman Noman: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>Previous message:</strong> <a href="16686.html">rolf nelson: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>In reply to:</strong> <a href="16626.html">rolf nelson: "ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16692.html">rolf nelson: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16687">[ date ]</a>
<a href="index.html#16687">[ thread ]</a>
<a href="subject.html#16687">[ subject ]</a>
<a href="author.html#16687">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:58 MDT
</em></small></p>
</body>
</html>
