<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ESSAY: How to deter a rogue AI by using your first-mover advantage</title>
<meta name="Author" content="Norman Noman (overturnedchair@gmail.com)">
<meta name="Subject" content="Re: ESSAY: How to deter a rogue AI by using your first-mover advantage">
<meta name="Date" content="2007-08-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ESSAY: How to deter a rogue AI by using your first-mover advantage</h1>
<!-- received="Mon Aug 27 21:46:40 2007" -->
<!-- isoreceived="20070828034640" -->
<!-- sent="Mon, 27 Aug 2007 22:45:02 -0500" -->
<!-- isosent="20070828034502" -->
<!-- name="Norman Noman" -->
<!-- email="overturnedchair@gmail.com" -->
<!-- subject="Re: ESSAY: How to deter a rogue AI by using your first-mover advantage" -->
<!-- id="46208cc60708272045g6b07075cvba052e14f2af4ab@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="f21c22e30708271942w6d24da24h754e4108ff847143@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Norman Noman (<a href="mailto:overturnedchair@gmail.com?Subject=Re:%20ESSAY:%20How%20to%20deter%20a%20rogue%20AI%20by%20using%20your%20first-mover%20advantage"><em>overturnedchair@gmail.com</em></a>)<br>
<strong>Date:</strong> Mon Aug 27 2007 - 21:45:02 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16709.html">Vladimir Nesov: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Previous message:</strong> <a href="16707.html">Norman Noman: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>In reply to:</strong> <a href="16705.html">Stathis Papaioannou: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16711.html">Stathis Papaioannou: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16711.html">Stathis Papaioannou: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16708">[ date ]</a>
<a href="index.html#16708">[ thread ]</a>
<a href="subject.html#16708">[ subject ]</a>
<a href="author.html#16708">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 8/27/07, Stathis Papaioannou &lt;<a href="mailto:stathisp@gmail.com?Subject=Re:%20ESSAY:%20How%20to%20deter%20a%20rogue%20AI%20by%20using%20your%20first-mover%20advantage">stathisp@gmail.com</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; On 27/08/07, Norman Noman &lt;<a href="mailto:overturnedchair@gmail.com?Subject=Re:%20ESSAY:%20How%20to%20deter%20a%20rogue%20AI%20by%20using%20your%20first-mover%20advantage">overturnedchair@gmail.com</a>&gt; wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; I'd like to say that CEV would both make people smart enough to realize
</em><br>
<em>&gt; &gt; religion is a load of hooey, and prevent people from threatening each
</em><br>
<em>&gt; other
</em><br>
<em>&gt; &gt; with simulations, but frankly I don't know what CEV does, it seems to be
</em><br>
<em>&gt; &gt; more of a mysterious treasure map than an actual target.
</em><br>
<em>&gt;
</em><br>
<em>&gt; What would the CEV of the Pope or Osama Bin Laden look like? I
</em><br>
<em>&gt; wouldn't discount the possibility of a theocratic FAI, unpleasant
</em><br>
<em>&gt; though it may be to contemplate.
</em><br>
<p><p>I don't think it's likely, but I wouldn't discount it either. If I was
<br>
writing the AI's goals, I would be rather specific in not tolerating willful
<br>
ignorance. I guess in saying this I've already strayed into forbidden
<br>
&quot;arguing about friendliness&quot; territory, so, moving on...
<br>
<p><em>&gt; &gt; If the movement further stipulates that the simulation
</em><br>
<em>&gt; &gt; &gt; will be recursive - simulations within simulations - you could argue
</em><br>
<em>&gt; &gt; &gt; that you are almost certainly in one of these simulations.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Except that, under the hypothesis where everybody and his brother is
</em><br>
<em>&gt; allowed
</em><br>
<em>&gt; &gt; to simulate the universe, there would be billions of recursive
</em><br>
<em>&gt; simulations
</em><br>
<em>&gt; &gt; and you might be in any one of them. The difficulty in calculating the
</em><br>
<em>&gt; &gt; average effect is partially due to complexity, but also due to the basic
</em><br>
<em>&gt; &gt; implausibility of this hypothetical situation.
</em><br>
<em>&gt;
</em><br>
<em>&gt; That's right, and my point is that for this reason the only rational
</em><br>
<em>&gt; course of action is to ignore the possibility of a simulation.
</em><br>
<p><p>I said it was difficult to calculate, not that it should be ignored. If your
<br>
scenario came to pass, although I certainly do not imagine it will, it would
<br>
be smart to give the issue considerable attention to say the least.
<br>
<p><em>&gt; In contrast, rolf's plan is quite plausible, because it's something that
</em><br>
<em>&gt; &gt; benefits everyone. Not just humanity and the Friendly AI, but the Rogue
</em><br>
<em>&gt; AI
</em><br>
<em>&gt; &gt; too. If everyone cooperates, then whether mistakes are made or not,
</em><br>
<em>&gt; humanity
</em><br>
<em>&gt; &gt; will be saved and C will be calculated.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think there would be more people interested in promoting their
</em><br>
<em>&gt; religion or increasing their profits than would be interested in
</em><br>
<em>&gt; making their descendants' future safe from a RAI. This might not be
</em><br>
<em>&gt; rational or moral or whatever, but it's what people would.
</em><br>
<p><p>It doesn't matter what pre-singularity people want, only what the
<br>
post-singularity entity or entities with the power to do the simulations
<br>
wants. I find it very difficult to believe that post-singularity big tobacco
<br>
and osama bin laden will even exist in any meaningful sense, let alone stay
<br>
true to the wildly out of character schemes you suggest they will soon have.
<br>
<p>And even if you're RIGHT, and there is a pandemonium of human infighting via
<br>
simulation which cancels out to nothing, there is no reason rolf's plan
<br>
cannot be implemented as well.
<br>
<p><em>&gt; Are you playing the devil's advocate or do you really think it's even
</em><br>
<em>&gt; &gt; remotely likely that big tobacco would invest in a karmic simulation of
</em><br>
<em>&gt; the
</em><br>
<em>&gt; &gt; universe in order to get people to smoke?
</em><br>
<em>&gt;
</em><br>
<em>&gt; As you put it, everybody and his brother could join in, with the
</em><br>
<em>&gt; result that the only rational action would be to ignore the
</em><br>
<em>&gt; possibility of a simulation.
</em><br>
<p><p>So, your answer is YES?
<br>
<p>(again, please note that I did not say the issue should be ignored, only
<br>
that it was very difficult to calculate what action should be taken.)
<br>
<p><em>&gt; I don't see how recursive simulations, if the primary simulator bothers to
</em><br>
<em>&gt; &gt; actually run them at all, would make a difference. They would just be
</em><br>
<em>&gt; more
</em><br>
<em>&gt; &gt; reasons to do the same things already being done.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It makes a difference to the probability calculations. In the simple
</em><br>
<em>&gt; case, if you can be sure that one simulation has been run, you have a
</em><br>
<em>&gt; 1/2 chance of being in that simulation. But if a recursive simulation
</em><br>
<em>&gt; has been run, you have a much higher chance of being in the
</em><br>
<em>&gt; simulation.
</em><br>
<p><p>If both parties run X simulations, your likelihood of being in one of A's
<br>
simulations rather than B's simulations is proportional to the likelihood of
<br>
A existing in the first place rather than B. As X goes to infinity, this
<br>
likelihood does not change.
<br>
<p>And even if it did change...
<br>
<p>If an actual Turing machine with infinite cycles available
<br>
<em>&gt; to it exists somewhere (and a priori there is no reason to suppose
</em><br>
<em>&gt; that this is impossible, even if it isn't possible in the universe we
</em><br>
<em>&gt; observe), then we might almost certainly be living in a simulation.
</em><br>
<em>&gt;
</em><br>
<p>...there is a priori no reason to suppose it is possible, either. Rationally
<br>
we must give not insignificant probability to both cases.
<br>
<p>But this realisation should have no effect on our behaviour.
<br>
<p><p>This does not follow.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16709.html">Vladimir Nesov: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Previous message:</strong> <a href="16707.html">Norman Noman: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>In reply to:</strong> <a href="16705.html">Stathis Papaioannou: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16711.html">Stathis Papaioannou: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16711.html">Stathis Papaioannou: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16708">[ date ]</a>
<a href="index.html#16708">[ thread ]</a>
<a href="subject.html#16708">[ subject ]</a>
<a href="author.html#16708">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:58 MDT
</em></small></p>
</body>
</html>
