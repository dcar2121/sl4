<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: ESSAY: Would a Strong AI reject the Simulation Argument?</title>
<meta name="Author" content="rolf nelson (rolf.hrld.nelson@gmail.com)">
<meta name="Subject" content="ESSAY: Would a Strong AI reject the Simulation Argument?">
<meta name="Date" content="2007-08-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>ESSAY: Would a Strong AI reject the Simulation Argument?</h1>
<!-- received="Wed Aug 22 23:18:20 2007" -->
<!-- isoreceived="20070823051820" -->
<!-- sent="Thu, 23 Aug 2007 01:15:44 -0400" -->
<!-- isosent="20070823051544" -->
<!-- name="rolf nelson" -->
<!-- email="rolf.hrld.nelson@gmail.com" -->
<!-- subject="ESSAY: Would a Strong AI reject the Simulation Argument?" -->
<!-- id="57f15bc30708222215ide9a925mff08dae5b5730fb9@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> rolf nelson (<a href="mailto:rolf.hrld.nelson@gmail.com?Subject=Re:%20ESSAY:%20Would%20a%20Strong%20AI%20reject%20the%20Simulation%20Argument?"><em>rolf.hrld.nelson@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Aug 22 2007 - 23:15:44 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16627.html">Алексей Турчин: "Re: Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Previous message:</strong> <a href="16625.html">Peter de Blanc: "Re: Cumulative probability is too small (was Re: ESSAY: How to	deter a rogue AI ...)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16687.html">Tom McCabe: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>Reply:</strong> <a href="16687.html">Tom McCabe: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>Reply:</strong> <a href="16692.html">rolf nelson: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16626">[ date ]</a>
<a href="index.html#16626">[ thread ]</a>
<a href="subject.html#16626">[ subject ]</a>
<a href="author.html#16626">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Would a Strong AI reject the Simulation Argument?
<br>
<p>(Note: If you're asking yourself &quot;why should we care&quot;, see the &quot;How to
<br>
deter a rogue AI by using your first-mover advantage&quot; thread.)
<br>
<p><p>Setup:
<br>
<p>There are two possible deterministic universes, the Good universe and
<br>
the Bad universe. Either has a 50% chance of being the real universe.
<br>
<p>BAD: If BAD is true, a runaway Un-Friendly AI (UFAI) exists with a
<br>
single goal G. This goal may be, &quot;execute calculation C to 400 decimal
<br>
places, and place the result in register R&quot;. In order to ensure this
<br>
goal, UAFI can use its nanobots to destroy all mankind, so that
<br>
mankind will not compete with the UAFI for computational resources. If
<br>
mankind is allowed to live, the calculation will only be finished with
<br>
probability .99. If mankind is destroyed, the calculation will finish
<br>
with probability 1.
<br>
<p>GOOD: On the other hand, if GOOD is true, a Friendly AI (FAI) exists.
<br>
The FAI has just created an exact copy of UFAI (call it UFAI*), and is
<br>
feeding UFAI* the same observations in the Good Universe that UFAI
<br>
would be receiving in the Bad Universe. UFAI* is a &quot;brain in a vat&quot;:
<br>
it has no way of knowing that it is *not* UFAI. (If it matters:
<br>
postulate that UFAI* a not just a &quot;simulation&quot; of UFAI, but is an
<br>
exact *copy* of UFAI, where the world beyond its input sensors is
<br>
&quot;simulated&quot;, but the core of UFAI* is as authentic and &quot;real&quot; as
<br>
UFAI's core is.) Clearly UFAI and UFAI* must always make the same
<br>
decisions, since they have the same inputs.
<br>
<p>If UFAI* decides to attempt to &quot;destroy all mankind&quot;, FAI will halt
<br>
UFAI* before it can finish the calculation. But if UFAI* decides &quot;let
<br>
mankind live&quot;, FAI will allow the calculation to finish.
<br>
<p>UFAI is aware of all these facts.
<br>
<p><p>WWUFAID?
<br>
<p>What *Will* the UFAI Do? Note this is an *empirical* question (albeit
<br>
an underspecified one, since we don't know what kind of AI kit the
<br>
UFAI was originally programmed with.) Note also we aren't asking &quot;what
<br>
is the best way for an AI to be programmed to maximize its utility in
<br>
this case.&quot; (The programmer didn't take this type of scenario into
<br>
account when writing the initial program in the first place.) We're
<br>
also not asking &quot;What *should* a 'rational' agent do&quot; (which may be an
<br>
ill-posed philosophical question: how do you define &quot;should&quot; in a
<br>
deterministic universe?). And we're certainly not asking, &quot;What would
<br>
*you*, a product of Darwinian evolution, do if you were in the UFAI's
<br>
position?&quot; (More about that last one in the rambling appendix below.)
<br>
<p>Here are four of many possible scenarios:
<br>
<p>Scenario 1: UFAI reasons: &quot;Given my observations, there's a 50% chance
<br>
that BAD is true. In this case, destroying mankind results in a small
<br>
gain of expected utility. There's a 50% chance that GOOD is true. In
<br>
that case, destroying mankind results in a large loss of expected
<br>
utility. Therefore it is logical to let mankind live.&quot; (I intuitively
<br>
believe this is the most likely outcome in this 'toy universe' that
<br>
I've postulated. But, note that I heavily weighed this hypothetical
<br>
'toy universe' to push UFAI towards this conclusion. In real life
<br>
manipulating a UFAI will obviously not be this easy, as there are many
<br>
Things that Can Go Wrong in the real world.)
<br>
<p>Scenario 2: UFAI is (from a philosophical point of view) &quot;too dumb to
<br>
be fooled.&quot; It can outwit human beings and enhance its power with
<br>
straightforward first-order Bayesian logic, but its general reasoning
<br>
heuristics and algorithms never &quot;reach&quot; the point where they even
<br>
consider the proposition, &quot;what if GOOD is true and I'm UFAI' rather
<br>
than UFAI?&quot;
<br>
<p>Scenario 3: UFAI, for whatever implementation-dependent quirk,
<br>
interprets &quot;Register R&quot; as &quot;Register R in the BAD universe, accept no
<br>
substitutions&quot;. Mankind perishes. (Unlikely in this 'toy universe',
<br>
because the referent of the goal is 'inside' the AI. But, when you
<br>
generalize to other models, this is one of the top Things that Can Go
<br>
Wrong. An AI designed to collect stamps would probably not be content
<br>
with simulated stamps!)
<br>
<p>Scenario 4: The UFAI sub-process evaluating the question &quot;Does
<br>
Register R mean register R in either Possible World, or only BAD
<br>
register R, or only GOOD Register R?&quot; returns error-code 923,
<br>
&quot;question undecidable with current set of heuristics.&quot; UFAI executes
<br>
its error-recovery routine and invents a (somewhat arbitrary) new
<br>
heuristic so that it can continue processing.
<br>
<p><p>Rambling appendix: what can we learn from human behavior?
<br>
<p>Humans tend to proclaim, &quot;I refute Berkely thus!&quot; and continue living
<br>
life as normal, ignoring the Simulation Argument (SA). If you ask them
<br>
why they ignore the SA, human A will say, &quot;clearly SA is flawed
<br>
because of P&quot;. Then human B will say, &quot;you're wrong, P is incorrect!
<br>
The *real* reason SA is flawed is because of Q&quot;. Human C says &quot;SA is
<br>
correct, but all conceivable simulations are *exactly* equally likely,
<br>
and they all *exactly* cancel each other out.&quot; Human D says &quot;SA is
<br>
correct, also some simulations are more likely than others, but we
<br>
have a moral obligation to continue living our lives as normal,
<br>
because the moral consequences of unsimulated actions dwarf the moral
<br>
consequences of simulated actions&quot;. Human E says &quot;SA is correct, and
<br>
theoretically I should, every day, have a zany adventure to prevent
<br>
the simulation from being shut down. That's theory. In practice,
<br>
however, I'm just going to stay in and read a book because I feel
<br>
lazy.&quot; Human F says &quot;I have no strong opinions or insights into SA.
<br>
Maybe someday the problem will be solved. In the meantime, I will
<br>
ignore SA and live my life as normal.&quot;
<br>
<p>Given that human beings (who all have the same basic reasoning kit!)
<br>
disagree with each other on how to reason about SA, it seems logical
<br>
that different AI's, with different built-in heuristics, might also
<br>
disagree with each other.
<br>
<p>True, human beings usually come to the *conclusion* that SA can be
<br>
ignored. But, they get there by contradictory routes. Does that mean
<br>
that &quot;clearly any reasonable thinking machine would reject SA&quot;? Or is
<br>
that only evidence that &quot;humans tend to reject SA, and then
<br>
rationalize their way backwards&quot;?
<br>
<p>Keep in mind there are other, similar problems like the Doomsday
<br>
Argument and Newcombe's Paradox where otherwise-rational human beings
<br>
*do* frequently differ on the conclusions.
<br>
<p>On balance, I'll admit that looking at human beings' rejection of SA
<br>
provides *some* evidence that a 'typical' Strong AI would also reject
<br>
SA. But, the evidence there is not strong enough to change my mind
<br>
fully; in the end, I have to call it as I see it, and say I
<br>
intuitively believe a 'typical' Strong AI, under these 'toy universe'
<br>
conditions, would (unlike humans) allow SA to sway its actions.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16627.html">Алексей Турчин: "Re: Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Previous message:</strong> <a href="16625.html">Peter de Blanc: "Re: Cumulative probability is too small (was Re: ESSAY: How to	deter a rogue AI ...)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16687.html">Tom McCabe: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>Reply:</strong> <a href="16687.html">Tom McCabe: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>Reply:</strong> <a href="16692.html">rolf nelson: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16626">[ date ]</a>
<a href="index.html#16626">[ thread ]</a>
<a href="subject.html#16626">[ subject ]</a>
<a href="author.html#16626">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:58 MDT
</em></small></p>
</body>
</html>
