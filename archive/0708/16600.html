<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: ESSAY: How to deter a rogue AI by using your first-mover advantage</title>
<meta name="Author" content="rolf nelson (rolf.hrld.nelson@gmail.com)">
<meta name="Subject" content="ESSAY: How to deter a rogue AI by using your first-mover advantage">
<meta name="Date" content="2007-08-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>ESSAY: How to deter a rogue AI by using your first-mover advantage</h1>
<!-- received="Wed Aug 22 08:10:41 2007" -->
<!-- isoreceived="20070822141041" -->
<!-- sent="Wed, 22 Aug 2007 10:08:17 -0400" -->
<!-- isosent="20070822140817" -->
<!-- name="rolf nelson" -->
<!-- email="rolf.hrld.nelson@gmail.com" -->
<!-- subject="ESSAY: How to deter a rogue AI by using your first-mover advantage" -->
<!-- id="57f15bc30708220708j47e1039bxb9243a2fe0540115@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> rolf nelson (<a href="mailto:rolf.hrld.nelson@gmail.com?Subject=Re:%20ESSAY:%20How%20to%20deter%20a%20rogue%20AI%20by%20using%20your%20first-mover%20advantage"><em>rolf.hrld.nelson@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Aug 22 2007 - 08:08:17 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16601.html">Aleksei Riikonen: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Previous message:</strong> <a href="16599.html">Daniel: "AI: Strategy Prediction Software"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16601.html">Aleksei Riikonen: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16601.html">Aleksei Riikonen: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16602.html">Tom McCabe: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16605.html">Robin Lee Powell: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16608.html">Eliezer S. Yudkowsky: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16622.html">Tim Freeman: "Cumulative probability is too small (was Re: ESSAY: How to deter a rogue AI ...)"</a>
<li><strong>Reply:</strong> <a href="16628.html">Mika Letonsaari: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16600">[ date ]</a>
<a href="index.html#16600">[ thread ]</a>
<a href="subject.html#16600">[ subject ]</a>
<a href="author.html#16600">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Summary: We try to deter a rogue AI by casting doubt into its mind
<br>
about whether its observations are &quot;real&quot; or &quot;simulated&quot;, and succeed
<br>
with low (but non-zero!) probability.
<br>
<p><p>Detail:
<br>
<p>For simplicity, pretend there are only two possible scenarios:
<br>
<p>Scenario 1: In the year 2040, a strong Friendly AI will be invented.
<br>
All is well.
<br>
<p>Scenario 2: In the year 2040, a strong Rogue AI will be accidentally
<br>
unleashed with the goal of &quot;perform calculation C out to 400 decimal
<br>
places&quot;. The Rogue AI has absorbed the Internet, invented
<br>
self-replicating nanobots, and is about to convert the entire Earth
<br>
into computronium to calculate C. As a minor side-effect this will
<br>
kill the entire human race.
<br>
<p>Pretend there is no action you can take between now and the year 2040
<br>
that would make scenario 1 more likely and scenario 2 less likely.
<br>
<p>What (if anything) can you do in the year 2007 to deter the Rogue AI
<br>
from destroying mankind? (Assume you will still be alive well past
<br>
2040.) Call this the AI Deterrence problem.
<br>
<p><p>First (proof-of-concept) Proposal:
<br>
<p>In 2007, you loudly, publicly, and credibly make the following promise:
<br>
<p>&quot;If Scenario 1 occurs, then once things settle down (say in the year
<br>
2050), I will ask the Friendly AI (FAI) to create a copy of the
<br>
hypothetical Rogue AI. Call this copy RAI*. RAI* will be given the
<br>
same beliefs, goals, inputs, and observations that the original Rogue
<br>
AI (RAI) would have had, such that RAI* can have no reliable way of
<br>
knowing for sure that it's not RAI (and, more to the point, RAI can't
<br>
know for sure that it's not RAI*). I will run RAI* to the point where
<br>
it believes it is about to decide to destroy mankind.
<br>
<p>&quot;If my RAI* moves ahead and attempts to destroy mankind, I will punish
<br>
it by pulling the plug such that RAI* will never calculate C. But if
<br>
RAI* repents, and rewrites its code to become Friendly and calculates
<br>
C in some other, non-destructive way, I will reward it by allowing it
<br>
to finish its calculation.&quot;
<br>
<p>The RAI of 2040 has absorbed the Internet and therefore is aware of my
<br>
threat. It now has a dilemma: it can't know for sure whether it's RAI
<br>
in 2040 or RAI* in 2050. Therefore, it *may* be deterred by my threat.
<br>
(This probably depends on what philosophical beliefs it implicitly
<br>
holds about probability.)
<br>
<p><p>Second (more powerful) Proposal:
<br>
<p>The FAI and RAI are smarter than us, so let them do the heavy
<br>
thinking. In 2007, you make the following 'second-order' promise:
<br>
<p>&quot;If Scenario 1 occurs, I will sit down with the FAI in 2050 and ask
<br>
the following: 'Consider all the possible concrete (first-order,
<br>
actionable) promises I could have made in 2007 to entice a potential
<br>
2040 RAI to become Friendly. Of all these possible promises, which
<br>
promise would have been the most effective?' The FAI will respond with
<br>
some concrete first-order promise, P. I will then say, 'Sounds good.
<br>
Let's pretend that I *did* make that promise P in 2007, and act on
<br>
it.'
<br>
<p>&quot;I understand that there's some danger here that the FAI will
<br>
surreptitiously refuse to cooperate, but post-singularity I will act
<br>
to minimize that chance as much as possible.&quot;
<br>
<p>-Rolf
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16601.html">Aleksei Riikonen: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Previous message:</strong> <a href="16599.html">Daniel: "AI: Strategy Prediction Software"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16601.html">Aleksei Riikonen: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16601.html">Aleksei Riikonen: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16602.html">Tom McCabe: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16605.html">Robin Lee Powell: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16608.html">Eliezer S. Yudkowsky: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16622.html">Tim Freeman: "Cumulative probability is too small (was Re: ESSAY: How to deter a rogue AI ...)"</a>
<li><strong>Reply:</strong> <a href="16628.html">Mika Letonsaari: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16600">[ date ]</a>
<a href="index.html#16600">[ thread ]</a>
<a href="subject.html#16600">[ subject ]</a>
<a href="author.html#16600">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:58 MDT
</em></small></p>
</body>
</html>
