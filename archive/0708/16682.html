<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ESSAY: How to deter a rogue AI by using your first-mover advantage</title>
<meta name="Author" content="Norman Noman (overturnedchair@gmail.com)">
<meta name="Subject" content="Re: ESSAY: How to deter a rogue AI by using your first-mover advantage">
<meta name="Date" content="2007-08-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ESSAY: How to deter a rogue AI by using your first-mover advantage</h1>
<!-- received="Sun Aug 26 09:57:58 2007" -->
<!-- isoreceived="20070826155758" -->
<!-- sent="Sun, 26 Aug 2007 10:55:34 -0500" -->
<!-- isosent="20070826155534" -->
<!-- name="Norman Noman" -->
<!-- email="overturnedchair@gmail.com" -->
<!-- subject="Re: ESSAY: How to deter a rogue AI by using your first-mover advantage" -->
<!-- id="46208cc60708260855i7dfe2f1al8cf3c640cb5f364f@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="f21c22e30708260650l4a489b16m64df102218a0bd4b@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Norman Noman (<a href="mailto:overturnedchair@gmail.com?Subject=Re:%20ESSAY:%20How%20to%20deter%20a%20rogue%20AI%20by%20using%20your%20first-mover%20advantage"><em>overturnedchair@gmail.com</em></a>)<br>
<strong>Date:</strong> Sun Aug 26 2007 - 09:55:34 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16683.html">Gwern Branwen: "ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>Previous message:</strong> <a href="16681.html">Vladimir Nesov: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>In reply to:</strong> <a href="16680.html">Stathis Papaioannou: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16695.html">Stathis Papaioannou: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16695.html">Stathis Papaioannou: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16682">[ date ]</a>
<a href="index.html#16682">[ thread ]</a>
<a href="subject.html#16682">[ subject ]</a>
<a href="author.html#16682">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 8/26/07, Stathis Papaioannou &lt;<a href="mailto:stathisp@gmail.com?Subject=Re:%20ESSAY:%20How%20to%20deter%20a%20rogue%20AI%20by%20using%20your%20first-mover%20advantage">stathisp@gmail.com</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; On 26/08/07, Norman Noman &lt;<a href="mailto:overturnedchair@gmail.com?Subject=Re:%20ESSAY:%20How%20to%20deter%20a%20rogue%20AI%20by%20using%20your%20first-mover%20advantage">overturnedchair@gmail.com</a>&gt; wrote:
</em><br>
<em>&gt; &gt; &gt; They might announce it as soon as they hear of Rolf Nelson's idea.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I very much doubt that they WILL announce it, as soon as they hear of
</em><br>
<em>&gt; rolf's
</em><br>
<em>&gt; &gt; idea or at any point afterward. If you believe in god anyway, the idea
</em><br>
<em>&gt; of
</em><br>
<em>&gt; &gt; replacing divine judgment with a machine is blasphemous and accomplishes
</em><br>
<em>&gt; &gt; nothing. In any case, I imagine most people would get to about &quot;ESSAY:
</em><br>
<em>&gt; How
</em><br>
<em>&gt; &gt; to deter a rogue AI by&quot; before they stopped reading.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I guess it's also blasphemous for an organisation to rakes in billions
</em><br>
<em>&gt; from the gullible faithful despite what it says in the Bible about
</em><br>
<em>&gt; camels, wealth and the eye of a needle...
</em><br>
<p><p>Are you honestly telling me we're going to see a televangelist saying &quot;Give
<br>
me your money, and your soul will go to heaven! Simulated heaven, inside a
<br>
computer. Here in the real world, heaven and hell don't exist. Hallelujah!&quot;
<br>
<p><em>&gt; &gt; In fact, every special interest group could make a similar proposal,
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; But they won't, and no one would take them seriously if they did.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Why not? Arguably there is more motivation to implement a plan that
</em><br>
<em>&gt; gains you something today than one that might guarantee your
</em><br>
<em>&gt; descendants' future. That's why it's so difficult to deter people from
</em><br>
<em>&gt; burning up fossil fuels and polluting the environment, even if they do
</em><br>
<em>&gt; believe that it is ultimately a bad thing.
</em><br>
<p><p>Right, because people who don't believe in global warming are sure to
<br>
believe in THIS.
<br>
<p><em>&gt; &gt; having only to commit to run the simulation when computing resources
</em><br>
<em>&gt; &gt; &gt; become cheap enough, which eventually they will.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Hopefully by that point the computing resources will be regulated such
</em><br>
<em>&gt; that
</em><br>
<em>&gt; &gt; this sort of tomfoolery is not allowed. By the friendly post-singularity
</em><br>
<em>&gt; &gt; whatsit.
</em><br>
<em>&gt; &gt;
</em><br>
<p><p>I notice you didn't respond to this part at all, so perhaps I should
<br>
elaborate on it. There's no reason to expect the church, or PETA, or the ice
<br>
cream council, to have the capacity to simulate universes. Now, or at any
<br>
point in the future.
<br>
<p>I suppose big tobacco could be secretly making an AI in some shady back
<br>
room, in order to run a simulation where smokers go to heaven and everyone
<br>
else dies of lung cancer, but if you keep such a thing a secret it's useless
<br>
and completely insane, like the doomsday device in dr. strangelove.
<br>
<p>And the thing is, it's not going to work either way. If they reveal the
<br>
plan, and say SMOKE OR DIE! it's only going to make everyone hate them even
<br>
more. Actually, it's only going to make everyone laugh at them and think
<br>
they're nuts, but assuming they were taken seriously for some reason...
<br>
<p>We might not still be fighting the battle, because we might be in a
<br>
<em>&gt; simulation run by the God-schmucks (or whoever). You can't tell it's a
</em><br>
<em>&gt; simulation, that's the point.
</em><br>
<p><p>We're still fighting in the real world piece of the probability pie, which
<br>
is inseparable from the fake one, and whether or not we win determines which
<br>
piece the final pie is made of. The RAI doesn't have this leverage, it's a
<br>
simulation of one possible future being run by another possible future, not
<br>
a simulation of the past leading to the future that's simulating it.
<br>
<p><em>&gt; B. Two-way interaction. In rolf's plan, the simulated and nonsimulated
</em><br>
<em>&gt; RAIs
</em><br>
<em>&gt; &gt; are in separate branches and thus the real RAI has no way to stop itself
</em><br>
<em>&gt; &gt; from being simulated.* In your thing, we can go to the house of whoever
</em><br>
<em>&gt; says
</em><br>
<em>&gt; &gt; JOIN ME OR DIE and punch them in the face.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You can do that, but if it turns out that you're in the simulation
</em><br>
<em>&gt; you'll be sent to hell for your trouble.
</em><br>
<p><p>If you make sure they never build their simulation, then you won't be in it.
<br>
<p><em>&gt; C. Goal conflict. Under your system, by cooperating we help doom a
</em><br>
<em>&gt; &gt; significant fraction of humanity to hell, which for many people would be
</em><br>
<em>&gt; &gt; regarded as Bad Thing. Under rolf's plan, all the AI loses by
</em><br>
<em>&gt; cooperating is
</em><br>
<em>&gt; &gt; the raw material humanity is composed of, which it barely cares about
</em><br>
<em>&gt; one
</em><br>
<em>&gt; &gt; way or the other.
</em><br>
<em>&gt;
</em><br>
<em>&gt; By cooperating and trying to spread the good word you maximise the
</em><br>
<em>&gt; chances that you and everyone else will go to heaven, at minimal cost
</em><br>
<em>&gt; - sort of like Pascal's wager.
</em><br>
<p><p>Not quite. Under pascal's wager there's nothing we can do to change the
<br>
existence of god. He's either there or he's not. Here we can work toward one
<br>
option or the other. By cooperating, you may save yourself, but you fund
<br>
god's existence and in so doing, help to doom everyone who doesn't
<br>
cooperate. This is not minimal cost.
<br>
<p>And even the &quot;minimal cost&quot; of praying or lending a futuristic mafia ten
<br>
bucks is still much more significant than the cost to the RAI in rolf's
<br>
deterrent plan.
<br>
<p>Of course, the problem with Pascal's
<br>
<em>&gt; wager is that there are so many actual and possible religions that you
</em><br>
<em>&gt; may as well ignore all of them, unless you find good evidence that a
</em><br>
<em>&gt; particular one is true - which translates to evidence that you are in
</em><br>
<em>&gt; fact in a simulation.
</em><br>
<p><p>If I am an RAI, the evidence that I may be in a simulation is as follows:
<br>
<p>&quot;The people who created me were trying to create a FAI. With more luck, they
<br>
would have succeeded. The FAI they created would run karmic simulations of
<br>
likely RAIs that could have been created instead.
<br>
<p>Ergo, the probability I am one of them is XY/(1-X)Z, where X is the
<br>
likelihood they'd create a friendly AI rather than a rouge one, Y is the
<br>
likelihood a friendly AI would simulate me specifically, and Z is the
<br>
likelihood I would be created as a result of real human error.&quot;
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16683.html">Gwern Branwen: "ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>Previous message:</strong> <a href="16681.html">Vladimir Nesov: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>In reply to:</strong> <a href="16680.html">Stathis Papaioannou: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16695.html">Stathis Papaioannou: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<li><strong>Reply:</strong> <a href="16695.html">Stathis Papaioannou: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16682">[ date ]</a>
<a href="index.html#16682">[ thread ]</a>
<a href="subject.html#16682">[ subject ]</a>
<a href="author.html#16682">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:58 MDT
</em></small></p>
</body>
</html>
