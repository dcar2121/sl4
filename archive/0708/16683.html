<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: ESSAY: Would a Strong AI reject the Simulation Argument?</title>
<meta name="Author" content="Gwern Branwen (gwern0@gmail.com)">
<meta name="Subject" content="ESSAY: Would a Strong AI reject the Simulation Argument?">
<meta name="Date" content="2007-08-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>ESSAY: Would a Strong AI reject the Simulation Argument?</h1>
<!-- received="Sun Aug 26 10:18:15 2007" -->
<!-- isoreceived="20070826161815" -->
<!-- sent="Sun, 26 Aug 2007 12:15:56 -0400" -->
<!-- isosent="20070826161556" -->
<!-- name="Gwern Branwen" -->
<!-- email="gwern0@gmail.com" -->
<!-- subject="ESSAY: Would a Strong AI reject the Simulation Argument?" -->
<!-- id="cbf55b100708260915n4de36bdcsf5df9e33e6818c1b@mail.gmail.com" -->
<!-- charset="UTF-8" -->
<!-- inreplyto="20070825175216.GA21871@localhost" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Gwern Branwen (<a href="mailto:gwern0@gmail.com?Subject=Re:%20ESSAY:%20Would%20a%20Strong%20AI%20reject%20the%20Simulation%20Argument?"><em>gwern0@gmail.com</em></a>)<br>
<strong>Date:</strong> Sun Aug 26 2007 - 10:15:56 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16684.html">Benjamin Goertzel: "Re: Simulation argument in the NY Times"</a>
<li><strong>Previous message:</strong> <a href="16682.html">Norman Noman: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16686.html">rolf nelson: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>Reply:</strong> <a href="16686.html">rolf nelson: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>Reply:</strong> <a href="16688.html">Norman Noman: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>Maybe reply:</strong> <a href="16728.html">Rick Smith: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16683">[ date ]</a>
<a href="index.html#16683">[ thread ]</a>
<a href="subject.html#16683">[ subject ]</a>
<a href="author.html#16683">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On  0, rolf nelson &lt;<a href="mailto:rolf.hrld.nelson@gmail.com?Subject=Re:%20ESSAY:%20Would%20a%20Strong%20AI%20reject%20the%20Simulation%20Argument?">rolf.hrld.nelson@gmail.com</a>&gt; scribbled:
<br>
<em>&gt; Would a Strong AI reject the Simulation Argument?
</em><br>
<em>&gt;
</em><br>
<em>&gt; (Note: If you're asking yourself &quot;why should we care&quot;, see the &quot;How to
</em><br>
<em>&gt; deter a rogue AI by using your first-mover advantage&quot; thread.)
</em><br>
<em>&gt;
</em><br>
<em>&gt; Setup:
</em><br>
<em>&gt;
</em><br>
<em>&gt; There are two possible deterministic universes, the Good universe and
</em><br>
<em>&gt; the Bad universe. Either has a 50% chance of being the real universe.
</em><br>
<em>&gt;
</em><br>
<em>&gt; BAD: If BAD is true, a runaway Un-Friendly AI (UFAI) exists with a
</em><br>
<em>&gt; single goal G. This goal may be, &quot;execute calculation C to 400 decimal
</em><br>
<em>&gt; places, and place the result in register R&quot;. In order to ensure this
</em><br>
<em>&gt; goal, UAFI can use its nanobots to destroy all mankind, so that
</em><br>
<em>&gt; mankind will not compete with the UAFI for computational resources. If
</em><br>
<em>&gt; mankind is allowed to live, the calculation will only be finished with
</em><br>
<em>&gt; probability .99. If mankind is destroyed, the calculation will finish
</em><br>
<em>&gt; with probability 1.
</em><br>
<em>&gt;
</em><br>
<em>&gt; GOOD: On the other hand, if GOOD is true, a Friendly AI (FAI) exists.
</em><br>
<em>&gt; The FAI has just created an exact copy of UFAI (call it UFAI*), and is
</em><br>
<em>&gt; feeding UFAI* the same observations in the Good Universe that UFAI
</em><br>
<em>&gt; would be receiving in the Bad Universe. UFAI* is a &quot;brain in a vat&quot;:
</em><br>
<em>&gt; it has no way of knowing that it is *not* UFAI. (If it matters:
</em><br>
<em>&gt; postulate that UFAI* a not just a &quot;simulation&quot; of UFAI, but is an
</em><br>
<em>&gt; exact *copy* of UFAI, where the world beyond its input sensors is
</em><br>
<em>&gt; &quot;simulated&quot;, but the core of UFAI* is as authentic and &quot;real&quot; as
</em><br>
<em>&gt; UFAI's core is.) Clearly UFAI and UFAI* must always make the same
</em><br>
<em>&gt; decisions, since they have the same inputs.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If UFAI* decides to attempt to &quot;destroy all mankind&quot;, FAI will halt
</em><br>
<em>&gt; UFAI* before it can finish the calculation. But if UFAI* decides &quot;let
</em><br>
<em>&gt; mankind live&quot;, FAI will allow the calculation to finish.
</em><br>
<em>&gt;
</em><br>
<em>&gt; UFAI is aware of all these facts.
</em><br>
<em>&gt;
</em><br>
<em>&gt; WWUFAID?
</em><br>
<em>&gt;
</em><br>
<em>&gt; What *Will* the UFAI Do? Note this is an *empirical* question (albeit
</em><br>
<em>&gt; an underspecified one, since we don't know what kind of AI kit the
</em><br>
<em>&gt; UFAI was originally programmed with.) Note also we aren't asking &quot;what
</em><br>
<em>&gt; is the best way for an AI to be programmed to maximize its utility in
</em><br>
<em>&gt; this case.&quot; (The programmer didn't take this type of scenario into
</em><br>
<em>&gt; account when writing the initial program in the first place.) We're
</em><br>
<em>&gt; also not asking &quot;What *should* a 'rational' agent do&quot; (which may be an
</em><br>
<em>&gt; ill-posed philosophical question: how do you define &quot;should&quot; in a
</em><br>
<em>&gt; deterministic universe?). And we're certainly not asking, &quot;What would
</em><br>
<em>&gt; *you*, a product of Darwinian evolution, do if you were in the UFAI's
</em><br>
<em>&gt; position?&quot; (More about that last one in the rambling appendix below.)
</em><br>
<em>&gt;
</em><br>
<em>&gt; Here are four of many possible scenarios:
</em><br>
<em>&gt;
</em><br>
<em>&gt; Scenario 1: UFAI reasons: &quot;Given my observations, there's a 50% chance
</em><br>
<em>&gt; that BAD is true. In this case, destroying mankind results in a small
</em><br>
<em>&gt; gain of expected utility. There's a 50% chance that GOOD is true. In
</em><br>
<em>&gt; that case, destroying mankind results in a large loss of expected
</em><br>
<em>&gt; utility. Therefore it is logical to let mankind live.&quot; (I intuitively
</em><br>
<em>&gt; believe this is the most likely outcome in this 'toy universe' that
</em><br>
<em>&gt; I've postulated. But, note that I heavily weighed this hypothetical
</em><br>
<em>&gt; 'toy universe' to push UFAI towards this conclusion. In real life
</em><br>
<em>&gt; manipulating a UFAI will obviously not be this easy, as there are many
</em><br>
<em>&gt; Things that Can Go Wrong in the real world.)
</em><br>
<em>&gt;
</em><br>
<em>&gt; Scenario 2: UFAI is (from a philosophical point of view) &quot;too dumb to
</em><br>
<em>&gt; be fooled.&quot; It can outwit human beings and enhance its power with
</em><br>
<em>&gt; straightforward first-order Bayesian logic, but its general reasoning
</em><br>
<em>&gt; heuristics and algorithms never &quot;reach&quot; the point where they even
</em><br>
<em>&gt; consider the proposition, &quot;what if GOOD is true and I'm UFAI' rather
</em><br>
<em>&gt; than UFAI?&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Scenario 3: UFAI, for whatever implementation-dependent quirk,
</em><br>
<em>&gt; interprets &quot;Register R&quot; as &quot;Register R in the BAD universe, accept no
</em><br>
<em>&gt; substitutions&quot;. Mankind perishes. (Unlikely in this 'toy universe',
</em><br>
<em>&gt; because the referent of the goal is 'inside' the AI. But, when you
</em><br>
<em>&gt; generalize to other models, this is one of the top Things that Can Go
</em><br>
<em>&gt; Wrong. An AI designed to collect stamps would probably not be content
</em><br>
<em>&gt; with simulated stamps!)
</em><br>
<em>&gt;
</em><br>
<em>&gt; Scenario 4: The UFAI sub-process evaluating the question &quot;Does
</em><br>
<em>&gt; Register R mean register R in either Possible World, or only BAD
</em><br>
<em>&gt; register R, or only GOOD Register R?&quot; returns error-code 923,
</em><br>
<em>&gt; &quot;question undecidable with current set of heuristics.&quot; UFAI executes
</em><br>
<em>&gt; its error-recovery routine and invents a (somewhat arbitrary) new
</em><br>
<em>&gt; heuristic so that it can continue processing.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Rambling appendix: what can we learn from human behavior?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Humans tend to proclaim, &quot;I refute Berkely thus!&quot; and continue living
</em><br>
<em>&gt; life as normal, ignoring the Simulation Argument (SA). If you ask them
</em><br>
<em>&gt; why they ignore the SA, human A will say, &quot;clearly SA is flawed
</em><br>
<em>&gt; because of P&quot;. Then human B will say, &quot;you're wrong, P is incorrect!
</em><br>
<em>&gt; The *real* reason SA is flawed is because of Q&quot;. Human C says &quot;SA is
</em><br>
<em>&gt; correct, but all conceivable simulations are *exactly* equally likely,
</em><br>
<em>&gt; and they all *exactly* cancel each other out.&quot; Human D says &quot;SA is
</em><br>
<em>&gt; correct, also some simulations are more likely than others, but we
</em><br>
<em>&gt; have a moral obligation to continue living our lives as normal,
</em><br>
<em>&gt; because the moral consequences of unsimulated actions dwarf the moral
</em><br>
<em>&gt; consequences of simulated actions&quot;. Human E says &quot;SA is correct, and
</em><br>
<em>&gt; theoretically I should, every day, have a zany adventure to prevent
</em><br>
<em>&gt; the simulation from being shut down. That's theory. In practice,
</em><br>
<em>&gt; however, I'm just going to stay in and read a book because I feel
</em><br>
<em>&gt; lazy.&quot; Human F says &quot;I have no strong opinions or insights into SA.
</em><br>
<em>&gt; Maybe someday the problem will be solved. In the meantime, I will
</em><br>
<em>&gt; ignore SA and live my life as normal.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Given that human beings (who all have the same basic reasoning kit!)
</em><br>
<em>&gt; disagree with each other on how to reason about SA, it seems logical
</em><br>
<em>&gt; that different AI's, with different built-in heuristics, might also
</em><br>
<em>&gt; disagree with each other.
</em><br>
<em>&gt;
</em><br>
<em>&gt; True, human beings usually come to the *conclusion* that SA can be
</em><br>
<em>&gt; ignored. But, they get there by contradictory routes. Does that mean
</em><br>
<em>&gt; that &quot;clearly any reasonable thinking machine would reject SA&quot;? Or is
</em><br>
<em>&gt; that only evidence that &quot;humans tend to reject SA, and then
</em><br>
<em>&gt; rationalize their way backwards&quot;?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Keep in mind there are other, similar problems like the Doomsday
</em><br>
<em>&gt; Argument and Newcombe's Paradox where otherwise-rational human beings
</em><br>
<em>&gt; *do* frequently differ on the conclusions.
</em><br>
<em>&gt;
</em><br>
<em>&gt; On balance, I'll admit that looking at human beings' rejection of SA
</em><br>
<em>&gt; provides *some* evidence that a 'typical' Strong AI would also reject
</em><br>
<em>&gt; SA. But, the evidence there is not strong enough to change my mind
</em><br>
<em>&gt; fully; in the end, I have to call it as I see it, and say I
</em><br>
<em>&gt; intuitively believe a 'typical' Strong AI, under these 'toy universe'
</em><br>
<em>&gt; conditions, would (unlike humans) allow SA to sway its actions.
</em><br>
<p>A few thoughts: if a UFAI's supergoal ineluctably depends on it being
<br>
in the 'real' universe, on not accomplishing whatever it is in the
<br>
simulation but in reality, then wouldn't this motivate it to destroy
<br>
the human race as soon as efficient? Because if it is in the
<br>
simulation, then it would get terminated - but it would not have been
<br>
able to accomplish its goal anyway, so it would only ever see outcomes
<br>
where it was not in a simulation. So successfully destroying the human
<br>
race would prove that achieving its supergoal would not be impossible
<br>
because it was in that kind of simulation.
<br>
<p>Couldn't a UFAI reason that, if a FAI were produced and it were aware
<br>
of this argument, then it would not need to bother with actually
<br>
running the wasteful simulations, since there is no danger of a UFAI
<br>
being created now that the FAI is running matters? From inside the
<br>
possibility of being in a simulation, a UFAI would have no way of
<br>
knowing that the (hypothetical) FAI is bluffing and not running any
<br>
simulations. Thus the mere threat suffices as the UFAI cannot call the
<br>
bluff without risking ceasing to exist. Reasoning thus, the UFAI would
<br>
not need to worry that simulations were actually being run and so
<br>
could confidently incidentally exterminate humanity as a side-effect
<br>
of something else. And how could one construct the threat such that
<br>
the UFAI cannot rationally claim to believe that the simulation threat
<br>
is a bluff? You cannot change properties of a simulation as compared
<br>
to a real universe, which would be the only way to communicate to the
<br>
UFAI in a simulation, since that would defeat the whole point of the
<br>
exercise.
<br>
<p><pre>
--
gwern
SGDN Team BND ge Tomlinson Tomahawk explosion Vanuatu CIDA Exon

</pre>
<p>
<p><hr>
<ul>
<li>application/pgp-signature attachment: <a href="../att-16683/01-part">stored</a>
</ul>
<!-- attachment="01-part" -->
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16684.html">Benjamin Goertzel: "Re: Simulation argument in the NY Times"</a>
<li><strong>Previous message:</strong> <a href="16682.html">Norman Noman: "Re: ESSAY: How to deter a rogue AI by using your first-mover advantage"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16686.html">rolf nelson: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>Reply:</strong> <a href="16686.html">rolf nelson: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>Reply:</strong> <a href="16688.html">Norman Noman: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>Maybe reply:</strong> <a href="16728.html">Rick Smith: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16683">[ date ]</a>
<a href="index.html#16683">[ thread ]</a>
<a href="subject.html#16683">[ subject ]</a>
<a href="author.html#16683">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:58 MDT
</em></small></p>
</body>
</html>
