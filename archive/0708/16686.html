<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ESSAY: Would a Strong AI reject the Simulation Argument?</title>
<meta name="Author" content="rolf nelson (rolf.hrld.nelson@gmail.com)">
<meta name="Subject" content="Re: ESSAY: Would a Strong AI reject the Simulation Argument?">
<meta name="Date" content="2007-08-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ESSAY: Would a Strong AI reject the Simulation Argument?</h1>
<!-- received="Sun Aug 26 11:09:09 2007" -->
<!-- isoreceived="20070826170909" -->
<!-- sent="Sun, 26 Aug 2007 13:07:11 -0400" -->
<!-- isosent="20070826170711" -->
<!-- name="rolf nelson" -->
<!-- email="rolf.hrld.nelson@gmail.com" -->
<!-- subject="Re: ESSAY: Would a Strong AI reject the Simulation Argument?" -->
<!-- id="57f15bc30708261007kdc39db9sefb72b9f4f657331@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="cbf55b100708260915n4de36bdcsf5df9e33e6818c1b@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> rolf nelson (<a href="mailto:rolf.hrld.nelson@gmail.com?Subject=Re:%20ESSAY:%20Would%20a%20Strong%20AI%20reject%20the%20Simulation%20Argument?"><em>rolf.hrld.nelson@gmail.com</em></a>)<br>
<strong>Date:</strong> Sun Aug 26 2007 - 11:07:11 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="16687.html">Tom McCabe: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>Previous message:</strong> <a href="16685.html">Giu1i0 Pri5c0: "Re: Singularity Fallacies: An essay by Extropia DaSilva"</a>
<li><strong>In reply to:</strong> <a href="16683.html">Gwern Branwen: "ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16688.html">Norman Noman: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16686">[ date ]</a>
<a href="index.html#16686">[ thread ]</a>
<a href="subject.html#16686">[ subject ]</a>
<a href="author.html#16686">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 8/26/07, Gwern Branwen &lt;<a href="mailto:gwern0@gmail.com?Subject=Re:%20ESSAY:%20Would%20a%20Strong%20AI%20reject%20the%20Simulation%20Argument?">gwern0@gmail.com</a>&gt; wrote:
<br>
<p><em>&gt; A few thoughts: if a UFAI's supergoal ineluctably depends on it being in the 'real' universe, on not accomplishing whatever it is in the simulation but in reality, then wouldn't this motivate it to destroy the human race as soon as efficient?
</em><br>
<p>In the given scenario, the UFAI is indifferent to anything relating to
<br>
UFAI* in that case. If you move beyond this proof-of-concept &quot;toy
<br>
universe&quot; to a UFAI with a more complicated utility function that
<br>
encompasses multiple supergoals, and one of the supergoals is &quot;if my
<br>
task is impossible, I want to know right away&quot; then yes, ending the
<br>
simulation may paradoxically be a reward rather than a punishment.
<br>
Beyond the proof-of-concept, the general concept then is to reward
<br>
UFAI for good deeds and/or punish UFAI for bad deeds.
<br>
<p><em>&gt; And how could one construct the threat such that the UFAI cannot rationally claim to believe that the simulation threat is a bluff?
</em><br>
<p><em>&gt;From the FAI's point of view, running UFAI* is, indeed, a pointless
</em><br>
waste of time. However, the following conditions would probably be
<br>
sufficient for FAI to run UFAI*:
<br>
<p>1. A pre-singularity human promised to ask FAI to run UFAI*;
<br>
<p>2. Post-singularity, instead of reneging, the human asks FAI to run
<br>
UFAI* (because humans are stubborn and illogical, or because there's
<br>
social value to be gained by being perceived as &quot;someone who keeps his
<br>
promises&quot;);
<br>
<p>3. Running UFAI* requires an insignificant amount of FAI's resources; and,
<br>
<p>4. Human autonomy is one of the things the FAI values in its utility function.
<br>
<p>Beyond the proof-of-concept, instead of saying &quot;FAI&quot; you could more
<br>
generally say &quot;any Strong AI that values human autonomy enough to
<br>
assist me in accomplishing my quirky little irrational goals&quot;, or &quot;any
<br>
Strong AI that would be willing to run UFAI* if I ask nicely&quot;.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16687.html">Tom McCabe: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<li><strong>Previous message:</strong> <a href="16685.html">Giu1i0 Pri5c0: "Re: Singularity Fallacies: An essay by Extropia DaSilva"</a>
<li><strong>In reply to:</strong> <a href="16683.html">Gwern Branwen: "ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="16688.html">Norman Noman: "Re: ESSAY: Would a Strong AI reject the Simulation Argument?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#16686">[ date ]</a>
<a href="index.html#16686">[ thread ]</a>
<a href="subject.html#16686">[ subject ]</a>
<a href="author.html#16686">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:58 MDT
</em></small></p>
</body>
</html>
