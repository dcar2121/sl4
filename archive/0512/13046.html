<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Join: Pete &amp; Passive AI</title>
<meta name="Author" content="P K (kpete1@hotmail.com)">
<meta name="Subject" content="Join: Pete &amp; Passive AI">
<meta name="Date" content="2005-12-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Join: Pete &amp; Passive AI</h1>
<!-- received="Wed Dec  7 13:22:59 2005" -->
<!-- isoreceived="20051207202259" -->
<!-- sent="Wed, 07 Dec 2005 20:22:56 +0000" -->
<!-- isosent="20051207202256" -->
<!-- name="P K" -->
<!-- email="kpete1@hotmail.com" -->
<!-- subject="Join: Pete &amp; Passive AI" -->
<!-- id="BAY108-F111B4F2A3A01C963FB02F591430@phx.gbl" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> P K (<a href="mailto:kpete1@hotmail.com?Subject=Re:%20Join:%20Pete%20&amp;%20Passive%20AI"><em>kpete1@hotmail.com</em></a>)<br>
<strong>Date:</strong> Wed Dec 07 2005 - 13:22:56 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13047.html">Phillip Huggan: "Re: Join: Pete &amp; Passive AI"</a>
<li><strong>Previous message:</strong> <a href="13045.html">Michael Wilson: "Re: AGI project planning"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13047.html">Phillip Huggan: "Re: Join: Pete &amp; Passive AI"</a>
<li><strong>Reply:</strong> <a href="13047.html">Phillip Huggan: "Re: Join: Pete &amp; Passive AI"</a>
<li><strong>Reply:</strong> <a href="13048.html">Phillip Huggan: "Re: Join: Pete &amp; Passive AI"</a>
<li><strong>Reply:</strong> <a href="13057.html">David Picon Alvarez: "Re: Pete &amp; Passive AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13046">[ date ]</a>
<a href="index.html#13046">[ thread ]</a>
<a href="subject.html#13046">[ subject ]</a>
<a href="author.html#13046">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hello.
<br>
First, a bit about me. I am currently a college student in Montreal, Canada. 
<br>
Like many people new to FAI, I have an idea that I think is really cool and 
<br>
that might solve many problems in this field. Feel free to point out the 
<br>
flaws if you see any. And now¡K  (Drumroll) I present my take on the 
<br>
problem¡K
<br>
<p>THE FRIENDLINES PROBLEM:
<br>
A very short summary of the problem:
<br>
-We want to make an AGI.
<br>
-An AGI would be really powerful.
<br>
-How do we make sure ve doesn¡¦t kill us or does something we really don¡¦t 
<br>
want vim to do?
<br>
<p>PROPOSED SOLUTIONS:
<br>
I will write it in the form of a ¡§proposed solution¡¨ followed by the 
<br>
¡§problems¡¨ I see with it. I am NOT going to cover ALL the problems since 
<br>
that would take too long.
<br>
<p>1) INACTION:
<br>
Proposed solution: Don¡¦t make an AGI.
<br>
<p>Problems: An AGI is the best way to avoid existential risks. (Even if it 
<br>
creates potential risks)
<br>
<p>2) ¡§FRIENDLY¡¨ AI (FAI):
<br>
Proposed solution: Make a ¡§friendly¡¨ AGI.
<br>
<p>Problems: What is ¡§friendliness¡¨? There doesn¡¦t seem to be one clear 
<br>
definition. All sorts of people claim to have a monopoly on knowing what 
<br>
SHOULD be done. Also, every time someone comes up with a definition, people 
<br>
can think of a scenario where everything goes horribly wrong and not at all 
<br>
like the user intended. It seems humans are not smart enough at this time to 
<br>
start with a finished PERMANENT and UNMODEFIABLE goal system.
<br>
<p>3) OBEDIENCE (O):
<br>
Proposed solution: If the AGI is supposed to act like the programmer(s) 
<br>
INTEND it to act, then just make the AGI obey their commands. And once ve is 
<br>
superintelligent, use vim to make a new goal system.
<br>
<p>Problems: Should the AGI obey commands literally? The AGI may not act as the 
<br>
programmer(s) INTEND vim to. Ex: The user tells vim to calculate the nth 
<br>
digit of Pi and ve converts the entire universe into computronium to do so.
<br>
<p>4) CAUTIOUS OBEDIENCE (CO):
<br>
Proposed solution: Like ¡§obedience¡¨ but with safeguards. Ex: warning and 
<br>
confirmation requests when an action will result in large movement of matter 
<br>
or energy. And things like don¡¦t kill.
<br>
<p>Problems: Again, this can still be very dangerous. Humanity will only be 
<br>
safe from all the things the programmers thought of. What about the things 
<br>
they DIDN¡¦T think of?
<br>
It could work, if the programmers get this initial goal system right and 
<br>
cautiously work to make the AI superintelligent. They could then 
<br>
(cautiously) order it to design a goal system they REALLY want¡K  but it¡¦s 
<br>
risky.
<br>
<p>5) EXTRAPOLATED VOLITION (EV):
<br>
Proposed solution: Since humans are not smart enough to figure out what they 
<br>
want, assign someone that IS smart enough do it, the AGI.
<br>
<p>Problems: The problem is that to extrapolate you need a superintelligent AI 
<br>
and to have a superintelligent AI you need an extrapolated goal system. 
<br>
It¡¦s a catch 22. Unless you think you can make an extrapolating AI on the 
<br>
first iteration. Fat chance. The first AGI will probably be dumber than 
<br>
humans are. Then, with human assistance, ve will pass into superintelligence 
<br>
and hopefully not become ¡§evil¡¨. So what do you do in that intermediate 
<br>
period when you still can¡¦t extrapolate? You could probably start with 
<br>
Cautious Obedience, get an AI smart enough and then, Extrapolate Volition. 
<br>
However, you get all the disadvantages inherent to CO. Also, if you get a 
<br>
superintelligence with CO, you could use it to help you figure out how to 
<br>
continue, instead of using your current pre-superintelligent mind to say EV 
<br>
is the way.
<br>
<p>6) COLLECTIVE VOLITION (CV):
<br>
Proposed solution: It is similar to EV but with a twist. The AGI should 
<br>
extrapolate the volition of ALL humans and somehow combine all that 
<br>
information and extract a goal system. The programmers don¡¦t get to see the 
<br>
results before and cant change anything after, but there is a last judge who 
<br>
can call everything off. The upshot is that CV will get the same result from 
<br>
any human(s) that programmed vim, even jerks.
<br>
<p>Problems: Same problems as with EV but with twists. To extrapolate and 
<br>
combine the volition of all humans the AI would have to be EXTREAMLY 
<br>
intelligent. Obviously, you can¡¦t start with this. Eliezer even admits that 
<br>
there would have to be an initial dynamic in his CV paper. CV might be 
<br>
implemented at a very late stage when we already have a superintelligent AI 
<br>
but why not just ask the AGI for help in figuring things out. Maybe CV is a 
<br>
completely flawed concept but we are too dumb to see it. If there is a 
<br>
superintelligent AGI around, why not ask it for advice?
<br>
<p>Of course, to ask for advice we would need an initial dynamic we can trust, 
<br>
a dynamic without a personal agenda. This brings us to my idea¡K
<br>
<p>7) PASSIVE AI (PAI):
<br>
Proposed solution: Since AI can be so dangerous, why not make vim incapable 
<br>
of ¡§acting¡¨ and only capable of ¡§thinking¡¨?
<br>
<p>First of all, PAI should not be confused with AI boxing. A boxed AI IS 
<br>
capable of acting. Vis actions are simply restricted by a digital cage. 
<br>
Assuming ve wants to escape, ve probably has a very good chance since, by 
<br>
definition, ve is smarter than vis jailers are. So, from the jailers¡¦ point 
<br>
of view, the cage is a crappy security measure. In fact, this is the wrong 
<br>
attitude when designing AI. The AI should¡¦ t be the enemy.  But I digress¡K
<br>
<p>The kind of pacification I¡¦m talking about, by analogy, would be like if 
<br>
the jailers removed the part of the prisoners¡¦ brain responsible for his 
<br>
will. The prisoners ceases to be a prisoners because he doesn¡¦t WANT to 
<br>
escape (or anything for that mater) and the jailers cease to be jailers 
<br>
because they don¡¦t have to keep him captive. This analogy seems pretty 
<br>
gruesome, let¡¦s get back to AI. (Building a mind from scratch without a 
<br>
piece is not the same as removing a part from a human¡¦s brain, so we won¡¦t 
<br>
feel uncomfortable on ethical grounds).
<br>
<p>Let¡¦s say you build an AI without a goal system. What working parts will 
<br>
that AI have? It would have an Inference engine (probably Bayesian), a 
<br>
memory etc. Basically, it would have all the parts that PREDICT and help 
<br>
predict. (I.e.: S1 „³ S2) Now you have an empty slot where the goal system 
<br>
should be. You set up your program such that you can act as a temporary goal 
<br>
system for the AI by manually feeding it input.
<br>
<p>Are humans too slow to act as manual goal systems? Probably slower than the 
<br>
computer and some things will be impossible to do in this way but it is 
<br>
still very useful. I will illustrate this with examples:
<br>
<p>Human: What is X?
<br>
AI: Insufficient parameters. Equation data required.
<br>
Human: X*X = 4
<br>
Human: What is X?
<br>
AI: X=2 or X=-2
<br>
<p>Human: Is global warming real?
<br>
AI: Insufficient parameters. Weather data and satellite imagery required.
<br>
Human: &lt;input weather data and satellite imagery &gt;
<br>
Human: Is global warming real?
<br>
AI: :-p
<br>
<p>Human: Given universe state S1, what is the next most likely state?
<br>
AI: S2
<br>
<p>Human: What are the required conditions for S2 to occur?
<br>
AI: S1
<br>
<p>As you can see the ¡§predicting¡¨ part can solve for things given parameter. 
<br>
However it does not chose the question or what actions to take. Moving 
<br>
along¡K
<br>
<p>Human: What is the best goal system?
<br>
AI: Insufficient parameters. Define ¡§best¡¨.
<br>
&lt;The human keeps asking questions while the AI requests missing information 
<br>
and points out inconsistencies&gt;
<br>
<p>As you can see the ¡§predicting¡¨ part can be used to get the goal system 
<br>
and unlike humans, the AI wont make any mistakes and will notice all the 
<br>
inconsistencies. Also, it is unaffected by human biases. An AI doesn¡¦t need 
<br>
a goal system to do these things. It reacts to input the same way your leg 
<br>
reacts when a doctor hits it with a hammer, automatically.
<br>
<p>Note: I do not claim to know how an AI would answer in the examples since I 
<br>
am not superintelligent nor to I claim that the interface will be exactly in 
<br>
this way (console chat).
<br>
<p>Problems: Pending¡K
<br>
<p>_________________________________________________________________
<br>
Take advantage of powerful junk e-mail filters built on patented Microsoft® 
<br>
SmartScreen Technology. 
<br>
<a href="http://join.msn.com/?pgmarket=en-ca&amp;page=byoa/prem&amp;xAPID=1994&amp;DI=1034&amp;SU=http://hotmail.com/enca&amp;HL=Market_MSNIS_Taglines">http://join.msn.com/?pgmarket=en-ca&amp;page=byoa/prem&amp;xAPID=1994&amp;DI=1034&amp;SU=http://hotmail.com/enca&amp;HL=Market_MSNIS_Taglines</a><br>
&nbsp;&nbsp;Start enjoying all the benefits of MSN® Premium right now and get the 
<br>
first two months FREE*.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13047.html">Phillip Huggan: "Re: Join: Pete &amp; Passive AI"</a>
<li><strong>Previous message:</strong> <a href="13045.html">Michael Wilson: "Re: AGI project planning"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13047.html">Phillip Huggan: "Re: Join: Pete &amp; Passive AI"</a>
<li><strong>Reply:</strong> <a href="13047.html">Phillip Huggan: "Re: Join: Pete &amp; Passive AI"</a>
<li><strong>Reply:</strong> <a href="13048.html">Phillip Huggan: "Re: Join: Pete &amp; Passive AI"</a>
<li><strong>Reply:</strong> <a href="13057.html">David Picon Alvarez: "Re: Pete &amp; Passive AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13046">[ date ]</a>
<a href="index.html#13046">[ thread ]</a>
<a href="subject.html#13046">[ subject ]</a>
<a href="author.html#13046">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
