<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Passive AI</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="Re: Passive AI">
<meta name="Date" content="2005-12-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Passive AI</h1>
<!-- received="Fri Dec  9 07:54:44 2005" -->
<!-- isoreceived="20051209145444" -->
<!-- sent="Fri, 9 Dec 2005 14:54:41 +0000 (GMT)" -->
<!-- isosent="20051209145441" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="Re: Passive AI" -->
<!-- id="20051209145441.52550.qmail@web26708.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="BAY108-F26DC84144643E883F76F6E91420@phx.gbl" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20Passive%20AI"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Fri Dec 09 2005 - 07:54:41 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13083.html">Michael Wilson: "Re: Pete &amp; Passive AI"</a>
<li><strong>Previous message:</strong> <a href="13081.html">Michael Wilson: "Re: On The Nature Of Qualia"</a>
<li><strong>In reply to:</strong> <a href="13075.html">P K: "Re:Passive AI     was[Join: Pete &amp; Passive AI]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13086.html">nuzz604: "Re: Passive AI"</a>
<li><strong>Maybe reply:</strong> <a href="13086.html">nuzz604: "Re: Passive AI"</a>
<li><strong>Reply:</strong> <a href="13088.html">Phillip Huggan: "Re: Passive AI"</a>
<li><strong>Reply:</strong> <a href="13105.html">Nick Bostrom: "Re: Passive AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13082">[ date ]</a>
<a href="index.html#13082">[ thread ]</a>
<a href="subject.html#13082">[ subject ]</a>
<a href="author.html#13082">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
This proposal seems to me rather like Nick Bostrom's
<br>
preffered option of building an 'Orcale'. This isn't
<br>
as easy as it looks, even if you start from the position
<br>
that it's pretty damn hard (sticking with the AGI norm
<br>
then :) ), but I've been studying ways to do it and it
<br>
does still look to me much easier than building any of
<br>
Yudkowsky's FAI proposals.
<br>
<p>An Oracle is of course still a tremendously dangerous
<br>
thing to have around. Ask it for any kind of plan of
<br>
action and you are allowing it to optimise reality to
<br>
the maximum degree permitted by using you as intermediary,
<br>
in effect bringing up a marginally diluted version of the
<br>
'understanding intent' problem without any efforts to
<br>
directly solve it (e.g. the way EV attempts to). I must
<br>
reluctently classify Nick Bostrom's proposal to make an
<br>
Oracle generally available (or at least, publically known
<br>
and available to experts) as hopelessly naive. Clearly
<br>
there is vast potential for misuse and abuse that would
<br>
be unavoidable if publically known, at least in the short
<br>
space of time before some fool asks one how to build a
<br>
seed AI that will help them with their personal goals. It
<br>
does seem likely to me that an Orcale built and used in
<br>
secret, by sufficiently moral and cautious researchers,
<br>
would be a net reduction in risk for an FAI project. 
<br>
<p><em>&gt; That would never happen. For the AI to give an order it
</em><br>
<em>&gt; would have to have a goal system. Passive AI does NOT
</em><br>
<em>&gt; have a goal system.
</em><br>
<p>What you mean by 'goal system', and what I would classify
<br>
a 'goal system', are almost certainly very different. I
<br>
would say that any system that consistently achieves
<br>
something specific (including answering questions) has at
<br>
least an implicit goal system, and that if you want to
<br>
make sure that an AGI-class system won't optimise parts of
<br>
the world that you want it to leave alone (i.e. actually
<br>
act like an Oracle) then you /must/ analyse it in
<br>
goal-seeking terms.
<br>
<p><em>&gt; Various other parts do various things. The important
</em><br>
<em>&gt; thing is that only the “wanting” part can initiate action
</em><br>
<p>The reasoning components have the goal of providing accurate
<br>
information about unseen parts of the external world. This
<br>
goal won't avoid optimising the external world unless you
<br>
explicitly constrain it thus. It may still make sense to
<br>
turn the world into computronium to solve some abstract
<br>
problem, or to optimise the world to make the answer easier
<br>
to compute, or just to perform experiments to gain useful
<br>
experimental data. Any attempt to rule all this out has to
<br>
be stable under self-modification, as you have to use seed
<br>
AI to get Oracle-grade predictive capability, which means
<br>
you still have to solve some basic structural FAI issues
<br>
before you can do anything useful safely.
<br>
<p><em>&gt; Now, we interface his brain with a computer such that we
</em><br>
<em>&gt; could send him “will” thoughts via electric pulses.
</em><br>
<p>Humans are a bad example for this because utility and
<br>
calibration are (near-)hopelessly entangled and conflated
<br>
in human thinking (a fact reflected by all those AGI designs
<br>
that implicitly use a single 'activation' parameter to
<br>
represent both). But your point stands if we imagine a
<br>
Platonic 'completely rational' human.
<br>
<p><em>&gt; As you can see, he is still quite useful. I can browse his
</em><br>
<em>&gt; knowledge and get various insights from him. However, Mr.
</em><br>
<em>&gt; A is completely passive. He doesn’t want ANYTHING.
</em><br>
<p>This kind of thing would work just fine for say a classic
<br>
symbolic AI reasoning system, but I'm pretty sure you can't
<br>
get away with anything so simplistic for any system capable
<br>
of AGI (even without considering the 'stability under
<br>
self-modification' issue).
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
How much free photo storage do you get? Store your holiday 
<br>
snaps for FREE with Yahoo! Photos <a href="http://uk.photos.yahoo.com">http://uk.photos.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13083.html">Michael Wilson: "Re: Pete &amp; Passive AI"</a>
<li><strong>Previous message:</strong> <a href="13081.html">Michael Wilson: "Re: On The Nature Of Qualia"</a>
<li><strong>In reply to:</strong> <a href="13075.html">P K: "Re:Passive AI     was[Join: Pete &amp; Passive AI]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13086.html">nuzz604: "Re: Passive AI"</a>
<li><strong>Maybe reply:</strong> <a href="13086.html">nuzz604: "Re: Passive AI"</a>
<li><strong>Reply:</strong> <a href="13088.html">Phillip Huggan: "Re: Passive AI"</a>
<li><strong>Reply:</strong> <a href="13105.html">Nick Bostrom: "Re: Passive AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13082">[ date ]</a>
<a href="index.html#13082">[ thread ]</a>
<a href="subject.html#13082">[ subject ]</a>
<a href="author.html#13082">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
