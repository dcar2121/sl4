<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Please Re-read CAFAI</title>
<meta name="Author" content="Tennessee Leeuwenburg (tennessee@tennessee.id.au)">
<meta name="Subject" content="Re: Please Re-read CAFAI">
<meta name="Date" content="2005-12-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Please Re-read CAFAI</h1>
<!-- received="Tue Dec 13 23:32:38 2005" -->
<!-- isoreceived="20051214063238" -->
<!-- sent="Wed, 14 Dec 2005 17:32:03 +1100" -->
<!-- isosent="20051214063203" -->
<!-- name="Tennessee Leeuwenburg" -->
<!-- email="tennessee@tennessee.id.au" -->
<!-- subject="Re: Please Re-read CAFAI" -->
<!-- id="439FBC63.1060004@tennessee.id.au" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="22360fa10512132213y150e752ame67301ab7cef961a@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tennessee Leeuwenburg (<a href="mailto:tennessee@tennessee.id.au?Subject=Re:%20Please%20Re-read%20CAFAI"><em>tennessee@tennessee.id.au</em></a>)<br>
<strong>Date:</strong> Tue Dec 13 2005 - 23:32:03 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13154.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<li><strong>Previous message:</strong> <a href="13152.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<li><strong>In reply to:</strong> <a href="13152.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13155.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<li><strong>Reply:</strong> <a href="13155.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13153">[ date ]</a>
<a href="index.html#13153">[ thread ]</a>
<a href="subject.html#13153">[ subject ]</a>
<a href="author.html#13153">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
First up, I may be guilty as charged of skim reading. I read the email, 
<br>
but I also had in mind a skim of the recent topics and emails.
<br>
<p>Jef Allbright wrote:
<br>
<p><em>&gt;On 12/13/05, Tennessee Leeuwenburg &lt;<a href="mailto:tennessee@tennessee.id.au?Subject=Re:%20Please%20Re-read%20CAFAI">tennessee@tennessee.id.au</a>&gt; wrote:
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;Jef Allbright wrote:
</em><br>
<em>&gt;&gt;    
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;Would you like to comment on the questions I posed to Michael?
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;      
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;I thought that I had done so. I will be specific.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&quot;Do you mean that to the extent an agent is rational, it will naturally
</em><br>
<em>&gt;&gt;use all of its instrumental knowledge to promote its own goals and from
</em><br>
<em>&gt;&gt;its point of view there would be no question that such action is good?&quot;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;The Categorial Imperative (CI) is not a source of morality, but a
</em><br>
<em>&gt;&gt;description of how to make (orignially, moral) rules. Any number of
</em><br>
<em>&gt;&gt;moral positions are possible under this system, and as such the CI is no
</em><br>
<em>&gt;&gt;guarantee of Friendliness.
</em><br>
<em>&gt;&gt;    
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;It seems to me that Michael pointed out that the CI is irrelevant to
</em><br>
<em>&gt;AI decision-making.  I would agree with him on that, and had intended
</em><br>
<em>&gt;to explore the implications of our (possibly) shared line of thought.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Frankly, it's difficult for me to see a connection between your
</em><br>
<em>&gt;response here and the intended question.  Note that I carefully said
</em><br>
<em>&gt;&quot;from it's point of view&quot; so as not to suggest that I'm assuming some
</em><br>
<em>&gt;sort of objective good.  I am curious as to how you (or Michael) might
</em><br>
<em>&gt;answer this question.  Wouldn't you agree that any agent would say
</em><br>
<em>&gt;that which promotes its goals is good from its point of view,
</em><br>
<em>&gt;independent of any moral theory?
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
Perhaps I misunderstood the question. If so, then it will be hard for me 
<br>
to answer clearly :)
<br>
<p>Well, the CI is by definition an objective thing. I do *not* believe 
<br>
that it's obvious that all AIs would be relativists about morality. 
<br>
However, I believe it's most likely so we don't need to argue about it.
<br>
<p>The intended question has a double-entendre. The first meaning is that 
<br>
&quot;all my goals are good from my point of view, by definition, so I don't 
<br>
need to question it&quot;. The second meaning is &quot;I have no conception of 
<br>
morality, therefore I don't need to question it&quot;.
<br>
<p>You can have Categorical Truths even under an amoral goal system, and 
<br>
you can have multiple possible moral goals systems in accordance with 
<br>
Categorical Imperatives.
<br>
<p>I think it's concievable that an AGI might have no moral sense, and be 
<br>
bound only by consequential reasoning about its goals. I also think it's 
<br>
concievable than an AGI would have a moral sense, but due to its mental 
<br>
model have varying beliefs about good and evil, despite its conclusions 
<br>
about what is objectively true.
<br>
<p><em>&gt;&gt;&quot;If this is true, then would it also see increasing its objective
</em><br>
<em>&gt;&gt;knowledge in support of its goals as rational and inherently good (from
</em><br>
<em>&gt;&gt;its point of view?)&quot;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Not necessarily. It may consider knowledge to be inherently morally
</em><br>
<em>&gt;&gt;neutral, although in consequential terms accumulated knowledge may be
</em><br>
<em>&gt;&gt;morally valuable. An AGI acting under CI would desire to accumulate
</em><br>
<em>&gt;&gt;objective knowledge as it related to its goals, but not necessarily see
</em><br>
<em>&gt;&gt;it as good in itself.
</em><br>
<em>&gt;&gt;    
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;I wasn't making any claim about something being good in itself.  I was
</em><br>
<em>&gt;careful each time to frame &quot;good&quot; as the subjective evaluation by an
</em><br>
<em>&gt;agent with regard to whether some action promoted its goals.
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
There are a lot of abstractions here. Subjectivity doesn't destroy my 
<br>
point. Even from its own point of view, it might be that some knowledge 
<br>
is good, and some is not good, and some is neutral. An AGI might regard 
<br>
knowledge as good, if it contributes to its goals, for example. If that 
<br>
were the litmus test, then subjectively speaking, some knowledge would 
<br>
be good, and some would be morally neutral.
<br>
<p>Perhaps you missed what I see as the obvious logical opposite -- that 
<br>
the AGI adopts, subjectively speaking, the &quot;belief&quot; 
<br>
(goal/desire/whatever) than Knowledge Is Good. In this case, the AGI 
<br>
desires knowledge *as an end in itself* and not *solely for its 
<br>
contribution to other goals*.
<br>
<p><em>&gt;&gt;&quot;If I'm still understanding the implications of what you said, would
</em><br>
<em>&gt;&gt;this also mean that cooperation with other like-minded agents, to the
</em><br>
<em>&gt;&gt;extent that this increased the promotion of its own goals, would be
</em><br>
<em>&gt;&gt;rational and good (from its point of view?)&quot;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Obviously, in the simple case.
</em><br>
<em>&gt;&gt;    
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Interesting that you seemed to disagree with the previous assertions,
</em><br>
<em>&gt;but seemed to agree with this one that I thought was posed within the
</em><br>
<em>&gt;same framework.  It seems as if you were not reading carefully, and
</em><br>
<em>&gt;responding to what you assumed might have been said.
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
Well, you just said something that was true this time :). I'm not going 
<br>
to change my mind because you said it in the context of a wider flawed 
<br>
argument ;).
<br>
<p>Let's suppose than an AGI assesses some possible course of action -- in 
<br>
this case one of interaction and co-operation. It works out that 
<br>
pursuing it will contribute to the furtherance of its own goals.
<br>
<p>It certainly isn't going to think that such a thing is either evil or 
<br>
bad. It may have no sense of morality, but in the sense of advantageous, 
<br>
such an action will be good.
<br>
<p>Let me then clarify: yes, such a thing will always be advantageous. 
<br>
Insofar as an AGI has a moral system, such a thing will also be morally 
<br>
good.
<br>
<p><em>&gt;&gt;I can't work out who made the top-level comment in this email, but the
</em><br>
<em>&gt;&gt;suggestion was that CI might be relevant to an AI,
</em><br>
<em>&gt;&gt;    
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Yes, another poster did make that assertion.
</em><br>
<em>&gt;
</em><br>
<em>&gt;So, if you would be interesting in responding to those questions, but
</em><br>
<em>&gt;understanding that I am certainly not arguing for the CI, I would be
</em><br>
<em>&gt;interested in your further comments.
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
I hope I have managed to stay &quot;on point&quot; this time around.
<br>
<p>Cheers,
<br>
-T
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13154.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<li><strong>Previous message:</strong> <a href="13152.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<li><strong>In reply to:</strong> <a href="13152.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13155.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<li><strong>Reply:</strong> <a href="13155.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13153">[ date ]</a>
<a href="index.html#13153">[ thread ]</a>
<a href="subject.html#13153">[ subject ]</a>
<a href="author.html#13153">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
