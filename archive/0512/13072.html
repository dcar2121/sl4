<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Pete &amp; Passive AI</title>
<meta name="Author" content="Phillip Huggan (cdnprodigy@yahoo.com)">
<meta name="Subject" content="Re: Pete &amp; Passive AI">
<meta name="Date" content="2005-12-08">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Pete &amp; Passive AI</h1>
<!-- received="Thu Dec  8 11:58:43 2005" -->
<!-- isoreceived="20051208185843" -->
<!-- sent="Thu, 8 Dec 2005 10:58:40 -0800 (PST)" -->
<!-- isosent="20051208185840" -->
<!-- name="Phillip Huggan" -->
<!-- email="cdnprodigy@yahoo.com" -->
<!-- subject="Re: Pete &amp; Passive AI" -->
<!-- id="20051208185840.7685.qmail@web61316.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="737b61f30512080954lcdb47ffm126f703ae1e91281@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Phillip Huggan (<a href="mailto:cdnprodigy@yahoo.com?Subject=Re:%20Pete%20&amp;%20Passive%20AI"><em>cdnprodigy@yahoo.com</em></a>)<br>
<strong>Date:</strong> Thu Dec 08 2005 - 11:58:40 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13073.html">Phillip Huggan: "Re: An evolutionary strategy for AGI"</a>
<li><strong>Previous message:</strong> <a href="13071.html">micah glasser: "An evolutionary strategy for AGI"</a>
<li><strong>In reply to:</strong> <a href="13070.html">Chris Capel: "Re: Pete &amp; Passive AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13083.html">Michael Wilson: "Re: Pete &amp; Passive AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13072">[ date ]</a>
<a href="index.html#13072">[ thread ]</a>
<a href="subject.html#13072">[ subject ]</a>
<a href="author.html#13072">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
No way.  Equating PAI with AI boxing is circular reasoning.  From the entire state-space of AGI optimization processes, FAI attempts to select an optimal solution from the state-space subset where we don't go extinct, among other things.  PAI further reduces the subset volume to those solutions where it must rely on human actors to act (not defining PAI computation as an act).  I don't know if it is actually possible to hit such a small target.  My examples of computronium as an active AGI danger and mass suicide prevention as a PAI safeguard were horrible.  Consider a boxed AGI that seemed was actually UFAI; was intent on killing us off.  It has been concluded that by using gravity wave magic or pychological persuasion, or even hypnotic screen interface flashes, the AGI would get out.  But the key question is would an AGI programmed in its unchangeable goal-structures to only offer suggestions be able to kill us off?  I'm pretty sure that if the answer is no, it would still be able
<br>
&nbsp;to be friendly and extinguish imminent extinction threats in all but the most dire geo-political/technological environments even with the human actor handicap.
<br>
<p>Chris Capel &lt;<a href="mailto:pdf23ds@gmail.com?Subject=Re:%20Pete%20&amp;%20Passive%20AI">pdf23ds@gmail.com</a>&gt; wrote:   On 12/8/05, Phillip Huggan wrote:
<br>
<em>&gt; But if PAI spit out
</em><br>
<em>&gt; a course of action like &quot;okay, now you have to let me online, and then all
</em><br>
<em>&gt; kill yourselves&quot;, we could blast ver servers with a shotgun. If FAI fails
</em><br>
<em>&gt; because of some fluke error in the philosophy or logic of the goal system we
</em><br>
<em>&gt; give ver, we are steamrolled. If PAI fails the same way, we have a chance
</em><br>
<em>&gt; for human intervention before we finish building the steamroller ve is
</em><br>
<em>&gt; giving us the blueprints to.
</em><br>
<p>Passive AI reduces to the AI boxing problem, plain and simple. If a
<br>
superintelligent PAI fails, it's no longer passive. Any constraints we
<br>
try to enable to ensure that it remains passive are an AI box, by
<br>
definition.
<br>
<p>Encouraging passivity is a good precaution to take, (and I presume one
<br>
that SIAI is planning on taking,) but it doesn't mitigate the pressing
<br>
need for Friendliness, not even a little.
<br>
<p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
---------------------------------
<br>
Yahoo! Shopping
<br>
&nbsp;Find Great Deals on Holiday Gifts at Yahoo! Shopping 
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13073.html">Phillip Huggan: "Re: An evolutionary strategy for AGI"</a>
<li><strong>Previous message:</strong> <a href="13071.html">micah glasser: "An evolutionary strategy for AGI"</a>
<li><strong>In reply to:</strong> <a href="13070.html">Chris Capel: "Re: Pete &amp; Passive AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13083.html">Michael Wilson: "Re: Pete &amp; Passive AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13072">[ date ]</a>
<a href="index.html#13072">[ thread ]</a>
<a href="subject.html#13072">[ subject ]</a>
<a href="author.html#13072">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
