<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: guaranteeing friendliness</title>
<meta name="Author" content="Herb Martin (HerbM@LearnQuick.Com)">
<meta name="Subject" content="RE: guaranteeing friendliness">
<meta name="Date" content="2005-12-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: guaranteeing friendliness</h1>
<!-- received="Sat Dec  3 23:07:03 2005" -->
<!-- isoreceived="20051204060703" -->
<!-- sent="Sat, 3 Dec 2005 22:06:56 -0800" -->
<!-- isosent="20051204060656" -->
<!-- name="Herb Martin" -->
<!-- email="HerbM@LearnQuick.Com" -->
<!-- subject="RE: guaranteeing friendliness" -->
<!-- id="EIQYMCX-0008W8-DH@mail2.learnquick.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="43924DC9.1000108@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Herb Martin (<a href="mailto:HerbM@LearnQuick.Com?Subject=RE:%20guaranteeing%20friendliness"><em>HerbM@LearnQuick.Com</em></a>)<br>
<strong>Date:</strong> Sat Dec 03 2005 - 23:06:56 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13011.html">Ben Goertzel: "Re: guaranteeing friendliness [more about reaching AGI now that Ben has improved the thread]"</a>
<li><strong>Previous message:</strong> <a href="13009.html">Herb Martin: "RE: guaranteeing friendliness [more about reaching AGI now that Ben has improved the thread]"</a>
<li><strong>In reply to:</strong> <a href="13005.html">Eliezer S. Yudkowsky: "Re: guaranteeing friendliness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13006.html">Eliezer S. Yudkowsky: "Re: guaranteeing friendliness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13010">[ date ]</a>
<a href="index.html#13010">[ thread ]</a>
<a href="subject.html#13010">[ subject ]</a>
<a href="author.html#13010">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; There is a fallacy oft-committed in discussion of Artificial 
</em><br>
<em>&gt; Intelligence, especially AI of superhuman capability.  Someone says: 
</em><br>
<em>&gt; &quot;When technology advances far enough we'll be able to build minds far 
</em><br>
From: Eliezer S. Yudkowsky
<br>
<em>&gt; Herb,
</em><br>
<p>This reply thoroughly snips and chops up
<br>
the (Eliezer's) original -- I am assuming
<br>
that anyone interested read that full and
<br>
incitely post:
<br>
<p><em>&gt; surpassing human intelligence.  A superintelligence could 
</em><br>
<em>&gt; build enormous 
</em><br>
<em>&gt; cheesecakes - cheesecakes the size of cities - by golly, the 
</em><br>
<em>&gt; future will 
</em><br>
<em>&gt; be full of giant cheesecakes!&quot;  The question is whether the 
</em><br>
<em>&gt; superintelligence wants to build giant cheesecakes.  The vision leaps 
</em><br>
<em>&gt; directly from capability to actuality, without considering 
</em><br>
<em>&gt; the necessary 
</em><br>
<em>&gt; intermediate of motive.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; People often immediately declare that Friendly AI is an 
</em><br>
<em>&gt; impossibility, 
</em><br>
<em>&gt; because any sufficiently powerful AI will be able to modify its own 
</em><br>
<em>&gt; source code to break any constraints placed upon it.
</em><br>
<p>Note please:  I never claimed this.  I claimed that 
<br>
guaranteeing a Friendly AI is not provably possible
<br>
and insisted that we should in fact try in any case
<br>
to do so.
<br>
<p>The problem is not with cheesecakes but with the real 
<br>
metagoals we give such an AI (and possibly with any
<br>
evolution, a word which gives the right flavor better
<br>
than development) of those goals.
<br>
<p>We certainly would not want to be the person who claimed
<br>
that &quot;man would never fly&quot; but we can agree with the
<br>
thousands of engineers who accept that it is a practical
<br>
impossibility to guarantee the safety of any particular
<br>
aircraft (so far.)
<br>
<p>With AI, we may not get the multitude of second chances
<br>
to learn from our mistakes -- it is highly improbable
<br>
that our first successes will be guaranteed to be friendly.
<br>
<p>This doesn't mean we won't succeed in FAI, but rather that
<br>
it is highly likely to take multiple tries, and we may not
<br>
get those additional attempts.
<br>
<p><em>&gt; The first flaw you should notice is a Giant Cheesecake 
</em><br>
<em>&gt; Fallacy.  Any AI 
</em><br>
<em>&gt; with free access to its own source would, in principle, possess the 
</em><br>
<em>&gt; ability to modify its own source code in a way that changed the AI's 
</em><br>
<em>&gt; optimization target (the region into which the AI tries to steer 
</em><br>
<em>&gt; possible futures).  This does not imply the AI has the motive 
</em><br>
<em>&gt; to change 
</em><br>
<em>&gt; its own motives.  I would not knowingly swallow a pill that made me 
</em><br>
<em>&gt; enjoy killing babies, because currently I prefer that babies not die.
</em><br>
<p>And if the AI is told &quot;don't kill any babies&quot; or &quot;do not allow
<br>
babies to die&quot; what possible actions might it take to do that?
<br>
<p>(And ignore movie-stupid misinterpretations where it just 
<br>
destroys everyone so there are never any babies which MIGHT 
<br>
die in the future.)
<br>
<p>Even the range of actions from such metagoals are very difficult
<br>
to predict.
<br>
<p><em>&gt; pretty nice and wish they were nicer.  At this point there are any 
</em><br>
<em>&gt; number of vaguely plausible reasons why Friendly AI might be humanly 
</em><br>
<em>&gt; impossible, and it is still more likely that the problem is 
</em><br>
<em>&gt; solvable but 
</em><br>
<em>&gt; no one will get around to solving it in time.  But one should not so 
</em><br>
<em>&gt; quickly write off the challenge, especially considering the stakes.
</em><br>
<em>&gt; 
</em><br>
<p>I do not believe that it is humanly impossible but rather
<br>
that it is impossible to PROVE (or guarantee) it, a priori.
<br>
<p>And yes, we should NOT write off attempts to do it correctly,
<br>
especially considering the stakes.
<br>
<p><p><pre>
--
Herb Martin
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13011.html">Ben Goertzel: "Re: guaranteeing friendliness [more about reaching AGI now that Ben has improved the thread]"</a>
<li><strong>Previous message:</strong> <a href="13009.html">Herb Martin: "RE: guaranteeing friendliness [more about reaching AGI now that Ben has improved the thread]"</a>
<li><strong>In reply to:</strong> <a href="13005.html">Eliezer S. Yudkowsky: "Re: guaranteeing friendliness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13006.html">Eliezer S. Yudkowsky: "Re: guaranteeing friendliness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13010">[ date ]</a>
<a href="index.html#13010">[ thread ]</a>
<a href="subject.html#13010">[ subject ]</a>
<a href="author.html#13010">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
