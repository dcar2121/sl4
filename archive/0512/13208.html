<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Destruction of All Humanity</title>
<meta name="Author" content="Olie L (neomorphy@hotmail.com)">
<meta name="Subject" content="Re: Destruction of All Humanity">
<meta name="Date" content="2005-12-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Destruction of All Humanity</h1>
<!-- received="Thu Dec 15 21:46:04 2005" -->
<!-- isoreceived="20051216044604" -->
<!-- sent="Fri, 16 Dec 2005 15:46:02 +1100" -->
<!-- isosent="20051216044602" -->
<!-- name="Olie L" -->
<!-- email="neomorphy@hotmail.com" -->
<!-- subject="Re: Destruction of All Humanity" -->
<!-- id="BAY106-F156C16B3E066D59512E5EDBA3A0@phx.gbl" -->
<!-- inreplyto="948b11e0512151834i4f81adb9i3901e91df805afd0@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Olie L (<a href="mailto:neomorphy@hotmail.com?Subject=Re:%20Destruction%20of%20All%20Humanity"><em>neomorphy@hotmail.com</em></a>)<br>
<strong>Date:</strong> Thu Dec 15 2005 - 21:46:02 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13209.html">Michael Vassar: "Re: Not the only way to build an AI"</a>
<li><strong>Previous message:</strong> <a href="13207.html">Samantha Atkins: "Re: Destruction of All Humanity"</a>
<li><strong>In reply to:</strong> <a href="13207.html">Samantha Atkins: "Re: Destruction of All Humanity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13111.html">pdugan: "RE: Destruction of All Humanity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13208">[ date ]</a>
<a href="index.html#13208">[ thread ]</a>
<a href="subject.html#13208">[ subject ]</a>
<a href="author.html#13208">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt;From: Samantha Atkins &lt;<a href="mailto:sjatkins@gmail.com?Subject=Re:%20Destruction%20of%20All%20Humanity">sjatkins@gmail.com</a>&gt;
</em><br>
<em>&gt;Reply-To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Destruction%20of%20All%20Humanity">sl4@sl4.org</a>
</em><br>
<em>&gt;To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Destruction%20of%20All%20Humanity">sl4@sl4.org</a>
</em><br>
<em>&gt;Subject: Re: Destruction of All Humanity
</em><br>
<em>&gt;Date: Thu, 15 Dec 2005 18:34:32 -0800
</em><br>
<em>&gt;
</em><br>
<em>&gt;Better for whom, Ben?
</em><br>
<p>How about better for the aggregate of sentient beings - human and non human 
<br>
alike?
<br>
<p>(I don't believe the following, I'm devilsadvocating)
<br>
<p>It's not that uncommon to believe that &quot;the world&quot; would be better off by 
<br>
killing particular sets of sentient beings - frinstance, the aggregate of 
<br>
humans would be better off by killing a set of humans (such as Nazis).
<br>
<p>It's not so un-logical to extend this to suggest that all sentient beings 
<br>
could be better off by removing a small proportionate set of sentient 
<br>
beings, even if that small proportionate set happens to include an entire 
<br>
species.
<br>
<p>If, for example, there were a set of hypothetical insects (very large in 
<br>
number), that are sentient and highly intelligent, that humans constantly 
<br>
and unavoidably killed (think of how often we kill ants, whether we mean to 
<br>
or not), and furthermore, since these intelligent insects happen to appear 
<br>
repulsive to us, many humans are strongly predilected to wanting to kill 
<br>
them, even if those humans should happen to learn that those ugly insects 
<br>
are sentient.  Does the idea of asking 5 billion humans to voluntarily end 
<br>
their lives, in order to save the lives of many many trillions of insects 
<br>
seem quite so unreasonable, when considering that the plans of humans could 
<br>
be perpetuated with AI?
<br>
<p>Of course, it does seem far-fetched that there wouldn't be &quot;better&quot; viable 
<br>
alternatives.
<br>
<p>==Out of ridiculous exampleville==
<br>
<p>Self sacrifice will never seem like the best choice from the personal 
<br>
self-interest POV of the sacrificee.  However, often our own experiences are 
<br>
less valuable than our interests.  We care more for our childrens' wellbeing 
<br>
than we do for our experiences of our childrens' wellbeing, and this is not 
<br>
only our hormones doing the thinking... self-sacrifice can be laudable under 
<br>
most rational moral systems.
<br>
<p>Forcibly killing a person for the betterment of others induces queasiness, 
<br>
but many humans can find this reasonable... a wise being persuading a person 
<br>
that it might be in the aggregate good to self-sacrifice can invoke the same 
<br>
queasiness, but that doesn't mean that such actions cannot be intellectually 
<br>
defensible.
<br>
<p>-- Olie
<br>
<p><em>&gt;Do you believe in some Universal Better that trumps
</em><br>
<em>&gt;the very existence of large groups of sentient beings - the only type of
</em><br>
<em>&gt;beings that &quot;better&quot; can have any meaning for?    How could it be better to
</em><br>
<em>&gt;an intelligence capable of simulating an entire world and even a universe
</em><br>
<em>&gt;for humans to exist in with an infinitesimal fraction of its abilities?  I
</em><br>
<em>&gt;do not believe simply destroying entire species of sentient beings when
</em><br>
<em>&gt;there are viable alternatives could qualify as &quot;better&quot; - certainly not 
</em><br>
<em>&gt;form
</em><br>
<em>&gt;the pov of said beings. I don't find it particularly intelligent to use our
</em><br>
<em>&gt;intelligence to make our own utter destruction &quot;reasonable&quot;.   I would 
</em><br>
<em>&gt;fight
</em><br>
<em>&gt;such an AI.  I might not last long but I wouldn't simply agree.
</em><br>
<em>&gt;
</em><br>
<em>&gt;-- samantha
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;On 12/12/05, Ben Goertzel &lt;<a href="mailto:ben@goertzel.org?Subject=Re:%20Destruction%20of%20All%20Humanity">ben@goertzel.org</a>&gt; wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Hi,
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I don't normally respond for other people nor for organizations I
</em><br>
<em>&gt; &gt; don't belong to, but in this case, since no one from SIAI has
</em><br>
<em>&gt; &gt; responded yet and the allegation is so silly, I'll make an exception.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; No, this is not SIAI's official opinion, and I am also quite sure that
</em><br>
<em>&gt; &gt; it is it not Eliezer's opinion.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Whether it is *like* anything Eliezer has ever said is a different
</em><br>
<em>&gt; &gt; question, and depends upon your similarity measure!
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Speaking for myself now (NOT Eliezer or anyone else): I can imagine a
</em><br>
<em>&gt; &gt; scenario where I created an AGI to decide, based on my own value
</em><br>
<em>&gt; &gt; system, what would be the best outcome for the universe.  I can
</em><br>
<em>&gt; &gt; imagine working with this AGI long enough that I really trusted it,
</em><br>
<em>&gt; &gt; and then having this AGI conclude that the best outcome for the
</em><br>
<em>&gt; &gt; universe involves having the human race (including me) stop existing
</em><br>
<em>&gt; &gt; and having our particles used in some different way.  I can imagine,
</em><br>
<em>&gt; &gt; in this scenario, having a significant desire to actually go along
</em><br>
<em>&gt; &gt; with the AGI's opinion, though I doubt that I would do so.  (Perhaps I
</em><br>
<em>&gt; &gt; would do so if I were wholly convinced that the overall state of the
</em><br>
<em>&gt; &gt; universe would be a LOT better if the human race's particles were thus
</em><br>
<em>&gt; &gt; re-purposed?)
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; And, I suppose someone could twist the above paragraph to say that
</em><br>
<em>&gt; &gt; &quot;Ben Goertzel says if a superintelligence should order all humans to
</em><br>
<em>&gt; &gt; die, then all humans should die.&quot;  But it would be quite a
</em><br>
<em>&gt; &gt; misrepresentation...
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; -- Ben G
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; On 12/12/05, 1Arcturus &lt;<a href="mailto:arcturus12453@yahoo.com?Subject=Re:%20Destruction%20of%20All%20Humanity">arcturus12453@yahoo.com</a>&gt; wrote:
</em><br>
<em>&gt; &gt; &gt; Someone on the wta-list recently posted an opinion that he attribtuted
</em><br>
<em>&gt; &gt; to
</em><br>
<em>&gt; &gt; &gt; Mr. Yudkowsky, something to the effect that if a superintelligence
</em><br>
<em>&gt; &gt; should
</em><br>
<em>&gt; &gt; &gt; order all humans to die, then all humans should die.
</em><br>
<em>&gt; &gt; &gt; Is that a wild misrepresentation, and like nothing that Mr. Yudkowsky
</em><br>
<em>&gt; &gt; has
</em><br>
<em>&gt; &gt; &gt; ever said?
</em><br>
<em>&gt; &gt; &gt; Or is it in fact his opinion, and that of SIAI?
</em><br>
<em>&gt; &gt; &gt; Just curious...
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; gej
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt; ________________________________
</em><br>
<em>&gt; &gt; &gt; Yahoo! Shopping
</em><br>
<em>&gt; &gt; &gt; Find Great Deals on Holiday Gifts at Yahoo! Shopping
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13209.html">Michael Vassar: "Re: Not the only way to build an AI"</a>
<li><strong>Previous message:</strong> <a href="13207.html">Samantha Atkins: "Re: Destruction of All Humanity"</a>
<li><strong>In reply to:</strong> <a href="13207.html">Samantha Atkins: "Re: Destruction of All Humanity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13111.html">pdugan: "RE: Destruction of All Humanity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13208">[ date ]</a>
<a href="index.html#13208">[ thread ]</a>
<a href="subject.html#13208">[ subject ]</a>
<a href="author.html#13208">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
