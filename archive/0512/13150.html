<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Please Re-read CAFAI</title>
<meta name="Author" content="Tennessee Leeuwenburg (tennessee@tennessee.id.au)">
<meta name="Subject" content="Re: Please Re-read CAFAI">
<meta name="Date" content="2005-12-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Please Re-read CAFAI</h1>
<!-- received="Tue Dec 13 22:27:55 2005" -->
<!-- isoreceived="20051214052755" -->
<!-- sent="Wed, 14 Dec 2005 16:27:20 +1100" -->
<!-- isosent="20051214052720" -->
<!-- name="Tennessee Leeuwenburg" -->
<!-- email="tennessee@tennessee.id.au" -->
<!-- subject="Re: Please Re-read CAFAI" -->
<!-- id="439FAD38.9000209@tennessee.id.au" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="22360fa10512132108m359e796eq80d980077985a15e@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tennessee Leeuwenburg (<a href="mailto:tennessee@tennessee.id.au?Subject=Re:%20Please%20Re-read%20CAFAI"><em>tennessee@tennessee.id.au</em></a>)<br>
<strong>Date:</strong> Tue Dec 13 2005 - 22:27:20 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13151.html">micah glasser: "Re: Please Re-read CAFAI"</a>
<li><strong>Previous message:</strong> <a href="13149.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<li><strong>In reply to:</strong> <a href="13149.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13152.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<li><strong>Reply:</strong> <a href="13152.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13150">[ date ]</a>
<a href="index.html#13150">[ thread ]</a>
<a href="subject.html#13150">[ subject ]</a>
<a href="author.html#13150">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Jef Allbright wrote:
<br>
<p><em>&gt;On 12/13/05, Tennessee Leeuwenburg &lt;<a href="mailto:tennessee@tennessee.id.au?Subject=Re:%20Please%20Re-read%20CAFAI">tennessee@tennessee.id.au</a>&gt; wrote:
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;Jef Allbright wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;    
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;On 12/13/05, Michael Vassar &lt;<a href="mailto:michaelvassar@hotmail.com?Subject=Re:%20Please%20Re-read%20CAFAI">michaelvassar@hotmail.com</a>&gt; wrote:
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;      
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;&gt;The same confusion relates to the discussion of the categorical imperative.
</em><br>
<em>&gt;&gt;&gt;&gt;The categorical imperative simply makes no sense for an AI.  It doesn't tell
</em><br>
<em>&gt;&gt;&gt;&gt;the AI what to want universally done.  Rational entities WILL do what their
</em><br>
<em>&gt;&gt;&gt;&gt;goal system tells them to do.  They don't need &quot;ethics&quot; in the human sense
</em><br>
<em>&gt;&gt;&gt;&gt;of rules countering other inclinations.  What they need is inclinations
</em><br>
<em>&gt;&gt;&gt;&gt;compatible with ours.
</em><br>
<em>&gt;&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;&gt;        
</em><br>
<em>&gt;&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;Let me see if I can understand what you're saying here.  Do you mean
</em><br>
<em>&gt;&gt;&gt;that to the extent an agent is rational, it will naturally use all of
</em><br>
<em>&gt;&gt;&gt;its instrumental knowledge to promote its own goals and from its point
</em><br>
<em>&gt;&gt;&gt;of view there would be no question that such action is good?
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;If this is true, then would it also see increasing its objective
</em><br>
<em>&gt;&gt;&gt;knowledge in support of its goals as rational and inherently good
</em><br>
<em>&gt;&gt;&gt;(from its point of view?)
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;If I'm still understanding the implications of what you said, would
</em><br>
<em>&gt;&gt;&gt;this also mean that cooperation with other like-minded agents, to the
</em><br>
<em>&gt;&gt;&gt;extent that this increased the promotion of its own goals, would be
</em><br>
<em>&gt;&gt;&gt;rational and good (from its point of view?)
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;If this makes sense, then I think you may be on to an effective and
</em><br>
<em>&gt;&gt;&gt;rational way of looking at decision-making about &quot;right&quot; and &quot;wrong&quot;
</em><br>
<em>&gt;&gt;&gt;that avoids much of the contradiction of conventional views of
</em><br>
<em>&gt;&gt;&gt;morality.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;- Jef
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;      
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;Perhaps I can simplify this argument.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;The Categorical Imperative theory is an &quot;is&quot; not an &quot;ought&quot;.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Cheers,
</em><br>
<em>&gt;&gt;-T
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;    
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Huh?  Thanks for playing.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Would you like to comment on the questions I posed to Michael?
</em><br>
<em>&gt;  
</em><br>
<em>&gt;
</em><br>
I thought that I had done so. I will be specific.
<br>
<p>&quot;Do you mean that to the extent an agent is rational, it will naturally 
<br>
use all of its instrumental knowledge to promote its own goals and from 
<br>
its point of view there would be no question that such action is good?&quot;
<br>
<p>The Categorial Imperative (CI) is not a source of morality, but a 
<br>
description of how to make (orignially, moral) rules. Any number of 
<br>
moral positions are possible under this system, and as such the CI is no 
<br>
guarantee of Friendliness.
<br>
<p>&quot;If this is true, then would it also see increasing its objective 
<br>
knowledge in support of its goals as rational and inherently good (from 
<br>
its point of view?)&quot;
<br>
<p>Not necessarily. It may consider knowledge to be inherently morally 
<br>
neutral, although in consequential terms accumulated knowledge may be 
<br>
morally valuable. An AGI acting under CI would desire to accumulate 
<br>
objective knowledge as it related to its goals, but not necessarily see 
<br>
it as good in itself.
<br>
<p>&quot;If I'm still understanding the implications of what you said, would 
<br>
this also mean that cooperation with other like-minded agents, to the 
<br>
extent that this increased the promotion of its own goals, would be 
<br>
rational and good (from its point of view?)&quot;
<br>
<p>Obviously, in the simple case.
<br>
<p>I can't work out who made the top-level comment in this email, but the 
<br>
suggestion was that CI might be relevant to an AI, and the confusion 
<br>
seemed to be related to what the CI is, and how it might affect 
<br>
somebody's goals. The CI is little more than an adoption of logical 
<br>
consistency, suggesting that one be quite careful about adopting moral 
<br>
principles that may not apply universally.
<br>
<p>In terms of an AI's goal system, something similar will be true. An AI 
<br>
may have no &quot;guilt&quot; as we understand things. It will, however, have a 
<br>
set of goals and analyses for making choices. Something like the CI will 
<br>
still apply to the logic of its goal system, but describing it as a 
<br>
morality is not necessarily true.
<br>
<p>An AI with the top-level goal of &quot;Steal Underpants&quot; will assess all 
<br>
other actions in consequential terms as they contribute towards the 
<br>
stealing of underpants. Morality will not enter into the equation unless 
<br>
there is a goal of &quot;Be Good&quot;.
<br>
<p>The CI is a description of an objective ethics, claimed to be superior 
<br>
because all CI expressions are intransitive. Their objectivity makes 
<br>
them superior, more logically desireable, less flawed etc.
<br>
<p>For any goal system, be it centered around &quot;moral goodness&quot; or &quot;stealing 
<br>
underpants&quot;, a set of principles may be found, and some of them may be 
<br>
objective.
<br>
<p>All AIs that I have seen described are consequentialists and (I think) 
<br>
objectivists.
<br>
<p>Cheers,
<br>
-T
<br>
<p>Cheers,
<br>
-T
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13151.html">micah glasser: "Re: Please Re-read CAFAI"</a>
<li><strong>Previous message:</strong> <a href="13149.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<li><strong>In reply to:</strong> <a href="13149.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13152.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<li><strong>Reply:</strong> <a href="13152.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13150">[ date ]</a>
<a href="index.html#13150">[ thread ]</a>
<a href="subject.html#13150">[ subject ]</a>
<a href="author.html#13150">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
