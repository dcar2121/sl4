<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Please Re-read CAFAI</title>
<meta name="Author" content="Jef Allbright (jef@jefallbright.net)">
<meta name="Subject" content="Re: Please Re-read CAFAI">
<meta name="Date" content="2005-12-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Please Re-read CAFAI</h1>
<!-- received="Wed Dec 14 00:37:18 2005" -->
<!-- isoreceived="20051214073718" -->
<!-- sent="Tue, 13 Dec 2005 23:37:17 -0800" -->
<!-- isosent="20051214073717" -->
<!-- name="Jef Allbright" -->
<!-- email="jef@jefallbright.net" -->
<!-- subject="Re: Please Re-read CAFAI" -->
<!-- id="22360fa10512132337u6bf5516bn76cb94d7cb701371@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="439FBC63.1060004@tennessee.id.au" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Jef Allbright (<a href="mailto:jef@jefallbright.net?Subject=Re:%20Please%20Re-read%20CAFAI"><em>jef@jefallbright.net</em></a>)<br>
<strong>Date:</strong> Wed Dec 14 2005 - 00:37:17 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13156.html">Richard Loosemore: "Not the only way to build an AI [WAS: Please Re-read CAFAI]"</a>
<li><strong>Previous message:</strong> <a href="13154.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<li><strong>In reply to:</strong> <a href="13153.html">Tennessee Leeuwenburg: "Re: Please Re-read CAFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13175.html">micah glasser: "Re: Please Re-read CAFAI"</a>
<li><strong>Reply:</strong> <a href="13175.html">micah glasser: "Re: Please Re-read CAFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13155">[ date ]</a>
<a href="index.html#13155">[ thread ]</a>
<a href="subject.html#13155">[ subject ]</a>
<a href="author.html#13155">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Jef (to Michael)
<br>
Do you mean that to the extent an agent is rational, it will naturally
<br>
use all of its
<br>
instrumental knowledge to promote its own goals and from its point of
<br>
view there would be no question that such action is good?
<br>
<p>Tennessee
<br>
Well, the CI is by definition an objective thing. I do *not* believe
<br>
that it's obvious that all AIs would be relativists about morality.
<br>
However, I believe it's most likely so we don't need to argue about
<br>
it.
<br>
<p>Jef
<br>
I certainly don't see any reason to argue about that particular topic
<br>
at this time.
<br>
<p>Tennessee
<br>
The intended question has a double-entendre. The first meaning is that
<br>
&quot;all my goals are good from my point of view, by definition, so I
<br>
don't need to question it&quot;. The second meaning is &quot;I have no
<br>
conception of morality, therefore I don't need to question it&quot;.
<br>
<p>Jef
<br>
My intent was closer to your first interpretation, but perhaps not
<br>
exact.  When I say &quot;good&quot; I don't mean it in a moral sense or as in
<br>
&quot;good&quot; vs. &quot;evil&quot;.  I am trying (and not doing a very good [oops, that
<br>
word again] job of it) to show that any agent (human, AGI, or other
<br>
form) must necessarily evaluate actions with respect to achieving its
<br>
goals, and that actions which promote its goals must be considered
<br>
good to some extent while actions that detract from its goals must be
<br>
considered bad.  I am arguing that what is called &quot;good&quot; is what
<br>
works, and while such evaluations are necessarily from a subjective
<br>
viewpoint, that we can all agree (objectively) that for each of us,
<br>
what works is considered good.
<br>
<p>&lt;snip&gt;
<br>
<p>Tennessee
<br>
I think it's concievable that an AGI might have no moral sense, and be
<br>
bound only by consequential reasoning about its goals. I also think
<br>
it's concievable than an AGI would have a moral sense, but due to its
<br>
mental model have varying beliefs about good and evil, despite its
<br>
conclusions about what is objectively true.
<br>
<p>Jef
<br>
I would argue, later, that any moral &quot;sense&quot; is ultimately due to
<br>
consequential effects at some level, but I don't want to jump ahead
<br>
yet.  I would also argue, later, that any moral sense based on
<br>
&quot;varying beliefs about good and evil, despite its conclusion about
<br>
what is objectively true&quot; (as you put it) would be an accurate
<br>
description of part of current human morality, but limited in its
<br>
capability to promote good due to its restrictions on its own
<br>
instrumental knowledge that must be employed in the promotion of its
<br>
values.
<br>
<p>Jef's question #2 to Michael
<br>
If this [question #1] is true, then would it also see increasing its
<br>
objective knowledge in support of its goals as rational and inherently
<br>
good (from its point of view?)
<br>
<p>Tennessee
<br>
Not necessarily. It may consider knowledge to be inherently morally
<br>
neutral, although in consequential terms accumulated knowledge may be
<br>
morally valuable. An AGI acting under CI would desire to accumulate
<br>
objective knowledge as it related to its goals, but not necessarily
<br>
see it as good in itself.
<br>
<p><p>Jef
<br>
I wasn't making any claim about something being good in itself. I was
<br>
careful each time to frame &quot;good&quot; as the subjective evaluation by an
<br>
agent with regard to whether some action promoted its goals.
<br>
<p>Tennessee
<br>
There are a lot of abstractions here. Subjectivity doesn't destroy my
<br>
point. Even from its own point of view, it might be that some
<br>
knowledge is good, and some is not good, and some is neutral. An AGI
<br>
might regard knowledge as good, if it contributes to its goals, for
<br>
example. If that were the litmus test, then subjectively speaking,
<br>
some knowledge would be good, and some would be morally neutral.
<br>
<p>Perhaps you missed what I see as the obvious logical opposite -- that
<br>
the AGI adopts, subjectively speaking, the &quot;belief&quot;
<br>
(goal/desire/whatever) than Knowledge Is Good. In this case, the AGI
<br>
desires knowledge *as an end in itself* and not *solely for its
<br>
contribution to other goals*.
<br>
<p><p>Jef's question #3 to Michael
<br>
If I'm still understanding the implications of what you said, would
<br>
this also mean that cooperation with other like-minded agents, to the
<br>
extent that this increased the promotion of its own goals, would be
<br>
rational and good (from its point of view?)
<br>
<p>Tennessee
<br>
Obviously, in the simple case.
<br>
<p>Jef
<br>
Interesting that you seemed to disagree with the previous assertions,
<br>
but seemed to agree with this one that I thought was posed within the
<br>
same framework. It seems as if you were not reading carefully, and
<br>
responding to what you assumed might have been said.
<br>
<p>Tennessee
<br>
Well, you just said something that was true this time :). I'm not
<br>
going to change my mind because you said it in the context of a wider
<br>
flawed argument ;).
<br>
<p>Let's suppose than an AGI assesses some possible course of action --
<br>
in this case one of interaction and co-operation. It works out that
<br>
pursuing it will contribute to the furtherance of its own goals.  It
<br>
certainly isn't going to think that such a thing is either evil or
<br>
bad. It may have no sense of morality, but in the sense of
<br>
advantageous, such an action will be good.
<br>
<p>Let me then clarify: yes, such a thing will always be advantageous.
<br>
Insofar as an AGI has a moral system, such a thing will also be
<br>
morally good.
<br>
<p>Jef
<br>
Yes, I consistently mean &quot;good&quot; in the sense of advantageous.  It
<br>
seems that making this clear from the beginning is key to more
<br>
effective communication on this topic.  My difficulty with this is I
<br>
see &quot;good&quot; as *always&quot; meaning advantageous, but with varying context.
<br>
<p>I think we are in agreement that for any agent, those actions which
<br>
promote its values will necessarily be seen by that agent as good.
<br>
<p>Thanks for taking the time to work through this, and thanks to the
<br>
list for tolerating what (initially, at least) appeared to be very low
<br>
signal to noise.
<br>
<p>- Jef
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13156.html">Richard Loosemore: "Not the only way to build an AI [WAS: Please Re-read CAFAI]"</a>
<li><strong>Previous message:</strong> <a href="13154.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<li><strong>In reply to:</strong> <a href="13153.html">Tennessee Leeuwenburg: "Re: Please Re-read CAFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13175.html">micah glasser: "Re: Please Re-read CAFAI"</a>
<li><strong>Reply:</strong> <a href="13175.html">micah glasser: "Re: Please Re-read CAFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13155">[ date ]</a>
<a href="index.html#13155">[ thread ]</a>
<a href="subject.html#13155">[ subject ]</a>
<a href="author.html#13155">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
