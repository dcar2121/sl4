<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: guaranteeing friendliness</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: guaranteeing friendliness">
<meta name="Date" content="2005-12-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: guaranteeing friendliness</h1>
<!-- received="Sat Dec  3 19:00:30 2005" -->
<!-- isoreceived="20051204020030" -->
<!-- sent="Sat, 03 Dec 2005 18:00:41 -0800" -->
<!-- isosent="20051204020041" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: guaranteeing friendliness" -->
<!-- id="43924DC9.1000108@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="EIQXY2E-000AO4-N8@mail2.learnquick.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20guaranteeing%20friendliness"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Dec 03 2005 - 19:00:41 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13006.html">Eliezer S. Yudkowsky: "Re: guaranteeing friendliness"</a>
<li><strong>Previous message:</strong> <a href="13004.html">Herb Martin: "RE: guaranteeing friendliness"</a>
<li><strong>In reply to:</strong> <a href="13002.html">Herb Martin: "RE: guaranteeing friendliness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13010.html">Herb Martin: "RE: guaranteeing friendliness"</a>
<li><strong>Reply:</strong> <a href="13010.html">Herb Martin: "RE: guaranteeing friendliness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13005">[ date ]</a>
<a href="index.html#13005">[ thread ]</a>
<a href="subject.html#13005">[ subject ]</a>
<a href="author.html#13005">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Herb,
<br>
<p>It disturbs me that none of the replies to you seem to have actually 
<br>
addressed, except perhaps peripherally, your point about goals changing 
<br>
during the AI's evolution.  The problem is indeed, as one poster said, 
<br>
that you cannot use &quot;evolution&quot; to describe the process of 
<br>
self-modification, but that's not an explanation without also explaining 
<br>
the key difference between natural selection and self-modification.
<br>
<p>Here's my own answer, hope it helps.
<br>
<p>There is a fallacy oft-committed in discussion of Artificial 
<br>
Intelligence, especially AI of superhuman capability.  Someone says: 
<br>
&quot;When technology advances far enough we'll be able to build minds far 
<br>
surpassing human intelligence.  A superintelligence could build enormous 
<br>
cheesecakes - cheesecakes the size of cities - by golly, the future will 
<br>
be full of giant cheesecakes!&quot;  The question is whether the 
<br>
superintelligence wants to build giant cheesecakes.  The vision leaps 
<br>
directly from capability to actuality, without considering the necessary 
<br>
intermediate of motive.
<br>
<p>People often immediately declare that Friendly AI is an impossibility, 
<br>
because any sufficiently powerful AI will be able to modify its own 
<br>
source code to break any constraints placed upon it.
<br>
<p>The first flaw you should notice is a Giant Cheesecake Fallacy.  Any AI 
<br>
with free access to its own source would, in principle, possess the 
<br>
ability to modify its own source code in a way that changed the AI's 
<br>
optimization target (the region into which the AI tries to steer 
<br>
possible futures).  This does not imply the AI has the motive to change 
<br>
its own motives.  I would not knowingly swallow a pill that made me 
<br>
enjoy killing babies, because currently I prefer that babies not die.
<br>
<p>But what if I try to modify myself, and make a mistake?  When computer 
<br>
engineers prove a chip valid - a good idea if the chip has 155 million 
<br>
transistors and you can't issue a patch afterward - the engineers use 
<br>
human-guided, machine-verified formal proof.  The glorious thing about 
<br>
formal mathematical proof, is that a proof of ten billion steps is just 
<br>
as reliable as a proof of ten steps.  But human beings are not 
<br>
trustworthy to peer over a purported proof of ten billion steps; we have 
<br>
too high a chance of missing an error.  And present-day theorem-proving 
<br>
techniques are not smart enough to design and prove an entire computer 
<br>
chip on their own - current algorithms undergo an exponential explosion 
<br>
in the search space.  Human mathematicians can prove theorems far more 
<br>
complex than modern theorem-provers can handle, without being defeated 
<br>
by exponential explosion.  But human mathematics is informal and 
<br>
unreliable; occasionally someone discovers a flaw in a previously 
<br>
accepted informal proof.  The upshot is that human engineers guide a 
<br>
theorem-prover through the intermediate steps of a proof.  The human 
<br>
chooses the next lemma, and a complex theorem-prover generates a formal 
<br>
proof, and a simple verifier checks the steps.  That's how modern 
<br>
engineers build reliable machinery with 155 million interdependent parts.
<br>
<p>Proving a computer chip correct requires a synergy of human intelligence 
<br>
and computer algorithms, as currently neither suffices on its own. 
<br>
Perhaps a true AI could use a similar combination of abilities when 
<br>
modifying its own code - would have both the capability to invent large 
<br>
designs without being defeated by exponential explosion, and also the 
<br>
ability to verify its steps with extreme reliability.  That is one way a 
<br>
true AI might remain knowably stable in its goals, even after carrying 
<br>
out a large number of self-modifications.
<br>
<p>So that's one possible answer.  More generally:  It is disrespectful to 
<br>
human ingenuity to declare a challenge unsolvable without taking a close 
<br>
look and maybe exercising a little creativity.  Especially when the 
<br>
stakes are this high.  It is an enormously strong statement to say that 
<br>
you cannot do a thing - that you cannot build a heavier-than-air flying 
<br>
machine, that you cannot get useful energy from nuclear reactions, that 
<br>
you cannot fly to the Moon.  Such statements are universal 
<br>
generalizations, quantified over every single approach that anyone ever 
<br>
has or ever will think up for solving the problem.  It only takes a 
<br>
single counterexample to falsify a universal quantifier.  The statement 
<br>
that Friendly (or friendly) AI is theoretically impossible, dares to 
<br>
quantify over every possible mind design and every possible optimization 
<br>
process - including human beings, who are also minds, some of whom are 
<br>
pretty nice and wish they were nicer.  At this point there are any 
<br>
number of vaguely plausible reasons why Friendly AI might be humanly 
<br>
impossible, and it is still more likely that the problem is solvable but 
<br>
no one will get around to solving it in time.  But one should not so 
<br>
quickly write off the challenge, especially considering the stakes.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13006.html">Eliezer S. Yudkowsky: "Re: guaranteeing friendliness"</a>
<li><strong>Previous message:</strong> <a href="13004.html">Herb Martin: "RE: guaranteeing friendliness"</a>
<li><strong>In reply to:</strong> <a href="13002.html">Herb Martin: "RE: guaranteeing friendliness"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13010.html">Herb Martin: "RE: guaranteeing friendliness"</a>
<li><strong>Reply:</strong> <a href="13010.html">Herb Martin: "RE: guaranteeing friendliness"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13005">[ date ]</a>
<a href="index.html#13005">[ thread ]</a>
<a href="subject.html#13005">[ subject ]</a>
<a href="author.html#13005">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
