<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Passive AI</title>
<meta name="Author" content="Nick Bostrom (nick.bostrom@philosophy.oxford.ac.uk)">
<meta name="Subject" content="Re: Passive AI">
<meta name="Date" content="2005-12-11">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Passive AI</h1>
<!-- received="Sun Dec 11 15:26:05 2005" -->
<!-- isoreceived="20051211222605" -->
<!-- sent="Sun, 11 Dec 2005 22:26:11 +0000" -->
<!-- isosent="20051211222611" -->
<!-- name="Nick Bostrom" -->
<!-- email="nick.bostrom@philosophy.oxford.ac.uk" -->
<!-- subject="Re: Passive AI" -->
<!-- id="6.0.0.22.2.20051211213852.02fb8de0@sfop0079.herald.ox.ac.uk" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20051209145441.52550.qmail@web26708.mail.ukl.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Nick Bostrom (<a href="mailto:nick.bostrom@philosophy.oxford.ac.uk?Subject=Re:%20Passive%20AI"><em>nick.bostrom@philosophy.oxford.ac.uk</em></a>)<br>
<strong>Date:</strong> Sun Dec 11 2005 - 15:26:11 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13106.html">Tyler Emerson: "List of envisioned global catastrophic risks"</a>
<li><strong>Previous message:</strong> <a href="13104.html">Maru Dubshinki: "Re: Hardcore SL4 Technology"</a>
<li><strong>In reply to:</strong> <a href="13082.html">Michael Wilson: "Re: Passive AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13077.html">P K: "Re:  Passive AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13105">[ date ]</a>
<a href="index.html#13105">[ thread ]</a>
<a href="subject.html#13105">[ subject ]</a>
<a href="author.html#13105">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michael Wilson wrote:
<br>
<p><em>&gt;This proposal seems to me rather like Nick Bostrom's
</em><br>
<em>&gt;preffered option of building an 'Orcale'. This isn't
</em><br>
<em>&gt;as easy as it looks, even if you start from the position
</em><br>
<em>&gt;that it's pretty damn hard (sticking with the AGI norm
</em><br>
<em>&gt;then :) ), but I've been studying ways to do it and it
</em><br>
<em>&gt;does still look to me much easier than building any of
</em><br>
<em>&gt;Yudkowsky's FAI proposals.
</em><br>
<p>(&quot;Preferred option&quot; might be too strong. I don't have a settled opinion on 
<br>
this matter.)
<br>
<p><em>&gt;An Oracle is of course still a tremendously dangerous
</em><br>
<em>&gt;thing to have around. Ask it for any kind of plan of
</em><br>
<em>&gt;action and you are allowing it to optimise reality to
</em><br>
<em>&gt;the maximum degree permitted by using you as intermediary,
</em><br>
<em>&gt;in effect bringing up a marginally diluted version of the
</em><br>
<em>&gt;'understanding intent' problem without any efforts to
</em><br>
<em>&gt;directly solve it (e.g. the way EV attempts to). I must
</em><br>
<em>&gt;reluctently classify Nick Bostrom's proposal to make an
</em><br>
<em>&gt;Oracle generally available (or at least, publically known
</em><br>
<em>&gt;and available to experts) as hopelessly naive. Clearly
</em><br>
<em>&gt;there is vast potential for misuse and abuse that would
</em><br>
<em>&gt;be unavoidable if publically known, at least in the short
</em><br>
<em>&gt;space of time before some fool asks one how to build a
</em><br>
<em>&gt;seed AI that will help them with their personal goals. It
</em><br>
<em>&gt;does seem likely to me that an Orcale built and used in
</em><br>
<em>&gt;secret, by sufficiently moral and cautious researchers,
</em><br>
<em>&gt;would be a net reduction in risk for an FAI project.
</em><br>
<p>There are different options for who should decide what questions could be 
<br>
posed to the Oracle. It might be difficult to ensure that the best such 
<br>
option is instantiated. But this problem is not unique to the 
<br>
Oracle-approach. It is also difficult to ensure that the first AGI is built 
<br>
by the best people to do it. The question here is, for whichever group has 
<br>
control over the first AGI - whether it's SIAI, the Pentagon, the UN, or 
<br>
whatever - what is the best way to build the AGI?
<br>
<p>The potential advantage with an Oracle is that it seems easier to specify 
<br>
the goal to answer questions subject to certain simple constraints about 
<br>
what changes may be brought about in the world to archive this objective 
<br>
than to specify the goal to remake the world in accordance with &quot;humanity's 
<br>
extrapolated volition&quot;. Something like: Find the most accurate answer to 
<br>
the question you can within 5 seconds by shuffling electrons in these 
<br>
circuits and accessing these sources of information, and output the answer 
<br>
in the form of 10 pages print-out.
<br>
<p>If we could specify safe rules for avoiding the risk of turning the 
<br>
universe into computronium and suchlike (rather crude failure modes), then 
<br>
we could use the Oracle to ask for advise on various aspects on how best to 
<br>
proceed with developing more ambitious AI if we want to, or we could just 
<br>
ask for drugs to cure cancer etc. It seems nice to be able to take the 
<br>
advise of a superintelligence about what would happen if we did various 
<br>
things, rather than to immediately launch a potentially world-transforming 
<br>
AI and hoping for the best.
<br>
<p>Those who could ask the Oracle questions would be in a very powerful 
<br>
position. The right to ask questions would obviously have to be limited 
<br>
(don't want a terrorist asking what is the best way to destroy the 
<br>
world...). But a wise asker could ask questions that could make future 
<br>
steps safer. An unwise asker might ask dangerous questions, but presumably 
<br>
the alternative is that they would build unsafe world-transforming AI 
<br>
directly. And it might be easier to persuade users to ask a few wise 
<br>
questions first, than to persuade them to build friendly world-transforming AI.
<br>
<p>It takes pretty high level of responsibility to worry about the Last Judge 
<br>
problem. If the askers are concerned with this problem, they could try 
<br>
asking for advise on how to solve this problem. One could use the Oracle 
<br>
without necessarily putting oneself in the Last Judge position.
<br>
<p><p><p><p>Nick Bostrom
<br>
Director, Future of Humanity Institute
<br>
Faculty of Philosophy, Oxford University
<br>
10 Merton Str., OX1 4JJ, Oxford     +44 (0)7789 74 42 42
<br>
Homepage: <a href="http://www.nickbostrom.com">http://www.nickbostrom.com</a>     FHI: <a href="http://www.fhi.ox.ac.uk">http://www.fhi.ox.ac.uk</a>
<br>
<p>For administrative matters, please contact my PA, Miriam Wood
<br>
+44(0)1865 27 69 34     <a href="mailto:miriam.wood@philosophy.ox.ac.uk?Subject=Re:%20Passive%20AI">miriam.wood@philosophy.ox.ac.uk</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13106.html">Tyler Emerson: "List of envisioned global catastrophic risks"</a>
<li><strong>Previous message:</strong> <a href="13104.html">Maru Dubshinki: "Re: Hardcore SL4 Technology"</a>
<li><strong>In reply to:</strong> <a href="13082.html">Michael Wilson: "Re: Passive AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13077.html">P K: "Re:  Passive AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13105">[ date ]</a>
<a href="index.html#13105">[ thread ]</a>
<a href="subject.html#13105">[ subject ]</a>
<a href="author.html#13105">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
