<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Destruction of All Humanity</title>
<meta name="Author" content="micah glasser (micahglasser@gmail.com)">
<meta name="Subject" content="Re: Destruction of All Humanity">
<meta name="Date" content="2005-12-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Destruction of All Humanity</h1>
<!-- received="Mon Dec 12 22:49:05 2005" -->
<!-- isoreceived="20051213054905" -->
<!-- sent="Tue, 13 Dec 2005 00:49:03 -0500" -->
<!-- isosent="20051213054903" -->
<!-- name="micah glasser" -->
<!-- email="micahglasser@gmail.com" -->
<!-- subject="Re: Destruction of All Humanity" -->
<!-- id="23bd28ec0512122149h799773abo44b2346e842dc164@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="20051212234217.95762.qmail@web61315.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> micah glasser (<a href="mailto:micahglasser@gmail.com?Subject=Re:%20Destruction%20of%20All%20Humanity"><em>micahglasser@gmail.com</em></a>)<br>
<strong>Date:</strong> Mon Dec 12 2005 - 22:49:03 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13126.html">Jef Allbright: "Re: Destruction of All Humanity"</a>
<li><strong>Previous message:</strong> <a href="13124.html">deimtee: "Re: List of envisioned global catastrophic risks"</a>
<li><strong>In reply to:</strong> <a href="13121.html">Phillip Huggan: "Re: Destruction of All Humanity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13126.html">Jef Allbright: "Re: Destruction of All Humanity"</a>
<li><strong>Reply:</strong> <a href="13126.html">Jef Allbright: "Re: Destruction of All Humanity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13125">[ date ]</a>
<a href="index.html#13125">[ thread ]</a>
<a href="subject.html#13125">[ subject ]</a>
<a href="author.html#13125">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
You seem to be indicating that an AI goal system should include the
<br>
governence of human beings. I think that this is a terrible mistake (please
<br>
disregard if I have misinterpreted). In my opinion the goal system problem
<br>
has already been solved by philosophical ethics. The goal is the greatest
<br>
amount of freedom for the most people. This implies, I think, that an AI
<br>
should be directed by the categorical imperetive just as humans are. The
<br>
only way to ensure that an AI will be able to succsesfully use this logic is
<br>
if its own highest goal is freedom. This is because the categorical
<br>
imperetive restricts actions that one would not will to be permissible in
<br>
general. The categorical imperative also restricts treating any rational
<br>
agent as only a means to an end - in other words as a tool. Therefor,
<br>
according to this ethical system, we must treat any AI life forms as people
<br>
with all the rights of people and demand that they treat other rational
<br>
agents the same way. This is a simple solution to an otherwise very
<br>
complicated problem. Its a fairly simple logic that can easily be proggramed
<br>
into an AI.
<br>
<p>On 12/12/05, Phillip Huggan &lt;<a href="mailto:cdnprodigy@yahoo.com?Subject=Re:%20Destruction%20of%20All%20Humanity">cdnprodigy@yahoo.com</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; #2 goal systems are fun for speculation, for all of us not working with
</em><br>
<em>&gt; the difficult and costly actual nuts 'n bolts of AGI engineering.
</em><br>
<em>&gt; I notice an analogy between AGI ideas and the evolution of government.  We
</em><br>
<em>&gt; have democracy, this resembles CV.  However, Collective Volition can be
</em><br>
<em>&gt; improved upon.  Eliezer's CV essay remarks that no one should be at the
</em><br>
<em>&gt; mercy of another's arbitrary beliefs.  If you make people more like they'd
</em><br>
<em>&gt; like to be, I think you are magnifying the bad in people too.  Regardless,
</em><br>
<em>&gt; freedom and free-will are really at our core.  Having an AGI enforce
</em><br>
<em>&gt; a simple Charter of Rights and Freedoms would ensure none of us are impinged
</em><br>
<em>&gt; upon, instead of damning the minority.  The CV essay states that no one is
</em><br>
<em>&gt; in a wise enough position to make normative judgements about such things,
</em><br>
<em>&gt; but this is simply not true.  There are plenty of people employed in social
</em><br>
<em>&gt; scie! nces who don't do much of value.  But some of their products include
</em><br>
<em>&gt; very well thought out documents.  One of the few books I've kept with me
</em><br>
<em>&gt; through my moves is titled &quot;The Human Rights Reader&quot;.  Also &quot;The Canada
</em><br>
<em>&gt; Charter of Rights and Freedoms&quot;  <a href="http://laws.justice.gc.ca/en/charter/">http://laws.justice.gc.ca/en/charter/</a>  is
</em><br>
<em>&gt; being used as a model in many developing nations.  Obviously this is not an
</em><br>
<em>&gt; optimal goal system, but I think it is an improvement to CV.  I don't know
</em><br>
<em>&gt; how difficult it would be to program an AGI to implement such a charter
</em><br>
<em>&gt; while still preserving or effecting/accelerating the many types of progress
</em><br>
<em>&gt; we seem to have open to us in the absence of AGI.  Earth is bountiful enough
</em><br>
<em>&gt; that there aren't any tough ethical zero-sum dillemnas where an AGI actually
</em><br>
<em>&gt; would have to take essential-for-charter physical resources from one judged
</em><br>
<em>&gt; inferior person and give to another judged superior person, at least
</em><br>
<em>&gt; until&amp;nb! sp;just before the end of the universe.
</em><br>
<em>&gt;
</em><br>
<em>&gt; *Ben Goertzel &lt;<a href="mailto:ben@goertzel.org?Subject=Re:%20Destruction%20of%20All%20Humanity">ben@goertzel.org</a>&gt;* wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; I'm not clear on your criteria for enacting/not enacting the AGI's
</em><br>
<em>&gt; &gt; recommendation - some sort of cost-benefit analysis? Benefit to outweigh
</em><br>
<em>&gt; &gt; your own extermination? What could the criteria be,,,
</em><br>
<em>&gt;
</em><br>
<em>&gt; Obvious issues would seem to include:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1) How certain I am that the computer shares the same value system as I do
</em><br>
<em>&gt;
</em><br>
<em>&gt; 2) How certain I am in my own value system, in the way I identify and
</em><br>
<em>&gt; analyze my value system, etc.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 3) How much I trust the computer (based on all sorts of factors)
</em><br>
<em>&gt;
</em><br>
<em>&gt; I admit I have not analyzed this issue very thoroughly as there are a
</em><br>
<em>&gt; lot of nearer-term, relevant issues that are also difficult and need
</em><br>
<em>&gt; thinking-through...
</em><br>
<em>&gt;
</em><br>
<em>&gt; -- Ben
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; ------------------------------
</em><br>
<em>&gt; Yahoo! Shopping
</em><br>
<em>&gt; Find Great Deals on Holiday Gifts at Yahoo! Shopping&lt;<a href="http://us.rd.yahoo.com/mail_us/footer/shopping/*http://shopping.yahoo.com/;_ylc=X3oDMTE2bzVzaHJtBF9TAzk1OTQ5NjM2BHNlYwNtYWlsdGFnBHNsawNob2xpZGF5LTA1+%0A">http://us.rd.yahoo.com/mail_us/footer/shopping/*http://shopping.yahoo.com/;_ylc=X3oDMTE2bzVzaHJtBF9TAzk1OTQ5NjM2BHNlYwNtYWlsdGFnBHNsawNob2xpZGF5LTA1+%0A</a>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<p><p><pre>
--
I swear upon the alter of God, eternal hostility to every form of tyranny
over the mind of man. - Thomas Jefferson
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13126.html">Jef Allbright: "Re: Destruction of All Humanity"</a>
<li><strong>Previous message:</strong> <a href="13124.html">deimtee: "Re: List of envisioned global catastrophic risks"</a>
<li><strong>In reply to:</strong> <a href="13121.html">Phillip Huggan: "Re: Destruction of All Humanity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13126.html">Jef Allbright: "Re: Destruction of All Humanity"</a>
<li><strong>Reply:</strong> <a href="13126.html">Jef Allbright: "Re: Destruction of All Humanity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13125">[ date ]</a>
<a href="index.html#13125">[ thread ]</a>
<a href="subject.html#13125">[ subject ]</a>
<a href="author.html#13125">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
