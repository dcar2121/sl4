<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI &amp; Kurweil's Singularity</title>
<meta name="Author" content="Eric Rauch (erauch@gmail.com)">
<meta name="Subject" content="Re: SIAI &amp; Kurweil's Singularity">
<meta name="Date" content="2005-12-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI &amp; Kurweil's Singularity</h1>
<!-- received="Thu Dec 15 12:26:58 2005" -->
<!-- isoreceived="20051215192658" -->
<!-- sent="Thu, 15 Dec 2005 14:26:56 -0500" -->
<!-- isosent="20051215192656" -->
<!-- name="Eric Rauch" -->
<!-- email="erauch@gmail.com" -->
<!-- subject="Re: SIAI &amp; Kurweil's Singularity" -->
<!-- id="c8ff78370512151126k4c831823y7ae7913affa2ae89@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="20051215191758.73403.qmail@web61120.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eric Rauch (<a href="mailto:erauch@gmail.com?Subject=Re:%20SIAI%20&amp;%20Kurweil's%20Singularity"><em>erauch@gmail.com</em></a>)<br>
<strong>Date:</strong> Thu Dec 15 2005 - 12:26:56 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13202.html">micah glasser: "Re: Destruction of All Humanity"</a>
<li><strong>Previous message:</strong> <a href="13200.html">1Arcturus: "SIAI &amp; Kurweil's Singularity"</a>
<li><strong>In reply to:</strong> <a href="13200.html">1Arcturus: "SIAI &amp; Kurweil's Singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13204.html">Jef Allbright: "Re: SIAI &amp; Kurweil's Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13201">[ date ]</a>
<a href="index.html#13201">[ thread ]</a>
<a href="subject.html#13201">[ subject ]</a>
<a href="author.html#13201">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
If superintelligent AI emerges before we are able to enhance ourselves it
<br>
shouldn't be too hard for the AI to do the job for us.
<br>
<p>Personally, I'm more interested in enhancing myself then in any of the other
<br>
benefits that the singularity might provide
<br>
<p><p>On 12/15/05, 1Arcturus &lt;<a href="mailto:arcturus12453@yahoo.com?Subject=Re:%20SIAI%20&amp;%20Kurweil's%20Singularity">arcturus12453@yahoo.com</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; I had another question about SIAI in relation to Kurzweil's latest book
</em><br>
<em>&gt; Singularity is Near.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If I have him right, Kurzweil predicts that humans will gradually merge
</em><br>
<em>&gt; with their technology - the technology becoming more humanlike, more
</em><br>
<em>&gt; biological-compatible, and integrating into the human body and mental
</em><br>
<em>&gt; processes, until eventually the purely 'biological' portion becomes less and
</em><br>
<em>&gt; less predominant or disappears entirely.
</em><br>
<em>&gt;
</em><br>
<em>&gt; SIAI seems to presuppose a very different scenario - that strongly
</em><br>
<em>&gt; superintelligent AI will arise first in pure machines, and never
</em><br>
<em>&gt; (apparently) in humans. There seems to be no indication of 'merger', more
</em><br>
<em>&gt; like a kind of AI-rule over mostly unmodified humans.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Some of this difference may be because Kurzweil predicts nanotechnology in
</em><br>
<em>&gt; the human body (including the brain) and very advanced human-machine
</em><br>
<em>&gt; interfaces will arise before strongly superintelligent AI, and that strong!
</em><br>
<em>&gt; ly superintelligent AI will require the completion of the
</em><br>
<em>&gt; reverse-engineering of the human brain. (Completed reverse-engineering of
</em><br>
<em>&gt; the brain + adequate brain scanning surely = ability to upload part or all
</em><br>
<em>&gt; of human selves?)
</em><br>
<em>&gt;
</em><br>
<em>&gt; But SIAI seems to assume AIs will become strongly superintelligent by
</em><br>
<em>&gt; their own design, arising from human designs, before humans ever finish
</em><br>
<em>&gt; reverse-engineering the human brain. The lack of a fully functional
</em><br>
<em>&gt; interface with the strongly intelligent AIs would cause humans to be
</em><br>
<em>&gt; dependent on the AIs to do the thinking from then on, and the AIs would take
</em><br>
<em>&gt; on the responsibility for the thinking of course also. This seems to assume
</em><br>
<em>&gt; the AIs would not be able to, or not want to, create interfaces or upload
</em><br>
<em>&gt; the humans -- that is, it would not 'uplift' the humans to its own level of
</em><br>
<em>&gt; intelligence so that they could then understand each other.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I am trying to understand SIAI's position, or at least the emphasis of
</em><br>
<em>&gt; posters here and some representatives I have heard, contrasted with
</em><br>
<em>&gt; Kuzweil's book. There seems to be a contrast to me, although I know Kurzweil
</em><br>
<em>&gt; is involved with SIAI also.
</em><br>
<em>&gt;
</em><br>
<em>&gt; One thing I would say - the prediction I attribute to Kurzweil eliminates
</em><br>
<em>&gt; many of the very troubling problems that seem to arise in what I think is
</em><br>
<em>&gt; the SIAI scenario: how to trust an AI? How to design it to be at least as
</em><br>
<em>&gt; kind (at least to us) as we are [my comment: not a very high standard :)]
</em><br>
<em>&gt; How to understand the AI and its actions after it becomes strongly
</em><br>
<em>&gt; superintelligent? Whether or not to follow the AI's advice when it sounds
</em><br>
<em>&gt; wrong?
</em><br>
<em>&gt;
</em><br>
<em>&gt; None of these things are problematic if humans merge with technology and
</em><br>
<em>&gt; acquire its capacity for strong superintelligence. That is, humans would be
</em><br>
<em>&gt; at the very center of the Singularity and direct its development, for better
</em><br>
<em>&gt; or worse, with 'open eyes', and taking responsibility themselves r! ather
</em><br>
<em>&gt; than lending it to an external machine.
</em><br>
<em>&gt;
</em><br>
<em>&gt; gej
</em><br>
<em>&gt;
</em><br>
<em>&gt; ------------------------------
</em><br>
<em>&gt; Yahoo! Shopping
</em><br>
<em>&gt; Find Great Deals on Holiday Gifts at Yahoo! Shopping&lt;<a href="http://us.rd.yahoo.com/mail_us/footer/shopping/*http://shopping.yahoo.com/;_ylc=X3oDMTE2bzVzaHJtBF9TAzk1OTQ5NjM2BHNlYwNtYWlsdGFnBHNsawNob2xpZGF5LTA1+%0A">http://us.rd.yahoo.com/mail_us/footer/shopping/*http://shopping.yahoo.com/;_ylc=X3oDMTE2bzVzaHJtBF9TAzk1OTQ5NjM2BHNlYwNtYWlsdGFnBHNsawNob2xpZGF5LTA1+%0A</a>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13202.html">micah glasser: "Re: Destruction of All Humanity"</a>
<li><strong>Previous message:</strong> <a href="13200.html">1Arcturus: "SIAI &amp; Kurweil's Singularity"</a>
<li><strong>In reply to:</strong> <a href="13200.html">1Arcturus: "SIAI &amp; Kurweil's Singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13204.html">Jef Allbright: "Re: SIAI &amp; Kurweil's Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13201">[ date ]</a>
<a href="index.html#13201">[ thread ]</a>
<a href="subject.html#13201">[ subject ]</a>
<a href="author.html#13201">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
