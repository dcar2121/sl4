<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]</title>
<meta name="Author" content="Eric Rauch (erauch@gmail.com)">
<meta name="Subject" content="Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]">
<meta name="Date" content="2005-12-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]</h1>
<!-- received="Wed Dec 14 07:26:30 2005" -->
<!-- isoreceived="20051214142630" -->
<!-- sent="Wed, 14 Dec 2005 06:26:28 -0800" -->
<!-- isosent="20051214142628" -->
<!-- name="Eric Rauch" -->
<!-- email="erauch@gmail.com" -->
<!-- subject="Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]" -->
<!-- id="c8ff78370512140626h53e8ad7bxe879cdacc4d7da91@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="43A01B59.7070602@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eric Rauch (<a href="mailto:erauch@gmail.com?Subject=Re:%20Not%20the%20only%20way%20to%20build%20an%20AI%20[WAS:%20Please%20Re-read%20CAFAI]"><em>erauch@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Dec 14 2005 - 07:26:28 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13158.html">David Picon Alvarez: "Re: Destruction of All Humanity"</a>
<li><strong>Previous message:</strong> <a href="13156.html">Richard Loosemore: "Not the only way to build an AI [WAS: Please Re-read CAFAI]"</a>
<li><strong>In reply to:</strong> <a href="13156.html">Richard Loosemore: "Not the only way to build an AI [WAS: Please Re-read CAFAI]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13163.html">Richard Loosemore: "Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]"</a>
<li><strong>Reply:</strong> <a href="13163.html">Richard Loosemore: "Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13157">[ date ]</a>
<a href="index.html#13157">[ thread ]</a>
<a href="subject.html#13157">[ subject ]</a>
<a href="author.html#13157">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Given your skepticism of goal guided agi what do you think of the
<br>
goaless agi I suggested.
<br>
<p>Also I'm curious as to how the members of this list maintain such a
<br>
high degree of certainty about the behavior of post singularity
<br>
intelligences which are almost by definition supposed to be beyond our
<br>
comprehension (richard this is not directed at you)
<br>
<p>-Eric
<br>
<p>On 12/14/05, Richard Loosemore &lt;<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Not%20the%20only%20way%20to%20build%20an%20AI%20[WAS:%20Please%20Re-read%20CAFAI]">rpwl@lightlink.com</a>&gt; wrote:
<br>
<em>&gt; Michael Vassar wrote:
</em><br>
<em>&gt; &gt; Some posters seem to be very seriously unaware of what was said in
</em><br>
<em>&gt; &gt; CAFAI, but having read and understood it should be a prerequisite to
</em><br>
<em>&gt; &gt; posting here.
</em><br>
<em>&gt; &gt; My complaints
</em><br>
<em>&gt; &gt; Friendly AIs are explicitly NOT prevented from messing with their
</em><br>
<em>&gt; &gt; source-code or with their goal systems.  However, they act according to
</em><br>
<em>&gt; &gt; decision theory.  ....
</em><br>
<em>&gt;    ^^^^^^^^^^^^^^^
</em><br>
<em>&gt;
</em><br>
<em>&gt; I have to go on record here as saying that I (and others who are poorly
</em><br>
<em>&gt; represented on this list) fundamentally disagree with this statement.  I
</em><br>
<em>&gt; would not want readers of these posts to get the idea that this is THE
</em><br>
<em>&gt; universally agreed way to build an artificial intelligence.  Moreover,
</em><br>
<em>&gt; many of the recent debates on this list are utterly dependent on the
</em><br>
<em>&gt; assumption that you state above, so to people like me these debates are
</em><br>
<em>&gt; just wheel-spinning built on nonsensical premises.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Here is why.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Friendly AIs built on decision theory have goal systems that specify
</em><br>
<em>&gt; their goals:   but in what form are the goals represented, and how are
</em><br>
<em>&gt; they interpreted?  Here is a nice example of a goal:
</em><br>
<em>&gt;
</em><br>
<em>&gt;      &quot;Put the blue block on top of the red block&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; In a Blocks World, the semantics of this goal - its &quot;meaning&quot; - are not
</em><br>
<em>&gt; at all difficult.  All fine and good: standard 1970's-issue artificial
</em><br>
<em>&gt; intelligence, etc.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But what happens when the goals become more abstract:
</em><br>
<em>&gt;
</em><br>
<em>&gt;      &quot;Maximize the utility function, where the utility function
</em><br>
<em>&gt; specifies that thinking is good&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I've deliberately chosen a silly UF (thinking is good) because people on
</em><br>
<em>&gt; this list frequently talk as if a goal like that has a meaning that is
</em><br>
<em>&gt; just as transparent as the meaning of &quot;put the blue block on top of the
</em><br>
<em>&gt; red block&quot;.  The semantics of &quot;thinking is good&quot; is clearly not trivial,
</em><br>
<em>&gt; and in fact it is by no means obvious that the phrase can be given a
</em><br>
<em>&gt; clear enough semantics to enable it to be used as a sensible input to a
</em><br>
<em>&gt; decision-theory-driven AGI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The behavior of an AGI with such a goal would depend crucially on what
</em><br>
<em>&gt; mechanisms it used to interpret the meaning of &quot;thinking is good&quot;.  So
</em><br>
<em>&gt; much so, in fact, that it becomes stupid to talk of the system as being
</em><br>
<em>&gt; governed by the decision theory component:  it is not, it is governed by
</em><br>
<em>&gt; whatever mechanisms you can cobble together to interpret that vague goal
</em><br>
<em>&gt; statement.  What initially looked like the dog's tail (the mechanisms
</em><br>
<em>&gt; that govern the interpretation of goals) starts to wag the dog (the
</em><br>
<em>&gt; decision-theory-based goal engine).
</em><br>
<em>&gt;
</em><br>
<em>&gt; The standard response to this criticism is that while the semantics are
</em><br>
<em>&gt; not obvious, the whole point of modern AI research is to build systems
</em><br>
<em>&gt; that do rigorously interpret the semantics in some kind of compositional
</em><br>
<em>&gt; way, even in the cases of abstract goals like &quot;thinking is good&quot;.  In
</em><br>
<em>&gt; other words, the claim is that I am seeing a fundamental problem where
</em><br>
<em>&gt; others only see a bunch of complex implementation details.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is infuriating nonsense:  there are many people out there who
</em><br>
<em>&gt; utterly disagree with this position, and who have solid reasons for
</em><br>
<em>&gt; doing so.  I am one of them.
</em><br>
<em>&gt;
</em><br>
<em>&gt; So when you say &quot;Friendly AIs [...] act according to decision theory.&quot;
</em><br>
<em>&gt; you mean &quot;The particular interpretation of how to build a Friendly AI
</em><br>
<em>&gt; that is common on this list, acts according to decision theory.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; And, as I say, much of the recent discussion about passive AI and goal
</em><br>
<em>&gt; systems is just content-free speculation, from my point of view.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Richard Loosemore
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13158.html">David Picon Alvarez: "Re: Destruction of All Humanity"</a>
<li><strong>Previous message:</strong> <a href="13156.html">Richard Loosemore: "Not the only way to build an AI [WAS: Please Re-read CAFAI]"</a>
<li><strong>In reply to:</strong> <a href="13156.html">Richard Loosemore: "Not the only way to build an AI [WAS: Please Re-read CAFAI]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13163.html">Richard Loosemore: "Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]"</a>
<li><strong>Reply:</strong> <a href="13163.html">Richard Loosemore: "Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13157">[ date ]</a>
<a href="index.html#13157">[ thread ]</a>
<a href="subject.html#13157">[ subject ]</a>
<a href="author.html#13157">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
