<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI &amp; Kurweil's Singularity</title>
<meta name="Author" content="1Arcturus (arcturus12453@yahoo.com)">
<meta name="Subject" content="Re: SIAI &amp; Kurweil's Singularity">
<meta name="Date" content="2005-12-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI &amp; Kurweil's Singularity</h1>
<!-- received="Fri Dec 16 09:19:22 2005" -->
<!-- isoreceived="20051216161922" -->
<!-- sent="Fri, 16 Dec 2005 08:19:19 -0800 (PST)" -->
<!-- isosent="20051216161919" -->
<!-- name="1Arcturus" -->
<!-- email="arcturus12453@yahoo.com" -->
<!-- subject="Re: SIAI &amp; Kurweil's Singularity" -->
<!-- id="20051216161919.50740.qmail@web61121.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="BAY101-F308AB78CF36C18CE723C29AC3A0@phx.gbl" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> 1Arcturus (<a href="mailto:arcturus12453@yahoo.com?Subject=Re:%20SIAI%20&amp;%20Kurweil's%20Singularity"><em>arcturus12453@yahoo.com</em></a>)<br>
<strong>Date:</strong> Fri Dec 16 2005 - 09:19:19 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13216.html">Phillip Huggan: "Re: Destruction of All Humanity"</a>
<li><strong>Previous message:</strong> <a href="13214.html">1Arcturus: "Re: SIAI &amp; Kurweil's Singularity"</a>
<li><strong>In reply to:</strong> <a href="13210.html">Michael Vassar: "Re: SIAI &amp; Kurweil's Singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13223.html">Michael Vassar: "Re: SIAI &amp; Kurweil's Singularity"</a>
<li><strong>Reply:</strong> <a href="13223.html">Michael Vassar: "Re: SIAI &amp; Kurweil's Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13215">[ date ]</a>
<a href="index.html#13215">[ thread ]</a>
<a href="subject.html#13215">[ subject ]</a>
<a href="author.html#13215">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michael Vassar &lt;<a href="mailto:michaelvassar@hotmail.com?Subject=Re:%20SIAI%20&amp;%20Kurweil's%20Singularity">michaelvassar@hotmail.com</a>&gt; wrote:   The chance of survival in a scenario of economic or other forms of competition between agents capable of recursive self-improvement seems close to zero though. The only way I can imagine it working out is if the agents are spread over a large region of space and protected by light speed lags.
<br>
<p>&nbsp;&nbsp;If recursive self-improvement is just an acceleration of natural-selection style evolution, I wouldn't expect it to be any more likely to lead to existential destruction than current evolution. Animals got more dangerous, but they also got better able to defend themselves. A few people get richer, then poorer people figure out how to get just as rich. It's a continual ratcheting up. 
<br>
&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;gej
<br>
&nbsp;&nbsp;
<br>
<p>Michael Vassar &lt;<a href="mailto:michaelvassar@hotmail.com?Subject=Re:%20SIAI%20&amp;%20Kurweil's%20Singularity">michaelvassar@hotmail.com</a>&gt; wrote:
<br>
&nbsp;&nbsp;I would assert that to date more intelligent and otherwise more capable 
<br>
instance of human being *are* particularly more trustworthy than other 
<br>
humans, at least in proportion to how much power they hold, but the 
<br>
relationship is only moderately strong and may be specific to Western 
<br>
culture.
<br>
<p>Unfortunately, this doesn't tell us much about radically augmented humans in 
<br>
any event. The difference among humans is too small to extrapolate to that 
<br>
between humans. Also, power selects for ambition and recklessness almost as 
<br>
much as for intelligence, both today and in a regime of human recursive 
<br>
self-improvement.
<br>
<p>My guess is that a human who understood existential risks well prior to 
<br>
recursive self-improvement and who had a substantial head-start on other 
<br>
humans could slow take-off safely, but I would not want to risk it. The 
<br>
chance of survival in a scenario of economic or other forms of competition 
<br>
between agents capable of recursive self-improvement seems close to zero 
<br>
though. The only way I can imagine it working out is if the agents are 
<br>
spread over a large region of space and protected by light speed lags.
<br>
<p><p>&nbsp;&nbsp;
<br>
<p><p>__________________________________________________
<br>
Do You Yahoo!?
<br>
Tired of spam?  Yahoo! Mail has the best spam protection around 
<br>
<a href="http://mail.yahoo.com">http://mail.yahoo.com</a> 
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13216.html">Phillip Huggan: "Re: Destruction of All Humanity"</a>
<li><strong>Previous message:</strong> <a href="13214.html">1Arcturus: "Re: SIAI &amp; Kurweil's Singularity"</a>
<li><strong>In reply to:</strong> <a href="13210.html">Michael Vassar: "Re: SIAI &amp; Kurweil's Singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13223.html">Michael Vassar: "Re: SIAI &amp; Kurweil's Singularity"</a>
<li><strong>Reply:</strong> <a href="13223.html">Michael Vassar: "Re: SIAI &amp; Kurweil's Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13215">[ date ]</a>
<a href="index.html#13215">[ thread ]</a>
<a href="subject.html#13215">[ subject ]</a>
<a href="author.html#13215">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
