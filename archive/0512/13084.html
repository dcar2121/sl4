<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: passive AI</title>
<meta name="Author" content="Michael Vassar (michaelvassar@hotmail.com)">
<meta name="Subject" content="passive AI">
<meta name="Date" content="2005-12-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>passive AI</h1>
<!-- received="Fri Dec  9 09:31:51 2005" -->
<!-- isoreceived="20051209163151" -->
<!-- sent="Fri, 09 Dec 2005 11:31:47 -0500" -->
<!-- isosent="20051209163147" -->
<!-- name="Michael Vassar" -->
<!-- email="michaelvassar@hotmail.com" -->
<!-- subject="passive AI" -->
<!-- id="BAY101-F37418B2D228AF751598124AC450@phx.gbl" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Vassar (<a href="mailto:michaelvassar@hotmail.com?Subject=Re:%20passive%20AI"><em>michaelvassar@hotmail.com</em></a>)<br>
<strong>Date:</strong> Fri Dec 09 2005 - 09:31:47 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13085.html">David Clark: "Re: On The Nature Of Qualia"</a>
<li><strong>Previous message:</strong> <a href="13083.html">Michael Wilson: "Re: Pete &amp; Passive AI"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13084">[ date ]</a>
<a href="index.html#13084">[ thread ]</a>
<a href="subject.html#13084">[ subject ]</a>
<a href="author.html#13084">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Look, it's very simply.  Telling an AI not to take actions is an ethical 
<br>
injunction.  It's a more powerful version of the 3 laws, but has all the 
<br>
same problems.  Ultimately &quot;take actions&quot; is not well defined, especially as 
<br>
distinct from &quot;think&quot; either for a human or for an AI but especially for the 
<br>
latter.  Processing inputs necessarily involves effecting the outside world.
<br>
Something without a goal system is not an AI.  The story about what such an 
<br>
&quot;AI&quot; would do implies that it has a goal system and that the creators easily 
<br>
load in other subgoals.
<br>
<p><em>&gt;Lets say Mr. A want ice cream. Some part of his brain “says”: “I want ice 
</em><br>
<em>&gt;cream.” Some other part of his brain has the definition of ice cream. Some 
</em><br>
<em>&gt;other part can infer things. I.e.: it can infer that he remains seated his 
</em><br>
<em>&gt;odds of getting ice cream are lower than if he goes to his fridge. Various 
</em><br>
<em>&gt;other parts do various things. The important thing is that only the 
</em><br>
<em>&gt;“wanting” part can initiate action
</em><br>
<p>This is a hideously crude analogy from folk psychology to AI behavior.
<br>
<p><em>&gt;Now, we remove the &quot;wanting&quot; part(s) without damaging any other parts.
</em><br>
<p>In a Fantasy story, but not in a sf story informed by modern popular 
<br>
science, not to mention not in reality.
<br>
<p><em>&gt;“Readout” displays thoughts crossing his mind. “Send” is the thoughts we 
</em><br>
<em>&gt;send through the interface.
</em><br>
<p>This reminds me of the &quot;Black box&quot; idea from early SIAI speculation
<br>
<p><em>&gt;Readout: &lt;empty&gt; Send: What is ice cream? Readout: &lt;definition of ice 
</em><br>
<em>&gt;cream&gt;
</em><br>
<p>[Playing along], and why would the AI want to think about what ice cream was 
<br>
when asked?
<br>
<p><em>&gt;Send: How can you increase your odds of getting ice cream?
</em><br>
<p>Are you giving it the goal system &quot;maximize user's knowledge of how to 
<br>
increase the odds of getting ice cream?&quot; or &quot;name an action that increases 
<br>
odds of getting ice cream&quot; or something different
<br>
<p><em>&gt;Readout: Maximum “ice cream getting” odds will occur if I go to the fridge.
</em><br>
<p>That's not what the above goals do.
<br>
The first goal kills/remakes the user and probably destroys the world as a 
<br>
side effect.
<br>
The second goal does nothing, because you didn't tell the &quot;AI&quot; when to do 
<br>
it.  Hey, how did the AI learn to understand natural language anyway?  To 
<br>
model the world?  Did it do all this without any independently directed 
<br>
action?
<br>
<p><em>&gt;Send: Do you want ice cream? Readout: No
</em><br>
<p>Why, without a goal system, is it even 'motivated' to answer?
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13085.html">David Clark: "Re: On The Nature Of Qualia"</a>
<li><strong>Previous message:</strong> <a href="13083.html">Michael Wilson: "Re: Pete &amp; Passive AI"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13084">[ date ]</a>
<a href="index.html#13084">[ thread ]</a>
<a href="subject.html#13084">[ subject ]</a>
<a href="author.html#13084">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
