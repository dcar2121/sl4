<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]">
<meta name="Date" content="2005-12-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]</h1>
<!-- received="Wed Dec 14 08:54:20 2005" -->
<!-- isoreceived="20051214155420" -->
<!-- sent="Wed, 14 Dec 2005 10:53:01 -0500" -->
<!-- isosent="20051214155301" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]" -->
<!-- id="43A03FDD.50506@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="c8ff78370512140626h53e8ad7bxe879cdacc4d7da91@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Not%20the%20only%20way%20to%20build%20an%20AI%20[WAS:%20Please%20Re-read%20CAFAI]"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Wed Dec 14 2005 - 08:53:01 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13164.html">Jef Allbright: "Re: Destruction of All Humanity"</a>
<li><strong>Previous message:</strong> <a href="13162.html">Jef Allbright: "Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]"</a>
<li><strong>In reply to:</strong> <a href="13157.html">Eric Rauch: "Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13162.html">Jef Allbright: "Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13163">[ date ]</a>
<a href="index.html#13163">[ thread ]</a>
<a href="subject.html#13163">[ subject ]</a>
<a href="author.html#13163">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eric Rauch wrote:
<br>
<em>&gt; Given your skepticism of goal guided agi what do you think of the
</em><br>
<em>&gt; goaless agi I suggested.
</em><br>
<em>&gt; 
</em><br>
<p>An AGI cannot be completely goalless, because the more you try to 
<br>
exclude *any* kind of goal mechanism, the more you make it into a 
<br>
conventional program that, because if its simplicity, is simply not 
<br>
going to have the flexibility to be intelligent.
<br>
<p>The tricky part is that &quot;goal system&quot; has many interpretations, and I am 
<br>
not denying the need for some kind of goal system, because that is the 
<br>
mechanism that causes the system to organize its model-building (the 
<br>
activity that senses the world, builds models of what is going out 
<br>
there, and uses those models to take actions .... all of thinking, in 
<br>
other words).  Without a goal mechanism of some sort, the AGI just sits 
<br>
there having random thoughts about whatever takes its fancy, and that 
<br>
kind of creature would never get smart in the first place.  My specific 
<br>
grouse is that people sometimes have a very narrow intepretation of what 
<br>
that goal system is, and so delude themselves into thinking that it 
<br>
works in a very deterministic way (and hence can be set up in such a way 
<br>
as to &quot;guarantee&quot; friendliness).  I think that that kind of goal system 
<br>
would not work, but a motivation/goal system, in which there were 
<br>
general motivations driving the overall behavior, as well as a flexible 
<br>
goal system that loosely governs the moment-to-moment processes, is the 
<br>
way we will eventually have to build the AGI.
<br>
<p>Answering your specific question:  I can see how one could build 
<br>
quasi-intelligent systems that did some knowledge acquisition (I would 
<br>
call it a &quot;drone&quot;) without having enough of a goal system to know what 
<br>
it was doing (without being self-aware).  Rather like an extreme version 
<br>
of a human savant.  But that is a slightly different issue... not quite 
<br>
a missing goal system of the sort you were after, just an impoverished 
<br>
goal system, and with other stuff missing.  But I think such a thing 
<br>
would have to be governed (managed, operated, controlled) by a real AGI. 
<br>
&nbsp;&nbsp;What I have in mind here is the idea that a real AGI could create vast 
<br>
numbers of drones that did specialized research on particular tasks 
<br>
without having much awareness of the nature of self, so they would be 
<br>
like the things we call &quot;number crunchers&quot;, except the AGI would call 
<br>
them &quot;concept crunchers&quot;.  Maybe such a system would satisfy some of the 
<br>
requirements you had in mind when you talked of a goalless AGI, although 
<br>
it would be no good if it had to have a real AGI to supervise it.
<br>
<p><em> &gt;
</em><br>
<em> &gt; Also I'm curious as to how the members of this list maintain such a
</em><br>
<em> &gt; high degree of certainty about the behavior of post singularity
</em><br>
<em> &gt; intelligences which are almost by definition supposed to be beyond our
</em><br>
<em> &gt; comprehension (richard this is not directed at you)
</em><br>
<em> &gt;
</em><br>
<p>I will comment anyway ;-).  I think you have put your finger on one of 
<br>
the big, glaring inconsistencies in the discussions that take place 
<br>
here.  I believe the root of this is the same dogmatic attachment to the 
<br>
&quot;Neat&quot; (mostly decision-theory-based) approach to AI.  (cf the Neats vs 
<br>
Scruffs war .... I count myself as a NeoScruff).
<br>
<p><p>Richard Loosemore
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13164.html">Jef Allbright: "Re: Destruction of All Humanity"</a>
<li><strong>Previous message:</strong> <a href="13162.html">Jef Allbright: "Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]"</a>
<li><strong>In reply to:</strong> <a href="13157.html">Eric Rauch: "Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13162.html">Jef Allbright: "Re: Not the only way to build an AI [WAS: Please Re-read CAFAI]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13163">[ date ]</a>
<a href="index.html#13163">[ thread ]</a>
<a href="subject.html#13163">[ subject ]</a>
<a href="author.html#13163">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
