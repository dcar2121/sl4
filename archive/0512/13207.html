<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Destruction of All Humanity</title>
<meta name="Author" content="Samantha Atkins (sjatkins@gmail.com)">
<meta name="Subject" content="Re: Destruction of All Humanity">
<meta name="Date" content="2005-12-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Destruction of All Humanity</h1>
<!-- received="Thu Dec 15 19:34:34 2005" -->
<!-- isoreceived="20051216023434" -->
<!-- sent="Thu, 15 Dec 2005 18:34:32 -0800" -->
<!-- isosent="20051216023432" -->
<!-- name="Samantha Atkins" -->
<!-- email="sjatkins@gmail.com" -->
<!-- subject="Re: Destruction of All Humanity" -->
<!-- id="948b11e0512151834i4f81adb9i3901e91df805afd0@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="638d4e150512120941u48209f2cp1439ab971704bf6e@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:sjatkins@gmail.com?Subject=Re:%20Destruction%20of%20All%20Humanity"><em>sjatkins@gmail.com</em></a>)<br>
<strong>Date:</strong> Thu Dec 15 2005 - 19:34:32 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13208.html">Olie L: "Re: Destruction of All Humanity"</a>
<li><strong>Previous message:</strong> <a href="13206.html">Samantha Atkins: "Re: SIAI &amp; Kurweil's Singularity"</a>
<li><strong>In reply to:</strong> <a href="13110.html">Ben Goertzel: "Re: Destruction of All Humanity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13208.html">Olie L: "Re: Destruction of All Humanity"</a>
<li><strong>Reply:</strong> <a href="13208.html">Olie L: "Re: Destruction of All Humanity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13207">[ date ]</a>
<a href="index.html#13207">[ thread ]</a>
<a href="subject.html#13207">[ subject ]</a>
<a href="author.html#13207">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Better for whom, Ben?  Do you believe in some Universal Better that trumps
<br>
the very existence of large groups of sentient beings - the only type of
<br>
beings that &quot;better&quot; can have any meaning for?    How could it be better to
<br>
an intelligence capable of simulating an entire world and even a universe
<br>
for humans to exist in with an infinitesimal fraction of its abilities?  I
<br>
do not believe simply destroying entire species of sentient beings when
<br>
there are viable alternatives could qualify as &quot;better&quot; - certainly not form
<br>
the pov of said beings. I don't find it particularly intelligent to use our
<br>
intelligence to make our own utter destruction &quot;reasonable&quot;.   I would fight
<br>
such an AI.  I might not last long but I wouldn't simply agree.
<br>
<p>-- samantha
<br>
<p><p>On 12/12/05, Ben Goertzel &lt;<a href="mailto:ben@goertzel.org?Subject=Re:%20Destruction%20of%20All%20Humanity">ben@goertzel.org</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; Hi,
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't normally respond for other people nor for organizations I
</em><br>
<em>&gt; don't belong to, but in this case, since no one from SIAI has
</em><br>
<em>&gt; responded yet and the allegation is so silly, I'll make an exception.
</em><br>
<em>&gt;
</em><br>
<em>&gt; No, this is not SIAI's official opinion, and I am also quite sure that
</em><br>
<em>&gt; it is it not Eliezer's opinion.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Whether it is *like* anything Eliezer has ever said is a different
</em><br>
<em>&gt; question, and depends upon your similarity measure!
</em><br>
<em>&gt;
</em><br>
<em>&gt; Speaking for myself now (NOT Eliezer or anyone else): I can imagine a
</em><br>
<em>&gt; scenario where I created an AGI to decide, based on my own value
</em><br>
<em>&gt; system, what would be the best outcome for the universe.  I can
</em><br>
<em>&gt; imagine working with this AGI long enough that I really trusted it,
</em><br>
<em>&gt; and then having this AGI conclude that the best outcome for the
</em><br>
<em>&gt; universe involves having the human race (including me) stop existing
</em><br>
<em>&gt; and having our particles used in some different way.  I can imagine,
</em><br>
<em>&gt; in this scenario, having a significant desire to actually go along
</em><br>
<em>&gt; with the AGI's opinion, though I doubt that I would do so.  (Perhaps I
</em><br>
<em>&gt; would do so if I were wholly convinced that the overall state of the
</em><br>
<em>&gt; universe would be a LOT better if the human race's particles were thus
</em><br>
<em>&gt; re-purposed?)
</em><br>
<em>&gt;
</em><br>
<em>&gt; And, I suppose someone could twist the above paragraph to say that
</em><br>
<em>&gt; &quot;Ben Goertzel says if a superintelligence should order all humans to
</em><br>
<em>&gt; die, then all humans should die.&quot;  But it would be quite a
</em><br>
<em>&gt; misrepresentation...
</em><br>
<em>&gt;
</em><br>
<em>&gt; -- Ben G
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; On 12/12/05, 1Arcturus &lt;<a href="mailto:arcturus12453@yahoo.com?Subject=Re:%20Destruction%20of%20All%20Humanity">arcturus12453@yahoo.com</a>&gt; wrote:
</em><br>
<em>&gt; &gt; Someone on the wta-list recently posted an opinion that he attribtuted
</em><br>
<em>&gt; to
</em><br>
<em>&gt; &gt; Mr. Yudkowsky, something to the effect that if a superintelligence
</em><br>
<em>&gt; should
</em><br>
<em>&gt; &gt; order all humans to die, then all humans should die.
</em><br>
<em>&gt; &gt; Is that a wild misrepresentation, and like nothing that Mr. Yudkowsky
</em><br>
<em>&gt; has
</em><br>
<em>&gt; &gt; ever said?
</em><br>
<em>&gt; &gt; Or is it in fact his opinion, and that of SIAI?
</em><br>
<em>&gt; &gt; Just curious...
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; gej
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; ________________________________
</em><br>
<em>&gt; &gt; Yahoo! Shopping
</em><br>
<em>&gt; &gt; Find Great Deals on Holiday Gifts at Yahoo! Shopping
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13208.html">Olie L: "Re: Destruction of All Humanity"</a>
<li><strong>Previous message:</strong> <a href="13206.html">Samantha Atkins: "Re: SIAI &amp; Kurweil's Singularity"</a>
<li><strong>In reply to:</strong> <a href="13110.html">Ben Goertzel: "Re: Destruction of All Humanity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13208.html">Olie L: "Re: Destruction of All Humanity"</a>
<li><strong>Reply:</strong> <a href="13208.html">Olie L: "Re: Destruction of All Humanity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13207">[ date ]</a>
<a href="index.html#13207">[ thread ]</a>
<a href="subject.html#13207">[ subject ]</a>
<a href="author.html#13207">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
