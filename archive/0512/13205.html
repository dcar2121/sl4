<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: SIAI &amp; Kurweil's Singularity</title>
<meta name="Author" content="Olie L (neomorphy@hotmail.com)">
<meta name="Subject" content="RE: SIAI &amp; Kurweil's Singularity">
<meta name="Date" content="2005-12-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: SIAI &amp; Kurweil's Singularity</h1>
<!-- received="Thu Dec 15 15:46:55 2005" -->
<!-- isoreceived="20051215224655" -->
<!-- sent="Fri, 16 Dec 2005 09:46:48 +1100" -->
<!-- isosent="20051215224648" -->
<!-- name="Olie L" -->
<!-- email="neomorphy@hotmail.com" -->
<!-- subject="RE: SIAI &amp; Kurweil's Singularity" -->
<!-- id="BAY106-F2335B38384B4008E60C553BA3B0@phx.gbl" -->
<!-- inreplyto="20051215191758.73403.qmail@web61120.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Olie L (<a href="mailto:neomorphy@hotmail.com?Subject=RE:%20SIAI%20&amp;%20Kurweil's%20Singularity"><em>neomorphy@hotmail.com</em></a>)<br>
<strong>Date:</strong> Thu Dec 15 2005 - 15:46:48 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13206.html">Samantha Atkins: "Re: SIAI &amp; Kurweil's Singularity"</a>
<li><strong>Previous message:</strong> <a href="13204.html">Jef Allbright: "Re: SIAI &amp; Kurweil's Singularity"</a>
<li><strong>In reply to:</strong> <a href="13200.html">1Arcturus: "SIAI &amp; Kurweil's Singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13213.html">1Arcturus: "RE: SIAI &amp; Kurweil's Singularity"</a>
<li><strong>Reply:</strong> <a href="13213.html">1Arcturus: "RE: SIAI &amp; Kurweil's Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13205">[ date ]</a>
<a href="index.html#13205">[ thread ]</a>
<a href="subject.html#13205">[ subject ]</a>
<a href="author.html#13205">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
(/This is a typical SL4 newbie-related question-answer thing.  How is it 
<br>
better having these fill up email lists than being put on a forum?/)
<br>
<p>As I read it, the big difference is that certain groups, including the SIAI, 
<br>
envisage that the ability of an AI that is able to directly modify itself 
<br>
could result in a rate of technological improvement far beyond current 
<br>
trends, even if current trends are exponential.  Such self-improving AIs 
<br>
(~seed AI) do not fit neatly into the GNR revolutions, as described by 
<br>
Kurzweil.
<br>
<p>Even if self-improving AIs take several years to integrate relevant 
<br>
infrastructure (to be able to have full impacts on humanity), the change in 
<br>
technological development rates will change much more quickly than the 
<br>
exponential trend. This matches more closely what is commonly described as 
<br>
&quot;hard takeoff&quot; (which could happen really, REALLY fast)
<br>
<p>(more)
<br>
<p><em>&gt;From: 1Arcturus &lt;<a href="mailto:arcturus12453@yahoo.com?Subject=RE:%20SIAI%20&amp;%20Kurweil's%20Singularity">arcturus12453@yahoo.com</a>&gt;
</em><br>
<em>&gt;Reply-To: <a href="mailto:sl4@sl4.org?Subject=RE:%20SIAI%20&amp;%20Kurweil's%20Singularity">sl4@sl4.org</a>
</em><br>
<em>&gt;To: <a href="mailto:sl4@sl4.org?Subject=RE:%20SIAI%20&amp;%20Kurweil's%20Singularity">sl4@sl4.org</a>
</em><br>
<em>&gt;Subject: SIAI &amp; Kurweil's Singularity
</em><br>
<em>&gt;Date: Thu, 15 Dec 2005 11:17:58 -0800 (PST)
</em><br>
<em>&gt;
</em><br>
<em>&gt;I had another question about SIAI in relation to Kurzweil's latest book 
</em><br>
<em>&gt;Singularity is Near.
</em><br>
<em>&gt;
</em><br>
<em>&gt;   If I have him right, Kurzweil predicts that humans will gradually merge 
</em><br>
<em>&gt;with their technology - the technology becoming more humanlike, more 
</em><br>
<em>&gt;biological-compatible, and integrating into the human body and mental 
</em><br>
<em>&gt;processes, until eventually the purely 'biological' portion becomes less 
</em><br>
<em>&gt;and less predominant or disappears entirely.
</em><br>
<p>That's not a question.
<br>
<p><em>&gt;SIAI seems to presuppose a very different scenario - that strongly 
</em><br>
<em>&gt;superintelligent AI will arise first in pure machines, and never 
</em><br>
<em>&gt;(apparently) in humans. There seems to be no indication of 'merger', more 
</em><br>
<em>&gt;like a kind of AI-rule over mostly unmodified humans.
</em><br>
<p>It's not so much a &quot;never&quot;.  It's that the best way to achieve 
<br>
superintelligence is to start with an expandable design.  Once there is 
<br>
superintelligence, it would be much easier to figure how to integrate human 
<br>
people.
<br>
<p><em>&gt;
</em><br>
<em>&gt;   Some of this difference may be because Kurzweil predicts nanotechnology 
</em><br>
<em>&gt;in the human body (including the brain) and very advanced human-machine 
</em><br>
<em>&gt;interfaces will arise before strongly superintelligent AI, and that 
</em><br>
<em>&gt;strongly superintelligent AI will require the completion of the 
</em><br>
<em>&gt;reverse-engineering of the human brain. (Completed reverse-engineering of 
</em><br>
<em>&gt;the brain + adequate brain scanning surely = ability to upload part or all 
</em><br>
<em>&gt;of human selves?)
</em><br>
<p>I don't see that being a necessity at all.  Uploading is _very_ different 
<br>
from a full understanding of brain ops.
<br>
<p><em>&gt;
</em><br>
<em>&gt;   But SIAI seems to assume AIs will become strongly superintelligent by 
</em><br>
<em>&gt;their own design, arising from human designs, before humans ever finish 
</em><br>
<em>&gt;reverse-engineering the human brain.
</em><br>
<p>Again, not will, /MAY/, which is relevant because of hard take-off
<br>
<p><em>&gt;The lack of a fully functional interface with the strongly intelligent AIs 
</em><br>
<em>&gt;would cause humans to be dependent on the AIs to do the thinking from then 
</em><br>
<em>&gt;on, and the AIs would take on the responsibility for the thinking of course 
</em><br>
<em>&gt;also. This seems to assume the AIs would not be able to, or not want to, 
</em><br>
<em>&gt;create interfaces or upload the humans -- that is, it would not 'uplift' 
</em><br>
<em>&gt;the humans to its own level of intelligence so that they could then 
</em><br>
<em>&gt;understand each other.
</em><br>
<p>Consider for a moment that a lot of people won't want to change themselves.  
<br>
Would they rather be dominated by upgraded humans (who have human foibles 
<br>
and motivations) or be &quot;governed&quot; by AIs who lack self-interested biases?
<br>
<p><em>&gt;
</em><br>
<em>&gt;   I am trying to understand SIAI's position, or at least the emphasis of 
</em><br>
<em>&gt;posters here and some representatives I have heard, contrasted with 
</em><br>
<em>&gt;Kuzweil's book. There seems to be a contrast to me, although I know 
</em><br>
<em>&gt;Kurzweil is involved with SIAI also.
</em><br>
<em>&gt;
</em><br>
...
<br>
<p><em>&gt;   None of these things are problematic if humans merge with technology and 
</em><br>
<em>&gt;acquire its capacity for strong superintelligence. That is, humans would be 
</em><br>
<em>&gt;at the very center of the Singularity and direct its development, for 
</em><br>
<em>&gt;better or worse, with 'open eyes', and taking responsibility themselves 
</em><br>
<em>&gt;rather than lending it to an external machine.
</em><br>
<p>Yes, and I'm sure you're aware of humanity's success in long-term planning.  
<br>
Foresight is not our greatest quality; being amongst rapid rates of 
<br>
technological development does not help us guide all the development by all 
<br>
the people...
<br>
<p>-- Olie
<br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13206.html">Samantha Atkins: "Re: SIAI &amp; Kurweil's Singularity"</a>
<li><strong>Previous message:</strong> <a href="13204.html">Jef Allbright: "Re: SIAI &amp; Kurweil's Singularity"</a>
<li><strong>In reply to:</strong> <a href="13200.html">1Arcturus: "SIAI &amp; Kurweil's Singularity"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13213.html">1Arcturus: "RE: SIAI &amp; Kurweil's Singularity"</a>
<li><strong>Reply:</strong> <a href="13213.html">1Arcturus: "RE: SIAI &amp; Kurweil's Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13205">[ date ]</a>
<a href="index.html#13205">[ thread ]</a>
<a href="subject.html#13205">[ subject ]</a>
<a href="author.html#13205">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
