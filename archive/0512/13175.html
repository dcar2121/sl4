<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Please Re-read CAFAI</title>
<meta name="Author" content="micah glasser (micahglasser@gmail.com)">
<meta name="Subject" content="Re: Please Re-read CAFAI">
<meta name="Date" content="2005-12-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Please Re-read CAFAI</h1>
<!-- received="Wed Dec 14 10:29:40 2005" -->
<!-- isoreceived="20051214172940" -->
<!-- sent="Wed, 14 Dec 2005 12:29:38 -0500" -->
<!-- isosent="20051214172938" -->
<!-- name="micah glasser" -->
<!-- email="micahglasser@gmail.com" -->
<!-- subject="Re: Please Re-read CAFAI" -->
<!-- id="23bd28ec0512140929u3f1135b7p8ae2ebb57bb04bf7@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="22360fa10512132337u6bf5516bn76cb94d7cb701371@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> micah glasser (<a href="mailto:micahglasser@gmail.com?Subject=Re:%20Please%20Re-read%20CAFAI"><em>micahglasser@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Dec 14 2005 - 10:29:38 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="13176.html">Jef Allbright: "Re: Destruction of All Humanity"</a>
<li><strong>Previous message:</strong> <a href="13174.html">Jef Allbright: "Re: Is furthering your own goals always good? Judged by whom, and for what purpose?"</a>
<li><strong>In reply to:</strong> <a href="13155.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13186.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<li><strong>Reply:</strong> <a href="13186.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13175">[ date ]</a>
<a href="index.html#13175">[ thread ]</a>
<a href="subject.html#13175">[ subject ]</a>
<a href="author.html#13175">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Jef,
<br>
I agree with you that 'good' is nothing more than furthering one's goal
<br>
regardless of what that goal is in one sense. On the other hand might it not
<br>
be possible that there is an objective best trajectory of advancement for
<br>
all of mankind, and would this not be 'the good' in an objective sense? If
<br>
so, then we would want to ensure that the goal of achieving this objective
<br>
good for mankind was also a goal system of an AGI. Also, I would like to
<br>
point out that my thinking is certainly informed by modern cognitive
<br>
psychology and evolutionary psychology. I just think that far to often
<br>
people's philosophical thinking gets mushy because they start trying to
<br>
solve philosophical questions in the same way one would solve a scientific
<br>
problem. It is often good to go back to the great philosophers of yesteryear
<br>
for a refreshing new perspective on present problems of mind.
<br>
<p>On 12/14/05, Jef Allbright &lt;<a href="mailto:jef@jefallbright.net?Subject=Re:%20Please%20Re-read%20CAFAI">jef@jefallbright.net</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; Jef (to Michael)
</em><br>
<em>&gt; Do you mean that to the extent an agent is rational, it will naturally
</em><br>
<em>&gt; use all of its
</em><br>
<em>&gt; instrumental knowledge to promote its own goals and from its point of
</em><br>
<em>&gt; view there would be no question that such action is good?
</em><br>
<em>&gt; Tennessee
</em><br>
<em>&gt; Well, the CI is by definition an objective thing. I do *not* believe
</em><br>
<em>&gt; that it's obvious that all AIs would be relativists about morality.
</em><br>
<em>&gt; However, I believe it's most likely so we don't need to argue about
</em><br>
<em>&gt; it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Jef
</em><br>
<em>&gt; I certainly don't see any reason to argue about that particular topic
</em><br>
<em>&gt; at this time.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Tennessee
</em><br>
<em>&gt; The intended question has a double-entendre. The first meaning is that
</em><br>
<em>&gt; &quot;all my goals are good from my point of view, by definition, so I
</em><br>
<em>&gt; don't need to question it&quot;. The second meaning is &quot;I have no
</em><br>
<em>&gt; conception of morality, therefore I don't need to question it&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Jef
</em><br>
<em>&gt; My intent was closer to your first interpretation, but perhaps not
</em><br>
<em>&gt; exact.  When I say &quot;good&quot; I don't mean it in a moral sense or as in
</em><br>
<em>&gt; &quot;good&quot; vs. &quot;evil&quot;.  I am trying (and not doing a very good [oops, that
</em><br>
<em>&gt; word again] job of it) to show that any agent (human, AGI, or other
</em><br>
<em>&gt; form) must necessarily evaluate actions with respect to achieving its
</em><br>
<em>&gt; goals, and that actions which promote its goals must be considered
</em><br>
<em>&gt; good to some extent while actions that detract from its goals must be
</em><br>
<em>&gt; considered bad.  I am arguing that what is called &quot;good&quot; is what
</em><br>
<em>&gt; works, and while such evaluations are necessarily from a subjective
</em><br>
<em>&gt; viewpoint, that we can all agree (objectively) that for each of us,
</em><br>
<em>&gt; what works is considered good.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &lt;snip&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Tennessee
</em><br>
<em>&gt; I think it's concievable that an AGI might have no moral sense, and be
</em><br>
<em>&gt; bound only by consequential reasoning about its goals. I also think
</em><br>
<em>&gt; it's concievable than an AGI would have a moral sense, but due to its
</em><br>
<em>&gt; mental model have varying beliefs about good and evil, despite its
</em><br>
<em>&gt; conclusions about what is objectively true.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Jef
</em><br>
<em>&gt; I would argue, later, that any moral &quot;sense&quot; is ultimately due to
</em><br>
<em>&gt; consequential effects at some level, but I don't want to jump ahead
</em><br>
<em>&gt; yet.  I would also argue, later, that any moral sense based on
</em><br>
<em>&gt; &quot;varying beliefs about good and evil, despite its conclusion about
</em><br>
<em>&gt; what is objectively true&quot; (as you put it) would be an accurate
</em><br>
<em>&gt; description of part of current human morality, but limited in its
</em><br>
<em>&gt; capability to promote good due to its restrictions on its own
</em><br>
<em>&gt; instrumental knowledge that must be employed in the promotion of its
</em><br>
<em>&gt; values.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Jef's question #2 to Michael
</em><br>
<em>&gt; If this [question #1] is true, then would it also see increasing its
</em><br>
<em>&gt; objective knowledge in support of its goals as rational and inherently
</em><br>
<em>&gt; good (from its point of view?)
</em><br>
<em>&gt;
</em><br>
<em>&gt; Tennessee
</em><br>
<em>&gt; Not necessarily. It may consider knowledge to be inherently morally
</em><br>
<em>&gt; neutral, although in consequential terms accumulated knowledge may be
</em><br>
<em>&gt; morally valuable. An AGI acting under CI would desire to accumulate
</em><br>
<em>&gt; objective knowledge as it related to its goals, but not necessarily
</em><br>
<em>&gt; see it as good in itself.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Jef
</em><br>
<em>&gt; I wasn't making any claim about something being good in itself. I was
</em><br>
<em>&gt; careful each time to frame &quot;good&quot; as the subjective evaluation by an
</em><br>
<em>&gt; agent with regard to whether some action promoted its goals.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Tennessee
</em><br>
<em>&gt; There are a lot of abstractions here. Subjectivity doesn't destroy my
</em><br>
<em>&gt; point. Even from its own point of view, it might be that some
</em><br>
<em>&gt; knowledge is good, and some is not good, and some is neutral. An AGI
</em><br>
<em>&gt; might regard knowledge as good, if it contributes to its goals, for
</em><br>
<em>&gt; example. If that were the litmus test, then subjectively speaking,
</em><br>
<em>&gt; some knowledge would be good, and some would be morally neutral.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Perhaps you missed what I see as the obvious logical opposite -- that
</em><br>
<em>&gt; the AGI adopts, subjectively speaking, the &quot;belief&quot;
</em><br>
<em>&gt; (goal/desire/whatever) than Knowledge Is Good. In this case, the AGI
</em><br>
<em>&gt; desires knowledge *as an end in itself* and not *solely for its
</em><br>
<em>&gt; contribution to other goals*.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Jef's question #3 to Michael
</em><br>
<em>&gt; If I'm still understanding the implications of what you said, would
</em><br>
<em>&gt; this also mean that cooperation with other like-minded agents, to the
</em><br>
<em>&gt; extent that this increased the promotion of its own goals, would be
</em><br>
<em>&gt; rational and good (from its point of view?)
</em><br>
<em>&gt;
</em><br>
<em>&gt; Tennessee
</em><br>
<em>&gt; Obviously, in the simple case.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Jef
</em><br>
<em>&gt; Interesting that you seemed to disagree with the previous assertions,
</em><br>
<em>&gt; but seemed to agree with this one that I thought was posed within the
</em><br>
<em>&gt; same framework. It seems as if you were not reading carefully, and
</em><br>
<em>&gt; responding to what you assumed might have been said.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Tennessee
</em><br>
<em>&gt; Well, you just said something that was true this time :). I'm not
</em><br>
<em>&gt; going to change my mind because you said it in the context of a wider
</em><br>
<em>&gt; flawed argument ;).
</em><br>
<em>&gt;
</em><br>
<em>&gt; Let's suppose than an AGI assesses some possible course of action --
</em><br>
<em>&gt; in this case one of interaction and co-operation. It works out that
</em><br>
<em>&gt; pursuing it will contribute to the furtherance of its own goals.  It
</em><br>
<em>&gt; certainly isn't going to think that such a thing is either evil or
</em><br>
<em>&gt; bad. It may have no sense of morality, but in the sense of
</em><br>
<em>&gt; advantageous, such an action will be good.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Let me then clarify: yes, such a thing will always be advantageous.
</em><br>
<em>&gt; Insofar as an AGI has a moral system, such a thing will also be
</em><br>
<em>&gt; morally good.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Jef
</em><br>
<em>&gt; Yes, I consistently mean &quot;good&quot; in the sense of advantageous.  It
</em><br>
<em>&gt; seems that making this clear from the beginning is key to more
</em><br>
<em>&gt; effective communication on this topic.  My difficulty with this is I
</em><br>
<em>&gt; see &quot;good&quot; as *always&quot; meaning advantageous, but with varying context.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think we are in agreement that for any agent, those actions which
</em><br>
<em>&gt; promote its values will necessarily be seen by that agent as good.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Thanks for taking the time to work through this, and thanks to the
</em><br>
<em>&gt; list for tolerating what (initially, at least) appeared to be very low
</em><br>
<em>&gt; signal to noise.
</em><br>
<em>&gt;
</em><br>
<em>&gt; - Jef
</em><br>
<em>&gt;
</em><br>
<p><p><p><pre>
--
I swear upon the alter of God, eternal hostility to every form of tyranny
over the mind of man. - Thomas Jefferson
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13176.html">Jef Allbright: "Re: Destruction of All Humanity"</a>
<li><strong>Previous message:</strong> <a href="13174.html">Jef Allbright: "Re: Is furthering your own goals always good? Judged by whom, and for what purpose?"</a>
<li><strong>In reply to:</strong> <a href="13155.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13186.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<li><strong>Reply:</strong> <a href="13186.html">Jef Allbright: "Re: Please Re-read CAFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#13175">[ date ]</a>
<a href="index.html#13175">[ thread ]</a>
<a href="subject.html#13175">[ subject ]</a>
<a href="author.html#13175">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:54 MDT
</em></small></p>
</body>
</html>
