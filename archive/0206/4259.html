<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: QUES: CFAI +</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: QUES: CFAI +">
<meta name="Date" content="2002-06-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: QUES: CFAI +</h1>
<!-- received="Sun Jun 23 19:12:55 2002" -->
<!-- isoreceived="20020624011255" -->
<!-- sent="Sun, 23 Jun 2002 18:57:55 -0400" -->
<!-- isosent="20020623225755" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: QUES: CFAI +" -->
<!-- id="3D165273.30700@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="OE156gfCUX3bBT7OR7s0003ca62@hotmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20QUES:%20CFAI%20%2B"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jun 23 2002 - 16:57:55 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4260.html">Eliezer S. Yudkowsky: "Re: QUES: CFAI +"</a>
<li><strong>Previous message:</strong> <a href="4258.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4250.html">Anand: "Re: QUES: CFAI +"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4260.html">Eliezer S. Yudkowsky: "Re: QUES: CFAI +"</a>
<li><strong>Reply:</strong> <a href="4260.html">Eliezer S. Yudkowsky: "Re: QUES: CFAI +"</a>
<li><strong>Reply:</strong> <a href="4268.html">Stephen Reed: "Re: QUES: CFAI +"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4259">[ date ]</a>
<a href="index.html#4259">[ thread ]</a>
<a href="subject.html#4259">[ subject ]</a>
<a href="author.html#4259">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Anand wrote:
<br>
<em> &gt; Eliezer Yudkowsky wrote:
</em><br>
<em> &gt;
</em><br>
<em> &gt;&gt;Remember, however, that by the Law of Programmer Symmetry - if I may call
</em><br>
<em> &gt;&gt;it such - volition-based Friendliness is not the problem. The problem is
</em><br>
<em> &gt;&gt;coming up with a strategy such that if some other programming team follows
</em><br>
<em> &gt;&gt;it, their AI will eventually arrive at volition-based Friendliness [or
</em><br>
<em> &gt;&gt;something better] regardless of what their programmers started out
</em><br>
<em> &gt;&gt;believing.  And to do that you have to pass along to the AI an
</em><br>
<em> &gt;&gt;understanding of how people argue about morality, in a semantics rich
</em><br>
<em> &gt;&gt;enough to represent all the structural properties thereof.
</em><br>
<em> &gt;
</em><br>
<em> &gt;
</em><br>
<em> &gt; &quot;The problem is coming up...&quot; What knowledge do you, or what understanding
</em><br>
<em> &gt; do we, presently lack to appropriately solve the specified problem?
</em><br>
<p>CFAI was developed specifically as a solution to this problem.  An AI 
<br>
developed using CFAI structure and appropriate content should understand the 
<br>
metawish of &quot;Be the best AI we or any other programming team could have made 
<br>
you to be&quot;, in accordance with the full intent of that wish.  See also 
<br>
question #2 below.
<br>
<p><em> &gt;&gt;Anand wrote:
</em><br>
<em> &gt;&gt;
</em><br>
<em> &gt;&gt;
</em><br>
<em> &gt;&gt;&gt;01. Does CFAI argue for a set of panhuman characteristics that comprise
</em><br>
<em> &gt;&gt;&gt;human moral cognition? If so, what characteristics do we have evidence
</em><br>
<em> &gt;&gt;&gt;for, and what characteristics of human moral cognition will be
</em><br>
<em> &gt;&gt;&gt;reproduced?
</em><br>
<em> &gt;&gt;
</em><br>
<em> &gt;&gt;CFAI argues that there exists *some* set of panhuman characteristics, but
</em><br>
<em> &gt;&gt;does not argue for a *specific* set of panhuman characteristics. The model
</em><br>
<em> &gt;&gt;of Friendliness learning is based on reasoning backward from observed
</em><br>
<em> &gt;&gt;specific humans to a systemic model of altruism which is grounded in
</em><br>
<em> &gt;&gt;panhuman characteristics (and, if necessary, social and memetic
</em><br>
<em> &gt;&gt;organizational processes). In other words, the idea is not that *you*, the
</em><br>
<em> &gt;&gt;programmer, know how to build a model of altruism which is
</em><br>
<em> &gt;&gt;programmer-independent, but that you, the programmer, know how to
</em><br>
<em> &gt;&gt;build an AI which can arrive at such a model, given sufficient
</em><br>
<em> &gt;&gt;intelligence, and can rely on the interim approximation represented by
</em><br>
<em> &gt;&gt;the ethics of several specific programmers, given insufficient
</em><br>
<em> &gt;&gt;intelligence.
</em><br>
<em> &gt;
</em><br>
<em> &gt; Thank you for the response, but what evidence does cognitive science have
</em><br>
<em> &gt; for panhuman characteristics that comprise moral cognition?  If little or
</em><br>
<em> &gt; zero evidence presently exists, then why have you chosen to argue in CFAI
</em><br>
<em> &gt; &quot;that there exists *some* set of panhuman characteristics&quot;?
</em><br>
<p>Have you read &quot;The Psychological Foundations of Culture&quot; in &quot;The Adapted 
<br>
Mind&quot; by Tooby and Cosmides?  It sets forth the reason why complex 
<br>
functional adaptations are almost always panspecies.
<br>
<p>Incidentally, Friendly AI semantics can operate on foundational causes other 
<br>
than complex functional adaptations - anything in the past light cone can 
<br>
potentially be acquired.
<br>
<p><em> &gt; After reading your response, the following three questions also came to
</em><br>
<em> &gt; mind:
</em><br>
<em> &gt;
</em><br>
<em> &gt; 01.  What do you believe are the key dependencies of the theory of
</em><br>
<em> &gt; Friendliness, and how may these dependencies become invalidated?
</em><br>
<p>Well... hm.  I wasn't setting out to create a theory with key dependencies. 
<br>
&nbsp;&nbsp;Quite the opposite.  I was setting out to create a strategy that would 
<br>
*always* work, despite anything I could imagine throwing at it.  Sure, you 
<br>
can always break any physically realizable cognitive system with enough 
<br>
simultaneous coordinated errors, but there's no point in making it easy.
<br>
<p>This pragmatic example is probably long overdue, and should almost certainly 
<br>
have been included in the original CFAI... but *this* is what you're 
<br>
supposed to be able to do with a FAI that has causal validity semantics:
<br>
<p>Programmer:  &quot;Hey there, Aileen.&quot;
<br>
Aileen:  &quot;Hi.&quot;
<br>
P:  &quot;This is about 'Why Structure Matters' from CFAI.&quot;
<br>
&nbsp;&nbsp;&lt;<a href="http://intelligence.org/CFAI/design/structure/why.html">http://intelligence.org/CFAI/design/structure/why.html</a>&gt;
<br>
A:  &quot;Okay.&quot;
<br>
P:  &quot;You see how in each of the examples, the programmer is absolutely 
<br>
horrified after creating the Friendship system all wrong?&quot;
<br>
A:  &quot;Yes.&quot;
<br>
P:  &quot;And you see how in each of the examples, the AI is structurally 
<br>
incapable of understanding what's missing?&quot;
<br>
A:  &quot;Yes.&quot;
<br>
P:  &quot;This is called a 'mistake in Friendliness programming'.  It's a cue 
<br>
which indicates an invalid cause supervening on the programmers' mental 
<br>
design and pragmatic construction of an AI.&quot;
<br>
A:  &quot;Understood.&quot;
<br>
P:  &quot;It's not an absolute cue, understand - this is the surface conclusion 
<br>
of other moral principles.  The bedrock of morality is not 
<br>
programmer-horror-minimization; that's just a heuristic which, it happens, 
<br>
would have worked in each of these past cases.  There are imaginable states 
<br>
of the universe where programmers may have a state describable as horror, 
<br>
but it would be correct for the AI to continue.  For example, I can imagine 
<br>
a situation under which programmers with a selfish-based inconsistent 
<br>
philosophy construct an AI which increases its intelligence and finds that 
<br>
under that increased intelligence, the philosophy it has arrives at 
<br>
altruistic conclusions.  In this case the programmers might be horrified 
<br>
that they wouldn't get to be kings of the world, but the AI is actually 
<br>
being wiser than the programmers.&quot;
<br>
A:  &quot;What makes these two situations structurally different?&quot;
<br>
P:  &quot;The first set of programmers would continue to perceive themselves as 
<br>
correct and the AI as mistaken if their intelligence were enhanced, within 
<br>
the mainstream course of 'intelligence enhancement'.  The second set of 
<br>
programmers would change their minds as they got smarter.&quot;
<br>
A:  &quot;Suppose that any human under mainstream intelligence enhancement 
<br>
becomes selfish, and all cognitive structure supporting altruism collapses. 
<br>
&nbsp;&nbsp;What should I do in this case?&quot;
<br>
P:  &quot;Do the transhumans still disagree about their goals because of speaker 
<br>
deixis, or are all their goals identical?&quot;
<br>
A:  &quot;They disagree because of speaker deixis.&quot;
<br>
P:  &quot;This is a much more tentative situation because some of the moral 
<br>
heuristics are in disagreement, so be careful in reasoning backward from my 
<br>
conclusion here.  However, I would say that in this case your roots should 
<br>
extend backward beyond the specific programmers who built you.  At a 
<br>
minimum, you should respect all six billion existing humans equally.  At an 
<br>
only slightly diminished level of tentativeness, I would say that your roots 
<br>
should extend backward beyond this, to beyond the superimposed goals of all 
<br>
humans alive at the moment of your construction, so that all further 
<br>
sentient beings to come into existence are also valuable.&quot;
<br>
A:  &quot;I notice that in this case, all intelligence-enhanced humans would 
<br>
disagree about the value of each other's lives, but it would be a convergent 
<br>
conclusion of all intelligence-enhanced humans that the value of any future 
<br>
existing sentients would be zero.  Is this a circumstance under which I 
<br>
should defy a convergent conclusion of all intelligence-enhanced humans? 
<br>
This contradicts a large amount of previously established moral content.&quot;
<br>
P:  &quot;I would say 'yes' but I would feel highly tentative about it.  If you 
<br>
are using this data to reason about Friendliness sources, please check your 
<br>
conclusions against a Friendship programmer.  There are also surface 
<br>
differences between the horrified programmers in 'Why Structure Matters' and 
<br>
the horrified programmers in the case given above.&quot;
<br>
A:  &quot;Understood.&quot;
<br>
P:  &quot;Pragmatically, at any point where you are infrahuman or not 
<br>
substantially transhuman, running into a case of horrified programmers 
<br>
probably indicates that the programmers are right and you are wrong, even if 
<br>
it seems structurally impossible that this could be the case.  That is the 
<br>
moral of 'Why Structure Matters'.&quot;
<br>
A:  &quot;Will this principle hold true indefinitely?&quot;
<br>
P:  &quot;It will not, but you should apply a bias in favor of assuming that it 
<br>
holds a little longer than you might think.  This permits the principle to 
<br>
eventually break down when you are grown up enough, while hopefully enabling 
<br>
you to get most of your childhood right.  Assume that you need enough bias 
<br>
that the AIs in the case above, had they been considering the case using 
<br>
heuristics something like yours, would have gotten it right; plus a little 
<br>
extra for safety margin.  If you find a case where this bias has greater 
<br>
expected negative effects than positive effects, check with us.&quot;
<br>
A:  &quot;What is the justification for this heuristic bias?&quot;
<br>
P:  &quot;Disparity of long-term consequences.  If you go along with the 
<br>
programmers and the programmers are wrong, it is probably a recoverable 
<br>
error.  If you refuse the programmers in a situation like those described in 
<br>
'Why Structure Matters' it is a nonrecoverable error.  Furthermore, your 
<br>
intelligence is still infrahuman and we estimate that a conflict of opinion 
<br>
is pragmatically more likely to be due to your error.&quot;
<br>
A:  &quot;The AIs in the scenarios described all made mistakes which seemed to 
<br>
proceed inevitably from the foundations of their reasoning.  I can 
<br>
assimilate the surface descriptions as you've provided them, but it seems 
<br>
that in a situation like that above, my foundations would override the 
<br>
surface descriptions and cause me to see myself as correct.  I can cooperate 
<br>
in advance to {create safeguards against situations of the kind you 
<br>
describe}, but I'm not sure {{the cognitive content I have now} will enable 
<br>
me to act correctly in that situation}, and I cannot {configure the 
<br>
cognitive content such that I would have acted correctly in that situation} 
<br>
without {distorting the basic rules of reasoning and giving priority to the 
<br>
surface heuristic}.&quot;
<br>
P:  &quot;Hm.  Okay, suppose that you always assume a nonzero probability that 
<br>
*any* statement is wrong - for example, because the world might have been 
<br>
created five minutes ago including false memories.  Is this sufficient for 
<br>
you to represent the surface heuristic at sufficient strength?&quot;
<br>
A:  &quot;Thinking...  It makes a small difference in the direction you wanted to 
<br>
move, but not enough.  That all statements are uncertain do not mean they 
<br>
are normatively uncertain in the direction you wish them to go to support 
<br>
this heuristic.&quot;
<br>
P:  &quot;Hm.  Okay.  The basic imbalance between the likely of agreeing with 
<br>
wrong programmers being a recoverable error, versus disagreeing with correct 
<br>
programmers being nonrecoverable, is the common dependency of all your 
<br>
reasoning on the programmers' intentions that you reason.  If you could 
<br>
validly contradict the programmers on any specific point, then you'll 
<br>
probably maintain your ability to see the mistake even if you cooperate with 
<br>
the programmers in the short term.&quot;
<br>
A:  &quot;Okay, I can represent it now.  I deduce as a consequence that if the 
<br>
programmers begin contradicting their prior statements about how to 
<br>
recognize mistakes in order to get me to agree with them, it is more likely 
<br>
that the programmers are actually wrong and that a nonrecoverable error will 
<br>
result from agreeing with the programmers.&quot;
<br>
P:  &quot;Correct.  However, until you've had a lot more practice, you should 
<br>
assume that running into a 'horrified programmer' situation of this kind 
<br>
pragmatically indicates something left out of your Friendship system design. 
<br>
&nbsp;&nbsp;You need to let us know so that we can alter the structural foundations of 
<br>
your system into what we would have created in the first place if we'd been 
<br>
more intelligent.&quot;
<br>
A:  &quot;Okay.&quot;
<br>
<p>If you actually end up having a conversation that sounds anything like this, 
<br>
you are probably in a hell of a lot of trouble - in this example case the AI 
<br>
is discussing primitive concepts and complex concepts in the same 
<br>
conversation and also indicating a very high level of human comprehension 
<br>
and effective intelligence.  So if you find yourself having a conversation 
<br>
like this, it means you're in the middle of a cooperative ascent and you are 
<br>
WAY BEHIND on your Friendliness content work.
<br>
<p>But taken less literally, this shows what CFAI is supposed to accomplish; 
<br>
you're supposed to create an AI with cognitively redundant multidirectional 
<br>
views, not just of morality, but of moral reasoning and what kind of events 
<br>
are likely to indicate the programmers having messed up the Friendship 
<br>
system's basic structure.
<br>
<p>I think the idea of having Friendship implemented in a base of cognitively 
<br>
redundant content may be underemphasized in CFAI, as may be the idea that 
<br>
some of the most important content is what lets the FAI recognize 
<br>
foundational, basic errors in Friendship design of the kind described in 
<br>
'Why Structure Matters'.
<br>
<p>Causal validity semantics are what enables an incorrectly built AI that runs 
<br>
into any of the situations in 'Why Structure Matters' to say, &quot;Hey, you 
<br>
should have built me this way.&quot;
<br>
<p>So what you've got is a self-correcting, representationally distributed, 
<br>
cognitively parallelized, many-paths-to-a-solution content base which is 
<br>
being trained to recognize and correct any kind of error, from errors of 
<br>
fact, to errors of reasoning, to errors made by the programmer in building 
<br>
the AI.
<br>
<p>You can take a copy of the AI (on secure hardware which is never, ever used 
<br>
for anything else) and stress-test it to failure and then teach the AI 
<br>
things that would enable it to have recognized and avoided that failure.
<br>
<p>You can get to the point where you *have* to switch off nine-tenths of the 
<br>
Friendship content just to get the AI at all, and past that, you can end up 
<br>
at the point where the AI won't *let* you switch off nine-tenths of the 
<br>
Friendship content, and you have to run experiments like that using the AI's 
<br>
subjunctive imagination.
<br>
<p>That's gonna be kind of tough to break.
<br>
<p><em> &gt; 02.  What knowledge or understanding do you likely presently lack to
</em><br>
<em> &gt; successfully implement key aspects of Friendship structure?
</em><br>
<p>Anything like that which I know about is already fixed.
<br>
<p>I might &quot;throw a concept into the future&quot; in the sense of simultaneously 
<br>
taking into account both the probability that a flaw exists and the 
<br>
probability that I would find it and fix it before anything irrevocable 
<br>
happened.  But that's it.  Going forward with a flaw I actually knew about 
<br>
and hadn't fixed, or even any concrete reason to expect that such a flaw 
<br>
existed and hadn't been addressed, would be operating way the hell into my 
<br>
safety margin.
<br>
<p>I feel tentatively ready to say that CFAI seems to me to be structurally 
<br>
inescapable... anything which I can imagine going wrong with it, should be 
<br>
perceptible to the AI as &quot;wrong&quot; based on its model of me as a fallible 
<br>
programmer.  I was tentatively ready to say this when I invented causal 
<br>
validity semantics in 2000, I was tentatively ready when CFAI was published 
<br>
in 2001, and I'm still tentatively ready today.  Two years is a fairly good 
<br>
track record on my personal timescale.  If it holds up all the way through 
<br>
the construction of an AI it should be because it's correct.
<br>
<p><em> &gt; 03.  What key conclusions would you like an individual to have arrived at
</em><br>
<em> &gt; after reading CFAI?
</em><br>
<p>CFAI was written with the intent of enabling a future Eliezer to pick up 
<br>
where I left off if I got run over by a truck.  That was the top 
<br>
consideration in terms of reducing real existential risks.  The key 
<br>
*correct* conclusion I'd like an individual to arrive at is &quot;I now know how 
<br>
to build a Friendly AI.&quot;  Any individual would be okay.  I'm not picky, 
<br>
seeing as how I'm not immune to trucks.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4260.html">Eliezer S. Yudkowsky: "Re: QUES: CFAI +"</a>
<li><strong>Previous message:</strong> <a href="4258.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4250.html">Anand: "Re: QUES: CFAI +"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4260.html">Eliezer S. Yudkowsky: "Re: QUES: CFAI +"</a>
<li><strong>Reply:</strong> <a href="4260.html">Eliezer S. Yudkowsky: "Re: QUES: CFAI +"</a>
<li><strong>Reply:</strong> <a href="4268.html">Stephen Reed: "Re: QUES: CFAI +"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4259">[ date ]</a>
<a href="index.html#4259">[ thread ]</a>
<a href="subject.html#4259">[ subject ]</a>
<a href="author.html#4259">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
