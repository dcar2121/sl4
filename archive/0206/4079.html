<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Threats to the Singularity.</title>
<meta name="Author" content="Gordon Worley (redbird@rbisland.cx)">
<meta name="Subject" content="Re: Threats to the Singularity.">
<meta name="Date" content="2002-06-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Threats to the Singularity.</h1>
<!-- received="Tue Jun 18 00:31:45 2002" -->
<!-- isoreceived="20020618063145" -->
<!-- sent="Tue, 18 Jun 2002 00:11:16 -0400" -->
<!-- isosent="20020618041116" -->
<!-- name="Gordon Worley" -->
<!-- email="redbird@rbisland.cx" -->
<!-- subject="Re: Threats to the Singularity." -->
<!-- id="6FDBEFA6-8271-11D6-94B0-000A27B4DEFC@rbisland.cx" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3D0EA0C9.4080904@objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Gordon Worley (<a href="mailto:redbird@rbisland.cx?Subject=Re:%20Threats%20to%20the%20Singularity."><em>redbird@rbisland.cx</em></a>)<br>
<strong>Date:</strong> Mon Jun 17 2002 - 22:11:16 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4080.html">Fabio Mascarenhas: "simulating human organs"</a>
<li><strong>Previous message:</strong> <a href="4078.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4078.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4121.html">Ben Goertzel: "RE: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4079">[ date ]</a>
<a href="index.html#4079">[ thread ]</a>
<a href="subject.html#4079">[ subject ]</a>
<a href="author.html#4079">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Monday, June 17, 2002, at 10:54  PM, Samantha Atkins wrote:
<br>
<p><em>&gt; Gordon Worley wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; On Monday, June 17, 2002, at 06:51  PM, Samantha Atkins wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; My core looks something like this.  I want to make the universe a 
</em><br>
<em>&gt;&gt; better place.  A better place to live.  A place that solves new, 
</em><br>
<em>&gt;&gt; interesting problems.  A place that I'd like to stay, but wouldn't 
</em><br>
<em>&gt;&gt; want to visit.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Thanks for offering it.  Now, if we could just get a bit more handle on 
</em><br>
<em>&gt; what you/we mean by &quot;hetter&quot; we could move right along.
</em><br>
<p>Well, this is the part that's fuzzy for me.  I have some idea of what 
<br>
would be better, but it's not all that clear.  I can't think of too many 
<br>
specific things that I could write down.  For most of us, I think that 
<br>
the `better' metric arises from the things we enjoy.  So long as those 
<br>
programming the Seed AI aren't serial killers or the like, everything 
<br>
should turn out Friendly.
<br>
<p><em>&gt;&gt; I'd like for this to include me in it, but if it turns out that the 
</em><br>
<em>&gt;&gt; universe can't be better so long as I'm still in it, then I'll get 
</em><br>
<em>&gt;&gt; out.  I think the &quot;yuck factor&quot; in this is that I think the same way 
</em><br>
<em>&gt;&gt; about everything.  If you're making the universe a worse place, I 
</em><br>
<em>&gt;&gt; don't really want you in it.  I hope that getting &quot;you&quot; out of it only 
</em><br>
<em>&gt;&gt; involves convincing you not to do whatever it is that is making the 
</em><br>
<em>&gt;&gt; universe worse.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Doesn't this assume that you and perhaps other entities are relatively 
</em><br>
<em>&gt; immutable over time no matter what your wishes?  I don't consider that 
</em><br>
<em>&gt; a particularly tenable notion assuming an ability to upload and/or 
</em><br>
<em>&gt; continuously augment, self-examine and change.  I would also challenge 
</em><br>
<em>&gt; any being to conclusively prove another being both of more harm than 
</em><br>
<em>&gt; good to the universe AND utterly incapable of ever changing.
</em><br>
<p>If one can change, that's the preferable solution.  If, however, a being 
<br>
refuses to be fixed, an SI would need to do something with ver (which 
<br>
may include killing ver, if ve refuse to be put in a VR).
<br>
<p><em>&gt;
</em><br>
<em>&gt;&gt; I think this seems yucky because this sounds just like the kind of 
</em><br>
<em>&gt;&gt; thing Hitler would say.  The difference is that I have compassion for 
</em><br>
<em>&gt;&gt; all life.  I want to see the universe better and would like for that 
</em><br>
<em>&gt;&gt; to include everyone and everything in it.  However, if all attempts at 
</em><br>
<em>&gt;&gt; this proves impossible, I'm not going to say &quot;well, okay, I guess the 
</em><br>
<em>&gt;&gt; universe is just going to suck&quot;, but &quot;okay, let's see what the 
</em><br>
<em>&gt;&gt; limiting factors are and what we have to do to get around them&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't believe in an &quot;improved universe&quot; by eradicating sentients that 
</em><br>
<em>&gt; seem problematic except in very very limited circumstances where an 
</em><br>
<em>&gt; entity is capable of such destruction not stoppable otherwise as to 
</em><br>
<em>&gt; force the decision.  I don't consider not being as productive as somme 
</em><br>
<em>&gt; might like to be a reasonable criteria.  I don't consider not being as 
</em><br>
<em>&gt; rational or intelligent such as criteria either.  If your goal is 
</em><br>
<em>&gt; maximizing life then I don't think you see these as criteria for 
</em><br>
<em>&gt; extermination either.
</em><br>
<p>Extermination is hardly something that I plan on anyone doing everyday, 
<br>
if ever.  Getting back to the original post, the point is not whether 
<br>
you'll actually execute someone, but whether you are unattached enough 
<br>
to see it as a possibility that you may have to act on.
<br>
<p><em>&gt;&gt;&gt;&gt;&gt;&gt; Some of us, myself included, see the creation of SI as important 
</em><br>
<em>&gt;&gt;&gt;&gt;&gt;&gt; enough to be more important than humanity's continuation.  Human 
</em><br>
<em>&gt;&gt;&gt;&gt;&gt;&gt; beings, being
</em><br>
<em>&gt;&gt;&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;&gt;&gt; How do you come to this conclusion?  What makes the SI worth more 
</em><br>
<em>&gt;&gt;&gt;&gt;&gt; than all of humanity?  That it can outperform them on some types of 
</em><br>
<em>&gt;&gt;&gt;&gt;&gt; computation?  Is computational complexity and speed the sole 
</em><br>
<em>&gt;&gt;&gt;&gt;&gt; measure of whether sentient beings have the right to continued 
</em><br>
<em>&gt;&gt;&gt;&gt;&gt; existence?  Can you really give a moral justification or a rational 
</em><br>
<em>&gt;&gt;&gt;&gt;&gt; one for this?
</em><br>
<em>&gt;&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;&gt; In many ways, humans are just over the threshold of intelligence.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; Whose threshold?  By what standards?  Established and verified as the 
</em><br>
<em>&gt;&gt;&gt; standards of value how?
</em><br>
<em>&gt;&gt; You're asking for a definition of intelligence.  Though question!
</em><br>
<em>&gt;
</em><br>
<em>&gt; A tougher one is why intelligence is on top of your value stack as the 
</em><br>
<em>&gt; measure of the worthiness of various beings to exist and as the 
</em><br>
<em>&gt; principle measure of &quot;better&quot;.
</em><br>
<p>Oops, I went on to answer the immediate question while ignoring the 
<br>
question that spawned the last question.  A wiser being would be better 
<br>
at making the universe better.
<br>
<p><em>&gt;&gt;  At any level of intelligence, though, all the problems at the limits 
</em><br>
<em>&gt;&gt; of solvability look interesting.  As great as we think we are, we can 
</em><br>
<em>&gt;&gt; already see that there are some interesting problems out there that we 
</em><br>
<em>&gt;&gt; can't find solutions to (like the halting problem).
</em><br>
<em>&gt;
</em><br>
<em>&gt; The halting problem is provably unsolvable by any and all levels of 
</em><br>
<em>&gt; intelligence.
</em><br>
<p>Well, that is under our theories of mathematics.  Using different 
<br>
theories, it may turn out to be solvable (and then we'll wonder how we 
<br>
missed the answer all those years), but those theories may simple be 
<br>
beyond what the human mind can comprehend.  ;-)
<br>
<p><em>&gt;&gt; And, unless it turns out that intelligence doesn't scale very well, 
</em><br>
<em>&gt;&gt; the trend tells us that even more interesting questions are out there 
</em><br>
<em>&gt;&gt; for more intelligent minds to solve.  I doubt that anything will ever 
</em><br>
<em>&gt;&gt; be so intelligent that it will be able to solve every problem.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Sure, but so?  Is this all there is?  Is it criteria enough for what is 
</em><br>
<em>&gt; and is not of value?
</em><br>
<p>Getting back to the original question that spawned this subthread, an SI 
<br>
should be more capable at making the universe better than a human.  So 
<br>
long as the SI is wiser than humans, it has more `right' than some 
<br>
humans to use some amount of matter.
<br>
<p><em>&gt;&gt; One should be humble, but not negative.  Being negative is just as 
</em><br>
<em>&gt;&gt; irrational as flattery.
</em><br>
<em>&gt;&gt; Much has humans get to clear away ants if they're keeping the universe 
</em><br>
<em>&gt;&gt; from getting better, an SI could clear away some humans if they got in 
</em><br>
<em>&gt;&gt; the way.  If the SI is compassionate, ve will see that the humans are 
</em><br>
<em>&gt;&gt; doing some good and, being self aware, are able to change themselves 
</em><br>
<em>&gt;&gt; to do more good.  Unlike the humans who is unable to solve the ant 
</em><br>
<em>&gt;&gt; problem by any means other than getting the ants out of the way (be 
</em><br>
<em>&gt;&gt; that killing them or displacing them), an SI can solve the human 
</em><br>
<em>&gt;&gt; problem by helping the humans.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ants, while they may be inconvenient at a picnic or marching across the 
</em><br>
<em>&gt; kitchen, are not in the way fo the universe getting better.  People are 
</em><br>
<em>&gt; likely to be even less so.  I agree of course that there are much 
</em><br>
<em>&gt; better solutions in the case of humans than extermination.
</em><br>
<p>You never know, those ants might be preventing Ben or Eliezer from 
<br>
getting Seed AI programming done.  Besides, ants are just an example; 
<br>
insert any kind of obstruction you want here that is clearly lesser than 
<br>
human level intelligence.
<br>
<p><em>&gt; From your earlier post, at what point would you not battle for an SI in 
</em><br>
<em>&gt; the process of being born?  Suppose you had super weapons capable of 
</em><br>
<em>&gt; laying waste to entire nations of opposition.  Would you use them?
</em><br>
<p>This all depends on the situation.  In most cases the project would be 
<br>
better of cutting its losses and trying again, since killing lots of 
<br>
people tends to get other people even more upset.  I would not use the 
<br>
weapons unless extermination was really, really, really the only option.
<br>
<p><em>&gt;&gt; If some humans prove to be beyond help, though, I don't think it's 
</em><br>
<em>&gt;&gt; totally wrong to clear them out in some way.  Maybe that just means 
</em><br>
<em>&gt;&gt; letting them live in a simulation where they can kill their virtual 
</em><br>
<em>&gt;&gt; selves.  I'll leave the solution up to a much more intelligent SI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Define &quot;beyond help&quot;.  I would support popping the ones that were too 
</em><br>
<em>&gt; great a danger to self and others into a safety zone of some kind (VR 
</em><br>
<em>&gt; or otherwise) until they learn better and/or can be cured in a way they 
</em><br>
<em>&gt; are willing to undergo.  I don't think we should leave it up to the 
</em><br>
<em>&gt; not-yet-existent SI now though.  It is these kinds of questions and the 
</em><br>
<em>&gt; answer to them that will make the difference in the level of support 
</em><br>
<em>&gt; and vilification.
</em><br>
<p>Yeah, I guess this is the kind of thing that people want answers to.
<br>
<p>Well, VR is a good choice.  But, what if an Evil SI surfaces.  In that 
<br>
case it might very well be a better choice to kill it rather than try to 
<br>
fix it.  I don't think we know enough about what kinds of situations 
<br>
will arise to know what would be good options.
<br>
<p><em>&gt;&gt;&gt;&gt; But, it's not nearly so simple.  All of us would probably agree that 
</em><br>
<em>&gt;&gt;&gt;&gt; given the choice between saving one of two lives, we would choose to 
</em><br>
<em>&gt;&gt;&gt;&gt; save the person who is most important to the completion of our 
</em><br>
<em>&gt;&gt;&gt;&gt; goals, be that reproduction, having fun, or creating the 
</em><br>
<em>&gt;&gt;&gt;&gt; Singularity.  In the same light, if a mob is about to come in to 
</em><br>
<em>&gt;&gt;&gt;&gt; destroy the SI just before it takes off and there is no way to stop 
</em><br>
<em>&gt;&gt;&gt;&gt; them other than killing them, you have on one hand the life of the 
</em><br>
<em>&gt;&gt;&gt;&gt; SI that is already more intelligent than the members of the mob and 
</em><br>
<em>&gt;&gt;&gt;&gt; will continue to get more intelligent, and on the other the life of 
</em><br>
<em>&gt;&gt;&gt;&gt; 100 or so humans.  Given such a choice, I pick the SI.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; But that is not the context of the question.  The context is whether 
</em><br>
<em>&gt;&gt;&gt; the increased well-being and possibilities of existing sentients, 
</em><br>
<em>&gt;&gt;&gt; regardless of their relative current intelligence, is  a high and 
</em><br>
<em>&gt;&gt;&gt; central value.  If it is not then I hardly see how such an SI can be 
</em><br>
<em>&gt;&gt;&gt; described as &quot;Friendly&quot;.
</em><br>
<em>&gt;&gt; To a Friendly intelligence, this is important.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; OK.  And that is what you wish to build, right?  :-)
</em><br>
<p>Yes.
<br>
<p><pre>
--
Gordon Worley                     `When I use a word,' Humpty Dumpty
<a href="http://www.rbisland.cx/">http://www.rbisland.cx/</a>            said, `it means just what I choose
<a href="mailto:redbird@rbisland.cx?Subject=Re:%20Threats%20to%20the%20Singularity.">redbird@rbisland.cx</a>                it to mean--neither more nor less.'
PGP:  0xBBD3B003                                  --Lewis Carroll
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4080.html">Fabio Mascarenhas: "simulating human organs"</a>
<li><strong>Previous message:</strong> <a href="4078.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4078.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4121.html">Ben Goertzel: "RE: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4079">[ date ]</a>
<a href="index.html#4079">[ thread ]</a>
<a href="subject.html#4079">[ subject ]</a>
<a href="author.html#4079">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
