<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Friendliness as an Approximation</title>
<meta name="Author" content="Michael Anissimov (altima@yifan.net)">
<meta name="Subject" content="Friendliness as an Approximation">
<meta name="Date" content="2002-06-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Friendliness as an Approximation</h1>
<!-- received="Mon Jun 17 06:16:24 2002" -->
<!-- isoreceived="20020617121624" -->
<!-- sent="Mon, 17 Jun 2002 05:46:52 -0400" -->
<!-- isosent="20020617094652" -->
<!-- name="Michael Anissimov" -->
<!-- email="altima@yifan.net" -->
<!-- subject="Friendliness as an Approximation" -->
<!-- id="3d0db00c.12e8.0@yifan.net" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Anissimov (<a href="mailto:altima@yifan.net?Subject=Re:%20Friendliness%20as%20an%20Approximation"><em>altima@yifan.net</em></a>)<br>
<strong>Date:</strong> Mon Jun 17 2002 - 03:46:52 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4061.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4059.html">Samantha Atkins: "Re: On wisdom"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4060">[ date ]</a>
<a href="index.html#4060">[ thread ]</a>
<a href="subject.html#4060">[ subject ]</a>
<a href="author.html#4060">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Anand wrote:
<br>
<p><em>      &gt;Eugen Leitl:
</em><br>
<em>&gt;
</em><br>
<em>&gt;Please consider summarizing, or referencing, your objections to non-
</em><br>
brute
<br>
<em>&gt;force seed AI development and SIAI's theoretical work on 
</em><br>
Friendliness.  In
<br>
<em>&gt;response, I would ask that Ben, Eliezer, Peter, and others, to consider
</em><br>
<em>&gt;providing refutations, or referencing specific refutations, to Eugen's
</em><br>
<em>&gt;objections. This information would assist some of my activities, and
</em><br>
<em>&gt;possibly the activity of others.
</em><br>
<p>Yes, this would greatly assist me in my activities as well, and I thank 
<br>
you, Anand, for this inquiry, as well as your inquiries on CFAI - 
<br>
Eliezer's answers were very complicated, but exceedingly crisp and 
<br>
explicit, and are answers to philosophical questions which many people 
<br>
are unfortunately stuck on while trying to think about Friendly AI as a 
<br>
concept.
<br>
<p>It's good to see you leaning towards incorporating extravolitional 
<br>
variables into your moral model, Eliezer - I've always seen the pure 
<br>
volition model as too absolute and simple to satisfy some people, 
<br>
including myself before I understood that the moral-model trajectory of 
<br>
an AI was not so much about the initial content but the initial 
<br>
architecture and semantics.  In any case, while it is still compatible 
<br>
with Friendliness as a philosophy for transferring moral complexity to 
<br>
an AI, a purely volitional model can unintentionally create a stumbling 
<br>
block while thinking about seed AI.
<br>
<p>It's also good to see Eliezer and Gordon arguing for an 
<br>
unanthropocentric ethical system - you would guess its the logical 
<br>
thing attitude to adopt, but I suppose it's easier than I think to get 
<br>
attached to humanity emotionally and make unfairly species-centric 
<br>
moral decisions, just like how it's easy, in a sufficiently undeveloped 
<br>
memetic environment, to get attached to a specific racial group 
<br>
emotionally and make race-centric moral decisions as well.  But since 
<br>
the Singularity will not necessarily effect only homonid sentients, but 
<br>
possibly all of sentientkind existing right before the Singularity, for 
<br>
all eternity, the moral model I tend to visualize being pertinent to 
<br>
Humanity's Final Invention does not favor any particular sentient 
<br>
species over any other.
<br>
<p>And as another example of what Friendliness is supposed to be:
<br>
Today, a poster on BJKlein.com remarked that Eliezer's writings 
<br>
were &quot;almost perfect definitions of objective morality...but they 
<br>
neglected how to treat animals&quot;.  Obviously, this person is missing the 
<br>
point - it doesn't matter if Eliezer doesn't mention how to treat 
<br>
animals in his writings, because he isn't trying to code a self-
<br>
improving robocop static morality AI, he's trying to code a Friendly 
<br>
seed AI.  The latter has the ability and desire to change and *improve* 
<br>
ver model of morality like any idealized moral arbiter would, the 
<br>
former is an obsolete Asimovian construct.  Personally, I dislike the 
<br>
idea of murdering any organism with a nervous system for food - I know 
<br>
Eliezer doesn't, and in the first month of hearing about Friendly AI 
<br>
and a little bit of the theory, I had a major problem with this, 
<br>
thinking he would &quot;tell the AI that killing animals is ok&quot;.  But then I 
<br>
read CFAI, and realized it didn't really matter - Eliezer is coding an 
<br>
AI for *sentience*, not solely for humanity (although humanity will 
<br>
likely represent all sentience at the advent of the Singularity), and 
<br>
certainly not for any race, person, or philosophy.  So why worry?  But 
<br>
in any case, non-Singularitarians often judge Singularitarians by their 
<br>
professed moral codes when estimating the validity of their theory, 
<br>
when considering whether additional investigation would be worthwhile.  
<br>
For this reason, it might be smart for Singularitarians to do what they 
<br>
are already often doing - set a moral ideal, strive for that ideal, 
<br>
while continuously making the point that Friendly AI is not about any 
<br>
specific set of moral content, but a morality-generating, self-
<br>
enhancing architecture that initially starts with a seed of observer-
<br>
independent volition-respecting altruism.
<br>
<p>A few years back back I had a moral/philosophical crisis - what to do 
<br>
if, for every casual, everyday motion of mine - taking a step, for 
<br>
example, corresponded to, and resulted in, an immense amount of 
<br>
suffering or pain for some large set of sentient beings in an parallel 
<br>
world?  If we're in a simulation, it *could*, *maybe* be wired that 
<br>
way.  But how could we know?  If we exited the simulation.  The lesson 
<br>
I learned from this, beyond realizing that pursuing the Singularity is 
<br>
the direct pursuit of higher ethics and morals, is that it is 
<br>
*impossible* to define any fixed point in morality without *infinite* 
<br>
intelligence - presumably impossible, because if the moral arbiter 
<br>
gained even a little bit of extra intelligence, ver whole moral system 
<br>
could be entirely overthrown!  All you can do is create an autonomous 
<br>
mind, that, like human beings, would be able to navigate the 
<br>
hyperdimensional hypothetical space of all possible moralities, seeking 
<br>
out the moral and philosophical ideals for all people, given enough 
<br>
technology to consensually implement them.
<br>
<p>Michael Anissimov
<br>
<p>-----------------------------------------------------
<br>
<a href="http://eo.yifan.net">http://eo.yifan.net</a>
<br>
Free POP3/Web Email, File Manager, Calendar and Address Book
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4061.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4059.html">Samantha Atkins: "Re: On wisdom"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4060">[ date ]</a>
<a href="index.html#4060">[ thread ]</a>
<a href="subject.html#4060">[ subject ]</a>
<a href="author.html#4060">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
