<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Controlled ascent (was: Military Friendly AI)</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Controlled ascent (was: Military Friendly AI)">
<meta name="Date" content="2002-06-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Controlled ascent (was: Military Friendly AI)</h1>
<!-- received="Sat Jun 29 09:13:12 2002" -->
<!-- isoreceived="20020629151312" -->
<!-- sent="Sat, 29 Jun 2002 08:43:43 -0400" -->
<!-- isosent="20020629124343" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Controlled ascent (was: Military Friendly AI)" -->
<!-- id="3D1DAB7F.4050008@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="4.3.2.7.2.20020628195220.01ce0828@mail.earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Controlled%20ascent%20(was:%20Military%20Friendly%20AI)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Jun 29 2002 - 06:43:43 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4545.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Previous message:</strong> <a href="4543.html">Mark Walker: "Re: Friendly Existential Wager"</a>
<li><strong>In reply to:</strong> <a href="4529.html">James Higgins: "RE: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4549.html">Ben Goertzel: "RE: Controlled ascent (was: Military Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="4549.html">Ben Goertzel: "RE: Controlled ascent (was: Military Friendly AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4544">[ date ]</a>
<a href="index.html#4544">[ thread ]</a>
<a href="subject.html#4544">[ subject ]</a>
<a href="author.html#4544">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
James Higgins wrote:
<br>
<em> &gt; At 06:32 PM 6/28/2002 -0600, Ben Goertzel wrote:
</em><br>
<em> &gt;
</em><br>
<em> &gt;&gt; Novamente does not yet have a goal system at all, this will be
</em><br>
<em> &gt;&gt; implemented, at my best guess, perhaps at the very end of 2002 or the
</em><br>
<em> &gt;&gt; start of 2003. Currently we are just testing various cognitive and
</em><br>
<em> &gt;&gt; perceptual mechanisms, and not yet experimenting with autonomous
</em><br>
<em> &gt;&gt; goal-directed behavior.
</em><br>
<em> &gt;
</em><br>
<em> &gt; Boy, you really don't have much chance of a hard takeoff yet.
</em><br>
<p>As you know, I think the threshold for hard takeoff is higher than Ben does, 
<br>
and that the approach embodied in the Novamente manuscript I read will never 
<br>
get there.  So what?  I could be wrong on both counts.  Any system with 
<br>
Turing-complete patterns optimizing themselves (that includes Eurisko) 
<br>
should have a controlled ascent mechanism.  You shouldn't stop and try to 
<br>
argue with yourself about whether a controlled ascent mechanism is needed; 
<br>
you should just *do* it.  Basic errors in your visualization of how and why 
<br>
a hard takeoff occurs is *exactly* what a controlled ascent mechanism is 
<br>
intended to guard against, so arguing about whether a given system has a 
<br>
significant chance of a hard takeoff misses the entire point.  The point 
<br>
where you think a system has a real, pragmatic chance of a hard takeoff is 
<br>
the point where a full Friendship design should be implemented, not the 
<br>
point where you first add on a controlled ascent mechanism.
<br>
<p><em> &gt;&gt; A failsafe mechanism has two parts
</em><br>
<p>Calling this a &quot;failsafe&quot; mechanism begs the question, because it most 
<br>
certainly isn't.  It just buys you a better chance.  That's all.
<br>
<p><em> &gt;&gt; 1) a basic mechanism for halting the system and alerting appropriate
</em><br>
<em> &gt;&gt; people when a &quot;rapid rate of intelligence increase&quot; is noted
</em><br>
<p>Personally, I would recommend that the system should pause when a certain 
<br>
number of self-improvements go past unobserved.  If that many improvements 
<br>
mount up while the programmers take a two-week vacation, or over five 
<br>
minutes, it shouldn't make a difference - what matters is that the 
<br>
programmers don't see it.  For whatever metric you use, measure how much 
<br>
improvement occurs in an average day, or on a good day for that matter.  At 
<br>
the end of each day, have a programmer hit a sense switch that validates all 
<br>
improvements up to one half-hour ago.  If, say, 30 days worth of 
<br>
self-improvement go past before the next hitting of the sense switch, the 
<br>
system pauses and sends out an email, or shuts down (preserving state) if 
<br>
somehow the activity continues.  (Incidentally, the &quot;send an email&quot; part 
<br>
only works if your system is connected to the Internet, but I understand 
<br>
that's your plan.)
<br>
<p><em> &gt;&gt; 2) a mechanism for detecting a rapid rate of intelligence increase
</em><br>
<p>You can't have Turing-complete patterns optimizing themselves without a 
<br>
definition of optimization.  Whatever criterion is being used to separate 
<br>
good patterns from bad patterns, use that criterion as the metric of 
<br>
intelligence.  If the patterns make improvements that they themselves can't 
<br>
detect, the improvements are likely to be discarded.  Unless the system 
<br>
mechanism or particular patterns that detect &quot;good tweaks&quot; have some kind of 
<br>
power granted by the overall system - the power to make more tweaks than 
<br>
other patterns, or to have their tweaks granted a greater chance of survival 
<br>
- it seems very unlikely that even a mathematical chance of a hard takeoff 
<br>
could exist, unless one of the heuristics simultaneously learns how to make 
<br>
intelligence improvements and cheat the system to enforce them &quot;outside the 
<br>
box&quot;, which is unlikely, especially given the amount of time you are likely 
<br>
to spend preventing cheating in any case.  An exception to this rule would 
<br>
be a sufficiently vast Internet pool of fractal self-modifying codelets, 
<br>
with no rules except Core Wars; in this case intelligence may simultaneously 
<br>
be the means of making improvements and the means of enforcing them.  That's 
<br>
why it would be very hard to create a controlled ascent mechanism for 
<br>
certain brute-force ways of searching for general intelligence.
<br>
<p>Anyway, the upshot is that even the mathematical possibility of a hard 
<br>
takeoff - I am not talking about &quot;significant probabilities&quot; but 
<br>
&quot;theoretical mathematical possibility&quot; - is directly related to the system's 
<br>
ability to measure goodness.  So you plug in whatever measure of goodness 
<br>
would be used to *feed* a hard takeoff, in the first place, into the 
<br>
controlled ascent mechanism.
<br>
<p>You also, when the time comes, measure a bunch of other correlates of 
<br>
intelligence.  This doesn't guarantee catching a hard takeoff that occurs 
<br>
&quot;outside the box&quot; but it gives you a much higher chance of doing so, as long 
<br>
as the takeoff &quot;outside the box&quot; still has any effect at all on the 
<br>
cognitive systems you know (which it might not).
<br>
<p><em> &gt;&gt; 1 is easy; 2 is hard ... there are obvious things one can do, but since
</em><br>
<em> &gt;&gt;  we've never dealt with this kind of event before, it's entirely
</em><br>
<em> &gt;&gt; possible that a &quot;deceptive intelligence increase&quot; could come upon us.
</em><br>
<em> &gt;&gt; Measuring general intelligence is tricky.
</em><br>
<p><em> &gt;&gt; It depends on the situation.  Of course, egoistic considerations of
</em><br>
<em> &gt;&gt; priority are not a concern.  But there's no point in delaying the
</em><br>
<em> &gt;&gt; Novamente-induced Singularity by 3 years to reduce risk from 4% to 3%,
</em><br>
<em> &gt;&gt; if in the interim some other AI team is going to induce a Singularity
</em><br>
<em> &gt;&gt; with a 33.456% risk...
</em><br>
<em> &gt;
</em><br>
<em> &gt; Excellent answer.  The best course of action would be to stop the team
</em><br>
<em> &gt; with the 33% risk of failure (at any cost I'd say given that number). But
</em><br>
<em> &gt; if they could not be stopped I'd endorse starting a less risky
</em><br>
<em> &gt; Singularity as an alternative.
</em><br>
<p>James Higgins, I suggest you read &quot;Policy recommendations&quot; in CFAI, where 
<br>
this kind of thinking is analyzed at length.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4545.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Previous message:</strong> <a href="4543.html">Mark Walker: "Re: Friendly Existential Wager"</a>
<li><strong>In reply to:</strong> <a href="4529.html">James Higgins: "RE: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4549.html">Ben Goertzel: "RE: Controlled ascent (was: Military Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="4549.html">Ben Goertzel: "RE: Controlled ascent (was: Military Friendly AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4544">[ date ]</a>
<a href="index.html#4544">[ thread ]</a>
<a href="subject.html#4544">[ subject ]</a>
<a href="author.html#4544">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
