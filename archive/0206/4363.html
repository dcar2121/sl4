<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Self-modifying FAI (was: How hard a Singularity?)</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="Re: Self-modifying FAI (was: How hard a Singularity?)">
<meta name="Date" content="2002-06-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Self-modifying FAI (was: How hard a Singularity?)</h1>
<!-- received="Wed Jun 26 11:54:14 2002" -->
<!-- isoreceived="20020626175414" -->
<!-- sent="Wed, 26 Jun 2002 08:43:48 -0700" -->
<!-- isosent="20020626154348" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="Re: Self-modifying FAI (was: How hard a Singularity?)" -->
<!-- id="4.3.2.7.2.20020626083319.02576968@mail.earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D19A58A.6000904@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=Re:%20Self-modifying%20FAI%20(was:%20How%20hard%20a%20Singularity?)"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Wed Jun 26 2002 - 09:43:48 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4364.html">James Higgins: "Re: Self-modifying FAI (was: How hard a Singularity?)"</a>
<li><strong>Previous message:</strong> <a href="4362.html">Eugen Leitl: "Re: Self-modifying FAI (was: How hard a Singularity?)"</a>
<li><strong>In reply to:</strong> <a href="4355.html">Eliezer S. Yudkowsky: "Self-modifying FAI (was: How hard a Singularity?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4372.html">Ben Goertzel: "RE: Self-modifying FAI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4372.html">Ben Goertzel: "RE: Self-modifying FAI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4375.html">Eliezer S. Yudkowsky: "Re: Self-modifying FAI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4377.html">Michael Roy Ames: "Re: Self-modifying FAI (was: How hard a Singularity?)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4363">[ date ]</a>
<a href="index.html#4363">[ thread ]</a>
<a href="subject.html#4363">[ subject ]</a>
<a href="author.html#4363">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 07:29 AM 6/26/2002 -0400, Eliezer S. Yudkowsky wrote:
<br>
<em>&gt;Stephen Reed wrote:
</em><br>
<em>&gt;&gt;On Tue, 25 Jun 2002, Ben Goertzel wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;The hard part is: If one creates a system that is able to change its concept
</em><br>
<em>&gt;&gt;&gt;of Friendliness over time, and is able to change the way it governs its
</em><br>
<em>&gt;&gt;&gt;behavior based on &quot;goals&quot; over time, then how does one guarantee (with high
</em><br>
<em>&gt;&gt;&gt;probability) that Friendliness (in the designer's sense) persists through
</em><br>
<em>&gt;&gt;&gt;these changes.
</em><br>
<em>&gt;&gt;I understand from CFAI that one grounds the concept of Friendliness in
</em><br>
<em>&gt;&gt;external referents - that the Seed AI attempts to model with increasing
</em><br>
<em>&gt;&gt;fidelity.  So the evolving Seed AI becomes more friendly as it reads more,
</em><br>
<em>&gt;&gt;experiments more and discovers more about what friendliness actually is.
</em><br>
<em>&gt;&gt;For Cyc, friendliness would not be an implementation term (e.g. some piece
</em><br>
<em>&gt;&gt;of code that can be replaced), but be a rich symbolic representation of
</em><br>
<em>&gt;&gt;something in the real world to be sensed directly or indirectly.
</em><br>
<em>&gt;&gt;So I regard the issue as one of properly educating the Seed AI as to what
</em><br>
<em>&gt;&gt;constitutes unfriendly behavior and why not to do it - via external
</em><br>
<em>&gt;&gt;referents.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I agree with your response to Ben.  We don't expect an AI's belief that 
</em><br>
<em>&gt;the sky is blue to drift over successive rounds of 
</em><br>
<em>&gt;self-modification.  Beliefs with an external referent should not &quot;drift&quot; 
</em><br>
<em>&gt;under self-modification except insofar as they &quot;drift&quot; into correspondence 
</em><br>
<em>&gt;with reality.  Write a definition of Friendliness made up of references to 
</em><br>
<em>&gt;things which exist outside the AI, and the content has no reason to 
</em><br>
<em>&gt;&quot;drift&quot;.  If content drifts it will begin making incorrect predictions and 
</em><br>
<em>&gt;will be corrected by further learning.
</em><br>
<p>Unfortunately, can we construct a definition of friendliness using external 
<br>
reference points which truly equals what we really want?  Given much 
<br>
greater knowledge and intelligence what we attribute to friendly behavior 
<br>
may end up looking quite different.
<br>
<p>Your definition of ethics is a good example.  If an alien landed tomorrow 
<br>
and the first person it met was a fantastic salesman, the salesman may 
<br>
appear to be exceedingly friendly.  When in fact their only goal is to open 
<br>
up a new trade route and they don't in fact care one iota about the alien, 
<br>
only the result!  ;)
<br>
<p>Have a look at all the problems in the Catholic church these days.  20 
<br>
years ago (which much of it was occurring), do you think anyone would have 
<br>
believed that reality?
<br>
<p>We may *think* we are defining friendliness via external reference points 
<br>
but actually be defining only the appearance of friendliness or something 
<br>
similar.  Thus the SI would only need to appear friendly to us, even while 
<br>
it was planning to turn the planet into computing resources.
<br>
<p><em>&gt;Furthermore, programmers are physical objects and the intentions of 
</em><br>
<em>&gt;programmers are real properties of those physical objects.  &quot;The intention 
</em><br>
<em>&gt;that was in the mind of the programmer when writing this line of code&quot; is 
</em><br>
<em>&gt;a real, external referent; a human can understand it, and an AI that 
</em><br>
<em>&gt;models causal systems and other agents should be able to understand it as 
</em><br>
<em>&gt;well. Not just the image of Friendliness itself, but the entire 
</em><br>
<em>&gt;philosophical model underlying the goal system, can be defined in terms of 
</em><br>
<em>&gt;things that exist outside the AI and are subject to discovery.
</em><br>
<p>A human can understand the words &quot;The intention that was in the mind of the 
<br>
programmer when writing this line of code&quot;, but they could never fully 
<br>
UNDERSTAND it.  This is why I think you need to have more real life 
<br>
experience, Eliezer.  Those of us that are married can easily understand 
<br>
why the above is not possible.  You can never FULLY understand what someone 
<br>
else intends by something.
<br>
<p>To use Eliezer's method, while I may not be correct I'm quite certain you 
<br>
are wrong.  (Does that make me an honorary Friendship Programmer?)
<br>
<p>James Higgins
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4364.html">James Higgins: "Re: Self-modifying FAI (was: How hard a Singularity?)"</a>
<li><strong>Previous message:</strong> <a href="4362.html">Eugen Leitl: "Re: Self-modifying FAI (was: How hard a Singularity?)"</a>
<li><strong>In reply to:</strong> <a href="4355.html">Eliezer S. Yudkowsky: "Self-modifying FAI (was: How hard a Singularity?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4372.html">Ben Goertzel: "RE: Self-modifying FAI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4372.html">Ben Goertzel: "RE: Self-modifying FAI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4375.html">Eliezer S. Yudkowsky: "Re: Self-modifying FAI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4377.html">Michael Roy Ames: "Re: Self-modifying FAI (was: How hard a Singularity?)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4363">[ date ]</a>
<a href="index.html#4363">[ thread ]</a>
<a href="subject.html#4363">[ subject ]</a>
<a href="author.html#4363">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
