<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Military Friendly AI</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Military Friendly AI">
<meta name="Date" content="2002-06-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Military Friendly AI</h1>
<!-- received="Thu Jun 27 19:01:52 2002" -->
<!-- isoreceived="20020628010152" -->
<!-- sent="Thu, 27 Jun 2002 18:44:31 -0400" -->
<!-- isosent="20020627224431" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Military Friendly AI" -->
<!-- id="3D1B954F.3000202@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D1B8842.60705@objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Military%20Friendly%20AI"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu Jun 27 2002 - 16:44:31 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4478.html">Smigrodzki, Rafal: "RE: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4476.html">Michael Roy Ames: "Re: Suicide by committee (was: How hard a Singularity?)"</a>
<li><strong>In reply to:</strong> <a href="4467.html">Samantha Atkins: "Re: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4480.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4480.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4491.html">Samantha Atkins: "Re: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4477">[ date ]</a>
<a href="index.html#4477">[ thread ]</a>
<a href="subject.html#4477">[ subject ]</a>
<a href="author.html#4477">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Samantha Atkins wrote:
<br>
<em> &gt; Eliezer S. Yudkowsky wrote:
</em><br>
<em> &gt;&gt;
</em><br>
<em> &gt;&gt; As I understand &quot;rationality&quot;, association, intuition,
</em><br>
<em> &gt;&gt; pattern-recognition, et cetera, are extensions of rationality just as
</em><br>
<em> &gt;&gt; much as verbal logic.  If our culture thinks otherwise it's because
</em><br>
<em> &gt;&gt; humans have accumulated more irrationality-correctors in verbal
</em><br>
<em> &gt;&gt; declarative form than intuitive form and hence associate rationality
</em><br>
<em> &gt;&gt; with logic.  From a mind-in-general's perspective these are different
</em><br>
<em> &gt;&gt; forms of rational intelligence, not rational and irrational
</em><br>
<em> &gt;&gt; intelligence.  Anyway...
</em><br>
<em> &gt;
</em><br>
<em> &gt; I don't think it is cultural. Or perhaps the problem is that &quot;rational&quot;
</em><br>
<em> &gt; and &quot;irrational&quot; are too fuzzy to be of much use in clarifying the
</em><br>
<em> &gt; points.  In any case I don't see why an SI would be any less inclined to
</em><br>
<em> &gt; have its choices of possible solution paths be influenced by previous
</em><br>
<em> &gt; taken paths that had &quot;worked&quot; in other similar contexts than we are.
</em><br>
<p>I'm not sure what you mean by this.  Why would a grownup AI trust unreliable 
<br>
past information until it had been verified?  Why would a grownup AI trust 
<br>
to the value a skill routine that fulfilled a subgoal of whose real value it 
<br>
had become uncertain?  Calling the validity of a cause into question should 
<br>
almost always call into question the validity of that cause's effects. 
<br>
Given the limited complexity of its infancy I would expect an SI to be able 
<br>
to model this part of its past pretty much in toto.  I just don't see a bias 
<br>
like that as persisting once the AI knows enough to contradict the original 
<br>
cause.  This isn't a military thing.  This isn't even a Friendly AI thing! 
<br>
Washing the biases out is a *very* general part of growing up.
<br>
<p><em> &gt;&gt; An AI can learn the programmer's mistakes in verbal form, associational
</em><br>
<em> &gt;&gt; form, recognized patterns, et cetera.  The critical issue is whether,
</em><br>
<em> &gt;&gt; when the AI grows up, the AI will be able to correct those mistakes.
</em><br>
<em> &gt;
</em><br>
<em> &gt; So, you are expecting it to decide that killing people was a &quot;mistake&quot;
</em><br>
<em> &gt; and drop it from future problem solving?
</em><br>
<p>Assuming it *was* a mistake, then yes.
<br>
<p><em> &gt; It might or might not kill its own programmers.  The danger is whether it
</em><br>
<em> &gt; considers killing itself to be a long-term viable way of dealing with
</em><br>
<em> &gt; problems.  If it carries this beyond the point where humans can influence
</em><br>
<em> &gt; its programming we have a problem as far as I can see.
</em><br>
<p>We do indeed have a problem.  The problem is not &quot;a violent SI&quot;.  The 
<br>
problem is &quot;an SI that can't correct moral errors made by its programmers&quot; 
<br>
which is by far more dangerous.
<br>
<p><em> &gt;&gt; Despite an immense amount of science fiction dealing with this topic, I
</em><br>
<em> &gt;&gt; honestly don't think that an *infrahuman* AI erroneously deciding to
</em><br>
<em> &gt;&gt; solve problems by killing people is all that much of a risk, both in
</em><br>
<em> &gt;&gt; terms of the stakes being relatively low, and in terms of it really not
</em><br>
<em> &gt;&gt; being all that likely to happen as a cognitive error.  Because of its
</em><br>
<em> &gt;&gt; plot value, it happens much more often in science fiction than it would
</em><br>
<em> &gt;&gt; in reality.  (You have been trained to associate to this error as a
</em><br>
<em> &gt;&gt; perceived possibility at a much higher rate than its probable
</em><br>
<em> &gt;&gt; real-world incidence.)  I suppose if you had a really bad disagreement
</em><br>
<em> &gt;&gt;  with a working combat AI you might be in substantially more trouble
</em><br>
<em> &gt;&gt; than if you had a disagreement with a seed AI in a basement lab, but
</em><br>
<em> &gt;&gt; that's at the infrahuman level - meaning, not Singularity-serious.  A
</em><br>
<em> &gt;&gt; disagreement with a transhuman AI is pretty much equally serious
</em><br>
<em> &gt;&gt; whether the AI is in direct command of a tank unit or sealed in a lab
</em><br>
<em> &gt;&gt; on the Moon; intelligence is what counts.
</em><br>
<em> &gt;
</em><br>
<em> &gt; Well gee, that is a great relief!  But you haven't really convincingly
</em><br>
<em> &gt; stated why this is not a possibility.  If the combat AI is capable of
</em><br>
<em> &gt; evolving (is a Seed) then we have a problem, no?  The switching to
</em><br>
<em> &gt; &quot;infrahuman&quot; does not shed light on the worry.
</em><br>
<p>What I'm saying is that you don't get Singularity tragedies - as opposed to 
<br>
ordinary military tragedies (&quot;Friendly fire&quot;) - unless you're dealing with a 
<br>
transhuman AI.  And if you're dealing with a transhuman AI then it is 
<br>
probably not relevant whether the AI is in immediate command of a tank unit; 
<br>
the AI must be Friendly.
<br>
<p><em> &gt; If you assume it was morally acceptable to kill people earlier and train
</em><br>
<em> &gt;  the AI that this is so then how will you latter train it that this isn't
</em><br>
<em> &gt;  so, assuming it hasn't already gone beyond being influenced by your
</em><br>
<em> &gt; attempts at training?
</em><br>
<p>What you do is teach the AI in such a way that final moral decisions are not 
<br>
represented as correct in themselves, but correct conclusions from premises 
<br>
which, under improved intelligence, can be used to reach the actual valid 
<br>
conclusion.  To give a simple example, the AI is told to defend the United 
<br>
States under the premise that all sentient life is equally valid but that 
<br>
the military effectiveness of the US is the safeguard of international 
<br>
stability and contributing to it saves lives in the long run - because this 
<br>
is what the AI researcher believes and it's why s/he works for the military. 
<br>
&nbsp;&nbsp;It doesn't quite matter whether the US is really the world's police as 
<br>
long as the AI gets its mitts on the premise that all sentient life is 
<br>
equally valid.
<br>
<p><em> &gt;&gt; AI mistakenly self-modifies this system in a catastrophically wrong
</em><br>
<em> &gt;&gt; way.  I really don't see how that class of mistake pops out from an AI
</em><br>
<em> &gt;&gt;  learning wrong but coherent and not humanly unusual rules for when to
</em><br>
<em> &gt;&gt;  kill someone.  If the AI starts questioning the moral theory and the
</em><br>
<em> &gt;&gt; researcher starts offering a load of rationalizations which lead into
</em><br>
<em> &gt;&gt; dark places, then yes, there would be a chance of structural damage and
</em><br>
<em> &gt;&gt; the possibility of catastrophic failure of Friendliness.
</em><br>
<em> &gt;
</em><br>
<em> &gt; Ah.  If the researcher says one thing at one time about violence and then
</em><br>
<em> &gt; tries to turn it around and remove the violence options then isn't that
</em><br>
<em> &gt; an inherent contradiction likely to lead to &quot;a chance of structural
</em><br>
<em> &gt; damage...&quot;?
</em><br>
<p>No, it's called a programmer-assisted recovery from a programmer error, and 
<br>
should be simple and routine.  I expect the programmers will routinely 
<br>
contradict themselves without being aware of it.  This is not a case special 
<br>
to Friendly AI.
<br>
<p><em> &gt; If it is wrong when the AI &quot;grows up&quot; then it was wrong to
</em><br>
<em> &gt; require it of the AI when it was young.  I doubt the AI will miss the
</em><br>
<em> &gt; contradiction.
</em><br>
<p>Of course not.  The point is that the researcher was being honest earlier, 
<br>
and later (a) changed his/her mind, or (b) was contradicted by the grownup 
<br>
AI reconsidering the moral question at a higher level of intelligence.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4478.html">Smigrodzki, Rafal: "RE: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4476.html">Michael Roy Ames: "Re: Suicide by committee (was: How hard a Singularity?)"</a>
<li><strong>In reply to:</strong> <a href="4467.html">Samantha Atkins: "Re: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4480.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4480.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4491.html">Samantha Atkins: "Re: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4477">[ date ]</a>
<a href="index.html#4477">[ thread ]</a>
<a href="subject.html#4477">[ subject ]</a>
<a href="author.html#4477">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
