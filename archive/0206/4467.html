<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Military Friendly AI</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Military Friendly AI">
<meta name="Date" content="2002-06-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Military Friendly AI</h1>
<!-- received="Thu Jun 27 17:55:31 2002" -->
<!-- isoreceived="20020627235531" -->
<!-- sent="Thu, 27 Jun 2002 14:48:50 -0700" -->
<!-- isosent="20020627214850" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Military Friendly AI" -->
<!-- id="3D1B8842.60705@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D1B5107.9020200@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Military%20Friendly%20AI"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Thu Jun 27 2002 - 15:48:50 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4468.html">Smigrodzki, Rafal: "RE: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4466.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>In reply to:</strong> <a href="4453.html">Eliezer S. Yudkowsky: "Re: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4477.html">Eliezer S. Yudkowsky: "Re: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4477.html">Eliezer S. Yudkowsky: "Re: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4467">[ date ]</a>
<a href="index.html#4467">[ thread ]</a>
<a href="subject.html#4467">[ subject ]</a>
<a href="author.html#4467">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer S. Yudkowsky wrote:
<br>
<em>&gt; Ben Goertzel wrote:
</em><br>
<em>&gt;  &gt;&gt; To summarize the summary, the main danger to Friendliness of military
</em><br>
<em>&gt;  &gt;&gt; AI is that the commanders might want a docile tool and therefore
</em><br>
<em>&gt;  &gt;&gt; cripple moral development.  As far as I can tell, there's no inherent
</em><br>
<em>&gt;  &gt;&gt; danger to Friendliness in an AI going into combat, like it or not.
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; In my view, the main danger to Friendliness of military AI is that 
</em><br>
<em>&gt; the AI
</em><br>
<em>&gt;  &gt; may get used to the idea that killing people for the right cause is not
</em><br>
<em>&gt;  &gt; such a bad thing...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes, this is the obvious thing to worry about.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;  &gt; Your arguments for why Friendliness is consistent with military AI are
</em><br>
<em>&gt;  &gt; based on your theory of a Friendly goal system as a fully logical,
</em><br>
<em>&gt;  &gt; rational thing.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; As I understand &quot;rationality&quot;, association, intuition, 
</em><br>
<em>&gt; pattern-recognition, et cetera, are extensions of rationality just as 
</em><br>
<em>&gt; much as verbal logic.  If our culture thinks otherwise it's because 
</em><br>
<em>&gt; humans have accumulated more irrationality-correctors in verbal 
</em><br>
<em>&gt; declarative form than intuitive form and hence associate rationality 
</em><br>
<em>&gt; with logic.  From a mind-in-general's perspective these are different 
</em><br>
<em>&gt; forms of rational intelligence, not rational and irrational 
</em><br>
<em>&gt; intelligence.  Anyway...
</em><br>
<em>&gt;
</em><br>
<p>I don't think it is cultural. Or perhaps the problem is that 
<br>
&quot;rational&quot; and &quot;irrational&quot; are too fuzzy to be of much use in 
<br>
clarifying the points.  In any case I don't see why an SI would 
<br>
be any less inclined to have its choices of possible solution 
<br>
paths be influenced by previous taken paths that had &quot;worked&quot; in 
<br>
other similar contexts than we are.
<br>
<p><p><em>&gt; An AI can learn the programmer's mistakes in verbal form, associational 
</em><br>
<em>&gt; form, recognized patterns, et cetera.  The critical issue is whether, 
</em><br>
<em>&gt; when the AI grows up, the AI will be able to correct those mistakes.
</em><br>
<em>&gt;
</em><br>
<p>So, you are expecting it to decide that killing people was a 
<br>
&quot;mistake&quot; and drop it from future problem solving?
<br>
<p><em>&gt;  &gt; This means that its logical reasoning is going to be *guided* by
</em><br>
<em>&gt;  &gt; associations that occur to it based on the sorts of things it's been
</em><br>
<em>&gt;  &gt; doing, and thinking about, in the past...
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; Thus, an AI that's been involved heavily in military matters, is 
</em><br>
<em>&gt; going to
</em><br>
<em>&gt;  &gt; be more likely to think of violent solutions to problems, because its
</em><br>
<em>&gt;  &gt; pool of associations will push it that way
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And its memories, its concepts, its problem-solving skills, and so on.  
</em><br>
<em>&gt; But this is only a structural error if the AI attempts to kill its own 
</em><br>
<em>&gt; programmers to solve a problem.  I suppose that's a possibility, but it 
</em><br>
<em>&gt; really sounds more like the kind of thing that (a) happens in science 
</em><br>
<em>&gt; fiction but not real life (as opposed to things which happen in science 
</em><br>
<em>&gt; fiction and real life), and (b) sounds like a failure of infrahuman AI, 
</em><br>
<em>&gt; which is probably not a Singularity matter.  Besides, I would expect a 
</em><br>
<em>&gt; combat AI to be extensively trained in how to avoid killing friendly 
</em><br>
<em>&gt; combatants.
</em><br>
<em>&gt;
</em><br>
<p>It might or might not kill its own programmers.  The danger is 
<br>
whether it considers killing itself to be a long-term viable way 
<br>
of dealing with problems.  If it carries this beyond the point 
<br>
where humans can influence its programming we have a problem as 
<br>
far as I can see.
<br>
<p><p><em>&gt;  &gt; I don't want an AGI whose experience and orientation incline it to
</em><br>
<em>&gt;  &gt; associations involving killing large numbers of humans!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Despite an immense amount of science fiction dealing with this topic, I 
</em><br>
<em>&gt; honestly don't think that an *infrahuman* AI erroneously deciding to 
</em><br>
<em>&gt; solve problems by killing people is all that much of a risk, both in 
</em><br>
<em>&gt; terms of the stakes being relatively low, and in terms of it really not 
</em><br>
<em>&gt; being all that likely to happen as a cognitive error.  Because of its 
</em><br>
<em>&gt; plot value, it happens much more often in science fiction than it would 
</em><br>
<em>&gt; in reality.  (You have been trained to associate to this error as a 
</em><br>
<em>&gt; perceived possibility at a much higher rate than its probable real-world 
</em><br>
<em>&gt; incidence.)  I suppose if you had a really bad disagreement with a 
</em><br>
<em>&gt; working combat AI you might be in substantially more trouble than if you 
</em><br>
<em>&gt; had a disagreement with a seed AI in a basement lab, but that's at the 
</em><br>
<em>&gt; infrahuman level - meaning, not Singularity-serious.  A disagreement 
</em><br>
<em>&gt; with a transhuman AI is pretty much equally serious whether the AI is in 
</em><br>
<em>&gt; direct command of a tank unit or sealed in a lab on the Moon; 
</em><br>
<em>&gt; intelligence is what counts.
</em><br>
<em>&gt;
</em><br>
<p>Well gee, that is a great relief!  But you haven't really 
<br>
convincingly stated why this is not a possibility.  If the 
<br>
combat AI is capable of evolving (is a Seed) then we have a 
<br>
problem, no?  The switching to &quot;infrahuman&quot; does not shed light 
<br>
on the worry.
<br>
<p><p><em>&gt;  &gt; You may say that *your* AGI is gonna be so totally rational that it will
</em><br>
<em>&gt;  &gt; always make the right decisions regardless of the pool of associations
</em><br>
<em>&gt;  &gt; that its experience provides to it....  But this does not reassure me
</em><br>
<em>&gt;  &gt; adequately. What if you're wrong, and your AI turns out, like the human
</em><br>
<em>&gt;  &gt; mind or Novamente, to allow associations to guide the course of its
</em><br>
<em>&gt;  &gt; reasoning sometimes?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Then the AI, when it's young, will kill a bunch of people it didn't 
</em><br>
<em>&gt; really have to.  But that moral risk is inherent in joining the army or 
</em><br>
<em>&gt; working on any military project.  The Singularity risk is if the AI's 
</em><br>
<p>And is unacceptable.
<br>
<p><em>&gt; training trashes the part of the Friendship system that would be 
</em><br>
<em>&gt; responsible for fixing the learned error when the AI grows up, or if the 
</em><br>
<p>If you assume it was morally acceptable to kill people earlier 
<br>
and train the AI that this is so then how will you latter train 
<br>
it that this isn't so, assuming it hasn't already gone beyond 
<br>
being influenced by your attempts at training?
<br>
<p><p><em>&gt; AI mistakenly self-modifies this system in a catastrophically wrong 
</em><br>
<em>&gt; way.  I really don't see how that class of mistake pops out from an AI 
</em><br>
<em>&gt; learning wrong but coherent and not humanly unusual rules for when to 
</em><br>
<em>&gt; kill someone.  If the AI starts questioning the moral theory and the 
</em><br>
<em>&gt; researcher starts offering a load of rationalizations which lead into 
</em><br>
<em>&gt; dark places, then yes, there would be a chance of structural damage and 
</em><br>
<em>&gt; the possibility of catastrophic failure of Friendliness.
</em><br>
<em>&gt; 
</em><br>
<p>Ah.  If the researcher says one thing at one time about violence 
<br>
and then tries to turn it around and remove the violence options 
<br>
then isn't that an inherent contradiction likely to lead to &quot;a 
<br>
chance of structural damage...&quot;?  If it is wrong when the AI 
<br>
&quot;grows up&quot; then it was wrong to require it of the AI when it was 
<br>
young.  I doubt the AI will miss the contradiction.
<br>
<p><p><p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4468.html">Smigrodzki, Rafal: "RE: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4466.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>In reply to:</strong> <a href="4453.html">Eliezer S. Yudkowsky: "Re: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4477.html">Eliezer S. Yudkowsky: "Re: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4477.html">Eliezer S. Yudkowsky: "Re: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4467">[ date ]</a>
<a href="index.html#4467">[ thread ]</a>
<a href="subject.html#4467">[ subject ]</a>
<a href="author.html#4467">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
