<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: How hard a Singularity?</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="RE: How hard a Singularity?">
<meta name="Date" content="2002-06-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: How hard a Singularity?</h1>
<!-- received="Thu Jun 27 05:03:29 2002" -->
<!-- isoreceived="20020627110329" -->
<!-- sent="Thu, 27 Jun 2002 01:50:28 -0700" -->
<!-- isosent="20020627085028" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="RE: How hard a Singularity?" -->
<!-- id="4.3.2.7.2.20020627012513.01ca02d8@mail.earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="Pine.LNX.4.33.0206270026160.18083-100000@crapgame.cyc.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=RE:%20How%20hard%20a%20Singularity?"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Thu Jun 27 2002 - 02:50:28 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4437.html">Helge Kautz: "Re: P2P"</a>
<li><strong>Previous message:</strong> <a href="4435.html">Samantha Atkins: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4432.html">Stephen Reed: "RE: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4443.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4436">[ date ]</a>
<a href="index.html#4436">[ thread ]</a>
<a href="subject.html#4436">[ subject ]</a>
<a href="author.html#4436">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 01:14 AM 6/27/2002 -0500, Stephen Reed wrote:
<br>
<em>&gt;On Wed, 26 Jun 2002, Ben Goertzel wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; I don't doubt that there are many trustworthy, moral individuals within the
</em><br>
<em>&gt; &gt; US  military.  However, I do not trust the US military as an organization,
</em><br>
<em>&gt; &gt; no way Jose'.  This is the crew that invaded Grenada... the organization
</em><br>
<em>&gt; &gt; that nuked Japan (probably both bombs were needless, but it's pretty damn
</em><br>
<em>&gt; &gt; clear the second one was), that systematically tortured Vietnamese 
</em><br>
<em>&gt; women and
</em><br>
<em>&gt; &gt; children in the name of democracy and justice....  I'll spare you a full
</em><br>
<em>&gt; &gt; list of examples.  Ever read the first-person account of how US soldiers
</em><br>
<em>&gt; &gt; chopped off the arms and legs of a Vietnamese woman, inserted dynamite in
</em><br>
<em>&gt; &gt; her vagina and blew her up?  Not an outlier occurence.  Excuse me if the
</em><br>
<em>&gt; &gt; impeccable morality of the US military seems a little questionable to me...
</em><br>
<em>&gt;
</em><br>
<em>&gt;You strengthen my point.  The US military has evolved ethics because of the
</em><br>
<em>&gt;terrible power that they are entrusted to use.  Why else have the strict
</em><br>
<em>&gt;honor system at military schools?  Furthermore, knowledge that individual
</em><br>
<p>Um, because a strict honor system is part of the conditioning used to get 
<br>
military personnel to always follow orders?  Think that may have something 
<br>
to do with it?  And, well, of course the US Military would prefer not to 
<br>
see the huge ethical problems.  But following orders comes first.
<br>
<p><em>&gt;soldiers are capable of war crimes only encourages future military
</em><br>
<em>&gt;planners to substitute an AGI for a soldier (or augment/monitor a
</em><br>
<em>&gt;soldier) in that situation.  According to my understanding of CFAI, the
</em><br>
<em>&gt;AGI should be resistant to criminal behavior due to a deep knowledge of
</em><br>
<em>&gt;ethics.  From which follows my conclusion that an AGI would not follow an
</em><br>
<em>&gt;illegal military order.
</em><br>
<p>Yes, a deep knowledge of ethics intended to prevent the AI from killing 
<br>
people.  In such a scenario your likely to get a HAL.  Due to 
<br>
irreconcilable differences in its morality/ethics it could end up doing 
<br>
virtually anything.  We're seriously worried if Friendliness can be applied 
<br>
at all, much less a lobotomized version that says its ok to kill people in 
<br>
groups A, B, C or D but not people in groups E or F.
<br>
<p>And some people really wonder why I'd like to see personal space travel 
<br>
before virtually anything else.  Because I want to get the hell out of 
<br>
here, ASAP.
<br>
<p><p><em>&gt;More ethics info regarding my recent notion that military ethics may serve
</em><br>
<em>&gt;to educate an AGI can be found at a military web site and its links list:
</em><br>
<em>&gt;
</em><br>
<em>&gt;<a href="http://www.usna.edu/Ethics/">http://www.usna.edu/Ethics/</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; The Singularity is directly opposed to the national interests of any
</em><br>
<em>&gt; &gt; particular national government, because it will almost inevitably lead to a
</em><br>
<em>&gt; &gt; &quot;revolution&quot; that will make national boundaries meaningless.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Well for me the Singularity is something that I cannot see beyond, so I
</em><br>
<em>&gt;will respond to your statement as though I agree with the premise and
</em><br>
<em>&gt;offer this counterexample of the second statement in your argument:
</em><br>
<em>&gt;
</em><br>
<em>&gt;Recent history shows that national governments will surrender sovereignty
</em><br>
<em>&gt;for a greater good.  Witness the EU and NAFTA.
</em><br>
<p>I didn't see anyone surrender any SOVERENTY.  They traded a little power 
<br>
from column A (control) for power from column B (financial).
<br>
<p><em>&gt; &gt; Thus, no government should be trusted to play a leading role in the
</em><br>
<em>&gt; &gt; Singularity.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Not only do I have the opposite opinion, I believe that as the evidence
</em><br>
<em>&gt;mounts that an AGI is possible, the government - and in particular the US
</em><br>
<em>&gt;government will take the leading role.  I believe this from my
</em><br>
<em>&gt;understanding of the dynamics of how our government institutions have
</em><br>
<em>&gt;responded to the great technical challenges over my lifetime -  The Space
</em><br>
<em>&gt;Race, the War On Cancer, Safe Cars, Solar Energy, The AIDS Epidemic ...
</em><br>
<em>&gt;I do not see any non-governmental organization as the front-runner.  The
</em><br>
<em>&gt;actual work of developing and educating the Seed AI would in my opinion be
</em><br>
<em>&gt;performed by private contracting companies - somewhat in competition with
</em><br>
<em>&gt;each other.
</em><br>
<p>Lovely, lets have the AI educated by the lowest bidder.  Not to mention 
<br>
that having multiple entities &quot;educating&quot; it would likely end up producing 
<br>
conflicting goals and external references.  Can't you see how this whole 
<br>
process could easily break the AI and, as a result, quite possible destroy 
<br>
the human race?
<br>
<p><em>&gt; &gt; I might accept gov't funding for AGI research, but I would never willingly
</em><br>
<em>&gt; &gt; place control of an AGI in the hands of any military organization.  That
</em><br>
<em>&gt; &gt; really scares me.  Those people are not Singularity-savvy, and I don't 
</em><br>
<em>&gt; trust
</em><br>
<em>&gt; &gt; they will become so in the future, not in any healthy way.  Plus, their
</em><br>
<em>&gt; &gt; interests are not those of the human race as a whole, let alone of 
</em><br>
<em>&gt; sentience
</em><br>
<em>&gt; &gt; as a whole.
</em><br>
<em>&gt;
</em><br>
<em>&gt;My belief that the funding will be from the defense budget stems from my
</em><br>
<em>&gt;conclusion that an AGI would be priceless for our nation's defense, and
</em><br>
<em>&gt;knowing this the military will fund before other government institutions.
</em><br>
<em>&gt;
</em><br>
<em>&gt;My comfort with military research organization (e.g. Darpa) leading and
</em><br>
<em>&gt;coordinating this effort is from my own experience and I am not going to
</em><br>
<em>&gt;persuade anyone else (lacking to skills to do so).
</em><br>
<p>This has nothing to do with comfort.  No one (well, maybe some will), ok 
<br>
I'm not saying DARPA is a bad organization.  Quite the contrary, they've 
<br>
done great things.  But ANY government, and yes that includes the US 
<br>
Government, is the wrong entity to create or control the 
<br>
Singularity.  Attempting to put any constraints on Friendly AI development 
<br>
much beyond being friendly to all humans is virtually impossible.  Yet 
<br>
Governemnts would have to try and do just that, they would attempt to force 
<br>
their will and viewpoint on the AI.  Simply the act of doing this could 
<br>
unravel the whole thing and, thus, we loose complete control of the AI.
<br>
<p>Go back and read some of the much earlier posts.  There is NO WAY to 
<br>
control or stop an AI which is significantly more intelligent than 
<br>
humans.  Which means if you screw it up we're all toast.  Not even DARPA 
<br>
can get around this.  The project either needs to be done as COMPLETELY 
<br>
Friendly or not at all.  A completely friendly AI would be useless to 
<br>
governments and they certainly wouldn't not try.
<br>
<p>James Higgins
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4437.html">Helge Kautz: "Re: P2P"</a>
<li><strong>Previous message:</strong> <a href="4435.html">Samantha Atkins: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4432.html">Stephen Reed: "RE: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4443.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4436">[ date ]</a>
<a href="index.html#4436">[ thread ]</a>
<a href="subject.html#4436">[ subject ]</a>
<a href="author.html#4436">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
