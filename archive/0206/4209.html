<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Seed AI (was: How hard a Singularity?)</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Seed AI (was: How hard a Singularity?)">
<meta name="Date" content="2002-06-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Seed AI (was: How hard a Singularity?)</h1>
<!-- received="Sun Jun 23 13:15:42 2002" -->
<!-- isoreceived="20020623191542" -->
<!-- sent="Sun, 23 Jun 2002 08:37:58 -0600" -->
<!-- isosent="20020623143758" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Seed AI (was: How hard a Singularity?)" -->
<!-- id="LAEGJLOGJIOELPNIOOAJEEFOCLAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D15A4CF.9030101@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Seed%20AI%20(was:%20How%20hard%20a%20Singularity?)"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sun Jun 23 2002 - 08:37:58 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4210.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4208.html">Eliezer S. Yudkowsky: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4194.html">Eliezer S. Yudkowsky: "Seed AI (was: How hard a Singularity?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4212.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4212.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4214.html">Michael Roy Ames: "Re: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4219.html">Eliezer S. Yudkowsky: "Re: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4235.html">James Higgins: "RE: Seed AI (was: How hard a Singularity?)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4209">[ date ]</a>
<a href="index.html#4209">[ thread ]</a>
<a href="subject.html#4209">[ subject ]</a>
<a href="author.html#4209">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer wrote:
<br>
<em>&gt; Perhaps.  Nonetheless there is more Cycish stuff in Novamente than I am
</em><br>
<em>&gt; comfortable with.  Novamente does contain structures along the lines of
</em><br>
<em>&gt; eat(cat, mice).  I realize you insist that these are not the only
</em><br>
<em>&gt; structures
</em><br>
<em>&gt; and that the eventual real version of Novamente will replace these small
</em><br>
<em>&gt; structures with big emergent structures (that nonetheless follow
</em><br>
<em>&gt; roughly the
</em><br>
<em>&gt; same rules as the small structures and can be translated into them for
</em><br>
<em>&gt; faster processing).
</em><br>
<p>The current Novamente version deals only with numerical inputs, and contains
<br>
no structures like
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;eat(cat, mice)
<br>
<p>The first version that deals with human language input, may or may not
<br>
create structures like this internally on its own...
<br>
<p>Such examples are used in the current documentation on the system, because
<br>
they're easy to write about.  In the revised documentation they are relied
<br>
upon less because they proved misleading to you and some other readers.
<br>
<p><em>&gt; I guess what I'm trying to say is we have different
</em><br>
<em>&gt; ideas about how much of mind is implemented by content, the sort
</em><br>
<em>&gt; of stuff we
</em><br>
<em>&gt; humans would regard as *transferable* knowledge - the kind of
</em><br>
<em>&gt; knowledge that
</em><br>
<em>&gt; we communicate through books.  I think you ascribe more mind to
</em><br>
<em>&gt; transferable
</em><br>
<em>&gt; content than I.  I am not saying that you ascribe all mind to
</em><br>
<em>&gt; transferable
</em><br>
<em>&gt; content, but definitely more than I do (and less than Cyc).
</em><br>
<p>I'd like to divide the &quot;Transferable content&quot; category in two
<br>
<p>1) content that is explicitly transmitted through books, DB's, etc.
<br>
<p>2) content that is *implicitly* transmitted through interacting with other
<br>
minds in a shared environment
<br>
<p>2 is at least as important as 1.
<br>
<p>Cyc tries to capture a lot of what is implicitly learned by humans, in
<br>
explicit form, but I am very skeptical about this.
<br>
<p><em>&gt; A tremendous part of an AI is brainware.  The most important
</em><br>
<em>&gt; content - maybe
</em><br>
<em>&gt; not the most content, but the most important content - is content that
</em><br>
<em>&gt; describes things that only AIs and AI programmers have names for,
</em><br>
<em>&gt; and much
</em><br>
<em>&gt; of it will be realtime skills that humans can help learn but
</em><br>
<em>&gt; which have no
</em><br>
<em>&gt; analogue in humans.
</em><br>
<p>In my view, the most important content of an AGI mind will be things that
<br>
neither the AI or its programmers can name, at first.  Namely: *abstract
<br>
thought-patterns, ways of organizing ideas and ways of approaching
<br>
problems*, which we humans use but know only implicitly, and which we will
<br>
be able to transmit to AI minds implicitly through interaction in
<br>
appropriate shared environments..
<br>
<p><p><em>&gt; You think my design is too complex.
</em><br>
<p>Actually I don't have much idea of what your AI design is, if indeed you
<br>
have one!
<br>
<p><em>&gt; Okay.  Nonetheless, the more
</em><br>
<em>&gt; complex a
</em><br>
<em>&gt; design is, the more mind arises from the stuff that implements
</em><br>
<em>&gt; that design,
</em><br>
<em>&gt; and the more opportunities there are to improve mind by improving the
</em><br>
<em>&gt; implementation (never mind actually *improving the design*!)  I
</em><br>
<em>&gt; think that
</em><br>
<em>&gt; the more specific a model one has of mind, the more ways you'll
</em><br>
<em>&gt; be able to
</em><br>
<em>&gt; think of improving it.
</em><br>
<p>Of course, this general statement is not true.  Often, in software
<br>
engineering and other kinds of engineering, a very complex design is HARDER
<br>
to improve than a simple one.
<br>
<p><em>&gt; it is
</em><br>
<em>&gt; knowledge about
</em><br>
<em>&gt; how to think, and an AI will think differently from humans.
</em><br>
<p>It will think differently from humans, but in the early stages, it will
<br>
learn a lot of what it knows about &quot;how to think&quot; from humans.
<br>
<p><em>&gt; Humans in
</em><br>
<em>&gt; general (as opposed to successful AI researchers) have very
</em><br>
<em>&gt; little knowledge
</em><br>
<em>&gt; of this kind, and what there is will be mostly untransferable
</em><br>
<em>&gt; because of the
</em><br>
<em>&gt; difference in cognitive systems.
</em><br>
<p>I think that a lot of transfer of thought-patterns will happen *implicitly*
<br>
through interaction in shared environments.
<br>
<p>For this to happen, explicit declarative knowledge of thought patterns is
<br>
not required, on the part of the human teachers.
<br>
<p><em>&gt; I think explicit education by humans will be an important part of
</em><br>
<em>&gt; bootstrapping an AI to the level of being able to solve its own problems.
</em><br>
<em>&gt; By the time human knowledge is even comprehensible to the AI, most of the
</em><br>
<em>&gt; hard problems will have already been solved and the AI will
</em><br>
<em>&gt; probably be in
</em><br>
<em>&gt; the middle of a hard takeoff.
</em><br>
<p>I doubt this is how things will go.  I think human knowledge will be
<br>
comprehensible by an AI *well before* the AI is capable of drastically
<br>
modifying its own sourcecode in the interest of vastly increased
<br>
intelligence.
<br>
<p><p><em>&gt; &gt; And I agree with that -- the question is *how fast* will the AI
</em><br>
<em>&gt; be able to
</em><br>
<em>&gt; &gt; improve itself.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; It's a quantitative question.  Your intuitive estimate is much
</em><br>
<em>&gt; faster than
</em><br>
<em>&gt; &gt; mine...
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ben, you're the one who insists that everything is &quot;intuition&quot;.
</em><br>
<em>&gt; I am happy
</em><br>
<em>&gt; to describe your estimates as &quot;intuitions&quot; if you wish, but I think that
</em><br>
<em>&gt; more detailed thoughts are both possible and desirable.
</em><br>
<p>I don't insist that everything is intuition.  I try to carefully distinguish
<br>
between conclusions based on evidence, and hypotheses based on intuition.
<br>
Both in my own thinking and in the thinking of others.
<br>
<p>(Of course, intuitions are at bottom based on evidence, but they integrate
<br>
large bodies of evidence in complex, hard-to-trace ways.)
<br>
<p><em>&gt; You seem to think that you create a general intelligence with all basic
</em><br>
<em>&gt; dynamics in place, thereby creating a baby, which then educates
</em><br>
<em>&gt; itself up to
</em><br>
<em>&gt; to human-adult-level intelligence, which can be done by studying
</em><br>
<em>&gt; signals of
</em><br>
<em>&gt; the kind which human adults use to communicate with each other.
</em><br>
<em>&gt; I don't see
</em><br>
<em>&gt; this as likely.  The process of going from baby to adult is likely to be
</em><br>
<em>&gt; around half brainware improvement and half the accumulation of knowledge
</em><br>
<em>&gt; that cannot be downloaded off the Internet.  The most the corpus of human
</em><br>
<em>&gt; knowledge can do is provide various little blackbox puzzles to be solved,
</em><br>
<em>&gt; and most of those puzzles won't be the kind the AI needs to grow.
</em><br>
<p>Yes, we differ here.  I think we will create an AGI with all basic dynamics
<br>
in place, thus creating a baby, which will educate itself up to
<br>
human-adult-level-intelligence, partly by interaction with adult humans in
<br>
shared environments.
<br>
<p>I also think that, during this education process, we will discover flaws in
<br>
the AGI's &quot;basic dynamics&quot;, so that engineering will be ongoing during the
<br>
teaching period.  As teaching progresses, the AGI itself will be more and
<br>
more useful in helping improve its own dynamics (and structures).
<br>
<p><em>&gt; Okay, now *you're* misinterpreting *me*.  I don't think that AGI can be
</em><br>
<em>&gt; bootstrapped to through seed AI, nor that human interaction is
</em><br>
<em>&gt; unimportant.
</em><br>
<em>&gt;   Humans are a seed AI's foundations of order.  Humans will teach
</em><br>
<em>&gt; the AI but
</em><br>
<em>&gt; what they will teach is not the corpus of human declarative
</em><br>
<em>&gt; knowledge.  What
</em><br>
<em>&gt; they teach will be domain problems that are at the right level
</em><br>
<em>&gt; the AI needs
</em><br>
<em>&gt; to grow, and what the AI will learn will be how to think.
</em><br>
<p>I think that humans will teach the AGI more than just &quot;domain problems at
<br>
the right level,&quot; I think that by cooperatively solving problems together
<br>
with the AGI, humans will teach it a network of interrelated
<br>
thought-patterns.  Just as we learn from other humans via interacting with
<br>
them.
<br>
<p><em>&gt; &gt; You may say that with good enough learning methods, no teaching is
</em><br>
<em>&gt; &gt; necessary.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Incorrect.  What I am saying is that what is taught will not be
</em><br>
<em>&gt; the corpus
</em><br>
<em>&gt; of human declarative knowledge, nor would trying to teach that
</em><br>
<em>&gt; corpus prove
</em><br>
<em>&gt; very useful.
</em><br>
<p>The corpus of human declarative knowledge is useful to an AGI for two
<br>
reasons
<br>
<p>a) directly, it's valuable knowledge
<br>
<p>b) it gives a context in which far MORE valuable abstract thought-patterns
<br>
can be transmitted from humans to the AGI
<br>
<p><p><em>&gt;  &gt; Maybe so.  I know you think Novamente's learning methods are too
</em><br>
<em>&gt; &gt; weak, though you have not explained why to me in detail, nor have you
</em><br>
<em>&gt; &gt; proposed any concrete alternatives.  However, I think that *culture and
</em><br>
<em>&gt; &gt; social interaction* help us humans to grow from babies into mature adult
</em><br>
<em>&gt; &gt; minds in spite of the weaknesses of our learning methods,
</em><br>
<em>&gt;
</em><br>
<em>&gt; Because humans have evolved to rely on culture and social
</em><br>
<em>&gt; interaction does
</em><br>
<em>&gt; not mean that an AI must do so.
</em><br>
<p>I agree, it does not mean that an AI *must* do so.  However, I hypothesize
<br>
that to allow an AI to learn its initial thought-patterns from humans based
<br>
on experiential interaction, is
<br>
<p>a) the fastest way to get to an AGI
<br>
<p>b) the best way to get an AGI that has a basic empathy for humans
<br>
<p><em>&gt; From an AI's-eye-view, the &quot;humans&quot; are
</em><br>
<em>&gt; external blackbox objects that pose problems which, when the AI
</em><br>
<em>&gt; solves them,
</em><br>
<em>&gt; turns out to lead to the acquisition of reusable reflective skills.  (At
</em><br>
<em>&gt; least, that's what happens if the humans are doing it right.)
</em><br>
<p>I think this is
<br>
<p>a) a less efficient way to train a mind than cooperating with it in a shared
<br>
environment [due to the fact that the latter allows more
<br>
abstract-thought-pattern transfer]
<br>
<p>b) a route much less likely to lead to a Friendly AI.  An AGI that has
<br>
learned to approach context and problems by doing so cooperatively with
<br>
humans, and has hence absorbed human thought-patterns, is a lot more likely
<br>
to have a real empathy for humans as it reaches the transcension phase.
<br>
<p><em>&gt;  &gt; and I think that
</em><br>
<em>&gt; &gt; these same things can probably help a baby AGI to grow from a piece of
</em><br>
<em>&gt; &gt; software into a mature AGI capable of directing its activities
</em><br>
<em>&gt; in a useful
</em><br>
<em>&gt; &gt; way and solving hard problems.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't think the software of a baby AGI will much resemble the
</em><br>
<em>&gt; software of
</em><br>
<em>&gt; a mature AGI, and I say &quot;AGI&quot;, not &quot;seed AI&quot;.
</em><br>
<p>Yes, you see more &quot;code self-modification&quot; occurring at the
<br>
&quot;pre-human-level-AI&quot; phase than I do.
<br>
<p>This is because I see &quot;intelligent goal-directed code self-modification&quot; as
<br>
being a very hard problem, harder than mastering human language, for
<br>
example.
<br>
<p><em>&gt;  &gt; And if
</em><br>
<em>&gt; &gt; we do get it started with our teaching &amp; our knowledge, then when it
</em><br>
<em>&gt; &gt; outstrips us, it will face a new set of challenges.  I'm sure it will be
</em><br>
<em>&gt; &gt; able to meet these challenges, but how fast?  I don't know, and
</em><br>
<em>&gt; neither do
</em><br>
<em>&gt; &gt; you!
</em><br>
<em>&gt;
</em><br>
<em>&gt; And this &quot;I don't know&quot; is used as an argument for it happening at
</em><br>
<em>&gt; humanscale speeds, or in a volume of uncertainty centered on
</em><br>
<em>&gt; humanscale speeds?
</em><br>
<p>I don't have a strong argument that the transition from human-level to
<br>
vastly superhuman level intelligence will take years rather than weeks.
<br>
<p>I consider &quot;humanscale speed&quot; as a likely upper bound (though not a definite
<br>
upper bound).
<br>
<p>I don't feel you have a strong argument that the transition will be vastly
<br>
faster than this upper bound suggests, though.
<br>
<p>Your argument was that &quot;there's nothing special about human level
<br>
intelligence.&quot;  I sought to refute that argument by pointing out that, to
<br>
the extent an AGI is taught by humans, there is something special about
<br>
human level intelligence after all.  Then you countered that, in your
<br>
envisioned approach to AI, teaching by humans plays a smaller role than in
<br>
my own envisioned approach.  And indeed, this suggests that if seed AI were
<br>
achieved first by your approach rather than mine, the gap between human
<br>
level and vastly superhuman level intelligence would be less.
<br>
<p><p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4210.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4208.html">Eliezer S. Yudkowsky: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4194.html">Eliezer S. Yudkowsky: "Seed AI (was: How hard a Singularity?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4212.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4212.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4214.html">Michael Roy Ames: "Re: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4219.html">Eliezer S. Yudkowsky: "Re: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4235.html">James Higgins: "RE: Seed AI (was: How hard a Singularity?)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4209">[ date ]</a>
<a href="index.html#4209">[ thread ]</a>
<a href="subject.html#4209">[ subject ]</a>
<a href="author.html#4209">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
