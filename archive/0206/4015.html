<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: How Kurzweil lost the Singularity</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="How Kurzweil lost the Singularity">
<meta name="Date" content="2002-06-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>How Kurzweil lost the Singularity</h1>
<!-- received="Sat Jun 15 12:40:38 2002" -->
<!-- isoreceived="20020615184038" -->
<!-- sent="Sat, 15 Jun 2002 12:39:49 -0400" -->
<!-- isosent="20020615163949" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="How Kurzweil lost the Singularity" -->
<!-- id="3D0B6DD5.4020202@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJIELGCKAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20How%20Kurzweil%20lost%20the%20Singularity"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Jun 15 2002 - 10:39:49 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4016.html">Ben Goertzel: "RE: How Kurzweil lost the Singularity"</a>
<li><strong>Previous message:</strong> <a href="4014.html">Tony Garnock-Jones: "Re: A CodeDOM-Aware Generative IDE"</a>
<li><strong>In reply to:</strong> <a href="3985.html">Ben Goertzel: "RE: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4016.html">Ben Goertzel: "RE: How Kurzweil lost the Singularity"</a>
<li><strong>Reply:</strong> <a href="4016.html">Ben Goertzel: "RE: How Kurzweil lost the Singularity"</a>
<li><strong>Reply:</strong> <a href="4018.html">Michael Roy Ames: "Re: How Kurzweil lost the Singularity"</a>
<li><strong>Reply:</strong> <a href="4032.html">Eugen Leitl: "Re: How Kurzweil lost the Singularity"</a>
<li><strong>Maybe reply:</strong> <a href="4111.html">Eliezer S. Yudkowsky: "Re: How Kurzweil lost the Singularity"</a>
<li><strong>Maybe reply:</strong> <a href="4117.html">Michael Anissimov: "Re: How Kurzweil lost the Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4015">[ date ]</a>
<a href="index.html#4015">[ thread ]</a>
<a href="subject.html#4015">[ subject ]</a>
<a href="author.html#4015">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em> &gt;
</em><br>
<em> &gt; Kurzweil, however, IS putting effort into helping people understand the
</em><br>
<em> &gt; Singularity.
</em><br>
<em> &gt;
</em><br>
<em> &gt; And I'm sure that part of his motivation for doing this, is a desire to
</em><br>
<em> &gt; nudge the Singularity in a better direction.  A direction not too thoroughly
</em><br>
<em> &gt; polluted by peoples' fear and uncomprehension.
</em><br>
<p>Ben, to the best of my ability to understand it, Kurzweil's entire *being*
<br>
is directed toward predicting the Singularity - *not* nudging the
<br>
Singularity in any direction.  On every occasion in which I have spoken to
<br>
Kurzweil, the concept of influencing the Singularity in any way is met with
<br>
blank incomprehension.  As far as Kurzweil is concerned, he wins the
<br>
argument when he convinces the audience that the Singularity will happen.
<br>
<p>Any conceptual model of the Singularity that allows for individual actions
<br>
to accelerate or influence the Singularity is seen by Kurzweil as a weakness
<br>
in the argument, because it appears to argue that &quot;the Singularity requires
<br>
individuals to do such-and-such.&quot;  Kurzweil will always argue for the
<br>
creation of AI based on neuroanatomical modeling of all cortical areas, and
<br>
will never admit that a general understanding of intelligence is necessary
<br>
or even that it could speed up the process, because in the current
<br>
scientific environment it is easier for Kurzweil to defend the proposition
<br>
that neurocomputational modeling is possible than it is for Kurzweil to
<br>
defend the proposition that an understanding of intelligence is possible.
<br>
As for the idea that &quot;We can do this using neurocomputational modeling, and
<br>
therefore the Singularity is provably possible, but an understanding of
<br>
intelligence may allow us to build AI earlier without reverse-engineering
<br>
the brain&quot; - why, that's too complex for Kurzweil to explain on television.
<br>
&nbsp;&nbsp;&nbsp;So it doesn't get said.  It doesn't get defended.  Ever.  It's easier for
<br>
Kurzweil to present a model of the Singularity in which *only*
<br>
reverse-engineering plays a role, and so his thoughts appear to have
<br>
conformed to the worldview that will let him win arguments in the current
<br>
memetic environment.
<br>
<p>Kurzweil has, deliberately or inadvertantly, accepted constraints upon his
<br>
thinking which prohibit his model from corresponding to reality, and which
<br>
prohibit him from accepting any role for individual action in the Singularity.
<br>
<p>Kurzweil believes in the inevitability of his curves, not in activism.
<br>
Kurzweil wants to believe in the benevolence and inevitability of the
<br>
Singularity and any argument of the form &quot;You can do X and it will improve
<br>
your chances of (a Singularity) / (a positive Singularity)&quot; appears to him
<br>
to be a vulnerability in his argument:  &quot;The Singularity *could* (go wrong)
<br>
/ (not happen) if not-X.&quot;  Kurzweil will therefore argue against it.
<br>
Kurzweil's entire worldview prohibits the possibility of Singularity activism.
<br>
<p>In fact, having watched Kurzweil debate Vinge, I've come to the conclusion
<br>
that Kurzweil's worldview prohibits Kurzweil from arriving at any real
<br>
understanding of the basic nature of the Singularity.  Over the course of my
<br>
personal interaction with Kurzweil, I've seen him say two really bizarre
<br>
things.  One was during the recent chat with Vinge, when Kurzweil predicted
<br>
superhuman AI intelligence in 2029, followed shortly thereafter by the
<br>
statement that the Singularity &quot;would not begin to tear the fabric of human
<br>
understanding until 2040&quot;.  The second really bizarre thing I've heard
<br>
Kurzweil say was at his SIG at the recent Foresight Gathering, when I asked
<br>
why AIs thinking at million-to-one speeds wouldn't speed up the development
<br>
of technology, and he said &quot;Well, that's another reason to expect Moore's
<br>
Law to remain on course.&quot;
<br>
<p>These statements are so absolutely bizarre that, after pondering what
<br>
Kurzweil could have been thinking, I've come to the conclusion that what
<br>
Kurzweil calls the &quot;Singularity&quot; is what we would call &quot;the ordinary
<br>
progress of technology.&quot;  In Kurzweil's world, the Grinding Gears of
<br>
Industry churn out AI, superhuman AI, uploading, brain-computer interfaces
<br>
and so on, but these developments do not affect the nature of technological
<br>
progress except insofar as they help to maintain Kurzweil's curves *exactly
<br>
on track*.  What we, and Vinge, call the &quot;Singularity&quot; are the events that
<br>
grow out of transhuman intelligence however and wherever it arises; industry
<br>
is of interest to us only insofar as it leads up to that point.  What
<br>
Kurzweil calls the &quot;Singularity&quot; is the inevitable, inexorable, and entirely
<br>
ordinary progress of technology, which, in Kurzweil's world, *causes*
<br>
developments such as transhumanity, but is not *changed* by transhumanity
<br>
except in the same ways that industry has been changed by previous
<br>
technological developments.
<br>
<p>What Kurzweil is selling, under the brand name of the &quot;Singularity&quot;, is the
<br>
idea that technological progress will continue to go on exactly as it has
<br>
done over the last century, and that the inexorable grinding of the gears of
<br>
industry will eventually churn out luxuries such as superintelligent AIs,
<br>
brain-computer interfaces, inloading, uploading, transhuman servants, and so
<br>
on.  The gears of industry will then continue grinding at exactly the same
<br>
pace, producing more and better superintelligent AIs, more and better
<br>
transhumans, and so on.  Kurzweil's timeline for Moore's Law continues
<br>
unblinkingly from &quot;Human-equivalent brainpower costs $1000&quot; to &quot;1000
<br>
brainpower costs $1000&quot; a decade later.  Kurzweil is not defending what we
<br>
would regard as the Singularity; he is defending the idea of ordinary
<br>
technological progress.  As part of defending the inevitability and
<br>
desirability of the Turning Gears of Industry, Kurzweil also defends the
<br>
idea that the Gears of Industry will churn out transhuman technologies, and
<br>
the idea that the transhuman technologies churned out by the Gears of
<br>
Industry are safe, desirable luxuries.  It so happens that one of the
<br>
branches of Kurzweil's worldview - the production of transhuman intelligence 
<br>
- is known to us as the &quot;Singularity&quot;.  But Kurzweil's worldview does not 
<br>
contain any of our beliefs about the consequences and nature of transhuman 
<br>
intelligence.
<br>
<p>On the whole, Kurzweil's actions are probably a net benefit to the
<br>
Singularity.  Kurzweil is promoting a safe, sanitized, comparatively
<br>
unalarming, optimized-for-defensibility meme, under the brand name of
<br>
&quot;Singularity&quot;, which bears a surface resemblance to the real concept of the
<br>
Singularity as created by Vernor Vinge and preserved here.  People who
<br>
become interested in Kurzweil's pseudo-Singularity may go on to google on
<br>
&quot;Singularity&quot; and subsequently end up at the Singularity Institute.  People
<br>
who learn to love transhumanity as a consequence of the Inexorable Gears of
<br>
Industry may choose to take on transhumanity as a personal goal.  But:
<br>
<p>1)  Kurzweil's positive effects on the Singularity are an accident.  Unless
<br>
he is being deliberately dishonest, the positive consequences of his actions
<br>
are unintended consequences.
<br>
<p>2)  Despite his much greater potential to make a difference, it currently
<br>
seems that Kurzweil will go on playing the role of a celebrity spokesperson,
<br>
nothing more.  His outlook prohibits him from seeing the possibility of 
<br>
influencing the Singularity in any way.
<br>
<p>3)  Kurzweil's model is wrong enough that I cannot ethically help spread it.
<br>
&nbsp;&nbsp;&nbsp;Kurzweil is providing a safe, sanitized, easily digestible view of
<br>
something that is *not* ordinary.  He is not being dishonest, but it would
<br>
be dishonest for *me* to help spread ideas that I know to be attractive but
<br>
untrue.
<br>
<p>At present Kurzweil is neither using his resources to accelerate the
<br>
Singularity (in his capacity as an entrepreneur), nor even urging others to
<br>
do so (in his capacity as an author).  I therefore question whether we
<br>
should be lined up around the block to congratulate Kurzweil on his
<br>
altruism, until he either (a) calls in his next book for college students to
<br>
enter Singularity-related professions or (b) throws a few bucks the way of
<br>
neurocomputational modeling research.  Right now Kurzweil appears to be a
<br>
man with an idea that he believes is true.  So he writes books about it,
<br>
speaks publicly about it, uses his celebrity status to promote it, and in
<br>
turn gains greater prestige and celebrity status as the idea comes to be
<br>
associated with him.  In this, Kurzweil is no different from anyone else
<br>
with an idea.  This does not make Kurzweil a bad person, but it doesn't make
<br>
him Gandhi either.  And it does not mean that Kurzweil is out to accelerate
<br>
or improve the Singularity, either directly or indirectly.
<br>
<p>We are people with a cause, and our cause bears a vague resemblance to
<br>
Kurzweil's idea, but we would be in error to try and see Kurzweil as a man
<br>
with a cause.  Currently, Kurzweil is a man with an idea.  I wish I knew how
<br>
to nudge people with ideas into becoming people with causes.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4016.html">Ben Goertzel: "RE: How Kurzweil lost the Singularity"</a>
<li><strong>Previous message:</strong> <a href="4014.html">Tony Garnock-Jones: "Re: A CodeDOM-Aware Generative IDE"</a>
<li><strong>In reply to:</strong> <a href="3985.html">Ben Goertzel: "RE: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4016.html">Ben Goertzel: "RE: How Kurzweil lost the Singularity"</a>
<li><strong>Reply:</strong> <a href="4016.html">Ben Goertzel: "RE: How Kurzweil lost the Singularity"</a>
<li><strong>Reply:</strong> <a href="4018.html">Michael Roy Ames: "Re: How Kurzweil lost the Singularity"</a>
<li><strong>Reply:</strong> <a href="4032.html">Eugen Leitl: "Re: How Kurzweil lost the Singularity"</a>
<li><strong>Maybe reply:</strong> <a href="4111.html">Eliezer S. Yudkowsky: "Re: How Kurzweil lost the Singularity"</a>
<li><strong>Maybe reply:</strong> <a href="4117.html">Michael Anissimov: "Re: How Kurzweil lost the Singularity"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4015">[ date ]</a>
<a href="index.html#4015">[ thread ]</a>
<a href="subject.html#4015">[ subject ]</a>
<a href="author.html#4015">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
