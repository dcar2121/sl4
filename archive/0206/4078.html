<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Threats to the Singularity.</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Threats to the Singularity.">
<meta name="Date" content="2002-06-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Threats to the Singularity.</h1>
<!-- received="Mon Jun 17 23:07:17 2002" -->
<!-- isoreceived="20020618050717" -->
<!-- sent="Mon, 17 Jun 2002 19:54:01 -0700" -->
<!-- isosent="20020618025401" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Threats to the Singularity." -->
<!-- id="3D0EA0C9.4080904@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="60337174-8259-11D6-94B0-000A27B4DEFC@rbisland.cx" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Threats%20to%20the%20Singularity."><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Mon Jun 17 2002 - 20:54:01 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4079.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4077.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4077.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4079.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4079.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4078">[ date ]</a>
<a href="index.html#4078">[ thread ]</a>
<a href="subject.html#4078">[ subject ]</a>
<a href="author.html#4078">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Gordon Worley wrote:
<br>
<p><em>&gt; 
</em><br>
<em>&gt; On Monday, June 17, 2002, at 06:51  PM, Samantha Atkins wrote:
</em><br>
<p><em>&gt; My core looks something like this.  I want to make the universe a better 
</em><br>
<em>&gt; place.  A better place to live.  A place that solves new, interesting 
</em><br>
<em>&gt; problems.  A place that I'd like to stay, but wouldn't want to visit.
</em><br>
<em>&gt;
</em><br>
<p><p>Thanks for offering it.  Now, if we could just get a bit more 
<br>
handle on what you/we mean by &quot;hetter&quot; we could move right along.
<br>
<p>&nbsp;
<br>
<em>&gt; I'd like for this to include me in it, but if it turns out that the 
</em><br>
<em>&gt; universe can't be better so long as I'm still in it, then I'll get out.  
</em><br>
<em>&gt; I think the &quot;yuck factor&quot; in this is that I think the same way about 
</em><br>
<em>&gt; everything.  If you're making the universe a worse place, I don't really 
</em><br>
<em>&gt; want you in it.  I hope that getting &quot;you&quot; out of it only involves 
</em><br>
<em>&gt; convincing you not to do whatever it is that is making the universe worse.
</em><br>
<em>&gt; 
</em><br>
<p><p>Doesn't this assume that you and perhaps other entities are 
<br>
relatively immutable over time no matter what your wishes?  I 
<br>
don't consider that a particularly tenable notion assuming an 
<br>
ability to upload and/or continuously augment, self-examine and 
<br>
change.  I would also challenge any being to conclusively prove 
<br>
another being both of more harm than good to the universe AND 
<br>
utterly incapable of ever changing.
<br>
<p><p><em>&gt; I think this seems yucky because this sounds just like the kind of thing 
</em><br>
<em>&gt; Hitler would say.  The difference is that I have compassion for all 
</em><br>
<em>&gt; life.  I want to see the universe better and would like for that to 
</em><br>
<em>&gt; include everyone and everything in it.  However, if all attempts at this 
</em><br>
<em>&gt; proves impossible, I'm not going to say &quot;well, okay, I guess the 
</em><br>
<em>&gt; universe is just going to suck&quot;, but &quot;okay, let's see what the limiting 
</em><br>
<em>&gt; factors are and what we have to do to get around them&quot;.
</em><br>
<em>&gt; 
</em><br>
<p><p>I don't believe in an &quot;improved universe&quot; by eradicating 
<br>
sentients that seem problematic except in very very limited 
<br>
circumstances where an entity is capable of such destruction not 
<br>
stoppable otherwise as to force the decision.  I don't consider 
<br>
not being as productive as somme might like to be a reasonable 
<br>
criteria.  I don't consider not being as rational or intelligent 
<br>
such as criteria either.  If your goal is maximizing life then I 
<br>
don't think you see these as criteria for extermination either.
<br>
<p><p><em>&gt;&gt;&gt;&gt; Whether we transform or simply cease to exist seems to me to be a 
</em><br>
<em>&gt;&gt;&gt;&gt; perfectly rational thing to be a bit concerned about.  Do you see it 
</em><br>
<em>&gt;&gt;&gt;&gt; otherwise?
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; Sure, you should be concerned.  I think that the vast majority of 
</em><br>
<em>&gt;&gt;&gt; humans, uploaded or not, have something positive to contribute, 
</em><br>
<em>&gt;&gt;&gt; however small.  It'd be great to see life get even better post 
</em><br>
<em>&gt;&gt;&gt; Singularity, with everyone doing new and interesting good things.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Then we shouldn't shoot for any less, right?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Right!
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; On what basis will you judge what is rational?  In terms of what 
</em><br>
<em>&gt;&gt; supergoals, if you will?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think that I answered this above:  making the universe better.
</em><br>
<em>&gt; 
</em><br>
<p><p>Fair enough.
<br>
<p><p><em>&gt;&gt;&gt;&gt;&gt; Some of us, myself included, see the creation of SI as important 
</em><br>
<em>&gt;&gt;&gt;&gt;&gt; enough to be more important than humanity's continuation.  Human 
</em><br>
<em>&gt;&gt;&gt;&gt;&gt; beings, being
</em><br>
<em>&gt;&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;&gt; How do you come to this conclusion?  What makes the SI worth more 
</em><br>
<em>&gt;&gt;&gt;&gt; than all of humanity?  That it can outperform them on some types of 
</em><br>
<em>&gt;&gt;&gt;&gt; computation?  Is computational complexity and speed the sole measure 
</em><br>
<em>&gt;&gt;&gt;&gt; of whether sentient beings have the right to continued existence?  
</em><br>
<em>&gt;&gt;&gt;&gt; Can you really give a moral justification or a rational one for this?
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; In many ways, humans are just over the threshold of intelligence.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Whose threshold?  By what standards?  Established and verified as the 
</em><br>
<em>&gt;&gt; standards of value how?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You're asking for a definition of intelligence.  Though question!
</em><br>
<em>&gt; 
</em><br>
<p><p>A tougher one is why intelligence is on top of your value stack 
<br>
as the measure of the worthiness of various beings to exist and 
<br>
as the principle measure of &quot;better&quot;.
<br>
<p><p><em>&gt; One way of looking at intelligence is the ability to solve interesting 
</em><br>
<em>&gt; problems.  Ants solve some mildly interesting problems, apes and 
</em><br>
<em>&gt; dolphins solves slightly more interesting problems, humans solve yet 
</em><br>
<em>&gt; more interesting problems.
</em><br>
<p><p>So the measure of goodness or worth is how interesting a set of 
<br>
problems one can solve?  The idea is admirabe in its nerdiness 
<br>
but rather problematic if the question is one of right to life.
<br>
<p><em>&gt;  At any level of intelligence, though, all 
</em><br>
<em>&gt; the problems at the limits of solvability look interesting.  As great as 
</em><br>
<em>&gt; we think we are, we can already see that there are some interesting 
</em><br>
<em>&gt; problems out there that we can't find solutions to (like the halting 
</em><br>
<em>&gt; problem).
</em><br>
<p><p>The halting problem is provably unsolvable by any and all levels 
<br>
of intelligence.
<br>
<p><em>&gt; And, unless it turns out that intelligence doesn't scale very 
</em><br>
<em>&gt; well, the trend tells us that even more interesting questions are out 
</em><br>
<em>&gt; there for more intelligent minds to solve.  I doubt that anything will 
</em><br>
<em>&gt; ever be so intelligent that it will be able to solve every problem.
</em><br>
<p><p>Sure, but so?  Is this all there is?  Is it criteria enough for 
<br>
what is and is not of value?
<br>
<p><p><em>&gt;&gt;&gt; Compared to past humans we are pretty smart, but compared to the 
</em><br>
<em>&gt;&gt;&gt; estimated potentials for intelligence we are intellectual ants.  Despite
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; So we are to think less of ourselves because of estimated potentials?  
</em><br>
<em>&gt;&gt; Do we consider ourselves expendable because an SI comes into existence 
</em><br>
<em>&gt;&gt; that is a million times faster and more capable in the scope of its 
</em><br>
<em>&gt;&gt; creations, decision making and understanding?  This does not follow.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<p><p>I would add to the above that humans, individual humans, are not 
<br>
any smarter than they have been for the last few thousand years 
<br>
of recorded history.  Go read some of the classics from the 
<br>
Greek and Roman era if you have doubts of this.  Culturally we 
<br>
have gotten much more intellectualy efficient and much better at 
<br>
accumulating, storing and processing information.
<br>
<p><p><em>&gt; One should be humble, but not negative.  Being negative is just as 
</em><br>
<em>&gt; irrational as flattery.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Much has humans get to clear away ants if they're keeping the universe 
</em><br>
<em>&gt; from getting better, an SI could clear away some humans if they got in 
</em><br>
<em>&gt; the way.  If the SI is compassionate, ve will see that the humans are 
</em><br>
<em>&gt; doing some good and, being self aware, are able to change themselves to 
</em><br>
<em>&gt; do more good.  Unlike the humans who is unable to solve the ant problem 
</em><br>
<em>&gt; by any means other than getting the ants out of the way (be that killing 
</em><br>
<em>&gt; them or displacing them), an SI can solve the human problem by helping 
</em><br>
<em>&gt; the humans.
</em><br>
<em>&gt; 
</em><br>
<p><p>Ants, while they may be inconvenient at a picnic or marching 
<br>
across the kitchen, are not in the way fo the universe getting 
<br>
better.  People are likely to be even less so.  I agree of 
<br>
course that there are much better solutions in the case of 
<br>
humans than extermination.
<br>
<p><p>&nbsp;From your earlier post, at what point would you not battle for 
<br>
an SI in the process of being born?  Suppose you had super 
<br>
weapons capable of laying waste to entire nations of opposition. 
<br>
&nbsp;&nbsp;Would you use them?
<br>
<p><p><em>&gt; If some humans prove to be beyond help, though, I don't think it's 
</em><br>
<em>&gt; totally wrong to clear them out in some way.  Maybe that just means 
</em><br>
<em>&gt; letting them live in a simulation where they can kill their virtual 
</em><br>
<em>&gt; selves.  I'll leave the solution up to a much more intelligent SI.
</em><br>
<em>&gt; 
</em><br>
<p><p>Define &quot;beyond help&quot;.  I would support popping the ones that 
<br>
were too great a danger to self and others into a safety zone of 
<br>
some kind (VR or otherwise) until they learn better and/or can 
<br>
be cured in a way they are willing to undergo.  I don't think we 
<br>
should leave it up to the not-yet-existent SI now though.  It is 
<br>
these kinds of questions and the answer to them that will make 
<br>
the difference in the level of support and vilification.
<br>
<p><p><em>&gt;&gt;&gt; But, it's not nearly so simple.  All of us would probably agree that 
</em><br>
<em>&gt;&gt;&gt; given the choice between saving one of two lives, we would choose to 
</em><br>
<em>&gt;&gt;&gt; save the person who is most important to the completion of our goals, 
</em><br>
<em>&gt;&gt;&gt; be that reproduction, having fun, or creating the Singularity.  In 
</em><br>
<em>&gt;&gt;&gt; the same light, if a mob is about to come in to destroy the SI just 
</em><br>
<em>&gt;&gt;&gt; before it takes off and there is no way to stop them other than 
</em><br>
<em>&gt;&gt;&gt; killing them, you have on one hand the life of the SI that is already 
</em><br>
<em>&gt;&gt;&gt; more intelligent than the members of the mob and will continue to get 
</em><br>
<em>&gt;&gt;&gt; more intelligent, and on the other the life of 100 or so humans.  
</em><br>
<em>&gt;&gt;&gt; Given such a choice, I pick the SI.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; But that is not the context of the question.  The context is whether 
</em><br>
<em>&gt;&gt; the increased well-being and possibilities of existing sentients, 
</em><br>
<em>&gt;&gt; regardless of their relative current intelligence, is  a high and 
</em><br>
<em>&gt;&gt; central value.  If it is not then I hardly see how such an SI can be 
</em><br>
<em>&gt;&gt; described as &quot;Friendly&quot;.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; To a Friendly intelligence, this is important.
</em><br>
<em>&gt; 
</em><br>
<p><p>OK.  And that is what you wish to build, right?  :-)
<br>
<p><p><p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4079.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4077.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4077.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4079.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4079.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4078">[ date ]</a>
<a href="index.html#4078">[ thread ]</a>
<a href="subject.html#4078">[ subject ]</a>
<a href="author.html#4078">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
