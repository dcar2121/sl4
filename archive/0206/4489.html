<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Military Friendly AI</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Military Friendly AI">
<meta name="Date" content="2002-06-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Military Friendly AI</h1>
<!-- received="Fri Jun 28 07:13:53 2002" -->
<!-- isoreceived="20020628131353" -->
<!-- sent="Thu, 27 Jun 2002 19:58:06 -0600" -->
<!-- isosent="20020628015806" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Military Friendly AI" -->
<!-- id="LAEGJLOGJIOELPNIOOAJEEAHCMAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D1BB180.2549E5E1@posthuman.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Military%20Friendly%20AI"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Thu Jun 27 2002 - 19:58:06 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4490.html">Brian Atkins: "Re: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4488.html">Sam Kennedy: "RE: Digest format??"</a>
<li><strong>In reply to:</strong> <a href="4487.html">Brian Atkins: "Re: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4490.html">Brian Atkins: "Re: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4490.html">Brian Atkins: "Re: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4504.html">Mark Walker: "Friendly Existential Wager"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4489">[ date ]</a>
<a href="index.html#4489">[ thread ]</a>
<a href="subject.html#4489">[ subject ]</a>
<a href="author.html#4489">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Brian Atkins wrote:
<br>
<p><em>&gt; James Higgins wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I would tend to worry very little if Ben was about to kick off a
</em><br>
<em>&gt; &gt; Singularity attempt, but I would worry very much if you, Eliezer, were.
</em><br>
<em>&gt;
</em><br>
<em>&gt; That's quite odd since last I checked Ben wasn't even interested in
</em><br>
<em>&gt; the idea of Friendliness until we invented it and started pointing out
</em><br>
<em>&gt; to SL4 exactly how important it is.
</em><br>
<p>Quite the contrary, Brian. I -- like Pei Wang, Minsky, Peter Voss, and many
<br>
other AI researchers -- have been thinking about Friendliness for many
<br>
years.  Since Eliezer was in diapers -- and in Minsky's case, since before I
<br>
or Eliezer were born!  These are not new ideas.  The term &quot;Friendly AI&quot; is
<br>
due to Eli, so far as I know, but the concept certainly is not.
<br>
<p>Over the last 15 years, I have chosen to focus my research work, and my
<br>
writing, on the creation of real AI, rather than on the Friendliness aspect
<br>
specifically.  This is not because I consider Friendliness unimportant.  It
<br>
is, rather, because -- unlike Eliezer -- I think that we don't yet know
<br>
enough about AGI to make a really detailed, meaningful analysis of the
<br>
Friendly AI issue.  I think it's good to think about it now, but it's
<br>
premature to focus on it now.  I think we will be able to develop a real
<br>
theory of Friendly AI only after some experience playing around with
<br>
infrahuman AGI's that have a lot more general intelligence than any program
<br>
now existing.
<br>
<p>I believe my attitude toward Friendliness is typical of AGI researchers.
<br>
It's not that no one but Eliezer realizes the issue exists, or is
<br>
important -- it's not that he brought the issue to the AI community's
<br>
intention.  It's rather that he's nearly the only one who believes it's
<br>
possible to create a detailed theory of Friendliness *at this stage* prior
<br>
to the existence of infrahuman AGI's with a decent level of general
<br>
intelligence.
<br>
<p>Personally, I think he's largely wrong on this; I think that his theory of
<br>
Friendly AI is not all that valuable, and that it will look somewhat
<br>
oversimplistic and naive, in hindsight, when we reach the point of having a
<br>
powerful infrahuman AGI.
<br>
<p>The idea of self-modifying AI causing exponentially increasing intelligence
<br>
is also something AI researchers have been talking about for years -- Minsky
<br>
since the 70's or earlier.  What distinguishes Eliezer is not his
<br>
understanding of the long-term relevance of this issue, but the fact that
<br>
he's one of very few AI researchers who thinks that this issue is worth
<br>
paying a lot of attention to *now*.  Most AI researchers, rather, believe
<br>
that only once we have an infrahuman AGI with a lot of intelligence, does it
<br>
make sense to pay a lot of attention to intelligence-increasing
<br>
self-modification.
<br>
<p>Now, no one has proved they know how to construct an AGI.  It is possible
<br>
that Eliezer is correct that it makes sense to spend a lot of time on these
<br>
issues *now*, before we have a decent infrahuman AGI.  But it is not right
<br>
to claim that others don't understand these issues, or think they're
<br>
serious, just because they think the task of creating a decent &quot;real AI&quot;
<br>
should come temporally first.
<br>
<p>I note that, while Eli has been focusing on these topics, he has not made
<br>
all that much observable progress on actually creating AGI.  He has
<br>
performed a valuable service by bringing ideas like AI morality and AI
<br>
self-modification to a segment of the population that was not familiar with
<br>
them (mostly, members of the futurist community who are not AI researchers).
<br>
But by making this choice as to how to spend his time, he has chosen not to
<br>
progress as far on the AI design front as he could have otherwise.
<br>
<p><em>&gt; Not that it seems to have had much
</em><br>
<em>&gt; effect since he still has no plans that I know of to alter his rather
</em><br>
<em>&gt; dramatically risky seed AI experimentation protocol (basically not
</em><br>
<em>&gt; adding any Friendliness features until /after/ he decides that the
</em><br>
<em>&gt; AI has advanced enough) (he has a gut feel you see, and there's certainly
</em><br>
<em>&gt; no chance of a hard takeoff, and even if it did he's quite sure it would
</em><br>
<em>&gt; all turn out ok... trust him on it)
</em><br>
<p>I think that it is not possible to create a meaningful &quot;Friendly AI&quot; aspect
<br>
to Novamente at this stage.  I am skeptical that it's possible to create a
<br>
meaningful &quot;Friendly AI&quot; aspect to any AI architecture in advance, before
<br>
one has a good understanding of the characteristics of the AI in action.
<br>
<p>Perhaps someone will create an AI system that is sufficiently deterministic
<br>
that it would be possible to create an effective Friendliness component for
<br>
it in advance of seeing how the system works as a fairly intelligent
<br>
infrahuman AGI.  However, my intuition is that no system with this level of
<br>
determinism will be able to achieve a high level of general intelligence.
<br>
<p>I do trust my intuition that there is no chance of Novamente having a hard
<br>
takeoff right now.  The damn design is only about 20% implemented!  We will
<br>
know when we have a system that has some autonomous general intelligence,
<br>
and at that point we will start putting Friendliness-oriented controls in
<br>
the system.  Putting this sort of control into our system now would really
<br>
just be silly -- pure window dressing.
<br>
<p>You may say &quot;Yeah, Ben, but you can't absolutely KNOW the system won't
<br>
achieve a hard takeoff tomorrow.&quot;  No, I can't absolutely know that, and I
<br>
can't absolutely know that I'm not really a gerbil dreaming I'm an AI
<br>
scientist, either; nor that the universe won't spontaneously explode three
<br>
seconds from now.  But there's such a thing as common sense.  There are a
<br>
dozen other people who know the Novamente codebase, and every single one of
<br>
them would agree: there is NO chance of Novamente as it is now, incomplete,
<br>
achieving any kind of takeoff.  It does not have significantly more chance
<br>
of doing so right now than Microsoft Windows does.  I am sure that if Eli
<br>
saw the codebase as it now exists he would agree -- not that it's bad, it's
<br>
just very incomplete.
<br>
<p><p><em>&gt;I guess it is because we go to the effort to
</em><br>
<em>&gt; put our plans out for public review and he sits in with the rest of the
</em><br>
<em>&gt; crowd picking them apart. At least we _have_ plans out for public
</em><br>
<em>&gt; review.
</em><br>
<em>&gt;
</em><br>
<p>Eliezer has a much more detailed plan for AI friendliness than I do, but in
<br>
my view it's sort of a &quot;castle in the air,&quot; because it's based on certain
<br>
assumptions about how an AI will work, and Eliezer does not have a detailed
<br>
design (let alone an implementation) for an AI fulfilling these assumptions.
<br>
The whole theory may be meaningless, if it turns out it's not possible to
<br>
make (or even thoroughly design) an AGI meeting the assumptions of the
<br>
theory.
<br>
<p>I am working on a book on the Novamente AI design.  It's a long and hard
<br>
process; I've been spending about 50% of my time on it since October 2001.
<br>
When done, the book will be 750+ pages and full of math, diagrams, etc. etc.
<br>
The draft I circulated to a few readers, a couple months ago, was badly
<br>
flawed and is being enhanced, repaired and extended significantly (based on
<br>
the early readers' suggestions and complaints).  I expect this book to be
<br>
published in 2003.  This will include a description of the Novamente goal
<br>
system and a discussion of Friendliness from a Novamente point of view.
<br>
<p>For now, a 50-page high-level overview of the Novamente system is available
<br>
on the site www.realai.net (go to the &quot;Novamente AI Engine&quot; page).
<br>
<p>Also on that page you will find a link to an essay I wrote on &quot;AI Morality&quot;.
<br>
(Eliezer and some others pointed out some minor flaws in that paper, which I
<br>
have not yet found time to correct, but it still basically represents my
<br>
views.)  I do not give a detailed theory of Friendly AI comparable to
<br>
Eliezer's there, but I do explain generally how I expect AI morality to
<br>
work, and discuss some of the issues I have with Eliezer's ideas on Friendly
<br>
AI.  I stress that this is something I've thought about &quot;in the background&quot;
<br>
for a long time, but NOT something that has been a major focus of my work
<br>
lately, because of my believe that the right way to do Friendly AI will only
<br>
be determinable via substantial experimentation with early-stage infrahuman
<br>
AGI's.
<br>
<p><em>&gt; How about we set July for picking Ben's plan apart. After all he is far
</em><br>
<em>&gt; closer to completion (he claims) than anyone else, yet few people here
</em><br>
<em>&gt; seem to have anywhere near as good a grasp of his ideas compared to
</em><br>
<em>&gt; SIAI's.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Disclaimer: this post is not intended to start any kind of us vs. them
</em><br>
<em>&gt; phenomena. It exists simply to point out a perceived important difference
</em><br>
<em>&gt; in the amount of critical discussion regarding the two
</em><br>
<em>&gt; organizations' plans.
</em><br>
<p>Regarding picking my ideas on Friendly AI apart, that sounds like a fun
<br>
discussion!  However, I will be on vacation from July 1-11 (though I will
<br>
check e-mail occasionally); hence I suggest to postpone a long and detailed
<br>
thread on this until mid-July when I get back.
<br>
<p>Regarding picking the Novamente AI design apart, unfortunately a really
<br>
detailed thread on that will have to wait until sometime in 2003, when the
<br>
book comes out.  There is a lot of depth there, much more than most of the
<br>
readers of the first draft saw (due to the flaws of the first draft), and a
<br>
detailed discussion of the design among a group who doesn't *know* the
<br>
details of the design, is unlikely to be productive.
<br>
<p><p>Yours,
<br>
Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4490.html">Brian Atkins: "Re: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4488.html">Sam Kennedy: "RE: Digest format??"</a>
<li><strong>In reply to:</strong> <a href="4487.html">Brian Atkins: "Re: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4490.html">Brian Atkins: "Re: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4490.html">Brian Atkins: "Re: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4504.html">Mark Walker: "Friendly Existential Wager"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4489">[ date ]</a>
<a href="index.html#4489">[ thread ]</a>
<a href="subject.html#4489">[ subject ]</a>
<a href="author.html#4489">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
