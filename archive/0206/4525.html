<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Military Friendly AI</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Military Friendly AI">
<meta name="Date" content="2002-06-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Military Friendly AI</h1>
<!-- received="Fri Jun 28 20:41:55 2002" -->
<!-- isoreceived="20020629024155" -->
<!-- sent="Fri, 28 Jun 2002 18:32:22 -0600" -->
<!-- isosent="20020629003222" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Military Friendly AI" -->
<!-- id="LAEGJLOGJIOELPNIOOAJMECFCMAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="4.3.2.7.2.20020628110024.01cbe720@mail.earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Military%20Friendly%20AI"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Fri Jun 28 2002 - 18:32:22 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4526.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Previous message:</strong> <a href="4524.html">Ben Goertzel: "RE: Ben vs. Ben"</a>
<li><strong>In reply to:</strong> <a href="4501.html">James Higgins: "RE: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4529.html">James Higgins: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4529.html">James Higgins: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="../0207/4633.html">Gordon Worley: "Re: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4525">[ date ]</a>
<a href="index.html#4525">[ thread ]</a>
<a href="subject.html#4525">[ subject ]</a>
<a href="author.html#4525">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; This sounds like a realistic view of the problem.  The system does have
</em><br>
<em>&gt; some basic friendliness implemented.  But it isn't clear how best to
</em><br>
<em>&gt; implement the whole Friendliness systems and, thus, would be a waste of
</em><br>
<em>&gt; time at this point (since at best it is very likely be useless and, at
</em><br>
<em>&gt; worst, could bog down or throw off the entire system).
</em><br>
<em>&gt;
</em><br>
<em>&gt; The Friendliness goals are already in place, then?
</em><br>
<p>Webmind had a simple Friendliness goal system in place, though it was never
<br>
tested (and it wasn't explicitly called a Friendliness goal system, it was
<br>
just called a goal system, one component of which was a desire to please
<br>
humans (to put it in very crude terms))
<br>
<p>Novamente does not yet have a goal system at all, this will be implemented,
<br>
at my best guess, perhaps at the very end of 2002 or the start of 2003.
<br>
Currently we are just testing various cognitive and perceptual mechanisms,
<br>
and not yet experimenting with autonomous goal-directed behavior.
<br>
<p><em>&gt; &gt;So, I think we don't even know how to build a good failsafe mechanism for
</em><br>
<em>&gt; &gt;Novamente or any other AI yet.  We will only know that when we
</em><br>
<em>&gt; know how to
</em><br>
<em>&gt; &gt;measure the intelligence of an AGI effectively, and we will only
</em><br>
<em>&gt; know *this*
</em><br>
<em>&gt; &gt;based on experimentation with AGI's smarter than the ones we have now.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Well, getting a basic fail safe system in sooner should also help
</em><br>
<em>&gt; you learn
</em><br>
<em>&gt; how to do this better in the long-run.
</em><br>
<p>A failsafe mechanism has two parts
<br>
<p>1) a basic mechanism for halting the system and alerting appropriate people
<br>
when a &quot;rapid rate of intelligence increase&quot; is noted
<br>
<p>2) a mechanism for detecting a rapid rate of intelligence increase
<br>
<p>1 is easy; 2 is hard ... there are obvious things one can do, but since
<br>
we've never dealt with this kind of event before, it's entirely possible
<br>
that a &quot;deceptive intelligence increase&quot; could come upon us.  Measuring
<br>
general intelligence is tricky.
<br>
<p><p><em>&gt;  If anything even close to this looks
</em><br>
<em>&gt; likely you better be getting opinions of hundreds or thousands of
</em><br>
<em>&gt; relevant
</em><br>
<em>&gt; experts.  Or I'll come kick yer ass.  ;)  Seriously.
</em><br>
<p>Seriously -- this would be a tough situation.
<br>
<p>What if one of these thousands of relevant experts decides the system is so
<br>
dangerous that they have to destroy it -- and me.  What if they have allies
<br>
with the means to do so?
<br>
<p>Emotions may run very high regarding such a situation....
<br>
<p><em>&gt; What happens if you get Novamenta working as an AI, it is proven that
</em><br>
<em>&gt; Friendliness can not be guaranteed and it looks like your design is
</em><br>
<em>&gt; somewhat more risky that the ideal system.  Lets say your AI has a 4%
</em><br>
<em>&gt; chance (totally arbitrary, just for illustration) of turning out
</em><br>
<em>&gt; un-friendly if allowed to proceed.  And a group of responsible
</em><br>
<em>&gt; experts (not
</em><br>
<em>&gt; crack pots, not government appointed, not-self interested parties, etc)
</em><br>
<em>&gt; strongly believe a different design could lower the risk to 3%.  Lets say
</em><br>
<em>&gt; you'd have to scrap 70% of your code base and logic to implement
</em><br>
<em>&gt; the other
</em><br>
<em>&gt; design and it would take you several years to do this.
</em><br>
<em>&gt;
</em><br>
<em>&gt; What is the trade-off point between risk and time?
</em><br>
<p>My own judgment would be, in your scenario, to spend 3 more years
<br>
engineering to lower the risk to 3%
<br>
<p>However, I would probably judge NOT to spend 3 more years engineering to
<br>
lower the risk to 3.9% from 4%
<br>
<p>These are really just intuitive judgments though -- to make them rigorous
<br>
would require estimating too many hard to estimate factors.
<br>
<p>I don't think we're ever going to be able to estimate such things with that
<br>
degree of precision.  I think the decisions will be more like a 1% risk
<br>
versus a 5% risk versus a 15% risk, say.  And this sort of decision will be
<br>
easier to make...
<br>
<p><em>&gt; What if another team was further ahead on this other design than yours?
</em><br>
<p>It depends on the situation.  Of course, egoistic considerations of priority
<br>
are not a concern.  But there's no point in delaying the Novamente-induced
<br>
Singularity by 3 years to reduce risk from 4% to 3%, if in the interim some
<br>
other AI team is going to induce a Singularity with a 33.456% risk...
<br>
<p><em>&gt; Another good reason why morality should not be decided by a single
</em><br>
<em>&gt; individual.  Eliezer or Ben's morality may not allow death, thus severely
</em><br>
<em>&gt; going against Ben's wife's morals.  Ben's wife's morals, however,
</em><br>
<em>&gt; would not
</em><br>
<em>&gt; prevent any deaths, and thus would go strongly against Eliezer's
</em><br>
<em>&gt; and Ben's
</em><br>
<em>&gt; (and mine).  So maybe preventing deaths except where the individual does
</em><br>
<em>&gt; not want this protection is the best answer.  But it takes more than one
</em><br>
<em>&gt; viewpoint to even see this questions.
</em><br>
<p>In fact neither Eliezer nor I wishes to *force* immortality on anyone, via
<br>
uploading or medication or anything else.
<br>
<p>We do have a personal difference, in that Eliezer seems emotionally
<br>
disturbed by the fact that some people *want* to die at the end of their
<br>
natural lifespan, whereas it really doesn't bother me much.  Death is a
<br>
terrible thing, yet it adds a certain poignancy and spice to life, and in my
<br>
view the removal of death &quot;by natural causes&quot; is not an unalloyed plus --
<br>
though for me personally, the plusses outweigh the minuses, and for sure, I
<br>
plan on living as long as humanly or superhumanly possible, myself!!
<br>
<p>Interestingly, in many conversations over the years I have found that more
<br>
women want to die after their natural lifespan has ended, whereas more men
<br>
are psyched about eternal life.  I'm not sure if this anecdotal would hold
<br>
up statistically, but if so, it's interesting.  Adding some meat to the idea
<br>
that women are more connected to Nature, I guess... ;)
<br>
<p>-- ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4526.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Previous message:</strong> <a href="4524.html">Ben Goertzel: "RE: Ben vs. Ben"</a>
<li><strong>In reply to:</strong> <a href="4501.html">James Higgins: "RE: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4529.html">James Higgins: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4529.html">James Higgins: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="../0207/4633.html">Gordon Worley: "Re: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4525">[ date ]</a>
<a href="index.html#4525">[ thread ]</a>
<a href="subject.html#4525">[ subject ]</a>
<a href="author.html#4525">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
