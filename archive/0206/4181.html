<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Threats to the Singularity.</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Threats to the Singularity.">
<meta name="Date" content="2002-06-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Threats to the Singularity.</h1>
<!-- received="Sun Jun 23 05:29:03 2002" -->
<!-- isoreceived="20020623112903" -->
<!-- sent="Sat, 22 Jun 2002 21:10:27 -0700" -->
<!-- isosent="20020623041027" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Threats to the Singularity." -->
<!-- id="3D154A33.8050202@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJGECLCLAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Threats%20to%20the%20Singularity."><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Sat Jun 22 2002 - 22:10:27 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4182.html">Samantha Atkins: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4180.html">Michael Roy Ames: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4121.html">Ben Goertzel: "RE: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4184.html">Mike & Donna Deering: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4184.html">Mike & Donna Deering: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4189.html">Eugen Leitl: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4204.html">Ben Goertzel: "RE: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4181">[ date ]</a>
<a href="index.html#4181">[ thread ]</a>
<a href="subject.html#4181">[ subject ]</a>
<a href="author.html#4181">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi Ben,
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Let me be clear on one thing: I was not *advocating* indifference toward
</em><br>
<em>&gt; humanity in my post!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I was merely pointing out that indifference to humanity is one possible
</em><br>
<em>&gt; motive behind caring more about future superinteligent beings -- contempt
</em><br>
<em>&gt; (which you mentioned in the post to which I was replying) being a
</em><br>
<em>&gt; *different* motive.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; As you know, my best guess is that superhuman AI's will rapidly become
</em><br>
<em>&gt; relatively indifferent to humans -- not competing with us for resources
</em><br>
<em>&gt; significantly, nor trying to harm us, but mostly being bored with us and
</em><br>
<em>&gt; probably helping us out in offhanded ways.
</em><br>
<em>&gt; 
</em><br>
<p>That is a relief! :-)  Indifference of this kind from an SI is 
<br>
less worrisome as long as the SIs don't decide we are expendable 
<br>
&nbsp;&nbsp;&nbsp;if one of their goals seems aided by our demise.  However, if 
<br>
the SIs are to be of any help to our suviving the Singularity a 
<br>
bit more than indifference seems to be required.
<br>
<p>I was originally more concerned with human indifference to the 
<br>
fate of human beings, especially if those humans are the very 
<br>
ones working on creating such SIs.
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt;&gt;If one is for increasing intelligence (how one defines that and
</em><br>
<em>&gt;&gt;why it is the only or most primary value are good questions) and
</em><br>
<em>&gt;&gt;the increase of sentience, I fail to see how one can be cavalier
</em><br>
<em>&gt;&gt;about the destruction of all currently known sentients.  How can
</em><br>
<em>&gt;&gt;one stand for intelligence and yet not care about billions of
</em><br>
<em>&gt;&gt;intelligent beings that already exist?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; How can one care about life and yet accept the immense murder of ants that
</em><br>
<em>&gt; comes along with, say, digging the foundation for a new house?
</em><br>
<em>&gt;
</em><br>
<p>We are not ants and I am not talking about ants or arbitrary 
<br>
living things.  I am talking about human beings - sentient 
<br>
beings and of the kind we ourselves are.
<br>
<p><p><em>&gt; An advanced superhuman AI may become aware of 1000's of other types of
</em><br>
<em>&gt; life-forms or mind-forms that we cannot conceive of now.  From its point of
</em><br>
<em>&gt; view, then, how critical will we be?  From your point of view, as an upload
</em><br>
<em>&gt; with 1000x human intelligence and direct contact with these 1000's other
</em><br>
<em>&gt; life forms as well, how important will humanity be to &quot;YOU&quot;?  Do you pretend
</em><br>
<em>&gt; to know the answers to these questions?
</em><br>
<em>&gt;
</em><br>
<p>I hope that all sentients will be critical to an SI. To myself 
<br>
as an upload un-uploaded humans will be of immense value as 
<br>
their well-being and increased quality and quantity of life is, 
<br>
after all, one of strongest motives for seeking increased 
<br>
abilities and powers in the first place or to participate in the 
<br>
creation of beings that have such great capabilities.  I am not 
<br>
pretending when I state an answer that is at the heart of my own 
<br>
values and goals.
<br>
<p>Also, none of us can answer but from where we are right now.  If 
<br>
the answer is that human beings are seen as expendable, even 
<br>
while we are ourselves fully human, then I think that needs to 
<br>
be examined and questioned carefully.
<br>
<p><em>&gt; which are very different attitudes.  I do not personally hold either
</em><br>
<em>&gt; attitude, but I can sympathize more with the &quot;indifference&quot; attitude --
</em><br>
<em>&gt; because, from the grand perspective, one relatively primitive intelligent
</em><br>
<em>&gt; species may not be all that important.
</em><br>
<em>&gt; 
</em><br>
<p>I don't see anything at all &quot;grand&quot; about such a perspective.
<br>
<p><em>&gt; What attitude do I take?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Personally I try (and occasionally succeed ;) to practice the two Buddhist
</em><br>
<em>&gt; virtues of compassion and nonattachment.    The combination of these is
</em><br>
<em>&gt; tricky to master, as in a shallow sense they may seem to contradict each
</em><br>
<em>&gt; other.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In the context of the present discussion, being compassionate toward humans
</em><br>
<em>&gt; means that one doesn't want them to suffer, and that one has respect for
</em><br>
<em>&gt; humans' right to continue even as more advanced beings come along.  And
</em><br>
<em>&gt; nonattachment means *simultaneously* with compassion, also understanding
</em><br>
<em>&gt; that the human race does not have some kind of intrinsic special value as
</em><br>
<em>&gt; compared to other forms of existence, intelligence and life -- it means
</em><br>
<em>&gt; moving beyond one's biologically-based attachment to one's own species.
</em><br>
<em>&gt; 
</em><br>
<p>I don't think my attitude is limited to human sentients.  It is 
<br>
not biologically based although everyone seems willing to 
<br>
continuously assert that it is.  I would also point out that the 
<br>
current form of a particular sentient and its current level of 
<br>
intelligence is not cast in stone and immutable for all time. 
<br>
Considering this it is even more difficult for me to dismiss any 
<br>
sentient as of no real value no matter how much more advanced I 
<br>
or other sentients may be or become.
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt;&gt;&gt;&gt;How about we just grow a lot more sane human beings instead of
</em><br>
<em>&gt;&gt;&gt;&gt;digging continuously for technological fixes that really aren't
</em><br>
<em>&gt;&gt;&gt;&gt;fixes to the too often cussedness of local sentients?
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think the right thing is for the human-race technological vanguard to
</em><br>
<em>&gt; simultaneously work on building artificial superintelligence, AND on
</em><br>
<em>&gt; creating better humans beings (genetic engineering, brain augmentation,
</em><br>
<em>&gt; etc.).
</em><br>
<em>&gt; 
</em><br>
<p>Ethics, morality, better social and economic systems...
<br>
<p><p><em>&gt; And in fact, this is what is happening.
</em><br>
<em>&gt;
</em><br>
<p>It is not at all clear to me that the non-hardware parts of the 
<br>
problem are being addressed much.
<br>
<p><em>&gt; 
</em><br>
<em>&gt;&gt;&gt;&gt;Replacing
</em><br>
<em>&gt;&gt;&gt;&gt;them with something that is faster and arguably smarter but may
</em><br>
<em>&gt;&gt;&gt;&gt;or may not be any more wise is not an answer.  Scrapping
</em><br>
<em>&gt;&gt;&gt;&gt;sentients is to be frowned upon even if you think you can and
</em><br>
<em>&gt;&gt;&gt;&gt;even do create sentients that are arguably better along some
</em><br>
<em>&gt;&gt;&gt;&gt;parameters.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes, our value systems agree on this point.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Like nearly all others on this list, I would like to see a future in which
</em><br>
<em>&gt; enhanced humans and superintelligent AI's coexist in harmony and with
</em><br>
<em>&gt; mutuall productive interactions.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I do reject the notion that preserving the human race is of *absolutely
</em><br>
<em>&gt; primary* importance, but according to my own ethics and aesthetics, it is
</em><br>
<em>&gt; certainly *highly* important.
</em><br>
<em>&gt; 
</em><br>
<p>Can you say under what conditons you would be willing to scrap 
<br>
or see the human race scrapped, not even transformed, but ended?
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt;&gt;&gt;&quot;Wisdom&quot; is a nebulous human concept that means different things to
</em><br>
<em>&gt;&gt;&gt;different people, and in different cultures.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;I can tell you what it isn't.  It isn't about writing off all
</em><br>
<em>&gt;&gt;existing sentients in favor of something you think can be but
</em><br>
<em>&gt;&gt;you have no idea what it will become.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I feel like you're attacking a straw man here, because no one on this list
</em><br>
<em>&gt; suggested &quot;writing off all existent sentients&quot;, did they?  I certainly
</em><br>
<em>&gt; didn't, far from it.
</em><br>
<em>&gt; 
</em><br>
<p>It has been suggested that humans don't particularly matter if 
<br>
we can build much more intelligent beings.  It has been 
<br>
suggested that concern with human beings is simply squemishness 
<br>
due to &quot;biological programming&quot;. I find suchs view short-sighted 
<br>
to say the least and extremely dangerous.  I am very glad to 
<br>
hear that you do not hold them.
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt;&gt;Human is the only
</em><br>
<em>&gt;&gt;sentient basis we have to reason from and in any event is what
</em><br>
<em>&gt;&gt;we ourselves are and we have no choice but to reason from that
</em><br>
<em>&gt;&gt;basis.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think it is possible to achieve some degree of nonattachment from one
</em><br>
<em>&gt; species, in terms of one's reasoning and one's value system.  Of course, one
</em><br>
<em>&gt; can never completely remove inferential and emotional bias from oneself --
</em><br>
<em>&gt; it's not even clear what this would mean!
</em><br>
<em>&gt; 
</em><br>
<p>I hardly see how to build a value system on the well-being of 
<br>
that which does not yet exist.
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4182.html">Samantha Atkins: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4180.html">Michael Roy Ames: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4121.html">Ben Goertzel: "RE: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4184.html">Mike & Donna Deering: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4184.html">Mike & Donna Deering: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4189.html">Eugen Leitl: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4204.html">Ben Goertzel: "RE: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4181">[ date ]</a>
<a href="index.html#4181">[ thread ]</a>
<a href="subject.html#4181">[ subject ]</a>
<a href="author.html#4181">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
