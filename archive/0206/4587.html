<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: FAI means no programmer-sensitive AI morality</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: FAI means no programmer-sensitive AI morality">
<meta name="Date" content="2002-06-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: FAI means no programmer-sensitive AI morality</h1>
<!-- received="Sun Jun 30 10:50:47 2002" -->
<!-- isoreceived="20020630165047" -->
<!-- sent="Sat, 29 Jun 2002 22:38:05 -0700" -->
<!-- isosent="20020630053805" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: FAI means no programmer-sensitive AI morality" -->
<!-- id="3D1E993D.3030003@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJKEDFCMAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20FAI%20means%20no%20programmer-sensitive%20AI%20morality"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Sat Jun 29 2002 - 23:38:05 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4588.html">Brian Atkins: "Re: Ben vs. Ben"</a>
<li><strong>Previous message:</strong> <a href="4586.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>In reply to:</strong> <a href="4554.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4589.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4589.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4587">[ date ]</a>
<a href="index.html#4587">[ thread ]</a>
<a href="subject.html#4587">[ subject ]</a>
<a href="author.html#4587">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt;&gt; The vast majority of religious
</em><br>
<em>&gt;&gt;people, especially what we would call &quot;fundamentalists&quot; and those outside
</em><br>
<em>&gt;&gt;the First World, adhere to a correspondence theory of the truth of their
</em><br>
<em>&gt;&gt;religion; when they say something is true, they mean that it is so; that
</em><br>
<em>&gt;&gt;outside reality corresponds to their belief.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't think you have it quite right.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What they mean is more nearly that *their experience corresponds to their
</em><br>
<em>&gt; beliefs*
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Modern scientific realism is based on accepting outside reality,
</em><br>
<em>&gt; observations of physical reality, as the fundamental determinant of truth.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; On the other hand, many other traditions are based on accepting *inner
</em><br>
<em>&gt; intuitions and experiences* as the fundamental determinants of truth.
</em><br>
<p><p>It is quite possible to accept both as determinants of truth, 
<br>
perhaps in different domains or aspects of a reality that is 
<br>
large enough to include both and more.   There is no reason one 
<br>
must be only in one camp or the other.  Or even look at it as 
<br>
two different and utterly incompatible camps.
<br>
<p>These inner intuitions and experiences are by no means just 
<br>
individual subjectivism with no commonality.  The writings and 
<br>
teachings of mystics across many traditions, cultures and times 
<br>
show a great deal of similarity below the particular cultural 
<br>
and relgiously imposed coloration of their experiences.  Now, it 
<br>
is certainly possible this similarity is one of similar brains 
<br>
reacting similarly under certain practices and tensions.
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt; It seems to me like you don't fully appreciate what it means for someone to
</em><br>
<em>&gt; have a truly non-rationalist, non-scientific point of view.  Probably this
</em><br>
<em>&gt; is because your life-course so far has not led you to spend significantly
</em><br>
<em>&gt; much time with such people.  Mine, as it happens, has.
</em><br>
<em>&gt;
</em><br>
<p>As above, a viewpoint that includes inner intutions and 
<br>
experiences is not necessarily non-rationalist except for very 
<br>
narrow definitions of the rational and is not necessarily 
<br>
non-scientific even if it is not limited to the strictly 
<br>
scientific.
<br>
<p><em>&gt; Take traditional Chinese medicine, or yoga, or Zen Buddhism, as examples.
</em><br>
<em>&gt; These are ancient traditions with a lot of depth and detail to them.  Their
</em><br>
<em>&gt; validity, such as it is, is primarily *experiential*.  It is largely not
</em><br>
<em>&gt; based on things that individuals outside the tradition in question can
</em><br>
<em>&gt; observe in empirical reality.  [Yeah, I know people have tried to test for
</em><br>
<em>&gt; enlightenment by studying brain waves and such (lots of work at Maharishi
</em><br>
<em>&gt; University on this), but this isn't what it's all about -- this is icing on
</em><br>
<em>&gt; the cake from the spiritual point of view.]
</em><br>
<em>&gt; 
</em><br>
<p>I am very familiar with yoga and have been involved in it at 
<br>
some level of practice for some time.  I have at least a reading 
<br>
acquantance with other traditions. I find yoga practices and 
<br>
philosophy excellent training of the mind and emotions and it 
<br>
gives me a centering that I cherish.  Does that make me less 
<br>
able to appreciate and participate in science and scientific 
<br>
dicussions and activities? No.
<br>
<p>I have also had various experiences in my life that I cannot put 
<br>
down to simply wacky brain chemistry or a mis-aligned &quot;God 
<br>
module&quot;.  I think explanations are quite possible for these 
<br>
states that include all of science but are not limited to it. In 
<br>
my opinion the very building of an SAI and its further 
<br>
development as well as our own transhuman development will make 
<br>
it perfectly clear how spirituality/science is not an either-or 
<br>
problem at all.
<br>
<p><p><em>&gt; When my wife for instance became interested in Zen, it wasn't because any
</em><br>
<em>&gt; kind of analysis of observations convinced her, it was because some things
</em><br>
<em>&gt; she read in a Zen book resonated with some experiences she'd already had...
</em><br>
<em>&gt; 
</em><br>
<p>Yes.  And many perfectly mundane wonderful aspects of life are 
<br>
also like that.
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt; Nitpicking about the definition of logic is not the point.  In Novamente we
</em><br>
<em>&gt; have a narrow technical definition of &quot;reasoning&quot; as opposed to other
</em><br>
<em>&gt; cognitive processes, and I can see that I've made the error in some posts of
</em><br>
<em>&gt; using this definition in nontechnical discussions, when in ordinary language
</em><br>
<em>&gt; &quot;reasoning&quot; means something broader.  Sorry about that.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But the point at hand is: many folks will be totally unconvinced by anything
</em><br>
<em>&gt; an intelligent, scientifically-minded AGI says -- just as they are
</em><br>
<em>&gt; unconvinced by your and my arguments that God probably didn't really create
</em><br>
<em>&gt; the world 6000 years ago, that there probably isn't really a Heaven into
</em><br>
<em>&gt; which only 144000 people will ever be accepted, etc.
</em><br>
<em>&gt;
</em><br>
<p>I doubt seriously it will be only &quot;scientifically-minded&quot; for 
<br>
very long. It is imho simply too narrow a filter to apprehend 
<br>
all of reality.  For sure neither God or anything else created 
<br>
the world 6000 years ago unless we are in a sim or VR with some 
<br>
very major pointless contradictions built-in and it started 
<br>
around then.  :-)
<br>
<p><em>&gt; 
</em><br>
<em>&gt; I do know that Zen Buddhism and yoga and Sufi-ist Islam are outside the
</em><br>
<em>&gt; correspondence theory of truth as you describe it, in the sense that they
</em><br>
<em>&gt; define truth more by correspondence with inner experience than by
</em><br>
<em>&gt; correspondence with physical reality.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Physical reality, according to these traditions, is an illusion.  Emotions
</em><br>
<em>&gt; are also illusions.  Only a certain kind of crystal-clear inner insight
</em><br>
<em>&gt; (yes, these words don't do it justice...) is to be &quot;trusted&quot; (though in a
</em><br>
<em>&gt; sense it's viewed as having a directness beyond trust/mistrust)..
</em><br>
<em>&gt;
</em><br>
<p>Yes, &quot;illusion&quot; sort of misses the mark also.  One way of 
<br>
approaching it that is more westernized, modern SF-ish is that 
<br>
we in fact are within a training/exploration/entertainment VR 
<br>
that we have gotten so caught up in we think it is the only 
<br>
reality.  Or perhaps we are really transhumans experiencing the 
<br>
&nbsp;&nbsp;point of view of pre-transhuman beings and their reaching for 
<br>
a state beyond their apparent limits through a variety of means. 
<br>
&nbsp;&nbsp;Only by learning to somewhat step beyond or outside (words are 
<br>
difficult for this) the apparent reality can we break the spell 
<br>
that keeps us limited.  Another view to play with is that our 
<br>
own transcendence requires letting go of what we think defines 
<br>
our limits and self.  We need to connect to wand work from a 
<br>
deeper core, perhaps project a different &quot;self&quot; we live from and 
<br>
into.
<br>
<p><em>&gt;&gt;Whether I could convince a
</em><br>
<em>&gt;&gt;rabbi of that in advance is a separate issue, but it does, in
</em><br>
<em>&gt;&gt;fact, happen
</em><br>
<em>&gt;&gt;to be true, and *that's* the important thing from the perspective of
</em><br>
<em>&gt;&gt;safeguarding the integrity of the Singularity, regardless of how it plays
</em><br>
<em>&gt;&gt;out in pre-Singularity politics.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So the important thing to you, is that the Singularity has integrity
</em><br>
<em>&gt; according to your scientific rationalist belief system.  Fine.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This doesn't mean the Singularity will have integrity according to the
</em><br>
<em>&gt; belief systems of the vast majority of people on the world.
</em><br>
<em>&gt;
</em><br>
<p>If it is &quot;any good&quot; at all I suspect that it will have integrity 
<br>
that may not be according to belief systems of almost any 
<br>
humans.  I think this is part of what Eliezer is getting at.  A 
<br>
provincial disagreement between scientific materialism and 
<br>
various religious/spiritual views may end up being utterly 
<br>
beside the point.  I very much hope so.
<br>
<p><p><em>&gt; I don't see how this doesn't constitute &quot;imposing your morality on the
</em><br>
<em>&gt; world.&quot;  In my view, it does.  What you're saying is basically that you want
</em><br>
<em>&gt; to ensure the Singularity is good according to your standards, where your
</em><br>
<em>&gt; standards have to do with a kind of rationalistic &quot;integrity&quot; that you (but
</em><br>
<em>&gt; not most others) see as extremely valuable.
</em><br>
<em>&gt;
</em><br>
<p>I don't see that at all.  I see him going out of his way to not 
<br>
impose his standards on what the outcome will be or even claim 
<br>
that he knows what the ultimate shape of it will be.  He 
<br>
actually says that it is impossible to impose his standards on 
<br>
the outcome even if he wanted to if this thing is a real AGI. 
<br>
That looks right to me.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; The AGI that I create is going to have a bias toward rationality and toward
</em><br>
<em>&gt; empiricism, because these are my values and those of the rest of the
</em><br>
<em>&gt; Novamente team.  Not an *absolutely bias*, but a bias.  When it's young, I'm
</em><br>
<em>&gt; going to teach it scientific knowledge *as probable though not definite
</em><br>
<em>&gt; truth*, and I'm going to show it the Koran as an example of an intersting
</em><br>
<em>&gt; human belief system.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Individuals who believe the scientific perspective is fundamentally wrong,
</em><br>
<em>&gt; might be offended by this, but that's just life....  I am not going to teach
</em><br>
<em>&gt; Novababy that the Koran and Torah and Vedas are just as valid as science,
</em><br>
<em>&gt; just in order to please others with these other belief systems.  Of course,
</em><br>
<em>&gt; I will also teach Novababy to think for itself, and once it becomes smarter
</em><br>
<em>&gt; than me (or maybe before) it will come to its own conclusions, directed by
</em><br>
<em>&gt; the initial conditions I've given it, but not constrained by them in any
</em><br>
<em>&gt; absolute sense.
</em><br>
<em>&gt;
</em><br>
<p>I certainly don't belief it is fundamentally wrong.  Science is 
<br>
perfectly valid and wonderful within its built-in applicable 
<br>
domain and limits.  I simply don't believe its boundaries are 
<br>
the end of the real.  Starting an SAI within those boundaries in 
<br>
no way keeps it from going beyond them if it finds it needful to 
<br>
do so.
<br>
<p>Sometimes, I look at SAI as a form of externalized jnana yoga. 
<br>
Science will be used to transcended views limited to only 
<br>
science - going fully into it in this way will transcend it and 
<br>
much else of the relgion/science and other current human 
<br>
tensions and reactive cultural knots. Time will tell if I am 
<br>
correct in this expectation.
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4588.html">Brian Atkins: "Re: Ben vs. Ben"</a>
<li><strong>Previous message:</strong> <a href="4586.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>In reply to:</strong> <a href="4554.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4589.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4589.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4587">[ date ]</a>
<a href="index.html#4587">[ thread ]</a>
<a href="subject.html#4587">[ subject ]</a>
<a href="author.html#4587">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
