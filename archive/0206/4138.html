<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: How hard a Singularity?</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: How hard a Singularity?">
<meta name="Date" content="2002-06-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: How hard a Singularity?</h1>
<!-- received="Sat Jun 22 14:46:24 2002" -->
<!-- isoreceived="20020622204624" -->
<!-- sent="Sat, 22 Jun 2002 12:50:03 -0600" -->
<!-- isosent="20020622185003" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: How hard a Singularity?" -->
<!-- id="LAEGJLOGJIOELPNIOOAJGEEICLAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D14B90E.2040002@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20How%20hard%20a%20Singularity?"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sat Jun 22 2002 - 12:50:03 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4139.html">Mike & Donna Deering: "Re: The Human Brain."</a>
<li><strong>Previous message:</strong> <a href="4137.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4136.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4140.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4140.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4165.html">James Higgins: "RE: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4138">[ date ]</a>
<a href="index.html#4138">[ thread ]</a>
<a href="subject.html#4138">[ subject ]</a>
<a href="author.html#4138">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
hi,
<br>
<p><em>&gt; &gt; I think the period of transition from human-level AI to
</em><br>
<em>&gt; superhuman-level AI
</em><br>
<em>&gt; &gt; will be a matter of months to years, not decades.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I suppose I could see a month, but anything longer than that is
</em><br>
<em>&gt; pretty hard
</em><br>
<em>&gt; to imagine unless the human-level AI is operating at a subjective
</em><br>
<em>&gt; slowdown
</em><br>
<em>&gt; of hundreds to one relative to human thought.
</em><br>
<p>I understand that this is your intuition, but what is the reasoning
<br>
underlying it?
<br>
<p>Say we have this AI mind with a nonhuman intelligence,  roughly as smart as
<br>
Ben or Eliezer.  Say this AI mind already uses a huge amount of
<br>
computational resources, and obtaining more rapidly is not financially
<br>
possible.
<br>
<p>This mind now has to re-engineer its software to make itself smarter.
<br>
<p>Maybe there are only a limited number of tweaks it can make to improve its
<br>
intelligence, without totally rearchitecting itself.
<br>
<p>So, with these tweaks, it becomes a bit smarter than Ben or Eliezer.
<br>
<p>OK, what's next?  It has to completely rearchitect itself, i.e. come up with
<br>
a new and better AI design.  Furthermore, it doesn't have that much hardware
<br>
available for experimentation, unless it wants to cannibalize its own
<br>
mind-hardware...
<br>
<p>Where do you come up with a &quot;one month upper bound&quot; for this rearchitecture
<br>
process?
<br>
<p>I think a one month estimate is plausible, but I don't see why &quot;anything
<br>
longer than that&quot; should be &quot;hard to imagine.&quot;
<br>
<p>Maybe it won't go this way -- maybe no conceptual/mathematical/AI-design
<br>
hurdles will be faced by a human-level AI seeking to make itself vastly
<br>
superhuman.  Or maybe turning a human-level mind into a vastly superhuman
<br>
mind will turn out to be a hard scientific problem, which takes our
<br>
human-level AI a nontrivial period of time to solve....
<br>
<p><em>&gt; &gt; Moravec-and-Kurzweil-style curve-plotting is interesting and
</em><br>
<em>&gt; important, but
</em><br>
<em>&gt; &gt; nevertheless, the problem of induction remains... .  All sorts of things
</em><br>
<em>&gt; &gt; could happen.  For instance, the superhuman AI's we build may
</em><br>
<em>&gt; continue to
</em><br>
<em>&gt; &gt; progress exponentially, but in directions other than those we
</em><br>
<em>&gt; foresee now.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Even if your goal is to progress exponentially in enlightened spiritual
</em><br>
<em>&gt; directions, exponential physical progress is still a good way to get the
</em><br>
<em>&gt; computing power to support that enlightened spiritual stuff and
</em><br>
<em>&gt; bring others
</em><br>
<em>&gt; in on the fun.
</em><br>
<p>Perhaps, or perhaps not.  Perhaps the super-AI will realize that more
<br>
brainpower and more knowledge are not the path to greater wisdom ... perhaps
<br>
it will decide it's more important to let some of its subprocesses run for a
<br>
few thousand years and see how they come out!
<br>
<p><em>&gt; &gt; In short, as I keep repeating, one of the unknown things about
</em><br>
<em>&gt; our coming
</em><br>
<em>&gt; &gt; plunge into the Great Unknown, is how rapidly the plunge will
</em><br>
<em>&gt; occur, and the
</em><br>
<em>&gt; &gt; trajectory that the plunge will follow.   Dead certainty on these points
</em><br>
<em>&gt; &gt; seems inappropriate to me.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I often encounter people who are amazed at my dead certainty that
</em><br>
<em>&gt; humanity
</em><br>
<em>&gt; evolved rather than being created.  Generic arguments against &quot;dead
</em><br>
<em>&gt; certainty&quot; are not relevant.
</em><br>
<p>It is your comment which is not relevant, dude -- because I was not making a
<br>
generic argument against dead certainty.
<br>
<p>I *could* make such an argument, but it's not the one I was making (it would
<br>
just get pedantic, because I know you don't really mean &quot;100% certain&quot;, you
<br>
understand that no knowledge is absolutely certain)
<br>
<p>I was making a specific argument against dead certainty *in the face of
<br>
minimal empirical evidence*, which is the case at hand
<br>
<p><em>&gt; If you like, don't think of me as being &quot;dead
</em><br>
<em>&gt; certain&quot; that
</em><br>
<em>&gt; the Singularity will be fast, just &quot;dead certain&quot; of the wrongness of the
</em><br>
<em>&gt; common reasons offered for why the Singularity would happen to run on a
</em><br>
<em>&gt; conveniently human timescale.
</em><br>
<p>I agree that many of the reasons commonly offered why the Singularity will
<br>
be slow, are poor reasons.
<br>
<p>And I also think it's very likely that at some point, superintelligent AI's
<br>
will progress tremendously faster than humans can comprehend.
<br>
<p>However, neither of these points gives me any knowledge about how long the
<br>
gap between human-level AI and vastly-superhuman-level AI will be.
<br>
<p>And nor do your posts, or intuitions, give me any knowledge about this.
<br>
<p>We don't yet fully understand how hard the scientific problem of creating a
<br>
human-level AI is.  And we don't yet fully understand how hard the
<br>
scientific problem of transforming a human-level AI into a vastly
<br>
superhuman-level AI is.  Until we understand these things, we can't forecast
<br>
the end-game of the path to the Singularity in any detail, though we can
<br>
certainly huff and puff about it a lot should we find such an occupation
<br>
entertaining...
<br>
<p><p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4139.html">Mike & Donna Deering: "Re: The Human Brain."</a>
<li><strong>Previous message:</strong> <a href="4137.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4136.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4140.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4140.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4165.html">James Higgins: "RE: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4138">[ date ]</a>
<a href="index.html#4138">[ thread ]</a>
<a href="subject.html#4138">[ subject ]</a>
<a href="author.html#4138">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
