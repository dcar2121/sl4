<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: How hard a Singularity?</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: How hard a Singularity?">
<meta name="Date" content="2002-06-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: How hard a Singularity?</h1>
<!-- received="Wed Jun 26 17:33:29 2002" -->
<!-- isoreceived="20020626233329" -->
<!-- sent="Wed, 26 Jun 2002 17:03:53 -0400" -->
<!-- isosent="20020626210353" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: How hard a Singularity?" -->
<!-- id="3D1A2C39.4050006@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="4.3.2.7.2.20020626132515.01c16430@mail.earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20How%20hard%20a%20Singularity?"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Jun 26 2002 - 15:03:53 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4396.html">Ben Goertzel: "FAI and SIAI as dangerous"</a>
<li><strong>Previous message:</strong> <a href="4394.html">James Higgins: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4394.html">James Higgins: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4397.html">James Higgins: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4397.html">James Higgins: "Re: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4395">[ date ]</a>
<a href="index.html#4395">[ thread ]</a>
<a href="subject.html#4395">[ subject ]</a>
<a href="author.html#4395">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
James Higgins wrote:
<br>
<em> &gt; At 03:59 PM 6/26/2002 -0400, Eliezer S. Yudkowsky wrote:
</em><br>
<em> &gt;
</em><br>
<em> &gt;&gt; James,
</em><br>
<em> &gt;&gt;
</em><br>
<em> &gt;&gt; Do you believe that a committee of experts could be assembled to
</em><br>
<em> &gt;&gt; successfully build an AI?  Or even to successfully judge which new AI
</em><br>
<em> &gt;&gt; theories are most likely to succeed?
</em><br>
<em> &gt;
</em><br>
<em> &gt; Do I believe a committee could successfully build an AI?  Maybe.  But I
</em><br>
<em> &gt; don't think it would be a good idea to do it that way.
</em><br>
<em> &gt;
</em><br>
<em> &gt;&gt; If not, why would they be able to do it for Friendly AI?
</em><br>
<em> &gt;
</em><br>
<em> &gt; I never said they could, or should, DESIGN anything.  Simply approve
</em><br>
<em> &gt; designs.
</em><br>
<p>Approving designs requires the ability to understand them.  Do you think a
<br>
committee could *approve* a working design for AI, if it had to pick one
<br>
proposal out of all those presented on the basis of which design proposal
<br>
had the best chance of simple success?  Why, then, should a committee do any
<br>
better at approving one Friendliness design?
<br>
<p>I think that handing something to a committee imposes an upper limit on the
<br>
intelligence of the resulting decisions.  Committees can be smart but they
<br>
cannot be geniuses.  If Friendly AI requires genius, then turning over the
<br>
problem to a committee guarantees failure, just as it would for the problem
<br>
of AI itself.
<br>
<p><em> &gt; Zoning committees don't build anything, but they are important to
</em><br>
<em> &gt; maintain order in a metropolitan area.  I believe a Singularity Committee
</em><br>
<em> &gt; (or whatever it should be called - I'd like to avoid the term
</em><br>
<em> &gt; &quot;committee&quot;)
</em><br>
<p>Yes, I'm sure that avoiding the name &quot;committee&quot; will completely prevent
<br>
committee organizational dynamics from operating.
<br>
<p><em> &gt; would be a very useful asset to the human race.  Although I can see where
</em><br>
<em> &gt; it could easily be seen as a detriment to will-full, single-minded, solo
</em><br>
<em> &gt; players or even like minded teams.
</em><br>
<em> &gt;
</em><br>
<em> &gt; Individual accomplishment is irrelevant in light of the Singularity,
</em><br>
<em> &gt; successful completion of the project in the safest manner possible is the
</em><br>
<em> &gt; only rational goal.
</em><br>
<p>I agree.
<br>
<p><em> &gt; I believe your goal, Eliezer, is to make the Singularity as friendly and
</em><br>
<em> &gt;  safe as possible, is it not?  If so you should welcome such a committee
</em><br>
<em> &gt;  as a way to ensure that the safest and most friendly design is the one
</em><br>
<em> &gt; launched.
</em><br>
<p>I should NOT welcome such a committee unless I believe the ACTUAL EFFECT of
<br>
such a committee will be to ensure that the safest and most friendly design
<br>
is launched.  Friendly AI design is not as complex as AI design but it is
<br>
still the second most complicated thing I have ever encountered in my life.
<br>
&nbsp;&nbsp;I would trust someone who built an AI to make it Friendly.  I would not
<br>
trust a committee to even understand what the real nature of the problem
<br>
was.  I would trust it to spend its whole time debating various versions of
<br>
Asimov Laws, never moving on the issue of structural Friendliness.
<br>
<p><em> &gt; You should under no circumstances fear such a committee since, if you
</em><br>
<em> &gt; really are destined to engineer the Singularity, the committee would
</em><br>
<em> &gt; certainly concede that your design was the best when it was presented to
</em><br>
<em> &gt; them.
</em><br>
<p>That's outright silly.  One, I don't think that destiny exists in our
<br>
universe, so I can't have one.  Two, there is no reason why a committee
<br>
would be capable of picking the best design when the problem is inherently
<br>
more complex than the intelligence of a committee permits.  The committee
<br>
will pick out a set of Asimov Laws designed by Marvin Minsky in accordance
<br>
with currently faddish AI principles.  If the committee has to build their
<br>
own AI, they'll pick a faddish design and fail.  I will not provide an AI
<br>
for them if they are not smart enough to build it themselves.
<br>
<p>The fact that, at this moment, it takes (I think) substantially more 
<br>
intelligence to *build* an AI, at all, than to build a Friendly AI, is one 
<br>
of the few advantages that humanity has in this - although Moore's Law is 
<br>
slowly but steadily eroding that advantage.  I have not and never will 
<br>
propose that SIAI (a 501(c)(3) nonprofit) be given supervisory capacity over 
<br>
the Friendliness efforts of other AI projects, regardless of whether future 
<br>
circumstances make this a plausible outcome.
<br>
<p>It is terribly dangerous to take away the job of Friendly AI from whoever 
<br>
was smart enough to crack the basic nature of intelligence!  Friendly AI is 
<br>
not as complex as AI but it is still the second hardest problem I have ever 
<br>
encountered.  A committee is not up to that!
<br>
<p><em> &gt;&gt; Sometimes committees are not very smart.  I fear them.
</em><br>
<em> &gt;
</em><br>
<em> &gt; I don't like committees either, and I can understand why you, in
</em><br>
<em> &gt; particular, would fear such a committee.  It would take away your ability
</em><br>
<em> &gt; to single handedly, permanently alter the fate of the human race.  Which
</em><br>
<em> &gt; is exactly why such a committee would be a good thing. Such decisions are
</em><br>
<em> &gt; too big for any one person to make.
</em><br>
<p>Then they're too big for N people to make and should be passed on to a
<br>
Friendly SI or other transhuman.
<br>
<p><em> &gt; If you were on trial for murder and up for the death penalty, would you
</em><br>
<em> &gt; want one single person to decide your fate or a jury of people?
</em><br>
<p>I'd study the past statistics and behavior of single judges and juries in
<br>
death-penalty murder cases before coming to a decision.
<br>
<p>Friendly AI is a test of intelligence.  If the minimum intelligence to crack 
<br>
Friendly AI is more than the maximum intelligence of a committee, turning 
<br>
the problem over to a committee guarantees a loss.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4396.html">Ben Goertzel: "FAI and SIAI as dangerous"</a>
<li><strong>Previous message:</strong> <a href="4394.html">James Higgins: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4394.html">James Higgins: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4397.html">James Higgins: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4397.html">James Higgins: "Re: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4395">[ date ]</a>
<a href="index.html#4395">[ thread ]</a>
<a href="subject.html#4395">[ subject ]</a>
<a href="author.html#4395">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
