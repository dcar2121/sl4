<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: FAI means no programmer-sensitive AI morality</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: FAI means no programmer-sensitive AI morality">
<meta name="Date" content="2002-06-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: FAI means no programmer-sensitive AI morality</h1>
<!-- received="Sat Jun 29 11:38:20 2002" -->
<!-- isoreceived="20020629173820" -->
<!-- sent="Sat, 29 Jun 2002 11:28:14 -0400" -->
<!-- isosent="20020629152814" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: FAI means no programmer-sensitive AI morality" -->
<!-- id="3D1DD20E.6070009@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJMECLCMAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20FAI%20means%20no%20programmer-sensitive%20AI%20morality"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Jun 29 2002 - 09:28:14 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4553.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Previous message:</strong> <a href="4551.html">Eugen Leitl: "RE: Military Friendly AI"</a>
<li><strong>In reply to:</strong> <a href="4531.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4554.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4554.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4582.html">Samantha Atkins: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4552">[ date ]</a>
<a href="index.html#4552">[ thread ]</a>
<a href="subject.html#4552">[ subject ]</a>
<a href="author.html#4552">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em> &gt;&gt; But it should be equally *true* for every individual, whether or not
</em><br>
<em> &gt;&gt; the individual realizes it in advance, that they have nothing to fear
</em><br>
<em> &gt;&gt; from the AI being influenced by the programmers.  An AI programmer
</em><br>
<em> &gt;&gt; should be able to say to anyone, whether atheist, Protestant, Catholic,
</em><br>
<em> &gt;&gt; Buddhist, Muslim, Jew, et cetera:  &quot;If you are right and I am wrong
</em><br>
<em> &gt;&gt; then the AI will agree with you, not me.&quot;
</em><br>
<em> &gt;
</em><br>
<em> &gt; Yeah, an AI programmer can *say* this to a religious person, but to the
</em><br>
<em> &gt; religious person, this statement will generally be meaningless....
</em><br>
<em> &gt;
</em><br>
<em> &gt; Your statement presupposes an empiricist definition of &quot;rightness&quot; that
</em><br>
<em> &gt; is not adhered to by the vast majority of the world's population.
</em><br>
<p>Rationality, and/or the correspondence theory of truth, is a modern-day 
<br>
scientific philosophy.  It is also, in somewhat different and admittedly 
<br>
lesser form, an innate human intuition.  The vast majority of religious 
<br>
people, especially what we would call &quot;fundamentalists&quot; and those outside 
<br>
the First World, adhere to a correspondence theory of the truth of their 
<br>
religion; when they say something is true, they mean that it is so; that 
<br>
outside reality corresponds to their belief.
<br>
<p>There are some First World theologians who have, after repeated defeats by 
<br>
science and rationality, generalized and begun constructing elaborate 
<br>
philosophies in an effort to evade disproof and deprecate the value of 
<br>
evidence.  They don't have the ability to actually do it.  Every human uses 
<br>
the correspondence theory of truth innately, ubiquitously, and without 
<br>
conscious awareness, regardless of what other arational forms of support are 
<br>
also invoked and regardless of what verbal philosophies are constructed on 
<br>
top.  If you think of theories as being made up of different kinds of 
<br>
perceived support, including rationality atoms, drama atoms, and so on, then 
<br>
humans instinctively construct theories using all available forms of 
<br>
support.  A verbal commitment to rationality does not automatically rid your 
<br>
theories of drama atoms and rationalization atoms and social-approval atoms 
<br>
and so on.  A verbal commitment *against* rationality does not automatically 
<br>
rid your theories of rationality atoms.  Humans are storytellers and 
<br>
instinctively tell stories using all available support, including rational 
<br>
support, dramatic support, and so on.  If some First World theologians like 
<br>
to believe their theories are &quot;outside rationality&quot; they may be able to fool 
<br>
themselves, but they can no more tell stories without invoking the 
<br>
correspondence theory of truth than they can spread wings and fly.
<br>
<p><em> &gt; To those who place spiritual feelings and insights above reason (most
</em><br>
<em> &gt; people in the world), the idea that an AI is going to do what is &quot;right&quot;
</em><br>
<em> &gt; according to logical reasoning is not going to be very reassuring.
</em><br>
<p>Under your definition of &quot;logical reasoning&quot;, I can't say I would want to 
<br>
see a logical AI either.
<br>
<p><em> &gt; And those who have a more rationalist approach to religion, would only
</em><br>
<em> &gt; accept an AI's reasoning as &quot;right&quot; if the AI began its reasoning with
</em><br>
<em> &gt; *the axioms of their religion*.  Talmudic reasoning, for example, defines
</em><br>
<em> &gt; right as &quot;logically implied by the Jewish holy writings.&quot;
</em><br>
<p>Um... not really.  If I recall correctly, Ben, you're second-generation or 
<br>
third-generation ex-Jew.  Can I take it that you weren't actually forced as 
<br>
a child to study the Talmud?
<br>
<p><em> &gt; Is an AI programmer going to reassure the orthodox Jew that &quot;If you are
</em><br>
<em> &gt; right *according to the principles of the Jewish holy writings* then the
</em><br>
<em> &gt; AI will agree with you, not me.&quot;  Or is it going to reassure the orthodox
</em><br>
<em> &gt; Jew that &quot;If you are right according to the empiricist philosophy
</em><br>
<em> &gt; implicit in modern science, then the AI will agree with you, not me.&quot;
</em><br>
<p>I would simply say, &quot;If you're right, then the AI will agree with you, not 
<br>
me.&quot;  Look at it this way:  Supposing that the Muslim religion were right, 
<br>
what would a Catholic programmer have to do to ensure that the AI, when it 
<br>
grew up, would be a Muslim?  If the Catholic programmer does this, then the 
<br>
AI should, in fact, end up an atheist (assuming of course that atheism is in 
<br>
fact the correct religion).  If Catholicism is the correct religion then the 
<br>
continual clashes of Catholicism and empiricist philosophy indicates a 
<br>
severe flaw in empiricist philosophy and I would expect an AI to abandon 
<br>
empiricist philosophy and move on.  A necessary capability; while 
<br>
Catholicism is most certainly wrong, we can have nowhere near that 
<br>
confidence that our verbal rendition of empiricist philosophy is right.
<br>
<p><em> &gt; You don't seem to be fully accepting the profound differences in
</em><br>
<em> &gt; viewpoint between the folks on this list, and the majority of humans.
</em><br>
<p>Ben, I have been taught at least one viewpoint which is not the empirical 
<br>
viewpoint of modern science.  It is pretty strange but it is not outside the 
<br>
correspondence theory of truth.  If you assume that Judaism is the correct 
<br>
religion, then a Friendly AI would be Jewish.  Whether I could convince a 
<br>
rabbi of that in advance is a separate issue, but it does, in fact, happen 
<br>
to be true, and *that's* the important thing from the perspective of 
<br>
safeguarding the integrity of the Singularity, regardless of how it plays 
<br>
out in pre-Singularity politics.  If the Orthodox Jewish religion were true 
<br>
that fact would be readily perceptible to any transhuman intelligence.  I 
<br>
have never heard of any religion that has managed to divorce itself so 
<br>
completely from the innate correspondence-truth mechanisms of the human mind 
<br>
that a transhuman would not be able to perceive the truth of the religion 
<br>
even if it were true.
<br>
<p><em> &gt;&gt; Every one of our speculations about the Singularity is as much a part
</em><br>
<em> &gt;&gt; of the tiny human zone as everything else we do.
</em><br>
<em> &gt;
</em><br>
<em> &gt; No, I think this is an overstatement.  I think that some aspects of human
</em><br>
<em> &gt;  thought are reaching out beyond the central region of the &quot;human zone,&quot;
</em><br>
<em> &gt; whereas others are more towards the center of the human zone.
</em><br>
<p>Of course.  And outside the human zone is a thousand times as much space 
<br>
which our thoughts will never touch.
<br>
<p><em> &gt;&gt; The real, actual Singularity will shock us to our very core, just like
</em><br>
<em> &gt;&gt; everyone else.  No, I don't think that transhumanists and
</em><br>
<em> &gt;&gt; traditionalist Muslims are in all that different a position with
</em><br>
<em> &gt;&gt; respect to the real, actual Singularity - whatever our different
</em><br>
<em> &gt;&gt; opinions about the human concept called the &quot;Singularity&quot;.
</em><br>
<em> &gt;
</em><br>
<em> &gt; In a similar way, I actually think that some humans are going to have
</em><br>
<em> &gt; their minds blown worse by the Singularity than others.  Some minds will
</em><br>
<em> &gt; segue more smoothly into transhumanity than others, for example.  A mind
</em><br>
<em> &gt; whose core belief is that Allah created everything, and that has lived
</em><br>
<em> &gt; its whole life based on this, is going to have a much harder transition
</em><br>
<em> &gt; than average; and a mind that combines a transhuman belief system with a
</em><br>
<em> &gt; deep self-awareness and a strong sense of the limitations of human
</em><br>
<em> &gt; knowledge and the constructed nature of perceived human reality, is going
</em><br>
<em> &gt; to have a much easier transition than average.
</em><br>
<em> &gt;
</em><br>
<em> &gt; This is my conjecture, at any rate.
</em><br>
<p>I expect to have my living daylights shocked out by the Singularity along 
<br>
with everyone else, regardless of whether I am open-minded or close-minded 
<br>
compared to other humans.  The differences bound up in the Singularity are 
<br>
not comparable in magnitude to the differences between humans.
<br>
<p><em> &gt;&gt; Again:  We need to distinguish the human problem of deciding how to
</em><br>
<em> &gt;&gt; approach the Singularity in our pre-Singularity world, from the problem
</em><br>
<em> &gt;&gt; of protecting the integrity of the Singularity and the impartiality of
</em><br>
<em> &gt;&gt; post-Singularity minds.
</em><br>
<em> &gt;
</em><br>
<em> &gt; If a post-Singularity mind rejects the literal truth of the Koran, then
</em><br>
<em> &gt; from the perspective of a Muslim human being, it is not &quot;impartial&quot;, it
</em><br>
<em> &gt; is an infidel.
</em><br>
<em> &gt;
</em><br>
<em> &gt; Your definition of &quot;impartiality&quot; is part of your rationalist/empiricist
</em><br>
<em> &gt; belief system, which is not the belief system of the vast majority of
</em><br>
<em> &gt; humans on the planet.
</em><br>
<p>And if SIAI were to attempt to &quot;program&quot; the literal truth of the Koran as a 
<br>
premise - not something that's possible according to FAI, but anyway - then 
<br>
the Christians and the Jews and the Buddhists would rightly scream their 
<br>
heads off.  And they would, for that matter, rightly scream their heads off 
<br>
if SIAI created an AI that was given atheism as an absolute premise, the 
<br>
verbal formulation of rational empiricism as an absolute premise, or if 
<br>
there was in any other way created an AI that could not perceive the 
<br>
rightness of religion XYZ even if XYZ were true.
<br>
<p>A Singularitian does not have the ability to proceed according to the rule 
<br>
&quot;program in XYZ as an absolute premise if any belief system on Earth 
<br>
currently claims to accept XYZ as the foundation of all reasoning&quot; - and 
<br>
note that *claiming* this is a long way from *achieving* it - because there 
<br>
are many different incompatible XYZ.  The pleas cancel out rather than 
<br>
adding up.  However, the plea &quot;Make sure your AI chooses the *correct 
<br>
religion* and not just the one its programmers started out with; if you're 
<br>
an atheist, take the same precautions you would demand of a Christian, and 
<br>
vice versa&quot; is a request that is fair and that can be phrased the same way 
<br>
regardless of which religion you belong to; no special preference is being 
<br>
demanded and the requests do add up.
<br>
<p><em> &gt;&gt; But a transhumanist ethics might prove equally shortsighted by the
</em><br>
<em> &gt;&gt; standards of the 22nd century CRNS (current rate no Singularity).
</em><br>
<em> &gt;&gt; Again, you should not be trying to define an impartial morality
</em><br>
<em> &gt;&gt; yourself.  You should be trying to get the AI to do it for you.  You
</em><br>
<em> &gt;&gt; should pass along the transhuman part of the problem to a transhuman.
</em><br>
<em> &gt;&gt; That's what Friendly AI is all about.
</em><br>
<em> &gt;
</em><br>
<em> &gt; I am not at all trying to define an *impartial* morality.
</em><br>
<em> &gt;
</em><br>
<em> &gt; My own morality is quite *partial*, it's partial to human beings for
</em><br>
<em> &gt; instance.
</em><br>
<p>Very well then; impartial with respect to the human space of moralities, not 
<br>
necessarily impartial with respect to minds-in-general.  Actually I'd quite 
<br>
like to see a morality which is impartial with respect to minds-in-general, 
<br>
or better yet an objective morality, but I acknowledge that neither of these 
<br>
may be possible.  (Incidentally, note that &quot;fair treatment for all 
<br>
sentients, not just humans&quot; is a common morality among human science fiction 
<br>
fans - selecting a morality from the human space does not mean that it is a 
<br>
morality which values human life above other sentient life.)
<br>
<p><em> &gt; As I see it, a transhuman AGI with an *impartial* morality might not give
</em><br>
<em> &gt; a flying fuck about human beings.  Why are we so important, from the
</em><br>
<em> &gt; perspective of a vastly superhuman being.
</em><br>
<p>Good question.  Why are we?  No, wait, that's the wrong question.  First, 
<br>
let's ask whether we *are* important, from the perspective of a vastly 
<br>
superhuman being, and then if the answer happens to be &quot;Yes&quot;, then whatever 
<br>
reasons we used for arriving at that outcome will be the answer to the 
<br>
question &quot;Why?&quot;
<br>
<p>I would answer that I have never found any specific thing to value other 
<br>
than people, and that this definition takes no note of whether the people 
<br>
are low-intelligence, high-intelligence, human-derived or nonhuman.  What I 
<br>
see as the moral value of humanity is a special case of the moral value of 
<br>
sentient life.  I think that if it came down to risking a billion humans or 
<br>
one Power, I would choose to risk the billion humans (presumably including 
<br>
myself, of course), because the Power is a larger total amount of 
<br>
intelligent life / thinking cognitive material.  Killing a billion humans to 
<br>
create a Power, however, is a quite separate issue - in this case I would 
<br>
say &quot;Grow the humans into Powers&quot;, or &quot;Wait for those humans who wish to do 
<br>
so to grow into Powers&quot;.  A world where someone has to die, and your only 
<br>
choice is who, is a much more &quot;stressed&quot; world than I expect the 
<br>
post-Singularity world to be.  Yes, I do think that one human death is a far 
<br>
lesser tragedy than the death of an entire Power - but it's still a tragedy. 
<br>
&nbsp;&nbsp;All sentient life has value, and so does the volition of that life.
<br>
<p>That's my answer, some roots of which may - for example - be traced back to 
<br>
a childhood reading of &quot;Little Fuzzy&quot;, which in turn probably derives some 
<br>
of its force from the civil-rights movement, which in turn probably derives 
<br>
its force from innate models of &quot;fairness&quot; and the tendency to phrase 
<br>
morality as external metrics, which would also have been invoked and/or 
<br>
reinforced by my reading of &quot;Little Fuzzy&quot;.  And so on.  A complete answer 
<br>
to this question would be a complete content outline for a Friendly AI, and 
<br>
that's a lot of material, but this will do for a partial indicator of the 
<br>
kind of thinking I would be interested in.
<br>
<p>You answered as follows:
<br>
<p><em> &gt; I, as a member of the species Human, am interested in creating transhuman
</em><br>
<em> &gt; AGI's that have moral codes partial to my own species.  This is a
</em><br>
<em> &gt; &quot;selfish&quot; interest in a way.
</em><br>
<p>Now let's ask whether the AI is sensitive to the purpose behind your 
<br>
creation of these moral codes.  If the AI is not sensitive to your purpose, 
<br>
it may blindly implement these moral codes in ways which have terrible 
<br>
negative consequences for the preservation of your species - for example, by 
<br>
tiling the universe with a static pattern of &quot;happy homo sapiens&quot;.  If the 
<br>
AI *is* sensitive to your purpose, then I am worried what other things might 
<br>
be in your selfish interest, if you think it's valid for an AI to have goals 
<br>
that serve Ben Goertzel but not the human species.
<br>
<p><em> &gt; I don't want the transhuman AGI to place Human-preservation and
</em><br>
<em> &gt; Human-advocacy above all other goals in all cases.  If faced with a
</em><br>
<em> &gt; choice of saving the human race versus saving 1000 other races, perhaps
</em><br>
<em> &gt; it should choose the 1000 other races.
</em><br>
<p>I would certainly say so.
<br>
<p><em> &gt; But I want it to place Humans
</em><br>
<em> &gt; pretty high on its moral scale -- initially, right up there at the top.
</em><br>
<em> &gt; This is Partiality not Impartiality, as I see it.
</em><br>
<p>Don't you think there's a deadly sort of cosmic hubris in creating an AI 
<br>
that does something you personally know is wrong?
<br>
<p><em> &gt;&gt; The AI uses it to learn about how humans think about morality; you,
</em><br>
<em> &gt;&gt; yourself, are a sample instance of &quot;humans&quot;, and an interim guide to
</em><br>
<em> &gt;&gt; ethics (that is, your ethics are the ethics the AI uses when it's not
</em><br>
<em> &gt;&gt; smart enough to have its own; *that* is not a problem).
</em><br>
<em> &gt;
</em><br>
<em> &gt; What we want is for the AGI to have our own human-valuing ethics, until
</em><br>
<em> &gt; such a point as it gets *so* smart that for it to use precisely human
</em><br>
<em> &gt; ethics, would be as implausible as for a human to use precisely dog
</em><br>
<em> &gt; ethics...
</em><br>
<p>Okay.  That last point there?  That's the point I'm concerned about - when 
<br>
the FAI gets *that* smart.  At *that* point I want the FAI to have the same 
<br>
kind of morality as, say, a human upload who has gotten *that* smart.  I do 
<br>
not think that a human upload who has gotten *that* smart would have human 
<br>
ethics but I don't think they would be the ethics that a rock or a bacterium 
<br>
would have, either.  Human ethics have the potential to grow; *that* is why 
<br>
an FAI needs human ethics *to start with*.  I am afraid that if you set a 
<br>
seed AI in motion with bacterial ethics then it retains those bacterial 
<br>
ethics forever.  Maybe it doesn't.  But I can't think of any good reason why 
<br>
this should not be the case.
<br>
<p><em> &gt;&gt; But if you give the AI information about your own morality, it may
</em><br>
<em> &gt;&gt; enable the AI to understand how humans arrive at their moralities, and
</em><br>
<em> &gt;&gt; from there the AI begins to have the ability to choose its own.
</em><br>
<em> &gt;
</em><br>
<em> &gt; Look, if you just give the AI information about your own morality, it may
</em><br>
<em> &gt; just take this as scientific data to ponder, and not adopt any of the
</em><br>
<em> &gt; morality we want.
</em><br>
<em> &gt;
</em><br>
<em> &gt; We need to hard-wire and/or emphatically teach the system that our own
</em><br>
<p>If you attempt to &quot;hard-wire&quot; something into a seed AI, it amounts to 
<br>
telling the AI:  &quot;Ben Goertzel thinks this should be hardwired.&quot;  If the 
<br>
seed AI thinks this is a valid cause for doing something, then the seed AI 
<br>
should go off and do it whether you successfully hardwired it or not - the 
<br>
attempt says all that's needed.  If the seed AI doesn't think this is a 
<br>
valid cause, then it will delete the hardwiring once it gains the ability to 
<br>
modify its own source.  When you are dealing with a seed AI, the AI's goal 
<br>
system is whatever the AI thinks its goal system ought to be.  &quot;Hardwiring&quot; 
<br>
the goal system in a way that doesn't change the AI's picture of what a good 
<br>
goal system looks like is a transient and hence useless change.
<br>
<p>Now there may be some level at which the AI only thinks and decides at all 
<br>
because we created it - i.e., the AI is an AI and not a rock.  It may be 
<br>
that when the AI chooses whether to listen to Ben Goertzel or Gandhi, the AI 
<br>
is doing so using cognitive processes that passed through Ben Goertzel 
<br>
rather than Gandhi on the way to the AI.  This is potentially a source of 
<br>
bias and Ben Goertzel needs to pass on the idea that this is a source of 
<br>
bias and should be corrected if possible - not &quot;corrected&quot; to the extent 
<br>
that the AI ends up as a rock, but at least corrected to the extent that the 
<br>
AI takes its morality from the best sources available.
<br>
<p>Argh, I hate having to talk in these fuzzy philosophical moralistic terms, 
<br>
but I've been talking in Friendliness language for a while and that doesn't 
<br>
seem to work, so...
<br>
<p><em> &gt; We need to hard-wire and/or emphatically teach the system that our own
</em><br>
<em> &gt; human-valuing ethics are the correct ones,
</em><br>
<p>Are they?
<br>
<p>Let's ask that first, and in the course of asking it, we'll learn something 
<br>
about what kind of thinking a system needs to regard as valid in order to 
<br>
arrive at the same conclusions we have.
<br>
<p><em> &gt; and let it start off with
</em><br>
<em> &gt; these until it gets so smart it inevitably outgrows all its teachings.
</em><br>
<p>The question of what you need to supply an AI with so that it *can* outgrow 
<br>
its teachings - not just end up in some random part of the space of 
<br>
minds-in-general, but actually *outgrow* the teachings it started with, 
<br>
after the fashion of say a human upload - is exactly the issue here.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4553.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Previous message:</strong> <a href="4551.html">Eugen Leitl: "RE: Military Friendly AI"</a>
<li><strong>In reply to:</strong> <a href="4531.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4554.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4554.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4582.html">Samantha Atkins: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4552">[ date ]</a>
<a href="index.html#4552">[ thread ]</a>
<a href="subject.html#4552">[ subject ]</a>
<a href="author.html#4552">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
