<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: How hard a Singularity?</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="Re: How hard a Singularity?">
<meta name="Date" content="2002-06-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: How hard a Singularity?</h1>
<!-- received="Wed Jun 26 18:10:15 2002" -->
<!-- isoreceived="20020627001015" -->
<!-- sent="Wed, 26 Jun 2002 14:42:55 -0700" -->
<!-- isosent="20020626214255" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="Re: How hard a Singularity?" -->
<!-- id="4.3.2.7.2.20020626142728.01c1d018@mail.earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D1A2C39.4050006@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=Re:%20How%20hard%20a%20Singularity?"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Wed Jun 26 2002 - 15:42:55 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4398.html">James Higgins: "Re: FAI and SIAI as dangerous"</a>
<li><strong>Previous message:</strong> <a href="4396.html">Ben Goertzel: "FAI and SIAI as dangerous"</a>
<li><strong>In reply to:</strong> <a href="4395.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4404.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4404.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4406.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4397">[ date ]</a>
<a href="index.html#4397">[ thread ]</a>
<a href="subject.html#4397">[ subject ]</a>
<a href="author.html#4397">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 05:03 PM 6/26/2002 -0400, Eliezer S. Yudkowsky wrote:
<br>
<em>&gt;James Higgins wrote:
</em><br>
<em>&gt; &gt; At 03:59 PM 6/26/2002 -0400, Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt; &gt; I never said they could, or should, DESIGN anything.  Simply approve
</em><br>
<em>&gt; &gt; designs.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I think that handing something to a committee imposes an upper limit on the
</em><br>
<em>&gt;intelligence of the resulting decisions.  Committees can be smart but they
</em><br>
<em>&gt;cannot be geniuses.  If Friendly AI requires genius, then turning over the
</em><br>
<em>&gt;problem to a committee guarantees failure, just as it would for the problem
</em><br>
<em>&gt;of AI itself.
</em><br>
<p>Aargh, this is frustrating.
<br>
<p>The committee is there for RISK MANAGEMENT.  A task which should very much 
<br>
be done thoroughly on such a task as creating a Singularity.  They do not 
<br>
have to, collectively, understand all the inner working of the 
<br>
design.  They simply have to be convinced to a reasonable degree that the 
<br>
design, as a whole, is safe.  There are many such examples of this in 
<br>
present day life, where an entity is responsible for ensuring safety.  If 
<br>
it is impossible for a group of 10 intelligent people to agree that it is 
<br>
safe to launch a Singularity then, frankly, it shouldn't be launched.
<br>
<p><em>&gt; &gt; I believe your goal, Eliezer, is to make the Singularity as friendly and
</em><br>
<em>&gt; &gt;  safe as possible, is it not?  If so you should welcome such a committee
</em><br>
<em>&gt; &gt;  as a way to ensure that the safest and most friendly design is the one
</em><br>
<em>&gt; &gt; launched.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I should NOT welcome such a committee unless I believe the ACTUAL EFFECT of
</em><br>
<em>&gt;such a committee will be to ensure that the safest and most friendly design
</em><br>
<em>&gt;is launched.  Friendly AI design is not as complex as AI design but it is
</em><br>
<em>&gt;still the second most complicated thing I have ever encountered in my life.
</em><br>
<em>&gt;  I would trust someone who built an AI to make it Friendly.  I would not
</em><br>
<em>&gt;trust a committee to even understand what the real nature of the problem
</em><br>
<em>&gt;was.  I would trust it to spend its whole time debating various versions of
</em><br>
<em>&gt;Asimov Laws, never moving on the issue of structural Friendliness.
</em><br>
<p>So, Eliezer, your saying that if YOU were appointed to such a committee you 
<br>
would all of a sudden stop thinking rationally and start spouting off 
<br>
Asimov Laws and such?  You think we should throw darts at the white pages 
<br>
to pick the members of the committee or something?  Your making my case for 
<br>
me here as to why a single individual should not be trusted with this decision.
<br>
<p><em>&gt; &gt; You should under no circumstances fear such a committee since, if you
</em><br>
<em>&gt; &gt; really are destined to engineer the Singularity, the committee would
</em><br>
<em>&gt; &gt; certainly concede that your design was the best when it was presented to
</em><br>
<em>&gt; &gt; them.
</em><br>
<em>&gt;
</em><br>
<em>&gt;That's outright silly.  One, I don't think that destiny exists in our
</em><br>
<em>&gt;universe, so I can't have one.  Two, there is no reason why a committee
</em><br>
<em>&gt;would be capable of picking the best design when the problem is inherently
</em><br>
<em>&gt;more complex than the intelligence of a committee permits.  The committee
</em><br>
<em>&gt;will pick out a set of Asimov Laws designed by Marvin Minsky in accordance
</em><br>
<em>&gt;with currently faddish AI principles.  If the committee has to build their
</em><br>
<em>&gt;own AI, they'll pick a faddish design and fail.  I will not provide an AI
</em><br>
<em>&gt;for them if they are not smart enough to build it themselves.
</em><br>
<p>Careful, your starting to look like little more than an ego maniac, 
<br>
Eliezer.  Using irrational arguments to defend the position that you should 
<br>
be free to decide the fate of the human race, as a whole, yourself won't 
<br>
work forever.
<br>
<p><em>&gt;The fact that, at this moment, it takes (I think) substantially more 
</em><br>
<em>&gt;intelligence to *build* an AI, at all, than to build a Friendly AI, is one 
</em><br>
<em>&gt;of the few advantages that humanity has in this - although Moore's Law is 
</em><br>
<em>&gt;slowly but steadily eroding that advantage.  I have not and never will 
</em><br>
<em>&gt;propose that SIAI (a 501(c)(3) nonprofit) be given supervisory capacity 
</em><br>
<em>&gt;over the Friendliness efforts of other AI projects, regardless of whether 
</em><br>
<em>&gt;future circumstances make this a plausible outcome.
</em><br>
<em>&gt;
</em><br>
<em>&gt;It is terribly dangerous to take away the job of Friendly AI from whoever 
</em><br>
<em>&gt;was smart enough to crack the basic nature of intelligence!  Friendly AI 
</em><br>
<em>&gt;is not as complex as AI but it is still the second hardest problem I have 
</em><br>
<em>&gt;ever encountered.  A committee is not up to that!
</em><br>
<p>A committee may not be up to designing a Friendly AI (because design by 
<br>
committee is slow for one) but there is no reason they could not decide if 
<br>
a given design was SAFE.  Your seem rather convinced that human beings 
<br>
can't be trusted to make their own decisions (based on post-Singularity 
<br>
speculation you've posted) so why should we trust whoever gets their first 
<br>
to make such major decisions?  Just because someone is INTELLIGENT enough 
<br>
to design an AI doesn't mean they are WISE enough to use it 
<br>
properly.  Intelligence does not equate to wisdom.
<br>
<p><em>&gt; &gt;&gt; Sometimes committees are not very smart.  I fear them.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I don't like committees either, and I can understand why you, in
</em><br>
<em>&gt; &gt; particular, would fear such a committee.  It would take away your ability
</em><br>
<em>&gt; &gt; to single handedly, permanently alter the fate of the human race.  Which
</em><br>
<em>&gt; &gt; is exactly why such a committee would be a good thing. Such decisions are
</em><br>
<em>&gt; &gt; too big for any one person to make.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Then they're too big for N people to make and should be passed on to a
</em><br>
<em>&gt;Friendly SI or other transhuman.
</em><br>
<p>So, how do you propose we find a Friendly SI or Transhuman to judge which 
<br>
Singularity attempts will be safe?  Since the decision would need to be 
<br>
made prior to the existence of any Friendly SIs or Transhumans that would 
<br>
seem to be quite difficult.
<br>
<p><em>&gt;Friendly AI is a test of intelligence.  If the minimum intelligence to 
</em><br>
<em>&gt;crack Friendly AI is more than the maximum intelligence of a committee, 
</em><br>
<em>&gt;turning the problem over to a committee guarantees a loss.
</em><br>
<p>Neither Friendly AI nor the Singularity is a TEST of any kind.  Neither is 
<br>
it a competition!  No one should be in a race to create the Singularity to 
<br>
prove anything.  Such thinking will certainly be the demise of us all.
<br>
<p>James Higgins
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4398.html">James Higgins: "Re: FAI and SIAI as dangerous"</a>
<li><strong>Previous message:</strong> <a href="4396.html">Ben Goertzel: "FAI and SIAI as dangerous"</a>
<li><strong>In reply to:</strong> <a href="4395.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4404.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4404.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4406.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4397">[ date ]</a>
<a href="index.html#4397">[ thread ]</a>
<a href="subject.html#4397">[ subject ]</a>
<a href="author.html#4397">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
