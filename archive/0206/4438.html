<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Suicide by committee (was: How hard a Singularity?)</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Suicide by committee (was: How hard a Singularity?)">
<meta name="Date" content="2002-06-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Suicide by committee (was: How hard a Singularity?)</h1>
<!-- received="Thu Jun 27 06:54:04 2002" -->
<!-- isoreceived="20020627125404" -->
<!-- sent="Thu, 27 Jun 2002 06:11:39 -0400" -->
<!-- isosent="20020627101139" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Suicide by committee (was: How hard a Singularity?)" -->
<!-- id="3D1AE4DB.9030300@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="4.3.2.7.2.20020626200615.01c32ec0@mail.earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Suicide%20by%20committee%20(was:%20How%20hard%20a%20Singularity?)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu Jun 27 2002 - 04:11:39 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4439.html">Eugen Leitl: "discussion so far"</a>
<li><strong>Previous message:</strong> <a href="4437.html">Helge Kautz: "Re: P2P"</a>
<li><strong>In reply to:</strong> <a href="4422.html">James Higgins: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4451.html">James Higgins: "Re: Suicide by committee (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4451.html">James Higgins: "Re: Suicide by committee (was: How hard a Singularity?)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4438">[ date ]</a>
<a href="index.html#4438">[ thread ]</a>
<a href="subject.html#4438">[ subject ]</a>
<a href="author.html#4438">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
James Higgins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; You don't endorse the existence of an organization to promote the meme 
</em><br>
<em>&gt; of SI friendliness?  Are you kidding?  I thought that was YOUR ENTIRE 
</em><br>
<em>&gt; GOAL!  Is this only ok if you are the one doing it?
</em><br>
<p>It is not my entire goal.  It is nowhere near my goal.  My goal is to 
<br>
*actually ensure* that any seed AI built is *successfully* Friendly.
<br>
<p>Right now, I can, in all humility, speak my complete mind about Friendly AI. 
<br>
&nbsp;&nbsp;This is &quot;humility&quot; because it doesn't trust that Earth will be okay if I 
<br>
start deciding on behalf of others what they ought to know.  In order to do 
<br>
this I have to say, along with reassuring true things, frightening true 
<br>
things, academically out-of-fashion true things, and politically incorrect 
<br>
true things.
<br>
<p>Right now I can focus solely on creating a theory that actually works, 
<br>
instead of one that looks good on paper.  I don't think that's possible once 
<br>
the problem is turned over to committees.
<br>
<p>This is a Singularity problem.  You cannot solve Singularity problems with 
<br>
silly little human solutions like committees.  All you can do is create the 
<br>
illusion of effectiveness and authority, while actually involving petty 
<br>
politics in the problem and thereby destroying all hope of a correct 
<br>
solution.  I will say it again:  You cannot solve Singularity problems by 
<br>
inventing committees.  The inability of any human to be entrusted with AI 
<br>
morality is a Singularity problem.  If the question were getting people to 
<br>
trust AI morality, instead of *how to actually do it* - or, more to the 
<br>
point, if I was dumb enough to see the issues in those terms - then yes, I 
<br>
could have &quot;solved&quot; this problem by creating a committee to decide on AI 
<br>
morality, which would have a greater appearance of authority and 
<br>
trustworthiness.  But you cannot solve Singularity problems like that. 
<br>
Political problems, yes; Singularity problems, no.
<br>
<p>My interest is *not* in convincing people that solutions will work.  I want 
<br>
a solution that *does work*.  I suppose, as a secondary goal, that I want 
<br>
people to know the truth, but that is not primary; solving the problem is 
<br>
primary.  It is not supposed to be persuasive, it is supposed to ACTUALLY 
<br>
WORK.  Lose sight of that and game over.
<br>
<p>I know a *lot* of AI morality concepts that sound appealing but are utterly 
<br>
unworkable.  For that reason, above all else, I am scared to death of 
<br>
committees.  It seems very predictable what the result will be and it will 
<br>
be absolutely deadly.
<br>
<p><em>&gt;&gt; That said:  This is a fucking stupid suicidal idea.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, alrighty then.  Could you please clarify your point a bit?  It 
</em><br>
<em>&gt; sounds like your reacting in a completely irrational manor, heavily 
</em><br>
<em>&gt; influenced by emotions.  I don't see anything suicidal about promoting 
</em><br>
<em>&gt; Friendliness in regard to the Singularity or trying to ensure that the 
</em><br>
<em>&gt; Singularity is attempted in a reasonable and safe manner.
</em><br>
<p>A Friendly AI designed by a committee?  Why aren't more people panicking 
<br>
over this?  It sounds like the backstory of Ed Merta's &quot;Worst-Case Scenario&quot;.
<br>
<p>And no, I will not design a Friendly AI to please a committee either.  I 
<br>
will not design a Friendly AI for any purpose other than being Friendly.  IF 
<br>
such a committee exists I will attempt to convince it that its first duty is 
<br>
to disband.  It is inherently difficult to convince a committee of this, 
<br>
*regardless of whether it is true*, which in itself shows that a committee 
<br>
is a bad idea.  Committees don't know what they don't know.
<br>
<p>Here's an idea:  Instead of convening the Committee to Fuck Up Friendly AI, 
<br>
let's convene the Committee to Decide Whether the CFUFAI Should Exist in the 
<br>
First Place, with a clear understanding that the members of CDWCSEFP will 
<br>
probably *not* serve on CFUFAI.
<br>
<p>Look at the mess we have right here on SL4!  You can't agree over whether 
<br>
CFUFAI should be a purely advisory organization, a small transhumanist 
<br>
organization with real powers (enforced how?), or a government committee; 
<br>
you can't agree whether or not military AI development is inherently 
<br>
frightening...
<br>
<p>The natural solution is the one we have right now.  If an AI project refuses 
<br>
to listen to advisory boards out of sheer pigheadedness, where those 
<br>
advisory boards are actually useful, then *for that reason* the project's 
<br>
hubris will likely be punished with a failure in the domain of AI as well. 
<br>
If a project fails to publish sufficient documentation of its Friendliness 
<br>
efforts or fails to convey an adequate understanding of its reasoning, then 
<br>
for that reason the project will have more difficulty finding funding (and, 
<br>
more importantly, Singularity-savvy workers).  If it's a commercial project 
<br>
or government project that can get funding anyway then it certainly isn't 
<br>
going to halt just because your little transhumanist committee says so.
<br>
<p>The natural situation is probably as good as we're going to get.  Random 
<br>
people fighting over who gets to give orders to AI projects will simply make 
<br>
things much, much worse.  If you want to influence the Singularity, do the 
<br>
moral thing and devote your entire life to doing nothing else, thereby 
<br>
gaining some measure of influence according to your talent and effort.  I 
<br>
see no benefit for the Singularity in transforming this admittedly imperfect 
<br>
situation into a human tribal fight over who gets to sit back far removed 
<br>
from the actual efforts and give orders.  I think it guarantees that the 
<br>
final resting state will be an unworkable compromise solution designed to 
<br>
please everyone and include a tidbit for every special interest.  And that's 
<br>
if the situation is in the hands of a committee of transhumanists.  An 
<br>
actual Congressional committee would either (a) fail to get the point 
<br>
completely or (b) impose an outright ban on AI, guaranteed.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4439.html">Eugen Leitl: "discussion so far"</a>
<li><strong>Previous message:</strong> <a href="4437.html">Helge Kautz: "Re: P2P"</a>
<li><strong>In reply to:</strong> <a href="4422.html">James Higgins: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4451.html">James Higgins: "Re: Suicide by committee (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4451.html">James Higgins: "Re: Suicide by committee (was: How hard a Singularity?)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4438">[ date ]</a>
<a href="index.html#4438">[ thread ]</a>
<a href="subject.html#4438">[ subject ]</a>
<a href="author.html#4438">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
