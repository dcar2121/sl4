<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Ben vs. Ben</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Ben vs. Ben">
<meta name="Date" content="2002-06-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Ben vs. Ben</h1>
<!-- received="Sun Jun 30 00:23:43 2002" -->
<!-- isoreceived="20020630062343" -->
<!-- sent="Sat, 29 Jun 2002 22:21:07 -0600" -->
<!-- isosent="20020630042107" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Ben vs. Ben" -->
<!-- id="LAEGJLOGJIOELPNIOOAJEEEDCMAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D1E5B40.D9848DC8@posthuman.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Ben%20vs.%20Ben"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sat Jun 29 2002 - 22:21:07 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4578.html">Stephen Reed: "Re: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4576.html">Ben Goertzel: "RE: Ben vs. Ben"</a>
<li><strong>In reply to:</strong> <a href="4573.html">Brian Atkins: "Re: Ben vs. Ben"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4588.html">Brian Atkins: "Re: Ben vs. Ben"</a>
<li><strong>Reply:</strong> <a href="4588.html">Brian Atkins: "Re: Ben vs. Ben"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4577">[ date ]</a>
<a href="index.html#4577">[ thread ]</a>
<a href="subject.html#4577">[ subject ]</a>
<a href="author.html#4577">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
hi,
<br>
<p><em>&gt; Famous last words? You seem uncertain about so many other issues, but on
</em><br>
<em>&gt; this one you are so utterly sure that an outside organization had to pay
</em><br>
<em>&gt; you just to get your coders to spend a few weeks adding a simple takeoff
</em><br>
<em>&gt; notification warning system? I think somehow you are still missing my
</em><br>
<em>&gt; point. The point to reiterate is not about what your own intuition is
</em><br>
<em>&gt; regarding the risk. The point is to always imagine that you might be
</em><br>
<em>&gt; wrong, and if there is a relatively simple addition you can make to the
</em><br>
<em>&gt; design to reduce or eliminate the risk you should always do that. What
</em><br>
<em>&gt; is three weeks compared to that paper I referenced above?
</em><br>
<p>The odds of the *current Novamente version* going superhuman, are on the
<br>
same order as the odds of my left elbow suddenly turning into a cauliflower,
<br>
or the next crepitation I exude accidentally incinerating the solar system.
<br>
I do not live my life or conduct my work based on paying serious attention
<br>
to events of such incredibly small probability!
<br>
<p>I am uncertain about a lot of things, and I'm pretty highly certain about a
<br>
lot of things too.  The discussion on this list tends to focus on really
<br>
deep, hard problems that I'm uncertain about....
<br>
<p><em>&gt; Good for you, although the .com era shows that some other people were not
</em><br>
<em>&gt; so good at taking risks with other people's property.
</em><br>
<p>Well, Webmind Inc. took risks and lost.  But the biggest risks we took were
<br>
in fact the ideas of our CEO, who was also our lead investor, and was
<br>
risking his own money.  He was a risk-taking guy, having made his millions
<br>
as a trader in some fairly speculative markets....
<br>
<p>However, in business all you're risking is money and time, whereas with AGI
<br>
the stakes are considerably higher, as we all know.
<br>
<p><em>&gt; Sometimes your ideas
</em><br>
<em>&gt; regarding embedding your own morality into your AI make me feel that you
</em><br>
<em>&gt; are doing almost the same type of thing: acting in a riskier and less
</em><br>
<em>&gt; critical fashion since you feel like there is going to be less or no risk
</em><br>
<em>&gt; to yourself due to the AI acting in accordance with your beliefs.
</em><br>
<p>This is just waaaaay off.  I think the risk to ME is exactly the same as the
<br>
risk to anyone else on Earth.  None of the beliefs I intend to teach
<br>
Novababy involve placing ME in a privileged position above other humans.
<br>
They only involve placing HUMANS in an initially privileged position in the
<br>
AI's value hierarchy.
<br>
<p><em>&gt; &gt; Of course.  There is time for that, because Novamente has
</em><br>
<em>&gt; effectively zero
</em><br>
<em>&gt; &gt; existential risk at the moment.
</em><br>
<em>&gt;
</em><br>
<em>&gt; In your opinion
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The design, if fully implemented, would in my view pose a real
</em><br>
<em>&gt; existential
</em><br>
<em>&gt; &gt; risk, but it is just not there yet.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; In your opinion
</em><br>
<em>&gt;
</em><br>
<em>&gt; You are certainly the wisest person on the planet to know all this with
</em><br>
<em>&gt; such certainty that you feel ok with playing dice with us all.
</em><br>
<p>In each case, we can substitute &quot;In my opinion, and that of everyone else
<br>
who has studied  the codebase.&quot;
<br>
<p>The idea that Novamente has more potential to *ever* be smarter than
<br>
Microsoft Word is *also* &quot;just my opinion&quot;... or rather, &quot;just the opinion
<br>
of me and the others who have studied the codebase&quot;
<br>
<p>Can't you see that if the odds of a certain software system going superhuman
<br>
are *sufficiently low*, then no protective measures are necessary, or even
<br>
meaningful?
<br>
<p>I could give you a long list of other people with would-be-AGI systems:
<br>
Peter Voss of A2I2, Pei Wang, Cyc,....  All these folks also have incomplete
<br>
would-be AGI systems, and all these folks also assess that their systems
<br>
have effectively no chance of going superhuman until much further coding
<br>
work is done on them.
<br>
<p>I guess the reason you're pushing me on this issue, and not them, may partly
<br>
be that you suspect I have a slightly higher chance of success than these
<br>
guys.  So I should be flattered....  I also think I have a higher chance of
<br>
success than these guys.  But my feeling that I have a higher chance of
<br>
success than these others, though quite strong, is much, much weaker than my
<br>
very solid knowledge that the current code base *cannot go superhuman*.
<br>
<p>-- ben g
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4578.html">Stephen Reed: "Re: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4576.html">Ben Goertzel: "RE: Ben vs. Ben"</a>
<li><strong>In reply to:</strong> <a href="4573.html">Brian Atkins: "Re: Ben vs. Ben"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4588.html">Brian Atkins: "Re: Ben vs. Ben"</a>
<li><strong>Reply:</strong> <a href="4588.html">Brian Atkins: "Re: Ben vs. Ben"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4577">[ date ]</a>
<a href="index.html#4577">[ thread ]</a>
<a href="subject.html#4577">[ subject ]</a>
<a href="author.html#4577">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
