<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: FAI means no programmer-sensitive AI morality</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: FAI means no programmer-sensitive AI morality">
<meta name="Date" content="2002-06-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: FAI means no programmer-sensitive AI morality</h1>
<!-- received="Sat Jun 29 08:05:17 2002" -->
<!-- isoreceived="20020629140517" -->
<!-- sent="Fri, 28 Jun 2002 22:58:37 -0700" -->
<!-- isosent="20020629055837" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: FAI means no programmer-sensitive AI morality" -->
<!-- id="3D1D4C8D.90002@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJMECLCMAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20FAI%20means%20no%20programmer-sensitive%20AI%20morality"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Fri Jun 28 2002 - 23:58:37 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4540.html">Eugen Leitl: "Re: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4538.html">Samantha Atkins: "Re: Friendly Existential Wager"</a>
<li><strong>In reply to:</strong> <a href="4531.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4545.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4545.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4553.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4539">[ date ]</a>
<a href="index.html#4539">[ thread ]</a>
<a href="subject.html#4539">[ subject ]</a>
<a href="author.html#4539">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt;&gt;But it should be equally *true* for every individual, whether or not the
</em><br>
<em>&gt;&gt;individual realizes it in advance, that they have nothing to fear
</em><br>
<em>&gt;&gt;from the
</em><br>
<em>&gt;&gt;AI being influenced by the programmers.  An AI programmer should
</em><br>
<em>&gt;&gt;be able to
</em><br>
<em>&gt;&gt;say to anyone, whether atheist, Protestant, Catholic, Buddhist,
</em><br>
<em>&gt;&gt;Muslim, Jew,
</em><br>
<em>&gt;&gt;et cetera:  &quot;If you are right and I am wrong then the AI will agree with
</em><br>
<em>&gt;&gt;you, not me.&quot;
</em><br>
<em>&gt; 
</em><br>
<p>Of course some breeds of religious people would simply claim 
<br>
that unless the AI has an immortal soul (or Buddha nature) and 
<br>
it is capable of communion with the Holy Ghost or some such that 
<br>
it cannot know about these religious matters at all.   As you 
<br>
say below, a non-empiricist element.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yeah, an AI programmer can *say* this to a religious person, but to the
</em><br>
<em>&gt; religious person, this statement will generally be meaningless....
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Your statement presupposes an empiricist definition of &quot;rightness&quot; that is
</em><br>
<em>&gt; not adhered to by the vast majority of the world's population.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; To those who place spiritual feelings and insights above reason (most people
</em><br>
<em>&gt; in the world), the idea that an AI is going to do what is &quot;right&quot; according
</em><br>
<em>&gt; to logical reasoning is not going to be very reassuring.
</em><br>
<em>&gt; 
</em><br>
<p>Hmmm.  A lot is hidden by &quot;spiritual feelings and insights&quot; and 
<br>
&quot;reason&quot;.  Depending on who you ask many things could be claimed 
<br>
of and included or excluded from either.  It is not at all clear 
<br>
that such an AI would be any less intuitive or &quot;spiritual&quot; than 
<br>
humans.  It might even be more so for some values of &quot;spiritual&quot;.
<br>
<p><p><em>&gt; And those who have a more rationalist approach to religion, would only
</em><br>
<em>&gt; accept an AI's reasoning as &quot;right&quot; if the AI began its reasoning with *the
</em><br>
<em>&gt; axioms of their religion*.  Talmudic reasoning, for example, defines right
</em><br>
<em>&gt; as &quot;logically implied by the Jewish holy writings.&quot;
</em><br>
<em>&gt;
</em><br>
<p>This goes too far.  Many more rational approaches to religion 
<br>
eschew axioms of religion.
<br>
<p><p><em>&gt; Is an AI programmer going to reassure the orthodox Jew that &quot;If you are
</em><br>
<em>&gt; right *according to the principles of the Jewish holy writings* then the AI
</em><br>
<em>&gt; will agree with you, not me.&quot;  Or is it going to reassure the orthodox Jew
</em><br>
<em>&gt; that &quot;If you are right according to the empiricist philosophy implicit in
</em><br>
<em>&gt; modern science, then the AI will agree with you, not me.&quot;
</em><br>
<em>&gt;
</em><br>
<p>Probably the latter but not necessarily.  It is almost certain 
<br>
it isn't going to be according to this or that holy writ though. 
<br>
&nbsp;&nbsp;One hopes.  :-)  The AI will find flaws in modern science and 
<br>
some aspects of the philosophy of science as well as in much 
<br>
else that came before it.
<br>
<p><em>&gt; You don't seem to be fully accepting the profound differences in viewpoint
</em><br>
<em>&gt; between the folks on this list, and the majority of humans.
</em><br>
<em>&gt; 
</em><br>
<p><em>&gt;&gt;The real, actual Singularity will shock us to our very core,
</em><br>
<em>&gt;&gt;just like
</em><br>
<em>&gt;&gt;everyone else.  No, I don't think that transhumanists and traditionalist
</em><br>
<em>&gt;&gt;Muslims are in all that different a position with respect to the real,
</em><br>
<em>&gt;&gt;actual Singularity - whatever our different opinions about the
</em><br>
<em>&gt;&gt;human concept
</em><br>
<em>&gt;&gt;called the &quot;Singularity&quot;.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, let me give you an imperfect analogy here.  An LSD trip is an
</em><br>
<em>&gt; experience that often causes one to feel that all the assumptions one has
</em><br>
<em>&gt; made all one's life -- cognitive, perceptual, emotional -- are just
</em><br>
<em>&gt; meaningless constructs.  It brings one &quot;beyond oneself&quot; in a really
</em><br>
<em>&gt; significant way.  If you've not tripped a lot (and I know you haven't), you
</em><br>
<em>&gt; probably don't understand.  (In case anyone is curious, it's been a very
</em><br>
<em>&gt; long time since I took LSD, but the memory is definitely still with me!)
</em><br>
<em>&gt; However, some people can handle this better than others, because some people
</em><br>
<em>&gt; are &quot;more attached to&quot; their own habit-patterns and beliefs than others.
</em><br>
<em>&gt;
</em><br>
<p>I know a bit about this from my own perhaps partially misspent 
<br>
youth.  I found LSD a useful tool for understanding how my 
<br>
current personality and other constructs were put together, 
<br>
blowing them apart and having some degree of freedom to 
<br>
reconstruct in a way that seemed, well, &quot;better&quot;.  After a while 
<br>
it felt as if I went to pretty much the same sorts of states and 
<br>
saw infinite reiterations of the same basic information.  The 
<br>
trouble was actually making use of what was learned there in the 
<br>
&quot;real&quot; world. It wasn't scary for me because my sense of, not so 
<br>
much identity as center, was deeper or different from or more 
<br>
fluid than most.  I believe this sort of fluidity is indeed 
<br>
essential as the world and one's very self changes more and more 
<br>
rapidly.
<br>
<p>If this is so then some forms of spirituality with their strong 
<br>
emphasis on stepping beyond individual ego identification and 
<br>
even teaching means of doing so may actually have a bit of an 
<br>
advantage over the type of agnostic or atheist materialists who 
<br>
are firmly identified with their current state, ego and 
<br>
personality structures.
<br>
<p><em>&gt; In a similar way, I actually think that some humans are going to have their
</em><br>
<em>&gt; minds blown worse by the Singularity than others.  Some minds will segue
</em><br>
<em>&gt; more smoothly into transhumanity than others, for example.  A mind whose
</em><br>
<em>&gt; core belief is that Allah created everything, and that has lived its whole
</em><br>
<em>&gt; life based on this, is going to have a much harder transition than average;
</em><br>
<p>Ah, but the question is what exactly is Allah?  It is left as an 
<br>
imponderable.  For all the believer knows Allah is the name of 
<br>
an SAI of a bygone era or another species that riched a certain 
<br>
level of Power and did indeed act in some ways that over time 
<br>
have become corrupted into the beliefs of today.  Or the name of 
<br>
the SAI under consideration in one of its time-travelling or 
<br>
simulation experiments on how humanity could have been reached 
<br>
earlier and perhaps had/have a less troubled history.  The point 
<br>
of this is that beliefing this or that is true of God is about 
<br>
as nailed down and inflexible as believing this or that about 
<br>
another imponderable, the Singularity.
<br>
<p>Most people have little flexibility regardless of the nature of 
<br>
their belief systems, especially when it comes to think of 
<br>
necessity beyond their capacity to grasp.
<br>
<p><em>&gt; and a mind that combines a transhuman belief system with a deep
</em><br>
<em>&gt; self-awareness and a strong sense of the limitations of human knowledge and
</em><br>
<em>&gt; the constructed nature of perceived human reality, is going to have a much
</em><br>
<em>&gt; easier transition than average.
</em><br>
<em>&gt;
</em><br>
<p>And who is not too attached to current reality/ego structures.
<br>
<p><em>&gt; 
</em><br>
<em>&gt;&gt;Again:  We need to distinguish the human problem of deciding how
</em><br>
<em>&gt;&gt;to approach
</em><br>
<em>&gt;&gt;the Singularity in our pre-Singularity world, from the problem of
</em><br>
<em>&gt;&gt;protecting
</em><br>
<em>&gt;&gt;the integrity of the Singularity and the impartiality of
</em><br>
<em>&gt;&gt;post-Singularity minds.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If a post-Singularity mind rejects the literal truth of the Koran, then from
</em><br>
<em>&gt; the perspective of a Muslim human being, it is not &quot;impartial&quot;, it is an
</em><br>
<em>&gt; infidel.
</em><br>
<em>&gt;
</em><br>
<p>&nbsp;From the point of view of a Muslim mystic, a Muslim (or anyone) 
<br>
that thinks/intuits/experiences Truth in so limited a fashion is 
<br>
the true infidel.  :-)
<br>
<p><em>&gt; Your definition of &quot;impartiality&quot; is part of your rationalist/empiricist
</em><br>
<em>&gt; belief system, which is not the belief system of the vast majority of humans
</em><br>
<em>&gt; on the planet.
</em><br>
<em>&gt;
</em><br>
<p>The most difficult thing for many is to understand that belief 
<br>
does not generate truth nor is it able to evaluate what is and 
<br>
is not true.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4540.html">Eugen Leitl: "Re: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4538.html">Samantha Atkins: "Re: Friendly Existential Wager"</a>
<li><strong>In reply to:</strong> <a href="4531.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4545.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4545.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4553.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4539">[ date ]</a>
<a href="index.html#4539">[ thread ]</a>
<a href="subject.html#4539">[ subject ]</a>
<a href="author.html#4539">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
