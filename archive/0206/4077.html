<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Threats to the Singularity.</title>
<meta name="Author" content="Gordon Worley (redbird@rbisland.cx)">
<meta name="Subject" content="Re: Threats to the Singularity.">
<meta name="Date" content="2002-06-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Threats to the Singularity.</h1>
<!-- received="Mon Jun 17 22:27:03 2002" -->
<!-- isoreceived="20020618042703" -->
<!-- sent="Mon, 17 Jun 2002 21:19:02 -0400" -->
<!-- isosent="20020618011902" -->
<!-- name="Gordon Worley" -->
<!-- email="redbird@rbisland.cx" -->
<!-- subject="Re: Threats to the Singularity." -->
<!-- id="60337174-8259-11D6-94B0-000A27B4DEFC@rbisland.cx" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3D0E67FD.5010809@objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Gordon Worley (<a href="mailto:redbird@rbisland.cx?Subject=Re:%20Threats%20to%20the%20Singularity."><em>redbird@rbisland.cx</em></a>)<br>
<strong>Date:</strong> Mon Jun 17 2002 - 19:19:02 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4078.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4076.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4075.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4078.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4078.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4077">[ date ]</a>
<a href="index.html#4077">[ thread ]</a>
<a href="subject.html#4077">[ subject ]</a>
<a href="author.html#4077">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Monday, June 17, 2002, at 06:51  PM, Samantha Atkins wrote:
<br>
<p><em>&gt; There must be some core, some set of fundamental values, that is 
</em><br>
<em>&gt; unassailable (at least at a point in time) for an ethical system to be 
</em><br>
<em>&gt; built.  It is only in the context of such that the question of &quot;some 
</em><br>
<em>&gt; reason&quot; can even be addressed meaningfully. The life and well-being of 
</em><br>
<em>&gt; sentients *is* part of my core.  It is not itself subject to further 
</em><br>
<em>&gt; breakdown to reasons why this is a core.  To further break it down 
</em><br>
<em>&gt; would require another core reason that this one could be examined in 
</em><br>
<em>&gt; terms of.  A large part of my questions here are an attempt to 
</em><br>
<em>&gt; determine what that core is for various parties.
</em><br>
<p>My core looks something like this.  I want to make the universe a better 
<br>
place.  A better place to live.  A place that solves new, interesting 
<br>
problems.  A place that I'd like to stay, but wouldn't want to visit.
<br>
<p>I'd like for this to include me in it, but if it turns out that the 
<br>
universe can't be better so long as I'm still in it, then I'll get out.  
<br>
I think the &quot;yuck factor&quot; in this is that I think the same way about 
<br>
everything.  If you're making the universe a worse place, I don't really 
<br>
want you in it.  I hope that getting &quot;you&quot; out of it only involves 
<br>
convincing you not to do whatever it is that is making the universe 
<br>
worse.
<br>
<p>I think this seems yucky because this sounds just like the kind of thing 
<br>
Hitler would say.  The difference is that I have compassion for all 
<br>
life.  I want to see the universe better and would like for that to 
<br>
include everyone and everything in it.  However, if all attempts at this 
<br>
proves impossible, I'm not going to say &quot;well, okay, I guess the 
<br>
universe is just going to suck&quot;, but &quot;okay, let's see what the limiting 
<br>
factors are and what we have to do to get around them&quot;.
<br>
<p><em>&gt;&gt;&gt; Whether we transform or simply cease to exist seems to me to be a 
</em><br>
<em>&gt;&gt;&gt; perfectly rational thing to be a bit concerned about.  Do you see it 
</em><br>
<em>&gt;&gt;&gt; otherwise?
</em><br>
<em>&gt;&gt; Sure, you should be concerned.  I think that the vast majority of 
</em><br>
<em>&gt;&gt; humans, uploaded or not, have something positive to contribute, 
</em><br>
<em>&gt;&gt; however small.  It'd be great to see life get even better post 
</em><br>
<em>&gt;&gt; Singularity, with everyone doing new and interesting good things.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Then we shouldn't shoot for any less, right?
</em><br>
<p>Right!
<br>
<p><em>&gt; On what basis will you judge what is rational?  In terms of what 
</em><br>
<em>&gt; supergoals, if you will?
</em><br>
<p>I think that I answered this above:  making the universe better.
<br>
<p><em>&gt;&gt;&gt;&gt; Some of us, myself included, see the creation of SI as important 
</em><br>
<em>&gt;&gt;&gt;&gt; enough to be more important than humanity's continuation.  Human 
</em><br>
<em>&gt;&gt;&gt;&gt; beings, being
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; How do you come to this conclusion?  What makes the SI worth more 
</em><br>
<em>&gt;&gt;&gt; than all of humanity?  That it can outperform them on some types of 
</em><br>
<em>&gt;&gt;&gt; computation?  Is computational complexity and speed the sole measure 
</em><br>
<em>&gt;&gt;&gt; of whether sentient beings have the right to continued existence?  
</em><br>
<em>&gt;&gt;&gt; Can you really give a moral justification or a rational one for this?
</em><br>
<em>&gt;&gt; In many ways, humans are just over the threshold of intelligence.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Whose threshold?  By what standards?  Established and verified as the 
</em><br>
<em>&gt; standards of value how?
</em><br>
<p>You're asking for a definition of intelligence.  Though question!
<br>
<p>One way of looking at intelligence is the ability to solve interesting 
<br>
problems.  Ants solve some mildly interesting problems, apes and 
<br>
dolphins solves slightly more interesting problems, humans solve yet 
<br>
more interesting problems.  At any level of intelligence, though, all 
<br>
the problems at the limits of solvability look interesting.  As great as 
<br>
we think we are, we can already see that there are some interesting 
<br>
problems out there that we can't find solutions to (like the halting 
<br>
problem).  And, unless it turns out that intelligence doesn't scale very 
<br>
well, the trend tells us that even more interesting questions are out 
<br>
there for more intelligent minds to solve.  I doubt that anything will 
<br>
ever be so intelligent that it will be able to solve every problem.
<br>
<p>With infinite problems, we see that all intelligence is &quot;just over the 
<br>
threshold&quot;, but it's clear that ants are closer to the threshold than 
<br>
humans and humans are closer than SIs.
<br>
<p>This is just one way of thinking about it, though.  Ask anyone and they 
<br>
could probably give you a different way of thinking about the same thing.
<br>
<p><em>&gt;&gt; Compared to past humans we are pretty smart, but compared to the 
</em><br>
<em>&gt;&gt; estimated potentials for intelligence we are intellectual ants.  
</em><br>
<em>&gt;&gt; Despite
</em><br>
<em>&gt;
</em><br>
<em>&gt; So we are to think less of ourselves because of estimated potentials?  
</em><br>
<em>&gt; Do we consider ourselves expendable because an SI comes into existence 
</em><br>
<em>&gt; that is a million times faster and more capable in the scope of its 
</em><br>
<em>&gt; creations, decision making and understanding?  This does not follow.
</em><br>
<p>One should be humble, but not negative.  Being negative is just as 
<br>
irrational as flattery.
<br>
<p>Much has humans get to clear away ants if they're keeping the universe 
<br>
from getting better, an SI could clear away some humans if they got in 
<br>
the way.  If the SI is compassionate, ve will see that the humans are 
<br>
doing some good and, being self aware, are able to change themselves to 
<br>
do more good.  Unlike the humans who is unable to solve the ant problem 
<br>
by any means other than getting the ants out of the way (be that killing 
<br>
them or displacing them), an SI can solve the human problem by helping 
<br>
the humans.
<br>
<p>If some humans prove to be beyond help, though, I don't think it's 
<br>
totally wrong to clear them out in some way.  Maybe that just means 
<br>
letting them live in a simulation where they can kill their virtual 
<br>
selves.  I'll leave the solution up to a much more intelligent SI.
<br>
<p><em>&gt;&gt; But, it's not nearly so simple.  All of us would probably agree that 
</em><br>
<em>&gt;&gt; given the choice between saving one of two lives, we would choose to 
</em><br>
<em>&gt;&gt; save the person who is most important to the completion of our goals, 
</em><br>
<em>&gt;&gt; be that reproduction, having fun, or creating the Singularity.  In the 
</em><br>
<em>&gt;&gt; same light, if a mob is about to come in to destroy the SI just before 
</em><br>
<em>&gt;&gt; it takes off and there is no way to stop them other than killing them, 
</em><br>
<em>&gt;&gt; you have on one hand the life of the SI that is already more 
</em><br>
<em>&gt;&gt; intelligent than the members of the mob and will continue to get more 
</em><br>
<em>&gt;&gt; intelligent, and on the other the life of 100 or so humans.  Given 
</em><br>
<em>&gt;&gt; such a choice, I pick the SI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But that is not the context of the question.  The context is whether 
</em><br>
<em>&gt; the increased well-being and possibilities of existing sentients, 
</em><br>
<em>&gt; regardless of their relative current intelligence, is  a high and 
</em><br>
<em>&gt; central value.  If it is not then I hardly see how such an SI can be 
</em><br>
<em>&gt; described as &quot;Friendly&quot;.
</em><br>
<p>To a Friendly intelligence, this is important.
<br>
<p><em>&gt;&gt;&gt;&gt; self aware, do present more of an ethical delima than cows if it 
</em><br>
<em>&gt;&gt;&gt;&gt; turns out that you might be forced to sacrifice some of them.  I 
</em><br>
<em>&gt;&gt;&gt;&gt; would like to see all of humanity make it into a post Singularity 
</em><br>
<em>&gt;&gt;&gt;&gt; existence and I am willing to help make this a reality.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; How kind of you.  However, from the above it seems you see them as an 
</em><br>
<em>&gt;&gt;&gt; ethical dilemna greater than that of cows but if your SI, whatever it 
</em><br>
<em>&gt;&gt;&gt; turns out really to be, seems to require or decides the death of one 
</em><br>
<em>&gt;&gt;&gt; or all of them, then you would have to side with the SI.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; Do I read you correctly?  If I do, then why do you hold this 
</em><br>
<em>&gt;&gt;&gt; position?  If I read you correctly then how can you expect the 
</em><br>
<em>&gt;&gt;&gt; majority of human beings, if they really understood you, to consider 
</em><br>
<em>&gt;&gt;&gt; you as other than a monster?
</em><br>
<em>&gt;&gt; If an SI said it needed to kill a bunch of humans, I would seriously 
</em><br>
<em>&gt;&gt; start questioning its motives.  Killing intelligent life is not 
</em><br>
<em>&gt;&gt; something to be taken lightly and done on a whim.  However, if we had 
</em><br>
<em>&gt;&gt; a FAI that was really Friendly and it said &quot;Gordon, believe me, the 
</em><br>
<em>&gt;&gt; only way is to kill this person&quot;, I would trust in the much wiser SI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; OK, that seems better.  But how would you evaluate how Friendly this 
</em><br>
<em>&gt; superintelligence really was?
</em><br>
<p>With my Friendly-O-Meter, of course.  ;-)
<br>
<p>That's a rather complex question that I don't really have the answer to 
<br>
(and if I ever came up with some kind of partial answer, I've forgotten 
<br>
what it was).  Maybe someone else knows?
<br>
<p><pre>
--
Gordon Worley                     `When I use a word,' Humpty Dumpty
<a href="http://www.rbisland.cx/">http://www.rbisland.cx/</a>            said, `it means just what I choose
<a href="mailto:redbird@rbisland.cx?Subject=Re:%20Threats%20to%20the%20Singularity.">redbird@rbisland.cx</a>                it to mean--neither more nor less.'
PGP:  0xBBD3B003                                  --Lewis Carroll
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4078.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4076.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4075.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4078.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4078.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4077">[ date ]</a>
<a href="index.html#4077">[ thread ]</a>
<a href="subject.html#4077">[ subject ]</a>
<a href="author.html#4077">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
