<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Seed AI (was: How hard a Singularity?)</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Seed AI (was: How hard a Singularity?)">
<meta name="Date" content="2002-06-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Seed AI (was: How hard a Singularity?)</h1>
<!-- received="Sun Jun 23 14:03:22 2002" -->
<!-- isoreceived="20020623200322" -->
<!-- sent="Sun, 23 Jun 2002 13:56:42 -0400" -->
<!-- isosent="20020623175642" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Seed AI (was: How hard a Singularity?)" -->
<!-- id="3D160BDA.6020202@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJEEFOCLAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Seed%20AI%20(was:%20How%20hard%20a%20Singularity?)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jun 23 2002 - 11:56:42 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4220.html">Ben Goertzel: "RE: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4218.html">James Higgins: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4209.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4226.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4226.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4240.html">James Higgins: "Re: Seed AI (was: How hard a Singularity?)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4219">[ date ]</a>
<a href="index.html#4219">[ thread ]</a>
<a href="subject.html#4219">[ subject ]</a>
<a href="author.html#4219">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em> &gt;
</em><br>
<em>&gt; In my view, the most important content of an AGI mind will be things that
</em><br>
<em>&gt; neither the AI or its programmers can name, at first.  Namely: *abstract
</em><br>
<em>&gt; thought-patterns, ways of organizing ideas and ways of approaching
</em><br>
<em>&gt; problems*, which we humans use but know only implicitly, and which we will
</em><br>
<em>&gt; be able to transmit to AI minds implicitly through interaction in
</em><br>
<em>&gt; appropriate shared environments..
</em><br>
<p>What kind of knowledge is this implicit knowledge?  How will the AI absorb 
<br>
it through interaction?  Let's take the mnemonic experiential record of a 
<br>
human interaction; what kind of algorithms will absorb &quot;abstract 
<br>
thought-patterns&quot; from the record of human statements?
<br>
<p>But then, you know my position on expecting things to happen without knowing 
<br>
how they work...
<br>
<p><em>&gt; Of course, this general statement is not true.  Often, in software
</em><br>
<em>&gt; engineering and other kinds of engineering, a very complex design is HARDER
</em><br>
<em>&gt; to improve than a simple one.
</em><br>
<p>Evolution managed to sneak around this trap.  An AI team will have to do so 
<br>
as well; for example, through constructing plugin satisficing architectures 
<br>
where any given task can be performed &quot;well enough&quot; through several 
<br>
different means.
<br>
<p><em>&gt; I think that a lot of transfer of thought-patterns will happen *implicitly*
</em><br>
<em>&gt; through interaction in shared environments.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; For this to happen, explicit declarative knowledge of thought patterns is
</em><br>
<em>&gt; not required, on the part of the human teachers.
</em><br>
<p>Okay.  If you don't know in advance how this will work, I predict that 
<br>
nothing will happen.
<br>
<p><em>&gt; I doubt this is how things will go.  I think human knowledge will be
</em><br>
<em>&gt; comprehensible by an AI *well before* the AI is capable of drastically
</em><br>
<em>&gt; modifying its own sourcecode in the interest of vastly increased
</em><br>
<em>&gt; intelligence.
</em><br>
<p>I would expect the AI's understanding of source code to run well ahead of 
<br>
its understanding of human language at any given point.  The AI lives right 
<br>
next to source code; human language is located in another galaxy by comparison.
<br>
<p><em>&gt; I think that humans will teach the AGI more than just &quot;domain problems at
</em><br>
<em>&gt; the right level,&quot; I think that by cooperatively solving problems together
</em><br>
<em>&gt; with the AGI, humans will teach it a network of interrelated
</em><br>
<em>&gt; thought-patterns.  Just as we learn from other humans via interacting with
</em><br>
<em>&gt; them.
</em><br>
<p>I'm not sure we learn thought-patterns, whatever those are, from other 
<br>
humans; but if so, it's because evolution explicitly designed us to do so. 
<br>
Standing 'in loco evolution' to an AI, you need to know what 
<br>
thought-patterns are, how they work, and what brainware mechanisms and 
<br>
biases support the learning of which thought-patterns from what kind of 
<br>
environmental cues.
<br>
<p><em>&gt; I agree, it does not mean that an AI *must* do so.  However, I hypothesize
</em><br>
<em>&gt; that to allow an AI to learn its initial thought-patterns from humans based
</em><br>
<em>&gt; on experiential interaction, is
</em><br>
<em>&gt; 
</em><br>
<em>&gt; a) the fastest way to get to an AGI
</em><br>
<em>&gt; 
</em><br>
<em>&gt; b) the best way to get an AGI that has a basic empathy for humans
</em><br>
<p>Empathy is a good analogy, unfortunately.  Humans are socialized by 
<br>
interacting with other humans because we are *explicitly evolutionarily 
<br>
programmed* to be socialized in this way.  We don't pick up empathy as an 
<br>
emergent result of our interaction with other humans.  Empathy is hardwired. 
<br>
&nbsp;&nbsp;It may be hardwired in such a way that it depends on environmental human 
<br>
interaction in order to develop, but this does not make it any less 
<br>
hardwired.  An AGI is not going to automatically pick up basic human empathy 
<br>
from interacting with humans any more than a rock would develop empathy for 
<br>
humans if constantly passed from hand to hand.  *Nothing* in AI is 
<br>
automatic.  Not morality, not implicit transfer of thought-patterns, not 
<br>
socialization, *nada*.  If you don't know how it works, it won't!
<br>
<p>Besides, I don't think that hardwired empathy is the way to go, and the kind 
<br>
of automatically, unintentionally acquired empathy you postulate strikes me 
<br>
as no better.  I don't think you can expect a seed AI to be enslaved by its 
<br>
brainware in the same way as a human.
<br>
<p><em>&gt; Yes, you see more &quot;code self-modification&quot; occurring at the
</em><br>
<em>&gt; &quot;pre-human-level-AI&quot; phase than I do.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is because I see &quot;intelligent goal-directed code self-modification&quot; as
</em><br>
<em>&gt; being a very hard problem, harder than mastering human language, for
</em><br>
<em>&gt; example.
</em><br>
<p>This honestly strikes me as extremely odd.  Code is vastly easier to 
<br>
experiment with than human language; the AI can accumulate more experience 
<br>
faster; there are no outside references to the black-box external world; the 
<br>
AI can find its own solutions rather than needing the human one; and the AI 
<br>
can use its own concepts to think rather than needing to manipulate 
<br>
human-sized concepts specialized for human modalities that the AI may not 
<br>
even have.  Code is not easy but I'd expect to be a heck of a lot easier 
<br>
than language.
<br>
<p><em>&gt; Your argument was that &quot;there's nothing special about human level
</em><br>
<em>&gt; intelligence.&quot;  I sought to refute that argument by pointing out that, to
</em><br>
<em>&gt; the extent an AGI is taught by humans, there is something special about
</em><br>
<em>&gt; human level intelligence after all.  Then you countered that, in your
</em><br>
<em>&gt; envisioned approach to AI, teaching by humans plays a smaller role than in
</em><br>
<em>&gt; my own envisioned approach.
</em><br>
<p>Not a smaller role; a very different role teaching a very different kind of 
<br>
knowledge.
<br>
<p><em> &gt; And indeed, this suggests that if seed AI were
</em><br>
<em>&gt; achieved first by your approach rather than mine, the gap between human
</em><br>
<em>&gt; level and vastly superhuman level intelligence would be less.
</em><br>
<p>Quite.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4220.html">Ben Goertzel: "RE: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4218.html">James Higgins: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4209.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4226.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4226.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4240.html">James Higgins: "Re: Seed AI (was: How hard a Singularity?)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4219">[ date ]</a>
<a href="index.html#4219">[ thread ]</a>
<a href="subject.html#4219">[ subject ]</a>
<a href="author.html#4219">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
