<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Self-modifying FAI (was: How hard a Singularity?)</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Self-modifying FAI (was: How hard a Singularity?)">
<meta name="Date" content="2002-06-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Self-modifying FAI (was: How hard a Singularity?)</h1>
<!-- received="Wed Jun 26 13:02:59 2002" -->
<!-- isoreceived="20020626190259" -->
<!-- sent="Wed, 26 Jun 2002 12:47:15 -0400" -->
<!-- isosent="20020626164715" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Self-modifying FAI (was: How hard a Singularity?)" -->
<!-- id="3D19F013.2090106@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="4.3.2.7.2.20020626083319.02576968@mail.earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Self-modifying%20FAI%20(was:%20How%20hard%20a%20Singularity?)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Jun 26 2002 - 10:47:15 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4376.html">Eliezer S. Yudkowsky: "Friendly AI koans"</a>
<li><strong>Previous message:</strong> <a href="4374.html">Michael Roy Ames: "Opposition to Ideas, and win-win scenarios."</a>
<li><strong>In reply to:</strong> <a href="4363.html">James Higgins: "Re: Self-modifying FAI (was: How hard a Singularity?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4387.html">James Higgins: "Re: Self-modifying FAI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4387.html">James Higgins: "Re: Self-modifying FAI (was: How hard a Singularity?)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4375">[ date ]</a>
<a href="index.html#4375">[ thread ]</a>
<a href="subject.html#4375">[ subject ]</a>
<a href="author.html#4375">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
James Higgins wrote:
<br>
<em>&gt; At 07:29 AM 6/26/2002 -0400, Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; I agree with your response to Ben.  We don't expect an AI's belief 
</em><br>
<em>&gt;&gt; that the sky is blue to drift over successive rounds of 
</em><br>
<em>&gt;&gt; self-modification.  Beliefs with an external referent should not 
</em><br>
<em>&gt;&gt; &quot;drift&quot; under self-modification except insofar as they &quot;drift&quot; into 
</em><br>
<em>&gt;&gt; correspondence with reality.  Write a definition of Friendliness made 
</em><br>
<em>&gt;&gt; up of references to things which exist outside the AI, and the content 
</em><br>
<em>&gt;&gt; has no reason to &quot;drift&quot;.  If content drifts it will begin making 
</em><br>
<em>&gt;&gt; incorrect predictions and will be corrected by further learning.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Unfortunately, can we construct a definition of friendliness using 
</em><br>
<em>&gt; external reference points which truly equals what we really want?
</em><br>
<p>And here begins the fun!
<br>
<p>Turn around the question and look at it another way.  Humans are building 
<br>
the AI.  Humans are external to the AI.  Everything inside the AI that we 
<br>
*want* to be there is there because of something outside the AI.  Every time 
<br>
you ask yourself a question about Friendship design, a decision takes place 
<br>
inside your mind.  That decision is a real thing and it is shaped by 
<br>
reference to a number of things, from many different places perhaps, but all 
<br>
outside the AI.  Whatever exists inside the AI should be, from the *AI's* 
<br>
perspective, an external reference to the intentions of the programmers, 
<br>
which in turn may be treated as an external reference to whatever 
<br>
considerations the programmer used to make the decision.  As long as the AI 
<br>
regards its own Friendship system in this way, it is externally anchored.
<br>
<p>Now it may be necessary at some point for the AI to begin snipping the 
<br>
programmers out of the loop and using the considerations that the 
<br>
programmers use, directly, but this process is something that occurs with 
<br>
the cooperation of the programmers and the supervision of the programmers, 
<br>
and should not need to be rushed beyond what the AI is capable of handling 
<br>
mentally at any given point.  And beyond that, if the AI is to really grow 
<br>
into the Singularity in the same way that humans grew beyond the goals of 
<br>
the evolutionary metaprogram, it may be necessary for the AI to add 
<br>
basic-level moral content beyond what was available to the programmers, but 
<br>
*if* so, it would be for reasons that made sense under the moral philosophy 
<br>
handed down from the humans, just as our own reasons for defying evolution 
<br>
are evolved ones.  In essence this just says that an AI would possess the 
<br>
same ability to &quot;grow beyond&quot; as a human upload.
<br>
<p><em> &gt; Given
</em><br>
<em>&gt; much greater knowledge and intelligence what we attribute to friendly 
</em><br>
<em>&gt; behavior may end up looking quite different.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Your definition of ethics is a good example.  If an alien landed 
</em><br>
<em>&gt; tomorrow and the first person it met was a fantastic salesman, the 
</em><br>
<em>&gt; salesman may appear to be exceedingly friendly.  When in fact their only 
</em><br>
<em>&gt; goal is to open up a new trade route and they don't in fact care one 
</em><br>
<em>&gt; iota about the alien, only the result!  ;)
</em><br>
<p>Which problem are we discussing here?  The idea that a hostile AI could 
<br>
deliberately lie in order to masquerade as Friendly?  Or the assertion that 
<br>
a Friendship programming team would wind up with a hostile AI that appears 
<br>
as Friendly because the specification was ambiguous?  These problems are 
<br>
very different structurally!
<br>
<p>If there may be more than one cause of Friendly-seeming behavior, that could 
<br>
break a system that anchors only in immediate feedback about what is and 
<br>
isn't Friendly in the real world - for example, a blind neural network 
<br>
training algorithm (if generic nets could be trained on problems like that, 
<br>
which they can't).  However, a system that asks questions about imaginary 
<br>
scenarios, or which receives information about imaginary scenarios, is 
<br>
likely to quickly receive information that distinguishes between the two 
<br>
models.  And a system that asks questions *about* reasons for good behavior 
<br>
can *directly* disambiguate between the two models.
<br>
<p><em>&gt; We may *think* we are defining friendliness via external reference 
</em><br>
<em>&gt; points but actually be defining only the appearance of friendliness or 
</em><br>
<em>&gt; something similar.  Thus the SI would only need to appear friendly to 
</em><br>
<em>&gt; us, even while it was planning to turn the planet into computing resources.
</em><br>
<p>That's why you discuss (anchor externally) the *reasons* for decisions, not 
<br>
just the decision outputs.  You aren't anchoring the final output of the 
<br>
causal system, you're anchoring *all* the nodes in the system.
<br>
<p><em>&gt;&gt; Furthermore, programmers are physical objects and the intentions of 
</em><br>
<em>&gt;&gt; programmers are real properties of those physical objects.  &quot;The 
</em><br>
<em>&gt;&gt; intention that was in the mind of the programmer when writing this 
</em><br>
<em>&gt;&gt; line of code&quot; is a real, external referent; a human can understand it, 
</em><br>
<em>&gt;&gt; and an AI that models causal systems and other agents should be able 
</em><br>
<em>&gt;&gt; to understand it as well. Not just the image of Friendliness itself, 
</em><br>
<em>&gt;&gt; but the entire philosophical model underlying the goal system, can be 
</em><br>
<em>&gt;&gt; defined in terms of things that exist outside the AI and are subject 
</em><br>
<em>&gt;&gt; to discovery.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; A human can understand the words &quot;The intention that was in the mind of 
</em><br>
<em>&gt; the programmer when writing this line of code&quot;, but they could never 
</em><br>
<em>&gt; fully UNDERSTAND it.  This is why I think you need to have more real 
</em><br>
<em>&gt; life experience, Eliezer.  Those of us that are married can easily 
</em><br>
<em>&gt; understand why the above is not possible.  You can never FULLY 
</em><br>
<em>&gt; understand what someone else intends by something.
</em><br>
<p>You don't need perfect understanding.  You just need approximate sensory 
<br>
information that provides enough information to build an approximate model 
<br>
that controls decisions well enough for a Friendly AI to get by at any given 
<br>
point.  When the FAI is infrahuman it can just *ask* whenever it's not 
<br>
unsure of something.  When the FAI is transhuman it can unpack all the 
<br>
references that didn't make sense earlier.  So infrahuman understanding 
<br>
should be enough to govern infrahuman decisions, and transhuman 
<br>
understanding should be enough to govern transhuman decisions.
<br>
<p>To solve the transhuman problem posed by transhuman AI, you have to figure 
<br>
out how to use the AI's transhuman intelligence to solve the problem, not 
<br>
rely on solving it yourself.  That's Friendly AI.
<br>
<p><em>&gt; To use Eliezer's method, while I may not be correct I'm quite certain 
</em><br>
<em>&gt; you are wrong.  (Does that make me an honorary Friendship Programmer?)
</em><br>
<p>No, that makes you a perfectionist by successive approximation.  But asking 
<br>
structural questions about metawish construction scores points.  (Whee! 
<br>
Eugenese!)
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4376.html">Eliezer S. Yudkowsky: "Friendly AI koans"</a>
<li><strong>Previous message:</strong> <a href="4374.html">Michael Roy Ames: "Opposition to Ideas, and win-win scenarios."</a>
<li><strong>In reply to:</strong> <a href="4363.html">James Higgins: "Re: Self-modifying FAI (was: How hard a Singularity?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4387.html">James Higgins: "Re: Self-modifying FAI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4387.html">James Higgins: "Re: Self-modifying FAI (was: How hard a Singularity?)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4375">[ date ]</a>
<a href="index.html#4375">[ thread ]</a>
<a href="subject.html#4375">[ subject ]</a>
<a href="author.html#4375">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
