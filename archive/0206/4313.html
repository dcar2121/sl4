<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: How hard a Singularity?</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: How hard a Singularity?">
<meta name="Date" content="2002-06-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: How hard a Singularity?</h1>
<!-- received="Tue Jun 25 13:01:23 2002" -->
<!-- isoreceived="20020625190123" -->
<!-- sent="Tue, 25 Jun 2002 11:02:03 -0600" -->
<!-- isosent="20020625170203" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: How hard a Singularity?" -->
<!-- id="LAEGJLOGJIOELPNIOOAJKEJMCLAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D189737.20702@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20How%20hard%20a%20Singularity?"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Tue Jun 25 2002 - 11:02:03 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4314.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Previous message:</strong> <a href="4312.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>In reply to:</strong> <a href="4311.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4319.html">James Higgins: "RE: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4319.html">James Higgins: "RE: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4313">[ date ]</a>
<a href="index.html#4313">[ thread ]</a>
<a href="subject.html#4313">[ subject ]</a>
<a href="author.html#4313">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eli wrote:
<br>
<em>&gt; Ben, Nick Bostrom has already written a formal analysis of this
</em><br>
<em>&gt; one, coming
</em><br>
<em>&gt; to the same conclusion; humanity's future is larger than its present, so
</em><br>
<em>&gt; *IF* there is a conflict, the moral thing is to increase the
</em><br>
<em>&gt; probability of
</em><br>
<em>&gt; a (safe) Singularity even at the expense of time-to-Singularity.
</em><br>
<em>&gt; *BUT* the
</em><br>
<em>&gt; only consideration I know of in which spending more time could
</em><br>
<em>&gt; conceivably
</em><br>
<em>&gt; buy you anything at all in the way of safety is the Friendly AI
</em><br>
<em>&gt; part of the
</em><br>
<em>&gt; AI problem
</em><br>
<p>More time could also buy us one other thing.  It could buy us time to
<br>
reflect on the Singularity more thoroughly -- collectively, as a species --
<br>
thus perhaps discovering other ways in which a delay could be helpful, which
<br>
are not apparent to us now ;)
<br>
<p>I mostly like Nick Bostrom's thinking on the Singularity, and I mostly like
<br>
yours as well, but I am not confident that even folks as smart as you, me
<br>
and Nick and the other folks on this list have fully understood the various
<br>
aspects of &quot;what humanity could do to make the Singularity work out better.&quot;
<br>
<p>Personally I am not NOW advocating trying to slow down the Singularity.  But
<br>
as things get closer, I'll certainly be watchful for evidence that this
<br>
makes sense.  (Although I recognize that even if I should, in the future,
<br>
decide the Singularity should be slowed down, I may well lack the power to
<br>
do anything about it.)
<br>
<p>I don't want to die, I don't want my children or any other living beings to
<br>
die (unless, like my wife, they wish to die when their biologically allotted
<br>
time is up), and I don't want to be stuck in this limited human form with
<br>
this limited human intelligence forever.  I want the Singularity.
<br>
<p>But I also don't trust YOUR, or MY, or anyone else's theory of &quot;how to make
<br>
AI's friendly&quot; or &quot;how to make the Singularity come out well.&quot;   It worries
<br>
me that you are so confident in your own theory of how to make the
<br>
Singularity come out well, when in fact you like all the rest of us are
<br>
confronting an unknown domain of experience, in which all our thoughts and
<br>
ideas may prove irrelevant and overly narrow.
<br>
<p>It would be nice to approach the Singularity with a far better understanding
<br>
of how to make Singularities come out nicely.
<br>
<p>One thing to hope for is that, once we create
<br>
slightly-more-than-human-level-intelligence AGI's, these AGI's will help us
<br>
to arrive at a better understanding of these issues -- and help us sculpt
<br>
the Singularity in ways that will make it beneficial for us and for them as
<br>
well as for their successors.
<br>
<p>This has a better chance of occurring if the takeoff is semihard (i.e.
<br>
exponential, but with a smaller exponent than you conjecture).
<br>
<p><em>&gt; In all other cases, the question is unambiguous; the sooner you
</em><br>
<em>&gt; make it to
</em><br>
<em>&gt; the Singularity, the safer you are.  A multipolar militarized
</em><br>
<em>&gt; technological
</em><br>
<em>&gt; civilization containing solely human-level intelligences is just not safe.
</em><br>
<p>Yes, if you assume the Singularity will come out well, then the sooner you
<br>
make it there, the better off you are.  Obviously.
<br>
<p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4314.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Previous message:</strong> <a href="4312.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>In reply to:</strong> <a href="4311.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4319.html">James Higgins: "RE: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4319.html">James Higgins: "RE: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4313">[ date ]</a>
<a href="index.html#4313">[ thread ]</a>
<a href="subject.html#4313">[ subject ]</a>
<a href="author.html#4313">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
