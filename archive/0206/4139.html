<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The Human Brain.</title>
<meta name="Author" content="Mike & Donna Deering (deering9@mchsi.com)">
<meta name="Subject" content="Re: The Human Brain.">
<meta name="Date" content="2002-06-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The Human Brain.</h1>
<!-- received="Sat Jun 22 15:03:09 2002" -->
<!-- isoreceived="20020622210309" -->
<!-- sent="Sat, 22 Jun 2002 13:46:48 -0500" -->
<!-- isosent="20020622184648" -->
<!-- name="Mike & Donna Deering" -->
<!-- email="deering9@mchsi.com" -->
<!-- subject="Re: The Human Brain." -->
<!-- id="001301c21a1d$2a986780$f040da0c@mchsi.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="3D143F26.1010603@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Mike & Donna Deering (<a href="mailto:deering9@mchsi.com?Subject=Re:%20The%20Human%20Brain."><em>deering9@mchsi.com</em></a>)<br>
<strong>Date:</strong> Sat Jun 22 2002 - 12:46:48 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4140.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4138.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4128.html">Eliezer S. Yudkowsky: "Re: The Human Brain."</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4139">[ date ]</a>
<a href="index.html#4139">[ thread ]</a>
<a href="subject.html#4139">[ subject ]</a>
<a href="author.html#4139">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer: &quot;I don't see how Mike Deering gets from his statements to the above estimate. 
<br>
&nbsp;&nbsp;It seems like a non sequitur.&quot;
<br>
<p>Mike:  Firstly, I'm not trying to faithfully simulate any existing or generic human mind in silicon.  That would be a waste of time since most of the functions are unnecessary, flawed, or poorly designed. I'm looking for something like the modern chess programs' level of expertise except in the task of technological engineering design functions in the areas of nanotech, genetics/proteomics, or AI science.  Of course, not using the chess programs' algorithms or design architecture, but rather those appropriate for the problem.  And I am not saying we know what these are, just that they could be implemented on a machine of a certain size.  Secondly, I'm saying that because these algorithms are implemented in the human brain and that they comprise such a small portion of that brain's total functionality and that implementation is so inefficient that a more specialized and optimized silicon implementation should be able to run on a much smaller machine computationally.  Which means that we don't need to have crossover point (2021) computers to have human level problem solving capability.  Naturally the time estimate would need to be revised downward.  Which is heavily dependent on the doubling schedule.  If we are at ten months now when do we reach eight months?  six months?  three months?  one month?  two weeks?  one week?  three days?  one day?  twelve hours?  six hours?  three hours?  one hour?  How will society adapt to such rates of change?  Adapt or be left behind.  There are plenty of clever untried ideas out there.  Everyone has their pet mind model.  There are plenty of hackers out there with the drive and genius to solve very complex problems.  How many times have you heard from even people on this list that the underlying philosophy of AI will look surprisingly simplistic ( although complex in the details ) in hindsight?  The clock is ticking faster than you think.  The Singularity is less than five years away.  Imagine what could happen if some teenage hacker with world domination on his mind gets there first?  Imagine if DARPA (the weapons guys) get there first?  Imagine if I get there first?  Even if you accept everything I'm saying what can you do about it?
<br>
<p>Pick a project and send whatever you can scrap together to them, or read everything you can find on mind models and start hacking.
<br>
<p>Mike.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4140.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4138.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4128.html">Eliezer S. Yudkowsky: "Re: The Human Brain."</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4139">[ date ]</a>
<a href="index.html#4139">[ thread ]</a>
<a href="subject.html#4139">[ subject ]</a>
<a href="author.html#4139">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
