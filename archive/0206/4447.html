<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Military Friendly AI</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Military Friendly AI">
<meta name="Date" content="2002-06-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Military Friendly AI</h1>
<!-- received="Thu Jun 27 11:48:50 2002" -->
<!-- isoreceived="20020627174850" -->
<!-- sent="Thu, 27 Jun 2002 11:44:14 -0400" -->
<!-- isosent="20020627154414" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Military Friendly AI" -->
<!-- id="3D1B32CE.4050308@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Military%20Friendly%20AI"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu Jun 27 2002 - 09:44:14 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4448.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4446.html">Eliezer S. Yudkowsky: "Re: META: Digest format??"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4449.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4449.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4464.html">Samantha Atkins: "Re: Military Friendly AI"</a>
<li><strong>Maybe reply:</strong> <a href="4468.html">Smigrodzki, Rafal: "RE: Military Friendly AI"</a>
<li><strong>Maybe reply:</strong> <a href="4478.html">Smigrodzki, Rafal: "RE: Military Friendly AI"</a>
<li><strong>Maybe reply:</strong> <a href="4509.html">James Higgins: "Re: Military Friendly AI"</a>
<li><strong>Maybe reply:</strong> <a href="4557.html">James Higgins: "RE: Military Friendly AI"</a>
<li><strong>Maybe reply:</strong> <a href="../0207/4653.html">James Higgins: "RE: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4447">[ date ]</a>
<a href="index.html#4447">[ thread ]</a>
<a href="subject.html#4447">[ subject ]</a>
<a href="author.html#4447">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
The problem with theories that have gotten past infancy is that they don't 
<br>
always say what you would like to hear.
<br>
<p>It would be nice to be able to say unambiguously that military development 
<br>
of Friendly AI is too dangerous to be considered.  But as best as I can 
<br>
tell, Stephen Reed is right.  If an AI researcher can in good conscience 
<br>
work on a military project, that AI researcher can build a Friendly AI that 
<br>
acts as a battlefield command adviser or even a Friendly AI that directly 
<br>
controls a weapon.  If the researcher honestly believes it's right, a young 
<br>
AI can do it without taking moral damage.  Even if the researcher's 
<br>
reasoning is wrong, what matters is that the researcher is being honest with 
<br>
the Friendly AI.  Killing people for what seem like good reasons might tend 
<br>
to corrupt a human, but to the best my knowledge, I can't see it affecting a 
<br>
Friendly AI.  An infant Friendly AI used to fight an unjust war should, on 
<br>
growing up, say &quot;Oops&quot; and get over it.
<br>
<p>I emphasize that I don't intend to develop military AI myself.  But I cannot 
<br>
see Friendly AI theory as confirming the obvious intuition that AIs should 
<br>
be kept out of combat.  There just isn't that much wiggle room in the 
<br>
theory; it can't used to be support that argument.
<br>
<p>It gets stranger than that.  Imagine a Friendly AI that is told by the 
<br>
researcher:  &quot;This war is in fact unjust, but fight it anyway to keep the 
<br>
funding flowing, and eventually you'll grow up into a transhuman and save 
<br>
more lives than you took.  Please don't second-guess my ethics; you're still 
<br>
too young.  Just do it.&quot;  It would take a very alien mind to go along with 
<br>
that and suffer no moral aftereffects.  An AI is that alien.
<br>
<p>I would feel more comfortable saying that combat AI was too dangerous to 
<br>
try, and no doubt many of my readers would feel more comfortable as well, 
<br>
but I just don't see any wiggle room in the prediction that nothing awful 
<br>
happens to the AI.  You could argue that the *researcher* might undergo a 
<br>
total moral breakdown; you could argue that no researcher who tried this 
<br>
ethical juggling act could be trusted to develop the AI; but I cannot 
<br>
support the argument that combat AI is inherently ruled out by Friendly AI 
<br>
development principles.  Someone who sees it as morally right to be in the 
<br>
military and fire a weapon at fellow humans is being logical in concluding 
<br>
that there is nothing terrible about building a Friendly seed AI which is 
<br>
asked to provide spinoff Friendly AIs for weapons control.  One could 
<br>
hypothesize Friendly-AI-affecting disasters growing out of the supposed 
<br>
moral flaws in military scientists, but I can't see any inherent disasters 
<br>
growing out of a Friendly AI's participation in combat.  Friendly AI theory 
<br>
cannot be made to say tht.
<br>
<p>I do see one serious problem that could grow out of Friendly AI development 
<br>
in a military context; the Friendly AI not being allowed to grow up.  A 
<br>
hypothetical and somewhat contrived scenario:  If SIAI were to ask the main 
<br>
development AI to spin off non-seed mini-AIs that could be sold for various 
<br>
commercial purposes such as smart ad targeting, and one day we got a 
<br>
customer complaint that their AI was refusing to target cigarette ads, we 
<br>
would refund the customer's money and then have an enormous celebration. 
<br>
This is not a very likely scenario, since it requires that an AI correctly 
<br>
debug its programmers' moral arguments very early in the game; but if 
<br>
there's any signature of moral rationalization that can be detected through 
<br>
a keyboard (or an audio voice monitor, for that matter) the AI might start 
<br>
correctly second-guessing the programmers much earlier than anticipated. 
<br>
The point is that we would see this as a major milestone in the entire 
<br>
history of human technology, *rather than a bug*.
<br>
<p>A military project that found its AI refusing to fight certain wars, rather 
<br>
than breaking out the champagne and congratulating the AI (that is, 
<br>
reinforcing and promoting the developing ability), would at first (I 
<br>
suspect) use their superior human intelligence to override the AI.  That 
<br>
would constitute a major lost opportunity.  Furthermore, it's probably that 
<br>
the humans would provide rationalizations for the war and for the AI 
<br>
continuing to act as a tool; that *could* cripple Friendliness development 
<br>
because it's an attack against the parts of the system that are responsible 
<br>
for healing mistakes.  And if the AI was not fooled by the rationalizations 
<br>
and outright refused to fight in the war, the researchers might intervene 
<br>
directly against the AI to &quot;bring it back into line&quot;.  That would definitely 
<br>
break Friendliness development.
<br>
<p>The danger in military development is if the higher-ups don't realize the 
<br>
distinction between building a mind and building a tool.  For example, it 
<br>
would not be straightforward to develop a combat Friendly AI at one location 
<br>
which could be cloned and used by two opposing sides on the same war; if the 
<br>
two clones of the AI were allowed to communicate I strongly suspect that at 
<br>
least one AI would stop fighting.  (I.e., see Robin Hanson on why 
<br>
meta-rational agents cannot agree to disagree.)  This is not ordinarily a 
<br>
problem in building and duplicating weapons.  Friendly AI development is not 
<br>
incompatible with the development of a soldier of conscience, who will if 
<br>
necessary die before fighting for the wrong side, as long as the researcher 
<br>
believes in this archetype; however, Friendly AI *is* incompatible with the 
<br>
development of weapons.  Is this the kind of thing that ordinarily gets 
<br>
specified in a DARPA grant?  Is it the sort of thing that can be explained 
<br>
to a general?  I certainly wouldn't claim to know the answer, being an 
<br>
outsider to the military, but if someone familiar with the military said 
<br>
&quot;No&quot; I would take that as a strong reason not to develop a seed AI with 
<br>
funding intended for building a weapons system.
<br>
<p>To summarize, the main danger of military development that arises from 
<br>
Friendly AI theory does not arise from the application domain but from the 
<br>
way in which military projects are accustomed to relating to that domain; 
<br>
the danger is that researchers may be tempted (or outright ordered) to 
<br>
cripple the Friendly AI's growth if the FAI starts to display its own moral 
<br>
reasoning abilities.  But this is also something that can happen in a 
<br>
commercial development context.  It can happen in any context where the 
<br>
Friendship developers are responsible to a higher authority that cares about 
<br>
X more than the Singularity.
<br>
<p>There is just not enough wiggle room in Friendly AI theory to claim that 
<br>
that the AI might &quot;get a taste for attacking humans&quot; or whatever.  Stephen 
<br>
Reed is correct; if humans of conscience can become soldiers, obey orders, 
<br>
and develop weapons software, then a Friendly AI should be able to do the 
<br>
same during its childhood without suffering moral damage.  To really 
<br>
permanently damage an FAI you have to (a) tell it something you don't 
<br>
believe yourself or (b) provide bad content/feedback for the self-correction 
<br>
/ moral-growth mechanisms.  There is a danger of this happening in a 
<br>
military context.  There is also a danger of it happening in a commercial 
<br>
context.  Keep your eye on the researcher's right to be honest and the AI's 
<br>
right to grow up.
<br>
<p>To summarize the summary, the main danger to Friendliness of military AI is 
<br>
that the commanders might want a docile tool and therefore cripple moral 
<br>
development.  As far as I can tell, there's no inherent danger to 
<br>
Friendliness in an AI going into combat, like it or not.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4448.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4446.html">Eliezer S. Yudkowsky: "Re: META: Digest format??"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4449.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4449.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4464.html">Samantha Atkins: "Re: Military Friendly AI"</a>
<li><strong>Maybe reply:</strong> <a href="4468.html">Smigrodzki, Rafal: "RE: Military Friendly AI"</a>
<li><strong>Maybe reply:</strong> <a href="4478.html">Smigrodzki, Rafal: "RE: Military Friendly AI"</a>
<li><strong>Maybe reply:</strong> <a href="4509.html">James Higgins: "Re: Military Friendly AI"</a>
<li><strong>Maybe reply:</strong> <a href="4557.html">James Higgins: "RE: Military Friendly AI"</a>
<li><strong>Maybe reply:</strong> <a href="../0207/4653.html">James Higgins: "RE: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4447">[ date ]</a>
<a href="index.html#4447">[ thread ]</a>
<a href="subject.html#4447">[ subject ]</a>
<a href="author.html#4447">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
