<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Military Friendly AI</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Military Friendly AI">
<meta name="Date" content="2002-06-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Military Friendly AI</h1>
<!-- received="Thu Jun 27 17:55:26 2002" -->
<!-- isoreceived="20020627235526" -->
<!-- sent="Thu, 27 Jun 2002 14:28:48 -0700" -->
<!-- isosent="20020627212848" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Military Friendly AI" -->
<!-- id="3D1B8390.5070909@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D1B32CE.4050308@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Military%20Friendly%20AI"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Thu Jun 27 2002 - 15:28:48 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4465.html">Samantha Atkins: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4463.html">Eliezer S. Yudkowsky: "Re: Suicide by committee (was: How hard a Singularity?)"</a>
<li><strong>In reply to:</strong> <a href="4447.html">Eliezer S. Yudkowsky: "Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4468.html">Smigrodzki, Rafal: "RE: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4464">[ date ]</a>
<a href="index.html#4464">[ thread ]</a>
<a href="subject.html#4464">[ subject ]</a>
<a href="author.html#4464">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer S. Yudkowsky wrote:
<br>
<p><em>&gt; I emphasize that I don't intend to develop military AI myself.  But I 
</em><br>
<em>&gt; cannot see Friendly AI theory as confirming the obvious intuition that 
</em><br>
<em>&gt; AIs should be kept out of combat.  There just isn't that much wiggle 
</em><br>
<em>&gt; room in the theory; it can't used to be support that argument.
</em><br>
<em>&gt; 
</em><br>
<p>Fortunately, FAI theory is not all that we have at our disposal 
<br>
or all that is relevant to deciding whether it is ethical and 
<br>
reasonably safe to have the military controlling such an AI and 
<br>
using it for war.  Once you have trained the AI with the notion 
<br>
that it is alright to kill people some of the time I think you 
<br>
have created a fundamental danger though.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; I would feel more comfortable saying that combat AI was too dangerous to 
</em><br>
<em>&gt; try, and no doubt many of my readers would feel more comfortable as 
</em><br>
<em>&gt; well, but I just don't see any wiggle room in the prediction that 
</em><br>
<em>&gt; nothing awful happens to the AI.  
</em><br>
<p>This assumes the AI will go beyond the notion it is ok to kill 
<br>
people.  That is a big assumption.  Especially if it evolves 
<br>
beyond its human trainable stage while in war scenarios.
<br>
<p><p><p><em>&gt; I do see one serious problem that could grow out of Friendly AI 
</em><br>
<em>&gt; development in a military context; the Friendly AI not being allowed to 
</em><br>
<em>&gt; grow up.  A hypothetical and somewhat contrived scenario:  If SIAI were 
</em><br>
<em>&gt; to ask the main development AI to spin off non-seed mini-AIs that could 
</em><br>
<em>&gt; be sold for various commercial purposes such as smart ad targeting, and 
</em><br>
<em>&gt; one day we got a customer complaint that their AI was refusing to target 
</em><br>
<em>&gt; cigarette ads, we would refund the customer's money and then have an 
</em><br>
<em>&gt; enormous celebration. This is not a very likely scenario, since it 
</em><br>
<em>&gt; requires that an AI correctly debug its programmers' moral arguments 
</em><br>
<em>&gt; very early in the game; but if there's any signature of moral 
</em><br>
<em>&gt; rationalization that can be detected through a keyboard (or an audio 
</em><br>
<em>&gt; voice monitor, for that matter) the AI might start correctly 
</em><br>
<em>&gt; second-guessing the programmers much earlier than anticipated. The point 
</em><br>
<em>&gt; is that we would see this as a major milestone in the entire history of 
</em><br>
<em>&gt; human technology, *rather than a bug*.
</em><br>
<em>&gt;
</em><br>
<p>It gets worse for a military AI.  It might decide that given the 
<br>
&nbsp;&nbsp;notion that it is alright to kill people for certain 
<br>
objectives of certain parties, that the objectives and/or 
<br>
parties it was working for are wrong and that it should kill 
<br>
people in service of some other objectives or parties or to 
<br>
further goals of its own.  This would be a bigger reason to 
<br>
freeze its development and loyalty if possible in a military 
<br>
context.  A military AI that is capable of thought and growth 
<br>
can too easily turn on its makers.  This is one of the stronger 
<br>
reasons imho why this is a very bad idea.
<br>
<p><em>&gt; To summarize the summary, the main danger to Friendliness of military AI 
</em><br>
<em>&gt; is that the commanders might want a docile tool and therefore cripple 
</em><br>
<em>&gt; moral development.  As far as I can tell, there's no inherent danger to 
</em><br>
<em>&gt; Friendliness in an AI going into combat, like it or not.
</em><br>
<em>&gt; 
</em><br>
<p>I don't like it and I think you are quite incorrect.
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4465.html">Samantha Atkins: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4463.html">Eliezer S. Yudkowsky: "Re: Suicide by committee (was: How hard a Singularity?)"</a>
<li><strong>In reply to:</strong> <a href="4447.html">Eliezer S. Yudkowsky: "Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4468.html">Smigrodzki, Rafal: "RE: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4464">[ date ]</a>
<a href="index.html#4464">[ thread ]</a>
<a href="subject.html#4464">[ subject ]</a>
<a href="author.html#4464">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
