<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Military Friendly AI</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Military Friendly AI">
<meta name="Date" content="2002-06-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Military Friendly AI</h1>
<!-- received="Thu Jun 27 13:54:39 2002" -->
<!-- isoreceived="20020627195439" -->
<!-- sent="Thu, 27 Jun 2002 13:53:11 -0400" -->
<!-- isosent="20020627175311" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Military Friendly AI" -->
<!-- id="3D1B5107.9020200@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJEEOJCLAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Military%20Friendly%20AI"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu Jun 27 2002 - 11:53:11 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4454.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4452.html">James Higgins: "RE: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4449.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4454.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4454.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4467.html">Samantha Atkins: "Re: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4453">[ date ]</a>
<a href="index.html#4453">[ thread ]</a>
<a href="subject.html#4453">[ subject ]</a>
<a href="author.html#4453">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em> &gt;&gt; To summarize the summary, the main danger to Friendliness of military
</em><br>
<em> &gt;&gt; AI is that the commanders might want a docile tool and therefore
</em><br>
<em> &gt;&gt; cripple moral development.  As far as I can tell, there's no inherent
</em><br>
<em> &gt;&gt; danger to Friendliness in an AI going into combat, like it or not.
</em><br>
<em> &gt;
</em><br>
<em> &gt; In my view, the main danger to Friendliness of military AI is that the AI
</em><br>
<em> &gt; may get used to the idea that killing people for the right cause is not
</em><br>
<em> &gt; such a bad thing...
</em><br>
<p>Yes, this is the obvious thing to worry about.
<br>
<p><em> &gt; Your arguments for why Friendliness is consistent with military AI are
</em><br>
<em> &gt; based on your theory of a Friendly goal system as a fully logical,
</em><br>
<em> &gt; rational thing.
</em><br>
<p>As I understand &quot;rationality&quot;, association, intuition, pattern-recognition, 
<br>
et cetera, are extensions of rationality just as much as verbal logic.  If 
<br>
our culture thinks otherwise it's because humans have accumulated more 
<br>
irrationality-correctors in verbal declarative form than intuitive form and 
<br>
hence associate rationality with logic.  From a mind-in-general's 
<br>
perspective these are different forms of rational intelligence, not rational 
<br>
and irrational intelligence.  Anyway...
<br>
<p>An AI can learn the programmer's mistakes in verbal form, associational 
<br>
form, recognized patterns, et cetera.  The critical issue is whether, when 
<br>
the AI grows up, the AI will be able to correct those mistakes.
<br>
<p><em> &gt; However, I think that any mind is also going to have an associational
</em><br>
<em> &gt; component, rivalling in power the logical component.
</em><br>
<p>As you are using the term &quot;logic&quot;, I do not believe in logical AI.  Also 
<br>
these are not &quot;components&quot;... oh, never mind.  Different theories.
<br>
<p><em> &gt; This means that its logical reasoning is going to be *guided* by
</em><br>
<em> &gt; associations that occur to it based on the sorts of things it's been
</em><br>
<em> &gt; doing, and thinking about, in the past...
</em><br>
<em> &gt;
</em><br>
<em> &gt; Thus, an AI that's been involved heavily in military matters, is going to
</em><br>
<em> &gt; be more likely to think of violent solutions to problems, because its
</em><br>
<em> &gt; pool of associations will push it that way
</em><br>
<p>And its memories, its concepts, its problem-solving skills, and so on.  But 
<br>
this is only a structural error if the AI attempts to kill its own 
<br>
programmers to solve a problem.  I suppose that's a possibility, but it 
<br>
really sounds more like the kind of thing that (a) happens in science 
<br>
fiction but not real life (as opposed to things which happen in science 
<br>
fiction and real life), and (b) sounds like a failure of infrahuman AI, 
<br>
which is probably not a Singularity matter.  Besides, I would expect a 
<br>
combat AI to be extensively trained in how to avoid killing friendly combatants.
<br>
<p><em> &gt; Remember, logic in itself does not tell you how to choose among the many
</em><br>
<em> &gt; possible series of logical derivations...
</em><br>
<p>I think you're working from a Spock stereotype of rationality.
<br>
<p><em> &gt; I don't want an AGI whose experience and orientation incline it to
</em><br>
<em> &gt; associations involving killing large numbers of humans!
</em><br>
<p>Despite an immense amount of science fiction dealing with this topic, I 
<br>
honestly don't think that an *infrahuman* AI erroneously deciding to solve 
<br>
problems by killing people is all that much of a risk, both in terms of the 
<br>
stakes being relatively low, and in terms of it really not being all that 
<br>
likely to happen as a cognitive error.  Because of its plot value, it 
<br>
happens much more often in science fiction than it would in reality.  (You 
<br>
have been trained to associate to this error as a perceived possibility at a 
<br>
much higher rate than its probable real-world incidence.)  I suppose if you 
<br>
had a really bad disagreement with a working combat AI you might be in 
<br>
substantially more trouble than if you had a disagreement with a seed AI in 
<br>
a basement lab, but that's at the infrahuman level - meaning, not 
<br>
Singularity-serious.  A disagreement with a transhuman AI is pretty much 
<br>
equally serious whether the AI is in direct command of a tank unit or sealed 
<br>
in a lab on the Moon; intelligence is what counts.
<br>
<p><em> &gt; You may say that *your* AGI is gonna be so totally rational that it will
</em><br>
<em> &gt; always make the right decisions regardless of the pool of associations
</em><br>
<em> &gt; that its experience provides to it....  But this does not reassure me
</em><br>
<em> &gt; adequately. What if you're wrong, and your AI turns out, like the human
</em><br>
<em> &gt; mind or Novamente, to allow associations to guide the course of its
</em><br>
<em> &gt; reasoning sometimes?
</em><br>
<p>Then the AI, when it's young, will kill a bunch of people it didn't really 
<br>
have to.  But that moral risk is inherent in joining the army or working on 
<br>
any military project.  The Singularity risk is if the AI's training trashes 
<br>
the part of the Friendship system that would be responsible for fixing the 
<br>
learned error when the AI grows up, or if the AI mistakenly self-modifies 
<br>
this system in a catastrophically wrong way.  I really don't see how that 
<br>
class of mistake pops out from an AI learning wrong but coherent and not 
<br>
humanly unusual rules for when to kill someone.  If the AI starts 
<br>
questioning the moral theory and the researcher starts offering a load of 
<br>
rationalizations which lead into dark places, then yes, there would be a 
<br>
chance of structural damage and the possibility of catastrophic failure of 
<br>
Friendliness.
<br>
<p>Ben, what makes you think that you and I, as we stand, right now, do not 
<br>
have equally awful moral errors embedded in our psyche?  A couple of 
<br>
centuries ago a lot of people thought slavery was a good thing.  That's why 
<br>
a Friendly AI's morals can't be frozen.  You need to pick a Friendly AI 
<br>
strategy that works if the researchers are honest with the AI and willing to 
<br>
see the AI grow up - not a strategy that requires the researchers be right 
<br>
about everything.  Yes, if you have a bunch of researchers who think it's 
<br>
okay to kill people, you get an AI that *starts out* thinking it's okay to 
<br>
kill people.  If you have researchers who share the Great Moral Error of the 
<br>
21st Century, whatever that is, the AI starts out by learning that moral 
<br>
error as well.  The question is whether the AI can outgrow moral errors. 
<br>
Civilizations have done so.  A human upload probably would.  Physical 
<br>
systems that outgrow moral errors exist; the question is building one.
<br>
<p>One must distinguish moral errors from metamoral errors.  Obviously, Ben's 
<br>
concept of morality with respect to military force is not shared by all AI 
<br>
researchers.  A young FAI built by honorable soldiers will draw on its 
<br>
programmers' advice to make decisions that Ben would regard as moral errors, 
<br>
and conversely, a young FAI built by Ben will make decisions that honorable 
<br>
soldiers might regard as moral cowardice.  Friendly AI is about making sure 
<br>
that both AIs grow up into essentially the same mind, whichever side that 
<br>
mature mind turns out to be on.  What if both sides are wrong?
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4454.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4452.html">James Higgins: "RE: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4449.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4454.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4454.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4467.html">Samantha Atkins: "Re: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4453">[ date ]</a>
<a href="index.html#4453">[ thread ]</a>
<a href="subject.html#4453">[ subject ]</a>
<a href="author.html#4453">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
