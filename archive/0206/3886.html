<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Books on rationality</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Books on rationality">
<meta name="Date" content="2002-06-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Books on rationality</h1>
<!-- received="Wed Jun 05 12:26:22 2002" -->
<!-- isoreceived="20020605182622" -->
<!-- sent="Wed, 05 Jun 2002 12:22:55 -0400" -->
<!-- isosent="20020605162255" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Books on rationality" -->
<!-- id="3CFE3ADF.8373916@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="06821C28-784C-11D6-9406-000A27B4DEFC@rbisland.cx" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Books%20on%20rationality"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Jun 05 2002 - 10:22:55 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="3887.html">Ben Goertzel: "RE: Books on rationality"</a>
<li><strong>Previous message:</strong> <a href="3885.html">Gordon Worley: "Re: Books on rationality"</a>
<li><strong>In reply to:</strong> <a href="3885.html">Gordon Worley: "Re: Books on rationality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3890.html">Cole Kitchen: "Re: Books on rationality"</a>
<li><strong>Reply:</strong> <a href="3890.html">Cole Kitchen: "Re: Books on rationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3886">[ date ]</a>
<a href="index.html#3886">[ thread ]</a>
<a href="subject.html#3886">[ subject ]</a>
<a href="author.html#3886">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Gordon Worley wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; You are playing a gambling game.  You have $500 dollars.  First you are
</em><br>
<em>&gt; given a choice:  you can either be given another $100 or you can try to
</em><br>
<em>&gt; win $500 more, but if you don't win, you get nothing.  It doesn't matter
</em><br>
<em>&gt; what the odds are, $100 is almost always the more rational choice
</em><br>
<em>&gt; because it guaranteed and most people will pick that one.  Then you are
</em><br>
<em>&gt; given a second choice:  you can either lose $100 or play a game where
</em><br>
<em>&gt; you might not lose any money, but if you loose you'll lose $500.  Most
</em><br>
<em>&gt; people will pick the latter in this case, which is an irrational
</em><br>
<em>&gt; choice.  The situation is the same as the first time, the only thing
</em><br>
<em>&gt; that changed was the sign on the numbers.  (example paraphrased from one
</em><br>
<em>&gt; in CFAI)
</em><br>
<p>This paraphrase is incorrect (or if not, I had better correct the original;
<br>
where is it in CFAI?).  CFAI should contain a paraphrase of Tversky and
<br>
Kahneman on the framing effect.  The framing effect experiment is as
<br>
follows:  Subjects are told to assume themselves $300 richer and are asked
<br>
to choose between a definite gain of $100 versus a 50% chance of gaining
<br>
$200, or subjects are told to assume themselves $500 richer and are asked to
<br>
choose between a sure loss of $100 versus a 50% chance of losing $200. 
<br>
Humans tend to choose the sure gain in the first case and gamble on avoiding
<br>
all loss in the second case.  The critical point is that this framing effect
<br>
holds true even though the outcome tree is exactly the same in both cases.
<br>
<p>What this shows is that human decisions are the outcome of a balance of
<br>
subjective cognitive forces, not a utility function.  (Caution:  My causal
<br>
account is based on my own understanding of intelligence and may differ
<br>
slightly from the conventional causal account.)  In one case the subjective
<br>
attractiveness of a novel $100 gain is balanced against the subjective
<br>
attractiveness of a $200 gain, and the subjective attractiveness of a sure
<br>
gain balanced against the subjective attractiveness of a risky gain.  Since
<br>
there is some subjective attractiveness that results simply from the charm
<br>
of gaining money, irrespective of amount, $200 is not twice as attractive as
<br>
$100.  Furthermore, being told that something has a 50% probability does not
<br>
multiply its subjective attractiveness by .5 - rather it is processed as a
<br>
&quot;risk&quot; of a certain subjective unpleasantness.
<br>
<p>Meanwhile, on the opposite side, subjects told to assume that they are $500
<br>
richer and choose between a sure $100 loss or a possible $200 loss.  I
<br>
expect that the primary force driving the decision is a sense of entitlement
<br>
to the full $500; having been told that it is theirs, the subjects do not
<br>
wish to give it up.  To make a decision to accept the full loss would
<br>
require giving up that sense of entitlement; giving up the possibility of
<br>
keeping everything.  This is difficult emotionally, so they choose to gamble
<br>
on a 50% chance of keeping everything.
<br>
<p>Or at least that is how I would explain it.  I would note for the record
<br>
that there are currently papers arguing that the current experimental
<br>
evidence cannot be accounted for by subjective utility functions, even
<br>
sliding utility functions that morph at different levels of wealth; and it
<br>
is IMHO more plausible a priori that the human mind makes decisions using
<br>
complex subjective emotional mechanisms.
<br>
<p>Since everyone knows that $200 * .5 = $100, the subjects would find that
<br>
&quot;rationality&quot; (as they know it) provides no advice on how to proceed, and
<br>
would make decisions based strictly on subjective value.  This is not true
<br>
under Gordon Worley's paraphrase above.  A 50% chance of winning $500 is
<br>
almost always a better bet than a sure gain of $100, unless for some reason
<br>
you desperately need an extra $100 and no more.  Likewise, a sure loss of
<br>
$100 is better than a 50% chance of losing $500, unless losing $500 would
<br>
put you under some critical threshold.  But note that Gordon Worley says
<br>
that the sure gain of $100 is &quot;better&quot; because it is &quot;guaranteed&quot;.  As a
<br>
deliberative thought, this statement seems to correspond to the perceptual
<br>
flow of subjective value which I hypothesize for humans.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="3887.html">Ben Goertzel: "RE: Books on rationality"</a>
<li><strong>Previous message:</strong> <a href="3885.html">Gordon Worley: "Re: Books on rationality"</a>
<li><strong>In reply to:</strong> <a href="3885.html">Gordon Worley: "Re: Books on rationality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="3890.html">Cole Kitchen: "Re: Books on rationality"</a>
<li><strong>Reply:</strong> <a href="3890.html">Cole Kitchen: "Re: Books on rationality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#3886">[ date ]</a>
<a href="index.html#3886">[ thread ]</a>
<a href="subject.html#3886">[ subject ]</a>
<a href="author.html#3886">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
