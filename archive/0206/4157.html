<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: How hard a Singularity?</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: How hard a Singularity?">
<meta name="Date" content="2002-06-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: How hard a Singularity?</h1>
<!-- received="Sat Jun 22 18:03:00 2002" -->
<!-- isoreceived="20020623000300" -->
<!-- sent="Sat, 22 Jun 2002 16:00:46 -0600" -->
<!-- isosent="20020622220046" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: How hard a Singularity?" -->
<!-- id="LAEGJLOGJIOELPNIOOAJMEEMCLAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D14D230.1080007@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20How%20hard%20a%20Singularity?"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sat Jun 22 2002 - 16:00:46 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4158.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4156.html">James Higgins: "Re: Why bother (was Re: Introducing myself)"</a>
<li><strong>In reply to:</strong> <a href="4140.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4162.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4162.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="../0207/4631.html">Gordon Worley: "Re: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4157">[ date ]</a>
<a href="index.html#4157">[ thread ]</a>
<a href="subject.html#4157">[ subject ]</a>
<a href="author.html#4157">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; Ben Goertzel wrote:
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt;&gt; I suppose I could see a month, but anything longer than that is pretty
</em><br>
<em>&gt;  &gt;&gt; hard to imagine unless the human-level AI is operating at a subjective
</em><br>
<em>&gt;  &gt;&gt; slowdown of hundreds to one relative to human thought.
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; I understand that this is your intuition, but what is the reasoning
</em><br>
<em>&gt;  &gt; underlying it?
</em><br>
<em>&gt;
</em><br>
<em>&gt; That there is *nothing special* about human-equivalent intelligence!
</em><br>
<p>Wrong! There is something VERY special about human-level intelligence, in
<br>
the context of a seed AI that is initially created and taught by humans.
<br>
Human-level intelligence is the intelligence level of the AI's creators and
<br>
teachers.
<br>
<p>Our seed AI is going to get its human-level intelligence, not purely by its
<br>
own efforts, but largely based on the human-level intelligence of millions
<br>
of humans working over years/decades/centuries.
<br>
<p>The process by which the seed AI gets human-level inteligence is going to be
<br>
a combination of software engineering, self-organization, explicit teaching
<br>
by humans, and explicit self-modification by the AI.
<br>
<p>The process by which it gets from human-level intelligence to superhuman
<br>
intelligence will probably be different: teaching by humans will be less
<br>
useful, software engineering by humans may be more difficult... so it's left
<br>
with self-organization and explicit  self-modification.
<br>
<p>I am not at all saying that human intelligence is some kind of basic limit.
<br>
I am saying that becoming smarter than your creators and teachers is harder
<br>
than becoming AS SMART as them, because you no longer have as much help
<br>
along the path.
<br>
<p>It seems that sometimes you attempt to refute one of my arguments by
<br>
recycling a refutation you have previously created to use against another,
<br>
different argument (usually a much more foolish one than the one I'm
<br>
actually making!).
<br>
<p><em>&gt; You are now furthermore assuming that our AI can find no sufficiently
</em><br>
<em>&gt; remunerative employment, cannot borrow sufficient funding, cannot get a
</em><br>
<em>&gt; large number of donated cycles from interested scientists, cannot rent a
</em><br>
<em>&gt; computing grid for long enough to expand its mind and reengineer itself,
</em><br>
<em>&gt; cannot (or chooses not) to steal cycles, cannot design new hardware...
</em><br>
<p>No.  I am not assuming that the AI cannot do these things.
<br>
<p>I am assuming that the AI may take more than your posited ONE MONTH UPPER
<br>
BOUND to do these things, and to use these resources it has thus obtained to
<br>
turn itself into a superhuman intelligence.
<br>
<p><em>&gt;  &gt; This mind now has to re-engineer its software to make itself smarter.
</em><br>
<em>&gt;
</em><br>
<em>&gt; By hypothesis, the AI just made the leap to human-equivalent
</em><br>
<em>&gt; smartness.  We
</em><br>
<em>&gt; know from evolutionary experience that this is a highly significant
</em><br>
<em>&gt; threshold that opens up a lot of doors.  Self-improvement should be going
</em><br>
<em>&gt; sixty at this point.
</em><br>
<p>A metaphor like &quot;going sixty&quot; is not very convincing in the context of a
<br>
quantitative debate about the rate of a certain process ;&gt;   Especially
<br>
because 60 is a slow speed limit.  Here in New Mexico we can legally drive
<br>
75 ;-&gt;
<br>
<p><em>&gt; Because of what I see as the earthshattering impact of an AI transforming
</em><br>
<em>&gt; itself to one intelligence grade level above &quot;Ben or Eliezer&quot;.  The doors
</em><br>
<em>&gt; opened by this should be more than enough to take the AI to serious
</em><br>
<em>&gt; transhumanity.  In many ways humans are *wimps*, *especially*
</em><br>
<em>&gt; when it comes
</em><br>
<em>&gt; to code!  I just don't see it taking all that much effort to beat
</em><br>
<em>&gt; the pants
</em><br>
<em>&gt; off us *at AI design*.
</em><br>
<p>You may be right.  However, creating and implementing a design for a
<br>
superhuman AI in ONE YEAR rather than ONE MONTH would still definitely
<br>
qualify as &quot;beating the pants off us at AI design.&quot;  Maybe even the
<br>
undergarments too!
<br>
<p><em>&gt; Perhaps your differing intuition on this has to do with your belief that
</em><br>
<em>&gt; there is a simple mathematical essence to intelligence; you are
</em><br>
<em>&gt; looking at
</em><br>
<em>&gt; this supposed essence and saying &quot;How the heck would I
</em><br>
<em>&gt; re-engineer whatever
</em><br>
<em>&gt; the mathematical essence turns out to be?  It's an arbitrarily
</em><br>
<em>&gt; hard problem;
</em><br>
<em>&gt; we know nothing about it.&quot;
</em><br>
<p>Of course, this is not my perspective.  You should know my perspective
<br>
better than that by now.  I do think there is a very simple mathematical
<br>
essence to intelligence, but I don't think there is any simple mathematical
<br>
essence to *achieving a certain degree of intelligence within certain
<br>
resource constraints*.  And I do not think that I know nothing about the
<br>
essence of intelligence; I think I know a great deal about it!!
<br>
<p><em>&gt;  &gt; Maybe it won't go this way -- maybe no
</em><br>
<em>&gt; conceptual/mathematical/AI-design
</em><br>
<em>&gt;  &gt; hurdles will be faced by a human-level AI seeking to make itself vastly
</em><br>
<em>&gt;  &gt; superhuman.  Or maybe turning a human-level mind into a vastly
</em><br>
<em>&gt; superhuman
</em><br>
<em>&gt;  &gt;  mind will turn out to be a hard scientific problem, which takes our
</em><br>
<em>&gt;  &gt; human-level AI a nontrivial period of time to solve....
</em><br>
<em>&gt;
</em><br>
<em>&gt; Which all sounds reasonable until you realize that there's
</em><br>
<em>&gt; nothing special
</em><br>
<em>&gt; about &quot;human-level&quot; intelligence.  If, under our uncertainty, the AI
</em><br>
<em>&gt; trajectory with a big bottleneck between &quot;human-level&quot; and &quot;superhuman&quot;
</em><br>
<em>&gt; intelligence is plausible, then the 40 other trajectories with big
</em><br>
<em>&gt; bottlenecks between various degrees of infrahuman and transhuman AI are
</em><br>
<em>&gt; equally plausible.
</em><br>
<p>No, because in getting up to human-level intelligence, the system has our
<br>
help, and we have human-level intelligence.  We can teach it.  We cannot do
<br>
nearly so good a job of teaching a system to be vastly smarter than
<br>
ourselves!  There may well be other counterbalancing factors, but this
<br>
factor in itself would cause a slowdown in intelligence increase once the
<br>
human level is passed.
<br>
<p><em>&gt;  &gt; Perhaps, or perhaps not.  Perhaps the super-AI will realize that more
</em><br>
<em>&gt;  &gt; brainpower and more knowledge are not the path to greater wisdom ...
</em><br>
<em>&gt;  &gt; perhaps it will decide it's more important to let some of its
</em><br>
<em>&gt;  &gt; subprocesses run for a few thousand years and see how they come out!
</em><br>
<em>&gt;
</em><br>
<em>&gt; Okay, now you say that and see something we &quot;just don't know&quot;.  I
</em><br>
<em>&gt; hear you
</em><br>
<em>&gt; say that and what I see are a specific, highly anthropmorphic and even
</em><br>
<em>&gt; contemporary-culture-morphic memes about &quot;wisdom&quot;, and how wisdom
</em><br>
<em>&gt; relates to
</em><br>
<em>&gt; ostentatious ignorance of material things, the wisdom of
</em><br>
<em>&gt; inaction, stopping
</em><br>
<em>&gt; to eat the roses, and so on.
</em><br>
<p>I think that the possibility I raised in the paragraph you responded to is
<br>
very unlikely, but I also think it shouldn't be ruled out.  Of course, the
<br>
possibilities I think of are biased by my human nature and cultural
<br>
background.  There are many other possibilities that are less humanly
<br>
natural, that would still have similar results.
<br>
<p><em>&gt; Being &quot;uncertain&quot; is the easy way out.  &quot;Uncertainty abuse&quot; is a major
</em><br>
<em>&gt; source of modern-day irrationality.  It's socially acceptable and is
</em><br>
<em>&gt; frequently mistaken for rationality, which makes it doubly dangerous.
</em><br>
<p>Uncertainty can be used as a psychologically &quot;easy way out&quot;, but so can
<br>
overconfidence in one's intuitions and opinions.
<br>
<p>I don't think that I suffer from a paralyzing excess of uncertainty, not at
<br>
all.  I tend to make my best guess and then act on it.  I don't know 100%
<br>
that the Novamente design will work (you think it won't) ... but my best
<br>
guess is that it will, so I'm spending most of my time on it.
<br>
<p>On the other hand, I think it is possible that YOU suffer from an excessive
<br>
confidence in your own intuitions and opinions and your own potential
<br>
historical role.
<br>
<p>I see much less &quot;uncertainty abuse&quot; around me, than I see &quot;overconfidence
<br>
abuse&quot; -- people being very narrow-minded and overconfident in the things
<br>
they've grown up believing, and not willing to doubt their beliefs or
<br>
consider other ideas.
<br>
<p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4158.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4156.html">James Higgins: "Re: Why bother (was Re: Introducing myself)"</a>
<li><strong>In reply to:</strong> <a href="4140.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4162.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4162.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="../0207/4631.html">Gordon Worley: "Re: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4157">[ date ]</a>
<a href="index.html#4157">[ thread ]</a>
<a href="subject.html#4157">[ subject ]</a>
<a href="author.html#4157">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
