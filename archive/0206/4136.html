<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: How hard a Singularity?</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: How hard a Singularity?">
<meta name="Date" content="2002-06-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: How hard a Singularity?</h1>
<!-- received="Sat Jun 22 14:06:31 2002" -->
<!-- isoreceived="20020622200631" -->
<!-- sent="Sat, 22 Jun 2002 13:51:10 -0400" -->
<!-- isosent="20020622175110" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: How hard a Singularity?" -->
<!-- id="3D14B90E.2040002@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJGEEECLAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20How%20hard%20a%20Singularity?"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Jun 22 2002 - 11:51:10 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4137.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4135.html">Brian Atkins: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4134.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4138.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4138.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4164.html">James Higgins: "Re: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4136">[ date ]</a>
<a href="index.html#4136">[ thread ]</a>
<a href="subject.html#4136">[ subject ]</a>
<a href="author.html#4136">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer, while we're all free to our own differing intuitions, it seems
</em><br>
<em>&gt; wrong to me to can feel &quot;dead certain&quot; about something we've never seen
</em><br>
<em>&gt; before, that depends on technologies we don't yet substantialy understand.
</em><br>
<p>If I can figure out how to solve a problem myself, I usually feel 
<br>
comfortable calling it &quot;dead certain&quot; that a superintelligence can solve it. 
<br>
&nbsp;&nbsp;Motivations might be different, although in this case it is very difficult 
<br>
to see why they would be, but if I can see at least one easy way to go from 
<br>
superintelligence to nanotechnology in a matter of days or weeks, there are 
<br>
probably others.
<br>
<p><em>&gt; I think the period of transition from human-level AI to superhuman-level AI
</em><br>
<em>&gt; will be a matter of months to years, not decades.
</em><br>
<p>I suppose I could see a month, but anything longer than that is pretty hard 
<br>
to imagine unless the human-level AI is operating at a subjective slowdown 
<br>
of hundreds to one relative to human thought.
<br>
<p><em>&gt; Perhaps for a while, a superhuman AI among humans will be like a human among
</em><br>
<em>&gt; dogs.  A human among dogs *does* have a different and deeper understanding,
</em><br>
<em>&gt; and can do things no dog can do, including many things revolutionizing dogly
</em><br>
<em>&gt; existence ... but still, a human among dogs is not a god.  How long might a
</em><br>
<em>&gt; phase like this last?  Hard to say.
</em><br>
<p>If the human is thinking thousands of times faster than the dogs, it 
<br>
probably won't last very long from the dogs' perspective, however long it 
<br>
might seem to the human.  The AI is not coming into existence in a vacuum; 
<br>
the human world is supersaturated with tools that can be used to construct 
<br>
rapid infrastructure.
<br>
<p><em>&gt; Moravec-and-Kurzweil-style curve-plotting is interesting and important, but
</em><br>
<em>&gt; nevertheless, the problem of induction remains... .  All sorts of things
</em><br>
<em>&gt; could happen.  For instance, the superhuman AI's we build may continue to
</em><br>
<em>&gt; progress exponentially, but in directions other than those we foresee now.
</em><br>
<p>Even if your goal is to progress exponentially in enlightened spiritual 
<br>
directions, exponential physical progress is still a good way to get the 
<br>
computing power to support that enlightened spiritual stuff and bring others 
<br>
in on the fun.
<br>
<p><em>&gt; In short, as I keep repeating, one of the unknown things about our coming
</em><br>
<em>&gt; plunge into the Great Unknown, is how rapidly the plunge will occur, and the
</em><br>
<em>&gt; trajectory that the plunge will follow.   Dead certainty on these points
</em><br>
<em>&gt; seems inappropriate to me.
</em><br>
<p>I often encounter people who are amazed at my dead certainty that humanity 
<br>
evolved rather than being created.  Generic arguments against &quot;dead 
<br>
certainty&quot; are not relevant.
<br>
<p>Ben, we're &quot;uncertain&quot; relative to different priors.  I would guess that for 
<br>
you, the sentences &quot;The Singularity takes place slowly&quot; and &quot;The Singularity 
<br>
takes place quickly&quot; are sentences of equivalent complexity and your 
<br>
uncertainty manifests as a perceived balance between them.  For me, there's 
<br>
the positive statement &quot;I impose my human expectations on the Singularity 
<br>
and expect it to run on a human timescale and be limited by humanish things 
<br>
like factories and venture capital&quot; and &quot;I don't know what the heck the 
<br>
Singularity will use for manufacturing and computronium, but it's not going 
<br>
to be insanely slow like our own, special, human way of doing things.&quot; 
<br>
Humanity is a special case and our &quot;uncertainty&quot; has to be expressed 
<br>
relative to the knowledge that humanity is a special case, not &quot;uncertainty&quot; 
<br>
as a spherical volume centered around our own little island in the cosmos. 
<br>
I feel comfortable saying that the Singularity will be faster than our own 
<br>
dead-slow existence because the Singularity is not going to be 
<br>
anthropomorphic enough to run on our own private timescale.
<br>
<p>I am similarly dead certain that we will never see an AI sincerely giving 
<br>
Agent Smith's speech in _The Matrix_ - &quot;Humans are obsolete; we'll kill them 
<br>
and steal their mates and territory&quot;.
<br>
<p>That I am uncertain about many things does not change the fact that many 
<br>
constructions that seem quite plausible to some people are easily revealed 
<br>
as dead wrong.  If you like, don't think of me as being &quot;dead certain&quot; that 
<br>
the Singularity will be fast, just &quot;dead certain&quot; of the wrongness of the 
<br>
common reasons offered for why the Singularity would happen to run on a 
<br>
conveniently human timescale.
<br>
<p>Uncertainty is a rational quantity and hesitancy is a social quantity which 
<br>
is sometimes but not always correlated with uncertainty.  In this case, 
<br>
uncertainty is appropriate but hesitancy is not.  If I am uncertain about 
<br>
the essential strangeness of the Singularity, it doesn't mean that I need to 
<br>
be the least bit hesitant about shooting down the various human-generated 
<br>
(hence, humanly comprehensible) fallacies commonly offered because &quot;Well, 
<br>
we're all uncertain and nobody knows better than anyone else.&quot;  That's just 
<br>
faking the stereotypical appearance of rationality as it is interpreted by 
<br>
nonrationalists in a social context.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4137.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4135.html">Brian Atkins: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4134.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4138.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4138.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4164.html">James Higgins: "Re: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4136">[ date ]</a>
<a href="index.html#4136">[ thread ]</a>
<a href="subject.html#4136">[ subject ]</a>
<a href="author.html#4136">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
