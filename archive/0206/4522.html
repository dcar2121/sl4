<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: FAI means no programmer-sensitive AI morality</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="FAI means no programmer-sensitive AI morality">
<meta name="Date" content="2002-06-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>FAI means no programmer-sensitive AI morality</h1>
<!-- received="Fri Jun 28 19:39:25 2002" -->
<!-- isoreceived="20020629013925" -->
<!-- sent="Fri, 28 Jun 2002 19:38:57 -0400" -->
<!-- isosent="20020628233857" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="FAI means no programmer-sensitive AI morality" -->
<!-- id="3D1CF391.8070908@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="4.3.2.7.2.20020628110024.01cbe720@mail.earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20FAI%20means%20no%20programmer-sensitive%20AI%20morality"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri Jun 28 2002 - 17:38:57 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4523.html">Ben Goertzel: "RE: Who will launch the Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4521.html">James Rogers: "Re: Friendly Existential Wager"</a>
<li><strong>In reply to:</strong> <a href="4501.html">James Higgins: "RE: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4526.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4526.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4556.html">Eugen Leitl: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4522">[ date ]</a>
<a href="index.html#4522">[ thread ]</a>
<a href="subject.html#4522">[ subject ]</a>
<a href="author.html#4522">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
James Higgins wrote:
<br>
<em> &gt; At 09:50 PM 6/27/2002 -0600, Ben Goertzel wrote:
</em><br>
<em> &gt;&gt;
</em><br>
<em> &gt;&gt; Eliezer's approach to Friendliness relies on his own personal morals as
</em><br>
<em> &gt;&gt; well.  his are pretty similar to mine; for instance, he thinks that
</em><br>
<em> &gt;&gt; preserving lives forever is a good thing.  My wife, who believes in
</em><br>
<em> &gt;&gt; reincarnation, disagrees with me and Eli on this -- according to her
</em><br>
<em> &gt;&gt; moral standards, ending death goes against the natural cycle of karma
</em><br>
<em> &gt;&gt; and is thus probably not a good thing....
</em><br>
<em> &gt;
</em><br>
<em> &gt; Another good reason why morality should not be decided by a single
</em><br>
<em> &gt; individual.  Eliezer or Ben's morality may not allow death, thus severely
</em><br>
<em> &gt; going against Ben's wife's morals.  Ben's wife's morals, however, would
</em><br>
<em> &gt; not prevent any deaths, and thus would go strongly against Eliezer's and
</em><br>
<em> &gt; Ben's (and mine).  So maybe preventing deaths except where the individual
</em><br>
<em> &gt; does not want this protection is the best answer.  But it takes more than
</em><br>
<em> &gt; one viewpoint to even see this questions.
</em><br>
<p>Okay.  Stop here.  Both of you need to reread at least the opening sections 
<br>
of Friendly AI, because this is a blatantly wrong representation of my views.
<br>
<p>The *entire point* of Friendly AI is to eliminate dependency on the 
<br>
programmers' morals.  You can argue that this is impossible or that the 
<br>
architecture laid out in CFAI will not achieve this purpose, but please do 
<br>
not represent me as in any way wishing to construct an AI that uses 
<br>
Eliezer's morals.  I consider this absolute anathema.  The creators of a 
<br>
seed AI should not occupy any privileged position with respect to its 
<br>
morality.  Implementing the Singularity is a duty which confers no moral 
<br>
privileges upon those who undertake it.  The programmers should find 
<br>
themselves in exactly the same position as the rest of humanity.  If 
<br>
morality is objective the AI should converge to it.  If morality is 
<br>
subjective, then you have to be content with the AI randomly selecting a 
<br>
morality from the space of moralities that are as good as any other.  This 
<br>
is what you're asking the rest of the planet to do; how can you ask them to 
<br>
do that if you're not willing to do it yourself?  Ben's statement that his 
<br>
AI is good for Ben Goertzel is anathema to me.  What about everyone else on 
<br>
the planet?  Is it rational for them to try and shut Ben down?  The 
<br>
programmers have to find a way to place themselves in the same position as 
<br>
everyone else on the planet.  Again, you can claim that this is impossible 
<br>
or that my proposal for doing it is unworkable, but this is what I believe 
<br>
is the critical responsibility of anyone undertaking to enter the Singularity.
<br>
<p>In the words of Gordon Worley:  &quot;Oh, well, plenty of us were 
<br>
anti-Friendliness until we actually sat down and read CFAI.&quot;
<br>
<p>At this point, Higgins, your representation of what Friendly AI is about has 
<br>
diverged so enormously from the actual content of &quot;Friendly AI&quot; that I think 
<br>
you really need to stop and reread the opening sections.  The fact that you 
<br>
are comparing my personal moral beliefs about the value of life or death to 
<br>
Ben Goertzel's, as an indicator of who would be a better AI programmer, 
<br>
indicates that we are simply not discussing the same thing when we use the 
<br>
words &quot;Friendly AI&quot;.
<br>
<p>If you read my statement that &quot;It takes more wisdom to build an AI than to 
<br>
be on a committee&quot; as &quot;Anyone with enough wisdom to build an AI has enough 
<br>
wisdom to directly specify its morality&quot;... yikes, no wonder you're running 
<br>
scared.  I would never, ever say that.
<br>
<p>Although, if you think an AI's morality can be directly specified by an 
<br>
advisory board... well, you must see the problem as being a lot smaller.  I 
<br>
don't think that any human or group of humans should attempt to directly 
<br>
specify an AI's morality.  I think the problem is out of our reach.
<br>
<p>It is the programmer's responsibility, in designing the Friendliness 
<br>
strategy and architecture, to make sure that Ben's wife has just as much or 
<br>
just as little to fear whether the Friendliness content source is Eliezer, 
<br>
Ben, or Ben's wife herself.  (Not that you'd use one person as a 
<br>
Friendliness source in any case.)
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4523.html">Ben Goertzel: "RE: Who will launch the Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4521.html">James Rogers: "Re: Friendly Existential Wager"</a>
<li><strong>In reply to:</strong> <a href="4501.html">James Higgins: "RE: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4526.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4526.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4556.html">Eugen Leitl: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4522">[ date ]</a>
<a href="index.html#4522">[ thread ]</a>
<a href="subject.html#4522">[ subject ]</a>
<a href="author.html#4522">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
