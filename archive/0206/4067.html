<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Threats to the Singularity.</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Threats to the Singularity.">
<meta name="Date" content="2002-06-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Threats to the Singularity.</h1>
<!-- received="Mon Jun 17 11:14:26 2002" -->
<!-- isoreceived="20020617171426" -->
<!-- sent="Mon, 17 Jun 2002 11:13:36 -0400" -->
<!-- isosent="20020617151336" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Threats to the Singularity." -->
<!-- id="3D0DFCA0.6000102@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="6F1B8958-81F8-11D6-9BD5-000A27B4DEFC@rbisland.cx" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Threats%20to%20the%20Singularity."><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Jun 17 2002 - 09:13:36 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4068.html">Eugen Leitl: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4066.html">Eliezer S. Yudkowsky: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4063.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4070.html">Michael Roy Ames: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4070.html">Michael Roy Ames: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4076.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4067">[ date ]</a>
<a href="index.html#4067">[ thread ]</a>
<a href="subject.html#4067">[ subject ]</a>
<a href="author.html#4067">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Gordon Worley wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; On Monday, June 17, 2002, at 05:31  AM, Samantha Atkins wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; Do I read you correctly?  If I do, then why do you hold this 
</em><br>
<em>&gt;&gt; position?  If I read you correctly then how can you expect the 
</em><br>
<em>&gt;&gt; majority of human beings, if they really understood you, to consider 
</em><br>
<em>&gt;&gt; you as other than a monster?
</em><br>
<p>Shouldn't you be trying to figure out what's right before discussing its PR 
<br>
value?  Or are you arguing that the &quot;yuck factor&quot; reaction of many humans is 
<br>
representative of an actual moral wrong?  If so, why not argue the moral 
<br>
wrong itself, rather than arguing from the agreement of a large number of 
<br>
people who have not actually been consulted?
<br>
<p><em>&gt; If an SI said it needed to kill a bunch of humans, I would seriously 
</em><br>
<em>&gt; start questioning its motives.  Killing intelligent life is not 
</em><br>
<em>&gt; something to be taken lightly and done on a whim.  However, if we had a 
</em><br>
<em>&gt; FAI that was really Friendly and it said &quot;Gordon, believe me, the only 
</em><br>
<em>&gt; way is to kill this person&quot;, I would trust in the much wiser SI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is the kind of reaction I expect and, while I'm a bit disappointed 
</em><br>
<em>&gt; to get so much of it on SL4, therefore avoid pointing this view out.  I 
</em><br>
<em>&gt; never go out of my way to say that human life is not the most important 
</em><br>
<em>&gt; thing to me in the universe, but sometimes it is worth talking about.
</em><br>
<p>Exactly.  Morality, like rationality, is never on anyone's side.  The most 
<br>
you can try to do is end up being on the side of morality.  The price of 
<br>
seeing the morality of a situation clearly is that you start out by asking 
<br>
which side you should be on, rather than looking for a way to rationalize 
<br>
one side.  Sometimes, just as in rationality, evidence (or valid moral 
<br>
argument) is weighted very heavily on one side of the scales and judgement 
<br>
is easy, but it doesn't mean that judgement can be replaced with prejudgement.
<br>
<p>It goes back to that same principle of building something eternal.  This 
<br>
isn't a contest to see who can say the nicest things about humanity.  The 
<br>
decision that a universe with humanity or human-derived minds in it is what 
<br>
we want to see lasting through eternity is not a decision for either a 
<br>
Friendly AI or a human philosopher to make lightly, whether &quot;eternity&quot; is 
<br>
taken to mean a few billion years or an actual infinity.  Either way that's 
<br>
a hell of a long time.  Isn't it worth an hour to think about it today? 
<br>
Even if the moral question is &quot;trivial&quot;, in the mathematical sense of being 
<br>
a trivial consequence of the basic rules of moral reasoning, then this 
<br>
itself needs to be established.
<br>
<p>There are also penalties to intelligence if you stop thinking too early. 
<br>
What if humanity's survival was morally worthwhile given a certain easily 
<br>
achievable enabling condition, but a snap judgement caused you to miss it? 
<br>
I can't think of any concrete scenario matching this description, but I 
<br>
think that growing into a strong thinker involves thinking through every 
<br>
possibility.  The conclusions may be obvious but you still have to do the 
<br>
math to arrive at the obvious conclusions.  Otherwise you *don't know* the 
<br>
math!  Maybe this doesn't matter much if you're willing to go through your 
<br>
life on autopilot, but it sure as heck matters for building AI.  And the 
<br>
only way you can know the math is by being willing to emotionally accept 
<br>
either outcome when you start thinking.  You can't pretend to be able to 
<br>
accept either outcome in order to find the math.  You have to be able to 
<br>
*actually* accept the moral outcome whatever it is.  This is why 
<br>
&quot;attachment&quot;, even to good things that really turn out to be good, is a bad 
<br>
thing.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4068.html">Eugen Leitl: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4066.html">Eliezer S. Yudkowsky: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4063.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4070.html">Michael Roy Ames: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4070.html">Michael Roy Ames: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4076.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4067">[ date ]</a>
<a href="index.html#4067">[ thread ]</a>
<a href="subject.html#4067">[ subject ]</a>
<a href="author.html#4067">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
