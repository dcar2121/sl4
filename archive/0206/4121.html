<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Threats to the Singularity.</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Threats to the Singularity.">
<meta name="Date" content="2002-06-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Threats to the Singularity.</h1>
<!-- received="Fri Jun 21 13:33:49 2002" -->
<!-- isoreceived="20020621193349" -->
<!-- sent="Fri, 21 Jun 2002 11:15:37 -0600" -->
<!-- isosent="20020621171537" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Threats to the Singularity." -->
<!-- id="LAEGJLOGJIOELPNIOOAJGECLCLAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D0BCFE7.7080107@objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Threats%20to%20the%20Singularity."><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Fri Jun 21 2002 - 11:15:37 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4122.html">Ben Goertzel: "RE: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4120.html">Samantha Atkins: "Re: How Kurzweil lost the Singularity"</a>
<li><strong>In reply to:</strong> <a href="4021.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4181.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4181.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4121">[ date ]</a>
<a href="index.html#4121">[ thread ]</a>
<a href="subject.html#4121">[ subject ]</a>
<a href="author.html#4121">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hello Samantha!
<br>
<p><em>&gt; &gt;&gt;Well, in my personal opinion and speaking plainly, disowning
</em><br>
<em>&gt; &gt;&gt;one's species, literally all of humanity include one's self and
</em><br>
<em>&gt; &gt;&gt;loved ones, for an unknown set of hopefully stable higher
</em><br>
<em>&gt; &gt;&gt;intelligences is a very deep form of treason and betrays a
</em><br>
<em>&gt; &gt;&gt;singular contempt for humanity that I find utterly apalling.
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;
</em><br>
[Ben had written]:
<br>
<em>&gt; &gt; But I really feel this is not correct.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The attitude you describe does not necessarily imply a *contempt* for
</em><br>
<em>&gt; &gt; humanity.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; On the contrary, it *could* imply this, but it could also imply a mere
</em><br>
<em>&gt; &gt; *indifference* to humanity.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; How can or should one be &quot;indifferent&quot; to one's species and to
</em><br>
<em>&gt; the survival of all existing higher sentients on this planet?
</em><br>
<p>Let me be clear on one thing: I was not *advocating* indifference toward
<br>
humanity in my post!
<br>
<p>I was merely pointing out that indifference to humanity is one possible
<br>
motive behind caring more about future superinteligent beings -- contempt
<br>
(which you mentioned in the post to which I was replying) being a
<br>
*different* motive.
<br>
<p>As you know, my best guess is that superhuman AI's will rapidly become
<br>
relatively indifferent to humans -- not competing with us for resources
<br>
significantly, nor trying to harm us, but mostly being bored with us and
<br>
probably helping us out in offhanded ways.
<br>
<p><em>&gt; If one is for increasing intelligence (how one defines that and
</em><br>
<em>&gt; why it is the only or most primary value are good questions) and
</em><br>
<em>&gt; the increase of sentience, I fail to see how one can be cavalier
</em><br>
<em>&gt; about the destruction of all currently known sentients.  How can
</em><br>
<em>&gt; one stand for intelligence and yet not care about billions of
</em><br>
<em>&gt; intelligent beings that already exist?
</em><br>
<p>How can one care about life and yet accept the immense murder of ants that
<br>
comes along with, say, digging the foundation for a new house?
<br>
<p>An advanced superhuman AI may become aware of 1000's of other types of
<br>
life-forms or mind-forms that we cannot conceive of now.  From its point of
<br>
view, then, how critical will we be?  From your point of view, as an upload
<br>
with 1000x human intelligence and direct contact with these 1000's other
<br>
life forms as well, how important will humanity be to &quot;YOU&quot;?  Do you pretend
<br>
to know the answers to these questions?
<br>
<p><em>&gt; Why would you disown what is of value to you?  On the basis of a
</em><br>
<em>&gt;   hypothetically better intelligence (along some dimensions of
</em><br>
<em>&gt; &quot;better&quot;)?  Why would you and how is it justified to also
</em><br>
<em>&gt; casually speak of &quot;it&quot; being more important than not only your
</em><br>
<em>&gt; own values and life but that of all other human beings also?
</em><br>
<em>&gt; All I am pushing for here is that we pause before writing off
</em><br>
<em>&gt; the human race as inconsequential as long as we can get to
</em><br>
<em>&gt; Singularity.  To do so writes off the known for something that
</em><br>
<em>&gt; is unknown and unknowable.  That does not seem reasonable to me.
</em><br>
<p>I hope you're not meaning to imply that *I* am somehow writing off the human
<br>
race as inconsequential.  Not at all.
<br>
<p>The intention of my post was merely to distinguish between
<br>
<p>a) contempt of humanity
<br>
<p>b) indifference to humanity
<br>
<p>which are very different attitudes.  I do not personally hold either
<br>
attitude, but I can sympathize more with the &quot;indifference&quot; attitude --
<br>
because, from the grand perspective, one relatively primitive intelligent
<br>
species may not be all that important.
<br>
<p>What attitude do I take?
<br>
<p>Personally I try (and occasionally succeed ;) to practice the two Buddhist
<br>
virtues of compassion and nonattachment.    The combination of these is
<br>
tricky to master, as in a shallow sense they may seem to contradict each
<br>
other.
<br>
<p>In the context of the present discussion, being compassionate toward humans
<br>
means that one doesn't want them to suffer, and that one has respect for
<br>
humans' right to continue even as more advanced beings come along.  And
<br>
nonattachment means *simultaneously* with compassion, also understanding
<br>
that the human race does not have some kind of intrinsic special value as
<br>
compared to other forms of existence, intelligence and life -- it means
<br>
moving beyond one's biologically-based attachment to one's own species.
<br>
<p><em>&gt; &gt;&gt;How about we just grow a lot more sane human beings instead of
</em><br>
<em>&gt; &gt;&gt;digging continuously for technological fixes that really aren't
</em><br>
<em>&gt; &gt;&gt;fixes to the too often cussedness of local sentients?
</em><br>
<p>I think the right thing is for the human-race technological vanguard to
<br>
simultaneously work on building artificial superintelligence, AND on
<br>
creating better humans beings (genetic engineering, brain augmentation,
<br>
etc.).
<br>
<p>And in fact, this is what is happening.
<br>
<p><em>&gt; &gt;&gt; Replacing
</em><br>
<em>&gt; &gt;&gt;them with something that is faster and arguably smarter but may
</em><br>
<em>&gt; &gt;&gt;or may not be any more wise is not an answer.  Scrapping
</em><br>
<em>&gt; &gt;&gt;sentients is to be frowned upon even if you think you can and
</em><br>
<em>&gt; &gt;&gt;even do create sentients that are arguably better along some
</em><br>
<em>&gt; &gt;&gt;parameters.
</em><br>
<p>Yes, our value systems agree on this point.
<br>
<p>Like nearly all others on this list, I would like to see a future in which
<br>
enhanced humans and superintelligent AI's coexist in harmony and with
<br>
mutuall productive interactions.
<br>
<p>I do reject the notion that preserving the human race is of *absolutely
<br>
primary* importance, but according to my own ethics and aesthetics, it is
<br>
certainly *highly* important.
<br>
<p><em>&gt; &gt; &quot;Wisdom&quot; is a nebulous human concept that means different things to
</em><br>
<em>&gt; &gt; different people, and in different cultures.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I can tell you what it isn't.  It isn't about writing off all
</em><br>
<em>&gt; existing sentients in favor of something you think can be but
</em><br>
<em>&gt; you have no idea what it will become.
</em><br>
<p>I feel like you're attacking a straw man here, because no one on this list
<br>
suggested &quot;writing off all existent sentients&quot;, did they?  I certainly
<br>
didn't, far from it.
<br>
<p><em>&gt; Human is the only
</em><br>
<em>&gt; sentient basis we have to reason from and in any event is what
</em><br>
<em>&gt; we ourselves are and we have no choice but to reason from that
</em><br>
<em>&gt; basis.
</em><br>
<p>I think it is possible to achieve some degree of nonattachment from one
<br>
species, in terms of one's reasoning and one's value system.  Of course, one
<br>
can never completely remove inferential and emotional bias from oneself --
<br>
it's not even clear what this would mean!
<br>
<p><p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4122.html">Ben Goertzel: "RE: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4120.html">Samantha Atkins: "Re: How Kurzweil lost the Singularity"</a>
<li><strong>In reply to:</strong> <a href="4021.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4181.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4181.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4121">[ date ]</a>
<a href="index.html#4121">[ thread ]</a>
<a href="subject.html#4121">[ subject ]</a>
<a href="author.html#4121">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
