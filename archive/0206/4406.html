<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: How hard a Singularity?</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: How hard a Singularity?">
<meta name="Date" content="2002-06-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: How hard a Singularity?</h1>
<!-- received="Wed Jun 26 19:02:20 2002" -->
<!-- isoreceived="20020627010220" -->
<!-- sent="Wed, 26 Jun 2002 18:50:13 -0400" -->
<!-- isosent="20020626225013" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: How hard a Singularity?" -->
<!-- id="3D1A4525.4090203@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="4.3.2.7.2.20020626142728.01c1d018@mail.earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20How%20hard%20a%20Singularity?"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Jun 26 2002 - 16:50:13 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4407.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4405.html">Samantha Atkins: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4397.html">James Higgins: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4411.html">James Higgins: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4411.html">James Higgins: "Re: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4406">[ date ]</a>
<a href="index.html#4406">[ thread ]</a>
<a href="subject.html#4406">[ subject ]</a>
<a href="author.html#4406">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
James Higgins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Aargh, this is frustrating.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The committee is there for RISK MANAGEMENT.  A task which should very 
</em><br>
<em>&gt; much be done thoroughly on such a task as creating a Singularity.
</em><br>
<p>A task at which committees are known to abjectly suck.
<br>
<p><em> &gt; They
</em><br>
<em>&gt; do not have to, collectively, understand all the inner working of the 
</em><br>
<em>&gt; design.  They simply have to be convinced to a reasonable degree that 
</em><br>
<em>&gt; the design, as a whole, is safe.  There are many such examples of this 
</em><br>
<em>&gt; in present day life, where an entity is responsible for ensuring 
</em><br>
<em>&gt; safety.  If it is impossible for a group of 10 intelligent people to 
</em><br>
<em>&gt; agree that it is safe to launch a Singularity then, frankly, it 
</em><br>
<em>&gt; shouldn't be launched.
</em><br>
<p>I see.  10 intelligent people?  Let me inquire about the selection process 
<br>
for this committee.  Is it a government committee?  Does it have legal 
<br>
enforcement powers?  Is it sponsored by a nonprofit such as, oh, SIAI?  Who 
<br>
gets to pick the guardians?  I might *maybe* consider trying to justify 
<br>
matters to a majority of a committee composed of Nick Bostrom, Mitchell 
<br>
Porter, Vernor Vinge, Greg Egan, and... damn, can't think of anyone else 
<br>
offhand who isn't already on my side.
<br>
<p><em>&gt; So, Eliezer, your saying that if YOU were appointed to such a committee 
</em><br>
<em>&gt; you would all of a sudden stop thinking rationally and start spouting 
</em><br>
<em>&gt; off Asimov Laws and such?  You think we should throw darts at the white 
</em><br>
<em>&gt; pages to pick the members of the committee or something?  Your making my 
</em><br>
<em>&gt; case for me here as to why a single individual should not be trusted 
</em><br>
<em>&gt; with this decision.
</em><br>
<p>I think that I would not be altered by my appointment to such a committee, 
<br>
but I have no confidence in my ability to keep a committee thinking 
<br>
rationally.  I can solve a problem posed by Nature because Nature is not as 
<br>
perverse as humans.  I can *maybe* explain to a committee composed of the 
<br>
smartest people I know, but I make no guarantees.
<br>
<p><em>&gt;&gt; It is terribly dangerous to take away the job of Friendly AI from 
</em><br>
<em>&gt;&gt; whoever was smart enough to crack the basic nature of intelligence!  
</em><br>
<em>&gt;&gt; Friendly AI is not as complex as AI but it is still the second hardest 
</em><br>
<em>&gt;&gt; problem I have ever encountered.  A committee is not up to that!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; A committee may not be up to designing a Friendly AI (because design by 
</em><br>
<em>&gt; committee is slow for one) but there is no reason they could not decide 
</em><br>
<em>&gt; if a given design was SAFE.
</em><br>
<p>The task of verification is easier than the task of invention but is *not* 
<br>
easy in any absolute sense.
<br>
<p><em> &gt; Your seem rather convinced that human
</em><br>
<em>&gt; beings can't be trusted to make their own decisions (based on 
</em><br>
<em>&gt; post-Singularity speculation you've posted)
</em><br>
<p>Eh?  What?  I think that there's a *possibility* that you won't be able to 
<br>
trust *all* of the people *all* of the time post-Singularity, and I've 
<br>
posted arguments that this need not be a catastrophe.
<br>
<p><em> &gt; so why should we trust
</em><br>
<em>&gt; whoever gets their first to make such major decisions?  Just because 
</em><br>
<em>&gt; someone is INTELLIGENT enough to design an AI doesn't mean they are WISE 
</em><br>
<em>&gt; enough to use it properly.  Intelligence does not equate to wisdom.
</em><br>
<p>Ah.
<br>
<p>&quot;Intelligence does not equate to wisdom.&quot;
<br>
<p>How many times I've heard that...
<br>
<p>Do you think it's possible to build an AI without wisdom?  Forget whether 
<br>
you think I'm wise.  Forget whether I, personally, manage to create AI. 
<br>
Consider how many times AI projects have failed, and the reasons for which 
<br>
they failed.  Consider how much self-awareness it takes, or creates, to 
<br>
reach an understanding of how minds work.  Building AI feeds off 
<br>
self-awareness and, in feeding on it, hones it.  If you don't believe this 
<br>
of me, fine; predict that I will fail to build AI.
<br>
<p>This is the task of building a mind.  It isn't a small thing to succeed in 
<br>
building a real mind.  I would consider it a far greater proof of wisdom 
<br>
than membership in any committee that ever existed.
<br>
<p>If I'm as lousy at this job as you seem to think, then I will fail to build 
<br>
AI in the first place.  The problem is harder than that.
<br>
<p><em>&gt;&gt; Then they're too big for N people to make and should be passed on to a
</em><br>
<em>&gt;&gt; Friendly SI or other transhuman.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So, how do you propose we find a Friendly SI or Transhuman to judge 
</em><br>
<em>&gt; which Singularity attempts will be safe?
</em><br>
<p>I don't see how intelligence on that particular task is any greater for N 
<br>
people than for one person, nor do I see how the moral problem gets any 
<br>
better unless you're conducting a majority vote of the entire human species, 
<br>
and even then I'm not sure it gets any better; minorities have rights too.
<br>
<p><em>&gt;&gt; Friendly AI is a test of intelligence.  If the minimum intelligence to 
</em><br>
<em>&gt;&gt; crack Friendly AI is more than the maximum intelligence of a 
</em><br>
<em>&gt;&gt; committee, turning the problem over to a committee guarantees a loss.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Neither Friendly AI nor the Singularity is a TEST of any kind.  Neither 
</em><br>
<em>&gt; is it a competition!  No one should be in a race to create the 
</em><br>
<em>&gt; Singularity to prove anything.  Such thinking will certainly be the 
</em><br>
<em>&gt; demise of us all.
</em><br>
<p>The Singularity is not a race, but it is a test.  All problems posed by 
<br>
Nature are tests - goals, problems, challenges, whatever you wish to call 
<br>
them.  I'm not sure what dreadful connotations this has for you but it seems 
<br>
pretty innocent to me.  Friendly AI is a problem domain in which the most 
<br>
in-demand qualities needed for success are creative intelligence, 
<br>
reflectivity, altruism, and good intentions for the Singularity, *in that 
<br>
order*.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4407.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4405.html">Samantha Atkins: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4397.html">James Higgins: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4411.html">James Higgins: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4411.html">James Higgins: "Re: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4406">[ date ]</a>
<a href="index.html#4406">[ thread ]</a>
<a href="subject.html#4406">[ subject ]</a>
<a href="author.html#4406">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
