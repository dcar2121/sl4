<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: How Kurzweil lost the Singularity</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: How Kurzweil lost the Singularity">
<meta name="Date" content="2002-06-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: How Kurzweil lost the Singularity</h1>
<!-- received="Wed Jun 19 18:41:30 2002" -->
<!-- isoreceived="20020620004130" -->
<!-- sent="Wed, 19 Jun 2002 16:57:37 -0400" -->
<!-- isosent="20020619205737" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: How Kurzweil lost the Singularity" -->
<!-- id="3D10F041.3070509@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D10D5E9.6010105@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20How%20Kurzweil%20lost%20the%20Singularity"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Jun 19 2002 - 14:57:37 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4105.html">Brian Atkins: "Re: Fwd: Eclectic Pseudoplague"</a>
<li><strong>Previous message:</strong> <a href="4103.html">Eliezer S. Yudkowsky: "Fwd: Re: How Kurzweil lost the Singularity"</a>
<li><strong>In reply to:</strong> <a href="4103.html">Eliezer S. Yudkowsky: "Fwd: Re: How Kurzweil lost the Singularity"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4104">[ date ]</a>
<a href="index.html#4104">[ thread ]</a>
<a href="subject.html#4104">[ subject ]</a>
<a href="author.html#4104">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ray Kurzweil wrote:
<br>
<em> &gt;
</em><br>
<em>&gt; I think that Eliezer is misunderstanding my statements, intentions, and
</em><br>
<em>&gt; efforts.
</em><br>
<p>If so, I do apologize.  Thank you for taking the time to respond.
<br>
<p><em> &gt; First of all, he states &quot;Kurzweil's entire *being* is directed
</em><br>
<em>&gt; toward predicting the Singularity - *not* nudging the Singularity in any
</em><br>
<em>&gt; direction.&quot;  The fact is that the bulk of my efforts are involved in
</em><br>
<em>&gt; technology creation efforts, with only a portion devoted to talking and
</em><br>
<em>&gt; writing about technology.  I do believe that as technologists, we have an
</em><br>
<em>&gt; ethical responsibility to apply our efforts in ways that will promote
</em><br>
<em>&gt; positive human values, albeit that we don't always have a consensus on what
</em><br>
<em>&gt; those are.  Most of my efforts have been devoted to developing technology
</em><br>
<em>&gt; for persons with disabilities, and towards enhancing human expression in
</em><br>
<em>&gt; areas such as music, and I do give a high priority to considering the impact
</em><br>
<em>&gt; that technologies I'm involved in creating will have on society.
</em><br>
<p>I understand and acknowledge that the majority of your efforts are directed 
<br>
toward creating technology, rather than talking and writing about 
<br>
technology.  I am not accusing you of being a talker rather than a doer.  I 
<br>
do feel, however, that the Singularity is specifically the creation of 
<br>
transhuman intelligence, and that not all technological progress contributes 
<br>
equally to this.  There are many (valid!) motives for contributing to 
<br>
general technological progress, but the Singularity should be more than a 
<br>
means of rationalizing whatever we were already doing.
<br>
<p>One of the most commonly voiced objections to the Singularity is that it is 
<br>
simply an atheistic religion - a way to replace the spiritual comfort lost 
<br>
by the rational contradiction of earlier beliefs.  I believe that if the 
<br>
Singularity is treated as passive justification for existing plans, this 
<br>
objection will become substantially correct.
<br>
<p>There are thousands or millions of organizations that contribute to general 
<br>
technological progress.  Someone fortunate enough to be aware of the 
<br>
Singularity at this point in history has the opportunity to directly 
<br>
participate in technologies that lie on the critical path to the 
<br>
Singularity, such as brain-computer interfaces and Artificial Intelligence. 
<br>
&nbsp;&nbsp;I often receive letters from young adults asking me how they can select a 
<br>
college major or profession in order to contribute more to the Singularity. 
<br>
&nbsp;&nbsp;&nbsp;&nbsp;I don't tell them to go on whatever they were doing and that it will 
<br>
probably contribute to the Singularity eventually in one way or another.  I 
<br>
advise them that the professions involved most directly in the Singularity 
<br>
will probably be cognitive science or computer programming.  My hope is that 
<br>
this makes it more likely that Singularity-related projects will have eager 
<br>
young geniuses available - such being a critical resource in science.
<br>
<p>There are many efforts which unintentionally contribute to the Singularity, 
<br>
but I believe that efforts which have been deliberately directed at the 
<br>
Singularity - planned from the beginning with that sole goal in mind - will 
<br>
prove beneficial, critical, and necessary to the Singularity.  I believe 
<br>
that along with the parts of the Singularity that depend on the efforts of 
<br>
thousands of people and the expenditure of billions of dollars - such as 
<br>
Moore's Law - there will also be the opportunity for critical discoveries 
<br>
and inventions produced by small groups, and even breakthroughs (most 
<br>
probably in the cognitive sciences) that are the product of individual 
<br>
genius.  I believe that by directing more resources at these leverage 
<br>
points, where the path to the Singularity depends critically on specific 
<br>
scientific or technological issues rather than on the whole world economy, 
<br>
it is possible to accelerate the Singularity.
<br>
<p>I am not denying that you have done a great deal to advance technology and 
<br>
that some of this will indirectly contribute to the Singularity.  But I am 
<br>
concerned that your view of the Singularity allows no role for intentional 
<br>
efforts to bring about the Singularity sooner - tying it solely to vast, 
<br>
inexorable forces such as Moore's Law, which would be difficult or 
<br>
impossible to accelerate without a planetary commitment of resources. 
<br>
History, especially scientific history, is not always made by the horde. 
<br>
The role of individuals in scientific history is often exaggerated but it is 
<br>
certainly no exaggeration to say that a stroke of genius can accelerate 
<br>
progress by years or decades.  I believe that by encouraging individuals to 
<br>
direct their efforts specifically toward the Singularity, it may be possible 
<br>
to place on a firm basis scientific projects that might otherwise suffer 
<br>
from poverty of resources or poverty of genius at a critical moment in history.
<br>
<p>Let's suppose that someone proposes a project to construct a map of all 
<br>
known brain areas and pathways, which map will contain links to all online 
<br>
scientific literature which deals with that area or pathway.  I would 
<br>
recognize this project as critical to Artificial Intelligence, and you would 
<br>
recognize it as critical to brain emulation.  Therefore this project is not 
<br>
just &quot;scientifically exciting&quot; but is *critical* for the whole human 
<br>
species, and I would recommend funding it on the same level of resources and 
<br>
attention that currently attaches to popular environmentalist efforts or 
<br>
disease cures, and for the same humanistic justification.
<br>
<p>But currently projects lying on a direct path to the Singularity are *not* 
<br>
universally recognized as critical to humanity's future.  Currently projects 
<br>
like these are not glamorous enough to be guaranteed of plentiful funding 
<br>
and many eager young researchers hoping to take part.  We who are fortunate 
<br>
enough to be aware of the future must therefore continue to spread awareness 
<br>
of the Singularity *as a justification for action*, and direct our own 
<br>
resources - whether time, money, or a scientific or entrepreneurial lifetime 
<br>
- at those projects which are currently underfunded at the level which the 
<br>
Singularity justifies.  We must use our awareness of the future to nudge 
<br>
humanity closer to the rational distribution of efforts.  By trying to place 
<br>
what we recognize as Singularity-critical projects on a firm basis *because 
<br>
of* their connection to the Singularity, the overall course of scientific 
<br>
progress toward the Singularity may be genuinely and significantly accelerated.
<br>
<p>It is in this way that the Singularity meme helps to bring about the 
<br>
*actual* Singularity, answering the objections of those who claim that the 
<br>
Singularity is a vague, passive religious belief.  The Singularity is a 
<br>
humanistic goal which translates into a concrete research direction.
<br>
<p><em>&gt; I am familiar with Eliezer's efforts at defining and articulating ways that
</em><br>
<em>&gt; we can promote what he calls &quot;friendly AI,&quot; and I applaud his concern and
</em><br>
<em>&gt; efforts in this direction.  By itself, I don't believe that such efforts are
</em><br>
<em>&gt; sufficient, and Eliezer would probably agree with this.  I don't think that
</em><br>
<em>&gt; we have enough knowledge today to define a reliable strategy to assuring
</em><br>
<em>&gt; that AI (or other advanced technologies) will remain &quot;friendly,&quot; but the
</em><br>
<em>&gt; dialogue on how to achieve this is certainly worthwhile and not premature.
</em><br>
<em>&gt; It's an effort we will need to maintain and intensify, particularly as we
</em><br>
<em>&gt; get closer.  I have said many times that these technologies are advancing on
</em><br>
<em>&gt; many fronts, and I believe that a critical aspect of assuring that these
</em><br>
<em>&gt; future technologies are helpful rather than harmful is that everyone
</em><br>
<em>&gt; consider and apply ethical issues in every project and in every decision.
</em><br>
<em>&gt; There's no one &quot;magic bullet&quot; strategy that is going to assure that we avoid
</em><br>
<em>&gt; catastrophic downside scenarios. I do agree, however, that it is not too
</em><br>
<em>&gt; early to define these downsides and to develop multiple strategies towards
</em><br>
<em>&gt; this end.
</em><br>
<p>I believe that our civilization can pass through the Singularity 
<br>
successfully; furthermore, that we can do so safely and smoothly rather than 
<br>
in a state of last-minute panic - but only if considerable resources are 
<br>
invested in safe passage *substantially in advance* of when it becomes 
<br>
immediately necessary!  And this will not happen if everyone who hears about 
<br>
it thinks &quot;Oh, someone else will do it,&quot; because right now someone else is 
<br>
*not* doing it.  I am not just talking about the Singularity Institute; the 
<br>
Foresight Institute, for example, continues to be severely underfunded even 
<br>
as our civilization hurtles headlong toward nanotechnology.
<br>
<p><em>&gt; So in summary I believe that Eliezer's efforts in this direction are
</em><br>
<em>&gt; important and worthwhile.  However, he is not correct that I am unconcerned
</em><br>
<em>&gt; with this critical issue.  I've said on many ocassions that it's the number
</em><br>
<em>&gt; one challenge facing our civilization in the 21st century.
</em><br>
<p>One of the most profound statements I have ever encountered was an anonymous 
<br>
quotation on Slashdot:  &quot;Beware 'we should...', extend a hand to 'how do 
<br>
I...'&quot;  Since then I have never encountered the statement 'we should' 
<br>
without thinking of this quote.  The Singularity is the number one challenge 
<br>
facing our civilization, with this I agree.  Is it the number one challenge 
<br>
facing you personally?  I do not accuse you of being unconcerned, but how 
<br>
does your concern change your actions from what they were before?
<br>
<p>To put it bluntly, you have enormously more resources at your disposal with 
<br>
which to effect a Singularity, but it appears to many of us in the 
<br>
Singularity community that those resources are going unused.  It's 
<br>
frustrating!  I understand that your resources are absolutely your own, to 
<br>
dispose of as you wish, but I want to understand *why* you dispose of them 
<br>
as you do.  How can you declare that the Singularity is the meaning of life 
<br>
and yet not change anything?  If brain-computer interfaces are the critical 
<br>
path to the Singularity, then why aren't you, say, investing in Neural 
<br>
Signals Inc., which *right now* is struggling for venture capital - rather 
<br>
than doubling their number of neural taps every 18 months, as they should 
<br>
be?  If nanotechnological safety is important, then why not fund the 
<br>
Foresight Institute, which *right now* is sorely underfunded?  If Friendly 
<br>
AI is important, why not fund the Singularity Institute?
<br>
<p>I understand your confidence in Moore's Law, which has hundreds of billions 
<br>
of dollars or momentum behind it, but this spear has no spearhead unless 
<br>
humanity can scrape up the tiny fraction of its resources needed to run the 
<br>
one last mile that *does* depend on deliberate effort rather than the 
<br>
planetary economy.  Neural Signals Inc. would not exist without previous 
<br>
research in MEMS and biotechnology and cognitive science, but you still need 
<br>
a Neural Signals Inc.  AI research benefits tremendously from Moore's Law, 
<br>
but you still need a Singularity Institute.
<br>
<p>I think some Singularity activists - I can't speak for all of them, but it 
<br>
isn't just me - are puzzled, and a bit frustrated, because someone of your 
<br>
stature finally &quot;gets&quot; the Singularity, and yet you seem content to 
<br>
contribute in ways that are only peripherally related.  If you go out of 
<br>
your way at the Foresight Gathering to praise the researcher assembling a 
<br>
neurocomputational map of cortical processing, why not toss a few bucks his 
<br>
way?  I understand that your life is your own.  But *why*?
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4105.html">Brian Atkins: "Re: Fwd: Eclectic Pseudoplague"</a>
<li><strong>Previous message:</strong> <a href="4103.html">Eliezer S. Yudkowsky: "Fwd: Re: How Kurzweil lost the Singularity"</a>
<li><strong>In reply to:</strong> <a href="4103.html">Eliezer S. Yudkowsky: "Fwd: Re: How Kurzweil lost the Singularity"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4104">[ date ]</a>
<a href="index.html#4104">[ thread ]</a>
<a href="subject.html#4104">[ subject ]</a>
<a href="author.html#4104">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
