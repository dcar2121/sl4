<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Threats to the Singularity.</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Threats to the Singularity.">
<meta name="Date" content="2002-06-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Threats to the Singularity.</h1>
<!-- received="Mon Jun 17 22:26:53 2002" -->
<!-- isoreceived="20020618042653" -->
<!-- sent="Mon, 17 Jun 2002 15:51:41 -0700" -->
<!-- isosent="20020617225141" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Threats to the Singularity." -->
<!-- id="3D0E67FD.5010809@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="6F1B8958-81F8-11D6-9BD5-000A27B4DEFC@rbisland.cx" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Threats%20to%20the%20Singularity."><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Mon Jun 17 2002 - 16:51:41 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4076.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4074.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4063.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4077.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4077.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4075">[ date ]</a>
<a href="index.html#4075">[ thread ]</a>
<a href="subject.html#4075">[ subject ]</a>
<a href="author.html#4075">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Gordon Worley wrote:
<br>
<p><em>&gt; 
</em><br>
<em>&gt; On Monday, June 17, 2002, at 05:31  AM, Samantha Atkins wrote:
</em><br>
<em>&gt; 
</em><br>
<p><p><p><em>&gt;&gt;&gt; First off, attachment to humanity is a bias that prevents rational 
</em><br>
<em>&gt;&gt;&gt; thought.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Rational?  By what measure?  How is attachment to the well-being of 
</em><br>
<em>&gt;&gt; ourselves and all like us irrational?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer addressed this in his reply to this thread earlier.  It is 
</em><br>
<em>&gt; irrational if the attachment is blind.  You must have some reason that 
</em><br>
<em>&gt; you need to stay alive, otherwise provisions for it will most likely get 
</em><br>
<em>&gt; in the way of making rational decisions.
</em><br>
<em>&gt; 
</em><br>
<p><p>There must be some core, some set of fundamental values, that is 
<br>
unassailable (at least at a point in time) for an ethical system 
<br>
to be built.  It is only in the context of such that the 
<br>
question of &quot;some reason&quot; can even be addressed meaningfully. 
<br>
The life and well-being of sentients *is* part of my core.  It 
<br>
is not itself subject to further breakdown to reasons why this 
<br>
is a core.  To further break it down would require another core 
<br>
reason that this one could be examined in terms of.  A large 
<br>
part of my questions here are an attempt to determine what that 
<br>
core is for various parties.
<br>
<p><p><em>&gt;&gt; Whether we transform or simply cease to exist seems to me to be a 
</em><br>
<em>&gt;&gt; perfectly rational thing to be a bit concerned about.  Do you see it 
</em><br>
<em>&gt;&gt; otherwise?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Sure, you should be concerned.  I think that the vast majority of 
</em><br>
<em>&gt; humans, uploaded or not, have something positive to contribute, however 
</em><br>
<em>&gt; small.  It'd be great to see life get even better post Singularity, with 
</em><br>
<em>&gt; everyone doing new and interesting good things.
</em><br>
<em>&gt; 
</em><br>
<p><p>Then we shouldn't shoot for any less, right?
<br>
<p><p><em>&gt;&gt;&gt; I and others have broken this attachment to keep it from clouding our 
</em><br>
<em>&gt;&gt;&gt; thinking.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; So you believe that becoming inhuman and uncaring about the fate of 
</em><br>
<em>&gt;&gt; humanity allows you to think better?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If only it were easy to become inhuman, but it's not.
</em><br>
<em>&gt; 
</em><br>
<p><p><p>There is a mine of semantics hidden in this!
<br>
<p><p><em>&gt; Uncaring is inaccurate.  I do care about humans and would like to see 
</em><br>
<em>&gt; them upload.  I care about any other intelligent life that might be out 
</em><br>
<em>&gt; there in the universe and helping it upload.  I just don't care about 
</em><br>
<em>&gt; humans so much that I'd give up everything to save humanity (unless that 
</em><br>
<em>&gt; was the most rational thing to do).
</em><br>
<em>&gt; 
</em><br>
<p><p>On what basis will you judge what is rational?  In terms of what 
<br>
supergoals, if you will?
<br>
<p><p><em>&gt;&gt;&gt; It is the result of being genetically related to the rest of 
</em><br>
<em>&gt;&gt;&gt; humanity, where the death of all human genes is a big enough problem 
</em><br>
<em>&gt;&gt;&gt; to cause a person to give up a goal or die to save humanity.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; I do not necesarily agree that we can just write it off as a genetic 
</em><br>
<em>&gt;&gt; relatedness issue at all.  Whether there is sentient life and whether 
</em><br>
<em>&gt;&gt; it continues, regardless of its form, is of intense interest to me.  
</em><br>
<em>&gt;&gt; That some forms are not genetically related is not of high relevance 
</em><br>
<em>&gt;&gt; to the form of my concern. So please don't assume that explains it 
</em><br>
<em>&gt;&gt; away or makes the issue go away.  It doesn't.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There is an ethical issue, however the irrational attachment is the 
</em><br>
<em>&gt; result of relatedness.  A proper ethic is not so strong that it prevents 
</em><br>
<em>&gt; you from even thinking about something, the way evolved ethics do.
</em><br>
<em>&gt; 
</em><br>
<p><p>You can call it &quot;irrational&quot; all you wish.  I consider it the 
<br>
very bedrock of rationality in our current context.
<br>
<p><p><em>&gt;&gt;&gt; Some of us, myself included, see the creation of SI as important 
</em><br>
<em>&gt;&gt;&gt; enough to be more important than humanity's continuation.  Human 
</em><br>
<em>&gt;&gt;&gt; beings, being
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; How do you come to this conclusion?  What makes the SI worth more than 
</em><br>
<em>&gt;&gt; all of humanity?  That it can outperform them on some types of 
</em><br>
<em>&gt;&gt; computation?  Is computational complexity and speed the sole measure 
</em><br>
<em>&gt;&gt; of whether sentient beings have the right to continued existence?  Can 
</em><br>
<em>&gt;&gt; you really give a moral justification or a rational one for this?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In many ways, humans are just over the threshold of intelligence.  
</em><br>
<p><p>Whose threshold?  By what standards?  Established and verified 
<br>
as the standards of value how?
<br>
<p><em>&gt; Compared to past humans we are pretty smart, but compared to the 
</em><br>
<em>&gt; estimated potentials for intelligence we are intellectual ants.  Despite 
</em><br>
<p><p>So we are to think less of ourselves because of estimated 
<br>
potentials?  Do we consider ourselves expendable because an SI 
<br>
comes into existence that is a million times faster and more 
<br>
capable in the scope of its creations, decision making and 
<br>
understanding?  This does not follow.
<br>
<p><em>&gt; our differences, all of us are roughly of equivalent intelligence and 
</em><br>
<em>&gt; therefore on equal footing when decided whose life is more important.  
</em><br>
<p><p>It would be best to not need to decide any such thing as much as 
<br>
possible.
<br>
<p><em>&gt; But, it's not nearly so simple.  All of us would probably agree that 
</em><br>
<em>&gt; given the choice between saving one of two lives, we would choose to 
</em><br>
<em>&gt; save the person who is most important to the completion of our goals, be 
</em><br>
<em>&gt; that reproduction, having fun, or creating the Singularity.  In the same 
</em><br>
<em>&gt; light, if a mob is about to come in to destroy the SI just before it 
</em><br>
<em>&gt; takes off and there is no way to stop them other than killing them, you 
</em><br>
<em>&gt; have on one hand the life of the SI that is already more intelligent 
</em><br>
<em>&gt; than the members of the mob and will continue to get more intelligent, 
</em><br>
<em>&gt; and on the other the life of 100 or so humans.  Given such a choice, I 
</em><br>
<em>&gt; pick the SI.
</em><br>
<em>&gt;
</em><br>
<p><p>But that is not the context of the question.  The context is 
<br>
whether the increased well-being and possibilities of existing 
<br>
sentients, regardless of their relative current intelligence, is 
<br>
&nbsp;&nbsp;a high and central value.  If it is not then I hardly see how 
<br>
such an SI can be described as &quot;Friendly&quot;.
<br>
<p>&nbsp;
<br>
<em>&gt; In my view, more intelligent life has more right to the space it uses 
</em><br>
<em>&gt; up.  Of course, we hope that intelligent life is compassionate and is 
</em><br>
<em>&gt; willing to share.  Actually, I should be more precise.  I think that 
</em><br>
<em>&gt; wiser life has more right to the space it uses (but you can't be wiser 
</em><br>
<em>&gt; without first being more intelligent).  I would choose a world full of 
</em><br>
<em>&gt; dumb humans trying hard to do some good over an Evil AI.
</em><br>
<em>&gt;
</em><br>
<p><p>If it is not willing to share  in the sense of respecting the 
<br>
life and well-being of other sentients then I consider it 
<br>
neither &quot;intelligent&quot; or desirable.   Do not use the disputed 
<br>
word &quot;wise&quot; to describe something that is only more intelligent. 
<br>
&nbsp;&nbsp;&nbsp;I dispute that the &quot;wise&quot; will destroy other sentients when 
<br>
it is within their means to preserve them.
<br>
<p>&nbsp;
<br>
<em>&gt;&gt;&gt; self aware, do present more of an ethical delima than cows if it 
</em><br>
<em>&gt;&gt;&gt; turns out that you might be forced to sacrifice some of them.  I 
</em><br>
<em>&gt;&gt;&gt; would like to see all of humanity make it into a post Singularity 
</em><br>
<em>&gt;&gt;&gt; existence and I am willing to help make this a reality.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; How kind of you.  However, from the above it seems you see them as an 
</em><br>
<em>&gt;&gt; ethical dilemna greater than that of cows but if your SI, whatever it 
</em><br>
<em>&gt;&gt; turns out really to be, seems to require or decides the death of one 
</em><br>
<em>&gt;&gt; or all of them, then you would have to side with the SI.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Do I read you correctly?  If I do, then why do you hold this 
</em><br>
<em>&gt;&gt; position?  If I read you correctly then how can you expect the 
</em><br>
<em>&gt;&gt; majority of human beings, if they really understood you, to consider 
</em><br>
<em>&gt;&gt; you as other than a monster?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If an SI said it needed to kill a bunch of humans, I would seriously 
</em><br>
<em>&gt; start questioning its motives.  Killing intelligent life is not 
</em><br>
<em>&gt; something to be taken lightly and done on a whim.  However, if we had a 
</em><br>
<em>&gt; FAI that was really Friendly and it said &quot;Gordon, believe me, the only 
</em><br>
<em>&gt; way is to kill this person&quot;, I would trust in the much wiser SI.
</em><br>
<em>&gt; 
</em><br>
<p><p>OK, that seems better.  But how would you evaluate how Friendly 
<br>
this superintelligence really was?
<br>
<p><p><em>&gt; This is the kind of reaction I expect and, while I'm a bit disappointed 
</em><br>
<em>&gt; to get so much of it on SL4, therefore avoid pointing this view out.  I 
</em><br>
<em>&gt; never go out of my way to say that human life is not the most important 
</em><br>
<em>&gt; thing to me in the universe, but sometimes it is worth talking about.
</em><br>
<p><p>I would be very disappointed if you did not get such reactions 
<br>
to your original statement.
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4076.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4074.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4063.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4077.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4077.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4075">[ date ]</a>
<a href="index.html#4075">[ thread ]</a>
<a href="subject.html#4075">[ subject ]</a>
<a href="author.html#4075">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
