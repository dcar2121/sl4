<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Threats to the Singularity.</title>
<meta name="Author" content="Eugen Leitl (eugen@leitl.org)">
<meta name="Subject" content="Re: Threats to the Singularity.">
<meta name="Date" content="2002-06-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Threats to the Singularity.</h1>
<!-- received="Mon Jun 17 11:07:13 2002" -->
<!-- isoreceived="20020617170713" -->
<!-- sent="Mon, 17 Jun 2002 16:39:27 +0200 (CEST)" -->
<!-- isosent="20020617143927" -->
<!-- name="Eugen Leitl" -->
<!-- email="eugen@leitl.org" -->
<!-- subject="Re: Threats to the Singularity." -->
<!-- id="Pine.LNX.4.33.0206171610010.23916-100000@hydrogen.leitl.org" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="6F1B8958-81F8-11D6-9BD5-000A27B4DEFC@rbisland.cx" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eugen Leitl (<a href="mailto:eugen@leitl.org?Subject=Re:%20Threats%20to%20the%20Singularity."><em>eugen@leitl.org</em></a>)<br>
<strong>Date:</strong> Mon Jun 17 2002 - 08:39:27 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4065.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4063.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4063.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4065.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4065.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4066.html">Eliezer S. Yudkowsky: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4064">[ date ]</a>
<a href="index.html#4064">[ thread ]</a>
<a href="subject.html#4064">[ subject ]</a>
<a href="author.html#4064">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Mon, 17 Jun 2002, Gordon Worley wrote:
<br>
<p><em>&gt; Eliezer addressed this in his reply to this thread earlier.  It is 
</em><br>
<em>&gt; irrational if the attachment is blind.  You must have some reason that 
</em><br>
<p>No, I don't need any reasons to stay live. That's a core axiom of
<br>
rationality (and I happen to like my evolutionary artifacts, dammit). If
<br>
you question that basic of rationality you're definitely a sick puppy.
<br>
<p><em>&gt; you need to stay alive, otherwise provisions for it will most likely get 
</em><br>
<em>&gt; in the way of making rational decisions.
</em><br>
<p>I think you would understand that it is very rational to preemptively kill
<br>
people who're effectively trying to kill me and those close to me (I'm not
<br>
very close to the rest of humanity, but there is an awful lot of them, so
<br>
this tends to cumulate). 
<br>
<p><em>&gt; Sure, you should be concerned.  I think that the vast majority of 
</em><br>
<em>&gt; humans, uploaded or not, have something positive to contribute, however 
</em><br>
<p>I don't think right to live is based on my assessment on who is
<br>
contributing something positive, or not. I hope that this position is
<br>
mutual, orelse we are so screwed.
<br>
<p><em>&gt; small.  It'd be great to see life get even better post Singularity, with 
</em><br>
<em>&gt; everyone doing new and interesting good things.
</em><br>
<p>Indeed. So let's try to go there.
<br>
&nbsp;
<br>
<em>&gt; If only it were easy to become inhuman, but it's not.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Uncaring is inaccurate.  I do care about humans and would like to see 
</em><br>
<em>&gt; them upload.  I care about any other intelligent life that might be out 
</em><br>
<em>&gt; there in the universe and helping it upload.  I just don't care about 
</em><br>
<em>&gt; humans so much that I'd give up everything to save humanity (unless that 
</em><br>
<em>&gt; was the most rational thing to do).
</em><br>
<p>So basically in your value system the right of humanity to exist and the
<br>
right for you to pursue your activities, which bears considerable
<br>
probability to teotwawki this place not even balance.
<br>
<p>You *are* a sick puppy.
<br>
&nbsp;
<br>
<em>&gt; There is an ethical issue, however the irrational attachment is the 
</em><br>
<em>&gt; result of relatedness.  A proper ethic is not so strong that it prevents 
</em><br>
<em>&gt; you from even thinking about something, the way evolved ethics do.
</em><br>
<p>I can think about a great many things. It doesn't mean I have to like 
<br>
them, and it specifically doesn't mean I have to do them.
<br>
<p>Your thoughts about the primate of rationality are rather irrational.
<br>
&nbsp;
<br>
<em>&gt; In many ways, humans are just over the threshold of intelligence.  
</em><br>
<p>Intelligence is just a trait. I happen to like it, but it doesn't make the 
<br>
rest of the world into one huge blind spot.
<br>
<p><em>&gt; Compared to past humans we are pretty smart, but compared to the 
</em><br>
<em>&gt; estimated potentials for intelligence we are intellectual ants.  Despite 
</em><br>
<p>I agree. Let's change this. Let's make everybody who is willing smarter.
<br>
<p><em>&gt; our differences, all of us are roughly of equivalent intelligence and 
</em><br>
<em>&gt; therefore on equal footing when decided whose life is more important.  
</em><br>
<em>&gt; But, it's not nearly so simple.  All of us would probably agree that 
</em><br>
<em>&gt; given the choice between saving one of two lives, we would choose to 
</em><br>
<em>&gt; save the person who is most important to the completion of our goals, be 
</em><br>
<em>&gt; that reproduction, having fun, or creating the Singularity.  In the same 
</em><br>
<em>&gt; light, if a mob is about to come in to destroy the SI just before it 
</em><br>
<em>&gt; takes off and there is no way to stop them other than killing them, you 
</em><br>
<em>&gt; have on one hand the life of the SI that is already more intelligent 
</em><br>
<em>&gt; than the members of the mob and will continue to get more intelligent, 
</em><br>
<em>&gt; and on the other the life of 100 or so humans.  Given such a choice, I 
</em><br>
<em>&gt; pick the SI.
</em><br>
<p>I figured that much. Please hang on while I gather the mob. You might find
<br>
the odds rather tough, being 1) in the vast minority 2) not an SI yourself
<br>
3) assuming the mob will be kind enough to wait until you're done with
<br>
your little Golem project
<br>
&nbsp;
<br>
<em>&gt; In my view, more intelligent life has more right to the space it uses 
</em><br>
<em>&gt; up.  Of course, we hope that intelligent life is compassionate and is 
</em><br>
<em>&gt; willing to share.  Actually, I should be more precise.  I think that 
</em><br>
<em>&gt; wiser life has more right to the space it uses (but you can't be wiser 
</em><br>
<em>&gt; without first being more intelligent).  I would choose a world full of 
</em><br>
<em>&gt; dumb humans trying hard to do some good over an Evil AI.
</em><br>
<p>Excellent. This implies you can prove that the AI is not going to be evil, 
<br>
and convince everybody else of the correctness of your proof. Given the 
<br>
magnitude of the impact, you'd better be damn convincing.
<br>
&nbsp;
<br>
<em>&gt; If an SI said it needed to kill a bunch of humans, I would seriously 
</em><br>
<p>There's one big thing wrong with this sentence: it implies the SI is 
<br>
already here and operating. I definitely can't let you do that, Dave.
<br>
<p><em>&gt; start questioning its motives.  Killing intelligent life is not 
</em><br>
<p>No, you won't &quot;seriously start questioning its motives&quot;. You would be too
<br>
busy with dying, along with everybody else.
<br>
<p><em>&gt; something to be taken lightly and done on a whim.  However, if we had a 
</em><br>
<em>&gt; FAI that was really Friendly and it said &quot;Gordon, believe me, the only 
</em><br>
<em>&gt; way is to kill this person&quot;, I would trust in the much wiser SI.
</em><br>
<p>Yeah, that's some really Friendly AI. &quot;Trust me, I'm a FAI! Kill that
<br>
person, Gordon!&quot;
<br>
&nbsp;
<br>
<em>&gt; This is the kind of reaction I expect and, while I'm a bit disappointed 
</em><br>
<em>&gt; to get so much of it on SL4, therefore avoid pointing this view out.  I 
</em><br>
<em>&gt; never go out of my way to say that human life is not the most important 
</em><br>
<em>&gt; thing to me in the universe, but sometimes it is worth talking about.
</em><br>
<p>This conversation is completely surreal. Do you realize that you're
<br>
considerably harming your goals by making this public?
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4065.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4063.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4063.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4065.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4065.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4066.html">Eliezer S. Yudkowsky: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4064">[ date ]</a>
<a href="index.html#4064">[ thread ]</a>
<a href="subject.html#4064">[ subject ]</a>
<a href="author.html#4064">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
