<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Military Friendly AI</title>
<meta name="Author" content="Eugen Leitl (eugen@leitl.org)">
<meta name="Subject" content="Re: Military Friendly AI">
<meta name="Date" content="2002-06-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Military Friendly AI</h1>
<!-- received="Sat Jun 29 08:05:20 2002" -->
<!-- isoreceived="20020629140520" -->
<!-- sent="Sat, 29 Jun 2002 12:11:09 +0200 (CEST)" -->
<!-- isosent="20020629101109" -->
<!-- name="Eugen Leitl" -->
<!-- email="eugen@leitl.org" -->
<!-- subject="Re: Military Friendly AI" -->
<!-- id="Pine.LNX.4.33.0206291130270.23305-100000@hydrogen.leitl.org" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="4.3.2.7.2.20020628124459.01c7bd68@mail.earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eugen Leitl (<a href="mailto:eugen@leitl.org?Subject=Re:%20Military%20Friendly%20AI"><em>eugen@leitl.org</em></a>)<br>
<strong>Date:</strong> Sat Jun 29 2002 - 04:11:09 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4541.html">Eugen Leitl: "RE: Ben vs. Ben"</a>
<li><strong>Previous message:</strong> <a href="4539.html">Samantha Atkins: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>In reply to:</strong> <a href="4509.html">James Higgins: "Re: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4548.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4548.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4540">[ date ]</a>
<a href="index.html#4540">[ thread ]</a>
<a href="subject.html#4540">[ subject ]</a>
<a href="author.html#4540">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Fri, 28 Jun 2002, James Higgins wrote:
<br>
<p><em>&gt; This appears to state that you believe that kicking off The
</em><br>
<em>&gt; Singularity is intrinsically evil.  Is this a correct interpretation
</em><br>
<em>&gt; of your statement?
</em><br>
<p>No. I believe that all flavours of Singularity in which we people don't
<br>
fall off the bus are good. However, I have objective reasons to believe
<br>
that all brands of Singularity with very hard edges right on the onset are
<br>
hostile to organic life in general, and not just to people. It is hard to
<br>
assess how hard it is to engineer a seed to make a hard-edged early
<br>
takeoff, but given the most probable outcome I think it's a Really Dumb
<br>
Idea to try.
<br>
&nbsp;
<br>
<em>&gt; Another things that is starting to appear hopeless is your belief in 
</em><br>
<em>&gt; regulation of the Singularity.
</em><br>
<p>I don't enjoy pointing this out, but you'll be surprised how hard it is to
<br>
be an outlaw on the run in an extremely transparent, high-surveillance
<br>
society. We're increasingly moving towards law &amp; order rather than
<br>
Brinworld/cryptoanarchy, and it is obvious that technology gives an edge
<br>
to centralistic gooberment agencies than to largely clueless, apathetic
<br>
agents who don't understand the power of synchronized action. It is not
<br>
obvious that the latter is changing, in fact so far people are buying the
<br>
line hook and sinker.
<br>
<p>I think the majority of people here are extremely overoptimistic at the
<br>
threshold of resources required to kick off a successful attempt, and
<br>
socities *do* change very rapidly now, and not necessarily towards greater
<br>
freedom.
<br>
&nbsp;
<br>
<em>&gt; Discussing regulation of *any* up-and-coming technology is pointless.  
</em><br>
<p>Nope. The big potential killer technologies are very few, and there are
<br>
distinct routes which can be blocked. Sure there are going to be
<br>
surprises, but if you keep track of things occuring they're going to be
<br>
quite few of them. If there's military AI research making the goverments
<br>
paranoid spy on each other is here clearly a Good Thing.
<br>
<p><em>&gt; New technologies move, adapt and change much too quickly for
</em><br>
<em>&gt; legislatures to deal with.  Only after a technology has matured (is
</em><br>
<p>Once again, you do not realize what it means to be classifed as armed,
<br>
dangerous, and on the run. Please do not point me towards the current,
<br>
very lenient low-tech socities as a demonstration that individuals and
<br>
groups of individuals engaged in low-profile activity can indefinitely
<br>
maintain their cover. As AI researchers you surely realize what it means
<br>
if cash is outlawed, and the entire technosphere spies on you, using data
<br>
warehousing methods to look for patterns.
<br>
<p>So far only bioterrorism and nuclear terrorism (trivial threats, as far as
<br>
Singularity tech is concerned) are on lawmaker's and LEO's radar. As soon
<br>
as AI is classified a credible threat the crackdown can be swift and hard.
<br>
If the threat is considered very credible, this has the power to transform
<br>
societies. (I hope you'll enjoy your anal probe).
<br>
<p>There's a window of free operation before that, but I think a number of AI 
<br>
people here are being very, very optimistic on the time scale. If you can 
<br>
pull it off before, and it doesn't kill us, the point is moot. If you 
<br>
can't, well, see above.
<br>
<p><em>&gt; substantially deployed)  can it be regulated.  I have provided
</em><br>
<p>Regulations so far worked for weapons of mass destruction. Hard takeoff
<br>
Singularity seeds can be seen as slow-starting but Armageddon class of
<br>
weapons. The threshold to generate a viable seed is high, so arguments
<br>
about brilliant teenage hackers operating from their bedrooms will be met 
<br>
with polite derision.
<br>
<p><em>&gt; examples of my view.  I have not seen ANY information in support of
</em><br>
<em>&gt; your beliefs, however.  Please provide us with a number of concrete
</em><br>
<p>I don't have to provide jack. I see a number of people talking deep
<br>
engineering hubris in face of the mothers of high-complexity efforts with
<br>
the highest impact imaginable. The usual RISKS thing, with the stakes
<br>
being more than a bit higher than usual. As I said, it's pointless trying
<br>
to argue with people who're deep in denial. It's self-selected, if they
<br>
didn't have the specific agnosia they wouldn't be doing this thing in the
<br>
first place.
<br>
<p><em>&gt; examples where effective regulations to govern a technology were
</em><br>
<em>&gt; passed prior to the release or deployment of the technology.  Note
</em><br>
<em>&gt; that technologies created by government agencies would be
</em><br>
<em>&gt; inappropriate and poor examples.
</em><br>
<p>Nope, technology is technology. 
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4541.html">Eugen Leitl: "RE: Ben vs. Ben"</a>
<li><strong>Previous message:</strong> <a href="4539.html">Samantha Atkins: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>In reply to:</strong> <a href="4509.html">James Higgins: "Re: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4548.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4548.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4540">[ date ]</a>
<a href="index.html#4540">[ thread ]</a>
<a href="subject.html#4540">[ subject ]</a>
<a href="author.html#4540">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
