<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: FAI means no programmer-sensitive AI morality</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: FAI means no programmer-sensitive AI morality">
<meta name="Date" content="2002-06-30">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: FAI means no programmer-sensitive AI morality</h1>
<!-- received="Sun Jun 30 15:12:46 2002" -->
<!-- isoreceived="20020630211246" -->
<!-- sent="Sun, 30 Jun 2002 15:09:44 -0400" -->
<!-- isosent="20020630190944" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: FAI means no programmer-sensitive AI morality" -->
<!-- id="3D1F5778.7070503@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJMEENCMAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20FAI%20means%20no%20programmer-sensitive%20AI%20morality"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jun 30 2002 - 13:09:44 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4603.html">Tomaz Kristan: "Re: Catholics and the Singularity"</a>
<li><strong>Previous message:</strong> <a href="4601.html">Eliezer S. Yudkowsky: "Re: Ben vs. Ben"</a>
<li><strong>In reply to:</strong> <a href="4595.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4605.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4605.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4602">[ date ]</a>
<a href="index.html#4602">[ thread ]</a>
<a href="subject.html#4602">[ subject ]</a>
<a href="author.html#4602">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Let's start with a moral question.  Some people even today, though
</em><br>
<em>&gt;&gt;thankfully not as many as there a few generations ago, believe that
</em><br>
<em>&gt;&gt;people of certain races (or at least, what they regard as &quot;races&quot;) are
</em><br>
<em>&gt;&gt;intrinsically worth less than others.  You have a different morality
</em><br>
<em>&gt;&gt;under which race makes no difference to intrinsic worth.  Now I'm not
</em><br>
<em>&gt;&gt;asking you why you believe these other people are wrong, because if so
</em><br>
<em>&gt;&gt;you'll just answer &quot;Because their morality conflicts with mine&quot;; rather
</em><br>
<em>&gt;&gt;I'm asking you why you don't share their morality.  If morality is
</em><br>
<em>&gt;&gt;genuinely arbitrary then one mapping of sentiences to intrinsic value is
</em><br>
<em>&gt;&gt;as good as any other; why is your morality different from theirs?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Being &quot;worth less&quot; is ambiguous...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If the argument is that blacks are extremely less intelligent than whites
</em><br>
<em>&gt; (an argument made in the past by  many people), this is an empirically
</em><br>
<em>&gt; testable statement,  and has been refuted [yes, I know there is a slight IQ
</em><br>
<em>&gt; difference among races, with whites getting higher than blacks and orientals
</em><br>
<em>&gt; getting higher than whites -- but this is not the sort of thing I'm tallking
</em><br>
<em>&gt; about...]
</em><br>
<p>Why do you suppose that people phrased their argument as &quot;race X is less 
<br>
intelligent than race Y and therefore worth less&quot;, rather than &quot;under my 
<br>
choice of arbitrary morals, I choose to assign less value to race X than 
<br>
race Y&quot;?
<br>
<p><em>&gt; If the argument is that blacks don't have souls, then I guess it's outside
</em><br>
<em>&gt; the domain of experiment and logic...
</em><br>
<p>(Parenthetically:  Not really.  You'd just ask &quot;Why do you think people 
<br>
have souls?&quot; and then ask whether this sounds like a reason that would 
<br>
apply equally to race X and race Y.)
<br>
<p>But why do you suppose that the *argument* would be that race X is 
<br>
soulless?  Why argue about morality?  Why not just say, &quot;race X maps to 
<br>
desirability 0 and race Y maps to desirability 1&quot;?
<br>
<p><em>&gt; I'm not sure I would agree that &quot;morality is genuinely arbitrary.&quot;  I would
</em><br>
<em>&gt; say that there is no objective scientific or logical way to judge one moral
</em><br>
<em>&gt; system versus another. Because any system of judging presumes some
</em><br>
<em>&gt; &quot;criterion of merit&quot;; the choice of criterion of merit will then determine
</em><br>
<em>&gt; which moral system is better.... of course, one can then ask which criterion
</em><br>
<em>&gt; of merit is better
</em><br>
<p>But you do, personally, have criteria of merit which you use to actually 
<br>
choose between moralities?  A desirability metric is a way of choosing 
<br>
between futures.  Do you have a &quot;criterion of merit&quot; that lets you 
<br>
choose between desirability metrics?  What is it?
<br>
<p><em>&gt; However, it could nonetheless be the case that highly intelligent systems
</em><br>
<em>&gt; tend toward certain moral systems, as opposed to others.  Just as modern
</em><br>
<em>&gt; technological culture tends toward different moral systems than tribal
</em><br>
<em>&gt; culture....
</em><br>
<p>If *human* intelligent systems, but not necessarily all theoretically 
<br>
possible minds-in-general, tend toward certain moral systems as opposed 
<br>
to others, then would you deem it desirable to construct an AI such that 
<br>
it shared with humans the property of tending toward these certain moral 
<br>
systems as intelligence increased?
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4603.html">Tomaz Kristan: "Re: Catholics and the Singularity"</a>
<li><strong>Previous message:</strong> <a href="4601.html">Eliezer S. Yudkowsky: "Re: Ben vs. Ben"</a>
<li><strong>In reply to:</strong> <a href="4595.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4605.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4605.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4602">[ date ]</a>
<a href="index.html#4602">[ thread ]</a>
<a href="subject.html#4602">[ subject ]</a>
<a href="author.html#4602">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
