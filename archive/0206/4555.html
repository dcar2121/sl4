<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: AI morality</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="AI morality">
<meta name="Date" content="2002-06-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>AI morality</h1>
<!-- received="Sat Jun 29 14:49:57 2002" -->
<!-- isoreceived="20020629204957" -->
<!-- sent="Sat, 29 Jun 2002 10:59:18 -0600" -->
<!-- isosent="20020629165918" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="AI morality" -->
<!-- id="LAEGJLOGJIOELPNIOOAJOEDFCMAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=Re:%20AI%20morality"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sat Jun 29 2002 - 10:59:18 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4556.html">Eugen Leitl: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Previous message:</strong> <a href="4554.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4555">[ date ]</a>
<a href="index.html#4555">[ thread ]</a>
<a href="subject.html#4555">[ subject ]</a>
<a href="author.html#4555">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer wrote:
<br>
<em>&gt;  The vast majority of religious
</em><br>
<em>&gt; people, especially what we would call &quot;fundamentalists&quot; and those outside
</em><br>
<em>&gt; the First World, adhere to a correspondence theory of the truth of their
</em><br>
<em>&gt; religion; when they say something is true, they mean that it is so; that
</em><br>
<em>&gt; outside reality corresponds to their belief.
</em><br>
<p>I don't think you have it quite right.
<br>
<p>What they mean is more nearly that *their experience corresponds to their
<br>
beliefs*
<br>
<p>Modern scientific realism is based on accepting outside reality,
<br>
observations of physical reality, as the fundamental determinant of truth.
<br>
<p>On the other hand, many other traditions are based on accepting *inner
<br>
intuitions and experiences* as the fundamental determinants of truth.
<br>
<p>It seems to me like you don't fully appreciate what it means for someone to
<br>
have a truly non-rationalist, non-scientific point of view.  Probably this
<br>
is because your life-course so far has not led you to spend significantly
<br>
much time with such people.  Mine, as it happens, has.  I know you grew up
<br>
in a religious home, but there are more and less rationalist varieties of
<br>
religion, too...
<br>
<p>Take yoga, or Zen Buddhism, as examples.  These are ancient traditions with
<br>
a lot of depth and detail to them.  Their validity, such as it is, is
<br>
primarily *experiential*.  It is largely not based on things that
<br>
individuals outside the tradition in question can observe in empirical
<br>
reality.  [Yeah, I know people have tried to test for enlightenment by
<br>
studying brain waves and such (lots of work at Maharishi University on
<br>
this), but this isn't what it's all about -- this is icing on the cake from
<br>
the spiritual point of view.]
<br>
<p>When my wife for instance became interested in Zen, it wasn't because any
<br>
kind of analysis of observations convinced her, it was because some things
<br>
she read in a Zen book resonated with some experiences she'd already had...
<br>
<p><p><em>&gt;  &gt; To those who place spiritual feelings and insights above reason (most
</em><br>
<em>&gt;  &gt; people in the world), the idea that an AI is going to do what
</em><br>
<em>&gt; is &quot;right&quot;
</em><br>
<em>&gt;  &gt; according to logical reasoning is not going to be very reassuring.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Under your definition of &quot;logical reasoning&quot;, I can't say I would want to
</em><br>
<em>&gt; see a logical AI either.
</em><br>
<p>Nitpicking about the definition of logic is not to-the-point here.  In
<br>
Novamente we have a narrow technical definition of &quot;reasoning&quot; as opposed to
<br>
other cognitive processes, and I can see that I've made the error in some
<br>
SL4 posts of using this technical definition in nontechnical discussions,
<br>
whereas in ordinary language &quot;reasoning&quot; means something broader.  Sorry
<br>
about that.
<br>
<p>But the point at hand is: many folks will be totally unconvinced by
<br>
*anything* an intelligent, scientifically-minded AGI says -- just as they
<br>
are unconvinced by your and my arguments that God probably didn't really
<br>
create the world 6000 years ago, that there probably isn't really a Heaven
<br>
into which only 144000 people will ever be accepted, etc.
<br>
<p><em>&gt;  &gt; And those who have a more rationalist approach to religion, would only
</em><br>
<em>&gt;  &gt; accept an AI's reasoning as &quot;right&quot; if the AI began its reasoning with
</em><br>
<em>&gt;  &gt; *the axioms of their religion*.  Talmudic reasoning, for
</em><br>
<em>&gt; example, defines
</em><br>
<em>&gt;  &gt; right as &quot;logically implied by the Jewish holy writings.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Um... not really.  If I recall correctly, Ben, you're
</em><br>
<em>&gt; second-generation or
</em><br>
<em>&gt; third-generation ex-Jew.  Can I take it that you weren't actually
</em><br>
<em>&gt; forced as  a child to study the Talmud?
</em><br>
<p>My parents were quasi-religious; my dad was a quaker, my mom was a jew.  I
<br>
never received a religious education.
<br>
<p>I know there are many varieties of Judaism, and I don't remember the right
<br>
labels for all of them.  What I'm thinking of is actually some orthodox jews
<br>
I used to argue with when I lived in Williamsburg, New York City, the world
<br>
center for Hasidic Jews...
<br>
<p><em>&gt; Ben, I have been taught at least one viewpoint which is not the empirical
</em><br>
<em>&gt; viewpoint of modern science.  It is pretty strange but it is not
</em><br>
<em>&gt; outside the
</em><br>
<em>&gt; correspondence theory of truth.
</em><br>
<p>There are a lot of kinds of Judaism.  I don't know what kind you were
<br>
taught, and I don't really know much about any of them.
<br>
<p>But -- I do know that Zen Buddhism and yoga and Sufi-ist Islam are outside
<br>
the correspondence theory of truth as you describe it, in the sense that
<br>
they define truth more by correspondence with inner experience than by
<br>
correspondence with physical reality.
<br>
<p>Physical reality, according to these traditions, is an illusion.  Emotions
<br>
are also illusions.  Only a certain kind of crystal-clear inner insight
<br>
(yes, these words don't do it justice...) is to be &quot;trusted&quot; (though in a
<br>
sense it's viewed as having a directness beyond trust/mistrust)..
<br>
<p><p><em>&gt; If you assume that Judaism is
</em><br>
<em>&gt; the correct
</em><br>
<em>&gt; religion, then a Friendly AI would be Jewish.
</em><br>
<p>The thing is that my wife, a fairly rational person and a Buddhist, would
<br>
not accept the statement &quot;If you assume that Buddhism is the correct
<br>
religion, then a Friendly AI would be Buddhist.&quot;
<br>
<p>The fact that you, Eliezer, accept this statement is a consequence of your
<br>
scientific rationalist philosophy, and your faith in the potential
<br>
rationality of AI's.  None of these religious folks -- or very few of
<br>
them --care what statements you choose to accept...
<br>
<p><em>&gt; Whether I could convince a
</em><br>
<em>&gt; rabbi of that in advance is a separate issue, but it does, in
</em><br>
<em>&gt; fact, happen
</em><br>
<em>&gt; to be true, and *that's* the important thing from the perspective of
</em><br>
<em>&gt; safeguarding the integrity of the Singularity, regardless of how it plays
</em><br>
<em>&gt; out in pre-Singularity politics.
</em><br>
<p>So the important thing to you, is that the Singularity has &quot;integrity&quot;
<br>
according to your scientific rationalist belief system.  Fine.
<br>
<p>This doesn't mean the Singularity will have &quot;integrity&quot; according to the
<br>
belief systems of the vast majority of people in the world.
<br>
<p>I don't see how this doesn't constitute &quot;imposing your morality on the
<br>
world.&quot;  In my view, it does.  What you're saying is basically that you want
<br>
to ensure the Singularity is good according to your standards, where your
<br>
standards have to do with a kind of rationalistic &quot;integrity&quot; that you (but
<br>
not most others) see as extremely valuable.
<br>
<p><p><em>&gt;  &gt; No, I think this is an overstatement.  I think that some
</em><br>
<em>&gt; aspects of human
</em><br>
<em>&gt;  &gt;  thought are reaching out beyond the central region of the
</em><br>
<em>&gt; &quot;human zone,&quot;
</em><br>
<em>&gt;  &gt; whereas others are more towards the center of the human zone.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Of course.  And outside the human zone is a thousand times as much space
</em><br>
<em>&gt; which our thoughts will never touch.
</em><br>
<p>Yep...
<br>
<p><p><em>&gt; I expect to have my living daylights shocked out by the Singularity along
</em><br>
<em>&gt; with everyone else, regardless of whether I am open-minded or
</em><br>
<em>&gt; close-minded
</em><br>
<em>&gt; compared to other humans.  The differences bound up in the
</em><br>
<em>&gt; Singularity are
</em><br>
<em>&gt; not comparable in magnitude to the differences between humans.
</em><br>
<p>True also
<br>
<p><em>&gt; And they would, for that matter, rightly scream their
</em><br>
<em>&gt; heads off
</em><br>
<em>&gt; if SIAI created an AI that was given atheism as an absolute premise, the
</em><br>
<em>&gt; verbal formulation of rational empiricism as an absolute premise, or if
</em><br>
<em>&gt; there was in any other way created an AI that could not perceive the
</em><br>
<em>&gt; rightness of religion XYZ even if XYZ were true.
</em><br>
<p>The AGI that I create is going to have a bias toward rationality and toward
<br>
empiricism, because these are my values and those of the rest of the
<br>
Novamente team.  Not an *absolute bias*, but a bias.  When it's young, I'm
<br>
going to teach it scientific knowledge *as probable though not definite
<br>
truth*, and I'm going to show it the Koran as an example of an interesting
<br>
but empirically unsupported human belief system.
<br>
<p>Individuals who believe the scientific perspective is fundamentally wrong,
<br>
might be offended by this, but that's just life....  I am not going to teach
<br>
Novababy that the Koran and Torah and Vedas are just as valid as science,
<br>
just in order to please others with these other belief systems.  Of course,
<br>
I will also teach Novababy to think for itself, and once it becomes smarter
<br>
than me (or maybe before) it will come to its own conclusions, directed by
<br>
the initial conditions I've given it, but not constrained by them in any
<br>
absolute sense.
<br>
<p><em>&gt; I would answer that I have never found any specific thing to value other
</em><br>
<em>&gt; than people,
</em><br>
<p>Well, we are very different.  I also value many other things, including
<br>
animals (although I'm a bit fed up with the 5 dogs I'm living with at the
<br>
moment!!), plants, mathematics (some of which, although humanly invented,
<br>
seems to me to have value going beyond the human), computer programs,...
<br>
<p><em>&gt;   All sentient life has value, and so does the volition of that life.
</em><br>
<p>And this is your personal value system, not a universal one... in my own
<br>
value system, nonsentient life also has a lot of value....  This is a common
<br>
perspective, though not universal.
<br>
<p>I'm not saying you don't value nonsentient life at all, but the fact that
<br>
you omitted to mention it, suggests that maybe it's not as important to you
<br>
as it is to me.
<br>
<p>These variations among individual value systems may possibly be passed along
<br>
to the first AGI's.  If the first AGI is raised by a nature-lover, it may be
<br>
less likely to grow up to destroy forests &amp; fluffy bunnies...
<br>
<p><p><em>&gt;  If the
</em><br>
<em>&gt; AI *is* sensitive to your purpose, then I am worried what other
</em><br>
<em>&gt; things might
</em><br>
<em>&gt; be in your selfish interest, if you think it's valid for an AI to
</em><br>
<em>&gt; have goals
</em><br>
<em>&gt; that serve Ben Goertzel but not the human species.
</em><br>
<p>it is not true that I want the AI to have goals that serve me but not the
<br>
human species.  I'm not *that* selfish or greedy.
<br>
<p>The point is that I am going to teach a baby AGI, initially, *one particular
<br>
vision* of what best serves the human species, which is different from the
<br>
vision that many other humans have about what best serves the human species.
<br>
<p>I do not think there is any way to get around this subjectivity.  By
<br>
involving others in the Novababy teaching process, I think I will avoid the
<br>
most detailed specifics of my personal morality from becoming important to
<br>
the AGI.  However, there will be no Taliban members in the Novababy teaching
<br>
team, nor any Vodou houngans most likely (though I do know one, so it's
<br>
actually a possibility ;)....  Novababy will be taught *one particular
<br>
version* of how to best serve the human species, and then as it grows it
<br>
will develop its own ideas...
<br>
<p>There are big differences from the teaching of a human child, but also some
<br>
similarities...
<br>
<p><em>&gt;  &gt; But I want it to place Humans
</em><br>
<em>&gt;  &gt; pretty high on its moral scale -- initially, right up there at the top.
</em><br>
<em>&gt;  &gt; This is Partiality not Impartiality, as I see it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Don't you think there's a deadly sort of cosmic hubris in creating an AI
</em><br>
<em>&gt; that does something you personally know is wrong?
</em><br>
<p>Not a deadly sort.
<br>
<p>I think there is a lot of hubris in creating an AGI, or launching a
<br>
Singularity, in the first place, yeah.  It's a gutsy thing to try to do.
<br>
<p>And I do not intend to create an AI to do something I know is wrong.  Quite
<br>
the contrary, I intend to create an AI that initially embodies roughly the
<br>
same sense of &quot;rightness&quot; as myself and my social group (modern rationalist,
<br>
scientifically-minded transhumanists, with a respect for life and diversity,
<br>
and a particular fondness for the human species and other Earthly
<br>
life-forms).
<br>
<p>I think it's a lot safer to start a baby AGI off with the moral system that
<br>
I and my colleagues hold, than to start it off with some abstract moral
<br>
system that values humans only because they're sentient life forms.
<br>
<p>I do have a selfish interest here: I want me and the rest of the human
<br>
species to continue to exist.  I want this *separately* from my desire for
<br>
sentience and life generally to flourish.  And I intend to embed this
<br>
species-selfish interest into my AGI to whatever extent is possible.
<br>
<p><p><em>&gt;  &gt;&gt; The AI uses it to learn about how humans think about morality; you,
</em><br>
<em>&gt;  &gt;&gt; yourself, are a sample instance of &quot;humans&quot;, and an interim guide to
</em><br>
<em>&gt;  &gt;&gt; ethics (that is, your ethics are the ethics the AI uses when it's not
</em><br>
<em>&gt;  &gt;&gt; smart enough to have its own; *that* is not a problem).
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; What we want is for the AGI to have our own human-valuing ethics, until
</em><br>
<em>&gt;  &gt; such a point as it gets *so* smart that for it to use precisely human
</em><br>
<em>&gt;  &gt; ethics, would be as implausible as for a human to use precisely dog
</em><br>
<em>&gt;  &gt; ethics...
</em><br>
<em>&gt;
</em><br>
<em>&gt; Okay.  That last point there?  That's the point I'm concerned
</em><br>
<em>&gt; about - when
</em><br>
<em>&gt; the FAI gets *that* smart.  At *that* point I want the FAI to
</em><br>
<em>&gt; have the same
</em><br>
<em>&gt; kind of morality as, say, a human upload who has gotten *that*
</em><br>
<em>&gt; smart.  I do
</em><br>
<em>&gt; not think that a human upload who has gotten *that* smart would
</em><br>
<em>&gt; have human
</em><br>
<em>&gt; ethics but I don't think they would be the ethics that a rock or
</em><br>
<em>&gt; a bacterium
</em><br>
<em>&gt; would have, either.  Human ethics have the potential to grow;
</em><br>
<em>&gt; *that* is why
</em><br>
<em>&gt; an FAI needs human ethics *to start with*.
</em><br>
<p>Right, we agree on all this, but the thing you don't seem to fully accept is
<br>
that there is NO SUCH THING as &quot;human ethics&quot; generally speaking.  Human
<br>
ethics are all over the map.  Are you a vegetarian?  No, then your ethics
<br>
are very different from that of a lot of the world's population.  Do you go
<br>
to the doctor when you're sick?  Oops, according to Christian Scientists,
<br>
that's immoral, it's against God's Law....  Etc. etc. etc. etc. etc. etc.
<br>
etc.
<br>
<p>An AGI cannot be started off with generic human ethics because there aren't
<br>
any.  Like it or not, it's got to be started out with some particular form
<br>
of human ethics.  Gee, I'll choose something resembling mine rather than
<br>
Mary Baker Eddy's, because I don't want the AGI to think initially that
<br>
medical intervention in human illnesses is immoral... I want it to help
<br>
create longevity drugs, which according to Christian Scientists' ethics is
<br>
immoral...
<br>
<p><p><em>&gt;  When you are dealing with a seed AI, the
</em><br>
<em>&gt; AI's goal
</em><br>
<em>&gt; system is whatever the AI thinks its goal system ought to be.
</em><br>
<p>You're describing a dynamical system
<br>
<p>GoalSystem (t+1) = F ( GoalSystem(t) )
<br>
<p>where the F is the self-modifying dynamcs of the AI holding the goal system.
<br>
<p>I'm describing how I plan to set the initial condition...
<br>
<p><em>&gt;  &gt; We need to hard-wire and/or emphatically teach the system that our own
</em><br>
<em>&gt;  &gt; human-valuing ethics are the correct ones,
</em><br>
<em>&gt;
</em><br>
<em>&gt; Are they?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Let's ask that first,
</em><br>
<p>Look, humans have been debating ethics for millennia.  No consensus has been
<br>
reached.
<br>
<p>There is no rational way to decide which ethical system is &quot;correct.&quot;
<br>
Rather, ethical systems DEFINE what is &quot;correct&quot; -- not based on reasoning
<br>
from any premises, just by fiat.
<br>
<p><em>&gt;From a rational/empirical perspective, the choice between
</em><br>
non-internally-contradictory ethical systems is an arbitrary one.  From
<br>
other perspectives, it's not arbitrary at all -- religious folks may say
<br>
that the correct ethical system can be *felt* if you open up your inner mind
<br>
in the right way.
<br>
<p>I can see that these notions of reason versus experience, axioms versus
<br>
derivations, and so forth, may be perceived by an superhuman AGI as just so
<br>
much primitive silliness....  But within the scope of human thought, there
<br>
is no way we're gonna rationally decide which ethics are correct, or arrive
<br>
at any sort of consensus among humans on what the correct ethics is.  Ethics
<br>
is not that sort of thing.
<br>
<p><p><em>&gt; and in the course of asking it, we'll learn
</em><br>
<em>&gt; something
</em><br>
<em>&gt; about what kind of thinking a system needs to regard as valid in order to
</em><br>
<em>&gt; arrive at the same conclusions we have.
</em><br>
<em>&gt;
</em><br>
<em>&gt;  &gt; and let it start off with
</em><br>
<em>&gt;  &gt; these until it gets so smart it inevitably outgrows all its teachings.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The question of what you need to supply an AI with so that it
</em><br>
<em>&gt; *can* outgrow
</em><br>
<em>&gt; its teachings - not just end up in some random part of the space of
</em><br>
<em>&gt; minds-in-general, but actually *outgrow* the teachings it started with,
</em><br>
<em>&gt; after the fashion of say a human upload - is exactly the issue here.
</em><br>
<p>Well, that is not the qeustion of Friendly AI, that is simply the question
<br>
of AGI.
<br>
<p>Any AGI worthy of the name will  be able to outgrow its initial teachings.
<br>
Even humans can largely outgrow their initial teachings.  You have outgrown
<br>
a lot of yours, huh?
<br>
<p>The goal of friendly aI, generally speaking, should be to supply the AGI
<br>
with initial conditions (mind processes AND beliefs) that are likely to
<br>
cause its progressive self-modification to lead it to a favorable future
<br>
state.   And defining &quot;favorable&quot; here is a subjective value judgment!
<br>
<p><p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4556.html">Eugen Leitl: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Previous message:</strong> <a href="4554.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4555">[ date ]</a>
<a href="index.html#4555">[ thread ]</a>
<a href="subject.html#4555">[ subject ]</a>
<a href="author.html#4555">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
