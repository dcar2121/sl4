<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Threats to the Singularity.</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Threats to the Singularity.">
<meta name="Date" content="2002-06-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Threats to the Singularity.</h1>
<!-- received="Sat Jun 15 22:10:36 2002" -->
<!-- isoreceived="20020616041036" -->
<!-- sent="Sat, 15 Jun 2002 16:38:15 -0700" -->
<!-- isosent="20020615233815" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Threats to the Singularity." -->
<!-- id="3D0BCFE7.7080107@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJOELOCKAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Threats%20to%20the%20Singularity."><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Sat Jun 15 2002 - 17:38:15 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4022.html">Ben Goertzel: "RE: How Kurzweil lost the Singularity"</a>
<li><strong>Previous message:</strong> <a href="4020.html">Michael Roy Ames: "Re: Advanced Programming Environments."</a>
<li><strong>In reply to:</strong> <a href="3995.html">Ben Goertzel: "RE: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4024.html">Michael Roy Ames: "Advanced Programming Environments."</a>
<li><strong>Reply:</strong> <a href="4024.html">Michael Roy Ames: "Advanced Programming Environments."</a>
<li><strong>Reply:</strong> <a href="4025.html">Michael Roy Ames: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4038.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4121.html">Ben Goertzel: "RE: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4021">[ date ]</a>
<a href="index.html#4021">[ thread ]</a>
<a href="subject.html#4021">[ subject ]</a>
<a href="author.html#4021">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<p><em>&gt; Samantha,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;Well, in my personal opinion and speaking plainly, disowning
</em><br>
<em>&gt;&gt;one's species, literally all of humanity include one's self and
</em><br>
<em>&gt;&gt;loved ones, for an unknown set of hopefully stable higher
</em><br>
<em>&gt;&gt;intelligences is a very deep form of treason and betrays a
</em><br>
<em>&gt;&gt;singular contempt for humanity that I find utterly apalling.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But I really feel this is not correct.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The attitude you describe does not necessarily imply a *contempt* for
</em><br>
<em>&gt; humanity.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; On the contrary, it *could* imply this, but it could also imply a mere
</em><br>
<em>&gt; *indifference* to humanity.
</em><br>
<p><p>How can or should one be &quot;indifferent&quot; to one's species and to 
<br>
the survival of all existing higher sentients on this planet? 
<br>
If one is for increasing intelligence (how one defines that and 
<br>
why it is the only or most primary value are good questions) and 
<br>
the increase of sentience, I fail to see how one can be cavalier 
<br>
about the destruction of all currently known sentients.  How can 
<br>
one stand for intelligence and yet not care about billions of 
<br>
intelligent beings that already exist?
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt; I confess that I feel somewhat indifferent to humanity, sometimes (not
</em><br>
<em>&gt; always!).  Sometimes I just think of humanity as a vehicle for intelligent
</em><br>
<em>&gt; mind.  And, I think: If a better vehicle comes along, why is the
</em><br>
<em>&gt; human-vehicle so important?  And why are our individual human minds --
</em><br>
<em>&gt; including mine -- so important.  In the big picture of the evolution of life
</em><br>
<em>&gt; and mind in the cosmos, surely they aren't....
</em><br>
<em>&gt;
</em><br>
<p><p>Sentient beings are important.  Wiping out existing ones for the 
<br>
next year's model is not exactly ethically neutral.  If we do 
<br>
not care about sentient beings as long as something we thing 
<br>
might be more powerful is on the horizon then we are uncaring of 
<br>
ourselves and our fellow beings.  There is no getting around this.
<br>
<p>&nbsp;
<br>
<em>&gt; Sure, the human race is important to me emotionally... just like my family
</em><br>
<em>&gt; is important to me emotionally ... just as my own limbs are important to me
</em><br>
<em>&gt; emotionally ...    but why are my personal human emotions so important?
</em><br>
<em>&gt;
</em><br>
<p><p>Why would you disown what is of value to you?  On the basis of a 
<br>
&nbsp;&nbsp;hypothetically better intelligence (along some dimensions of 
<br>
&quot;better&quot;)?  Why would you and how is it justified to also 
<br>
casually speak of &quot;it&quot; being more important than not only your 
<br>
own values and life but that of all other human beings also? 
<br>
All I am pushing for here is that we pause before writing off 
<br>
the human race as inconsequential as long as we can get to 
<br>
Singularity.  To do so writes off the known for something that 
<br>
is unknown and unknowable.  That does not seem reasonable to me.
<br>
<p>&nbsp;
<br>
<em>&gt; Indifference to humanity could come out of nihilism. But it could also come
</em><br>
<em>&gt; out of having values that go beyond any particular species, or any
</em><br>
<em>&gt; particular vehicle, and focus on general things like intelligence and
</em><br>
<em>&gt; creation.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In short, there are MANY different psychological motives that could underly
</em><br>
<em>&gt; the attitude Mike displays, contempt being only one of them.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;How about we just grow a lot more sane human beings instead of
</em><br>
<em>&gt;&gt;digging continuously for technological fixes that really aren't
</em><br>
<em>&gt;&gt;fixes to the too often cussedness of local sentients?  Replacing
</em><br>
<em>&gt;&gt;them with something that is faster and arguably smarter but may
</em><br>
<em>&gt;&gt;or may not be any more wise is not an answer.  Scrapping
</em><br>
<em>&gt;&gt;sentients is to be frowned upon even if you think you can and
</em><br>
<em>&gt;&gt;even do create sentients that are arguably better along some
</em><br>
<em>&gt;&gt;parameters.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &quot;Wisdom&quot; is a nebulous human concept that means different things to
</em><br>
<em>&gt; different people, and in different cultures.
</em><br>
<em>&gt; 
</em><br>
<p><p>I can tell you what it isn't.  It isn't about writing off all 
<br>
existing sentients in favor of something you think can be but 
<br>
you have no idea what it will become.  Human is the only 
<br>
sentient basis we have to reason from and in any event is what 
<br>
we ourselves are and we have no choice but to reason from that 
<br>
basis.
<br>
<p><p><em>&gt; However, I think it's pretty likely that intelligent software WILL be wiser
</em><br>
<em>&gt; than humans, due to reasons Eliezer has pointed out nicely in his writings.
</em><br>
<em>&gt; We have an evolutionary heritage that makes it really tough for us to be
</em><br>
<em>&gt; wise, and there seems to be no reason why intelligent software would have
</em><br>
<em>&gt; any similar problem.
</em><br>
<em>&gt; 
</em><br>
<p><p>It is not at all clear that not having an evolutionary history 
<br>
or processing more information faster leads to wisdom or 
<br>
&quot;better&quot; for sufficient values of &quot;better&quot;.  How will we even 
<br>
evaluate the question?  What are the criteria and how do we know 
<br>
they are the correct criteria?  I think we should be very sure 
<br>
of the answers to such questions since it is nothing less than 
<br>
the survival of the human race that is at stake.
<br>
<p><p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4022.html">Ben Goertzel: "RE: How Kurzweil lost the Singularity"</a>
<li><strong>Previous message:</strong> <a href="4020.html">Michael Roy Ames: "Re: Advanced Programming Environments."</a>
<li><strong>In reply to:</strong> <a href="3995.html">Ben Goertzel: "RE: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4024.html">Michael Roy Ames: "Advanced Programming Environments."</a>
<li><strong>Reply:</strong> <a href="4024.html">Michael Roy Ames: "Advanced Programming Environments."</a>
<li><strong>Reply:</strong> <a href="4025.html">Michael Roy Ames: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4038.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4121.html">Ben Goertzel: "RE: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4021">[ date ]</a>
<a href="index.html#4021">[ thread ]</a>
<a href="subject.html#4021">[ subject ]</a>
<a href="author.html#4021">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
