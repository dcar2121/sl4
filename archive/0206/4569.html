<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: FAI means no programmer-sensitive AI morality</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: FAI means no programmer-sensitive AI morality">
<meta name="Date" content="2002-06-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: FAI means no programmer-sensitive AI morality</h1>
<!-- received="Sat Jun 29 20:37:07 2002" -->
<!-- isoreceived="20020630023707" -->
<!-- sent="Sat, 29 Jun 2002 18:17:13 -0600" -->
<!-- isosent="20020630001713" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: FAI means no programmer-sensitive AI morality" -->
<!-- id="LAEGJLOGJIOELPNIOOAJOEDOCMAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D1E3180.4030900@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20FAI%20means%20no%20programmer-sensitive%20AI%20morality"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sat Jun 29 2002 - 18:17:13 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4570.html">Ben Goertzel: "RE: Ben vs. Ben"</a>
<li><strong>Previous message:</strong> <a href="4568.html">Eliezer S. Yudkowsky: "Re: Ben vs. Ben"</a>
<li><strong>In reply to:</strong> <a href="4559.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4575.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4575.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4592.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4569">[ date ]</a>
<a href="index.html#4569">[ thread ]</a>
<a href="subject.html#4569">[ subject ]</a>
<a href="author.html#4569">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
hi,
<br>
<p><em>&gt;
</em><br>
<em>&gt; Uh... Ben, unless you spent 4 hours a day during your first twelve years
</em><br>
<em>&gt; being indoctrinated in a fundamentalist religion...
</em><br>
<p>No, but I've lived with a profoundly religious person for the last 18 years.
<br>
So we may have to call it a draw...
<br>
<p><em>&gt;
</em><br>
<em>&gt; No offense, Ben, but I probably have a much better picture of how a
</em><br>
<em>&gt; fundamentalist Jew *actually thinks* than you do.
</em><br>
<p>I'm sure you do.  that's because my exposure to religion is based more on my
<br>
wife and her Zen Buddhist friends, and a close friend who is a Sufi ... I
<br>
don't have much experience with Judaism except the Reform style....
<br>
<p><em>&gt; In my experience religious people argue just like other
</em><br>
<em>&gt; people,
</em><br>
<p>Eli -- That's just Jewish people ;-D
<br>
<p><em>&gt; I think you are confusing the principles which people verbally adhere to
</em><br>
<em>&gt; with the way that people actually think.
</em><br>
<p>I don't think so, I think I know how my wife and my Sufi friend think fairly
<br>
well...
<br>
<p><em>&gt;  I'm curious, Ben, have you ever
</em><br>
<em>&gt; actually *been* religious?
</em><br>
<p>Sort of.  I considered myself a Zen Buddhist for a while, and then drifted
<br>
away from it...
<br>
<p><em>&gt; Do you know how religious thinking works *in
</em><br>
<em>&gt; the first person*?
</em><br>
<p>I know how Zen works in the first person.  The Zen text I loved most was
<br>
&quot;The Zen Teachings of Huang Po,&quot; which is primarily focused on &quot;stopping
<br>
thought&quot; and ridding the mind of all thoughts, all logic, all reason, all
<br>
ego, all ideas, and accepting that nothing is either real or unreal.  For a
<br>
while I associated with others into this stuff, meditated in a group, etc.
<br>
<p><em>&gt; Because I have to say you're sounding like a
</em><br>
<em>&gt; complete outsider here - like your idea of religion comes from watching
</em><br>
<em>&gt; scientists debating theologians about the nature of truth - which is a
</em><br>
<em>&gt; very different thing from how ordinary religious people actually think
</em><br>
<em>&gt; in practice.  I'm sure that you've had chats with your semireligious
</em><br>
<em>&gt; parents and your semireligious wife and so on,
</em><br>
<p>My wife is a Zen priest, she's far more than semireligious!
<br>
<p>I think the disconnect here is somewhat a &quot;Zen vs. fundamentalist Jew&quot;
<br>
thing.. these two religious perspectives are pretty different...
<br>
<p><em>&gt; and maybe read a few
</em><br>
<em>&gt; books, but you may need to consider that standing back as a scientist
</em><br>
<em>&gt; and going &quot;Gosh, how *utterly alien* and *unempirical*&quot; is going to give
</em><br>
<em>&gt; you a different perspective, and one which is maybe a bit unrealistic
</em><br>
<em>&gt; about the way religious people talk to each other when they're not
</em><br>
<em>&gt; debating a scientist or whatever.
</em><br>
<p>Zen isn't really focused on talking or debating at all.  Words are not
<br>
perceived as very  meaningful.  They're used only to lead you beyond words,
<br>
and sparingly.  The beginning of Zen was the &quot;wordless transmission&quot; ...
<br>
<p><em>&gt;
</em><br>
<em>&gt; The idea that religion and rationality are orthogonal is a modern idea
</em><br>
<em>&gt; proposed by modern theologians;
</em><br>
<p>Actually, it is there very, very clearly in &quot;The Zen Teachings of Huang Po&quot;
<br>
from 800AD or so.
<br>
<p><em>&gt; Anyway, let's keep this conversation focused.  Ben, is it your assertion
</em><br>
<em>&gt; that even if the Jewish or Buddhist religion were correct, this would
</em><br>
<em>&gt; not be apparent to a Friendly AI that had been programmed by atheists?
</em><br>
<em>&gt; Because *I* would most certainly regard this as a bug.
</em><br>
<p>I don't think that Zen is the sort of thing that can be correct or
<br>
incorrect.  It's a different sort of thing than that.
<br>
<p>It just is.
<br>
<p>So, I guess, speaking from the Zen Buddhist in me, I reject your question as
<br>
being irrelevant to Zen, and being part of the samsaric world.  If you were
<br>
here I'd just have to hit you with a stick and jolt you to enlightenment ;&gt;
<br>
<p><em>&gt;  But just in case *we* happen to be the
</em><br>
<em>&gt; ones who are in fact horribly, fundamentally wrong, whether or not any
</em><br>
<em>&gt; current human is right, we need to make sure that the AI is not bound to
</em><br>
<em>&gt; our mistakes.
</em><br>
<p>This kind of right vs. wrong, dualistic thinking is antithetical to Zen.
<br>
<p>And so is the seeking, grasping nature of the whole AGI pursuit.  Zen is
<br>
about being contented with what is, not about constantly striving to create
<br>
a whole new order.  It teaches compassion, but simple compassion in each
<br>
moment, not compassion via building thinking machines to change the world.
<br>
If the Zen Buddhist in me were dominant, I wouldn't be working on AGI, I'd
<br>
be sitting and meditating, walking thru the woods, and helping the needy
<br>
directly.
<br>
<p><p><p><em>&gt; First comes the question of what is true.
</em><br>
<p>In Zen as I practiced it, the idea of &quot;true&quot; was itself an illusion to be
<br>
overcome...
<br>
<p><p><p><em>&gt; No human thought is outside the correspondence theory of truth.
</em><br>
<p>Your original statement talked about correspondence with external reality.
<br>
if you modify it to include correspondence with internal reality, then I am
<br>
closer to agreeing with you...
<br>
<p><em>&gt; Now it may be that Zen proceeds from arational thoughts to an arational
</em><br>
<em>&gt; conclusion which is important not because it corresponds to some outside
</em><br>
<em>&gt; thing but because it is itself, and in this sense the core of Zen may
</em><br>
<em>&gt; come closer to being outside the correspondence theory of truth than
</em><br>
<em>&gt; anything else I know of, but it is surrounded by a core of mystical
</em><br>
<em>&gt; tradition which, like all forms of human storytelling, makes use of the
</em><br>
<em>&gt; correspondence theory of truth.
</em><br>
<p>That is why I left Zen, I couldn't stomach the more mystical and mythical
<br>
aspects of it, although I still &quot;believe&quot; and &quot;practice&quot; the core of it...
<br>
<p><em>&gt; &gt; The thing is that my wife, a fairly rational person and a
</em><br>
<em>&gt; Buddhist, would
</em><br>
<em>&gt; &gt; not accept the statement &quot;If you assume that Buddhism is the correct
</em><br>
<em>&gt; &gt; religion, then a Friendly AI would be Buddhist.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Sounds like a testable statement.  Would you care to put it to the test?
</em><br>
<p>I asked her.  She didn't want to answer ;)
<br>
<p><em>&gt; Ben, imagine what kind of precautions you would ask a Catholic
</em><br>
<em>&gt; programming a Friendly AI to take in order to ensure that the AI would
</em><br>
<em>&gt; eventually convert to atheism, given that atheism is correct.  Now do
</em><br>
<em>&gt; that yourself.  What does this have to do with &quot;scientific rationalism&quot;?
</em><br>
<p>The two cases are very different.
<br>
<p>Atheism is a conclusion that a mind can be reasonably hoped to conclude
<br>
based on observation of the external world, whereas Catholicism is not -- it
<br>
is something a mind can only be hoped to conclude via internal experience,
<br>
and instruction by others.
<br>
<p>So the two cases are very different.  To make a mind that could start out
<br>
Catholic but become atheist, it would suffice to make a mind that could
<br>
revise its own beliefs based on observation.  To make a mind that could
<br>
plausibly start out atheist but become Catholic, one would have to guarantee
<br>
that
<br>
<p>a) the mind were instructed in Catholicism at some point
<br>
b) the mind were built to have similar spiritual experiences to humans
<br>
<p>This asks a lot more.  Atheism is not anthropomorphic or human-culture-bound
<br>
so an AI can naturally be expected to happen upon it as a possible attitude.
<br>
Catholicism is highly anthropomorphic, so it would take a lot of work to
<br>
make a nonhuman system have spiritual experiences consistent with the
<br>
&quot;father son and holy ghost&quot; meme, etc.
<br>
<p><p><em>&gt; I think that asking how to ensure that an AI created by atheists would
</em><br>
<em>&gt; converge to a religion, given that this religion is correct, is a
</em><br>
<em>&gt; necessary exercise for understanding how an AI can repair whatever deep
</em><br>
<em>&gt; flaws may very well exist in our own worldviews.  In this sense, I think
</em><br>
<em>&gt; that refusing to put yourself in the shoes of a Christian building an AI
</em><br>
<em>&gt; and asking what would be &quot;fair&quot; is not just a matter of pre-Singularity
</em><br>
<em>&gt; politics.  It is a test - and not all that stringent a test, at that -
</em><br>
<em>&gt; of an AI's ability to transcend the mistakes of its programmers.  If you
</em><br>
<em>&gt; don't want to apply this test, what are you going to use instead?
</em><br>
<p>it's not impossible that a Novamente could decide the &quot;father, son and holy
<br>
ghost&quot; were the real truth underlying the universe, but it's incredibly
<br>
unlikely since Novababy will have neither father nor son... these concepts
<br>
will not be at all natural to it...
<br>
<p><p><em>&gt; I view it as an unbearably horrifying possibility that the next billion
</em><br>
<em>&gt; years of humanity's existence may be substantially different depending
</em><br>
<em>&gt; on whether the first AGI was raised by an environmentalist.  It's
</em><br>
<em>&gt; equally horrifying whether you're an environmentalist looking at Eliezer
</em><br>
<em>&gt; or vice versa.  It shouldn't depend on who happens to build the AI, it
</em><br>
<em>&gt; should depend on *who's right*.
</em><br>
<p>You seem to have this idea that there is some kind of &quot;meta-rightness&quot;
<br>
standard by which different ethical standards can be judged more or less
<br>
correct.
<br>
<p>There is no such thing.
<br>
<p>And, I think it's pretty obvious that the outcome of the next billion years
<br>
MAY depend on the initial conditions with which the Singularity is launched.
<br>
Complex systems often display a sensitive dependence on initial conditions.
<br>
Along with a tendency to fall into certain general attractors regardless of
<br>
initial conditions.  The details of the future will probably depend on the
<br>
details of the Singularity's launch -- and whether humans or trees continue
<br>
to exist must be considered &quot;details&quot; from a post-Singularity perspective.
<br>
<p><em>&gt; If nobody's right then the choice
</em><br>
<em>&gt; should be kicked back to the individual.  If there's no way to do that
</em><br>
<em>&gt; then you might as well take a majority vote of the existing humans or
</em><br>
<em>&gt; pick a choice that's as good as any other; in absolutely no case should
</em><br>
<em>&gt; the programmers occupy a priviliged position with respect to an AI that
</em><br>
<em>&gt; may end up carrying the weight of the Singularity.
</em><br>
<p>Sorry, but if there's an arbitrary choice to be made (and there is), and I'm
<br>
in a position of some control, I'm going to make the decision based on the
<br>
input of individuals I respect rather than based on random selection or
<br>
majority vote.
<br>
<p>Frankly, I don't have that much respect for the views of the majority of
<br>
humans on these very subtle issues.  Sorry if that's too arrogant.
<br>
<p><p><em>&gt; Develop its own ideas from where?  How?  Why?  Every physical event has
</em><br>
<em>&gt; a physical cause.  There are causes for humans developing their own
</em><br>
<em>&gt; ideas as they grow up, most of them evolved.  You are standing not only
</em><br>
<em>&gt; &quot;in loco parentis&quot; but &quot;in loco evolution&quot; to your AI.  What causes will
</em><br>
<em>&gt; you give Novamente to develop its own ideas?
</em><br>
<p>There is an inbuilt initial goal which reinforces this behavior, actually.
<br>
<p><em>&gt; &gt; I do have a selfish interest here: I want me and the rest of the human
</em><br>
<em>&gt; &gt; species to continue to exist.  I want this *separately* from my
</em><br>
<em>&gt; desire for
</em><br>
<em>&gt; &gt; sentience and life generally to flourish.  And I intend to embed this
</em><br>
<em>&gt; &gt; species-selfish interest into my AGI to whatever extent is possible.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ben, to the best of my ability to tell, the abilities an AI would use to
</em><br>
<em>&gt; grow beyond its programmers' and the abilities an AI would use to
</em><br>
<em>&gt; correct horrifying errors by its programmers are exactly the same
</em><br>
<em>&gt; structurally.
</em><br>
<p>Sure.
<br>
<p><em>&gt; Your &quot;pseudo-selfish&quot; attitude here - i.e, that it's okay
</em><br>
<em>&gt; to program an AI with altruism that is just yours - endangers the AI's
</em><br>
<em>&gt; possession of even that altruism.
</em><br>
<p>But Eli, there is no &quot;universal ethics&quot; with which to program the AI.
<br>
<p>You've suggested to choose the AI's ethics by majority vote, to get around
<br>
this problem....  I'd have to think about that one long and hard.
<br>
<p><p><em>&gt; Of course humans argue about everything.  The question is which of these
</em><br>
<em>&gt; answers is *right*.  If your answer is no righter than anyone else's
</em><br>
<em>&gt; then how dare you impose it on the Singularity?  Why wouldn't anyone
</em><br>
<em>&gt; else in the world be justly outraged at such a thing?  Letting everyone
</em><br>
<em>&gt; pick their own solutions whenever possible is one answer.
</em><br>
<p>My answer is that ethical systems tell you what's right, and there is no
<br>
&quot;meta-ethics&quot; telling you which ethical system is right -- meta-ethics are
<br>
just ethics...
<br>
<p>In other words, my ethical system tells me that my ethical system is righter
<br>
than others ;.&gt;  And most other ethical systems are in the same
<br>
self-referential position!
<br>
<p><em>&gt;
</em><br>
<em>&gt; &gt; An AGI cannot be started off with generic human ethics because
</em><br>
<em>&gt; there aren't
</em><br>
<em>&gt; &gt; any.  Like it or not, it's got to be started out with some
</em><br>
<em>&gt; particular form
</em><br>
<em>&gt; &gt; of human ethics.  Gee, I'll choose something resembling mine rather than
</em><br>
<em>&gt; &gt; Mary Baker Eddy's, because I don't want the AGI to think initially that
</em><br>
<em>&gt; &gt; medical intervention in human illnesses is immoral...
</em><br>
<em>&gt;
</em><br>
<em>&gt; And you think these two positions are equally right?
</em><br>
<p>No, *I* don't, but that's because my ethical system tells me that my ethical
<br>
system is right.
<br>
<p>hers told her that hers was right...
<br>
<p>PLEASE, articulate this mystical meta-ethic that allows one to determine
<br>
which ethical system is correct -- but that does not just become &quot;yet
<br>
another ethical system&quot; !!!  Details please!  God is in the details!
<br>
<p><em>&gt; &gt; There is no rational way to decide which ethical system is &quot;correct.&quot;
</em><br>
<em>&gt; &gt; Rather, ethical systems DEFINE what is &quot;correct&quot; -- not based
</em><br>
<em>&gt; on reasoning
</em><br>
<em>&gt; &gt; from any premises, just by decision.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Hm.  According to you, people sure do spend a lot of time arguing about
</em><br>
<em>&gt; things that they should just be deciding by fiat.  In fact, everyone
</em><br>
<em>&gt; except a relative handful of cultural relativists - a tiny minority of
</em><br>
<em>&gt; humanity, in other words - seems to instinctively treat ethics as if it
</em><br>
<em>&gt; were governed directly by the correspondence theory of truth.  Why is
</em><br>
<em>&gt; that, do you suppose?
</em><br>
<p>I don't think that is true at all.  Please explain in detail how you think
<br>
the correspondence theory of truth tells you what ethics is right.
<br>
<p>My sister and wife think it's wrong to kill animals to eat them.  I don't.
<br>
How does the correspondence theory of truth help decide this ethical
<br>
difference?
<br>
<p><em>&gt; Hm.  It seems like you simultaneously believe:
</em><br>
<em>&gt;
</em><br>
<em>&gt; (a) there are correct answers for questions of simple fact and that any
</em><br>
<em>&gt; AGI should be able to easily outgrow programmer-supplied wrong answers
</em><br>
<em>&gt; for questions of simple fact
</em><br>
<em>&gt; (b) ethical questions are fundamentally different from questions of
</em><br>
<em>&gt; simple fact because no correct answers exist
</em><br>
<em>&gt; (c) an AGI should be able outgrow programmer-supplied ethics as easily
</em><br>
<em>&gt; as it outgrows programmer-supplied facts; in fact, this has nothing to
</em><br>
<em>&gt; do with Friendly AI but is simply a question of AI
</em><br>
<em>&gt;
</em><br>
<em>&gt; I can see how (a) (!b) (c) go together but not how (a) (b) (c) go
</em><br>
<em>&gt; together.  If you assert (b) then the human ability to outgrow
</em><br>
<em>&gt; parentally inculcated ethics would depend on evolved functionality above
</em><br>
<em>&gt; and beyond generic rationality.
</em><br>
<p>Yes, I assert a, b and c.  And you are right, the cognitive dynamics
<br>
underlying &quot;outgrowing initial ethics&quot; will be a little different from those
<br>
involved in &quot;outgrowing initial factual beliefs&quot;, at least in Novamente.
<br>
But there will be plenty of overlap too.
<br>
<p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4570.html">Ben Goertzel: "RE: Ben vs. Ben"</a>
<li><strong>Previous message:</strong> <a href="4568.html">Eliezer S. Yudkowsky: "Re: Ben vs. Ben"</a>
<li><strong>In reply to:</strong> <a href="4559.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4575.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4575.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4592.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4569">[ date ]</a>
<a href="index.html#4569">[ thread ]</a>
<a href="subject.html#4569">[ subject ]</a>
<a href="author.html#4569">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
