<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: How hard a Singularity?</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: How hard a Singularity?">
<meta name="Date" content="2002-06-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: How hard a Singularity?</h1>
<!-- received="Sun Jun 23 05:29:01 2002" -->
<!-- isoreceived="20020623112901" -->
<!-- sent="Sat, 22 Jun 2002 21:40:28 -0600" -->
<!-- isosent="20020623034028" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: How hard a Singularity?" -->
<!-- id="LAEGJLOGJIOELPNIOOAJIEFGCLAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D14FBC4.9010803@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20How%20hard%20a%20Singularity?"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sat Jun 22 2002 - 21:40:28 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4180.html">Michael Roy Ames: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4178.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4162.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4194.html">Eliezer S. Yudkowsky: "Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4194.html">Eliezer S. Yudkowsky: "Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4269.html">Stephen Reed: "RE: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4179">[ date ]</a>
<a href="index.html#4179">[ thread ]</a>
<a href="subject.html#4179">[ subject ]</a>
<a href="author.html#4179">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt;  &gt; Our seed AI is going to get its human-level intelligence, not purely by
</em><br>
<em>&gt;  &gt; its own efforts, but largely based on the human-level intelligence of
</em><br>
<em>&gt;  &gt; millions of humans working over years/decades/centuries.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ah.  Well, again we have very different models of intelligence.  I don't
</em><br>
<em>&gt; think you can use human knowledge as the mindstuff of an AI.
</em><br>
<em>&gt;  I
</em><br>
<em>&gt; don't think
</em><br>
<em>&gt; AI can be built by borrowing human content.
</em><br>
<p>Eliezer, I don't think that &quot;AI can be built by borrowing human content&quot;
<br>
either, and you should know that.
<br>
<p>The Cyc project comes close to the perspective you cite, but that is not my
<br>
project.
<br>
<p>I think that the first AGI will be created by
<br>
<p>a) engineering a system embodying cognitive, perception and action
<br>
mechanisms that are only loosely based on human cognitive science
<br>
<p>b) having this system grow from a baby into a useful intelligent mind by a
<br>
combination of: autonomous exploration of digital and physical environments,
<br>
self-organization, goal-oriented self-modification, and explicit teaching by
<br>
humans
<br>
<p>In terms of part b), having human knowledge to read, and humans to
<br>
communicate with, will be a big help to the system in boosting itself up to
<br>
human-level intelligence, and MUCH LESS of a help to the system in boosting
<br>
itself up further to superhuman intelligence.
<br>
<p>I guess what you're saying is that you think learning from human knowledge,
<br>
and explicit education by humans, will not be an important part of getting
<br>
an AGI up to human level.  I disagree with you on this, but I can't prove
<br>
I'm right, of course.
<br>
<p>However, just because I think human knowledge and human teaching will be
<br>
helpful to an AI in reaching human-level intelligence, does NOT imply that I
<br>
want to &quot;build an AI by borrowing human content.&quot;
<br>
<p>If an AI is not taught by us, and doesn't fill its mind with books we've
<br>
written and theorems we've proved, how is it going to get intelligent?
<br>
Purely by interacting with the environment on its own?  This sounds a lot
<br>
less efficient to me... and also a lot less likely to result in a
<br>
human-sympathetic AI...
<br>
<p><em>&gt; So you look at pouring human content into an AI, and say, &quot;When we reach
</em><br>
<em>&gt; human-level, we will run out of mindstuff.&quot;
</em><br>
<p>No, this is NOT AT ALL what I was saying.  Yet again, you are putting words
<br>
into my mouth, making it seem as if I were making a much different, and much
<br>
weaker, point than I was actually making.
<br>
<p>I do NOT envision &quot;pouring human content into an AI&quot;, in terms of directly
<br>
force-feeding its mind with human knowledge databases a la Cyc.  What I
<br>
envision is that an AI will turn itself from a baby into a mature mind by
<br>
learning from human teachers and by studying human knowledge, including
<br>
books and mathematics and software, and perhaps also explicit knowledge DB's
<br>
like Cyc.  And I believe that this knowledge will accelerate its development
<br>
to the roughly human-level, much beyond the pace that would be possible if
<br>
all this knowledge and all this teaching were not available.
<br>
<p><em>&gt; And I look at creating AI as
</em><br>
<em>&gt; the task of building more and more of that essential spark that *creates*
</em><br>
<em>&gt; content - with the transfer of any content the AI could not have
</em><br>
<em>&gt; created on
</em><br>
<em>&gt; her own, basically a bootstrap method or side issue
</em><br>
<p>You should know by now that I also view the job of building AI as primarily
<br>
a job of creating the right cognitive mechanisms.
<br>
<p>Novamente is intended as an autonomous, totally adaptive experiential
<br>
learning system, not as a system that confronts the world based on a fixed,
<br>
pre-provided set of knowledge.
<br>
<p>However, I think that the right set of cognitive mechanisms gives you a
<br>
*baby* AI.  I don't think that teaching the baby is &quot;basically a bootstrap
<br>
method or side issue.&quot;  (I understand that the word &quot;baby&quot; has some
<br>
undesirable anthropomorphic connotations, but the alternative would be to
<br>
fabricate a new word for a new Ai with a content-free mind, and I can't
<br>
think of a good coinage at the moment!)  I think that what the AGI learns
<br>
from human knowledge and interactive human teaching, will be just as
<br>
important a part of its mind as the cognitive mechanisms that are initially
<br>
put in.
<br>
<p><em>&gt; &quot;When the AI
</em><br>
<em>&gt; reaches human level, she will be able to swallow the thoughts
</em><br>
<em>&gt; that went into
</em><br>
<em>&gt; her own creation; she will be able to improve her own spark, recursively.&quot;
</em><br>
<p>And I agree with that -- the question is *how fast* will the AI be able to
<br>
improve itself.
<br>
<p>It's a quantitative question.  Your intuitive estimate is much faster than
<br>
mine...
<br>
<p><em>&gt; An AI will have much to learn from human mind-content, but by the time
</em><br>
<em>&gt; reaching human-level is anything like an issue, the most
</em><br>
<em>&gt; important part of
</em><br>
<em>&gt; what she knows will belong to her; it won't be borrowed from humans.
</em><br>
<p>I doubt that very  much.  I think that a key part of getting a baby AI to
<br>
useful human-adult-level intelligence will be imbibing human patterns of
<br>
thought and interaction -- learning from people via reading text and
<br>
databases, and via interaction.  Hence I think the first AGI, even if built
<br>
on radically nonhuman data structures and dynamics, will have a lot of
<br>
humanity in its emergent mind-patterns at first...
<br>
<p>I think you place way too much faith in &quot;bootstrapping&quot; style
<br>
self-organization.  Creating a smart system that can modify its own code,
<br>
and giving it good perceptors and actuators, will lead to a mature, usefully
<br>
self-improving and world-understanding AGI *eventually*, but how long will
<br>
it take?  I think the process will go faster by far if teaching by humans is
<br>
a big part of the process.  I also think that a human-friendly AGI is more
<br>
likely to result if the system achieves its intelligence partly thru being
<br>
taught by humans.
<br>
<p>You may say that with good enough learning methods, no teaching is
<br>
necessary.  Maybe so.  I know you think Novamente's learning methods are too
<br>
weak, though you have not explained why to me in detail, nor have you
<br>
proposed any concrete alternatives.  However, I think that *culture and
<br>
social interaction* help us humans to grow from babies into mature adult
<br>
minds in spite of the weaknesses of our learning methods, and I think that
<br>
these same things can probably help a baby AGI to grow from a piece of
<br>
software into a mature AGI capable of directing its activities in a useful
<br>
way and solving hard problems.
<br>
<p>You may say that I'm just anthropomorphizing here, but I don't think so.
<br>
Clearly teaching an AGI will be very different from teaching a human.  But
<br>
it seems so dumb not to give our early-stage would-be AGI the benefit of
<br>
human knowledge and intuition, which is considerable, though flawed.  And if
<br>
we do get it started with our teaching &amp; our knowledge, then when it
<br>
outstrips us, it will face a new set of challenges.  I'm sure it will be
<br>
able to meet these challenges, but how fast?  I don't know, and neither do
<br>
you!
<br>
<p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4180.html">Michael Roy Ames: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4178.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4162.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4194.html">Eliezer S. Yudkowsky: "Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4194.html">Eliezer S. Yudkowsky: "Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4269.html">Stephen Reed: "RE: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4179">[ date ]</a>
<a href="index.html#4179">[ thread ]</a>
<a href="subject.html#4179">[ subject ]</a>
<a href="author.html#4179">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
