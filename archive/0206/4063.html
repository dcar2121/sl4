<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Threats to the Singularity.</title>
<meta name="Author" content="Gordon Worley (redbird@rbisland.cx)">
<meta name="Subject" content="Re: Threats to the Singularity.">
<meta name="Date" content="2002-06-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Threats to the Singularity.</h1>
<!-- received="Mon Jun 17 09:54:04 2002" -->
<!-- isoreceived="20020617155404" -->
<!-- sent="Mon, 17 Jun 2002 09:45:06 -0400" -->
<!-- isosent="20020617134506" -->
<!-- name="Gordon Worley" -->
<!-- email="redbird@rbisland.cx" -->
<!-- subject="Re: Threats to the Singularity." -->
<!-- id="6F1B8958-81F8-11D6-9BD5-000A27B4DEFC@rbisland.cx" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3D0DAC71.3040004@objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Gordon Worley (<a href="mailto:redbird@rbisland.cx?Subject=Re:%20Threats%20to%20the%20Singularity."><em>redbird@rbisland.cx</em></a>)<br>
<strong>Date:</strong> Mon Jun 17 2002 - 07:45:06 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4064.html">Eugen Leitl: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4062.html">F Baube: "Re: On wisdom"</a>
<li><strong>In reply to:</strong> <a href="4058.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4064.html">Eugen Leitl: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4064.html">Eugen Leitl: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4067.html">Eliezer S. Yudkowsky: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4075.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4063">[ date ]</a>
<a href="index.html#4063">[ thread ]</a>
<a href="subject.html#4063">[ subject ]</a>
<a href="author.html#4063">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Monday, June 17, 2002, at 05:31  AM, Samantha Atkins wrote:
<br>
<p><em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Gordon Worley wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; On Saturday, June 15, 2002, at 07:38  PM, Samantha Atkins wrote:
</em><br>
<em>&gt;&gt;&gt; How can or should one be &quot;indifferent&quot; to one's species and to the 
</em><br>
<em>&gt;&gt;&gt; survival of all existing higher sentients on this planet? If one is 
</em><br>
<em>&gt;&gt;&gt; for increasing intelligence (how one defines that and why it is the 
</em><br>
<em>&gt;&gt;&gt; only or most primary value are good questions) and the increase of 
</em><br>
<em>&gt;&gt;&gt; sentience, I fail to see how one can be cavalier about the 
</em><br>
<em>&gt;&gt;&gt; destruction of all currently known sentients.  How can one stand for 
</em><br>
<em>&gt;&gt;&gt; intelligence and yet not care about billions of intelligent beings 
</em><br>
<em>&gt;&gt;&gt; that already exist?
</em><br>
<em>&gt;&gt; First off, attachment to humanity is a bias that prevents rational 
</em><br>
<em>&gt;&gt; thought.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Rational?  By what measure?  How is attachment to the well-being of 
</em><br>
<em>&gt; ourselves and all like us irrational?
</em><br>
<p>Eliezer addressed this in his reply to this thread earlier.  It is 
<br>
irrational if the attachment is blind.  You must have some reason that 
<br>
you need to stay alive, otherwise provisions for it will most likely get 
<br>
in the way of making rational decisions.
<br>
<p><em>&gt; Whether we transform or simply cease to exist seems to me to be a 
</em><br>
<em>&gt; perfectly rational thing to be a bit concerned about.  Do you see it 
</em><br>
<em>&gt; otherwise?
</em><br>
<p>Sure, you should be concerned.  I think that the vast majority of 
<br>
humans, uploaded or not, have something positive to contribute, however 
<br>
small.  It'd be great to see life get even better post Singularity, with 
<br>
everyone doing new and interesting good things.
<br>
<p><em>&gt;&gt; I and others have broken this attachment to keep it from clouding our 
</em><br>
<em>&gt;&gt; thinking.
</em><br>
<em>&gt;
</em><br>
<em>&gt; So you believe that becoming inhuman and uncaring about the fate of 
</em><br>
<em>&gt; humanity allows you to think better?
</em><br>
<p>If only it were easy to become inhuman, but it's not.
<br>
<p>Uncaring is inaccurate.  I do care about humans and would like to see 
<br>
them upload.  I care about any other intelligent life that might be out 
<br>
there in the universe and helping it upload.  I just don't care about 
<br>
humans so much that I'd give up everything to save humanity (unless that 
<br>
was the most rational thing to do).
<br>
<p><em>&gt;&gt; It is the result of being genetically related to the rest of humanity, 
</em><br>
<em>&gt;&gt; where the death of all human genes is a big enough problem to cause a 
</em><br>
<em>&gt;&gt; person to give up a goal or die to save humanity.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I do not necesarily agree that we can just write it off as a genetic 
</em><br>
<em>&gt; relatedness issue at all.  Whether there is sentient life and whether 
</em><br>
<em>&gt; it continues, regardless of its form, is of intense interest to me.  
</em><br>
<em>&gt; That some forms are not genetically related is not of high relevance to 
</em><br>
<em>&gt; the form of my concern. So please don't assume that explains it away or 
</em><br>
<em>&gt; makes the issue go away.  It doesn't.
</em><br>
<p>There is an ethical issue, however the irrational attachment is the 
<br>
result of relatedness.  A proper ethic is not so strong that it prevents 
<br>
you from even thinking about something, the way evolved ethics do.
<br>
<p><em>&gt;&gt; Some of us, myself included, see the creation of SI as important 
</em><br>
<em>&gt;&gt; enough to be more important than humanity's continuation.  Human 
</em><br>
<em>&gt;&gt; beings, being
</em><br>
<em>&gt;
</em><br>
<em>&gt; How do you come to this conclusion?  What makes the SI worth more than 
</em><br>
<em>&gt; all of humanity?  That it can outperform them on some types of 
</em><br>
<em>&gt; computation?  Is computational complexity and speed the sole measure of 
</em><br>
<em>&gt; whether sentient beings have the right to continued existence?  Can you 
</em><br>
<em>&gt; really give a moral justification or a rational one for this?
</em><br>
<p>In many ways, humans are just over the threshold of intelligence.  
<br>
Compared to past humans we are pretty smart, but compared to the 
<br>
estimated potentials for intelligence we are intellectual ants.  Despite 
<br>
our differences, all of us are roughly of equivalent intelligence and 
<br>
therefore on equal footing when decided whose life is more important.  
<br>
But, it's not nearly so simple.  All of us would probably agree that 
<br>
given the choice between saving one of two lives, we would choose to 
<br>
save the person who is most important to the completion of our goals, be 
<br>
that reproduction, having fun, or creating the Singularity.  In the same 
<br>
light, if a mob is about to come in to destroy the SI just before it 
<br>
takes off and there is no way to stop them other than killing them, you 
<br>
have on one hand the life of the SI that is already more intelligent 
<br>
than the members of the mob and will continue to get more intelligent, 
<br>
and on the other the life of 100 or so humans.  Given such a choice, I 
<br>
pick the SI.
<br>
<p>In my view, more intelligent life has more right to the space it uses 
<br>
up.  Of course, we hope that intelligent life is compassionate and is 
<br>
willing to share.  Actually, I should be more precise.  I think that 
<br>
wiser life has more right to the space it uses (but you can't be wiser 
<br>
without first being more intelligent).  I would choose a world full of 
<br>
dumb humans trying hard to do some good over an Evil AI.
<br>
<p><em>&gt;&gt; self aware, do present more of an ethical delima than cows if it turns 
</em><br>
<em>&gt;&gt; out that you might be forced to sacrifice some of them.  I would like 
</em><br>
<em>&gt;&gt; to see all of humanity make it into a post Singularity existence and I 
</em><br>
<em>&gt;&gt; am willing to help make this a reality.
</em><br>
<em>&gt;
</em><br>
<em>&gt; How kind of you.  However, from the above it seems you see them as an 
</em><br>
<em>&gt; ethical dilemna greater than that of cows but if your SI, whatever it 
</em><br>
<em>&gt; turns out really to be, seems to require or decides the death of one or 
</em><br>
<em>&gt; all of them, then you would have to side with the SI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Do I read you correctly?  If I do, then why do you hold this position?  
</em><br>
<em>&gt; If I read you correctly then how can you expect the majority of human 
</em><br>
<em>&gt; beings, if they really understood you, to consider you as other than a 
</em><br>
<em>&gt; monster?
</em><br>
<p>If an SI said it needed to kill a bunch of humans, I would seriously 
<br>
start questioning its motives.  Killing intelligent life is not 
<br>
something to be taken lightly and done on a whim.  However, if we had a 
<br>
FAI that was really Friendly and it said &quot;Gordon, believe me, the only 
<br>
way is to kill this person&quot;, I would trust in the much wiser SI.
<br>
<p>This is the kind of reaction I expect and, while I'm a bit disappointed 
<br>
to get so much of it on SL4, therefore avoid pointing this view out.  I 
<br>
never go out of my way to say that human life is not the most important 
<br>
thing to me in the universe, but sometimes it is worth talking about.
<br>
<p><pre>
--
Gordon Worley                     `When I use a word,' Humpty Dumpty
<a href="http://www.rbisland.cx/">http://www.rbisland.cx/</a>            said, `it means just what I choose
<a href="mailto:redbird@rbisland.cx?Subject=Re:%20Threats%20to%20the%20Singularity.">redbird@rbisland.cx</a>                it to mean--neither more nor less.'
PGP:  0xBBD3B003                                  --Lewis Carroll
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4064.html">Eugen Leitl: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4062.html">F Baube: "Re: On wisdom"</a>
<li><strong>In reply to:</strong> <a href="4058.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4064.html">Eugen Leitl: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4064.html">Eugen Leitl: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4067.html">Eliezer S. Yudkowsky: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4075.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4063">[ date ]</a>
<a href="index.html#4063">[ thread ]</a>
<a href="subject.html#4063">[ subject ]</a>
<a href="author.html#4063">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
