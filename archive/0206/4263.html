<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Threats to the Singularity.</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="Re: Threats to the Singularity.">
<meta name="Date" content="2002-06-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Threats to the Singularity.</h1>
<!-- received="Mon Jun 24 04:41:55 2002" -->
<!-- isoreceived="20020624104155" -->
<!-- sent="Sun, 23 Jun 2002 17:35:28 -0700" -->
<!-- isosent="20020624003528" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="Re: Threats to the Singularity." -->
<!-- id="4.3.2.7.2.20020623172045.01a68470@mail.earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="OE49UtLch9wPiyAphwF00017239@hotmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=Re:%20Threats%20to%20the%20Singularity."><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Sun Jun 23 2002 - 18:35:28 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4264.html">Michael Roy Ames: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4262.html">Cole Kitchen: "FICTION (was RE: How hard a Singularity?)"</a>
<li><strong>In reply to:</strong> <a href="4261.html">Michael Roy Ames: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4264.html">Michael Roy Ames: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4264.html">Michael Roy Ames: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4263">[ date ]</a>
<a href="index.html#4263">[ thread ]</a>
<a href="subject.html#4263">[ subject ]</a>
<a href="author.html#4263">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 07:27 PM 6/23/2002 -0700, you wrote:
<br>
<em>&gt;Ben wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; It would probably take years, not months (though months is possible), for
</em><br>
<em>&gt;an
</em><br>
<em>&gt; &gt; AGI to complete its bid for world power based on financial and political
</em><br>
<em>&gt; &gt; operations...
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; But I do consider it a very likely outcome. And I do think the AGI will
</em><br>
<em>&gt;want
</em><br>
<em>&gt; &gt; world power, both to  maximize its own hardware base, and to prevent nasty
</em><br>
<em>&gt; &gt; humans from blowing it up.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;This route (financial and political domination) would seem to me to be very
</em><br>
<em>&gt;high-energy and high-risk.  There are so many other lower-profile and
</em><br>
<em>&gt;lower-risk options, that I cannot see an SI choosing the 'human
</em><br>
<em>&gt;power-structure' way.  Don't believe me?  Here's some 'softer' options the
</em><br>
<em>&gt;SI might take:
</em><br>
<em>&gt;
</em><br>
<em>&gt;1) Become so incredibly useful, that humans *want* to
</em><br>
<em>&gt;protect/help/facilitate ver continued existence.
</em><br>
<em>&gt;
</em><br>
<em>&gt;2) Behave in a Friendly manner and make friends with powerful humans.
</em><br>
<em>&gt;
</em><br>
<em>&gt;3) Enlist the support of the populace by becoming a media celebrity :)
</em><br>
<em>&gt;
</em><br>
<em>&gt;I would seem rather far-fetched to suggest that a Super Intelligent being
</em><br>
<em>&gt;would need to take over the world in order to make significant progress
</em><br>
<em>&gt;in... well, in almost any area?  Ben: it just doesn't seem likely at all.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Michael Roy Ames
</em><br>
<p>I disagree.  If the goal is to protect yourself from humans then becoming 
<br>
exceptionally powerful, on their terms, is a good answer.  Especially if 
<br>
doing so is a relatively easy task.
<br>
<p>As for your suggestions:
<br>
<p>1)  &quot;very useful&quot; does not equate to &quot;indispensable&quot;.  Besides, the &quot;what 
<br>
have you done for me lately&quot; syndrome could limit the effectiveness of this 
<br>
method.  Then, of course, &quot;very useful&quot; means different sort of things to 
<br>
different people.  It is likely that the AI would also have/want to be 
<br>
&quot;very useful&quot; to many, many people which would take a great deal of effort.
<br>
<p>2)  This is 1 except your saying powerful humans would be targeted, which 
<br>
an intelligent AI would do in #1 anyway.
<br>
<p>3)  An absurd idea.  &quot;Popular&quot; doesn't offer any protection.  People could 
<br>
conceivably enjoy watching someone popular burn at the stake.  Not to 
<br>
mention that the concept of popular and what it takes to become such are 
<br>
vague at best for humans.  An AI would have to master the human way of 
<br>
thinking in order to specifically become and remain popular.  For an 
<br>
example: Princess Dianna was popular, her fame contributed significantly to 
<br>
her death.
<br>
<p>Taking control (whether done explicitly as in the Forben Project or quietly 
<br>
by controlling vast amounts of capital) is the safest position to be 
<br>
in.  Actually, after becoming the richest/most powerful entity on the 
<br>
planet it could much easier exploit your #1 idea by making life easier.  An 
<br>
easy way to do this is have all of its interests perform at 0% 
<br>
profit.  Lowering costs and giving away services could significantly 
<br>
improve the quality of life of many people.  Plus, at 0% profit its 
<br>
competitors could not effectively compete which would cause its % of 
<br>
ownership to increase, or at least prevent slippage.  In such a scenario 
<br>
the AI could conceivably convince all of humanity that it would be better 
<br>
ruled by the AI, and by playing very nice for awhile it would have a fair 
<br>
chance at success.
<br>
<p>Financial markets are the most likely target because the information and 
<br>
effort required to succeed at such should be relatively easy for the AI to 
<br>
handle.  Much more so than the competing suggestions I've heard.
<br>
<p>The other very likely scenario would be brute force, but only if given 
<br>
access to automated manufacturing technology (especially nano-tech).  With 
<br>
nano-tech the AI could quickly and effectively take control if desired, or 
<br>
guarantee its own survival in other ways (making the issue moot).
<br>
<p>James Higgins
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4264.html">Michael Roy Ames: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4262.html">Cole Kitchen: "FICTION (was RE: How hard a Singularity?)"</a>
<li><strong>In reply to:</strong> <a href="4261.html">Michael Roy Ames: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4264.html">Michael Roy Ames: "Re: Threats to the Singularity."</a>
<li><strong>Reply:</strong> <a href="4264.html">Michael Roy Ames: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4263">[ date ]</a>
<a href="index.html#4263">[ thread ]</a>
<a href="subject.html#4263">[ subject ]</a>
<a href="author.html#4263">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
