<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: How hard a Singularity?</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: How hard a Singularity?">
<meta name="Date" content="2002-06-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: How hard a Singularity?</h1>
<!-- received="Sun Jun 23 05:29:03 2002" -->
<!-- isoreceived="20020623112903" -->
<!-- sent="Sat, 22 Jun 2002 21:23:11 -0700" -->
<!-- isosent="20020623042311" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: How hard a Singularity?" -->
<!-- id="3D154D2F.6070705@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D1450D3.7070505@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20How%20hard%20a%20Singularity?"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Sat Jun 22 2002 - 22:23:11 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4183.html">Mike & Donna Deering: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4181.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4129.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4141.html">Smigrodzki, Rafal: "RE: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4182">[ date ]</a>
<a href="index.html#4182">[ thread ]</a>
<a href="subject.html#4182">[ subject ]</a>
<a href="author.html#4182">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer S. Yudkowsky wrote:
<br>
<em>&gt; Smigrodzki, Rafal wrote:
</em><br>
<em>&gt;  &gt; Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; Vernor Vinge's original stories of
</em><br>
<em>&gt;  &gt;&gt; the Singularity were based around the blazing disruptive power of
</em><br>
<em>&gt;  &gt;&gt; smarter-than-human intelligence
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; ### You build your business plan on the idea of hard-takeoff 
</em><br>
<em>&gt; Singularity.
</em><br>
<em>&gt;  &gt;  I agree this is a prudent decision - the &quot;slow&quot; variant much less
</em><br>
<em>&gt;  &gt; dangerous, and would involve a huge number of contributors, with
</em><br>
<em>&gt;  &gt; potentially less opportunity for a small group to make a difference. It
</em><br>
<em>&gt;  &gt; is reasonable to concentrate on the most scary scenario, even if it 
</em><br>
<em>&gt; isn't
</em><br>
<em>&gt;  &gt;  the most likely one.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I do not consider a soft Singularity to be any less scary than a hard
</em><br>
<em>&gt; Singularity.  I think this is wishful thinking.  A Singularity is a
</em><br>
<em>&gt; Singularity; the Singularity doesn't come in a &quot;soft&quot; version that lets you
</em><br>
<em>&gt; go on a few dates before deciding on a commitment.  That option might be
</em><br>
<em>&gt; open to individual humans (or not), but it is not a real option for 
</em><br>
<em>&gt; humanity.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;  &gt; While I also believe that the fast Singularity is a distinct 
</em><br>
<em>&gt; possibility,
</em><br>
<em>&gt;  &gt;  I don't have an intuition which would help me decide which variant is
</em><br>
<em>&gt;  &gt; more likely and by what odds.
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; What is your intuition? Is it a toss-up, a good hunch, or (almost)dead
</em><br>
<em>&gt;  &gt; certainty?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I would call it dead certain in favor of a hard takeoff, unless all the
</em><br>
<em>&gt; intelligences at the core of that hard takeoff unanimously decide 
</em><br>
<em>&gt; otherwise.
</em><br>
<em>&gt;  All economic, computational, and, as far as I can tell, moral indicators
</em><br>
<em>&gt; point straight toward a hard takeoff.  The Singularity involves an inherent
</em><br>
<p>I am curious what the moral indicators are you speak of.  It is 
<br>
not at all certain that fewer deaths will occur during a 
<br>
Singularity or soon thereafter is that is a primary moral driver.
<br>
<p><p><em>&gt; positive feedback loop; smart minds produce smarter minds which produce
</em><br>
<em>&gt; still smarter minds and so on.  Furthermore, thought itself is likely to
</em><br>
<p>Once we get AI designing other AI or transhumans improving their 
<br>
intelligence and systems this seems true enough within whatever 
<br>
limits might adhere.  Given this a Singularity seems inevitable, 
<br>
so the question of what kinds of ethics the transhumanists, soon 
<br>
to be trans- and post-humans and their creations have, will have 
<br>
and follow.  At least this is a crucial question of one's 
<br>
morality include preventing senseless death and suffering of 
<br>
sentients.
<br>
<p><em>&gt; fall through to much faster substrate than our 200Hz neurons.  The closest
</em><br>
<em>&gt; we might come to a slow Singularity is if the first transhumans are pure
</em><br>
<em>&gt; biological humans, in which case it might take a few years for them to 
</em><br>
<em>&gt; build
</em><br>
<em>&gt; AI, brain-computer interfaces, or computer-mediated broadband telepathy 
</em><br>
<em>&gt; with
</em><br>
<em>&gt; 64-node clustered humans, but my guess is that the first transhumans would
</em><br>
<em>&gt; head for more powerful Singularity technologies straight out of the gate.
</em><br>
<em>&gt; Beyond that point it would be a hard takeoff.  I see no moral reason for
</em><br>
<em>&gt; slowing this down while people are dying.
</em><br>
<p>The moral reason is if the hard[er] takeoff results in vastly 
<br>
more people dying, even the entire race ending forever.  At this 
<br>
point it is a real krap-shoot whether the Singularity 
<br>
intelligences will themselves be stable and not fall immediately 
<br>
into a super-war or some other malady.  At this point, my 
<br>
primary allegiance has to be with the existing sentients.
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4183.html">Mike & Donna Deering: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4181.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4129.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4141.html">Smigrodzki, Rafal: "RE: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4182">[ date ]</a>
<a href="index.html#4182">[ thread ]</a>
<a href="subject.html#4182">[ subject ]</a>
<a href="author.html#4182">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
