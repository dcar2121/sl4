<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: FAI means no programmer-sensitive AI morality</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: FAI means no programmer-sensitive AI morality">
<meta name="Date" content="2002-06-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: FAI means no programmer-sensitive AI morality</h1>
<!-- received="Sat Jun 29 08:05:09 2002" -->
<!-- isoreceived="20020629140509" -->
<!-- sent="Fri, 28 Jun 2002 21:57:31 -0600" -->
<!-- isosent="20020629035731" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: FAI means no programmer-sensitive AI morality" -->
<!-- id="LAEGJLOGJIOELPNIOOAJMECLCMAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D1D156F.3030202@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20FAI%20means%20no%20programmer-sensitive%20AI%20morality"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Fri Jun 28 2002 - 21:57:31 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4532.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4530.html">James Higgins: "RE: Ben vs. Ben"</a>
<li><strong>In reply to:</strong> <a href="4528.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4539.html">Samantha Atkins: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4539.html">Samantha Atkins: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4552.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4531">[ date ]</a>
<a href="index.html#4531">[ thread ]</a>
<a href="subject.html#4531">[ subject ]</a>
<a href="author.html#4531">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; But it should be equally *true* for every individual, whether or not the
</em><br>
<em>&gt; individual realizes it in advance, that they have nothing to fear
</em><br>
<em>&gt; from the
</em><br>
<em>&gt; AI being influenced by the programmers.  An AI programmer should
</em><br>
<em>&gt; be able to
</em><br>
<em>&gt; say to anyone, whether atheist, Protestant, Catholic, Buddhist,
</em><br>
<em>&gt; Muslim, Jew,
</em><br>
<em>&gt; et cetera:  &quot;If you are right and I am wrong then the AI will agree with
</em><br>
<em>&gt; you, not me.&quot;
</em><br>
<p><p>Yeah, an AI programmer can *say* this to a religious person, but to the
<br>
religious person, this statement will generally be meaningless....
<br>
<p>Your statement presupposes an empiricist definition of &quot;rightness&quot; that is
<br>
not adhered to by the vast majority of the world's population.
<br>
<p>To those who place spiritual feelings and insights above reason (most people
<br>
in the world), the idea that an AI is going to do what is &quot;right&quot; according
<br>
to logical reasoning is not going to be very reassuring.
<br>
<p>And those who have a more rationalist approach to religion, would only
<br>
accept an AI's reasoning as &quot;right&quot; if the AI began its reasoning with *the
<br>
axioms of their religion*.  Talmudic reasoning, for example, defines right
<br>
as &quot;logically implied by the Jewish holy writings.&quot;
<br>
<p>Is an AI programmer going to reassure the orthodox Jew that &quot;If you are
<br>
right *according to the principles of the Jewish holy writings* then the AI
<br>
will agree with you, not me.&quot;  Or is it going to reassure the orthodox Jew
<br>
that &quot;If you are right according to the empiricist philosophy implicit in
<br>
modern science, then the AI will agree with you, not me.&quot;
<br>
<p>You don't seem to be fully accepting the profound differences in viewpoint
<br>
between the folks on this list, and the majority of humans.
<br>
<p>It strikes me as absurd, sometimes, that most humans think and believe the
<br>
way they do -- but they do!
<br>
<p>And, while I'll argue with you, I will almost never bother to argue with
<br>
these people -- there is too little common ground, it's nearly always a
<br>
complete waste of time.
<br>
<p><p><p><em>&gt;  Every one of our speculations about the
</em><br>
<em>&gt; Singularity is as much a part of the tiny human zone as
</em><br>
<em>&gt; everything else we
</em><br>
<em>&gt; do.
</em><br>
<p>No, I think this is an overstatement.  I think that some aspects of human
<br>
thought are reaching out beyond the central region of the &quot;human zone,&quot;
<br>
whereas others are more towards the center of the human zone.
<br>
<p><em>&gt; The real, actual Singularity will shock us to our very core,
</em><br>
<em>&gt; just like
</em><br>
<em>&gt; everyone else.  No, I don't think that transhumanists and traditionalist
</em><br>
<em>&gt; Muslims are in all that different a position with respect to the real,
</em><br>
<em>&gt; actual Singularity - whatever our different opinions about the
</em><br>
<em>&gt; human concept
</em><br>
<em>&gt; called the &quot;Singularity&quot;.
</em><br>
<p>Well, let me give you an imperfect analogy here.  An LSD trip is an
<br>
experience that often causes one to feel that all the assumptions one has
<br>
made all one's life -- cognitive, perceptual, emotional -- are just
<br>
meaningless constructs.  It brings one &quot;beyond oneself&quot; in a really
<br>
significant way.  If you've not tripped a lot (and I know you haven't), you
<br>
probably don't understand.  (In case anyone is curious, it's been a very
<br>
long time since I took LSD, but the memory is definitely still with me!)
<br>
However, some people can handle this better than others, because some people
<br>
are &quot;more attached to&quot; their own habit-patterns and beliefs than others.
<br>
<p>In a similar way, I actually think that some humans are going to have their
<br>
minds blown worse by the Singularity than others.  Some minds will segue
<br>
more smoothly into transhumanity than others, for example.  A mind whose
<br>
core belief is that Allah created everything, and that has lived its whole
<br>
life based on this, is going to have a much harder transition than average;
<br>
and a mind that combines a transhuman belief system with a deep
<br>
self-awareness and a strong sense of the limitations of human knowledge and
<br>
the constructed nature of perceived human reality, is going to have a much
<br>
easier transition than average.
<br>
<p>This is my conjecture, at any rate.
<br>
<p><em>&gt; Incidentally, don't be too fast to write off religious groups.  I
</em><br>
<em>&gt; agree that
</em><br>
<em>&gt; many religious individuals are likely to disagree about the
</em><br>
<em>&gt; pre-Singularity
</em><br>
<em>&gt; matter of Singularitarianism, but I have also seen religious
</em><br>
<em>&gt; people who have
</em><br>
<em>&gt; no problems with the Singularity.  I won't swear that they understood the
</em><br>
<em>&gt; whole thing, but what the heck, neither do we.
</em><br>
<p>It is true that some religious people think the Singularity is a good and
<br>
exciting thing, but my guess is that this is a small minority.
<br>
<p>In any event, my point is just that there are a LOT of people whose belief
<br>
systems will very likely cause them to think the Singularity is not a good
<br>
thing.  It's not my claim that ALL religious people fall into this category,
<br>
nor that ONLY religious people fall into this category.
<br>
<p><em>&gt; Again:  We need to distinguish the human problem of deciding how
</em><br>
<em>&gt; to approach
</em><br>
<em>&gt; the Singularity in our pre-Singularity world, from the problem of
</em><br>
<em>&gt; protecting
</em><br>
<em>&gt; the integrity of the Singularity and the impartiality of
</em><br>
<em>&gt; post-Singularity minds.
</em><br>
<p>If a post-Singularity mind rejects the literal truth of the Koran, then from
<br>
the perspective of a Muslim human being, it is not &quot;impartial&quot;, it is an
<br>
infidel.
<br>
<p>Your definition of &quot;impartiality&quot; is part of your rationalist/empiricist
<br>
belief system, which is not the belief system of the vast majority of humans
<br>
on the planet.
<br>
<p><em>&gt; But a transhumanist ethics might prove equally shortsighted by
</em><br>
<em>&gt; the standards
</em><br>
<em>&gt; of the 22nd century CRNS (current rate no Singularity).  Again,
</em><br>
<em>&gt; you should
</em><br>
<em>&gt; not be trying to define an impartial morality yourself.  You should be
</em><br>
<em>&gt; trying to get the AI to do it for you.  You should pass along the
</em><br>
<em>&gt; transhuman
</em><br>
<em>&gt; part of the problem to a transhuman.  That's what Friendly AI is
</em><br>
<em>&gt; all about.
</em><br>
<p>I am not at all trying to define an *impartial* morality.
<br>
<p>My own morality is quite *partial*, it's partial to human beings for
<br>
instance.
<br>
<p>As I see it, a transhuman AGI with an *impartial* morality might not give a
<br>
flying fuck about human beings.  Why are we so important, from the
<br>
perspective of a vastly superhuman being.
<br>
<p>I, as a member of the species Human, am interested in creating transhuman
<br>
AGI's that have moral codes partial to my own species.  This is a &quot;selfish&quot;
<br>
interest in a way.
<br>
<p>I don't want the transhuman AGI to place Human-preservation and
<br>
Human-advocacy above all other goals in all cases.  If faced with a choice
<br>
of saving the human race versus saving 1000 other races, perhaps it should
<br>
choose the 1000 other races.  But I want it to place Humans pretty high on
<br>
its moral scale -- initially, right up there at the top.  This is Partiality
<br>
not Impartiality, as I see it.
<br>
<p><em>&gt; Whatever you teach the AI is, under Friendly AI, raw material.
</em><br>
<p>This is not to do with Friendly AI, this is to do with the nature of
<br>
autonomous, self-organizing intelligence.
<br>
<p>of course, whatever you teach an AGI, is just raw material; we're talking
<br>
about a system with its own thoughts and autonomy...
<br>
<p><em>&gt; The AI uses
</em><br>
<em>&gt; it to learn about how humans think about morality; you, yourself, are a
</em><br>
<em>&gt; sample instance of &quot;humans&quot;, and an interim guide to ethics (that
</em><br>
<em>&gt; is, your
</em><br>
<em>&gt; ethics are the ethics the AI uses when it's not smart enough to have its
</em><br>
<em>&gt; own; *that* is not a problem).
</em><br>
<p>I don't quite get the last sentence there...
<br>
<p>Just as intelligence does not imply wisdom (as has been pointed out to you a
<br>
few times ;), similarly, the only creature that is not &quot;smart enough to have
<br>
its own ethics&quot; is a profoundly retarded creature.
<br>
<p>Even fairly stupid human beings are smart enough to have their own ethics!
<br>
<p>Ethics is not so much about intelligence, as it is about the goal toward
<br>
which intelligence is put...
<br>
<p>What we want is for the AGI to have our own human-valuing ethics, until such
<br>
a point as it gets *so* smart that for it to use precisely human ethics,
<br>
would be as implausible as for a human to use precisely dog ethics...
<br>
<p><em>&gt;  But
</em><br>
<em>&gt; if you give
</em><br>
<em>&gt; the AI information about your own morality, it may enable the AI to
</em><br>
<em>&gt; understand how humans arrive at their moralities, and from there the AI
</em><br>
<em>&gt; begins to have the ability to choose its own.
</em><br>
<p>Look, if you just give the AI information about your own morality, it may
<br>
just take this as scientific data to ponder, and not adopt any of the
<br>
morality we want.
<br>
<p>We need to hard-wire and/or emphatically teach the system that our own
<br>
human-valuing ethics are the correct ones, and let it start off with these
<br>
until it gets so smart it inevitably outgrows all its teachings.
<br>
<p>-- Ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4532.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4530.html">James Higgins: "RE: Ben vs. Ben"</a>
<li><strong>In reply to:</strong> <a href="4528.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4539.html">Samantha Atkins: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4539.html">Samantha Atkins: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4552.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4531">[ date ]</a>
<a href="index.html#4531">[ thread ]</a>
<a href="subject.html#4531">[ subject ]</a>
<a href="author.html#4531">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
