<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Military Friendly AI</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="RE: Military Friendly AI">
<meta name="Date" content="2002-06-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Military Friendly AI</h1>
<!-- received="Sat Jun 29 08:05:07 2002" -->
<!-- isoreceived="20020629140507" -->
<!-- sent="Fri, 28 Jun 2002 20:13:24 -0700" -->
<!-- isosent="20020629031324" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="RE: Military Friendly AI" -->
<!-- id="4.3.2.7.2.20020628195220.01ce0828@mail.earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJMECFCMAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=RE:%20Military%20Friendly%20AI"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Fri Jun 28 2002 - 21:13:24 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4530.html">James Higgins: "RE: Ben vs. Ben"</a>
<li><strong>Previous message:</strong> <a href="4528.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>In reply to:</strong> <a href="4525.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4532.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4532.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4544.html">Eliezer S. Yudkowsky: "Controlled ascent (was: Military Friendly AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4529">[ date ]</a>
<a href="index.html#4529">[ thread ]</a>
<a href="subject.html#4529">[ subject ]</a>
<a href="author.html#4529">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 06:32 PM 6/28/2002 -0600, Ben Goertzel wrote:
<br>
<em>&gt;Novamente does not yet have a goal system at all, this will be implemented,
</em><br>
<em>&gt;at my best guess, perhaps at the very end of 2002 or the start of 2003.
</em><br>
<em>&gt;Currently we are just testing various cognitive and perceptual mechanisms,
</em><br>
<em>&gt;and not yet experimenting with autonomous goal-directed behavior.
</em><br>
<p>Boy, you really don't have much chance of a hard takeoff yet.
<br>
<p><em>&gt;A failsafe mechanism has two parts
</em><br>
<em>&gt;
</em><br>
<em>&gt;1) a basic mechanism for halting the system and alerting appropriate people
</em><br>
<em>&gt;when a &quot;rapid rate of intelligence increase&quot; is noted
</em><br>
<em>&gt;
</em><br>
<em>&gt;2) a mechanism for detecting a rapid rate of intelligence increase
</em><br>
<em>&gt;
</em><br>
<em>&gt;1 is easy; 2 is hard ... there are obvious things one can do, but since
</em><br>
<em>&gt;we've never dealt with this kind of event before, it's entirely possible
</em><br>
<em>&gt;that a &quot;deceptive intelligence increase&quot; could come upon us.  Measuring
</em><br>
<em>&gt;general intelligence is tricky.
</em><br>
<p>I agree with this.  But you could start with a fail safe with somewhat of a 
<br>
hair trigger.  Put in numerous types of heuristics that detect rapid or 
<br>
substantial change in any system or in the quality or frequency of the 
<br>
output.  In such a case have it pause the system and notify the 
<br>
staff.  Tune the heuristics over time to produce less unnecessary 
<br>
pauses.  But at any time it would be better to pause too often than not to 
<br>
pause at a critical junction.
<br>
<p>It would be far from perfect, obviously.  But it would be a good point to 
<br>
start from and better than having nothing.
<br>
<p>Based on your current state of progress I wouldn't say this is required 
<br>
ASAP, but you should be at least planning this.  And you should have it 
<br>
implemented and tested before the goal system goes live I'd suggest.
<br>
<p><em>&gt; &gt;  If anything even close to this looks
</em><br>
<em>&gt; &gt; likely you better be getting opinions of hundreds or thousands of
</em><br>
<em>&gt; &gt; relevant
</em><br>
<em>&gt; &gt; experts.  Or I'll come kick yer ass.  ;)  Seriously.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Seriously -- this would be a tough situation.
</em><br>
<p><em>&gt;What if one of these thousands of relevant experts decides the system is so
</em><br>
<em>&gt;dangerous that they have to destroy it -- and me.  What if they have allies
</em><br>
<em>&gt;with the means to do so?
</em><br>
<p>Yeah, well I wasn't serious about literally consulting thousands of 
<br>
experts.  But you should, in my opinion, consult very many if your belief 
<br>
is that Friendliness can't be sufficiently implemented.  And you should 
<br>
consult a fair number before kicking off any Singularity shot.
<br>
<p>If one person thought the system and you should be destroyed I'd most 
<br>
likely disregard it.  Unless, that is, they were able to start convincing 
<br>
others to switch their vote.  At which point you'd have to seriously 
<br>
reconsider it (except for the destroying YOU part - that is insane).  I 
<br>
don't believe you need complete consensus to proceed.
<br>
<p><em>&gt; &gt; What is the trade-off point between risk and time?
</em><br>
<em>&gt;
</em><br>
<em>&gt;My own judgment would be, in your scenario, to spend 3 more years
</em><br>
<em>&gt;engineering to lower the risk to 3%
</em><br>
<em>&gt;
</em><br>
<em>&gt;However, I would probably judge NOT to spend 3 more years engineering to
</em><br>
<em>&gt;lower the risk to 3.9% from 4%
</em><br>
<p>Well, considering how inaccurate it is to discuss this risk as percentages 
<br>
I imagine 0.1% is actually below the margin of error.  So, while I still 
<br>
don't like even a 0.1% chance of failure (at all), I can see your point.
<br>
<p><em>&gt;These are really just intuitive judgments though -- to make them rigorous
</em><br>
<em>&gt;would require estimating too many hard to estimate factors.
</em><br>
<p>Yes.
<br>
<p><em>&gt;I don't think we're ever going to be able to estimate such things with that
</em><br>
<em>&gt;degree of precision.  I think the decisions will be more like a 1% risk
</em><br>
<em>&gt;versus a 5% risk versus a 15% risk, say.  And this sort of decision will be
</em><br>
<em>&gt;easier to make...
</em><br>
<p>Very true.
<br>
<p><em>&gt; &gt; What if another team was further ahead on this other design than yours?
</em><br>
<em>&gt;
</em><br>
<em>&gt;It depends on the situation.  Of course, egoistic considerations of priority
</em><br>
<em>&gt;are not a concern.  But there's no point in delaying the Novamente-induced
</em><br>
<em>&gt;Singularity by 3 years to reduce risk from 4% to 3%, if in the interim some
</em><br>
<em>&gt;other AI team is going to induce a Singularity with a 33.456% risk...
</em><br>
<p>Excellent answer.  The best course of action would be to stop the team with 
<br>
the 33% risk of failure (at any cost I'd say given that number).  But if 
<br>
they could not be stopped I'd endorse starting a less risky Singularity as 
<br>
an alternative.
<br>
<p><em>&gt;In fact neither Eliezer nor I wishes to *force* immortality on anyone, via
</em><br>
<em>&gt;uploading or medication or anything else.
</em><br>
<p>Yeah, I know.  That was just a convenient example of differing morality.
<br>
<p><em>&gt;Interestingly, in many conversations over the years I have found that more
</em><br>
<em>&gt;women want to die after their natural lifespan has ended, whereas more men
</em><br>
<em>&gt;are psyched about eternal life.  I'm not sure if this anecdotal would hold
</em><br>
<em>&gt;up statistically, but if so, it's interesting.  Adding some meat to the idea
</em><br>
<em>&gt;that women are more connected to Nature, I guess... ;)
</em><br>
<p>I've had similar indications to yours, it seems.  Though, thankfully, my 
<br>
wife is open to the idea of immortality.  Actually, let me hold off on that 
<br>
&quot;thankfully&quot; for a thousand years or so.  No one knows what a 1,000+ year 
<br>
marriage would be like yet.  ;)
<br>
<p>James Higgins
<br>
<p>P.S.  Can anyone explain why on earth I can spell heuristics correctly but 
<br>
not manner?
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4530.html">James Higgins: "RE: Ben vs. Ben"</a>
<li><strong>Previous message:</strong> <a href="4528.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>In reply to:</strong> <a href="4525.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4532.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4532.html">Ben Goertzel: "RE: Military Friendly AI"</a>
<li><strong>Reply:</strong> <a href="4544.html">Eliezer S. Yudkowsky: "Controlled ascent (was: Military Friendly AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4529">[ date ]</a>
<a href="index.html#4529">[ thread ]</a>
<a href="subject.html#4529">[ subject ]</a>
<a href="author.html#4529">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
