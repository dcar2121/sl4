<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Seed AI (was: How hard a Singularity?)</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Seed AI (was: How hard a Singularity?)">
<meta name="Date" content="2002-06-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Seed AI (was: How hard a Singularity?)</h1>
<!-- received="Sun Jun 23 06:50:15 2002" -->
<!-- isoreceived="20020623125015" -->
<!-- sent="Sun, 23 Jun 2002 06:37:03 -0400" -->
<!-- isosent="20020623103703" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Seed AI (was: How hard a Singularity?)" -->
<!-- id="3D15A4CF.9030101@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJIEFGCLAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Seed%20AI%20(was:%20How%20hard%20a%20Singularity?)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jun 23 2002 - 04:37:03 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4195.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4193.html">Eugen Leitl: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4179.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4209.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4209.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4221.html">James Higgins: "Re: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Maybe reply:</strong> <a href="4305.html">James Higgins: "Re: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Maybe reply:</strong> <a href="4318.html">James Higgins: "Re: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Maybe reply:</strong> <a href="4371.html">Smigrodzki, Rafal: "RE: Seed AI (was: How hard a Singularity?)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4194">[ date ]</a>
<a href="index.html#4194">[ thread ]</a>
<a href="subject.html#4194">[ subject ]</a>
<a href="author.html#4194">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer, I don't think that &quot;AI can be built by borrowing human content&quot;
</em><br>
<em>&gt; either, and you should know that.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The Cyc project comes close to the perspective you cite, but that is not my
</em><br>
<em>&gt; project.
</em><br>
<p>Perhaps.  Nonetheless there is more Cycish stuff in Novamente than I am 
<br>
comfortable with.  Novamente does contain structures along the lines of 
<br>
eat(cat, mice).  I realize you insist that these are not the only structures 
<br>
and that the eventual real version of Novamente will replace these small 
<br>
structures with big emergent structures (that nonetheless follow roughly the 
<br>
same rules as the small structures and can be translated into them for 
<br>
faster processing).  I guess what I'm trying to say is we have different 
<br>
ideas about how much of mind is implemented by content, the sort of stuff we 
<br>
humans would regard as *transferable* knowledge - the kind of knowledge that 
<br>
we communicate through books.  I think you ascribe more mind to transferable 
<br>
content than I.  I am not saying that you ascribe all mind to transferable 
<br>
content, but definitely more than I do (and less than Cyc).
<br>
<p><em>&gt; I think that the first AGI will be created by
</em><br>
<em>&gt; 
</em><br>
<em>&gt; a) engineering a system embodying cognitive, perception and action
</em><br>
<em>&gt; mechanisms that are only loosely based on human cognitive science
</em><br>
<p>We agree on this, of course, although we have different ideas of what 
<br>
constitutes &quot;loosely based&quot;.
<br>
<p><em>&gt; b) having this system grow from a baby into a useful intelligent mind by a
</em><br>
<em>&gt; combination of: autonomous exploration of digital and physical environments,
</em><br>
<em>&gt; self-organization, goal-oriented self-modification, and explicit teaching by
</em><br>
<em>&gt; humans
</em><br>
<p>I think that the system will grow from a baby into a useful intelligent mind 
<br>
through the improvement of (I) underlying brainware systems for memory 
<br>
association, forming concepts in modalities, associating to concepts, 
<br>
imposing and satisfying concepts, searching for beliefs that satisfy 
<br>
sequiturs, combinatorial search within design problems and logic problems, 
<br>
(II) accumulation of reflective realtime skills, accumulation of reflective 
<br>
beliefs and concepts, accumulation of beliefs, concepts, and skills relative 
<br>
to virtual microenvironments, optimization of skill/belief/concept 
<br>
representations and processes, (III) learning to communicate with humans and 
<br>
make better use of human assistance, and learning to abstractly model the 
<br>
blackbox &quot;real world&quot; through the interpretation of human-written knowledge.
<br>
<p>A tremendous part of an AI is brainware.  The most important content - maybe 
<br>
not the most content, but the most important content - is content that 
<br>
describes things that only AIs and AI programmers have names for, and much 
<br>
of it will be realtime skills that humans can help learn but which have no 
<br>
analogue in humans.
<br>
<p>You think my design is too complex.  Okay.  Nonetheless, the more complex a 
<br>
design is, the more mind arises from the stuff that implements that design, 
<br>
and the more opportunities there are to improve mind by improving the 
<br>
implementation (never mind actually *improving the design*!)  I think that 
<br>
the more specific a model one has of mind, the more ways you'll be able to 
<br>
think of improving it.  And that, from my perspective, is why I see things 
<br>
moving faster than your intuitions call for.  There's more to improve, and 
<br>
far more of intelligence is dependent on underlying brainware and on content 
<br>
that is specific to AI minds.  Not all of intelligence, but more of it than 
<br>
in the Novamente design.  And I don't put much reliance at all on abstract 
<br>
models of the blackbox processes of our &quot;real world&quot;, which is, at best, 
<br>
what the corpus of most human knowledge communicates to an AI.
<br>
<p>Everything is about seed AI.  The most critical knowledge an AI needs, 
<br>
especially in the process of growing up, is the knowledge that makes the AI 
<br>
smarter.  This knowledge may be gained in the course of solving 
<br>
microenvironmental problems posed by humans, and may indeed not be gainable 
<br>
in any other way, but it is not actually knowledge of how to navigate Rube 
<br>
Goldberg design problems with a billiard-ball toolset; it is knowledge about 
<br>
how to think, and an AI will think differently from humans.  Humans in 
<br>
general (as opposed to successful AI researchers) have very little knowledge 
<br>
of this kind, and what there is will be mostly untransferable because of the 
<br>
difference in cognitive systems.
<br>
<p>The real, underlying problems that an AI needs to solve in order to grow up, 
<br>
learning how to think - these problems may be solved with the help of 
<br>
humans, but they will not be drawing on existing explicit human knowledge, 
<br>
and there will not be a one-to-one correspondence between the AI's 
<br>
competence in problem domains and the AI's competence at the problem domain 
<br>
of thinking.  Even if the AI got up to &quot;human-level&quot; competence and no 
<br>
farther at the problem of &quot;thinking using an AI mind&quot;, there is no reason 
<br>
why the *actual thinking* that resulted would be at a human level!  My guess 
<br>
is that it could be transhuman, though infrahumanity is a possibility.
<br>
<p>Transferance of human content is not the key issue in seed AI.  The parts of 
<br>
the AI that could conceivably get &quot;stuck at the human level&quot; are the 
<br>
programming quality of underlying brainware, and reflective skills that are 
<br>
acquired with human assistance.  This is not the same level of organization 
<br>
as actual thinking!  In humans these processes are &quot;stuck at the level&quot; of 
<br>
evolution's competency and internal brainware competency respectively, and 
<br>
yet we ourselves are stuck at the human level, which is completely 
<br>
different.  If you build an AI with human-competence brainware and 
<br>
human-competence reflective skills, you have not built a human-level AI!  It 
<br>
could be anywhere, but it won't be human-level; there's no reason why it 
<br>
would be.
<br>
<p><em>&gt; In terms of part b), having human knowledge to read, and humans to
</em><br>
<em>&gt; communicate with, will be a big help to the system in boosting itself up to
</em><br>
<em>&gt; human-level intelligence, and MUCH LESS of a help to the system in boosting
</em><br>
<em>&gt; itself up further to superhuman intelligence.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I guess what you're saying is that you think learning from human knowledge,
</em><br>
<em>&gt; and explicit education by humans, will not be an important part of getting
</em><br>
<em>&gt; an AGI up to human level.  I disagree with you on this, but I can't prove
</em><br>
<em>&gt; I'm right, of course.
</em><br>
<p>I think explicit education by humans will be an important part of 
<br>
bootstrapping an AI to the level of being able to solve its own problems. 
<br>
By the time human knowledge is even comprehensible to the AI, most of the 
<br>
hard problems will have already been solved and the AI will probably be in 
<br>
the middle of a hard takeoff.
<br>
<p><em>&gt; If an AI is not taught by us, and doesn't fill its mind with books we've
</em><br>
<em>&gt; written and theorems we've proved, how is it going to get intelligent?
</em><br>
<em>&gt; Purely by interacting with the environment on its own?  This sounds a lot
</em><br>
<em>&gt; less efficient to me... and also a lot less likely to result in a
</em><br>
<em>&gt; human-sympathetic AI...
</em><br>
<p>It will learn intelligence by solving human-posed problems whose actual 
<br>
solutions are not as important as the AI learning to think in the course of 
<br>
solving the problem; and, of roughly equal importance, will improve in 
<br>
underlying hardware intelligence as the humans, and later the AI, create 
<br>
more and more powerful brainware.
<br>
<p><em>&gt; I do NOT envision &quot;pouring human content into an AI&quot;, in terms of directly
</em><br>
<em>&gt; force-feeding its mind with human knowledge databases a la Cyc.  What I
</em><br>
<em>&gt; envision is that an AI will turn itself from a baby into a mature mind by
</em><br>
<em>&gt; learning from human teachers and by studying human knowledge, including
</em><br>
<em>&gt; books and mathematics and software, and perhaps also explicit knowledge DB's
</em><br>
<em>&gt; like Cyc.
</em><br>
<p>You are correct in that I do not see this as being very useful.  A little 
<br>
useful, maybe; not a lot useful.
<br>
<p><em> &gt; And I believe that this knowledge will accelerate its development
</em><br>
<em>&gt; to the roughly human-level, much beyond the pace that would be possible if
</em><br>
<em>&gt; all this knowledge and all this teaching were not available.
</em><br>
<p><p><em>&gt; And I agree with that -- the question is *how fast* will the AI be able to
</em><br>
<em>&gt; improve itself.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It's a quantitative question.  Your intuitive estimate is much faster than
</em><br>
<em>&gt; mine...
</em><br>
<p>Ben, you're the one who insists that everything is &quot;intuition&quot;.  I am happy 
<br>
to describe your estimates as &quot;intuitions&quot; if you wish, but I think that 
<br>
more detailed thoughts are both possible and desirable.
<br>
<p><em>&gt; I doubt that very  much.  I think that a key part of getting a baby AI to
</em><br>
<em>&gt; useful human-adult-level intelligence will be imbibing human patterns of
</em><br>
<em>&gt; thought and interaction -- learning from people via reading text and
</em><br>
<em>&gt; databases, and via interaction.  Hence I think the first AGI, even if built
</em><br>
<em>&gt; on radically nonhuman data structures and dynamics, will have a lot of
</em><br>
<em>&gt; humanity in its emergent mind-patterns at first...
</em><br>
<p>You seem to think that you create a general intelligence with all basic 
<br>
dynamics in place, thereby creating a baby, which then educates itself up to 
<br>
to human-adult-level intelligence, which can be done by studying signals of 
<br>
the kind which human adults use to communicate with each other.  I don't see 
<br>
this as likely.  The process of going from baby to adult is likely to be 
<br>
around half brainware improvement and half the accumulation of knowledge 
<br>
that cannot be downloaded off the Internet.  The most the corpus of human 
<br>
knowledge can do is provide various little blackbox puzzles to be solved, 
<br>
and most of those puzzles won't be the kind the AI needs to grow.
<br>
<p><em>&gt; I think you place way too much faith in &quot;bootstrapping&quot; style
</em><br>
<em>&gt; self-organization.  Creating a smart system that can modify its own code,
</em><br>
<em>&gt; and giving it good perceptors and actuators, will lead to a mature, usefully
</em><br>
<em>&gt; self-improving and world-understanding AGI *eventually*, but how long will
</em><br>
<em>&gt; it take?  I think the process will go faster by far if teaching by humans is
</em><br>
<em>&gt; a big part of the process.  I also think that a human-friendly AGI is more
</em><br>
<em>&gt; likely to result if the system achieves its intelligence partly thru being
</em><br>
<em>&gt; taught by humans.
</em><br>
<p>Okay, now *you're* misinterpreting *me*.  I don't think that AGI can be 
<br>
bootstrapped to through seed AI, nor that human interaction is unimportant. 
<br>
&nbsp;&nbsp;Humans are a seed AI's foundations of order.  Humans will teach the AI but 
<br>
what they will teach is not the corpus of human declarative knowledge.  What 
<br>
they teach will be domain problems that are at the right level the AI needs 
<br>
to grow, and what the AI will learn will be how to think.
<br>
<p><em>&gt; You may say that with good enough learning methods, no teaching is
</em><br>
<em>&gt; necessary.
</em><br>
<p>Incorrect.  What I am saying is that what is taught will not be the corpus 
<br>
of human declarative knowledge, nor would trying to teach that corpus prove 
<br>
very useful.
<br>
<p><em> &gt; Maybe so.  I know you think Novamente's learning methods are too
</em><br>
<em>&gt; weak, though you have not explained why to me in detail, nor have you
</em><br>
<em>&gt; proposed any concrete alternatives.  However, I think that *culture and
</em><br>
<em>&gt; social interaction* help us humans to grow from babies into mature adult
</em><br>
<em>&gt; minds in spite of the weaknesses of our learning methods,
</em><br>
<p>Because humans have evolved to rely on culture and social interaction does 
<br>
not mean that an AI must do so.  From an AI's-eye-view, the &quot;humans&quot; are 
<br>
external blackbox objects that pose problems which, when the AI solves them, 
<br>
turns out to lead to the acquisition of reusable reflective skills.  (At 
<br>
least, that's what happens if the humans are doing it right.)
<br>
<p><em> &gt; and I think that
</em><br>
<em>&gt; these same things can probably help a baby AGI to grow from a piece of
</em><br>
<em>&gt; software into a mature AGI capable of directing its activities in a useful
</em><br>
<em>&gt; way and solving hard problems.
</em><br>
<p>I don't think the software of a baby AGI will much resemble the software of 
<br>
a mature AGI, and I say &quot;AGI&quot;, not &quot;seed AI&quot;.
<br>
<p><em>&gt; You may say that I'm just anthropomorphizing here, but I don't think so.
</em><br>
<em>&gt; Clearly teaching an AGI will be very different from teaching a human.  But
</em><br>
<em>&gt; it seems so dumb not to give our early-stage would-be AGI the benefit of
</em><br>
<em>&gt; human knowledge and intuition, which is considerable, though flawed.
</em><br>
<p>The hardest part of AI is doing *anything* that will have a benefit.  Most 
<br>
things won't.  Transferring over the corpus of human knowledge, in the form 
<br>
of those signals in which it is stored for communication between humans, 
<br>
will accomplish very little.  It's not the kind of problem that an AI needs 
<br>
to grow, or that an early AGI can solve at all.
<br>
<p><em> &gt; And if
</em><br>
<em>&gt; we do get it started with our teaching &amp; our knowledge, then when it
</em><br>
<em>&gt; outstrips us, it will face a new set of challenges.  I'm sure it will be
</em><br>
<em>&gt; able to meet these challenges, but how fast?  I don't know, and neither do
</em><br>
<em>&gt; you!
</em><br>
<p>And this &quot;I don't know&quot; is used as an argument for it happening at 
<br>
humanscale speeds, or in a volume of uncertainty centered on humanscale speeds?
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4195.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4193.html">Eugen Leitl: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4179.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4209.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4209.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4221.html">James Higgins: "Re: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Maybe reply:</strong> <a href="4305.html">James Higgins: "Re: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Maybe reply:</strong> <a href="4318.html">James Higgins: "Re: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Maybe reply:</strong> <a href="4371.html">Smigrodzki, Rafal: "RE: Seed AI (was: How hard a Singularity?)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4194">[ date ]</a>
<a href="index.html#4194">[ thread ]</a>
<a href="subject.html#4194">[ subject ]</a>
<a href="author.html#4194">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
