<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Military Friendly AI</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Military Friendly AI">
<meta name="Date" content="2002-06-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Military Friendly AI</h1>
<!-- received="Fri Jun 28 07:13:56 2002" -->
<!-- isoreceived="20020628131356" -->
<!-- sent="Thu, 27 Jun 2002 20:09:23 -0700" -->
<!-- isosent="20020628030923" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Military Friendly AI" -->
<!-- id="3D1BD363.40005@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D1B954F.3000202@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Military%20Friendly%20AI"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Thu Jun 27 2002 - 21:09:23 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4492.html">Brian Atkins: "Ben vs. Ben"</a>
<li><strong>Previous message:</strong> <a href="4490.html">Brian Atkins: "Re: Military Friendly AI"</a>
<li><strong>In reply to:</strong> <a href="4477.html">Eliezer S. Yudkowsky: "Re: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4464.html">Samantha Atkins: "Re: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4491">[ date ]</a>
<a href="index.html#4491">[ thread ]</a>
<a href="subject.html#4491">[ subject ]</a>
<a href="author.html#4491">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer S. Yudkowsky wrote:
<br>
<em>&gt; Samantha Atkins wrote:
</em><br>
<p><em>&gt;  &gt; I don't think it is cultural. Or perhaps the problem is that &quot;rational&quot;
</em><br>
<em>&gt;  &gt; and &quot;irrational&quot; are too fuzzy to be of much use in clarifying the
</em><br>
<em>&gt;  &gt; points.  In any case I don't see why an SI would be any less inclined to
</em><br>
<em>&gt;  &gt; have its choices of possible solution paths be influenced by previous
</em><br>
<em>&gt;  &gt; taken paths that had &quot;worked&quot; in other similar contexts than we are.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'm not sure what you mean by this.  Why would a grownup AI trust 
</em><br>
<em>&gt; unreliable past information until it had been verified?  Why would a 
</em><br>
<em>&gt; grownup AI trust to the value a skill routine that fulfilled a subgoal 
</em><br>
<em>&gt; of whose real value it had become uncertain?  Calling the validity of a 
</em><br>
<em>&gt; cause into question should almost always call into question the validity 
</em><br>
<em>&gt; of that cause's effects. Given the limited complexity of its infancy I 
</em><br>
<em>&gt; would expect an SI to be able to model this part of its past pretty much 
</em><br>
<em>&gt; in toto.  I just don't see a bias like that as persisting once the AI 
</em><br>
<em>&gt; knows enough to contradict the original cause.  This isn't a military 
</em><br>
<em>&gt; thing.  This isn't even a Friendly AI thing! Washing the biases out is a 
</em><br>
<em>&gt; *very* general part of growing up.
</em><br>
<p>OK.  I see your point on this.  So now we have a FAI that 
<br>
hopefully, when it reexamines all its past programming and 
<br>
experience, concludes that it should make peace not war, if you 
<br>
will. So if that is the best conclusion given its supergoal then 
<br>
it makes no difference what it did before.
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt;  &gt;&gt; An AI can learn the programmer's mistakes in verbal form, associational
</em><br>
<em>&gt;  &gt;&gt; form, recognized patterns, et cetera.  The critical issue is whether,
</em><br>
<em>&gt;  &gt;&gt; when the AI grows up, the AI will be able to correct those mistakes.
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; So, you are expecting it to decide that killing people was a &quot;mistake&quot;
</em><br>
<em>&gt;  &gt; and drop it from future problem solving?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Assuming it *was* a mistake, then yes.  
</em><br>
<p>If it concludes it is not a mistake, that it is a perfectly 
<br>
viable method, then can it be said to be &quot;Friendly&quot; in any sense 
<br>
we would recognize?
<br>
<p><em>&gt; 
</em><br>
<em>&gt;  &gt; It might or might not kill its own programmers.  The danger is 
</em><br>
<em>&gt; whether it
</em><br>
<em>&gt;  &gt; considers killing itself to be a long-term viable way of dealing with
</em><br>
<em>&gt;  &gt; problems.  If it carries this beyond the point where humans can 
</em><br>
<em>&gt; influence
</em><br>
<em>&gt;  &gt; its programming we have a problem as far as I can see.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; We do indeed have a problem.  The problem is not &quot;a violent SI&quot;.  The 
</em><br>
<em>&gt; problem is &quot;an SI that can't correct moral errors made by its 
</em><br>
<em>&gt; programmers&quot; which is by far more dangerous.
</em><br>
<em>&gt; 
</em><br>
<p>But we still have the path open to the FAI deciding that killing 
<br>
is perfectly reasonable and moral,  Would you please refresh my 
<br>
memory as to how the Friendliness supergoal (or what it arises 
<br>
from) is stated?  Is it sufficient that the FAI will not come to 
<br>
the conclusion that killing humans is moral as it &quot;grows up&quot;? 
<br>
If it is so constructed then having it kill humans when it is 
<br>
too young to catch the contradiction is evil.  It seems quite 
<br>
counter to training it to understand and apply its own 
<br>
supergoals successfully.
<br>
<p><p><em>&gt;  &gt;&gt; Despite an immense amount of science fiction dealing with this topic, I
</em><br>
<em>&gt;  &gt;&gt; honestly don't think that an *infrahuman* AI erroneously deciding to
</em><br>
<em>&gt;  &gt;&gt; solve problems by killing people is all that much of a risk, both in
</em><br>
<em>&gt;  &gt;&gt; terms of the stakes being relatively low, and in terms of it really not
</em><br>
<em>&gt;  &gt;&gt; being all that likely to happen as a cognitive error.  Because of its
</em><br>
<em>&gt;  &gt;&gt; plot value, it happens much more often in science fiction than it would
</em><br>
<em>&gt;  &gt;&gt; in reality.  (You have been trained to associate to this error as a
</em><br>
<em>&gt;  &gt;&gt; perceived possibility at a much higher rate than its probable
</em><br>
<em>&gt;  &gt;&gt; real-world incidence.)  I suppose if you had a really bad disagreement
</em><br>
<em>&gt;  &gt;&gt;  with a working combat AI you might be in substantially more trouble
</em><br>
<em>&gt;  &gt;&gt; than if you had a disagreement with a seed AI in a basement lab, but
</em><br>
<em>&gt;  &gt;&gt; that's at the infrahuman level - meaning, not Singularity-serious.  A
</em><br>
<em>&gt;  &gt;&gt; disagreement with a transhuman AI is pretty much equally serious
</em><br>
<em>&gt;  &gt;&gt; whether the AI is in direct command of a tank unit or sealed in a lab
</em><br>
<em>&gt;  &gt;&gt; on the Moon; intelligence is what counts.
</em><br>
<em>&gt;  &gt;
</em><br>
<em>&gt;  &gt; Well gee, that is a great relief!  But you haven't really convincingly
</em><br>
<em>&gt;  &gt; stated why this is not a possibility.  If the combat AI is capable of
</em><br>
<em>&gt;  &gt; evolving (is a Seed) then we have a problem, no?  The switching to
</em><br>
<em>&gt;  &gt; &quot;infrahuman&quot; does not shed light on the worry.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What I'm saying is that you don't get Singularity tragedies - as opposed 
</em><br>
<em>&gt; to ordinary military tragedies (&quot;Friendly fire&quot;) - unless you're dealing 
</em><br>
<em>&gt; with a transhuman AI.  And if you're dealing with a transhuman AI then 
</em><br>
<em>&gt; it is probably not relevant whether the AI is in immediate command of a 
</em><br>
<em>&gt; tank unit; the AI must be Friendly.
</em><br>
<em>&gt;
</em><br>
<p>Perhaps I am still not getting your point.  It looks as if you 
<br>
are saying that if we get &quot;Friendly fire&quot; then it is not a FAI 
<br>
but only a transhuman AI.  Is that right?  So an FAI cannot 
<br>
conclude killing humans is ok?  If not then the mistraining of 
<br>
having it involved in combat is obvious.
<br>
<p><p><em>&gt;  &gt; If you assume it was morally acceptable to kill people earlier and train
</em><br>
<em>&gt;  &gt;  the AI that this is so then how will you latter train it that this 
</em><br>
<em>&gt; isn't
</em><br>
<em>&gt;  &gt;  so, assuming it hasn't already gone beyond being influenced by your
</em><br>
<em>&gt;  &gt; attempts at training?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What you do is teach the AI in such a way that final moral decisions are 
</em><br>
<em>&gt; not represented as correct in themselves, but correct conclusions from 
</em><br>
<em>&gt; premises which, under improved intelligence, can be used to reach the 
</em><br>
<em>&gt; actual valid conclusion.  To give a simple example, the AI is told to 
</em><br>
<p>Hmmm.  Are we sure there is one &quot;actual valid conclusion&quot; in 
<br>
many scenarios?
<br>
<p><em>&gt; defend the United States under the premise that all sentient life is 
</em><br>
<em>&gt; equally valid but that the military effectiveness of the US is the 
</em><br>
<em>&gt; safeguard of international stability and contributing to it saves lives 
</em><br>
<em>&gt; in the long run - because this is what the AI researcher believes and 
</em><br>
<p>But that would be giving it a quite questionable premise from 
<br>
the get-go.  How could that be in the least good for its 
<br>
training unless the purpose was to tests its error detection and 
<br>
correction abilities?  I can see the researcher can guarantee to 
<br>
military funders that the young FAI will actually do anything 
<br>
they are paying for it to do.  If it is not &quot;dumbed down&quot; it may 
<br>
likely become an immediate ACO - conscientous objector.
<br>
<p><p><p><p><em>&gt; it's why s/he works for the military.  It doesn't quite matter whether 
</em><br>
<em>&gt; the US is really the world's police as long as the AI gets its mitts on 
</em><br>
<em>&gt; the premise that all sentient life is equally valid.
</em><br>
<em>&gt; 
</em><br>
<p>But the premise it is asked to act on is that the US sentient 
<br>
life and its stated military goals is of higher value than all 
<br>
other sentient life.
<br>
<p><p><p><em>&gt;  &gt; If it is wrong when the AI &quot;grows up&quot; then it was wrong to
</em><br>
<em>&gt;  &gt; require it of the AI when it was young.  I doubt the AI will miss the
</em><br>
<em>&gt;  &gt; contradiction.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Of course not.  The point is that the researcher was being honest 
</em><br>
<em>&gt; earlier, and later (a) changed his/her mind, or (b) was contradicted by 
</em><br>
<em>&gt; the grownup AI reconsidering the moral question at a higher level of 
</em><br>
<em>&gt; intelligence.
</em><br>
<em>&gt; 
</em><br>
<p>I am a little worried that you leave open the possibility that 
<br>
the &quot;grown&quot; AI will decide that killing humans is still ok.  But 
<br>
I understand your reasoning better.
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4492.html">Brian Atkins: "Ben vs. Ben"</a>
<li><strong>Previous message:</strong> <a href="4490.html">Brian Atkins: "Re: Military Friendly AI"</a>
<li><strong>In reply to:</strong> <a href="4477.html">Eliezer S. Yudkowsky: "Re: Military Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4464.html">Samantha Atkins: "Re: Military Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4491">[ date ]</a>
<a href="index.html#4491">[ thread ]</a>
<a href="subject.html#4491">[ subject ]</a>
<a href="author.html#4491">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
