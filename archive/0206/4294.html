<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Deep self-modification (was Re: How hard a Singularity?)</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: Deep self-modification (was Re: How hard a Singularity?)">
<meta name="Date" content="2002-06-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Deep self-modification (was Re: How hard a Singularity?)</h1>
<!-- received="Mon Jun 24 19:50:00 2002" -->
<!-- isoreceived="20020625015000" -->
<!-- sent="Mon, 24 Jun 2002 17:19:04 -0600" -->
<!-- isosent="20020624231904" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: Deep self-modification (was Re: How hard a Singularity?)" -->
<!-- id="LAEGJLOGJIOELPNIOOAJMEIPCLAA.ben@goertzel.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="1024954964.19216.40.camel@avalon" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20Deep%20self-modification%20(was%20Re:%20How%20hard%20a%20Singularity?)"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Mon Jun 24 2002 - 17:19:04 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4295.html">James Rogers: "RE: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4293.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4284.html">James Rogers: "Deep self-modification (was Re: How hard a Singularity?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4169.html">Mike & Donna Deering: "Re: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4294">[ date ]</a>
<a href="index.html#4294">[ thread ]</a>
<a href="subject.html#4294">[ subject ]</a>
<a href="author.html#4294">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi james,
<br>
<p>I understand your perspective.  I know one other venerable AI theorist who
<br>
agrees with you -- Pei Wang.
<br>
<p>He reckons that the optimal near-AI architecture is fairly simple (his NARS
<br>
system), and that what a superintelligent AI would need to do would be to
<br>
<p>a) learn within this architecture, including learning new high-level
<br>
cognitive schema to govern its thought-patterns, etc.
<br>
<p>b) get itself more hardware, design more efficient hardware, etc.
<br>
<p>I don't see things this way however ... because, I don't agree that an AGI
<br>
design is going to have to be near-optimal in order to be workable...
<br>
<p>The best way to explain why I think you're dead wrong is to look at
<br>
supercompilation technology, e.g. www.supercompilers.com, which contains a
<br>
whitepaper I circulated a while back...
<br>
<p>Human-written programs are almost NEVER anywhere near-optimal, automated
<br>
supercompilation can speed them up tremendously by introducing huge nasty
<br>
bits of code -- unrolling loops, specializing functions to particular
<br>
argument values, etc.
<br>
<p>I think similar sorts of optimizations can be done to improve AGI
<br>
intelligence by orders of magnitude on fixed hardware.
<br>
<p>Supercompilation is different than intelligence-oriented program
<br>
self-modification, because supercompilation *only* affects speed, leaving
<br>
program behavior identical.  It's a special case of intelligence-oriented
<br>
program self-modification, which aims to improve both speed and the
<br>
intelligence of behaviors...
<br>
<p>For instance, let's consider some examples of &quot;subprogram specialization&quot; --
<br>
one among very many things the supercompiler does -- as they  may occur one
<br>
day in Novamente.
<br>
<p>In Novamente, we have one core data structure for storing knowledge, and a
<br>
couple specialized knowledge representations for particular kinds of
<br>
knowledge.  An advanced Novamente could create 100 different specialized
<br>
knowledge represntations for itself, and modify them dynamically.  Human
<br>
programmers *could* do this, but it would overcomplicate things hugely.....
<br>
<p>Similarly, in Novamente we have a few pre-specified &quot;parameter vectors&quot; for
<br>
the system as a whole, and some adaptive rules for altering them.  An
<br>
advanced Novamente could create a shitload of complicated rules for
<br>
governing parameter adaptation, invoking its full inteligence to do so.
<br>
This is easy enough that we've previously experimented with it....
<br>
<p>We have a few inference control strategies... and the system can learn its
<br>
own inference control strategies (called &quot;cognitive schema&quot;).  Once it
<br>
learns enough of these, it may become WAY WAY smarter than it was before,
<br>
due to its ability to  control its inferential processes in a cleverer
<br>
context-dependent way
<br>
<p>We have a scheduling algorithm regulating the activity of the various mental
<br>
processes in the system... it's not a bad one. But maybe we need 373
<br>
different scheduling algorithms which may be selected between, on a
<br>
context-dependent basis...
<br>
<p>These examples I'm reeling off (and I could give many more) are the port to
<br>
AGI of &quot;program specialization&quot;, which is just *one* kind of optimization
<br>
that the supercompiler now does for Java programs.  I'm not saying this is
<br>
the only kind of self-modification a Novamente will be able to do... no way.
<br>
But this kind of optimization alone could yield huge intelligence
<br>
improvements.
<br>
<p>Essentially any complex software system can be immensely optimized via
<br>
supercompilation.  Similarly, I think essentially any complex AGI system can
<br>
be made immensely more intelligent by the application of
<br>
supercompilation-style optimizations.
<br>
<p>How much of this optimization will involve the &quot;basic architecture&quot; versus
<br>
the &quot;cognitive processes implemented on top of the architecture&quot;?  That I
<br>
don't know.  But my guess is that the basic architecture will be strongly
<br>
affected by automated optimization, e.g. by things like scheduling and basic
<br>
knowledge structures being expanded into multiple specialized subcases via
<br>
subprogram specialization heuristics.
<br>
<p>In any case, I hope this e-mail has served to indicate that
<br>
&quot;intelligence-oriented self-modification&quot; is not a completely abstract
<br>
pie-in-the-sky idea.  I have a concrete idea of what kinds of optimizations
<br>
and improvements are going to be made to Novamente via self-modification....
<br>
<p>However, we have a lot of engineering and testing and teaching to do, before
<br>
we have a system that is able to do the sorts of self-modifications I'm
<br>
describing here (which are only simple examples).
<br>
<p>In fact, I'd like to approach Novamente self-mod by hybridizing Novamente
<br>
with the supercompiler, but the supercompiler is currently only written for
<br>
two languages: Refal and Java (and the java version is still incomplete).
<br>
When we get to that stage, either we'll need to make a C++ supercompiler
<br>
(very hard) or port Novamente to Java or some other pointerless language
<br>
(easier) ...
<br>
<p>-- Ben g
<br>
<p><p><p><p><p><p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20Deep%20self-modification%20(was%20Re:%20How%20hard%20a%20Singularity?)">owner-sl4@sysopmind.com</a> [mailto:<a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20Deep%20self-modification%20(was%20Re:%20How%20hard%20a%20Singularity?)">owner-sl4@sysopmind.com</a>]On Behalf
</em><br>
<em>&gt; Of James Rogers
</em><br>
<em>&gt; Sent: Monday, June 24, 2002 3:43 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sysopmind.com?Subject=RE:%20Deep%20self-modification%20(was%20Re:%20How%20hard%20a%20Singularity?)">sl4@sysopmind.com</a>
</em><br>
<em>&gt; Subject: Deep self-modification (was Re: How hard a Singularity?)
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I actually disagree that the ability to self-modify an AI architecture
</em><br>
<em>&gt; at some fundamental level is even important, at least in the sense that
</em><br>
<em>&gt; I get the strong impression people are using it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The value of deep self-modifying code is apparently premised on the
</em><br>
<em>&gt; first implementations of AGI being duct-taped architectures that barely
</em><br>
<em>&gt; function, due in no small part to their extreme complexity.  My own take
</em><br>
<em>&gt; is that if AGI is actually as complicated as most believe, then the only
</em><br>
<em>&gt; likely and plausible human engineered implementations will almost have
</em><br>
<em>&gt; to be very close approximations of &quot;optimal intelligence&quot; (read: elegant
</em><br>
<em>&gt; and properly generalized models) as a consequence. Compounding this is
</em><br>
<em>&gt; the probable fragility of AGI architectures with respect to the various
</em><br>
<em>&gt; forms of computational complexity, meaning that most design vectors that
</em><br>
<em>&gt; stray away from optimal architectures may not be able to reach seed AI
</em><br>
<em>&gt; level due to tractability problems.  From this perspective, I don't
</em><br>
<em>&gt; think it is unreasonable to assert that architectural self-modification
</em><br>
<em>&gt; is an unnecessary capability as all likely human implementations of an
</em><br>
<em>&gt; AI will almost have to be optimal (or close approximations) to even be
</em><br>
<em>&gt; practical.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If this is the case and the first &quot;real&quot; AGI architecture is a close
</em><br>
<em>&gt; approximation of optimal, then the qualitative bootstrap process will
</em><br>
<em>&gt; essentially be hardware limited no matter how intelligent the AGI
</em><br>
<em>&gt; actually is.  Obviously there has to be some self-modification at higher
</em><br>
<em>&gt; abstractions or a system couldn't learn, but that doesn't need to impact
</em><br>
<em>&gt; the underlying architecture (and is essentially orthogonal to the
</em><br>
<em>&gt; question in any case).
</em><br>
<em>&gt;
</em><br>
<em>&gt; -James Rogers
</em><br>
<em>&gt;  <a href="mailto:jamesr@best.com?Subject=RE:%20Deep%20self-modification%20(was%20Re:%20How%20hard%20a%20Singularity?)">jamesr@best.com</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4295.html">James Rogers: "RE: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4293.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4284.html">James Rogers: "Deep self-modification (was Re: How hard a Singularity?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4169.html">Mike & Donna Deering: "Re: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4294">[ date ]</a>
<a href="index.html#4294">[ thread ]</a>
<a href="subject.html#4294">[ subject ]</a>
<a href="author.html#4294">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
