<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Threats to the Singularity.</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Threats to the Singularity.">
<meta name="Date" content="2002-06-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Threats to the Singularity.</h1>
<!-- received="Mon Jun 17 22:26:56 2002" -->
<!-- isoreceived="20020618042656" -->
<!-- sent="Mon, 17 Jun 2002 16:15:13 -0700" -->
<!-- isosent="20020617231513" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Threats to the Singularity." -->
<!-- id="3D0E6D81.7020709@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D0DFCA0.6000102@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Threats%20to%20the%20Singularity."><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Mon Jun 17 2002 - 17:15:13 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4077.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4075.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4067.html">Eliezer S. Yudkowsky: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4075.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4076">[ date ]</a>
<a href="index.html#4076">[ thread ]</a>
<a href="subject.html#4076">[ subject ]</a>
<a href="author.html#4076">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer S. Yudkowsky wrote:
<br>
<p><em>&gt; Gordon Worley wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; On Monday, June 17, 2002, at 05:31  AM, Samantha Atkins wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; Do I read you correctly?  If I do, then why do you hold this 
</em><br>
<em>&gt;&gt;&gt; position?  If I read you correctly then how can you expect the 
</em><br>
<em>&gt;&gt;&gt; majority of human beings, if they really understood you, to consider 
</em><br>
<em>&gt;&gt;&gt; you as other than a monster?
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Shouldn't you be trying to figure out what's right before discussing its 
</em><br>
<em>&gt; PR value?  Or are you arguing that the &quot;yuck factor&quot; reaction of many 
</em><br>
<em>&gt; humans is representative of an actual moral wrong?  If so, why not argue 
</em><br>
<em>&gt; the moral wrong itself, rather than arguing from the agreement of a 
</em><br>
<em>&gt; large number of people who have not actually been consulted?
</em><br>
<em>&gt; 
</em><br>
<p><p>What do you think that I am doing other than attempting to 
<br>
figure out what is being said and why?  Killing other sentients 
<br>
without very dire reasons is in my book a moral wrong.  It has 
<br>
nothing to do with &quot;yuck factor&quot; and I am quite disappointed to 
<br>
see you taking this line.  I thought I remembered you taking the 
<br>
veiw that a FAI will safeguard the lives of humans, uploaded or 
<br>
not, as much as possible and this is part of what it means to be 
<br>
&quot;Friendly&quot;.  I certainly remember you on several occassions 
<br>
using the final deaths of so many humans today and the 
<br>
possibility of giga-death destruction just around the corner as 
<br>
a strong motivator for the Work.  So I hardly see why concern 
<br>
for the continued existence of our fellow sentience can now be 
<br>
dismmised as simply evolution-programmed &quot;YUCK&quot; factor.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Exactly.  Morality, like rationality, is never on anyone's side.  The 
</em><br>
<em>&gt; most you can try to do is end up being on the side of morality.  The 
</em><br>
<em>&gt; price of seeing the morality of a situation clearly is that you start 
</em><br>
<em>&gt; out by asking which side you should be on, rather than looking for a way 
</em><br>
<em>&gt; to rationalize one side.  Sometimes, just as in rationality, evidence 
</em><br>
<em>&gt; (or valid moral argument) is weighted very heavily on one side of the 
</em><br>
<em>&gt; scales and judgement is easy, but it doesn't mean that judgement can be 
</em><br>
<em>&gt; replaced with prejudgement.
</em><br>
<em>&gt; 
</em><br>
<p><p>What is the basis of your morality? Who is talking of 
<br>
prejudgement?  A pre-judgement that humans are expendable if 
<br>
need be (to be strictly determined by the &quot;Friendly&quot; AI) seems 
<br>
to be being made.
<br>
<p><p><em>&gt; It goes back to that same principle of building something eternal.  This 
</em><br>
<em>&gt; isn't a contest to see who can say the nicest things about humanity.
</em><br>
<p><p>Who in the hell ever said it was?  I am concerned with the 
<br>
well-being and the freeing of humanity, not with saying nice 
<br>
things about them.
<br>
<p>&nbsp;&nbsp;
<br>
<em>&gt; The decision that a universe with humanity or human-derived minds in it 
</em><br>
<em>&gt; is what we want to see lasting through eternity is not a decision for 
</em><br>
<em>&gt; either a Friendly AI or a human philosopher to make lightly, whether 
</em><br>
<em>&gt; &quot;eternity&quot; is taken to mean a few billion years or an actual infinity.  
</em><br>
<p><p>Well, I never said the minds have to be strictly human.  I never 
<br>
said that human derived minds have to exist forever either. I 
<br>
said that sentients and their continued well-being to the 
<br>
maximum extent possible should be a top priority if I am to 
<br>
believe what is proposed is &quot;Friendly&quot; or even remotely 
<br>
palatable.  I have no problem with the forms of the sentients 
<br>
changing as they find useful and convenient to quite non-human 
<br>
modalities or if they join with an SI and so on. I have no 
<br>
problem with all of certain lines eventually falling behind and 
<br>
even becoming extinct.  I have a very large problem with saying 
<br>
that it is alright for us to build something where it is an open 
<br>
option to exterminate all human life by choice. I have a large 
<br>
problem also if the upliftment of human beings (voltionally of 
<br>
course) is not a high priority.
<br>
<p><p><em>&gt; Either way that's a hell of a long time.  Isn't it worth an hour to 
</em><br>
<em>&gt; think about it today? Even if the moral question is &quot;trivial&quot;, in the 
</em><br>
<em>&gt; mathematical sense of being a trivial consequence of the basic rules of 
</em><br>
<em>&gt; moral reasoning, then this itself needs to be established.
</em><br>
<em>&gt; 
</em><br>
<p><p>Since it is a strawman this paragraph is without much meaning.
<br>
<p><p><em>&gt; There are also penalties to intelligence if you stop thinking too early. 
</em><br>
<em>&gt; What if humanity's survival was morally worthwhile given a certain 
</em><br>
<em>&gt; easily achievable enabling condition, but a snap judgement caused you to 
</em><br>
<em>&gt; miss it? I can't think of any concrete scenario matching this 
</em><br>
<em>&gt; description, but I think that growing into a strong thinker involves 
</em><br>
<em>&gt; thinking through every possibility.  The conclusions may be obvious but 
</em><br>
<em>&gt; you still have to do the math to arrive at the obvious conclusions.  
</em><br>
<p><p>I don't need to do math to prove something so fundamental to me 
<br>
is in fact fundamental to me!  The well-being and freeing of 
<br>
humankind and other sentients is a supergoal for me.  I will 
<br>
judge your work in terms of its utility for that supergoal.
<br>
<p><em>&gt; Otherwise you *don't know* the math!  Maybe this doesn't matter much if 
</em><br>
<em>&gt; you're willing to go through your life on autopilot, but it sure as heck 
</em><br>
<em>&gt; matters for building AI.  And the only way you can know the math is by 
</em><br>
<p><p>You have to start somewhere.  How will you prove the supergoal 
<br>
of Friendliness to the SI?
<br>
<p><p><em>&gt; being willing to emotionally accept either outcome when you start 
</em><br>
<em>&gt; thinking.  You can't pretend to be able to accept either outcome in 
</em><br>
<em>&gt; order to find the math.  You have to be able to *actually* accept the 
</em><br>
<em>&gt; moral outcome whatever it is.  This is why &quot;attachment&quot;, even to good 
</em><br>
<em>&gt; things that really turn out to be good, is a bad thing.
</em><br>
<em>&gt; 
</em><br>
<p><p>You seem to be arguing that ethics can be done in a complete 
<br>
vacuum with no starting basis whatsoever.  I do not believe this 
<br>
can be done.
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4077.html">Gordon Worley: "Re: Threats to the Singularity."</a>
<li><strong>Previous message:</strong> <a href="4075.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<li><strong>In reply to:</strong> <a href="4067.html">Eliezer S. Yudkowsky: "Re: Threats to the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4075.html">Samantha Atkins: "Re: Threats to the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4076">[ date ]</a>
<a href="index.html#4076">[ thread ]</a>
<a href="subject.html#4076">[ subject ]</a>
<a href="author.html#4076">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
