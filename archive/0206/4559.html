<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: FAI means no programmer-sensitive AI morality</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: FAI means no programmer-sensitive AI morality">
<meta name="Date" content="2002-06-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: FAI means no programmer-sensitive AI morality</h1>
<!-- received="Sat Jun 29 18:15:53 2002" -->
<!-- isoreceived="20020630001553" -->
<!-- sent="Sat, 29 Jun 2002 18:15:28 -0400" -->
<!-- isosent="20020629221528" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: FAI means no programmer-sensitive AI morality" -->
<!-- id="3D1E3180.4030900@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJKEDFCMAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20FAI%20means%20no%20programmer-sensitive%20AI%20morality"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Jun 29 2002 - 16:15:28 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4560.html">Brian Atkins: "Re: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4558.html">James Higgins: "Re: Who will launch the Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4554.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4569.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4569.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4559">[ date ]</a>
<a href="index.html#4559">[ thread ]</a>
<a href="subject.html#4559">[ subject ]</a>
<a href="author.html#4559">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt;&gt; The vast majority of religious
</em><br>
<em>&gt;&gt;people, especially what we would call &quot;fundamentalists&quot; and those outside
</em><br>
<em>&gt;&gt;the First World, adhere to a correspondence theory of the truth of their
</em><br>
<em>&gt;&gt;religion; when they say something is true, they mean that it is so; that
</em><br>
<em>&gt;&gt;outside reality corresponds to their belief.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't think you have it quite right.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What they mean is more nearly that *their experience corresponds to their
</em><br>
<em>&gt; beliefs*
</em><br>
<p>Yes.  They mean that their experience corresponds to their beliefs, that 
<br>
their beliefs give them the experience of personal validation, that 
<br>
(they think) their beliefs support altruism and uphold the common social 
<br>
fabric, that their beliefs are acceptable to their peers... and that 
<br>
their beliefs correspond to external reality.
<br>
<p>The human mind comes with many reasons to believe.  Science includes 
<br>
some of them.  Religion and all other human stories include all of them. 
<br>
&nbsp;&nbsp;Only in modern times have theologians actually attempted to eliminate 
<br>
rationality from religion, and of course they have not succeeded; plus, 
<br>
this is a handful of theologians with no relation whatsoever to either 
<br>
the average dude's belief or the actual spirit of the religion.
<br>
<p><em>&gt; Modern scientific realism is based on accepting outside reality,
</em><br>
<em>&gt; observations of physical reality, as the fundamental determinant of truth.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; On the other hand, many other traditions are based on accepting *inner
</em><br>
<em>&gt; intuitions and experiences* as the fundamental determinants of truth.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It seems to me like you don't fully appreciate what it means for someone to
</em><br>
<em>&gt; have a truly non-rationalist, non-scientific point of view.  Probably this
</em><br>
<em>&gt; is because your life-course so far has not led you to spend significantly
</em><br>
<em>&gt; much time with such people.  Mine, as it happens, has.
</em><br>
<p>Uh... Ben, unless you spent 4 hours a day during your first twelve years 
<br>
being indoctrinated in a fundamentalist religion...
<br>
<p>No offense, Ben, but I probably have a much better picture of how a 
<br>
fundamentalist Jew *actually thinks* than you do.  Your understanding of 
<br>
religion appears to be built around a very abstract outsider's 
<br>
viewpoint.  In my experience religious people argue just like other 
<br>
people, regardless of what they say they believe about empiricism and so 
<br>
on.  For that matter, the vast majority of scientists argue just like 
<br>
other people, regardless of what *they* say they believe about 
<br>
empiricism and so on.  During my first twenty years on Earth I had the 
<br>
opportunity to hear far more Devrai Torah than any sane being would 
<br>
voluntarily undergo, and yes, there is rationally structured argument, 
<br>
cause and effect, evidence, et cetera, mixed in with the social morals 
<br>
and the appeals to internal experience and so on.
<br>
<p>I think you are confusing the principles which people verbally adhere to 
<br>
with the way that people actually think.
<br>
<p>And while I'm not proud of it, yes, I *was* religious up until the age 
<br>
of, oh, I would say around eleven years or so.  We don't choose our 
<br>
parents.  I do remember how it worked.  I'm curious, Ben, have you ever 
<br>
actually *been* religious?  Do you know how religious thinking works *in 
<br>
the first person*?  Because I have to say you're sounding like a 
<br>
complete outsider here - like your idea of religion comes from watching 
<br>
scientists debating theologians about the nature of truth - which is a 
<br>
very different thing from how ordinary religious people actually think 
<br>
in practice.  I'm sure that you've had chats with your semireligious 
<br>
parents and your semireligious wife and so on, and maybe read a few 
<br>
books, but you may need to consider that standing back as a scientist 
<br>
and going &quot;Gosh, how *utterly alien* and *unempirical*&quot; is going to give 
<br>
you a different perspective, and one which is maybe a bit unrealistic 
<br>
about the way religious people talk to each other when they're not 
<br>
debating a scientist or whatever.
<br>
<p>And finally:  Ben, could you please stop with the condescending &quot;Oh, you 
<br>
must not have XYZ&quot; stuff?  I've held off complaining until now but it's 
<br>
starting to get outright silly.  As I've illustrated above, two can play 
<br>
at that game, and in this case I'm starting to get a bit sick of it.  I 
<br>
don't know whether you actually have any in-depth experience of how 
<br>
religious people think - maybe you do and maybe you don't; you certainly 
<br>
think you do - but it is foolish to pull rank on someone who was, 
<br>
however involuntarily, raised by Orthodox Jews for twenty years.  I'm 
<br>
sure I don't know everything there is to know about Orthodox Jewish 
<br>
thinking, much less Buddhism or any of the other religions, but if you 
<br>
think there's something I'm missing, kindly explain it to me as an equal 
<br>
instead of attempting to pull rank due to your supposed greater depth of 
<br>
religious experience.
<br>
<p><em>&gt; Take traditional Chinese medicine, or yoga, or Zen Buddhism, as examples.
</em><br>
<em>&gt; These are ancient traditions with a lot of depth and detail to them.  Their
</em><br>
<em>&gt; validity, such as it is, is primarily *experiential*.  It is largely not
</em><br>
<em>&gt; based on things that individuals outside the tradition in question can
</em><br>
<em>&gt; observe in empirical reality.  [Yeah, I know people have tried to test for
</em><br>
<em>&gt; enlightenment by studying brain waves and such (lots of work at Maharishi
</em><br>
<em>&gt; University on this), but this isn't what it's all about -- this is icing on
</em><br>
<em>&gt; the cake from the spiritual point of view.]
</em><br>
<em>&gt; 
</em><br>
<em>&gt; When my wife for instance became interested in Zen, it wasn't because any
</em><br>
<em>&gt; kind of analysis of observations convinced her, it was because some things
</em><br>
<em>&gt; she read in a Zen book resonated with some experiences she'd already had...
</em><br>
<p>Ben, I think that you are again being much too restrictive about what 
<br>
constitutes rationality and evidence.  People's internal experiences can 
<br>
be evidence too.  If Zen enlightenment takes place it is a real event. 
<br>
(Incidentally, I think Zen enlightenment is probably a real thing even 
<br>
if it is not mystical in nature, and I've always wanted to try and defy 
<br>
the Zen rules and come up with a purely rational train of thought which 
<br>
ends in Zen enlightenment... but this may actually be as impossible as 
<br>
much Zen tradition insists.)
<br>
<p>Is the fountain of human altruism a piece of external evidence or is it 
<br>
a subjective experience?  I can certainly imagine a Power discovering an 
<br>
objective morality which exists outside of all observers and which is 
<br>
apparent to any sufficiently intelligent entity regardless of its 
<br>
previous goals.  This would certainly simplify Friendly AI immensely, 
<br>
but unfortunately we cannot assume it to be the case.  In that case, one 
<br>
of the foundations of Friendliness, the shaper of altruism, will be the 
<br>
human subjective experience of altruism.  Subjective experiences are not 
<br>
outside the scope of Friendly AI, nor do they exist outside the physical 
<br>
universe.
<br>
<p>The idea that religion and rationality are orthogonal is a modern idea 
<br>
proposed by modern theologians; it also happens to be wrong.  If I am to 
<br>
speak from my personal experience then every Orthodox Jew with whom I 
<br>
have ever discussed religion has made any number of readily testable 
<br>
statements about the external world and about the comparative subjective 
<br>
experiences of Orthodox Jews and everyone else.
<br>
<p>Anyway, let's keep this conversation focused.  Ben, is it your assertion 
<br>
that even if the Jewish or Buddhist religion were correct, this would 
<br>
not be apparent to a Friendly AI that had been programmed by atheists? 
<br>
Because *I* would most certainly regard this as a bug.
<br>
<p>Obviously religious people are not thinking like Friendly AIs, or, given 
<br>
that we are correct in asserting that their religions are in fact wrong, 
<br>
they would not be religious.  But just in case *we* happen to be the 
<br>
ones who are in fact horribly, fundamentally wrong, whether or not any 
<br>
current human is right, we need to make sure that the AI is not bound to 
<br>
our mistakes.  This could conceivably include mistakes about the nature 
<br>
of scientific evidence - I've heard several pro-science people make 
<br>
statements about science that I would regard as erroneous, even 
<br>
fundamentally erroneous.
<br>
<p>In any case, I must say that from a first-person perspective on 
<br>
religion, with respect to Judaism (the one religion which I know from a 
<br>
first-person perspective, albeit through no fault of my own), while 
<br>
religious thinking is not like Friendly AI thinking, there is more than 
<br>
enough evidence (given the correctness of a given religion) for a 
<br>
Friendly AI to find, lock onto, and revise itself into accordance with 
<br>
that religion even if that Friendly AI is unfortunate enough to have 
<br>
been programmed by spiritually empty atheists.  And I have seen no 
<br>
reason why any other religion would be different.
<br>
<p><em>&gt; Nitpicking about the definition of logic is not the point.  In Novamente we
</em><br>
<em>&gt; have a narrow technical definition of &quot;reasoning&quot; as opposed to other
</em><br>
<em>&gt; cognitive processes, and I can see that I've made the error in some posts of
</em><br>
<em>&gt; using this definition in nontechnical discussions, when in ordinary language
</em><br>
<em>&gt; &quot;reasoning&quot; means something broader.  Sorry about that.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But the point at hand is: many folks will be totally unconvinced by anything
</em><br>
<em>&gt; an intelligent, scientifically-minded AGI says -- just as they are
</em><br>
<em>&gt; unconvinced by your and my arguments that God probably didn't really create
</em><br>
<em>&gt; the world 6000 years ago, that there probably isn't really a Heaven into
</em><br>
<em>&gt; which only 144000 people will ever be accepted, etc.
</em><br>
<p>First comes the question of what is true.  Then comes the question of 
<br>
how to convince others.  It's not the responsibility of a Friendly AI 
<br>
programmer to convert other humans to atheism given that atheism is the 
<br>
correct religion; it is the responsibility of a Friendly AI programmer 
<br>
to build an AI that would, in fact, convert to religion X if religion X 
<br>
were, in fact, true.  Again, you've got to distinguish between 
<br>
protecting the integrity of the post-Singularity world and 
<br>
pre-Singularity politics.
<br>
<p><em>&gt;&gt;Um... not really.  If I recall correctly, Ben, you're
</em><br>
<em>&gt;&gt;second-generation or
</em><br>
<em>&gt;&gt;third-generation ex-Jew.  Can I take it that you weren't actually
</em><br>
<em>&gt;&gt;forced as  a child to study the Talmud?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; My parents were quasi-religious.  I know there are many varieties of
</em><br>
<em>&gt; Judaism, and I don't remember the right labels for all of them.  What I'm
</em><br>
<em>&gt; thinking of is actually some Kabalist stuff that I read years ago ;&gt;
</em><br>
<p>Ben, this no more enables you to speculate on the nature of Talmudic 
<br>
truth than reading a few books on Zen and Taoism makes me an Eastern 
<br>
philosopher.  *I'm* not qualified to talk about the nature of truth 
<br>
under Talmudic philosophy as seen by Talmudic philosophy, although I've 
<br>
been forced to read enough Talmud that I have some vague experience of 
<br>
how it works in practice.  I'll tell you this much: it's absurdly 
<br>
complicated.  In fact, I would have to say that I don't think I've ever 
<br>
in my life seen so much unnecessary complexity in one place in one time. 
<br>
&nbsp;&nbsp;&nbsp;The Talmud may very well be the single most unnecessarily complicated 
<br>
thing on the entire Earth at this time.  Reading some stuff on Kabbalah 
<br>
isn't going to cut it.
<br>
<p>Want some idea of how it looks from the inside?  Read this halachic 
<br>
guide to cloning:
<br>
<a href="http://www.jewsweek.com/society/056.htm">http://www.jewsweek.com/society/056.htm</a>
<br>
<p>Nonetheless, supposing that the Talmud were true, even in a purely 
<br>
spiritual sense as spiritual truth is understood in most religions, it 
<br>
would be straightforward for a Friendly AI programmed by atheists to 
<br>
lock onto it sufficiently to deduce that it (the Friendly AI) should 
<br>
have been programmed by Talmudic scholars rather than atheists and wash 
<br>
out the remaining errors.
<br>
<p>And if it happens that the Talmud says something intelligent, apposite, 
<br>
and enlightening, the right conclusion for the right reasons, about some 
<br>
moral or ethical question, then it would be a distortion of rationality 
<br>
to reject it because of the source.
<br>
<p><em>&gt;&gt;Ben, I have been taught at least one viewpoint which is not the empirical
</em><br>
<em>&gt;&gt;viewpoint of modern science.  It is pretty strange but it is not
</em><br>
<em>&gt;&gt;outside the
</em><br>
<em>&gt;&gt;correspondence theory of truth.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There are a lot of kinds of Judaism.  I don't know what kind you were
</em><br>
<em>&gt; taught, and I don't really know much about any of them.
</em><br>
<p>Well, I'm certainly pretty pathetic by Jewish standards, but I still 
<br>
think I may have a better idea at this point than you do.
<br>
<p><em>&gt; I do know that Zen Buddhism and yoga and Sufi-ist Islam are outside the
</em><br>
<em>&gt; correspondence theory of truth as you describe it, in the sense that they
</em><br>
<em>&gt; define truth more by correspondence with inner experience than by
</em><br>
<em>&gt; correspondence with physical reality.
</em><br>
<p>No human thought is outside the correspondence theory of truth.  It is 
<br>
as pervasive as the Force, although it does take some Jedi training to 
<br>
see its action and presence in all things.  The correspondence theory of 
<br>
truth runs much deeper in human nature and the cognitive foundations of 
<br>
belief itself than do our surface traditions of scientific empiricism.
<br>
<p>Now it may be that Zen proceeds from arational thoughts to an arational 
<br>
conclusion which is important not because it corresponds to some outside 
<br>
thing but because it is itself, and in this sense the core of Zen may 
<br>
come closer to being outside the correspondence theory of truth than 
<br>
anything else I know of, but it is surrounded by a core of mystical 
<br>
tradition which, like all forms of human storytelling, makes use of the 
<br>
correspondence theory of truth.  From what I know of Zen, if Zen is 
<br>
correct, then a rational mind would be able to perceive this enough to 
<br>
begin studying Zen and breaking itself of rationality.
<br>
<p><em>&gt; Physical reality, according to these traditions, is an illusion.  Emotions
</em><br>
<em>&gt; are also illusions.  Only a certain kind of crystal-clear inner insight
</em><br>
<em>&gt; (yes, these words don't do it justice...) is to be &quot;trusted&quot; (though in a
</em><br>
<em>&gt; sense it's viewed as having a directness beyond trust/mistrust)..
</em><br>
<p>And Zen makes testable assertions about this crystal-clear inner 
<br>
insight, the Satori, even though Satori is neither a testable assertion 
<br>
in itself, nor can it be obtained by confirming testable assertions. 
<br>
That's enough of a handle for a Friendly AI to lock in on Zen if Zen is 
<br>
a necessary foundation or greater apotheosis of altruism.  Though it 
<br>
would take a rather drastic amount of self-recreation before the AI was 
<br>
actually capable of studying Zen.
<br>
<p><em>&gt;&gt;If you assume that Judaism is
</em><br>
<em>&gt;&gt;the correct
</em><br>
<em>&gt;&gt;religion, then a Friendly AI would be Jewish.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The thing is that my wife, a fairly rational person and a Buddhist, would
</em><br>
<em>&gt; not accept the statement &quot;If you assume that Buddhism is the correct
</em><br>
<em>&gt; religion, then a Friendly AI would be Buddhist.&quot;
</em><br>
<p>Sounds like a testable statement.  Would you care to put it to the test? 
<br>
&nbsp;&nbsp;Again, strictly speaking this is just pre-Singularity politics as long 
<br>
as a Friendly AI *would in fact* be Buddhist given the correctness of 
<br>
Buddhism, but you may underestimate the extent to which religion and 
<br>
rationality are *not* orthogonal.
<br>
<p><em>&gt; The fact that you, Eliezer, accept this statement is a consequence of your
</em><br>
<em>&gt; scientific rationalist philosophy.  None of these religious folks -- or very
</em><br>
<em>&gt; few of them --care what statements you choose to accept...
</em><br>
<p>I still have responsibilities to them.  Anyway, from my own experience 
<br>
with people who are deeply or matter-of-factly religious, I don't think 
<br>
I believe you.  There might be some argument about the nature of AI, but 
<br>
if I were to avoid using the term &quot;AI&quot; and describe what a Friendly AI 
<br>
really is and how it would think, I think that most religious people 
<br>
would readily agree that a Friendly AI ought to be of their religion.
<br>
<p><em>&gt;&gt;Whether I could convince a
</em><br>
<em>&gt;&gt;rabbi of that in advance is a separate issue, but it does, in
</em><br>
<em>&gt;&gt;fact, happen
</em><br>
<em>&gt;&gt;to be true, and *that's* the important thing from the perspective of
</em><br>
<em>&gt;&gt;safeguarding the integrity of the Singularity, regardless of how it plays
</em><br>
<em>&gt;&gt;out in pre-Singularity politics.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So the important thing to you, is that the Singularity has integrity
</em><br>
<em>&gt; according to your scientific rationalist belief system.  Fine.
</em><br>
<em> &gt;
</em><br>
<em>&gt; This doesn't mean the Singularity will have integrity according to the
</em><br>
<em>&gt; belief systems of the vast majority of people on the world.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't see how this doesn't constitute &quot;imposing your morality on the
</em><br>
<em>&gt; world.&quot;  In my view, it does.  What you're saying is basically that you want
</em><br>
<em>&gt; to ensure the Singularity is good according to your standards, where your
</em><br>
<em>&gt; standards have to do with a kind of rationalistic &quot;integrity&quot; that you (but
</em><br>
<em>&gt; not most others) see as extremely valuable.
</em><br>
<p>Ben, imagine what kind of precautions you would ask a Catholic 
<br>
programming a Friendly AI to take in order to ensure that the AI would 
<br>
eventually convert to atheism, given that atheism is correct.  Now do 
<br>
that yourself.  What does this have to do with &quot;scientific rationalism&quot;?
<br>
<p><em>&gt;&gt;And they would, for that matter, rightly scream their
</em><br>
<em>&gt;&gt;heads off
</em><br>
<em>&gt;&gt;if SIAI created an AI that was given atheism as an absolute premise, the
</em><br>
<em>&gt;&gt;verbal formulation of rational empiricism as an absolute premise, or if
</em><br>
<em>&gt;&gt;there was in any other way created an AI that could not perceive the
</em><br>
<em>&gt;&gt;rightness of religion XYZ even if XYZ were true.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The AGI that I create is going to have a bias toward rationality and toward
</em><br>
<em>&gt; empiricism, because these are my values and those of the rest of the
</em><br>
<em>&gt; Novamente team.  Not an *absolutely bias*, but a bias.  When it's young, I'm
</em><br>
<em>&gt; going to teach it scientific knowledge *as probable though not definite
</em><br>
<em>&gt; truth*, and I'm going to show it the Koran as an example of an intersting
</em><br>
<em>&gt; human belief system.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Individuals who believe the scientific perspective is fundamentally wrong,
</em><br>
<em>&gt; might be offended by this, but that's just life....  I am not going to teach
</em><br>
<em>&gt; Novababy that the Koran and Torah and Vedas are just as valid as science,
</em><br>
<em>&gt; just in order to please others with these other belief systems.  Of course,
</em><br>
<em>&gt; I will also teach Novababy to think for itself, and once it becomes smarter
</em><br>
<em>&gt; than me (or maybe before) it will come to its own conclusions, directed by
</em><br>
<em>&gt; the initial conditions I've given it, but not constrained by them in any
</em><br>
<em>&gt; absolute sense.
</em><br>
<p>I think that asking how to ensure that an AI created by atheists would 
<br>
converge to a religion, given that this religion is correct, is a 
<br>
necessary exercise for understanding how an AI can repair whatever deep 
<br>
flaws may very well exist in our own worldviews.  In this sense, I think 
<br>
that refusing to put yourself in the shoes of a Christian building an AI 
<br>
and asking what would be &quot;fair&quot; is not just a matter of pre-Singularity 
<br>
politics.  It is a test - and not all that stringent a test, at that - 
<br>
of an AI's ability to transcend the mistakes of its programmers.  If you 
<br>
don't want to apply this test, what are you going to use instead?
<br>
<p><em>&gt;&gt;  All sentient life has value, and so does the volition of that life.
</em><br>
<em>&gt;
</em><br>
<em>&gt; And this is your personal value system, not a universal one... in my own
</em><br>
<em>&gt; value system, nonsentient life also has a lot of value....  This is a common
</em><br>
<em>&gt; perspective, though not universal.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'm not saying you don't value nonsentient life at all, but the fact that
</em><br>
<em>&gt; you omitted to mention it, suggests that maybe it's not as important to you
</em><br>
<em>&gt; as it is to me.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; These variations among individual value systems may possibly be passed along
</em><br>
<em>&gt; to the first AGI's.  If the first AGI is raised by a nature-lover, it may be
</em><br>
<em>&gt; less likely to grow up to destroy forests.
</em><br>
<p>I view it as an unbearably horrifying possibility that the next billion 
<br>
years of humanity's existence may be substantially different depending 
<br>
on whether the first AGI was raised by an environmentalist.  It's 
<br>
equally horrifying whether you're an environmentalist looking at Eliezer 
<br>
or vice versa.  It shouldn't depend on who happens to build the AI, it 
<br>
should depend on *who's right*.  If nobody's right then the choice 
<br>
should be kicked back to the individual.  If there's no way to do that 
<br>
then you might as well take a majority vote of the existing humans or 
<br>
pick a choice that's as good as any other; in absolutely no case should 
<br>
the programmers occupy a priviliged position with respect to an AI that 
<br>
may end up carrying the weight of the Singularity.
<br>
<p><em>&gt;&gt; If the
</em><br>
<em>&gt;&gt;AI *is* sensitive to your purpose, then I am worried what other
</em><br>
<em>&gt;&gt;things might
</em><br>
<em>&gt;&gt;be in your selfish interest, if you think it's valid for an AI to
</em><br>
<em>&gt;&gt;have goals
</em><br>
<em>&gt;&gt;that serve Ben Goertzel but not the human species.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The point is not that I want the AI to have goals that serve me but not the
</em><br>
<em>&gt; human species.  I'm not *that* selfish or greedy.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The point is that I am going to teach a baby AGI one particular vision of
</em><br>
<em>&gt; what best serves the human species, which is different from the vision that
</em><br>
<em>&gt; many other humans have about what best serves the human species.
</em><br>
<p>If your view is no better than anyone else's, why not ask the AI to pick 
<br>
a view at random, or ask the AI to do what a majority vote determines? 
<br>
I think that a majority vote is an absolute last resort, by the way, but 
<br>
it's still better than seizing power as an individual.
<br>
<p><em>&gt; I do not think there is any way to get around this subjectivity.  By
</em><br>
<em>&gt; involving others in the Novababy teaching process, I think I will avoid the
</em><br>
<em>&gt; most detailed specifics of my personal morality from becoming important to
</em><br>
<em>&gt; the AGI.  However, there will be no Taliban members in the Novababy teaching
</em><br>
<em>&gt; team, nor any Vodou houngans most likely (though I do know one, so it's
</em><br>
<em>&gt; actually a possibility ;)....  Novababy will be taught *one particular
</em><br>
<em>&gt; version* of how to best serve the human species, and then as it grows it
</em><br>
<em>&gt; will develop its own ideas...
</em><br>
<p>Develop its own ideas from where?  How?  Why?  Every physical event has 
<br>
a physical cause.  There are causes for humans developing their own 
<br>
ideas as they grow up, most of them evolved.  You are standing not only 
<br>
&quot;in loco parentis&quot; but &quot;in loco evolution&quot; to your AI.  What causes will 
<br>
you give Novamente to develop its own ideas?
<br>
<p><em>&gt;&gt; &gt; But I want it to place Humans
</em><br>
<em>&gt;&gt; &gt; pretty high on its moral scale -- initially, right up there at the top.
</em><br>
<em>&gt;&gt; &gt; This is Partiality not Impartiality, as I see it.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Don't you think there's a deadly sort of cosmic hubris in creating an AI
</em><br>
<em>&gt;&gt;that does something you personally know is wrong?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Not a deadly sort.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think there is a lot of hubris in creating an AGI, or launching a
</em><br>
<em>&gt; Singularity, in the first place, yeah.  It's a gutsy thing to try to do.
</em><br>
<p>That's certainly one motive which could contribute to launching a 
<br>
Singularity.  It's not the only motive.  I think that a Friendly AI 
<br>
would tell me to do the same thing.  For that rather, quite a large 
<br>
number of people who *aren't* me have told me to get out there and 
<br>
launch a Singularity as soon as possible; relatively few of them are 
<br>
donors to the Singularity Institute, which is kind of depressing, but 
<br>
it's at least suggestive evidence that no personal hubris needs to be 
<br>
involved.  For humanity at this point in history it's Singularity or 
<br>
bust, and someone has to do it.  &quot;Someone has to do it&quot; should generally 
<br>
be followed by the statement &quot;And I volunteer&quot;, but that's just a 
<br>
personal opinion.
<br>
<p><em>&gt; And I do not intend to create an AI to do something I know is wrong.  Quite
</em><br>
<em>&gt; the contrary, I intend to create an AI that initially embodies roughly the
</em><br>
<em>&gt; same morals as myself and my social group (modern rationalist,
</em><br>
<em>&gt; scientifically-minded transhumanists, with a respect for life and diversity,
</em><br>
<em>&gt; and a particular fondness for the human species and other Earthly
</em><br>
<em>&gt; life-forms).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think it's a lot safer to start a baby AGI off with the moral system that
</em><br>
<em>&gt; I and my colleagues hold, than to start it off with some abstract moral
</em><br>
<em>&gt; system that values humans only because they're sentient life forms.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I do have a selfish interest here: I want me and the rest of the human
</em><br>
<em>&gt; species to continue to exist.  I want this *separately* from my desire for
</em><br>
<em>&gt; sentience and life generally to flourish.  And I intend to embed this
</em><br>
<em>&gt; species-selfish interest into my AGI to whatever extent is possible.
</em><br>
<p>Ben, to the best of my ability to tell, the abilities an AI would use to 
<br>
grow beyond its programmers' and the abilities an AI would use to 
<br>
correct horrifying errors by its programmers are exactly the same 
<br>
structurally.  Your &quot;pseudo-selfish&quot; attitude here - i.e, that it's okay 
<br>
to program an AI with altruism that is just yours - endangers the AI's 
<br>
possession of even that altruism.
<br>
<p>The moral hubris of &quot;pseudo-selfish&quot; AI creation can have very real and 
<br>
very drastic consequences even under your own morality.
<br>
<p><em>&gt;&gt;Okay.  That last point there?  That's the point I'm concerned
</em><br>
<em>&gt;&gt;about - when
</em><br>
<em>&gt;&gt;the FAI gets *that* smart.  At *that* point I want the FAI to
</em><br>
<em>&gt;&gt;have the same
</em><br>
<em>&gt;&gt;kind of morality as, say, a human upload who has gotten *that*
</em><br>
<em>&gt;&gt;smart.  I do
</em><br>
<em>&gt;&gt;not think that a human upload who has gotten *that* smart would
</em><br>
<em>&gt;&gt;have human
</em><br>
<em>&gt;&gt;ethics but I don't think they would be the ethics that a rock or
</em><br>
<em>&gt;&gt;a bacterium
</em><br>
<em>&gt;&gt;would have, either.  Human ethics have the potential to grow;
</em><br>
<em>&gt;&gt;*that* is why
</em><br>
<em>&gt;&gt;an FAI needs human ethics *to start with*.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Right, we agree on all this, but the problem you don't seem to understand is
</em><br>
<em>&gt; that there is NO SUCH THING as &quot;human ethics&quot; generally speaking.  Human
</em><br>
<em>&gt; ethics are all over the map.  Are you a vegetarian?  No, then your ethics
</em><br>
<em>&gt; are very different from that of a lot of the world's population.  Do you go
</em><br>
<em>&gt; to the doctor when you're sick?  Oops, according to Christian Scientists,
</em><br>
<em>&gt; that's immoral, it's against God's Law....  Etc. etc. etc. etc. etc. etc.
</em><br>
<em>&gt; etc.
</em><br>
<p>Of course humans argue about everything.  The question is which of these 
<br>
answers is *right*.  If your answer is no righter than anyone else's 
<br>
then how dare you impose it on the Singularity?  Why wouldn't anyone 
<br>
else in the world be justly outraged at such a thing?  Letting everyone 
<br>
pick their own solutions whenever possible is one answer.
<br>
<p><em>&gt; An AGI cannot be started off with generic human ethics because there aren't
</em><br>
<em>&gt; any.  Like it or not, it's got to be started out with some particular form
</em><br>
<em>&gt; of human ethics.  Gee, I'll choose something resembling mine rather than
</em><br>
<em>&gt; Mary Baker Eddy's, because I don't want the AGI to think initially that
</em><br>
<em>&gt; medical intervention in human illnesses is immoral...
</em><br>
<p>And you think these two positions are equally right?
<br>
<p><em>&gt;&gt; &gt; We need to hard-wire and/or emphatically teach the system that our own
</em><br>
<em>&gt;&gt; &gt; human-valuing ethics are the correct ones,
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Are they?
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Let's ask that first,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Look, humans have been debating ethics for millennia.  No consensus has been
</em><br>
<em>&gt; reached.
</em><br>
<p>Under the BPT, the fact that no consensus has been reached indicates 
<br>
that no correct answer exists only if we would expect, given that a 
<br>
correct answer exists, for consensus to be reached.  We still don't have 
<br>
universal consensus that the Earth is not flat.  Guess what?  It's still 
<br>
not flat.
<br>
<p><em>&gt; There is no rational way to decide which ethical system is &quot;correct.&quot;
</em><br>
<em>&gt; Rather, ethical systems DEFINE what is &quot;correct&quot; -- not based on reasoning
</em><br>
<em>&gt; from any premises, just by decision.
</em><br>
<p>Hm.  According to you, people sure do spend a lot of time arguing about 
<br>
things that they should just be deciding by fiat.  In fact, everyone 
<br>
except a relative handful of cultural relativists - a tiny minority of 
<br>
humanity, in other words - seems to instinctively treat ethics as if it 
<br>
were governed directly by the correspondence theory of truth.  Why is 
<br>
that, do you suppose?
<br>
<p><em>&gt; From a rational/empirical perspective, the choice between ethical systems is
</em><br>
<em>&gt; an arbitrary one.  From other perspectives, it's not arbitrary at all --
</em><br>
<em>&gt; religious folks may say that the correct ethical system can be *felt* if you
</em><br>
<em>&gt; open up your inner mind in the right way.
</em><br>
<p>I beg your pardon?  Since when does rational empiricism assert that the 
<br>
choice between ethical systems is arbitrary?  I'm a rational empiricist 
<br>
and I assert no such thing.
<br>
<p><em>&gt; I can see that these notions of reason versus experience, axioms versus
</em><br>
<em>&gt; derivations, and so forth, may be perceived by an superhuman AGI as just so
</em><br>
<em>&gt; much primitive silliness....  But within the scope of human thought, there
</em><br>
<em>&gt; is no way we're gonna rationally decide which ethics are correct.  Ethics is
</em><br>
<em>&gt; not that sort of thing.
</em><br>
<p>We aren't dealing with human thought any more.  We're dealing with 
<br>
humans creating a seed AI that goes forth to become transhuman, and 
<br>
asking what happens then.
<br>
<p><em>&gt;&gt;The question of what you need to supply an AI with so that it
</em><br>
<em>&gt;&gt;*can* outgrow
</em><br>
<em>&gt;&gt;its teachings - not just end up in some random part of the space of
</em><br>
<em>&gt;&gt;minds-in-general, but actually *outgrow* the teachings it started with,
</em><br>
<em>&gt;&gt;after the fashion of say a human upload - is exactly the issue here.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; No, that is not the qeustion of Friendly AI, that is simply the question of
</em><br>
<em>&gt; AGI.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Any AGI worthy of the name will  be able to outgrow its initial teachings.
</em><br>
<em>&gt; Even humans can largely outgrow their initial teachings.  You have outgrown
</em><br>
<em>&gt; a lot of yours, huh?
</em><br>
<p>Hm.  It seems like you simultaneously believe:
<br>
<p>(a) there are correct answers for questions of simple fact and that any 
<br>
AGI should be able to easily outgrow programmer-supplied wrong answers 
<br>
for questions of simple fact
<br>
(b) ethical questions are fundamentally different from questions of 
<br>
simple fact because no correct answers exist
<br>
(c) an AGI should be able outgrow programmer-supplied ethics as easily 
<br>
as it outgrows programmer-supplied facts; in fact, this has nothing to 
<br>
do with Friendly AI but is simply a question of AI
<br>
<p>I can see how (a) (!b) (c) go together but not how (a) (b) (c) go 
<br>
together.  If you assert (b) then the human ability to outgrow 
<br>
parentally inculcated ethics would depend on evolved functionality above 
<br>
and beyond generic rationality.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4560.html">Brian Atkins: "Re: Military Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="4558.html">James Higgins: "Re: Who will launch the Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4554.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4569.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Reply:</strong> <a href="4569.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4559">[ date ]</a>
<a href="index.html#4559">[ thread ]</a>
<a href="subject.html#4559">[ subject ]</a>
<a href="author.html#4559">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
