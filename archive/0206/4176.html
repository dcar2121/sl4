<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: How hard a Singularity?</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="Re: How hard a Singularity?">
<meta name="Date" content="2002-06-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: How hard a Singularity?</h1>
<!-- received="Sun Jun 23 05:28:56 2002" -->
<!-- isoreceived="20020623112856" -->
<!-- sent="Sat, 22 Jun 2002 18:34:51 -0700" -->
<!-- isosent="20020623013451" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="Re: How hard a Singularity?" -->
<!-- id="4.3.2.7.2.20020622182540.00ce0d28@mail.earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D14FBC4.9010803@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=Re:%20How%20hard%20a%20Singularity?"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Sat Jun 22 2002 - 19:34:51 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4177.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4175.html">James Higgins: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4162.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4183.html">Mike & Donna Deering: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4183.html">Mike & Donna Deering: "Re: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4176">[ date ]</a>
<a href="index.html#4176">[ thread ]</a>
<a href="subject.html#4176">[ subject ]</a>
<a href="author.html#4176">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
(last post for awhile - I promise; see what happens when I don't read the 
<br>
list for a year...)
<br>
<p>At 06:35 PM 6/22/2002 -0400, you wrote:
<br>
<em>&gt;Ben Goertzel wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&gt; That there is *nothing special* about human-equivalent intelligence!
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Wrong! There is something VERY special about human-level intelligence, in
</em><br>
<em>&gt; &gt;  the context of a seed AI that is initially created and taught by humans.
</em><br>
<em>&gt; &gt;  Human-level intelligence is the intelligence level of the AI's creators
</em><br>
<em>&gt; &gt; and teachers.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Our seed AI is going to get its human-level intelligence, not purely by
</em><br>
<em>&gt; &gt; its own efforts, but largely based on the human-level intelligence of
</em><br>
<em>&gt; &gt; millions of humans working over years/decades/centuries.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Ah.  Well, again we have very different models of intelligence.  I don't 
</em><br>
<em>&gt;think you can use human knowledge as the mindstuff of an AI.  I don't 
</em><br>
<em>&gt;think AI can be built by borrowing human content.  What we're after is 
</em><br>
<em>&gt;what produced that content.  One is building brainware and, in the early 
</em><br>
<em>&gt;stages, guiding that brainware into creating content.  There may be some 
</em><br>
<em>&gt;opportunities to nudge the AI into creating better content than she 
</em><br>
<em>&gt;otherwise would have, but this violates the self-encapsulation rule of 
</em><br>
<em>&gt;seed AI; an AI doesn't own a thought until she can produce it on her own. 
</em><br>
<em>&gt;Initially almost all of AI development will violate this rule, of course, 
</em><br>
<em>&gt;but the measure of how far you've gotten in constructing real, independent 
</em><br>
<em>&gt;AI, an AI that has her own thoughts and isn't just churning through yours, 
</em><br>
<em>&gt;is not having to do this any more.
</em><br>
<p>I think a good model to use is humans.  The human mind has been virtually 
<br>
unchanged for thousands of years, yet we make much, much more progress in 
<br>
one day today than was made in thousands of years 10,000 years 
<br>
ago.  Why?  Becuase we, collectively, know more and are taught more in our 
<br>
early years.  Does this make a human from today smarter than a human who 
<br>
lived 10,000 years ago?  I guess that would depend on your criteria.
<br>
<p>In order to educate the AI you must teach it using human knowledge.  Unless 
<br>
you expect it to go from nothing to high-level and assembly programming on 
<br>
complex microprocessors on its own (hope you've figured in decades for 
<br>
that).  &quot;Human knowledge&quot; is the basis and springboard from which an AI 
<br>
will start out.  You could have an AI that was 4x as intelligent as a 
<br>
human, but if it starts with a completely blank slate it could take 
<br>
centuries to just catch up to our present knowledge.  Intelligence without 
<br>
knowledge is useless.
<br>
<p><em>&gt;So you look at pouring human content into an AI, and say, &quot;When we reach 
</em><br>
<em>&gt;human-level, we will run out of mindstuff.&quot;  And I look at creating AI as 
</em><br>
<em>&gt;the task of building more and more of that essential spark that *creates* 
</em><br>
<em>&gt;content - with the transfer of any content the AI could not have created 
</em><br>
<em>&gt;on her own, basically a bootstrap method or side issue - and say:  &quot;When 
</em><br>
<em>&gt;the AI reaches human level, she will be able to swallow the thoughts that 
</em><br>
<em>&gt;went into her own creation; she will be able to improve her own spark, 
</em><br>
<em>&gt;recursively.&quot;
</em><br>
<p>You wouldn't run out of human knowledge until well after the AI exceeded 
<br>
human-level intelligence (no one human can know even 1% of all that is 
<br>
known today).
<br>
<p><em>&gt;(Hm.  I think ve/vis/ver was better after all.)
</em><br>
<p>Many of us, however, don't.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4177.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4175.html">James Higgins: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4162.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4183.html">Mike & Donna Deering: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4183.html">Mike & Donna Deering: "Re: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4176">[ date ]</a>
<a href="index.html#4176">[ thread ]</a>
<a href="subject.html#4176">[ subject ]</a>
<a href="author.html#4176">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
