<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: How hard a Singularity?</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: How hard a Singularity?">
<meta name="Date" content="2002-06-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: How hard a Singularity?</h1>
<!-- received="Sat Jun 22 15:47:08 2002" -->
<!-- isoreceived="20020622214708" -->
<!-- sent="Sat, 22 Jun 2002 15:38:24 -0400" -->
<!-- isosent="20020622193824" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: How hard a Singularity?" -->
<!-- id="3D14D230.1080007@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJGEEICLAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20How%20hard%20a%20Singularity?"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Jun 22 2002 - 13:38:24 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4141.html">Smigrodzki, Rafal: "RE: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4139.html">Mike & Donna Deering: "Re: The Human Brain."</a>
<li><strong>In reply to:</strong> <a href="4138.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4142.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4142.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4157.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4140">[ date ]</a>
<a href="index.html#4140">[ thread ]</a>
<a href="subject.html#4140">[ subject ]</a>
<a href="author.html#4140">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em> &gt;
</em><br>
<em> &gt;&gt; I suppose I could see a month, but anything longer than that is pretty
</em><br>
<em> &gt;&gt; hard to imagine unless the human-level AI is operating at a subjective
</em><br>
<em> &gt;&gt; slowdown of hundreds to one relative to human thought.
</em><br>
<em> &gt;
</em><br>
<em> &gt; I understand that this is your intuition, but what is the reasoning
</em><br>
<em> &gt; underlying it?
</em><br>
<p>That there is *nothing special* about human-equivalent intelligence!
<br>
<p>&nbsp;From LOGI:
<br>
<em> &gt;&gt;
</em><br>
<em>&gt;&gt; Once AI exists it can develop in a number of different ways; for an AI to
</em><br>
<em>&gt;&gt; develop to the point of human-equivalence and then remain at the point of
</em><br>
<em>&gt;&gt; human-equivalence for an extended period would require that all liberties
</em><br>
<em>&gt;&gt; be simultaneously blocked at exactly the level which happens to be
</em><br>
<em>&gt;&gt; occupied by Homo sapiens sapiens.  This is too much coincidence.  Again,
</em><br>
<em>&gt;&gt; we observe Homo sapiens sapiens intelligence in our vicinity, not because
</em><br>
<em>&gt;&gt; Homo sapiens sapiens represents a basic limit, but because Homo sapiens
</em><br>
<em>&gt;&gt; sapiens is the very first hominid subspecies to cross the minimum line
</em><br>
<em>&gt;&gt; that permits the development of evolutionary psychologists.
</em><br>
<p><em> &gt; Say we have this AI mind with a nonhuman intelligence,  roughly as smart
</em><br>
<em> &gt; as Ben or Eliezer.  Say this AI mind already uses a huge amount of
</em><br>
<em> &gt; computational resources, and obtaining more rapidly is not financially
</em><br>
<em> &gt; possible.
</em><br>
<p>You are now furthermore assuming that our AI can find no sufficiently 
<br>
remunerative employment, cannot borrow sufficient funding, cannot get a 
<br>
large number of donated cycles from interested scientists, cannot rent a 
<br>
computing grid for long enough to expand its mind and reengineer itself, 
<br>
cannot (or chooses not) to steal cycles, cannot design new hardware...
<br>
<p>The problem, as I said, is that for an AI to bottleneck at human 
<br>
intelligence all liberties must be *simultaneously* blocked.
<br>
<p><em> &gt; This mind now has to re-engineer its software to make itself smarter.
</em><br>
<p>By hypothesis, the AI just made the leap to human-equivalent smartness.  We 
<br>
know from evolutionary experience that this is a highly significant 
<br>
threshold that opens up a lot of doors.  Self-improvement should be going 
<br>
sixty at this point.
<br>
<p><em> &gt; Maybe there are only a limited number of tweaks it can make to improve
</em><br>
<em> &gt; its intelligence, without totally rearchitecting itself.
</em><br>
<p>Then why wouldn't it totally rearchitect itself?
<br>
<p><em> &gt; So, with these tweaks, it becomes a bit smarter than Ben or Eliezer.
</em><br>
<em> &gt;
</em><br>
<em> &gt; OK, what's next?
</em><br>
<p>Every single decision that Ben or Eliezer made, while creating the AI, 
<br>
becomes open to reconsideration at that higher level of intelligence.
<br>
<p><em> &gt; It has to completely rearchitect itself, i.e. come up
</em><br>
<em> &gt; with a new and better AI design.  Furthermore, it doesn't have that much
</em><br>
<em> &gt; hardware available for experimentation, unless it wants to cannibalize
</em><br>
<em> &gt; its own mind-hardware...
</em><br>
<p>It can experiment on arbitrarily small pieces of itself.
<br>
<p><em> &gt; Where do you come up with a &quot;one month upper bound&quot; for this
</em><br>
<em> &gt; rearchitecture process?
</em><br>
<em> &gt;
</em><br>
<em> &gt; I think a one month estimate is plausible, but I don't see why &quot;anything
</em><br>
<em> &gt; longer than that&quot; should be &quot;hard to imagine.&quot;
</em><br>
<p>Because of what I see as the earthshattering impact of an AI transforming 
<br>
itself to one intelligence grade level above &quot;Ben or Eliezer&quot;.  The doors 
<br>
opened by this should be more than enough to take the AI to serious 
<br>
transhumanity.  In many ways humans are *wimps*, *especially* when it comes 
<br>
to code!  I just don't see it taking all that much effort to beat the pants 
<br>
off us *at AI design*.
<br>
<p>Perhaps your differing intuition on this has to do with your belief that 
<br>
there is a simple mathematical essence to intelligence; you are looking at 
<br>
this supposed essence and saying &quot;How the heck would I re-engineer whatever 
<br>
the mathematical essence turns out to be?  It's an arbitrarily hard problem; 
<br>
we know nothing about it.&quot;  But I do not believe intelligence has a simple 
<br>
mathematical essence.  I am looking at the complex system which implements 
<br>
human intelligence and saying:  &quot;I can see how this system produces 
<br>
intelligence, and it's a beautiful piece of crap, but it's still a piece of 
<br>
crap.&quot;
<br>
<p><em> &gt; Maybe it won't go this way -- maybe no conceptual/mathematical/AI-design
</em><br>
<em> &gt; hurdles will be faced by a human-level AI seeking to make itself vastly
</em><br>
<em> &gt; superhuman.  Or maybe turning a human-level mind into a vastly superhuman
</em><br>
<em> &gt;  mind will turn out to be a hard scientific problem, which takes our
</em><br>
<em> &gt; human-level AI a nontrivial period of time to solve....
</em><br>
<p>Which all sounds reasonable until you realize that there's nothing special 
<br>
about &quot;human-level&quot; intelligence.  If, under our uncertainty, the AI 
<br>
trajectory with a big bottleneck between &quot;human-level&quot; and &quot;superhuman&quot; 
<br>
intelligence is plausible, then the 40 other trajectories with big 
<br>
bottlenecks between various degrees of infrahuman and transhuman AI are 
<br>
equally plausible.  From *our* perspective this looks like a direct jump 
<br>
from human-equivalent to transhuman AI.  Arguing a privileged bottleneck at 
<br>
the human level is *not* just as plausible as anything else, just as 
<br>
Pascal's Wager involving a privileged Christian God is a rationalization 
<br>
rather than a good bet.
<br>
<p><em> &gt;&gt; Even if your goal is to progress exponentially in enlightened spiritual
</em><br>
<em> &gt;&gt;  directions, exponential physical progress is still a good way to get
</em><br>
<em> &gt;&gt; the computing power to support that enlightened spiritual stuff and
</em><br>
<em> &gt;&gt; bring others in on the fun.
</em><br>
<em> &gt;
</em><br>
<em> &gt; Perhaps, or perhaps not.  Perhaps the super-AI will realize that more
</em><br>
<em> &gt; brainpower and more knowledge are not the path to greater wisdom ...
</em><br>
<em> &gt; perhaps it will decide it's more important to let some of its
</em><br>
<em> &gt; subprocesses run for a few thousand years and see how they come out!
</em><br>
<p>Okay, now you say that and see something we &quot;just don't know&quot;.  I hear you 
<br>
say that and what I see are a specific, highly anthropmorphic and even 
<br>
contemporary-culture-morphic memes about &quot;wisdom&quot;, and how wisdom relates to 
<br>
ostentatious ignorance of material things, the wisdom of inaction, stopping 
<br>
to eat the roses, and so on.
<br>
<p><em> &gt; We don't yet fully understand how hard the scientific problem of creating
</em><br>
<em> &gt; a human-level AI is.  And we don't yet fully understand how hard the
</em><br>
<em> &gt; scientific problem of transforming a human-level AI into a vastly
</em><br>
<em> &gt; superhuman-level AI is.  Until we understand these things, we can't
</em><br>
<em> &gt; forecast the end-game of the path to the Singularity in any detail,
</em><br>
<em> &gt; though we can certainly huff and puff about it a lot should we find such
</em><br>
<em> &gt; an occupation entertaining...
</em><br>
<p>By asking about &quot;human-level&quot; and not any of the twenty surrounding equally 
<br>
plausible intelligence gradients, you are manipulating your uncertainty to 
<br>
support one answer, just as Pascal expressed his uncertainty about the 
<br>
existence of the Christian God and not the millions of other possible deities.
<br>
<p>I see your uncertainty about AIs chanting mantras in the same way; you're 
<br>
being &quot;uncertain&quot; about something that has very specific cultural imagery 
<br>
behind it.
<br>
<p>Uncertainty is very easy to manipulate, and it's very easy to get away with 
<br>
socially.  Can't be anything wrong with admitting you don't know, right?  So 
<br>
why not admit that you don't know whether the stars control your fate, and 
<br>
read your horoscope just in case?  I don't understand how you can be so 
<br>
&quot;dead certain&quot; about such a thing when millions of people disagree with you...
<br>
<p>Being &quot;uncertain&quot; is the easy way out.  &quot;Uncertainty abuse&quot; is a major 
<br>
source of modern-day irrationality.  It's socially acceptable and is 
<br>
frequently mistaken for rationality, which makes it doubly dangerous.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4141.html">Smigrodzki, Rafal: "RE: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4139.html">Mike & Donna Deering: "Re: The Human Brain."</a>
<li><strong>In reply to:</strong> <a href="4138.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4142.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4142.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4157.html">Ben Goertzel: "RE: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4140">[ date ]</a>
<a href="index.html#4140">[ thread ]</a>
<a href="subject.html#4140">[ subject ]</a>
<a href="author.html#4140">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
