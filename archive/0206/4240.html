<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Seed AI (was: How hard a Singularity?)</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="Re: Seed AI (was: How hard a Singularity?)">
<meta name="Date" content="2002-06-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Seed AI (was: How hard a Singularity?)</h1>
<!-- received="Sun Jun 23 15:42:00 2002" -->
<!-- isoreceived="20020623214200" -->
<!-- sent="Sun, 23 Jun 2002 12:32:57 -0700" -->
<!-- isosent="20020623193257" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="Re: Seed AI (was: How hard a Singularity?)" -->
<!-- id="4.3.2.7.2.20020623121254.0250d6f0@mail.earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D160BDA.6020202@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=Re:%20Seed%20AI%20(was:%20How%20hard%20a%20Singularity?)"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Sun Jun 23 2002 - 13:32:57 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4241.html">Aaron McBride: "RE: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4239.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>In reply to:</strong> <a href="4219.html">Eliezer S. Yudkowsky: "Re: Seed AI (was: How hard a Singularity?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4245.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4245.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="../0207/4632.html">Gordon Worley: "Re: Seed AI (was: How hard a Singularity?)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4240">[ date ]</a>
<a href="index.html#4240">[ thread ]</a>
<a href="subject.html#4240">[ subject ]</a>
<a href="author.html#4240">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
At 01:56 PM 6/23/2002 -0400, Eliezer wrote:
<br>
<em>&gt;Ben Goertzel wrote:
</em><br>
<em>&gt;&gt;Of course, this general statement is not true.  Often, in software
</em><br>
<em>&gt;&gt;engineering and other kinds of engineering, a very complex design is HARDER
</em><br>
<em>&gt;&gt;to improve than a simple one.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Evolution managed to sneak around this trap.  An AI team will have to do 
</em><br>
<em>&gt;so as well; for example, through constructing plugin satisficing architectures
</em><br>
<p>Yes, it did, given massive amounts of time.  We are not talking about 
<br>
hundreds of thousands of years or more, though...
<br>
<p><em>&gt;&gt;I doubt this is how things will go.  I think human knowledge will be
</em><br>
<em>&gt;&gt;comprehensible by an AI *well before* the AI is capable of drastically
</em><br>
<em>&gt;&gt;modifying its own sourcecode in the interest of vastly increased
</em><br>
<em>&gt;&gt;intelligence.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I would expect the AI's understanding of source code to run well ahead of 
</em><br>
<em>&gt;its understanding of human language at any given point.  The AI lives 
</em><br>
<em>&gt;right next to source code; human language is located in another galaxy by 
</em><br>
<em>&gt;comparison.
</em><br>
<p>There is a universe between understanding source code and understanding 
<br>
what changes will improve your intelligence, however.  Certainly it could 
<br>
write a software application, but knowing what changes will improve its own 
<br>
intelligence is a whole different story.
<br>
<p><em>&gt;&gt;I think that humans will teach the AGI more than just &quot;domain problems at
</em><br>
<em>&gt;&gt;the right level,&quot; I think that by cooperatively solving problems together
</em><br>
<em>&gt;&gt;with the AGI, humans will teach it a network of interrelated
</em><br>
<em>&gt;&gt;thought-patterns.  Just as we learn from other humans via interacting with
</em><br>
<em>&gt;&gt;them.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I'm not sure we learn thought-patterns, whatever those are, from other 
</em><br>
<em>&gt;humans; but if so, it's because evolution explicitly designed us to do so. 
</em><br>
<em>&gt;Standing 'in loco evolution' to an AI, you need to know what 
</em><br>
<em>&gt;thought-patterns are, how they work, and what brainware mechanisms and 
</em><br>
<em>&gt;biases support the learning of which thought-patterns from what kind of 
</em><br>
<em>&gt;environmental cues.
</em><br>
<p>Lets use &quot;design patterns&quot; in software development as an 
<br>
example.  Understanding the use of design patterns offers a major 
<br>
improvement in software architecture &amp; engineering capability.  These days 
<br>
I use this type of thinking extensively when designing systems.  However, I 
<br>
can easily remember a time before I knew of (or thought along the lines of) 
<br>
design patterns.  Thus, I have obviously learned to think in this 
<br>
way.  Moving from procedural to object oriented programming also required a 
<br>
major shift in thinking (it took me roughly a year to digest and comprehend 
<br>
WHY it was good and how it was useful).  But today I can't imagine not 
<br>
thinking in OOP terms.  Thus we can learn thought-patterns via human 
<br>
knowledge and interaction.  Unfortunately, I believe that we don't 
<br>
understand the most basic &amp; powerful thought patterns which are used by the 
<br>
human mind (because they are below the conscious level).
<br>
<p><em>&gt;&gt;I agree, it does not mean that an AI *must* do so.  However, I hypothesize
</em><br>
<em>&gt;&gt;that to allow an AI to learn its initial thought-patterns from humans based
</em><br>
<em>&gt;&gt;on experiential interaction, is
</em><br>
<em>&gt;&gt;a) the fastest way to get to an AGI
</em><br>
<em>&gt;&gt;b) the best way to get an AGI that has a basic empathy for humans
</em><br>
<em>&gt;
</em><br>
<em>&gt;Empathy is a good analogy, unfortunately.  Humans are socialized by 
</em><br>
<em>&gt;interacting with other humans because we are *explicitly evolutionarily 
</em><br>
<em>&gt;programmed* to be socialized in this way.  We don't pick up empathy as an 
</em><br>
<em>&gt;emergent result of our interaction with other humans.  Empathy is hardwired.
</em><br>
<p>Um, I don't know that I completely agree with that.  It is possible for the 
<br>
degree of empathy a given human has to change over time.  Thus I would not 
<br>
say that it is hardwired.
<br>
<p><em>&gt;It may be hardwired in such a way that it depends on environmental human 
</em><br>
<em>&gt;interaction in order to develop, but this does not make it any less 
</em><br>
<em>&gt;hardwired.  An AGI is not going to automatically pick up basic human 
</em><br>
<em>&gt;empathy from interacting with humans any more than a rock would develop 
</em><br>
<em>&gt;empathy for humans if constantly passed from hand to hand.  *Nothing* in 
</em><br>
<em>&gt;AI is automatic.  Not morality, not implicit transfer of thought-patterns, 
</em><br>
<em>&gt;not socialization,
</em><br>
<em>&gt;*nada*.  If you don't know how it works, it won't!
</em><br>
<p>Well, unless empathy was a natural consequences of general 
<br>
intelligence.  Can you prove otherwise?  The same goes for the other 
<br>
aspects you mention.  They may not be &quot;automatic&quot;, but no one really knows 
<br>
either way.  If you feel strongly one way or the other feel free to say so, 
<br>
but be aware (and make others aware) that it is purely your &quot;feeling&quot; and 
<br>
is not based on any factual evidence what-so-ever.
<br>
<p><em>&gt;&gt;Yes, you see more &quot;code self-modification&quot; occurring at the
</em><br>
<em>&gt;&gt;&quot;pre-human-level-AI&quot; phase than I do.
</em><br>
<em>&gt;&gt;This is because I see &quot;intelligent goal-directed code self-modification&quot; as
</em><br>
<em>&gt;&gt;being a very hard problem, harder than mastering human language, for
</em><br>
<em>&gt;&gt;example.
</em><br>
<em>&gt;
</em><br>
<em>&gt;This honestly strikes me as extremely odd.  Code is vastly easier to 
</em><br>
<em>&gt;experiment with than human language; the AI can accumulate more experience 
</em><br>
<em>&gt;faster; there are no outside references to the black-box external world; 
</em><br>
<em>&gt;the AI can find its own solutions rather than needing the human one; and 
</em><br>
<em>&gt;the AI can use its own concepts to think rather than needing to manipulate 
</em><br>
<em>&gt;human-sized concepts specialized for human modalities that the AI may not 
</em><br>
<em>&gt;even have.  Code is not easy but I'd expect to be a heck of a lot easier 
</em><br>
<em>&gt;than language.
</em><br>
<p>But the code for an extremely complex system is very, very hard to 
<br>
experiment with in any *intelligent* manor.  Much as I imagine that if you 
<br>
gave all the code to a working pre-human AI to a &quot;typical&quot; programmer they 
<br>
would have little or no idea what to do with it.  They could run it and 
<br>
poke around at it a bit, but making an improvement would be by pure luck, 
<br>
not design.  And, if the matter is further complicated by having to clear 
<br>
reference point by which to gain improvement vs setback, progress becomes 
<br>
very difficult (at best).
<br>
<p>You seem to frequently miss the fact that there is a vast difference 
<br>
between code and the system the code implements.
<br>
<p>James Higgins
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4241.html">Aaron McBride: "RE: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4239.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>In reply to:</strong> <a href="4219.html">Eliezer S. Yudkowsky: "Re: Seed AI (was: How hard a Singularity?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4245.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="4245.html">Ben Goertzel: "RE: Seed AI (was: How hard a Singularity?)"</a>
<li><strong>Reply:</strong> <a href="../0207/4632.html">Gordon Worley: "Re: Seed AI (was: How hard a Singularity?)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4240">[ date ]</a>
<a href="index.html#4240">[ thread ]</a>
<a href="subject.html#4240">[ subject ]</a>
<a href="author.html#4240">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
