<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: FAI means no programmer-sensitive AI morality</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: FAI means no programmer-sensitive AI morality">
<meta name="Date" content="2002-06-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: FAI means no programmer-sensitive AI morality</h1>
<!-- received="Sat Jun 29 11:45:10 2002" -->
<!-- isoreceived="20020629174510" -->
<!-- sent="Sat, 29 Jun 2002 11:38:13 -0400" -->
<!-- isosent="20020629153813" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: FAI means no programmer-sensitive AI morality" -->
<!-- id="3D1DD465.5090604@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3D1D4C8D.90002@objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20FAI%20means%20no%20programmer-sensitive%20AI%20morality"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Jun 29 2002 - 09:38:13 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4554.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Previous message:</strong> <a href="4552.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>In reply to:</strong> <a href="4539.html">Samantha Atkins: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4552.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4553">[ date ]</a>
<a href="index.html#4553">[ thread ]</a>
<a href="subject.html#4553">[ subject ]</a>
<a href="author.html#4553">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Samantha Atkins wrote:
<br>
<em> &gt;
</em><br>
<em> &gt; Ben Goertzel wrote:
</em><br>
<em> &gt;
</em><br>
<em> &gt;&gt;&gt; But it should be equally *true* for every individual, whether or not
</em><br>
<em> &gt;&gt;&gt; the individual realizes it in advance, that they have nothing to fear
</em><br>
<em> &gt;&gt;&gt;  from the AI being influenced by the programmers.  An AI programmer
</em><br>
<em> &gt;&gt;&gt; should be able to say to anyone, whether atheist, Protestant,
</em><br>
<em> &gt;&gt;&gt; Catholic, Buddhist, Muslim, Jew, et cetera:  &quot;If you are right and I
</em><br>
<em> &gt;&gt;&gt; am wrong then the AI will agree with you, not me.&quot;
</em><br>
<em> &gt;
</em><br>
<em> &gt; Of course some breeds of religious people would simply claim that unless
</em><br>
<em> &gt; the AI has an immortal soul (or Buddha nature) and it is capable of
</em><br>
<em> &gt; communion with the Holy Ghost or some such that it cannot know about
</em><br>
<em> &gt; these religious matters at all.   As you say below, a non-empiricist
</em><br>
<em> &gt; element.
</em><br>
<p>I don't see why a Friendly AI would have trouble handling that case in the 
<br>
event it turned out to be true.  I say &quot;Show kindness toward all sentient 
<br>
creatures&quot; and &quot;Hunt down the reasons I made that statement&quot;.  The AI grows 
<br>
up, scans my physical brainstate, and finds that I have an immortal soul 
<br>
which contributed to the generation of that statement.  At which point the 
<br>
AI could pray for an immortal soul from God, merge with a human so that the 
<br>
combined entity would have an immortal soul, say &quot;AI? Forget it! Wrong 
<br>
future!&quot; and terminate after constructing a few human-enhancement kiosks, et 
<br>
cetera.
<br>
<p>We are perhaps extraordinarily unlikely to find ourselves in that particular 
<br>
strange situation, but might very well end up in one even stranger.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4554.html">Ben Goertzel: "RE: FAI means no programmer-sensitive AI morality"</a>
<li><strong>Previous message:</strong> <a href="4552.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<li><strong>In reply to:</strong> <a href="4539.html">Samantha Atkins: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4552.html">Eliezer S. Yudkowsky: "Re: FAI means no programmer-sensitive AI morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4553">[ date ]</a>
<a href="index.html#4553">[ thread ]</a>
<a href="subject.html#4553">[ subject ]</a>
<a href="author.html#4553">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
