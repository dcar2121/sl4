<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: QUES: CFAI</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: QUES: CFAI">
<meta name="Date" content="2002-06-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: QUES: CFAI</h1>
<!-- received="Mon Jun 17 03:46:31 2002" -->
<!-- isoreceived="20020617094631" -->
<!-- sent="Mon, 17 Jun 2002 03:34:01 -0400" -->
<!-- isosent="20020617073401" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: QUES: CFAI" -->
<!-- id="3D0D90E9.9020606@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="F35KQu1bCdUg07MJJQ8000002bf@hotmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20QUES:%20CFAI"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Jun 17 2002 - 01:34:01 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4057.html">Mitch Howe: "Eclectic Pseudoplague"</a>
<li><strong>Previous message:</strong> <a href="4055.html">Anand AI: "RES&lt;: Leitl's objections to non-brute force seed AI and Friendliness"</a>
<li><strong>In reply to:</strong> <a href="4031.html">Anand AI: "QUES: CFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4250.html">Anand: "Re: QUES: CFAI +"</a>
<li><strong>Reply:</strong> <a href="4250.html">Anand: "Re: QUES: CFAI +"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4056">[ date ]</a>
<a href="index.html#4056">[ thread ]</a>
<a href="subject.html#4056">[ subject ]</a>
<a href="author.html#4056">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Anand AI wrote:
<br>
<em>&gt; 01. Does CFAI argue for a set of panhuman characteristics that comrpise
</em><br>
<em>&gt; human moral cognition? If so, what characteristics do we have evidence for,
</em><br>
<em>&gt; and what characteristics of human moral cognition will be reproduced?
</em><br>
<p>CFAI argues that there exists *some* set of panhuman characteristics, but 
<br>
does not argue for a *specific* set of panhuman characteristics.  The model 
<br>
of Friendliness learning is based on reasoning backward from observed 
<br>
specific humans to a systemic model of altruism which is grounded in 
<br>
panhuman characteristics (and, if necessary, social and memetic 
<br>
organizational processes).  In other words, the idea is not that *you*, the 
<br>
programmer, know how to build a model of altruism which is 
<br>
programmer-independent, but that you, the programmer, know how to build an 
<br>
AI which can arrive at such a model, given sufficient intelligence, and can 
<br>
rely on the interim approximation represented by the ethics of several 
<br>
specific programmers, given insufficient intelligence.
<br>
<p>There often seems to be some confusion about the question to which &quot;Creating 
<br>
Friendly AI&quot; is intended as an answer.  In science fiction and the popular 
<br>
press, a question often raised is &quot;How do we know our AIs won't turn on us 
<br>
and kill us?&quot;  CFAI happens to provide some answers for this question, but 
<br>
that's not the question CFAI is intended to answer.  The question CFAI is 
<br>
intended to answer is:  &quot;How can you make sure that it doesn't matter who 
<br>
the programmers are?&quot;  Or to be more precise:  &quot;Is there a strategy, of 
<br>
bounded complexity, which if followed arrives at the same optimal good AI 
<br>
regardless of who builds and teaches it?&quot;  This general form contains within 
<br>
it some critical subproblems of the moral philosophy of AI creation:
<br>
<p>1)  In building a seed AI, you may (or may not) be building something 
<br>
eternal - something that has a beginning, but not an end, and no end of 
<br>
consequences.  The AI itself might be eternal, or it might make a choice 
<br>
that has an eternal effect; the morality is the same.  If there's a 
<br>
sensitive dependency of the goodness of the outcome on small variances in 
<br>
the initial conditions, then *any* compromise from absolute perfection of 
<br>
the *programmers* represents an existential risk.  While I can imagine 
<br>
someone arguing that it might be a moral necessity to build something 
<br>
imperfect and eternal if the alternative is the complete extinction of 
<br>
humanity, I am currently more inclined to label any permanent compromise of 
<br>
humanity's potential absolutely unacceptable.  If you can't build something 
<br>
eternal and optimal, don't build something eternal.
<br>
<p>2)  In building a seed AI, a small group of programmers are standing in as a 
<br>
proxy for humanity.  The Singularity is something that belongs to humanity 
<br>
and it would be - under my understanding of moral philosophy - deeply 
<br>
immoral to steal it.  This is the wellspring of a seed AI programmer's 
<br>
professional ethics.  One of the panhuman atoms from which individual moral 
<br>
philosophies are built is the act we call &quot;empathy&quot; or &quot;sympathy&quot;; taking 
<br>
another agent's viewpoint; putting yourself in someone else's shoes.  If 
<br>
there is a privileged correlation of the programmers' moral patterns with 
<br>
the AI's moral pattern, this means it is no longer possible for an external 
<br>
observer to put himself or herself in the shoes of an AI programmer, or vice 
<br>
versa.  If so you cannot be standing in as a proxy for humanity; you are not 
<br>
building transhumanity; you are stealing it.  You have to pick a strategy 
<br>
such that you'd be comfortable with a different group of programmers 
<br>
following it, then follow that strategy yourself.  If there's a central 
<br>
optimum you have to aim for the optimum.  If there's a space of optima with 
<br>
no central point (a question Rafal once raised) then the best you can do is 
<br>
probably picking out a &quot;typical&quot; point in that space.  Think of the 
<br>
structural core of Friendliness as a (specific, fleshed-out) way of saying: 
<br>
&nbsp;&nbsp;&quot;Be the AI that the best possible programmer would have built.&quot;  If there 
<br>
is no one &quot;best programmer&quot; under the definition, just a space of equal 
<br>
optima, then you have to be comfortable saying to the AI:  &quot;Be the AI that a 
<br>
typical 'optimal' programmer would have built, without reference to where I 
<br>
myself happen to be relative to that space.&quot;  Because from the perspective 
<br>
of an external observer, that's what you're asking them to accept.
<br>
<p><em>&gt; 02. Why is volition-based Friendliness the assumed model of Friendliness
</em><br>
<em>&gt; content? What will it and what will it not constitute and allow? If the
</em><br>
<em>&gt; model is entirely incorrect, how is this predicted to affect the AI's
</em><br>
<em>&gt; architecture?
</em><br>
<p>Volition-based Friendliness is the best model of morality Yudkowsky-2001 
<br>
could come up with and is still current as of Yudkowsky-2002.  As for &quot;What 
<br>
will it and what will it not constitute and allow?&quot;, I would suggest asking 
<br>
specific questions and looking over the specific answers to see what pattern 
<br>
is present, since this is how volition-based Friendliness would be passed 
<br>
along to a Friendly AI.
<br>
<p>Remember, however, that by the Law of Programmer Symmetry - if I may call it 
<br>
such - volition-based Friendliness is not the problem.  The problem is 
<br>
coming up with a strategy such that if some other programming team follows 
<br>
it, their AI will eventually arrive at volition-based Friendliness [or 
<br>
something better] regardless of what their programmers started out 
<br>
believing.  And to do that you have to pass along to the AI an understanding 
<br>
of how people argue about morality, in a semantics rich enough to represent 
<br>
all the structural properties thereof.
<br>
<p>In terms of the actual moral philosophy behind volition-based morality - 
<br>
well, let me throw this over to the next question:
<br>
<p><em>&gt; 03. What alternatives to volition-based Friendliness have been considered,
</em><br>
<em>&gt; and why were they not chosen?
</em><br>
<p>Why volition-based morality?  Well, previously, I had a more informal model 
<br>
of a concrete morality based on an appreciation of life, truth, and joy. 
<br>
(Incidentally, I'm sorry if I start sounding unbearably goody two-shoes 
<br>
during any of this, but Anand has asked a direct question and a straight 
<br>
answer takes precedence over the usual social rules about self-deprecation.) 
<br>
&nbsp;&nbsp;The question, as I see it, is whether appreciating life, truth, and joy is 
<br>
a universal or something about which individuals may legitimately disagree. 
<br>
&nbsp;&nbsp;Absent an objective morality (Friendliness can handle this too, BTW) it 
<br>
seems to me that it is something about which individuals may legitimately 
<br>
disagree, and that if someone says &quot;I want to die,&quot; their opinion on this 
<br>
overrides what I see as the value of life.  The shift in basic values might 
<br>
be described as seeing *freedom* as the central good, with the goodness of 
<br>
life, truth, and joy being special cases of my freedom to value these things.
<br>
<p>The next moral question is whether a Friendly AI should value *only* freedom 
<br>
or whether a Friendly AI should also value life, truth, and joy.  The 
<br>
structural power of Friendly AI means that the programmers don't necessarily 
<br>
have to answer this question *correctly* - but Friendly AI *does* require 
<br>
that the programmers do their best job to answer the question as such, so 
<br>
that the AI gets a chance to see what kind of cognitive forces are involved 
<br>
in producing a *concrete* moral answer and not just the meta-moral answer of 
<br>
&quot;Let an SI figure it out.&quot;  So what's the best answer I can come up with? 
<br>
Currently I'm leaning slightly away from the &quot;pure&quot; volition-based 
<br>
Friendliness expressed in CFAI and toward a Friendly AI that respects 
<br>
freedom but also has its own conception of a moral good.  The Law of 
<br>
Programmer Symmetry says that I should only do this if I'd be comfortable 
<br>
with someone else using the same strategy to create a FAI that respected my 
<br>
freedom but also had morals whose content might differ from my own, and that 
<br>
the FAI can't actually get the morals directly from me using this method.
<br>
<p>Two possibilities, failing objective morality or a unique attractor, are (a) 
<br>
that extra-volitional morality is determined by majority vote of the 
<br>
extra-volitional moralities of everyone involved with a Friendly AI playing 
<br>
a given social role, or alternatively (the first alternative may not be 
<br>
self-consistent) that the FAI has a &quot;typical&quot; personal morality selected 
<br>
from a space of optimal moralities that has no central point.  Currently I 
<br>
am leaning toward the second alternative on the grounds that a Friendly AI 
<br>
should be a human-equivalent philosopher; I'm not sure that going along with 
<br>
a majority vote - as the ultimate cognitive grounding of morality, rather 
<br>
than because you respect majority votes - is cognitively the same thing as 
<br>
having your own morality.  (This is also about achieving &quot;upload 
<br>
equivalence&quot;; the system embodied by a Friendly seed AI has to be at least 
<br>
as good, heading into the Singularity, as the system embodied by any upload 
<br>
or social structure of uploads.  An upload would have a growable personal 
<br>
morality that was &quot;owned&quot; by the upload and not borrowed from someone else.)
<br>
<p>A Friendly AI embodies a system that produces morality.  The CFAI semantics 
<br>
are supposed to be expansive enough to create an AI that learns, embodies, 
<br>
and improves-under-its-own-rules *any* system that produces morality - an 
<br>
individual programmer, a group of programmers, or a planet that evolves an 
<br>
ecology in which evolves a species that passes around memes that are 
<br>
eventually picked up by a programmer who sets out to create an AI.  You can 
<br>
dig back as far into the past light cone as seems philosophically necessary 
<br>
- given enough intelligence to infer the events of interest, which doesn't 
<br>
currently seem to require unreasonable intelligence (the events of interest 
<br>
take place on a tractably high level of abstraction; it doesn't require 
<br>
knowing the past position of individual atoms or anything of the sort).
<br>
<p>So from this perspective, the question is whether the AI is *modeling* a 
<br>
moral system or *being* a moral system.  Should a mature FAI, dealing with a 
<br>
question of truth or falsity, model the factual answer a human would arrive 
<br>
at and then model the moral judgement a human would make based on that 
<br>
factual answer?  Or should a mature FAI actually use its *own* model of the 
<br>
world to arrive at the best factual answer the *FAI* knows of, wherever a 
<br>
judgement of desirability relies on a question of fact?  Once you step into 
<br>
the second territory, the AI is starting to cross the line from *modeling* a 
<br>
moral system into *being* a moral entity.  I think this is what we want and 
<br>
it's why I keep using phrases like &quot;building an independent moral 
<br>
philosopher&quot;.  But it also looks to me like this may imply that to build a 
<br>
self-consistent moral philosophy, it has to be an individual philosophy - it 
<br>
can't be the borrowed (modeled) moral philosophy of a group.  If I value 
<br>
life, truth, joy, and freedom and choose to make freedom primary, then it 
<br>
may not be possible to self-consistently build, within the human frame of 
<br>
reference, an FAI that values freedom *only*.  You have to convey the 
<br>
process that arrives at the decision, not the decision itself, and the 
<br>
question is which we should identify the AI's mind with.
<br>
<p><em>&gt; 04. How will the AI know and decide what constitutes &quot;normativeness&quot;?
</em><br>
<p>According to the above, in the case of questions of fact, &quot;normative&quot; is the 
<br>
*AI's* best model of the facts themselves, not the AI's best judgement of 
<br>
what an idealized human would think about the facts.  In this case there is 
<br>
no doubt that the AI &quot;owns&quot; this portion of the moral philosophy and is not 
<br>
borrowing it.  Some other things may need to start out as being borrowed, 
<br>
though, because without a good model the AI can't make the final decision as 
<br>
to which parts of the model to identify with.  What would be borrowed in 
<br>
this way, and finally - when the AI was sure - absorbed?  What the AI starts 
<br>
out by borrowing is the final, surface decisions of the programmers, which 
<br>
can be learned experientially, and moreover can be filled in, if blanks are 
<br>
encountered, by directly asking the programmers.  But this doesn't define 
<br>
what's normative, and it isn't something that an FAI would finally identify 
<br>
with and absorb as its own - as a final output, maybe, if the programmers 
<br>
are *right*, but not as the actual system of moral philosophy.
<br>
<p>So the FAI starts digging into the programmers' past light cones to arrive 
<br>
at a model of where the morality given it came from.  At first what the FAI 
<br>
ends up with is just a model of the programmers' thoughts - the proximal 
<br>
causes of the programmers' statements.  Most of this will also be individual 
<br>
material, and hence not something an FAI could absorb.  But some of the 
<br>
elements that play a role in the production of moral thoughts may be 
<br>
emotions, panhuman chunks of brainware.  Let's temporarily suppose that an 
<br>
emotion is something which, in a certain context, recognizes certain kinds 
<br>
of thoughts, binds to those thoughts, and shades those thoughts in certain 
<br>
ways that may make them directly joyful or sorrowful, prideful or shameful, 
<br>
or the other various kinds of subjective negative and positive feedback 
<br>
that, in various ways, uplift certain thoughts and cast others down.  Within 
<br>
a certain context, the thought of helping someone else - altruism - is 
<br>
joyful.  If the FAI is modeling an FAI programmer, the matter is probably 
<br>
more complicated than that because the programmer may be aware of the 
<br>
emotion and using it deliberately, or the programmer may have been 
<br>
influenced by this emotion in childhood to choose a moral philosophy of 
<br>
altruism in which altruism is *not* dependent on the contextual conditions 
<br>
that are necessary to activate the emotion.
<br>
<p>The point is that, at this point in the construction of the model, there is 
<br>
an 'atom' of morality that is not unique to the programmer.  The atom is not 
<br>
a moral judgement.  The programmer's moral judgements are made by very 
<br>
complex 'molecules' that bear the individual signature of the programmer, 
<br>
but one of the 'atoms' happens to be panhuman.  Maybe the strength of the 
<br>
atom is an individual variable, probably distributed along a gaussian curve 
<br>
as most quantitative individual variables are, in which case you might 
<br>
either substitute an 'average' strength for the altruism emotion, or arrive 
<br>
at the judgement that 'more altruism is better' and select a value for the 
<br>
altruism emotion from the far right of the curve, or off the curve, or a 
<br>
'maximal' value if that works.  This doesn't necessarily mean the FAI has 
<br>
'absorbed' the emotion - just that the FAI is trying to model the production 
<br>
of altruism through a causal system in which this emotion is an element, and 
<br>
testing out what happens if you replace the programmer's settings for this 
<br>
emotion with &quot;typical&quot; or &quot;maximal&quot; settings.  If the FAI replaces the 
<br>
programmer's factory settings for the altruism emotion with &quot;maximal&quot; 
<br>
settings, and the end result is recognized by that programmer or by the 
<br>
other programmers as a stronger and more altruistic philosophy, then the FAI 
<br>
may decide to take a tentative step backward from the experientially learned 
<br>
final outputs of the individual programmers and say that the programmers' 
<br>
statements are moral because they are altruistic, rather than altruism being 
<br>
interesting because it plays a role in the programmers' statements.  This is 
<br>
how an FAI would start to work its way back from the programmers to 
<br>
humanity.  It is also how the programmers would begin to learn to trust the 
<br>
FAI's moral judgement over their own.  It might or might not be possible to 
<br>
take further steps backward into the past light cone; to ask whether 
<br>
altruism itself has &quot;evolved wrong&quot; or could have evolved better, for example.
<br>
<p>So the question is, how does an FAI decide when to take a step backward in 
<br>
the chain of causality?  Under causal validity semantics, when do you move 
<br>
back the acausal level another notch?  At first, &quot;normativeness&quot; in this 
<br>
case might be operationally defined as &quot;what the programmers say is 
<br>
normative&quot;, but this is also something where the system that produces the 
<br>
programmers' judgements can be deduced by examining those judgements.  At 
<br>
some point, the programmers acknowledge that the AI's judgement of what is 
<br>
&quot;normative&quot; is better than the programmers' judgement of what is normative. 
<br>
&nbsp;&nbsp;At the point where the AI's judgement of normativeness, and the AI's 
<br>
judgement of Friendliness system architecture, and the AI's judgement of 
<br>
morality, all appear to the programmers to be of transhuman competency, it 
<br>
would be time (perhaps past time) to &quot;launch&quot; the AI.  At this point the AI 
<br>
might not have finished implementing the Law of Programmer Symmetry - 
<br>
fulfilling the wish &quot;Be the best AI that we could possibly have designed&quot; - 
<br>
but you would have to rely on the AI to decide how to ground itself in a 
<br>
programmer-independent way.
<br>
<p>Much of the thinking I have been describing so far is thinking that could be 
<br>
described to a comparatively young AI, but which it would take a very mature 
<br>
intelligence to implement.  If the critical flashpoint of a seed AI is 
<br>
substantially infrahuman intelligence, there would be either the option of 
<br>
cooperative ascent so that the AI can actually talk to the programmers' and 
<br>
check the programmers' judgement, or the option of trying to describe the 
<br>
structural properties of the entire Friendliness development scenario above, 
<br>
to a young AI, in sufficient detail that the AI could grow to transhuman 
<br>
intelligence with much of the 'target' of Friendliness still undefined, then 
<br>
use that transhuman intelligence to simulate a typical Friendliness 
<br>
development scenario and thereby define the target.  Both of these scenarios 
<br>
have certain risks.  I think that perhaps the critical pragmatic challenge 
<br>
of Friendly AI will be creating the &quot;definition of the definition of the 
<br>
definition&quot; of Friendliness in such a way that a very young AI can not only 
<br>
be given the definition, but that the young AI can actually *practice* 
<br>
&quot;filling out definitions of definitions of definitions&quot;, so that you can see 
<br>
whether the AI might be able to fill out the definition of the definition of 
<br>
the definition of Friendliness - to what extent one would need a cooperative 
<br>
ascent, or alternatively be able to go directly into the &quot;throw&quot; and &quot;catch&quot; 
<br>
of a seed AI racing full speed ahead with an incompletely filled-out but 
<br>
structurally complete model of Friendliness.
<br>
<p>I hope at least part of this email was not total gibberish.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4057.html">Mitch Howe: "Eclectic Pseudoplague"</a>
<li><strong>Previous message:</strong> <a href="4055.html">Anand AI: "RES&lt;: Leitl's objections to non-brute force seed AI and Friendliness"</a>
<li><strong>In reply to:</strong> <a href="4031.html">Anand AI: "QUES: CFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4250.html">Anand: "Re: QUES: CFAI +"</a>
<li><strong>Reply:</strong> <a href="4250.html">Anand: "Re: QUES: CFAI +"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4056">[ date ]</a>
<a href="index.html#4056">[ thread ]</a>
<a href="subject.html#4056">[ subject ]</a>
<a href="author.html#4056">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
