<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: How hard a Singularity?</title>
<meta name="Author" content="Eugen Leitl (eugen@leitl.org)">
<meta name="Subject" content="Re: How hard a Singularity?">
<meta name="Date" content="2002-06-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: How hard a Singularity?</h1>
<!-- received="Sat Jun 22 09:29:01 2002" -->
<!-- isoreceived="20020622152901" -->
<!-- sent="Sat, 22 Jun 2002 15:13:34 +0200 (CEST)" -->
<!-- isosent="20020622131334" -->
<!-- name="Eugen Leitl" -->
<!-- email="eugen@leitl.org" -->
<!-- subject="Re: How hard a Singularity?" -->
<!-- id="Pine.LNX.4.33.0206221400290.507-100000@hydrogen.leitl.org" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3D146377.5030809@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eugen Leitl (<a href="mailto:eugen@leitl.org?Subject=Re:%20How%20hard%20a%20Singularity?"><em>eugen@leitl.org</em></a>)<br>
<strong>Date:</strong> Sat Jun 22 2002 - 07:13:34 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="4133.html">Michael Roy Ames: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4131.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4131.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4133.html">Michael Roy Ames: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4133.html">Michael Roy Ames: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4135.html">Brian Atkins: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4163.html">James Higgins: "Re: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4132">[ date ]</a>
<a href="index.html#4132">[ thread ]</a>
<a href="subject.html#4132">[ subject ]</a>
<a href="author.html#4132">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Sat, 22 Jun 2002, Eliezer S. Yudkowsky wrote:
<br>
<p><em>&gt;  &gt; Considerable leverage is available to people to inhibit the kinetics 
</em><br>
<em>&gt;  &gt; of early stages via legacy methods.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Name EVEN ONE method.
</em><br>
<p>Isn't it quite obvious? Laws and their enforcement.
<br>
<p>If you're working with radioactive materials, especially fissibles, nerve
<br>
agents, pathogens, recombinant DNA you're subject to them. I distinctly
<br>
hope that anything involving molecular self-replication in free
<br>
environment and ~human level naturally intelligent systems will see heavy
<br>
regulation, at least initially.
<br>
<p>Maybe your AI will turn out to be all love and light. Then maybe not.
<br>
<p><em>&gt;  &gt; No such leverage is available for later stages. This is our window of
</em><br>
<em>&gt;  &gt; operation to reduce our vulnerabilities by addressing some of our key
</em><br>
<em>&gt;  &gt; limitations.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; How?  I've been asking you this for the last few years and have yet to
</em><br>
<em>&gt; receive a straight answer.  Pardon me; I got a straight answer over IRC
</em><br>
<p>This is hardly accurate. You haven't been asking for the last few years,
<br>
and whenever you've asked I answered (unless unable to due to real life
<br>
intrusions). Perhaps you're just not listening.
<br>
<p><em>&gt; once, which you later disclaimed as soon as I mentioned it.
</em><br>
<p>Once again, we're talking about avoiding specific developments. The
<br>
relevant threat for this forum is runaway superintelligence, a Singularity
<br>
turned Blight and death of us all due to side effects.
<br>
<p>Don't expect a specific scenario. The more specific it is, the more
<br>
irrelevant. What we could do is draft broad guidelines, which necessarily
<br>
need to be adaptive in nature.
<br>
<p>I told you the inhibiting aspects: regulating, tracking, enforcing. This
<br>
is not pretty, nor does it guarantee 100% success, but it's a lot better
<br>
than nothing. Talk to your friendly molecular biologist working in Level 3
<br>
facilities how they're coping with the regulation load. Remember that the
<br>
temporal scope of the regulations is limited, the hard limit being the
<br>
late stages of Singularity, which will shrug off any regulations imposed
<br>
by previour players due to raising power gradient.
<br>
<p>The positive aspects involve a number of issues. As you frequently 
<br>
mention, people are dying. We need to address this immediately with life 
<br>
extension and validation of cryonics and deploying this on a large scale.
<br>
<p>This is a stopgap measure, the long shot is individual molecular therapies 
<br>
and medical nanotechnology. This issues so far are about stabilizing life,
<br>
the next facet is enhancement.
<br>
<p>The Achilles heel of our limited adaptability is biology. We need to 
<br>
either enhance this substrate, or switch to a new one in a discontinous 
<br>
process. At this stage of the game it is too early to tell which of these 
<br>
approaches will prevail. I tended to favour discontinuous migration, but 
<br>
the complexities of biology make it extremely demanding to model. The 
<br>
general objective is to become indistinguishable from the enemy, albeit in 
<br>
a slow, gradual process with minimal amount of dead people on our hands.
<br>
<p>However, it is too early to tell. We need to sample as many paths as 
<br>
possible, given the limit on time and financial resources.
<br>
<p>There's more, but I've got something to do this Saturday afternoon.
<br>
&nbsp;
<br>
<em>&gt;  &gt; I have to disagree on the latter, since the
</em><br>
<em>&gt;  &gt; foothills (defined by presence of basically unmodified people) can be
</em><br>
<em>&gt;  &gt; obviously engineered.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; How?
</em><br>
<p>I've begun to answer this in the above. It would actually cast a good 
<br>
light on the transhumanist community if we once get off our collective 
<br>
asses and provide a set of policy guidelines before the likes of Fukuyama 
<br>
do. 
<br>
&nbsp;
<br>
<em>&gt; What concrete reason do you have for expecting a &quot;wonder&quot; in this case?
</em><br>
<p>I was being merely sarcastic. I don't expect any wonders.
<br>
&nbsp;
<br>
<em>&gt; I guess that makes &quot;human intelligence&quot; immoral, then, because I don't know
</em><br>
<em>&gt; of any path into the future that involves zero existential risk.
</em><br>
<p>Life is uncertain. It doesn't mean we should stroll into a minefield just 
<br>
because we can.
<br>
&nbsp;
<br>
<em>&gt;  &gt; I should hope not. It would seem to be much more ethical to offer
</em><br>
<em>&gt;  &gt; assistance to those yet unmodified to get onboard, while you're still
</em><br>
<em>&gt;  &gt; encrusted with the nicer human artifacts and the player delta has not yet
</em><br>
<em>&gt;  &gt; grown sufficiently large that empathy gets eroded into indifference.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You know, maybe I shouldn't mention this, since you'll probably choose to
</em><br>
<em>&gt; respond to it instead of my repeated questions for any concrete way of
</em><br>
<em>&gt; producing a soft Singularity; but if you believe that all altruism is
</em><br>
<p>I don't think the archives show many of your repeated questions which are 
<br>
not answered. I'd wish you'd stop claiming that. In this post alone you 
<br>
did that twice.
<br>
<p><em>&gt; irrational, why do you claim to be currently altruistic?  Do you see
</em><br>
<p>I claimed no such thing. 
<br>
<p>What I would claim (though I can present no evidence either way) is that
<br>
the human primate definitely shows capacity to act nice towards players
<br>
which are unable to meaningfully reciprocate (like helping a trapped bug).  
<br>
This strikes me as an evolutionary artifact, and irrational (at least I've
<br>
missed plausible explanations as to why this is compatible with rationally
<br>
selfish behaviour). I like this behaviour.
<br>
<p><em>&gt; yourself as having chosen altruism &quot;over&quot; rationality as the result of your
</em><br>
<p>I haven't chosen much, being raised a human. I don't know what your
<br>
definition of altruism is, so I can't say whether I'm an altruist, or not.
<br>
It seems that the ROI favours cooperative strategies of agents, if they're
<br>
smart and iterative. Latter both should increase considerably within our 
<br>
lifetimes, thus favouring more benign cooperation strategies.
<br>
<p>I tend to adhere to being nice to lesser beings (notice that I worked in 
<br>
an animal research facility, so clearly there are priorities), even if I'm 
<br>
not aware of a rational reason to do so. 
<br>
<p><em>&gt; &quot;legacy&quot; empathy?  I can't see trusting someone who sees the inside of their
</em><br>
<em>&gt; mind that way.
</em><br>
<p>I guess that's only fair, since I don't trust you either with FAI, or 
<br>
howewer that thing is called today.
<br>
<p>Lest anyone misinterpreted what I said: &quot;...while you're still encrusted
<br>
with the nicer human artifacts and the player delta has not yet grown
<br>
sufficiently large that empathy gets eroded into indifference.&quot;
<br>
<p>What I'm saying the rationally selfish strategy doesn't seem to favour
<br>
agents who engage in symmetric transactions with agents unable to
<br>
reciprocate. Since we're running risk of losing empathy with the rest of 
<br>
humanity when moving away via Lamarckian and Darwinian evolution it is 
<br>
imperative we make very good use of that empathy while it lasts.
<br>
<p>It may of course turn out that there's some uknown higher order ethics at 
<br>
play here, and sustainability and quality of empathy will be asserted. But 
<br>
since we don't know for certain we have to play safe.
<br>
<p><em>&gt; You have yet to give even a single reason why we should think earlier
</em><br>
<em>&gt; stages are controllable.  What is an &quot;inhibition agent&quot; and how does
</em><br>
<em>&gt; it differ from magical fairy dust?
</em><br>
<p>Fairies typically don't manifest as jackbooted thugs wielding projectile
<br>
weapons.
<br>
<p>The inhibition agent is a metaphor. Ever seen runaway polymerization? It's
<br>
a nucleated runaway (in large volume, due to reaction enthalpy) reaction.  
<br>
It's actually a good metaphor, because it has a plateau. Inhibitors
<br>
terminate nuclei. Concentration of inhibitor influences early stage of
<br>
kinetics.
<br>
&nbsp;
<br>
<em>&gt; They are at stake.  The slower the Singularity, the more die in the
</em><br>
<em>&gt; meanwhile; and all known proposals for attempting to deliberately slow the 
</em><br>
<p>We'll just have to agree that our reality model is different. I'd wish
<br>
we'd have validated cryonics, there's a considerable unknown lurking in
<br>
there about radical life extension approaches available to us today.
<br>
<p><em>&gt; Singularity increase total existential risk.  (I'm not talking about your 
</em><br>
<em>&gt; proposals, since you have to yet to make any, but rather the proposals of 
</em><br>
<p>I wonder whether you've been deleting all my posts on all those lists
<br>
we've been on for years. Since you failed to see a proposal (which is
<br>
lurking behind about every odd line).
<br>
<p><em>&gt; Bill Joy and the like.)
</em><br>
<p>What's Bill Joy's specific proposals? I haven't run into a concise list 
<br>
yet.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="4133.html">Michael Roy Ames: "Re: How hard a Singularity?"</a>
<li><strong>Previous message:</strong> <a href="4131.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<li><strong>In reply to:</strong> <a href="4131.html">Eliezer S. Yudkowsky: "Re: How hard a Singularity?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4133.html">Michael Roy Ames: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4133.html">Michael Roy Ames: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4135.html">Brian Atkins: "Re: How hard a Singularity?"</a>
<li><strong>Reply:</strong> <a href="4163.html">James Higgins: "Re: How hard a Singularity?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#4132">[ date ]</a>
<a href="index.html#4132">[ thread ]</a>
<a href="subject.html#4132">[ subject ]</a>
<a href="author.html#4132">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:39 MDT
</em></small></p>
</body>
</html>
