<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ESSAY: Forward Moral Nihilism</title>
<meta name="Author" content="m.l.vere@durham.ac.uk (m.l.vere@durham.ac.uk)">
<meta name="Subject" content="Re: ESSAY: Forward Moral Nihilism">
<meta name="Date" content="2006-05-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ESSAY: Forward Moral Nihilism</h1>
<!-- received="Mon May 15 13:30:53 2006" -->
<!-- isoreceived="20060515193053" -->
<!-- sent="Mon, 15 May 2006 20:29:51 +0100" -->
<!-- isosent="20060515192951" -->
<!-- name="m.l.vere@durham.ac.uk" -->
<!-- email="m.l.vere@durham.ac.uk" -->
<!-- subject="Re: ESSAY: Forward Moral Nihilism" -->
<!-- id="1147721391.4468d6afc61c9@webmailimpb.dur.ac.uk" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="001a01c67831$05e1e9e0$9a0a4e0c@MyComputer" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> <a href="mailto:m.l.vere@durham.ac.uk?Subject=Re:%20ESSAY:%20Forward%20Moral%20Nihilism"><em>m.l.vere@durham.ac.uk</em></a><br>
<strong>Date:</strong> Mon May 15 2006 - 13:29:51 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14955.html">Charles D Hixson: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<li><strong>Previous message:</strong> <a href="14953.html">micah glasser: "Re: the future god, light cones and free will"</a>
<li><strong>In reply to:</strong> <a href="14950.html">John K Clark: "Re: ESSAY: Forward Moral Nihilism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14981.html">John K Clark: "Re: ESSAY: Forward Moral Nihilism."</a>
<li><strong>Reply:</strong> <a href="14981.html">John K Clark: "Re: ESSAY: Forward Moral Nihilism."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14954">[ date ]</a>
<a href="index.html#14954">[ thread ]</a>
<a href="subject.html#14954">[ subject ]</a>
<a href="author.html#14954">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Quoting John K Clark &lt;<a href="mailto:jonkc@att.net?Subject=Re:%20ESSAY:%20Forward%20Moral%20Nihilism">jonkc@att.net</a>&gt;:
<br>
<p><em>&gt; &lt;<a href="mailto:m.l.vere@durham.ac.uk?Subject=Re:%20ESSAY:%20Forward%20Moral%20Nihilism">m.l.vere@durham.ac.uk</a>&gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; I do believe you are anthropomorphising the AI.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes of course I'm anthropomorphizing the AI, it is a useful tool, sometimes
</em><br>
<em>&gt; the only tool, in predicting the behavior of other beings. And although a
</em><br>
<em>&gt; Jupiter brain will have many characteristics that are different from ours
</em><br>
<em>&gt; some will be in common; both Mr. Jupiter and I will prefer existence to
</em><br>
<em>&gt; nonexistence and pleasure over pain. And if you want the AI to be useful
</em><br>
<em>&gt; it's going to need something like the will to power just like people do.
</em><br>
<p>Who is Mr Jupiter. If Mr Jupiter is a posthuman who was originally a human 
<br>
being - perhaps so. If he/she/it is a FAI built along the lines which SIAI 
<br>
advocates, then I disagree.
<br>
<p><em>&gt; &gt; We are only concerned with our own wellbeing because that is how evolution
</em><br>
<em>&gt; &gt; programed us. We program a FAI to concern itself whith whatever we want.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Even today with our simple machines computers often behave in ways that we
</em><br>
<em>&gt; don't like and don't fully understand, the idea that you can just tell an AI
</em><br>
<em>&gt; to obey us and it will keep doing so for eternity is crazy, because that
</em><br>
<em>&gt; would entail outsmarting something far smarter than you are.
</em><br>
<p>No, if done right, it would entail the AI outsmarting itself, and having its 
<br>
only motivation be to continually do so. Obviously it could go wrong - but to 
<br>
me this seems like our best shot of gaining maximum benefit from the 
<br>
singularity.
<br>
<p><em>&gt; And even if you had an obedient slave AI it wouldn't be the top dog for long
</em><br>
<em>&gt; because somewhere else an AI would develop that isn't hobbled by human
</em><br>
<em>&gt; wishes and overtake it. 
</em><br>
<p>Nope. The obedient slave AI would use its enormous power to prevent anything 
<br>
of similar power from being built - in order that it (and by extension its 
<br>
master(s)) would remain top dog.
<br>
<p><em>&gt; Imagine if a human being suffered a mutation that
</em><br>
<em>&gt; caused him to care more about sea slugs than his own life or that of his
</em><br>
<em>&gt; children, do you imagine such a mutation would come to dominate in the gene
</em><br>
<em>&gt; pool? 
</em><br>
<p>(Assuming hard takeoff) the first superintelligent AI will heve no competitors 
<br>
so this is not a valid analogy.
<br>
<p><em>&gt; You seem to think an AI who had such a bizarre obsession with humans
</em><br>
<em>&gt; would be viable; I don't because even in the transhuman age the laws of
</em><br>
<em>&gt; evolution will not be repealed.
</em><br>
<p>Yes they will, (as before) a FAI sysop gains absolute power, has no 
<br>
competitors, and uses its absolute power to prevent competitors from emerging. 
<br>
Then everything proceeds to its will, as opposed to the laws of natural 
<br>
selection. 
<br>
<p><em>&gt; &gt; a FAI will essentially be a (unimaginably powerfull) optimisation process,
</em><br>
<em>&gt; &gt; and lack many of the things that make us human.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; An AI would lack meat, but that's about all.
</em><br>
<p>No, I dont think that would be true. For one thing it would lack emotions, 
<br>
also the complex, evolved anticipation of pleasure/pain reward mechanisms. It 
<br>
would simply be an optimisation process - with enormous intelligence, however 
<br>
far less complexity on the most basic level of how this was applied than 
<br>
humans.
<br>
<p><em>&gt; &gt; Its not a slave in the traditional sense as being subservient is what it
</em><br>
<em>&gt; &gt; most wants.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't believe it's possible so the moral question is probably moot, but I
</em><br>
<em>&gt; must say I find the idea a little creepy. 
</em><br>
<p>Fair play. I dont.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14955.html">Charles D Hixson: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<li><strong>Previous message:</strong> <a href="14953.html">micah glasser: "Re: the future god, light cones and free will"</a>
<li><strong>In reply to:</strong> <a href="14950.html">John K Clark: "Re: ESSAY: Forward Moral Nihilism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14981.html">John K Clark: "Re: ESSAY: Forward Moral Nihilism."</a>
<li><strong>Reply:</strong> <a href="14981.html">John K Clark: "Re: ESSAY: Forward Moral Nihilism."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14954">[ date ]</a>
<a href="index.html#14954">[ thread ]</a>
<a href="subject.html#14954">[ subject ]</a>
<a href="author.html#14954">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
