<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Imposing ideas, eg: morality</title>
<meta name="Author" content="Tennessee Leeuwenburg (tennessee@tennessee.id.au)">
<meta name="Subject" content="Re: Imposing ideas, eg: morality">
<meta name="Date" content="2006-05-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Imposing ideas, eg: morality</h1>
<!-- received="Tue May 16 03:03:53 2006" -->
<!-- isoreceived="20060516090353" -->
<!-- sent="Tue, 16 May 2006 19:03:48 +1000" -->
<!-- isosent="20060516090348" -->
<!-- name="Tennessee Leeuwenburg" -->
<!-- email="tennessee@tennessee.id.au" -->
<!-- subject="Re: Imposing ideas, eg: morality" -->
<!-- id="43c8685c0605160203u384c683fmdee9025f5f66b7c3@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="f3afeba0605160027u1ad2d2b2w2afd74ec85b17dab@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tennessee Leeuwenburg (<a href="mailto:tennessee@tennessee.id.au?Subject=Re:%20Imposing%20ideas,%20eg:%20morality"><em>tennessee@tennessee.id.au</em></a>)<br>
<strong>Date:</strong> Tue May 16 2006 - 03:03:48 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14987.html">Dani Eder: "Re: Chimp genetics"</a>
<li><strong>Previous message:</strong> <a href="14985.html">Tennessee Leeuwenburg: "Re: Learning math..."</a>
<li><strong>In reply to:</strong> <a href="14983.html">Olie Lamb: "Imposing ideas, eg: morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14990.html">m.l.vere@durham.ac.uk: "Re: Imposing ideas, eg: morality"</a>
<li><strong>Reply:</strong> <a href="14990.html">m.l.vere@durham.ac.uk: "Re: Imposing ideas, eg: morality"</a>
<li><strong>Reply:</strong> <a href="15021.html">Olie Lamb: "Re: Imposing ideas, eg: morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14986">[ date ]</a>
<a href="index.html#14986">[ thread ]</a>
<a href="subject.html#14986">[ subject ]</a>
<a href="author.html#14986">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I propose that we take this the hell out of SL4 and into some
<br>
philoso-ranting list. I agree that it's relevant to the issue, but I don't
<br>
feel like it's got a damn thing to do with AGI beyond the obvious
<br>
fundamental debate on the issue.
<br>
<p>Here's my response.
<br>
<p>1.) Moral relatavism. Bummer, huh?
<br>
2.) AGIs have a lot of power, don't they?
<br>
3.) ???
<br>
4.) Profit
<br>
<p>I think that if you resolve the fundamental questions surrounding moral
<br>
relativism, you'll also resolve the AGI debate. If morality is objective,
<br>
and that's discoverably, provably so, then we have nothing we can rightfully
<br>
worry about from an AGI system.
<br>
<p>If morality is objective, but (a) hard to discover and (b) easy to mistake,
<br>
then we're in trouble. But we're less in trouble than an AGI, which is
<br>
presumably more able to deal with (a) and less likely to fall for (b).
<br>
<p>If morality is purely relative, then by definition we cannot instill an AGI
<br>
with our own morality (otherwise it would, from the perspective of the AGI
<br>
at least, be objective -- a given)
<br>
<p>Q1.) Can we take our relative morality and make it objective for the AGI?
<br>
Not without understanding AGI we can't. Hmmm, tough one.
<br>
Q2.) Can we force an AGI to value our relative morality, or at least our
<br>
desires? Not without understanding AGI we can't. Hmm, tough one.
<br>
<p>As you can see, this problem boils down to two things:
<br>
&nbsp;&nbsp;(a)  Debates about morality
<br>
&nbsp;&nbsp;(b)  Understanding AGI
<br>
<p>I suggest that (a) is not SL4 or easily solvable, and that (b) is what we
<br>
should concern ourself with?
<br>
<p>On 5/16/06, Olie Lamb &lt;<a href="mailto:neomorphy@gmail.com?Subject=Re:%20Imposing%20ideas,%20eg:%20morality">neomorphy@gmail.com</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; The old bugbear about letting people do what they like without
</em><br>
<em>&gt; imposing on others has reared its ugly head.
</em><br>
<em>&gt;
</em><br>
<em>&gt; In this case, rather than a set of rules for the people, it's an
</em><br>
<em>&gt; &quot;operational methodology&quot; for an expected superhuman intelligence,
</em><br>
<em>&gt; that might become a sysop.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Some ethicists have said that
</em><br>
<em>&gt;
</em><br>
<em>&gt; (**Stipulative characterisation**)
</em><br>
<em>&gt;
</em><br>
<em>&gt; Morals =  preferences that you want to apply to everyone.
</em><br>
<em>&gt;
</em><br>
<em>&gt; F'rinstance, if you don't like bullfighting because sport bores you,
</em><br>
<em>&gt; that's a matter of individual preference.  If you don't like anyone
</em><br>
<em>&gt; liking bullfighting because it hurts the bull, by the above
</em><br>
<em>&gt; definition, it's a &quot;moral statement&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; (Nb: this is NOT my definition.  I'm just using it for one post.)
</em><br>
<em>&gt;
</em><br>
<em>&gt; With this characterisation, it's very hard to imagine an
</em><br>
<em>&gt; anthropomorphic Sysop not effectively enforcing their &quot;morality&quot; on
</em><br>
<em>&gt; others.  Their operational methodology for weighing the requirements
</em><br>
<em>&gt; of conflicting expressed wills would, in effect, be the Sysop's
</em><br>
<em>&gt; &quot;morality&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Just say that a Sysop adopted <a href="mailto:m.l.vere@durham.ac.uk?Subject=Re:%20Imposing%20ideas,%20eg:%20morality">m.l.vere@durham.ac.uk</a> 's two axioms:
</em><br>
<em>&gt; &gt; 1. Prohibiting any action which affects another member of the group,
</em><br>
<em>&gt; &gt; unless that member has wilfully expressed for that action to be
</em><br>
<em>&gt; &gt; allowed (a form of domain protection).
</em><br>
<em>&gt;
</em><br>
<em>&gt; (Nb: can you say &quot;Golden Rule&quot;?)
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; 2. Giving all group members equal resource entitlement
</em><br>
<em>&gt;
</em><br>
<em>&gt; Would you expect such a sysop to not only enforce the axioms directly,
</em><br>
<em>&gt; but also for others to adhere to them where they were operating
</em><br>
<em>&gt; outside the Sysop's influence?  As in, would you expect a Sysop to
</em><br>
<em>&gt; allow Robin to voluntarily accompany Leslie into the woods*, when
</em><br>
<em>&gt; Leslie has admitted that Leslie has a secret plan to &quot;affect another
</em><br>
<em>&gt; member of the group&quot; with an action that is has not been allowed by
</em><br>
<em>&gt; that member of the group, (eg: maim, torture, kill etc Robin).
</em><br>
<em>&gt;
</em><br>
<em>&gt; * Yes, although a Sysop would normally have influence on temperate
</em><br>
<em>&gt; forested areas, shut up.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Of course the Sysop is going to influence others to adhere to its
</em><br>
<em>&gt; moral axioms.  Leslie and Robins future actions might take place away
</em><br>
<em>&gt; from the Sysop's field of influence, but the Sysop will always be
</em><br>
<em>&gt; making actions that affect the future, because you can't make actions
</em><br>
<em>&gt; that affect the present!  (Insert TangentT here)
</em><br>
<em>&gt;
</em><br>
<em>&gt; Brief ad hominem interlude...
</em><br>
<em>&gt;
</em><br>
<em>&gt; If you expect others to respect your domain, what's that but a form of
</em><br>
<em>&gt; morality?  Hell, you even suggest giving resources out equally.
</em><br>
<em>&gt; Communist!  I happen to own large tracts of land that have more than
</em><br>
<em>&gt; 1/6billionth of the planet's solar collection potential and also
</em><br>
<em>&gt; fossil energy reserves buried beneath.*  You ain't stealing my land/
</em><br>
<em>&gt; energy resources!
</em><br>
<em>&gt;
</em><br>
<em>&gt; * This is a lie.  My point is that one 2006human's share of the
</em><br>
<em>&gt; earth's crust is 85ha, less than what some people own.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Back to sysops...
</em><br>
<em>&gt;
</em><br>
<em>&gt; If the sysop is vastly more powerful than other entities, it may be
</em><br>
<em>&gt; able to act in a genie-like way, and grants wishes that don't
</em><br>
<em>&gt; interfere with other human's &quot;domains&quot;.  Why should humans/post-humans
</em><br>
<em>&gt; be forced not to interfere with each others domains?
</em><br>
<em>&gt;
</em><br>
<em>&gt; For a Sysop, because &quot;might makes right&quot; ;P
</em><br>
<em>&gt;
</em><br>
<em>&gt; Otherwise, because there might be some (objective?) reason not to.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Furthermore, why should the sysop not adversely affect humans?
</em><br>
<em>&gt; Because the Sysop's progenitors decided to make it that way.
</em><br>
<em>&gt;
</em><br>
<em>&gt; As long as an AI is (1) taking actions that affect others (2) Weighing
</em><br>
<em>&gt; the (conflicting interests of other parties (3) weighing its interests
</em><br>
<em>&gt; against those of other parties, it would need some sort of methodology
</em><br>
<em>&gt; to evaluate potential courses of action.  Those that it chooses could
</em><br>
<em>&gt; be called its &quot;preferences&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If the AI-builder thinks that the AI should selfish (!!don't try this
</em><br>
<em>&gt; at home, kids!!), the Builder is projecting their preferences onto
</em><br>
<em>&gt; others.  The AI doesn't even need to be conscious for the AI-builder's
</em><br>
<em>&gt; preferences to match the stipulative definition of &quot;moral statements&quot;
</em><br>
<em>&gt; above.
</em><br>
<em>&gt;
</em><br>
<em>&gt; -- Olie
</em><br>
<em>&gt;
</em><br>
<em>&gt; On 5/15/06, <a href="mailto:m.l.vere@durham.ac.uk?Subject=Re:%20Imposing%20ideas,%20eg:%20morality">m.l.vere@durham.ac.uk</a> &lt;<a href="mailto:m.l.vere@durham.ac.uk?Subject=Re:%20Imposing%20ideas,%20eg:%20morality">m.l.vere@durham.ac.uk</a>&gt; wrote:
</em><br>
<em>&gt; &gt; So, where would i take my 'moral nihilism'? The reasons I advocated it
</em><br>
<em>&gt; are the
</em><br>
<em>&gt; &gt; following:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; All morality is artificial/manmade. This is not an intrisnic negative,
</em><br>
<em>&gt; however
</em><br>
<em>&gt; &gt; it is negative in this case, as:
</em><br>
<em>&gt; &gt; 1. Morality made by mere humans would very likely not be suitable/a net
</em><br>
<em>&gt; &gt; postivie for posthumans. Therefore we need to go into the singularity
</em><br>
<em>&gt; without
</em><br>
<em>&gt; &gt; imposing morality on our/other posthumans (ie as moral nihilists).
</em><br>
<em>&gt; &gt; 2. As morality is artificial, there is no one (or finite number of)
</em><br>
<em>&gt; 'correct'
</em><br>
<em>&gt; &gt; moralit(y)/(ies). Thus it would be better for each individual posthuman
</em><br>
<em>&gt; to be
</em><br>
<em>&gt; &gt; able to develop his/her/its own (or remain a nihlist), than have one
</em><br>
<em>&gt; posthuman
</em><br>
<em>&gt; &gt; morality developed by a sysop.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; At the moment, what i would advocate is that
</em><br>
<em>&gt; &gt; universal egoists (or moralists who dont want to constrain others with
</em><br>
<em>&gt; their
</em><br>
<em>&gt; &gt; morals) build
</em><br>
<em>&gt; &gt; a sysop which grants them all complete self-determination in becoming
</em><br>
<em>&gt; &gt; posthuman. My ideas so far (written previously):
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &quot;The best posssible singularity instigator I can imagine would be a
</em><br>
<em>&gt; &gt; genie style seed AI, its supergoal being to execute my expressed
</em><br>
<em>&gt; &gt; individual will. From here I could do anything that the person/group
</em><br>
<em>&gt; &gt; instigating the singularity could do (including asking for any other
</em><br>
<em>&gt; &gt; set of goals). In addition I would have the ability to ask for
</em><br>
<em>&gt; &gt; advice from a post singularity entity. This is better than having me
</em><br>
<em>&gt; &gt; as the instigator, as the AI can function as my guide to
</em><br>
<em>&gt; &gt; posthumanity.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; If anyone can think of better, please tell.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The chances of such a singularity instigator being built are very
</em><br>
<em>&gt; &gt; slim. As such I recomend that a group of people have their expressed
</em><br>
<em>&gt; &gt; individual wills excecuted, thus all being motivated to build such
</em><br>
<em>&gt; &gt; an AI.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The problem of conflicting expressed wills can be dealt with by
</em><br>
<em>&gt; &gt; 1. Prohibiting any action which affects another member of the group,
</em><br>
<em>&gt; &gt; unless that member has wilfully expressed for that action to be
</em><br>
<em>&gt; &gt; allowed (a form of domain protection).
</em><br>
<em>&gt; &gt; 2. Giving all group members equal resource entitlement
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The first condition would only be a problem for moralists and
</em><br>
<em>&gt; &gt; megalomaniacs (and not entirely for the latter as there could exist
</em><br>
<em>&gt; solipism
</em><br>
<em>&gt; &gt; stlye simulations for them to control).
</em><br>
<em>&gt; &gt; The second seems an inevitable price of striking the best balance
</em><br>
<em>&gt; &gt; between the quality of posthumanity and the probability of it
</em><br>
<em>&gt; &gt; occuring.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I tentatively recomend that the group in question be all humanity.
</em><br>
<em>&gt; &gt; This is to prevent infighting within the group about who is
</em><br>
<em>&gt; &gt; included, gain the support of libertarian moralists and weaken the
</em><br>
<em>&gt; &gt; strength of opposition - all making it more likely to happen.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; This is a theory in progress. Idealy, we would have an organisation
</em><br>
<em>&gt; &gt; similar to SIAI working on its development/actuallisation. As it is,
</em><br>
<em>&gt; &gt; Ive brought it here. Note, I hope to develop this further (preferably
</em><br>
<em>&gt; from
</em><br>
<em>&gt; &gt; the standpoint of moral nihilism).
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Whilst the AI interpereting commands may be an issue, I dont see it
</em><br>
<em>&gt; &gt; as an unsolvable problem.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Note: I see this as a far better solution to singularity regret than
</em><br>
<em>&gt; &gt; SIAI's CV.&quot;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14987.html">Dani Eder: "Re: Chimp genetics"</a>
<li><strong>Previous message:</strong> <a href="14985.html">Tennessee Leeuwenburg: "Re: Learning math..."</a>
<li><strong>In reply to:</strong> <a href="14983.html">Olie Lamb: "Imposing ideas, eg: morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14990.html">m.l.vere@durham.ac.uk: "Re: Imposing ideas, eg: morality"</a>
<li><strong>Reply:</strong> <a href="14990.html">m.l.vere@durham.ac.uk: "Re: Imposing ideas, eg: morality"</a>
<li><strong>Reply:</strong> <a href="15021.html">Olie Lamb: "Re: Imposing ideas, eg: morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14986">[ date ]</a>
<a href="index.html#14986">[ thread ]</a>
<a href="subject.html#14986">[ subject ]</a>
<a href="author.html#14986">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
