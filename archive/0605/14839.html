<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: the Singularity Summit and regulation of AI</title>
<meta name="Author" content="Bill Hibbard (test@demedici.ssec.wisc.edu)">
<meta name="Subject" content="the Singularity Summit and regulation of AI">
<meta name="Date" content="2006-05-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>the Singularity Summit and regulation of AI</h1>
<!-- received="Wed May 10 06:17:39 2006" -->
<!-- isoreceived="20060510121739" -->
<!-- sent="Wed, 10 May 2006 07:16:41 -0500 (CDT)" -->
<!-- isosent="20060510121641" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@demedici.ssec.wisc.edu" -->
<!-- subject="the Singularity Summit and regulation of AI" -->
<!-- id="Pine.GSO.4.44.0605100715400.1181-100000@demedici.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20the%20Singularity%20Summit%20and%20regulation%20of%20AI"><em>test@demedici.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Wed May 10 2006 - 06:16:41 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14840.html">Ben Goertzel: "Re: [agi] the Singularity Summit and regulation of AI"</a>
<li><strong>Previous message:</strong> <a href="14838.html">Dani Eder: "Re: Changing the value system of FAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14840.html">Ben Goertzel: "Re: [agi] the Singularity Summit and regulation of AI"</a>
<li><strong>Reply:</strong> <a href="14840.html">Ben Goertzel: "Re: [agi] the Singularity Summit and regulation of AI"</a>
<li><strong>Reply:</strong> <a href="14842.html">Eliezer S. Yudkowsky: "Re: the Singularity Summit and regulation of AI"</a>
<li><strong>Maybe reply:</strong> <a href="14861.html">Joshua Fox: "Re: the Singularity Summit and regulation of AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14839">[ date ]</a>
<a href="index.html#14839">[ thread ]</a>
<a href="subject.html#14839">[ subject ]</a>
<a href="author.html#14839">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I am concerned that the Singularity Summit will not include
<br>
any speaker advocating government regulation of intelligent
<br>
machines. The purpose of this message is not to convince you
<br>
of the need for such regulation, but just to say that the
<br>
Summit should include someone speaking in favor of it. Note
<br>
that, to be effective, regulation should be linked to a
<br>
widespread public movement like the environmental and
<br>
consumer safety movements. Intelligent weapons could be
<br>
regulated by treaties similar to those for nuclear, chemical
<br>
and biological weapons.
<br>
<p>The obvious choice to advocate this position would be James
<br>
Hughes, and it is puzzling that he is not included among the
<br>
speakers. Can anyone explain why he is not included?
<br>
<p>Nick Bostrom is a speaker, and it is possible that he will
<br>
advocate such regulation. However, while he has written
<br>
about the regulation of nanotechnology and biotechnology,
<br>
I not aware of anything he has written advocating regulation
<br>
of intelligent machines. He has been very clear about the
<br>
need to avoid existential threats from new technologies
<br>
including artificial intelligence, and presumably he feels
<br>
that regulation is needed to avoid these threats. I hope
<br>
he will address this issue explicitly. Machine intelligence
<br>
poses other threats to human happiness that are not
<br>
existential but should be addressed by regulation.
<br>
<p>Ray Kurzweil has advocated regulation of biotechnology and
<br>
nanotechnology, but appears to be pessimistic about
<br>
regulation of AI. In The Singularity is Near, he writes
<br>
&quot;But there is no purely technical strategy that is
<br>
workable in this area, because greater intelligence will
<br>
always find a way to circumvent measures that are the
<br>
product of a lesser intelligence.&quot; I think the answer is to
<br>
design AI to not want to harm humans (I think SIAI agrees
<br>
with this, although we disagree on the details). Kurzweil
<br>
also writes that AI will be &quot;intimately embedded in our
<br>
bodies and brains&quot; and hence &quot;it will reflect our values
<br>
because it will be us.&quot; But the values of some humans have
<br>
led to much misery for other humans. If some humans are
<br>
radically more intelligent than others and retain all their
<br>
human competitive instincts, this could create a society
<br>
that the vast majority will not want. If they are given a
<br>
choice.  Meetings like the Singularity Summit should help
<br>
educate the public about the ethical choices they face with
<br>
new technologies.
<br>
<p>Eliezer Yudkowsky is very clear about the dangers from
<br>
artificial intelligence but is equally clear about his
<br>
contempt for any regulation. Rather, it appears that his
<br>
SIAI organization intends to be the first to create AI,
<br>
which will be friendly and take over the world before
<br>
governments have time to react. I think this scenario is
<br>
very unlikely.
<br>
<p>Bill McKibben wants a total prohibition of all the radical
<br>
new technolgies. Used correctly, these technologies can
<br>
give all humans much better lives, and it would be shameful
<br>
to ban them completely. It would also be politically
<br>
impossible to convince all governments to ban them. Rather
<br>
than preserving the world exactly as it is, we need to be
<br>
more specific about the values we want to preserve and
<br>
find ways to enjoy the benefits of new technologies while
<br>
preserving those values.
<br>
<p>I have read the statements of the other speakers, included
<br>
on the Singularity Summit web site, and none of them suggest
<br>
that they will advocate regulation of intelligent machines.
<br>
<p>Tenzin Gyatso, the 14th Dalai Lama, would be an interesting
<br>
speaker for the Singularity Summit. Not as a religous
<br>
leader, but as an ethical leader. He is very interested in
<br>
new technologies and spoke to the Society for Neuroscience
<br>
on 12 November 2005. On that same day he wrote, in an op-ed
<br>
in the New York Times:
<br>
<p>&nbsp;&nbsp;&quot;It is all too evident that our moral thinking simply
<br>
&nbsp;&nbsp;has not been able to keep pace with the speed of
<br>
&nbsp;&nbsp;scientific advancement. Yet the ramifications of this
<br>
&nbsp;&nbsp;progress are such that it is no longer adequate to say
<br>
&nbsp;&nbsp;that the choice of what to do with this knowledge
<br>
&nbsp;&nbsp;should be left in the hands of individuals.&quot;
<br>
<p>If you are uneasy about listening to a religous leader,
<br>
consider that in the same op-ed he also wrote:
<br>
<p>&nbsp;&nbsp;&quot;If science proves some belief of Buddhism wrong,
<br>
&nbsp;&nbsp;then Buddhism will have to change.&quot;
<br>
<p>The Singularity Summit should include all points of
<br>
view, including advocates for regulation of intelligent
<br>
machines. It will weaken the Summit to exclude this
<br>
point of view.
<br>
<p>A copy of this message and other writings about the
<br>
singularity are available at:
<br>
<p>&nbsp;&nbsp;<a href="http://www.ssec.wisc.edu/~billh/g/Singularity_Notes.html">http://www.ssec.wisc.edu/~billh/g/Singularity_Notes.html</a>
<br>
<p>Bill Hibbard
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14840.html">Ben Goertzel: "Re: [agi] the Singularity Summit and regulation of AI"</a>
<li><strong>Previous message:</strong> <a href="14838.html">Dani Eder: "Re: Changing the value system of FAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14840.html">Ben Goertzel: "Re: [agi] the Singularity Summit and regulation of AI"</a>
<li><strong>Reply:</strong> <a href="14840.html">Ben Goertzel: "Re: [agi] the Singularity Summit and regulation of AI"</a>
<li><strong>Reply:</strong> <a href="14842.html">Eliezer S. Yudkowsky: "Re: the Singularity Summit and regulation of AI"</a>
<li><strong>Maybe reply:</strong> <a href="14861.html">Joshua Fox: "Re: the Singularity Summit and regulation of AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14839">[ date ]</a>
<a href="index.html#14839">[ thread ]</a>
<a href="subject.html#14839">[ subject ]</a>
<a href="author.html#14839">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
