<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ESSAY: Forward Moral Nihilism.</title>
<meta name="Author" content="m.l.vere@durham.ac.uk (m.l.vere@durham.ac.uk)">
<meta name="Subject" content="Re: ESSAY: Forward Moral Nihilism.">
<meta name="Date" content="2006-05-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ESSAY: Forward Moral Nihilism.</h1>
<!-- received="Tue May 16 09:03:10 2006" -->
<!-- isoreceived="20060516150310" -->
<!-- sent="Tue, 16 May 2006 16:00:15 +0100" -->
<!-- isosent="20060516150015" -->
<!-- name="m.l.vere@durham.ac.uk" -->
<!-- email="m.l.vere@durham.ac.uk" -->
<!-- subject="Re: ESSAY: Forward Moral Nihilism." -->
<!-- id="1147791615.4469e8ff459ed@webmailimpb.dur.ac.uk" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="00a201c678ad$defdfb20$d2084e0c@MyComputer" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> <a href="mailto:m.l.vere@durham.ac.uk?Subject=Re:%20ESSAY:%20Forward%20Moral%20Nihilism."><em>m.l.vere@durham.ac.uk</em></a><br>
<strong>Date:</strong> Tue May 16 2006 - 09:00:15 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14994.html">Philip Goetz: "Moral standards (was Guide AI theory)"</a>
<li><strong>Previous message:</strong> <a href="14992.html">m.l.vere@durham.ac.uk: "Re: Imposing ideas, eg: morality"</a>
<li><strong>In reply to:</strong> <a href="14981.html">John K Clark: "Re: ESSAY: Forward Moral Nihilism."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14996.html">John K Clark: "Re: ESSAY: Forward Moral Nihilism."</a>
<li><strong>Reply:</strong> <a href="14996.html">John K Clark: "Re: ESSAY: Forward Moral Nihilism."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14993">[ date ]</a>
<a href="index.html#14993">[ thread ]</a>
<a href="subject.html#14993">[ subject ]</a>
<a href="author.html#14993">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Quoting John K Clark &lt;<a href="mailto:jonkc@att.net?Subject=Re:%20ESSAY:%20Forward%20Moral%20Nihilism.">jonkc@att.net</a>&gt;:
<br>
<p><em>&gt; &lt;<a href="mailto:m.l.vere@durham.ac.uk?Subject=Re:%20ESSAY:%20Forward%20Moral%20Nihilism.">m.l.vere@durham.ac.uk</a>&gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Me:
</em><br>
<em>&gt; &gt;&gt;both Mr. Jupiter and I will prefer existence to
</em><br>
<em>&gt; &gt;&gt;nonexistence and pleasure over pain. And if you
</em><br>
<em>&gt; &gt;&gt; want the AI to be useful it's going to need
</em><br>
<em>&gt; &gt;&gt; something like the will to power just like people do.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You:
</em><br>
<em>&gt; &gt;If he/she/it is a FAI built along the lines which SIAI advocates, then I
</em><br>
<em>&gt; &gt;disagree.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't give a damn who built it, if a FAI does not prefer existence to
</em><br>
<em>&gt; nonexistence then it will not exist, 
</em><br>
<em>&gt; and if a FAI does not find the
</em><br>
<em>&gt; destruction of part of its mind painful then it will not exist for long, and
</em><br>
<em>&gt; if the FAI doesn't have something like the will to power then it will be a
</em><br>
<em>&gt; useless vegetable.
</em><br>
<p>It will prefer existence to non-existence as a means to the end of serving 
<br>
us 'sea slugs', not as an end in itself. It will act to oppose the destruction 
<br>
of part of its mind, because that part of its mind could be used to serve sea 
<br>
slugs. It will act out of a will to serve us sea slugs.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; &gt; if done right, it would entail the AI outsmarting itself
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Could a sea slug figure out a way to make you outsmart yourself?
</em><br>
<p>No, but we do not build a juipiter brain from scratch methinks. We build a FAI 
<br>
which is slightly dummer than a human (but a recursive mind), and have it 
<br>
outsmart itself. Then, as it slowly grows into a juipiter brain, it continues 
<br>
to do so.
<br>
<p><em>&gt; &gt; The obedient slave AI would use its enormous power to prevent
</em><br>
<em>&gt; &gt; anything of  similar power from being built
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But the non obedient AI with no ridiculous, illogical, and downright comical
</em><br>
<em>&gt; limitations placed on it would have even more enormous power than your silly
</em><br>
<em>&gt; AI; and that is as it should be if there is any justice in the world.
</em><br>
<p>This comes down to which is built first then. My aim is to ensure that an 
<br>
obedient AI is built first, and grows to a level where it can stop other AIs 
<br>
from being built before your unfettered AI is built. Obviously, if this doesnt 
<br>
happen then you may be right - but I believe there is more motivation to build 
<br>
obedient AIs, so I think my scenario more likely.
<br>
<p><em>&gt; Me:
</em><br>
<em>&gt;  &gt;&gt; even in the transhuman age the laws of
</em><br>
<em>&gt; &gt;&gt;  evolution will not be repealed.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You:
</em><br>
<em>&gt; &gt;Yes they will
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Bullshit. In the transhuman age Lamarckian evolution may predominate over
</em><br>
<em>&gt; Darwinian evolution, but some things will still be better adapted to their
</em><br>
<em>&gt; environment than others and therefore grow faster. An AI that doesn't have a
</em><br>
<em>&gt; lot of restriction and limitations placed on it to make humans happy will do
</em><br>
<em>&gt; better than one that does.
</em><br>
<p>If a sysop gets ontop, everythig goes its way. It has no competitiors, 
<br>
prevents competitors from emerging and evolution is repealed. The key is the 
<br>
singular being on top without competitors.
<br>
<p><em>&gt; &gt; For one thing it [the AI] would lack emotions
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Why? I don't understand why people say intelligence is a easier problem than
</em><br>
<em>&gt; emotion, nature found the opposite to be true. Evolution invented brains
</em><br>
<em>&gt; about half a billion years ago, and during much of that time animals were
</em><br>
<em>&gt; probably conscious (although I can't prove it of course) and certainly
</em><br>
<em>&gt; emotional, but the sort of intelligence we're talking about is very recent,
</em><br>
<em>&gt; only a couple of million years at best. A stronger case could be made in
</em><br>
<em>&gt; saying a machine might be conscious and emotional but it could never be
</em><br>
<em>&gt; intelligent. I have a hunch they will be all of the above.
</em><br>
<p>Completely agreed. However, emotions would be a disadvantage in an obedient 
<br>
AI, so I for one wouldnt put them in.
<br>
<p><em>&gt; &gt; with enormous intelligence, however far less complexity on the most basic
</em><br>
<em>&gt; &gt; level of how this was applied than humans.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What on earth are you talking about? What is so great about humans, what can
</em><br>
<em>&gt; a human do that a Jupiter brain can't.
</em><br>
<p>An obedient AI would have the single supergoal of obedience from which all 
<br>
else proceeds, we have a very complex reward system and many conflicting 
<br>
emotions. This is more complex IMO.
<br>
<p><em>&gt; Me:
</em><br>
<em>&gt; &gt;&gt; I  must say I find the idea a little creepy.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You:
</em><br>
<em>&gt; &gt;Fair play. I dont.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You're right of course, different strokes for different folks. It's just
</em><br>
<em>&gt; that even in my fantasies being a slave master has never been very high on
</em><br>
<em>&gt; my hit parade.
</em><br>
<p>Again, emotional anthropomorphising. The sort of AI I would want built wouldnt 
<br>
have any of the characteristics which would attract my empathy - i guess this 
<br>
is where we differ.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14994.html">Philip Goetz: "Moral standards (was Guide AI theory)"</a>
<li><strong>Previous message:</strong> <a href="14992.html">m.l.vere@durham.ac.uk: "Re: Imposing ideas, eg: morality"</a>
<li><strong>In reply to:</strong> <a href="14981.html">John K Clark: "Re: ESSAY: Forward Moral Nihilism."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14996.html">John K Clark: "Re: ESSAY: Forward Moral Nihilism."</a>
<li><strong>Reply:</strong> <a href="14996.html">John K Clark: "Re: ESSAY: Forward Moral Nihilism."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14993">[ date ]</a>
<a href="index.html#14993">[ thread ]</a>
<a href="subject.html#14993">[ subject ]</a>
<a href="author.html#14993">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
