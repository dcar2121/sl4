<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ESSAY: Forward Moral Nihilism (EP)</title>
<meta name="Author" content="Charles D Hixson (charleshixsn@earthlink.net)">
<meta name="Subject" content="Re: ESSAY: Forward Moral Nihilism (EP)">
<meta name="Date" content="2006-05-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ESSAY: Forward Moral Nihilism (EP)</h1>
<!-- received="Mon May 15 13:43:15 2006" -->
<!-- isoreceived="20060515194315" -->
<!-- sent="Mon, 15 May 2006 12:42:18 -0700" -->
<!-- isosent="20060515194218" -->
<!-- name="Charles D Hixson" -->
<!-- email="charleshixsn@earthlink.net" -->
<!-- subject="Re: ESSAY: Forward Moral Nihilism (EP)" -->
<!-- id="4468D99A.3060507@earthlink.net" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="22360fa10605150835r38d810a2jd75815d19ca94647@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Charles D Hixson (<a href="mailto:charleshixsn@earthlink.net?Subject=Re:%20ESSAY:%20Forward%20Moral%20Nihilism%20(EP)"><em>charleshixsn@earthlink.net</em></a>)<br>
<strong>Date:</strong> Mon May 15 2006 - 13:42:18 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14956.html">Charles D Hixson: "Re: the future god, light cones and free will"</a>
<li><strong>Previous message:</strong> <a href="14954.html">m.l.vere@durham.ac.uk: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>In reply to:</strong> <a href="14951.html">Jef Allbright: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14958.html">Ricardo Barreira: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<li><strong>Reply:</strong> <a href="14958.html">Ricardo Barreira: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<li><strong>Reply:</strong> <a href="14961.html">Jef Allbright: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<li><strong>Reply:</strong> <a href="14980.html">Keith Henson: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14955">[ date ]</a>
<a href="index.html#14955">[ thread ]</a>
<a href="subject.html#14955">[ subject ]</a>
<a href="author.html#14955">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Jef Allbright wrote:
<br>
<em>&gt; On 5/15/06, Charles D Hixson &lt;<a href="mailto:charleshixsn@earthlink.net?Subject=Re:%20ESSAY:%20Forward%20Moral%20Nihilism%20(EP)">charleshixsn@earthlink.net</a>&gt; wrote:
</em><br>
<em>&gt;&gt; ...
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Xenophobia, in a mild form, is useful to split the tribe into groups
</em><br>
<em>&gt;&gt; that act separately and divide the hunting areas.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think you might have this backwards.  Evolved traits are a result of
</em><br>
<em>&gt; adaptation rather than drivers of &quot;useful&quot; change.
</em><br>
You are correct about the genesis of traits, but if a complex trait
<br>
isn't useful, it won't survive.  Thus is a complex trait exists it is
<br>
reasonable to consider how it is useful.  This must be seen as shorthand
<br>
for &quot;Why this trait was preserved?&quot; rather than for &quot;Why was this
<br>
created?&quot;, but it *is* a valid consideration.
<br>
<em>&gt;
</em><br>
<em>&gt;&gt; I'd be really
</em><br>
<em>&gt;&gt; surprised if it turned out that they often fought seriously before the
</em><br>
<em>&gt;&gt; invention of the arrow, or possibly the spear-thrower, and by that time
</em><br>
<em>&gt;&gt; we were pretty much evolved into modern form.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Why would you be surprised?  Are you following the &quot;noble savage&quot; line
</em><br>
<em>&gt; of thought?  Biological evolution is demonstrably &quot;bloody in tooth and
</em><br>
<em>&gt; claw&quot; and it is only recently that a  higher level of organization
</em><br>
<em>&gt; offers hope (from the human viewpoint) for this to improve.
</em><br>
I'd be surprised because with only clubs it usually requires a large
<br>
advantage in numbers to inflict significant damage to a group (as
<br>
opposed to an individual).  Early populations were both sparse migrant,
<br>
and if a neighborhood was seen as dangerous populations would relocate
<br>
away.  (When I say migrant don't think of cross continental treks, but
<br>
do think of an area hundreds of miles on a side.)  Sparse is significant
<br>
because there wasn't large pressure to prevent the migration.  Later on
<br>
populations became denser and technologies improved.  At that point you
<br>
do start to get massive combat between tribes, but this didn't really
<br>
spring into full swing until after the invention of agriculture and
<br>
sessile communities.
<br>
<p>OTOH, there does seem to be evidence indicating that the rate of mugging
<br>
in the early old stone age was higher than it is in today's worse
<br>
neighborhoods.  This might have been a kind of guerrilla version of gang
<br>
warfare.  Or it might have been simple banditry.  You can tell lots of
<br>
different stories based on the available evidence, but putting much
<br>
credence in any one of them is probably a mistake.  (But I'm no
<br>
expert.   This is just an opinion.  etc.)
<br>
<em>&gt;&gt; That said, this doesn't appear to me to relate significantly to the
</em><br>
<em>&gt;&gt; instincts that we should create for the AI that we build.  It might be
</em><br>
<em>&gt;&gt; wise to have it be wary of strangers, but one would definitely want to
</em><br>
<em>&gt;&gt; put limits on how strong that reaction could be...and remember, the
</em><br>
<em>&gt;&gt; instincts need to be designed to operate on a system without a
</em><br>
<em>&gt;&gt; predictable sensoria.
</em><br>
<em>&gt; Why do you think an AI would lack &quot;a predictable sensoria&quot;?  Are you
</em><br>
<em>&gt; saying something like &quot;we can't know the qualia of an AI&quot;?  If that's
</em><br>
<em>&gt; the case, we would have to veer off into the whole ugly discussion of
</em><br>
<em>&gt; what people think they mean by qualia.  On the other hand, don't
</em><br>
<em>&gt; designed systems have better defined sensoria than non-designed
</em><br>
<em>&gt; systems?
</em><br>
I'm saying that you don't want to be redesigning a new AI for every
<br>
machine configuration that comes along.  I'm sure that it would be
<br>
feasible to design an AI to operate in one particular hardware
<br>
configuration.  I will submit that any such AI would be crippled for
<br>
further evolution.  I'm not saying anything about qualia.  I don't know
<br>
of an operational definition that allows one to speak usefully about
<br>
them except as the experiencer of the qualia.  But any particular set of
<br>
computer hardware will have certain meaninful signals.  One abstraction
<br>
of those hardware signals is the POSIX protocol, which is implemented by
<br>
Unix systems, and is essentially implemented by Linux systems.  Even
<br>
MSWind comes (can come?) close.  So that is one abstract set of
<br>
communication between the hardware and software layers that is
<br>
(reasonably) well specified, and is also common.
<br>
<em>&gt;&gt; sensitive to OS calls, unless you intend to have it operate on the bare
</em><br>
<em>&gt;&gt; hardware.  Say you can guarantee that it lives in a POSIX compliant
</em><br>
<em>&gt;&gt; universe (or one close to that).  It may have USB or firewire cameras
</em><br>
<em>&gt;&gt; installed, it may not.  It can probably depend on at least intermittent
</em><br>
<em>&gt;&gt; internet access.
</em><br>
<em>&gt;  You can probably predict that it will be
</em><br>
<em>&gt;
</em><br>
<em>&gt; Why would any application necessarily be aware of its operating
</em><br>
<em>&gt; system?  Are humans necessarily aware of their supporting subsystems?
</em><br>
It would need to operate within the OS.  You may not be aware of your
<br>
blood pressure, but your mental processes have direct and indirect
<br>
effects on it.  The OS is the interface between the hardware layer and
<br>
the software layer.  It allows the same program to run on significantly
<br>
different hardware merely by rewriting a limited interface layer. 
<br>
(Well, that and recompiling for the new CPU codes.)
<br>
<p>People's minds are closely tied into the hardware of their bodies.  Some
<br>
even deny that it will ever prove feasible to separate them.  Some
<br>
people say this will &quot;only provide a fake of continued life, while the
<br>
real person was killed during the transfer&quot;.  This is not my desired
<br>
approach to an AI.  I want an AI that is able to operate in reduced
<br>
fashion on low end hardware, and able to scale seamlessly as fancier
<br>
hardware becomes available.
<br>
<em>&gt;&gt; The only shape for an instinct for friendliness that has occurred to me
</em><br>
<em>&gt;&gt; is &quot;Attempt to talk to those who attempt to talk with you.&quot;  I.e., learn
</em><br>
<em>&gt;&gt; the handshaking protocols of your environment.
</em><br>
<em>&gt; I think this is indeed touching upon a fundamental principle for
</em><br>
<em>&gt; success.  All the interesting stuff (the potential for growth) is in
</em><br>
<em>&gt; the interactions between Self and Other, the adjacent possible.
</em><br>
<em>&gt; However this principle is just as applicable for offense as it is for
</em><br>
<em>&gt; &quot;friendliness&quot;.
</em><br>
A hostile AI wouldn't necessarily need to communicate with those it
<br>
wished to be hostile towards.  A friendly AI would.  The situation is
<br>
not symmetric.  (OTOH, I also admit that this is merely a place to start.)
<br>
<em>&gt;&gt; only that, but it can be useful for dealing with disk drives and
</em><br>
<em>&gt;&gt; internet sites as well as with people.  I can't even think of how one
</em><br>
<em>&gt;&gt; could say &quot;don't impose your will on an unwilling sentient&quot; at the level
</em><br>
<em>&gt;&gt; of instincts.  That's got too many undefinable terms in it.  &quot;impose&quot;,
</em><br>
<em>&gt;&gt; &quot;will&quot;, &quot;sentient&quot;.  With enough effort I can vaguely see how will could
</em><br>
<em>&gt;&gt; be defined...nothing that's jelled yet.
</em><br>
<em>&gt; This is a point where many people get stuck with conventional ideas of
</em><br>
<em>&gt; morality.  A full explanation is not possible within the confines of
</em><br>
<em>&gt; this email discussion, but moral decision-making *requires* that you
</em><br>
<em>&gt; attempt to impose your will at every opportunity, but that will should
</em><br>
<em>&gt; be as well informed as possible of the long-term consequences of its
</em><br>
<em>&gt; actions.  The degree of sentience of the Other is irrelevant to this
</em><br>
<em>&gt; basic principle, but very relevant to the actual interaction.
</em><br>
<em>&gt;
</em><br>
<em>&gt; - Jef
</em><br>
This may be a necessity in the moral structure that you have chosen. 
<br>
Mine does not require of me that I impose my will upon others, merely
<br>
that I attempt to prevent them imposing their will upon me.  I may
<br>
*decide* that circumstances are such that practicality requires me to
<br>
impose my will upon them, but this is not a moral requirement.
<br>
<p>I would assert that you, also, find no such moral requirement.  There
<br>
are many people in the world who are behaving immorally, whatever your
<br>
particular code may say morality *is*.  Yet you sat there and
<br>
corresponded with me rather than stopping them.  Therefore you are not
<br>
morally commanded by any code that you actually accept to stop them. 
<br>
And I certainly would not want an AI that felt morally compelled to make
<br>
everyone behave.  That might not be the worst possible outcome, but it
<br>
would be a very bad one.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14956.html">Charles D Hixson: "Re: the future god, light cones and free will"</a>
<li><strong>Previous message:</strong> <a href="14954.html">m.l.vere@durham.ac.uk: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>In reply to:</strong> <a href="14951.html">Jef Allbright: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14958.html">Ricardo Barreira: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<li><strong>Reply:</strong> <a href="14958.html">Ricardo Barreira: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<li><strong>Reply:</strong> <a href="14961.html">Jef Allbright: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<li><strong>Reply:</strong> <a href="14980.html">Keith Henson: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14955">[ date ]</a>
<a href="index.html#14955">[ thread ]</a>
<a href="subject.html#14955">[ subject ]</a>
<a href="author.html#14955">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
