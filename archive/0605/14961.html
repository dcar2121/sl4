<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ESSAY: Forward Moral Nihilism (EP)</title>
<meta name="Author" content="Jef Allbright (jef@jefallbright.net)">
<meta name="Subject" content="Re: ESSAY: Forward Moral Nihilism (EP)">
<meta name="Date" content="2006-05-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ESSAY: Forward Moral Nihilism (EP)</h1>
<!-- received="Mon May 15 15:41:30 2006" -->
<!-- isoreceived="20060515214130" -->
<!-- sent="Mon, 15 May 2006 14:41:14 -0700" -->
<!-- isosent="20060515214114" -->
<!-- name="Jef Allbright" -->
<!-- email="jef@jefallbright.net" -->
<!-- subject="Re: ESSAY: Forward Moral Nihilism (EP)" -->
<!-- id="22360fa10605151441v7763545fy5c32a04347408a3b@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="4468D99A.3060507@earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Jef Allbright (<a href="mailto:jef@jefallbright.net?Subject=Re:%20ESSAY:%20Forward%20Moral%20Nihilism%20(EP)"><em>jef@jefallbright.net</em></a>)<br>
<strong>Date:</strong> Mon May 15 2006 - 15:41:14 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14962.html">Woody Long: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>Previous message:</strong> <a href="14960.html">Ricardo Barreira: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>In reply to:</strong> <a href="14955.html">Charles D Hixson: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14980.html">Keith Henson: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14961">[ date ]</a>
<a href="index.html#14961">[ thread ]</a>
<a href="subject.html#14961">[ subject ]</a>
<a href="author.html#14961">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 5/15/06, Charles D Hixson &lt;<a href="mailto:charleshixsn@earthlink.net?Subject=Re:%20ESSAY:%20Forward%20Moral%20Nihilism%20(EP)">charleshixsn@earthlink.net</a>&gt; wrote:
<br>
<em>&gt; Jef Allbright wrote:
</em><br>
<p><em>&gt; &gt; This is a point where many people get stuck with conventional ideas of
</em><br>
<em>&gt; &gt; morality.  A full explanation is not possible within the confines of
</em><br>
<em>&gt; &gt; this email discussion, but moral decision-making *requires* that you
</em><br>
<em>&gt; &gt; attempt to impose your will at every opportunity, but that will should
</em><br>
<em>&gt; &gt; be as well informed as possible of the long-term consequences of its
</em><br>
<em>&gt; &gt; actions.  The degree of sentience of the Other is irrelevant to this
</em><br>
<em>&gt; &gt; basic principle, but very relevant to the actual interaction.
</em><br>
<p><em>&gt; This may be a necessity in the moral structure that you have chosen.
</em><br>
<p>I'm not talking about any chosen moral code or structure.  I am saying
<br>
that logical consistency requires that to the extent your will is
<br>
based on understanding of the extended consequences of your actions,
<br>
it is morally imperative that you act in accordance with your will.
<br>
Looking at it from the other direction, it is impossible by definition
<br>
to act against your own will, and your actions (including intentional
<br>
inaction) will tend to lead to good to the extent that they are based
<br>
on awareness of the extended consequences.
<br>
<p><em>&gt; Mine does not require of me that I impose my will upon others, merely
</em><br>
<em>&gt; that I attempt to prevent them imposing their will upon me.
</em><br>
<p>I would expect that your will includes providing a healthy respect for
<br>
the autonomy of others, and that you wholeheartedly impose that will
<br>
upon them.
<br>
<p><em>&gt;I may
</em><br>
<em>&gt; *decide* that circumstances are such that practicality requires me to
</em><br>
<em>&gt; impose my will upon them, but this is not a moral requirement.
</em><br>
<p>If you think it is the right thing to do, based on your understanding
<br>
of the extended consequences of your actions, then such action is
<br>
moral.
<br>
<p><p><em>&gt;
</em><br>
<em>&gt; I would assert that you, also, find no such moral requirement.  There
</em><br>
<em>&gt; are many people in the world who are behaving immorally, whatever your
</em><br>
<em>&gt; particular code may say morality *is*.  Yet you sat there and
</em><br>
<em>&gt; corresponded with me rather than stopping them.  Therefore you are not
</em><br>
<em>&gt; morally commanded by any code that you actually accept to stop them.
</em><br>
<p>Moral decision-making is necessarily from the subjective point of view
<br>
of the actor.  We should not confuse this with ethical codes which may
<br>
more or less correspond with subjective moral judgement.  Note also
<br>
that it would be clearly immoral for me to take the position that I
<br>
must directly and immediately address all the wrongs in the world as
<br>
you suggest, because such an attempt by me would be ineffective and
<br>
the consequences therefore undesirable.
<br>
<p><em>&gt; And I certainly would not want an AI that felt morally compelled to make
</em><br>
<em>&gt; everyone behave.  That might not be the worst possible outcome, but it
</em><br>
<em>&gt; would be a very bad one.
</em><br>
<p>See the inconsistency?  What basis do you assume the AI would use for
<br>
making everyone behave, if it can be seen in advance that the outcome
<br>
would be bad?
<br>
<p>But some actions do indeed deliver better results than other actions.
<br>
With increasing awareness of the extended consequences of our actions
<br>
over increasing scope, we tend to make more effective decisions.
<br>
Apply this increasing awareness of effective methods, to increasing
<br>
awareness of our inter-subjective values (those which have been tested
<br>
and seen to work) and the result is decision-making that is seen as
<br>
increasingly moral.
<br>
<p>Now if an AI were implemented as an engine for such moral
<br>
decision-making, it's moral decision-making process could easily be
<br>
more effective than any human's given our limited capability for
<br>
awareness.  This &quot;AI&quot; could be implemented as a social framework using
<br>
actual humans as input, and it would provide outputs at a higher level
<br>
of wisdom than any human, and that might be a very good start, but
<br>
would be limited by human speed and capacity.
<br>
<p>Note that applying this metaethical thinking gives you increasingly
<br>
moral decision-making--it facilitates discovery of increasingly
<br>
effective principles which promote shared values which work over
<br>
increasing scope--but it says nothing directly about the ends.  In
<br>
fact two such moral engines, started in two separate environments,
<br>
could possibly diverge for quite some time, with each seen as becoming
<br>
increasingly moral.
<br>
<p>- Jef
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14962.html">Woody Long: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>Previous message:</strong> <a href="14960.html">Ricardo Barreira: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>In reply to:</strong> <a href="14955.html">Charles D Hixson: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14980.html">Keith Henson: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14961">[ date ]</a>
<a href="index.html#14961">[ thread ]</a>
<a href="subject.html#14961">[ subject ]</a>
<a href="author.html#14961">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
