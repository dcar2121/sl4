<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Imposing ideas, eg: morality</title>
<meta name="Author" content="Olie Lamb (neomorphy@gmail.com)">
<meta name="Subject" content="Imposing ideas, eg: morality">
<meta name="Date" content="2006-05-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Imposing ideas, eg: morality</h1>
<!-- received="Tue May 16 01:29:19 2006" -->
<!-- isoreceived="20060516072919" -->
<!-- sent="Tue, 16 May 2006 17:27:05 +1000" -->
<!-- isosent="20060516072705" -->
<!-- name="Olie Lamb" -->
<!-- email="neomorphy@gmail.com" -->
<!-- subject="Imposing ideas, eg: morality" -->
<!-- id="f3afeba0605160027u1ad2d2b2w2afd74ec85b17dab@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Olie Lamb (<a href="mailto:neomorphy@gmail.com?Subject=Re:%20Imposing%20ideas,%20eg:%20morality"><em>neomorphy@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue May 16 2006 - 01:27:05 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14984.html">Ricardo Barreira: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>Previous message:</strong> <a href="14982.html">John K Clark: "Re: ESSAY: Forward Moral Nihilism."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14986.html">Tennessee Leeuwenburg: "Re: Imposing ideas, eg: morality"</a>
<li><strong>Reply:</strong> <a href="14986.html">Tennessee Leeuwenburg: "Re: Imposing ideas, eg: morality"</a>
<li><strong>Reply:</strong> <a href="14992.html">m.l.vere@durham.ac.uk: "Re: Imposing ideas, eg: morality"</a>
<li><strong>Reply:</strong> <a href="14997.html">Phillip Huggan: "Re: Imposing ideas, eg: morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14983">[ date ]</a>
<a href="index.html#14983">[ thread ]</a>
<a href="subject.html#14983">[ subject ]</a>
<a href="author.html#14983">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
The old bugbear about letting people do what they like without
<br>
imposing on others has reared its ugly head.
<br>
<p>In this case, rather than a set of rules for the people, it's an
<br>
&quot;operational methodology&quot; for an expected superhuman intelligence,
<br>
that might become a sysop.
<br>
<p>Some ethicists have said that
<br>
<p>(**Stipulative characterisation**)
<br>
<p>Morals =  preferences that you want to apply to everyone.
<br>
<p>F'rinstance, if you don't like bullfighting because sport bores you,
<br>
that's a matter of individual preference.  If you don't like anyone
<br>
liking bullfighting because it hurts the bull, by the above
<br>
definition, it's a &quot;moral statement&quot;.
<br>
<p>(Nb: this is NOT my definition.  I'm just using it for one post.)
<br>
<p>With this characterisation, it's very hard to imagine an
<br>
anthropomorphic Sysop not effectively enforcing their &quot;morality&quot; on
<br>
others.  Their operational methodology for weighing the requirements
<br>
of conflicting expressed wills would, in effect, be the Sysop's
<br>
&quot;morality&quot;.
<br>
<p>Just say that a Sysop adopted <a href="mailto:m.l.vere@durham.ac.uk?Subject=Re:%20Imposing%20ideas,%20eg:%20morality">m.l.vere@durham.ac.uk</a> 's two axioms:
<br>
<em>&gt; 1. Prohibiting any action which affects another member of the group,
</em><br>
<em>&gt; unless that member has wilfully expressed for that action to be
</em><br>
<em>&gt; allowed (a form of domain protection).
</em><br>
<p>(Nb: can you say &quot;Golden Rule&quot;?)
<br>
<p><em>&gt; 2. Giving all group members equal resource entitlement
</em><br>
<p>Would you expect such a sysop to not only enforce the axioms directly,
<br>
but also for others to adhere to them where they were operating
<br>
outside the Sysop's influence?  As in, would you expect a Sysop to
<br>
allow Robin to voluntarily accompany Leslie into the woods*, when
<br>
Leslie has admitted that Leslie has a secret plan to &quot;affect another
<br>
member of the group&quot; with an action that is has not been allowed by
<br>
that member of the group, (eg: maim, torture, kill etc Robin).
<br>
<p>* Yes, although a Sysop would normally have influence on temperate
<br>
forested areas, shut up.
<br>
<p>Of course the Sysop is going to influence others to adhere to its
<br>
moral axioms.  Leslie and Robins future actions might take place away
<br>
from the Sysop's field of influence, but the Sysop will always be
<br>
making actions that affect the future, because you can't make actions
<br>
that affect the present!  (Insert TangentT here)
<br>
<p>Brief ad hominem interlude...
<br>
<p>If you expect others to respect your domain, what's that but a form of
<br>
morality?  Hell, you even suggest giving resources out equally.
<br>
Communist!  I happen to own large tracts of land that have more than
<br>
1/6billionth of the planet's solar collection potential and also
<br>
fossil energy reserves buried beneath.*  You ain't stealing my land/
<br>
energy resources!
<br>
<p>* This is a lie.  My point is that one 2006human's share of the
<br>
earth's crust is 85ha, less than what some people own.
<br>
<p>Back to sysops...
<br>
<p>If the sysop is vastly more powerful than other entities, it may be
<br>
able to act in a genie-like way, and grants wishes that don't
<br>
interfere with other human's &quot;domains&quot;.  Why should humans/post-humans
<br>
be forced not to interfere with each others domains?
<br>
<p>For a Sysop, because &quot;might makes right&quot; ;P
<br>
<p>Otherwise, because there might be some (objective?) reason not to.
<br>
<p>Furthermore, why should the sysop not adversely affect humans?
<br>
Because the Sysop's progenitors decided to make it that way.
<br>
<p>As long as an AI is (1) taking actions that affect others (2) Weighing
<br>
the (conflicting interests of other parties (3) weighing its interests
<br>
against those of other parties, it would need some sort of methodology
<br>
to evaluate potential courses of action.  Those that it chooses could
<br>
be called its &quot;preferences&quot;.
<br>
<p>If the AI-builder thinks that the AI should selfish (!!don't try this
<br>
at home, kids!!), the Builder is projecting their preferences onto
<br>
others.  The AI doesn't even need to be conscious for the AI-builder's
<br>
preferences to match the stipulative definition of &quot;moral statements&quot;
<br>
above.
<br>
<p>-- Olie
<br>
<p>On 5/15/06, <a href="mailto:m.l.vere@durham.ac.uk?Subject=Re:%20Imposing%20ideas,%20eg:%20morality">m.l.vere@durham.ac.uk</a> &lt;<a href="mailto:m.l.vere@durham.ac.uk?Subject=Re:%20Imposing%20ideas,%20eg:%20morality">m.l.vere@durham.ac.uk</a>&gt; wrote:
<br>
<em>&gt; So, where would i take my 'moral nihilism'? The reasons I advocated it are the
</em><br>
<em>&gt; following:
</em><br>
<em>&gt;
</em><br>
<em>&gt; All morality is artificial/manmade. This is not an intrisnic negative, however
</em><br>
<em>&gt; it is negative in this case, as:
</em><br>
<em>&gt; 1. Morality made by mere humans would very likely not be suitable/a net
</em><br>
<em>&gt; postivie for posthumans. Therefore we need to go into the singularity without
</em><br>
<em>&gt; imposing morality on our/other posthumans (ie as moral nihilists).
</em><br>
<em>&gt; 2. As morality is artificial, there is no one (or finite number of) 'correct'
</em><br>
<em>&gt; moralit(y)/(ies). Thus it would be better for each individual posthuman to be
</em><br>
<em>&gt; able to develop his/her/its own (or remain a nihlist), than have one posthuman
</em><br>
<em>&gt; morality developed by a sysop.
</em><br>
<em>&gt;
</em><br>
<em>&gt; At the moment, what i would advocate is that
</em><br>
<em>&gt; universal egoists (or moralists who dont want to constrain others with their
</em><br>
<em>&gt; morals) build
</em><br>
<em>&gt; a sysop which grants them all complete self-determination in becoming
</em><br>
<em>&gt; posthuman. My ideas so far (written previously):
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;The best posssible singularity instigator I can imagine would be a
</em><br>
<em>&gt; genie style seed AI, its supergoal being to execute my expressed
</em><br>
<em>&gt; individual will. From here I could do anything that the person/group
</em><br>
<em>&gt; instigating the singularity could do (including asking for any other
</em><br>
<em>&gt; set of goals). In addition I would have the ability to ask for
</em><br>
<em>&gt; advice from a post singularity entity. This is better than having me
</em><br>
<em>&gt; as the instigator, as the AI can function as my guide to
</em><br>
<em>&gt; posthumanity.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If anyone can think of better, please tell.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The chances of such a singularity instigator being built are very
</em><br>
<em>&gt; slim. As such I recomend that a group of people have their expressed
</em><br>
<em>&gt; individual wills excecuted, thus all being motivated to build such
</em><br>
<em>&gt; an AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The problem of conflicting expressed wills can be dealt with by
</em><br>
<em>&gt; 1. Prohibiting any action which affects another member of the group,
</em><br>
<em>&gt; unless that member has wilfully expressed for that action to be
</em><br>
<em>&gt; allowed (a form of domain protection).
</em><br>
<em>&gt; 2. Giving all group members equal resource entitlement
</em><br>
<em>&gt;
</em><br>
<em>&gt; The first condition would only be a problem for moralists and
</em><br>
<em>&gt; megalomaniacs (and not entirely for the latter as there could exist solipism
</em><br>
<em>&gt; stlye simulations for them to control).
</em><br>
<em>&gt; The second seems an inevitable price of striking the best balance
</em><br>
<em>&gt; between the quality of posthumanity and the probability of it
</em><br>
<em>&gt; occuring.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I tentatively recomend that the group in question be all humanity.
</em><br>
<em>&gt; This is to prevent infighting within the group about who is
</em><br>
<em>&gt; included, gain the support of libertarian moralists and weaken the
</em><br>
<em>&gt; strength of opposition - all making it more likely to happen.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is a theory in progress. Idealy, we would have an organisation
</em><br>
<em>&gt; similar to SIAI working on its development/actuallisation. As it is,
</em><br>
<em>&gt; Ive brought it here. Note, I hope to develop this further (preferably from
</em><br>
<em>&gt; the standpoint of moral nihilism).
</em><br>
<em>&gt;
</em><br>
<em>&gt; Whilst the AI interpereting commands may be an issue, I dont see it
</em><br>
<em>&gt; as an unsolvable problem.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Note: I see this as a far better solution to singularity regret than
</em><br>
<em>&gt; SIAI's CV.&quot;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14984.html">Ricardo Barreira: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>Previous message:</strong> <a href="14982.html">John K Clark: "Re: ESSAY: Forward Moral Nihilism."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14986.html">Tennessee Leeuwenburg: "Re: Imposing ideas, eg: morality"</a>
<li><strong>Reply:</strong> <a href="14986.html">Tennessee Leeuwenburg: "Re: Imposing ideas, eg: morality"</a>
<li><strong>Reply:</strong> <a href="14992.html">m.l.vere@durham.ac.uk: "Re: Imposing ideas, eg: morality"</a>
<li><strong>Reply:</strong> <a href="14997.html">Phillip Huggan: "Re: Imposing ideas, eg: morality"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14983">[ date ]</a>
<a href="index.html#14983">[ thread ]</a>
<a href="subject.html#14983">[ subject ]</a>
<a href="author.html#14983">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
