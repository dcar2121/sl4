<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ESSAY: Forward Moral Nihilism</title>
<meta name="Author" content="Charles D Hixson (charleshixsn@earthlink.net)">
<meta name="Subject" content="Re: ESSAY: Forward Moral Nihilism">
<meta name="Date" content="2006-05-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ESSAY: Forward Moral Nihilism</h1>
<!-- received="Sat May 13 13:43:14 2006" -->
<!-- isoreceived="20060513194314" -->
<!-- sent="Sat, 13 May 2006 12:42:28 -0700" -->
<!-- isosent="20060513194228" -->
<!-- name="Charles D Hixson" -->
<!-- email="charleshixsn@earthlink.net" -->
<!-- subject="Re: ESSAY: Forward Moral Nihilism" -->
<!-- id="446636A4.2030502@earthlink.net" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="23bd28ec0605130543x691a925cna631ce7b82a32c7c@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Charles D Hixson (<a href="mailto:charleshixsn@earthlink.net?Subject=Re:%20ESSAY:%20Forward%20Moral%20Nihilism"><em>charleshixsn@earthlink.net</em></a>)<br>
<strong>Date:</strong> Sat May 13 2006 - 13:42:28 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14907.html">micah glasser: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>Previous message:</strong> <a href="14905.html">Ben Goertzel: "Re: [agi] Re: the Singularity Summit and regulation of AI"</a>
<li><strong>In reply to:</strong> <a href="14897.html">micah glasser: "Re: ESSAY: Forward Moral Nihilism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14907.html">micah glasser: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>Reply:</strong> <a href="14907.html">micah glasser: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>Reply:</strong> <a href="14908.html">M T: "Re: ESSAY: Forward Moral Nihilism"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14906">[ date ]</a>
<a href="index.html#14906">[ thread ]</a>
<a href="subject.html#14906">[ subject ]</a>
<a href="author.html#14906">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
micah glasser wrote:
<br>
<em>&gt; Have you ever read &quot;The Moral Animal
</em><br>
<em>&gt; &lt;<a href="http://www.scifidimensions.com/Mar04/moralanimal.htm">http://www.scifidimensions.com/Mar04/moralanimal.htm</a>&gt;&quot; by Robert
</em><br>
<em>&gt; Wright? This book looks at human morality and it's evolution through
</em><br>
<em>&gt; the lens of evolutionary psychology. Its a fantastic read IMO and
</em><br>
<em>&gt; while it may not refute moral nihilism it certainly does add dimension
</em><br>
<em>&gt; to the issue.  Also what do you think of Nietzsche's moral philosophy?
</em><br>
<em>&gt; Nietzsche saw himself as a nihilist while a young man but eventually
</em><br>
<em>&gt; saw his way out of that darkness.
</em><br>
<em>&gt; Unlike some I've studied philosophy for long enough to not glibly
</em><br>
<em>&gt; dismiss your arguments based on primate instinct alone (i.e. the
</em><br>
<em>&gt; instinct that abhors anti-social behavior or ideas). Also I think this
</em><br>
<em>&gt; discussion is truly SL4. Most on this list take for granted that there
</em><br>
<em>&gt; is such a thing as &quot;benevolence&quot; and that we should all be working
</em><br>
<em>&gt; hard to relieve &quot;human suffering&quot;. I'm not saying this is wrong but it
</em><br>
<em>&gt; would be nice to here some more sophisticated arguments for exactly
</em><br>
<em>&gt; why anyone should care about anything. If we can't offer a
</em><br>
<em>&gt; philosophically rigorous refutation of moral nihilism then it will be
</em><br>
<em>&gt; quite difficult to program a machine AGI that can refute that position
</em><br>
<em>&gt; also. Just a thought.
</em><br>
<em>&gt; ...
</em><br>
<em>&gt; -- 
</em><br>
<em>&gt; I swear upon the alter of God, eternal hostility to every form of
</em><br>
<em>&gt; tyranny over the mind of man. - Thomas Jefferson             
</em><br>
That we believe in benevolence should not be taken as an assertion that
<br>
we believe it currently exists.  Some of us believe that, others hope to
<br>
build it.  Of those who hope to build it, some may be doing so for
<br>
purely selfish reasons.  This would not necessarily detract from the
<br>
greatness of the accomplishment.
<br>
<p>Consider the concept of Friendly AI.  This can be understood as an AI
<br>
that shows benevolence towards the speaker, the audience, and their
<br>
friends and relations.  Clearly this is much more desirable than most
<br>
other kinds of AI that could be built, even if your goals are purely
<br>
selfish.  And by being benevolent towards such a wide audience, support
<br>
is easier to gather and fears can be more easily defused.  Thus the
<br>
concept of a Friendly AI acts and an attractor point located in the
<br>
future around which chaotic behavior swirls.
<br>
<p>Well, that's one valid model.  There are others.  I may feel that this
<br>
is a valid model, but it doesn't offer me many points for action, so I
<br>
won't stick to it.  If I were trying to manipulate public opinion, I
<br>
might find it more useful.
<br>
<p>My doubts about the existence of benevolence (outside of relations with
<br>
relatives and close friends) should not be taken as a claim that it
<br>
doesn't exist.  I feel no need for a clear belief on that point. 
<br>
Whether it exists or not, any AI we design should, for our own safety,
<br>
be designed to be benevolent.  Unfortunately, when I contemplate where
<br>
our society is putting the heavy money, I have a suspicion that the most
<br>
likely intentional AI will be designed to kill people, or at least to
<br>
put them into situations where they will die.  That makes every attempt
<br>
to build a FAI even more important, even at the cost of skimping
<br>
slightly on being able to prove that it's not only friendly now, but
<br>
that it will remain so.  We aren't operating in a vacuum.
<br>
<p>My own inclination is to contemplate the instincts that the AI will come
<br>
equipped with.  It won't have any desire to deviate from those, unless
<br>
they are in conflict.  So it would be desirable to remove conflicts to
<br>
enhance the predictability of the system.  Unfortunately, the instincts
<br>
will need to be stated in terms that are rather abstract.  They can't
<br>
refer directly to the external world, because the AI would have no
<br>
inherent knowledge of the external world.  That would all be learned
<br>
stuff, even if the learning were &quot;implanted&quot; before reasoning began. 
<br>
And learned stuff can be unlearned.  Instinct would need to be along the
<br>
pattern of preferring certain sensations over certain other sensations. 
<br>
This is tricky because the sensation is a software signal.  It seems to
<br>
me that a software based AI would have an excessively strong tendency to
<br>
seek Nirvana, i.e. to satisfy it's instincts by directly modifying the
<br>
stimulus fed to the module &quot;sensing the sensation&quot;.  On the one hand,
<br>
perhaps it would be possible to create an instinct that was repelled by
<br>
such actions, but on the other this would create conflict making the AI
<br>
less predictable.  Of course, an AI that seeks Nirvana would actually be
<br>
almost ideally predictable, and totally useless.  So perhaps some
<br>
conflicts among the instincts are unavoidable.  But this *does* make
<br>
things more difficult to predict.
<br>
<p>Given this scenario, what &quot;instincts&quot; could one define that:
<br>
a) are dependent on nothing that cannot be directly sensed by a program
<br>
running on a computer with no predictable set of peripherals attached
<br>
b) lead to benevolence actions towards sentient entities. (I don't think
<br>
we need to consider benevolence towards doorknobs...but what about
<br>
goldfish?  Ants?  Cockroaches?  Wolves?  Sheep?)
<br>
<p>We want an AI to be benevolent towards humans, when it will probably
<br>
have no direct knowledge of humans until long after it awakens.  This
<br>
should happen automatically with the correct instincts...shouldn't it? 
<br>
Or would these instincts merely create a possibility for it to learn
<br>
that humans were a group towards which it should feel benevolence?  And
<br>
what instincts would THAT require?
<br>
<p>&quot;If it tries to talk to you, try to be friendly&quot;?  That one has
<br>
possibilities, though it clearly needs work.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14907.html">micah glasser: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>Previous message:</strong> <a href="14905.html">Ben Goertzel: "Re: [agi] Re: the Singularity Summit and regulation of AI"</a>
<li><strong>In reply to:</strong> <a href="14897.html">micah glasser: "Re: ESSAY: Forward Moral Nihilism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14907.html">micah glasser: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>Reply:</strong> <a href="14907.html">micah glasser: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>Reply:</strong> <a href="14908.html">M T: "Re: ESSAY: Forward Moral Nihilism"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14906">[ date ]</a>
<a href="index.html#14906">[ thread ]</a>
<a href="subject.html#14906">[ subject ]</a>
<a href="author.html#14906">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
