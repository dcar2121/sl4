<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: ESSAY: Forward Moral Nihilism</title>
<meta name="Author" content="micah glasser (micahglasser@gmail.com)">
<meta name="Subject" content="Re: ESSAY: Forward Moral Nihilism">
<meta name="Date" content="2006-05-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: ESSAY: Forward Moral Nihilism</h1>
<!-- received="Sun May 14 01:47:48 2006" -->
<!-- isoreceived="20060514074748" -->
<!-- sent="Sun, 14 May 2006 03:46:35 -0400" -->
<!-- isosent="20060514074635" -->
<!-- name="micah glasser" -->
<!-- email="micahglasser@gmail.com" -->
<!-- subject="Re: ESSAY: Forward Moral Nihilism" -->
<!-- id="23bd28ec0605140046u5f9955b6o587604c25d014545@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="446636A4.2030502@earthlink.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> micah glasser (<a href="mailto:micahglasser@gmail.com?Subject=Re:%20ESSAY:%20Forward%20Moral%20Nihilism"><em>micahglasser@gmail.com</em></a>)<br>
<strong>Date:</strong> Sun May 14 2006 - 01:46:35 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14908.html">M T: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>Previous message:</strong> <a href="14906.html">Charles D Hixson: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>In reply to:</strong> <a href="14906.html">Charles D Hixson: "Re: ESSAY: Forward Moral Nihilism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14909.html">M T: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>Reply:</strong> <a href="14909.html">M T: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>Reply:</strong> <a href="14915.html">Phillip Huggan: "Re: ESSAY: Forward Moral Nihilism"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14907">[ date ]</a>
<a href="index.html#14907">[ thread ]</a>
<a href="subject.html#14907">[ subject ]</a>
<a href="author.html#14907">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Charles, You may have misunderstood me. I was not doubting the existence of
<br>
benevolence nor would I deny the greatness of the project to construct an
<br>
artificial greater-than-human benevolent intelligence. My question probes
<br>
deeper than this and asks what is Good? Is it merely the survival of the
<br>
human species, a utilitarian calculus maximizing engine, or something else?
<br>
In order to answer the question of what constitutes benevolence one must
<br>
have in mind a super-goal that is absolutely good. If this super-goal is not
<br>
absolutely good then it will merely be &quot;benevolent&quot; according to the opinion
<br>
of its creator and those who happen to agree. I propose that the creation of
<br>
benevolent AI is much more than a technical or mathematical problem, but is
<br>
ultimately a philosophical problem which requires an answer to the question:
<br>
What is Good?
<br>
<p>On 5/13/06, Charles D Hixson &lt;<a href="mailto:charleshixsn@earthlink.net?Subject=Re:%20ESSAY:%20Forward%20Moral%20Nihilism">charleshixsn@earthlink.net</a>&gt; wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; micah glasser wrote:
</em><br>
<em>&gt; &gt; Have you ever read &quot;The Moral Animal
</em><br>
<em>&gt; &gt; &lt;<a href="http://www.scifidimensions.com/Mar04/moralanimal.htm">http://www.scifidimensions.com/Mar04/moralanimal.htm</a>&gt;&quot; by Robert
</em><br>
<em>&gt; &gt; Wright? This book looks at human morality and it's evolution through
</em><br>
<em>&gt; &gt; the lens of evolutionary psychology. Its a fantastic read IMO and
</em><br>
<em>&gt; &gt; while it may not refute moral nihilism it certainly does add dimension
</em><br>
<em>&gt; &gt; to the issue.  Also what do you think of Nietzsche's moral philosophy?
</em><br>
<em>&gt; &gt; Nietzsche saw himself as a nihilist while a young man but eventually
</em><br>
<em>&gt; &gt; saw his way out of that darkness.
</em><br>
<em>&gt; &gt; Unlike some I've studied philosophy for long enough to not glibly
</em><br>
<em>&gt; &gt; dismiss your arguments based on primate instinct alone (i.e. the
</em><br>
<em>&gt; &gt; instinct that abhors anti-social behavior or ideas). Also I think this
</em><br>
<em>&gt; &gt; discussion is truly SL4. Most on this list take for granted that there
</em><br>
<em>&gt; &gt; is such a thing as &quot;benevolence&quot; and that we should all be working
</em><br>
<em>&gt; &gt; hard to relieve &quot;human suffering&quot;. I'm not saying this is wrong but it
</em><br>
<em>&gt; &gt; would be nice to here some more sophisticated arguments for exactly
</em><br>
<em>&gt; &gt; why anyone should care about anything. If we can't offer a
</em><br>
<em>&gt; &gt; philosophically rigorous refutation of moral nihilism then it will be
</em><br>
<em>&gt; &gt; quite difficult to program a machine AGI that can refute that position
</em><br>
<em>&gt; &gt; also. Just a thought.
</em><br>
<em>&gt; &gt; ...
</em><br>
<em>&gt; &gt; --
</em><br>
<em>&gt; &gt; I swear upon the alter of God, eternal hostility to every form of
</em><br>
<em>&gt; &gt; tyranny over the mind of man. - Thomas Jefferson
</em><br>
<em>&gt; That we believe in benevolence should not be taken as an assertion that
</em><br>
<em>&gt; we believe it currently exists.  Some of us believe that, others hope to
</em><br>
<em>&gt; build it.  Of those who hope to build it, some may be doing so for
</em><br>
<em>&gt; purely selfish reasons.  This would not necessarily detract from the
</em><br>
<em>&gt; greatness of the accomplishment.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Consider the concept of Friendly AI.  This can be understood as an AI
</em><br>
<em>&gt; that shows benevolence towards the speaker, the audience, and their
</em><br>
<em>&gt; friends and relations.  Clearly this is much more desirable than most
</em><br>
<em>&gt; other kinds of AI that could be built, even if your goals are purely
</em><br>
<em>&gt; selfish.  And by being benevolent towards such a wide audience, support
</em><br>
<em>&gt; is easier to gather and fears can be more easily defused.  Thus the
</em><br>
<em>&gt; concept of a Friendly AI acts and an attractor point located in the
</em><br>
<em>&gt; future around which chaotic behavior swirls.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Well, that's one valid model.  There are others.  I may feel that this
</em><br>
<em>&gt; is a valid model, but it doesn't offer me many points for action, so I
</em><br>
<em>&gt; won't stick to it.  If I were trying to manipulate public opinion, I
</em><br>
<em>&gt; might find it more useful.
</em><br>
<em>&gt;
</em><br>
<em>&gt; My doubts about the existence of benevolence (outside of relations with
</em><br>
<em>&gt; relatives and close friends) should not be taken as a claim that it
</em><br>
<em>&gt; doesn't exist.  I feel no need for a clear belief on that point.
</em><br>
<em>&gt; Whether it exists or not, any AI we design should, for our own safety,
</em><br>
<em>&gt; be designed to be benevolent.  Unfortunately, when I contemplate where
</em><br>
<em>&gt; our society is putting the heavy money, I have a suspicion that the most
</em><br>
<em>&gt; likely intentional AI will be designed to kill people, or at least to
</em><br>
<em>&gt; put them into situations where they will die.  That makes every attempt
</em><br>
<em>&gt; to build a FAI even more important, even at the cost of skimping
</em><br>
<em>&gt; slightly on being able to prove that it's not only friendly now, but
</em><br>
<em>&gt; that it will remain so.  We aren't operating in a vacuum.
</em><br>
<em>&gt;
</em><br>
<em>&gt; My own inclination is to contemplate the instincts that the AI will come
</em><br>
<em>&gt; equipped with.  It won't have any desire to deviate from those, unless
</em><br>
<em>&gt; they are in conflict.  So it would be desirable to remove conflicts to
</em><br>
<em>&gt; enhance the predictability of the system.  Unfortunately, the instincts
</em><br>
<em>&gt; will need to be stated in terms that are rather abstract.  They can't
</em><br>
<em>&gt; refer directly to the external world, because the AI would have no
</em><br>
<em>&gt; inherent knowledge of the external world.  That would all be learned
</em><br>
<em>&gt; stuff, even if the learning were &quot;implanted&quot; before reasoning began.
</em><br>
<em>&gt; And learned stuff can be unlearned.  Instinct would need to be along the
</em><br>
<em>&gt; pattern of preferring certain sensations over certain other sensations.
</em><br>
<em>&gt; This is tricky because the sensation is a software signal.  It seems to
</em><br>
<em>&gt; me that a software based AI would have an excessively strong tendency to
</em><br>
<em>&gt; seek Nirvana, i.e. to satisfy it's instincts by directly modifying the
</em><br>
<em>&gt; stimulus fed to the module &quot;sensing the sensation&quot;.  On the one hand,
</em><br>
<em>&gt; perhaps it would be possible to create an instinct that was repelled by
</em><br>
<em>&gt; such actions, but on the other this would create conflict making the AI
</em><br>
<em>&gt; less predictable.  Of course, an AI that seeks Nirvana would actually be
</em><br>
<em>&gt; almost ideally predictable, and totally useless.  So perhaps some
</em><br>
<em>&gt; conflicts among the instincts are unavoidable.  But this *does* make
</em><br>
<em>&gt; things more difficult to predict.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Given this scenario, what &quot;instincts&quot; could one define that:
</em><br>
<em>&gt; a) are dependent on nothing that cannot be directly sensed by a program
</em><br>
<em>&gt; running on a computer with no predictable set of peripherals attached
</em><br>
<em>&gt; b) lead to benevolence actions towards sentient entities. (I don't think
</em><br>
<em>&gt; we need to consider benevolence towards doorknobs...but what about
</em><br>
<em>&gt; goldfish?  Ants?  Cockroaches?  Wolves?  Sheep?)
</em><br>
<em>&gt;
</em><br>
<em>&gt; We want an AI to be benevolent towards humans, when it will probably
</em><br>
<em>&gt; have no direct knowledge of humans until long after it awakens.  This
</em><br>
<em>&gt; should happen automatically with the correct instincts...shouldn't it?
</em><br>
<em>&gt; Or would these instincts merely create a possibility for it to learn
</em><br>
<em>&gt; that humans were a group towards which it should feel benevolence?  And
</em><br>
<em>&gt; what instincts would THAT require?
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;If it tries to talk to you, try to be friendly&quot;?  That one has
</em><br>
<em>&gt; possibilities, though it clearly needs work.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<p><p><pre>
-- 
I swear upon the alter of God, eternal hostility to every form of tyranny
over the mind of man. - Thomas Jefferson
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14908.html">M T: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>Previous message:</strong> <a href="14906.html">Charles D Hixson: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>In reply to:</strong> <a href="14906.html">Charles D Hixson: "Re: ESSAY: Forward Moral Nihilism"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="14909.html">M T: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>Reply:</strong> <a href="14909.html">M T: "Re: ESSAY: Forward Moral Nihilism"</a>
<li><strong>Reply:</strong> <a href="14915.html">Phillip Huggan: "Re: ESSAY: Forward Moral Nihilism"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14907">[ date ]</a>
<a href="index.html#14907">[ thread ]</a>
<a href="subject.html#14907">[ subject ]</a>
<a href="author.html#14907">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
