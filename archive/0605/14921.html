<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Guide AI theory (was Forward Moral Nihilism)</title>
<meta name="Author" content="m.l.vere@durham.ac.uk (m.l.vere@durham.ac.uk)">
<meta name="Subject" content="Guide AI theory (was Forward Moral Nihilism)">
<meta name="Date" content="2006-05-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Guide AI theory (was Forward Moral Nihilism)</h1>
<!-- received="Sun May 14 16:31:40 2006" -->
<!-- isoreceived="20060514223140" -->
<!-- sent="Sun, 14 May 2006 23:31:21 +0100" -->
<!-- isosent="20060514223121" -->
<!-- name="m.l.vere@durham.ac.uk" -->
<!-- email="m.l.vere@durham.ac.uk" -->
<!-- subject="Guide AI theory (was Forward Moral Nihilism)" -->
<!-- id="1147645881.4467afb9af17c@webmailimpc.dur.ac.uk" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> <a href="mailto:m.l.vere@durham.ac.uk?Subject=Re:%20Guide%20AI%20theory%20(was%20Forward%20Moral%20Nihilism)"><em>m.l.vere@durham.ac.uk</em></a><br>
<strong>Date:</strong> Sun May 14 2006 - 16:31:21 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="14922.html">Keith Henson: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<li><strong>Previous message:</strong> <a href="14920.html">Phillip Huggan: "Re: ESSAY: Forward Moral Nihilism"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14921">[ date ]</a>
<a href="index.html#14921">[ thread ]</a>
<a href="subject.html#14921">[ subject ]</a>
<a href="author.html#14921">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
So, where would i take my 'moral nihilism'? The reasons I advocated it are the 
<br>
following:
<br>
<p>All morality is artificial/manmade. This is not an intrisnic negative, however 
<br>
it is negative in this case, as:
<br>
1. Morality made by mere humans would very likely not be suitable/a net 
<br>
postivie for posthumans. Therefore we need to go into the singularity without 
<br>
imposing morality on our/other posthumans (ie as moral nihilists).
<br>
2. As morality is artificial, there is no one (or finite number of) 'correct' 
<br>
moralit(y)/(ies). Thus it would be better for each individual posthuman to be 
<br>
able to develop his/her/its own (or remain a nihlist), than have one posthuman 
<br>
morality developed by a sysop.
<br>
<p>At the moment, what i would advocate is that 
<br>
universal egoists (or moralists who dont want to constrain others with their 
<br>
morals) build 
<br>
a sysop which grants them all complete self-determination in becoming 
<br>
posthuman. My ideas so far (written previously):
<br>
<p>&quot;The best posssible singularity instigator I can imagine would be a
<br>
genie style seed AI, its supergoal being to execute my expressed
<br>
individual will. From here I could do anything that the person/group
<br>
instigating the singularity could do (including asking for any other
<br>
set of goals). In addition I would have the ability to ask for
<br>
advice from a post singularity entity. This is better than having me
<br>
as the instigator, as the AI can function as my guide to
<br>
posthumanity.
<br>
<p>If anyone can think of better, please tell.
<br>
<p>The chances of such a singularity instigator being built are very
<br>
slim. As such I recomend that a group of people have their expressed
<br>
individual wills excecuted, thus all being motivated to build such
<br>
an AI.
<br>
<p>The problem of conflicting expressed wills can be dealt with by
<br>
1. Prohibiting any action which affects another member of the group,
<br>
unless that member has wilfully expressed for that action to be
<br>
allowed (a form of domain protection).
<br>
2. Giving all group members equal resource entitlement
<br>
<p>The first condition would only be a problem for moralists and
<br>
megalomaniacs (and not entirely for the latter as there could exist solipism 
<br>
stlye simulations for them to control).
<br>
The second seems an inevitable price of striking the best balance
<br>
between the quality of posthumanity and the probability of it
<br>
occuring.
<br>
<p>I tentatively recomend that the group in question be all humanity.
<br>
This is to prevent infighting within the group about who is
<br>
included, gain the support of libertarian moralists and weaken the
<br>
strength of opposition - all making it more likely to happen.
<br>
<p>This is a theory in progress. Idealy, we would have an organisation
<br>
similar to SIAI working on its development/actuallisation. As it is,
<br>
Ive brought it here. Note, I hope to develop this further (preferably from
<br>
the standpoint of moral nihilism).
<br>
<p>Whilst the AI interpereting commands may be an issue, I dont see it
<br>
as an unsolvable problem.
<br>
<p>Note: I see this as a far better solution to singularity regret than
<br>
SIAI's CV.&quot;
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="14922.html">Keith Henson: "Re: ESSAY: Forward Moral Nihilism (EP)"</a>
<li><strong>Previous message:</strong> <a href="14920.html">Phillip Huggan: "Re: ESSAY: Forward Moral Nihilism"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#14921">[ date ]</a>
<a href="index.html#14921">[ thread ]</a>
<a href="subject.html#14921">[ subject ]</a>
<a href="author.html#14921">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:56 MDT
</em></small></p>
</body>
</html>
