<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] Friendly AIs vs Friendly Humans</title>
<meta name="Author" content="Philip Goetz (philgoetz@gmail.com)">
<meta name="Subject" content="Re: [sl4] Friendly AIs vs Friendly Humans">
<meta name="Date" content="2011-10-31">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] Friendly AIs vs Friendly Humans</h1>
<!-- received="Mon Oct 31 13:36:26 2011" -->
<!-- isoreceived="20111031193626" -->
<!-- sent="Mon, 31 Oct 2011 15:36:18 -0400" -->
<!-- isosent="20111031193618" -->
<!-- name="Philip Goetz" -->
<!-- email="philgoetz@gmail.com" -->
<!-- subject="Re: [sl4] Friendly AIs vs Friendly Humans" -->
<!-- id="CALqisM=aysuGwyjQbLiwLw-XJH5YATCyuC0DNOaiqTiD3NGogQ@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="BANLkTi=6djzBJeBXG+PDeVLgDZnD9BUeQQ@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Philip Goetz (<a href="mailto:philgoetz@gmail.com?Subject=Re:%20[sl4]%20Friendly%20AIs%20vs%20Friendly%20Humans"><em>philgoetz@gmail.com</em></a>)<br>
<strong>Date:</strong> Mon Oct 31 2011 - 13:36:18 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="21197.html">natasha@natasha.cc: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<li><strong>Previous message:</strong> <a href="21195.html">Gabriel Charron: "Re: [sl4] Singularity Summit 2011"</a>
<li><strong>In reply to:</strong> <a href="../1106/21168.html">DataPacRat: "[sl4] Friendly AIs vs Friendly Humans"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21197.html">natasha@natasha.cc: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<li><strong>Reply:</strong> <a href="21197.html">natasha@natasha.cc: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21196">[ date ]</a>
<a href="index.html#21196">[ thread ]</a>
<a href="subject.html#21196">[ subject ]</a>
<a href="author.html#21196">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Tue, Jun 21, 2011 at 2:36 AM, DataPacRat &lt;<a href="mailto:datapacrat@gmail.com?Subject=Re:%20[sl4]%20Friendly%20AIs%20vs%20Friendly%20Humans">datapacrat@gmail.com</a>&gt; wrote:
<br>
<em>&gt; My understanding of the Friendly AI problem is, roughly, that AIs
</em><br>
<em>&gt; could have all sorts of goal systems, many of which are rather
</em><br>
<em>&gt; unhealthy for humanity as we know it; and, due to the potential for
</em><br>
<em>&gt; rapid self-improvement, once any AI exists, it is highly likely to
</em><br>
<em>&gt; rapidly gain the power required to implement its goals whether we want
</em><br>
<em>&gt; it to or not. Thus certain people are trying to develop the parameters
</em><br>
<em>&gt; for a Friendly AI, one that will allow us humans to continue doing our
</em><br>
<em>&gt; own things (or some approximation thereof), or at least for avoiding
</em><br>
<em>&gt; the development of an Unfriendly AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; From what I've overheard, one of the biggest difficulties with FAI is
</em><br>
<em>&gt; that there are a wide variety of possible forms of AI, making it
</em><br>
<em>&gt; difficult to determine what it would take to ensure Friendliness for
</em><br>
<em>&gt; any potential AI design.
</em><br>
<p>There are 4 chief difficulties with FAI; and the one that is
<br>
most important and most difficult is the one that people
<br>
in the SIAI say is not a problem.
<br>
<p>The fourth-most-difficult and fourth-most-important problem is
<br>
how to design an AI so that you can define a set of goals,
<br>
and ensure that it will always and forever seek to satisfy
<br>
those goals and no others.
<br>
<p>The third-most-difficult problem is that the ideas, assumptions,
<br>
probability estimates, and lack of exponential time-discounting
<br>
put forth by Eliezer justify killing billions of people in order to slightly
<br>
reduce the possibility of an &quot;UFAI&quot;.  Since the SIAI definition
<br>
of UFAI is in practice &quot;anything not designed by Eliezer Yudkowsky&quot;,
<br>
it is not improbable that people on this mailing list will eventually
<br>
be assassinated by people influenced by SIAI.
<br>
I know that various people have denied this is the case;
<br>
but that's like Euclid putting forth his five postulates and
<br>
then denying that they imply that three line segments uniquely
<br>
determine a triangle.
<br>
See <a href="http://lesswrong.com/lw/3xg/put_all_your_eggs_in_one_basket">http://lesswrong.com/lw/3xg/put_all_your_eggs_in_one_basket</a>
<br>
<p>The second-most difficult problem is how to figure out
<br>
what our goals are.  The proposed solution, Coherent
<br>
Extrapolated Volition (CEV), has a number of problems
<br>
that I have explained at length to people in SIAI,
<br>
largely at these links, without having any impact as far as I can tell:
<br>
<p><a href="http://lesswrong.com/lw/256/only_humans_can_have_human_values/">http://lesswrong.com/lw/256/only_humans_can_have_human_values/</a>
<br>
<a href="http://lesswrong.com/lw/262/averaging_value_systems_is_worse_than_choosing_one/">http://lesswrong.com/lw/262/averaging_value_systems_is_worse_than_choosing_one/</a>
<br>
<a href="http://lesswrong.com/lw/1xa/human_values_differ_as_much_as_values_can_differ/">http://lesswrong.com/lw/1xa/human_values_differ_as_much_as_values_can_differ/</a>
<br>
<a href="http://lesswrong.com/lw/55n/human_errors_human_values/">http://lesswrong.com/lw/55n/human_errors_human_values/</a>
<br>
<a href="http://lesswrong.com/lw/256/biases_are_values/">http://lesswrong.com/lw/256/biases_are_values/</a>
<br>
<a href="http://lesswrong.com/lw/5q9/values_vs_parameters/">http://lesswrong.com/lw/5q9/values_vs_parameters/</a>
<br>
<p>One problem not touched on in those essays is that CEV would cause our
<br>
AI to kill gays and do other things that most of us think would be
<br>
horrible, but that are the values held by most people in the world.
<br>
The usual reply is to say that people wouldn't value these things if
<br>
they were smarter because they are instrumental and not terminal
<br>
values.  I disagree; hating gays does not appear to be instrumental to
<br>
any goal.  It's just a value some of us don't share.  Values are
<br>
irrational.  That's a prerequisite for being a value.
<br>
<p>The most difficult, most important, and least-thought-about
<br>
problem, is that the SIAI's approach is to build an AI that will
<br>
take over the entire universe and use it to optimize whatever
<br>
utility function is built into it, for ever and ever;
<br>
and this might be bad even if we have a single utility
<br>
function and figure out what it is and the AI succeeds at optimizing it.  See
<br>
<p><a href="http://lesswrong.com/lw/20x/the_human_problem">http://lesswrong.com/lw/20x/the_human_problem</a>
<br>
<p>It doesn't get into some important philosophical roots of the problem,
<br>
which are also not acknowledged to be real problems by anyone I have
<br>
spoken to in SIAI:
<br>
- Why should I, a mortal being having a utility function, feel
<br>
obligated to create something that will keep optimizing that utility
<br>
function after I am dead?
<br>
- Is it possible to optimize &quot;my&quot; utility function after &quot;I&quot; am gone
<br>
if the utility function is indexical (refers to &quot;the owner of this
<br>
utility function&quot; in its goals)?
<br>
- If we should logically strive to create something that does not
<br>
change its utility function over time, why do we change our own
<br>
utility functions over time?  Shouldn't we prevent that, too?
<br>
<p>The answer most SIAI people will give, eventually, after we figure out
<br>
what each mean, is that it does make sense if we define the utility
<br>
function at the highest level of abstraction.  Some problems with that
<br>
answer are:
<br>
- I doubt that the &quot;real human values&quot;, supposing they exist, are ones
<br>
that are so abstract that most people alive today are incapable of
<br>
conceiving of them.
<br>
- In every discussion I have read about CEV, people discuss values two
<br>
or four levels of abstraction too low; and nobody from SIAI jumps in
<br>
and says, &quot;But of course really we would use more abstract values.&quot;
<br>
- The high-level abstract values useful for running a universe throw
<br>
out most of the low-level, concrete, true human values, like the
<br>
values of winning football games and beating up gays.
<br>
<p>The biggest meta-problem with the SIAI's FAI project is that there is
<br>
a large set of difficult, barely-studied problems, for each of which
<br>
most people in the SIAI have already adopted Eliezer's answer as
<br>
proven.  Getting everybody working on the problem to live together in
<br>
a communal-living situation has worsened the pre-existing premature
<br>
convergence of ideas.  What the SIAI needs most is for people not in
<br>
the SIAI to think about the problem.
<br>
<p>- Phil Goetz
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="21197.html">natasha@natasha.cc: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<li><strong>Previous message:</strong> <a href="21195.html">Gabriel Charron: "Re: [sl4] Singularity Summit 2011"</a>
<li><strong>In reply to:</strong> <a href="../1106/21168.html">DataPacRat: "[sl4] Friendly AIs vs Friendly Humans"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21197.html">natasha@natasha.cc: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<li><strong>Reply:</strong> <a href="21197.html">natasha@natasha.cc: "Re: [sl4] Friendly AIs vs Friendly Humans"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21196">[ date ]</a>
<a href="index.html#21196">[ thread ]</a>
<a href="subject.html#21196">[ subject ]</a>
<a href="author.html#21196">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
