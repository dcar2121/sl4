<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Is generalisation a limit to intelligence?</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Is generalisation a limit to intelligence?">
<meta name="Date" content="2000-12-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Is generalisation a limit to intelligence?</h1>
<!-- received="Sat Dec 02 16:33:12 2000" -->
<!-- isoreceived="20001202233312" -->
<!-- sent="Sat, 02 Dec 2000 16:21:18 -0500" -->
<!-- isosent="20001202212118" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Is generalisation a limit to intelligence?" -->
<!-- id="3A2967CE.E8B1D097@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="JBEPKOGDDIKKAHFPOEFIOEAKCDAA.ben@webmind.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Is%20generalisation%20a%20limit%20to%20intelligence?"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Dec 02 2000 - 14:21:18 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0338.html">Ben Goertzel: "RE: Is generalisation a limit to intelligence?"</a>
<li><strong>Previous message:</strong> <a href="0336.html">Samantha Atkins: "RE: Is generalisation a limit to intelligence?"</a>
<li><strong>In reply to:</strong> <a href="0330.html">Ben Goertzel: "RE: Is generalisation a limit to intelligence?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0338.html">Ben Goertzel: "RE: Is generalisation a limit to intelligence?"</a>
<li><strong>Reply:</strong> <a href="0338.html">Ben Goertzel: "RE: Is generalisation a limit to intelligence?"</a>
<li><strong>Reply:</strong> <a href="0342.html">Joaquim Almgren Gândara: "Re: Is generalisation a limit to intelligence?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#337">[ date ]</a>
<a href="index.html#337">[ thread ]</a>
<a href="subject.html#337">[ subject ]</a>
<a href="author.html#337">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &gt; To sum it up: Is there some way to combine the fuzzy quality that
</em><br>
<em>&gt; &gt; intelligence
</em><br>
<em>&gt; &gt; relies on with the rigid quality of not making a single mistake? Is
</em><br>
<em>&gt; &gt; generalisation a limit to intelligence?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; With infinite memory, a mind could store both the generalization it had made
</em><br>
<em>&gt; from some
</em><br>
<em>&gt; raw data, and all the raw data itself.  Thus it could use the generalization
</em><br>
<em>&gt; to make predictions,
</em><br>
<em>&gt; and it could also use the raw data to continually re-evaluate its
</em><br>
<em>&gt; generalization to be sure it
</em><br>
<em>&gt; still worked.
</em><br>
<p>&quot;Infinite memory&quot; depends on whether you just want to record experiential,
<br>
external-reality data, or record internal-reality data as well, or just
<br>
record a few checkpoints and store the seed data in the random-number
<br>
generators.  Storing every internal state is not possible, but it may not
<br>
take all *that* much memory to create a perfect flight recorder capable of
<br>
exactly recreating the mind's state at any given clock tick.  There will,
<br>
however, be a substantial computational cost for accessing that &quot;memory&quot;,
<br>
which will increase with increasing time.  Let's say you've got a mind
<br>
running on 100TB of RAM with experience coming in at 1GB/second (including
<br>
'Net connection) and access to 100PB of disk space.  If half the disk
<br>
space is used for experience, then the mind can store 50,000,000 seconds
<br>
of experience - around one and a half years.  If the remaining 50PB is
<br>
used for snapshots, then that's enough to store a total of 500 snapshots. 
<br>
The first 50 snapshots could be hundredth-second snapshots, the next 50
<br>
snapshots could be tenth-second snapshots, and so on, going up
<br>
(theoretically) to 50 different million-second snapshots.  Accessing an
<br>
exact internal state, to the exact clock tick, that occurred within the
<br>
last half second (50 hundredth-second snapshots), would cost at most one
<br>
hundredth of a second.  All of this assumes infinite disk access speeds. 
<br>
In any case, the upshot is that a Diasporan upload might find &quot;perfect&quot;
<br>
memory to not actually be that much of a demand, especially if they have
<br>
vastly more living space than they're using (yet!).
<br>
<p>Leave that aside.  Ben Goertzel clarified a part of Gandara's thesis that
<br>
hadn't made any sense to me; e.g., that forced generalization may result
<br>
in necessary creativity.  I had thought Gandara was proposing that perfect
<br>
memory *prevented* generalization.  I think that perhaps the answer here
<br>
may lie in the difference between *producing* creativity and *verifying*
<br>
creativity; creativity, especially the kind of brilliant solutions
<br>
produced by generalization, is often much easier to verify than produce. 
<br>
Thus you can create all the degrees of generalization, from zero to nearly
<br>
complete abstraction, run them all, and pluck off the best solutions from
<br>
each.  Or you could just use the most frequently-useful generalization
<br>
most of the time, while still checking more specific and more general
<br>
levels every so often, just in case... the kind of &quot;heuristics tuning
<br>
heuristics&quot; thing that EURISKO was so good at.
<br>
<p>As Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; But, the refutation: A sufficiently intelligent, self-aware system is
</em><br>
<em>&gt; quite capable of modifying itself to make itself MORE ERROR-PRONE if it
</em><br>
<em>&gt; finds through experimentation that this makes it more intelligent ;&gt;
</em><br>
<p>Precisely.
<br>
<p>Given a specified memory base, you have the precise experiences, the
<br>
first-order generalization from a group of nearly identical experiences,
<br>
the second-order generalization from groups of experiences that share many
<br>
but not all characteristics but still share a common usage or
<br>
manipulability, the fourth-order generalization from groups that are not
<br>
treated in the same way but still have a single characteristic of
<br>
interest, and so on.  (&quot;Apple 3132&quot;, &quot;Red Delicious Apple&quot;, &quot;Apple&quot;,
<br>
&quot;Plant Life&quot;).  I don't see that storing all or none experience changes
<br>
the nature of this classical hierarchy; rather, it increases your ability
<br>
to generalize specifically from Apple 3132 if you wind up in a situation
<br>
that is almost precisely similar to Apple 3132 rather than any of the
<br>
other categories you have available.  This is very rarely required for an
<br>
intelligent being, which is why we don't store all of our memories using
<br>
snapshots...
<br>
<p>Actually, I don't know that!  It could be precisely the other way around. 
<br>
It could be that generalizing from Apple 3132 is incredibly useful, but we
<br>
don't have the neural disk space available to store specific experiences,
<br>
so we've evolved this entire mode of thinking that's so utterly tuned to
<br>
generalization that, in our poverty of existence, it looks to us like
<br>
generalizing from precise experiences is &quot;very rarely required&quot;.  But I
<br>
think evolution is right on this one.  Precise experiences are almost
<br>
always superfluous.  I propose the snapshot method simply on the principle
<br>
that information should never be lost... if you have the disk space
<br>
available.
<br>
<p><em>&gt; However, we lack a quantitative science that can tell us exactly how quickly
</em><br>
<em>&gt; the error rate approaches
</em><br>
<em>&gt; zero as the memory (&amp;, in a real-time situation, processing power able to
</em><br>
<em>&gt; exploit this memory)
</em><br>
<em>&gt; approaches infinity.  Eliezer and I differ in that I believe such a science
</em><br>
<em>&gt; will someday exist ;&gt;
</em><br>
<em>&gt; We also differ in that he intuits this error rate approaches zero faster
</em><br>
<em>&gt; than I intuit it does.
</em><br>
<p>Let us also note that there is a single cause behind both of my beliefs; I
<br>
believe that generalizing is a creative and intelligent task, which to me
<br>
means there's room for arbitrarily brilliant solutions.  The error rate
<br>
approaches zero very quickly, not for mathematical reasons, but because
<br>
someone came up with a brilliant solution - or a thousand different
<br>
brilliant solutions for a thousand different domains.  Each different
<br>
solution has a different mathematical behavior, if it has any mathematical
<br>
behavior at all.  The act of coming up with a brilliant solution changes
<br>
whatever mathematical behavior previously existed, and will not be a
<br>
continuation of the previous curve.  This drama is played out on so many
<br>
different levels of the system - mathematics is so easily broken by the
<br>
application of intelligence, at any level - as to render it likely that
<br>
mathematics will simply not be used at all.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0338.html">Ben Goertzel: "RE: Is generalisation a limit to intelligence?"</a>
<li><strong>Previous message:</strong> <a href="0336.html">Samantha Atkins: "RE: Is generalisation a limit to intelligence?"</a>
<li><strong>In reply to:</strong> <a href="0330.html">Ben Goertzel: "RE: Is generalisation a limit to intelligence?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0338.html">Ben Goertzel: "RE: Is generalisation a limit to intelligence?"</a>
<li><strong>Reply:</strong> <a href="0338.html">Ben Goertzel: "RE: Is generalisation a limit to intelligence?"</a>
<li><strong>Reply:</strong> <a href="0342.html">Joaquim Almgren Gândara: "Re: Is generalisation a limit to intelligence?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#337">[ date ]</a>
<a href="index.html#337">[ thread ]</a>
<a href="subject.html#337">[ subject ]</a>
<a href="author.html#337">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
