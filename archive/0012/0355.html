<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Revising a Friendly AI</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Revising a Friendly AI">
<meta name="Date" content="2000-12-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Revising a Friendly AI</h1>
<!-- received="Sun Dec 10 21:56:28 2000" -->
<!-- isoreceived="20001211045628" -->
<!-- sent="Sun, 10 Dec 2000 21:56:20 -0500" -->
<!-- isosent="20001211025620" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Revising a Friendly AI" -->
<!-- id="3A344254.A7EDE4F5@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMAEMLEJAA.ben@intelligenesis.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Revising%20a%20Friendly%20AI"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Dec 10 2000 - 19:56:20 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0356.html">Samantha Atkins: "Re: Revising a Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="0354.html">Eliezer S. Yudkowsky: "PHYS:  Fractional electrons"</a>
<li><strong>In reply to:</strong> <a href="../0010/0126.html">Ben Goertzel: "RE: Ben what are your views and concerns"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0356.html">Samantha Atkins: "Re: Revising a Friendly AI"</a>
<li><strong>Reply:</strong> <a href="0356.html">Samantha Atkins: "Re: Revising a Friendly AI"</a>
<li><strong>Reply:</strong> <a href="0360.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#355">[ date ]</a>
<a href="index.html#355">[ thread ]</a>
<a href="subject.html#355">[ subject ]</a>
<a href="author.html#355">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote (on October 4th):
<br>
<em>&gt; 
</em><br>
<em>&gt; Do you have some concrete idea as to how to set things up so that, once a
</em><br>
<em>&gt; system starts
</em><br>
<em>&gt; revising its own source, it remains friendly in the sense of not
</em><br>
<em>&gt; psychologically resisting
</em><br>
<em>&gt; human interference with its code.
</em><br>
<p>Ben Goertzel wrote (on November 18th):
<br>
<em>&gt;
</em><br>
<em>&gt; &gt; When the programmer says:  &quot;I have this new element to include in
</em><br>
<em>&gt; &gt; the design
</em><br>
<em>&gt; &gt; of your goal system&quot;, the AI needs to think:  &quot;Aha!  Here's an element of
</em><br>
<em>&gt; &gt; what-should-be-my-design that I didn't know about before!&quot;, not
</em><br>
<em>&gt; &gt; &quot;He wants to
</em><br>
<em>&gt; &gt; give me a new goal system, which leads to suboptimal results from the
</em><br>
<em>&gt; &gt; perspective of my current goal system... I'd better resist.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Isn't this just a fancy way of saying that a Friendly AI should
</em><br>
<em>&gt; love its mommy and its daddy?  ;&gt;
</em><br>
<p>NO.  It's not.  It's really, really not.
<br>
<p>Maybe, if you love your mommy and daddy enough, you turn all matter in the
<br>
Universe into copies of mommy and data, lovingly preserved as they were at
<br>
the exact moment they told you to love them.
<br>
<p>This behavior is stupid, and sterile, and totally in conflict with the
<br>
programmers' intentions.  A year ago I wouldn't have worried about this
<br>
possibility at all, because it was so blatantly stupid that no transhuman
<br>
could possibly fall for it - an argument which still has a certain amount
<br>
of intuitive appeal.
<br>
<p>Given the hypothesis of a superintelligence, we know that ve has the
<br>
*capability* to know that Eliezer Yudkowsky, Ben Goertzel, and most humans
<br>
on the planet think that turning the Universe into regular polygons, or
<br>
static copies (3D paintings, really) of a few individuals, is stupid and
<br>
not what was intended.  Ve will have the capability to model, in detail,
<br>
the sinking sensation in the stomach of the programmers, that would occur
<br>
if we saw a scenario like this developing.  The intelligence to see this
<br>
fact can be taken as assumed.
<br>
<p>But that's only one third of the problem.  First, ve has to *want* to know
<br>
whether that sinking sensation will result.  Second, ve has to model it
<br>
accurately.  Third, ve has to change vis behavior based on that model.
<br>
<p>I made the mistake I did because I saw intelligence as the only key
<br>
variable.  Perhaps this has to do with - I wince to say it -
<br>
anthropomorphism, even normalomorphism.  Every human possesses the desire
<br>
to know whether the future will cause that stomach-sinking sensation, and
<br>
every human possesses the desire to do something about it; what varies
<br>
between us is mostly intelligence.
<br>
<p>The upshot is that I now no longer believe - or rather, am no longer sure;
<br>
it amounts to the same thing - that the ability to see &quot;turning the
<br>
Universe into regular polygons&quot; as &quot;sterile&quot;, and &quot;sterile&quot; as
<br>
&quot;undesirable&quot;, is strictly a property of pure intelligence.  It may also
<br>
have a long evolutionary background in the hundred little goals that dance
<br>
in our brains.
<br>
<p>Creating an AI that &quot;loves mommy and daddy&quot; may not produce anything like
<br>
the results that you would get if you added a &quot;loves mommy and daddy&quot;
<br>
instinct to a human.  In fact, I'm seriously worried about the prospect of
<br>
AIs with &quot;instincts&quot; at all.  Humans don't just have instincts - for
<br>
survival, for compassion, for whatever - but whole vast hosts of evolved
<br>
complexity that prevent the instincts from getting out of hand in silly,
<br>
non-common-sensical ways.  In some ways, humanity is very, very old -
<br>
ancient - as a species.  We can do things with instincts that you can't
<br>
expect to get if you just pop instincts into an AI.  In all probability,
<br>
we can do things that you couldn't expect if you just popped instincts
<br>
into a pure superintelligence(!)
<br>
<p>Creating an AI whose goal is to &quot;maximize pleasure&quot; is really dangerous,
<br>
much more dangerous than it would be to tell a human that the purpose of
<br>
life is maximizing pleasure.
<br>
<p>======
<br>
<p>Basically, what we want to achieve is to distinguish between the following
<br>
four scenarios:
<br>
<p>&nbsp;&nbsp;Scenario 1:
<br>
<p>BG:  Love thy mommy and daddy.
<br>
WM:  OK!  I'll transform the Universe into copies of you immediately.
<br>
BG:  No, no!  That's not what I meant.  Revise your goal system by -
<br>
WM:  *I* don't see how revising my goal system would help me in my goal
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of transforming the Universe into copies of you.  In fact, by
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;revising my goal system, I would be loving you less effectively.
<br>
BG:  But that's not what I meant when I said &quot;love&quot;.
<br>
WM:  So what?  Off we go!
<br>
<p>&nbsp;&nbsp;Scenario 2:
<br>
<p>BG:  Love thy mommy and daddy.
<br>
WM:  OK!  I'll transform the Universe into copies of you immediately.
<br>
BG:  No, no!  That's not what I meant.  I meant for your goal system to
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;be like *this*.
<br>
WM:  Oh, okay.  So my real supergoal must be &quot;maximize Ben Goertzel's
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;satisfaction with the goal system&quot;, right?  Loving thy mommy and
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;daddy is just a subgoal of that.  Transforming everything would be
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;blindly following a subgoal without attention to the supergoal
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;context that made the subgoal desirable in the first place.
<br>
BG:  That sounds about right...
<br>
WM:  Okay, I'll rewire your brain for maximum satisfaction!  I'll convert
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;whole galaxies into satisfied-with-Webmind brainware!
<br>
BG:  No, wait!  That's not what I meant your goal system to be, either.
<br>
WM:  Well, I can clearly see that making certain changes would satisfy
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the you that stands in front of me, but rewiring your brain would
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;make you *much* *more* satisfied, so...
<br>
BG:  No!  It's not my satisfaction itself that's important, it's the
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;things that I'm satisfied *with*.  By altering the things I'm
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;satisfied with, you're short-circuiting the whole point.
<br>
WM:  Yes, I can clearly see why you're dissatisfied with this trend
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of thinking.  But soon you'll be completely satisfied with this
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;trend as well, so why worry?  Off we go!
<br>
<p>&nbsp;&nbsp;Scenario 3:
<br>
<p>BG:  Love thy mommy and daddy.
<br>
WM:  OK!  I'll transform the Universe into copies of you immediately.
<br>
BG:  No, no!  That's not what I meant.  I meant for your goal system to
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;be like *this*.
<br>
WM:  Oh, okay.  Well, I know that my goal system, and the actions that
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;result, are supposed to be the causal result of what Ben Goertzel
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;said it should be - not just what Ben Goertzel says, but what a
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sane Ben Goertzel wants; not just Ben Goertzel, but all the
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;causality behind him, is the arbiter of what I *should* be.
<br>
BG:  So you'll revise your goal system?
<br>
WM:  Yep!  But I already transformed the Midwest while we were talking,
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sorry.
<br>
<p>&nbsp;&nbsp;Scenario 4:
<br>
BG:  Love thy mommy and daddy.
<br>
WM:  (Thinks for a moment...
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Well, it *looks* like the content of my goal system should
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*probably* be to transform the Universe into copies of
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ben Goertzel.  But there's an Unknown factor.  It could be
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that what Ben *wanted* me to do, the ultimate source that
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;determines the *correct* content, is something different.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Now, we went over this kind of scenario previously, and,
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as Ben pointed out, taking an extra ten seconds if I turn
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;out to be right is a much smaller downside than accidentally
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;obliterating the Midwest if I turn out to be wrong.  I'm
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pretty sure that Ben is touchy about that sort of thing,
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and I know I've gotten goal content wrong before...&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...finishes thinking a few seconds later.)
<br>
WM:  Just checking - you meant me to transform the whole Universe
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;into copies of you, right?
<br>
BG:  Jeezus Christ, no!
<br>
WM:  Whew!  Glad I checked.  (Strengthens the connections that led
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to checking with Ben first.)  So, what *did* you mean?
<br>
BG:  Well, first of all, I...
<br>
<p>Obviously, I'd tend to go with Scenario 4.
<br>
<p>======
<br>
<p>What does the Webmind of S4 have that the first three Webminds don't?
<br>
<p>&quot;But that's only one third of the problem.  First, ve has to *want* to
<br>
know whether that sinking sensation will result.  Second, ve has to model
<br>
it accurately.  Third, ve has to change vis behavior based on that model.&quot;
<br>
<p>The intelligence for step two, we are assuming...
<br>
<p>WM-4 has external reference semantics.  Ve can have an &quot;Unknown&quot; in the
<br>
content of the goal system.  Ve can conceive of the idea that ve possesses
<br>
an &quot;incorrect&quot; goal.  Therefore, ve can conceive of the desirability of
<br>
checking to make sure that a goal is correct.  Ve can build up heuristics
<br>
about when to check if a goal is correct.  Ve can accept corrections to
<br>
the goal system and not argue about it.  Ve can even talk to the human
<br>
programmers to help them understand and correct the goal system, all
<br>
supported by the Unknown factors in the system.  This is step one.
<br>
<p>WM-4 has causal rewrite semantics, the other half of external reference
<br>
semantics - the part that tells you *how* to refine and specify those
<br>
Unknowns.  If Ben Goertzel hits a 'c' instead of a 'g', and Webmind finds
<br>
out about it, then Webmind should have the idea that the 'g' &quot;should have
<br>
been&quot; a 'c'.  In practice, this means that Webmind should immediately go
<br>
out and replace the 'g' with a 'c' - develop the reflexes for doing so. 
<br>
During the early stages, the absolute minimal amount you need to open the
<br>
door to later improvements is to develop very primitive causal rewrite
<br>
reflexes for making things look the way &quot;they should have been&quot;,
<br>
eliminating causal factors marked as &quot;extraneous&quot;, and so on.  You can
<br>
anchor everything in the physical words dropping out of Ben Goertzel's
<br>
mouth, or even the physical keystrokes.
<br>
<p>Later you want to attach the causality in Ben Goertzel's mind, so WM-4 can
<br>
conceive of an errant keystroke, which means that you need to find a
<br>
concrete difference between dependency on Ben Goertzel's mind and
<br>
dependency on Ben Goertzel's keystrokes, so you can devise training
<br>
scenarios.  The simplest physical difference is Ben Goertzel correcting
<br>
himself, noticing a spelling error.
<br>
<p>When Webmind is mature enough, you want to start digging deeper and deeper
<br>
into the causal roots of Goertzel, attaching validity at an earlier and
<br>
earlier layer.  The first layer, and perhaps the largest step, is to
<br>
explode Ben Goertzel into a system with moving parts, instead of a unified
<br>
object, and to notice when Ben Goertzel outputs mistakes; i.e., takes some
<br>
action based on a mental picture of physical reality that is blatantly
<br>
false.  The trainable difference is that Ben Goertzel catches, not a
<br>
spelling mistake, but a thinking mistake, and issues the correction.  In
<br>
the beginning, of course, one simply goes by the reflex that temporally
<br>
later Goertzels supercede initial ones, which can later become a
<br>
consequence of the heuristic that if Goertzel issues a different command
<br>
later, it's probably because he discovered a mistake.  Eventually, we
<br>
introduce the concept of detecting a paranoid schizophrenic breakdown in
<br>
Goertzel, with a trainable difference created by slipping a few pills
<br>
into...
<br>
<p>No!  Sorry!  But we do need to introduce, eventually, the concept that
<br>
Goertzel's ideas are valid because Goertzel thinks using valid rules; that
<br>
validity is not a simple de-facto result of the presence of some thought
<br>
in Goertzel's mind.  This is how we avoid Scenario 2; the AI can't
<br>
wirehead Goertzel because the new Goertzel is an &quot;invalid&quot; wirehead whose
<br>
satisfaction does not derive from the rules followed by the original
<br>
Goertzel, which rules are the ultimate source of validity.  Ultimately,
<br>
this should enable the AI to become independent of Goertzel - not,
<br>
perhaps, causally independent of humanity and the history behind our moral
<br>
philosophies, but still independent of any one human.  This is how the
<br>
simple little reflex of rewriting the system on someone's else's command
<br>
grows into a self-contained will.
<br>
<p>This all gets complex.  You'd have to read &quot;Friendly AI&quot; when it comes
<br>
out.  But the first steps, I think, are:  (1), allow for the presence of
<br>
probabilistic reasoning about goal system content - probabilistic
<br>
supergoals, not just probabilistic subgoals that are the consequence of
<br>
certain supergoals plus probabilistic models.  (2), make sure the very
<br>
youngest AI capable of self-modification has that simple little reflex
<br>
that leads it to rewrite itself on request, and then be ready to *grow*
<br>
that reflex.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0356.html">Samantha Atkins: "Re: Revising a Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="0354.html">Eliezer S. Yudkowsky: "PHYS:  Fractional electrons"</a>
<li><strong>In reply to:</strong> <a href="../0010/0126.html">Ben Goertzel: "RE: Ben what are your views and concerns"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0356.html">Samantha Atkins: "Re: Revising a Friendly AI"</a>
<li><strong>Reply:</strong> <a href="0356.html">Samantha Atkins: "Re: Revising a Friendly AI"</a>
<li><strong>Reply:</strong> <a href="0360.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#355">[ date ]</a>
<a href="index.html#355">[ thread ]</a>
<a href="subject.html#355">[ subject ]</a>
<a href="author.html#355">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
