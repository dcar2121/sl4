<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=Windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Is generalisation a limit to intelligence?</title>
<meta name="Author" content="Ben Goertzel (ben@intelligenesis.net)">
<meta name="Subject" content="RE: Is generalisation a limit to intelligence?">
<meta name="Date" content="2000-12-03">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Is generalisation a limit to intelligence?</h1>
<!-- received="Sun Dec 03 14:20:57 2000" -->
<!-- isoreceived="20001203212057" -->
<!-- sent="Sun, 3 Dec 2000 08:42:36 -0500" -->
<!-- isosent="20001203134236" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@intelligenesis.net" -->
<!-- subject="RE: Is generalisation a limit to intelligence?" -->
<!-- id="NDBBIBGFAPPPBODIPJMMOEAKEOAA.ben@intelligenesis.net" -->
<!-- charset="Windows-1252" -->
<!-- inreplyto="00120301463101.00894@tachyon" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@intelligenesis.net?Subject=RE:%20Is%20generalisation%20a%20limit%20to%20intelligence?"><em>ben@intelligenesis.net</em></a>)<br>
<strong>Date:</strong> Sun Dec 03 2000 - 06:42:36 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0348.html">Joaquim Almgren Gândara: "Re: Is generalisation a limit to intelligence?"</a>
<li><strong>Previous message:</strong> <a href="0346.html">James Rogers: "RE: Is generalisation a limit to intelligence?"</a>
<li><strong>In reply to:</strong> <a href="0346.html">James Rogers: "RE: Is generalisation a limit to intelligence?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0350.html">James Rogers: "RE: Is generalisation a limit to intelligence?"</a>
<li><strong>Reply:</strong> <a href="0350.html">James Rogers: "RE: Is generalisation a limit to intelligence?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#347">[ date ]</a>
<a href="index.html#347">[ thread ]</a>
<a href="subject.html#347">[ subject ]</a>
<a href="author.html#347">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
hi,
<br>
<p><p><em>&gt; Information theoretic approaches have already demonstrated much of what is
</em><br>
<em>&gt; being questioned, or at least insofar as finite-state machines are
</em><br>
<em>&gt; concerned.  Generally speaking, given a finite amount of memory and an
</em><br>
<em>&gt; arbitrarily long sequence of data (generated by any finite state machine
</em><br>
<em>&gt; no matter how complex), it is possible to attain the minimum possible
</em><br>
<em>&gt; predictive error rate using universal prediction schemes.
</em><br>
<p><p>Yes, I am well aware of these theorems.
<br>
<p>However, this kind of &quot;in principle&quot; calculation is not very useful in
<br>
practice,
<br>
now is it?  only in very narrow domains, like text compression...
<br>
<p>Here is an excerpt from an e-mail on an internal Webmind Inc. list (the one
<br>
devoted to
<br>
philosophy rather than practical stuff), on a related topic ...
<br>
<p><p>*****
<br>
What do I mean when I say I believe that EC (evol. computing, GP) is enough
<br>
to create a thinking
<br>
machine, in principle?
<br>
<p>Here we go...
<br>
<p>You have to posit a system with a goal(s) (e.g. survival, mating)
<br>
<p>You have to assume it has a way to assess the degree to which these goals
<br>
are satisfied at a given time, at least approximately
<br>
<p>You have to assume it has a memory of, at each moment in time: what programs
<br>
it was running at each time,
<br>
and the extent to which it achieved its goals.  [A memory of what stimuli it
<br>
was receiving is optional, but can obviously enhance efficiency of
<br>
learning....]
<br>
<p>Then, if the system has enough time to learn, EC (or more simply, Monte
<br>
Carlo search over program space),
<br>
will cause the system to arrive at a program that can achieve its goals
<br>
optimally
<br>
<p>The assumptions one needs to make an EC-based thinking machine possible are
<br>
then
<br>
<p>-- fast processors
<br>
-- huge memory
<br>
-- a very very long lifespan
<br>
-- a relatively benign environment (the latter so the system doesn't die in
<br>
the early stages of its
<br>
very slow and stupid learning process)
<br>
<p>Since none of these conditions obtain, we need something much  more
<br>
specialized and more complicated
<br>
than EC inside our thinking machine...
<br>
<p>********
<br>
<p><em>&gt; An optimal
</em><br>
<em>&gt; prediction scheme can be algorithmically generated and the error rate
</em><br>
<em>&gt; figured for any data generated by finite-state machinery. &lt;much lengthy
</em><br>
<em>&gt; theory omitted&gt;  In short, it has been demonstrated that for any finite
</em><br>
<em>&gt; state machine, it is possible to ascertain the minimum possible predictive
</em><br>
<em>&gt; error rate for any data sequence given any finite amount of memory.
</em><br>
<p>Yes, but this method will not perform adequately
<br>
under the conditions under which a real intelligent systems have
<br>
to operate.
<br>
<p><p><em>&gt; An
</em><br>
<em>&gt; optimal prediction scheme will typically approach the theoretical error
</em><br>
<em>&gt; limit quite fast.  However, sub-optimal prediction schemes, nonparametric
</em><br>
<em>&gt; or unknown models, and similar types of situations may approach their
</em><br>
<em>&gt; theoretical error rates quite slowly.
</em><br>
<p>The problem is that the prediction schemes that are &quot;optimal&quot; under standard
<br>
mathematical assumptions, are NOT optimal given the real-world conditions
<br>
under
<br>
which organisms operate.
<br>
<p><em>&gt; It would be trivial for a
</em><br>
<em>&gt; computer today to calculate error rates for any optimal universal
</em><br>
<em>&gt; predictive scheme.  These would seem to answer the above question and
</em><br>
<em>&gt; quite a few others I've seen on this thread.
</em><br>
<p>Yes, it answers the question under the glaringly false assumption that minds
<br>
embody
<br>
general optimal predictive schemes
<br>
<p><em>&gt; Among the interesting things that have been shown with respect to this is
</em><br>
<em>&gt; that humans are quite apparently finite state-machines.  The first example
</em><br>
<em>&gt; of this was Hagelbarger at Bell Labs (and later Claude Shannon), who first
</em><br>
<em>&gt; demonstrated that humans are apparently unable to generate truly random
</em><br>
<em>&gt; sequences of any kind; computers using information theoretic prediction
</em><br>
<em>&gt; algorithms were able to successfully predict the behavior of humans
</em><br>
<em>&gt; intentionally attempting to generate random data, with an error rate many,
</em><br>
<em>&gt; many orders of magnitude below what would be expected if the human
</em><br>
<em>&gt; participants were actually generating random data.
</em><br>
<p>These experiments certainly do not demonstrate that humans are finite-state
<br>
machines.
<br>
There are many  many other explanations for this data.  I won't bore you by
<br>
reciting them.
<br>
<p><em>&gt; I've actually been using information theoretic approaches in my engines
</em><br>
<em>&gt; for several years now, and with generally superb results across many
</em><br>
<em>&gt; fields.  It has been widely rumored that Claude Shannon made his
</em><br>
<em>&gt; fortune by
</em><br>
<em>&gt; &quot;working&quot; the stock market (as an aside, a couple years ago I calculated
</em><br>
<em>&gt; that running an optimal predictive engine against the entire NASDAQ in
</em><br>
<em>&gt; realtime, based on the best engine I had produced to date, would require a
</em><br>
<em>&gt; machine capable of 10^11 Flops sustained. The amount of memory was
</em><br>
<em>&gt; reasonably attainable though.)   I've found it odd that information theory
</em><br>
<em>&gt; is routinely overlooked in AI research since it provides such a solid
</em><br>
<em>&gt; foundation for the mathematical basis of the topic.
</em><br>
<p>Information theory is actually NOT a solid basis for AI.  It's useful for
<br>
very
<br>
simple aspects of AI, mostly on the perceptual side.
<br>
<p>But let's face it -- the key properties of the human  mind are all about
<br>
dealing with
<br>
lack of memory and processor speed.  The distinction between STM and LTM,
<br>
the balance
<br>
between reason (a thorough analysis method that's resource-intensive, when
<br>
applied across
<br>
a broad scope) and intuition (cheaper but less accurate), and so forth.
<br>
Most of what makes
<br>
real minds interesting is NOT about optimal prediction or modeling, but
<br>
about pretty good ways
<br>
of achieving pretty good intelligence within very limited space and time
<br>
resources.  This is a whole
<br>
different story from information theory.
<br>
<p>For example, in the area of computational linguistics, Denis Yuret's
<br>
excellent MIT PhD thesis from a few
<br>
years back uses information theory to model language (&quot;lexical attraction&quot;
<br>
he calls it).  All well and
<br>
good.  It doesn't help you deal with the translation of language into
<br>
meaning.  I think I know how to do the
<br>
latter, but not using information theory explicitly....
<br>
<p>&nbsp;I mean, how do you construct a mind
<br>
given current computational resources and real-time learning constraints,
<br>
inspired primarily by information
<br>
theory?  You don't have to tell me all the details, just give me the broad
<br>
outlines...
<br>
<p><p><em>&gt; I am currently working on putting together a website with a lot of
</em><br>
<em>&gt; the theory and actual application of my work, quite a few parts of which
</em><br>
<em>&gt; have been applied in the commercial sector.
</em><br>
<p>I look forward to reading your stuff!
<br>
<p>But still, a deep yet unrealistic general theory, plus some narrow-domain
<br>
cool applications, does not
<br>
make a viable theory of mind-construction or mind-analysis in the real
<br>
world...
<br>
<p><p>ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0348.html">Joaquim Almgren Gândara: "Re: Is generalisation a limit to intelligence?"</a>
<li><strong>Previous message:</strong> <a href="0346.html">James Rogers: "RE: Is generalisation a limit to intelligence?"</a>
<li><strong>In reply to:</strong> <a href="0346.html">James Rogers: "RE: Is generalisation a limit to intelligence?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0350.html">James Rogers: "RE: Is generalisation a limit to intelligence?"</a>
<li><strong>Reply:</strong> <a href="0350.html">James Rogers: "RE: Is generalisation a limit to intelligence?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#347">[ date ]</a>
<a href="index.html#347">[ thread ]</a>
<a href="subject.html#347">[ subject ]</a>
<a href="author.html#347">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
