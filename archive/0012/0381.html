<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Revising a Friendly AI</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Revising a Friendly AI">
<meta name="Date" content="2000-12-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Revising a Friendly AI</h1>
<!-- received="Tue Dec 12 19:43:05 2000" -->
<!-- isoreceived="20001213024305" -->
<!-- sent="Tue, 12 Dec 2000 19:42:04 -0500" -->
<!-- isosent="20001213004204" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Revising a Friendly AI" -->
<!-- id="3A36C5DC.EA2360D6@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMGEHPEOAA.ben@intelligenesis.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Revising%20a%20Friendly%20AI"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Dec 12 2000 - 17:42:04 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0382.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="0380.html">Gordon Worley: "Re: Diaspora, the future of AI &amp; value systems, etc."</a>
<li><strong>In reply to:</strong> <a href="0360.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0382.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<li><strong>Reply:</strong> <a href="0382.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#381">[ date ]</a>
<a href="index.html#381">[ thread ]</a>
<a href="subject.html#381">[ subject ]</a>
<a href="author.html#381">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &gt; But the first steps, I think, are:  (1), allow for the presence of
</em><br>
<em>&gt; &gt; probabilistic reasoning about goal system content - probabilistic
</em><br>
<em>&gt; &gt; supergoals, not just probabilistic subgoals that are the consequence of
</em><br>
<em>&gt; &gt; certain supergoals plus probabilistic models.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is a key point, and really gets at why your &quot;I love mommy and daddy, so
</em><br>
<em>&gt; I'll clone them&quot; example seems weird.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; We try to teach our children to adopt our value systems.  But our
</em><br>
<em>&gt; explicit teachings in this regard generally are LESS useful than
</em><br>
<em>&gt; our concrete examples.  Children adopt goals and values from their
</em><br>
<em>&gt; parents in all kinds of obvious and subtle ways, which come from
</em><br>
<em>&gt; emotionally-charged interaction in a shared environment.
</em><br>
<p>Yes, but those are *human* children.  I think that asking a young *AI* to
<br>
pull off a trick like that is asking far too much.  Absorbing goals and
<br>
values isn't automatic.  We've been doing it for millions of years.
<br>
<p>To invoke a very old debate in cognitive science, human children may not
<br>
be born with the words, but they're born with the syntax...
<br>
<p><em>&gt; Choice of new goals is not a rational thing: rationality is a tool for
</em><br>
<em>&gt; achieving goals, and dividing goals into subgoals, but not for
</em><br>
<em>&gt; replacing one's goals with supergoals....
</em><br>
<p>*This* is the underlying fallacy of cultural relativism.  Even if
<br>
rationality does not suffice to *completely* specify supergoals, this does
<br>
not mean that rationality plays *no* role in choosing supergoals.
<br>
<p>We choose our supergoals through processes of enormous complexity, which I
<br>
have chose, for reasons of brevity, to label &quot;philosophical&quot;, meaning
<br>
supergoal-affecting.  (Maybe I ought to distinguish between philosophical
<br>
and Philosophical, in the same spirit as friendliness and Friendliness,
<br>
but I don't think that would work.)  These processes are so complex that
<br>
there is plenty of room for rationality and irrationality and emotion and
<br>
memes and everything else.  The concept of a goal hiearchy, with arbitrary
<br>
supergoals on top and a neat chain of world-model-dependent subgoals on
<br>
the bottom, is historically very recent.  The human mind is built to
<br>
recognize no firm borderline between reasoning about subgoals and
<br>
reasoning about supergoals, applying essentially the same rational
<br>
heuristics in each case - in fact, the human mind is not built to
<br>
recognize any borderline between &quot;subgoals&quot; and &quot;supergoals&quot; at all;
<br>
cognitively, it's just &quot;goals&quot;.  The human mind is built to recognize
<br>
distinctions between goals that are more &quot;super&quot; and more &quot;sub&quot;, and it is
<br>
built to recognize the relations &quot;supergoal-of&quot; and &quot;subgoal-of&quot;.  From
<br>
these cognitive relations, modern philosophers have invented the
<br>
*artificial* idea of a supergoal.
<br>
<p>Do you really believe that you can alter someone's level of intelligence
<br>
without altering the set of supergoals they tend to come up with? 
<br>
Especially when choice of verbally declared supergoal is dependent on the
<br>
emotional appeal of a supergoal which is dependent on visualized factual
<br>
context and consequences of said supergoal?  I'm damn sure I would have
<br>
wound up with drastically different supergoals.
<br>
<p><em>&gt; Perhaps this is one of the key values of &quot;emotion.&quot;  It causes us to replace
</em><br>
<em>&gt; our goals
</em><br>
<em>&gt; with supergoals, by latching onto the goals of our parents and others around
</em><br>
<em>&gt; us.
</em><br>
<p>And overriding evolution's supergoals with a verbally transferred
<br>
supergoal (as your schema would seem to have it?) is an evolutionary
<br>
advantage because?
<br>
<p><em>&gt; &gt;(2), make sure the very
</em><br>
<em>&gt; &gt; youngest AI capable of self-modification has that simple little reflex
</em><br>
<em>&gt; &gt; that leads it to rewrite itself on request, and then be ready to *grow*
</em><br>
<em>&gt; &gt; that reflex.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Rewriting itself on request is only useful if the system has a strong
</em><br>
<em>&gt; understanding of HOW
</em><br>
<em>&gt; to rewrite itself...
</em><br>
<p>The first requests will be on the order of &quot;swap out this old module,
<br>
which we wrote, for this new module, which we wrote&quot;.  To call this a
<br>
&quot;reflex&quot; is overstating the point, perhaps, but making it a reflex,
<br>
instead of an entirely external action, enables the reflex to grow.  Next
<br>
comes the ability to help out in a few simple ways - checking change
<br>
propagation and the like; still just compiler-level activities, but
<br>
there.  Next would come observing facts about the exchange, like the
<br>
degree of speedup or slowdown, the degree to which problem-solving methods
<br>
change, and so on.  Then would come the ability to make small changes on
<br>
request; changes to code would come later; first, and more useful, would
<br>
be changes to declarative internal data.  As time progresses, the AI
<br>
builds up an experiential database about how external changes work, how to
<br>
help, and the purpose of those external changes.  At some point, when the
<br>
goal system reaches a sufficient level of sophistication, you can start to
<br>
explain the design purpose of the goal system; that is also the moment to
<br>
explain how changes to goal system content, or even goal system structure,
<br>
serve the *referent* of the goal system.
<br>
<p>Instead of the AI suddenly waking up one morning and realizing that it can
<br>
modify itself instead of waiting around for you to do it, there can be a
<br>
smooth transition - a continuum - so that when the AI does &quot;wake up one
<br>
morning&quot;, it has an experiential base that guides its very fast
<br>
decisions.  Remember that Scenario 4 included an explicit reference to an
<br>
experiential base?
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0382.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="0380.html">Gordon Worley: "Re: Diaspora, the future of AI &amp; value systems, etc."</a>
<li><strong>In reply to:</strong> <a href="0360.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0382.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<li><strong>Reply:</strong> <a href="0382.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#381">[ date ]</a>
<a href="index.html#381">[ thread ]</a>
<a href="subject.html#381">[ subject ]</a>
<a href="author.html#381">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
