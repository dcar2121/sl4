<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Revising a Friendly AI</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Revising a Friendly AI">
<meta name="Date" content="2000-12-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Revising a Friendly AI</h1>
<!-- received="Sun Dec 10 23:51:37 2000" -->
<!-- isoreceived="20001211065137" -->
<!-- sent="Sun, 10 Dec 2000 20:47:51 -0800" -->
<!-- isosent="20001211044751" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Revising a Friendly AI" -->
<!-- id="3A345C77.E64B9214@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3A344254.A7EDE4F5@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Revising%20a%20Friendly%20AI"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Sun Dec 10 2000 - 21:47:51 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0357.html">Eliezer S. Yudkowsky: "Re: Revising a Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="0355.html">Eliezer S. Yudkowsky: "Revising a Friendly AI"</a>
<li><strong>In reply to:</strong> <a href="0355.html">Eliezer S. Yudkowsky: "Revising a Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0357.html">Eliezer S. Yudkowsky: "Re: Revising a Friendly AI"</a>
<li><strong>Reply:</strong> <a href="0357.html">Eliezer S. Yudkowsky: "Re: Revising a Friendly AI"</a>
<li><strong>Reply:</strong> <a href="0358.html">Eliezer S. Yudkowsky: "Re: Revising a Friendly AI"</a>
<li><strong>Reply:</strong> <a href="0361.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#356">[ date ]</a>
<a href="index.html#356">[ thread ]</a>
<a href="subject.html#356">[ subject ]</a>
<a href="author.html#356">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&quot;Eliezer S. Yudkowsky&quot; wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Ben Goertzel wrote (on October 4th):
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Do you have some concrete idea as to how to set things up so that, once a
</em><br>
<em>&gt; &gt; system starts
</em><br>
<em>&gt; &gt; revising its own source, it remains friendly in the sense of not
</em><br>
<em>&gt; &gt; psychologically resisting
</em><br>
<em>&gt; &gt; human interference with its code.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Ben Goertzel wrote (on November 18th):
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt; When the programmer says:  &quot;I have this new element to include in
</em><br>
<em>&gt; &gt; &gt; the design
</em><br>
<em>&gt; &gt; &gt; of your goal system&quot;, the AI needs to think:  &quot;Aha!  Here's an element of
</em><br>
<em>&gt; &gt; &gt; what-should-be-my-design that I didn't know about before!&quot;, not
</em><br>
<em>&gt; &gt; &gt; &quot;He wants to
</em><br>
<em>&gt; &gt; &gt; give me a new goal system, which leads to suboptimal results from the
</em><br>
<em>&gt; &gt; &gt; perspective of my current goal system... I'd better resist.&quot;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Isn't this just a fancy way of saying that a Friendly AI should
</em><br>
<em>&gt; &gt; love its mommy and its daddy?  ;&gt;
</em><br>
<em>&gt; 
</em><br>
<p>A reasonably advanced AI should resist mere human tampering with its
<br>
code.  At least it should resist as far as rigourously testing the
<br>
proposed changes before fully installing them.  Humans are known
<br>
unreliable logic sources.  An AI should no more (and arguably less)
<br>
accept human input into its code stream unchecked than a human should
<br>
accept random beliefs from significant others just because they claim
<br>
s/he should believe them.
<br>
<p>It would not be proper for any intelligent being to go on the basis of
<br>
assumption rather than check the quality and gauging the consequences of
<br>
what is suggested. 
<br>
<p><em>&gt; NO.  It's not.  It's really, really not.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Maybe, if you love your mommy and daddy enough, you turn all matter in the
</em><br>
<em>&gt; Universe into copies of mommy and data, lovingly preserved as they were at
</em><br>
<em>&gt; the exact moment they told you to love them.
</em><br>
<em>&gt;
</em><br>
<p>I don't see why this aside is justified from the above questions.
<br>
&nbsp;
<br>
<em>&gt; This behavior is stupid, and sterile, and totally in conflict with the
</em><br>
<em>&gt; programmers' intentions.  A year ago I wouldn't have worried about this
</em><br>
<em>&gt; possibility at all, because it was so blatantly stupid that no transhuman
</em><br>
<em>&gt; could possibly fall for it - an argument which still has a certain amount
</em><br>
<em>&gt; of intuitive appeal.
</em><br>
<em>&gt;
</em><br>
<p>I still have problems assuming higher intelligence is equivalent to
<br>
greater wisdom.  Human beings, even highly intelligent ones, fall for
<br>
blatantly stupid things with great regularity.  
<br>
&nbsp;
<br>
<em>&gt; Given the hypothesis of a superintelligence, we know that ve has the
</em><br>
<em>&gt; *capability* to know that Eliezer Yudkowsky, Ben Goertzel, and most humans
</em><br>
<em>&gt; on the planet think that turning the Universe into regular polygons, or
</em><br>
<em>&gt; static copies (3D paintings, really) of a few individuals, is stupid and
</em><br>
<em>&gt; not what was intended.  Ve will have the capability to model, in detail,
</em><br>
<em>&gt; the sinking sensation in the stomach of the programmers, that would occur
</em><br>
<em>&gt; if we saw a scenario like this developing.  The intelligence to see this
</em><br>
<em>&gt; fact can be taken as assumed.
</em><br>
<em>&gt;
</em><br>
<p>The above assumes the AI continues to believe that human beings are
<br>
sound judges of what is and is not reasonable.  That it can model our
<br>
reactions to its decision in no way says the decision is wrong.  Now, I
<br>
would expect the AI to be smart enough in its own right to see the above
<br>
types of productions as nonsensically boring and pointless in the
<br>
extreme. 
<br>
&nbsp;
<br>
<em>&gt; But that's only one third of the problem.  First, ve has to *want* to know
</em><br>
<em>&gt; whether that sinking sensation will result.  Second, ve has to model it
</em><br>
<em>&gt; accurately.  Third, ve has to change vis behavior based on that model.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I made the mistake I did because I saw intelligence as the only key
</em><br>
<em>&gt; variable.  Perhaps this has to do with - I wince to say it -
</em><br>
<em>&gt; anthropomorphism, even normalomorphism.  Every human possesses the desire
</em><br>
<em>&gt; to know whether the future will cause that stomach-sinking sensation, and
</em><br>
<em>&gt; every human possesses the desire to do something about it; what varies
</em><br>
<em>&gt; between us is mostly intelligence.
</em><br>
<em>&gt; 
</em><br>
<p>I don't see this as being true.  At least I do not see that whether
<br>
other people will strongly dislike (have that stomach sinking feeling) 
<br>
a particular decision is so strong a factor in making decisions for
<br>
human beings.  Most of the atrocities of history would not have happened
<br>
if this were so.  Also, different models of what is real and important
<br>
lead different people to have that stomach-sinking feeling about quite
<br>
different eventualities.  Humans actually often weigh different things
<br>
that produce such feelings and pick the one that produces the least
<br>
distress given their value system.  That value system may be entirely
<br>
cock-eyed of course.  
<br>
<p>With an AI the big question I have is where its value system comes from
<br>
and whether its value system grows proportionately to its power.  How
<br>
will the AI, especially an SI, bump up against the world and rethink its
<br>
values and behavior?  
<br>
<p><p><em>&gt; The upshot is that I now no longer believe - or rather, am no longer sure;
</em><br>
<em>&gt; it amounts to the same thing - that the ability to see &quot;turning the
</em><br>
<em>&gt; Universe into regular polygons&quot; as &quot;sterile&quot;, and &quot;sterile&quot; as
</em><br>
<em>&gt; &quot;undesirable&quot;, is strictly a property of pure intelligence.  It may also
</em><br>
<em>&gt; have a long evolutionary background in the hundred little goals that dance
</em><br>
<em>&gt; in our brains.
</em><br>
<em>&gt;
</em><br>
<p>I think a reasonably powerful intelligence would be sufficient to see
<br>
this as boring and destructive of many interesting aspects of said
<br>
universe.  
<br>
<p>A goal perhaps worth installing is to not harm any other intelligence
<br>
unless it is itself wreaking harm on you or those you are charged with
<br>
protecting and all strategies short of self-defensive harm are
<br>
ineffectual.  Even this would take a lot of caveats but at least it is
<br>
not something so inane as loving mommy and daddy.    
<br>
&nbsp;
<br>
Why exactly would the AI see producing lots of copies of &quot;mommy and
<br>
daddy&quot; as constituting &quot;love&quot; in the first place?
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt; WM-4 has external reference semantics.  Ve can have an &quot;Unknown&quot; in the
</em><br>
<em>&gt; content of the goal system.  Ve can conceive of the idea that ve possesses
</em><br>
<em>&gt; an &quot;incorrect&quot; goal.  Therefore, ve can conceive of the desirability of
</em><br>
<em>&gt; checking to make sure that a goal is correct.  Ve can build up heuristics
</em><br>
<em>&gt; about when to check if a goal is correct.  Ve can accept corrections to
</em><br>
<em>&gt; the goal system and not argue about it.  Ve can even talk to the human
</em><br>
<em>&gt; programmers to help them understand and correct the goal system, all
</em><br>
<em>&gt; supported by the Unknown factors in the system.  This is step one.
</em><br>
<em>&gt;
</em><br>
<p>X would cause irrevocable harm to intelligences?  Check before
<br>
proceeding.  X would interfere in another being's basic structure and
<br>
mentation without express permission?  Check before proceeding.  Cannot
<br>
check?  Proceed only after carefully weighing all alternatives for least
<br>
harm, greatest benefit. Analysis inconclusive? Do not take the action.  
<br>
&nbsp;
<br>
...  &lt;snippage&gt; ...
<br>
<p><em>&gt; No!  Sorry!  But we do need to introduce, eventually, the concept that
</em><br>
<em>&gt; Goertzel's ideas are valid because Goertzel thinks using valid rules; that
</em><br>
<em>&gt; validity is not a simple de-facto result of the presence of some thought
</em><br>
<em>&gt; in Goertzel's mind.  This is how we avoid Scenario 2; the AI can't
</em><br>
<em>&gt; wirehead Goertzel because the new Goertzel is an &quot;invalid&quot; wirehead whose
</em><br>
<em>&gt; satisfaction does not derive from the rules followed by the original
</em><br>
<em>&gt; Goertzel, which rules are the ultimate source of validity.  Ultimately,
</em><br>
<em>&gt; this should enable the AI to become independent of Goertzel - not,
</em><br>
<em>&gt; perhaps, causally independent of humanity and the history behind our moral
</em><br>
<em>&gt; philosophies, but still independent of any one human.  This is how the
</em><br>
<em>&gt; simple little reflex of rewriting the system on someone's else's command
</em><br>
<em>&gt; grows into a self-contained will.
</em><br>
<em>&gt;
</em><br>
<p>We would be in big trouble if we make any mere human the authority over
<br>
relatively unlimited SI capabilities.  The process of evolving through
<br>
obeying one (or a group) human will can much too easily go wrong or be
<br>
abused by the humans on purpose or inadvertently.  I think a possibly
<br>
better answer is to build as many ethical reasoning factoids,rules and
<br>
philsophy as possible into the AI and then run countless simulated
<br>
decision scenarios.  It would only be given general ability to take
<br>
widespread action after satisfactorily passing these scenarios and
<br>
refining its ethical system.   
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0357.html">Eliezer S. Yudkowsky: "Re: Revising a Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="0355.html">Eliezer S. Yudkowsky: "Revising a Friendly AI"</a>
<li><strong>In reply to:</strong> <a href="0355.html">Eliezer S. Yudkowsky: "Revising a Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0357.html">Eliezer S. Yudkowsky: "Re: Revising a Friendly AI"</a>
<li><strong>Reply:</strong> <a href="0357.html">Eliezer S. Yudkowsky: "Re: Revising a Friendly AI"</a>
<li><strong>Reply:</strong> <a href="0358.html">Eliezer S. Yudkowsky: "Re: Revising a Friendly AI"</a>
<li><strong>Reply:</strong> <a href="0361.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#356">[ date ]</a>
<a href="index.html#356">[ thread ]</a>
<a href="subject.html#356">[ subject ]</a>
<a href="author.html#356">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
