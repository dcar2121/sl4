<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: When Subgoals Attack</title>
<meta name="Author" content="Durant Schoon (durant@ilm.com)">
<meta name="Subject" content="Re: When Subgoals Attack">
<meta name="Date" content="2000-12-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: When Subgoals Attack</h1>
<!-- received="Wed Dec 13 17:29:15 2000" -->
<!-- isoreceived="20001214002915" -->
<!-- sent="Wed, 13 Dec 2000 14:26:49 -0800 (PST)" -->
<!-- isosent="20001213222649" -->
<!-- name="Durant Schoon" -->
<!-- email="durant@ilm.com" -->
<!-- subject="Re: When Subgoals Attack" -->
<!-- id="durant-1001213142649.A11810675@sleeper" -->
<!-- inreplyto="f05001900b65d77ddd650@[10.50.227.33]" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Durant Schoon (<a href="mailto:durant@ilm.com?Subject=Re:%20When%20Subgoals%20Attack"><em>durant@ilm.com</em></a>)<br>
<strong>Date:</strong> Wed Dec 13 2000 - 15:26:49 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0398.html">Eliezer S. Yudkowsky: "Re: When Subgoals Attack"</a>
<li><strong>Previous message:</strong> <a href="0396.html">Eliezer S. Yudkowsky: "Re: When Subgoals Attack"</a>
<li><strong>In reply to:</strong> <a href="0391.html">Gordon Worley: "Re: When Subgoals Attack"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0398.html">Eliezer S. Yudkowsky: "Re: When Subgoals Attack"</a>
<li><strong>Reply:</strong> <a href="0398.html">Eliezer S. Yudkowsky: "Re: When Subgoals Attack"</a>
<li><strong>Reply:</strong> <a href="0399.html">Gordon Worley: "Re: When Subgoals Attack"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#397">[ date ]</a>
<a href="index.html#397">[ thread ]</a>
<a href="subject.html#397">[ subject ]</a>
<a href="author.html#397">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ok, I feel inclined to reply to everyone who replied to me (then
<br>
I'll descend back into the deep, dark depths of lurkdom).
<br>
<p><p><em>&gt; From: Gordon Worley &lt;<a href="mailto:redbird@rbisland.cx?Subject=Re:%20When%20Subgoals%20Attack">redbird@rbisland.cx</a>&gt;
</em><br>
<em>&gt; &gt;Observation: In modern human minds, these subgoals are often not
</em><br>
<em>&gt; &gt;              intelligent and do not constitute a sentience in and
</em><br>
<em>&gt; &gt;              of themselves. Thirst-&gt;drink-&gt;pick up glass of milk-&gt;...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Off the top of my head, I can think of no basic goal of survival 
</em><br>
<em>&gt; which forces sentience.  Humans could get along with out it, but 
</em><br>
<em>&gt; knowing that they are alive probably makes it easier to survive 
</em><br>
<em>&gt; better, since they care more about surviving if they know they could 
</em><br>
<em>&gt; be in another state (i.e. dead).
</em><br>
<p>I was merely trying to establish that human minds don't really have
<br>
intellegent subgoal seeking processes. The processes are all &quot;dumb&quot;,
<br>
like &quot;send that grasping signal to the fingers and thumb&quot;(*). I
<br>
wanted to make this distinction before I considered a transhuman AI 
<br>
which *might* spawn intelligent sub-agents to solve sub-problems.
<br>
<p>The issue of why sentience might be a survival trait is an interesting
<br>
one, but I think a different topic.
<br>
<p><em>&gt; &gt;          So the problem is this: what would stop subgoals from
</em><br>
<em>&gt; &gt;          overthrowing supergoals. How might this happen? The subgoal
</em><br>
<em>&gt; &gt;          might determine that to satisfy the supergoal, a coup is
</em><br>
<em>&gt; &gt;          just the thing. Furthermore, the subgoal determines that to
</em><br>
<em>&gt; &gt;          successfully supplant the supergoal, the supergoal process
</em><br>
<em>&gt; &gt;          must not know that &quot;overthrow&quot; has become part the
</em><br>
<em>&gt; &gt;          subgoal's agenda. The subgoal might know or learn that its
</em><br>
<em>&gt; &gt;          results will influence the supergoal. The subgoal might
</em><br>
<em>&gt; &gt;          know of learn that it can influence other subgoals in
</em><br>
<em>&gt; &gt;          secret, so a conspiracy may form. Maybe not a lot of the
</em><br>
<em>&gt; &gt;          time, but maybe once every hundred billion years or so.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'm trying to think of a subgoal that would want to overthrow a 
</em><br>
<em>&gt; supergoal, but am having a hard time.  Something like getting a glass 
</em><br>
<em>&gt; of water overriding the goal of not killing other intelligences 
</em><br>
<em>&gt; because they are worth more alive than dead is very unlikely to 
</em><br>
<em>&gt; happen and only under *very* extreme circumstances.  This does not 
</em><br>
<em>&gt; mean it can be discounted, since beings with transhuman and greater 
</em><br>
<em>&gt; intelligences are much more dangerous than the average human, but for 
</em><br>
<em>&gt; the time being this small problem can be overlooked.
</em><br>
<p>Think of an organization of humans, as in a company. Share holders want
<br>
value, so executive management comes up with a way to create value and/or
<br>
delegates the task of value creation to others in the company. These 
<br>
&quot;subordinates&quot; then determine how they can reach their goals, etc., 
<br>
etc.
<br>
<p>Now imagine a pathological case where a charismatic, evil-genious
<br>
janitor somehow gets the ear of the CEO and convinces him to change
<br>
the direction of the company...maybe not likely. So consider an
<br>
ambitious junior executive who manipulates his coworkers in an 
<br>
undetectable way and causes the CEO to be ousted by the board and 
<br>
replaced with someone who promotes him (the junior exec to a 
<br>
senior exec). Doesn't seem so fantastical, probably happens often
<br>
enough.
<br>
<p>I think you know what I mean because you mentioned to the &quot;Nation as 
<br>
a Goal Seeking Entity&quot; example.
<br>
<p>One has to be very careful with the company example, though, and
<br>
replace people with processes (intelligent ones) and with a CEO that
<br>
could monitor every thought of every person (well, if enough energy
<br>
was expended). Better monitoring can mean better evasion, though,
<br>
hence the idea of a secrecy arms race. It's not that the subprocesses
<br>
are inherently selfish (the way the junior executive might be), it's
<br>
more like: the subtask realizes that the best way to achieve its
<br>
goal is to influence it's supergoal by returning misleading (but 
<br>
perhaps correct) information and then influencing other subgoals 
<br>
(if that's possible) to lead the supergoal down a different path...
<br>
perhaps to the supergoal's own detriment. This might not be common,
<br>
but the question is how does one guard against it?
<br>
<p><em>&gt; An overthrow might not be bad, depending on what level it happens. 
</em><br>
<p>The problem is this: You are the King. How do ensure that your subjects
<br>
never depose you? Ever. And in this case, yes, an overthrow would
<br>
be bad. You would disappear.
<br>
<p>I might be reading too much into Eliezer's reply, but his solution 
<br>
sounds to me like: &quot;You can't trust any smart subjects, so don't have 
<br>
any. Only create dumb automotons to follow out your orders exactly 
<br>
(the details of which, you have considered extremely carefully).&quot;
<br>
<p>Which means you have to spend all your time considering the details
<br>
of your orders and can't expedite things by trusting those tasks to
<br>
any other process...you are serial and your *vast* resources are 
<br>
likely sitting idle, waiting for your next instructions...and you
<br>
better wipe out any uploaded humans, because they can just get in
<br>
the way ;-)
<br>
<p>or maybe there's a third alternative...
<br>
<p><p>(*) Well maybe we have intelligent subprocesses, like when we can't
<br>
solve a math problem and then wake up one morning with the fully 
<br>
formed answer suddenly surfacing. Isn't there a book called &quot;Aha!&quot;
<br>
or something...Since we don't really know how our subconsciouses
<br>
work, it might be the case that we do have subprocesses that try
<br>
simulate various scenarios to get predicted results before bubbling
<br>
up an answer. 
<br>
<p><p>Off to lurkdom! &quot;Hello Darkness, here I come...&quot;
<br>
<p><pre>
--
Durant    
 
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0398.html">Eliezer S. Yudkowsky: "Re: When Subgoals Attack"</a>
<li><strong>Previous message:</strong> <a href="0396.html">Eliezer S. Yudkowsky: "Re: When Subgoals Attack"</a>
<li><strong>In reply to:</strong> <a href="0391.html">Gordon Worley: "Re: When Subgoals Attack"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0398.html">Eliezer S. Yudkowsky: "Re: When Subgoals Attack"</a>
<li><strong>Reply:</strong> <a href="0398.html">Eliezer S. Yudkowsky: "Re: When Subgoals Attack"</a>
<li><strong>Reply:</strong> <a href="0399.html">Gordon Worley: "Re: When Subgoals Attack"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#397">[ date ]</a>
<a href="index.html#397">[ thread ]</a>
<a href="subject.html#397">[ subject ]</a>
<a href="author.html#397">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
