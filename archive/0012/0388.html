<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Revising a Friendly AI</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Revising a Friendly AI">
<meta name="Date" content="2000-12-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Revising a Friendly AI</h1>
<!-- received="Wed Dec 13 14:16:07 2000" -->
<!-- isoreceived="20001213211607" -->
<!-- sent="Wed, 13 Dec 2000 01:57:21 -0500" -->
<!-- isosent="20001213065721" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Revising a Friendly AI" -->
<!-- id="3A371DD1.797EC571@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMMEKNEOAA.ben@intelligenesis.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Revising%20a%20Friendly%20AI"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Dec 12 2000 - 23:57:21 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0389.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="0387.html">Durant Schoon: "When Subgoals Attack"</a>
<li><strong>In reply to:</strong> <a href="0385.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0389.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<li><strong>Reply:</strong> <a href="0389.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#388">[ date ]</a>
<a href="index.html#388">[ thread ]</a>
<a href="subject.html#388">[ subject ]</a>
<a href="author.html#388">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &gt; Another way of looking at it is that all actual decisions, as in the ones
</em><br>
<em>&gt; &gt; that eventually get sent to motor control, are made by verbal thoughts.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is just not true... but I don't think it's crucial to your point...
</em><br>
<p>Okay, I'll expand.  There are reflex decisions that are carried out
<br>
directly by the spinal cord, although it is possible for these reflexes to
<br>
be inhibited in advance by signals sent from higher systems.
<br>
<p>Actual motor signals do not, of course, originate from the auditory cortex
<br>
- that's not what I meant by &quot;verbal thoughts&quot;.  (Though one current
<br>
theory holds that motor decisions originate in the entire Layer 4 of the
<br>
cerebral cortex, so almost any theory is neurotopologically plausible.) 
<br>
When you think &quot;I will get up and make coffee&quot; - not just the verbal
<br>
sounds themselves, of course, but the sounds plus your belief in them,
<br>
which is technically an emotional binding (I think).  Anyway, that verbal
<br>
thought creates several mental images.  For example, coffee as an
<br>
immediate subgoal, the expectation that this subgoal will be fulfilled
<br>
shortly, the anticipation of the associated taste sensation and
<br>
satisfaction, and so on.  However, the important image is not really a
<br>
declarative image as such, but a sort of plan-in-potentia - the default
<br>
set of instructions associated with getting up, making coffee, and so on. 
<br>
Unless conscious attention is focused on this default plan imagery (which
<br>
was created by the verbal thought, please note), it will start to feed
<br>
into motor cortex, or rather trigger the very-high-level motor actions
<br>
like &quot;get out of chair&quot;.
<br>
<p>I could be wrong about the picture I just drew, but it's my current best
<br>
guess for what happens when you think &quot;I will get up and make coffee&quot;. 
<br>
(Not that I drink coffee, of course.)
<br>
<p>There's quite possibly an opportunity for emotions to bind directly to the
<br>
plan-in-potentia, either before or after the plan is visualized, if the
<br>
plan has negative consequences (projected pain).  The emotions can exert
<br>
strong influences toward:  Removing the plan from being the &quot;default&quot;,
<br>
removing the plan from the focus of attention, replacing the plan with a
<br>
substitute, or giving rise to any number of verbal thoughts about the
<br>
desirability of replacing the plan.  Perhaps the plan may even be
<br>
suppressed directly via low-level negative feedback, which would involve,
<br>
in essence, emotions seizing the steering wheel.
<br>
<p>Under routine circumstances, however, the verbal thoughts are in immediate
<br>
control.  (Note that I do not say the conscious mind is in control, since
<br>
emotions can also exert major influence over verbal thoughts.)  With
<br>
respect to long-term goals, verbal thoughts are in effectively complete
<br>
control.
<br>
<p><em>&gt; &gt; &gt;From this perspective, the ability to do a verbal override is not
</em><br>
<em>&gt; &gt; necessarily an evolutionary advantage.  It is an inevitable consequence of
</em><br>
<em>&gt; &gt; our underlying cognitive architecture.  Our cognitive architecture is a
</em><br>
<em>&gt; &gt; huge evolutionary advantage compared to nonconsciousness; thus, remaining
</em><br>
<em>&gt; &gt; Neanderthal is not an option.  But, from the genes' perspectives, the
</em><br>
<em>&gt; &gt; first conscious architecture that happened to arise, while crushing the
</em><br>
<em>&gt; &gt; Neanderthals, has its own problems.  Such as allowing verbal overrides -
</em><br>
<em>&gt; &gt; and therefore, memetic overrides - of the built-in desires.  This is a
</em><br>
<em>&gt; &gt; &quot;problem&quot; from the perspective of the genes, anyway.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I understand your view, I think ... but I think it's wrong.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Culture makes us smarter and more adaptable, so genes that lead to culture
</em><br>
<em>&gt; should e selected, and they are.
</em><br>
<p>&quot;Culture&quot; is a big word.  Are we talking about replacing sex with
<br>
religious faith, or learning how to use a fork?  My view is that the two
<br>
are inseparable consequences of cognitive architecture; yours seems to be
<br>
that the two are each, individually, evolutionary advantages.
<br>
<p><em>&gt; &gt; I regard none of this as a factor in my picture of how a mind *should*
</em><br>
<em>&gt; &gt; work.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, our evolutionary heritage has many  minusses, e.g. the dark aspects of
</em><br>
<em>&gt; sexuality (jealousy, etc.); aggression; etc.
</em><br>
<p>Actually, I would regard the whole pleasure-pain-mental-energy
<br>
architecture as a &quot;minus&quot;.  But my perspective on this is not necessarily
<br>
unbiased.
<br>
<p><em>&gt; But I was noting one plus: it integrates relatively useful goal systems all
</em><br>
<em>&gt; through our minds in subtle &amp; complex ways.
</em><br>
<p>The return on integration is scarcely greater than the investment in
<br>
instinct... maybe less.  Remember, evolution is not just trying to
<br>
integrate novel and archaic goal systems, it is trying to *control* a
<br>
complex system with a simple one, which is why my description of a human
<br>
is so tangled and invokes so many different steps.
<br>
<p>Forget about whether imitating this is a *good* idea; I don't think we
<br>
*can*... not this side of the Singularity, anyway.
<br>
<p><em>&gt; If Ai systems don't evolve, they'll have to get this some other way, that's
</em><br>
<em>&gt; all.  It's far from impossible.
</em><br>
<p>What AIs need is the correct decision, the Friendliness.  Why do they need
<br>
the tangle to get it?  What's wrong with supergoal and subgoal?
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0389.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<li><strong>Previous message:</strong> <a href="0387.html">Durant Schoon: "When Subgoals Attack"</a>
<li><strong>In reply to:</strong> <a href="0385.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0389.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<li><strong>Reply:</strong> <a href="0389.html">Ben Goertzel: "RE: Revising a Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#388">[ date ]</a>
<a href="index.html#388">[ thread ]</a>
<a href="subject.html#388">[ subject ]</a>
<a href="author.html#388">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
