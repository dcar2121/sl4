<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: SL5</title>
<meta name="Author" content="Ben Goertzel (ben@intelligenesis.net)">
<meta name="Subject" content="SL5">
<meta name="Date" content="2000-11-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>SL5</h1>
<!-- received="Tue Nov 21 14:02:24 2000" -->
<!-- isoreceived="20001121210224" -->
<!-- sent="Tue, 21 Nov 2000 10:05:02 -0500" -->
<!-- isosent="20001121150502" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@intelligenesis.net" -->
<!-- subject="SL5" -->
<!-- id="NDBBIBGFAPPPBODIPJMMMEDICPAA.ben@intelligenesis.net" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="3A10D737.958D4E9B@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@intelligenesis.net?Subject=Re:%20SL5"><em>ben@intelligenesis.net</em></a>)<br>
<strong>Date:</strong> Tue Nov 21 2000 - 08:05:02 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0258.html">Spudboy100@aol.com: "Re: SL5"</a>
<li><strong>Previous message:</strong> <a href="0256.html">Eliezer S. Yudkowsky: "META: About &quot;the buzzer&quot;"</a>
<li><strong>In reply to:</strong> <a href="0189.html">Eliezer S. Yudkowsky: "Re: Ben's &quot;Extropian Creed&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0258.html">Spudboy100@aol.com: "Re: SL5"</a>
<li><strong>Maybe reply:</strong> <a href="0258.html">Spudboy100@aol.com: "Re: SL5"</a>
<li><strong>Reply:</strong> <a href="0260.html">Eliezer S. Yudkowsky: "Re: SL5"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#257">[ date ]</a>
<a href="index.html#257">[ thread ]</a>
<a href="subject.html#257">[ subject ]</a>
<a href="author.html#257">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
What follows are some moderately disorganized thoughts -- take them or
<br>
delete them as you wish...
<br>
<p>OK, so if we take Eliezer's definitions
<br>
<p>***
<br>
SL0:  The legendary average person is comfortable with modern technology -
<br>
not so much the frontiers of modern technology, but the technology used in
<br>
everyday life.  Most people, TV anchors, journalists, politicians.
<br>
SL1:  Virtual reality, living to be a hundred, &quot;The Road Ahead&quot;, &quot;To Renew
<br>
America&quot;, &quot;Future Shock&quot;, the frontiers of modern technology as seen by
<br>
Wired magazine.  Scientists, novelty-seekers, early-adopters, programmers,
<br>
technophiles.
<br>
SL2:  Medical immortality, interplanetary exploration, major genetic
<br>
engineering, and new (&quot;alien&quot;) cultures.  The average SF fan.
<br>
SL3:  Nanotechnology, human-equivalent AI, minor intelligence enhancement,
<br>
uploading, total body revision, intergalactic exploration. Extropians and
<br>
transhumanists.
<br>
SL4:  The Singularity, Jupiter Brains, Powers, complete mental revision,
<br>
ultraintelligence, posthumanity, Alpha-Point computing, Apotheosis, the
<br>
total evaporation of &quot;life as we know it.&quot;  Singularitarians and not much
<br>
else.
<br>
***
<br>
<p><p>then, it occurs to me that the proper Zen-Buddhistic answer to the question
<br>
&quot;What is SL5?&quot; is:
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;~Everyday, pretechnological, embodied life~
<br>
<p>;&gt;
<br>
<p><p>To be slightly less enigmatic, it seems to me that &quot;shock level&quot; has to do
<br>
not only with one's technological exposure, but also (and perhaps
<br>
more so) with one's fundamental existential outlook.
<br>
<p>To the person who has really come to grips with the [elusive,
<br>
unreal/real/semireal/surreal] nature of everyday life, none of these things
<br>
are shocking ...
<br>
<p>Shocking-ness comes about when a mind has given a falsely solid reality to
<br>
something that really isn't all that solid or definite at all
<br>
... and then finds out this falsely solid reality is indeed falsely solid...
<br>
<p>What's wonderful about SL4 is that at this stage,
<br>
science and technology are finally subverting themselves -- sci. and tech.
<br>
are the ultimate manifestations
<br>
of the Western mindset that focuses on concrete, solid, over-reified
<br>
external reality, and at the SL4 technology stage, they'll be truly
<br>
subverting the notion of external reality....  (In case you're curious, I'm
<br>
currently visualizing the previous sentence being uttered
<br>
by the talking asshole in William Burroughs' Naked Lunch ;)
<br>
<p>In terms of the ethical issues I raised earlier on this list: SL1-3 do
<br>
indeed correspond more easily with an elitist view in which only the top x%
<br>
of
<br>
the population get to partake in technological improvements.  SL4 posits a
<br>
level of being at which the notion of &quot;population&quot; and
<br>
&quot;individual&quot; are no longer necessarily meaningful, moving into a phase where
<br>
current ethical concerns don't really have grounding --
<br>
ethics as we know it is based on a notion of the individual and society
<br>
which is ephemeral on the grand scale... the principle of
<br>
compassion is timeless, but its manifestations will vary with the epoch...
<br>
<p>It's never really the technology that's shocking.  Shock is always the same
<br>
thing... the shattering of provisional assumptions, which  minds
<br>
need to make in order to cope with the lack of enough data to make definite
<br>
assertions.  Making provisional assumptions and realizing all
<br>
the while that they're provisional is a big trick ... hard for us to master
<br>
... will it be possible for other organisms, later on, to master
<br>
this trick consistently?  If so then Eliezer is right and transhuman AI's
<br>
really will avoid insanity nearly all the time.
<br>
<p>I'm almost converted!  Not to libertarianism, mind you ... but back to the
<br>
more digital-utopian perspective on AI that I had a few years
<br>
ago....
<br>
<p>But wait!  not quite....  Hold on.  It's always going to be MORE efficient
<br>
to hold a provisional assumption and forget that it's
<br>
provisional, on some level....  Given fixed resources, intelligence and
<br>
mental-health/enlightenment/inability-to-be-shocked will always
<br>
contradict each other.  The question is, if the fixed resources are LARGE
<br>
ENOUGH, then perhaps this inevitable tradeoff will become
<br>
less of a significant factor than it is in the human mind....
<br>
<p><p>ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0258.html">Spudboy100@aol.com: "Re: SL5"</a>
<li><strong>Previous message:</strong> <a href="0256.html">Eliezer S. Yudkowsky: "META: About &quot;the buzzer&quot;"</a>
<li><strong>In reply to:</strong> <a href="0189.html">Eliezer S. Yudkowsky: "Re: Ben's &quot;Extropian Creed&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0258.html">Spudboy100@aol.com: "Re: SL5"</a>
<li><strong>Maybe reply:</strong> <a href="0258.html">Spudboy100@aol.com: "Re: SL5"</a>
<li><strong>Reply:</strong> <a href="0260.html">Eliezer S. Yudkowsky: "Re: SL5"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#257">[ date ]</a>
<a href="index.html#257">[ thread ]</a>
<a href="subject.html#257">[ subject ]</a>
<a href="author.html#257">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
