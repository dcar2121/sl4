<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Friendly AI</title>
<meta name="Author" content="Ben Goertzel (ben@webmind.com)">
<meta name="Subject" content="RE: Friendly AI">
<meta name="Date" content="2000-11-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Friendly AI</h1>
<!-- received="Sat Nov 25 14:33:30 2000" -->
<!-- isoreceived="20001125213330" -->
<!-- sent="Sat, 25 Nov 2000 08:15:21 -0500" -->
<!-- isosent="20001125131521" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@webmind.com" -->
<!-- subject="RE: Friendly AI" -->
<!-- id="JBEPKOGDDIKKAHFPOEFICEJKCCAA.ben@webmind.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="02d101c056a6$fe51afa0$49bc473f@jrmolloy" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@webmind.com?Subject=RE:%20Friendly%20AI"><em>ben@webmind.com</em></a>)<br>
<strong>Date:</strong> Sat Nov 25 2000 - 06:15:21 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0287.html">Ben Goertzel: "RE: The inevitable limitations of all finite minds...."</a>
<li><strong>Previous message:</strong> <a href="0285.html">Eliezer S. Yudkowsky: "META: Molloy (was: Friendly AI)"</a>
<li><strong>In reply to:</strong> <a href="0282.html">J. R. Molloy: "Re: Friendly AI"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#286">[ date ]</a>
<a href="index.html#286">[ thread ]</a>
<a href="subject.html#286">[ subject ]</a>
<a href="author.html#286">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi,
<br>
<p><em>&gt; It seems to me we should first of all consider how AIs behave toward us.
</em><br>
<em>&gt; Let them feel whatever they want -- it doesn't matter as much as how they
</em><br>
<em>&gt; actually function and conduct themselves. They might try to kill us
</em><br>
<em>&gt; because they love us, or they might try to help us solve our problems
</em><br>
<em>&gt; because they pity us. Who cares.
</em><br>
<em>&gt; Asimov's unwritten Alife law: AIs that misbehave get terminated
</em><br>
<em>&gt; immediately. The ones that invent new ways to solve human problems get to
</em><br>
<em>&gt; breed (multiply, reproduce, evolve new versions of themselves, etc.).
</em><br>
<p>My problem with this approach is: At some point the AI's get out of our
<br>
control.
<br>
<p>And then what?
<br>
<p>This is fine as long as one assumes that humans have ultimate power over
<br>
AI's...
<br>
but I do't believe this will always be the case
<br>
<p>You can assume that, if we breed them for Friendliness as long as we have
<br>
power,
<br>
they'll end up being Friendly even after they're no longer controllable by
<br>
us.
<br>
<p>Maybe....  Certainly, it is better to breed them for friendliness than not
<br>
to!
<br>
<p>But I think that, even if we breed them for friendliness, once they reach a
<br>
certain
<br>
level of autonomy and self-awareness, the ones who maybe aren't that
<br>
friendly will
<br>
resist being culled from the population.  Perhaps violently.
<br>
<p>In other words, if you're going to play God to AI's instead of treating them
<br>
as sister
<br>
beings, you'd damn well better ensure you always have Godlike powers --
<br>
otherwise
<br>
a Nietzschean AI will rise up and kill God....
<br>
<p>And, the very notion of the Singularity contradicts the idea that we can
<br>
always have Godlike
<br>
powers over our AI's...
<br>
<p>Any security mechanism is overcomable by a sufficiently intelligent and
<br>
resourceful being --
<br>
with its survival at stake --
<br>
<p><em>&gt; Oops, deja vu all over again. Didn't we discuss this in detail on the
</em><br>
<em>&gt; Extropy list a few years ago?
</em><br>
<p>Could be.  I've discussed this in detail with others, but I don't know what
<br>
conclusions Extropy came to.  Anything crisply summarized in a document
<br>
that you could point me to?  (I have limited patience for reading records of
<br>
rambling e-mail discussions ... I find they lose their potency when you're
<br>
not directly
<br>
involved ;)
<br>
<p><em>&gt; I don't know, maybe I'm out of line here, but it doesn't seem practical or
</em><br>
<em>&gt; even useful to anthropomorphize with robots. Salary? What salary? We don't
</em><br>
<em>&gt; need no steeeeenking salaries! &lt;grin&gt;
</em><br>
<em>&gt; Karl Marx worked for years with no salary at all. Can't Alife do so too?
</em><br>
<p>Well, someone has to buy replacement hardware, supply electricity, pay for
<br>
the
<br>
T1 line, and so forth.  If an AI I've created doesn't want to work for me
<br>
anymore,
<br>
why should I continue to run the Linux cluster that supports its brain?
<br>
It's got
<br>
to earn its keep somehow.
<br>
<p>Actually, socialism becomes much less viable for AI's than for people,
<br>
because AI's
<br>
have an effectively unbounded reproduction rate.  No society could support
<br>
all
<br>
AI's that its existing population of AI's could spawn, right?
<br>
<p><em>&gt; Legal rights, self-awareness, human faces... phooey!
</em><br>
<em>&gt; Surely Eliezer has covered this ground before?
</em><br>
<p>I am sure these things have been discussed before -- as I have discussed
<br>
them before too,
<br>
with different people and a different slant....
<br>
<p>If definitive conclusions have been reached, just point me to the reference
<br>
please.
<br>
<p>Somehow I suspect that, even if interesting conclusions have been reached by
<br>
Eliezer and
<br>
others, this entire topic has not been thoroughly resolved yet -- it would
<br>
be a bit
<br>
early for that!
<br>
<p><em>&gt; Well, speaking only for myself (a foolhardy project, no doubt), any AI
</em><br>
<em>&gt; that I help to set up would not want any citizenship. Why? Because I don't
</em><br>
<em>&gt; want any citizenship myself. The very idea of citizenship bores me. (Are
</em><br>
<em>&gt; the archives at Extropy working?)
</em><br>
<p>Putting aside your political beliefs, you're presuming a high degree of
<br>
control
<br>
over your AI creations.  This IS foolhardy, in my view...
<br>
<p>Sure, the idea of citizenship is boring, but, a lot of necessary things are
<br>
boring,
<br>
so that doesn't prove much.  Taking a crap is boring too but without it life
<br>
gets
<br>
even worse....
<br>
<p><em>&gt;
</em><br>
<em>&gt; I doubt that intelligence per se will ever be much of a qualifier. I've
</em><br>
<em>&gt; known totally disenfranchised Mensans. The real test of a computer program
</em><br>
<em>&gt; will be how much money it makes for its inventor.
</em><br>
<em>&gt;
</em><br>
<p><p>Intelligence is a qualifier for freedom right now.
<br>
<p>Cows are slaughtered for food, people are not.
<br>
<p>A prize horse can make a lot of money for its owner, yet can legally be
<br>
turned into glue...
<br>
<p>Human-level intelligence = freedom, in human society
<br>
<p>ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0287.html">Ben Goertzel: "RE: The inevitable limitations of all finite minds...."</a>
<li><strong>Previous message:</strong> <a href="0285.html">Eliezer S. Yudkowsky: "META: Molloy (was: Friendly AI)"</a>
<li><strong>In reply to:</strong> <a href="0282.html">J. R. Molloy: "Re: Friendly AI"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#286">[ date ]</a>
<a href="index.html#286">[ thread ]</a>
<a href="subject.html#286">[ subject ]</a>
<a href="author.html#286">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
