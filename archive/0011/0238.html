<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Static uploading is SL3 (was: the 69 of us)</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Static uploading is SL3 (was: the 69 of us)">
<meta name="Date" content="2000-11-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Static uploading is SL3 (was: the 69 of us)</h1>
<!-- received="Sat Nov 18 13:44:02 2000" -->
<!-- isoreceived="20001118204402" -->
<!-- sent="Sat, 18 Nov 2000 13:40:19 -0500" -->
<!-- isosent="20001118184019" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Static uploading is SL3 (was: the 69 of us)" -->
<!-- id="3A16CD13.C4CCED22@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMKEOEEMAA.ben@intelligenesis.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Static%20uploading%20is%20SL3%20(was:%20the%2069%20of%20us)"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Nov 18 2000 - 11:40:19 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0239.html">Eliezer S. Yudkowsky: "META: Evolving minds"</a>
<li><strong>Previous message:</strong> <a href="0237.html">Dale Johnstone: "Re: Evolving minds"</a>
<li><strong>In reply to:</strong> <a href="0234.html">Ben Goertzel: "RE: Static uploading is SL3 (was: the 69 of us)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0245.html">Ben Goertzel: "RE: Static uploading is SL3 (was: the 69 of us)"</a>
<li><strong>Reply:</strong> <a href="0245.html">Ben Goertzel: "RE: Static uploading is SL3 (was: the 69 of us)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#238">[ date ]</a>
<a href="index.html#238">[ thread ]</a>
<a href="subject.html#238">[ subject ]</a>
<a href="author.html#238">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &gt; But I
</em><br>
<em>&gt; &gt; can't see a transhuman upload as vulnerable to insanity, even
</em><br>
<em>&gt; &gt; leaving aside
</em><br>
<em>&gt; &gt; fine-grained control of simulated neurology or the ability to
</em><br>
<em>&gt; &gt; self-modify.  A
</em><br>
<em>&gt; &gt; transhuman should simply be able to think faster than the forces that are
</em><br>
<em>&gt; &gt; responsible for insanity in biological humans - see any problems
</em><br>
<em>&gt; &gt; a mile away.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Frankly, I think this is an excessively Utopian point of view
</em><br>
<p>If so, it's an excessively Utopian view about something that I have absolutely
<br>
no responsibility to do anything about, which is the best kind of excessively
<br>
Utopian view.
<br>
<p><em>&gt; First: I don't think that insanity has to do with thinking fast or thinking
</em><br>
<em>&gt; slowly -- if so, why
</em><br>
<em>&gt; do intelligent people go nuts just as often as stupid people?
</em><br>
<p>My instinctive response is to say &quot;They don't&quot;, after which we could try to
<br>
look up the statistics... but that really wouldn't settle anything.
<br>
<p><em>&gt; Of course,
</em><br>
<em>&gt; you could say that
</em><br>
<em>&gt; insanity is cured by fast thinking only once thinking gets beyond a certain
</em><br>
<em>&gt; critical threshold
</em><br>
<em>&gt; of speed, but this seems like &quot;special pleading&quot;...
</em><br>
<p>Not so much speed, but proficiency.  The ordinary speed of thought of the
<br>
average human would be enough to outthink insanity *if* we had sufficient
<br>
knowledge of how our own minds worked.  We're starting to get into that, but
<br>
we're not really there yet.  It seems more like a matter of augmenting the
<br>
specific cognitive abilities of self-awareness than general intelligence.  I
<br>
shalln't argue the point excessively, since a much stronger argument is - as
<br>
you say:
<br>
<p><em>&gt; Second: It's a better argument that a system with the ability to modify
</em><br>
<em>&gt; itself, would be able to
</em><br>
<em>&gt; fix the types of problems we refer to as 'insanity' -- or ask others to fix
</em><br>
<em>&gt; them.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This second argument makes some sense.  It's not fast thinking, but rather
</em><br>
<em>&gt; the ability to modify
</em><br>
<em>&gt; one's 'hardware', that will eliminate a lot of what we call insanity from
</em><br>
<em>&gt; transhuman minds...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; However, even self-modification isn't necessarily a magic cure for
</em><br>
<em>&gt; craziness.  Much insanity is
</em><br>
<em>&gt; motivational and emotional in nature -- with cognitive consequences that
</em><br>
<em>&gt; follow from these underlying
</em><br>
<em>&gt; problems.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I.e., why can't a transhuman get into a state where it's nuts and doesn't
</em><br>
<em>&gt; want to modify itself into
</em><br>
<em>&gt; &quot;non-nuts-ness&quot;?
</em><br>
<p>The essential argument here is that a transhuman can *occupy* such a state,
<br>
but ve is unlikely to *reach* such a state.  The transhuman would see it
<br>
coming; if ve didn't see it coming, there would be early warning signs,
<br>
flashing red lights on the exoself console, and so on.  Before a transhuman
<br>
can reach such a state, ve would need to first need to desire such a state. 
<br>
This desire can occur for one of three reasons: due to a new, existing mental
<br>
problem that should have been spotted, corrected, and prevented (recurse on
<br>
argument); due to an old mental problem imported from humanity; or for reasons
<br>
that are as philosophically valid as any other.  The first case should be
<br>
preventable; the third case constitutes a sovereign case of individual rights;
<br>
the second case probably constitutes a sovereign case of individual rights as
<br>
well, but note that it would probably require the individual to deliberately
<br>
ignore warning lights.
<br>
<p>In any case, the original argument was not that SOME transhumans might go
<br>
insane by choice, but that ALL transhumans would go insane inevitably; the
<br>
latter statement is almost certainly false.
<br>
<p><em>&gt; In this case, should other transhumans intervene and fix
</em><br>
<em>&gt; its mental structures, so it's
</em><br>
<em>&gt; not nuts anymore?
</em><br>
<p>I don't know.  Perhaps the answer will take the form of &quot;Sometimes&quot; rather
<br>
than &quot;Yes&quot; or &quot;No&quot;.
<br>
<p><em>&gt; This will presumably almost always be possible (although
</em><br>
<em>&gt; there may be tough mathematical
</em><br>
<em>&gt; problems in determining how to tweak someone's mind to leave them with their
</em><br>
<em>&gt; &quot;self&quot; but not their insane
</em><br>
<em>&gt; features) ... at least to some extent... but even so this gives rise to
</em><br>
<em>&gt; serious ethical issues to do with
</em><br>
<em>&gt; individual freedom.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I do not believe that you guys have resolved this issue in a definitive way.
</em><br>
<em>&gt; It seems, rather, that you've
</em><br>
<em>&gt; brushed the issue aside due to optimism and confidence in the power of
</em><br>
<em>&gt; intelligence (a habit that I also
</em><br>
<em>&gt; possess, to be sure).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In my view, it's wrong to consider it likely that transhuman minds will be
</em><br>
<em>&gt; driven nuts by the blurring of
</em><br>
<em>&gt; the mind-reality boundary.  But it's also wrong to say that transhumans will
</em><br>
<em>&gt; be intrinsically sane due to their
</em><br>
<em>&gt; intelligence and self-modifying abilities.
</em><br>
<p>I think that transhumans will have the power to be intrinsically sane.  Choice
<br>
is another issue, but it's not *my* choice - I don't think - so I'll stick
<br>
with my optimism and confidence.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0239.html">Eliezer S. Yudkowsky: "META: Evolving minds"</a>
<li><strong>Previous message:</strong> <a href="0237.html">Dale Johnstone: "Re: Evolving minds"</a>
<li><strong>In reply to:</strong> <a href="0234.html">Ben Goertzel: "RE: Static uploading is SL3 (was: the 69 of us)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0245.html">Ben Goertzel: "RE: Static uploading is SL3 (was: the 69 of us)"</a>
<li><strong>Reply:</strong> <a href="0245.html">Ben Goertzel: "RE: Static uploading is SL3 (was: the 69 of us)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#238">[ date ]</a>
<a href="index.html#238">[ thread ]</a>
<a href="subject.html#238">[ subject ]</a>
<a href="author.html#238">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
