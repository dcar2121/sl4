<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Ben's &quot;Extropian Creed&quot;</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Ben's &quot;Extropian Creed&quot;">
<meta name="Date" content="2000-11-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Ben's &quot;Extropian Creed&quot;</h1>
<!-- received="Tue Nov 14 00:49:47 2000" -->
<!-- isoreceived="20001114074947" -->
<!-- sent="Tue, 14 Nov 2000 00:45:54 -0500" -->
<!-- isosent="20001114054554" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Ben's &quot;Extropian Creed&quot;" -->
<!-- id="3A10D192.F0844E51@pobox.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMCEKIEMAA.ben@intelligenesis.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Ben's%20&quot;Extropian%20Creed&quot;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Nov 13 2000 - 22:45:54 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0182.html">Patrick McCuller: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<li><strong>Previous message:</strong> <a href="0180.html">Michael LaTorra: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<li><strong>In reply to:</strong> <a href="0158.html">Ben Goertzel: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0183.html">Ben Goertzel: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<li><strong>Reply:</strong> <a href="0183.html">Ben Goertzel: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#181">[ date ]</a>
<a href="index.html#181">[ thread ]</a>
<a href="subject.html#181">[ subject ]</a>
<a href="author.html#181">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; There are many things to say on this topic, and for starters I'll say only a
</em><br>
<em>&gt; few of them
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I knew Sasha really well, and I think I have a deep understanding of what
</em><br>
<em>&gt; his views were, and I don't think I misrepresented them.
</em><br>
<p>I can accept that.
<br>
<p><em>&gt; This doesn't mean I think Eliezer's views are the same.
</em><br>
<p>==
<br>
<p><em>&gt; I love the human
</em><br>
<em>&gt; warmth and teeming mental diversity of important thinkers like Max
</em><br>
<em>&gt; More, Hans  Moravec, Eliezer Yudkowsky and Sasha Chislenko,
</em><br>
<em>&gt; and great thinkers like Nietzsche – and I hope and expect that these
</em><br>
<em>&gt; qualities will outlast the more simplistic, ambiguity-fearing aspects of
</em><br>
<em>&gt; their philosophies.
</em><br>
<p>&quot;Their philosophies&quot;...
<br>
<p>I think that &quot;Max More&quot; and &quot;Eliezer Yudkowsky&quot; do not belong in the referent
<br>
of &quot;their&quot;.  I don't think that either of us are simplistic, I don't think
<br>
that either of us fear ambiguity, and I don't think that either of us are
<br>
Social Darwinists.
<br>
<p>==
<br>
<p><em>&gt; Another line of thinking, which Sasha explicitly maintained in many
</em><br>
<em>&gt; conversations with me -- and Moravec and
</em><br>
<em>&gt; many other libertarians hint at in their writings -- is that the &quot;hi-tech&quot;
</em><br>
<em>&gt; breed of humans is somehow superior
</em><br>
<em>&gt; to the rest of humanity and hence more deserving of living on forever in the
</em><br>
<em>&gt; grand and glorious cyber-afterlife...
</em><br>
<em>&gt;
</em><br>
<em>&gt; The former line of thinking is one I have some emotional sympathy for --
</em><br>
<em>&gt; I've definitely felt that way myself
</em><br>
<em>&gt; at some times in my life.  The latter line of thinking really gets on my
</em><br>
<em>&gt; nerves.
</em><br>
<p>I have no sympathy for the second viewpoint.  If Sascha believed it, then boo
<br>
for him.
<br>
<p><em>&gt; &gt; I also don't believe that the poor will be left out of the Singularity...
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; One of the major reasons I am *in* the Singularity biz is to wipe out
</em><br>
<em>&gt; &gt; nonconsensual poverty.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Ben Goertzel, does that satisfy your call for a humanist transhumanism?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, Eliezer Yudkowsky, it does and it doesn't...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; On the one hand, I'd like to say: &quot;It's not enough to ~believe~ that the
</em><br>
<em>&gt; poor will automatically
</em><br>
<em>&gt; be included in the flight to cyber-transcendence.  One should actively
</em><br>
<em>&gt; strive to ensure that this happens.
</em><br>
<em>&gt; This is a very CONVENIENT belief, in that it tells you that you can help
</em><br>
<em>&gt; other people by ignoring them...
</em><br>
<em>&gt; this convenience is somehow psychologically suspicious...&quot;
</em><br>
<p>I can actively strive to ensure that the poor are not left out of the
<br>
Singularity by building a Friendly AI, such that the referent for Friendliness
<br>
treats all humans as symmetrical and such that property ownership within human
<br>
society is not seen as relevant to dividing up the Solar System outside of
<br>
Earth.
<br>
<p>I used to have the extremely convenient belief that Friendly AI would happen
<br>
completely on automatic because morality was objective.  I now believe that
<br>
Friendly AI needs to be more complex than that, both because morality might be
<br>
nonobjective, or it might be objective but specifiable, or it might take more
<br>
underlying complexity for the AI to reach objective morality from its starting
<br>
point, and also because it may take more underlying complexity to create a
<br>
self-modifying AI which is stable during the prehuman phase of development.
<br>
<p>Recently, I've been putting considerable work into building a foundation for
<br>
Friendly AI.  I should also note that I don't believe cyber-transcendence
<br>
*itself* can be assumed; I think that humanity could easily be wiped out by
<br>
any number of catastrophes.  That's why I'm devoting my life to making the
<br>
Singularity happen earlier - a benefit that encompasses all of humanity,
<br>
including the poor.
<br>
<p>So I *am* striving.
<br>
<p><em>&gt; On the other hand,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; a) I'm not really doing anything to actively strive to ensure that this
</em><br>
<em>&gt; happens, at the moment, because building
</em><br>
<em>&gt; a thinking machine while running a business is a pretty all-consuming
</em><br>
<em>&gt; occupation.  So to criticize you for doing
</em><br>
<em>&gt; like I'm doing would be rather hypocritical.  Of course, once Webmind has
</em><br>
<em>&gt; reached a certain point, I ~plan~ to
</em><br>
<em>&gt; explicitly devote attention to ensuring that the benefits of it and other
</em><br>
<em>&gt; advanced technology encompass everyone ...
</em><br>
<p>I'm working out the docs on Friendly AI now, but let me just say that this
<br>
will need to be *before* Webmind becomes capable of self-modification. 
<br>
*Cough*.  Sorry.  Back to the moral philosophy...
<br>
<p><em>&gt; b) hey, maybe you're right.  I don't KNOW that you're wrong....  Sometimes
</em><br>
<em>&gt; you get lucky and the convenient
</em><br>
<em>&gt; beliefs are the right ones!!
</em><br>
<p>I don't think it is convenient.  Ifni knows I wish it were.  Life was a lot
<br>
simpler back when I was treating objective morality as the only important
<br>
branch of reality.
<br>
<p><em>&gt; Saying that the best way to help the poor is to bring the Singularity about
</em><br>
<em>&gt; is definitely an &quot;ends justifies
</em><br>
<em>&gt; the means&quot; philosophy -- because the means involves people like you and I
</em><br>
<em>&gt; devoting our talents to bringing
</em><br>
<em>&gt; the Singularity about rather than to helping needy people NOW....
</em><br>
<p>Who is &quot;needy&quot;?  The Singularity isn't just for the poor; it's for *everyone*,
<br>
including me.  Should the person struggling along on $12K/year feel guilty for
<br>
not sending 75% of his income to South Africa?  On the scale of the
<br>
Singularity, we're all needy, including me.  This isn't something I'm doing to
<br>
&quot;lend a helping hand to the lower classes&quot;; this is something that I'm doing
<br>
because *all* of humanity is in desperate straits, and the only way out is
<br>
through a Singularity.
<br>
<p>Be it far from me to oversimplify.  I acknowledge that this is a complex,
<br>
emotional issue.  I think that there exists an emotion within us that makes us
<br>
feel an obligation to those less fortunate, and a collection of cognitive
<br>
drives which lend an intuitional appeal to the philosophy that says that we
<br>
have more than enough, and that by failing to give everything we own to
<br>
charity, we are trading off an immediate good we could do for some nebulous
<br>
future good.  But we also have emotional hardware which binds to a different
<br>
model of reality; a model in which we are not rich, in which we do *not* have
<br>
enough, in which everything that our money can buy is the smallest fraction of
<br>
what every human being deserves.  A view in which we stand alongside the most
<br>
poverty-stricken orphan, equally deserving, simply by virtue of being merely
<br>
human.  A view under which we're all equally unhappy and we're all in this
<br>
together.  I think this viewpoint has equal emotional validity.
<br>
<p>The viewpoint that says &quot;all concentration of wealth is bad&quot; has emotional
<br>
validity, but it's emotional validity which is false - that is, which involves
<br>
false-to-fact mental imagery.  Producing more wealth requires concentration of
<br>
wealth in venture capital.  Let us consider the &quot;wealth function&quot; of a
<br>
planet.  If the bumps in the wealth function just smoothed out, flowing away
<br>
with complete liquidity, the planet would not be well-served; the whole Earth
<br>
would be a Third World country, without enough wealth in any one location to
<br>
create an advanced industrial base.  Earth is equally ill-served if wealth is
<br>
completely illiquid; bumps in the wealth function may get higher, but without
<br>
any of it flowing over... and particularly, without any of it flowing over to
<br>
start new bumps.
<br>
<p>The happy medium is moderate liquidity of wealth, which translates back into
<br>
our own minds as a justified emotional validity for giving part-but-not-all of
<br>
your income to charity.  If the percentage of income is significant, above
<br>
average, and above average for your wealth bracket, then there is no logical
<br>
reason for you to feel guilty.  If you spend a lot of your time investing
<br>
venture capital or CEOing or otherwise creating new wealth, then there is no
<br>
logical reason for you to feel guilty.
<br>
<p>I don't feel guilty about enjoying what I have.  I don't have all that much. 
<br>
Others have less, others have more.  We're all humans and in this together. 
<br>
And as far as I'm concerned, spending my ENtire adult life trying to save the
<br>
world from unspeakable horrors pays all my guilt dues for the next three
<br>
hundred years.
<br>
<p><em>&gt; In other words, I'm happy enough to critique Extropianism as a collection of
</em><br>
<em>&gt; words written down on paper,
</em><br>
<em>&gt; a conceptual philosophy....  I'm happy enough to critique my friends, like
</em><br>
<em>&gt; Sasha, whom I know well (and I know he
</em><br>
<em>&gt; wouldn't mind me picking on him even after his death, he loved this kind of
</em><br>
<em>&gt; argumentation).  But I have no desire
</em><br>
<em>&gt; to pass judgment, positive or negative, on Eliezer and others whom I don't
</em><br>
<em>&gt; know....  I guess this makes me a weirdo, but that's not really news now is
</em><br>
<em>&gt; it...
</em><br>
<p>Basically, I'm saying that Social Darwinism isn't part of Extropianism; it was
<br>
Sascha's private opinion.  That's the part of the FAZ article that I'm
<br>
objecting to.  I think you got a mistaken picture of Extropy from Sascha and
<br>
then passed it on to the FAZ readers.  I totally understand if that was an
<br>
honest mistake - that's why I'm writing this response, isn't it?
<br>
<p>Ben Goertzel wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; to be totally frank, I have to admit to being a bit too sensationalistic in
</em><br>
<em>&gt; that article, in order to
</em><br>
<em>&gt; get the publisher excited.  Hey, I'm human too... as of now at any rate ;&gt;
</em><br>
<p>Holding the sensationalism invariant, I would like to have seen something
<br>
along the lines of:
<br>
<p>&quot;But there's a streak of Social Darwinism in some Extropians, and that worries
<br>
me.  I don't want to imply that all Extropians are Social Darwinists; some
<br>
explicitly see the march of technology as a means of combating poverty. 
<br>
Nonetheless, I think that part of the appeal of libertarian transhumanism
<br>
rests on an over-simplification, a neat moral counter to guilt, that says that
<br>
people get what they deserve.  The Extropians don't seem to be overly
<br>
concerned about whether the humans below the poverty line will get to
<br>
participate in this wonderful world of theirs.  To be fair, some of them think
<br>
that today's differences in wealth will be smoothed over or wiped out entirely
<br>
by the changes ahead, or that the new world will be a vast improvement even
<br>
for the futuristic poor.  I don't think that's enough.  I don't think that
<br>
universal participation, or a universal chance to participate, is automatic,
<br>
and it worries me that those most intimately involved with the future seem to
<br>
have lost some essential, human compassion.&quot;
<br>
<p>You'd still be dead wrong, of course, but you'd be much less dead wrong.  In
<br>
particular, you would be explicitly saying that any Social Darwinism is an
<br>
undiscovered worm eating at the heart of Extropy, not a toast publicly made at
<br>
parties.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0182.html">Patrick McCuller: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<li><strong>Previous message:</strong> <a href="0180.html">Michael LaTorra: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<li><strong>In reply to:</strong> <a href="0158.html">Ben Goertzel: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0183.html">Ben Goertzel: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<li><strong>Reply:</strong> <a href="0183.html">Ben Goertzel: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#181">[ date ]</a>
<a href="index.html#181">[ thread ]</a>
<a href="subject.html#181">[ subject ]</a>
<a href="author.html#181">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
