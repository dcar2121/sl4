<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Pain vs. negative feedback (was: Evolving minds)</title>
<meta name="Author" content="Ben Goertzel (ben@intelligenesis.net)">
<meta name="Subject" content="RE: Pain vs. negative feedback (was: Evolving minds)">
<meta name="Date" content="2000-11-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Pain vs. negative feedback (was: Evolving minds)</h1>
<!-- received="Sun Nov 19 00:05:36 2000" -->
<!-- isoreceived="20001119070536" -->
<!-- sent="Sat, 18 Nov 2000 23:25:07 -0500" -->
<!-- isosent="20001119042507" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@intelligenesis.net" -->
<!-- subject="RE: Pain vs. negative feedback (was: Evolving minds)" -->
<!-- id="NDBBIBGFAPPPBODIPJMMKEPGEMAA.ben@intelligenesis.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3A171FE8.98358010@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@intelligenesis.net?Subject=RE:%20Pain%20vs.%20negative%20feedback%20(was:%20Evolving%20minds)"><em>ben@intelligenesis.net</em></a>)<br>
<strong>Date:</strong> Sat Nov 18 2000 - 21:25:07 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0252.html">Ben Goertzel: "RE: Pain vs. negative feedback (was: Evolving minds)"</a>
<li><strong>Previous message:</strong> <a href="0250.html">J. R. Molloy: "Re: Evolving minds"</a>
<li><strong>In reply to:</strong> <a href="0249.html">Eliezer S. Yudkowsky: "Pain vs. negative feedback (was: Evolving minds)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0252.html">Ben Goertzel: "RE: Pain vs. negative feedback (was: Evolving minds)"</a>
<li><strong>Reply:</strong> <a href="0252.html">Ben Goertzel: "RE: Pain vs. negative feedback (was: Evolving minds)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#251">[ date ]</a>
<a href="index.html#251">[ thread ]</a>
<a href="subject.html#251">[ subject ]</a>
<a href="author.html#251">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; Ben Goertzel wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; In Feb. we'll start a new phase, when we'll make operational
</em><br>
<em>&gt; &gt; the &quot;psyche&quot; component of the system (goals, feelings,
</em><br>
<em>&gt; &gt; motivations) ... we then will be quite precisely dealing with
</em><br>
<em>&gt; &gt; issues of friendliness and unfriendliness.  Questions like:
</em><br>
<em>&gt; &gt; What attitude does the system have when we insert new knowledge
</em><br>
<em>&gt; &gt; into its mind, which causes it annoyance and pain because it
</em><br>
<em>&gt; &gt; forces it to revise its hard-won beliefs....
</em><br>
<em>&gt;
</em><br>
<em>&gt; I really think you're making unnecessary problems for yourselves!  The
</em><br>
<em>&gt; human brain uses instincts because it was built that way.  Why use
</em><br>
<em>&gt; instincts when you can use declarative, rational, context-sensitive
</em><br>
<em>&gt; thoughts to accomplish the same function with more finesse?  Because
</em><br>
<em>&gt; thoughts are slower?  True.
</em><br>
<p>Because higher-order inference (even of the probabilistic variety)
<br>
requires a lot of data to draw plausibly
<br>
accurate judgments.  In the absence of quality data, less explicitly
<br>
inferential,
<br>
more intuitive (and neural-nettish) methods work better.
<br>
<p>As it happens, the achievement of important goals for an organism tends to
<br>
involve a lot of inference in domains where data is scanty....  Thus
<br>
intuition
<br>
becomes important...
<br>
<p>And &quot;Goals&quot; are necessary to avoid a mind being a completely disorganized
<br>
chaotic mess.
<br>
The goal of survival is a particularly handy one, as is the goal of
<br>
procreation of one's genetic
<br>
material.  AI's that have this goal will tend to proliferate, for obvious
<br>
reasons.
<br>
<p>A &quot;feeling&quot; is really just an internal sensor, specialized for some purpose.
<br>
<p>In humans, due to the weirdness of evolution, some of our feelings have
<br>
become
<br>
detached from their original purposes.  This won't be so much of a problem
<br>
for
<br>
self-modifying AI's.....
<br>
<p>For instance, the feeling of lust when you see an attractive member of the
<br>
opposite
<br>
sex is a useful one.  (The human race is better off because we have it;
<br>
otherwise why would
<br>
we bother reproducing ourselves.)  But the need for variety in sexual
<br>
partners, even, say, once one has
<br>
had a vasectomy and cannot produce more children, serves no purpose and in
<br>
many cases can
<br>
fuck up one's life.  (This isn't an example from my own personal life, by
<br>
the way; it's something
<br>
I observed in a friend and found particularly ironic ;).
<br>
<p>For another example, aggressive and angry feelings are less useful now than
<br>
they were in
<br>
preindustrial societies...
<br>
<p>In a Webmind context, we give our system an instinctively bad feeling when
<br>
it runs out of memory,
<br>
or answers queries too slowly, or fails to gain any new information for a
<br>
while. Are you suggesting
<br>
that the system should learn, by abstract reason, that running out of memory
<br>
is bad?
<br>
<p>This kind of thing can't be effectively learned over an individual lifespan,
<br>
because once the system has
<br>
run out of memory, it can no longer reason!!  The data for reasoning is
<br>
REALLY not present in this case.
<br>
Thus, it can only be learned on an evolutionary time-scale.
<br>
Evolution results in a system being born with an instinctive aversion to
<br>
running out of memory.  Instead of
<br>
requiring this to evolve, we've built it in as an instinctive feeling.
<br>
(This is just the simplest example.)
<br>
<p><em>&gt; Still, why use blind instincts when you can
</em><br>
<em>&gt; use context-sensitive instincts?  Why should the system experience pain
</em><br>
<em>&gt; when propagating updates to old knowledge, any more than it experiences
</em><br>
<em>&gt; pain on updating its visual field?  Why is that kind of pain necessary?
</em><br>
<em>&gt; How does it make Webmind more intelligent?
</em><br>
<p>Because, throwing out a lot of your knowledge can make you idiotic for a
<br>
while,
<br>
which, in an evolutionary setting, is very bad for your survival value.
<br>
<p>Actually, we want the system to generally be skeptical about other agents
<br>
wanting to
<br>
feed it new brain matter!  But we want it to trust us when we feed it new
<br>
brain matter...
<br>
<p><em>&gt; &gt; How does the system
</em><br>
<em>&gt; &gt; feel about us changing the way it evaluates its own health... or
</em><br>
<em>&gt; &gt; the degree to which it &quot;feels it&quot; when humans are unhappy with it...
</em><br>
<em>&gt;
</em><br>
<em>&gt; It looks to me like it would take an extremely sophisticated design for
</em><br>
<em>&gt; Webmind to feel anything at all.  I mean, you and I might not like it if
</em><br>
<em>&gt; someone started tweaking our own feedback systems to increase the amount
</em><br>
<em>&gt; of pain - because we map ourselves onto our future selves and sympathize
</em><br>
<em>&gt; with our future selves.  That is not a trivial ability.
</em><br>
<p>It's not ~trivial~, but it's well within the capability of WM's inference
<br>
and
<br>
prediction components.
<br>
<p><em>&gt; Webmind would need to realize that it had more pain than it would have had
</em><br>
<em>&gt; otherwise, trace back the causality for that to the action of the human,
</em><br>
<em>&gt; categorize the presence of &quot;more pain than in a subjunctive alternate
</em><br>
<em>&gt; reality&quot; as &quot;undesirable&quot; (regardless of the purpose that pain is supposed
</em><br>
<em>&gt; to accomplish), and combine the fact of &quot;human responsibility&quot; with the
</em><br>
<em>&gt; &quot;undesirable outcome&quot; to resent the humans.
</em><br>
<p>This kind of reasoning is really not very hard for a probababilistic
<br>
inference
<br>
engine like the one implicit in WM's inheritance links, given adequate
<br>
experiential
<br>
data.  I mean, I could write out the exact inference steps involved in the
<br>
train
<br>
of thought you describe, in terms of WM nodes and links (but I won't...).
<br>
We have
<br>
not achieved this kind of application of WM inference yet, but this is where
<br>
we expect
<br>
to be in a year's time or less, using the tools we have now...
<br>
<p><em>&gt; I don't think Webmind should use an anthropomorphic pain architecture,
</em><br>
<p>Well, it doesn't really, but it's more so than you seem to find intuitively
<br>
optimal
<br>
<p>I feel you haven't come to grips with the limitations of rationality, due to
<br>
not yet
<br>
having experimented with large-scale prob. reasoning systems, and their
<br>
strong need to
<br>
be guided by simpler intuitive/associative (even instinctive) methods...
<br>
<p><em>&gt;  Webmind 3.0, or whenever Webmind
</em><br>
<em>&gt; starts getting into sophisticated self-imagery, can analyze its own mind,
</em><br>
<em>&gt; trace back undesirable behaviors to their causal origin, and perform
</em><br>
<em>&gt; design adjustments that would have prevented that undesirable outcome and
</em><br>
<em>&gt; as many related undesirable outcomes as possible.
</em><br>
<p>WM 1.0 will have self-imagery ... the system already does, actually.
<br>
<p>The design adjustment part will only come with 2.0 however
<br>
<p><em>&gt; In this latter case, Webmind should have no objection to your tweaking
</em><br>
<em>&gt; with the &quot;negative feedback&quot; (not &quot;pain&quot;) mechanisms, if by doing so, you
</em><br>
<em>&gt; increase the probability of desirable outcomes in the future.
</em><br>
<p>Sure....  The instinct part, though, is the &quot;a priori&quot; probability that a
<br>
tweak with
<br>
its mechanisms is good (to use a Bayesian term that isn't entirely apt).
<br>
<p>Try as you might, you can't make probability theory entirely objective.  In
<br>
practice
<br>
it always rests on assumptions -- a prior probability distribution, an
<br>
assumption
<br>
of independence between various factors, etc.  These assumptions are, well,
<br>
instinct...
<br>
feeling...
<br>
<p>This is not an abstract philosophical point, it's a philosophical ~twist~ on
<br>
well-known
<br>
maths that you just can't get around...
<br>
<p><em>&gt; This is why I'm so heavy on the necessity of pain being a design subgoal
</em><br>
<em>&gt; of Friendliness, rather than Friendliness being a way of achieving
</em><br>
<em>&gt; pleasure or avoiding pain.  By adopting that design stance, you are making
</em><br>
<em>&gt; huge problems for yourselves which are entirely unnecessary.
</em><br>
<p>In my view, friendliness and happiness are both subcomponents of each other.
<br>
That's how it works in Webmind, anyway.  And in me.  the happier I am, the
<br>
more
<br>
friendly I am; and the friendlier I am, the happier I am.
<br>
<p>Each one is a subgoal of the other ;&gt;
<br>
<p><em>&gt; &gt; Because we're so close to this phase (just a couple more months
</em><br>
<em>&gt; &gt; of testing &amp; debugging simpler components), this conversation
</em><br>
<em>&gt; &gt; is particularly interesting to me
</em><br>
<em>&gt;
</em><br>
<em>&gt; I'm very much interested as well, especially insofar as the choices you
</em><br>
<em>&gt; make now may constrain the options you have available later.
</em><br>
<p>Probably not, really.  Not for a while...
<br>
<p><em>&gt; &gt; There certainly is something to be known in advance... but the
</em><br>
<em>&gt; percentage
</em><br>
<em>&gt; &gt; of relevant knowledge that can be known in advance is NOT one
</em><br>
<em>&gt; of the things
</em><br>
<em>&gt; &gt; that can be known in advance ;&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ah, yes, but once you know something in advance, you can take a pretty
</em><br>
<em>&gt; good guess as to whether that particular thing is something you need to
</em><br>
<em>&gt; know in advance.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Obviously, one of the fundamental goals in Friendly AI should be to use
</em><br>
<em>&gt; methods that minimize the number of things you need to know in advance.
</em><br>
<em>&gt; It also follows that those methods are one of the things you most need to
</em><br>
<em>&gt; know in advance.  (See?  Now we know that in advance!)
</em><br>
<em>&gt;
</em><br>
<em>&gt; Try saying all that with a straight face... but it's all true.
</em><br>
<p>Sure... this is basic to any kind of holistic, realistic AI design, not just
<br>
friendly AI...
<br>
<p>ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0252.html">Ben Goertzel: "RE: Pain vs. negative feedback (was: Evolving minds)"</a>
<li><strong>Previous message:</strong> <a href="0250.html">J. R. Molloy: "Re: Evolving minds"</a>
<li><strong>In reply to:</strong> <a href="0249.html">Eliezer S. Yudkowsky: "Pain vs. negative feedback (was: Evolving minds)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0252.html">Ben Goertzel: "RE: Pain vs. negative feedback (was: Evolving minds)"</a>
<li><strong>Reply:</strong> <a href="0252.html">Ben Goertzel: "RE: Pain vs. negative feedback (was: Evolving minds)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#251">[ date ]</a>
<a href="index.html#251">[ thread ]</a>
<a href="subject.html#251">[ subject ]</a>
<a href="author.html#251">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
