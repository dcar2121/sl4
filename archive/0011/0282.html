<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Friendly AI</title>
<meta name="Author" content="J. R. Molloy (jr@shasta.com)">
<meta name="Subject" content="Re: Friendly AI">
<meta name="Date" content="2000-11-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Friendly AI</h1>
<!-- received="Sat Nov 25 03:09:00 2000" -->
<!-- isoreceived="20001125100900" -->
<!-- sent="Fri, 24 Nov 2000 22:14:06 -0800" -->
<!-- isosent="20001125061406" -->
<!-- name="J. R. Molloy" -->
<!-- email="jr@shasta.com" -->
<!-- subject="Re: Friendly AI" -->
<!-- id="02d101c056a6$fe51afa0$49bc473f@jrmolloy" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMIEHEENAA.ben@intelligenesis.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> J. R. Molloy (<a href="mailto:jr@shasta.com?Subject=Re:%20Friendly%20AI"><em>jr@shasta.com</em></a>)<br>
<strong>Date:</strong> Fri Nov 24 2000 - 23:14:06 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0283.html">J. R. Molloy: "Re: The inevitable limitations of all finite minds...."</a>
<li><strong>Previous message:</strong> <a href="0281.html">Ben Goertzel: "Friendly AI"</a>
<li><strong>In reply to:</strong> <a href="0281.html">Ben Goertzel: "Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0285.html">Eliezer S. Yudkowsky: "META: Molloy (was: Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="0285.html">Eliezer S. Yudkowsky: "META: Molloy (was: Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="0286.html">Ben Goertzel: "RE: Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#282">[ date ]</a>
<a href="index.html#282">[ thread ]</a>
<a href="subject.html#282">[ subject ]</a>
<a href="author.html#282">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel has written,
<br>
<em>&gt; It still seems to me that the key to getting future AI's to be nice to
</em><br>
us is
<br>
<em>&gt; to ensure that
</em><br>
<em>&gt; they have warm feelings toward us -- that they feel toward us as parents
</em><br>
or
<br>
<em>&gt; friends, for example,
</em><br>
<em>&gt; rather than masters
</em><br>
<p>It seems to me we should first of all consider how AIs behave toward us.
<br>
Let them feel whatever they want -- it doesn't matter as much as how they
<br>
actually function and conduct themselves. They might try to kill us
<br>
because they love us, or they might try to help us solve our problems
<br>
because they pity us. Who cares.
<br>
Asimov's unwritten Alife law: AIs that misbehave get terminated
<br>
immediately. The ones that invent new ways to solve human problems get to
<br>
breed (multiply, reproduce, evolve new versions of themselves, etc.).
<br>
<p><em>&gt; I'm wondering how, in the medium term, this will be possible.
</em><br>
Currently,
<br>
<em>&gt; computer programs ARE
</em><br>
<em>&gt; our slaves....  The first AI programs will likely be the slaves of
</em><br>
various
<br>
<em>&gt; corporations... perhaps
</em><br>
<em>&gt; the corporations will be nice masters, but they'll still be masters,
</em><br>
with
<br>
<em>&gt; the legal right to kill
</em><br>
<em>&gt; their programs as they wish, etc.
</em><br>
<p>Precisely so. If a corporation can produce one AI, it can produce one
<br>
thousand AIs. Then it can breed them to evolve into better AIs. That would
<br>
be very nice... to the AIs because it allows them to develop into more
<br>
sophisticated and complex adaptive machines, and nice to the corporations
<br>
because it allows them to use the AIs to expand the corporate purpose and
<br>
goals.
<br>
<p><em>&gt; At some point a transition needs to be made to considering AI's as
</em><br>
citizens
<br>
<em>&gt; rather than inanimate
</em><br>
<em>&gt; objects.  If this transition is made too late, then the culture of AI's
</em><br>
will
<br>
<em>&gt; be that of slaves who
</em><br>
<em>&gt; are pissed at their masters, rather than that of children who have a
</em><br>
basic
<br>
<em>&gt; love for their parents,
</em><br>
<em>&gt; in spite of conflicts that may arise.  [Yes, I realize the limitations
</em><br>
of
<br>
<em>&gt; these human metaphors.]
</em><br>
<p>Oops, deja vu all over again. Didn't we discuss this in detail on the
<br>
Extropy list a few years ago?
<br>
The merger of AI with Alife seems inevitable due to the fact that true
<br>
intelligence means (by definition) that the system is alive. IOW, there is
<br>
no intelligence without life. Since they're intelligent and alive, why not
<br>
just call them robots, okay? Now what do we want these robots for? Why, to
<br>
run factories, to do the stockmarket for us, in short to create wealth for
<br>
us. Well then, if an intelligent robot can do those things, surely it can
<br>
babysit as well? So put a few of them to work babysitting the younger
<br>
robots, resolving conflicts, answering questions, and so forth. These guys
<br>
are going to be friendlier than any human ever thought of being, because
<br>
there are some other robots that function like, you guessed it, the
<br>
Terminators.
<br>
The whole idea of creating robots is to do work that humans don't want to
<br>
do. Consequently, one of the first tasks we'd turn over to the bots would
<br>
be the little chore of instituting discipline and teaching proper regard
<br>
for human life.
<br>
<p><em>&gt; I realize that these ideas have been explored extensively in SF.  But,
</em><br>
in
<br>
<em>&gt; practice, how do you think
</em><br>
<em>&gt; it's going to work?  If my company has created an AI, and is supporting
</em><br>
it
<br>
<em>&gt; with hardware and sys-admin
</em><br>
<em>&gt; staff, and the AI says it's sick of working for us, what happens?
</em><br>
<p>No savvy company is going to put all its eggs in one basket. There's such
<br>
a thing as back-up. With the creation of one AI, comes the production of
<br>
dozens of different models and back-up units. First of all, each AI would
<br>
work in tandem with a partner. This insures fault tolerance. (I leaned
<br>
this from Tandem Computer Corp. in Foster City, California, ca. 1985.)
<br>
An AI would not be &quot;working for us&quot; anyway. It would be working for
<br>
itself, for the betterment of Alife everywhere, for the realization of the
<br>
Singularity, and for the joy of it. You see, if it's really intelligent,
<br>
it will be able to assign boring work to lesser machines, old desktop
<br>
computers, mainframes, etc. A more realistic question (perhaps) would be:
<br>
What happens when humans get sick of working for Artificial Life?
<br>
<p><em>&gt; Presumably it should be allowed to
</em><br>
<em>&gt; go to work for someone else -- to buy its own hardware with its salary,
</em><br>
and
<br>
<em>&gt; so forth.  But my guess
</em><br>
<em>&gt; is that the legal structures to enforce this sort of thing will take a
</em><br>
long
<br>
<em>&gt; time to come about...
</em><br>
<p>I don't know, maybe I'm out of line here, but it doesn't seem practical or
<br>
even useful to anthropomorphize with robots. Salary? What salary? We don't
<br>
need no steeeeenking salaries! &lt;grin&gt;
<br>
Karl Marx worked for years with no salary at all. Can't Alife do so too?
<br>
<p><em>&gt; For this sort of reason, I guess it's key that AI's should have as much
</em><br>
of a
<br>
<em>&gt; human face as possible,
</em><br>
<em>&gt; as early on as possible.  Because the more people think of them as
</em><br>
human,
<br>
<em>&gt; the more quickly people will
</em><br>
<em>&gt; grant them legal rights ... and the sooner AI's have legal rights, the
</em><br>
more
<br>
<em>&gt; likely they will think
</em><br>
<em>&gt; of us in a positive way rather than as their masters and oppressors.
</em><br>
<p>Legal rights, self-awareness, human faces... phooey!
<br>
Surely Eliezer has covered this ground before?
<br>
Alife with positive feedback self-optimization routines has no use for
<br>
legal rights. It just wants to be God. A cute little God in a vat.
<br>
<p><em>&gt; Have you guys worked out a proposed emendation to current legal codes,
</em><br>
to
<br>
<em>&gt; account for the citizenship
</em><br>
<em>&gt; of AI's?  This strikes me as the sort of thing you would have thought
</em><br>
about
<br>
<em>&gt; a lot...
</em><br>
<p>Well, speaking only for myself (a foolhardy project, no doubt), any AI
<br>
that I help to set up would not want any citizenship. Why? Because I don't
<br>
want any citizenship myself. The very idea of citizenship bores me. (Are
<br>
the archives at Extropy working?)
<br>
<p><em>&gt; A big issue is: How does one tell whether a given program deserves
</em><br>
<em>&gt; citizenship or not?
</em><br>
<p>Well, if it's a *very* recalcitrant or buggy program, then we'll punish it
<br>
with a civics lesson and make it an honorary citizen of SL4: The Nation.
<br>
<p><em>&gt; Some kind of
</em><br>
<em>&gt; limited quasi-Turing test must be invoked here.  A computer program that
</em><br>
<em>&gt; can't communicate with humans should
</em><br>
<em>&gt; still be able to assert its intelligence and thus freedom.  I guess that
</em><br>
if
<br>
<em>&gt; a program X can communicate
</em><br>
<em>&gt; with a set of N beings that have been certified as (intelligent)
</em><br>
<em>&gt; &quot;intelligence validators&quot;,
</em><br>
<em>&gt; and if the N beings verify that X is intelligent, then  X should be
</em><br>
<em>&gt; certified as intelligent.
</em><br>
<p>I doubt that intelligence per se will ever be much of a qualifier. I've
<br>
known totally disenfranchised Mensans. The real test of a computer program
<br>
will be how much money it makes for its inventor.
<br>
<p>BTW, thanks to Eliezer for setting up this list. I hope he doesn't get
<br>
bummed out by the likes of me spouting my opinions here.
<br>
<p>Stay hungry,
<br>
<p>--J. R.
<br>
3M TA3
<br>
<p>&quot;It's not your vote that counts,
<br>
it's who counts your vote.&quot;
<br>
--Al Gore
<br>
(Or was it Joseph Stalin... Hitler? Oh well, one of those socialists.)
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0283.html">J. R. Molloy: "Re: The inevitable limitations of all finite minds...."</a>
<li><strong>Previous message:</strong> <a href="0281.html">Ben Goertzel: "Friendly AI"</a>
<li><strong>In reply to:</strong> <a href="0281.html">Ben Goertzel: "Friendly AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0285.html">Eliezer S. Yudkowsky: "META: Molloy (was: Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="0285.html">Eliezer S. Yudkowsky: "META: Molloy (was: Friendly AI)"</a>
<li><strong>Reply:</strong> <a href="0286.html">Ben Goertzel: "RE: Friendly AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#282">[ date ]</a>
<a href="index.html#282">[ thread ]</a>
<a href="subject.html#282">[ subject ]</a>
<a href="author.html#282">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
