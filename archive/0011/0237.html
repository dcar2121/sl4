<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Evolving minds</title>
<meta name="Author" content="Dale Johnstone (DaleJohnstone@email.com)">
<meta name="Subject" content="Re: Evolving minds">
<meta name="Date" content="2000-11-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Evolving minds</h1>
<!-- received="Sat Nov 18 13:34:02 2000" -->
<!-- isoreceived="20001118203402" -->
<!-- sent="Sat, 18 Nov 2000 18:33:39 -0000" -->
<!-- isosent="20001118183339" -->
<!-- name="Dale Johnstone" -->
<!-- email="DaleJohnstone@email.com" -->
<!-- subject="Re: Evolving minds" -->
<!-- id="000901c0518e$12697180$ac279fd4@xanadu" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMIEOLEMAA.ben@intelligenesis.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Dale Johnstone (<a href="mailto:DaleJohnstone@email.com?Subject=Re:%20Evolving%20minds"><em>DaleJohnstone@email.com</em></a>)<br>
<strong>Date:</strong> Sat Nov 18 2000 - 11:33:39 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0238.html">Eliezer S. Yudkowsky: "Re: Static uploading is SL3 (was: the 69 of us)"</a>
<li><strong>Previous message:</strong> <a href="0236.html">Ben Goertzel: "RE: Evolving minds"</a>
<li><strong>In reply to:</strong> <a href="0236.html">Ben Goertzel: "RE: Evolving minds"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0241.html">Ben Goertzel: "RE: Evolving minds"</a>
<li><strong>Reply:</strong> <a href="0241.html">Ben Goertzel: "RE: Evolving minds"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#237">[ date ]</a>
<a href="index.html#237">[ thread ]</a>
<a href="subject.html#237">[ subject ]</a>
<a href="author.html#237">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
----- Original Message -----
<br>
From: &quot;Ben Goertzel&quot; &lt;<a href="mailto:ben@intelligenesis.net?Subject=Re:%20Evolving%20minds">ben@intelligenesis.net</a>&gt;
<br>
To: &lt;<a href="mailto:sl4@sysopmind.com?Subject=Re:%20Evolving%20minds">sl4@sysopmind.com</a>&gt;
<br>
Sent: Saturday, November 18, 2000 3:43 PM
<br>
Subject: RE: Evolving minds
<br>
<p><p><em>&gt;
</em><br>
<em>&gt; hi,
</em><br>
<p>hiya,
<br>
<p><em>&gt;
</em><br>
<em>&gt; &gt; You'll have to forgive me for not having time to look properly into your
</em><br>
<em>&gt; &gt; work, but basically I find myself in broad agreement with what I
</em><br>
<em>&gt; &gt; read on the
</em><br>
<em>&gt; &gt; webmind philosophy page about AI &amp; wish you luck. :)
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; However, if it works, what's to stop it becoming too successful &amp;
</em><br>
turning
<br>
<em>&gt; &gt; all the matter of Earth into webmind nodes?
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; For instance if your fitness function rewards successful algorithms,
</em><br>
what
<br>
<em>&gt; &gt; happens if one of them comes up with the bright idea of tricking a human
</em><br>
<em>&gt; &gt; into giving it access to nanotech, then proceeds to build more computer
</em><br>
<em>&gt; &gt; power for itself?
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I'm not one for disasterbation but that stakes are high. We can't
</em><br>
<em>&gt; &gt; afford to
</em><br>
<em>&gt; &gt; get it wrong.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Once AI systems are smart enough to restructure all the matter of Earth
</em><br>
into
<br>
<em>&gt; their
</em><br>
<em>&gt; own mind-stuff, we won't be ABLE to guide their development via
</em><br>
intelligent
<br>
<em>&gt; tinkering, one would suspect...
</em><br>
<p>Granted, once they're replicating it will be pretty much impossible to stop
<br>
them, intelligent or not. Hence my concern.
<br>
<p><em>&gt;
</em><br>
<em>&gt; So, your intuition is that by carefully guiding our AI systems in the very
</em><br>
<em>&gt; early stages,
</em><br>
<em>&gt; we can mold the early phases of the next generation of intelligence so
</em><br>
<em>&gt; carefully that
</em><br>
<em>&gt; later phases will be unlikely to go awry
</em><br>
<em>&gt;
</em><br>
<em>&gt; I tend to doubt it.
</em><br>
<p>Not really, my intuition is to sandbox AIs tightly (especially the smart
<br>
ones) until we can be sure they aren't going to go do something stupid
<br>
(which is highly likely to begin with) in the real world.
<br>
<p>I get very nervous when I hear about AI improving itself. It's like doing
<br>
brain surgery on yourself. What if you damage an area that affects your
<br>
judgement &amp; you start making random modifications? Instabilities are most
<br>
likely to result, to say the least.
<br>
<p>You could have some nice trustworthy AI's perception suddenly change because
<br>
of some unforeseen modification, and because the AI is modifying itself,
<br>
it's new perception will further affect it's modifications.
<br>
<p>I prefer an architecture without that kind of internal feedback.
<br>
<p>I'm well aware that thinking is itself a form of self-modification, but on a
<br>
different level. We can't modify major parts of our neural architecture by
<br>
thought alone.
<br>
<p>Actually this stability problem is a large part of the AI problem itself.
<br>
<p><em>&gt;
</em><br>
<em>&gt; Based on practical experience, it seems to me that even the AI systems we
</em><br>
<em>&gt; now are experimenting
</em><br>
<em>&gt; with at Webmind Inc. -- which are pretty primitive and use only about half
</em><br>
<em>&gt; of the AI code
</em><br>
<em>&gt; we've written -- are fucking HARD to control.  We're already using
</em><br>
<em>&gt; evolutionary means to adapt
</em><br>
<em>&gt; system parameters... as a complement to, not a substitute for,
</em><br>
experimental
<br>
<em>&gt; re-engineering of various
</em><br>
<em>&gt; components, of course.
</em><br>
<p>I'm not a big fan of evolutionary methods.
<br>
<p><em>&gt;
</em><br>
<em>&gt; So the idea that we can proceed with more advanced AI systems based
</em><br>
<em>&gt; primarily on conscious human
</em><br>
<em>&gt; engineering rather than evolutionary programming, seems pretty unlikely to
</em><br>
<em>&gt; be.  It directly goes against
</em><br>
<em>&gt; the complex, self-organizing, (partially) chaotic nature of intelligent
</em><br>
<em>&gt; systems.
</em><br>
<p>I'm not advocating proceeding primarily with conscious human engineering. At
<br>
the moment I'm advocating people think more carefully about how their AI
<br>
projects could turn nasty. We all wear rose-tinted glasses with our own
<br>
projects, and the future in general, but we're playing with powerful juju
<br>
here that deserves more care.
<br>
<p>My current thoughts are to have an independant peer intelligence in the
<br>
design feedback process, be it human or not. And thorough sandboxing.
<br>
<p>If you don't, then what's to stop it self-organising into something simpler
<br>
that just copies itself once it thinks up the idea?
<br>
<p><em>&gt;
</em><br>
<em>&gt; &gt; Have you compared random mutation to intelligent design?
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; We have aircraft that fly faster, further, higher and for longer than
</em><br>
any
<br>
<em>&gt; &gt; bird. It didn't take us millions of years either.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; yeah, but an aircraft is not a mind, or a body.  Minds are intrinsically
</em><br>
<em>&gt; complex, self-organizing
</em><br>
<em>&gt; and hard to predict.
</em><br>
<p>Again you underline my concern.
<br>
<p><em>&gt;
</em><br>
<em>&gt; If minds could be built like airplanes, then rule-based AI would be a lot
</em><br>
<em>&gt; further along than it is!
</em><br>
<p>Urgh! Are you equating intelligent design with rule-based AI? - If so, I
<br>
don't subscribe to that point of view. Rule-based 'AI' doesn't even qualify
<br>
as AI in my view! :P
<br>
<p><em>&gt;
</em><br>
<em>&gt; Ben
</em><br>
<em>&gt;
</em><br>
<p>Cheers,
<br>
Dale.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0238.html">Eliezer S. Yudkowsky: "Re: Static uploading is SL3 (was: the 69 of us)"</a>
<li><strong>Previous message:</strong> <a href="0236.html">Ben Goertzel: "RE: Evolving minds"</a>
<li><strong>In reply to:</strong> <a href="0236.html">Ben Goertzel: "RE: Evolving minds"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0241.html">Ben Goertzel: "RE: Evolving minds"</a>
<li><strong>Reply:</strong> <a href="0241.html">Ben Goertzel: "RE: Evolving minds"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#237">[ date ]</a>
<a href="index.html#237">[ thread ]</a>
<a href="subject.html#237">[ subject ]</a>
<a href="author.html#237">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
