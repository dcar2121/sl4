<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Evolving minds</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Evolving minds">
<meta name="Date" content="2000-11-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Evolving minds</h1>
<!-- received="Sat Nov 18 14:38:19 2000" -->
<!-- isoreceived="20001118213819" -->
<!-- sent="Sat, 18 Nov 2000 14:34:51 -0500" -->
<!-- isosent="20001118193451" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Evolving minds" -->
<!-- id="3A16D9DB.392ADD8B@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMIEOLEMAA.ben@intelligenesis.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Evolving%20minds"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Nov 18 2000 - 12:34:51 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0241.html">Ben Goertzel: "RE: Evolving minds"</a>
<li><strong>Previous message:</strong> <a href="0239.html">Eliezer S. Yudkowsky: "META: Evolving minds"</a>
<li><strong>In reply to:</strong> <a href="0236.html">Ben Goertzel: "RE: Evolving minds"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0243.html">Ben Goertzel: "RE: Evolving minds"</a>
<li><strong>Reply:</strong> <a href="0243.html">Ben Goertzel: "RE: Evolving minds"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#240">[ date ]</a>
<a href="index.html#240">[ thread ]</a>
<a href="subject.html#240">[ subject ]</a>
<a href="author.html#240">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Once AI systems are smart enough to restructure all the matter of Earth into
</em><br>
<em>&gt; their
</em><br>
<em>&gt; own mind-stuff, we won't be ABLE to guide their development via intelligent
</em><br>
<em>&gt; tinkering, one would suspect...
</em><br>
<p>I am not sure I agree with your exact phrasing.  I would rephrase as follows:
<br>
<p>1)  Once an AI system is smart enough to restructure all the matter of the
<br>
Solar System into vis own mind-stuff, we will not be able to guide vis
<br>
development *if ve doesn't want us to*.
<br>
<p>1a)  Once AI systems have the physical capability to resist tinkering and to
<br>
continue improving themselves, you'd better have all the basics right (see
<br>
below).
<br>
<p>2)  After some point - probably, the point where the AI can predict everything
<br>
the human programmers will say - the human programmers will not be able to
<br>
tinker with the AI, although the AI may take &quot;what the programmers would
<br>
think&quot; or &quot;what an idealized programmer would think&quot; into account when
<br>
developing its moral system further.
<br>
<p>I would also add:
<br>
<p>3)  Programmer-independent morality (morality which does not contain quirks or
<br>
errors which were unique to the particular human programmers) is a desirable
<br>
property of a Sysop.
<br>
<p><em>&gt; So, your intuition is that by carefully guiding our AI systems in the very
</em><br>
<em>&gt; early stages,
</em><br>
<em>&gt; we can mold the early phases of the next generation of intelligence so
</em><br>
<em>&gt; carefully that
</em><br>
<em>&gt; later phases will be unlikely to go awry
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I tend to doubt it.
</em><br>
<p>I agree with Ben Goertzel, but only with respect to that exact phrasing.  The
<br>
core of seed AI is the idea that a system can become intelligent enough to
<br>
redesign itself, so that the programmers don't need to get everything exactly
<br>
right to design an intelligent mind.  The core of Friendly AI is the idea that
<br>
a system can be Friendly enough to try and become Friendlier.  Friendly AI
<br>
does not consist of getting the early systems precisely right; it consists of
<br>
getting those early systems *right enough* that the system grows into
<br>
Friendliness, convergently rather than divergently.
<br>
<p>In practice, what is necessary is that the early Friendly AI make reference to
<br>
the intentions of the programmer.  Not the specific intentions that have
<br>
already been embodied, but &quot;the intentions of the programmer&quot;, in general,
<br>
including the intentions that the early AI doesn't know about yet.
<br>
<p>When the programmer says:  &quot;I have this new element to include in the design
<br>
of your goal system&quot;, the AI needs to think:  &quot;Aha!  Here's an element of
<br>
what-should-be-my-design that I didn't know about before!&quot;, not &quot;He wants to
<br>
give me a new goal system, which leads to suboptimal results from the
<br>
perspective of my current goal system... I'd better resist.&quot;
<br>
<p>In &quot;Friendly AI&quot;, I'm working on describing the specific cognitive imagery
<br>
necessary for all that to take place.
<br>
<p>When the AI encounters a new moral decision which is ambiguous under its
<br>
existing goal material, or when it encounters a design decision with respect
<br>
to the goal system itself, it needs to think &quot;What would my programmers want
<br>
me to do?&quot;, come up with the correct answer (by means which may include just
<br>
*asking* the programmers), and implement it.
<br>
<p>Eventually, the AI needs to grok the complexity which underlies those
<br>
judgement calls by the programmers, and internalize that complexity - both the
<br>
process we use to make moral decisions about what constitutes Friendliness,
<br>
and also the process we use to define which kinds of moral reasoning are valid
<br>
and extend our philosophy to cover new situations.
<br>
<p><em>&gt; Based on practical experience, it seems to me that even the AI systems we
</em><br>
<em>&gt; now are experimenting
</em><br>
<em>&gt; with at Webmind Inc. -- which are pretty primitive and use only about half
</em><br>
<em>&gt; of the AI code
</em><br>
<em>&gt; we've written -- are fucking HARD to control.  We're already using
</em><br>
<em>&gt; evolutionary means to adapt
</em><br>
<em>&gt; system parameters... as a complement to, not a substitute for, experimental
</em><br>
<em>&gt; re-engineering of various
</em><br>
<em>&gt; components, of course.
</em><br>
<p>Well, without more detailed knowledge of Webmind, I can't be sure; however, I
<br>
don't *think* you're encountering challenges of the same underlying class as
<br>
the challenges that would be involved in Friendly AI.  But I don't know.  I'm
<br>
not a Webminder.
<br>
<p>Anyway, I know you don't think that it's possible to do all the work on
<br>
Friendly AI in advance.  I agree.  In the course of building a Friendly AI,
<br>
you or I (or both) will probably learn far more about Friendly AI than we
<br>
started with.  However, there is also work about Friendly AI that *can* be
<br>
done in advance.  There are classes of avoidable mistakes.  There's
<br>
terminology needed to know what you're seeing when you see it.  There are
<br>
things you need to do in the first versions to pave the way for later
<br>
versions.  That's what's going into &quot;Friendly AI&quot;.
<br>
<p>A year ago, I believed there was nothing you or I or anyone could or should
<br>
know about Friendly AI in advance.  I now recognize that this belief was quite
<br>
convenient.
<br>
<p><em>&gt; So the idea that we can proceed with more advanced AI systems based
</em><br>
<em>&gt; primarily on conscious human
</em><br>
<em>&gt; engineering rather than evolutionary programming, seems pretty unlikely to
</em><br>
<em>&gt; be.  It directly goes against
</em><br>
<em>&gt; the complex, self-organizing, (partially) chaotic nature of intelligent
</em><br>
<em>&gt; systems.
</em><br>
<p>If you will pardon a bit of mystical terminology - there is a balance between
<br>
order and chaos in living systems.
<br>
<p>Again, this is a statement that needs to be split up.  As intelligence
<br>
advances, moved by evolutionary programming or self-modification, the
<br>
underlying mechanisms of that intelligence may become steadily more
<br>
incomprehensible to the human programmers.  As the intelligence's rules of
<br>
reasoning and models of reality advance, the high-level behavior of the
<br>
intelligence may become steadily more reliable, comprehensible, easy to
<br>
summarize to a human.  As the intelligence advances beyond the human, the
<br>
decisions of that intelligence may become impossible to predict, because the
<br>
intelligence is smarter than we are.
<br>
<p>There is still a discipline of seed AI in Artificial Intelligence, and a
<br>
discipline of &quot;seed morality&quot; in Friendly AI.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0241.html">Ben Goertzel: "RE: Evolving minds"</a>
<li><strong>Previous message:</strong> <a href="0239.html">Eliezer S. Yudkowsky: "META: Evolving minds"</a>
<li><strong>In reply to:</strong> <a href="0236.html">Ben Goertzel: "RE: Evolving minds"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0243.html">Ben Goertzel: "RE: Evolving minds"</a>
<li><strong>Reply:</strong> <a href="0243.html">Ben Goertzel: "RE: Evolving minds"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#240">[ date ]</a>
<a href="index.html#240">[ thread ]</a>
<a href="subject.html#240">[ subject ]</a>
<a href="author.html#240">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
