<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Ben's &quot;Extropian Creed&quot;</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Ben's &quot;Extropian Creed&quot;">
<meta name="Date" content="2000-11-13">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Ben's &quot;Extropian Creed&quot;</h1>
<!-- received="Tue Nov 14 00:25:02 2000" -->
<!-- isoreceived="20001114072502" -->
<!-- sent="Tue, 14 Nov 2000 00:24:31 -0500" -->
<!-- isosent="20001114052431" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Ben's &quot;Extropian Creed&quot;" -->
<!-- id="3A10CC8F.1CF8C829@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMKEKMEMAA.ben@intelligenesis.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Ben's%20&quot;Extropian%20Creed&quot;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Mon Nov 13 2000 - 22:24:31 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0174.html">Patrick McCuller: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<li><strong>Previous message:</strong> <a href="0172.html">Ben Goertzel: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<li><strong>In reply to:</strong> <a href="0166.html">Ben Goertzel: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0177.html">Ben Goertzel: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<li><strong>Reply:</strong> <a href="0177.html">Ben Goertzel: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#173">[ date ]</a>
<a href="index.html#173">[ thread ]</a>
<a href="subject.html#173">[ subject ]</a>
<a href="author.html#173">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; The analogy
</em><br>
<em>&gt; 
</em><br>
<em>&gt;         AI: humans
</em><br>
<em>&gt;         humans: animals
</em><br>
<em>&gt; 
</em><br>
<em>&gt; is flawed because humans and animals compete for resources far more than AI
</em><br>
<em>&gt; and humans will have to,
</em><br>
<em>&gt; due to our different natures
</em><br>
<p>Why do you assume that humans and AIs will need to compete at all?  I think
<br>
the happiest scenario is the one where everyone runs on protected memory/mass
<br>
partitions, with the &quot;laws of physics&quot; enforced by a Sysop that takes up a
<br>
minor amount of overhead and isn't interested in competing for space beyond
<br>
that.
<br>
<p>Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; It seems to me that in any resource-limited environment populated by
</em><br>
<em>&gt; autonomous agents
</em><br>
<em>&gt;  -- before or after the
</em><br>
<em>&gt; Singularity -- it will be true that greater power/intelligence/ability leads
</em><br>
<em>&gt; to
</em><br>
<em>&gt; social dominance....
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You disagree with this?
</em><br>
<p>Yes, I do.  In the Sysop Scenario, a &quot;static&quot; uploaded human can &quot;walk&quot; up to
<br>
the most terrifyingly intelligent entity the world has ever produced and blow
<br>
a great big raspberry without incurring the most infinitesimal risk of dying
<br>
or even being hurt.
<br>
<p>At most, there might be some kind of social scenario among the static uploaded
<br>
humans, so that people manage to spend all their time obsessing about how to
<br>
score points, and take revenge, and dreading the possibility of
<br>
embarassment... as long as we're mere humans, we'll never run out of ways to
<br>
make ourselves unhappy.  But I expect that the transhumans, including my
<br>
future self, will have better things to do.
<br>
<p><em>&gt; Of course, humans have particular biological quirks as regards social
</em><br>
<em>&gt; dominance, but the
</em><br>
<em>&gt; desire on the part of each agent for more resources is pretty much an
</em><br>
<em>&gt; inevitable consequence
</em><br>
<em>&gt; of evolution... which leads at least to a kind of abstract social dominance
</em><br>
<em>&gt; hierarchy...
</em><br>
<p>It's quite possible that the Citizens will be capable of trading computational
<br>
resources back and forth, but hopefully some &quot;minimal living space&quot;
<br>
requirement will be enforced... possibly as a restriction upon the trading
<br>
activities of the Citizens who exist, but certainly before a Child Citizen is
<br>
created.  (The ethical rationale for placing this restriction on trading
<br>
activities would be that it represents an irrevocable harm to any future
<br>
versions of the Citizen's self, which may have changed their minds, or may
<br>
even be changed so much as to be different entities.)
<br>
<p>Note that I would expect a six-billionth of the Solar System's mass
<br>
(approximately 10^23 grams), to be a very substantial multiple (many orders of
<br>
magnitude) of &quot;minimal living space&quot;.
<br>
<p>Disclaimer:  The Sysop Scenario is a comprehensible Singularity and
<br>
automatically suspect.  The above mass estimates do not take into account
<br>
possible dodges such as the manufacture of negative and positive matter in
<br>
equal quantities, the Linde Scenario, the creation of private Universes,
<br>
faster-than-light travel to other star systems, reality engineering,
<br>
ontotechnology, or outright magic.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0174.html">Patrick McCuller: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<li><strong>Previous message:</strong> <a href="0172.html">Ben Goertzel: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<li><strong>In reply to:</strong> <a href="0166.html">Ben Goertzel: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0177.html">Ben Goertzel: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<li><strong>Reply:</strong> <a href="0177.html">Ben Goertzel: "RE: Ben's &quot;Extropian Creed&quot;"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#173">[ date ]</a>
<a href="index.html#173">[ thread ]</a>
<a href="subject.html#173">[ subject ]</a>
<a href="author.html#173">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
