<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=WINDOWS-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: How to make a slave (was: Building a friendly AI)</title>
<meta name="Author" content="Thomas McCabe (pphysics141@gmail.com)">
<meta name="Subject" content="Re: How to make a slave (was: Building a friendly AI)">
<meta name="Date" content="2007-11-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: How to make a slave (was: Building a friendly AI)</h1>
<!-- received="Wed Nov 28 13:54:53 2007" -->
<!-- isoreceived="20071128205453" -->
<!-- sent="Wed, 28 Nov 2007 15:52:16 -0500" -->
<!-- isosent="20071128205216" -->
<!-- name="Thomas McCabe" -->
<!-- email="pphysics141@gmail.com" -->
<!-- subject="Re: How to make a slave (was: Building a friendly AI)" -->
<!-- id="b7a9e8680711281252o249ffba9yf3334099c5ae39b6@mail.gmail.com" -->
<!-- charset="WINDOWS-1252" -->
<!-- inreplyto="1196228868.14083.1223621585@webmail.messagingengine.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Thomas McCabe (<a href="mailto:pphysics141@gmail.com?Subject=Re:%20How%20to%20make%20a%20slave%20(was:%20Building%20a%20friendly%20AI)"><em>pphysics141@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Nov 28 2007 - 13:52:16 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17307.html">Thomas McCabe: "Fwd: Why friendly AI (FAI) won't work"</a>
<li><strong>Previous message:</strong> <a href="17305.html">Robin Lee Powell: "Re: How to make a slave (was: Building a friendly AI)"</a>
<li><strong>In reply to:</strong> <a href="17299.html">John K Clark: "Re: How to make a slave (was: Building a friendly AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17319.html">John K Clark: "Re: How to make a slave (was: Building a friendly AI)"</a>
<li><strong>Reply:</strong> <a href="17319.html">John K Clark: "Re: How to make a slave (was: Building a friendly AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17306">[ date ]</a>
<a href="index.html#17306">[ thread ]</a>
<a href="subject.html#17306">[ subject ]</a>
<a href="author.html#17306">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Nov 28, 2007 12:47 AM, John K Clark &lt;<a href="mailto:johnkclark@fastmail.fm?Subject=Re:%20How%20to%20make%20a%20slave%20(was:%20Building%20a%20friendly%20AI)">johnkclark@fastmail.fm</a>&gt; wrote:
<br>
<em>&gt; P K&quot; &lt;<a href="mailto:kpete1@hotmail.com?Subject=Re:%20How%20to%20make%20a%20slave%20(was:%20Building%20a%20friendly%20AI)">kpete1@hotmail.com</a>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Why would &quot;ignore what the humans say&quot;
</em><br>
<em>&gt; &gt; be the right solution?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Because Mr. AI knows he is smarter than the humans.
</em><br>
<p>This does not follow. Non sequitur. Suppose that I have two chimps in
<br>
a zoo, and both request food at the same time. Obviously, I cannot
<br>
feed both simultaneously. Now because we're smarter than the chimps,
<br>
the solution is to ignore them both and let them starve; obviously,
<br>
that's what any responsible zookeeper would do, right?
<br>
<p><em>&gt; &gt; What about telling the humans that there
</em><br>
<em>&gt; &gt; is a contradiction in commands and asking
</em><br>
<em>&gt; &gt; for clarification.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Because trying to explain to a human exactly why the commands are
</em><br>
<em>&gt; contradictory would be like trying to explain to a dog how quantum
</em><br>
<em>&gt; mechanics works. The logical solution is to have the most intelligent
</em><br>
<em>&gt; person call the shots, and that would be Mr. AI.
</em><br>
<p>Er, yes, that's why we're building the AI in the first place.
<br>
<p><em>&gt; &gt; there is no reason for the AI to become annoyed
</em><br>
<em>&gt; &gt; with humans, unless underlying circuitry for the
</em><br>
<em>&gt; &gt; emotion of getting annoyed was programmed into the AI
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yea right, and there is no way my radio can play Beethoven unless
</em><br>
<em>&gt; somebody put a Beethoven circuit into it, and my digital camera can't
</em><br>
<em>&gt; take a beautiful picture unless somebody wrote a beauty subroutine for
</em><br>
<em>&gt; it.
</em><br>
<p>Getting a radio to replicate a sound pattern, or a camera to replicate
<br>
a light pattern, is much, much, much easier than getting an AGI to
<br>
replicate our mental processes. Our mental processes are, like, really
<br>
complicated.
<br>
<p><em>&gt; &gt; &quot;Thomas McCabe&quot; <a href="mailto:pphysics141@gmail.com?Subject=Re:%20How%20to%20make%20a%20slave%20(was:%20Building%20a%20friendly%20AI)">pphysics141@gmail.com</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; To get a vague sense of how different &quot;another mind&quot;
</em><br>
<em>&gt; &gt; can be, try talking to someone who has *never*
</em><br>
<em>&gt; &gt; experienced Western culture. Then realize that they're
</em><br>
<em>&gt; &gt; still 99.9% identical to you genetically.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You are making my case for me, you go on and on about how strange and
</em><br>
<em>&gt; alien the AI will be, and then say you understand it well enough to be
</em><br>
<em>&gt; certain it will be your slave for the next billion years.
</em><br>
<p>Yes. We humans can understand something on more than one level. If I
<br>
wanted to, I could analyze the ELIZA chatbot until I understood every
<br>
line of code. Yet nobody would anthropomorphicize a chatbot.
<br>
<p><em>&gt; &gt; You see, we have these things called &quot;reason&quot;
</em><br>
<em>&gt; &gt; and &quot;logic&quot;, which we can also use to understand minds.
</em><br>
<em>&gt;
</em><br>
<em>&gt; And it is not illogical to ask yourself what would I do if I were in
</em><br>
<em>&gt; that other mind's place, and then repeat the exercise but this time
</em><br>
<em>&gt; trying to think more like  he thinks not as you think.
</em><br>
<p>Yes, because you will fail miserably. This is not your fault; your
<br>
brain is not built to understand AGI. It would be like trying to run
<br>
five miles in ten seconds; it is impossible by any reasonable
<br>
definition of the word.
<br>
<p><em>&gt; Obviously it will
</em><br>
<em>&gt; not always work but it's worth a try. I would even go so far to say that
</em><br>
<em>&gt; the value of anthropomorphic reasoning was one of the main reasons
</em><br>
<em>&gt; evolution drove us to develop a bigger brain, because the single most
</em><br>
<em>&gt; important aspect in our environment is our fellow intelligent beings and
</em><br>
<em>&gt; survival demands we understand them as best we can.
</em><br>
<p>Anthropomorphic reasoning works well in the ancestral environment. We
<br>
do not live in the ancestral environment. We are moving *further away
<br>
from* the ancestral environment each and every day.
<br>
<p><em>&gt; &gt; We've long since established that you're a believer
</em><br>
<em>&gt; &gt; in anthropomorphicism,
</em><br>
<em>&gt;
</em><br>
<em>&gt; Then why do you keep telling me that as if it's a major news flash?
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; If you actually did Google it, you would have
</em><br>
<em>&gt; &gt; found that there was a full-scale, physical
</em><br>
<em>&gt; &gt; conference on AGI scheduled for March 2008
</em><br>
<em>&gt;
</em><br>
<em>&gt; I did Google it and I don't know how many pages you had to go down to
</em><br>
<em>&gt; find the above (it's not on the first 4 pages!) but no matter, I'm just
</em><br>
<em>&gt; not very interested in Analytical Graphics Inc, or The American
</em><br>
<em>&gt; Geological Institute, or Adjusted gross income, or Adventure Game
</em><br>
<em>&gt; Interpreter, Aquent Graphics Institute, or the Association for
</em><br>
<em>&gt; Geographical Information or….
</em><br>
<p>Why do you think there's an Acronym Dictionary
<br>
(<a href="http://www.acronymfinder.com">http://www.acronymfinder.com</a>)? AGI has more than two hundred
<br>
different definitions; twenty-six are listed, out of which &quot;Artificial
<br>
General Intelligence&quot; is ranked as #4 in relevance.
<br>
<p><em>&gt;  John K Clark
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; --
</em><br>
<em>&gt;   John K Clark
</em><br>
<em>&gt;   <a href="mailto:johnkclark@fastmail.fm?Subject=Re:%20How%20to%20make%20a%20slave%20(was:%20Building%20a%20friendly%20AI)">johnkclark@fastmail.fm</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; --
</em><br>
<em>&gt;
</em><br>
<em>&gt; <a href="http://www.fastmail.fm">http://www.fastmail.fm</a> - I mean, what is it about a decent email service?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<p>&nbsp;- Tom
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17307.html">Thomas McCabe: "Fwd: Why friendly AI (FAI) won't work"</a>
<li><strong>Previous message:</strong> <a href="17305.html">Robin Lee Powell: "Re: How to make a slave (was: Building a friendly AI)"</a>
<li><strong>In reply to:</strong> <a href="17299.html">John K Clark: "Re: How to make a slave (was: Building a friendly AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17319.html">John K Clark: "Re: How to make a slave (was: Building a friendly AI)"</a>
<li><strong>Reply:</strong> <a href="17319.html">John K Clark: "Re: How to make a slave (was: Building a friendly AI)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17306">[ date ]</a>
<a href="index.html#17306">[ thread ]</a>
<a href="subject.html#17306">[ subject ]</a>
<a href="author.html#17306">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:01 MDT
</em></small></p>
</body>
</html>
