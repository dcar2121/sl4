<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: General summary of FAI theory</title>
<meta name="Author" content="justin corwin (outlawpoet@gmail.com)">
<meta name="Subject" content="Re: General summary of FAI theory">
<meta name="Date" content="2007-11-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: General summary of FAI theory</h1>
<!-- received="Wed Nov 21 12:56:16 2007" -->
<!-- isoreceived="20071121195616" -->
<!-- sent="Wed, 21 Nov 2007 11:53:56 -0800" -->
<!-- isosent="20071121195356" -->
<!-- name="justin corwin" -->
<!-- email="outlawpoet@gmail.com" -->
<!-- subject="Re: General summary of FAI theory" -->
<!-- id="3ad827f30711211153q1b5111d3m8a0ce056e829712d@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="994720.3735.qm@web56504.mail.re3.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> justin corwin (<a href="mailto:outlawpoet@gmail.com?Subject=Re:%20General%20summary%20of%20FAI%20theory"><em>outlawpoet@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Nov 21 2007 - 12:53:56 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17196.html">Wei Dai: "Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI"</a>
<li><strong>Previous message:</strong> <a href="17194.html">Anne Corwin: "Re: General summary of FAI theory"</a>
<li><strong>In reply to:</strong> <a href="17194.html">Anne Corwin: "Re: General summary of FAI theory"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17187.html">Byrne Hobart: "Re: General summary of FAI theory"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17195">[ date ]</a>
<a href="index.html#17195">[ thread ]</a>
<a href="subject.html#17195">[ subject ]</a>
<a href="author.html#17195">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Comments below:
<br>
<p>On Nov 21, 2007 9:42 AM, Anne Corwin &lt;<a href="mailto:sparkle_robot@yahoo.com?Subject=Re:%20General%20summary%20of%20FAI%20theory">sparkle_robot@yahoo.com</a>&gt; wrote:
<br>
<em>&gt; One thing to look at in this regard is the psychology of power.  Dictators
</em><br>
<em>&gt; generally want as much power as possible, and in that respect they actually
</em><br>
<em>&gt; have an incentive not to use powerful destructive technologies -- if there's
</em><br>
<em>&gt; no &quot;world&quot;, there's no-one to have power over!
</em><br>
<p>One problem with this kind of safety factor is that it's limited by
<br>
attention and motivation. One obvious example is that in national
<br>
governance, officials are presented with options on such a scale that
<br>
decisions are made where deaths and disadvantages for many are, in
<br>
context, considered less important than other options, resulting in
<br>
extremely terrible situations created for people, by folk who would
<br>
largely consider themselves decent, and probably would never
<br>
intentionally inflict the concrete situation they caused in the
<br>
abstract.
<br>
<p>RJ Smeed proposed that there is a certain amount of risk people are
<br>
willing to psychologically tolerate, and that this level of acceptable
<br>
risk is invariant as to scale and context. Which implies, among other
<br>
things, that car accidents will remain at a stable level regardless of
<br>
regulation (so long as the perception of risk does not change) and the
<br>
people will tolerate the same percentage of risk with national
<br>
economies that they do with household savings (which is quite
<br>
terrifying when corrected for scale). There is some evidence for this.
<br>
<p><em>&gt; I don't comment much on this list but I've been reading for a while, and I
</em><br>
<em>&gt; get the distinct impression that in this entire AGI discussion, &quot;power&quot; is
</em><br>
<em>&gt; the central issue moreso than anything that might be termed &quot;intelligence&quot;.
</em><br>
<em>&gt; Basically, when you guys talk about &quot;Friendly AGI&quot; and &quot;Unfriendly AGI&quot;, you
</em><br>
<em>&gt; seem to be referring to entities that will act as *influences* on reality to
</em><br>
<em>&gt; an as-yet-unprecedented extent.
</em><br>
<p>As a precedent, human effects upon the environment are fairly widespread.
<br>
<p><em>&gt; You (generic &quot;you&quot;, referring to FAGI proponents) want to build what amounts
</em><br>
<em>&gt; to a &quot;magnifier&quot; for the Forces of Good (however those might be defined),
</em><br>
<em>&gt; and prevent UFAGI from magnifying the Forces of Evil (or the Forces of
</em><br>
<em>&gt; Stupid, if you prefer).  The commonly-invoked &quot;let's build a Good AI before
</em><br>
<em>&gt; someone builds a Bad AI!&quot; scenario has always struck me as another way of
</em><br>
<em>&gt; saying, &quot;let's make sure the power is concentrated in the hands of the Good
</em><br>
<em>&gt; Guys, so that the Bad Guys don't cause harm&quot;.
</em><br>
<p>You can also formulate it as safety features of a class of new
<br>
engineering designs, to be more power politics neutral. To be honest,
<br>
I've always seen outcompeting bad alternatives as a much better option
<br>
than constraining designs, though. Would you see syringe exchange
<br>
programs, or youth basketball programs, or doctor accredidation (all
<br>
of which are attempts to construct and deploy positive attempts
<br>
before, in place of, or in a superior position to bad alternates) as
<br>
power concentration?
<br>
<p><em>&gt; No parent can assure that their future offspring will not someday destroy
</em><br>
<em>&gt; the world, and it would seem rather ridiculous (in the practical, if not the
</em><br>
<em>&gt; e-risk sense) to try and ban people from having kids until it can be assured
</em><br>
<em>&gt; that no child will ever grow up to destroy the world.  Right now, we deal
</em><br>
<em>&gt; with that kind of quandary through setting up barriers to extreme power
</em><br>
<em>&gt; accumulation in any one individual, through making and enforcing laws, and
</em><br>
<em>&gt; through social pressures (e.g., shunning and shaming of persons who commit
</em><br>
<em>&gt; acts like child abuse).
</em><br>
<p>I'm fairly certain that most parents of such a hypothetical person
<br>
would support their neutralization, if it were possible. And we do, in
<br>
fact, try to prevent certain persons from having children indirectly,
<br>
by incarcerating them, or removing their children to safer situations.
<br>
<p><em>&gt; This system doesn't work perfectly, since abuses of both persons and power
</em><br>
<em>&gt; still exist (and exist in horrible manifestations at times), but it is
</em><br>
<em>&gt; better than nothing.  With that in mind, perhaps the aim should be not to
</em><br>
<em>&gt; create &quot;an AGI&quot;, but to create a colony or group of AGIs (with
</em><br>
<em>&gt; differently-weighted priorities) to serve as a &quot;checks and balances&quot; system.
</em><br>
<em>&gt; The key is to avoid creating a situation that permits extreme concentration
</em><br>
<em>&gt; of power. &quot;Intelligence&quot; is an afterthought in that regard.
</em><br>
<p>A checks and balances system of the type you're envisioning, where
<br>
differing actors of opposing or orthogonal intent balance their
<br>
influence, is neccesary when certain intents and influences cannot be
<br>
removed or nullified, and that the stable situation of averaged
<br>
actions of those actors is preferable to letting those intents run
<br>
free. There are many situations, even now, where that's not the case.
<br>
You don't create a stable situation of rowdy children by providing the
<br>
children with equivalent and opposite positions and letting them
<br>
self-balance, you'd end up with a messy, violent, pecking order of
<br>
kids, because that's the stable result of their opposing and
<br>
orthogonal intents and actions they take, regardless of how you try to
<br>
organize it.
<br>
<p>&nbsp;Similarly, if you assume that AI goal systems, or more concretely, AI
<br>
actions, must have certain characteristics, and that you can't change
<br>
them, and that you must set them in opposition to each other to
<br>
control the outcomes in a stable fashion, I think you need to
<br>
determine exactly what you think would happen, and why that's better
<br>
than trying to remove or change those characteristics, or provide an
<br>
adult supervisor, so to speak.
<br>
<p><pre>
-- 
Justin Corwin
<a href="mailto:outlawpoet@hell.com?Subject=Re:%20General%20summary%20of%20FAI%20theory">outlawpoet@hell.com</a>
<a href="http://outlawpoet.blogspot.com">http://outlawpoet.blogspot.com</a>
<a href="http://www.adaptiveai.com">http://www.adaptiveai.com</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17196.html">Wei Dai: "Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI"</a>
<li><strong>Previous message:</strong> <a href="17194.html">Anne Corwin: "Re: General summary of FAI theory"</a>
<li><strong>In reply to:</strong> <a href="17194.html">Anne Corwin: "Re: General summary of FAI theory"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17187.html">Byrne Hobart: "Re: General summary of FAI theory"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17195">[ date ]</a>
<a href="index.html#17195">[ thread ]</a>
<a href="subject.html#17195">[ subject ]</a>
<a href="author.html#17195">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:00 MDT
</em></small></p>
</body>
</html>
