<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Why friendly AI (FAI) won't work</title>
<meta name="Author" content="Thomas McCabe (pphysics141@gmail.com)">
<meta name="Subject" content="Re: Why friendly AI (FAI) won't work">
<meta name="Date" content="2007-11-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Why friendly AI (FAI) won't work</h1>
<!-- received="Wed Nov 28 19:13:44 2007" -->
<!-- isoreceived="20071129021344" -->
<!-- sent="Wed, 28 Nov 2007 21:11:44 -0500" -->
<!-- isosent="20071129021144" -->
<!-- name="Thomas McCabe" -->
<!-- email="pphysics141@gmail.com" -->
<!-- subject="Re: Why friendly AI (FAI) won't work" -->
<!-- id="b7a9e8680711281811j38f4f909kd32327de415eb993@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="474E1941.5090908@acm.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Thomas McCabe (<a href="mailto:pphysics141@gmail.com?Subject=Re:%20Why%20friendly%20AI%20(FAI)%20won't%20work"><em>pphysics141@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Nov 28 2007 - 19:11:44 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17313.html">Thomas McCabe: "Re: Fwd: Why friendly AI (FAI) won't work"</a>
<li><strong>Previous message:</strong> <a href="17311.html">Harry Chesley: "Re: Fwd: Why friendly AI (FAI) won't work"</a>
<li><strong>In reply to:</strong> <a href="17310.html">Harry Chesley: "Re: Why friendly AI (FAI) won't work"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17315.html">Harry Chesley: "Re: Why friendly AI (FAI) won't work"</a>
<li><strong>Reply:</strong> <a href="17315.html">Harry Chesley: "Re: Why friendly AI (FAI) won't work"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17312">[ date ]</a>
<a href="index.html#17312">[ thread ]</a>
<a href="subject.html#17312">[ subject ]</a>
<a href="author.html#17312">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Nov 28, 2007 8:43 PM, Harry Chesley &lt;<a href="mailto:chesley@acm.org?Subject=Re:%20Why%20friendly%20AI%20(FAI)%20won't%20work">chesley@acm.org</a>&gt; wrote:
<br>
<em>&gt; Robin Lee Powell wrote:
</em><br>
<em>&gt; &gt; On Wed, Nov 28, 2007 at 08:49:39AM -0800, Harry Chesley wrote:
</em><br>
<em>&gt; &gt;&gt; First, to be useful, FAI needs to be bullet-proof, with no way for
</em><br>
<em>&gt; &gt;&gt; the AI to circumvent it.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; If you're talking about circumvention, you've already missed the
</em><br>
<em>&gt; &gt; point. An FA no more tries to circumvent its friendliness then you
</em><br>
<em>&gt; &gt; have a deep-seated desire to want to slaughter babxes.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&gt; This equates to writing a bug-free program, which we all know is
</em><br>
<em>&gt; &gt;&gt; next to impossible.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I don't know who &quot;we all&quot; is there, but they are wrong.
</em><br>
<em>&gt; &gt; <a href="http://en.wikipedia.org/wiki/Six_Sigma">http://en.wikipedia.org/wiki/Six_Sigma</a>
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; It's hard, and requires concerted effort, but when was the last time
</em><br>
<em>&gt; &gt; you hard of a bug in an air traffic control program? It happens, but
</em><br>
<em>&gt; &gt; it's an extremely rare thing isolated to *particular* ATC programs;
</em><br>
<em>&gt; &gt; most of them are basically bug free. Same with the space shuttle.
</em><br>
<em>&gt; &gt; Same with most hospital equipment.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Those techniques work when you have a very well-defined specification of
</em><br>
<em>&gt; the application you're developing, and when you're willing to put lots
</em><br>
<em>&gt; of resources into making the implementation correct. Do you really
</em><br>
<em>&gt; believe that the AIs created in some random research lab or some garage
</em><br>
<em>&gt; will meet either of those criteria?
</em><br>
<p>They had better darn meet both of those criteria, or we're hosed. What
<br>
do you think SIAI is for? To develop an AGI which *is* well-defined
<br>
and well-built, before some random research lab or garage kills us
<br>
all.
<br>
<p><em>&gt; &gt;&gt; Second, I believe there are other ways to achieve the same goal,
</em><br>
<em>&gt; &gt;&gt; rendering FAI an unnecessary and onerous burden. These include
</em><br>
<em>&gt; &gt;&gt; separating input from output, and separating intellect from
</em><br>
<em>&gt; &gt;&gt; motivation. In the former, you just don't supply any output
</em><br>
<em>&gt; &gt;&gt; channels except ones that can be monitored and edited.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; OMFG has that topic been done to death. Read the archives on AI
</em><br>
<em>&gt; &gt; boxing.
</em><br>
<em>&gt;
</em><br>
<em>&gt; And nothing that I've read about it has yet convinced me. What I've seen
</em><br>
<em>&gt; seems to come down to one of two arguments: 1) Intelligence is like a
</em><br>
<em>&gt; corrosive substance that will leak out, overflow, or corrode any
</em><br>
<em>&gt; container. This seems too simplistic an argument to me. Intelligence is
</em><br>
<em>&gt; far to complex to be analyzed as a commodity.
</em><br>
<p>See <a href="http://www.intelligence.org/blog/2007/07/10/the-power-of-intelligence/">http://www.intelligence.org/blog/2007/07/10/the-power-of-intelligence/</a>.
<br>
<p><em>&gt; Or 2) intelligence is
</em><br>
<em>&gt; anthropomorphic in that, like us, it will never stand for being boxed up
</em><br>
<em>&gt; and, being so very smart, will figure out a way out of the box. That
</em><br>
<em>&gt; strikes me as too anthropomorphic. (Anthropomorphism has its place, but
</em><br>
<em>&gt; every part of it is not a required part of every AI.)
</em><br>
<p>You are correct that &quot;standing&quot; for being &quot;boxed up&quot; is too
<br>
anthropomorphic, but more resources have higher expected utility to
<br>
the vast majority of utility functions.
<br>
<p><em>&gt; Nor do I buy the
</em><br>
<em>&gt; argument that a super-AI can talk its way out.
</em><br>
<p>It's already been done, twice, with an ordinary human in place of the AI.
<br>
<p><em>&gt; (I'll leave out 3) that
</em><br>
<em>&gt; it will take over the world to get more computing power, since, although
</em><br>
<em>&gt; an entertaining thought, I don't see it as a serious scenario, more like
</em><br>
<em>&gt; the plot to a science fiction novel -- oh, wait, it's already been done,
</em><br>
<em>&gt; The God Machine by Martin Caidin.)
</em><br>
<p>See <a href="http://www.overcomingbias.com/2007/10/fictional-evide.html">http://www.overcomingbias.com/2007/10/fictional-evide.html</a>,
<br>
<a href="http://www.imminst.org/forum/index.php?s=&amp;act=ST&amp;f=67&amp;t=1097&amp;st=0">http://www.imminst.org/forum/index.php?s=&amp;act=ST&amp;f=67&amp;t=1097&amp;st=0</a><br>
<a href="http://www.overcomingbias.com/2007/09/stranger-than-h.html">http://www.overcomingbias.com/2007/09/stranger-than-h.html</a>.
<br>
<p><em>&gt; &gt; Why should we go to the effort of doing your research for you? How
</em><br>
<em>&gt; &gt; arrogant is *that*?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Please don't do any more than you feel like. But please do understand
</em><br>
<em>&gt; that my questions to this list are not just an attempt to stir up the
</em><br>
<em>&gt; ant hill (tempting though that may be). I am actively working on AI, and
</em><br>
<em>&gt; though I'm unlikely create a singularity, I do feel I should worry about
</em><br>
<em>&gt; these issues. At present, my AI architecture has no facilities for FAI
</em><br>
<em>&gt; as discussed here because I think it's a waste of time.
</em><br>
<p>For the love of &lt;whatever deity you do or do not believe in&gt;, stop
<br>
working until you get a clear idea of what you've gotten yourself
<br>
into. See <a href="http://www.sl4.org/wiki/SoYouWantToBeASeedAIProgrammer">http://www.sl4.org/wiki/SoYouWantToBeASeedAIProgrammer</a> and
<br>
<a href="http://www.intelligence.org/aboutus/opportunities/research-fellow">http://www.intelligence.org/aboutus/opportunities/research-fellow</a> on
<br>
what's necessary to actually program an AI.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17313.html">Thomas McCabe: "Re: Fwd: Why friendly AI (FAI) won't work"</a>
<li><strong>Previous message:</strong> <a href="17311.html">Harry Chesley: "Re: Fwd: Why friendly AI (FAI) won't work"</a>
<li><strong>In reply to:</strong> <a href="17310.html">Harry Chesley: "Re: Why friendly AI (FAI) won't work"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17315.html">Harry Chesley: "Re: Why friendly AI (FAI) won't work"</a>
<li><strong>Reply:</strong> <a href="17315.html">Harry Chesley: "Re: Why friendly AI (FAI) won't work"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17312">[ date ]</a>
<a href="index.html#17312">[ thread ]</a>
<a href="subject.html#17312">[ subject ]</a>
<a href="author.html#17312">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:01 MDT
</em></small></p>
</body>
</html>
