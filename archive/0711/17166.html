<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: General summary of FAI theory</title>
<meta name="Author" content="Thomas McCabe (pphysics141@gmail.com)">
<meta name="Subject" content="General summary of FAI theory">
<meta name="Date" content="2007-11-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>General summary of FAI theory</h1>
<!-- received="Tue Nov 20 14:23:26 2007" -->
<!-- isoreceived="20071120212326" -->
<!-- sent="Tue, 20 Nov 2007 17:20:56 -0400" -->
<!-- isosent="20071120212056" -->
<!-- name="Thomas McCabe" -->
<!-- email="pphysics141@gmail.com" -->
<!-- subject="General summary of FAI theory" -->
<!-- id="b7a9e8680711201320l2966e341t61b78c217d666321@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Thomas McCabe (<a href="mailto:pphysics141@gmail.com?Subject=Re:%20General%20summary%20of%20FAI%20theory"><em>pphysics141@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Nov 20 2007 - 14:20:56 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17167.html">Peter de Blanc: "Re: How to make a slave (was: Building a friendly AI)"</a>
<li><strong>Previous message:</strong> <a href="17165.html">Jef Allbright: "Re: How to make a slave (was: Building a friendly AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17176.html">Stefan Pernar: "Re: General summary of FAI theory"</a>
<li><strong>Reply:</strong> <a href="17176.html">Stefan Pernar: "Re: General summary of FAI theory"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17166">[ date ]</a>
<a href="index.html#17166">[ thread ]</a>
<a href="subject.html#17166">[ subject ]</a>
<a href="author.html#17166">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
SL4 is supposed to be for advanced topics in futurism, not endlessly
<br>
rehashing the basics. Some of the things which have already been
<br>
covered years ago, and are therefore ineligible for rehashing:
<br>
<p>1). An AGI will not act like an &quot;enslaved&quot; human, a resentful human,
<br>
an emotionally repressed human, or any other kind of human. See
<br>
<a href="http://www.intelligence.org/upload/CFAI//anthro.html">http://www.intelligence.org/upload/CFAI//anthro.html</a>.
<br>
<p>2). Friendliness content is the morality stuff that says &quot;X is good, Y
<br>
is bad&quot;. Friendliness content is what we think the FAI *should* do.
<br>
FAI theory describes *how* to get an FAI to do what you want it to do.
<br>
See <a href="http://www.intelligence.org/upload/CEV.html">http://www.intelligence.org/upload/CEV.html</a>.
<br>
<p>3). FAI theory is damn hard; it is much harder than Friendliness
<br>
content. So far as I know, nobody knows how to make sure that some AGI
<br>
design reliably produces paperclips, which is much *simpler* than
<br>
ensuring reliable Friendliness. Keep in mind that the Friendliness
<br>
content must be maintained during recursive self-improvement, or the
<br>
FAI may wind up destroying us all on programming iteration
<br>
#1,576,169,123.
<br>
<p>4). CEV is a way of deriving Friendliness content from humanity's
<br>
collective cognitive architecture. CEV is a morality-constructor, not
<br>
a morality in and of itself; if you speak programming, think of CEV as
<br>
a function that takes the human race as an argument and returns a
<br>
morality.
<br>
<p>5). Goal systems naturally maintain themselves (under most
<br>
conditions). If the AGI has a supergoal of X, changing to a supergoal
<br>
of X' will mean that less effort is put towards accomplishing X.
<br>
Because the AGI *currently* has a supergoal of X, the switch will
<br>
therefore be seen as undesirable. It's not like you have to point a
<br>
gun at the AGI's head and say, &quot;Do X or else!&quot;; no external coercion
<br>
is necessary. See
<br>
<a href="http://www.intelligence.org/upload/CFAI//design/structure/external.html">http://www.intelligence.org/upload/CFAI//design/structure/external.html</a>.
<br>
<p>6). An AGI has the goals we give it. It does not have human-like goals
<br>
such as &quot;reproduce&quot;, &quot;survive&quot;, &quot;be nice&quot;, &quot;get revenge&quot;, &quot;avoid
<br>
external manipulation&quot;, etc., unless we insert them or they turn out
<br>
to be useful for fulfillment of supergoals. See
<br>
<a href="http://www.intelligence.org/upload/CFAI//anthro.html#observer">http://www.intelligence.org/upload/CFAI//anthro.html#observer</a>.
<br>
<p>7). The vast, vast majority of goal systems lead to the destruction of
<br>
the Earth. The Earth's actual destruction would be more complicated
<br>
than this, but essentially, more energy, matter, computing power, etc.
<br>
are almost always desirable, and so the AGI won't stop consuming the
<br>
planet for its own use until it runs out of matter.
<br>
<p>8). Just because the AGI can do something doesn't mean it will. This
<br>
is what Eli calls the Giant Cheesecake Fallacy- &quot;A superintelligent
<br>
AGI could make huge cheesecakes, cheesecakes larger than any ever made
<br>
before; wow, the future will be full of giant cheesecakes!&quot; Some
<br>
examples of this in action:
<br>
<p>&quot;The AGI, being superintelligent, has all the computational power it
<br>
needs to understand natural language. Therefore, it will start
<br>
analyzing natural language, instead of analyzing the nearest random
<br>
quark.&quot;
<br>
<p>&quot;The AGI will be powerful enough to figure out exactly what humans
<br>
mean when they give an instruction. Therefore, the AGI will choose to
<br>
obey the intended meanings of human instructions, rather than obey the
<br>
commands of the nearest lemur.&quot;
<br>
<p>9). In general, it is much easier to work with simple examples than
<br>
complicated examples. If you can't do the simple stuff, you can't do
<br>
the complicated stuff. If you can't prove that an AGI will flood the
<br>
universe with paperclips and not iron crystals, you can't prove that
<br>
an AGI will be Friendly.
<br>
<p>&nbsp;- Tom
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17167.html">Peter de Blanc: "Re: How to make a slave (was: Building a friendly AI)"</a>
<li><strong>Previous message:</strong> <a href="17165.html">Jef Allbright: "Re: How to make a slave (was: Building a friendly AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17176.html">Stefan Pernar: "Re: General summary of FAI theory"</a>
<li><strong>Reply:</strong> <a href="17176.html">Stefan Pernar: "Re: General summary of FAI theory"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17166">[ date ]</a>
<a href="index.html#17166">[ thread ]</a>
<a href="subject.html#17166">[ subject ]</a>
<a href="author.html#17166">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:00 MDT
</em></small></p>
</body>
</html>
