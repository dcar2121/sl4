<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: General summary of FAI theory</title>
<meta name="Author" content="Anne Corwin (sparkle_robot@yahoo.com)">
<meta name="Subject" content="Re: General summary of FAI theory">
<meta name="Date" content="2007-11-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: General summary of FAI theory</h1>
<!-- received="Wed Nov 21 10:45:27 2007" -->
<!-- isoreceived="20071121174527" -->
<!-- sent="Wed, 21 Nov 2007 09:42:57 -0800 (PST)" -->
<!-- isosent="20071121174257" -->
<!-- name="Anne Corwin" -->
<!-- email="sparkle_robot@yahoo.com" -->
<!-- subject="Re: General summary of FAI theory" -->
<!-- id="994720.3735.qm@web56504.mail.re3.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="eafe728f0711202226h242c9016j68b8b2de3a90c999@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Anne Corwin (<a href="mailto:sparkle_robot@yahoo.com?Subject=Re:%20General%20summary%20of%20FAI%20theory"><em>sparkle_robot@yahoo.com</em></a>)<br>
<strong>Date:</strong> Wed Nov 21 2007 - 10:42:57 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17195.html">justin corwin: "Re: General summary of FAI theory"</a>
<li><strong>Previous message:</strong> <a href="17193.html">sl4.20.pris@spamgourmet.com: "Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI"</a>
<li><strong>In reply to:</strong> <a href="17186.html">Daniel Burfoot: "Re: General summary of FAI theory"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17195.html">justin corwin: "Re: General summary of FAI theory"</a>
<li><strong>Reply:</strong> <a href="17195.html">justin corwin: "Re: General summary of FAI theory"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17194">[ date ]</a>
<a href="index.html#17194">[ thread ]</a>
<a href="subject.html#17194">[ subject ]</a>
<a href="author.html#17194">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Dan said:
<br>
&nbsp;&nbsp;&nbsp;
<br>
<em>  &gt; As a reasonably moral person, or at least a person
</em><br>
<em>&gt; who doesn't want to play into the hands of tyrants, should I give up
</em><br>
<em>&gt; my AI research?
</em><br>
&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;Who are you asking?
<br>
&nbsp;&nbsp;&nbsp;
<br>
<em>  &gt; Or are we in an arms race against unspecified enemies, where the only
</em><br>
<em>&gt; way to be sure that they won't get the superweapon first is to build
</em><br>
<em>&gt; it ourselves, as fast as possible?
</em><br>
&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;Only if you're living in a comic book.
<br>
&nbsp;&nbsp;&nbsp;
<br>
<em>  &gt; It seems to me that FAI theory, to be successful, must also describe
</em><br>
<em>&gt; ways in which to prevent  dictators and other random idiots from
</em><br>
<em>&gt; constructing non-Friendly AGI, once the theory of AGI becomes widely
</em><br>
<em>&gt; known.
</em><br>
&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;Well, there are a lot of dangerous things in the world already today.  It's worth looking at how those dangerous things are already kept out of the hands of &quot;dictators and other random idiots&quot;, as well as how dictators and random idiots behave when they have access to dangerous things.  Nuclear theory is &quot;widely known&quot;, but nobody has blown up the world yet.  So either we've merely been incredibly lucky, or there are forces (not forces that can be trusted as infallible sentries, but forces nonetheless) at work keeping those who would do great evil from doing it.  
<br>
&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;One thing to look at in this regard is the psychology of power.  Dictators generally want as much power as possible, and in that respect they actually have an incentive not to use powerful destructive technologies -- if there's no &quot;world&quot;, there's no-one to have power over!  
<br>
&nbsp;&nbsp;I don't comment much on this list but I've been reading for a while, and I get the distinct impression that in this entire AGI discussion, &quot;power&quot; is the central issue moreso than anything that might be termed &quot;intelligence&quot;.  Basically, when you guys talk about &quot;Friendly AGI&quot; and &quot;Unfriendly AGI&quot;, you seem to be referring to entities that will act as *influences* on reality to an as-yet-unprecedented extent.  
<br>
&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;Regardless of any controversies over the neuropsychological definition of &quot;intelligence&quot;, it seems clear that in AGI circles, &quot;intelligence&quot; is very much conflated with a capacity to influence the environment in what humans would term &quot;complex&quot; ways.  That is, matter and energy accessible an intelligent agent are subject to being transmuted into forms quite disparate from the shape in which they were found -- per the will and perceived needs of that agent (e.g., via this formulation of intelligence, &quot;intelligence&quot; is the property that permits metal ore to be processed into a bicycle or car).  
<br>
&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;And despite the fact that humans tend to find cars and bicycles both useful (and dangerous, depending on the context in which they are used), it seems that the primary factor here is not the cognitive process that leads one to posit a bicycle, but the means by which a person executes the act of transmiting ore into bicycles.  In short, it's *capacity to influence* that matters in discussions of safety, much more so than *capacity to think*!
<br>
&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;You (generic &quot;you&quot;, referring to FAGI proponents) want to build what amounts to a &quot;magnifier&quot; for the Forces of Good (however those might be defined), and prevent UFAGI from magnifying the Forces of Evil (or the Forces of Stupid, if you prefer).  The commonly-invoked &quot;let's build a Good AI before someone builds a Bad AI!&quot; scenario has always struck me as another way of saying, &quot;let's make sure the power is concentrated in the hands of the Good Guys, so that the Bad Guys don't cause harm&quot;.  
<br>
&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;This isn't a new problem specific to theoretical AGI by any means; it's an ancient problem, and one fraught with the exact same controversies that come up on this list and others over and over and over again.  In fact, the only (important?) difference between the &quot;FAGI vs. UFAGI&quot; discussion and other &quot;Good-vs-Evil&quot; discourses is the anticipated magnitude of the power of those whose will ends up comprising the primary inputs to an AGI.
<br>
&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;And that brings up another aspect of the power discussion -- one thing I've never seen stated outright by AGI researchers/proponents is whether the intent is for the AGI itself to hold the majority of power, or for those who &quot;control&quot; or build the AGI to hold the majority of the power.  When bringing up the matter of &quot;dictators and random idiots&quot; getting their hands on AGI, what seems to be the underlying assumption is that the AGI will magnify the will and/or actions of anyone.  
<br>
&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;To me, this implies that &quot;AGI&quot; is intended (and perhaps expected) to not be autonomous so much as instrumental -- e.g., a person with evil intentions who &quot;gets their hands on&quot; AGI theory will almost assuredly create an &quot;evil AGI&quot;.  Is this really the intended conceptualization of AGI?  Or is this just an implicit assumption that hasn't been extensively examined?  Because if AGI is &quot;nonautonomous&quot; in the sense that it will always reflect the will (or the foolishness) of its creator(s), it would seem that this would imply that we don't need to &quot;stop evil AGIs&quot;, but rather, that we need to &quot;stop evil humans from accumulating power&quot;.  And that is a struggle being waged by people everywhere in the world right now, and one that does not require knowledge of AI theory in order to participate in.
<br>
&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;Now, if we are talking about *autonomous* AGIs (that is, AGIs that will not necessarily reflect, and therefore will not magnify the will of their creators), it almost seems as if trying to assure &quot;safety&quot; in advance of building them is utterly futile.  The closest thing we have right now to the capacity to create &quot;autonomous AGIs&quot; is the capacity to create more humans via reproduction.  
<br>
&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;No parent can assure that their future offspring will not someday destroy the world, and it would seem rather ridiculous (in the practical, if not the e-risk sense) to try and ban people from having kids until it can be assured that no child will ever grow up to destroy the world.  Right now, we deal with that kind of quandary through setting up barriers to extreme power accumulation in any one individual, through making and enforcing laws, and through social pressures (e.g., shunning and shaming of persons who commit acts like child abuse).  
<br>
&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;This system doesn't work perfectly, since abuses of both persons and power still exist (and exist in horrible manifestations at times), but it is better than nothing.  With that in mind, perhaps the aim should be not to create &quot;an AGI&quot;, but to create a colony or group of AGIs (with differently-weighted priorities) to serve as a &quot;checks and balances&quot; system.  The key is to avoid creating a situation that permits extreme concentration of power. &quot;Intelligence&quot; is an afterthought in that regard.
<br>
&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;- Anne
<br>
<p><p><p><p>&quot;Like and equal are not the same thing at all!&quot;
<br>
- Meg Murry, &quot;A Wrinkle In Time&quot;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
---------------------------------
<br>
Be a better sports nut! Let your teams follow you with Yahoo Mobile. Try it now.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17195.html">justin corwin: "Re: General summary of FAI theory"</a>
<li><strong>Previous message:</strong> <a href="17193.html">sl4.20.pris@spamgourmet.com: "Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI"</a>
<li><strong>In reply to:</strong> <a href="17186.html">Daniel Burfoot: "Re: General summary of FAI theory"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17195.html">justin corwin: "Re: General summary of FAI theory"</a>
<li><strong>Reply:</strong> <a href="17195.html">justin corwin: "Re: General summary of FAI theory"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17194">[ date ]</a>
<a href="index.html#17194">[ thread ]</a>
<a href="subject.html#17194">[ subject ]</a>
<a href="author.html#17194">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:00 MDT
</em></small></p>
</body>
</html>
