<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: General summary of FAI theory</title>
<meta name="Author" content="Tom McCabe (rocketjet314@yahoo.com)">
<meta name="Subject" content="General summary of FAI theory">
<meta name="Date" content="2007-11-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>General summary of FAI theory</h1>
<!-- received="Tue Nov 20 15:30:08 2007" -->
<!-- isoreceived="20071120223008" -->
<!-- sent="Tue, 20 Nov 2007 14:27:57 -0800 (PST)" -->
<!-- isosent="20071120222757" -->
<!-- name="Tom McCabe" -->
<!-- email="rocketjet314@yahoo.com" -->
<!-- subject="General summary of FAI theory" -->
<!-- id="850659.47959.qm@web51301.mail.re2.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tom McCabe (<a href="mailto:rocketjet314@yahoo.com?Subject=Re:%20General%20summary%20of%20FAI%20theory"><em>rocketjet314@yahoo.com</em></a>)<br>
<strong>Date:</strong> Tue Nov 20 2007 - 15:27:57 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17170.html">Thomas McCabe: "Re: How to make a slave (was: Building a friendly AI)"</a>
<li><strong>Previous message:</strong> <a href="17168.html">John K Clark: "Re: How to make a slave (was: Building a friendly AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17182.html">Daniel Burfoot: "Re: General summary of FAI theory"</a>
<li><strong>Reply:</strong> <a href="17182.html">Daniel Burfoot: "Re: General summary of FAI theory"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17169">[ date ]</a>
<a href="index.html#17169">[ thread ]</a>
<a href="subject.html#17169">[ subject ]</a>
<a href="author.html#17169">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
SL4 is supposed to be for advanced topics in futurism,
<br>
not endlessly
<br>
rehashing the basics. Some of the things which have
<br>
already been
<br>
covered years ago, and are therefore ineligible for
<br>
rehashing:
<br>
<p>1). An AGI will not act like an &quot;enslaved&quot; human, a
<br>
resentful human,
<br>
an emotionally repressed human, or any other kind of
<br>
human. See
<br>
<a href="http://www.intelligence.org/upload/CFAI//anthro.html">http://www.intelligence.org/upload/CFAI//anthro.html</a>.
<br>
<p>2). Friendliness content is the morality stuff that
<br>
says &quot;X is good, Y
<br>
is bad&quot;. Friendliness content is what we think the FAI
<br>
*should* do.
<br>
FAI theory describes *how* to get an FAI to do what
<br>
you want it to do.
<br>
See <a href="http://www.intelligence.org/upload/CEV.html">http://www.intelligence.org/upload/CEV.html</a>.
<br>
<p>3). FAI theory is damn hard; it is much harder than
<br>
Friendliness
<br>
content. So far as I know, nobody knows how to make
<br>
sure that some AGI
<br>
design reliably produces paperclips, which is much
<br>
*simpler* than
<br>
ensuring reliable Friendliness. Keep in mind that the
<br>
Friendliness
<br>
content must be maintained during recursive
<br>
self-improvement, or the
<br>
FAI may wind up destroying us all on programming
<br>
iteration
<br>
#1,576,169,123.
<br>
<p>4). CEV is a way of deriving Friendliness content from
<br>
humanity's
<br>
collective cognitive architecture. CEV is a
<br>
morality-constructor, not
<br>
a morality in and of itself; if you speak programming,
<br>
think of CEV as
<br>
a function that takes the human race as an argument
<br>
and returns a
<br>
morality.
<br>
<p>5). Goal systems naturally maintain themselves (under
<br>
most
<br>
conditions). If the AGI has a supergoal of X, changing
<br>
to a supergoal
<br>
of X' will mean that less effort is put towards
<br>
accomplishing X.
<br>
Because the AGI *currently* has a supergoal of X, the
<br>
switch will
<br>
therefore be seen as undesirable. It's not like you
<br>
have to point a
<br>
gun at the AGI's head and say, &quot;Do X or else!&quot;; no
<br>
external coercion
<br>
is necessary. See
<br>
<a href="http://www.intelligence.org/upload/CFAI//design/structure/external.html">http://www.intelligence.org/upload/CFAI//design/structure/external.html</a>.
<br>
<p>6). An AGI has the goals we give it. It does not have
<br>
human-like goals
<br>
such as &quot;reproduce&quot;, &quot;survive&quot;, &quot;be nice&quot;, &quot;get
<br>
revenge&quot;, &quot;avoid
<br>
external manipulation&quot;, etc., unless we insert them or
<br>
they turn out
<br>
to be useful for fulfillment of supergoals. See
<br>
<a href="http://www.intelligence.org/upload/CFAI//anthro.html#observer">http://www.intelligence.org/upload/CFAI//anthro.html#observer</a>.
<br>
<p>7). The vast, vast majority of goal systems lead to
<br>
the destruction of
<br>
the Earth. The Earth's actual destruction would be
<br>
more complicated
<br>
than this, but essentially, more energy, matter,
<br>
computing power, etc.
<br>
are almost always desirable, and so the AGI won't stop
<br>
consuming the
<br>
planet for its own use until it runs out of matter.
<br>
<p>8). Just because the AGI can do something doesn't mean
<br>
it will. This
<br>
is what Eli calls the Giant Cheesecake Fallacy- &quot;A
<br>
superintelligent
<br>
AGI could make huge cheesecakes, cheesecakes larger
<br>
than any ever made
<br>
before; wow, the future will be full of giant
<br>
cheesecakes!&quot; Some
<br>
examples of this in action:
<br>
<p>&quot;The AGI, being superintelligent, has all the
<br>
computational power it
<br>
needs to understand natural language. Therefore, it
<br>
will start
<br>
analyzing natural language, instead of analyzing the
<br>
nearest random
<br>
quark.&quot;
<br>
<p>&quot;The AGI will be powerful enough to figure out exactly
<br>
what humans
<br>
mean when they give an instruction. Therefore, the AGI
<br>
will choose to
<br>
obey the intended meanings of human instructions,
<br>
rather than obey the
<br>
commands of the nearest lemur.&quot;
<br>
<p>9). In general, it is much easier to work with simple
<br>
examples than
<br>
complicated examples. If you can't do the simple
<br>
stuff, you can't do
<br>
the complicated stuff. If you can't prove that an AGI
<br>
will flood the
<br>
universe with paperclips and not iron crystals, you
<br>
can't prove that
<br>
an AGI will be Friendly.
<br>
<p>&nbsp;- Tom
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;____________________________________________________________________________________
<br>
Be a better pen pal. 
<br>
Text or chat with friends inside Yahoo! Mail. See how.  <a href="http://overview.mail.yahoo.com/">http://overview.mail.yahoo.com/</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17170.html">Thomas McCabe: "Re: How to make a slave (was: Building a friendly AI)"</a>
<li><strong>Previous message:</strong> <a href="17168.html">John K Clark: "Re: How to make a slave (was: Building a friendly AI)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17182.html">Daniel Burfoot: "Re: General summary of FAI theory"</a>
<li><strong>Reply:</strong> <a href="17182.html">Daniel Burfoot: "Re: General summary of FAI theory"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17169">[ date ]</a>
<a href="index.html#17169">[ thread ]</a>
<a href="subject.html#17169">[ subject ]</a>
<a href="author.html#17169">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:00 MDT
</em></small></p>
</body>
</html>
