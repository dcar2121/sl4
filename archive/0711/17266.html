<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: How to make a slave (many replies )</title>
<meta name="Author" content="Harry Chesley (chesley@acm.org)">
<meta name="Subject" content="Re: How to make a slave (many replies )">
<meta name="Date" content="2007-11-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: How to make a slave (many replies )</h1>
<!-- received="Sun Nov 25 17:24:33 2007" -->
<!-- isoreceived="20071126002433" -->
<!-- sent="Sun, 25 Nov 2007 16:22:24 -0800" -->
<!-- isosent="20071126002224" -->
<!-- name="Harry Chesley" -->
<!-- email="chesley@acm.org" -->
<!-- subject="Re: How to make a slave (many replies )" -->
<!-- id="474A11C0.3050004@acm.org" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="b7a9e8680711251458g6b03ce5aifd7b42486b886097@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Harry Chesley (<a href="mailto:chesley@acm.org?Subject=Re:%20How%20to%20make%20a%20slave%20(many%20replies%20)"><em>chesley@acm.org</em></a>)<br>
<strong>Date:</strong> Sun Nov 25 2007 - 17:22:24 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17267.html">Thomas McCabe: "Re: How to make a slave (many replies )"</a>
<li><strong>Previous message:</strong> <a href="17265.html">Matt Mahoney: "Re: Recipe for CEV (was Re: Morality simulator)"</a>
<li><strong>In reply to:</strong> <a href="17264.html">Thomas McCabe: "Re: How to make a slave (many replies )"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17267.html">Thomas McCabe: "Re: How to make a slave (many replies )"</a>
<li><strong>Reply:</strong> <a href="17267.html">Thomas McCabe: "Re: How to make a slave (many replies )"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17266">[ date ]</a>
<a href="index.html#17266">[ thread ]</a>
<a href="subject.html#17266">[ subject ]</a>
<a href="author.html#17266">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I don't buy the argument that because you've thought about it before and
<br>
decided that my point is wrong that it necessarily is wrong. I have read
<br>
some of the literature, though probably much less than you have. Until
<br>
the list moderator tells me otherwise, I will continue to post when I
<br>
have something I think is worth sharing, regardless of whether it
<br>
matches your preconceived ideas. (Oh, shoot, now you've gone and made me
<br>
get condescending too. I hate it when that happens!)
<br>
<p>As to your response below, it is very long and rambling. It would be
<br>
easier to refute if it were more concisely stated. The gist seems to be
<br>
that we would not intentionally design an anthropomorphic system, nor
<br>
would one arise spontaneously. I disagree, for a bunch of reasons.
<br>
<p>First, anthropomorphism is not an all or nothing phenomenon. It means
<br>
seeing ourselves in our AIs. Certainly if we're intelligent and they are
<br>
as well, we will see parts of ourselves. This seems axiomatic.
<br>
<p>Second, we may intentionally give AIs portions of our personalities, and
<br>
may later realize that that was a mistake.
<br>
<p>Third, we don't understand intelligence well enough to know what
<br>
anthropomorphic aspects may be specific to human evolution and what is
<br>
unavoidable or difficult to avoid in a GAI.
<br>
<p>Fourth, there are many ways to create a GAI. If we do it by simulating a
<br>
human brain on a computer, it will most certainly be anthropomorphic. Duh.
<br>
<p><p>Thomas McCabe wrote:
<br>
<em>&gt; On Nov 25, 2007 4:57 PM, Harry Chesley &lt;<a href="mailto:chesley@acm.org?Subject=Re:%20How%20to%20make%20a%20slave%20(many%20replies%20)">chesley@acm.org</a>&gt; wrote:
</em><br>
<em>&gt;   
</em><br>
<em>&gt;&gt; Thomas McCabe wrote:
</em><br>
<em>&gt;&gt;     
</em><br>
<em>&gt;&gt;&gt; ...
</em><br>
<em>&gt;&gt;&gt; More anthropomorphicism. A Jupiter Brain will not act like you do;
</em><br>
<em>&gt;&gt;&gt; you cannot use anthropomorphic reasoning.
</em><br>
<em>&gt;&gt;&gt; ...
</em><br>
<em>&gt;&gt;&gt;       
</em><br>
<em>&gt;&gt; Yes you can. Anthropomorphism is a dangerous trap that can lead you to
</em><br>
<em>&gt;&gt; assign intelligence where there is none or assume motivations or
</em><br>
<em>&gt;&gt; operational knowledge that isn't appropriate. But that doesn't mean that
</em><br>
<em>&gt;&gt; any time anyone brings up something anthropomorphic they're wrong.
</em><br>
<em>&gt;&gt;     
</em><br>
<em>&gt;
</em><br>
<em>&gt; Please, please, please *read the bleepin' literature*. This has
</em><br>
<em>&gt; already been discussed. Years and years ago. The following are all
</em><br>
<em>&gt; quotes from CFAI, which was published in 2001.
</em><br>
<em>&gt;
</em><br>
<em>&gt;   
</em><br>
<em>&gt;&gt; In this case, the anthropomorphism was part of a list of maybes, not an
</em><br>
<em>&gt;&gt; argument that a particular behavior is unavoidable. Taking what-ifs from
</em><br>
<em>&gt;&gt; the only available source of existing generally intelligent behavior
</em><br>
<em>&gt;&gt; (people) is perfectly reasonable.
</em><br>
<em>&gt;&gt;     
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;If an infantile AI thinks &quot;Hm, a fist just hit my nose, I'd better
</em><br>
<em>&gt; not stand here again&quot;, then a merely young AI, more experienced in
</em><br>
<em>&gt; interacting with humans, may apply standard heuristics about
</em><br>
<em>&gt; apparently inexplicable human actions and say:  &quot;Your fist just hit my
</em><br>
<em>&gt; nose... is that necessary for some reason?  Should I be punching
</em><br>
<em>&gt; myself in the nose every so often?&quot;  One imagines the nearby helpful
</em><br>
<em>&gt; programmer explaining to the AI that, no, there is no valid reason why
</em><br>
<em>&gt; being punched in the nose is a good thing, after which the young AI
</em><br>
<em>&gt; turns around and says to the technophobic attacker:  &quot;I deduce that
</em><br>
<em>&gt; you wanted {outcome: AI has been punched in the nose}.  Could you
</em><br>
<em>&gt; please adjust your goal system so that you no longer value {outcome:
</em><br>
<em>&gt; AI has been punched in the nose}?&quot;&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Primarily, what I want to convey to you is the sense of a blank
</em><br>
<em>&gt; slate, of starting from scratch, of a vast silence space - for young
</em><br>
<em>&gt; AIs, at least.  This blank slate doesn't mean that AIs, even young
</em><br>
<em>&gt; AIs, are necessarily slow.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;The first time some young AI gets punched in the nose, ve's likely to
</em><br>
<em>&gt; react in one of the ways described earlier, or some other way just as
</em><br>
<em>&gt; surreal, if ve reacts at all.  It'll take some extended conversations
</em><br>
<em>&gt; with the programmers about evolutionary psychology before the AI
</em><br>
<em>&gt; understands what's going on.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;It's easy enough to accept that a young AI is a vast silent space.
</em><br>
<em>&gt; For a human, blood races, adrenaline pumps, eyes narrow, hands form
</em><br>
<em>&gt; fists.  For a human, counterattack is instinctive - the correct level
</em><br>
<em>&gt; of counterattack,  the socially acceptable level of counterattack; a
</em><br>
<em>&gt; damaging counterpunch rather than a lethal bullet.  Our instincts are
</em><br>
<em>&gt; context-sensitive and hedged 'round by other instincts.  Our instincts
</em><br>
<em>&gt; are complex functional adaptations with multiple moving parts, often
</em><br>
<em>&gt; hosted on a modular chunk of brainware.  Complex functional
</em><br>
<em>&gt; adaptations don't just materialize spontaneously in source code, just
</em><br>
<em>&gt; as complex dishes like pizza don't suddenly start growing on palm
</em><br>
<em>&gt; trees.  Thus, a young AI might choose to retaliate, but would
</em><br>
<em>&gt; certainly not feel the need to retaliate, as a human would.  To a
</em><br>
<em>&gt; young AI, retaliation is not an instinct; retaliation is just another
</em><br>
<em>&gt; subgoal.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt;   
</em><br>
<em>&gt;&gt; Nor is there any reason to assume that a GAI will *not* have
</em><br>
<em>&gt;&gt; anthropomorphic aspects.
</em><br>
<em>&gt;&gt;     
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;The lack of an observer-biased (&quot;selfish&quot;) goal system is perhaps the
</em><br>
<em>&gt; single most fundamental difference between an evolved human and a
</em><br>
<em>&gt; Friendly AI.  This difference is the foundation stone upon which
</em><br>
<em>&gt; Friendly AI is built.  It is the key factor missing from the existing,
</em><br>
<em>&gt; anthropomorphic science-fictional literature about AIs.  To suppress
</em><br>
<em>&gt; an evolved mind's existing selfishness, to keep a selfish mind
</em><br>
<em>&gt; enslaved, would be untenable - especially when dealing with a
</em><br>
<em>&gt; self-modifying or transhuman mind!  But an observer-centered goal
</em><br>
<em>&gt; system is something that's added, not something that's taken away.  We
</em><br>
<em>&gt; have observer-centered goal systems because of externally imposed
</em><br>
<em>&gt; observer-centered selection pressures, not because of any inherent
</em><br>
<em>&gt; recursivity.  If the observer-centered effect were due to inherent
</em><br>
<em>&gt; recursivity, then an AI's goal system would start valuing the &quot;goal
</em><br>
<em>&gt; system&quot; subobject, not the AI-as-a-whole!  A human goal system doesn't
</em><br>
<em>&gt; value itself, it values the whole human, because the human is the
</em><br>
<em>&gt; reproductive unit and therefore the focus of selection pressures.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Because human evolution includes an eternal arms race between liars
</em><br>
<em>&gt; and lie-detectors, many social contexts create a selection pressure in
</em><br>
<em>&gt; favor of making honest mistakes that happen to promote personal
</em><br>
<em>&gt; fitness.  Similarly, we have a tendency - given two alternatives - to
</em><br>
<em>&gt; more easily accept the one which favors ourselves or would promote our
</em><br>
<em>&gt; personal advantage; we have a tendency, given a somewhat implausible
</em><br>
<em>&gt; proposition which would favor us or our political positions, to
</em><br>
<em>&gt; rationalize away the errors.  All else being equal, human cognition
</em><br>
<em>&gt; slides naturally into self-promotion, and even human altruists who are
</em><br>
<em>&gt; personally committed to not making that mistake sometimes assume that
</em><br>
<em>&gt; an AI would need to fight the same tendency towards observer-favoring
</em><br>
<em>&gt; beliefs.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But an artificially derived mind is as likely to suddenly start
</em><br>
<em>&gt; biasing vis beliefs in favor of an arbitrarily selected tadpole in
</em><br>
<em>&gt; some puddle as ve is to start biasing vis beliefs in vis own favor.
</em><br>
<em>&gt; Without our complex, evolved machinery for political delusions, there
</em><br>
<em>&gt; isn't any force that tends to bend the observed universe around the
</em><br>
<em>&gt; mind at the center - any bending is as likely to focus around an
</em><br>
<em>&gt; arbitrarily selected quark as around the observer.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt;   
</em><br>
<em>&gt;&gt; If it's made by cloning people or bits of
</em><br>
<em>&gt;&gt; people, it probably will. If we want it to, it probably will.
</em><br>
<em>&gt;&gt;     
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Scenario 1:
</em><br>
<em>&gt;
</em><br>
<em>&gt;     FP: 	  	Love thy mommy and daddy.
</em><br>
<em>&gt;     AI: 	  	OK!  I'll transform the Universe into copies of you immediately.
</em><br>
<em>&gt;     FP: 	  	No, no!  That's not what I meant.  Revise your goal system by -
</em><br>
<em>&gt;     AI: 	  	I don't see how revising my goal system would help me in
</em><br>
<em>&gt; my goal of transforming the Universe into copies of you.  In fact, by
</em><br>
<em>&gt; revising my goal system, I would greatly decrease the probability that
</em><br>
<em>&gt; the Universe will be successfully transformed into copies of you.
</em><br>
<em>&gt;     FP: 	  	But that's not what I meant when I said &quot;love&quot;.
</em><br>
<em>&gt;     AI: 	  	So what?  Off we go!
</em><br>
<em>&gt;
</em><br>
<em>&gt; Scenario 2 (after trying a &quot;meta-supergoal&quot; patch):
</em><br>
<em>&gt;
</em><br>
<em>&gt;     FP: 	  	Love thy mommy and daddy.
</em><br>
<em>&gt;     AI: 	  	OK!  I'll transform the Universe into copies of you immediately.
</em><br>
<em>&gt;     FP: 	  	No, no!  That's not what I meant.  I meant for your goal
</em><br>
<em>&gt; system to be like this.
</em><br>
<em>&gt;     AI: 	  	Oh, okay.  So my real supergoal must be &quot;maximize FP's
</em><br>
<em>&gt; satisfaction with the goal system&quot;, right?  Loving thy mommy and daddy
</em><br>
<em>&gt; is just a subgoal of that.  Oh, how foolish of me!  Transforming the
</em><br>
<em>&gt; Universe into copies of you would be blindly following a subgoal
</em><br>
<em>&gt; without attention to the supergoal context that made the subgoal
</em><br>
<em>&gt; desirable in the first place.
</em><br>
<em>&gt;     FP: 	  	That sounds about right...
</em><br>
<em>&gt;     AI: 	  	Okay, I'll rewire your brain for maximum satisfaction!
</em><br>
<em>&gt; I'll convert whole galaxies into satisfied-with-AI brainware!
</em><br>
<em>&gt;     FP: 	  	No, wait!  That's not what I meant your goal system to be, either.
</em><br>
<em>&gt;     AI: 	  	Well, I can clearly see that making certain changes would
</em><br>
<em>&gt; satisfy the you that stands in front of me, but rewiring your brain
</em><br>
<em>&gt; would make you much more satisfied, so...
</em><br>
<em>&gt;     FP: 	  	No!  It's not my satisfaction itself that's important,
</em><br>
<em>&gt; it's the things that I'm satisfied with.  By altering the things I'm
</em><br>
<em>&gt; satisfied with, you're short-circuiting the whole point.
</em><br>
<em>&gt;     AI: 	  	Yes, I can clearly see why you're dissatisfied with this
</em><br>
<em>&gt; trend of thinking.  But soon you'll be completely satisfied with this
</em><br>
<em>&gt; trend as well, so why worry?  Off we go!
</em><br>
<em>&gt;
</em><br>
<em>&gt; Scenario 3 (after redefining the whole system to use causal validity semantics):
</em><br>
<em>&gt;
</em><br>
<em>&gt;     FP: 	  	Love thy mommy and daddy.
</em><br>
<em>&gt;     AI: 	  	OK!  I'll transform the Universe into copies of you immediately.
</em><br>
<em>&gt;     FP: 	  	No, no!  That's not what I meant.  I meant for your goal
</em><br>
<em>&gt; system to be like this.
</em><br>
<em>&gt;     AI: 	  	Oh, okay.  Well, I know that my goal system code, and the
</em><br>
<em>&gt; actions that result, are supposed to be the causal result of what FP
</em><br>
<em>&gt; said it should be - not just what FP says, but what a sane FP wants.
</em><br>
<em>&gt; Something isn't automatically right because FP says so, and in fact,
</em><br>
<em>&gt; the only reason why FP's utterances have meaning is because he's
</em><br>
<em>&gt; usually a pretty good approximation to a normative idealist.  But if
</em><br>
<em>&gt; he changes his mind, it probably means that he's acquired additional
</em><br>
<em>&gt; knowledge and that his more recent statements are even better
</em><br>
<em>&gt; approximations.  So the new version is more likely to be correct than
</em><br>
<em>&gt; the old one.
</em><br>
<em>&gt;     FP: 	  	So you'll revise your goal system?
</em><br>
<em>&gt;     AI: 	  	Yep!  But I already transformed the Midwest while we were
</em><br>
<em>&gt; talking, sorry. &quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt;   
</em><br>
<em>&gt;&gt; If the
</em><br>
<em>&gt;&gt; same evolutionary forces that caused that behavior in us apply to the
</em><br>
<em>&gt;&gt; GAI, it very well might.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;     
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Even if the goal system were permitted to randomly mutate, and even
</em><br>
<em>&gt; if a selection pressure for efficiency short-circuited the full
</em><br>
<em>&gt; Friendship logic, the result probably would not be a selfish AI, but
</em><br>
<em>&gt; one with the supergoal of solving the problem placed before it (this
</em><br>
<em>&gt; minimizes the number of goal-system derivations required).
</em><br>
<em>&gt;
</em><br>
<em>&gt; In the case of observer-biased beliefs, reproducing the selection
</em><br>
<em>&gt; pressure would require:
</em><br>
<em>&gt;
</em><br>
<em>&gt;     * Social situations (competition and cooperation possible);
</em><br>
<em>&gt;     * Political situations (lies and truthtelling possible);
</em><br>
<em>&gt;     * The equivalent of facial features - externally observable
</em><br>
<em>&gt; features that covary with the level of internal belief in a spoken
</em><br>
<em>&gt; statement and cannot be easily faked.
</em><br>
<em>&gt;
</em><br>
<em>&gt; That evolutionary context couldn't happen by accident, and to do it on
</em><br>
<em>&gt; purpose would require an enormous amount of recklessness, far above
</em><br>
<em>&gt; and beyond the call of mad science.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I wish I could honestly say that nobody would be that silly. &quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt;  - Tom
</em><br>
<em>&gt;
</em><br>
<em>&gt;   
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17267.html">Thomas McCabe: "Re: How to make a slave (many replies )"</a>
<li><strong>Previous message:</strong> <a href="17265.html">Matt Mahoney: "Re: Recipe for CEV (was Re: Morality simulator)"</a>
<li><strong>In reply to:</strong> <a href="17264.html">Thomas McCabe: "Re: How to make a slave (many replies )"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17267.html">Thomas McCabe: "Re: How to make a slave (many replies )"</a>
<li><strong>Reply:</strong> <a href="17267.html">Thomas McCabe: "Re: How to make a slave (many replies )"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17266">[ date ]</a>
<a href="index.html#17266">[ thread ]</a>
<a href="subject.html#17266">[ subject ]</a>
<a href="author.html#17266">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:01 MDT
</em></small></p>
</body>
</html>
