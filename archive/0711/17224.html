<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: UCaRtMaAI paper</title>
<meta name="Author" content="Tim Freeman (tim@fungible.com)">
<meta name="Subject" content="Re: UCaRtMaAI paper">
<meta name="Date" content="2007-11-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: UCaRtMaAI paper</h1>
<!-- received="Fri Nov 23 14:24:13 2007" -->
<!-- isoreceived="20071123212413" -->
<!-- sent="Fri, 23 Nov 2007 09:20:36 -0700" -->
<!-- isosent="20071123162036" -->
<!-- name="Tim Freeman" -->
<!-- email="tim@fungible.com" -->
<!-- subject="Re: UCaRtMaAI paper" -->
<!-- id="20071123212204.D7885D262C@fungible.com" -->
<!-- inreplyto="CA3E8753336F4CCBA36B13CBBA929E75@weidaim1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tim Freeman (<a href="mailto:tim@fungible.com?Subject=Re:%20UCaRtMaAI%20paper"><em>tim@fungible.com</em></a>)<br>
<strong>Date:</strong> Fri Nov 23 2007 - 09:20:36 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17225.html">Harry Chesley: "Re: How to make a slave"</a>
<li><strong>Previous message:</strong> <a href="17223.html">Norman Noman: "Re: Morality simulator"</a>
<li><strong>In reply to:</strong> <a href="17213.html">Wei Dai: "Re: UCaRtMaAI paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17228.html">Eliezer S. Yudkowsky: "Re: UCaRtMaAI paper"</a>
<li><strong>Reply:</strong> <a href="17228.html">Eliezer S. Yudkowsky: "Re: UCaRtMaAI paper"</a>
<li><strong>Reply:</strong> <a href="17231.html">Wei Dai: "Re: UCaRtMaAI paper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17224">[ date ]</a>
<a href="index.html#17224">[ thread ]</a>
<a href="subject.html#17224">[ subject ]</a>
<a href="author.html#17224">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
From: &quot;Wei Dai&quot; &lt;<a href="mailto:weidai@weidai.com?Subject=Re:%20UCaRtMaAI%20paper">weidai@weidai.com</a>&gt;
<br>
<em>&gt;Tim, have you read my recent posts titled &quot;answers I'd like from an SI&quot; and 
</em><br>
<em>&gt;&quot;answers I'd like, part 2&quot;?
</em><br>
<p>No, I hadn't, but now I have.  Your issue there seems to be that you
<br>
want some scheme that has verbal communication near enough to its core
<br>
so that answering a question is regarded as something more fundamental
<br>
than making noises to satisfy the stupid humans.  You feel frustrated
<br>
that none of the existing schemes based on optimizing some utility
<br>
function do that.  I agree that that would be nice to have.  I don't
<br>
see it as essential, in the sense that I can imagine a useful and
<br>
friendly AI that doesn't treat verbal communication as special in any
<br>
way.
<br>
<p>There is a laundry list of questions in your 13 Nov 2007 &quot;answers I'd
<br>
like, part 2&quot; post, and some concerns about what might go wrong if
<br>
those questions are answered wrong.  However, I still don't see
<br>
question-answering as essential, in the sense that making the errors
<br>
you described seems similar in kind to me to incorrectly guessing that
<br>
everyone wants to die and then helping them out.  There are all sorts
<br>
of incorrect guesses that could be made, and the ones you list do not
<br>
seem to me to be more likely than other mistakes that don't revolve
<br>
around philosophical questions.
<br>
<p><em>&gt;I don't think that helps too much, because if everyone knows that the AI 
</em><br>
<em>&gt;will stop helping only temporarily, they will take that into account and 
</em><br>
<em>&gt;still not act in ways that reveal their true utilities.
</em><br>
<p>It would give some useful information.  If I'm hungry for an apple
<br>
now, and the AI says it won't help me until tomorrow, thoughts about
<br>
the AI's help tomorrow aren't going to affect much my present attempts
<br>
to get the apple.  
<br>
<p>So far as I can tell, you are arguing that my scheme isn't perfect.  I
<br>
agree with that statement, but in the absence of a perfect or even a
<br>
conjectured-to-be-better scheme, I'm not very interested.
<br>
<p><em>&gt;...helping whichever child you think needs your help most (which
</em><br>
<em>&gt;encourages them to exaggerate how much help they need)
</em><br>
<p>I agree that having the AI help those who appear to need the most
<br>
would encourage people to exaggerate how much help they need.
<br>
Guessing that &quot;Joe is doing X because he thinks it will make the AI
<br>
give him Y&quot; isn't a special case in the UCaRtMaAI algorithm, so maybe
<br>
things will get back into equilibrium if the AI becomes better at
<br>
detecting exaggeration than Joe is at exaggerating.
<br>
<p>Human motivation doesn't vary all that much from human to human, and
<br>
in the UCaRtMaAI algorithm there is one explanation of human
<br>
motivation that takes an agent-id as input. (Specifically, in the
<br>
diagram at <a href="http://www.fungible.com/respect/paper.html#beliefs">http://www.fungible.com/respect/paper.html#beliefs</a>,
<br>
Compute-Utility is one-per-explanation, and it takes A as input.)  The
<br>
simplest explanations have Joe wanting apples about the same amount
<br>
that everyone else wants apples, so it shouldn't be too hard to figure
<br>
out about how much Joe wants an apple.
<br>
<p>Do you have any ideas about how to help people without interfering
<br>
with the ability to infer what they want from their behavior?
<br>
<p><em>&gt;First, you haven't showed that the AI will actually draw the obvious-to-us 
</em><br>
<em>&gt;conclusions correctly. 
</em><br>
<p>There is a collection of unit tests at
<br>
<a href="http://www.fungible.com/respect/code/#tests">http://www.fungible.com/respect/code/#tests</a>.  They are all gross
<br>
simplifications, of course.  The most complex ones are baby catching
<br>
at <a href="http://www.fungible.com/respect/code/baby_catch_test.py.txt">http://www.fungible.com/respect/code/baby_catch_test.py.txt</a> and
<br>
grocery shopping at
<br>
<a href="http://www.fungible.com/respect/code/grocery_test.py.txt">http://www.fungible.com/respect/code/grocery_test.py.txt</a>.  Do you have
<br>
an obvious-to-us conclusion that you'd like to see that's about as
<br>
complex as these two?
<br>
<p><em>&gt;Second, if we eventually discover a moral philosophy 
</em><br>
<em>&gt;that is a big improvement over what is hard coded into the AI, we are 
</em><br>
<em>&gt;screwed because we won't be able to reason with it and get it to change.
</em><br>
<p>That's true in some scenarios but not others.  The AI has a time
<br>
horizon and all influences it has past that time horizon are done
<br>
because the humans have a desire, before the time horizon, to
<br>
reasonably expect those consequences after the time horizon.
<br>
So if the time horizon is one hour from now, and AI is able to figure
<br>
out that right now we want a new AI after one hour from now that
<br>
implements some shiny new moral philosophy, it could arrange for that.
<br>
We could lose if the present AI has bugs that prevent it from seeing
<br>
that we want the new moral philosophy to be in effect past the time
<br>
horizon.
<br>
<p>So we wouldn't reason with the AI, we'd just make it clear what we
<br>
want, wait for the time horizon to come, and hope it functions well
<br>
enough to give us what we said we wanted.
<br>
<p>This seems to be a problem with any AI that controls the world --
<br>
there is no guarantee that if you have some future inspiration about
<br>
how you want it to work, it will let you implement the change.  This
<br>
can be a good thing, for example if your future inspiration is genocidal.
<br>
<p><em>&gt;But if arbitrariness is not a problem, then why not just pick the utility 
</em><br>
<em>&gt;function of an arbitrary person instead of trying to average them?
</em><br>
<p>I feel comfortable making an arbitrary choice among the best known
<br>
alternatives.  Having one guy be the dictator is not a best known
<br>
alternative.  If I'm the dictator, then your dinner tonight will be
<br>
what I want your dinner tonight to be, and your tastes don't matter.
<br>
If we average, then because I don't care much what you have for
<br>
dinner, and you probably do care, you will probably have the dinner
<br>
you want.  So averaging is better than taking just one, at least
<br>
because it avoids some conflict about which one we'll take.
<br>
<p><pre>
-- 
Tim Freeman               <a href="http://www.fungible.com">http://www.fungible.com</a>           <a href="mailto:tim@fungible.com?Subject=Re:%20UCaRtMaAI%20paper">tim@fungible.com</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17225.html">Harry Chesley: "Re: How to make a slave"</a>
<li><strong>Previous message:</strong> <a href="17223.html">Norman Noman: "Re: Morality simulator"</a>
<li><strong>In reply to:</strong> <a href="17213.html">Wei Dai: "Re: UCaRtMaAI paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17228.html">Eliezer S. Yudkowsky: "Re: UCaRtMaAI paper"</a>
<li><strong>Reply:</strong> <a href="17228.html">Eliezer S. Yudkowsky: "Re: UCaRtMaAI paper"</a>
<li><strong>Reply:</strong> <a href="17231.html">Wei Dai: "Re: UCaRtMaAI paper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17224">[ date ]</a>
<a href="index.html#17224">[ thread ]</a>
<a href="subject.html#17224">[ subject ]</a>
<a href="author.html#17224">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:00 MDT
</em></small></p>
</body>
</html>
