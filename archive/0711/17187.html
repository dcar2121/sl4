<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: General summary of FAI theory</title>
<meta name="Author" content="Byrne Hobart (sometimesfunnyalwaysright@gmail.com)">
<meta name="Subject" content="Re: General summary of FAI theory">
<meta name="Date" content="2007-11-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: General summary of FAI theory</h1>
<!-- received="Tue Nov 20 23:30:02 2007" -->
<!-- isoreceived="20071121063002" -->
<!-- sent="Wed, 21 Nov 2007 01:27:59 -0500" -->
<!-- isosent="20071121062759" -->
<!-- name="Byrne Hobart" -->
<!-- email="sometimesfunnyalwaysright@gmail.com" -->
<!-- subject="Re: General summary of FAI theory" -->
<!-- id="2F3F727C-8556-475D-AF9F-42C91525605C@gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="b7a9e8680711202120q119b2b1ajcbaf1e7b0fbcc5f6@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Byrne Hobart (<a href="mailto:sometimesfunnyalwaysright@gmail.com?Subject=Re:%20General%20summary%20of%20FAI%20theory"><em>sometimesfunnyalwaysright@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Nov 20 2007 - 23:27:59 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17188.html">John K Clark: "Re: How to make a slave (was: Building a friendly AI)"</a>
<li><strong>Previous message:</strong> <a href="17186.html">Daniel Burfoot: "Re: General summary of FAI theory"</a>
<li><strong>In reply to:</strong> <a href="17183.html">Thomas McCabe: "Re: General summary of FAI theory"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17268.html">Rolf Nelson: "Re: General summary of FAI theory"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17187">[ date ]</a>
<a href="index.html#17187">[ thread ]</a>
<a href="subject.html#17187">[ subject ]</a>
<a href="author.html#17187">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; It is straightforward (not easy, but straightforward) for any
</em><br>
<em>&gt; dedicated, reasonably intelligent person to become rich in modern-day
</em><br>
<em>&gt; America. All you have to do is start a company, and get millions of
</em><br>
<em>&gt; users for a product or service, no AI necessary. See the essays at
</em><br>
<em>&gt; <a href="http://www.paulgraham.com/">http://www.paulgraham.com/</a>, in particular
</em><br>
<em>&gt; <a href="http://www.paulgraham.com/hiring.html">http://www.paulgraham.com/hiring.html</a> and
</em><br>
<em>&gt; <a href="http://www.paulgraham.com/start.html">http://www.paulgraham.com/start.html</a>.
</em><br>
<p>PG usually claims that the average startup fails: that if you do  
<br>
everything right, you have a good chance of ending up earning exactly  
<br>
nothing (which may be worth something, especially on a résumé). In  
<br>
that context, using an AI to predict the market sounds like a cost- 
<br>
effective solution (and a scalable one -- I'm not sure PG's advice is  
<br>
worse at turning $10,000 into $1 million than it as at turning that  
<br>
$1 million into $100 million.
<br>
<p>On the other hand, it's not like Dr. Evil won't have competition. I  
<br>
can think of three firms that, collective, run about $50 billion with  
<br>
over 100 PhD-educated researchers cranking out predictive software,  
<br>
and even a very smart AI may not have the combined intellectual  
<br>
firepower of 100 people with 130+ IQs each, and much more experience.
<br>
<em>&gt;
</em><br>
<em>&gt; No. For that to have a reasonable chance of success, you would have to
</em><br>
<em>&gt; get competent transhumanists (if not professional AI researchers)
</em><br>
<em>&gt; writing the regulations, and the bureaucrats aren't going to let that
</em><br>
<em>&gt; happen. Otherwise, you just end up having to fill out meaningless AI
</em><br>
<em>&gt; Safety Permit Application Form #581,215,102.
</em><br>
<p>True. And what's especially bad is that Dr. Evil isn't going to fill  
<br>
out AI Safety Permit forms, so his overhead is that much lower.
<br>
<p>Basically, the protection you have to have is that the people pushing  
<br>
hardest for AI are doing it so Dr. Evil won't be the first to get it  
<br>
(although the record of well-meaning people with enormous power is a  
<br>
lot shabbier than that of self-interested people with the same -- Dr.  
<br>
Evil might extort money from us, but he'll probably do it as  
<br>
efficiently as possible, and on the right part of the Laffer curve,  
<br>
so we may end up happier after all).
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17188.html">John K Clark: "Re: How to make a slave (was: Building a friendly AI)"</a>
<li><strong>Previous message:</strong> <a href="17186.html">Daniel Burfoot: "Re: General summary of FAI theory"</a>
<li><strong>In reply to:</strong> <a href="17183.html">Thomas McCabe: "Re: General summary of FAI theory"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17268.html">Rolf Nelson: "Re: General summary of FAI theory"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17187">[ date ]</a>
<a href="index.html#17187">[ thread ]</a>
<a href="subject.html#17187">[ subject ]</a>
<a href="author.html#17187">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:00 MDT
</em></small></p>
</body>
</html>
