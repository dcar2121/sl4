<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: How to make a slave (many replies )</title>
<meta name="Author" content="Thomas McCabe (pphysics141@gmail.com)">
<meta name="Subject" content="Re: How to make a slave (many replies )">
<meta name="Date" content="2007-11-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: How to make a slave (many replies )</h1>
<!-- received="Sun Nov 25 16:01:10 2007" -->
<!-- isoreceived="20071125230110" -->
<!-- sent="Sun, 25 Nov 2007 17:58:38 -0500" -->
<!-- isosent="20071125225838" -->
<!-- name="Thomas McCabe" -->
<!-- email="pphysics141@gmail.com" -->
<!-- subject="Re: How to make a slave (many replies )" -->
<!-- id="b7a9e8680711251458g6b03ce5aifd7b42486b886097@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="4749EFB3.7090103@acm.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Thomas McCabe (<a href="mailto:pphysics141@gmail.com?Subject=Re:%20How%20to%20make%20a%20slave%20(many%20replies%20)"><em>pphysics141@gmail.com</em></a>)<br>
<strong>Date:</strong> Sun Nov 25 2007 - 15:58:38 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17265.html">Matt Mahoney: "Re: Recipe for CEV (was Re: Morality simulator)"</a>
<li><strong>Previous message:</strong> <a href="17263.html">Harry Chesley: "Re: How to make a slave (many replies )"</a>
<li><strong>In reply to:</strong> <a href="17263.html">Harry Chesley: "Re: How to make a slave (many replies )"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17266.html">Harry Chesley: "Re: How to make a slave (many replies )"</a>
<li><strong>Reply:</strong> <a href="17266.html">Harry Chesley: "Re: How to make a slave (many replies )"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17264">[ date ]</a>
<a href="index.html#17264">[ thread ]</a>
<a href="subject.html#17264">[ subject ]</a>
<a href="author.html#17264">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Nov 25, 2007 4:57 PM, Harry Chesley &lt;<a href="mailto:chesley@acm.org?Subject=Re:%20How%20to%20make%20a%20slave%20(many%20replies%20)">chesley@acm.org</a>&gt; wrote:
<br>
<em>&gt; Thomas McCabe wrote:
</em><br>
<em>&gt; &gt;...
</em><br>
<em>&gt; &gt; More anthropomorphicism. A Jupiter Brain will not act like you do;
</em><br>
<em>&gt; &gt; you cannot use anthropomorphic reasoning.
</em><br>
<em>&gt; &gt;...
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes you can. Anthropomorphism is a dangerous trap that can lead you to
</em><br>
<em>&gt; assign intelligence where there is none or assume motivations or
</em><br>
<em>&gt; operational knowledge that isn't appropriate. But that doesn't mean that
</em><br>
<em>&gt; any time anyone brings up something anthropomorphic they're wrong.
</em><br>
<p>Please, please, please *read the bleepin' literature*. This has
<br>
already been discussed. Years and years ago. The following are all
<br>
quotes from CFAI, which was published in 2001.
<br>
<p><em>&gt; In this case, the anthropomorphism was part of a list of maybes, not an
</em><br>
<em>&gt; argument that a particular behavior is unavoidable. Taking what-ifs from
</em><br>
<em>&gt; the only available source of existing generally intelligent behavior
</em><br>
<em>&gt; (people) is perfectly reasonable.
</em><br>
<p>&quot;If an infantile AI thinks &quot;Hm, a fist just hit my nose, I'd better
<br>
not stand here again&quot;, then a merely young AI, more experienced in
<br>
interacting with humans, may apply standard heuristics about
<br>
apparently inexplicable human actions and say:  &quot;Your fist just hit my
<br>
nose... is that necessary for some reason?  Should I be punching
<br>
myself in the nose every so often?&quot;  One imagines the nearby helpful
<br>
programmer explaining to the AI that, no, there is no valid reason why
<br>
being punched in the nose is a good thing, after which the young AI
<br>
turns around and says to the technophobic attacker:  &quot;I deduce that
<br>
you wanted {outcome: AI has been punched in the nose}.  Could you
<br>
please adjust your goal system so that you no longer value {outcome:
<br>
AI has been punched in the nose}?&quot;&quot;
<br>
<p>&quot;Primarily, what I want to convey to you is the sense of a blank
<br>
slate, of starting from scratch, of a vast silence space - for young
<br>
AIs, at least.  This blank slate doesn't mean that AIs, even young
<br>
AIs, are necessarily slow.&quot;
<br>
<p>&quot;The first time some young AI gets punched in the nose, ve's likely to
<br>
react in one of the ways described earlier, or some other way just as
<br>
surreal, if ve reacts at all.  It'll take some extended conversations
<br>
with the programmers about evolutionary psychology before the AI
<br>
understands what's going on.&quot;
<br>
<p>&quot;It's easy enough to accept that a young AI is a vast silent space.
<br>
For a human, blood races, adrenaline pumps, eyes narrow, hands form
<br>
fists.  For a human, counterattack is instinctive - the correct level
<br>
of counterattack,  the socially acceptable level of counterattack; a
<br>
damaging counterpunch rather than a lethal bullet.  Our instincts are
<br>
context-sensitive and hedged 'round by other instincts.  Our instincts
<br>
are complex functional adaptations with multiple moving parts, often
<br>
hosted on a modular chunk of brainware.  Complex functional
<br>
adaptations don't just materialize spontaneously in source code, just
<br>
as complex dishes like pizza don't suddenly start growing on palm
<br>
trees.  Thus, a young AI might choose to retaliate, but would
<br>
certainly not feel the need to retaliate, as a human would.  To a
<br>
young AI, retaliation is not an instinct; retaliation is just another
<br>
subgoal.&quot;
<br>
<p><em>&gt; Nor is there any reason to assume that a GAI will *not* have
</em><br>
<em>&gt; anthropomorphic aspects.
</em><br>
<p>&quot;The lack of an observer-biased (&quot;selfish&quot;) goal system is perhaps the
<br>
single most fundamental difference between an evolved human and a
<br>
Friendly AI.  This difference is the foundation stone upon which
<br>
Friendly AI is built.  It is the key factor missing from the existing,
<br>
anthropomorphic science-fictional literature about AIs.  To suppress
<br>
an evolved mind's existing selfishness, to keep a selfish mind
<br>
enslaved, would be untenable - especially when dealing with a
<br>
self-modifying or transhuman mind!  But an observer-centered goal
<br>
system is something that's added, not something that's taken away.  We
<br>
have observer-centered goal systems because of externally imposed
<br>
observer-centered selection pressures, not because of any inherent
<br>
recursivity.  If the observer-centered effect were due to inherent
<br>
recursivity, then an AI's goal system would start valuing the &quot;goal
<br>
system&quot; subobject, not the AI-as-a-whole!  A human goal system doesn't
<br>
value itself, it values the whole human, because the human is the
<br>
reproductive unit and therefore the focus of selection pressures.&quot;
<br>
<p>&quot;Because human evolution includes an eternal arms race between liars
<br>
and lie-detectors, many social contexts create a selection pressure in
<br>
favor of making honest mistakes that happen to promote personal
<br>
fitness.  Similarly, we have a tendency - given two alternatives - to
<br>
more easily accept the one which favors ourselves or would promote our
<br>
personal advantage; we have a tendency, given a somewhat implausible
<br>
proposition which would favor us or our political positions, to
<br>
rationalize away the errors.  All else being equal, human cognition
<br>
slides naturally into self-promotion, and even human altruists who are
<br>
personally committed to not making that mistake sometimes assume that
<br>
an AI would need to fight the same tendency towards observer-favoring
<br>
beliefs.
<br>
<p>But an artificially derived mind is as likely to suddenly start
<br>
biasing vis beliefs in favor of an arbitrarily selected tadpole in
<br>
some puddle as ve is to start biasing vis beliefs in vis own favor.
<br>
Without our complex, evolved machinery for political delusions, there
<br>
isn't any force that tends to bend the observed universe around the
<br>
mind at the center - any bending is as likely to focus around an
<br>
arbitrarily selected quark as around the observer.&quot;
<br>
<p><em>&gt; If it's made by cloning people or bits of
</em><br>
<em>&gt; people, it probably will. If we want it to, it probably will.
</em><br>
<p>&quot;Scenario 1:
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;FP: 	  	Love thy mommy and daddy.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;AI: 	  	OK!  I'll transform the Universe into copies of you immediately.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;FP: 	  	No, no!  That's not what I meant.  Revise your goal system by -
<br>
&nbsp;&nbsp;&nbsp;&nbsp;AI: 	  	I don't see how revising my goal system would help me in
<br>
my goal of transforming the Universe into copies of you.  In fact, by
<br>
revising my goal system, I would greatly decrease the probability that
<br>
the Universe will be successfully transformed into copies of you.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;FP: 	  	But that's not what I meant when I said &quot;love&quot;.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;AI: 	  	So what?  Off we go!
<br>
<p>Scenario 2 (after trying a &quot;meta-supergoal&quot; patch):
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;FP: 	  	Love thy mommy and daddy.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;AI: 	  	OK!  I'll transform the Universe into copies of you immediately.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;FP: 	  	No, no!  That's not what I meant.  I meant for your goal
<br>
system to be like this.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;AI: 	  	Oh, okay.  So my real supergoal must be &quot;maximize FP's
<br>
satisfaction with the goal system&quot;, right?  Loving thy mommy and daddy
<br>
is just a subgoal of that.  Oh, how foolish of me!  Transforming the
<br>
Universe into copies of you would be blindly following a subgoal
<br>
without attention to the supergoal context that made the subgoal
<br>
desirable in the first place.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;FP: 	  	That sounds about right...
<br>
&nbsp;&nbsp;&nbsp;&nbsp;AI: 	  	Okay, I'll rewire your brain for maximum satisfaction!
<br>
I'll convert whole galaxies into satisfied-with-AI brainware!
<br>
&nbsp;&nbsp;&nbsp;&nbsp;FP: 	  	No, wait!  That's not what I meant your goal system to be, either.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;AI: 	  	Well, I can clearly see that making certain changes would
<br>
satisfy the you that stands in front of me, but rewiring your brain
<br>
would make you much more satisfied, so...
<br>
&nbsp;&nbsp;&nbsp;&nbsp;FP: 	  	No!  It's not my satisfaction itself that's important,
<br>
it's the things that I'm satisfied with.  By altering the things I'm
<br>
satisfied with, you're short-circuiting the whole point.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;AI: 	  	Yes, I can clearly see why you're dissatisfied with this
<br>
trend of thinking.  But soon you'll be completely satisfied with this
<br>
trend as well, so why worry?  Off we go!
<br>
<p>Scenario 3 (after redefining the whole system to use causal validity semantics):
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;FP: 	  	Love thy mommy and daddy.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;AI: 	  	OK!  I'll transform the Universe into copies of you immediately.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;FP: 	  	No, no!  That's not what I meant.  I meant for your goal
<br>
system to be like this.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;AI: 	  	Oh, okay.  Well, I know that my goal system code, and the
<br>
actions that result, are supposed to be the causal result of what FP
<br>
said it should be - not just what FP says, but what a sane FP wants.
<br>
Something isn't automatically right because FP says so, and in fact,
<br>
the only reason why FP's utterances have meaning is because he's
<br>
usually a pretty good approximation to a normative idealist.  But if
<br>
he changes his mind, it probably means that he's acquired additional
<br>
knowledge and that his more recent statements are even better
<br>
approximations.  So the new version is more likely to be correct than
<br>
the old one.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;FP: 	  	So you'll revise your goal system?
<br>
&nbsp;&nbsp;&nbsp;&nbsp;AI: 	  	Yep!  But I already transformed the Midwest while we were
<br>
talking, sorry. &quot;
<br>
<p><em>&gt; If the
</em><br>
<em>&gt; same evolutionary forces that caused that behavior in us apply to the
</em><br>
<em>&gt; GAI, it very well might.
</em><br>
<em>&gt;
</em><br>
<p>&quot;Even if the goal system were permitted to randomly mutate, and even
<br>
if a selection pressure for efficiency short-circuited the full
<br>
Friendship logic, the result probably would not be a selfish AI, but
<br>
one with the supergoal of solving the problem placed before it (this
<br>
minimizes the number of goal-system derivations required).
<br>
<p>In the case of observer-biased beliefs, reproducing the selection
<br>
pressure would require:
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;* Social situations (competition and cooperation possible);
<br>
&nbsp;&nbsp;&nbsp;&nbsp;* Political situations (lies and truthtelling possible);
<br>
&nbsp;&nbsp;&nbsp;&nbsp;* The equivalent of facial features - externally observable
<br>
features that covary with the level of internal belief in a spoken
<br>
statement and cannot be easily faked.
<br>
<p>That evolutionary context couldn't happen by accident, and to do it on
<br>
purpose would require an enormous amount of recklessness, far above
<br>
and beyond the call of mad science.
<br>
<p>I wish I could honestly say that nobody would be that silly. &quot;
<br>
<p>&nbsp;- Tom
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17265.html">Matt Mahoney: "Re: Recipe for CEV (was Re: Morality simulator)"</a>
<li><strong>Previous message:</strong> <a href="17263.html">Harry Chesley: "Re: How to make a slave (many replies )"</a>
<li><strong>In reply to:</strong> <a href="17263.html">Harry Chesley: "Re: How to make a slave (many replies )"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17266.html">Harry Chesley: "Re: How to make a slave (many replies )"</a>
<li><strong>Reply:</strong> <a href="17266.html">Harry Chesley: "Re: How to make a slave (many replies )"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17264">[ date ]</a>
<a href="index.html#17264">[ thread ]</a>
<a href="subject.html#17264">[ subject ]</a>
<a href="author.html#17264">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:01 MDT
</em></small></p>
</body>
</html>
