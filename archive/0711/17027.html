<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: What best evidence for fast AI?</title>
<meta name="Author" content="Robin Hanson (rhanson@gmu.edu)">
<meta name="Subject" content="Re: What best evidence for fast AI?">
<meta name="Date" content="2007-11-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: What best evidence for fast AI?</h1>
<!-- received="Sat Nov 10 13:42:45 2007" -->
<!-- isoreceived="20071110204245" -->
<!-- sent="Sat, 10 Nov 2007 15:40:34 -0500" -->
<!-- isosent="20071110204034" -->
<!-- name="Robin Hanson" -->
<!-- email="rhanson@gmu.edu" -->
<!-- subject="Re: What best evidence for fast AI?" -->
<!-- id="0JRB00AGQ5FNQ1E0@caduceus1.gmu.edu" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="79ecaa350711100935l4e5d9f56x301f6ef10dbcd2d8@mail.gmail.co m" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Robin Hanson (<a href="mailto:rhanson@gmu.edu?Subject=Re:%20What%20best%20evidence%20for%20fast%20AI?"><em>rhanson@gmu.edu</em></a>)<br>
<strong>Date:</strong> Sat Nov 10 2007 - 13:40:34 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17028.html">Eliezer S. Yudkowsky: "Re: answers I'd like from an SI"</a>
<li><strong>Previous message:</strong> <a href="17026.html">Thomas McCabe: "Re: What best evidence for fast AI?"</a>
<li><strong>Maybe in reply to:</strong> <a href="17012.html">Robin Hanson: "What best evidence for fast AI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17041.html">Rolf Nelson: "Re: What best evidence for fast AI?"</a>
<li><strong>Reply:</strong> <a href="17041.html">Rolf Nelson: "Re: What best evidence for fast AI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17027">[ date ]</a>
<a href="index.html#17027">[ thread ]</a>
<a href="subject.html#17027">[ subject ]</a>
<a href="author.html#17027">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->

<body>
At 12:35 PM 11/10/2007, Rolf wrote:<br>
<blockquote type=cite class=cite cite="">From a decision-theory
perspective, the odds of AGI would have to be incredibly small to justify
the current low level of Friendly AI funding. </blockquote><br>
Maybe, but that isn't the issue I'm addressing.&nbsp; <br><br>
<blockquote type=cite class=cite cite="">You've probably heard the common
arguments for AGI, mostly it's about debunking counter-arguments to AGI
at this point.<br>
1. It's already been pointed out that the track-record of human invention
to match or outdo evolution, when &quot;compactness&quot; is not a
criterion, is very good. You've heard the flight analogy, allegedly many
experts were surprised by the Wright Brothers.</blockquote><br>
I'm not asking about the eventual achievement, but about the rate of
progress to expect.&nbsp; <br><br>
<blockquote type=cite class=cite cite="">2. When invention matches or
exceeds evolution, it's usually sudden. </blockquote><br>
I'm not questioning rapid change once a certain threshold is
reached.&nbsp; <br><br>
<blockquote type=cite class=cite cite="">3. Adjust for overconfidence
bias, if an expert says 95% confidence that AGI won't happen, then it's
probably less than that, unless it's part of a larger well-calibrated
model (which it isn't). <br>
4. Some people's algorithm seems to be, &quot;if it hasn't happened in
the last X years, then surely it won't happen in the next X years.&quot;
This is a *terrible* algorithm. ...<br>
5. Another poor algorithm: &quot;If someone predicts X will happen in 50
years, and it doesn't happen, then that means it will surely never
happen.&quot; ...</blockquote><br>
Surely you can see these are very weak arguments in favor of any
particular estimate.  <br><br>
<x-sigsep><p></x-sigsep>
Robin Hanson&nbsp; rhanson@gmu.edu&nbsp;
<a href="http://hanson.gmu.edu">http://hanson.gmu.edu</a> <br>
Research Associate, Future of Humanity Institute at Oxford
University<br>
Associate Professor of Economics, George Mason University<br>
MSN 1D3, Carow Hall, Fairfax VA 22030-4444<br>
703-993-2326&nbsp; FAX: 703-993-2323<br>
&nbsp; </body>

<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17028.html">Eliezer S. Yudkowsky: "Re: answers I'd like from an SI"</a>
<li><strong>Previous message:</strong> <a href="17026.html">Thomas McCabe: "Re: What best evidence for fast AI?"</a>
<li><strong>Maybe in reply to:</strong> <a href="17012.html">Robin Hanson: "What best evidence for fast AI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17041.html">Rolf Nelson: "Re: What best evidence for fast AI?"</a>
<li><strong>Reply:</strong> <a href="17041.html">Rolf Nelson: "Re: What best evidence for fast AI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17027">[ date ]</a>
<a href="index.html#17027">[ thread ]</a>
<a href="subject.html#17027">[ subject ]</a>
<a href="author.html#17027">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:00 MDT
</em></small></p>
</body>
</html>
