<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI</title>
<meta name="Author" content="Tim Freeman (tim@fungible.com)">
<meta name="Subject" content="Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI">
<meta name="Date" content="2007-11-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI</h1>
<!-- received="Wed Nov 21 06:49:42 2007" -->
<!-- isoreceived="20071121134942" -->
<!-- sent="Wed, 21 Nov 2007 04:24:09 -0700" -->
<!-- isosent="20071121112409" -->
<!-- name="Tim Freeman" -->
<!-- email="tim@fungible.com" -->
<!-- subject="Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI" -->
<!-- id="20071121134715.D7C7AD27A1@fungible.com" -->
<!-- inreplyto="74eb4bb50711201728i1d8e167eo779a138158ba4652@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tim Freeman (<a href="mailto:tim@fungible.com?Subject=Re:%20Building%20a%20friendly%20AI%20from%20a%20&quot;just%20do%20what%20I%20tell%20you&quot;%20AI"><em>tim@fungible.com</em></a>)<br>
<strong>Date:</strong> Wed Nov 21 2007 - 04:24:09 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17193.html">sl4.20.pris@spamgourmet.com: "Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI"</a>
<li><strong>Previous message:</strong> <a href="17191.html">Stathis Papaioannou: "Re: How to make a slave (was: Building a friendly AI)"</a>
<li><strong>In reply to:</strong> <a href="17177.html">sl4.20.pris@spamgourmet.com: "Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17193.html">sl4.20.pris@spamgourmet.com: "Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI"</a>
<li><strong>Reply:</strong> <a href="17193.html">sl4.20.pris@spamgourmet.com: "Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI"</a>
<li><strong>Reply:</strong> <a href="17196.html">Wei Dai: "Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI"</a>
<li><strong>Reply:</strong> <a href="../0712/17359.html">Joshua Fox: "Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17192">[ date ]</a>
<a href="index.html#17192">[ thread ]</a>
<a href="subject.html#17192">[ subject ]</a>
<a href="author.html#17192">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
From: <a href="mailto:sl4.20.pris@spamgourmet.com?Subject=Re:%20Building%20a%20friendly%20AI%20from%20a%20&quot;just%20do%20what%20I%20tell%20you&quot;%20AI">sl4.20.pris@spamgourmet.com</a>
<br>
Date: Sat, 17 Nov 2007 19:46:24 -0200
<br>
<em>&gt;So we go to the OAI [=obedient AI] and say: &quot;Tell me how I can build
</em><br>
<em>&gt;a friendly AI in a manner that I can prove and understand that it
</em><br>
<em>&gt;will be friendly.&quot;
</em><br>
<p>There might be something useful there.  You'd need to have a clear
<br>
definition of &quot;friendly&quot; to have much confidence that the result was
<br>
what you wanted.  A crisp definition of &quot;friendly&quot; would be a
<br>
specification that you understand, so the question you're asking your
<br>
OAI would be &quot;how do I make a practical implementation of this
<br>
specification&quot;?
<br>
<p>From: <a href="mailto:sl4.20.pris@spamgourmet.com?Subject=Re:%20Building%20a%20friendly%20AI%20from%20a%20&quot;just%20do%20what%20I%20tell%20you&quot;%20AI">sl4.20.pris@spamgourmet.com</a>
<br>
<em>&gt;* The OAI is not able and doesn't want to do anything besides
</em><br>
<em>&gt;answering questions in a restricted way: it will output media
</em><br>
<em>&gt;(text/pictures/videos/audio). Think of it as a glorified calculator AI
</em><br>
<em>&gt;(GCAI). The reason this is so, is because this is the way it was
</em><br>
<em>&gt;designed.
</em><br>
<em>&gt;* It will not go into an infinite loop or decide that it needs to turn
</em><br>
<em>&gt;the whole earth/universum into computronium if it is faced with a
</em><br>
<em>&gt;question beyond it's capabilities. If you use your desk calculator and
</em><br>
<em>&gt;press the key &quot;pi&quot; the calculator doesn't start an infinite loop in
</em><br>
<em>&gt;order to calculate all digits of pi, but it just outputs pi to some
</em><br>
<em>&gt;digits. If you ask the GCAI:
</em><br>
<em>&gt;- &quot;Calculate pi&quot; it would ask back:
</em><br>
<em>&gt;- &quot;How many digits do you want?&quot;
</em><br>
<em>&gt;- &quot;I want them all!&quot;
</em><br>
<em>&gt;- &quot;Sorry, I cannot do that.&quot;
</em><br>
<em>&gt;- &quot;Ok, give me the first 3^^^3 digits.&quot;
</em><br>
<em>&gt;- &quot;Sorry, but the universe will be dead before I can finish this task.
</em><br>
<p>Then the dialogue can continue:
<br>
<p>- &quot;How many digits of pi could you compute in a year?&quot;
<br>
- The calculator gives some large number, say M.  (The calculator may
<br>
&nbsp;&nbsp;not have a self-concept, so the word &quot;you&quot; in the question might not
<br>
&nbsp;&nbsp;make sense.  In that case the question would be &quot;How many digits of
<br>
&nbsp;&nbsp;pi could a device constructed according to these plans compute in a
<br>
&nbsp;&nbsp;year?&quot;, then you give the calculator its own plans as part of the
<br>
&nbsp;&nbsp;question.) 
<br>
- &quot;Give me plans for a device that can compute 2*M digits of pi in a
<br>
&nbsp;&nbsp;year.&quot;
<br>
- The calculator gives detailed plans for some device.
<br>
- &quot;Give me plans for a device that can compute 4*M digits of pi in a
<br>
&nbsp;&nbsp;year.&quot;
<br>
- The calculator takes a little longer, and outputs a new bunch of
<br>
&nbsp;&nbsp;plans for a more complex device.
<br>
- The user continues the previous line of questioning until computing the next
<br>
&nbsp;&nbsp;batch of plans takes longer than the user is willing to wait for an
<br>
&nbsp;&nbsp;answer. 
<br>
- The (idiot) user then implements the last set of plans received, and
<br>
&nbsp;&nbsp;starts the new device.  Since the calculator was smarter than the
<br>
&nbsp;&nbsp;user, the user can't understand the plans, so we don't benefit from
<br>
&nbsp;&nbsp;the user's oversight.
<br>
- The new device achieves its goal by converting the Earth, among
<br>
&nbsp;&nbsp;other things, into computronium.
<br>
<p>The calculator was hoped to be safe because by hypothesis it can't
<br>
self-modify.  Self-modification isn't the main issue, because
<br>
fundamentally nothing in the world has a &quot;self&quot; to start with.  The
<br>
concept of a persistent, modifiable &quot;self&quot; assumes that main
<br>
consequence of the past is a future that resembles the past.  We then
<br>
identify the future and the past, and label some of the
<br>
approximately-shared features as the &quot;self&quot; of whatever entity we're
<br>
talking about, and if the future &quot;self&quot; is different from the past
<br>
&quot;self&quot; we talk about self-modification.  Once you get an AI doing
<br>
engineering work, this model stops working because there's little
<br>
reason to believe the future will closely resemble the past.  
<br>
<p>The real issue is, when you get an AI to do engineering work, you need
<br>
to ensure that the AI understands its social context well enough so
<br>
the consequences of the engineering work satisfy the other unstated
<br>
desires of its master, and its master's in-group, which would ideally
<br>
be all humans.  (Realistically, all large human projects seem to be
<br>
about dominating other humans, so it seems unlikely that the in-group
<br>
will consist of all humans.  It's better to have some survivors than
<br>
no survivors, so a probably-not-all-inclusive ingroup is a smaller
<br>
problem than the likelihood of killing everybody.)
<br>
<p>Saying &quot;the idiot user shouldn't have implemented a plan he didn't
<br>
understand&quot; doesn't work.  Humans can't tell with any reliability
<br>
whether they accurately understand something.  There is unavoidable
<br>
risk here, but eventually, if the AI is smarter than the humans, we
<br>
have to rely on the AI understanding the humans, not the humans
<br>
understanding the AI.
<br>
<p>I have some ideas about how to do this in the paper at
<br>
<a href="http://www.fungible.com/respect/index.html">http://www.fungible.com/respect/index.html</a>.  Unfortunately the paper
<br>
needs revision and hasn't yet made sense to someone who I didn't
<br>
explain it to personally.  Maybe I'll be able to make it readable over
<br>
Thanksgiving.
<br>
<p>In that paper I specify a machine that will infer the goals of a given
<br>
group of people and pursue a weighted average of those goals, given
<br>
sufficient training data about the perceptions and voluntary actions
<br>
of those people.  Your voluntary actions are the contractions of your
<br>
voluntary muscles, so the problem of providing the training data is
<br>
conceptually simpler than the problem we started with.
<br>
<p>Unfortunately, I can't prove that a good implementation of this
<br>
wouldn't kill everybody because I can't prove the original group of
<br>
people didn't want to kill everybody.  All I can do is point at the
<br>
design and say that by construction, with sufficient (unobtainable)
<br>
computational resources, it would apparently do what people want.  I
<br>
can't think of anything else useful to say about it.
<br>
<p><pre>
-- 
Tim Freeman               <a href="http://www.fungible.com">http://www.fungible.com</a>           <a href="mailto:tim@fungible.com?Subject=Re:%20Building%20a%20friendly%20AI%20from%20a%20&quot;just%20do%20what%20I%20tell%20you&quot;%20AI">tim@fungible.com</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17193.html">sl4.20.pris@spamgourmet.com: "Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI"</a>
<li><strong>Previous message:</strong> <a href="17191.html">Stathis Papaioannou: "Re: How to make a slave (was: Building a friendly AI)"</a>
<li><strong>In reply to:</strong> <a href="17177.html">sl4.20.pris@spamgourmet.com: "Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17193.html">sl4.20.pris@spamgourmet.com: "Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI"</a>
<li><strong>Reply:</strong> <a href="17193.html">sl4.20.pris@spamgourmet.com: "Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI"</a>
<li><strong>Reply:</strong> <a href="17196.html">Wei Dai: "Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI"</a>
<li><strong>Reply:</strong> <a href="../0712/17359.html">Joshua Fox: "Re: Building a friendly AI from a &quot;just do what I tell you&quot; AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17192">[ date ]</a>
<a href="index.html#17192">[ thread ]</a>
<a href="subject.html#17192">[ subject ]</a>
<a href="author.html#17192">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:00 MDT
</em></small></p>
</body>
</html>
