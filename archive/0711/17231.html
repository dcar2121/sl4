<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=gb2312">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: UCaRtMaAI paper</title>
<meta name="Author" content="Wei Dai (weidai@weidai.com)">
<meta name="Subject" content="Re: UCaRtMaAI paper">
<meta name="Date" content="2007-11-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: UCaRtMaAI paper</h1>
<!-- received="Fri Nov 23 18:47:27 2007" -->
<!-- isoreceived="20071124014727" -->
<!-- sent="Fri, 23 Nov 2007 17:45:15 -0800" -->
<!-- isosent="20071124014515" -->
<!-- name="Wei Dai" -->
<!-- email="weidai@weidai.com" -->
<!-- subject="Re: UCaRtMaAI paper" -->
<!-- id="D0719B5256614765A37C40B628F8477F@weidaim1" -->
<!-- charset="gb2312" -->
<!-- inreplyto="20071123212204.D7885D262C@fungible.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Wei Dai (<a href="mailto:weidai@weidai.com?Subject=Re:%20UCaRtMaAI%20paper"><em>weidai@weidai.com</em></a>)<br>
<strong>Date:</strong> Fri Nov 23 2007 - 18:45:15 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17232.html">Stefan Pernar: "Re: Morality simulator"</a>
<li><strong>Previous message:</strong> <a href="17230.html">Thomas McCabe: "Re: Morality simulator"</a>
<li><strong>In reply to:</strong> <a href="17224.html">Tim Freeman: "Re: UCaRtMaAI paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17245.html">Tim Freeman: "Re: UCaRtMaAI paper"</a>
<li><strong>Reply:</strong> <a href="17245.html">Tim Freeman: "Re: UCaRtMaAI paper"</a>
<li><strong>Reply:</strong> <a href="17247.html">Tim Freeman: "What is stability in a FAI? (was Re: UCaRtMaAI paper)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17231">[ date ]</a>
<a href="index.html#17231">[ thread ]</a>
<a href="subject.html#17231">[ subject ]</a>
<a href="author.html#17231">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Tim Freeman wrote:
<br>
<em>&gt; So far as I can tell, you are arguing that my scheme isn't perfect.  I
</em><br>
<em>&gt; agree with that statement, but in the absence of a perfect or even a
</em><br>
<em>&gt; conjectured-to-be-better scheme, I'm not very interested.
</em><br>
<p>My conjectured-to-be-better scheme is to not build an AGI until we're more 
<br>
sure that we know what we are doing. My real point with the &quot;answers I'd 
<br>
like&quot; is that we are really quite far from knowing what we are doing with 
<br>
regard to AGI. It's not just that an optimizing process can't answer those 
<br>
questions for us, but it also can't answer those questions for itself. Since 
<br>
we don't know what the right answers are yet, we don't want to hard code 
<br>
answers into an AGI in a way that won't allow them to be changed later. 
<br>
(Perhaps this doesn't apply to your scheme, but see below.)
<br>
<p><em>&gt; Do you have any ideas about how to help people without interfering
</em><br>
<em>&gt; with the ability to infer what they want from their behavior?
</em><br>
<p>I made the suggestion of giving each person a fixed quota of resources. Is 
<br>
that something you've considered already?
<br>
<p><em>&gt; There is a collection of unit tests at
</em><br>
<em>&gt; <a href="http://www.fungible.com/respect/code/#tests">http://www.fungible.com/respect/code/#tests</a>.  They are all gross
</em><br>
<em>&gt; simplifications, of course.  The most complex ones are baby catching
</em><br>
<em>&gt; at <a href="http://www.fungible.com/respect/code/baby_catch_test.py.txt">http://www.fungible.com/respect/code/baby_catch_test.py.txt</a> and
</em><br>
<em>&gt; grocery shopping at
</em><br>
<em>&gt; <a href="http://www.fungible.com/respect/code/grocery_test.py.txt">http://www.fungible.com/respect/code/grocery_test.py.txt</a>.  Do you have
</em><br>
<em>&gt; an obvious-to-us conclusion that you'd like to see that's about as
</em><br>
<em>&gt; complex as these two?
</em><br>
<p>I think you need to write an explanation of what those unit tests are doing. 
<br>
I'm not able to figure it out from the code. You might want to walk through 
<br>
an example step by step. What are the agents' utility functions and beliefs, 
<br>
what do the agents do, what does the AI infer, etc. I might be able to 
<br>
suggest more scenarios for you to test once I have a better intuitive 
<br>
understand of how the AI works.
<br>
<p><em>&gt; That's true in some scenarios but not others.  The AI has a time
</em><br>
<em>&gt; horizon and all influences it has past that time horizon are done
</em><br>
<em>&gt; because the humans have a desire, before the time horizon, to
</em><br>
<em>&gt; reasonably expect those consequences after the time horizon.
</em><br>
<em>&gt; So if the time horizon is one hour from now, and AI is able to figure
</em><br>
<em>&gt; out that right now we want a new AI after one hour from now that
</em><br>
<em>&gt; implements some shiny new moral philosophy, it could arrange for that.
</em><br>
<em>&gt; We could lose if the present AI has bugs that prevent it from seeing
</em><br>
<em>&gt; that we want the new moral philosophy to be in effect past the time
</em><br>
<em>&gt; horizon.
</em><br>
<p>Ok, I didn't realize that one implication of a limited planning horizon is 
<br>
that the AI will allow itself to be replaced by another AI. That brings up 
<br>
another set of problems though, which is how to make sure there aren't 
<br>
loopholes that will allow the AI to be replaced by an unfriendly one that 
<br>
serves someone or some group's self interests.
<br>
<p>To take the simplest example, suppose I get a group of friends together and 
<br>
we all tell the AI, &quot;at the end of this planning period please replace 
<br>
yourself with an AI that serves only us.&quot; The rest of humanity does not know 
<br>
about this, so they don't do anything that would let the AI infer that they 
<br>
would assign this outcome a low utility. I don't understand your design well 
<br>
enough to claim that this exploit would definitely work, but neither do I 
<br>
see an argument for why such loopholes do not exist.
<br>
<p><em>&gt; I feel comfortable making an arbitrary choice among the best known
</em><br>
<em>&gt; alternatives.
</em><br>
<p>Among the infinite number of algorithms for averaging people's utility 
<br>
functions, you've somehow picked one. How did you pick it? Given that the 
<br>
vast majority of those algorithms are not among the best known alternatives, 
<br>
what makes you think that the algorithm you picked *is* among the best known 
<br>
alternatives?
<br>
<p>For example, consider explicit calibration as an alternative. Design a 
<br>
standard basket of goods and services, and calibrate each person's utility 
<br>
function so that his utility of obtaining one standard basket is 1, and his 
<br>
utility of obtaining two standard baskets is 2. To me, this seems a lot more 
<br>
likely to be at least somewhat fair than an algorithm that relies on the 
<br>
side effects of integer overflow.
<br>
&nbsp;
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17232.html">Stefan Pernar: "Re: Morality simulator"</a>
<li><strong>Previous message:</strong> <a href="17230.html">Thomas McCabe: "Re: Morality simulator"</a>
<li><strong>In reply to:</strong> <a href="17224.html">Tim Freeman: "Re: UCaRtMaAI paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17245.html">Tim Freeman: "Re: UCaRtMaAI paper"</a>
<li><strong>Reply:</strong> <a href="17245.html">Tim Freeman: "Re: UCaRtMaAI paper"</a>
<li><strong>Reply:</strong> <a href="17247.html">Tim Freeman: "What is stability in a FAI? (was Re: UCaRtMaAI paper)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17231">[ date ]</a>
<a href="index.html#17231">[ thread ]</a>
<a href="subject.html#17231">[ subject ]</a>
<a href="author.html#17231">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:01 MDT
</em></small></p>
</body>
</html>
