<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Why friendly AI (FAI) won't work</title>
<meta name="Author" content="Kaj Sotala (xuenay@gmail.com)">
<meta name="Subject" content="Re: Why friendly AI (FAI) won't work">
<meta name="Date" content="2007-11-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Why friendly AI (FAI) won't work</h1>
<!-- received="Thu Nov 29 12:12:04 2007" -->
<!-- isoreceived="20071129191204" -->
<!-- sent="Thu, 29 Nov 2007 21:09:46 +0200" -->
<!-- isosent="20071129190946" -->
<!-- name="Kaj Sotala" -->
<!-- email="xuenay@gmail.com" -->
<!-- subject="Re: Why friendly AI (FAI) won't work" -->
<!-- id="6a13bb8f0711291109y5c2a7786k69f86286f1f1f29b@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="474E1941.5090908@acm.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Kaj Sotala (<a href="mailto:xuenay@gmail.com?Subject=Re:%20Why%20friendly%20AI%20(FAI)%20won't%20work"><em>xuenay@gmail.com</em></a>)<br>
<strong>Date:</strong> Thu Nov 29 2007 - 12:09:46 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17322.html">Jeff Herrlich: "Re: How to make a slave (was: Building a friendly AI)"</a>
<li><strong>Previous message:</strong> <a href="17320.html">Randall Randall: "Re: How to make a slave (was: Building a friendly AI)"</a>
<li><strong>In reply to:</strong> <a href="17310.html">Harry Chesley: "Re: Why friendly AI (FAI) won't work"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17327.html">Stathis Papaioannou: "Re: Why friendly AI (FAI) won't work"</a>
<li><strong>Reply:</strong> <a href="17327.html">Stathis Papaioannou: "Re: Why friendly AI (FAI) won't work"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17321">[ date ]</a>
<a href="index.html#17321">[ thread ]</a>
<a href="subject.html#17321">[ subject ]</a>
<a href="author.html#17321">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 11/29/07, Harry Chesley &lt;<a href="mailto:chesley@acm.org?Subject=Re:%20Why%20friendly%20AI%20(FAI)%20won't%20work">chesley@acm.org</a>&gt; wrote:
<br>
<em>&gt; Robin Lee Powell wrote:
</em><br>
<em>&gt; &gt; OMFG has that topic been done to death. Read the archives on AI
</em><br>
<em>&gt; &gt; boxing.
</em><br>
<em>&gt;
</em><br>
<em>&gt; And nothing that I've read about it has yet convinced me. What I've seen
</em><br>
<p>I'm not entirely sure I agree with the traditional AI boxing
<br>
arguments, either - even if boxing wasn't a bulletproof method of
<br>
designing a Friendly AI, it'd sure be a heck of a lot *easier* to
<br>
build an AI that was content to remain in the box than to build an AI
<br>
that knew what was the morally correct course of action in all
<br>
situations.
<br>
<p>However, even if you rejected the AI boxing arguments, there's still a
<br>
crucial flaw with the idea of just restricting the AI's output, one
<br>
which I find much easier to accept. Quoted from
<br>
<a href="http://www.saunalahti.fi/~tspro1/objections.html#advisors">http://www.saunalahti.fi/~tspro1/objections.html#advisors</a> :
<br>
<p>&quot;Objection 13: Couldn't AIs be built as pure advisors, so they
<br>
wouldn't do anything themselves? That way, we wouldn't need to worry
<br>
about Friendly AI.
<br>
<p>Answer: The problem with this argument is the inherent slowness in all
<br>
human activity - things are much more /efficient/ if you can cut
<br>
humans out of the loop, and the system can carry out decisions and
<br>
formulate objectives on its own. Consider, for instance, two competing
<br>
corporations (or nations), each with their own advisor AI that only
<br>
carries out the missions it is given. Even if the advisor was the one
<br>
collecting all the information for the humans (a dangerous situation
<br>
in itself), the humans would have to spend time making the actual
<br>
decisions of how to have the AI act in response to that information.
<br>
If the competitor had turned over all the control to their own,
<br>
independently acting AI, it could react much faster than the one that
<br>
relied on the humans to give all the assignments. Therefore the
<br>
temptation would be immense to build an AI that could act without
<br>
human intervention.
<br>
<p>Also, there are numerous people who would /want/ an independently
<br>
acting AI, for the simple reason that an AI built only to carry out
<br>
goals given to it by humans could be used for vast harm - while an AI
<br>
built to actually care for humanity could act in humanity's best
<br>
interests, in a neutral and bias-free fashion. Therefore, in either
<br>
case, the motivation to build independently-acting AIs is there, and
<br>
the cheaper computing power becomes, the easier it will be for even
<br>
small groups to build AIs.
<br>
<p>It doesn't matter if an AI's Friendliness could trivially be
<br>
guaranteed by giving it a piece of electronic cheese, if nobody cares
<br>
about Friendliness enough to think about giving it some cheese, or if
<br>
giving the cheese costs too much in terms of what you could achieve
<br>
otherwise. Any procedures which rely on handicapping an AI enough to
<br>
make it powerless also handicap it enough to severly restrict its
<br>
usefulness to most potential funders. Eventually there will be
<br>
somebody who chooses not to handicap their own AI, and then the
<br>
guaranteed-to-be-harmless AI will end up dominated by the more
<br>
powerful AI.&quot;
<br>
<p><p><pre>
-- 
<a href="http://www.saunalahti.fi/~tspro1/">http://www.saunalahti.fi/~tspro1/</a> | <a href="http://xuenay.livejournal.com/">http://xuenay.livejournal.com/</a>
Organizations worth your time:
<a href="http://www.intelligence.org/">http://www.intelligence.org/</a> | <a href="http://www.crnano.org/">http://www.crnano.org/</a> | <a href="http://lifeboat.com/">http://lifeboat.com/</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17322.html">Jeff Herrlich: "Re: How to make a slave (was: Building a friendly AI)"</a>
<li><strong>Previous message:</strong> <a href="17320.html">Randall Randall: "Re: How to make a slave (was: Building a friendly AI)"</a>
<li><strong>In reply to:</strong> <a href="17310.html">Harry Chesley: "Re: Why friendly AI (FAI) won't work"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17327.html">Stathis Papaioannou: "Re: Why friendly AI (FAI) won't work"</a>
<li><strong>Reply:</strong> <a href="17327.html">Stathis Papaioannou: "Re: Why friendly AI (FAI) won't work"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17321">[ date ]</a>
<a href="index.html#17321">[ thread ]</a>
<a href="subject.html#17321">[ subject ]</a>
<a href="author.html#17321">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:01 MDT
</em></small></p>
</body>
</html>
