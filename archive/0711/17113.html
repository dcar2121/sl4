<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=WINDOWS-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Morality simulator</title>
<meta name="Author" content="Stefan Pernar (stefan.pernar@gmail.com)">
<meta name="Subject" content="Re: Morality simulator">
<meta name="Date" content="2007-11-15">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Morality simulator</h1>
<!-- received="Thu Nov 15 05:11:48 2007" -->
<!-- isoreceived="20071115121148" -->
<!-- sent="Thu, 15 Nov 2007 20:09:33 +0800" -->
<!-- isosent="20071115120933" -->
<!-- name="Stefan Pernar" -->
<!-- email="stefan.pernar@gmail.com" -->
<!-- subject="Re: Morality simulator" -->
<!-- id="944947f20711150409o2ab29af2p3c814ca8d4b01d60@mail.gmail.com" -->
<!-- charset="WINDOWS-1252" -->
<!-- inreplyto="8760b3f20711140733m39899de1l55ba17511128a686@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Stefan Pernar (<a href="mailto:stefan.pernar@gmail.com?Subject=Re:%20Morality%20simulator"><em>stefan.pernar@gmail.com</em></a>)<br>
<strong>Date:</strong> Thu Nov 15 2007 - 05:09:33 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17114.html">Gwern Branwen: "Re: answers I'd like from an SI"</a>
<li><strong>Previous message:</strong> <a href="17112.html">Stathis Papaioannou: "Re: answers I'd like, part 2"</a>
<li><strong>In reply to:</strong> <a href="17106.html">Joshua Fox: "Re: Morality simulator"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17133.html">Joshua Fox: "Re: Morality simulator"</a>
<li><strong>Reply:</strong> <a href="17133.html">Joshua Fox: "Re: Morality simulator"</a>
<li><strong>Reply:</strong> <a href="17134.html">Stefan Pernar: "Re: Morality simulator"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17113">[ date ]</a>
<a href="index.html#17113">[ thread ]</a>
<a href="subject.html#17113">[ subject ]</a>
<a href="author.html#17113">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Nov 14, 2007 11:33 PM, Joshua Fox &lt;<a href="mailto:joshua@joshuafox.com?Subject=Re:%20Morality%20simulator">joshua@joshuafox.com</a>&gt; wrote:
<br>
<p><em>&gt; Bill,
</em><br>
<em>&gt; &gt;When it comes to explaining the evolution of human cooperation,
</em><br>
<em>&gt; &gt; researchers have traditionally looked to the iterated Prisoner's
</em><br>
<em>&gt; &gt; Dilemma (IPD) game as the paradigm
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes, IPD-type simulations are similar to one of the sorts of &quot;morality
</em><br>
<em>&gt; simulator&quot; I'm thinking about. But the &quot;morality simulator&quot; would
</em><br>
<em>&gt; focus on utility functions which include morality, whatever that means
</em><br>
<em>&gt; -- perhaps the assigning of value to others' welfare in addition to
</em><br>
<em>&gt; one's own.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Towards the end of Steve Omohundro's recent Stanford talk
</em><br>
<em>&gt; (
</em><br>
<em>&gt; <a href="http://www.intelligence.org/blog/2007/11/09/steve-omohundro-at-stanford-october-24-2007/">http://www.intelligence.org/blog/2007/11/09/steve-omohundro-at-stanford-october-24-2007/</a>
</em><br>
<em>&gt; )
</em><br>
<em>&gt; he mentioned the need for a mathematics of values. This is also
</em><br>
<em>&gt; related to the &quot;morality simulator&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; He gave the example of the &quot;Trolley Problem&quot; -- a moral
</em><br>
<em>&gt; thought-experiment which could also be developed as simple computer
</em><br>
<em>&gt; simulation. (Near-trivial in this case, and simply allowing the user
</em><br>
<em>&gt; to explicitly play with the utility functions rather than using
</em><br>
<em>&gt; implicit utility functions bundled into their intuition.)
</em><br>
<em>&gt;
</em><br>
<em>&gt; Joshua
</em><br>
<em>&gt;
</em><br>
<p>I have been toying with such an idea lately and the preliminary results are
<br>
as following:
<br>
<p>Assuming the rationally unobjectionable utility
<br>
function&lt;<a href="http://www.jame5.com/?p=45">http://www.jame5.com/?p=45</a>&gt;of 'ensure continued co-existence'
<br>
one must assume it to be at least the
<br>
implicit guiding principle i.e utility function of at least every human
<br>
being. But who is running around chanting 'Must. Ensure. Continued.
<br>
Co-existence.'? Not many. It follows that the implicit utility function
<br>
Fi(i) generally diverges from the explicit utility function Fe(i) in humans
<br>
and that those whose Fe(i) best approximates Fi(i) have the best chance for
<br>
ensuring continued co-existence.
<br>
<p>Fe(i) can be best understood as an evolved
<br>
belief&lt;<a href="http://www.jame5.com/?p=40">http://www.jame5.com/?p=40</a>&gt;in regards to what should guide an
<br>
individual's actions while Fi(i) is what
<br>
rationally should guide an individual's actions.
<br>
<p>Not long ago Eliezer proposed two
<br>
philosophers&lt;<a href="http://www.overcomingbias.com/2007/11/fake-morality.html">http://www.overcomingbias.com/2007/11/fake-morality.html</a>&gt;with
<br>
the following statements:
<br>
<p>Philosopher 1: &quot;You should be selfish, because when people set out to
<br>
improve society, they meddle in their neighbors' affairs and pass laws and
<br>
seize control and make everyone unhappy. Take whichever job that pays the
<br>
most money: the reason the job pays more is that the efficient market thinks
<br>
it produces more value than its alternatives. Take a job that pays less, and
<br>
you're second-guessing what the market thinks will benefit society most.&quot;
<br>
<p>Philosopher 2: &quot;You should be altruistic, because the world is an iterated
<br>
Prisoner's Dilemma, and the strategy that fares best is Tit for Tat with
<br>
initial cooperation. People don't *like* jerks. Nice guys really do finish
<br>
first. Studies show that people who contribute to society and have a sense
<br>
of meaning in their lives, are happier than people who don't; being selfish
<br>
will only make you unhappy in the long run.&quot;
<br>
<p>Philosopher 1 is promoting altruism on the basis of selfishness
<br>
Philosopher 2 is promoting selfishness on the basis of altruism
<br>
<p>It is a contradiction - a paradox. But only in thought – not in reality.
<br>
What is actually taking place, is that both philosophers have intuitively
<br>
realized part of Fi(i) and are merely rationalizing differently as to why to
<br>
change their respective Fe(i).
<br>
<p>The first one by wrongly applying the term selfishness on the fallacy that a
<br>
higher paid job contributes only to his personal continued existence by
<br>
giving him more resources while in reality it contributes to ensuring
<br>
continued co-existence because he is taking the job that is considered to
<br>
benefit society the most.
<br>
<p>The second one by wrongly applying the term altruistic on the fallacy that
<br>
his recommendations are detrimental to his personal continued existence due
<br>
to loosing resources by being Mr nice guy while it actually contributes to
<br>
ensuring continued co-existence as it not only benefits him but other people
<br>
around him as well.
<br>
<p>The solution thus becomes that the intuitive concepts of altruism and
<br>
selfishness are rather worthless.
<br>
<p>An altruist giving up resources in a way that would lead to a reduction in
<br>
his personal continued existence would be irrationally acting against the
<br>
universal utility function thus being detrimental to all other agents not
<br>
only himself.
<br>
<p>An egoist acting truly selfish would use resources in a way that leads to
<br>
sub-optimal usage of resources towards maximizing the universal utility
<br>
function thus being detrimental to himself and not only all other agents.
<br>
<p>It follows that in reality there is neither altruistic nor egoistic behavior
<br>
- just irrational and rational behavior.
<br>
<pre>
---
Concrete specs for a evolutionary simulation to follow. I predict that that
agent that has Fi(i) = Fe(i) will do best.
-- 
Stefan Pernar
3-E-101 Silver Maple Garden
#6 Cai Hong Road, Da Shan Zi
Chao Yang District
100015 Beijing
P.R. CHINA
Mobil: +86 1391 009 1931
Skype: Stefan.Pernar
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17114.html">Gwern Branwen: "Re: answers I'd like from an SI"</a>
<li><strong>Previous message:</strong> <a href="17112.html">Stathis Papaioannou: "Re: answers I'd like, part 2"</a>
<li><strong>In reply to:</strong> <a href="17106.html">Joshua Fox: "Re: Morality simulator"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17133.html">Joshua Fox: "Re: Morality simulator"</a>
<li><strong>Reply:</strong> <a href="17133.html">Joshua Fox: "Re: Morality simulator"</a>
<li><strong>Reply:</strong> <a href="17134.html">Stefan Pernar: "Re: Morality simulator"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17113">[ date ]</a>
<a href="index.html#17113">[ thread ]</a>
<a href="subject.html#17113">[ subject ]</a>
<a href="author.html#17113">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:00 MDT
</em></small></p>
</body>
</html>
