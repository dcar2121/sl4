<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=gb2312">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: UCaRtMaAI paper</title>
<meta name="Author" content="Wei Dai (weidai@weidai.com)">
<meta name="Subject" content="Re: UCaRtMaAI paper">
<meta name="Date" content="2007-11-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: UCaRtMaAI paper</h1>
<!-- received="Thu Nov 22 19:27:14 2007" -->
<!-- isoreceived="20071123022714" -->
<!-- sent="Thu, 22 Nov 2007 18:24:43 -0800" -->
<!-- isosent="20071123022443" -->
<!-- name="Wei Dai" -->
<!-- email="weidai@weidai.com" -->
<!-- subject="Re: UCaRtMaAI paper" -->
<!-- id="CA3E8753336F4CCBA36B13CBBA929E75@weidaim1" -->
<!-- charset="gb2312" -->
<!-- inreplyto="20071123002425.AACA8D27A1@fungible.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Wei Dai (<a href="mailto:weidai@weidai.com?Subject=Re:%20UCaRtMaAI%20paper"><em>weidai@weidai.com</em></a>)<br>
<strong>Date:</strong> Thu Nov 22 2007 - 19:24:43 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17214.html">Stefan Pernar: "Re: Morality simulator"</a>
<li><strong>Previous message:</strong> <a href="17212.html">Tim Freeman: "Re: UCaRtMaAI paper"</a>
<li><strong>In reply to:</strong> <a href="17212.html">Tim Freeman: "Re: UCaRtMaAI paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17224.html">Tim Freeman: "Re: UCaRtMaAI paper"</a>
<li><strong>Reply:</strong> <a href="17224.html">Tim Freeman: "Re: UCaRtMaAI paper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17213">[ date ]</a>
<a href="index.html#17213">[ thread ]</a>
<a href="subject.html#17213">[ subject ]</a>
<a href="author.html#17213">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Tim Freeman wrote:
<br>
<em>&gt; Do you know of a reasonable prior I could have used that dominates the
</em><br>
<em>&gt; speed prior without breaking the algorithm?
</em><br>
<p>Tim, have you read my recent posts titled &quot;answers I'd like from an SI&quot; and 
<br>
&quot;answers I'd like, part 2&quot;? I think the problem is not just picking a prior, 
<br>
but that we still don't understand the nature of induction well enough. I 
<br>
think there are probably some insights beyond Solomonoff Induction that we 
<br>
are missing.
<br>
<p><em>&gt; The AI might want to periodically
</em><br>
<em>&gt; temporarily stop helping you so it can get more information about your
</em><br>
<em>&gt; utility function and make better decisions about how to prioritize.
</em><br>
<p>I don't think that helps too much, because if everyone knows that the AI 
<br>
will stop helping only temporarily, they will take that into account and 
<br>
still not act in ways that reveal their true utilities.
<br>
<p><em>&gt; The same issue arises when raising kids -- if you give them too much
</em><br>
<em>&gt; then everybody involved loses all sense of what's important.  It's an
</em><br>
<em>&gt; essential issue with helping people, not something specific about this
</em><br>
<em>&gt; algorithm.
</em><br>
<p>No, it is something specific about this algorithm. Suppose the AI instead 
<br>
gives each person a fixed quota of resources, and tells him it will only 
<br>
help him until his quota is used up. This would be similar to giving your 
<br>
children fixed trust funds instead of helping whichever child you think 
<br>
needs your help most (which encourages them to exaggerate how much help they 
<br>
need). (Caveat: I haven't thought this through. It might solve this problem, 
<br>
but have other bad consequences.)
<br>
<p><em>&gt; The main issue here seems to be that I put a plausible algorithm for
</em><br>
<em>&gt; &quot;do-what-we-want&quot; on the table, and we don't have any other
</em><br>
<em>&gt; specification of &quot;do-what-we-want&quot; so there's no way to judge whether
</em><br>
<em>&gt; the algorithm is any good.  I can see no approaches to solving that
</em><br>
<em>&gt; problem other than implementing and running some practical
</em><br>
<em>&gt; approximation to the algorithm.  This seems unsafe, but less unsafe
</em><br>
<em>&gt; than a bunch of other AGI projects presently in progress that don't
</em><br>
<em>&gt; have a model of friendliness.  I would welcome any ideas.
</em><br>
<p>That's why I think your paper is more valuable as an illustration of how 
<br>
hard friendliness is, by showing that even a non-practical specification of 
<br>
&quot;do-what-we-want&quot; is non-trivial. Maybe showing your paper to other AGI 
<br>
projects will help convince them that pursuing AGI before solving 
<br>
friendliness is not a good idea.
<br>
<p><em>&gt; Yes, sometimes the AI will decide arbitrarily because the situation
</em><br>
<em>&gt; really is ambiguous.  If it gets plausible answers to the important
</em><br>
<em>&gt; questions, I'll be satisfied with it.  People really are dying from
</em><br>
<em>&gt; many different things, the world is burning in places, etc., so there
</em><br>
<em>&gt; are lots of obvious conclusions to draw about interpersonal comparison
</em><br>
<em>&gt; of utilities.
</em><br>
<p>First, you haven't showed that the AI will actually draw the obvious-to-us 
<br>
conclusions correctly. Second, if we eventually discover a moral philosophy 
<br>
that is a big improvement over what is hard coded into the AI, we are 
<br>
screwed because we won't be able to reason with it and get it to change.
<br>
<p><em>&gt; I agree about having an infinite number of algorithms, but I don't see
</em><br>
<em>&gt; it as a problem.  Life seems to require arbitrary choices.
</em><br>
<p>But if arbitrariness is not a problem, then why not just pick the utility 
<br>
function of an arbitrary person instead of trying to average them?
<br>
<p><em>&gt; All of the algorithmic priors I've run into depend on measuring the
</em><br>
<em>&gt; complexity of something by counting the bits in an encoded
</em><br>
<em>&gt; representation of an algorithm.  There are infinitely many ways to do
</em><br>
<em>&gt; the encoding, but people don't seem to mind it too much.  If you're
</em><br>
<em>&gt; looking for indefensible arbitrary choices, the choice of what
</em><br>
<em>&gt; language to use for knowledge representation seems less defensible
</em><br>
<em>&gt; than the algorithm for interpersonal utility comparison we're talking
</em><br>
<em>&gt; about here.
</em><br>
<p>Yep, that's another problem. I mentioned it in the two posts I cited 
<br>
earlier.
<br>
&nbsp;
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17214.html">Stefan Pernar: "Re: Morality simulator"</a>
<li><strong>Previous message:</strong> <a href="17212.html">Tim Freeman: "Re: UCaRtMaAI paper"</a>
<li><strong>In reply to:</strong> <a href="17212.html">Tim Freeman: "Re: UCaRtMaAI paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17224.html">Tim Freeman: "Re: UCaRtMaAI paper"</a>
<li><strong>Reply:</strong> <a href="17224.html">Tim Freeman: "Re: UCaRtMaAI paper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17213">[ date ]</a>
<a href="index.html#17213">[ thread ]</a>
<a href="subject.html#17213">[ subject ]</a>
<a href="author.html#17213">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:00 MDT
</em></small></p>
</body>
</html>
