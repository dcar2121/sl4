<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: UCaRtMaAI paper</title>
<meta name="Author" content="Tim Freeman (tim@fungible.com)">
<meta name="Subject" content="Re: UCaRtMaAI paper">
<meta name="Date" content="2007-11-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: UCaRtMaAI paper</h1>
<!-- received="Sat Nov 24 16:48:46 2007" -->
<!-- isoreceived="20071124234846" -->
<!-- sent="Sat, 24 Nov 2007 14:15:04 -0700" -->
<!-- isosent="20071124211504" -->
<!-- name="Tim Freeman" -->
<!-- email="tim@fungible.com" -->
<!-- subject="Re: UCaRtMaAI paper" -->
<!-- id="20071124234626.62B5FD262C@fungible.com" -->
<!-- inreplyto="D0719B5256614765A37C40B628F8477F@weidaim1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tim Freeman (<a href="mailto:tim@fungible.com?Subject=Re:%20UCaRtMaAI%20paper"><em>tim@fungible.com</em></a>)<br>
<strong>Date:</strong> Sat Nov 24 2007 - 14:15:04 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="17246.html">David Picón Álvarez: "Re: How to make a slave"</a>
<li><strong>Previous message:</strong> <a href="17244.html">Eliezer S. Yudkowsky: "Re: Recipe for CEV (was Re: Morality simulator)"</a>
<li><strong>In reply to:</strong> <a href="17231.html">Wei Dai: "Re: UCaRtMaAI paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17248.html">Eliezer S. Yudkowsky: "Re: UCaRtMaAI paper"</a>
<li><strong>Reply:</strong> <a href="17248.html">Eliezer S. Yudkowsky: "Re: UCaRtMaAI paper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17245">[ date ]</a>
<a href="index.html#17245">[ thread ]</a>
<a href="subject.html#17245">[ subject ]</a>
<a href="author.html#17245">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
From: &quot;Wei Dai&quot; &lt;<a href="mailto:weidai@weidai.com?Subject=Re:%20UCaRtMaAI%20paper">weidai@weidai.com</a>&gt;
<br>
<em>&gt;My conjectured-to-be-better scheme is to not build an AGI until we're more 
</em><br>
<em>&gt;sure that we know what we are doing.
</em><br>
<p>Who is this &quot;we&quot;?  By chance, I know of four AGI projects that seem to
<br>
be making reasonable progress without any concerns about friendliness.
<br>
I stumbled across four without making any effort to search for them,
<br>
so there are surely more out there.  For different values of &quot;we&quot;, you
<br>
might be proposing to stop them all, or just the hypothetical one
<br>
based on the ideas in my paper, or perhaps some other subset.
<br>
<p><em>&gt;I made the suggestion of giving each person a fixed quota of resources. Is 
</em><br>
<em>&gt;that something you've considered already?
</em><br>
<p>That's a plausible idea.  It seems suboptimal, since if person A is in
<br>
one aisle of the grocery store choosing apples, and person B is in the
<br>
next aisle is having a heart attack, I would prefer a scheme in which
<br>
the person B gets more resources than person A.  Offhand, I don't know
<br>
how to define &quot;resources&quot; so the idea is implementable, but there
<br>
might be something doable there.
<br>
<p>Another idea that came up recently is that the FAI might require each
<br>
person to indicate what they want by doing some fixed percentage of
<br>
the work to get it, say 10%, then the AI does the other 90%.  As the
<br>
AI gets more powerful it can do a larger fraction of the work, and as
<br>
it gets to know more about human nature it will be able to accurately
<br>
guess what people want with them contributing less of the effort.  The
<br>
UCaRtMaAI proposal can, in principle, choose to do experiments, so it
<br>
might figure out something like this itself.  Making someone show
<br>
their enthusiasm by working toward the goal before you help them isn't
<br>
different in kind from paying attention to what they say they want --
<br>
in both cases some symbolic action is interpreted as a clue about the
<br>
utilities.
<br>
<p><em>&gt;...You might want to walk through an example step by step...
</em><br>
<p>That's a good idea.  When I've done it, I'll add it to the paper and
<br>
post a pointer.
<br>
<p><em>&gt;To take the simplest example, suppose I get a group of friends together and 
</em><br>
<em>&gt;we all tell the AI, &quot;at the end of this planning period please replace 
</em><br>
<em>&gt;yourself with an AI that serves only us.&quot; The rest of humanity does not know 
</em><br>
<em>&gt;about this, so they don't do anything that would let the AI infer that they 
</em><br>
<em>&gt;would assign this outcome a low utility.
</em><br>
<p>I'll make a separate post for this, since I'll have a question there
<br>
that might merit a wider audience.
<br>
<p><em>&gt;Among the infinite number of algorithms for averaging people's utility 
</em><br>
<em>&gt;functions, you've somehow picked one. How did you pick it?
</em><br>
<p>I didn't have any better ideas, and this one was implementable and
<br>
seemed to work well in the cases I could imagine.
<br>
<p><em>&gt;Given that the vast majority of those algorithms are not among the
</em><br>
<em>&gt;best known alternatives, what makes you think that the algorithm you
</em><br>
<em>&gt;picked *is* among the best known alternatives?
</em><br>
<p>I'm sure it's among the best alternatves known *to me*, simply because
<br>
I'm ignorant of better alternatives.
<br>
<p>I know that's not worth much.  The main point of the paper isn't &quot;this
<br>
is an excellent algorithm we should use&quot;.  The main point is &quot;here's
<br>
an example of how to use Solomonoff induction to specify AI systems
<br>
that interact constructively with the real world&quot;.  If someone finds
<br>
something better to plug in there, that's great.  It's an unambiguous
<br>
specification of a better-than-nothing algorithm, and we didn't have
<br>
that before, to my knowledge.
<br>
<p><em>&gt;For example, consider explicit calibration as an alternative. Design a 
</em><br>
<em>&gt;standard basket of goods and services, and calibrate each person's utility 
</em><br>
<em>&gt;function so that his utility of obtaining one standard basket is 1, and his 
</em><br>
<em>&gt;utility of obtaining two standard baskets is 2.
</em><br>
<p>That's a plausible idea, if it could be implemented.  How could it be
<br>
implemented?
<br>
<p>I imagine some odd mechanical man with a basket containing a tube
<br>
of toothpaste, a loaf of bread, and a one gallon jug of crude oil
<br>
walking up to a hut in rural Uganda and saying &quot;What would you be
<br>
willing to give me in exchange for this?&quot;  The inhabitants have never
<br>
seen any of those things before, so the subsequent conversation
<br>
doesn't seem likely to go well.  There must be a better idea than
<br>
measuring their value of a basket of goods &amp; services by making them
<br>
trade for it, but I don't know what it is.
<br>
<p>In general, measurement by trading doesn't work for people who aren't
<br>
hooked into the world economy, and it doesn't work well when
<br>
transaction costs are large compared to the value of the basket.  If
<br>
you control transaction costs by having a very large basket, it seems
<br>
you're likely to misunderstand the low-wealth end of the curve, such
<br>
as starving people and kidnapped children.  (Think of the chidren! :-)
<br>
<p><em>&gt;To me, this seems a lot more likely to be at least somewhat fair than
</em><br>
<em>&gt;an algorithm that relies on the side effects of integer overflow.
</em><br>
<p>You may be right about the calibration scheme being better, if it can be
<br>
implemented.  However, let's be fair about my scheme -- it doesn't
<br>
play with integer overflow.  If the utility is too big, we discard the
<br>
explanation rather than truncating the high bits and using the low
<br>
bits.  
<br>
<p>My scheme imposes a maximum utility and finite resolution.  If utility
<br>
is a model of some neurological event, this is plausible because the
<br>
human brain is a physical system of finite complexity.  (This might be
<br>
wrong if Penrose is right about human consciousness using quantum
<br>
entanglement, but I'm not worried about that.  If we were quantum
<br>
computers, we'd be smarter.)
<br>
<p><pre>
-- 
Tim Freeman               <a href="http://www.fungible.com">http://www.fungible.com</a>           <a href="mailto:tim@fungible.com?Subject=Re:%20UCaRtMaAI%20paper">tim@fungible.com</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="17246.html">David Picón Álvarez: "Re: How to make a slave"</a>
<li><strong>Previous message:</strong> <a href="17244.html">Eliezer S. Yudkowsky: "Re: Recipe for CEV (was Re: Morality simulator)"</a>
<li><strong>In reply to:</strong> <a href="17231.html">Wei Dai: "Re: UCaRtMaAI paper"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="17248.html">Eliezer S. Yudkowsky: "Re: UCaRtMaAI paper"</a>
<li><strong>Reply:</strong> <a href="17248.html">Eliezer S. Yudkowsky: "Re: UCaRtMaAI paper"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#17245">[ date ]</a>
<a href="index.html#17245">[ thread ]</a>
<a href="subject.html#17245">[ subject ]</a>
<a href="author.html#17245">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:01 MDT
</em></small></p>
</body>
</html>
