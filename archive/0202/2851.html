<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Futurism at its most pathetic</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Futurism at its most pathetic">
<meta name="Date" content="2002-02-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Futurism at its most pathetic</h1>
<!-- received="Sat Feb 02 02:23:47 2002" -->
<!-- isoreceived="20020202092347" -->
<!-- sent="Sat, 02 Feb 2002 02:22:50 -0500" -->
<!-- isosent="20020202072250" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Futurism at its most pathetic" -->
<!-- id="3C5B93CA.796BF76F@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="8c.136e40b8.298c9f1e@aol.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Futurism%20at%20its%20most%20pathetic"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Feb 02 2002 - 00:22:50 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2852.html">Chris Cooper: "Re: Futurism at its most pathetic"</a>
<li><strong>Previous message:</strong> <a href="2850.html">Gordon Worley: "Re: ExI:  Announcing ExI-Community at Yahoo!"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2852.html">Chris Cooper: "Re: Futurism at its most pathetic"</a>
<li><strong>Reply:</strong> <a href="2852.html">Chris Cooper: "Re: Futurism at its most pathetic"</a>
<li><strong>Maybe reply:</strong> <a href="2854.html">gabriel C: "Re: Futurism at its most pathetic"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2851">[ date ]</a>
<a href="index.html#2851">[ thread ]</a>
<a href="subject.html#2851">[ subject ]</a>
<a href="author.html#2851">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
This gets my vote for the most pathetic bit of futurism I have ever
<br>
encountered, even worse than the Morgan Stanley &quot;Fashions of the Third
<br>
Millennium&quot; report (google on &quot;idiots predict future&quot;).  Forwarded from
<br>
the Extropians list, where it was posted by Spudboy100@aol.com.
<br>
<p><em>&gt; Courtesy: Dr Clifford Pickover (clifford pickover group on Yahoo.com)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; <a href="http://www.globalideasbank.org/wbi/WBI-118.HTML">http://www.globalideasbank.org/wbi/WBI-118.HTML</a>
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ...How to stop machines enslaving humans
</em><br>
<em>&gt; Kevin Warwick, professor of cybernetics at Reading University and author of
</em><br>
<em>&gt; the book March of the Machines, introduced a brainstorming salon at the
</em><br>
<em>&gt; Institute for Social Inventions in London in April 1998. He explained how
</em><br>
<em>&gt; computers and robots are already as intelligent as ants and that within 20
</em><br>
<em>&gt; years they will have a computational power far greater than the human brain
</em><br>
<em>&gt; and will be evolving at a far faster rate than humans.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 'Advanced machine beings likely to treat humans with as little consideration
</em><br>
<em>&gt; as we treat lower forms of life'.
</em><br>
<em>&gt; These advanced machine beings, with a consciousness of their own, are likely
</em><br>
<em>&gt; to treat humans with as little consideration as we treat lower forms of life,
</em><br>
<em>&gt; especially since the military are largely responsible for creating them.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 'Within the lifetime of our children, Warwick believes, humans will become
</em><br>
<em>&gt; slaves who are culled, farmed and genetically interfered with'
</em><br>
<em>&gt; The race to develop artificial intelligence systems is very difficult to
</em><br>
<em>&gt; halt. Within the lifetime of our children, Warwick believes, humans will
</em><br>
<em>&gt; become powerless slaves who are ruthlessly culled, farmed and genetically
</em><br>
<em>&gt; interfered with.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The brainstorming
</em><br>
<em>&gt; The salon participants suspended any disbelief in this eventuality and
</em><br>
<em>&gt; brainstormed possible ways of preventing it happening. The normal
</em><br>
<em>&gt; brainstorming rules applied, with wild and crazy ideas all welcomed. Here is
</em><br>
<em>&gt; a sample of the suggestions:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Make a peace treaty with the robots, giving them total control of a planet of
</em><br>
<em>&gt; their own to live on, perhaps the moon, if that's not too close; or even just
</em><br>
<em>&gt; all underground parts of the planet, since they may not require daylight.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Intensively train a James-Bond-style team capable of destroying robots at the
</em><br>
<em>&gt; first sign of hostile intent.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Develop gradually enlarging Computer Free Zones which are utterly free of
</em><br>
<em>&gt; dependence on computer systems.......
</em><br>
<p>And there's more.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2852.html">Chris Cooper: "Re: Futurism at its most pathetic"</a>
<li><strong>Previous message:</strong> <a href="2850.html">Gordon Worley: "Re: ExI:  Announcing ExI-Community at Yahoo!"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2852.html">Chris Cooper: "Re: Futurism at its most pathetic"</a>
<li><strong>Reply:</strong> <a href="2852.html">Chris Cooper: "Re: Futurism at its most pathetic"</a>
<li><strong>Maybe reply:</strong> <a href="2854.html">gabriel C: "Re: Futurism at its most pathetic"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2851">[ date ]</a>
<a href="index.html#2851">[ thread ]</a>
<a href="subject.html#2851">[ subject ]</a>
<a href="author.html#2851">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
