<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Hawking misquoted on computers taking over</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Hawking misquoted on computers taking over">
<meta name="Date" content="2001-09-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Hawking misquoted on computers taking over</h1>
<!-- received="Fri Sep 14 15:26:29 2001" -->
<!-- isoreceived="20010914212629" -->
<!-- sent="Fri, 14 Sep 2001 14:31:26 -0400" -->
<!-- isosent="20010914183126" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Hawking misquoted on computers taking over" -->
<!-- id="3BA24CFE.E958EF3D@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3BA1B386.7672B4F0@posthuman.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Hawking%20misquoted%20on%20computers%20taking%20over"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri Sep 14 2001 - 12:31:26 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2174.html">Gordon Worley: "New Sysop paper"</a>
<li><strong>Previous message:</strong> <a href="2172.html">Brian Atkins: "Re: Hawking misquoted on computers taking over"</a>
<li><strong>In reply to:</strong> <a href="2172.html">Brian Atkins: "Re: Hawking misquoted on computers taking over"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2173">[ date ]</a>
<a href="index.html#2173">[ thread ]</a>
<a href="subject.html#2173">[ subject ]</a>
<a href="author.html#2173">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Brian Atkins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Well to be clear, he was not apparently misquoted on &quot;computers taking
</em><br>
<em>&gt; over&quot;, he was misquoted in his proposed solution to that perceived problem.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Not that I see how it makes one whit of a difference as to whether the
</em><br>
<em>&gt; &quot;computers&quot; take over if they are attached to your mind or run separately.
</em><br>
<em>&gt; Perhaps what he meant to express was that he wants humans with augmented
</em><br>
<em>&gt; intelligence rather than AIs. He still comes across as someone who hasn't
</em><br>
<em>&gt; fully grasped the situation.
</em><br>
<p>I think he unambiguously expressed that he wanted humans with augmented
<br>
intelligence rather than AIs, and that he considered &quot;silicon neurons&quot;
<br>
connected to biological neurons to be instances of augmented humans rather
<br>
than augmented computers.  I suppose this is realistic as long as the
<br>
overall mind starts out with the cognitive architecture and emotional
<br>
makeup of the human core.  Perhaps he would feel the same way about
<br>
uploaded humans.
<br>
<p>I have no objection to uploaded humans or computer-augmented humans,
<br>
unless interim experience with Friendly AI shows that FAI is not only *as*
<br>
likely but actually substantially *more* likely to produce an altruist.  I
<br>
see Friendly AI as our best chance, and the most important variable,
<br>
because I think AI will substantially proceed uploading or even real
<br>
computer augmentation.
<br>
<p>There are a lot of very powerful, unambiguous reasons to pursue Friendly
<br>
AI.  The untrustworthiness of human uploads, however, is not one of them. 
<br>
It looks to me like if you take the human emotional core and gradually
<br>
increase intelligence and self-awareness, the end result should be
<br>
altruistic in most cases (at least).  It could be that interim results in
<br>
Friendly AI will unambiguously show that FAI works, in which case the
<br>
burden of proof shifts to the human-pathway advocates, but that hasn't
<br>
happened yet.
<br>
<p>The essential flaw in the debate as conducted by both Hawking and Kurzweil
<br>
is the concept that pure nonbiological AIs are necessarily the enemy - or
<br>
at least the Other, a different species with different interests.  The
<br>
underlying anthropomorphism is the expectation that a nonbiological
<br>
organism will have an observer-centered goal system (note:
<br>
&quot;observer-centered&quot; != &quot;observer-dependent&quot;).  You can pursue a human
<br>
Singularity through purely nonbiological substrate; in fact, that verges
<br>
on being the definition of Friendly AI.
<br>
<p>Once you accept that it's not carbon versus silicon, and that you can get
<br>
the same Singularity via AI, you can take an unbiased look at the relative
<br>
technological rates and realize that AI comes first.  The question is
<br>
whether it's a Friendly AI developed by a Singularity-aware project,
<br>
whether humanity is wiped out by biological or nanotechnological weapons
<br>
during the current window of vulnerability, and how many fatalities
<br>
humanity suffers in the interim period.
<br>
<p>I'd take a biological Singularity if I could get one, and would expect the
<br>
augmented humans to turn right around and develop a Friendly AI - at
<br>
least, that's what I'd expect if they were ethical.  But if Kurzweil is
<br>
correct in expecting human enhancement in 2030, and AI first becomes
<br>
feasible in 2010, then human enhancement is as irrelevant as genetic
<br>
engineering.  I don't expect to get a biological Singularity, and I think
<br>
a nonbiological Singularity is just as good, so I concentrate on
<br>
nonbiological Singularities.  And I think that if you start fighting over
<br>
whether you want a biological or nonbiological Singularity, then humanity
<br>
wipes itself out while you're bickering, or an unFriendly AI is developed
<br>
first because all the Singularity-aware Friendly AI projects have been
<br>
shut down.
<br>
<p>The important thing is to get to *some* positive Singularity as fast as
<br>
possible.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2174.html">Gordon Worley: "New Sysop paper"</a>
<li><strong>Previous message:</strong> <a href="2172.html">Brian Atkins: "Re: Hawking misquoted on computers taking over"</a>
<li><strong>In reply to:</strong> <a href="2172.html">Brian Atkins: "Re: Hawking misquoted on computers taking over"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2173">[ date ]</a>
<a href="index.html#2173">[ thread ]</a>
<a href="subject.html#2173">[ subject ]</a>
<a href="author.html#2173">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
