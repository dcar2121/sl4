<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Time and Minds/Big Daddy</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Time and Minds/Big Daddy">
<meta name="Date" content="2001-09-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Time and Minds/Big Daddy</h1>
<!-- received="Sun Sep 23 14:42:46 2001" -->
<!-- isoreceived="20010923204246" -->
<!-- sent="Sun, 23 Sep 2001 14:41:44 -0400" -->
<!-- isosent="20010923184144" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Time and Minds/Big Daddy" -->
<!-- id="3BAE2CE8.EE9AD3D4@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="F506I1YbpIQMVUltZ6400001a5b@hotmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Time%20and%20Minds/Big%20Daddy"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Sep 23 2001 - 12:41:44 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2222.html">Eliezer S. Yudkowsky: "Re: Invisible, Incomprehensible Gods (was: Time and Minds)"</a>
<li><strong>Previous message:</strong> <a href="2220.html">Xavier Lumine: "Re: Time and Minds"</a>
<li><strong>In reply to:</strong> <a href="2219.html">Xavier Lumine: "RE: Time and Minds/Big Daddy"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2224.html">Brian Phillips: "Re: Time and Minds/Big Daddy"</a>
<li><strong>Reply:</strong> <a href="2224.html">Brian Phillips: "Re: Time and Minds/Big Daddy"</a>
<li><strong>Reply:</strong> <a href="2227.html">Gordon Worley: "Re: Time and Minds/Big Daddy"</a>
<li><strong>Maybe reply:</strong> <a href="2228.html">Xavier Lumine: "Re: Time and Minds/Big Daddy"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2221">[ date ]</a>
<a href="index.html#2221">[ thread ]</a>
<a href="subject.html#2221">[ subject ]</a>
<a href="author.html#2221">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Xavier Lumine wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I often find self-describing 'bystanders' posting on this list, following
</em><br>
<em>&gt; their often important thought with something along the lines of &quot;but I'm no
</em><br>
<em>&gt; one to speak&quot; or &quot;I probably don't know what I'm talking about&quot; or &quot;I'm
</em><br>
<em>&gt; getting back to my work because the people who actually write AI will sneer
</em><br>
<em>&gt; at my post&quot;. The individuals on the Friendly AI project under the
</em><br>
<em>&gt; Singularity Institute for Artificial Intelligence are not superhuman. We are
</em><br>
<em>&gt; a small team of computer programmers, self-educated cognitive scientists and
</em><br>
<em>&gt; ethusiasts. Although I will not dispute that bright engineers and designers
</em><br>
<em>&gt; are necessary to create true Friendly AI, we are still people with an
</em><br>
<em>&gt; incomplete view of the world and posts to this list are not laffable nor
</em><br>
<em>&gt; shunned.
</em><br>
<p>Hm, I'm not so sure about that.  Creating AI is not like anything else in
<br>
the world, up to and including Flare.  There is a Flare team, but there is
<br>
not yet an AI team.  I'm reasonably confident I can transfer over the
<br>
knowledge needed to create Flare.  I'm a bit more worried about
<br>
transferring over the knowledge needed to create AI.  The real knowledge,
<br>
I mean; not design specs, but the skill needed to come up with design
<br>
specs.  Programming languages have been created before and will be created
<br>
again.  AI is something else.  There are no superhumans, and so we'd
<br>
better *hope* it doesn't take superhumans, but I'm not so sure it can be
<br>
done with your average bright 99th-percentile programmer either.  Sixty
<br>
percent of real AI is knowing how to refuse the path of ideology, or pause
<br>
and invent complex solutions to complex problems, but that still leaves
<br>
forty percent blinding flash of intuition.
<br>
<p>Yes, my view of the world is still ultimately incomplete, but I know how a
<br>
mind works, not just in the abstract but in the specific, and that makes a
<br>
huge difference.  I don't live in a fog of nervous confusion.  If I come
<br>
to a conclusion, the conclusion may be strong or weak, but either way I'll
<br>
know why I came to that conclusion, and furthermore I'll be able to check
<br>
that the reasoning proceeded according to the normative rules for
<br>
rationality - giving rise to an outlook that is often mistaken for
<br>
&quot;confidence&quot; on a first meeting, but which is actually just a case of
<br>
knowing exactly how uncertain I am and why.
<br>
<p>There are no superhumans, unfortunately, yet.  But let's not understate
<br>
what it takes to be an AI programmer either.  It's not enough to be very
<br>
bright, or even smart enough to be written up as a genius in Wired
<br>
magazine.  People of that caliber have hit the problem of AI and bounced. 
<br>
If we are lucky, it will turn out that being &quot;very bright&quot; is enough to
<br>
join an existing AI project already headed in the right direction and make
<br>
useful contributions, but this is by no means certain.
<br>
<p>As for what it takes to post to the list, I wish I could say that everyone
<br>
had what it takes, or that everyone would automatically know whether or
<br>
not they have what it takes, or that at the very least all the good
<br>
posters would estimate themselves to be good posters even if some bad
<br>
posters did so as well.  Unfortunately, all three hypotheses have been
<br>
disconfirmed by experience, and so I can't tell people &quot;If you're not
<br>
sure, don't post&quot; because that would wipe out at least two-thirds of the
<br>
sufficiently-good first posts I've seen.
<br>
<p>Still, I think that in the end the list is better served by artificially
<br>
high standards than artificially low standards.  I think the list is
<br>
better served by perfectionism than tolerance.  Smart (and grammatical!)
<br>
lurkers eventually overcome their nervousness and post... though
<br>
admittedly, I would have no way of knowing if good people got scared off
<br>
entirely.  But to consciously profess universal tolerance can destroy a
<br>
list.  I've seen it happen.
<br>
<p>This list was created to provide a refuge for the best damned posts in the
<br>
whole damned Solar System - or that, at any rate, is the ideal.  This is
<br>
not a list that is *supposed* to be easy to post to.  It is a list that is
<br>
supposed to be fun to read.
<br>
<p>I appreciate useful criticism, but the vast majority of criticism I get is
<br>
not useful.  You don't find the rare people who can catch you in a mistake
<br>
by sweeping your net as wide as possible; you find them by raising and
<br>
raising your standards until your environment meets such high standards of
<br>
rationality that useful critics can hang out there.  Useful criticism is
<br>
not something that you find by seeking criticism, it's something you find
<br>
by creating an environment where rational arguments can grow and prosper,
<br>
free from distraction; among those rational arguments will eventually be
<br>
found rational criticisms.  And that, in turn, means being willing to do
<br>
something when it seems like standards might be dropping.  It means that
<br>
instead of soliciting easy fun criticism, you solicit correctly spelled
<br>
rational arguments that thread through complex (but fun!) technical
<br>
issues, and hope that the people who can manage *that* will manage a few
<br>
pieces of useful criticism as well.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2222.html">Eliezer S. Yudkowsky: "Re: Invisible, Incomprehensible Gods (was: Time and Minds)"</a>
<li><strong>Previous message:</strong> <a href="2220.html">Xavier Lumine: "Re: Time and Minds"</a>
<li><strong>In reply to:</strong> <a href="2219.html">Xavier Lumine: "RE: Time and Minds/Big Daddy"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2224.html">Brian Phillips: "Re: Time and Minds/Big Daddy"</a>
<li><strong>Reply:</strong> <a href="2224.html">Brian Phillips: "Re: Time and Minds/Big Daddy"</a>
<li><strong>Reply:</strong> <a href="2227.html">Gordon Worley: "Re: Time and Minds/Big Daddy"</a>
<li><strong>Maybe reply:</strong> <a href="2228.html">Xavier Lumine: "Re: Time and Minds/Big Daddy"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2221">[ date ]</a>
<a href="index.html#2221">[ thread ]</a>
<a href="subject.html#2221">[ subject ]</a>
<a href="author.html#2221">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:37 MDT
</em></small></p>
</body>
</html>
