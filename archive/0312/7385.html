<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: An essay I just wrote on the Singularity.</title>
<meta name="Author" content="Tommy McCabe (rocketjet314@yahoo.com)">
<meta name="Subject" content="Re: An essay I just wrote on the Singularity.">
<meta name="Date" content="2003-12-31">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: An essay I just wrote on the Singularity.</h1>
<!-- received="Wed Dec 31 07:46:13 2003" -->
<!-- isoreceived="20031231144613" -->
<!-- sent="Wed, 31 Dec 2003 06:46:11 -0800 (PST)" -->
<!-- isosent="20031231144611" -->
<!-- name="Tommy McCabe" -->
<!-- email="rocketjet314@yahoo.com" -->
<!-- subject="Re: An essay I just wrote on the Singularity." -->
<!-- id="20031231144611.5173.qmail@web11701.mail.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20031231094734.GE11973@digitalkingdom.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tommy McCabe (<a href="mailto:rocketjet314@yahoo.com?Subject=Re:%20An%20essay%20I%20just%20wrote%20on%20the%20Singularity."><em>rocketjet314@yahoo.com</em></a>)<br>
<strong>Date:</strong> Wed Dec 31 2003 - 07:46:11 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7386.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7384.html">Mitchell Porter: "RE: An essay I just wrote on the Singularity."</a>
<li><strong>In reply to:</strong> <a href="7383.html">Robin Lee Powell: "An essay I just wrote on the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7397.html">Robin Lee Powell: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="7397.html">Robin Lee Powell: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="../0401/7460.html">Samantha Atkins: "Re: An essay I just wrote on the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7385">[ date ]</a>
<a href="index.html#7385">[ thread ]</a>
<a href="subject.html#7385">[ subject ]</a>
<a href="author.html#7385">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I have several objections to the points raised in this
<br>
essay. You say that the Singularity is a bad term for
<br>
these reasons:
<br>
<p>&quot;We can't see what lies beyond twenty minutes from now
<br>
regardless. Just ask a stock broker.&quot;
<br>
<p>We can't predict the future down to the letter, true,
<br>
but I'd be willing to bet in twenty years, let alone
<br>
twenty minutes, the planet called Earth will be here,
<br>
the stockbroker will be either still human, or dead,
<br>
and there will still be a stock market. If the
<br>
Singularity happened, you couldn't guarantee any of
<br>
those.
<br>
<p>&quot;This has next to nothing to do with the mathematical
<br>
term 'singularity' it was derived from, as that
<br>
implies a total cessation of the graph beyond a
<br>
certain point, which isn't the kind of implication I
<br>
want to have made about humanity's future!&quot;
<br>
<p>Total cessation? Yes, on a hyperbolic graph, at a
<br>
certain point, the function will be undefined.
<br>
However, 1), actual values are not constrained to
<br>
follow mathematical equations, and 2), that signifies
<br>
something unknown happens. Maybe it stops, maybe it
<br>
slows down to exponential growth, or even linear,
<br>
maybe it somehow will go to infinity. We're currently
<br>
too dumb to know.
<br>
<p>&quot;It fails, IMO, to address the important point: the
<br>
coming change, whatever you call it, will not be
<br>
created by technology in any respect (technological
<br>
growth, new technologies, what have you). It will
<br>
happen if, and only if, greater than human
<br>
intelligence becomes part of humanity's reality. Then,
<br>
and only then, will we truly have reached a point
<br>
where things have fundamentally changed because then
<br>
we're dealing with minds creating technology that are
<br>
fundamentally alien to us, or at least better. At that
<br>
point, things have truly changed.&quot;
<br>
<p>Technology is defined as &quot;The application of science
<br>
to practical problems&quot;, or something like that.
<br>
Therefore, any superintelligence we create must be
<br>
technology, or at least the product of technology. An
<br>
infrahuman AI is a mind that is fundamentally alien to
<br>
us, but it's not going to outperform a human on at
<br>
least some things (by definition). The bridge here is
<br>
at the point where technology creates something
<br>
smarter than us, which is unlike other technology,
<br>
because it could think of things no human could ever
<br>
possibly think of.
<br>
<p>As a matter of fact, my only complaint against the
<br>
term 'Singularity' has been that people tend to see it
<br>
as a Bible-type doomsday.
<br>
<p>Quote from the essay: &quot; speaking of the (still very
<br>
theoretical) possibility of human-level generalized 
<br>
intelligence.&quot; Even though a human-level AI is
<br>
extermely likely to become transhuman in about 15
<br>
seconds, human-level AI isn't the ultimate goal.
<br>
Transhuman AI is.
<br>
<p>Next point: Even though transhuman AIs can (and
<br>
should) have the capability to understand emotions,
<br>
having emotions built into the AI is not a good idea
<br>
by any means; you could end up with critical failure
<br>
scenarios #'s 9, 12, 19, 21, and probably some others.
<br>
<p>I agree with most of the things said about nanotech,
<br>
except for the last one: &quot;The big difference with
<br>
nanotech versus nukes is that there is a first strike
<br>
advantage. A huge one.&quot; The first strike advantage in
<br>
this case, is probably going to be being responsible
<br>
for the destruction of the planet.
<br>
<p>A Friendly AI doesn't have the supergoal of being nice
<br>
to humans; it has the supergoal of acting friendly
<br>
toward other sentients in general. A Friendly AI that
<br>
is Friendly with humans shouldn't try to blow the same
<br>
humans to smithereens the minute they upload.
<br>
<p>I agree with the assumption that transhuman AI is
<br>
possible, however, creating copies of yourself and
<br>
then modifying the copies is not only an inefficient
<br>
use of computing power, but any AI smart enough to
<br>
have reached the stage of reprogramming ver own source
<br>
code is likely to have the power to destroy the
<br>
planet, or will have it very soon, and thus modelling
<br>
the AI in it's full runs the risk of the model, or the
<br>
copy, or whatever you want to call it, destroying the
<br>
planet. Recursive self-improvement is a vastly more
<br>
powerful version of design-and-test; the testing part
<br>
for any AI with the capability to blow up the planet
<br>
should use the partial AI shadowself described in
<br>
CFAI: Wisdom tournaments.
<br>
<p>I also agree that strong nanotech is possible.
<br>
<p>The last assumption, namely, that A Friendly,
<br>
Superintelligent AI Gets To Nanotech First shouldn't
<br>
be taken as an assumption at all. It's what I'd like
<br>
to happen, certainly, but there's no money-back
<br>
guarantee on it. Nanotech is already far more advanced
<br>
than AI (see MistakesOfClassicalAI on the Wiki).
<br>
<p>I agree that getting nanotech when you are a
<br>
transhuman is likely to be easy, but note that you
<br>
don't even need to convince the programmers to hook up
<br>
nanotech tools. A superintelligent AI could simply
<br>
copy itself, send the copy over the Internet, and take
<br>
over all the computers in the most advanced nanotech
<br>
lab on the planet. I also think that even having to
<br>
'convince' the programmers is overkill; if the AI is
<br>
Friendly, the programmers should just give it the
<br>
necessary tools; no tricking required.
<br>
<p>I also agree that the first one to nanotech pretty
<br>
much has ultimate control of the planet.
<br>
<p>Ruling The World is a very, very bad term for it; it
<br>
confuses the possession of absolute physical power
<br>
with the exercise of absolute social power. The former
<br>
is probably wanted; the latter isn't, and we're
<br>
dealing with a being without a tendency to abuse
<br>
power. I agree that a superintelligence in certainly
<br>
capable of the examples given, however, that's
<br>
probably just the beginning of the list of stuff a
<br>
superintelligence would be capable of.
<br>
<p>I also agree with the idea of the SysOp scenario
<br>
presented.
<br>
<p>I agree that the chances of that particular scenario
<br>
happening isn't that good; however, given Friendly
<br>
superintelligence, whatever scenario does play out is
<br>
as likely to be just as good as the one given. The
<br>
only lottery is the chances of achieving a Friendly
<br>
transhuman before we're blown to smithereens. And
<br>
those odds we can improve directly. Although I'm still
<br>
searching for some way to do so....
<br>
<p>__________________________________
<br>
Do you Yahoo!?
<br>
Find out what made the Top Yahoo! Searches of 2003
<br>
<a href="http://search.yahoo.com/top2003">http://search.yahoo.com/top2003</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7386.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7384.html">Mitchell Porter: "RE: An essay I just wrote on the Singularity."</a>
<li><strong>In reply to:</strong> <a href="7383.html">Robin Lee Powell: "An essay I just wrote on the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7397.html">Robin Lee Powell: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="7397.html">Robin Lee Powell: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="../0401/7460.html">Samantha Atkins: "Re: An essay I just wrote on the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7385">[ date ]</a>
<a href="index.html#7385">[ thread ]</a>
<a href="subject.html#7385">[ subject ]</a>
<a href="author.html#7385">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:43 MDT
</em></small></p>
</body>
</html>
