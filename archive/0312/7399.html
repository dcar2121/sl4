<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: An essay I just wrote on the Singularity.</title>
<meta name="Author" content="Robin Lee Powell (rlpowell@digitalkingdom.org)">
<meta name="Subject" content="Re: An essay I just wrote on the Singularity.">
<meta name="Date" content="2003-12-31">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: An essay I just wrote on the Singularity.</h1>
<!-- received="Wed Dec 31 12:59:07 2003" -->
<!-- isoreceived="20031231195907" -->
<!-- sent="Wed, 31 Dec 2003 11:59:04 -0800" -->
<!-- isosent="20031231195904" -->
<!-- name="Robin Lee Powell" -->
<!-- email="rlpowell@digitalkingdom.org" -->
<!-- subject="Re: An essay I just wrote on the Singularity." -->
<!-- id="20031231195904.GN11973@digitalkingdom.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3FF309F8.6000502@yifan.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Robin Lee Powell (<a href="mailto:rlpowell@digitalkingdom.org?Subject=Re:%20An%20essay%20I%20just%20wrote%20on%20the%20Singularity."><em>rlpowell@digitalkingdom.org</em></a>)<br>
<strong>Date:</strong> Wed Dec 31 2003 - 12:59:04 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7400.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7398.html">Robin Lee Powell: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>In reply to:</strong> <a href="7389.html">Michael Anissimov: "Re: An essay I just wrote on the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7403.html">Brian Atkins: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="7403.html">Brian Atkins: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="7404.html">Ben Goertzel: "RE: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="../0401/7445.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7399">[ date ]</a>
<a href="index.html#7399">[ thread ]</a>
<a href="subject.html#7399">[ subject ]</a>
<a href="author.html#7399">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Wed, Dec 31, 2003 at 09:40:08AM -0800, Michael Anissimov wrote:
<br>
<em>&gt; Robin, this is an interesting and entertaining essay!
</em><br>
<em>&gt; Congratulations on getting the motivation to write down some of
</em><br>
<em>&gt; your ideas and reasoning regarding the world-shaking issue of how
</em><br>
<em>&gt; humanity ought to approach the Singularity.  
</em><br>
<p>Thanks!
<br>
<p><em>&gt; I disagree with the way you present/argue some things though, so
</em><br>
<em>&gt; here I go with all the comments:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1.  Why do you call Singularitarianism your &quot;new religion&quot;?  I
</em><br>
<em>&gt; know it's basically all in jest, 
</em><br>
<p>No, it's not.
<br>
<p>I define religion as a set of beliefs held in absence of evidence or
<br>
proof.  Singularitarianism fits perfectly, regardless of how much I
<br>
might wish otherwise.
<br>
<p><em>&gt; but thousands of people have already misinterpreted the
</em><br>
<em>&gt; Singularity as a result of the &quot;Singularity = Rapture&quot; meme, 
</em><br>
<p>Yeah, I really haven't found any way to present the ideas without
<br>
invoking that comparison.  However, bear in mind that the reason
<br>
everyone makes that comparison is because it's a perfectly *valid*
<br>
one.  Seriously.  The only substantial difference between the
<br>
singularity and the rapture is that no-one involved in the
<br>
singularity claims to have had a vision from god.
<br>
<p><em>&gt; and I don't think they need any more encouragement.  I would
</em><br>
<em>&gt; personally prefer that Singularitarians have the reputation of
</em><br>
<em>&gt; being extremely atheistic and humanistic. 
</em><br>
<p>I *am* atheisitic and humanistic.  But that doesn't change the fact
<br>
that I have no proof, and barely any evidence, that the singularity,
<br>
let alone the sysop scenario, will actually come about.  That makes
<br>
it religion.
<br>
<p><em>&gt; 2.  Like Tommy McCabe, I too have a problem with the &quot;FAI means
</em><br>
<em>&gt; being nice to humans&quot; line.  This gives a lot of people the
</em><br>
<em>&gt; mistaken impression that FAI is going to be anthropocentric,
</em><br>
<em>&gt; unfortunately.
</em><br>
<p>See my respons to him.
<br>
<p><em>&gt; 3.  This is a fun paragraph:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &quot;Combining a few issues here. I believe that strong
</em><br>
<em>&gt; superintelligence is possible. Furthermore, I believe that to
</em><br>
<em>&gt; argue to the contrary is amazingly rank anthropocentrism, and
</em><br>
<em>&gt; should be laughed at. Beyond that, I think full AI is possible.
</em><br>
<em>&gt; It's the combination of the two that's interesting.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I agree that people who believe strong superintelligence is
</em><br>
<em>&gt; impossible are memetically distant enough from Singularity-aware
</em><br>
<em>&gt; thought that trying to avoid offending/confusing them is
</em><br>
<em>&gt; pointless.  Saying that the combination of the two is what's
</em><br>
<em>&gt; interesting unfortunately gives the reader the impression that AI
</em><br>
<em>&gt; and strong superintelligence in concert is the only thing capable
</em><br>
<em>&gt; of initiating a Singularity (when self-improving IA seeds are
</em><br>
<em>&gt; indeed possible, albeit unlikely.)  
</em><br>
<p>IA?  Is that a typo, or a term I'm unfamiliar with.
<br>
<p><em>&gt; It might cause readers to mistakenly overestimate the safety of
</em><br>
<em>&gt; the IA path.  The Singularity is complicated and confusing enough
</em><br>
<em>&gt; that little wording issues such as these can actually influence
</em><br>
<em>&gt; how the paper is interpreted by casual surfers (if that matters.)
</em><br>
<p>My point was that sentience in a computer substrate allows easy
<br>
self-modification; I've edited that paragraph a bit, please let me
<br>
know if it's more clear.
<br>
<p><em>&gt; 4.  It seems like you're saying the range of possible
</em><br>
<em>&gt; Singularities basically breaks down into either &quot;seed AI&quot; or
</em><br>
<em>&gt; &quot;uploading&quot;, when other IA techniques are indeed possible.
</em><br>
<em>&gt; Pre-uploading technology could probably be applied to yield
</em><br>
<em>&gt; substantial human intelligence enhancements, even though AI would
</em><br>
<em>&gt; almost certainly come before that as well.
</em><br>
<p>Added:
<br>
<p>There are probably other ways to produce superhuman intelligence,
<br>
but I honestly don't think that any of the interesting ones (i.e.
<br>
ones that produce intelligences meaningfully smarter than every
<br>
human has ever been) are likely to come before superintelligent AI,
<br>
and a comparison of possibilities is beyond the scope of this essay.
<br>
<p><em>&gt; 5.  &quot; Source code, /any/ source code, is a paragon of clarity by
</em><br>
<em>&gt; comparison.&quot; gives the audience the impression that you are
</em><br>
<em>&gt; worshipping code.  :)  Of course code will be &quot;clearer&quot; in a
</em><br>
<em>&gt; mathematical sense but &quot;paragon of clarity&quot; in the sense of &quot;it
</em><br>
<em>&gt; works cleanly&quot; would take a lot of programming effort, of course,
</em><br>
<em>&gt; and not any code would qualify.
</em><br>
<p>Note the 'by comparison'.  Added: &quot;Only by comparison, of course;
<br>
there's some &lt;em&gt;really&lt;/em&gt; bad source code out there.&quot;
<br>
<p><em>&gt; 6.  &quot;You see, Eliezer &lt;<a href="http://yudkowsky.net/beyond.html">http://yudkowsky.net/beyond.html</a>&gt; has
</em><br>
<em>&gt; convinced me that a Friendly AI must be the first being to develop
</em><br>
<em>&gt; strong nanotech on Earth, or one of the first, or we are all going
</em><br>
<em>&gt; to die in a mass of grey goo.&quot; makes you sound like a cult victim,
</em><br>
<em>&gt; unfortunately.  :(  
</em><br>
<p>One of the comments I've made to my friends is that I've always
<br>
wanted to be in a cult.  Nice easy sense of belonging and purpose.
<br>
But I'm too self-analytical to allow it.
<br>
<p>However, your point is well taken.  Edited.
<br>
<p><em>&gt; I know it's fun to write down stuff exactly as it sounds in our
</em><br>
<em>&gt; heads, but with Singularity issues, the wrong presentation can
</em><br>
<em>&gt; really damage your credibility...  I also think it's important
</em><br>
<em>&gt; that we present the FAI meme in a way that doesn't focus on
</em><br>
<em>&gt; Eliezer so much - even though he originated the idea, FAI-esque
</em><br>
<em>&gt; thinking has been going on for the past decade or two, and its
</em><br>
<em>&gt; present day supporters include people like Nick Bostrom, Brian
</em><br>
<em>&gt; Atkins, etc, not just Eliezer.  Placing too much emphasis on
</em><br>
<em>&gt; Eliezer will also make you look like a cult victim.
</em><br>
<p>Show me someone else that's actually got a coherent general AI
<br>
design, then.
<br>
<p>I have no attachment to Eliezer himself, but I honestly don't know
<br>
anyone else that's doing what he's doing.
<br>
<p><em>&gt; 7.  &quot;Please understand that if someone gets to strong nanotech
</em><br>
<em>&gt; before everyone else, they rule the world. This is not a subject
</em><br>
<em>&gt; for debate, you can't fight back, there is no passing Go or
</em><br>
<em>&gt; collecting two hundred dollars.&quot; is put very clearly, and
</em><br>
<em>&gt; concisely, and correctly.  A little skimp on the explanations
</em><br>
<em>&gt; again, but I suppose that if people seriously question you here,
</em><br>
<em>&gt; they aren't likely to understand the issues surrounding FAI
</em><br>
<em>&gt; anyway.
</em><br>
<p>That's kind of where I'm at, yeah.  And as I said elsewhere, this is
<br>
really directed at SL1 and 2.
<br>
<p><em>&gt; 8.  It could be nitpicking, but near the end of the essay, I would
</em><br>
<em>&gt; personally say we're working towards a &quot;successful&quot; or
</em><br>
<em>&gt; &quot;benevolent&quot; Singularity, rather than a &quot;sysop scenario&quot;.  &quot;Sysop
</em><br>
<em>&gt; scenario&quot;, sadly, gives people the wrong idea 90% of the time.
</em><br>
<p>Good point.  Edited.
<br>
<p><em>&gt; Anyway, congratulations again on writing something.  Politics is
</em><br>
<em>&gt; indeed largely irrelevant.  This becomes clear around high SL2, as
</em><br>
<em>&gt; a matter of fact.  At the very least, politics is something we
</em><br>
<em>&gt; cannot influence unless we pursue high-leverage goals, like
</em><br>
<em>&gt; devoting our lives to politics, or, far better yet, building a
</em><br>
<em>&gt; Friendly AI.
</em><br>
<p>Thanks again.
<br>
<p>-Robin
<br>
<p><pre>
-- 
Me: <a href="http://www.digitalkingdom.org/~rlpowell/">http://www.digitalkingdom.org/~rlpowell/</a>  ***   I'm a *male* Robin.
&quot;Constant neocortex override is the only thing that stops us all
from running out and eating all the cookies.&quot;  -- Eliezer Yudkowsky
<a href="http://www.lojban.org/">http://www.lojban.org/</a>             ***              .i cimo'o prali .ui
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7400.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7398.html">Robin Lee Powell: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>In reply to:</strong> <a href="7389.html">Michael Anissimov: "Re: An essay I just wrote on the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7403.html">Brian Atkins: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="7403.html">Brian Atkins: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="7404.html">Ben Goertzel: "RE: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="../0401/7445.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7399">[ date ]</a>
<a href="index.html#7399">[ thread ]</a>
<a href="subject.html#7399">[ subject ]</a>
<a href="author.html#7399">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:43 MDT
</em></small></p>
</body>
</html>
