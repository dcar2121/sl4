<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: An essay I just wrote on the Singularity.</title>
<meta name="Author" content="Tommy McCabe (rocketjet314@yahoo.com)">
<meta name="Subject" content="Re: An essay I just wrote on the Singularity.">
<meta name="Date" content="2003-12-31">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: An essay I just wrote on the Singularity.</h1>
<!-- received="Wed Dec 31 13:04:31 2003" -->
<!-- isoreceived="20031231200431" -->
<!-- sent="Wed, 31 Dec 2003 12:04:28 -0800 (PST)" -->
<!-- isosent="20031231200428" -->
<!-- name="Tommy McCabe" -->
<!-- email="rocketjet314@yahoo.com" -->
<!-- subject="Re: An essay I just wrote on the Singularity." -->
<!-- id="20031231200428.20004.qmail@web11702.mail.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="87vfnw4wty.fsf@snark.piermont.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Tommy McCabe (<a href="mailto:rocketjet314@yahoo.com?Subject=Re:%20An%20essay%20I%20just%20wrote%20on%20the%20Singularity."><em>rocketjet314@yahoo.com</em></a>)<br>
<strong>Date:</strong> Wed Dec 31 2003 - 13:04:28 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7401.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7399.html">Robin Lee Powell: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>In reply to:</strong> <a href="7395.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0401/7447.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="../0401/7447.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7400">[ date ]</a>
<a href="index.html#7400">[ thread ]</a>
<a href="subject.html#7400">[ subject ]</a>
<a href="author.html#7400">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
--- &quot;Perry E. Metzger&quot; &lt;<a href="mailto:perry@piermont.com?Subject=Re:%20An%20essay%20I%20just%20wrote%20on%20the%20Singularity.">perry@piermont.com</a>&gt; wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Tommy McCabe &lt;<a href="mailto:rocketjet314@yahoo.com?Subject=Re:%20An%20essay%20I%20just%20wrote%20on%20the%20Singularity.">rocketjet314@yahoo.com</a>&gt; writes:
</em><br>
<em>&gt; &gt; True, but no disproof exists.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Operating on the assumption that that something
</em><br>
<em>&gt; which may or may not
</em><br>
<em>&gt; be possible will happen seems imprudent.
</em><br>
<p>It seems like a very reasonable idea that what can be
<br>
done, by dumb evolution, in a few gigabytes of DNA can
<br>
be done in programming code by humans. And if you have
<br>
Friendly human-equivalent AI, it's going to be a very
<br>
short while until you have Friendly transhuman AI.
<br>
There could, in theory, be some sort of upper bound on
<br>
intelligence, but to argue that it is at the exact
<br>
level represented by Homo sapiens sapiens is
<br>
ungrounded anthropocentrism.
<br>
&nbsp;
<br>
<em>&gt; &gt; If anyone thinks they
</em><br>
<em>&gt; &gt; have one, I would be very interested. And there's
</em><br>
<em>&gt; &gt; currently no good reason I can see why Friendly AI
</em><br>
<em>&gt; &gt; shouldn't be possible.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I can -- or at least, why it wouldn't be stable.
</em><br>
<p>Then please, by all means, show me the proof.
<br>
<p><em>&gt; There are several
</em><br>
<em>&gt; problems here, including the fact that there is no
</em><br>
<em>&gt; absolute morality (and
</em><br>
<em>&gt; thus no way to universally determine &quot;the good&quot;),
</em><br>
<p>This is the postition of subjective morality, which is
<br>
far from proven. It's not a 'fact', it is a
<br>
possibility.
<br>
<p><em>&gt; that it is not
</em><br>
<em>&gt; obvious that one could construct something far more
</em><br>
<em>&gt; intelligent than
</em><br>
<em>&gt; yourself
</em><br>
<p>Perhaps we truly can't construct something vastly more
<br>
intelligent than ourselves. But it doesn't take that:
<br>
it just takes some sort of seed with decent general
<br>
intelligence, reprogrammable code, and Friendliness.
<br>
<p><em>&gt; and still manage to constrain its behavior
</em><br>
<em>&gt; effectively,
</em><br>
<p>You can't 'constrain' a transhuman. If you can tell me
<br>
a reasonable proposal for constraining a transhuman, I
<br>
will immediately reject it on the grounds that the
<br>
'constraints' will turn out to have a simple
<br>
workaround that humans, myself and Einstein included,
<br>
are too dumb to see. A transhuman, by definition, is
<br>
smarter than humans and thus will almost certainly
<br>
find a quick workaround to any constraining proposal
<br>
that we implement that we can't see in advance because
<br>
we're not smart enough. Read CFAI on the adversarial
<br>
attitude. And even if we could 'constrain'
<br>
transhumans, what would the world be like if mice
<br>
could 'constrain' us? The differential in intelligence
<br>
between us and transhumans is a lot bigger than that
<br>
between us and mice.
<br>
<p><em>&gt; that
</em><br>
<em>&gt; it is not clear that a construct like this would be
</em><br>
<em>&gt; able to battle it
</em><br>
<em>&gt; out effectively against other constructs from
</em><br>
<em>&gt; societies that do not
</em><br>
<em>&gt; construct Friendly AIs (or indeed that the winner in
</em><br>
<em>&gt; the universe
</em><br>
<em>&gt; won't be the societies that produce the meanest,
</em><br>
<em>&gt; baddest-assed
</em><br>
<em>&gt; intelligences rather than the friendliest -- see
</em><br>
<em>&gt; evolution on earth),
</em><br>
<em>&gt; etc.
</em><br>
<p>Battle it out? The 'winner'? The 'winner' in this case
<br>
is the AI who makes it to superintelligence first.
<br>
Probably the first thing a superintelligence would do
<br>
is go to all the unFriendly AI projects and not only
<br>
say &quot;This is a really, really bad idea&quot;, but persuade
<br>
everybody of it. A superintelligent AI would have
<br>
better persuasion capabilities than a politician.
<br>
<p><em>&gt; Anyway, I find it interesting to speculate on
</em><br>
<em>&gt; possible constructs like
</em><br>
<em>&gt; The Friendly AI, but not safe to assume that they're
</em><br>
<em>&gt; going to be in
</em><br>
<em>&gt; one's future.
</em><br>
<p>Of course you can't assume that there will be a
<br>
Singularity caused by a Friendly AI, but I'm pretty
<br>
darn sure I want it to happen!
<br>
<p><em>&gt; The prudent transhumanist considers
</em><br>
<em>&gt; survival in wide
</em><br>
<em>&gt; variety of scenarios.
</em><br>
<p>Survival? If the first transhuman is Friendly,
<br>
survival is a given, unless you decide to commit
<br>
suicide. If the first transhuman is unFriendly, you're
<br>
either dead, or have an INU (Infinite Negative
<br>
Utility) future.
<br>
<p>__________________________________
<br>
Do you Yahoo!?
<br>
Find out what made the Top Yahoo! Searches of 2003
<br>
<a href="http://search.yahoo.com/top2003">http://search.yahoo.com/top2003</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7401.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7399.html">Robin Lee Powell: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>In reply to:</strong> <a href="7395.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0401/7447.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="../0401/7447.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7400">[ date ]</a>
<a href="index.html#7400">[ thread ]</a>
<a href="subject.html#7400">[ subject ]</a>
<a href="author.html#7400">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:43 MDT
</em></small></p>
</body>
</html>
