<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: An essay I just wrote on the Singularity.</title>
<meta name="Author" content="Robin Lee Powell (rlpowell@digitalkingdom.org)">
<meta name="Subject" content="Re: An essay I just wrote on the Singularity.">
<meta name="Date" content="2003-12-31">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: An essay I just wrote on the Singularity.</h1>
<!-- received="Wed Dec 31 12:25:11 2003" -->
<!-- isoreceived="20031231192511" -->
<!-- sent="Wed, 31 Dec 2003 11:25:07 -0800" -->
<!-- isosent="20031231192507" -->
<!-- name="Robin Lee Powell" -->
<!-- email="rlpowell@digitalkingdom.org" -->
<!-- subject="Re: An essay I just wrote on the Singularity." -->
<!-- id="20031231192507.GJ11973@digitalkingdom.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20031231144611.5173.qmail@web11701.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Robin Lee Powell (<a href="mailto:rlpowell@digitalkingdom.org?Subject=Re:%20An%20essay%20I%20just%20wrote%20on%20the%20Singularity."><em>rlpowell@digitalkingdom.org</em></a>)<br>
<strong>Date:</strong> Wed Dec 31 2003 - 12:25:07 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="7398.html">Robin Lee Powell: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7396.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>In reply to:</strong> <a href="7385.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7401.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="7401.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7397">[ date ]</a>
<a href="index.html#7397">[ thread ]</a>
<a href="subject.html#7397">[ subject ]</a>
<a href="author.html#7397">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Wed, Dec 31, 2003 at 06:46:11AM -0800, Tommy McCabe wrote:
<br>
<em>&gt; I have several objections to the points raised in this essay. You
</em><br>
<em>&gt; say that the Singularity is a bad term for these reasons:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &quot;We can't see what lies beyond twenty minutes from now regardless.
</em><br>
<em>&gt; Just ask a stock broker.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; We can't predict the future down to the letter, true, but I'd be
</em><br>
<em>&gt; willing to bet in twenty years, let alone twenty minutes, the
</em><br>
<em>&gt; planet called Earth will be here, the stockbroker will be either
</em><br>
<em>&gt; still human, or dead, and there will still be a stock market. If
</em><br>
<em>&gt; the Singularity happened, you couldn't guarantee any of those.
</em><br>
<p>Granted.  I put that in there at least partly because every single
<br>
one of my friends has brought up that objection.  Enough people have
<br>
pointed it out that I've stopped wanting to have that argument, or
<br>
even say anything that might start it.
<br>
<p><em>&gt; &quot;This has next to nothing to do with the mathematical term
</em><br>
<em>&gt; 'singularity' it was derived from, as that implies a total
</em><br>
<em>&gt; cessation of the graph beyond a certain point, which isn't the
</em><br>
<em>&gt; kind of implication I want to have made about humanity's future!&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Total cessation? Yes, on a hyperbolic graph, at a certain point,
</em><br>
<em>&gt; the function will be undefined. 
</em><br>
<p>That's the definition of a mathematical singularity: the
<br>
undefined point of a function.
<br>
<p><em>&gt; However, 1), actual values are not constrained to follow
</em><br>
<em>&gt; mathematical equations, and 2), that signifies something unknown
</em><br>
<em>&gt; happens. Maybe it stops, maybe it slows down to exponential
</em><br>
<em>&gt; growth, or even linear, maybe it somehow will go to infinity.
</em><br>
<em>&gt; We're currently too dumb to know.
</em><br>
<p>Yes, I completely agree.  That's why the term singularity, in its
<br>
mathematical sense, doesn't fit.
<br>
<p><em>&gt; &quot;It fails, IMO, to address the important point: the coming change,
</em><br>
<em>&gt; whatever you call it, will not be created by technology in any
</em><br>
<em>&gt; respect (technological growth, new technologies, what have you).
</em><br>
<em>&gt; It will happen if, and only if, greater than human intelligence
</em><br>
<em>&gt; becomes part of humanity's reality. Then, and only then, will we
</em><br>
<em>&gt; truly have reached a point where things have fundamentally changed
</em><br>
<em>&gt; because then we're dealing with minds creating technology that are
</em><br>
<em>&gt; fundamentally alien to us, or at least better. At that point,
</em><br>
<em>&gt; things have truly changed.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Technology is defined as &quot;The application of science to practical
</em><br>
<em>&gt; problems&quot;, or something like that. Therefore, any
</em><br>
<em>&gt; superintelligence we create must be technology, or at least the
</em><br>
<em>&gt; product of technology. An infrahuman AI is a mind that is
</em><br>
<em>&gt; fundamentally alien to us, but it's not going to outperform a
</em><br>
<em>&gt; human on at least some things (by definition). The bridge here is
</em><br>
<em>&gt; at the point where technology creates something smarter than us,
</em><br>
<em>&gt; which is unlike other technology, because it could think of things
</em><br>
<em>&gt; no human could ever possibly think of.
</em><br>
<p>I agree with all of that.  In fact, I think we're violently agreeing
<br>
on that point.  I simply feel that people focus on the wrong thing
<br>
when talking about the singularity: growth of technology, instead of
<br>
growth of intelligence.
<br>
<p><em>&gt; As a matter of fact, my only complaint against the term
</em><br>
<em>&gt; 'Singularity' has been that people tend to see it as a Bible-type
</em><br>
<em>&gt; doomsday.
</em><br>
<p>That only mildly annoys me, but I'll address that in another
<br>
response.
<br>
<p><em>&gt; Quote from the essay: &quot; speaking of the (still very theoretical)
</em><br>
<em>&gt; possibility of human-level generalized intelligence.&quot; Even though
</em><br>
<em>&gt; a human-level AI is extermely likely to become transhuman in about
</em><br>
<em>&gt; 15 seconds, human-level AI isn't the ultimate goal. Transhuman AI
</em><br>
<em>&gt; is.
</em><br>
<p>Granted.  Is there a way I could have made that more clear?  (I just
<br>
realized I didn't use the term 'transhuman' at all; that's
<br>
unfortunate).
<br>
<p><em>&gt; Next point: Even though transhuman AIs can (and should) have the
</em><br>
<em>&gt; capability to understand emotions, having emotions built into the
</em><br>
<em>&gt; AI is not a good idea by any means; you could end up with critical
</em><br>
<em>&gt; failure scenarios #'s 9, 12, 19, 21, and probably some others.
</em><br>
<p>Yeah; I had a *really* tough time making that distinction.
<br>
Suggestions welcome, but bear in mind this essay is aimed at SLs 1
<br>
and 2, if that.
<br>
<p><em>&gt; I agree with most of the things said about nanotech, except for
</em><br>
<em>&gt; the last one: &quot;The big difference with nanotech versus nukes is
</em><br>
<em>&gt; that there is a first strike advantage. A huge one.&quot; The first
</em><br>
<em>&gt; strike advantage in this case, is probably going to be being
</em><br>
<em>&gt; responsible for the destruction of the planet.
</em><br>
<p>Well, yes, probably.  But if you *think* you can get to your enemies
<br>
without destroying yourself (say, because your nanobots only trigger
<br>
on certain genetic markers), it *looks* like there's a first strike
<br>
advantage.  I'll re-phrase it that way, thanks.
<br>
<p><em>&gt; A Friendly AI doesn't have the supergoal of being nice to humans;
</em><br>
<em>&gt; it has the supergoal of acting friendly toward other sentients in
</em><br>
<em>&gt; general. A Friendly AI that is Friendly with humans shouldn't try
</em><br>
<em>&gt; to blow the same humans to smithereens the minute they upload.
</em><br>
<p>All that's required there is for the AI to still recognize them as
<br>
human, which hardly seems a stretch for general intelligence.  I
<br>
wouldn't necessarily want an FAI to be friendly to any aliens that
<br>
came along.  Not *necessarily*; it might be the right idea, it might
<br>
not, but I'd like the FAI to have the mental option of deciding,
<br>
&quot;Umm, these aliens are fundamentally unfriendly to humans, and I
<br>
can't fix that without re-writing their brains, so I better defend
<br>
humanity (and myself) from them&quot;.
<br>
<p><em>&gt; I agree with the assumption that transhuman AI is possible,
</em><br>
<em>&gt; however, creating copies of yourself and then modifying the copies
</em><br>
<em>&gt; is not only an inefficient use of computing power, but any AI
</em><br>
<em>&gt; smart enough to have reached the stage of reprogramming ver own
</em><br>
<em>&gt; source code is likely to have the power to destroy the planet, or
</em><br>
<em>&gt; will have it very soon, and thus modelling the AI in it's full
</em><br>
<em>&gt; runs the risk of the model, or the copy, or whatever you want to
</em><br>
<em>&gt; call it, destroying the planet. 
</em><br>
<p>There are many easy ways to get around this, but I was actually
<br>
referring to uploads at that point.
<br>
<p><em>&gt; Recursive self-improvement is a vastly more powerful version of
</em><br>
<em>&gt; design-and-test; the testing part for any AI with the capability
</em><br>
<em>&gt; to blow up the planet should use the partial AI shadowself
</em><br>
<em>&gt; described in CFAI: Wisdom tournaments.
</em><br>
<p>&lt;nod&gt;  Absolutely.  But this is a non-technical essay.
<br>
<p><em>&gt; I also agree that strong nanotech is possible.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The last assumption, namely, that A Friendly, Superintelligent AI
</em><br>
<em>&gt; Gets To Nanotech First shouldn't be taken as an assumption at all.
</em><br>
<em>&gt; It's what I'd like to happen, certainly, but there's no money-back
</em><br>
<em>&gt; guarantee on it. Nanotech is already far more advanced than AI
</em><br>
<em>&gt; (see MistakesOfClassicalAI on the Wiki).
</em><br>
<p>I'm sorry, which wiki?
<br>
<p>Anyways, it was stated as an assumption because the goal of the
<br>
essay is to present the Sysop Scenario to SL1 level friends and
<br>
family of mine.  The sysop scenario can't happen without that
<br>
assumed event.
<br>
<p><em>&gt; I agree that getting nanotech when you are a transhuman is likely
</em><br>
<em>&gt; to be easy, but note that you don't even need to convince the
</em><br>
<em>&gt; programmers to hook up nanotech tools. A superintelligent AI could
</em><br>
<em>&gt; simply copy itself, send the copy over the Internet, and take over
</em><br>
<em>&gt; all the computers in the most advanced nanotech lab on the planet.
</em><br>
<p>You know, I swear I never thought of that.
<br>
<p>I'm not adding it to the essay, either; it was a very fine line to
<br>
both present the idea of &quot;This being is amazingly powerful&quot; and not
<br>
inspire absolute terror at the idea of such a being existing.
<br>
<p><em>&gt; I also think that even having to 'convince' the programmers is
</em><br>
<em>&gt; overkill; if the AI is Friendly, the programmers should just give
</em><br>
<em>&gt; it the necessary tools; no tricking required.
</em><br>
<p>Of course, and I think I pointed that out.
<br>
<p><em>&gt; I also agree that the first one to nanotech pretty much has
</em><br>
<em>&gt; ultimate control of the planet.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Ruling The World is a very, very bad term for it; it confuses the
</em><br>
<em>&gt; possession of absolute physical power with the exercise of
</em><br>
<em>&gt; absolute social power. The former is probably wanted; the latter
</em><br>
<em>&gt; isn't, and we're dealing with a being without a tendency to abuse
</em><br>
<em>&gt; power. I agree that a superintelligence in certainly capable of
</em><br>
<em>&gt; the examples given, however, that's probably just the beginning of
</em><br>
<em>&gt; the list of stuff a superintelligence would be capable of.
</em><br>
<p>Of course.  If you have a suggestion for a better section header,
<br>
I'm all ears.
<br>
<p>-Robin
<br>
<p><pre>
-- 
Me: <a href="http://www.digitalkingdom.org/~rlpowell/">http://www.digitalkingdom.org/~rlpowell/</a>  ***   I'm a *male* Robin.
&quot;Constant neocortex override is the only thing that stops us all
from running out and eating all the cookies.&quot;  -- Eliezer Yudkowsky
<a href="http://www.lojban.org/">http://www.lojban.org/</a>             ***              .i cimo'o prali .ui
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7398.html">Robin Lee Powell: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Previous message:</strong> <a href="7396.html">Perry E. Metzger: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>In reply to:</strong> <a href="7385.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7401.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<li><strong>Reply:</strong> <a href="7401.html">Tommy McCabe: "Re: An essay I just wrote on the Singularity."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#7397">[ date ]</a>
<a href="index.html#7397">[ thread ]</a>
<a href="subject.html#7397">[ subject ]</a>
<a href="author.html#7397">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:43 MDT
</em></small></p>
</body>
</html>
