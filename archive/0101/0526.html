<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Basement Education</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Basement Education">
<meta name="Date" content="2001-01-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Basement Education</h1>
<!-- received="Mon Jan 29 21:52:32 2001" -->
<!-- isoreceived="20010130045232" -->
<!-- sent="Mon, 29 Jan 2001 18:34:28 -0800" -->
<!-- isosent="20010130023428" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Basement Education" -->
<!-- id="3A762834.CEA56153@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3A75B614.471CD889@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Basement%20Education"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Mon Jan 29 2001 - 19:34:28 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0527.html">Samantha Atkins: "Re: friendly ai"</a>
<li><strong>Previous message:</strong> <a href="0525.html">Samantha Atkins: "Re: Event horizon &amp; Moravec"</a>
<li><strong>In reply to:</strong> <a href="0523.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0529.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<li><strong>Reply:</strong> <a href="0529.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#526">[ date ]</a>
<a href="index.html#526">[ thread ]</a>
<a href="subject.html#526">[ subject ]</a>
<a href="author.html#526">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&quot;Eliezer S. Yudkowsky&quot; wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Samantha Atkins wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt; Every minute that I ask an AI to deliberately
</em><br>
<em>&gt; &gt; &gt; delay takeoff puts another hundred deaths on *my* *personal*
</em><br>
<em>&gt; &gt; &gt; responsibility as a Friendship programmer.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; This is not balanced thinking.  You are not personally responsible for
</em><br>
<em>&gt; &gt; all the misery of the world.  That you think you have a fix for a large
</em><br>
<em>&gt; &gt; part of it, potentially, does not mean that delaying that fix for
</em><br>
<em>&gt; &gt; safeties sake makes you responsible personally for what it may (or may
</em><br>
<em>&gt; &gt; not) have fixed.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The key word in that paragraph is &quot;potentially&quot;.  See below.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; &gt; In introducing an artificial
</em><br>
<em>&gt; &gt; &gt; delay, I would be gambling with human lives - gambling that the
</em><br>
<em>&gt; &gt; &gt; probability of error is great enough to warrant deliberate slowness,
</em><br>
<em>&gt; &gt; &gt; gambling on the possibility that the AI wouldn't just zip off to
</em><br>
<em>&gt; &gt; &gt; superintelligence and Friendliness.  With six billion lives on the line, a
</em><br>
<em>&gt; &gt; &gt; little delay may be justified, but it has to be the absolute minimum
</em><br>
<em>&gt; &gt; &gt; delay.  Unless major problems turn up, a one-week delay would be entering
</em><br>
<em>&gt; &gt; &gt; Hitler/Stalin territory.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; No.  It has to be enough delay to be as certain as possible that it will
</em><br>
<em>&gt; &gt; not eat the 6 billion people for lunch.  In the face of that as even a
</em><br>
<em>&gt; &gt; remote possibility there is no way it is sane to speak of being a Hitler
</em><br>
<em>&gt; &gt; if you delay one week.  Please recalibrate on this.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If I take a vacation to decompress, *today*, I don't feel guilty; that
</em><br>
<em>&gt; comes under the classification of sane self-management.  Doing a one-week
</em><br>
<em>&gt; delay *after* the AI reaches the point of hard takeoff... I guess my mind
</em><br>
<em>&gt; just processes it differently.  It's like the difference between saying
</em><br>
<em>&gt; that &quot;ExI is a more effective charity than CARE&quot;, and actually looting
</em><br>
<em>&gt; CARE's bank account.  Logically, giving eight dollars of your money to ExI
</em><br>
<em>&gt; instead of CARE should have the same consequences as stealing eight
</em><br>
<em>&gt; dollars from CARE instead of giving it to ExI... but, morally, that's not
</em><br>
<em>&gt; how it works.
</em><br>
<em>&gt; 
</em><br>
<p>After hard take off point won't it be irrelevant whether any mere
<br>
mortal, even yourself, takes a week off or not?  
<br>
<p>I am really confused by your analogy.  Logically not giving your money
<br>
to X is not the same at all as stealing from X since the concept of
<br>
stealing requires taking something from its rightful owner.  But X
<br>
doesn't own your money.  You do.  
<br>
<p><em>&gt; Before the AI reaches hard takeoff, it's your time that you're investing
</em><br>
<em>&gt; in the AI, to the benefit of everyone in the world perhaps, but yours to
</em><br>
<em>&gt; invest in whatever payoff-maximizing strategy seems best.  After the AI
</em><br>
<em>&gt; reaches the potential for hard takeoff, it's *their* time - and lives -
</em><br>
<em>&gt; that you're stealing.
</em><br>
<em>&gt; 
</em><br>
<p>I still don't get it.  You are not synonymous with the SI.  You still
<br>
don't know the SI will actually be a net salvation for humanity.  It is
<br>
pointless to see this as stealing or in such moralistic terms.  If you
<br>
do not take extremely reasonable care then you will be unleashing mass
<br>
destruction instead of mass salvation.  That is a moral responsibility I
<br>
understand.
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0527.html">Samantha Atkins: "Re: friendly ai"</a>
<li><strong>Previous message:</strong> <a href="0525.html">Samantha Atkins: "Re: Event horizon &amp; Moravec"</a>
<li><strong>In reply to:</strong> <a href="0523.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0529.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<li><strong>Reply:</strong> <a href="0529.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#526">[ date ]</a>
<a href="index.html#526">[ thread ]</a>
<a href="subject.html#526">[ subject ]</a>
<a href="author.html#526">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
