<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Basement Education</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Basement Education">
<meta name="Date" content="2001-01-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Basement Education</h1>
<!-- received="Wed Jan 24 22:01:50 2001" -->
<!-- isoreceived="20010125050150" -->
<!-- sent="Wed, 24 Jan 2001 22:01:45 -0500" -->
<!-- isosent="20010125030145" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Basement Education" -->
<!-- id="3A6F9719.3463B9B3@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3A6F90A3.AB05D9BD@objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Basement%20Education"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Jan 24 2001 - 20:01:45 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0464.html">Dale Johnstone: "Re: Basement Education"</a>
<li><strong>Previous message:</strong> <a href="0462.html">Samantha Atkins: "Re: Basement Education"</a>
<li><strong>In reply to:</strong> <a href="0462.html">Samantha Atkins: "Re: Basement Education"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0515.html">Samantha Atkins: "Re: Basement Education"</a>
<li><strong>Reply:</strong> <a href="0515.html">Samantha Atkins: "Re: Basement Education"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#463">[ date ]</a>
<a href="index.html#463">[ thread ]</a>
<a href="subject.html#463">[ subject ]</a>
<a href="author.html#463">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Samantha Atkins wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &quot;Eliezer S. Yudkowsky&quot; wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; What kind of experience?  Experience about grain futures, or experience
</em><br>
<em>&gt; &gt; about how to design a seed AI?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In order for the SI to run much of anything other than its own
</em><br>
<em>&gt; development it will need both.
</em><br>
<p>An SI may need both; an SI can get both, very easily, through a few
<br>
nondestructive brain scans.  The question is how much experience is
<br>
required during the pre-nanotechnology stage.
<br>
<p><em>&gt; &gt; &gt; I'm sure you'd agree giving an inexperienced newborn AI access to nanotech
</em><br>
<em>&gt; &gt; &gt; is a bad idea.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; If it's an inexperienced newborn superintelligent AI, then I don't have
</em><br>
<em>&gt; &gt; much of a choice.  If not, then it seems to me that the operative form of
</em><br>
<em>&gt; &gt; experience, for this app, is experience in Friendliness.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You certainly do have a choice. If you do not hook the system up in such
</em><br>
<em>&gt; a way that it controls hardware manufacturing at all levels until it is
</em><br>
<em>&gt; a bit more seasoned, that would be a quite prudent step.
</em><br>
<p>Prudent, maybe; effective, almost certainly not.  A superintelligence has
<br>
access to *me*.  Ve has access to external reality... would ve really
<br>
notice all that much of a difference whether the particular quark-swirls
<br>
ve contacts are labeled &quot;hardware manufacturing&quot; or &quot;Eliezer Yudkowsky&quot;?
<br>
<p>If you're gonna win, win *before* you have a hostile superintelligence on
<br>
your hands.  That's common sense.
<br>
<p><em>&gt; &gt; Where does experience in Friendliness come from?  Probably
</em><br>
<em>&gt; &gt; question-and-answer sessions with the programmers, plus examination of
</em><br>
<em>&gt; &gt; online social material and technical literature to fill in references to
</em><br>
<em>&gt; &gt; underlying causes.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; That would not be enough to develop common sense by itself.  Too much is
</em><br>
<em>&gt; assumed of the underlying presumed human context in the literature.
</em><br>
<p>I think you're wrong about this.
<br>
<p><em>&gt; I think you are guessing wrong unless quite a bit of the detailed common
</em><br>
<em>&gt; sense is developed or entered before the young AI goes off examining
</em><br>
<em>&gt; papers and running simulations. Knowing the architecture of human minds
</em><br>
<em>&gt; is not sufficient for having working knowledge of hwo to deal with human
</em><br>
<em>&gt; beings.
</em><br>
<p>Well, I disagree.  In my own experience, the amount of real-world
<br>
experience needed decreases pretty sharply as a function of the ability to
<br>
theorize about the causation of the observed experiences you already have.
<br>
<p><em>&gt; &gt; Incidentally, I should note that, as I visualize this &quot;gradual growth&quot;
</em><br>
<em>&gt; &gt; process, it shouldn't take very long.  From the moment the AI realizes a
</em><br>
<em>&gt; &gt; hard takeoff lies ahead to the moment human-timescale phase terminates,
</em><br>
<em>&gt; &gt; should be... oh... twelve hours or so.  Because the instant that the AI
</em><br>
<em>&gt; &gt; says it's ready for a hard takeoff, you are operating on Singularity time
</em><br>
<em>&gt; &gt; - in other words, six thousand people are dying for every hour delayed.
</em><br>
<em>&gt; &gt; Ideally we'd see that the AI was getting all the Friendliness decisions
</em><br>
<em>&gt; &gt; more or less right during the controlled ascent, in which case we could
</em><br>
<em>&gt; &gt; push ahead as fast as humanly possible.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; How can human programmers can answer a sufficient number of the AIs
</em><br>
<em>&gt; questions in a mere 12 hours?
</em><br>
<p>If the human programmers need to provide serious new Friendship content
<br>
rather than just providing feedback on the AI's own actions, then one may
<br>
be justified in going a little slower.  If the AI is getting everything
<br>
right and the humans are just watching, then zip along as fast as
<br>
possible.
<br>
<p><em>&gt; AI time is not the gating factor in this
</em><br>
<em>&gt; phase.  And there is no reason to rush it.  So many people dying per
</em><br>
<em>&gt; hour is irrelevant and emotionalizes the conversation unnecessarily.
</em><br>
<em>&gt; Letting the AI loose too early can easily terminate all 6 billion+ of
</em><br>
<em>&gt; us.
</em><br>
<p>Yes, that is the only reason why it makes sense to take the precaution at
<br>
all.  I do not believe that so many people dying per hour is
<br>
&quot;irrelevant&quot;.  I think that, day in, day out, one hundred and fifty
<br>
thousand people die - people with experiences and memories and lives every
<br>
bit as valuable as my own.  Every minute that I ask an AI to deliberately
<br>
delay takeoff puts another hundred deaths on *my* *personal*
<br>
responsibility as a Friendship programmer.  In introducing an artificial
<br>
delay, I would be gambling with human lives - gambling that the
<br>
probability of error is great enough to warrant deliberate slowness,
<br>
gambling on the possibility that the AI wouldn't just zip off to
<br>
superintelligence and Friendliness.  With six billion lives on the line, a
<br>
little delay may be justified, but it has to be the absolute minimum
<br>
delay.  Unless major problems turn up, a one-week delay would be entering
<br>
Hitler/Stalin territory.
<br>
<p><em>&gt; &gt; If the AI was Friendliness-savvy enough during the prehuman training
</em><br>
<em>&gt; &gt; phase, we might want to eliminate the gradual phase entirely, thus
</em><br>
<em>&gt; &gt; removing what I frankly regard as a dangerous added step.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; How does it become dependably Friendliness-savvy without the feedback?
</em><br>
<em>&gt; Or do I misunderstand what gradual phase you want to eliminate?
</em><br>
<p>I think so - the scenario I was postulating was that the AI became
<br>
Friendliness-savvy during the pre-hard-takeoff phase, so that you're
<br>
already pretty confident by the time the AI reaches the hard-takeoff
<br>
level.  This doesn't require perfection, it just requires that the AI
<br>
display the minimal &quot;seed Friendliness&quot; needed to not take any precipitate
<br>
actions until ve can fill in the blanks by examining a nondestructive
<br>
brain scan.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0464.html">Dale Johnstone: "Re: Basement Education"</a>
<li><strong>Previous message:</strong> <a href="0462.html">Samantha Atkins: "Re: Basement Education"</a>
<li><strong>In reply to:</strong> <a href="0462.html">Samantha Atkins: "Re: Basement Education"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0515.html">Samantha Atkins: "Re: Basement Education"</a>
<li><strong>Reply:</strong> <a href="0515.html">Samantha Atkins: "Re: Basement Education"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#463">[ date ]</a>
<a href="index.html#463">[ thread ]</a>
<a href="subject.html#463">[ subject ]</a>
<a href="author.html#463">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
