<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: friendly ai</title>
<meta name="Author" content="Ben Goertzel (ben@webmind.com)">
<meta name="Subject" content="RE: friendly ai">
<meta name="Date" content="2001-01-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: friendly ai</h1>
<!-- received="Sun Jan 28 15:40:37 2001" -->
<!-- isoreceived="20010128224037" -->
<!-- sent="Sun, 28 Jan 2001 15:38:26 -0500" -->
<!-- isosent="20010128203826" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@webmind.com" -->
<!-- subject="RE: friendly ai" -->
<!-- id="NDBBIBGFAPPPBODIPJMMIEDLFBAA.ben@webmind.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3A74810C.13E77477@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@webmind.com?Subject=RE:%20friendly%20ai"><em>ben@webmind.com</em></a>)<br>
<strong>Date:</strong> Sun Jan 28 2001 - 13:38:26 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0505.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>Previous message:</strong> <a href="0503.html">Pvthur@aol.com: "Re: Beyond evolution"</a>
<li><strong>In reply to:</strong> <a href="0501.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0509.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0509.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0511.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#504">[ date ]</a>
<a href="index.html#504">[ thread ]</a>
<a href="subject.html#504">[ subject ]</a>
<a href="author.html#504">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; I can't visualize an AI incapable of learning making it out of the lab or
</em><br>
<em>&gt; even walking across the room, much less doing one darn thing towards
</em><br>
<em>&gt; bringing citizenship rights to the Solar System.
</em><br>
<p>No, I was unclear.  Honest Annie was an AI that got so smart that it just
<br>
shut up and stopped communicating with people altogether....  Radio silence.
<br>
<p><em>&gt; Remember, the hypothesis is that Friendliness is the top layer of a good
</em><br>
<em>&gt; design, and discovering and creation the subgoals; if you postulate an AI
</em><br>
<em>&gt; that violates this rule and see horrifying consequences, it should
</em><br>
<em>&gt; probably be taken as an argument in favor of _Friendly AI_.  &lt;grin&gt;
</em><br>
<p>I don't think I'm envisioning horrifying consequences at all.  AI's getting
<br>
bored
<br>
with humans isn't all that horrifying, is it?  Especially if humans are all
<br>
uploading
<br>
themselves... then, most humans are going to be bored with old-style flesher
<br>
humans
<br>
too...
<br>
<p><em>&gt; If the system isn't smart enough to see the massive importance of
</em><br>
<em>&gt; learning, use a programmer intervention to add the fact to the system that
</em><br>
<em>&gt; &quot;Ben Goertzel says learning is massively important&quot;.  If the system
</em><br>
<em>&gt; assumes that &quot;Ben Goertzel says X&quot; translates to &quot;as a default, X has a
</em><br>
<em>&gt; high probability of being true&quot;, and a prehuman AI should make this
</em><br>
<em>&gt; assumption (probably due to another programmer intervention), then this
</em><br>
<em>&gt; should raise the weight of the learning subgoal.
</em><br>
<p>Yeah, this is basically what we've done by explicitly  making learning
<br>
a system goal
<br>
<p><p><em>&gt; Speaking as one of the six billion people who gets toasted if you make one
</em><br>
<em>&gt; little mistake, would you *please* consider adding Friendliness to that
</em><br>
<em>&gt; list?  I really don't think it will cost you anything.
</em><br>
<p>Friendliness is indeed one of webmind's goals ;&gt;
<br>
<p><em>&gt; I've evolved from subgoal-driven to supergoal-driven over time.  I can see
</em><br>
<em>&gt; this as possible, but I really can't see it as inevitable, not if the AI
</em><br>
<em>&gt; is on guard and doesn't want it to happen.  Evolution has to happen in
</em><br>
<em>&gt; steps, and steps can be observed, detected, and counteracted.  A failure
</em><br>
<em>&gt; of Friendliness in a seed AI vanishes as soon as the AI realizes it's a
</em><br>
<em>&gt; failure; it takes a catastrophic failure of Friendliness, something that
</em><br>
<em>&gt; makes the AI stop *wanting* to be Friendly, before errors can build up in
</em><br>
<em>&gt; the system.
</em><br>
<p>I don't know.  Can't &quot;stopping wanting to be friendly&quot; creep up gradually
<br>
too?
<br>
<p>Then it's &quot;wanting to want to be friendly&quot; that's supposed to stop this from
<br>
happening??
<br>
<p><p><em>&gt; If there's a society of Friendly AIs, they'll *notice* that new AIs are a
</em><br>
<em>&gt; little bit less Friendly than the originals,
</em><br>
<p>Unless the society slowly drifts away from a human-centric focus ...
<br>
gradual culture drift is not exactly unknown...
<br>
<p><em>&gt; AIs who are dreadfully panicked
</em><br>
<em>&gt; about the prospect of drifting away from Friendship because Friendship is
</em><br>
<em>&gt; the only important thing in the world to them...
</em><br>
<em>&gt;
</em><br>
<p>aha!  Caught you!
<br>
<p>Now you're proposing to make AI's neurotic and mentally unhealthy... to make
<br>
them
<br>
fear becoming unfriendly
<br>
<p>But isn't this a recipe for backlash of some sort??  Fear breeds aggression,
<br>
no?
<br>
<p>Took took took...
<br>
<p>ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0505.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>Previous message:</strong> <a href="0503.html">Pvthur@aol.com: "Re: Beyond evolution"</a>
<li><strong>In reply to:</strong> <a href="0501.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0509.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0509.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0511.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#504">[ date ]</a>
<a href="index.html#504">[ thread ]</a>
<a href="subject.html#504">[ subject ]</a>
<a href="author.html#504">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
