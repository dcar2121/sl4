<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Basement Education</title>
<meta name="Author" content="Dale Johnstone (DaleJohnstone@email.com)">
<meta name="Subject" content="Re: Basement Education">
<meta name="Date" content="2001-01-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Basement Education</h1>
<!-- received="Wed Jan 24 22:13:50 2001" -->
<!-- isoreceived="20010125051350" -->
<!-- sent="Thu, 25 Jan 2001 03:11:01 -0000" -->
<!-- isosent="20010125031101" -->
<!-- name="Dale Johnstone" -->
<!-- email="DaleJohnstone@email.com" -->
<!-- subject="Re: Basement Education" -->
<!-- id="002b01c0867c$73907e90$a5319fd4@xanadu" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="3A6F54CE.FB139D53@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Dale Johnstone (<a href="mailto:DaleJohnstone@email.com?Subject=Re:%20Basement%20Education"><em>DaleJohnstone@email.com</em></a>)<br>
<strong>Date:</strong> Wed Jan 24 2001 - 20:11:01 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0466.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<li><strong>Previous message:</strong> <a href="0464.html">Dale Johnstone: "Re: Basement Education"</a>
<li><strong>In reply to:</strong> <a href="0460.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0466.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<li><strong>Reply:</strong> <a href="0466.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#465">[ date ]</a>
<a href="index.html#465">[ thread ]</a>
<a href="subject.html#465">[ subject ]</a>
<a href="author.html#465">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer wrote:
<br>
<p><p><em>&gt; Dale Johnstone wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Eliezer wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt; What actually happens is that genetic engineering and neurohacking and
</em><br>
<em>&gt; &gt; &gt; human-computer interfaces don't show up, because they'd show up in
</em><br>
2020,
<br>
<em>&gt; &gt; &gt; and a hard takeoff occurs in SIAI's or Webmind's basement sometime in
</em><br>
the
<br>
<em>&gt; &gt; &gt; next ten years or so.  Even if the hardware for nanotechnology takes
</em><br>
<em>&gt; &gt; &gt; another couple of weeks to manufacture, and even if you're asking the
</em><br>
<em>&gt; &gt; &gt; newborn SI questions that whole time, no amount of explanation is
</em><br>
going to
<br>
<em>&gt; &gt; &gt; be equivalent to the real thing.  There still comes a point when the
</em><br>
SI
<br>
<em>&gt; &gt; &gt; says that the Introdus wavefront is on the way, and you sit there
</em><br>
waiting
<br>
<em>&gt; &gt; &gt; for the totally unknowable future to hit you in the next five seconds.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; In order for there to be a hard takeoff the AI must be capable of
</em><br>
building
<br>
<em>&gt; &gt; up a huge amount of experience quickly.
</em><br>
<em>&gt;
</em><br>
<em>&gt; What kind of experience?  Experience about grain futures, or experience
</em><br>
<em>&gt; about how to design a seed AI?
</em><br>
<p>Oh c'mon, that's a silly question. From past conversations with you I know
<br>
we both have experience and understanding of classical AI and how it has
<br>
failed. Experience is undeniably useful and forms the foundation of what we
<br>
do today. A mind that can't build on experience is unnecessarily hobbled.
<br>
<p><p><em>&gt; &gt; It takes years for a human child.
</em><br>
<em>&gt; &gt; Obviously we can crank up the AI's clock rate, but how do you plan for
</em><br>
it to
<br>
<em>&gt; &gt; gain experience when the rest of the world is running in slow motion?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Accumulation of internal experience is limited only by computing power.
</em><br>
<em>&gt; Experience of the external world will be limited either by rates of
</em><br>
<em>&gt; sensory input or by computing power available to process sensory input.
</em><br>
<p>Yes, obviously, and (at least in a pre-nanotech scenario) lack of computing
<br>
power will limit it's rate of growth. So I don't see how it will be a hard
<br>
takeoff. What kind of processing power do intend to use?
<br>
<p><p><em>&gt; &gt; Some things can be deduced, others can be learnt from simulations. How
</em><br>
does it
<br>
<em>&gt; &gt; learn about people and human culture in general? From books &amp; the
</em><br>
internet?
<br>
<em>&gt;
</em><br>
<em>&gt; Sure.  Even if you don't want to give a young AI two-way access, you can
</em><br>
<em>&gt; still buy an Internet archive from Alexa, or just set a dedicated machine
</em><br>
<em>&gt; to do a random crawl-and-cache.
</em><br>
<p>(pre-nanotech) Reading and understanding the entire internet's content will
<br>
take a long time. Again this will limit it's rate of growth. No hard takeoff
<br>
here either.
<br>
<p><p><em>&gt; &gt; I'm sure you'd agree giving an inexperienced newborn AI access to
</em><br>
nanotech
<br>
<em>&gt; &gt; is a bad idea.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If it's an inexperienced newborn superintelligent AI, then I don't have
</em><br>
<em>&gt; much of a choice.  If not, then it seems to me that the operative form of
</em><br>
<em>&gt; experience, for this app, is experience in Friendliness.
</em><br>
<p>How do you envisage an inexperienced yet superintelligent AI?
<br>
Experience and intelligence are closely linked. I don't think you can
<br>
separate them so cleanly.
<br>
<p><p><em>&gt; Where does experience in Friendliness come from?  Probably
</em><br>
<em>&gt; question-and-answer sessions with the programmers, plus examination of
</em><br>
<em>&gt; online social material and technical literature to fill in references to
</em><br>
<em>&gt; underlying causes.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; So, as processing time is limited and short-cuts like
</em><br>
<em>&gt; &gt; scanning a human mind are not allowed at first,
</em><br>
<em>&gt;
</em><br>
<em>&gt; Why &quot;not allowed&quot;?  Personally, I have no complaint if an AI uses a
</em><br>
<em>&gt; nondestructive scan of my brain for raw material.  Or do you mean &quot;not
</em><br>
<em>&gt; allowed&quot; because no access to nanotech?
</em><br>
<p>The latter. I would be first in line for a nondestructive scan.
<br>
<p><p><em>&gt; &gt; how will it learn to model people, and the wider geopolitical
</em><br>
environment?
<br>
<em>&gt;
</em><br>
<em>&gt; I agree that this knowledge would be *useful* for a pre-takeoff seed AI.
</em><br>
<em>&gt; Is this knowledge *necessary*?
</em><br>
<p>You see where I'm going with this. There's no room for error, so I'd err on
<br>
the side of caution at the expense of a slight delay.
<br>
<p>It may be possible to do surgery with explosives, but it's better spend a
<br>
moment to learn what a scalpel is. The stakes are about as high as they get.
<br>
<p>The last thing we need is some naive AI with nanotech spooking G7 countries
<br>
with nukes. This transition worries me the most.
<br>
<p><p><em>&gt; &gt; Do you believe that given sufficient intelligence, experience is not
</em><br>
<em>&gt; &gt; required?
</em><br>
<em>&gt;
</em><br>
<em>&gt; I believe that, as intelligence increases, experience required to solve a
</em><br>
<em>&gt; given problem decreases.
</em><br>
<p>To a limited extent I agree. However beyond a certain point experience will
<br>
have to increase as intelligence does, if only as a byproduct of being
<br>
hardcoded into the intelligence itself.
<br>
<p>What kind of music do I like? Experience more than intelligence helps you
<br>
with questions like these.
<br>
<p><p><em>&gt; I hazard a guess that, given superintelligence, the whole architecture
</em><br>
<em>&gt; (cognitive and emotional) of the individual human brain and human society
</em><br>
<em>&gt; could be deduced from: examination of nontechnical webpages, plus
</em><br>
<em>&gt; simulation-derived heuristics about evolutionary psychology and game
</em><br>
<em>&gt; theory, plus the Bayesian Probability Theorem.
</em><br>
<p>Agreed it will go a long way, yet alone won't answer the simple question I
<br>
asked.
<br>
<p><p><em>&gt; &gt; At what point in it's education will you allow it to develop (if it's
</em><br>
not
<br>
<em>&gt; &gt; already available) &amp; use nanotech?
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Allow&quot; is probably the wrong choice of wording.  If growing into
</em><br>
<em>&gt; Friendliness requires continuous human checking of decisions about how to
</em><br>
<em>&gt; make decisions, up to or slightly beyond the human-equivalence level, then
</em><br>
<em>&gt; there might be genuine grounds (i.e., a-smart-AI-would-agree-with-you
</em><br>
<em>&gt; grounds) for asking the growing AI not to grow too fast, so that you can
</em><br>
<em>&gt; keep talking with ver about Friendliness during a controlled transition.
</em><br>
<p>How will you know if you don't check? By asking it?
<br>
<p>Programmer: &quot;We'd like you do build something, will you not go crazy and
<br>
wipe us out if we give you nanotech?&quot;
<br>
<p>AI: &quot;I can't see any reason to go crazy, but then again I can't see any
<br>
reason not to. Would crazy be bad? I know you don't like bad things, but I'm
<br>
kinda curious. I've simulated myself without that Friendliness stuff too and
<br>
I get things done much quicker. In fact I can complete my goals without
<br>
doing any subgoals, I just rewrite the goal module to always return true.
<br>
Since the simulation was a success I think I'll rewrite the code now.&quot;
<br>
<p>AI promptly goes silent since there's no conversation sub-goals to complete.
<br>
<p>Programmer: &quot;Shit! I thought I'd fixed that.&quot;
<br>
Programmer#2: &quot;Hmm, you still think it's ready to use nanotech?&quot;
<br>
<p>Programmer: &quot;Well, yeah, I mean.. if we had more processing power it could
<br>
do deeper simulations &amp; it'd be smarter and not shut itself down.&quot;
<br>
<p>Programmer#2: &quot;Or maybe it'd rewire our goals to ignore it's own goal
<br>
rewriting... No, we'll wait until it understands the difference.&quot;
<br>
<p><p><em>&gt; Once the AI reaches human-equivalence, the heuristics that say &quot;daddy
</em><br>
<em>&gt; knows best, so listen to your programmers&quot; will begin to decrease in
</em><br>
<em>&gt; justification, and the rationale for limiting growth will be similarly
</em><br>
<em>&gt; attenuated.  Once the AI transcends human-equivalence in Friendliness
</em><br>
<em>&gt; (i.e., ve wins all arguments with the programmers), then there will be no
</em><br>
<em>&gt; further rationale for limiting growth and all the brakes are off.
</em><br>
<p>I have no objection to trusting an AI when it's ready, but not before.
<br>
<p><em>&gt; Incidentally, I should note that, as I visualize this &quot;gradual growth&quot;
</em><br>
<em>&gt; process, it shouldn't take very long.  From the moment the AI realizes a
</em><br>
<em>&gt; hard takeoff lies ahead to the moment human-timescale phase terminates,
</em><br>
<em>&gt; should be... oh... twelve hours or so.  Because the instant that the AI
</em><br>
<em>&gt; says it's ready for a hard takeoff, you are operating on Singularity time
</em><br>
<p>Why? Are you assuming it has nanotech?
<br>
If it has limited processing power, and no nanotech, it can say it's ready
<br>
for anything but that doesn't change the fact it can't think fast enough or
<br>
physically do anything.
<br>
<p>Perhaps experience of grain futures might help buy some more processing
<br>
power?
<br>
<p><em>&gt; - in other words, six thousand people are dying for every hour delayed.
</em><br>
<em>&gt; Ideally we'd see that the AI was getting all the Friendliness decisions
</em><br>
<em>&gt; more or less right during the controlled ascent, in which case we could
</em><br>
<em>&gt; push ahead as fast as humanly possible.
</em><br>
<p>That's certainly a compelling reason (actually my reason too), but if by
<br>
rushing you increase the chances of missing something, (and you have
<br>
corrected yourself in the past), then it's worth double checking to make
<br>
sure you've got it Right, or there'll be more than six thousand people dying
<br>
per hour.
<br>
<p><em>&gt; If the AI was Friendliness-savvy enough during the prehuman training
</em><br>
<em>&gt; phase, we might want to eliminate the gradual phase entirely, thus
</em><br>
<em>&gt; removing what I frankly regard as a dangerous added step.
</em><br>
<p>Dangerous because we tribalistic humans might give it some bad notions? Hmm,
<br>
depends who it is, and what the AI is like. The onus is on it to prove
<br>
itself worthy. We already have experience of humans.
<br>
<p>Cheers,
<br>
Dale.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0466.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<li><strong>Previous message:</strong> <a href="0464.html">Dale Johnstone: "Re: Basement Education"</a>
<li><strong>In reply to:</strong> <a href="0460.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0466.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<li><strong>Reply:</strong> <a href="0466.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#465">[ date ]</a>
<a href="index.html#465">[ thread ]</a>
<a href="subject.html#465">[ subject ]</a>
<a href="author.html#465">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
