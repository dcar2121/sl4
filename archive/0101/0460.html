<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Basement Education</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: Basement Education">
<meta name="Date" content="2001-01-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Basement Education</h1>
<!-- received="Wed Jan 24 17:19:31 2001" -->
<!-- isoreceived="20010125001931" -->
<!-- sent="Wed, 24 Jan 2001 17:18:54 -0500" -->
<!-- isosent="20010124221854" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: Basement Education" -->
<!-- id="3A6F54CE.FB139D53@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="001f01c0864e$735cb340$14249fd4@xanadu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Basement%20Education"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Jan 24 2001 - 15:18:54 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0461.html">Samantha Atkins: "Re: Basement Education"</a>
<li><strong>Previous message:</strong> <a href="0459.html">Ben Goertzel: "RE: Basement Education"</a>
<li><strong>In reply to:</strong> <a href="0458.html">Dale Johnstone: "Basement Education"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0462.html">Samantha Atkins: "Re: Basement Education"</a>
<li><strong>Reply:</strong> <a href="0462.html">Samantha Atkins: "Re: Basement Education"</a>
<li><strong>Reply:</strong> <a href="0465.html">Dale Johnstone: "Re: Basement Education"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#460">[ date ]</a>
<a href="index.html#460">[ thread ]</a>
<a href="subject.html#460">[ subject ]</a>
<a href="author.html#460">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Dale Johnstone wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &gt; What actually happens is that genetic engineering and neurohacking and
</em><br>
<em>&gt; &gt; human-computer interfaces don't show up, because they'd show up in 2020,
</em><br>
<em>&gt; &gt; and a hard takeoff occurs in SIAI's or Webmind's basement sometime in the
</em><br>
<em>&gt; &gt; next ten years or so.  Even if the hardware for nanotechnology takes
</em><br>
<em>&gt; &gt; another couple of weeks to manufacture, and even if you're asking the
</em><br>
<em>&gt; &gt; newborn SI questions that whole time, no amount of explanation is going to
</em><br>
<em>&gt; &gt; be equivalent to the real thing.  There still comes a point when the SI
</em><br>
<em>&gt; &gt; says that the Introdus wavefront is on the way, and you sit there waiting
</em><br>
<em>&gt; &gt; for the totally unknowable future to hit you in the next five seconds.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In order for there to be a hard takeoff the AI must be capable of building
</em><br>
<em>&gt; up a huge amount of experience quickly.
</em><br>
<p>What kind of experience?  Experience about grain futures, or experience
<br>
about how to design a seed AI?
<br>
<p><em>&gt; It takes years for a human child.
</em><br>
<em>&gt; Obviously we can crank up the AI's clock rate, but how do you plan for it to
</em><br>
<em>&gt; gain experience when the rest of the world is running in slow motion?
</em><br>
<p>Accumulation of internal experience is limited only by computing power. 
<br>
Experience of the external world will be limited either by rates of
<br>
sensory input or by computing power available to process sensory input.
<br>
<p><em>&gt; Some things can be deduced, others can be learnt from simulations. How does it
</em><br>
<em>&gt; learn about people and human culture in general? From books &amp; the internet?
</em><br>
<p>Sure.  Even if you don't want to give a young AI two-way access, you can
<br>
still buy an Internet archive from Alexa, or just set a dedicated machine
<br>
to do a random crawl-and-cache.
<br>
<p><em>&gt; I'm sure you'd agree giving an inexperienced newborn AI access to nanotech
</em><br>
<em>&gt; is a bad idea.
</em><br>
<p>If it's an inexperienced newborn superintelligent AI, then I don't have
<br>
much of a choice.  If not, then it seems to me that the operative form of
<br>
experience, for this app, is experience in Friendliness.
<br>
<p>Where does experience in Friendliness come from?  Probably
<br>
question-and-answer sessions with the programmers, plus examination of
<br>
online social material and technical literature to fill in references to
<br>
underlying causes.
<br>
<p><em>&gt; So, as processing time is limited and short-cuts like
</em><br>
<em>&gt; scanning a human mind are not allowed at first,
</em><br>
<p>Why &quot;not allowed&quot;?  Personally, I have no complaint if an AI uses a
<br>
nondestructive scan of my brain for raw material.  Or do you mean &quot;not
<br>
allowed&quot; because no access to nanotech?
<br>
<p><em>&gt; how will it learn to model people, and the wider geopolitical environment?
</em><br>
<p>I agree that this knowledge would be *useful* for a pre-takeoff seed AI. 
<br>
Is this knowledge *necessary*?
<br>
<p><em>&gt; Do you believe that given sufficient intelligence, experience is not
</em><br>
<em>&gt; required?
</em><br>
<p>I believe that, as intelligence increases, experience required to solve a
<br>
given problem decreases.
<br>
<p>I hazard a guess that, given superintelligence, the whole architecture
<br>
(cognitive and emotional) of the individual human brain and human society
<br>
could be deduced from: examination of nontechnical webpages, plus
<br>
simulation-derived heuristics about evolutionary psychology and game
<br>
theory, plus the Bayesian Probability Theorem.
<br>
<p><em>&gt; At what point in it's education will you allow it to develop (if it's not
</em><br>
<em>&gt; already available) &amp; use nanotech?
</em><br>
<p>&quot;Allow&quot; is probably the wrong choice of wording.  If growing into
<br>
Friendliness requires continuous human checking of decisions about how to
<br>
make decisions, up to or slightly beyond the human-equivalence level, then
<br>
there might be genuine grounds (i.e., a-smart-AI-would-agree-with-you
<br>
grounds) for asking the growing AI not to grow too fast, so that you can
<br>
keep talking with ver about Friendliness during a controlled transition. 
<br>
Once the AI reaches human-equivalence, the heuristics that say &quot;daddy
<br>
knows best, so listen to your programmers&quot; will begin to decrease in
<br>
justification, and the rationale for limiting growth will be similarly
<br>
attenuated.  Once the AI transcends human-equivalence in Friendliness
<br>
(i.e., ve wins all arguments with the programmers), then there will be no
<br>
further rationale for limiting growth and all the brakes are off.
<br>
<p>Incidentally, I should note that, as I visualize this &quot;gradual growth&quot;
<br>
process, it shouldn't take very long.  From the moment the AI realizes a
<br>
hard takeoff lies ahead to the moment human-timescale phase terminates,
<br>
should be... oh... twelve hours or so.  Because the instant that the AI
<br>
says it's ready for a hard takeoff, you are operating on Singularity time
<br>
- in other words, six thousand people are dying for every hour delayed. 
<br>
Ideally we'd see that the AI was getting all the Friendliness decisions
<br>
more or less right during the controlled ascent, in which case we could
<br>
push ahead as fast as humanly possible.
<br>
<p>If the AI was Friendliness-savvy enough during the prehuman training
<br>
phase, we might want to eliminate the gradual phase entirely, thus
<br>
removing what I frankly regard as a dangerous added step.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0461.html">Samantha Atkins: "Re: Basement Education"</a>
<li><strong>Previous message:</strong> <a href="0459.html">Ben Goertzel: "RE: Basement Education"</a>
<li><strong>In reply to:</strong> <a href="0458.html">Dale Johnstone: "Basement Education"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0462.html">Samantha Atkins: "Re: Basement Education"</a>
<li><strong>Reply:</strong> <a href="0462.html">Samantha Atkins: "Re: Basement Education"</a>
<li><strong>Reply:</strong> <a href="0465.html">Dale Johnstone: "Re: Basement Education"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#460">[ date ]</a>
<a href="index.html#460">[ thread ]</a>
<a href="subject.html#460">[ subject ]</a>
<a href="author.html#460">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
