<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Basement Education</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Basement Education">
<meta name="Date" content="2001-01-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Basement Education</h1>
<!-- received="Mon Jan 29 10:45:08 2001" -->
<!-- isoreceived="20010129174508" -->
<!-- sent="Mon, 29 Jan 2001 01:23:38 -0800" -->
<!-- isosent="20010129092338" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Basement Education" -->
<!-- id="3A75369A.2FC6C76A@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3A6F9719.3463B9B3@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Basement%20Education"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Mon Jan 29 2001 - 02:23:38 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0516.html">Samantha Atkins: "Re: Event horizon &amp; Moravec"</a>
<li><strong>Previous message:</strong> <a href="0514.html">Ben Goertzel: "POLITICAL BILE  RE: Friendliness design philosophy for beginners"</a>
<li><strong>In reply to:</strong> <a href="0463.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0523.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<li><strong>Reply:</strong> <a href="0523.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#515">[ date ]</a>
<a href="index.html#515">[ thread ]</a>
<a href="subject.html#515">[ subject ]</a>
<a href="author.html#515">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&quot;Eliezer S. Yudkowsky&quot; wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Samantha Atkins wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<p><em>&gt; &gt; You certainly do have a choice. If you do not hook the system up in such
</em><br>
<em>&gt; &gt; a way that it controls hardware manufacturing at all levels until it is
</em><br>
<em>&gt; &gt; a bit more seasoned, that would be a quite prudent step.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Prudent, maybe; effective, almost certainly not.  A superintelligence has
</em><br>
<em>&gt; access to *me*.  Ve has access to external reality... would ve really
</em><br>
<em>&gt; notice all that much of a difference whether the particular quark-swirls
</em><br>
<em>&gt; ve contacts are labeled &quot;hardware manufacturing&quot; or &quot;Eliezer Yudkowsky&quot;?
</em><br>
<em>&gt; 
</em><br>
<p>Are you assuming this SI is so intelligent to be able to reach hardware
<br>
manufacturing facilities by some unknown means BEFORE it has developed
<br>
enough to be trusted?  If so then we are pretty thoroughly screwed. Yes?
<br>
<p>It depends on the level of access to *you*.  
<br>
<p><em>&gt; If you're gonna win, win *before* you have a hostile superintelligence on
</em><br>
<em>&gt; your hands.  That's common sense.
</em><br>
<em>&gt; 
</em><br>
<p>I assume it is not hostile at this point but simply inexperienced and
<br>
likely to make devastating errors of judgement.
<br>
<p><em>&gt; &gt; &gt; Where does experience in Friendliness come from?  Probably
</em><br>
<em>&gt; &gt; &gt; question-and-answer sessions with the programmers, plus examination of
</em><br>
<em>&gt; &gt; &gt; online social material and technical literature to fill in references to
</em><br>
<em>&gt; &gt; &gt; underlying causes.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; That would not be enough to develop common sense by itself.  Too much is
</em><br>
<em>&gt; &gt; assumed of the underlying presumed human context in the literature.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think you're wrong about this.
</em><br>
<em>&gt; 
</em><br>
<p>Really?  How many AI systems do you know of that can read texts for
<br>
humans, especially in sociology and history, and make sense of them? 
<br>
How many can parse even the most technical books and abstract useable
<br>
information?  
<br>
<p><em>&gt; &gt; I think you are guessing wrong unless quite a bit of the detailed common
</em><br>
<em>&gt; &gt; sense is developed or entered before the young AI goes off examining
</em><br>
<em>&gt; &gt; papers and running simulations. Knowing the architecture of human minds
</em><br>
<em>&gt; &gt; is not sufficient for having working knowledge of hwo to deal with human
</em><br>
<em>&gt; &gt; beings.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Well, I disagree.  In my own experience, the amount of real-world
</em><br>
<em>&gt; experience needed decreases pretty sharply as a function of the ability to
</em><br>
<em>&gt; theorize about the causation of the observed experiences you already have.
</em><br>
<em>&gt; 
</em><br>
<p>But your own experience is still as a human with quite a bit of
<br>
developmental RT work in acquiring this common sense.  I don't think you
<br>
can reliably reason from your own introspection to predictions  what the
<br>
young SI will experience or what will or will not be adequate for
<br>
understanding to this degree.
<br>
<p><em>&gt; &gt;
</em><br>
<em>&gt; &gt; How can human programmers can answer a sufficient number of the AIs
</em><br>
<em>&gt; &gt; questions in a mere 12 hours?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If the human programmers need to provide serious new Friendship content
</em><br>
<em>&gt; rather than just providing feedback on the AI's own actions, then one may
</em><br>
<em>&gt; be justified in going a little slower.  If the AI is getting everything
</em><br>
<em>&gt; right and the humans are just watching, then zip along as fast as
</em><br>
<em>&gt; possible.
</em><br>
<em>&gt; 
</em><br>
<p>For humans to really evaluate would probably take longer.  Within reason
<br>
we have to err on the side of caution.
<br>
<p><em>&gt; &gt; AI time is not the gating factor in this
</em><br>
<em>&gt; &gt; phase.  And there is no reason to rush it.  So many people dying per
</em><br>
<em>&gt; &gt; hour is irrelevant and emotionalizes the conversation unnecessarily.
</em><br>
<em>&gt; &gt; Letting the AI loose too early can easily terminate all 6 billion+ of
</em><br>
<em>&gt; &gt; us.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes, that is the only reason why it makes sense to take the precaution at
</em><br>
<em>&gt; all.  I do not believe that so many people dying per hour is
</em><br>
<em>&gt; &quot;irrelevant&quot;.  I think that, day in, day out, one hundred and fifty
</em><br>
<em>&gt; thousand people die - people with experiences and memories and lives every
</em><br>
<em>&gt; bit as valuable as my own.  
</em><br>
<p>Tragic yes.  But irrelevant to deciding how much to hurry a very
<br>
dangerous development out the door. 
<br>
<p><em>&gt;Every minute that I ask an AI to deliberately
</em><br>
<em>&gt; delay takeoff puts another hundred deaths on *my* *personal*
</em><br>
<em>&gt; responsibility as a Friendship programmer.
</em><br>
<p>This is not balanced thinking.  You are not personally responsible for
<br>
all the misery of the world.  That you think you have a fix for a large
<br>
part of it, potentially, does not mean that delaying that fix for
<br>
safeties sake makes you responsible personally for what it may (or may
<br>
not) have fixed.  
<br>
<p><em>&gt;  In introducing an artificial
</em><br>
<em>&gt; delay, I would be gambling with human lives - gambling that the
</em><br>
<em>&gt; probability of error is great enough to warrant deliberate slowness,
</em><br>
<em>&gt; gambling on the possibility that the AI wouldn't just zip off to
</em><br>
<em>&gt; superintelligence and Friendliness.  With six billion lives on the line, a
</em><br>
<em>&gt; little delay may be justified, but it has to be the absolute minimum
</em><br>
<em>&gt; delay.  Unless major problems turn up, a one-week delay would be entering
</em><br>
<em>&gt; Hitler/Stalin territory.
</em><br>
<em>&gt; 
</em><br>
<p>No.  It has to be enough delay to be as certain as possible that it will
<br>
not eat the 6 billion people for lunch.  In the face of that as even a
<br>
remote possibility there is no way it is sane to speak of being a Hitler
<br>
if you delay one week.  Please recalibrate on this.  
<br>
<p><em>&gt; &gt; &gt; If the AI was Friendliness-savvy enough during the prehuman training
</em><br>
<em>&gt; &gt; &gt; phase, we might want to eliminate the gradual phase entirely, thus
</em><br>
<em>&gt; &gt; &gt; removing what I frankly regard as a dangerous added step.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; How does it become dependably Friendliness-savvy without the feedback?
</em><br>
<em>&gt; &gt; Or do I misunderstand what gradual phase you want to eliminate?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I think so - the scenario I was postulating was that the AI became
</em><br>
<em>&gt; Friendliness-savvy during the pre-hard-takeoff phase, so that you're
</em><br>
<em>&gt; already pretty confident by the time the AI reaches the hard-takeoff
</em><br>
<em>&gt; level.  This doesn't require perfection, it just requires that the AI
</em><br>
<em>&gt; display the minimal &quot;seed Friendliness&quot; needed to not take any precipitate
</em><br>
<em>&gt; actions until ve can fill in the blanks by examining a nondestructive
</em><br>
<em>&gt; brain scan.
</em><br>
<em>&gt; 
</em><br>
<p>Exactly what brain will we trust to be scanned into the SI?  Wouldn't it
<br>
also pick up a lot of human traits that might not be so wonderful in
<br>
such a super-intelligence?  Remember the old Star Trek episode with the
<br>
super-computer imprinted from its inventor's mind?  
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0516.html">Samantha Atkins: "Re: Event horizon &amp; Moravec"</a>
<li><strong>Previous message:</strong> <a href="0514.html">Ben Goertzel: "POLITICAL BILE  RE: Friendliness design philosophy for beginners"</a>
<li><strong>In reply to:</strong> <a href="0463.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0523.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<li><strong>Reply:</strong> <a href="0523.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#515">[ date ]</a>
<a href="index.html#515">[ thread ]</a>
<a href="subject.html#515">[ subject ]</a>
<a href="author.html#515">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
