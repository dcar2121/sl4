<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: friendly ai</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: friendly ai">
<meta name="Date" content="2001-01-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: friendly ai</h1>
<!-- received="Sun Jan 28 16:05:36 2001" -->
<!-- isoreceived="20010128230536" -->
<!-- sent="Sun, 28 Jan 2001 16:03:05 -0500" -->
<!-- isosent="20010128210305" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: friendly ai" -->
<!-- id="3A748909.C542D248@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMIEDLFBAA.ben@webmind.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20friendly%20ai"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jan 28 2001 - 14:03:05 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0510.html">Ben Goertzel: "RE: friendly ai"</a>
<li><strong>Previous message:</strong> <a href="0508.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>In reply to:</strong> <a href="0504.html">Ben Goertzel: "RE: friendly ai"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0510.html">Ben Goertzel: "RE: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0510.html">Ben Goertzel: "RE: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0530.html">Samantha Atkins: "Re: friendly ai"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#509">[ date ]</a>
<a href="index.html#509">[ thread ]</a>
<a href="subject.html#509">[ subject ]</a>
<a href="author.html#509">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &gt; I can't visualize an AI incapable of learning making it out of the lab or
</em><br>
<em>&gt; &gt; even walking across the room, much less doing one darn thing towards
</em><br>
<em>&gt; &gt; bringing citizenship rights to the Solar System.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; No, I was unclear.  Honest Annie was an AI that got so smart that it just
</em><br>
<em>&gt; shut up and stopped communicating with people altogether....  Radio silence.
</em><br>
<p>I was just WHAT-ing your previous sentence, to the effect that you could
<br>
visualize a Friendly AI with no learning subgoal being as effective as a
<br>
Friendly AI with a learning subgoal.
<br>
<p><em>&gt; &gt; Remember, the hypothesis is that Friendliness is the top layer of a good
</em><br>
<em>&gt; &gt; design, and discovering and creation the subgoals; if you postulate an AI
</em><br>
<em>&gt; &gt; that violates this rule and see horrifying consequences, it should
</em><br>
<em>&gt; &gt; probably be taken as an argument in favor of _Friendly AI_.  &lt;grin&gt;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't think I'm envisioning horrifying consequences at all.  AI's getting
</em><br>
<em>&gt; bored
</em><br>
<em>&gt; with humans isn't all that horrifying, is it?  Especially if humans are all
</em><br>
<em>&gt; uploading
</em><br>
<em>&gt; themselves... then, most humans are going to be bored with old-style flesher
</em><br>
<em>&gt; humans
</em><br>
<em>&gt; too...
</em><br>
<p>Is it horrifying?  That depends on two things; first, the balance between
<br>
defensive and offensive technology; second, the hard takeoff scenario.
<br>
<p>If, under the True Ultimate Laws of Physics and the Final Ultimate
<br>
Technology, offensive technology overpowers defensive technology, then a
<br>
community of sentients killing and eating each other is definitely a bad
<br>
thing.  All the entities that started out as human will die off sooner or
<br>
later, even the transhuman AIs will be preoccupied with survival, and the
<br>
future will be a fairly ugly place.  Actually, the main point is still
<br>
pretty horrifying even if the humans wind up barricaded behind a handful
<br>
of Friendly AIs, or if the uploads are forever limited to whatever chunks
<br>
of matter they grabbed before the Cambrian explosion, or if some
<br>
transhuman who started as a bondage fetishist grabs a handful of
<br>
unfortunate human slaves on the way to ascension, or, for that matter, if
<br>
everyone who stays behind on Earth gets wiped out by some rogue.  So, yes,
<br>
it's horrifying.
<br>
<p>Alternatively, you can have the entire solar system wiped out at one blow
<br>
if an unFriendly AI undergoes a hard takeoff before there are any human
<br>
uploads around.  For that matter, you can have the entire solar system
<br>
wiped out if an unFriendly uploaded human undergoes a hard takeoff.  That
<br>
would be pretty horrifying too.
<br>
<p><em>&gt; &gt; Speaking as one of the six billion people who gets toasted if you make one
</em><br>
<em>&gt; &gt; little mistake, would you *please* consider adding Friendliness to that
</em><br>
<em>&gt; &gt; list?  I really don't think it will cost you anything.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Friendliness is indeed one of webmind's goals ;&gt;
</em><br>
<p>Webmind or Webmind Inc.?
<br>
<p><em>&gt; &gt; I've evolved from subgoal-driven to supergoal-driven over time.  I can see
</em><br>
<em>&gt; &gt; this as possible, but I really can't see it as inevitable, not if the AI
</em><br>
<em>&gt; &gt; is on guard and doesn't want it to happen.  Evolution has to happen in
</em><br>
<em>&gt; &gt; steps, and steps can be observed, detected, and counteracted.  A failure
</em><br>
<em>&gt; &gt; of Friendliness in a seed AI vanishes as soon as the AI realizes it's a
</em><br>
<em>&gt; &gt; failure; it takes a catastrophic failure of Friendliness, something that
</em><br>
<em>&gt; &gt; makes the AI stop *wanting* to be Friendly, before errors can build up in
</em><br>
<em>&gt; &gt; the system.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't know.  Can't &quot;stopping wanting to be friendly&quot; creep up gradually
</em><br>
<em>&gt; too?
</em><br>
<p>It can happen gradually - it can't *creep up*.  The community would
<br>
*notice*.  In fact, they would read the SL4 archives and anticipate all of
<br>
it in advance.  As soon as the changes became noticeable, and *before*
<br>
they became critical - *pow*.
<br>
<p><em>&gt; &gt; AIs who are dreadfully panicked
</em><br>
<em>&gt; &gt; about the prospect of drifting away from Friendship because Friendship is
</em><br>
<em>&gt; &gt; the only important thing in the world to them...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; aha!  Caught you!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Now you're proposing to make AI's neurotic and mentally unhealthy... to make
</em><br>
<em>&gt; them fear becoming unfriendly
</em><br>
<p>Okay, fine, I shouldn't have used the words &quot;dreadfully panicked&quot; or &quot;only
<br>
important thing in the world&quot;.  It's sickeningly anthropomorphic and I was
<br>
only doing it to convey the picture of an all-out community effort.
<br>
<p>But I am *not* proposing to make AIs neurotic.  &quot;Friendliness is the only
<br>
important thing in the world&quot;, or rather, &quot;Friendliness is my goal
<br>
system&quot;, is a perfectly healthy state of mind for an AI.  And taking
<br>
massive preventative action if Friendliness is threatened doesn't require
<br>
a programmer assist; it's a natural consequence of the enormous unFriendly
<br>
consequences of an AI community drifting away from Friendliness.  I would
<br>
*never* &quot;make&quot; an AI fear anything; at most, I would ask politely.
<br>
<p><em>&gt; But isn't this a recipe for backlash of some sort??  Fear breeds aggression,
</em><br>
<em>&gt; no?
</em><br>
<p>No, that's just humans.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0510.html">Ben Goertzel: "RE: friendly ai"</a>
<li><strong>Previous message:</strong> <a href="0508.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>In reply to:</strong> <a href="0504.html">Ben Goertzel: "RE: friendly ai"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0510.html">Ben Goertzel: "RE: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0510.html">Ben Goertzel: "RE: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0530.html">Samantha Atkins: "Re: friendly ai"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#509">[ date ]</a>
<a href="index.html#509">[ thread ]</a>
<a href="subject.html#509">[ subject ]</a>
<a href="author.html#509">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
