<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: friendly ai</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: friendly ai">
<meta name="Date" content="2001-01-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: friendly ai</h1>
<!-- received="Sun Jan 28 02:31:45 2001" -->
<!-- isoreceived="20010128093145" -->
<!-- sent="Sun, 28 Jan 2001 02:29:46 -0500" -->
<!-- isosent="20010128072946" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: friendly ai" -->
<!-- id="3A73CA6A.F3A773A@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMMECBFBAA.ben@webmind.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20friendly%20ai"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jan 28 2001 - 00:29:46 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0481.html">Ben Goertzel: "RE: friendly ai"</a>
<li><strong>Previous message:</strong> <a href="0479.html">Ben Goertzel: "mathematician wanted..."</a>
<li><strong>In reply to:</strong> <a href="0475.html">Ben Goertzel: "friendly ai"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0481.html">Ben Goertzel: "RE: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0481.html">Ben Goertzel: "RE: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0482.html">Dale Johnstone: "Re: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0484.html">xgl: "Re: friendly ai"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#480">[ date ]</a>
<a href="index.html#480">[ thread ]</a>
<a href="subject.html#480">[ subject ]</a>
<a href="author.html#480">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Again, no time for a thorough response to your paper, but here's a
</em><br>
<em>&gt; thought...
</em><br>
<p>(Ben Goertzel is referring to a partial, interim version of &quot;Friendly
<br>
AI&quot;.  The full version - or *a* full version - should be published
<br>
sometime Real Soon Now.)
<br>
<p><em>&gt; You make a very good case that due to
</em><br>
[snip]
<br>
<em>&gt; and other related facts, AI's are probably going to be vastly mentally
</em><br>
<em>&gt; healthier than humans,
</em><br>
<em>&gt; without our strong inclinations toward aggression, jealousy, and so forth.
</em><br>
<p>Actually, for purposes of Friendly AI we should say that they don't have
<br>
innate inclinations (and certainly would never find themselves expending
<br>
quote mental energy unquote to overcome such inclinations).  Friendliness
<br>
is a separate issue, over and above the nonanthropomorphic background.
<br>
<p><em>&gt; But, the case is weaker that this is going to make AI's consistently and
</em><br>
<em>&gt; persistently friendly.
</em><br>
<p>Well, yes, your version has the antianthropomorphic parts of the paper but
<br>
only a quickie summary of how the actual Friendship system works &lt;smile&gt;.
<br>
<p><em>&gt; There are 2 main points here
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1)
</em><br>
<em>&gt; AI's may well end up ~indifferent~ to humans.  My guess is that even if
</em><br>
<em>&gt; initial AI's are
</em><br>
<em>&gt; explicitly programmed to be warm &amp; friendly to humans, eventually
</em><br>
<em>&gt; &quot;indifference to humans&quot; may become an inexorable attractor...
</em><br>
<p>What forces create this attractor?  My visualization of your visualization
<br>
is that you're thinking in terms of an evolutionary scenario with vicious
<br>
competition between AIs, such that all AIs have a finite lifespan before
<br>
they are eventually killed and devoured by nearby conspecifics; the humans
<br>
are eaten early in the game and AIs that expend energy on Friendliness
<br>
become extinct soon after.
<br>
<p><em>&gt; 2)
</em><br>
<em>&gt; There WILL be an evolutionary aspect to the growth of AI, because there are
</em><br>
<em>&gt; finite computer resources and AI's can replicate themselves potentially infinitely.
</em><br>
<em>&gt; So there will be a
</em><br>
<em>&gt; &quot;survival of the fittest&quot; aspect to AI, meaning that AI's with greater
</em><br>
<em>&gt; initiative, motivation, etc. will be more likely to survive.
</em><br>
<p>You need two things for evolution: first, replication; second, imperfect
<br>
replication.  It's not clear that a human-equivalent Friendly AI would
<br>
wish to replicate verself at all - how does this goal subserve
<br>
Friendliness?  And if the Friendly AI does replicate verself, why would
<br>
the Friendship system be permitted to change in offspring?  Why would
<br>
cutthroat competition be permitted to emerge?  Either of these outcomes,
<br>
if predictable, would seem to rule out replication as necessarily
<br>
unFriendly, unless these problems can be overcome.
<br>
<p><em>&gt; Points 1 and 2 tie in together.  Because all my experimentation with genetic
</em><br>
<em>&gt; algorithms shows that,
</em><br>
<em>&gt; for evolutionary processes, initial conditions are fairly irrelevant.  The
</em><br>
<em>&gt; system evolves fit things that
</em><br>
<em>&gt; live in large basins of attraction, no matter where you start them.  If
</em><br>
<em>&gt; 'warm &amp; friendly to humans' has a smaller basin
</em><br>
<em>&gt; of attraction than 'indifferent to humans', then randomness plus genetic
</em><br>
<em>&gt; drift is going to lead the latter
</em><br>
<em>&gt; to dominate before long regardless of initial condition.
</em><br>
<p>I guess you'd better figure out how to use directed evolution and
<br>
externally imposed selection pressures to manipulate the fitness metric
<br>
and the basins of attraction, so that the first AIs capable of replication
<br>
without human assistance are Friendly enough to want to deliberately
<br>
ensure Friendliness in their offspring.  Frankly I prefer the Sysop
<br>
(singleton seed AI) scenario; it looks a *lot* safer, for reasons you've
<br>
just outlined.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0481.html">Ben Goertzel: "RE: friendly ai"</a>
<li><strong>Previous message:</strong> <a href="0479.html">Ben Goertzel: "mathematician wanted..."</a>
<li><strong>In reply to:</strong> <a href="0475.html">Ben Goertzel: "friendly ai"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0481.html">Ben Goertzel: "RE: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0481.html">Ben Goertzel: "RE: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0482.html">Dale Johnstone: "Re: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0484.html">xgl: "Re: friendly ai"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#480">[ date ]</a>
<a href="index.html#480">[ thread ]</a>
<a href="subject.html#480">[ subject ]</a>
<a href="author.html#480">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
