<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Beyond evolution</title>
<meta name="Author" content="Ben Goertzel (ben@webmind.com)">
<meta name="Subject" content="RE: Beyond evolution">
<meta name="Date" content="2001-01-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Beyond evolution</h1>
<!-- received="Sun Jan 28 13:20:57 2001" -->
<!-- isoreceived="20010128202057" -->
<!-- sent="Sun, 28 Jan 2001 13:18:49 -0500" -->
<!-- isosent="20010128181849" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@webmind.com" -->
<!-- subject="RE: Beyond evolution" -->
<!-- id="JBEPKOGDDIKKAHFPOEFIIELKCEAA.ben@webmind.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3A745FD8.DCAF4497@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@webmind.com?Subject=RE:%20Beyond%20evolution"><em>ben@webmind.com</em></a>)<br>
<strong>Date:</strong> Sun Jan 28 2001 - 11:18:49 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0490.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<li><strong>Previous message:</strong> <a href="0488.html">Ben Goertzel: "RE: friendly ai"</a>
<li><strong>In reply to:</strong> <a href="0487.html">Eliezer S. Yudkowsky: "Beyond evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0497.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>Reply:</strong> <a href="0497.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#489">[ date ]</a>
<a href="index.html#489">[ thread ]</a>
<a href="subject.html#489">[ subject ]</a>
<a href="author.html#489">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I think you sell evolution short... it's a much  more general and powerful
<br>
process than you
<br>
give it credit for
<br>
<p>I tend to buy into those theories of the origins of the universe, like John
<br>
Wheeler's, which argue
<br>
that in the early universe, physical law itself evolved...
<br>
<p>Furthermore, I believe that complex high-level cognition is ITSELF
<br>
intrinsically evolutionary.
<br>
<p>This is a technical point that is at the fore of our AI development within
<br>
Webmind Inc. at this
<br>
exact moment.  We have some nice higher-order inference going on, but, the
<br>
key issue is cognition
<br>
control.  How, in practice, to control the direction of inference, when
<br>
applied to complex tasks like
<br>
learning schema for acting or language processing, etc.?  The heuristics
<br>
found in the standard AI
<br>
literature just don't work in practice.  Evolution, I submit, is the only
<br>
effective method for
<br>
general cognition control.   Evolution forms complex logical relations and
<br>
then higher-order inference
<br>
verifies their utility or lack thereof.  I don't have a mathematical proof
<br>
that this is the only effective
<br>
way to do things -- so I could be wrong.  But
<br>
no one has ever found another way, in their theoretical or practical AI
<br>
work.  And there is much
<br>
evidence that the human brain is itself evolutionary (see Edelman's work on
<br>
Neural Darwinism; earlier
<br>
work by neuroscientists like Vernon Mountcastle and Szentogothai)
<br>
<p>-- ben
<br>
<p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20Beyond%20evolution">owner-sl4@sysopmind.com</a> [mailto:<a href="mailto:owner-sl4@sysopmind.com?Subject=RE:%20Beyond%20evolution">owner-sl4@sysopmind.com</a>]On Behalf
</em><br>
<em>&gt; Of Eliezer S. Yudkowsky
</em><br>
<em>&gt; Sent: Sunday, January 28, 2001 1:07 PM
</em><br>
<em>&gt; To: SL4
</em><br>
<em>&gt; Subject: Beyond evolution
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Evolution is the simplest way for a system to evolve greater complexity in
</em><br>
<em>&gt; the absence of intelligence.  Not the best way - the simplest way.  The
</em><br>
<em>&gt; first way hit upon.  Evolution prevails, not because it's better, or best,
</em><br>
<em>&gt; or morally right in any way whatsoever, but because it's there and there
</em><br>
<em>&gt; are no forces acting against it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Luke: &quot;Is the Dark side stronger?&quot;
</em><br>
<em>&gt; Yoda: &quot;No! no...quicker, easier, more seductive...&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Evolution is not the brilliant idea of solving the problem more
</em><br>
<em>&gt; effectively by subtracting intelligence; evolution is the result of adding
</em><br>
<em>&gt; the *constraint* that the problem must be solved in a way that does not
</em><br>
<em>&gt; invoke intelligence.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I'm steeped in the antiteleological precautions of evolutionary
</em><br>
<em>&gt; psychology.  You're spending your time with the computer science version
</em><br>
<em>&gt; of evolution - one in which &quot;evolutionary programming&quot; is a brilliant way
</em><br>
<em>&gt; to overcome some of the inherent limits of human intelligence using
</em><br>
<em>&gt; processes that don't invoke general cognition.  (Of course, the processes
</em><br>
<em>&gt; *can't* invoke general cognition because nobody's programmed that yet.)
</em><br>
<em>&gt;
</em><br>
<em>&gt; Evolutionary programming is not superior to a seed AI with a modality,
</em><br>
<em>&gt; 2Ghz transistors, and patience; but EP can reach spaces inaccessible to a
</em><br>
<em>&gt; human, with no modality, who operates in 200hz time and is easily bored
</em><br>
<em>&gt; and is especially bored by simple things; a human who instinctively tries
</em><br>
<em>&gt; to use all those parallel neurons on each design attempt, and has neither
</em><br>
<em>&gt; the patience nor the time to use 10^14 synapses to test lots of simple
</em><br>
<em>&gt; local optimizations.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Evolution is not something I like.  Evolution is something that *is*.
</em><br>
<em>&gt; Moreover, it's a something-that-is that I think humanity (and our new
</em><br>
<em>&gt; friends) should move away from.  I think evolution is something we should
</em><br>
<em>&gt; grow out of as we grow up.  Evolution is not the best way, or even a good
</em><br>
<em>&gt; way, it's simply the first way.  In casting aside evolution, we will lose
</em><br>
<em>&gt; nothing, gain everything, because there is nothing whatsoever that
</em><br>
<em>&gt; evolution can do that can't be done by a sufficiently powerful general
</em><br>
<em>&gt; intelligence.  Humans are not &quot;sufficiently powerful&quot;, but a seed AI is.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Evolution is what happens in the *total absence* of morality or
</em><br>
<em>&gt; intelligence.  I'm not talking about the evolution of moral and
</em><br>
<em>&gt; intelligent beings, which has been known to happen; I mean that morality
</em><br>
<em>&gt; and intelligence have no influence on the systemic structure of evolution
</em><br>
<em>&gt; itself.  Evolution is the way things are because it's the first way that
</em><br>
<em>&gt; unintelligent reality hits upon.  Evolution, like death, like pain, like
</em><br>
<em>&gt; the constant struggle to survive, is a part of default-state reality that
</em><br>
<em>&gt; humanity shall CONQUER as we attain our place in the Universe.  I believe
</em><br>
<em>&gt; in the *triumph* of altruistic general intelligence over evolution as part
</em><br>
<em>&gt; of the Singularity.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think that's the underlying reason why we disagree about the ability of
</em><br>
<em>&gt; evolution to affect the Singularity: you see evolution as a force that's
</em><br>
<em>&gt; strong and necessary; I see it as a force that's pretty weak compared to
</em><br>
<em>&gt; intelligence, and was never all that attractive to begin with.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Because all my experimentation with genetic algorithms shows that,
</em><br>
<em>&gt; &gt; for evolutionary processes, initial conditions are fairly irrelevant.
</em><br>
<em>&gt; &gt; The system evolves fit things that live in large basins of attraction,
</em><br>
<em>&gt; &gt; no matter where you start them.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Well, yeah, sure... in the total absence of moral intelligence.  What else
</em><br>
<em>&gt; would you expect?
</em><br>
<em>&gt;
</em><br>
<em>&gt; --              --              --              --              --
</em><br>
<em>&gt; Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
</em><br>
<em>&gt; Research Fellow, Singularity Institute for Artificial Intelligence
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0490.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<li><strong>Previous message:</strong> <a href="0488.html">Ben Goertzel: "RE: friendly ai"</a>
<li><strong>In reply to:</strong> <a href="0487.html">Eliezer S. Yudkowsky: "Beyond evolution"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0497.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<li><strong>Reply:</strong> <a href="0497.html">Eliezer S. Yudkowsky: "Re: Beyond evolution"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#489">[ date ]</a>
<a href="index.html#489">[ thread ]</a>
<a href="subject.html#489">[ subject ]</a>
<a href="author.html#489">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
