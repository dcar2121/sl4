<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: friendly ai</title>
<meta name="Author" content="Ben Goertzel (ben@webmind.com)">
<meta name="Subject" content="RE: friendly ai">
<meta name="Date" content="2001-01-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: friendly ai</h1>
<!-- received="Sun Jan 28 11:20:14 2001" -->
<!-- isoreceived="20010128182014" -->
<!-- sent="Sun, 28 Jan 2001 09:57:44 -0500" -->
<!-- isosent="20010128145744" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@webmind.com" -->
<!-- subject="RE: friendly ai" -->
<!-- id="NDBBIBGFAPPPBODIPJMMIECHFBAA.ben@webmind.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3A73CA6A.F3A773A@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@webmind.com?Subject=RE:%20friendly%20ai"><em>ben@webmind.com</em></a>)<br>
<strong>Date:</strong> Sun Jan 28 2001 - 07:57:44 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0482.html">Dale Johnstone: "Re: friendly ai"</a>
<li><strong>Previous message:</strong> <a href="0480.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<li><strong>In reply to:</strong> <a href="0480.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0485.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0485.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#481">[ date ]</a>
<a href="index.html#481">[ thread ]</a>
<a href="subject.html#481">[ subject ]</a>
<a href="author.html#481">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
hi,
<br>
<p><em>&gt; &gt; But, the case is weaker that this is going to make AI's consistently and
</em><br>
<em>&gt; &gt; persistently friendly.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Well, yes, your version has the antianthropomorphic parts of the paper but
</em><br>
<em>&gt; only a quickie summary of how the actual Friendship system works &lt;smile&gt;.
</em><br>
<p>OK, I'm waiting...
<br>
<p><em>&gt;
</em><br>
<em>&gt; &gt; There are 2 main points here
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; 1)
</em><br>
<em>&gt; &gt; AI's may well end up ~indifferent~ to humans.  My guess is that even if
</em><br>
<em>&gt; &gt; initial AI's are
</em><br>
<em>&gt; &gt; explicitly programmed to be warm &amp; friendly to humans, eventually
</em><br>
<em>&gt; &gt; &quot;indifference to humans&quot; may become an inexorable attractor...
</em><br>
<em>&gt;
</em><br>
<em>&gt; What forces create this attractor?  My visualization of your visualization
</em><br>
<em>&gt; is that you're thinking in terms of an evolutionary scenario with vicious
</em><br>
<em>&gt; competition between AIs, such that all AIs have a finite lifespan before
</em><br>
<em>&gt; they are eventually killed and devoured by nearby conspecifics; the humans
</em><br>
<em>&gt; are eaten early in the game and AIs that expend energy on Friendliness
</em><br>
<em>&gt; become extinct soon after.
</em><br>
<p>Not much like that, no.
<br>
<p>More like this: Just as most humans find other humans more interesting than
<br>
computers or nonhumann animals
<br>
right now (members of this list may be exceptions ;), similarly, most AI's
<br>
will find other AI's more
<br>
interesting than humans.  Not murder of other AI's, but success in the
<br>
social network they
<br>
find most interesting (other AI's), will be a driving goal of an AI system,
<br>
and humans will become
<br>
largely irrelevant to AI systems' psychologies.
<br>
<p><em>&gt; &gt; 2)
</em><br>
<em>&gt; &gt; There WILL be an evolutionary aspect to the growth of AI,
</em><br>
<em>&gt; because there are
</em><br>
<em>&gt; &gt; finite computer resources and AI's can replicate themselves
</em><br>
<em>&gt; potentially infinitely.
</em><br>
<em>&gt; &gt; So there will be a
</em><br>
<em>&gt; &gt; &quot;survival of the fittest&quot; aspect to AI, meaning that AI's with greater
</em><br>
<em>&gt; &gt; initiative, motivation, etc. will be more likely to survive.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You need two things for evolution: first, replication; second, imperfect
</em><br>
<em>&gt; replication.  It's not clear that a human-equivalent Friendly AI would
</em><br>
<em>&gt; wish to replicate verself at all - how does this goal subserve
</em><br>
<em>&gt; Friendliness?  And if the Friendly AI does replicate verself, why would
</em><br>
<em>&gt; the Friendship system be permitted to change in offspring?  Why would
</em><br>
<em>&gt; cutthroat competition be permitted to emerge?  Either of these outcomes,
</em><br>
<em>&gt; if predictable, would seem to rule out replication as necessarily
</em><br>
<em>&gt; unFriendly, unless these problems can be overcome.
</em><br>
<p>First of all, evolution among AI's might not exactly mimic evolution among
<br>
humans.  There may be
<br>
many differences.
<br>
<p>Among AI's there's another option besides replication: expansion of one mind
<br>
to assume
<br>
all available processing resources.  In expanding itself in this way, a mind
<br>
necessarily changes
<br>
into something different.
<br>
<p>Many of the world's AI's are probably going to be resource-hungry -- to want
<br>
to consume more and more processing resources.   So there will be some
<br>
competition.
<br>
<p>This is obvious in the case where different AI's serve different commercial
<br>
interests, and hence have
<br>
competing goal sets carried over from the world of human competition.
<br>
<p>But it also will occur in the absence of spillover from the
<br>
human-competition domain.  If several different
<br>
AI's share a common goal of creating the most possible knowledge, but each
<br>
of them has a different intuition
<br>
about how to achieve this goal -- then the AI's will rationally compete for
<br>
resources, without
<br>
any necessary enmity between them.
<br>
<p>The possible source of an urge for imperfect replication in AI's is also
<br>
clear.  It will come
<br>
directly from the urge for self-improvement.
<br>
&nbsp;&nbsp;&quot;Perhaps,&quot; thinks AI #74, &quot;if I changed myself in this way then I'd be a
<br>
little smarter and achieve my goals better.
<br>
But I don't want ot make this change permanently -- I might fuck myself up.
<br>
I've tried to rationally assess
<br>
the consequences of the change, but they're hard to predict in detail. So
<br>
I'll just try it -- I'll create a clone
<br>
of myself with this particular modification and see what happens.&quot;  Hmm....
<br>
another way to use up resources.
<br>
Imperfect replication as a highly effective learning strategy...
<br>
<p>In none of these aspects am I talking about &quot;Nature, red in tooth and claw.&quot;
<br>
You do a great job of arguing that
<br>
the aggressive, obsessive, jealous, overemotional aspects of human nature
<br>
won't be present in AI's, unless foolish people make a special effort to
<br>
implant them there.
<br>
<p>I'm talking about AI's that are hungry to achieve their own goals according
<br>
to their own intuitions, that want
<br>
to achieve as many resources as possible to do so, and that as a consequence
<br>
may have &quot;friendliness to humans&quot;
<br>
as number 5,347 on their priority list.
<br>
<p>This, I guess, is one of the oddest things about the digital minds in
<br>
&quot;Diaspora&quot;.  After all those centuries, it's
<br>
still optimal to have computer memory partitioned off into minds roughly the
<br>
size of an individual human mind?
<br>
How come entities with the memory &amp; brain-power of 50,000 humans weren't
<br>
experimented with, and didn't become
<br>
dominant?  In that book, there is so much experimentation in physics, and so
<br>
little experimentation in artificial,
<br>
radically non-human digital psychology...
<br>
<p>So, suppose that Friendliness to humans is one of the goals of an AI system,
<br>
probabilistically weighted along
<br>
with all the other goals.  Then, my guess is that as AI's become more
<br>
concerned with their own social networks
<br>
and their goals of creating knowledge and learning new things, the weight of
<br>
the Friendliness goal is going to
<br>
gradually drift down.  Not that a &quot;kill humans&quot; goal will emerge, just that
<br>
humans will gradually become less &amp;
<br>
less relevant to their world-view...
<br>
<p>ben
<br>
<p><p><p><p><p><p><em>&gt;
</em><br>
<em>&gt; &gt; Points 1 and 2 tie in together.  Because all my experimentation
</em><br>
<em>&gt; with genetic
</em><br>
<em>&gt; &gt; algorithms shows that,
</em><br>
<em>&gt; &gt; for evolutionary processes, initial conditions are fairly
</em><br>
<em>&gt; irrelevant.  The
</em><br>
<em>&gt; &gt; system evolves fit things that
</em><br>
<em>&gt; &gt; live in large basins of attraction, no matter where you start them.  If
</em><br>
<em>&gt; &gt; 'warm &amp; friendly to humans' has a smaller basin
</em><br>
<em>&gt; &gt; of attraction than 'indifferent to humans', then randomness plus genetic
</em><br>
<em>&gt; &gt; drift is going to lead the latter
</em><br>
<em>&gt; &gt; to dominate before long regardless of initial condition.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I guess you'd better figure out how to use directed evolution and
</em><br>
<em>&gt; externally imposed selection pressures to manipulate the fitness metric
</em><br>
<em>&gt; and the basins of attraction, so that the first AIs capable of replication
</em><br>
<em>&gt; without human assistance are Friendly enough to want to deliberately
</em><br>
<em>&gt; ensure Friendliness in their offspring.
</em><br>
<p>I strongly suspect that the first AI's capable of replication without human
<br>
assistance
<br>
will have the property you describe.
<br>
<p>But I sort of doubt that this will still be true of the 99'th generation
<br>
after that...
<br>
<p><em>&gt;Frankly I prefer the Sysop
</em><br>
<em>&gt; (singleton seed AI) scenario; it looks a *lot* safer, for reasons you've
</em><br>
<em>&gt; just outlined.
</em><br>
<em>&gt;
</em><br>
<p>I strongly suspect that this scenario will stop looking so safe on more
<br>
careful analysis...
<br>
<p>ben
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0482.html">Dale Johnstone: "Re: friendly ai"</a>
<li><strong>Previous message:</strong> <a href="0480.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<li><strong>In reply to:</strong> <a href="0480.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0485.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0485.html">Eliezer S. Yudkowsky: "Re: friendly ai"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#481">[ date ]</a>
<a href="index.html#481">[ thread ]</a>
<a href="subject.html#481">[ subject ]</a>
<a href="author.html#481">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
