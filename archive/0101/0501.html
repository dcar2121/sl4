<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: friendly ai</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: friendly ai">
<meta name="Date" content="2001-01-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: friendly ai</h1>
<!-- received="Sun Jan 28 15:30:37 2001" -->
<!-- isoreceived="20010128223037" -->
<!-- sent="Sun, 28 Jan 2001 15:29:00 -0500" -->
<!-- isosent="20010128202900" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: friendly ai" -->
<!-- id="3A74810C.13E77477@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMCEDGFBAA.ben@webmind.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20friendly%20ai"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Jan 28 2001 - 13:29:00 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0502.html">Pvthur@aol.com: "Re: Beyond evolution"</a>
<li><strong>Previous message:</strong> <a href="0500.html">Ben Goertzel: "RE: Beyond evolution"</a>
<li><strong>In reply to:</strong> <a href="0495.html">Ben Goertzel: "RE: friendly ai"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0504.html">Ben Goertzel: "RE: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0504.html">Ben Goertzel: "RE: friendly ai"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#501">[ date ]</a>
<a href="index.html#501">[ thread ]</a>
<a href="subject.html#501">[ subject ]</a>
<a href="author.html#501">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; &gt; Do you seriously think that a Friendly AI which totally lacked the
</em><br>
<em>&gt; &gt; behaviors and cognitive complexity associated with learning would be more
</em><br>
<em>&gt; &gt; effective in making Friendliness real?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Quite possibly, YES
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is the &quot;Honest Annie&quot; scenario envisioned by Stanislaw Lem
</em><br>
<p>Didn't read that story, sorry... but...
<br>
<p>WHAT?
<br>
<p>I can't visualize an AI incapable of learning making it out of the lab or
<br>
even walking across the room, much less doing one darn thing towards
<br>
bringing citizenship rights to the Solar System.
<br>
<p><em>&gt; The possibility is that an AI, interested in discovering and creative new
</em><br>
<em>&gt; things,
</em><br>
<em>&gt; rapidly evolves to the point where humans and their various dilemmas,
</em><br>
<em>&gt; puzzles and problems
</em><br>
<em>&gt; are not very intriguing to it
</em><br>
<p>Okay.  Here, again, you seem to be assuming bad design and pointing out
<br>
the awful consequences.  Consider the counterpart hypothesis:  That an AI,
<br>
interested in humans and their various dilemnas, rapidly evolves to the
<br>
point where puzzles and problems are not very intriguing to ver.
<br>
<p>Remember, the hypothesis is that Friendliness is the top layer of a good
<br>
design, and discovering and creation the subgoals; if you postulate an AI
<br>
that violates this rule and see horrifying consequences, it should
<br>
probably be taken as an argument in favor of _Friendly AI_.  &lt;grin&gt;
<br>
<p><em>&gt; &gt; Ergo, the behaviors associated with learning are valid subgoals of
</em><br>
<em>&gt; &gt; Friendliness.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; They are indeed valid subgoals of friendliness
</em><br>
<em>&gt; 
</em><br>
<em>&gt; However, the weight that they would be assigned as subgoals of friendliness
</em><br>
<em>&gt; might not be very high
</em><br>
<p>If the system isn't smart enough to see the massive importance of
<br>
learning, use a programmer intervention to add the fact to the system that
<br>
&quot;Ben Goertzel says learning is massively important&quot;.  If the system
<br>
assumes that &quot;Ben Goertzel says X&quot; translates to &quot;as a default, X has a
<br>
high probability of being true&quot;, and a prehuman AI should make this
<br>
assumption (probably due to another programmer intervention), then this
<br>
should raise the weight of the learning subgoal.
<br>
<p><em>&gt; (In constructing Webmind's goal system, I suspect we're assigning a higher
</em><br>
<em>&gt; weight to learning &amp;
</em><br>
<em>&gt; creativity than would be necessary if they were considered only as subgoals
</em><br>
<em>&gt; of friendliness -- because
</em><br>
<em>&gt; I'm interested in evolving the smartest, most knowledgeable Ai system
</em><br>
<em>&gt; possible)
</em><br>
<p>Speaking as one of the six billion people who gets toasted if you make one
<br>
little mistake, would you *please* consider adding Friendliness to that
<br>
list?  I really don't think it will cost you anything.
<br>
<p><em>&gt; And, they're very strong candidates for long-term, self-organizing,
</em><br>
<em>&gt; spontaneous subgoal alienation...
</em><br>
<p>I've evolved from subgoal-driven to supergoal-driven over time.  I can see
<br>
this as possible, but I really can't see it as inevitable, not if the AI
<br>
is on guard and doesn't want it to happen.  Evolution has to happen in
<br>
steps, and steps can be observed, detected, and counteracted.  A failure
<br>
of Friendliness in a seed AI vanishes as soon as the AI realizes it's a
<br>
failure; it takes a catastrophic failure of Friendliness, something that
<br>
makes the AI stop *wanting* to be Friendly, before errors can build up in
<br>
the system.
<br>
<p>If there's a society of Friendly AIs, they'll *notice* that new AIs are a
<br>
little bit less Friendly than the originals, and *all* of them, new and
<br>
old alike, will go into a screaming panic and start doing something about
<br>
it.  It takes *time* for evolution to make big changes.  Evolution has to
<br>
cause a small failure of Friendliness before it can cause a big,
<br>
catastrophic failure of Friendliness, and that will put *everyone* on
<br>
guard, new AIs and old AIs alike, because they haven't yet undergone
<br>
catastrophic failure and they still *want* to be Friendly.  And I don't
<br>
believe that evolution can cause catastrophic blatant &quot;Friendship drift&quot;
<br>
if it's evolution operating on an entire community of seed AIs who are on
<br>
their guard, who can examine their own code and make communal agreements
<br>
to impose artificial selection pressures, AIs who are dreadfully panicked
<br>
about the prospect of drifting away from Friendship because Friendship is
<br>
the only important thing in the world to them...
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0502.html">Pvthur@aol.com: "Re: Beyond evolution"</a>
<li><strong>Previous message:</strong> <a href="0500.html">Ben Goertzel: "RE: Beyond evolution"</a>
<li><strong>In reply to:</strong> <a href="0495.html">Ben Goertzel: "RE: friendly ai"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0504.html">Ben Goertzel: "RE: friendly ai"</a>
<li><strong>Reply:</strong> <a href="0504.html">Ben Goertzel: "RE: friendly ai"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#501">[ date ]</a>
<a href="index.html#501">[ thread ]</a>
<a href="subject.html#501">[ subject ]</a>
<a href="author.html#501">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
