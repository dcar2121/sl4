<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Basement Education</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Basement Education">
<meta name="Date" content="2001-01-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Basement Education</h1>
<!-- received="Wed Jan 24 21:37:19 2001" -->
<!-- isoreceived="20010125043719" -->
<!-- sent="Wed, 24 Jan 2001 18:34:11 -0800" -->
<!-- isosent="20010125023411" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Basement Education" -->
<!-- id="3A6F90A3.AB05D9BD@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3A6F54CE.FB139D53@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Basement%20Education"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Wed Jan 24 2001 - 19:34:11 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0463.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<li><strong>Previous message:</strong> <a href="0461.html">Samantha Atkins: "Re: Basement Education"</a>
<li><strong>In reply to:</strong> <a href="0460.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0463.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<li><strong>Reply:</strong> <a href="0463.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#462">[ date ]</a>
<a href="index.html#462">[ thread ]</a>
<a href="subject.html#462">[ subject ]</a>
<a href="author.html#462">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&quot;Eliezer S. Yudkowsky&quot; wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Dale Johnstone wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Eliezer wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt; What actually happens is that genetic engineering and neurohacking and
</em><br>
<em>&gt; &gt; &gt; human-computer interfaces don't show up, because they'd show up in 2020,
</em><br>
<em>&gt; &gt; &gt; and a hard takeoff occurs in SIAI's or Webmind's basement sometime in the
</em><br>
<em>&gt; &gt; &gt; next ten years or so.  Even if the hardware for nanotechnology takes
</em><br>
<em>&gt; &gt; &gt; another couple of weeks to manufacture, and even if you're asking the
</em><br>
<em>&gt; &gt; &gt; newborn SI questions that whole time, no amount of explanation is going to
</em><br>
<em>&gt; &gt; &gt; be equivalent to the real thing.  There still comes a point when the SI
</em><br>
<em>&gt; &gt; &gt; says that the Introdus wavefront is on the way, and you sit there waiting
</em><br>
<em>&gt; &gt; &gt; for the totally unknowable future to hit you in the next five seconds.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; In order for there to be a hard takeoff the AI must be capable of building
</em><br>
<em>&gt; &gt; up a huge amount of experience quickly.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What kind of experience?  Experience about grain futures, or experience
</em><br>
<em>&gt; about how to design a seed AI?
</em><br>
<p>In order for the SI to run much of anything other than its own
<br>
development it will need both.
<br>
<p><p><em>&gt; 
</em><br>
<em>&gt; &gt; I'm sure you'd agree giving an inexperienced newborn AI access to nanotech
</em><br>
<em>&gt; &gt; is a bad idea.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If it's an inexperienced newborn superintelligent AI, then I don't have
</em><br>
<em>&gt; much of a choice.  If not, then it seems to me that the operative form of
</em><br>
<em>&gt; experience, for this app, is experience in Friendliness.
</em><br>
<em>&gt; 
</em><br>
<p>You certainly do have a choice. If you do not hook the system up in such
<br>
a way that it controls hardware manufacturing at all levels until it is
<br>
a bit more seasoned, that would be a quite prudent step.  Friendliness
<br>
is not enough.  Forgive me if I forgot, but where are you getting common
<br>
sense?  From something like Cyc?  
<br>
<p><em>&gt; Where does experience in Friendliness come from?  Probably
</em><br>
<em>&gt; question-and-answer sessions with the programmers, plus examination of
</em><br>
<em>&gt; online social material and technical literature to fill in references to
</em><br>
<em>&gt; underlying causes.
</em><br>
<em>&gt; 
</em><br>
<p>That would not be enough to develop common sense by itself.  Too much is
<br>
assumed of the underlying presumed human context in the literature.
<br>
<p><em>&gt; &gt; So, as processing time is limited and short-cuts like
</em><br>
<em>&gt; &gt; scanning a human mind are not allowed at first,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Why &quot;not allowed&quot;?  Personally, I have no complaint if an AI uses a
</em><br>
<em>&gt; nondestructive scan of my brain for raw material.  Or do you mean &quot;not
</em><br>
<em>&gt; allowed&quot; because no access to nanotech?
</em><br>
<em>&gt; 
</em><br>
<p><em>&gt; &gt; how will it learn to model people, and the wider geopolitical environment?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I agree that this knowledge would be *useful* for a pre-takeoff seed AI.
</em><br>
<em>&gt; Is this knowledge *necessary*?
</em><br>
<em>&gt; 
</em><br>
<p>Before it runs anything real-world I would say it is pretty necessary.
<br>
<p><em>&gt; &gt; Do you believe that given sufficient intelligence, experience is not
</em><br>
<em>&gt; &gt; required?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I believe that, as intelligence increases, experience required to solve a
</em><br>
<em>&gt; given problem decreases.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I hazard a guess that, given superintelligence, the whole architecture
</em><br>
<em>&gt; (cognitive and emotional) of the individual human brain and human society
</em><br>
<em>&gt; could be deduced from: examination of nontechnical webpages, plus
</em><br>
<em>&gt; simulation-derived heuristics about evolutionary psychology and game
</em><br>
<em>&gt; theory, plus the Bayesian Probability Theorem.
</em><br>
<em>&gt; 
</em><br>
<p>I think you are guessing wrong unless quite a bit of the detailed common
<br>
sense is developed or entered before the young AI goes off examining
<br>
papers and running simulations. Knowing the architecture of human minds
<br>
is not sufficient for having working knowledge of hwo to deal with human
<br>
beings.
<br>
<p>There is no way that understanding the fundamentals of human beings
<br>
would give understanding of the actual geo-political situation as it
<br>
exists.  And extracting it from news sources and broadcasts again
<br>
requires a lot of common sense knowledge. 
<br>
<p><em>&gt; &gt; At what point in it's education will you allow it to develop (if it's not
</em><br>
<em>&gt; &gt; already available) &amp; use nanotech?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &quot;Allow&quot; is probably the wrong choice of wording.  If growing into
</em><br>
<em>&gt; Friendliness requires continuous human checking of decisions about how to
</em><br>
<em>&gt; make decisions, up to or slightly beyond the human-equivalence level, then
</em><br>
<em>&gt; there might be genuine grounds (i.e., a-smart-AI-would-agree-with-you
</em><br>
<em>&gt; grounds) for asking the growing AI not to grow too fast, so that you can
</em><br>
<em>&gt; keep talking with ver about Friendliness during a controlled transition.
</em><br>
<em>&gt; Once the AI reaches human-equivalence, the heuristics that say &quot;daddy
</em><br>
<em>&gt; knows best, so listen to your programmers&quot; will begin to decrease in
</em><br>
<em>&gt; justification, and the rationale for limiting growth will be similarly
</em><br>
<em>&gt; attenuated.  Once the AI transcends human-equivalence in Friendliness
</em><br>
<em>&gt; (i.e., ve wins all arguments with the programmers), then there will be no
</em><br>
<em>&gt; further rationale for limiting growth and all the brakes are off.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Incidentally, I should note that, as I visualize this &quot;gradual growth&quot;
</em><br>
<em>&gt; process, it shouldn't take very long.  From the moment the AI realizes a
</em><br>
<em>&gt; hard takeoff lies ahead to the moment human-timescale phase terminates,
</em><br>
<em>&gt; should be... oh... twelve hours or so.  Because the instant that the AI
</em><br>
<em>&gt; says it's ready for a hard takeoff, you are operating on Singularity time
</em><br>
<em>&gt; - in other words, six thousand people are dying for every hour delayed.
</em><br>
<em>&gt; Ideally we'd see that the AI was getting all the Friendliness decisions
</em><br>
<em>&gt; more or less right during the controlled ascent, in which case we could
</em><br>
<em>&gt; push ahead as fast as humanly possible.
</em><br>
<em>&gt; 
</em><br>
<p>How can human programmers can answer a sufficient number of the AIs
<br>
questions in a mere 12 hours?  AI time is not the gating factor in this
<br>
phase.  And there is no reason to rush it.  So many people dying per
<br>
hour is irrelevant and emotionalizes the conversation unnecessarily. 
<br>
Letting the AI loose too early can easily terminate all 6 billion+ of
<br>
us.
<br>
<p><em>&gt; If the AI was Friendliness-savvy enough during the prehuman training
</em><br>
<em>&gt; phase, we might want to eliminate the gradual phase entirely, thus
</em><br>
<em>&gt; removing what I frankly regard as a dangerous added step.
</em><br>
<em>&gt; 
</em><br>
<p>How does it become dependably Friendliness-savvy without the feedback? 
<br>
Or do I misunderstand what gradual phase you want to eliminate?
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0463.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<li><strong>Previous message:</strong> <a href="0461.html">Samantha Atkins: "Re: Basement Education"</a>
<li><strong>In reply to:</strong> <a href="0460.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0463.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<li><strong>Reply:</strong> <a href="0463.html">Eliezer S. Yudkowsky: "Re: Basement Education"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#462">[ date ]</a>
<a href="index.html#462">[ thread ]</a>
<a href="subject.html#462">[ subject ]</a>
<a href="author.html#462">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
