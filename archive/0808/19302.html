<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=WINDOWS-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: [sl4] Niceness to humanity as a useful limitation</title>
<meta name="Author" content="Stuart Armstrong (dragondreaming@googlemail.com)">
<meta name="Subject" content="[sl4] Niceness to humanity as a useful limitation">
<meta name="Date" content="2008-08-07">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>[sl4] Niceness to humanity as a useful limitation</h1>
<!-- received="Thu Aug  7 20:40:02 2008" -->
<!-- isoreceived="20080808024002" -->
<!-- sent="Fri, 8 Aug 2008 03:37:53 +0100" -->
<!-- isosent="20080808023753" -->
<!-- name="Stuart Armstrong" -->
<!-- email="dragondreaming@googlemail.com" -->
<!-- subject="[sl4] Niceness to humanity as a useful limitation" -->
<!-- id="38f493f10808071937r4542e3a9o46a9cdfee0070653@mail.gmail.com" -->
<!-- charset="WINDOWS-1252" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Stuart Armstrong (<a href="mailto:dragondreaming@googlemail.com?Subject=Re:%20[sl4]%20Niceness%20to%20humanity%20as%20a%20useful%20limitation"><em>dragondreaming@googlemail.com</em></a>)<br>
<strong>Date:</strong> Thu Aug 07 2008 - 20:37:53 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19303.html">Eric Burton: "[sl4] Je recommende"</a>
<li><strong>Previous message:</strong> <a href="19301.html">Harvey Newstrom: "Re: [sl4] Unlikely singularity?"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19302">[ date ]</a>
<a href="index.html#19302">[ thread ]</a>
<a href="subject.html#19302">[ subject ]</a>
<a href="author.html#19302">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Some built-in limitations are very useful for an entity, even if they
<br>
depart from perfect rationality. For instance, a propensity for
<br>
violent anger is a very worthwhile trait in many societies and
<br>
situations; the ability to reject an advantageous deal is a necessary
<br>
trait for negotiators; the inability to break a contract brings great
<br>
dividends. Often, organisations impose such limits on themselves; the
<br>
logic of MAD worked as long as you can convince your enemy that you
<br>
cannot refuse to retaliate after an attack – which means making
<br>
changes in your own organisation so that retaliation can't de
<br>
derailed. An old strategy of the swiss army was based around the fact
<br>
that generals could never order their soldiers to surrender.
<br>
<p>Whereas for the singularity, we want the AI(s) to have a particular
<br>
limitation: be &quot;nice&quot; to humans (for whatever value of &quot;nice&quot;). Is
<br>
there some way to set up the situation so that being nice to humans is
<br>
an advantageous limitation?
<br>
<p>Comparative advantage suggests one possibility; if the resources
<br>
required to run humans is small, it might be in the AI's interests to
<br>
farm out certain problems to humankind, rather than designing its own
<br>
solving software. But I wouldn't count too much on that. Trying to
<br>
implement advantageous limitations in the case of the a fast
<br>
singularity is probably pointless as well.
<br>
<p>I propose an idea that might be useful in the case of a slow
<br>
singularity where the AI's code is obscure and AI competition is rife
<br>
(this is a situation I have not seen dealt with much so far – it's
<br>
also highly dangerous).
<br>
<p>(Assumptions: the notions of &quot;niceness&quot;, obscure code, and &quot;keeping
<br>
one's word&quot; are notions that can be sensibly extended to higher levels
<br>
of intelligence).
<br>
<p>The idea would be that, once a reasonably intelligent AI is designed
<br>
with an obscure code (obscure meaning that a comparable intelligence
<br>
can get little information from the code, compared with observing the
<br>
AI's behaviour), it be copied extensively and spread about the world,
<br>
with certain crucial differences: that the level of &quot;niceness&quot; to
<br>
humans be randomised, but correlated strongly with an advantageous
<br>
limitation (for instance, the inability to break one's word). Also,
<br>
these would both be correlated with a desire to keep one's code
<br>
obscure. A few AI's should be added where those behaviours vary in
<br>
correlated fashion through time.
<br>
<p>The idea then is that AI's would be unable to determine if the entity
<br>
they are interacting with is trustworthy. All they would know is that
<br>
one that is &quot;nasty&quot; to humans, or tries to clarify its source code, is
<br>
untrustworthy. Hence it will be advantageous to AI's to always be nice
<br>
to humans; hopefully it will be in the AI's interest to integrate
<br>
&quot;niceness&quot; into themselves as they upgrade their intelligence (this
<br>
would be the case if, for instance, it were partially possible to
<br>
distinguish between true niceness and just faking it). As long as the
<br>
cost of being nice is low, and the AI's are unable to clear up their
<br>
source code in a single upgrade, this should be the stable attractor.
<br>
<p>Note to those familiar with signalling in evolution: this is not the
<br>
same set-up. Signalling positive attributes generally requires
<br>
behaviours that are costly to fake; here, the behaviour – niceness to
<br>
humans – is cheap to fake, and is a necessary, not sufficient, signal
<br>
of another attribute. The equilibrium will be different, in that it
<br>
should be in every AI's interest to fake.
<br>
<p>If people think this is interesting, I'll try and model it
<br>
mathematically. And get a handle on its stability.
<br>
<p>Stuart
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19303.html">Eric Burton: "[sl4] Je recommende"</a>
<li><strong>Previous message:</strong> <a href="19301.html">Harvey Newstrom: "Re: [sl4] Unlikely singularity?"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19302">[ date ]</a>
<a href="index.html#19302">[ thread ]</a>
<a href="subject.html#19302">[ subject ]</a>
<a href="author.html#19302">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
