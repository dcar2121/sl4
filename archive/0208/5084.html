<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Why is Friendliness sacrosanct?</title>
<meta name="Author" content="Samantha Atkins (samantha@objectent.com)">
<meta name="Subject" content="Re: Why is Friendliness sacrosanct?">
<meta name="Date" content="2002-08-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Why is Friendliness sacrosanct?</h1>
<!-- received="Sat Aug 24 02:04:39 2002" -->
<!-- isoreceived="20020824080439" -->
<!-- sent="Fri, 23 Aug 2002 21:39:09 -0700" -->
<!-- isosent="20020824043909" -->
<!-- name="Samantha Atkins" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Why is Friendliness sacrosanct?" -->
<!-- id="3D670DED.6080000@objectent.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="DAV78QUm1OP3nTWoUO4000445a9@hotmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:samantha@objectent.com?Subject=Re:%20Why%20is%20Friendliness%20sacrosanct?"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Fri Aug 23 2002 - 22:39:09 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5085.html">Samantha Atkins: "Re: Metarationality (was: JOIN: Alden Streeter)"</a>
<li><strong>Previous message:</strong> <a href="5083.html">Ben Goertzel: "RE: Metarationality (was: JOIN: Alden Streeter)"</a>
<li><strong>In reply to:</strong> <a href="5067.html">Alden Streeter: "Why is Friendliness sacrosanct?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5109.html">Alden Streeter: "Re: Why is Friendliness sacrosanct?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5084">[ date ]</a>
<a href="index.html#5084">[ thread ]</a>
<a href="subject.html#5084">[ subject ]</a>
<a href="author.html#5084">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Alden Streeter wrote:
<br>
<em>&gt; As humans, our interests have been shaped by nothing more than evolution.
</em><br>
<em>&gt;&gt;From the strictly scientific viewpoint, our only reason for our existence
</em><br>
<em>&gt; (and that of all life) is to continue our existence by fulfilling our
</em><br>
<em>&gt; biological imperatives.  Our intellectual persuits are not technically goals
</em><br>
<em>&gt; in and of themselves, but only adaptations of survival methods given to us
</em><br>
<em>&gt; by evolution, which still have the ultimate purpose to simply promote our
</em><br>
<em>&gt; continued existence.
</em><br>
<p>This is not a &quot;strictly scientific viewpoint&quot;.  No &quot;viewpoint&quot; 
<br>
is strictly scientific although it may be based on scientific 
<br>
facts and theories.  As soon as it becomes a viewpoint it starts 
<br>
moving into the realms of philosophy and opinion.  A statement 
<br>
like &quot;the only reason for our existence&quot; is highly 
<br>
over-inflated. We, as conscious beings, have more than a little 
<br>
to say about what we will make the reason for our existence and 
<br>
the goals of lives.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; The entire concept of a &quot;Friendly&quot; AI to me seems irrationally
</em><br>
<em>&gt; anthropocentric.  Why should our human goals of survival take precident over
</em><br>
<p>Well, do you care whether or not a super-intelligence you create 
<br>
has any goals of protecting other sentient beings at all?  If 
<br>
not then this may say a lot about your personal values but it is 
<br>
hardly a blanket condemnation, much less a &quot;scientific&quot; one of 
<br>
the concept of Friendly AI or its importance.
<br>
<p><em>&gt; any of the AI's goals?  Our human goals, including the ultimate goal of
</em><br>
<em>&gt; survival, as well as our subgoals (is it valid to apply such AI terminology
</em><br>
<em>&gt; to humans as well?  I don't see why not) of happiness, pleasure, desire for
</em><br>
<em>&gt; knowledge, etc., were determined by our primitive evolution, and are
</em><br>
<em>&gt; ultimately determined by our physiology; so when we have the ultimate power
</em><br>
<p>You are talking about how we got here, not where we go from here.
<br>
<p><em>&gt; to control our evolution in the future, instead of enhancing our ability to
</em><br>
<em>&gt; achieve those goals (apotheosis), why couldn't we instead just change the
</em><br>
<em>&gt; goals?  But then the conundrum is that our existing goals should determine
</em><br>
<em>&gt; what future goals we should want to have instead, but if we change them,
</em><br>
<em>&gt; then we might not have wanted those new goals in the first place.  So does
</em><br>
<em>&gt; that mean that we must be stuck with the primitive goals we have evolved?
</em><br>
<em>&gt; 
</em><br>
<p>Nope.
<br>
<p><em>&gt; So then the same question can be asked of a Sysop-level AI - instead of
</em><br>
<em>&gt; working to help humans to acheive their petty, primitive, evolutionarily
</em><br>
<em>&gt; determined goals, why not just use its power to change the humans so they
</em><br>
<em>&gt; have different goals?  Shouldn't it, with its vastly superior intelligence,
</em><br>
<em>&gt; be able to think up better goals for the humans to have than the humans have
</em><br>
<em>&gt; thought of for themselves?  And why should humans not want the AI to have
</em><br>
<em>&gt; this type of power? - if the AI changed their goals for them, they would of
</em><br>
<em>&gt; course immediately realize that their new goals were the right goals all
</em><br>
<em>&gt; along.
</em><br>
<p>I don't consider the goal of continuous improving life to be in 
<br>
the least &quot;petty&quot; or &quot;primitive&quot;.  Do you?
<br>
<p>Do you believe it is the right of any brighter being that comes 
<br>
along to rewrite all &quot;lesser&quot; beings in whatever matter it 
<br>
choses or to destroy them?  Do you believe it should be?  If you 
<br>
are helping to design such a being (or that which becomes such a 
<br>
being) would you consider it just &quot;petty&quot; to look for a way to 
<br>
encourage it to be a help to human beings rather their doom?
<br>
<p>Do you believe it shows superior intellect to consider the well 
<br>
being of humanity as mere petty pre-programmed meaninglessness?
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5085.html">Samantha Atkins: "Re: Metarationality (was: JOIN: Alden Streeter)"</a>
<li><strong>Previous message:</strong> <a href="5083.html">Ben Goertzel: "RE: Metarationality (was: JOIN: Alden Streeter)"</a>
<li><strong>In reply to:</strong> <a href="5067.html">Alden Streeter: "Why is Friendliness sacrosanct?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5109.html">Alden Streeter: "Re: Why is Friendliness sacrosanct?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5084">[ date ]</a>
<a href="index.html#5084">[ thread ]</a>
<a href="subject.html#5084">[ subject ]</a>
<a href="author.html#5084">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
