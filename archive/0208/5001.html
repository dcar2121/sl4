<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Game theoretic concerns and the singularity (was RE: Are we Gods yet?)</title>
<meta name="Author" content="James Higgins (jameshiggins@earthlink.net)">
<meta name="Subject" content="Re: Game theoretic concerns and the singularity (was RE: Are we Gods yet?)">
<meta name="Date" content="2002-08-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Game theoretic concerns and the singularity (was RE: Are we Gods yet?)</h1>
<!-- received="Fri Aug 02 15:01:27 2002" -->
<!-- isoreceived="20020802210127" -->
<!-- sent="Fri, 02 Aug 2002 11:51:44 -0700" -->
<!-- isosent="20020802185144" -->
<!-- name="James Higgins" -->
<!-- email="jameshiggins@earthlink.net" -->
<!-- subject="Re: Game theoretic concerns and the singularity (was RE: Are we Gods yet?)" -->
<!-- id="3D4AD4C0.3030602@earthlink.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3d4a4a8a.27bc.0@yifan.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> James Higgins (<a href="mailto:jameshiggins@earthlink.net?Subject=Re:%20Game%20theoretic%20concerns%20and%20the%20singularity%20(was%20RE:%20Are%20we%20Gods%20yet?)"><em>jameshiggins@earthlink.net</em></a>)<br>
<strong>Date:</strong> Fri Aug 02 2002 - 12:51:44 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="5002.html">Evan Reese: "Re: Educating an AI."</a>
<li><strong>Previous message:</strong> <a href="5000.html">Michael Anissimov: "Re: Game theoretic concerns and the singularity (was RE: Are we Gods yet?)"</a>
<li><strong>In reply to:</strong> <a href="5000.html">Michael Anissimov: "Re: Game theoretic concerns and the singularity (was RE: Are we Gods yet?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4999.html">James Higgins: "Re: Are we Gods yet?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5001">[ date ]</a>
<a href="index.html#5001">[ thread ]</a>
<a href="subject.html#5001">[ subject ]</a>
<a href="author.html#5001">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michael Anissimov wrote:
<br>
<em>&gt; James Higgins wrote:
</em><br>
<em>&gt; Days?  I would estimate that the window of difference allowed for two 
</em><br>
<em>&gt; transhumans newly initiating a cycle of strong self-improvement to 
</em><br>
<em>&gt; maintain game-theoretic equivalency would be somewhere in the 
</em><br>
<em>&gt; nanosecond or microsecond range.  In addition, I would find it highly 
</em><br>
<p>Depends on the actual rate of progress and at which point, exactly, the 
<br>
intelligence it considered to be an SI vs transhuman.  The fact is that 
<br>
any discussion of these matters is mostly opinion based on a handfull of 
<br>
(possibly) related facts.  I was being conservative by using days, since 
<br>
I would imagine that being behind by more than days (if we see hard 
<br>
takeoff with exponential progress) could easily make any lagging AIs 
<br>
irrelevant.
<br>
<p><em>&gt; likely that posthuman SIs would choose to reconcile their differences 
</em><br>
<em>&gt; (likely to be minor, in terms of goals) rather than burning resources 
</em><br>
<em>&gt; in a mutually wasteful physical conflict.  Looking at your rhetoric, 
</em><br>
<p>Well, it is likely that it would take an insignificant amount of 
<br>
resources for an SI to halt or destroy even thousands of trans-humans. 
<br>
Much like it takes an insignificant amount of human resources to keep a 
<br>
large number of Apes (and the like) confined in zoos.
<br>
<p>As stated many times previously on this list, it is impossible to say 
<br>
anything about what an SI is, or is not, likely to do.  So theorizing 
<br>
that it is &quot;likely that posthuman SIs would choose to reconsile their 
<br>
differences&quot; is pure speculation.  Also, why is it likely that their 
<br>
differences (in terms of goals) will be minor?  Please explain how you 
<br>
determined that?
<br>
<p>Also, while it has little effect on me personally, I'd suggest not using 
<br>
terms like &quot;rhetoric&quot; unless you are purposfully trying to piss someone off.
<br>
<p><em>&gt; you seem to be thinking in terms of &quot;all sentients will necessarily 
</em><br>
<em>&gt; have strong observer bias&quot;, which, of course, is dangerous when 
</em><br>
<em>&gt; seriously considering the motivations of entities outside of the 
</em><br>
<em>&gt; familiar phase space.
</em><br>
<p>Well, as much as possible, I try not to consider what SIs will be like 
<br>
(since it is impossible to know).  Transhumans are a tiny bit easier (at 
<br>
least at the lower end of the spectrum), but it is still virtually 
<br>
impossible to predict much, if anything.
<br>
<p>What I was trying to point out is that, unless two or more SIs are at 
<br>
least within days of each other, the SI that gets there first calls all 
<br>
the shots.  That SI will be able to do whatever it likes with any 
<br>
transhuman AIs and even with us.  I was not trying to speculate on what 
<br>
it would do, just what it could do.
<br>
<p><em>&gt; Wow, you've got a powerful Us/Them complex going on when you talk about 
</em><br>
<em>&gt; SIs.  You talk as if all humans upgrading to Powerhood isn't the only 
</em><br>
<em>&gt; long-term inevitability, as if an indifferent SI could come into 
</em><br>
<p>I think your trying to read too much into my grammar.  I believe we will 
<br>
create *one* SI (unless that SI wants company).  I also believe that if 
<br>
it is possible here, it will occur elsewhere in the universe.  Thus the 
<br>
SI created by us will likely encounter other (non-terran) SIs.
<br>
<p>Also, I don't at all think that &quot;all humans upgrading to Powerhood&quot; is 
<br>
at all inevitable.  It is probably more likely that we won't be 
<br>
upgraded.  This depends on the goals of the first SI of course, which we 
<br>
can't predict.
<br>
<p><em>&gt; existence but not see mankind as building blocks, and as if the first 
</em><br>
<em>&gt; benevolent transhuman won't create a moral singleton to protect 
</em><br>
<em>&gt; individual rights (in the case of a malevolent or indifferent 
</em><br>
<em>&gt; transhuman, everything goes black immediately).  Out of curiosity, may 
</em><br>
<p>There are many problems here.  First, this &quot;moral&quot; singleton is moral in 
<br>
who's eyes?  It is incredibly unlikely that any single morality would be 
<br>
acceptable to all humans.  So, best case, is that most humans consider 
<br>
this SI moral, but even then a large number will still consider it imoral.
<br>
<p>Second, if we get transhumans before an SI, the odds that we will get a 
<br>
single SI go way down.  If we somehow go the transhuman route (instead 
<br>
of the AI route) then humans will eventually become SIs.  And it is much 
<br>
more likely, on that path, that we get a large number of SIs occuring at 
<br>
the same relative time.  It is difficult to predict how this might end 
<br>
up, could be good or bad.  Depends on the nature of most of the SIs, 
<br>
which we can't predict.
<br>
<p>There is a problem with having a single SI that oversees / protects a 
<br>
population of lesser intelligences.  Should it ever encounter another SI 
<br>
which is violent it may have great difficulty defending itself since it 
<br>
would have to spend significant energy being &quot;moral&quot; to its charges. 
<br>
And, depending on its concept of morality, it may not be able to defend 
<br>
itself at all.  This has been previously discussed on the list extensively.
<br>
<p><em>&gt; I ask you if you've mulled over any of these concepts before?  Also, 
</em><br>
<p>Yes, quite a lot actually.
<br>
<p><em>&gt; even if Earth-originating SIs ran into extraterrestrial SIs, wouldn't 
</em><br>
<em>&gt; that potential occurence be so insanely far into the subjective future 
</em><br>
<em>&gt; as to render it irrelevant to us today?
</em><br>
<p>I don't know, do you consider 20 (real time) years from now to be 
<br>
irrelevant?  Is there some magic way we can predict when such an event 
<br>
could occur?  The presence of extraterrestrial SIs could become aparent 
<br>
within micro-seconds of a terran SI forming.  In any case, I don't see 
<br>
how such a significant event could be considered irrelevant.
<br>
<p>James Higgins
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5002.html">Evan Reese: "Re: Educating an AI."</a>
<li><strong>Previous message:</strong> <a href="5000.html">Michael Anissimov: "Re: Game theoretic concerns and the singularity (was RE: Are we Gods yet?)"</a>
<li><strong>In reply to:</strong> <a href="5000.html">Michael Anissimov: "Re: Game theoretic concerns and the singularity (was RE: Are we Gods yet?)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="4999.html">James Higgins: "Re: Are we Gods yet?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#5001">[ date ]</a>
<a href="index.html#5001">[ thread ]</a>
<a href="subject.html#5001">[ subject ]</a>
<a href="author.html#5001">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:40 MDT
</em></small></p>
</body>
</html>
