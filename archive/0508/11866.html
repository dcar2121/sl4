<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: On Our Duty to Not Be Responsible for Artificial Minds</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: On Our Duty to Not Be Responsible for Artificial Minds">
<meta name="Date" content="2005-08-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: On Our Duty to Not Be Responsible for Artificial Minds</h1>
<!-- received="Tue Aug  9 11:09:46 2005" -->
<!-- isoreceived="20050809170946" -->
<!-- sent="Tue, 09 Aug 2005 10:09:46 -0700" -->
<!-- isosent="20050809170946" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: On Our Duty to Not Be Responsible for Artificial Minds" -->
<!-- id="42F8E35A.9060705@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="00ee01c59c5e$c7cd5a30$9a00a8c0@markcomputer" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20On%20Our%20Duty%20to%20Not%20Be%20Responsible%20for%20Artificial%20Minds"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Aug 09 2005 - 11:09:46 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11867.html">Thomas Petersen: "RE: &quot;Objective&quot; Morality"</a>
<li><strong>Previous message:</strong> <a href="11865.html">H C: "RE: &quot;Objective&quot; Morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11870.html">Eliezer S. Yudkowsky: "Re: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<li><strong>Maybe reply:</strong> <a href="11870.html">Eliezer S. Yudkowsky: "Re: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<li><strong>Reply:</strong> <a href="11880.html">Ben Goertzel: "RE: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<li><strong>Reply:</strong> <a href="11887.html">Samantha Atkins: "Re: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<li><strong>Reply:</strong> <a href="../0603/14295.html">Peter Voss: "Proving the Impossibility of Stable Goal Systems"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11866">[ date ]</a>
<a href="index.html#11866">[ thread ]</a>
<a href="subject.html#11866">[ subject ]</a>
<a href="author.html#11866">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Mark Walker wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; I think we are making progress, for I think I see what you mean here. If 
</em><br>
<em>&gt; I understand you, you are saying that someone who FALSELY claims that 
</em><br>
<em>&gt; &quot;this entity I created is an autonomous being, I'm not responsible for 
</em><br>
<em>&gt; its actions&quot; has no defense. If this is what you are saying, I agree. 
</em><br>
<em>&gt; Suppose, however, the AI passes whatever tests we have for autonomy and 
</em><br>
<em>&gt; does as well as you or I. Can the creators of the AI not now claim that 
</em><br>
<em>&gt; they are no more responsible for the entities actions than your parents 
</em><br>
<em>&gt; are responsible for your actions? If not then why is this not speciesism?
</em><br>
<p>Just because you invent a word does not give it a real referent nor even a 
<br>
well-defined meaning.
<br>
<p>What exactly is &quot;autonomy&quot;?  What are these tests that measure it?  I cannot 
<br>
even conceive of such a test.
<br>
<p>Considering the relation between my parents and myself, &quot;autonomy&quot; consists of 
<br>
my parents being able to control a small set of variables in my upbringing and 
<br>
unable to control a much larger set of variables in my cognitive design.  Not 
<br>
because my parents *chose* to control those variables and no other, but 
<br>
because my parents were physically and cognitively *unable* to select my 
<br>
genome on the basis of its consequences.  Furthermore, my cognitive design - 
<br>
fixed beyond parental control - determined how I reacted to parental 
<br>
upbringing.  My fixed cognitive design placed some variables within my 
<br>
parents' deliberate control, in the sense that they could, by speaking 
<br>
English, ensure I would grow up speaking English.  However, some variables 
<br>
that my parents greatly desired to control, such as my religion, were beyond 
<br>
the reach of their best efforts at upbringing.  It is not that they chose not 
<br>
to control this variable but that they were incapable of controlling it.
<br>
<p>In the case of an AI researcher we have many, many possibilities.  Here are 
<br>
some possibilities that occur to me:
<br>
<p>1)  The AI researcher is fully capable of choosing between AI designs on the 
<br>
basis of their consequences, and chooses an AI design which invokes no 
<br>
significant moral processing within the AI.  In this case I would assign moral 
<br>
responsibility to the AI researcher alone, for all consequences good or ill; 
<br>
the AI itself is not a moral agent.
<br>
<p>I assign full responsibility to the AI researcher for all consequences, 
<br>
intended or unintended.  An AI researcher has a responsibility to choose an AI 
<br>
design with predictable consequences.  If the AI researcher negligently uses 
<br>
an AI design the AI researcher can't predict, the AI researcher is still fully 
<br>
responsible for all actual consequences.
<br>
<p>2)  A competent AI researcher, acting on a perceived moral responsibility to 
<br>
create moral children, deliberately constructs an AI which is tightly or 
<br>
loosely analogous to a human - such that the initial design responds to 
<br>
environmental stimuli and parental upbringing much as does a human, learns and 
<br>
argues morality much as does a human, is partially unaware of its own emotions 
<br>
like a human, etc.
<br>
<p>We presume that this is a deliberate attempt to create a child of humankind, a 
<br>
worthy participant in the story of Earth-originating life.
<br>
<p>In this case I would assign a mixture of moral responsibility to the AI 
<br>
researcher and to the AI.  The AI, having been deliberately constructed as a 
<br>
moral agent, bears responsibility for its actions.  If we hold the AI to 
<br>
account, it will understand what it means to be held to account, and treat 
<br>
this as a moral argument in the same way we do.  (In contrast a paperclip 
<br>
maximizer would care about human moral arguments only as a sort of cognitive 
<br>
activity in humans that might be exploited to create paperclips.)  The AI 
<br>
researcher is responsible for all predictable consequences of &quot;constructing a 
<br>
humanlike moral agent&quot;, including liability for child abuse if later 
<br>
authorities determine the initial design to have been botched.  But I would 
<br>
not say that the AI researcher is responsible for all actions of the created 
<br>
AI, presuming that the created AI was at least as initially benevolent as an 
<br>
average human.  Deliberately creating an AI that is worse than average, for 
<br>
example, an AI that starts out with the same emotional makeup as an autistic 
<br>
or a serial killer, makes the AI researcher liable for both child abuse and 
<br>
for the consequences of the AI's actions.
<br>
<p>3)  The AI researcher deliberately chooses an AI design which involves complex 
<br>
moral processing, but a different sort of complex moral processing than a 
<br>
human being.  Coherent Extrapolated Volition, for example.  In this case, 
<br>
assigning moral responsibility becomes difficult; we're operating outside the 
<br>
customary problem space.  An AI researcher, responding to a perceived moral 
<br>
duty, invents an AI which takes its direction from a complexly computed 
<br>
property of the human species as a whole.  If this AI saves a life, to whom 
<br>
belongs the credit?  The researcher?  The human species?  The AI?
<br>
<p>I would assign moral responsibility to the AI programmer for the predictable 
<br>
consequences of creating such an AI, but not the unpredictable consequences, 
<br>
provided that the AI as a whole has predominantly good effects (even if there 
<br>
are some negative ones).  If the AI has a predominantly negative effect, 
<br>
whether by bug or by unintended consequence, then I would assign full 
<br>
responsibility to the programmer.
<br>
<p>If a CEV saves you from dying, I would call that a predictable (positive) 
<br>
consequence and assign at least partial responsibility to the programmers and 
<br>
their supporters.  I would not assign them responsibility for the entire 
<br>
remaining course of your life in detail, positive or negative, even though 
<br>
this life would not have existed without the CEV.  I would forgive the 
<br>
programmers that your evil mother-in-law will also live forever; they didn't 
<br>
mean to do that to you specifically.
<br>
<p>**
<br>
<p>I don't believe there exists any such thing as &quot;autonomy&quot;.
<br>
<p>The causal graph of physics goes back at least to the Big Bang.  If you don't 
<br>
know the cause, that's your own ignorance; it doesn't mean there is no cause.
<br>
<p>I am not &quot;autonomous&quot;.  I am a Word spoken by evolution, which determined both 
<br>
my tendencies, and my susceptibility to environmental influence.  Where there 
<br>
is randomness in me it is because my design permits randomness effects. 
<br>
Evolution created me via a subtle and broken algorithm, which caused the goals 
<br>
of my internal psychology to depart far from natural selection's sole 
<br>
criterion of inclusive genetic fitness.  Either way, evolution bears no moral 
<br>
responsibility because natural selection is too far outside the humane space 
<br>
of optimization processes to internally represent moral arguments.
<br>
<p>My parents were almost entirely powerless compared to an AI designer.  My 
<br>
parents can bear moral responsibility only for what they could control.  Given 
<br>
those fixed background circumstances, I understand, respect, and am grateful 
<br>
to my parents where they deliberately chose not to exercise a possible 
<br>
control, seeing an obligation to let me make up my own mind.  Which is to say 
<br>
that my parents handed determination back to the internal forces in my mind, 
<br>
which they did not choose to create.  My parents let me make my own decision 
<br>
rather than crushing me, in a case where my internal cognitive forces would 
<br>
exist regardless.  Had my parents also knowingly selected my nature, their 
<br>
decision not to nurture too hard would take on a stranger meaning.
<br>
<p>It is not clear what, if anything, an AI researcher can deliberately do that 
<br>
is analogous to the choice a human parent faces - even if we understand and 
<br>
respect and attach significant moral value to a human parent's choice not to 
<br>
determine offspring too strongly.  The mechanisms of &quot;autonomy&quot;, if we value 
<br>
them, would need to be deliberately created in a nonhuman mind.  It is 
<br>
predictable that if you construct a mind to love it will love, and if you 
<br>
construct a mind to hate it will hate.  In what sense would the AI programmer 
<br>
*not* be responsible?  Perhaps we can rule that we value human likeness in 
<br>
artificial minds, that it is good to grant them many emotions sometimes in 
<br>
conflict.  We could hold the AI researcher responsible for the choice to 
<br>
construct a humanlike mind, but not for the specific and unpredictable outcome 
<br>
of the humanlike emotional conflicts.
<br>
<p>This exception requires that the AI researcher gets it right and creates a 
<br>
healthy child of humankind.  Screw it up - create a mind whose internal 
<br>
conflicts turn out to be simpler and less interesting than human average, or 
<br>
whose internal conflicts turn out to be more painful - and I would hold the 
<br>
designers fully responsible.  If you can't do it right, then DON'T DO IT.  If 
<br>
you aren't sure you can do it right, WAIT until you are.  I would like to see 
<br>
humankind get through the 21st century without inventing new and horrible 
<br>
forms of child abuse.
<br>
<p>An AI researcher who deliberately builds an AI unpredictable to the designer, 
<br>
but which AI does not qualify as a healthy child of humankind, bears full 
<br>
responsibility for the consequences of the AI's unpredictable actions whatever 
<br>
they may be.  This is so even if the AI researcher claims deliberate refusal 
<br>
to understand in order to preserve the quote autonomy unquote of the AI.  I 
<br>
would advise that you not believe the claim.  Incompetence is not a moral 
<br>
duty, but people often try to excuse it as a moral duty.  &quot;Moral autonomy&quot; is 
<br>
not randomness.  There is nothing moral about randomness.  Nor is everything 
<br>
that you're too incompetent to predict &quot;autonomous&quot;.
<br>
<p>Moral autonomy requires a specific kind of cognitive complexity which will 
<br>
take high artistry to create in an artificial mind.  The designers might 
<br>
*choose* not to compute out in advance the child's destiny, nor fine-tune the 
<br>
design on the basis of such predictions.  But be very sure, the designers do 
<br>
understand *all* the forces involved - if they possess the art to create a 
<br>
healthy child of humankind.
<br>
<p>Ignorance exists in the mind, not in reality.  The blank spot on the map does 
<br>
not correspond to a blank spot on the territory.  To whatever extent &quot;moral 
<br>
autonomy&quot; invokes designer ignorance about outcomes, &quot;moral autonomy&quot; must be 
<br>
a two-place predicate relating a designer and a designee, not a one-place 
<br>
predicate intrinsically true of the designee.  There are mysterious questions 
<br>
but never mysterious answers, etc.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11867.html">Thomas Petersen: "RE: &quot;Objective&quot; Morality"</a>
<li><strong>Previous message:</strong> <a href="11865.html">H C: "RE: &quot;Objective&quot; Morality"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11870.html">Eliezer S. Yudkowsky: "Re: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<li><strong>Maybe reply:</strong> <a href="11870.html">Eliezer S. Yudkowsky: "Re: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<li><strong>Reply:</strong> <a href="11880.html">Ben Goertzel: "RE: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<li><strong>Reply:</strong> <a href="11887.html">Samantha Atkins: "Re: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<li><strong>Reply:</strong> <a href="../0603/14295.html">Peter Voss: "Proving the Impossibility of Stable Goal Systems"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11866">[ date ]</a>
<a href="index.html#11866">[ thread ]</a>
<a href="subject.html#11866">[ subject ]</a>
<a href="author.html#11866">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
