<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: On the dangers of AI</title>
<meta name="Author" content="justin corwin (outlawpoet@gmail.com)">
<meta name="Subject" content="Re: On the dangers of AI">
<meta name="Date" content="2005-08-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: On the dangers of AI</h1>
<!-- received="Tue Aug 16 17:58:18 2005" -->
<!-- isoreceived="20050816235818" -->
<!-- sent="Tue, 16 Aug 2005 16:58:16 -0700" -->
<!-- isosent="20050816235816" -->
<!-- name="justin corwin" -->
<!-- email="outlawpoet@gmail.com" -->
<!-- subject="Re: On the dangers of AI" -->
<!-- id="3ad827f30508161658611531ca@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="JNEIJCJJHIEAILJBFHILCEACFGAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> justin corwin (<a href="mailto:outlawpoet@gmail.com?Subject=Re:%20On%20the%20dangers%20of%20AI"><em>outlawpoet@gmail.com</em></a>)<br>
<strong>Date:</strong> Tue Aug 16 2005 - 17:58:16 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11928.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<li><strong>Previous message:</strong> <a href="11926.html">Peter de Blanc: "Re: On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11925.html">Ben Goertzel: "RE: On the dangers of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11937.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11937.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11927">[ date ]</a>
<a href="index.html#11927">[ thread ]</a>
<a href="subject.html#11927">[ subject ]</a>
<a href="author.html#11927">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
'Dangerous!' cried Gandalf. 'And so am I, very dangerous: more
<br>
dangerous than anything you will ever meet, unless you are brought
<br>
alive before the seat of the Dark Lord. And Aragorn is dangerous, and
<br>
Legolas is dangerous. You are beset with Dangers, Gimli son of Gloin;
<br>
for you are dangerous yourself, in your own fashion....'
<br>
<p>Dangerous is a very poor indicator of desireability. Anything of any
<br>
capability whatsoever is dangerous, often in relationship to it's
<br>
capability in some respects.
<br>
<p>It's tempting to imagine that the space of things an Intelligence
<br>
might do is limited merely to human-relative moral decisions.
<br>
Unfortunately, this isn't true, even for humans. We can occasionally
<br>
make decisions vastly removed from moral choices that have
<br>
consequences we might call moral, and make many moral decisions which
<br>
have no such consequences.
<br>
<p>This is probably the point where someone comes in and mentions
<br>
paperclips and the conversation devolves.
<br>
<p>AIs need not have human-like goals to make human-relative moral
<br>
mistakes. It helps to have nasty things like xenophobic discounting of
<br>
the value of those different than you, and other nice human mental
<br>
corruptions, but they are not neccesary to do bad things.
<br>
<p>Ben also makes a good point here on the complexity of moral decisions.
<br>
Any standard moral puzzler could be posed here in it's place as well.
<br>
<p>Your concept of a one-way filter is naive. Many people have fallen
<br>
into depravity when moral context was removed or changed. The choice
<br>
is not between 'being good' and 'being bad', but rather at every
<br>
junction, a complex evaluation of the state of the world, the actions
<br>
you may take, and the goals you have (assuming a singleton action
<br>
model, of course). One of these goals may well be 'be good', but that
<br>
makes the evaluation no simpler. Actions must be chosen, and sadly, we
<br>
may not simply filter all that do not fall into the 'good' bin.
<br>
<p>I suspect there is some theory of behavior, objectively extractable
<br>
from the semantics of goal-oriented agents interacting that we might
<br>
call a moral calculus. But short of demonstrating such a system and
<br>
it's direct benefits, no one can appeal to the universality of morals,
<br>
any more than an angry libertine can use the Constitution to 'prove'
<br>
government troops will not go to unpleasant places with unpleasant
<br>
agendas.
<br>
<p><pre>
-- 
Justin Corwin
<a href="mailto:outlawpoet@hell.com?Subject=Re:%20On%20the%20dangers%20of%20AI">outlawpoet@hell.com</a>
<a href="http://outlawpoet.blogspot.com">http://outlawpoet.blogspot.com</a>
<a href="http://www.adaptiveai.com">http://www.adaptiveai.com</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11928.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<li><strong>Previous message:</strong> <a href="11926.html">Peter de Blanc: "Re: On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11925.html">Ben Goertzel: "RE: On the dangers of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11937.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11937.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11927">[ date ]</a>
<a href="index.html#11927">[ thread ]</a>
<a href="subject.html#11927">[ subject ]</a>
<a href="author.html#11927">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
