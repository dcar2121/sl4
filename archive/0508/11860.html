<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: 6 points about Coherent Extrapolated Volition</title>
<meta name="Author" content="Jef Allbright (jef@jefallbright.net)">
<meta name="Subject" content="Re: 6 points about Coherent Extrapolated Volition">
<meta name="Date" content="2005-08-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: 6 points about Coherent Extrapolated Volition</h1>
<!-- received="Sat Aug  6 21:34:58 2005" -->
<!-- isoreceived="20050807033458" -->
<!-- sent="Sat, 06 Aug 2005 20:34:54 -0700" -->
<!-- isosent="20050807033454" -->
<!-- name="Jef Allbright" -->
<!-- email="jef@jefallbright.net" -->
<!-- subject="Re: 6 points about Coherent Extrapolated Volition" -->
<!-- id="42F5815E.4010604@jefallbright.net" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="42F56B9B.6020502@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Jef Allbright (<a href="mailto:jef@jefallbright.net?Subject=Re:%206%20points%20about%20Coherent%20Extrapolated%20Volition"><em>jef@jefallbright.net</em></a>)<br>
<strong>Date:</strong> Sat Aug 06 2005 - 21:34:54 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11861.html">Jeff Medina: "H+/S'n ethics, New Hampshire"</a>
<li><strong>Previous message:</strong> <a href="11859.html">Eliezer S. Yudkowsky: "Re: 6 points about Coherent Extrapolated Volition"</a>
<li><strong>In reply to:</strong> <a href="11859.html">Eliezer S. Yudkowsky: "Re: 6 points about Coherent Extrapolated Volition"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11860">[ date ]</a>
<a href="index.html#11860">[ thread ]</a>
<a href="subject.html#11860">[ subject ]</a>
<a href="author.html#11860">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Coherence is a worthy objective for any model, but it is fundamentally 
<br>
limited by context.  Increasing coherence tends to imply increasing 
<br>
&quot;truth&quot;, all else being equal, but requires a broad base of independent 
<br>
inputs with substantial individual credibility.  Otherwise, you have 
<br>
just one coherent model within the vast space of possible coherent 
<br>
models exhibiting fitness.  Looking back on evolutionary trajectories we 
<br>
see a coherent path, but this does not provide for looking forward and 
<br>
predicting a chaotic future.
<br>
<p>It seems clear to me that the best strategy is a broad based one, 
<br>
developing the tools that will amplify the awareness and effective 
<br>
decision-making of those dispersed around us in meme-space, with 
<br>
increasingly encompassing spheres of interaction--growth directed from 
<br>
the expanding interests identified with Self via interaction with the 
<br>
adjacent possible.
<br>
<p>We're not going to find a convergent set of goals in the increasing 
<br>
diversity of evolutionary progress.   The best we can do is discover and 
<br>
apply principles of effective interaction within this dynamical system 
<br>
at this metastable level and enjoy the ride.
<br>
<p>- Jef
<br>
<a href="http://www.jefallbright.net">http://www.jefallbright.net</a>
<br>
<p><p>Eliezer S. Yudkowsky wrote:
<br>
<p><em>&gt; Michael Anissimov wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; Hi Eliezer,
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Few quick questions on the CEV post - notice that you've turned 
</em><br>
<em>&gt;&gt; &quot;Collective Extrapolated Volition&quot; into &quot;Coherent Extrapolated 
</em><br>
<em>&gt;&gt; Volition&quot; here - is this a permanent jargon change or are you just 
</em><br>
<em>&gt;&gt; using the term &quot;coherent&quot; to make some sort of point in this 
</em><br>
<em>&gt;&gt; context?  Please explain.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think it will be a permanent jargon change, though perhaps not a 
</em><br>
<em>&gt; final equilibrium; who knows but that there may be more in store.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; 3.  The CEV writes an AI.  This AI may or may not work in any way 
</em><br>
<em>&gt;&gt;&gt; remotely resembling a volition-extrapolator.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; ...though it's extremely likely it would, right?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; What?  No.  Possible, sure.  Where would be the justification for 
</em><br>
<em>&gt; calling it 'extremely likely'?  Asked what we want at the 
</em><br>
<em>&gt; object-level, we may or may not want anything that treats with our 
</em><br>
<em>&gt; wants at the meta-level.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; In the broadest sense, &quot;volition extrapolation&quot; basically means 
</em><br>
<em>&gt;&gt; &quot;guessing what people want&quot;, right?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;&gt; 4.  The CEV returns one coherent answer.  The AI it returns may or may
</em><br>
<em>&gt;&gt;&gt; not display any given sort of coherence in how it treats different
</em><br>
<em>&gt;&gt;&gt; people, or create any given sort of coherent world.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Of course, if it doesn't display any sort of coherence in how it 
</em><br>
<em>&gt;&gt; treats different people, or doesn't create any sort of coherent 
</em><br>
<em>&gt;&gt; world, that would be a failure, right?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is only a licensable inference because many of our goals require 
</em><br>
<em>&gt; coherence; not because coherence is a goal in itself.  Survival 
</em><br>
<em>&gt; implies at least local continuity between past and future selves; 
</em><br>
<em>&gt; challenge, success, and fun implies at least local continuity between 
</em><br>
<em>&gt; past and future worlds.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think I would personally prefer that roughly the same thing happen 
</em><br>
<em>&gt; to the whole human species, so that we are not split to go one way and 
</em><br>
<em>&gt; another never to meet again.  But perhaps that will prove to be only a 
</em><br>
<em>&gt; personal preference on my part, or only a transient delusion of morality.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; Is this statement being put forth to help people distinguish the 
</em><br>
<em>&gt;&gt; difference between the CEV and the AI it creates?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;&gt; 5.  The CEV runs for five minutes before producing an output.  It is
</em><br>
<em>&gt;&gt;&gt; not meant to govern for centuries.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Though of course, there could be substantial mutual information 
</em><br>
<em>&gt;&gt; between the CEV and the AI it creates - correct?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Mutual information in the Shannon sense?  Absolutely!  Of course!
</em><br>
<em>&gt;
</em><br>
<em>&gt; On the other hand, I'd be really disturbed to see sections of code 
</em><br>
<em>&gt; copied verbatim.  I would regard this as prima facie evidence of 
</em><br>
<em>&gt; malfunction.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; Though such an AI (nor the CEV which created it) would not &quot;govern&quot; 
</em><br>
<em>&gt;&gt; in the anthropomorphic sense, it would surely exert optimization 
</em><br>
<em>&gt;&gt; pressure upon the world.  There are probably some people out there 
</em><br>
<em>&gt;&gt; who feel infinitely uncomfortable
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Wow, how does their brain pack in an infinite amount of uncomfort?  Up 
</em><br>
<em>&gt; until this point I'd been an infinite set atheist, on the grounds that 
</em><br>
<em>&gt; no reliable witness has ever reported encountering an infinite set.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; with the idea of a superintelligent AI with initial conditions set by 
</em><br>
<em>&gt;&gt; a human programming team creating changes in the world, and will 
</em><br>
<em>&gt;&gt; hence object to any such proposals, but of course it seems like this 
</em><br>
<em>&gt;&gt; event is basically unavoidable...  I think it's important to 
</em><br>
<em>&gt;&gt; distinguish between people who are objecting to *any* FAI theory on 
</em><br>
<em>&gt;&gt; the grounds that they haven't come to terms with the reality of 
</em><br>
<em>&gt;&gt; recursive self-improvement yet, and people who have already accepted 
</em><br>
<em>&gt;&gt; that superintelligent AI will eventually come into existence whether 
</em><br>
<em>&gt;&gt; we like it or not, and that it's merely our duty to set the initial 
</em><br>
<em>&gt;&gt; conditions as best we can.  It's sometimes difficult to tell the 
</em><br>
<em>&gt;&gt; difference between the two, people because it seems like people in 
</em><br>
<em>&gt;&gt; group #1 may occasionally pretend to be in group #2 for the sake of 
</em><br>
<em>&gt;&gt; argument (which ends up going nowhere).
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt; 6.  The CEV by itself does not mess around with your life.  The CEV
</em><br>
<em>&gt;&gt;&gt; just decides which AI to replace itself with.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; ...but the CEV isn't explicitly being programmed to create an AI 
</em><br>
<em>&gt;&gt; output - aye?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Pretty much, although I may have to make certain assumptions about the 
</em><br>
<em>&gt; class of thingydingies wo which the output belongs, in order to create 
</em><br>
<em>&gt; a clearly defined CEV computation producing the output.  For example, 
</em><br>
<em>&gt; one might require that the output be a computer program placed in 
</em><br>
<em>&gt; charge of the existing RPOP infrastructure.  That computer program 
</em><br>
<em>&gt; could be an AI; or it could clean up the infrastructure and delete 
</em><br>
<em>&gt; itself; or it could execute a predefined set of actions and then clean 
</em><br>
<em>&gt; up and delete itself.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; The AI output is based on the assumption that our wish if we knew 
</em><br>
<em>&gt;&gt; more, thought faster, were more the people we wished we were, had 
</em><br>
<em>&gt;&gt; grown up farther together; where the extrapolation converges rather 
</em><br>
<em>&gt;&gt; than diverges, where our wishes cohere rather than interfere; 
</em><br>
<em>&gt;&gt; extrapolated as we wish that extrapolated, interpreted as we wish 
</em><br>
<em>&gt;&gt; that interpreted, we would decide to construct an AI that exerts a 
</em><br>
<em>&gt;&gt; sort of optimizing pressure on the world such that it makes it a 
</em><br>
<em>&gt;&gt; better place to live?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; No, that's exactly the sort of assumption you don't want to build into 
</em><br>
<em>&gt; CEV. That's CEV as Nice Place To Live, which is a strong assumption 
</em><br>
<em>&gt; about the sort of world humanity would enjoy inhabiting; quite 
</em><br>
<em>&gt; distinct from CEV as Initial Dynamic.  For example, in the original 
</em><br>
<em>&gt; &quot;Collective Volition&quot; I suggested that we might coherently 
</em><br>
<em>&gt; extrapolatedly wish the CEV output to create a small set of 
</em><br>
<em>&gt; understandable background rules for our new world, and &quot;hands off!&quot; 
</em><br>
<em>&gt; for individual destinies.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; I would agree with this assumption - I just think it's worthwhile to 
</em><br>
<em>&gt;&gt; point out explicitly for the sake of clarity.  Theoretically, the 
</em><br>
<em>&gt;&gt; (extremely improbable) output of CEV could merely be a single object, 
</em><br>
<em>&gt;&gt; like a banana, or something along those lines, yes?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; If a banana belongs to the class of possible outputs, then you're 
</em><br>
<em>&gt; allowing the CEV to construct arbitrary physical devices as its 
</em><br>
<em>&gt; output, rather than writing arbitrary computer programs.  That 
</em><br>
<em>&gt; requires dynamic action and planning by CEV in the real world, in the 
</em><br>
<em>&gt; process of producing its first-order output, which therefore occurs 
</em><br>
<em>&gt; before the CEV's replacement by its first-order output.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Perhaps the (extremely improbable) output of CEV would be a program 
</em><br>
<em>&gt; that uses SI infrastructure to construct one banana, and then cleans 
</em><br>
<em>&gt; up the SI infrastructure and thereby deletes itself.
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11861.html">Jeff Medina: "H+/S'n ethics, New Hampshire"</a>
<li><strong>Previous message:</strong> <a href="11859.html">Eliezer S. Yudkowsky: "Re: 6 points about Coherent Extrapolated Volition"</a>
<li><strong>In reply to:</strong> <a href="11859.html">Eliezer S. Yudkowsky: "Re: 6 points about Coherent Extrapolated Volition"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11860">[ date ]</a>
<a href="index.html#11860">[ thread ]</a>
<a href="subject.html#11860">[ subject ]</a>
<a href="author.html#11860">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
