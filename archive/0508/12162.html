<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: drives ABC &gt; XYZ</title>
<meta name="Author" content="Michael Vassar (michaelvassar@hotmail.com)">
<meta name="Subject" content="Re: drives ABC &gt; XYZ">
<meta name="Date" content="2005-08-30">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: drives ABC &gt; XYZ</h1>
<!-- received="Tue Aug 30 14:43:51 2005" -->
<!-- isoreceived="20050830204351" -->
<!-- sent="Tue, 30 Aug 2005 16:43:49 -0400" -->
<!-- isosent="20050830204349" -->
<!-- name="Michael Vassar" -->
<!-- email="michaelvassar@hotmail.com" -->
<!-- subject="Re: drives ABC &gt; XYZ" -->
<!-- id="BAY101-F37C8B59716FBC9A9FA3E73ACAE0@phx.gbl" -->
<!-- inreplyto="drives ABC &gt; XYZ" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Vassar (<a href="mailto:michaelvassar@hotmail.com?Subject=Re:%20drives%20ABC%20&gt;%20XYZ"><em>michaelvassar@hotmail.com</em></a>)<br>
<strong>Date:</strong> Tue Aug 30 2005 - 14:43:49 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12163.html">Richard Loosemore: "Re: emotions"</a>
<li><strong>Previous message:</strong> <a href="12161.html">Phil Goetz: "Re: drives ABC &gt; XYZ"</a>
<li><strong>Maybe in reply to:</strong> <a href="12159.html">Michael Vassar: "drives ABC &gt; XYZ"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12164.html">Phil Goetz: "Re: drives ABC &gt; XYZ"</a>
<li><strong>Reply:</strong> <a href="12164.html">Phil Goetz: "Re: drives ABC &gt; XYZ"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12162">[ date ]</a>
<a href="index.html#12162">[ thread ]</a>
<a href="subject.html#12162">[ subject ]</a>
<a href="author.html#12162">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; &gt; Three discrete top-level goals might easily interact in such a
</em><br>
<em>&gt; &gt; manner as to alter or remove one-another, but they should not
</em><br>
<em>&gt; &gt; ever generate novel top-level goals, only sub-goals.
</em><br>
<em>&gt;
</em><br>
<em>&gt;That's a design decision.  We have already supposed that the
</em><br>
<em>&gt;organism has write access to its top-level goals, thus
</em><br>
<em>&gt;violating that design decision.
</em><br>
<p>I don't think so.  The top level goals *can* write new top level goals (you 
<br>
really couldn't prevent an AI from doing this by denying itself access to 
<br>
its top level goals.  If you tried it would just write a new AI with 
<br>
different goals and delete itself), but it will only do so if the expected 
<br>
utility of instituting a new goal as top level is greater than that of 
<br>
instituting it as a sub-goal *from the perspective of its current top level 
<br>
goal system*.  In a human, this might indeed be the case.  Our goals work 
<br>
better when they are independently supported.  Lower level goals suffer from 
<br>
attenuated affect, etc.  As a result, it frequently serves our top level 
<br>
goals to add new top level goals.  There is no reason to design an AI which 
<br>
suffers from our difficulties in the implementation of goal heirarchies 
<br>
however, so why would it implement new supergoals?  Seriously, it seems to 
<br>
me that you are saying that you can see a way in which its behavior could 
<br>
accidentally lead to low utility outcomes, yet if that is the case, why 
<br>
don't you expect it to see that same potential outcome and avoid it, at 
<br>
least once it is human equivalent.  At any given time, a FAI will be acting 
<br>
to maximize its utility function.  It is possible that in some cases, 
<br>
changing supergoals would maximize its current utility function, and in 
<br>
those cases it would do so, but those cases are specifically not something 
<br>
we want to avoid.  We want it to maximize its utility function, so long as 
<br>
the utility function in question is Friendly.
<br>
<p><em>&gt; &gt; In so far as the meme complex is a semantic
</em><br>
<em>&gt; &gt; web, it cannot even directly interface with the perceptions
</em><br>
<em>&gt; &gt; which it is referring to,
</em><br>
<em>&gt;
</em><br>
<em>&gt;This statement combines two large assumptions - that the meme
</em><br>
<em>&gt;complex &quot;is&quot; a semantic web, and that a semantic network can't
</em><br>
<em>&gt;&quot;directly&quot; interface with perceptions.  My belief, as expressed
</em><br>
<em>&gt;in my 2000 Cognitive Science paper &quot;A neuronal basis for the
</em><br>
<em>&gt;fan effect&quot;, is that the nodes in semantic networks can and
</em><br>
<em>&gt;probably should actually be recalled, content-addressable memories
</em><br>
<em>&gt;stored in networks.  One of the reasons for doing this is so
</em><br>
<em>&gt;that they can interface with perceptions.  (In fact, the inability
</em><br>
<em>&gt;of symbolic AI to interface with perception is one of the main
</em><br>
<em>&gt;motivating factors for adding a subsymbolic level.)
</em><br>
<p>I was aware that I was making large assumptions about how human minds 
<br>
actually work, at a very crude but useful approximation, hence &quot;in so far 
<br>
as&quot;.  My statement, like yours, can be taken to be plausible speculations, 
<br>
or if you find it implausible I will be happy to hear why in detail off 
<br>
list.  The argument is not dependent on the assumptions in question.
<br>
<p><em>&gt;The use of the term &quot;friendliness&quot; has fooled many of us into
</em><br>
<em>&gt;thinking that we know what we're talking about, when others on
</em><br>
<em>&gt;the list apparently have a technical definition in mind.
</em><br>
<p>I am afraid that this is the case.  At any rate, the document CAFAI is not 
<br>
very long.
<br>
<p>It would probably be helpful to everyone if we consistantly used the phrase 
<br>
Powerful Optimization Process Encompassing Human Moral Complexity POPEHMC or 
<br>
some more euphonious acronym, rather than FAI.  It's probably too late to 
<br>
conveniently change names to
<br>
&quot;The Singularity Institute For Optimal Processes&quot;, &quot;The Singularity 
<br>
Institute for Compatible Preferences&quot;, or anything of the sort, but it might 
<br>
not be.  The phrase AI has LOTS of emotional baggage and is associated with 
<br>
all sorts of inappropriate associations.  The phrase for Artifical 
<br>
Intelligence is also somewhat misleading, as we are only *for* certain types 
<br>
of Artificial Intelligence.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12163.html">Richard Loosemore: "Re: emotions"</a>
<li><strong>Previous message:</strong> <a href="12161.html">Phil Goetz: "Re: drives ABC &gt; XYZ"</a>
<li><strong>Maybe in reply to:</strong> <a href="12159.html">Michael Vassar: "drives ABC &gt; XYZ"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12164.html">Phil Goetz: "Re: drives ABC &gt; XYZ"</a>
<li><strong>Reply:</strong> <a href="12164.html">Phil Goetz: "Re: drives ABC &gt; XYZ"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12162">[ date ]</a>
<a href="index.html#12162">[ thread ]</a>
<a href="subject.html#12162">[ subject ]</a>
<a href="author.html#12162">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
