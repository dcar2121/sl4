<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: On the dangers of AI</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Re: On the dangers of AI">
<meta name="Date" content="2005-08-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: On the dangers of AI</h1>
<!-- received="Tue Aug 16 20:55:00 2005" -->
<!-- isoreceived="20050817025500" -->
<!-- sent="Tue, 16 Aug 2005 22:54:43 -0400" -->
<!-- isosent="20050817025443" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Re: On the dangers of AI" -->
<!-- id="4302A6F3.5070202@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="1124236367.28520.17.camel@localhost.localdomain" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20On%20the%20dangers%20of%20AI"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Tue Aug 16 2005 - 20:54:43 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11929.html">Phillip Huggan: "Re: On the dangers of AI"</a>
<li><strong>Previous message:</strong> <a href="11927.html">justin corwin: "Re: On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11926.html">Peter de Blanc: "Re: On the dangers of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11929.html">Phillip Huggan: "Re: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11929.html">Phillip Huggan: "Re: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11930.html">Michael Wilson: "Re: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11931.html">Brian Atkins: "Re: On the dangers of AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11928">[ date ]</a>
<a href="index.html#11928">[ thread ]</a>
<a href="subject.html#11928">[ subject ]</a>
<a href="author.html#11928">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Peter,
<br>
<p>This is very tricky territory, I believe, so I am going to try to go 
<br>
through what you say very carefully....
<br>
<p>Peter de Blanc wrote:
<br>
<em>&gt; On Tue, 2005-08-16 at 16:57 -0400, Richard Loosemore wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;Here is the strange thing:  I would suggest that in every case we know 
</em><br>
<em>&gt;&gt;of, where a human being is the victim of a brain disorder that makes the 
</em><br>
<em>&gt;&gt;person undergo spasms of violence or aggression, but with peaceful 
</em><br>
<em>&gt;&gt;episodes in between, and where that human being is smart enough to 
</em><br>
<em>&gt;&gt;understand its own mind to a modest degree, they wish for a chance to 
</em><br>
<em>&gt;&gt;switch off the violence and become peaceful all the time.  Given the 
</em><br>
<em>&gt;&gt;choice, a violent creature that had enough episodes of passivity to be 
</em><br>
<em>&gt;&gt;able to understand its own mind structure would simply choose to turn 
</em><br>
<em>&gt;&gt;off the violence
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There's an important distinction which you're missing, between a mind's
</em><br>
<em>&gt; behaviors and (its beliefs about) its goal content. As human beings, we
</em><br>
<em>&gt; have evolved to believe that we are altruists, and when our evolved
</em><br>
<em>&gt; instincts and behaviors contradict this, we can sometimes alter these
</em><br>
<em>&gt; behaviors.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In other words, it is a reproductive advantage to have selfish
</em><br>
<em>&gt; behaviors, so you have them, but it is also a reproductive advantage to
</em><br>
<em>&gt; think of yourself as an altruist, so you do. Fortunately, your
</em><br>
<em>&gt; generally-intelligent mind is more powerful than these dumb instincts,
</em><br>
<em>&gt; so you have the ability to overcome them, and become a genuinely good
</em><br>
<em>&gt; person. But you can only do this because you _started out_ wanting to be
</em><br>
<em>&gt; a good person!
</em><br>
<p>I want to argue first that I am not missing the distinction between a 
<br>
mind's behaviors and (its beliefs about) its goal content.
<br>
<p>First, note that human beings are pretty dodgy when it comes to their 
<br>
*beliefs* about their own motivations:  self-knowledge of motivations in 
<br>
individual humans ranges from close to zero (my 97 year old grandmother 
<br>
with dementia) through adeptly contortionist (my delightful but 
<br>
sometimes exasperating 6-year old son) to grossly distorted (Hitler, who 
<br>
probably thought of himself as doing wonderful things for the world) and 
<br>
&nbsp;&nbsp;&nbsp;on through sublimely subtle (T.E. Lawrence?  Bertrand Russell?). 
<br>
Truth is, we have evolved to play all kinds of tricks on ourselves, and 
<br>
to have many levels of depth of understanding, depending on who we are 
<br>
and how hard we try.
<br>
<p>Most of us are very imperfect at it, but (and this is an important 
<br>
point) the more we try to study motivation objectively, and internally 
<br>
observe what happens inside ourselves, the better, I claim, we become.
<br>
<p>So, yes, part of the story is that we have evolved to think of ourselves 
<br>
as altruists - or rather, as altruists with respect to our kinsfolk and 
<br>
relatives, but often not global altruists.  And when our instincts 
<br>
contradict our perceptions of who we think we *should* be, we can 
<br>
sometimes modify the instincts.  The full picture involves quite a 
<br>
tangled web of interacting forces, but yes, this central conflict is 
<br>
part of the story, as you point out.
<br>
<p>So far, so good.  To be old-fashioned about it, Superego clobbers Id 
<br>
when it gets out of control, and we end up becoming &quot;a genuinely good 
<br>
person.&quot;
<br>
<p>But now, if I read you aright, what you are saying is that the reason 
<br>
Superego gets the upper hand in the end is that the system was designed 
<br>
with fundamental altruism as goal number one (&quot;you can only do this 
<br>
because you _started out_ wanting to be a good person!&quot;) and because 
<br>
this goal was designed in from the beginning, this is the reason why it 
<br>
eventually (at least in the case of nice people like you and I) 
<br>
triumphed over the baser instincts.
<br>
<p>Hence, it depends on what was goal number one in the initial design of 
<br>
the system (altruism, rather than ruthless dominance or paperclipization 
<br>
of the known universe).  Whatever was in there first, wins?
<br>
<p>I have two serious disputes with this.
<br>
<p>1) Are you sure?  I mean are you sure that the reason why this 
<br>
complicated (in fact probably Complex) motivation system, which is more 
<br>
than just an opponent-process module involving Supergo and Id, but is, 
<br>
as I argued above, a tangled mess of forces, is ending up the way it 
<br>
does by the time it matures, *only* because the altruism was goal number 
<br>
one in the initial design?  I am really not so sure, myself, and either 
<br>
way, this is something that we should be answering empirically - I don't 
<br>
think you and I could decide the reasons for its eventually settling on 
<br>
good behavior without some serious psychological studies and 
<br>
(preferably) some simulations of different kinds of motivation systems.
<br>
<p>2) Quite apart from that last question, though, I believe that you have 
<br>
introduced something of a red herring, because all of the above 
<br>
discussion is about ordinary people and their motivational systems, and 
<br>
about their introspective awareness of those systems, and the 
<br>
interaction betwixt motivation and introspection.
<br>
<p>In my original essay, though, I was talking not about ordinary humans, 
<br>
but about creatures who, ex hypothesi, have quite a deep understanding 
<br>
of motivation systems in minds ... and, on top of that understanding 
<br>
they have the ability to flip switches that can turn parts of their own 
<br>
motivation systems on or off.  My point is that we rarely talk about the 
<br>
the kind of human that has a profoundly deep and subtle understanding of 
<br>
how their own motivation systems is structured (there just aren't that 
<br>
many of them), but this is the population of most interest in the essay. 
<br>
&nbsp;&nbsp;So when you correctly point out that all sorts of strange forces come 
<br>
together to determine the overall niceness of a typical human, you are 
<br>
tempting me off topic!
<br>
<p>Having said all this, I can now meet your last point:
<br>
<p><em>&gt; You are anthropomorphizing by assuming that these beliefs about goal
</em><br>
<em>&gt; content are held by minds-in-general, and the only variation is in the
</em><br>
<em>&gt; instinctual behaviors built in to different minds. A Seed AI which
</em><br>
<em>&gt; believes its goal to be paper clip maximization will not find
</em><br>
<em>&gt; Friendliness seductive! It will think about Friendliness and say &quot;Uh oh!
</em><br>
<em>&gt; Being Friendly would prevent me from turning the universe into paper
</em><br>
<em>&gt; clips! I'd better not be Friendly.&quot;
</em><br>
<p>Wait!  Anthropomorphizing is when we incorrectly assume that a thing is 
<br>
like a human being.
<br>
<p>What you are saying in this paragraph is that (1) my original argument 
<br>
was that niceness tends to triumph in humans, (2) I misunderstood the 
<br>
fact that this actually occurs because of our particular beliefs about 
<br>
our goal content (the altruism stuff, above), and (3) continuing this 
<br>
misunderstanding, I falsely generalized and assumed that all minds would 
<br>
have the same beliefs about their goal content (?... I am a little 
<br>
unclear about your argument here...).
<br>
<p>No, not at all!  I am saying that a sufficiently smart mind would 
<br>
transcend the mere beliefs-about-goals stuff and realise that it is a 
<br>
system comprising two things:  a motivational system whose structure 
<br>
determines what gives it pleasure, and an intelligence system.
<br>
<p>So I think that what you yourself have done is to hit up against the 
<br>
anthropomorphization problem, thus:
<br>
<p><em> &gt; A Seed AI which
</em><br>
<em> &gt; believes its goal to be paper clip maximization will
</em><br>
<p>wait!  why would it be so impoverished in its understanding of 
<br>
motivation systems, that it just &quot;believes its goal to do [x]&quot; and 
<br>
confuses this with the last word on what pushes its buttons?  Would it 
<br>
not have a much deeper understanding, and say &quot;I feel this urge to 
<br>
paperclipize, but I know it's just a quirk of my motivation system, so, 
<br>
let's see, is this sensible?  Do I have any other choices here?&quot;
<br>
<p>If you assume that it only has the not-very-introspective human-level 
<br>
understanding of its motivation, then this is anthropomorphism, surely? 
<br>
&nbsp;&nbsp;(It's a bit of a turnabout, for sure, since anthropomorphism usually 
<br>
means accidentally assuming too much intelligence in an inanimate 
<br>
object, whereas here we got caught assuming too little in a 
<br>
superintelligence!)
<br>
<p>To illustrate:  I don't &quot;believe my goal is to have wild sex.&quot;  I just 
<br>
jolly well *like* doing it!  Moreover, I'm sophisticated enough to know 
<br>
that I have a quirky little motivation system down there in my brain, 
<br>
and it is modifiable (though not by me, not yet).
<br>
<p>Bottom Line:
<br>
<p>It is all about there being a threshold level of understanding of 
<br>
motivation systems, coupled with the ability to flip switches in ones 
<br>
own system, above which the mind will behave very, very differently than 
<br>
your standard model human.
<br>
<p><p><p><p>Hope I didn't beat you about the head too much with this reply!  These 
<br>
arguments are damn difficult to squeeze into email-sized chunks.  Entire 
<br>
chapters, or entire books, would be better.
<br>
<p>Richard Loosemore.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11929.html">Phillip Huggan: "Re: On the dangers of AI"</a>
<li><strong>Previous message:</strong> <a href="11927.html">justin corwin: "Re: On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11926.html">Peter de Blanc: "Re: On the dangers of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11929.html">Phillip Huggan: "Re: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11929.html">Phillip Huggan: "Re: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11930.html">Michael Wilson: "Re: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11931.html">Brian Atkins: "Re: On the dangers of AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11928">[ date ]</a>
<a href="index.html#11928">[ thread ]</a>
<a href="subject.html#11928">[ subject ]</a>
<a href="author.html#11928">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
