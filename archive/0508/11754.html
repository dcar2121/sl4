<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: On the dangers of AI</title>
<meta name="Author" content="Phil Goetz (philgoetz@yahoo.com)">
<meta name="Subject" content="Re: On the dangers of AI">
<meta name="Date" content="2005-08-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: On the dangers of AI</h1>
<!-- received="Wed Aug 17 09:26:25 2005" -->
<!-- isoreceived="20050817152625" -->
<!-- sent="Wed, 17 Aug 2005 08:26:22 -0700 (PDT)" -->
<!-- isosent="20050817152622" -->
<!-- name="Phil Goetz" -->
<!-- email="philgoetz@yahoo.com" -->
<!-- subject="Re: On the dangers of AI" -->
<!-- id="20050817152622.24289.qmail@web54509.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="4302D206.8000908@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Phil Goetz (<a href="mailto:philgoetz@yahoo.com?Subject=Re:%20On%20the%20dangers%20of%20AI"><em>philgoetz@yahoo.com</em></a>)<br>
<strong>Date:</strong> Wed Aug 17 2005 - 09:26:22 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11755.html">Eliezer S. Yudkowsky: "Re: Shock Level 5 (SL5) - 'The Theory Of Everything'"</a>
<li><strong>Previous message:</strong> <a href="11753.html">Christopher Healey: "Re: On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11742.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11738.html">Brian Atkins: "Re: On the dangers of AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11754">[ date ]</a>
<a href="index.html#11754">[ thread ]</a>
<a href="subject.html#11754">[ subject ]</a>
<a href="author.html#11754">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
--- Richard Loosemore &lt;<a href="mailto:rpwl@lightlink.com?Subject=Re:%20On%20the%20dangers%20of%20AI">rpwl@lightlink.com</a>&gt; wrote:
<br>
<em>&gt; If a paperclip maximiser is not aware of such things as
</em><br>
<em>&gt; goals and 
</em><br>
<em>&gt; motivations, it is not smart enough to be relevant, for
</em><br>
<em>&gt; the following 
</em><br>
<em>&gt; sequence of reasons:
</em><br>
<p>Richard - You're using the term &quot;goal&quot; in a sort of
<br>
common-sense way.  Think of a goal rather as a goal that
<br>
is assigned to a rule-based system.  The RBS seeks to
<br>
accomplish that goal.  It does not reflect on whether
<br>
that goal is a worthy one.
<br>
<p>The human equivalent of such a &quot;goal&quot; is not any of the
<br>
everyday &quot;goals&quot; we have such as sex, money, and power.
<br>
Jeff Hawkins' recent book /On Intelligence/, for instance,
<br>
would say that the &quot;goal&quot; is to maximize the accuracy
<br>
of our predictions.  The top-level goal is unconscious,
<br>
not stored in declarative form, and not accessible to
<br>
reflection.  It's more akin to a drive, like enjoying
<br>
good food.  You don't sit around and wonder whether
<br>
the act of eating an ice-cream sundae is actually one
<br>
noble enough to merit the good feelings that you assign
<br>
to that act.  You can't control that.
<br>
<p>It is interesting that you can condition your response
<br>
to ice-cream sundaes, by, for instance, giving yourself
<br>
a shock every time you eat one.  I heard about a guy
<br>
who connected a smoke-detector to an electrode to give
<br>
himself a shock every time he smoked.  He ended up
<br>
addicted to both cigarettes and electrical shocks.
<br>
So it may be that our hypothetical AI can in some way
<br>
re-condition its motivations.
<br>
<p><em>&gt; c) A successful Seed AI will bootstrap and then eliminate
</em><br>
<em>&gt; rival projects 
</em><br>
<em>&gt; quickly (except for case (d) below).  After that, it will
</em><br>
<em>&gt; not allow 
</em><br>
<em>&gt; experiments such as the construction of superintelligent
</em><br>
<em>&gt; paperclip 
</em><br>
<em>&gt; maximisers.
</em><br>
<p>That's the problem, not the answer.
<br>
<p><em>&gt; So, again:  I am not anthropomorphising (accidentally
</em><br>
<em>&gt; attributing 
</em><br>
<em>&gt; human-like qualities where they don't belong), but making
</em><br>
<em>&gt; the specific 
</em><br>
<em>&gt; statement that a seed AI worth worrying about would be
</em><br>
<em>&gt; impossibly 
</em><br>
<em>&gt; crippled if it did not have awareness of such design
</em><br>
<em>&gt; issues.
</em><br>
<p>The problem is not that the seed AI is not aware of such
<br>
design issues.  The problem is that the basic motivations
<br>
a creature has are not logical, are not a priori, and
<br>
maximizing paper clips is ultimately as reasonable as,
<br>
say, maximizing the number of sexual partners one has,
<br>
or maximizing the number of ice-cream sundaes one eats,
<br>
or some combination thereof.  Many philosophers have
<br>
agreed that humans pursue pleasure and avoid pain and do
<br>
nothing else, and deriving pleasure from skin-to-skin
<br>
contact or from ingesting ice-cream sundaes is no more
<br>
rational than deriving pleasure from the production of
<br>
another paperclip.
<br>
<p>A pure intellect with no irrational motivations would
<br>
do nothing at all.  Even self-preservation is irrational.
<br>
<p>- Phil Goetz
<br>
<p>__________________________________________________
<br>
Do You Yahoo!?
<br>
Tired of spam?  Yahoo! Mail has the best spam protection around 
<br>
<a href="http://mail.yahoo.com">http://mail.yahoo.com</a> 
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11755.html">Eliezer S. Yudkowsky: "Re: Shock Level 5 (SL5) - 'The Theory Of Everything'"</a>
<li><strong>Previous message:</strong> <a href="11753.html">Christopher Healey: "Re: On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11742.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11738.html">Brian Atkins: "Re: On the dangers of AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11754">[ date ]</a>
<a href="index.html#11754">[ thread ]</a>
<a href="subject.html#11754">[ subject ]</a>
<a href="author.html#11754">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:01 MST
</em></small></p>
</body>
</html>
