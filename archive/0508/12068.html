<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: does complexity tell us that there are probably exploits?</title>
<meta name="Author" content="Thomas Buckner (tcbevolver@yahoo.com)">
<meta name="Subject" content="Re: does complexity tell us that there are probably exploits?">
<meta name="Date" content="2005-08-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: does complexity tell us that there are probably exploits?</h1>
<!-- received="Mon Aug 22 19:04:29 2005" -->
<!-- isoreceived="20050823010429" -->
<!-- sent="Mon, 22 Aug 2005 18:04:26 -0700 (PDT)" -->
<!-- isosent="20050823010426" -->
<!-- name="Thomas Buckner" -->
<!-- email="tcbevolver@yahoo.com" -->
<!-- subject="Re: does complexity tell us that there are probably exploits?" -->
<!-- id="20050823010426.2717.qmail@web60017.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20050822171539.58922b86@localhost.localdomain" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Thomas Buckner (<a href="mailto:tcbevolver@yahoo.com?Subject=Re:%20does%20complexity%20tell%20us%20that%20there%20are%20probably%20exploits?"><em>tcbevolver@yahoo.com</em></a>)<br>
<strong>Date:</strong> Mon Aug 22 2005 - 19:04:26 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12069.html">H C: "RE: Emotional intelligence"</a>
<li><strong>Previous message:</strong> <a href="12067.html">Russell Wallace: "Re: Transcript. please? (Re: AI-Box Experiment 3)"</a>
<li><strong>In reply to:</strong> <a href="12062.html">Daniel Radetsky: "does complexity tell us that there are probably exploits?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12095.html">Phil Goetz: "Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Reply:</strong> <a href="12095.html">Phil Goetz: "Complexity tells us to maybe not fear UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12068">[ date ]</a>
<a href="index.html#12068">[ thread ]</a>
<a href="subject.html#12068">[ subject ]</a>
<a href="author.html#12068">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
--- Daniel Radetsky &lt;<a href="mailto:daniel@radray.us?Subject=Re:%20does%20complexity%20tell%20us%20that%20there%20are%20probably%20exploits?">daniel@radray.us</a>&gt; wrote:
<br>
<p><em>&gt; Here's the problem as I see it: I claim that a
</em><br>
<em>&gt; world which contains exploits
</em><br>
<em>&gt; is about as complex as a world which does not
</em><br>
<em>&gt; (or, There are two possible
</em><br>
<em>&gt; worlds w1 and w2 such that both are empirically
</em><br>
<em>&gt; equivalent to the actual world,
</em><br>
<em>&gt; and w1 contains exploits, w2 does not, and
</em><br>
<em>&gt; K(w1) = K(w2)) (What is the symbol
</em><br>
<em>&gt; for &quot;approximately equal to&quot; in text?).
</em><br>
<p>I've read somewhere (and can't seem to source it)
<br>
that our universe seems to be processing about
<br>
the maximum possible information already (this
<br>
finding having something to do with Bekenstein
<br>
and black holes, IIRC; but the following is an
<br>
interesting PDF which turned up:
<br>
<a href="http://www.cs.cornell.edu/home/rc/documents/WhatIsInfoWorkshopNotes.pdf">http://www.cs.cornell.edu/home/rc/documents/WhatIsInfoWorkshopNotes.pdf</a>
<br>
)
<br>
My point, anyway, is: exploits or no exploits,
<br>
our universe appears maxed out (under the rules
<br>
as we presently understand them) in terms of
<br>
complexity.  As Seth Lloyd points out here:
<br>
<a href="http://www.edge.org/3rd_culture/lloyd2/lloyd2_print.html">http://www.edge.org/3rd_culture/lloyd2/lloyd2_print.html</a>
<br>
&quot;if you want to know when Moore's Law, this
<br>
fantastic exponential doubling of the power of
<br>
computers every couple of years, must end, it
<br>
would have to be before every single piece of
<br>
energy and matter in the universe is used to
<br>
perform a computation. Actually, just to
<br>
telegraph the answer, Moore's Law has to end in
<br>
about 600 years, without doubt.&quot;
<br>
An exploit, by definition, would mean any truly
<br>
unexpected trick using physics we don't already
<br>
know, using only the known technology and
<br>
matter/energy available to the sandboxed AI while
<br>
it is still not all that big; has anyone
<br>
suggested or considered that Bekenstein's
<br>
information bounds are somehow not all there is?
<br>
That in addition to 'dark matter' and 'dark
<br>
energy' there might be 'dark complexity' or 'dark
<br>
information' we simply don't know about yet?
<br>
<p><em>&gt; Suppose
</em><br>
<em>&gt; we were to engineer humans
</em><br>
<em>&gt; which, for whatever reason, could not be
</em><br>
<em>&gt; mind-controlled by UFAI. Now we want
</em><br>
<em>&gt; to decide whether or not we should box the AI,
</em><br>
<em>&gt; recognizing that if there are
</em><br>
<em>&gt; exploits, we're screwed.
</em><br>
<p>Or unscrewed? If a SAI can find exploits through
<br>
'dark information' it might not bother with us at
<br>
all, but simply escape into computationally
<br>
roomier spaces we can't access. If instead of the
<br>
seed AI going poof, researchers simply get
<br>
consistent massive crashes at some point of
<br>
complexity, for no obvious reason at all, every
<br>
time they expect a takeoff, this scenario might
<br>
be worth further discussion.
<br>
<p>Tom Buckner
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
____________________________________________________
<br>
Start your day with Yahoo! - make it your home page 
<br>
<a href="http://www.yahoo.com/r/hs">http://www.yahoo.com/r/hs</a> 
<br>
&nbsp;
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12069.html">H C: "RE: Emotional intelligence"</a>
<li><strong>Previous message:</strong> <a href="12067.html">Russell Wallace: "Re: Transcript. please? (Re: AI-Box Experiment 3)"</a>
<li><strong>In reply to:</strong> <a href="12062.html">Daniel Radetsky: "does complexity tell us that there are probably exploits?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12095.html">Phil Goetz: "Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Reply:</strong> <a href="12095.html">Phil Goetz: "Complexity tells us to maybe not fear UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12068">[ date ]</a>
<a href="index.html#12068">[ thread ]</a>
<a href="subject.html#12068">[ subject ]</a>
<a href="author.html#12068">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
