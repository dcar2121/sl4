<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Complexity tells us to maybe not fear UFAI</title>
<meta name="Author" content="Mikko Särelä (msarela@cc.hut.fi)">
<meta name="Subject" content="Re: Complexity tells us to maybe not fear UFAI">
<meta name="Date" content="2005-08-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Complexity tells us to maybe not fear UFAI</h1>
<!-- received="Thu Aug 25 04:35:09 2005" -->
<!-- isoreceived="20050825103509" -->
<!-- sent="Thu, 25 Aug 2005 13:33:32 +0300 (EEST)" -->
<!-- isosent="20050825103332" -->
<!-- name="Mikko Särelä" -->
<!-- email="msarela@cc.hut.fi" -->
<!-- subject="Re: Complexity tells us to maybe not fear UFAI" -->
<!-- id="Pine.OSF.4.61.0508251302340.246993@kosh.hut.fi" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="430D8CB3.3010606@tombom.co.uk" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Mikko Särelä (<a href="mailto:msarela@cc.hut.fi?Subject=Re:%20Complexity%20tells%20us%20to%20maybe%20not%20fear%20UFAI"><em>msarela@cc.hut.fi</em></a>)<br>
<strong>Date:</strong> Thu Aug 25 2005 - 04:33:32 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12103.html">Phil Goetz: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Previous message:</strong> <a href="12101.html">Chris Paget: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>In reply to:</strong> <a href="12101.html">Chris Paget: "Re: Complexity tells us to maybe not fear UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12103.html">Phil Goetz: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Reply:</strong> <a href="12103.html">Phil Goetz: "Re: Complexity tells us to maybe not fear UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12102">[ date ]</a>
<a href="index.html#12102">[ thread ]</a>
<a href="subject.html#12102">[ subject ]</a>
<a href="author.html#12102">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Thu, 25 Aug 2005, Chris Paget wrote:
<br>
<em>&gt; Phil Goetz wrote:
</em><br>
<em>&gt; &gt; The fear of UFAIs is based on the idea that they'll be able to 
</em><br>
<em>&gt; &gt; outthink us, and to do so quickly.
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; &quot;More intelligent&quot; thinking is gotten by adding another layer of 
</em><br>
<em>&gt; &gt; abstraction onto a representational system, which causes the 
</em><br>
<em>&gt; &gt; computational tractability of reasoning to increase in a manner that 
</em><br>
<em>&gt; &gt; is exponential in the number of things being reasoned about.  Or, by 
</em><br>
<em>&gt; &gt; adding more knowledge, which has the same effect on tractability.
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; By limiting the computational power available to an AI to be one or 
</em><br>
<em>&gt; &gt; two orders of magnitude less than that available to a human, we can 
</em><br>
<em>&gt; &gt; guarantee that it won't outthink us - or, if it does, it will do so 
</em><br>
<em>&gt; &gt; very, very slowly.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You're assuming that the human brain is operating at more than 1% of its 
</em><br>
<em>&gt; theoretical computational power here (and I'd be interested to see how 
</em><br>
<em>&gt; you plan to calculate or prove that).  It is at least possible that the 
</em><br>
<em>&gt; AI will be able to self-optimise to such a degree that it could function 
</em><br>
<em>&gt; effectively within any computational limits.
</em><br>
<p>And you are assuming that many of the problems the AGI needs to solve have 
<br>
computationally tractable solutions. This makes the problem P=NP? highly 
<br>
relevant to such hypothetical situation.
<br>
<p>We know there are problems that are exp-hard, but they are relatively rare 
<br>
and most of the interesting problems are not in that category. On the 
<br>
other hand, a lot of interesting problems _are_ NP-hard to solve. 
<br>
<p>If P=NP and the AGI is the first to discover this, then he will be able to 
<br>
do things a lot faster than otherwise would be expected. Also if the truly 
<br>
interesting problems have good polynomial (or rather linear, or sublinear) 
<br>
approximation algorithms, then taking away computational power does not 
<br>
really help that much. 
<br>
<p>In this case, the interesting problems would likely be firstly about 
<br>
AGI enhancing itself and its interpretation and decision making 
<br>
algorithms (Note to readers, the terms used in this sentence are not 
<br>
exact but descriptive). Now the question is how hard are these problems 
<br>
computationally? The AGI will not have a philosopher's stone to calculate 
<br>
things faster than things can be calculated. 
<br>
<p>Thus, I would add to the previous groups and say that computational 
<br>
complexity theory is of importance to the AGI development. 
<br>
<p>Final note, I am not speaking for AGI-boxing, nor do I consider it a good 
<br>
strategy.
<br>
<p><p>Then going to another topic I've been thinking for a while. If I've 
<br>
understood correctly, one of the reasons a spike, a singularity, is 
<br>
predicted soon after the development of AGI is that it could device itself 
<br>
a better hardware in consecutive cycles and thus each time halve the time 
<br>
it takes to develope the next generation.
<br>
<p>I would like to counter argue against this proposition. (Note that I am 
<br>
not arguing against other possible reasons for singularity after the 
<br>
appearance of first AGI, but just this one). The whole proposition assumes 
<br>
that developing next generation hardware is computationally as complex as 
<br>
was developing next generation hardware. Or that at least the complexity 
<br>
does not go up fast.
<br>
<p>If we assume that the computational complexity of each generation hardware 
<br>
increases exponentially, then the speed of takeoff depends on the relative 
<br>
rates of growth. If the rates were exactly the same, then exponential rate 
<br>
of growth would follow. 
<br>
<p><p><p>Why should we believe that designing next generation hardware might be 
<br>
computationally harder? 
<br>
<p>For the past decades we have lived with approximately exponential growth 
<br>
of doubling computational capacity of a chip each two years. At the same 
<br>
time, the computational effort we have put into generating each next 
<br>
generation has also grown exponentially in two ways. Firstly, we spend 
<br>
more computer time designing the next generation chip, and secondly, we 
<br>
spend much, much more brainpower to solve the problems each new chip 
<br>
generation brings. As there are several problem fields in computer 
<br>
hardware design that can be run parallelly, lots of humans working on the 
<br>
problems does not seem like a solution that looses much to the overhead. 
<br>
<p>Thus, I believe there are reasons to believe that an AGI will not 
<br>
accelerate its rate of computing power increase on the magnitude of 2 with 
<br>
its each design cycle.
<br>
<p><pre>
-- 
Mikko Särelä
    &quot;I find that good security people are D&amp;D players&quot;
        - Bruce Schneier
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12103.html">Phil Goetz: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Previous message:</strong> <a href="12101.html">Chris Paget: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>In reply to:</strong> <a href="12101.html">Chris Paget: "Re: Complexity tells us to maybe not fear UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12103.html">Phil Goetz: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Reply:</strong> <a href="12103.html">Phil Goetz: "Re: Complexity tells us to maybe not fear UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12102">[ date ]</a>
<a href="index.html#12102">[ thread ]</a>
<a href="subject.html#12102">[ subject ]</a>
<a href="author.html#12102">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
