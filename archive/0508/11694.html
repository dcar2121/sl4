<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: On Our Duty to Not Be Responsible for Artificial Minds</title>
<meta name="Author" content="Samantha Atkins (sjatkins@gmail.com)">
<meta name="Subject" content="Re: On Our Duty to Not Be Responsible for Artificial Minds">
<meta name="Date" content="2005-08-11">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: On Our Duty to Not Be Responsible for Artificial Minds</h1>
<!-- received="Thu Aug 11 22:35:23 2005" -->
<!-- isoreceived="20050812043523" -->
<!-- sent="Thu, 11 Aug 2005 21:35:20 -0700" -->
<!-- isosent="20050812043520" -->
<!-- name="Samantha Atkins" -->
<!-- email="sjatkins@gmail.com" -->
<!-- subject="Re: On Our Duty to Not Be Responsible for Artificial Minds" -->
<!-- id="948b11e05081121355c86e4f5@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="42F8E35A.9060705@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha Atkins (<a href="mailto:sjatkins@gmail.com?Subject=Re:%20On%20Our%20Duty%20to%20Not%20Be%20Responsible%20for%20Artificial%20Minds"><em>sjatkins@gmail.com</em></a>)<br>
<strong>Date:</strong> Thu Aug 11 2005 - 22:35:20 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11695.html">Marc Geddes: "RE: 'Collective Volition' ripped to pieces"</a>
<li><strong>Previous message:</strong> <a href="11693.html">Brian Atkins: "Re: META:  Not SL4 material (was Re: Hiroshima Day)"</a>
<li><strong>In reply to:</strong> <a href="11673.html">Eliezer S. Yudkowsky: "Re: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11694">[ date ]</a>
<a href="index.html#11694">[ thread ]</a>
<a href="subject.html#11694">[ subject ]</a>
<a href="author.html#11694">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 8/9/05, Eliezer S. Yudkowsky &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20On%20Our%20Duty%20to%20Not%20Be%20Responsible%20for%20Artificial%20Minds">sentience@pobox.com</a>&gt; wrote:
<br>
<em>&gt; Considering the relation between my parents and myself, &quot;autonomy&quot; consists of
</em><br>
<em>&gt; my parents being able to control a small set of variables in my upbringing and
</em><br>
<em>&gt; unable to control a much larger set of variables in my cognitive design.  Not
</em><br>
<em>&gt; because my parents *chose* to control those variables and no other, but
</em><br>
<em>&gt; because my parents were physically and cognitively *unable* to select my
</em><br>
<em>&gt; genome on the basis of its consequences.  Furthermore, my cognitive design -
</em><br>
<em>&gt; fixed beyond parental control - determined how I reacted to parental
</em><br>
<em>&gt; upbringing.  My fixed cognitive design placed some variables within my
</em><br>
<em>&gt; parents' deliberate control, in the sense that they could, by speaking
</em><br>
<em>&gt; English, ensure I would grow up speaking English.  However, some variables
</em><br>
<em>&gt; that my parents greatly desired to control, such as my religion, were beyond
</em><br>
<em>&gt; the reach of their best efforts at upbringing.  It is not that they chose not
</em><br>
<em>&gt; to control this variable but that they were incapable of controlling it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In the case of an AI researcher we have many, many possibilities.  Here are
</em><br>
<em>&gt; some possibilities that occur to me:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1)  The AI researcher is fully capable of choosing between AI designs on the
</em><br>
<em>&gt; basis of their consequences, and chooses an AI design which invokes no
</em><br>
<em>&gt; significant moral processing within the AI.  In this case I would assign moral
</em><br>
<em>&gt; responsibility to the AI researcher alone, for all consequences good or ill;
</em><br>
<em>&gt; the AI itself is not a moral agent.
</em><br>
<em>&gt;
</em><br>
<p>As the AI designers are of considerably limited intelligence
<br>
concerning designing an intelligence as powerfull and especially more
<br>
powerful than their own it seems pointless to speculate that they
<br>
somehow should have known all the consequences of choosing those
<br>
design elements they are capable of choosing within the limited
<br>
abilities at their command.
<br>
<p>If the AI is as capable of making free (relatively) choices as we are
<br>
then it is just as much a moral agent as we are.
<br>
&nbsp;
<br>
<em>&gt; I assign full responsibility to the AI researcher for all consequences,
</em><br>
<em>&gt; intended or unintended.  
</em><br>
<p>This seems arbitraty and capricious.  How can one be resposible for
<br>
unintended consequences utterly beyond one's ability to predict?
<br>
<p><em>&gt; An AI researcher has a responsibility to choose an AI
</em><br>
<em>&gt; design with predictable consequences.  If the AI researcher negligently uses
</em><br>
<em>&gt; an AI design the AI researcher can't predict, the AI researcher is still fully
</em><br>
<em>&gt; responsible for all actual consequences.
</em><br>
<em>&gt; 
</em><br>
<p>Such a designed artifact almost of necessity cannot be as intelligent
<br>
as the designer.
<br>
<p><em>&gt; 2)  A competent AI researcher, acting on a perceived moral responsibility to
</em><br>
<em>&gt; create moral children, deliberately constructs an AI which is tightly or
</em><br>
<em>&gt; loosely analogous to a human - such that the initial design responds to
</em><br>
<em>&gt; environmental stimuli and parental upbringing much as does a human, learns and
</em><br>
<em>&gt; argues morality much as does a human, is partially unaware of its own emotions
</em><br>
<em>&gt; like a human, etc.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; We presume that this is a deliberate attempt to create a child of humankind, a
</em><br>
<em>&gt; worthy participant in the story of Earth-originating life.
</em><br>
<em>&gt; 
</em><br>
<p>OK,  but why would attempting to create something like an evolved
<br>
being which is not an evolved being at all, be reasonable or at all
<br>
compassionate?  It seems to me that augmenting exisitng humans would
<br>
be a more rational and acheivable course if human like higher
<br>
intelligences is the desired outcome.
<br>
<p><em>&gt; In this case I would assign a mixture of moral responsibility to the AI
</em><br>
<em>&gt; researcher and to the AI.  The AI, having been deliberately constructed as a
</em><br>
<em>&gt; moral agent, bears responsibility for its actions.  If we hold the AI to
</em><br>
<em>&gt; account, it will understand what it means to be held to account, and treat
</em><br>
<em>&gt; this as a moral argument in the same way we do.  (In contrast a paperclip
</em><br>
<em>&gt; maximizer would care about human moral arguments only as a sort of cognitive
</em><br>
<em>&gt; activity in humans that might be exploited to create paperclips.)  The AI
</em><br>
<em>&gt; researcher is responsible for all predictable consequences of &quot;constructing a
</em><br>
<em>&gt; humanlike moral agent&quot;, including liability for child abuse if later
</em><br>
<em>&gt; authorities determine the initial design to have been botched.  But I would
</em><br>
<em>&gt; not say that the AI researcher is responsible for all actions of the created
</em><br>
<em>&gt; AI, presuming that the created AI was at least as initially benevolent as an
</em><br>
<em>&gt; average human.  Deliberately creating an AI that is worse than average, for
</em><br>
<em>&gt; example, an AI that starts out with the same emotional makeup as an autistic
</em><br>
<em>&gt; or a serial killer, makes the AI researcher liable for both child abuse and
</em><br>
<em>&gt; for the consequences of the AI's actions.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 3)  The AI researcher deliberately chooses an AI design which involves complex
</em><br>
<em>&gt; moral processing, but a different sort of complex moral processing than a
</em><br>
<em>&gt; human being.  Coherent Extrapolated Volition, for example.  In this case,
</em><br>
<em>&gt; assigning moral responsibility becomes difficult; we're operating outside the
</em><br>
<em>&gt; customary problem space.  An AI researcher, responding to a perceived moral
</em><br>
<em>&gt; duty, invents an AI which takes its direction from a complexly computed
</em><br>
<em>&gt; property of the human species as a whole.  If this AI saves a life, to whom
</em><br>
<em>&gt; belongs the credit?  The researcher?  The human species?  The AI?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I would assign moral responsibility to the AI programmer for the predictable
</em><br>
<em>&gt; consequences of creating such an AI, but not the unpredictable consequences,
</em><br>
<em>&gt; provided that the AI as a whole has predominantly good effects (even if there
</em><br>
<em>&gt; are some negative ones).  If the AI has a predominantly negative effect,
</em><br>
<em>&gt; whether by bug or by unintended consequence, then I would assign full
</em><br>
<em>&gt; responsibility to the programmer.
</em><br>
<em>&gt; 
</em><br>
<p>Why?  The consequences in this case are certainly not predictable so
<br>
how is the creator culpable for any and all consequences?
<br>
<p><em>&gt; If a CEV saves you from dying, I would call that a predictable (positive)
</em><br>
<em>&gt; consequence and assign at least partial responsibility to the programmers and
</em><br>
<em>&gt; their supporters.  I would not assign them responsibility for the entire
</em><br>
<em>&gt; remaining course of your life in detail, positive or negative, even though
</em><br>
<em>&gt; this life would not have existed without the CEV.  I would forgive the
</em><br>
<em>&gt; programmers that your evil mother-in-law will also live forever; they didn't
</em><br>
<em>&gt; mean to do that to you specifically.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; **
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't believe there exists any such thing as &quot;autonomy&quot;.
</em><br>
<em>&gt; 
</em><br>
<p>Then I think you are arguing a less than useful (and perhaps ironic)  position. 
<br>
<p><em>&gt; The causal graph of physics goes back at least to the Big Bang.  If you don't
</em><br>
<em>&gt; know the cause, that's your own ignorance; it doesn't mean there is no cause.
</em><br>
<em>&gt;
</em><br>
<p>Cause?  Are we still caught up in pointless clockwork universe models?
<br>
&nbsp;Everything can be reduced to physics but not everything can
<br>
*usefully* be reduced to phhysics.
<br>
&nbsp;
<br>
<em>&gt; I am not &quot;autonomous&quot;.  I am a Word spoken by evolution, which determined both
</em><br>
<em>&gt; my tendencies, and my susceptibility to environmental influence.  
</em><br>
<p>But not your detailed choices.  Our ability to even meaningfully
<br>
conceive of going beyond our evolved nature illustrates nicely the
<br>
lack of predictability regardless of how many causes we may enumerate.
<br>
<p><em>&gt; Where there
</em><br>
<em>&gt; is randomness in me it is because my design permits randomness effects.
</em><br>
<em>&gt; Evolution created me via a subtle and broken algorithm, which caused the goals
</em><br>
<em>&gt; of my internal psychology to depart far from natural selection's sole
</em><br>
<em>&gt; criterion of inclusive genetic fitness.  Either way, evolution bears no moral
</em><br>
<em>&gt; responsibility because natural selection is too far outside the humane space
</em><br>
<em>&gt; of optimization processes to internally represent moral arguments.
</em><br>
<em>&gt;
</em><br>
<p>Since evolution is not a being much less a moral being it obviously
<br>
cannot bear &quot;moral responsibility&quot;.
<br>
&nbsp;
<br>
<em>&gt; My parents were almost entirely powerless compared to an AI designer.
</em><br>
<p>The poor AI designer is powerless to do much beyond set up some
<br>
hopefully fruitful and benevolent intial conditions.  Since the young
<br>
AI is a totally different species it may be that the programmer has
<br>
even less insight and use as a model for the AI child.
<br>
<p><em>&gt;  My
</em><br>
<em>&gt; parents can bear moral responsibility only for what they could control.  Given
</em><br>
<em>&gt; those fixed background circumstances, I understand, respect, and am grateful
</em><br>
<em>&gt; to my parents where they deliberately chose not to exercise a possible
</em><br>
<em>&gt; control, seeing an obligation to let me make up my own mind.  Which is to say
</em><br>
<em>&gt; that my parents handed determination back to the internal forces in my mind,
</em><br>
<em>&gt; which they did not choose to create.  My parents let me make my own decision
</em><br>
<em>&gt; rather than crushing me, in a case where my internal cognitive forces would
</em><br>
<em>&gt; exist regardless.  Had my parents also knowingly selected my nature, their
</em><br>
<em>&gt; decision not to nurture too hard would take on a stranger meaning.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It is not clear what, if anything, an AI researcher can deliberately do that
</em><br>
<em>&gt; is analogous to the choice a human parent faces - even if we understand and
</em><br>
<em>&gt; respect and attach significant moral value to a human parent's choice not to
</em><br>
<em>&gt; determine offspring too strongly.  The mechanisms of &quot;autonomy&quot;, if we value
</em><br>
<em>&gt; them, would need to be deliberately created in a nonhuman mind.  It is
</em><br>
<em>&gt; predictable that if you construct a mind to love it will love, and if you
</em><br>
<em>&gt; construct a mind to hate it will hate.  
</em><br>
<p>It is barely predictable how a human mind raised to love will turn
<br>
out.  I doubt such a complex quality can simply be programmed in.
<br>
<p><em>&gt; In what sense would the AI programmer
</em><br>
<em>&gt; *not* be responsible?  
</em><br>
<p>In that the self-improving nature of the AI will take it and its
<br>
choice space far beyond the competence of any merely human creator to
<br>
reliably map and predict.
<br>
<p><p><em>&gt; Perhaps we can rule that we value human likeness in
</em><br>
<em>&gt; artificial minds, that it is good to grant them many emotions sometimes in
</em><br>
<em>&gt; conflict.  We could hold the AI researcher responsible for the choice to
</em><br>
<em>&gt; construct a humanlike mind, but not for the specific and unpredictable outcome
</em><br>
<em>&gt; of the humanlike emotional conflicts.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This exception requires that the AI researcher gets it right and creates a
</em><br>
<em>&gt; healthy child of humankind.  Screw it up - create a mind whose internal
</em><br>
<em>&gt; conflicts turn out to be simpler and less interesting than human average, or
</em><br>
<em>&gt; whose internal conflicts turn out to be more painful - and I would hold the
</em><br>
<em>&gt; designers fully responsible.  If you can't do it right, then DON'T DO IT. 
</em><br>
<p>So unless we are much more competent than I have reason to believe we
<br>
are we should just let human level minds bungle their way into [almost
<br>
inevitiable] oblivion?
<br>
<p><em>&gt; If
</em><br>
<em>&gt; you aren't sure you can do it right, WAIT until you are.  I would like to see
</em><br>
<em>&gt; humankind get through the 21st century without inventing new and horrible
</em><br>
<em>&gt; forms of child abuse.
</em><br>
<em>&gt; 
</em><br>
<p>Unless we radically augment ourselves and perhaps even then, that day
<br>
we are that uber-competent will in my opinion never come.
<br>
<p><em>&gt; An AI researcher who deliberately builds an AI unpredictable to the designer,
</em><br>
<em>&gt; but which AI does not qualify as a healthy child of humankind, bears full
</em><br>
<em>&gt; responsibility for the consequences of the AI's unpredictable actions whatever
</em><br>
<em>&gt; they may be.  
</em><br>
<p>Of course no AI can qualify as a healthy child because that isn't
<br>
remotely what it is. So how is this a valid distinciton?
<br>
<p><em>&gt; This is so even if the AI researcher claims deliberate refusal
</em><br>
<em>&gt; to understand in order to preserve the quote autonomy unquote of the AI.  I
</em><br>
<em>&gt; would advise that you not believe the claim.  Incompetence is not a moral
</em><br>
<em>&gt; duty, but people often try to excuse it as a moral duty.
</em><br>
<p>Claiming competence beyond what is possible for mere humans is not useful.
<br>
<p><em>&gt;  &quot;Moral autonomy&quot; is
</em><br>
<em>&gt; not randomness.  There is nothing moral about randomness.  Nor is everything
</em><br>
<em>&gt; that you're too incompetent to predict &quot;autonomous&quot;.
</em><br>
<em>&gt;
</em><br>
<p>Autonomous here largely means capable of choosing among alternatives
<br>
without undue coercion.  Anything which makes such choices and learns
<br>
in the process from the outcome of those choices is in principal not
<br>
fully predictable unless the set of possible states is artificially
<br>
constrained.  I don't see any more how to constrain such an AI in this
<br>
way than I see how to constrain it with the three laws of robotics.
<br>
&nbsp;
<br>
<em>&gt; Moral autonomy requires a specific kind of cognitive complexity which will
</em><br>
<em>&gt; take high artistry to create in an artificial mind.  The designers might
</em><br>
<em>&gt; *choose* not to compute out in advance the child's destiny, nor fine-tune the
</em><br>
<em>&gt; design on the basis of such predictions.  But be very sure, the designers do
</em><br>
<em>&gt; understand *all* the forces involved - if they possess the art to create a
</em><br>
<em>&gt; healthy child of humankind.
</em><br>
<p>This is a mere assertion that I consider  bogus.  We may be competent
<br>
enough to create the seed of a real AI but I do not belief we are
<br>
competent to design it to this fine degree of predictability.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Ignorance exists in the mind, not in reality.
</em><br>
<p>Not useful as in reality the minds of the designers have quite finite
<br>
limits that may fall far short of *the answer* in reality.  Ignorance
<br>
is an inescapable aspect of limited intelligence.
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11695.html">Marc Geddes: "RE: 'Collective Volition' ripped to pieces"</a>
<li><strong>Previous message:</strong> <a href="11693.html">Brian Atkins: "Re: META:  Not SL4 material (was Re: Hiroshima Day)"</a>
<li><strong>In reply to:</strong> <a href="11673.html">Eliezer S. Yudkowsky: "Re: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11694">[ date ]</a>
<a href="index.html#11694">[ thread ]</a>
<a href="subject.html#11694">[ subject ]</a>
<a href="author.html#11694">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:00 MST
</em></small></p>
</body>
</html>
