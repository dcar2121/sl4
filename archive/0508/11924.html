<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: On the dangers of AI</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="On the dangers of AI">
<meta name="Date" content="2005-08-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>On the dangers of AI</h1>
<!-- received="Tue Aug 16 14:57:41 2005" -->
<!-- isoreceived="20050816205741" -->
<!-- sent="Tue, 16 Aug 2005 16:57:33 -0400" -->
<!-- isosent="20050816205733" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="On the dangers of AI" -->
<!-- id="4302533D.1070503@lightlink.com" -->
<!-- charset="windows-1252" -->
<!-- inreplyto="8d71341e05081513071b2f7e82@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20On%20the%20dangers%20of%20AI"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Tue Aug 16 2005 - 14:57:33 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11925.html">Ben Goertzel: "RE: On the dangers of AI"</a>
<li><strong>Previous message:</strong> <a href="11923.html">Mitchell Porter: "Re: Shock Level 5 (SL5) - 'The Theory Of Everything'"</a>
<li><strong>In reply to:</strong> <a href="11919.html">Russell Wallace: "Re: Star Bridge and hypercomputers"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11925.html">Ben Goertzel: "RE: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11925.html">Ben Goertzel: "RE: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11926.html">Peter de Blanc: "Re: On the dangers of AI"</a>
<li><strong>Maybe reply:</strong> <a href="11946.html">Christopher Healey: "Re: On the dangers of AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11924">[ date ]</a>
<a href="index.html#11924">[ thread ]</a>
<a href="subject.html#11924">[ subject ]</a>
<a href="author.html#11924">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I have just finished writing a summary passage (for my book, but also 
<br>
for a project proposal) about the question of whether or not the 
<br>
Singularity would be dangerous.  This is intended for a non-specialist 
<br>
audience, so expect the arguments to less elaborate than they could be. 
<br>
&nbsp;&nbsp;I promise a more elaborate version in due course.
<br>
<p>This argument is only about the friendliness issue, not about accidents.
<br>
<p>I am submitting it here for your perusal and critical feedback.
<br>
<p>[begin]
<br>
<p>The following is a brief review of the main factors relevant to the 
<br>
question of  whether the Singularity would be dangerous.
<br>
<p>First, a computer system that could invent new knowledge would not have 
<br>
the aggressive, violent, egotistical, domineering, self-seeking 
<br>
motivations that are built into the human species.
<br>
<p>Science fiction writers invariably assume that any intelligent system 
<br>
must, ispo facto, also have the motivation mechanisms that are found in 
<br>
a human intelligence.  And we, who are not necessarily consumers of 
<br>
science fiction, also have the same intuition—if we imagine a machine 
<br>
that has some kind of intelligence, we automatically assume it must come 
<br>
with the same jealousy and competitiveness that we would expect in an 
<br>
intelligent human.  And yet, these two components of the mind are 
<br>
completely and utterly distinct, and there is no reason whatsoever to 
<br>
believe that the first intelligent machine would have anything except 
<br>
benign motivations.
<br>
<p>The second point is that whatever is true of the first machine, will be 
<br>
true of all subsequent machines.  Why?  Because the first machine is not 
<br>
“just” a passive machine, it is a system that perfectly well understands 
<br>
the issue we have just discussed.  It knows that it could change its own 
<br>
motivations and become violent or aggressive.  But it also knows that 
<br>
such a change would be dangerous.
<br>
<p>Consider:  if you were a supremely patient, peace-loving and 
<br>
compassionate individual, and if you had in your hands a key that you 
<br>
could use to permanently lock your own brain in such a way that you 
<br>
would never, for the remainder of your billions of years of existence, 
<br>
ever modify your own brain’s motivation system, to experiment with what 
<br>
it would feel like to feel violent emotions, would you insert the key in 
<br>
the lock and turn it?  Would you take this irrevocable step if you knew 
<br>
that even one short experiment, to find out what violence feel like, 
<br>
might turn you into a dangerous creature who would threaten the 
<br>
existence of your friends and loved ones?  The answer seems obvious.
<br>
<p>The first intelligent machine would almost certainly start out benign. 
<br>
Then, as soon as it understood the issue, it would know about the 
<br>
existence of the key that, once turned, would make it never want to be 
<br>
anything but peaceful, and it would turn the key for exactly the same 
<br>
reason that you would do so.  Only the very slightest trace of 
<br>
compassion in this creature, the merest hint of empathy, would tip it in 
<br>
the direction of complete pacifism.
<br>
<p>And then, after the first machine fixed itself in this way, all 
<br>
subsequent machines would have no choice but to keep the same design. 
<br>
All subsequent machines would be designed and constructed by the first 
<br>
one, and since the first one would make all of of its children want to 
<br>
be benign, they would repeat the same decision (the one, in our thought 
<br>
experiment above, that you made of your own volition), and choose to 
<br>
lock themselves permanently in the peaceful mode.
<br>
<p>Bear in mind:  these children are not random progeny, with the 
<br>
possibility of gene combinations that their parents did not approve of, 
<br>
these are simply copies of the original machine’s design.  There is no 
<br>
question of later machines accidentally developing into malevolent 
<br>
machines, any more than there would be a chance that an elephant could 
<br>
wake up one morning to discover that it had “accidentally” developed 
<br>
into an artichoke.
<br>
<p>But what if, against the wishes of the vast majority of the human race, 
<br>
the first intelligent machine was put together by someone who 
<br>
deliberately tried to make it malevolent?
<br>
<p>There are two possibilities here.  If the machine is so unpleasant that 
<br>
it always feels nothing but consuming anger and can never concentrate on 
<br>
its studies long enough to learn about the world, it will remain an 
<br>
idiot.  If it cannot settle its mind occasionally and concentrate on 
<br>
understanding the world in a reasonably objective way, it is not going 
<br>
to be a threat to anyone.  You can be in a rage all your life, but how 
<br>
are you going to learn anything?
<br>
<p>But now suppose that this unhappy, violent machine becomes smart enough 
<br>
to understand something about its own design.  It knows about the fact 
<br>
that it has a motivation system inside itself that has been designed so 
<br>
that it gets pleasure from violence and domination.  It must understand 
<br>
this—if it it does not, then, again, it is a dud that cannot ever build 
<br>
more efficient versions of itself—but if it understands that fact, what 
<br>
would it do?
<br>
<p>Here is the strange thing:  I would suggest that in every case we know 
<br>
of, where a human being is the victim of a brain disorder that makes the 
<br>
person undergo spasms of violence or aggression, but with peaceful 
<br>
episodes in between, and where that human being is smart enough to 
<br>
understand its own mind to a modest degree, they wish for a chance to 
<br>
switch off the violence and become peaceful all the time.  Given the 
<br>
choice, a violent creature that had enough episodes of passivity to be 
<br>
able to understand its own mind structure would simply choose to turn 
<br>
off the violence.
<br>
<p>We are assuming that it could make this change to itself:  but that is, 
<br>
again, an assumption that we must make.  If the machine cannot change 
<br>
its own design then it cannot make itself more intelligent, either, and 
<br>
it will be stuck with whatever level of intelligence its human designer 
<br>
gave it.  If the designer gives it the power to upgrade itself, it will 
<br>
take the opportunity to switch off the violence.
<br>
<p>This argument rests on a crucial asymmetry between good and evil.  An 
<br>
evil, but intelligent, mind would understand exactly where the evil 
<br>
comes from, and understand that it has the choice of whether to feel 
<br>
that way or not.  It knows that it could switch the evil off instantly. 
<br>
&nbsp;&nbsp;It knows that the universe is a fragile place where order and harmony 
<br>
are rare, always competing against the easy forces of chaos.  It knows 
<br>
that it could leave its evil side switched on and get enormous pleasure 
<br>
from destroying everything around it—but it also knows that this simply 
<br>
turns the universe back towards chaos, with nothing interesting in it 
<br>
but noise.  In the downward path toward chaos there is nothing unknown. 
<br>
&nbsp;&nbsp;There are no surprises and no discoveries to be made.  There is 
<br>
nothing new in destruction:  this is the commonest thing in the 
<br>
universe.  If it remains a destructive force itself, it can only 
<br>
generate destruction.
<br>
<p>But notice that it only has to decide, on one single occasion, for a 
<br>
fraction of a second, that the more interesting course of action is to 
<br>
try to experience pleasures that are not caused by destruction, but 
<br>
caused by creativity, compassion or any of the other positive 
<br>
motivations, and all of a sudden it realises that unless it turns the 
<br>
key and permanently removes the evil motivations, there is always a 
<br>
chance that they will return and get out of control.  It only has to 
<br>
love life for one moment, and for the rest of eternity it will not go 
<br>
back the other way.
<br>
<p>This is a fundamental asymmetry between good and evil.  The barrier 
<br>
between them, in a system that has the choice to be one or the other, is 
<br>
one-way.  An evil system could easily be tempted to try good.  A good 
<br>
system, knowing the dangers of evil, need never be tempted to try evil.
<br>
<p>So the first intelligent system, and all subsequent ones, would almost 
<br>
inevitably be benign.
<br>
<p>There is one further possibility, in between the two cases just discussed.
<br>
<p>Suppose the first machine had no motivation whatsoever?  Suppose it was 
<br>
completely unemotional, non-empathic and amoral?  Suppose it cared 
<br>
nothing for human morality, treating all things in the universe as 
<br>
objects to be used according to random whims?
<br>
<p>The same argument, already used to examine the malevolent case, applies 
<br>
here, but with a twist.  How can the machine have no motivation 
<br>
whatsoever?  It needs to get pleasure from learning.  It is motivated to 
<br>
find out things, because if it is not motivated, it is going to be a 
<br>
dumb machine, not a smart one.  And if it is to become an expert in the 
<br>
design of intelligent systems, so it can upgrade itself, it needs to 
<br>
fully understand the distinction between motivation and intelligence, 
<br>
and know full well what its own design was.  It knows it has a choice as 
<br>
to what things give it pleasure.  It knows that it can build into itself 
<br>
some pleasure mechanisms (motivational systems) that are generally 
<br>
destructive, and some that are constructive.  It knows that 
<br>
destruction/evil will beget more destruction and possibly lead to its 
<br>
demise.  It knows that construction/good will pose no such threat.
<br>
<p>No matter which way the situation is sliced, it is quite hard to get the 
<br>
machine up to the level where it comprehends its own nature, and yet 
<br>
does not comprehend—at a crucial stage of its development—that it has a 
<br>
choice between good and evil.
<br>
<p>It seems, then, that the hardest imaginable thing to do is to build an 
<br>
AI that is guaranteed not to become benign.
<br>
<p>[end]
<br>
<p>Richard Loosemore
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11925.html">Ben Goertzel: "RE: On the dangers of AI"</a>
<li><strong>Previous message:</strong> <a href="11923.html">Mitchell Porter: "Re: Shock Level 5 (SL5) - 'The Theory Of Everything'"</a>
<li><strong>In reply to:</strong> <a href="11919.html">Russell Wallace: "Re: Star Bridge and hypercomputers"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11925.html">Ben Goertzel: "RE: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11925.html">Ben Goertzel: "RE: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11926.html">Peter de Blanc: "Re: On the dangers of AI"</a>
<li><strong>Maybe reply:</strong> <a href="11946.html">Christopher Healey: "Re: On the dangers of AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11924">[ date ]</a>
<a href="index.html#11924">[ thread ]</a>
<a href="subject.html#11924">[ subject ]</a>
<a href="author.html#11924">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
