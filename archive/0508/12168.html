<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: drives ABC &gt; XYZ</title>
<meta name="Author" content="Michael Vassar (michaelvassar@hotmail.com)">
<meta name="Subject" content="Re: drives ABC &gt; XYZ">
<meta name="Date" content="2005-08-30">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: drives ABC &gt; XYZ</h1>
<!-- received="Tue Aug 30 20:06:17 2005" -->
<!-- isoreceived="20050831020617" -->
<!-- sent="Tue, 30 Aug 2005 22:06:14 -0400" -->
<!-- isosent="20050831020614" -->
<!-- name="Michael Vassar" -->
<!-- email="michaelvassar@hotmail.com" -->
<!-- subject="Re: drives ABC &gt; XYZ" -->
<!-- id="BAY101-F901A1101D2B47619A35A0ACA10@phx.gbl" -->
<!-- inreplyto="20050831001738.53971.qmail@web54502.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Vassar (<a href="mailto:michaelvassar@hotmail.com?Subject=Re:%20drives%20ABC%20&gt;%20XYZ"><em>michaelvassar@hotmail.com</em></a>)<br>
<strong>Date:</strong> Tue Aug 30 2005 - 20:06:14 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12169.html">Michael Wilson: "Re: drives ABC &gt; XYZ"</a>
<li><strong>Previous message:</strong> <a href="12167.html">Tennessee Leeuwenburg: "Re: drives ABC &gt; XYZ"</a>
<li><strong>In reply to:</strong> <a href="12164.html">Phil Goetz: "Re: drives ABC &gt; XYZ"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12170.html">Joshua Amy: "remove"</a>
<li><strong>Reply:</strong> <a href="12170.html">Joshua Amy: "remove"</a>
<li><strong>Reply:</strong> <a href="12174.html">Phil Goetz: "Bayesians &amp; Pascal's wager"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12168">[ date ]</a>
<a href="index.html#12168">[ thread ]</a>
<a href="subject.html#12168">[ subject ]</a>
<a href="author.html#12168">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt;We're already
</em><br>
<em>&gt;assuming that.  The A B C -&gt; X Y Z example shows how, one step at
</em><br>
<em>&gt;a time, the system can take actions that provide greater utility
</em><br>
<em>&gt;from the perspective of its top-level goals, that nonetheless end
</em><br>
<em>&gt;up replacing all those top-level goals.
</em><br>
<p>Well then, so long as the ultimate goals are higher utility, from the 
<br>
perspective of the original goals, than the original goals were, why is this 
<br>
a problem?  A human would typically not be able to predict the long term 
<br>
expected utility of a change to its top level goals, but a FAI wouldn't make 
<br>
such changes unless it could.
<br>
<p><em>&gt;Another question entirely is whether, if the AI is told to maximize
</em><br>
<em>&gt;a score relating to the attainment of its top-level goals, and is
</em><br>
<em>&gt;given write access to those goals, it will rewrite those goals into
</em><br>
<em>&gt;ones more easily attainable?  (We could call this the &quot;Buddhist AI&quot;,
</em><br>
<em>&gt;perhaps?)  The REAL top-level goal in that case
</em><br>
<em>&gt;is &quot;maximize a score defined by the contents of memory locations X&quot;,
</em><br>
<em>&gt;but it doesn't help us to say that &quot;maximization&quot; won't be replaced.
</em><br>
<em>&gt;The kinds of goals we don't want to be replaced have referents
</em><br>
<em>&gt;in the real world.
</em><br>
<p>This really is a very very old insight for this list.  Try to familiarize 
<br>
yourself with the list archive or at least with the major articles.  That 
<br>
really applies to everyone who hasn't done so.  Suffice it to say that such 
<br>
concerns were addressed very thoroughly years ago.
<br>
<p><em>&gt;You seem to be proposing that an AI will never make mistakes.
</em><br>
<p>In the human sense, yes.  If an AI is superintelligent and Friendly for any 
<br>
significant time it will reach a state from which it will not ever make the 
<br>
sort of errors of reasoning which humans mean by mistakes.  In fact, any 
<br>
well calibrated Bayesian built on a sufficiently redundant substrate should 
<br>
never make mistakes in the sense of either acting on implicit beliefs other 
<br>
than its explicit beliefs or holding a belief with unjustified confidence.  
<br>
Obviously, computing power, architectural details, and knowledge will 
<br>
determine the degree to which it will or will not act in the manner which 
<br>
actually maximized its utility function, but that is not what we humans mean 
<br>
by a mistake.  We are used to constantly taking actions which we have every 
<br>
reason to expect to regret.  A FAI shouldn't do that.  This is an important 
<br>
distinction and not at all a natural one.  It shouldn't be terribly 
<br>
shocking, but is.  But by now we should be used to the idea that computers 
<br>
can perform long series of mathematical operations without error, and that 
<br>
performing the right long series of mathematical operations is equivalent to 
<br>
making a decision under uncertainty, so they should be able to make 
<br>
decisions under uncertainty without error, though due to the uncertainty 
<br>
such decisions will usually be less optimal that the decisions that would 
<br>
have been available given more information.
<br>
<p><em>&gt;Making mistakes is a second way in which top-level goals can
</em><br>
<em>&gt;drift away from where they started.
</em><br>
<p>Making sub-optimal decisions can cause top-level goals to drift, but this 
<br>
problem is absolutely unoavoidable, but should not be critical (and if it is 
<br>
critical, that is, fundamental to the way reason works, we will just have to 
<br>
do as well as we can).  Account must be taken of it when designing an FAI, 
<br>
but this only requires an incremental development beyond that needed to 
<br>
protect it from Pascal's Wagers.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12169.html">Michael Wilson: "Re: drives ABC &gt; XYZ"</a>
<li><strong>Previous message:</strong> <a href="12167.html">Tennessee Leeuwenburg: "Re: drives ABC &gt; XYZ"</a>
<li><strong>In reply to:</strong> <a href="12164.html">Phil Goetz: "Re: drives ABC &gt; XYZ"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12170.html">Joshua Amy: "remove"</a>
<li><strong>Reply:</strong> <a href="12170.html">Joshua Amy: "remove"</a>
<li><strong>Reply:</strong> <a href="12174.html">Phil Goetz: "Bayesians &amp; Pascal's wager"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12168">[ date ]</a>
<a href="index.html#12168">[ thread ]</a>
<a href="subject.html#12168">[ subject ]</a>
<a href="author.html#12168">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
