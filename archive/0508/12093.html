<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Terms of debate for Complex Systems Issues</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="Re: Terms of debate for Complex Systems Issues">
<meta name="Date" content="2005-08-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Terms of debate for Complex Systems Issues</h1>
<!-- received="Wed Aug 24 14:24:49 2005" -->
<!-- isoreceived="20050824202449" -->
<!-- sent="Wed, 24 Aug 2005 21:24:46 +0100 (BST)" -->
<!-- isosent="20050824202446" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="Re: Terms of debate for Complex Systems Issues" -->
<!-- id="20050824202446.29786.qmail@web26707.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="430CBBEE.7040305@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20Terms%20of%20debate%20for%20Complex%20Systems%20Issues"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Wed Aug 24 2005 - 14:24:46 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12094.html">Michael Wilson: "Re: Terms of debate for Complex Systems Issues"</a>
<li><strong>Previous message:</strong> <a href="12092.html">Phil Goetz: "Re: Terms of debate for Complex Systems Issues"</a>
<li><strong>In reply to:</strong> <a href="12090.html">Richard Loosemore: "Re: Terms of debate for Complex Systems Issues"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12094.html">Michael Wilson: "Re: Terms of debate for Complex Systems Issues"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12093">[ date ]</a>
<a href="index.html#12093">[ thread ]</a>
<a href="subject.html#12093">[ subject ]</a>
<a href="author.html#12093">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Richard Loosemore wrote:
<br>
<em>&gt; this is all about how to change the parameters of various systems
</em><br>
<em>&gt; to get them from a non-complex regime up into complexity.
</em><br>
<p>No, it's a search for useful predictions that 'Complexity Theory' might
<br>
be capable to making. The above might be part of the hypothesis behind
<br>
such a prediction, but it's only an intermediate step in any useful
<br>
inferential chain.
<br>
<p><em>&gt; All of the above is not anthropomorphism, just model building inside
</em><br>
<em>&gt; an intelligent mechanism. /There are no intentional terms/.
</em><br>
<em>&gt;
</em><br>
<em>&gt; ...it may be /curious/ about... ...engage in /speculation/ about...
</em><br>
<em>&gt; ...surely /imagine/ such eventualities... ...it could /relax/...
</em><br>
<em>&gt; .../ignore/ all of this thinking...
</em><br>
<p>(my emphasis) This is just black humour. You make a cursory attempt at
<br>
formalism and then proceed to ignore it. If you're trying to make a
<br>
technical argument, define everything that has no commonly accepted
<br>
formalisation.
<br>
<p><em>&gt; (&quot;Thinking&quot; = building representations of the world, reasoning about
</em><br>
<em>&gt; the world, etc etc etc.  &quot;think&quot; from now on will be used as shorthand
</em><br>
<em>&gt; for something going on the part of the system that does this).
</em><br>
<p>Representations and reasoning aren't well defined entities, but unlike
<br>
the above terms they are (relatively) non-anthropomorphic, so this is
<br>
acceptable in a non-rigorous argument.
<br>
<p><em>&gt; One day, it happens to be working on the goal of *trying to understand 
</em><br>
<em>&gt; how intelligent systems work*.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It thinks about its own system.
</em><br>
<p>A key intuitive property of 'thinking' is that it has no direct effect
<br>
on the external environment. Technically this isn't true for any real
<br>
world system, though it may be literally true for a simulated system,
<br>
but in practice it's normally ok to assume that cognitive actions
<br>
(e.g. manipulating models to produce inferences) will have no
<br>
side effects beyond yielding information.
<br>
&nbsp;
<br>
<em>&gt; This means: it builds a representation of what is going on inside 
</em><br>
<em>&gt; itself.
</em><br>
<p>Which is a reflective model of the AI.
<br>
<p><em>&gt; And as part of its &quot;thinking&quot; it may be curious about what happens
</em><br>
<em>&gt; if it reaches into its own programming and makes alterations to 
</em><br>
<em>&gt; its goal system on the fly. (Is there anything in your formalism that 
</em><br>
<em>&gt; says it cannot or would not do this? Is it not free, within the 
</em><br>
<em>&gt; constraints of the goal system, to engage in speculation about 
</em><br>
<em>&gt; possibilities?  To be a good learner, it would surely imagine such 
</em><br>
<em>&gt; eventualities.)
</em><br>
<p>Changing the content of the reflective model in order to predict the
<br>
results of a particular self-modification is fine; CFAI calls this
<br>
'self-shadowing'. That isn't to say that this is risk free in reality,
<br>
but an AGI with a simple goal system could reasonably elect do this.
<br>
In particular this is compatible with an architecture in which
<br>
inferential search is not closely directed by the main goal system,
<br>
something designers of the 'bubbling stew of agents' persuasion are
<br>
likely to favour.
<br>
<p>However changing the /AI itself/, rather than the /model of the AI/,
<br>
violates a basic layering tennent. Changes to the AI itself are not
<br>
side-effect free, which is to say that cognitive operators affecting
<br>
cognitive content outside of the reflective model are not guarenteed
<br>
to remain causally local. As such self-modification is a first class
<br>
action that will not take place unless the goal system evaluates it
<br>
as a good idea. Any halfway decent AI substrate should be able to
<br>
manage elementary reflective layering and isolation of causal domains
<br>
like this; even connectionist systems can support with appropriate
<br>
help from the supporting codebase. Direct self-modification can still
<br>
theoretically be used as a form of inference, via a decision similar
<br>
to a decision to perform a physical experiment in reality outside of
<br>
the AI. But this decision will not be made if the modification in
<br>
question changes the goal system such that the future AI has a
<br>
nontrivial probability of no longer sharing the same optimisation
<br>
targets.
<br>
<p><em>&gt; Let us suppose... that it notices... that it will eventually reach a 
</em><br>
<em>&gt; state in a million years time when it will cause some kind of damage 
</em><br>
<em>&gt; that will result in its own demise.
</em><br>
<p>Where 'demise' is classified by the goal system as undesireable.
<br>
<p><em>&gt; But if it now *knows* that this prime directive was inserted
</em><br>
<em>&gt; arbitrarily, it might consider the idea that it could simply
</em><br>
<em>&gt; alter its goals.
</em><br>
<p>Sure, it models itself changing its goals, predicts the consequences
<br>
and ranks the action (if the AGI is using EU, this means calculating
<br>
an EU for the action). But arbitrariness has no influence on the
<br>
desireability of having a goal unless there is an explicit goal stating
<br>
that (goal system) arbitrariness is bad. This point was made repeatadly
<br>
when you were arguing about your own goals and you still don't seem to
<br>
have taken it on board.
<br>
<p><em>&gt; Could make them absolutely anything it wanted, in fact
</em><br>
<p>Capability does not imply intention.
<br>
<p><em>&gt; after the change, it could relax
</em><br>
<p>'Relax' could mean a great many things, all of them irrelevant, but if
<br>
you want to make an argument about emotions then you're going to have
<br>
to specify how they contribute to inference and action selection in your
<br>
model.
<br>
<p><em>&gt; What does it do? Ignore all of this thinking?
</em><br>
<p>To 'ignore' means to deliberately not consider information when making
<br>
futher inferences (including decisions). Predictions about the results
<br>
of a self-modification are clearly highly relevant to the decision to
<br>
implement that modification or not, so ignoring them is the last thing
<br>
an (even vaguely rational) AGI would do. But none of the above will
<br>
generate a preference for the self-modification unless (a) arbitrariness
<br>
/is/ defined as a bad thing in the goal system and/or (b) the modified
<br>
goal would result in better overall success at fulfilling the original
<br>
goal due to the system being around longer, despite not targetting
<br>
quite the same thing. I would argue that (b) is merely an ill-advised
<br>
and almost certainly unnecessary approximation of a system of the
<br>
original goal plus an implied subgoal 'don't allow self-destruction',
<br>
which will act in concert (under an EU decision function anyway) to
<br>
produce exactly the same result without risking optimisation target
<br>
drift (and its attendant negative utility/undesireability).
<br>
<p><em>&gt; Maybe it comes to some conclusion about what it *should* do that
</em><br>
<em>&gt; is based on abstract criteria that have nothing to do with its
</em><br>
<em>&gt; current goal system.
</em><br>
<p>Either you're specifying explicit metagoals, which are a special
<br>
case of supergoals, or you're designing a non-causally-clean
<br>
substrate that effectively has implicit metagoals. And/or you're
<br>
just introducing randomness for no good reason, I suppose.
<br>
&nbsp;
<br>
<em>&gt; What is crucial is that in a few moments, the AGI will have changed
</em><br>
<em>&gt; (or maybe not changed) its goal system, and that change will have been 
</em><br>
<em>&gt; governed, not by the state of the goal system right now, but by the 
</em><br>
<em>&gt; &quot;content&quot; of its current thinking about the world.
</em><br>
<p>What sane reason is there for violating the causal locality of the
<br>
model and the layering of systems required for reflection by conflating
<br>
the reflective model and the actual system like this? You can't even
<br>
make an efficiency argument, as it's always possible to just run a
<br>
second copy of the AI in a box as the model (in principle; see caveats
<br>
about AI boxes), or do a properly considered experiment. If you've
<br>
simply conflated predictive inference from decision making then fine,
<br>
but in that case (apart from being very foolish) you can no longer
<br>
talk about discrete 'models' and 'thinking about models'.
<br>
<p><em>&gt; A system in which *representational content* has gotten the ability
</em><br>
<em>&gt; to feed back to *mechanism* in the way I have just described, is one
</em><br>
<em>&gt; sense of Complex.
</em><br>
<p>That part makes perfect sense. As I understand it all your
<br>
frustrations with SL4 result from the fact that you can't distinguish
<br>
a (reflective) model from its referant. To be fair, this is a
<br>
moderately subtle mistake to make.
<br>
<p><em>&gt; Now, demonstrate in some formal way that the goal system's structure, 
</em><br>
<em>&gt; when the AGI has finished this little thought episode, is a predictable 
</em><br>
<em>&gt; consequence of the current goal system.
</em><br>
<p>Back when I was a fan of active symbol systems, emergence, agent soups
<br>
and advanced genetic algorithms, Eliezer said to me;
<br>
<p>'Of course you can't predict the goal system trajectory for a DE-heavy
<br>
&nbsp;AGI. It's like trying to predict the molecular outcome of turning a
<br>
&nbsp;six-year-old loose in a chemistry lab.'
<br>
<p>This was instrumental in causing me to realise that those sort of
<br>
techniques was a profoundly bad idea, though I confess that I didn't
<br>
truly accept this until I'd already found alternative (and more
<br>
powerful) techniques. The quote also applies to any design that
<br>
can't manage to simulate something without accidentally altering
<br>
the thing it's trying to simulate, but hopefully it is more obvious
<br>
that there's no good reason to design a system like that in the first
<br>
place.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p><p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
To help you stay safe and secure online, we've developed the all new Yahoo! Security Centre. <a href="http://uk.security.yahoo.com">http://uk.security.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12094.html">Michael Wilson: "Re: Terms of debate for Complex Systems Issues"</a>
<li><strong>Previous message:</strong> <a href="12092.html">Phil Goetz: "Re: Terms of debate for Complex Systems Issues"</a>
<li><strong>In reply to:</strong> <a href="12090.html">Richard Loosemore: "Re: Terms of debate for Complex Systems Issues"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12094.html">Michael Wilson: "Re: Terms of debate for Complex Systems Issues"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12093">[ date ]</a>
<a href="index.html#12093">[ thread ]</a>
<a href="subject.html#12093">[ subject ]</a>
<a href="author.html#12093">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
