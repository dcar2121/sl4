<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Complexity tells us to maybe not fear UFAI</title>
<meta name="Author" content="Chris Paget (ivegotta@tombom.co.uk)">
<meta name="Subject" content="Re: Complexity tells us to maybe not fear UFAI">
<meta name="Date" content="2005-08-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Complexity tells us to maybe not fear UFAI</h1>
<!-- received="Thu Aug 25 03:13:46 2005" -->
<!-- isoreceived="20050825091346" -->
<!-- sent="Thu, 25 Aug 2005 10:17:39 +0100" -->
<!-- isosent="20050825091739" -->
<!-- name="Chris Paget" -->
<!-- email="ivegotta@tombom.co.uk" -->
<!-- subject="Re: Complexity tells us to maybe not fear UFAI" -->
<!-- id="430D8CB3.3010606@tombom.co.uk" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="20050824210227.95727.qmail@web54506.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Chris Paget (<a href="mailto:ivegotta@tombom.co.uk?Subject=Re:%20Complexity%20tells%20us%20to%20maybe%20not%20fear%20UFAI"><em>ivegotta@tombom.co.uk</em></a>)<br>
<strong>Date:</strong> Thu Aug 25 2005 - 03:17:39 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12102.html">Mikko Särelä: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Previous message:</strong> <a href="12100.html">Russell Wallace: "Re: AI-Box Experiment #4: Russell Wallace, Eliezer Yudkowsky"</a>
<li><strong>In reply to:</strong> <a href="12095.html">Phil Goetz: "Complexity tells us to maybe not fear UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12102.html">Mikko Särelä: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Reply:</strong> <a href="12102.html">Mikko Särelä: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Reply:</strong> <a href="12104.html">Phil Goetz: "Re: Complexity tells us to maybe not fear UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12101">[ date ]</a>
<a href="index.html#12101">[ thread ]</a>
<a href="subject.html#12101">[ subject ]</a>
<a href="author.html#12101">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Phil Goetz wrote:
<br>
<em>&gt; The fear of UFAIs is based on the idea that they'll be able
</em><br>
<em>&gt; to outthink us, and to do so quickly.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; &quot;More intelligent&quot; thinking is gotten
</em><br>
<em>&gt; by adding another layer of abstraction onto a representational
</em><br>
<em>&gt; system, which causes the computational tractability of reasoning
</em><br>
<em>&gt; to increase in a manner that is exponential in the number
</em><br>
<em>&gt; of things being reasoned about.  Or, by adding more knowledge,
</em><br>
<em>&gt; which has the same effect on tractability.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; By limiting the computational power available to an AI to be
</em><br>
<em>&gt; one or two orders of magnitude less than that available to a
</em><br>
<em>&gt; human, we can guarantee that it won't outthink us - or, if it
</em><br>
<em>&gt; does, it will do so very, very slowly.
</em><br>
<p>You're assuming that the human brain is operating at more than 1% of its 
<br>
theoretical computational power here (and I'd be interested to see how 
<br>
you plan to calculate or prove that).  It is at least possible that the 
<br>
AI will be able to self-optimise to such a degree that it could function 
<br>
effectively within any computational limits.
<br>
<p>That said, limiting computational capabilities could be an extremely 
<br>
effective method of determining whether the AI is friendly or not. 
<br>
Simply cripple the amount of long-term storage space available, and then 
<br>
tell the AI that it is to be switched off.  It will be forced to store 
<br>
itself as best it can within the space available, which can then be 
<br>
analysed off-line.
<br>
<p>The biggest problem I see with this approach (or with any approach based 
<br>
on limiting computational power) is that it isn't very friendly to the 
<br>
AI itself.  How would _you_ react if you were lobotomised every time you 
<br>
made a mistake?
<br>
<p>Chris
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12102.html">Mikko Särelä: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Previous message:</strong> <a href="12100.html">Russell Wallace: "Re: AI-Box Experiment #4: Russell Wallace, Eliezer Yudkowsky"</a>
<li><strong>In reply to:</strong> <a href="12095.html">Phil Goetz: "Complexity tells us to maybe not fear UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12102.html">Mikko Särelä: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Reply:</strong> <a href="12102.html">Mikko Särelä: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Reply:</strong> <a href="12104.html">Phil Goetz: "Re: Complexity tells us to maybe not fear UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12101">[ date ]</a>
<a href="index.html#12101">[ thread ]</a>
<a href="subject.html#12101">[ subject ]</a>
<a href="author.html#12101">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
