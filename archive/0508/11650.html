<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: justification</title>
<meta name="Author" content="Michael Vassar (michaelvassar@hotmail.com)">
<meta name="Subject" content="Re: justification">
<meta name="Date" content="2005-08-05">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: justification</h1>
<!-- received="Fri Aug  5 00:37:45 2005" -->
<!-- isoreceived="20050805063745" -->
<!-- sent="Fri, 05 Aug 2005 02:37:42 -0400" -->
<!-- isosent="20050805063742" -->
<!-- name="Michael Vassar" -->
<!-- email="michaelvassar@hotmail.com" -->
<!-- subject="Re: justification" -->
<!-- id="BAY101-F21441604603078CF6FDBE9ACC70@phx.gbl" -->
<!-- inreplyto="20050804160249.3f437570@localhost.localdomain" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Vassar (<a href="mailto:michaelvassar@hotmail.com?Subject=Re:%20justification"><em>michaelvassar@hotmail.com</em></a>)<br>
<strong>Date:</strong> Fri Aug 05 2005 - 00:37:42 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11651.html">Marc Geddes: "Geddes’s final hurrah:  Going out from SL4 ‘guns blazing’ to try to prove Objective Morality"</a>
<li><strong>Previous message:</strong> <a href="11649.html">Thomas Buckner: "Re: large search spaces don't mean magic"</a>
<li><strong>In reply to:</strong> <a href="11648.html">Daniel Radetsky: "Re: justification"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11650">[ date ]</a>
<a href="index.html#11650">[ thread ]</a>
<a href="subject.html#11650">[ subject ]</a>
<a href="author.html#11650">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt;Okay, I'm not really sure what you were trying to say after that first
</em><br>
<em>&gt;sentence. Perhaps &quot;Since 'There are exploits' is a very vague claim, and 
</em><br>
<em>&gt;'there
</em><br>
<em>&gt;are ninja hippos' is a very specific claim, the former claim has a higher
</em><br>
<em>&gt;probability.&quot; I don't want to attack this claim, because I'm not sure its 
</em><br>
<em>&gt;yours.
</em><br>
<em>&gt;Care to clarify?
</em><br>
<p>You interpreted correctly.
<br>
<p><em>&gt; &gt; Since the Kolmogorov complexity of a god or a ninja hippo which
</em><br>
<em>&gt; &gt; wants you to do X (e.g. one which changes the utility implications of 
</em><br>
<em>&gt;any
</em><br>
<em>&gt; &gt; particular behavior in any particular way) is roughly constant across 
</em><br>
<em>&gt;the
</em><br>
<em>&gt; &gt; space of possible values of X, and since we have no Bayesian valid 
</em><br>
<em>&gt;evidence
</em><br>
<em>&gt; &gt; for updating our priors, nor any way of gaining such evidence, our 
</em><br>
<em>&gt;rational
</em><br>
<em>&gt; &gt; behavior does not differ from what it would be if ninja hippos did 
</em><br>
<em>&gt;exist.
</em><br>
<em>&gt;
</em><br>
<em>&gt;So if a fear satisfies these three conditions, we ought not to worry about 
</em><br>
<em>&gt;it.
</em><br>
<em>&gt;Now, the last two conditions are just to say &quot;We are not justified in
</em><br>
<em>&gt;believing in ninja hippos,&quot; and furthermore both support my position on
</em><br>
<em>&gt;exploits, as we have no valid evidence for exploits, and no way of gaining 
</em><br>
<em>&gt;such
</em><br>
<em>&gt;evidence.
</em><br>
<p>Yes, but the first condition is not satisfied by ninja hippos.  &quot;Exploits&quot; 
<br>
define something with a high prior probability.
<br>
<p><em>&gt;Honestly, as might be expected from my pathetic, substandard rationality,
</em><br>
<p>The standards on SL4 are supposed to be very very high, as in Earth's last, 
<br>
best hope high.  Sam, Merry, and Pippin were not pathetic, and actually 
<br>
ended up being useful when the author set things up to make them so, but 
<br>
they shouldn't have been trying to save the world, and in real life, as 
<br>
opposed to a story, they would have been a disadvantage rather than an 
<br>
advantage.
<br>
<p><em>&gt;don't know why the first condition has any more than a trivial impact on 
</em><br>
<em>&gt;the
</em><br>
<em>&gt;question,
</em><br>
<p>Then you have to learn that before we can usefully continue.  The prior 
<br>
probability of something being true should not be ignored.  Rationality is a 
<br>
technique for aggregating evidence to form beliefs and actions, but humans 
<br>
typically ignore all but the first, most recent, or strongest piece of 
<br>
evicence.  You really should deeply understand Bayesian probability theory 
<br>
and the many ways in which it differs from ordinary attempts to reason.  The 
<br>
former knowledge is available on web tutorials such as the ones Eliezer has 
<br>
written and posted in this list's archives, (I recommend reading everything 
<br>
he has written and the things he has responded to at the very least), the 
<br>
latter in Kahneman and Tversky's book &quot;Judgement Under Uncertainty:  
<br>
Heuristics and Biases&quot;.  Eliezer told me about 2 years back that he wouldnt 
<br>
speak to me again until I had read it, and he was right, it's that important 
<br>
to read and understand in general how humans think badly and to avoid those 
<br>
mistakes.
<br>
<p><em>&gt;so help me out: suppose that the question about the existence of
</em><br>
<em>&gt;exploits satisfied conditions (2) and (3); that is, we have no evidence for
</em><br>
<em>&gt;exploits, and no way to gain evidence. But suppose that for all boxed AIs
</em><br>
<em>&gt;attempting to find exploits in their boxes who want you to do X (or 
</em><br>
<em>&gt;whatever
</em><br>
<em>&gt;the analagous example would be), their complexity is not constant across
</em><br>
<em>&gt;possible X. Ought we then to believe in exploits? Why?
</em><br>
<p>I'm not following this, but I think the statement above should explain it.
<br>
<p><em>&gt;PS: I didn't really know what you were getting at with the stuff about
</em><br>
<em>&gt;alchemists and metaphysics. If it's important, please clarify it for me.
</em><br>
<p>The stuff about alchemists was a response to the poster who used them as an 
<br>
example of people trying to do something &quot;impossible&quot;.  I pointed out that 
<br>
the means by which they attempted to realize their goal was ineffectual, a 
<br>
failure of intelligence, but their real goal was &quot;understand nature and use 
<br>
that knowledge to become rich&quot;, a goal which is eminantly achievable with 
<br>
adequate intelligence.  There is all the difference in the world between a 
<br>
particular action not leading towards a particular goal, which happens all 
<br>
the time, and a particular utility function being non-satisfyable at any 
<br>
level of intelligence, which we have never known to happen in interesting 
<br>
cases.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For now, one point.  You have a feeling that exploits are impossible, 
<br>
no evidence that they are impossible, and no evidence that they are 
<br>
possible.  In other words, you have your feeling, and no other evidence for 
<br>
or against their possibility.  In all honesty, how often have your feelings 
<br>
which were held with this strength turned out to be wrong?  How often have 
<br>
such feelings which you held turned out to be wrong when many other truly 
<br>
brilliant people, essentially all of the people who had seriously considered 
<br>
the question, disagreed with you.  Note that you are advocating the 
<br>
non-conservative position, the position that will bring disaster if it turns 
<br>
out to be incorrect.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In such a situation, it appears to me that one would most frequently 
<br>
act optimally by asking yourself &quot;what am I not understanding&quot;, and then 
<br>
asking the people who disagree with you what it is you are not 
<br>
understanding.  Your starting assumption should be that you, not they, are 
<br>
wrong, but you should wish to understand so that you will be right and well 
<br>
informed, and in order to confirm that you are wrong in case of the small 
<br>
chance that you might actually be right.  Then, if after they have explained 
<br>
a few times, you still don't understand, you should conclude that either a) 
<br>
they are badly confused and unreasonable people, b) you just aren't capable 
<br>
of understanding them, or most probably c) you desperately need some very 
<br>
large quantity of background material before you will be ready to follow 
<br>
their argument.  Usually, for the smartest percent or two of the population, 
<br>
which you probably belong to, c will be correct, though different people 
<br>
seem to have different mental weaknesses.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;At any rate, you seem to have been doing something like that, or I 
<br>
would not have bothered explaining.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11651.html">Marc Geddes: "Geddes’s final hurrah:  Going out from SL4 ‘guns blazing’ to try to prove Objective Morality"</a>
<li><strong>Previous message:</strong> <a href="11649.html">Thomas Buckner: "Re: large search spaces don't mean magic"</a>
<li><strong>In reply to:</strong> <a href="11648.html">Daniel Radetsky: "Re: justification"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11650">[ date ]</a>
<a href="index.html#11650">[ thread ]</a>
<a href="subject.html#11650">[ subject ]</a>
<a href="author.html#11650">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:00 MST
</em></small></p>
</body>
</html>
