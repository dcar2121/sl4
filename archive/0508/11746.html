<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: On the dangers of AI</title>
<meta name="Author" content="justin corwin (outlawpoet@gmail.com)">
<meta name="Subject" content="Re: On the dangers of AI">
<meta name="Date" content="2005-08-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: On the dangers of AI</h1>
<!-- received="Wed Aug 17 01:05:32 2005" -->
<!-- isoreceived="20050817070532" -->
<!-- sent="Wed, 17 Aug 2005 00:05:29 -0700" -->
<!-- isosent="20050817070529" -->
<!-- name="justin corwin" -->
<!-- email="outlawpoet@gmail.com" -->
<!-- subject="Re: On the dangers of AI" -->
<!-- id="3ad827f3050817000572dcf284@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="4302D4B4.4090807@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> justin corwin (<a href="mailto:outlawpoet@gmail.com?Subject=Re:%20On%20the%20dangers%20of%20AI"><em>outlawpoet@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Aug 17 2005 - 01:05:29 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11747.html">Richard Loosemore: "On the dangers of AI (Phase 2)"</a>
<li><strong>Previous message:</strong> <a href="11745.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11744.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11748.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11748.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11746">[ date ]</a>
<a href="index.html#11746">[ thread ]</a>
<a href="subject.html#11746">[ subject ]</a>
<a href="author.html#11746">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 8/16/05, Richard Loosemore &lt;<a href="mailto:rpwl@lightlink.com?Subject=Re:%20On%20the%20dangers%20of%20AI">rpwl@lightlink.com</a>&gt; wrote:
<br>
<em>&gt; All I can say is that you are not talking about the issue that I raised:
</em><br>
<em>&gt;   what happens when a cognitive system is designed with a thinking part
</em><br>
<em>&gt; on top, and, driving it from underneath, a motivation part?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You've taken aim from the vantage point of pure philosophy, ignoring the
</em><br>
<em>&gt; cognitive systems perspective that I tried to introduce ... and my goal
</em><br>
<em>&gt; was to get us out of the philosophy so we could start talking practical
</em><br>
<em>&gt; details.
</em><br>
<p>Rather the opposite. I too have little patience with philosophy. I
<br>
spoke from a practical perspective. If, like you say, a sentience
<br>
deserving of the title will always choose to become more moral, but
<br>
never less moral, why do humans occasionally move from moral actions,
<br>
to immoral actions? This occurs, even to very intelligent people. It
<br>
is a fact.
<br>
<p>First, and most promising, is the idea that humans are insufficiently
<br>
intelligent to 'really know what they want'. This line of reasoning
<br>
holds that more intelligent being will tend on the whole to converge
<br>
towards niceness. This is supported weakly by local experience in
<br>
smart people being gentle and nice and understanding, and stupid
<br>
people being ignorant and hateful. Unfortunately, it's not an
<br>
unbreakable trend, because we have two very significant outliers.
<br>
<p>The first, as you may anticipate, is sociopathy. Sociopaths can be
<br>
very intelligent, and occasionally relatively effective in most of the
<br>
life tasks we would consider challenging. What can this human outlier
<br>
tell us? More intelligent sociopaths do not tend to be more moral than
<br>
unintelligent sociopaths. In fact, the more intelligent sociopaths are
<br>
simply more destructive, difficult to detect, and problematic to
<br>
contain. And this is a very very small change in human design, as is
<br>
obvious by their proximity to human baseline in all respects, save a
<br>
few(empathetic modelling, affective association, etc). Their
<br>
intelligence does not increase their moral ability, because they have
<br>
no reason to apply it in such a direction. A sociopath sees no benefit
<br>
in considering the cost of certain actions, because he lacks the
<br>
affective associations which color those actions for normal humans.
<br>
<p>The second is human organizations. Human organizations can very
<br>
loosely be considered organisms, in that they make decisions, have
<br>
reflective structure, can change themselves in a weak way, and can be
<br>
said to have goals. Do organizations with increasing ability and power
<br>
converge to altruism? Rather not. Why is this? Clearly they face the
<br>
same pressures as a very intelligent, powerful person. They have
<br>
intellectual capacity, diffuse as it is, they have 'motivational
<br>
structure' inasmuch as they must survive and achieve their goals, and
<br>
they must exist in the world of men. So why don't they increasingly
<br>
respect individuals? Well, simply, because they have no need of it. As
<br>
Mr. Wilson pointed out before, a company like GM can be seen as
<br>
maximizing simply what it wants, which is money. So if building banana
<br>
republics on near slavery is cost effective, then it is what is done.
<br>
<p>These are both human examples. An AI can be much much stranger. You
<br>
express doubt that a simple utility maximizer could generate self
<br>
improvement. This is not the case. In fact, a utility maximizer could
<br>
likely self improve much easier than a complicated inconsistent
<br>
structure like a human. Paperclip++ is a lot easier to translate into
<br>
new and interesting forms without loss. A utility maximizer is a scary
<br>
thing. You are probably imagining a program which can only think about
<br>
paperclips, and is thus uninteresting. Unfortunately, a utility
<br>
calculation 'concerning' paperclips can contain arbitrary data. You
<br>
could for example, compare two cognitive designs based on which would
<br>
produce the most paperclips. Or whether turning left at St. Georges
<br>
Street will get you to the paperclip factory faster which will allow
<br>
you to take control of the assembly line faster, which will lead to
<br>
increased certainty and control over paperclip production which will
<br>
lead to more paperclips. And so on.
<br>
<p>I dislike the paperclip example because it sounds stupid and seems to
<br>
turn people's brains off. The first example of this problem I heard,
<br>
was the Riemann Hypothesis Catastrophe. Suppose you build a giant AI,
<br>
and ask it to solve the Riemann Hypothesis, and then it promptly
<br>
disassembles the solar system to use as calculating elements, and
<br>
solves it. This is a perfectly valid chain of actions, given the sole
<br>
goal of solving the Riemann Hypothesis, is it not? When exactly, and
<br>
why, would the AI stop at any point?
<br>
<p>Humans have a complex soup of 'sort of' goals and motivations, and
<br>
have a major problem of attempting to divine other human's
<br>
motivations. So we have this lovely ability to self-decieve, to hold
<br>
inconsistent thoughts, and other such nastiness. Thus, a human can
<br>
spend fruitful time dissecting his thoughts, and derive satisfaction
<br>
from 'changing his goals' via inspection and thought. But do not make
<br>
the mistake of thinking you have actually changed your motivational
<br>
structure. You *started* with the goal of revising your stated
<br>
motivations to be more convincing, so you could live in a society of
<br>
deceptive and sincere beings. You *started* with a weak push towards
<br>
exhibiting altruism, because it's an effective strategy in near-peer
<br>
competing groups of humans. Your motivational structure already
<br>
includes all these things. Do not assume that an AI will even include
<br>
that much.
<br>
<p>&nbsp;You do mention some physical differences between good and bad in your
<br>
response to Ben. Things like &quot;low-entropy&quot;, conserving, etc. Consider
<br>
that humans are giant meatbags that need to kill plants and animals to
<br>
live(perhaps just plants, for vegetarians), and generate massive
<br>
amounts of entropy in our endeavors. It's clear that we're not the
<br>
conservative option. Allowing us to continue in our ways wastes a
<br>
great deal of entropy. (Nothing compared to stars and black holes, of
<br>
course, but we must start somewhere). The amount of information
<br>
contained in the human race could be easily stored in a much more
<br>
environmentally conscious format, some optical media, perhaps.
<br>
<p>It's not neccesary to invent an ethical dilemma, because any 'dilemma'
<br>
has to be framed in terms of motivations that the AI *already has* in
<br>
order for the issue to be even interesting to the AI.
<br>
<p>Now if the AI's job is to implement some &quot;Citizen's Bill of Rights&quot;,
<br>
or be a respectful human-like morality, or to preserve us for zoo
<br>
display, then certainly, it would have to be in a very very strange
<br>
situation to use our molecules for something else.
<br>
<p><p><p><pre>
-- 
Justin Corwin
<a href="mailto:outlawpoet@hell.com?Subject=Re:%20On%20the%20dangers%20of%20AI">outlawpoet@hell.com</a>
<a href="http://outlawpoet.blogspot.com">http://outlawpoet.blogspot.com</a>
<a href="http://www.adaptiveai.com">http://www.adaptiveai.com</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11747.html">Richard Loosemore: "On the dangers of AI (Phase 2)"</a>
<li><strong>Previous message:</strong> <a href="11745.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11744.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11748.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11748.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11746">[ date ]</a>
<a href="index.html#11746">[ thread ]</a>
<a href="subject.html#11746">[ subject ]</a>
<a href="author.html#11746">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:01 MST
</em></small></p>
</body>
</html>
