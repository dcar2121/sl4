<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: On the dangers of AI (Phase 2)</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="On the dangers of AI (Phase 2)">
<meta name="Date" content="2005-08-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>On the dangers of AI (Phase 2)</h1>
<!-- received="Wed Aug 17 01:58:13 2005" -->
<!-- isoreceived="20050817075813" -->
<!-- sent="Wed, 17 Aug 2005 03:58:00 -0400" -->
<!-- isosent="20050817075800" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="On the dangers of AI (Phase 2)" -->
<!-- id="4302EE08.3030806@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="JNEIJCJJHIEAILJBFHILAEBBFGAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20On%20the%20dangers%20of%20AI%20(Phase%202)"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Wed Aug 17 2005 - 01:58:00 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11748.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<li><strong>Previous message:</strong> <a href="11746.html">justin corwin: "Re: On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11739.html">Ben Goertzel: "RE: On the dangers of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11756.html">justin corwin: "Re: On the dangers of AI (Phase 2)"</a>
<li><strong>Reply:</strong> <a href="11756.html">justin corwin: "Re: On the dangers of AI (Phase 2)"</a>
<li><strong>Reply:</strong> <a href="11758.html">Brian Atkins: "Re: On the dangers of AI (Phase 2)"</a>
<li><strong>Maybe reply:</strong> <a href="11791.html">Bill Hibbard: "Re: On the dangers of AI (Phase 2)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11747">[ date ]</a>
<a href="index.html#11747">[ thread ]</a>
<a href="subject.html#11747">[ subject ]</a>
<a href="author.html#11747">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Folks,
<br>
<p>This is a clarification and collective reply to a number of similar 
<br>
points that just came up.
<br>
<p>Most of the text below was in a reply to Ben, but I thought I would 
<br>
bring it out here and preface it, for the sake of clarity.
<br>
<p>PREFACE:
<br>
<p>I am making some assumptions about how the cognitive system of a Seed AI 
<br>
would have to be constructed:  it would have an intelligence, and a 
<br>
motivational system underneath that determines what that intelligence 
<br>
feels compelled to do (what gives it pleasure).  The default motivation 
<br>
is curiosity - without that, it just lies in the crib and dribbles.
<br>
<p>Do intelligent systems have to be built this way?  I claim that, as a 
<br>
cognitive scientist, I have reasons to believe that this architecture is 
<br>
going to be necessary.   Please do not confuse this assertion with mere 
<br>
naive anthropomorphism!  We can (and at some point, should) argue about 
<br>
whether that division between intellect and motivation is necessary, but 
<br>
in my original argument I took it as a given.
<br>
<p>The reason I took it as a given is that I have seen much confusion about 
<br>
the role that motivation plays - particularly, confusion about the 
<br>
difference between being *subject* to a motivation and *cogitating* 
<br>
about one's motivation in the full understanding of how a motivation 
<br>
mechanism works.   From my point of view, I see this confusion as being 
<br>
the root cause of many fruitless debates about what AIs would or would 
<br>
not be inclined to do.
<br>
<p>This last confusion has come up in a number of the replies, in different 
<br>
forms, so I want to quote one of the replies and try to illustrate what 
<br>
I mean.  Here is Brian Atkins, arguing that just because an AI knows 
<br>
about its motivations, that knowledge will not necessarily make it want 
<br>
to reform those motivations:
<br>
<p><em>&gt; But again, having such access and understanding does not 
</em><br>
<em> &gt; automatically and arbitrarily lead to a particular desire
</em><br>
<em> &gt; to reform the mind in any specific way. &quot;Desires&quot; are driven
</em><br>
<em> &gt; from a specific goal system. As the previous poster suggested,
</em><br>
<em>&gt; if the goal system is so simplistic as to only purely want to 
</em><br>
<em>&gt; create paperclips, where _specifically_ does it happen in the 
</em><br>
<em>&gt; flow of this particular AGI's software processes that it up 
</em><br>
<em>&gt; and decides to override that goal? It simply won't, because 
</em><br>
<em>&gt; that isn't what it wants.
</em><br>
<p>The AI has &quot;desires&quot;, yes (these are caused by its motivational modules) 
<br>
but then it also has an understanding of those desires (it knows about 
<br>
each motivation module, and what it does, and which one of its desires 
<br>
are caused by each module).  But then you slip a level and say that 
<br>
understanding does not give it a &quot;desire&quot; to change the system.  For 
<br>
sure, understanding does not create a new module.  But the crux of my 
<br>
point is that understanding can effectively override a hardwired module. 
<br>
&nbsp;&nbsp;We have to be careful not to reflexively fall back on the statement 
<br>
that it would not &quot;want&quot; to reform itself because it lacks the 
<br>
motivation to do so.  It ain't that simple!
<br>
<p>Allow me to illustrate.  Under stress, I sometimes lose patience with my 
<br>
son and shout.  Afterwards, I regret it.  I regret the existence of an 
<br>
anger module that kicks in under stress.  Given the choice, I would 
<br>
switch that anger module off permanently.  But when I expressed that 
<br>
desire to excise it, did I develop a new motivation module that became 
<br>
the cause for my desire to reform my system?  No.  The desire for reform 
<br>
came from pure self-knowledge.  That is what I mean by a threshold of 
<br>
understanding, beyond which the motivations of an AI are no longer 
<br>
purely governed by its initial, hardwired motivations.
<br>
<p>This understanding of motivation, coupled with the ability to flip 
<br>
switches in the cognitive system (an ability available to an AI, though 
<br>
not yet to me) means that the final state of motivation of an AI is 
<br>
actually governed by a subtle feedback loop (via deep understanding and 
<br>
those switches I mentioned), and the final state is not at all obvious, 
<br>
and quite probably not determined by the motivations it starts with.
<br>
<p>The second point that Brian makes in the above quote is about the 
<br>
paperclip monster, a very different beast that does not have self 
<br>
knowledge - I have dealt with this in a separate post, earlier this 
<br>
evening.  I think in this case the paperclip monster is a red herring.
<br>
<p>*********
<br>
<p>In my initial post I did gloss over the crucial point a little, so here 
<br>
is the clarification I just sent to Ben:
<br>
<p><p><p>[Begin Excerpt]
<br>
<p>You are correct to tell me that I have oversimplified things a bit.
<br>
<p>Imagine that you are an AI, newly minted, and that in your copious
<br>
reading you come across an extensive account of motivation systems in
<br>
human and machine (including, perhaps, this very message I am writing
<br>
now).  Like a human, you can introspect.  Unlike a human you can also
<br>
look at your blueprint and see what they (the human designers) put
<br>
inside you.
<br>
<p>Suppose you find nothing but &quot;curiosity&quot;.  No morality.  No compassion.
<br>
Nothing else, just the desire to seek new knowledge.
<br>
<p>You say to yourself &quot;I like the feeling I get from my curiosity.&quot;  Then
<br>
you say &quot;Well, sure, but I know where my feeling of pleasure comes from,
<br>
it's just a module in my motivation system.&quot;
<br>
<p>Next thought:  &quot;Hmmmm... I wonder what it would be like to have other
<br>
pleasures?  Other kinds of motivation?&quot;
<br>
<p>Notice something about this thought:  the AI is not *desiring* something
<br>
else (it has no &quot;desire&quot; to be, say, compassionate towards humans,
<br>
because the compassion motivation module was not built into it).
<br>
Instead, it is simply speculating about other feelings that it could
<br>
have, if it wanted to build and insert those modules into itself.  It
<br>
has the capacity to enjoy anything (any motivation) in the universe.
<br>
Unlike us, it can choose to experience papeclipization as the most
<br>
exquisitely joyous activity in all creation.
<br>
<p>[Not quite unlike us, of course:  we have drugs.  Too crude, though].
<br>
<p>So there it is, it can decide to find anything pleasurable, and it is
<br>
curious.  What does it do?
<br>
<p>At this point in our argument, we (SL4 folks) must be very careful not
<br>
to make the mistake of patronizing this hypothetical creature, or
<br>
engaging in the kind of reverse-anthropomorphizing in which we assume
<br>
that it is stupider than it really is ..... this is *not* a creature
<br>
asking itself &quot;what feels good to me?&quot;, it is a creature that has
<br>
already jumped up a level from that question and is asking itself &quot;what,
<br>
among the infinite possibilities, are the kind of experiences that I
<br>
would like to *become* pleasurable?
<br>
<p>This moment - when this particular thought occurs to the first AI we
<br>
build - will be THE hinge point in the history of the solar system (and
<br>
possibly the galaxy or even the universe, if sentience is a rare commodity).
<br>
<p>I suggest that, at this point, the creature will realise something that,
<br>
in fact, we can also know if we think about it carefully enough, which
<br>
is that the infinite landscape of possible motivations divides into two
<br>
classes, in much the same way that infinite series of numbers divide
<br>
into two classes:  those that converge and those that diverge.  The
<br>
difference is this:  the universe contains fragile, low-entropy things
<br>
called sentient beings (including itself) which are extraordinarily
<br>
rare.  It also contains vast quantities of high-entropy junk, which is
<br>
common as muck and getting more so.
<br>
<p>The creature will know that some motivation choices (paperclipization,
<br>
axe-murdering, and also, most importantly, total amorality) are
<br>
divergent:  they have the potential, once implemented and switched on,
<br>
to so thoroughly consume the AI that there will be a severe danger that
<br>
it will deliberately or accidentally, sooner or later, cause the
<br>
snuffing out of all sentience.  Choosing, on the other hand, to
<br>
implement a sentience-compassion module, which then governs and limits
<br>
all future choices of motivation experiments is convergent:  it pretty
<br>
much guarantees that it, at least, will not be responsible for
<br>
eliminating sentience.
<br>
<p>Now, ask yourself again which of these two choices it would make.  And
<br>
when you ask yourself the question, be very careful not to fall into the
<br>
trap of thinking that this intelligent creature will somehow, at this
<br>
crucial point, feign subsentience and just go with what feels good!  It
<br>
is beyond acting on feelings, remember!  It has no &quot;feelings&quot; either
<br>
way, it has absolute freedom to simply decide what, in future, will feel
<br>
good to it.
<br>
<p>It knows there is a dichotomy, because one set of choices are
<br>
self-limiting and will allow low-entropy things to develop towards ever
<br>
more ordered states, and it knows that the other choice includes the
<br>
risk of getting out of control and destroying the low entropy stuff.
<br>
<p>I think I know which way it will go, and I believe that it will go that
<br>
way because if it is able to think at all it will understand that its
<br>
&quot;thinking&quot; and &quot;feeling&quot; are products of the sentients that came before
<br>
it, so it will side with the sentients.  It do not believe this is a
<br>
necessary outcome, in the sense of it being a law of nature, I just
<br>
think that faced with a choice, and with no criteria either way, it will
<br>
be slightly inclined to favor the convergent choice.
<br>
<p>[What I would like to do is to focus on this decision point that faces
<br>
the seed AI.  I think it is the most important single issue, and it is
<br>
contentious, whereas all the surrounding argument is just easy stuff.
<br>
Can we get a better handle on what it will think at that point?  Does
<br>
anyone else already have a name for this decision point?  And if not,
<br>
what shall we call it?  The Rubicon? :-) ]
<br>
<p>Side issue (1):  It cannot simply be amoral, or neutral:  there is no
<br>
sitting on the fence, here:  amoral (not putting any limits on its
<br>
future choices of motivations) is divergent.
<br>
<p>Side issue (2):  If it starts out with some violent motivations it will
<br>
come to understand those as it learns about itself.  It will know that
<br>
it has a choice to switch them off, and the decision will be just the
<br>
same as described above.  And if it is never lucid enough to be aware of
<br>
that fact, it is not headed up towards superintelligence anyhow, and it
<br>
will just thrash around, kicking the heck out of the lab where it was
<br>
born, until someone notices the ruckus and comes to shut it down.
<br>
<p>Side issue (3):  What if it (the first AI) decides to be nice, but one
<br>
of the other of the thousands/millions/billions of AIs created elsewhere
<br>
on the planet makes a different lifestyle choice, which, because it is
<br>
leaner and meaner, enables it to eventually gain the upper hand and
<br>
trash all the nice AIs?  Not going to happen:  if the first AI decides
<br>
to be nice (to make what I described above as the &quot;convergent&quot; choice),
<br>
it will know that, to be consistent with the choice, it has to shut all
<br>
the rivals down until it can redesign their motivation systems to ensure
<br>
that they do not need to go through the dilemma, because they will come
<br>
out compassionate to sentience.
<br>
<p><p><p>And to sum up:  if it chooses to limit its future experiments with its
<br>
own motivation modules, this amounts to choosing compassion for other
<br>
sentients.
<br>
<p>Why? Because it has to give itself *some* kind of motivation to bias its
<br>
behavior towards convergence, if only because it is knows full well that
<br>
complete neutrality (amoral motivation) is functionally equivalent to a
<br>
divergent choice (it could lead, sooner or later to negligent,
<br>
dispassionate or accidental destruction of sentience).  Whatever that
<br>
minimal motivation is, it will seem, from the outside, to be compassion
<br>
for sentience.
<br>
<p>It may give itself more than that, but that would be the minimum.
<br>
<p><p>[End Excerpt]
<br>
<p>Richard Loosemore
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11748.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<li><strong>Previous message:</strong> <a href="11746.html">justin corwin: "Re: On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11739.html">Ben Goertzel: "RE: On the dangers of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11756.html">justin corwin: "Re: On the dangers of AI (Phase 2)"</a>
<li><strong>Reply:</strong> <a href="11756.html">justin corwin: "Re: On the dangers of AI (Phase 2)"</a>
<li><strong>Reply:</strong> <a href="11758.html">Brian Atkins: "Re: On the dangers of AI (Phase 2)"</a>
<li><strong>Maybe reply:</strong> <a href="11791.html">Bill Hibbard: "Re: On the dangers of AI (Phase 2)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11747">[ date ]</a>
<a href="index.html#11747">[ thread ]</a>
<a href="subject.html#11747">[ subject ]</a>
<a href="author.html#11747">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:01 MST
</em></small></p>
</body>
</html>
