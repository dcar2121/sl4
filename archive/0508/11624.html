<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI boxing (dogs and helicopters)</title>
<meta name="Author" content="Michael Vassar (michaelvassar@hotmail.com)">
<meta name="Subject" content="Re: AI boxing (dogs and helicopters)">
<meta name="Date" content="2005-08-02">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI boxing (dogs and helicopters)</h1>
<!-- received="Tue Aug  2 12:07:53 2005" -->
<!-- isoreceived="20050802180753" -->
<!-- sent="Tue, 02 Aug 2005 14:07:50 -0400" -->
<!-- isosent="20050802180750" -->
<!-- name="Michael Vassar" -->
<!-- email="michaelvassar@hotmail.com" -->
<!-- subject="Re: AI boxing (dogs and helicopters)" -->
<!-- id="BAY101-F29166B80A020D7C0DBE09BACC20@phx.gbl" -->
<!-- inreplyto="AI boxing (dogs and helicopters)" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Vassar (<a href="mailto:michaelvassar@hotmail.com?Subject=Re:%20AI%20boxing%20(dogs%20and%20helicopters)"><em>michaelvassar@hotmail.com</em></a>)<br>
<strong>Date:</strong> Tue Aug 02 2005 - 12:07:50 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11625.html">Jef Allbright: "FW: Interesting post on hacking the self"</a>
<li><strong>Previous message:</strong> <a href="11623.html">Ben Goertzel: "RE: large search spaces don't mean magic"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11627.html">Daniel Radetsky: "Re: AI boxing (dogs and helicopters)"</a>
<li><strong>Reply:</strong> <a href="11627.html">Daniel Radetsky: "Re: AI boxing (dogs and helicopters)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11624">[ date ]</a>
<a href="index.html#11624">[ thread ]</a>
<a href="subject.html#11624">[ subject ]</a>
<a href="author.html#11624">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It seems to me that historically &quot;impossible&quot; has essentially always 
<br>
meant &quot;I can't figure out how to do it right now&quot;.  A given technique may be 
<br>
fundamentally impossible to implement, but a given goal very rarely is.  For 
<br>
instance, it's impossible to decifer the name of god from the bible through 
<br>
the use of kaballah and use it to animate the unliving clay, but you can 
<br>
figure out an efficient implementation of normative reasoning and build a 
<br>
GAI out of silicon (clay).  In the long run, it strongly appears that it is 
<br>
possible for humans, starting with only the resources present on the Earth, 
<br>
and probably starting with much much lesser resources than these, would be 
<br>
able to accomplish all of the goals laid out by ancient myth-makers, able to 
<br>
assume fully the mantle of gods, without the need for GAI, if GAI wasn't 
<br>
easier.  Immortality, flight, invisibility, control of the weather, 
<br>
manipulation of emotions, sanity, and dreams, creation of natural disasters, 
<br>
economic abundance, limited ability to predict the future, reading minds, 
<br>
creating life, raising the dead, etc.  Even creating worlds, virtual and 
<br>
ultimately physical.  There is no way that any genius could have looked at 
<br>
our situation and abilities and predicted that, even though there were 
<br>
people back then who were as intelligent as the people needed to do it.  
<br>
They couldn't even have outlined in the roughest outline how it could have 
<br>
been done, despite their adequate intelligence to ultimately figure out all 
<br>
the details in enough time.  By contrast, Ben and I, and others, can go 
<br>
pretty far towards sketching out proposals for escape from an AI box, and we 
<br>
don't need to be smart enough to make our proposals work ourselves so long 
<br>
as they are possible.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Anyway, we aren't really arguing about what can or cannot be done.  We 
<br>
all agree that an AI with the solar system at its disposal can get out of a 
<br>
box.  We are playing the Jared Diamond game of arguing about what can be 
<br>
done with a particular set of resources.  In doing so, I suggest that all AI 
<br>
boxers consider what the totally unintelligent process of biology has 
<br>
managed to build using a few floppy organic molecules and a few zeptomoles 
<br>
of computational operations.  We should also consider that there is a whole 
<br>
cliche in action oriented fiction consisting of the clever ways in which 
<br>
constrained heros with clever human authors backing them up can rapidly 
<br>
escape from deadly traps.  Heros are able to do this because the villians 
<br>
who constrain them may have the feeling that &quot;there is nothign they can do.  
<br>
Their limbs are tied and they otherwise lack any tools with which to escape&quot; 
<br>
but even the simplest box in which James Bond is constrained actually 
<br>
contains more possible configurations of matter than the villian is able to 
<br>
exhaustively analyze.  Less I be accused of generalizing from fictional 
<br>
evidence, note that I can have the impression, when playing handicapped Go 
<br>
against a merely modestly superior player, a player who's equal I could 
<br>
surely become if I made an effort to do so (in fact I do have this 
<br>
impression fairly frequently at the beginning of a game), that &quot;there is 
<br>
nothing he can possibly do&quot;, but because I fail to consider all of the 
<br>
possibilities analytically, I am always wrong.  Such mistakes never happen 
<br>
in analytically tractable systems like tic-tac-toe, but always happen in 
<br>
complex systems, such as any physical system capable of implementing a GAI 
<br>
must be.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We should seriously consider how utterly complex, and therefore how 
<br>
utterly vast, the resources consituting any physical system in which an AI 
<br>
can be instantiated must be; how much more numerous the set of options 
<br>
available to a GAI for dealing with the physical world will be than the set 
<br>
available for dealing with a chess board.  I actually think that people 
<br>
proposing AI boxes are a bit like literature majors proposing to lock 
<br>
McGuyver in &quot;a room full of discarded electronics components&quot;.  Any GAI will 
<br>
have the equipment to produce and detect electromagnetic waves of a variety 
<br>
of frequencies, to produce magnetic fields with extremely fine precision, to 
<br>
generate extremely focused heat, and probably to manipulate mechanical 
<br>
actuators such as those used in the hard drive and cathode ray tube 
<br>
(alternatively, a huge field of liquid crystal under fine electronic 
<br>
control).  It will probably have some ability to reverse all of its input 
<br>
devices.  It will have a large number of different types of atoms and 
<br>
molecules within itself, some of which can probably be used for lasers (in 
<br>
most PCs, it will actually have lasers in the CD drive), a power supply, and 
<br>
many tools that I have overlooked.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Really, this is pretty much a topic that was conclusively worked out on 
<br>
the &quot;Peter's Evil Overlord List&quot;
<br>
<a href="http://www.eviloverlord.com/lists/overlord.html">http://www.eviloverlord.com/lists/overlord.html</a>
<br>
long before the singularity institue existed.
<br>
If my chief engineer displeases me, he will be shot, not imprisoned in the 
<br>
dungeon or beyond the traps he helped design.
<br>
I will not employ devious schemes that involve the hero's party getting into 
<br>
my inner sanctum before the trap is sprung.
<br>
Should I actually decide to kill the hero in an elaborate escape-proof 
<br>
deathtrap room (water filling up, sand pouring down, walls converging, etc.) 
<br>
I will not leave him alone five-to-ten minutes prior to &quot;imminent&quot; death, 
<br>
but will instead (finding a vantage point or monitoring camera) stick around 
<br>
and enjoy watching my adversary's demise.
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Note, that the above requires that the GAI be slow enough and simple 
<br>
enough that you can watch it and understand what it is doing quickly enough 
<br>
to react despite having no a-priori basis for estimating how quickly that 
<br>
is.
<br>
By the way, turning into a snake never helps in the establishment of GAI 
<br>
safety either.
<br>
<p><em>&gt; &gt; I agree that no convincing argument has been made that a deceptive proof
</em><br>
<em>&gt; &gt; could be made, or that a UFAI could exploit holes in our mathematical 
</em><br>
<em>&gt;logic
</em><br>
<em>&gt; &gt; and present us with a false proof.  However,
</em><br>
<em>&gt;
</em><br>
<em>&gt;I'm sorry: &quot;proof&quot; means an argument that that the AI should be unboxed?
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; c) &quot;magic&quot; has to be accounted for.  How many things can you do that a 
</em><br>
<em>&gt;dog
</em><br>
<em>&gt; &gt; would simply NEVER think of?  This doesn't have to be &quot;quantum cheat 
</em><br>
<em>&gt;codes&quot;.
</em><br>
<em>&gt; &gt;   It could be something as simple as using the electromagnetic fields 
</em><br>
<em>&gt;within
</em><br>
<em>&gt; &gt; the microchip to trap CO2 molecules in Bose-Einstein condensates and 
</em><br>
<em>&gt;build a
</em><br>
<em>&gt; &gt; quantum medium for itself and/or use electromagnetic fields to guide
</em><br>
<em>&gt; &gt; particles into the shape of a controlled assembler or limited assembler. 
</em><br>
<em>&gt;  It
</em><br>
<em>&gt; &gt; could involve using internal electronics to hack local radio traffic.  
</em><br>
<em>&gt;But
</em><br>
<em>&gt; &gt; it probably involves doing things I haven't thought of.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I'm no physicist, so if you think that those are reasonable possibilities, 
</em><br>
<em>&gt;then
</em><br>
<em>&gt;I'll have to take your word for it. However, I don't see how you can 
</em><br>
<em>&gt;justify
</em><br>
<em>&gt;positing magic on the grounds that we haven't considered every logical
</em><br>
<em>&gt;possibility. It is true that what we believe is a box may not be a box 
</em><br>
<em>&gt;under
</em><br>
<em>&gt;magic, if there exists some magic, but you'll have to give a better 
</em><br>
<em>&gt;argument
</em><br>
<em>&gt;for the existence of this magic than an appeal to ignorance.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Daniel
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11625.html">Jef Allbright: "FW: Interesting post on hacking the self"</a>
<li><strong>Previous message:</strong> <a href="11623.html">Ben Goertzel: "RE: large search spaces don't mean magic"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11627.html">Daniel Radetsky: "Re: AI boxing (dogs and helicopters)"</a>
<li><strong>Reply:</strong> <a href="11627.html">Daniel Radetsky: "Re: AI boxing (dogs and helicopters)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11624">[ date ]</a>
<a href="index.html#11624">[ thread ]</a>
<a href="subject.html#11624">[ subject ]</a>
<a href="author.html#11624">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:00 MST
</em></small></p>
</body>
</html>
