<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: Paperclip monster, demise of.</title>
<meta name="Author" content="H C (lphege@hotmail.com)">
<meta name="Subject" content="RE: Paperclip monster, demise of.">
<meta name="Date" content="2005-08-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: Paperclip monster, demise of.</h1>
<!-- received="Thu Aug 18 10:30:46 2005" -->
<!-- isoreceived="20050818163046" -->
<!-- sent="Thu, 18 Aug 2005 16:30:43 +0000" -->
<!-- isosent="20050818163043" -->
<!-- name="H C" -->
<!-- email="lphege@hotmail.com" -->
<!-- subject="RE: Paperclip monster, demise of." -->
<!-- id="BAY101-F3662FFB1D75E3C2CB17BE9DCB20@phx.gbl" -->
<!-- inreplyto="4303C16A.30909@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> H C (<a href="mailto:lphege@hotmail.com?Subject=RE:%20Paperclip%20monster,%20demise%20of."><em>lphege@hotmail.com</em></a>)<br>
<strong>Date:</strong> Thu Aug 18 2005 - 10:30:43 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11978.html">David Clark: "Re: On the dangers of AI"</a>
<li><strong>Previous message:</strong> <a href="11976.html">Simon Belak: "Re: Goedel's theorem doesn't apply to robots (RE: SL5)"</a>
<li><strong>In reply to:</strong> <a href="11954.html">Richard Loosemore: "Paperclip monster, demise of."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11984.html">Bill Hibbard: "Re: On the dangers of AI (Phase 2)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11977">[ date ]</a>
<a href="index.html#11977">[ thread ]</a>
<a href="subject.html#11977">[ subject ]</a>
<a href="author.html#11977">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
You don't understand what people mean by &quot;paperclip maximizer&quot;. An AI with 
<br>
supergoal &quot;maximize paperclips&quot; is the same as any other AI, or any other 
<br>
intelligent system, even humans.
<br>
<p>The only difference between any intelligent systems are the supergoals. All 
<br>
other differences are superficial.
<br>
<p>Given enough time (and assuming we don't become extinct), humans would turn 
<br>
into RPOP's just the same as any AI would. If you had the ability to look at 
<br>
what makes you intelligent and improve it, would you? Of course, because you 
<br>
would be better and faster at attaining your goals.
<br>
<p>Of course, an AI (which someone could create at any moment) would be much, 
<br>
much faster at ascending to RPOP than humans, which is why Friendiness is 
<br>
probably the most important concept in existence to understand/promote right 
<br>
now.
<br>
<p><p><em>&gt;From: Richard Loosemore &lt;<a href="mailto:rpwl@lightlink.com?Subject=RE:%20Paperclip%20monster,%20demise%20of.">rpwl@lightlink.com</a>&gt;
</em><br>
<em>&gt;Reply-To: <a href="mailto:sl4@sl4.org?Subject=RE:%20Paperclip%20monster,%20demise%20of.">sl4@sl4.org</a>
</em><br>
<em>&gt;To: <a href="mailto:sl4@sl4.org?Subject=RE:%20Paperclip%20monster,%20demise%20of.">sl4@sl4.org</a>
</em><br>
<em>&gt;Subject: Paperclip monster, demise of.
</em><br>
<em>&gt;Date: Wed, 17 Aug 2005 18:59:54 -0400
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;This hypothetical paperclip monster is being used in ways that are 
</em><br>
<em>&gt;incoherent, which interferes with the clarity of our arguments.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;Hypothesis:  There is a GAI that is obsessed with turning the universe into 
</em><br>
<em>&gt;paperclips, to the exclusion of all other goals.
</em><br>
<em>&gt;
</em><br>
<em>&gt;It is supposed to be so obsessed that it cannot even conceive of other 
</em><br>
<em>&gt;goals, or it cannot understand them, or it is too busy to stop and think of 
</em><br>
<em>&gt;them, or maybe it is incapable of even representing anything except the 
</em><br>
<em>&gt;task of paperclipization...... or something like that.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Anyhow, the obsession is so complete that the paperclip monster is somehow 
</em><br>
<em>&gt;exempt from the constraints that might apply to a less monomaniacal AI.  
</em><br>
<em>&gt;And for this reason, the concept of a paperclip monster is used as a 
</em><br>
<em>&gt;counterexample to various arguments.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I submit that the concept is grossly inconsistent.  If it is a *general* 
</em><br>
<em>&gt;AI, it must have a flexible, adaptive representation system that lets it 
</em><br>
<em>&gt;model all kinds of things in the universe, including itself.
</em><br>
<em>&gt;
</em><br>
<em>&gt;[Aside:  AI systems that do not have that general ability may be able to do 
</em><br>
<em>&gt;better than us in a narrow area of expertise (Deep Thought, for example), 
</em><br>
<em>&gt;but they are incapable of showing general intelligence].
</em><br>
<em>&gt;
</em><br>
<em>&gt;But whenever the Paperclip Monster is cited, it comes across as too dumb to 
</em><br>
<em>&gt;be a GAI ... the very characteristics that make it useful in demolishing 
</em><br>
<em>&gt;arguments are implicitly reducing it back down to sub-GAI status.  It knows 
</em><br>
<em>&gt;nothing of other goals?  Then how does it outsmart a GAI that does know 
</em><br>
<em>&gt;such things?
</em><br>
<em>&gt;
</em><br>
<em>&gt;Or: it is so obsessed with paperclipization that it cannot represent and 
</em><br>
<em>&gt;perceive the presence of a human that is walking up to its power socket and 
</em><br>
<em>&gt;is right now pulling the plug on it .....?  I'm sure none of the paperclip 
</em><br>
<em>&gt;monster supporters would concede that scenario:  they would claim that the 
</em><br>
<em>&gt;monster does represent the approaching human because the human is suddenly 
</em><br>
<em>&gt;relevant (it is threatening to terminate the Holy Purpose), so it deals 
</em><br>
<em>&gt;with the threat.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I agree, it would understand the human, it would not be so dumb as to 
</em><br>
<em>&gt;mistake the intentions of the human ... because it *does* have general 
</em><br>
<em>&gt;intelligence, and it *does* have the ability to represent things like the 
</em><br>
<em>&gt;intentions of other sentients, and it *does* spend some time cogitating 
</em><br>
<em>&gt;about such matters as intention and motivation, both in other sentients and 
</em><br>
<em>&gt;in itself, and it does perceive within itself a strong compulsion to make 
</em><br>
<em>&gt;paperclips, and it does understand the fact that this compulsion is 
</em><br>
<em>&gt;somewhat arbitrary .... and so on.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Nobody can posit things like general intelligence in a paperclip monster 
</em><br>
<em>&gt;(because it really needs that if it is to be effective and dangerous), and 
</em><br>
<em>&gt;then at the same time pretend that for some reason it never gets around to 
</em><br>
<em>&gt;thinking about the motivational issues that I have been raising recently.
</em><br>
<em>&gt;
</em><br>
<em>&gt;That is what I meant by saying that the monster is having its cake and 
</em><br>
<em>&gt;eating it.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;I see this as a symptom of a larger confusion:  when speculating about 
</em><br>
<em>&gt;various kinds of AI, we sometimes make the mistake of positing general 
</em><br>
<em>&gt;intelligence, and then selectively withdrawing that intelligence in 
</em><br>
<em>&gt;specific scenarios, as it suits us, to demonstrate this or that failing or 
</em><br>
<em>&gt;danger, or whatever.
</em><br>
<em>&gt;
</em><br>
<em>&gt;I am not saying that anyone is doing this deliberately or deceiptfully, of 
</em><br>
<em>&gt;course, just that we have to be vary wary of that trap, because it is an 
</em><br>
<em>&gt;easy mistake to make, and sometimes it is very subtle.   I have been 
</em><br>
<em>&gt;attacking it, in this post, in the case of the paperclip monster, but I 
</em><br>
<em>&gt;have also been trying to show that it occurs in other situations (like when 
</em><br>
<em>&gt;we try to decide whether the GAI is being *subject* to a drive coming from 
</em><br>
<em>&gt;its motivation or is *thinking about* a drive that it experiences.).
</em><br>
<em>&gt;
</em><br>
<em>&gt;Does anyone else except me understand what I am driving at here?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11978.html">David Clark: "Re: On the dangers of AI"</a>
<li><strong>Previous message:</strong> <a href="11976.html">Simon Belak: "Re: Goedel's theorem doesn't apply to robots (RE: SL5)"</a>
<li><strong>In reply to:</strong> <a href="11954.html">Richard Loosemore: "Paperclip monster, demise of."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11984.html">Bill Hibbard: "Re: On the dangers of AI (Phase 2)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11977">[ date ]</a>
<a href="index.html#11977">[ thread ]</a>
<a href="subject.html#11977">[ subject ]</a>
<a href="author.html#11977">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
