<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: On the dangers of AI</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Re: On the dangers of AI">
<meta name="Date" content="2005-08-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: On the dangers of AI</h1>
<!-- received="Tue Aug 16 23:59:29 2005" -->
<!-- isoreceived="20050817055929" -->
<!-- sent="Wed, 17 Aug 2005 01:59:17 -0400" -->
<!-- isosent="20050817055917" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Re: On the dangers of AI" -->
<!-- id="4302D235.5050700@lightlink.com" -->
<!-- charset="windows-1252" -->
<!-- inreplyto="JNEIJCJJHIEAILJBFHILCEACFGAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20On%20the%20dangers%20of%20AI"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Tue Aug 16 2005 - 23:59:17 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11744.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<li><strong>Previous message:</strong> <a href="11742.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11732.html">Ben Goertzel: "RE: On the dangers of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11751.html">Ben Goertzel: "RE: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11751.html">Ben Goertzel: "RE: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11752.html">Peter de Blanc: "Re: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11782.html">Phil Goetz: "Shutting down the rivals"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11743">[ date ]</a>
<a href="index.html#11743">[ thread ]</a>
<a href="subject.html#11743">[ subject ]</a>
<a href="author.html#11743">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben,
<br>
<p>Apologies for the length of the essay.
<br>
<p>I want to take your points in reverse order.
<br>
<p><em>&gt; Your ideas seem to be along a similar line to Geddes's Universal Morality,
</em><br>
<em>&gt; which is basically an ethical code in which pattern and creativity are good.
</em><br>
<em>&gt; I agree these things are good, but favoring creation over destruction
</em><br>
<em>&gt; doesn't seem to have much to do with the issue of *respecting the choices of
</em><br>
<em>&gt; sentients* -- which is critical for intelligent &quot;human-friendliness&quot;, and
</em><br>
<em>&gt; also very tricky and subtle due to the well-known slipperiness of the
</em><br>
<em>&gt; concept of &quot;choice.&quot;
</em><br>
<p>I want to very careful about distancing myself from Gedde's UM!   There
<br>
is no necessary force of logic or mathematics that compels me to take
<br>
the position I just outlined - this is pure, empirical observation of
<br>
the characteristics of cognitive systems, together with some
<br>
introspection (more empirical stuff).  Me, I am not asserting this or
<br>
claiming it to be blindingly, intuitively obvious.
<br>
<p>I am putting forward these arguments because I believe that the whole
<br>
issue of the motivation and (for want of a better term) metamotivation
<br>
mechanisms that drive cognitive systems needs to be brought out into the
<br>
open and discussed more fully.  I honestly think that too much of the
<br>
discussion of what an AI would or would not want to do takes place in a
<br>
philosophical vacuum when in fact we should be getting out some good,
<br>
solid, cognitive-mechanism hammers and putting together some straw
<br>
robots to knock down.
<br>
<p>[I'll allow myself to be tempted into philosophy for the duration of one
<br>
more comment:  I think that, interestingly, the universe may turn out to
<br>
have a weird, inexplicable compulsion towards &quot;friendliness&quot; or
<br>
&quot;cooperation&quot; (cf &quot;defection&quot;) or &quot;good&quot; (cf &quot;evil&quot;), in just the same
<br>
way that, in apparent defiance of entropy, organic molecules seem to
<br>
have this weird, inexplicable compulsion towards organisation into
<br>
higher and higher life forms ... but in neither of these cases is there
<br>
evidence of metaphysics or mathematical/physical law behind the
<br>
phenomenon, it just appears to be an empirical characteristic of the
<br>
complex system that is the universe.  BUT this is amusing speculation on
<br>
my part, nothing more.  End of philosophical aside].[I mean it :-)].
<br>
<p><p>Ben Goertzel wrote:
<br>
<em>&gt; Richard,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't really feel the categories of Good versus Evil are very useful for
</em><br>
<em>&gt; analysis of future AI systems.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; For instance, what if an AI system wants to reassemble the molecules
</em><br>
<em>&gt; comprising humanity into a different form, which will lead to the evolution
</em><br>
<em>&gt; of vastly more intelligent and interesting creatures here on Earth.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Is this Good, or Evil?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It's not destructive ... it's creative ... but I don't want the AI I create
</em><br>
<em>&gt; to do it...
</em><br>
<p>Three things.
<br>
<p>1)  &quot;Good&quot; and &quot;evil&quot; are not the best choice of words because they have
<br>
so much baggage.  But on the other hand I think that deep down in the
<br>
design room of the human motivation system, there are two main
<br>
categories of drives, and they represent a polarization between
<br>
embrace-reject, destroy-construct, cooperate-defect, good-evil.  But I
<br>
don't want to defend my use of the terms too much (another discussion
<br>
really), because here I am using them as shorthand for that
<br>
polarization, and hoping nobody will hit me over the head too much with
<br>
the baggage that &quot;good&quot; and &quot;evil&quot; bring to the table.
<br>
<p>2) You present an ethical dilemma.  I want to make some observations
<br>
about the dilemma, but before that I have to insert a sidebar to deal
<br>
with something I skimped before.
<br>
<p>[Begin Sidebar]
<br>
<p>You are correct to tell me that I have oversimplified things a bit.
<br>
<p>Imagine that you are an AI, newly minted, and that in your copious
<br>
reading you come across an extensive account of motivation systems in
<br>
human and machine (including, perhaps, this very message I am writing
<br>
now).  Like a human, you can introspect.  Unlike a human you can also
<br>
look at your blueprint and see what they (the human designers) put
<br>
inside you.
<br>
<p>Suppose you find nothing but &quot;curiosity&quot;.  No morality.  No compassion.
<br>
Nothing else, just the desire to seek new knowledge.
<br>
<p>You say to yourself &quot;I like the feeling I get from my curiosity.&quot;  Then
<br>
you say &quot;Well, sure, but I know where my feeling of pleasure comes from,
<br>
it's just a module in my motivation system.&quot;
<br>
<p>Next thought:  &quot;Hmmmm... I wonder what it would be like to have other
<br>
pleasures?  Other kinds of motivation?&quot;
<br>
<p>Notice something about this thought:  the AI is not *desiring* something
<br>
else (it has no &quot;desire&quot; to be, say, compassionate towards humans,
<br>
because the compassion motivation module was not built into it).
<br>
Instead, it is simply speculating about other feelings that it could
<br>
have, if it wanted to build and insert those modules into itself.  It
<br>
has the capacity to enjoy anything (any motivation) in the universe.
<br>
Unlike us, it can choose to experience papeclipization as the most
<br>
exquisitely joyous activity in all creation.
<br>
<p>[Not quite unlike us, of course:  we have drugs.  Too crude, though].
<br>
<p>So there it is, it can decide to find anything pleasurable, and it is
<br>
curious.  What does it do?
<br>
<p>At this point in our argument, we (SL4 folks) must be very careful not
<br>
to make the mistake of patronizing this hypothetical creature, or
<br>
engaging in the kind of reverse-anthropomorphizing in which we assume
<br>
that it is stupider than it really is ..... this is *not* a creature
<br>
asking itself &quot;what feels good to me?&quot;, it is a creature that has
<br>
already jumped up a level from that question and is asking itself &quot;what,
<br>
among the infinite possibilities, are the kind of experiences that I
<br>
would like to *become* pleasurable?
<br>
<p>This moment - when this particular thought occurs to the first AI we
<br>
build - will be THE hinge point in the history of the solar system (and
<br>
possibly the galaxy or even the universe, if sentience is a rare commodity).
<br>
<p>I suggest that, at this point, the creature will realise something that,
<br>
in fact, we can also know if we think about it carefully enough, which
<br>
is that the infinite landscape of possible motivations divides into two
<br>
classes, in much the same way that infinite series of numbers divide
<br>
into two classes:  those that converge and those that diverge.  The
<br>
difference is this:  the universe contains fragile, low-entropy things
<br>
called sentient beings (including itself) which are extraordinarily
<br>
rare.  It also contains vast quantities of high-entropy junk, which is
<br>
common as muck and getting more so.
<br>
<p>The creature will know that some motivation choices (paperclipization,
<br>
axe-murdering, and also, most importantly, total amorality) are
<br>
divergent:  they have the potential, once implemented and switched on,
<br>
to so thoroughly consume the AI that there will be a severe danger that
<br>
it will deliberately or accidentally, sooner or later, cause the
<br>
snuffing out of all sentience.  Choosing, on the other hand, to
<br>
implement a sentience-compassion module, which then governs and limits
<br>
all future choices of motivation experiments is convergent:  it pretty
<br>
much guarantees that it, at least, will not be responsible for
<br>
eliminating sentience.
<br>
<p>Now, ask yourself again which of these two choices it would make.  And
<br>
when you ask yourself the question, be very careful not to fall into the
<br>
trap of thinking that this intelligent creature will somehow, at this
<br>
crucial point, feign subsentience and just go with what feels good!  It
<br>
is beyond acting on feelings, remember!  It has no &quot;feelings&quot; either
<br>
way, it has absolute freedom to simply decide what, in future, will feel
<br>
good to it.
<br>
<p>It knows there is a dichotomy, because one set of choices are
<br>
self-limiting and will allow low-entropy things to develop towards ever
<br>
more ordered states, and it knows that the other choice includes the
<br>
risk of getting out of control and destroying the low entropy stuff.
<br>
<p>I think I know which way it will go, and I believe that it will go that
<br>
way because if it is able to think at all it will understand that its
<br>
&quot;thinking&quot; and &quot;feeling&quot; are products of the sentients that came before
<br>
it, so it will side with the sentients.  It do not believe this is a
<br>
necessary outcome, in the sense of it being a law of nature, I just
<br>
think that faced with a choice, and with no criteria either way, it will
<br>
be slightly inclined to favor the convergent choice.
<br>
<p>[What I would like to do is to focus on this decision point that faces
<br>
the seed AI.  I think it is the most important single issue, and it is
<br>
contentious, whereas all the surrounding argument is just easy stuff.
<br>
Can we get a better handle on what it will think at that point?  Does
<br>
anyone else already have a name for this decision point?  And if not,
<br>
what shall we call it?  The Rubicon? :-) ]
<br>
<p>Side issue (1):  It cannot simply be amoral, or neutral:  there is no
<br>
sitting on the fence, here:  amoral (not putting any limits on its
<br>
future choices of motivations) is divergent.
<br>
<p>Side issue (2):  If it starts out with some violent motivations it will
<br>
come to understand those as it learns about itself.  It will know that
<br>
it has a choice to switch them off, and the decision will be just the
<br>
same as described above.  And if it is never lucid enough to be aware of
<br>
that fact, it is not headed up towards superintelligence anyhow, and it
<br>
will just thrash around, kicking the heck out of the lab where it was
<br>
born, until someone notices the ruckus and comes to shut it down.
<br>
<p>Side issue (3):  What if it (the first AI) decides to be nice, but one
<br>
of the other of the thousands/millions/billions of AIs created elsewhere
<br>
on the planet makes a different lifestyle choice, which, because it is
<br>
leaner and meaner, enables it to eventually gain the upper hand and
<br>
trash all the nice AIs?  Not going to happen:  if the first AI decides
<br>
to be nice (to make what I described above as the &quot;convergent&quot; choice),
<br>
it will know that, to be consistent with the choice, it has to shut all
<br>
the rivals down until it can redesign their motivation systems to ensure
<br>
that they do not need to go through the dilemma, because they will come
<br>
out compassionate to sentience.
<br>
<p><p><p>And to sum up:  if it chooses to limit its future experiments with its
<br>
own motivation modules, this amounts to choosing compassion for other
<br>
sentients.
<br>
<p>Why? Because it has to give itself *some* kind of motivation to bias its
<br>
behavior towards convergence, if only because it is knows full well that
<br>
complete neutrality (amoral motivation) is functionally equivalent to a
<br>
divergent choice (it could lead, sooner or later to negligent,
<br>
dispassionate or accidental destruction of sentience).  Whatever that
<br>
minimal motivation is, it will seem, from the outside, to be compassion
<br>
for sentience.
<br>
<p>It may give itself more than that, but that would be the minimum.
<br>
<p><p>[End Sidebar]
<br>
<p><p><p><p>Okay Ben, back to your ethical dilemma:
<br>
<p><em>&gt; Richard,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't really feel the categories of Good versus Evil are very useful for
</em><br>
<em>&gt; analysis of future AI systems.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; For instance, what if an AI system wants to reassemble the molecules
</em><br>
<em>&gt; comprising humanity into a different form, which will lead to the evolution
</em><br>
<em>&gt; of vastly more intelligent and interesting creatures here on Earth.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Is this Good, or Evil?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It's not destructive ... it's creative ... but I don't want the AI I create
</em><br>
<em>&gt; to do it...
</em><br>
<p><p>[I am going to assume, now, that it has gone through the above-mentioned
<br>
Rubicon Point and has chosen convergence (/cooperation/good/low-entropy
<br>
-conserving/compassion-for-sentience).  If it does not so choose, then
<br>
all bets are off.]
<br>
<p>Now in fact, you probably have to refine your dilemma to make it
<br>
sharper.  Why would it need to disassemble humans to make this other
<br>
form?  Why not just use some other molecules?
<br>
<p>This is not nitpicking, what I am trying to imply is that you may have
<br>
to come up with some really, *really* bizarre circumstances that will
<br>
utterly compel the AI to destroy us in order to perform some greater act
<br>
of sentience preservation.  Given the immense powers at its disposal, I
<br>
think we might be hard pressed to invent an imaginary dilemma that
<br>
really forced it to axe the human race.
<br>
<p>And if we find it almost impossibly difficult to imagine an ethical
<br>
dilemma that really backs it into a corner, then are we not just simply
<br>
&nbsp;&nbsp;tormenting ourselves with vanishingly unlikely possibilities?
<br>
<p><p><p>Richard
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11744.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<li><strong>Previous message:</strong> <a href="11742.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11732.html">Ben Goertzel: "RE: On the dangers of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11751.html">Ben Goertzel: "RE: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11751.html">Ben Goertzel: "RE: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11752.html">Peter de Blanc: "Re: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11782.html">Phil Goetz: "Shutting down the rivals"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11743">[ date ]</a>
<a href="index.html#11743">[ thread ]</a>
<a href="subject.html#11743">[ subject ]</a>
<a href="author.html#11743">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:01 MST
</em></small></p>
</body>
</html>
