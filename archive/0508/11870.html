<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: On Our Duty to Not Be Responsible for Artificial Minds</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: On Our Duty to Not Be Responsible for Artificial Minds">
<meta name="Date" content="2005-08-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: On Our Duty to Not Be Responsible for Artificial Minds</h1>
<!-- received="Tue Aug  9 20:02:40 2005" -->
<!-- isoreceived="20050810020240" -->
<!-- sent="Tue, 09 Aug 2005 19:02:43 -0700" -->
<!-- isosent="20050810020243" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: On Our Duty to Not Be Responsible for Artificial Minds" -->
<!-- id="42F96043.5090108@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="048901c59d48$65e6d540$9a00a8c0@markcomputer" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20On%20Our%20Duty%20to%20Not%20Be%20Responsible%20for%20Artificial%20Minds"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Tue Aug 09 2005 - 20:02:43 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11871.html">Marc Geddes: "Re: &quot;Objective&quot; Morality"</a>
<li><strong>Previous message:</strong> <a href="11869.html">Jeff Medina: "Re: [wta-talk] Re: On Our Duty to Not Be Responsible for ArtificialMinds"</a>
<li><strong>Maybe in reply to:</strong> <a href="11866.html">Eliezer S. Yudkowsky: "Re: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11872.html">Mark Walker: "Re: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<li><strong>Reply:</strong> <a href="11872.html">Mark Walker: "Re: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11870">[ date ]</a>
<a href="index.html#11870">[ thread ]</a>
<a href="subject.html#11870">[ subject ]</a>
<a href="author.html#11870">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Mark Walker wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; ----- Original Message ----- From: &quot;Eliezer S. Yudkowsky&quot;:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; Considering the relation between my parents and myself, &quot;autonomy&quot; 
</em><br>
<em>&gt;&gt; consists of my parents being able to control a small set of variables 
</em><br>
<em>&gt;&gt; in my upbringing and unable to control a much larger set of variables 
</em><br>
<em>&gt;&gt; in my cognitive design.  Not because my parents *chose* to control 
</em><br>
<em>&gt;&gt; those variables and no other, but because my parents were physically 
</em><br>
<em>&gt;&gt; and cognitively *unable* to select my genome on the basis of its 
</em><br>
<em>&gt;&gt; consequences. Furthermore, my cognitive design - fixed beyond parental 
</em><br>
<em>&gt;&gt; control - determined how I reacted to parental upbringing.  My fixed 
</em><br>
<em>&gt;&gt; cognitive design placed some variables within my parents' deliberate 
</em><br>
<em>&gt;&gt; control, in the sense that they could, by speaking English, ensure I 
</em><br>
<em>&gt;&gt; would grow up speaking English.  However, some variables that my 
</em><br>
<em>&gt;&gt; parents greatly desired to control, such as my religion, were beyond 
</em><br>
<em>&gt;&gt; the reach of their best efforts at upbringing.  It is not that they 
</em><br>
<em>&gt;&gt; chose not to control this variable but that they were incapable of 
</em><br>
<em>&gt;&gt; controlling it.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt; I can't speak for your parent's abilities, but if I were trying to bring 
</em><br>
<em>&gt; you up religious, and the fate of the universe rested on your believing 
</em><br>
<em>&gt; in God, I think I would have made sure that you did not learn to read or 
</em><br>
<em>&gt; write for a start.
</em><br>
<p>That's pretty difficult if the target is supposed to grow up into a 
<br>
conventional Orthodox Jew.
<br>
<p><em>&gt;&gt; In the case of an AI researcher we have many, many possibilities.  
</em><br>
<em>&gt;&gt; Here are some possibilities that occur to me:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; 1)  The AI researcher is fully capable of choosing between AI designs 
</em><br>
<em>&gt;&gt; on the basis of their consequences, and chooses an AI design which 
</em><br>
<em>&gt;&gt; invokes no significant moral processing within the AI.  In this case I 
</em><br>
<em>&gt;&gt; would assign moral responsibility to the AI researcher alone, for all 
</em><br>
<em>&gt;&gt; consequences good or ill; the AI itself is not a moral agent.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt; As I said, I am a consequentialist whore so I think there might be cases 
</em><br>
<em>&gt; where this is permissible. However, I think it is prima facie 
</em><br>
<em>&gt; impermissible to make persons who are not moral agents. (We are 
</em><br>
<em>&gt; imagining the AI is a person right?)
</em><br>
<p>In case (1) I'm presuming a pure Bayesian decision system or like optimization 
<br>
process, without quirks of reflectivity that lead into the human delusion of 
<br>
consciousness.  Exhibition of possibility: natural selection is an 
<br>
optimization process with cumulative pressure powerful enough to poof 
<br>
primordial soup into zebras, a feat we would call intelligent if a human 
<br>
performed it.  But natural selection is neither a person, nor sentient, as I 
<br>
currently define those terms.
<br>
<p><em>&gt;&gt; I assign full responsibility to the AI researcher for all 
</em><br>
<em>&gt;&gt; consequences, intended or unintended.  An AI researcher has a 
</em><br>
<em>&gt;&gt; responsibility to choose an AI design with predictable consequences.  
</em><br>
<em>&gt;&gt; If the AI researcher negligently uses an AI design the AI researcher 
</em><br>
<em>&gt;&gt; can't predict, the AI researcher is still fully responsible for all 
</em><br>
<em>&gt;&gt; actual consequences.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yup.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; 2)  A competent AI researcher, acting on a perceived moral 
</em><br>
<em>&gt;&gt; responsibility to create moral children, deliberately constructs an AI 
</em><br>
<em>&gt;&gt; which is tightly or loosely analogous to a human - such that the 
</em><br>
<em>&gt;&gt; initial design responds to environmental stimuli and parental 
</em><br>
<em>&gt;&gt; upbringing much as does a human, learns and argues morality much as 
</em><br>
<em>&gt;&gt; does a human, is partially unaware of its own emotions like a human, etc.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; We presume that this is a deliberate attempt to create a child of 
</em><br>
<em>&gt;&gt; humankind, a worthy participant in the story of Earth-originating life.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; In this case I would assign a mixture of moral responsibility to the 
</em><br>
<em>&gt;&gt; AI researcher and to the AI.  The AI, having been deliberately 
</em><br>
<em>&gt;&gt; constructed as a moral agent, bears responsibility for its actions.  
</em><br>
<em>&gt;&gt; If we hold the AI to account, it will understand what it means to be 
</em><br>
<em>&gt;&gt; held to account, and treat this as a moral argument in the same way we 
</em><br>
<em>&gt;&gt; do.  (In contrast a paperclip maximizer would care about human moral 
</em><br>
<em>&gt;&gt; arguments only as a sort of cognitive activity in humans that might be 
</em><br>
<em>&gt;&gt; exploited to create paperclips.)  The AI researcher is responsible for 
</em><br>
<em>&gt;&gt; all predictable consequences of &quot;constructing a humanlike moral 
</em><br>
<em>&gt;&gt; agent&quot;, including liability for child abuse if later authorities 
</em><br>
<em>&gt;&gt; determine the initial design to have been botched.  But I would not 
</em><br>
<em>&gt;&gt; say that the AI researcher is responsible for all actions of the 
</em><br>
<em>&gt;&gt; created AI, presuming that the created AI was at least as initially 
</em><br>
<em>&gt;&gt; benevolent as an average human. Deliberately creating an AI that is 
</em><br>
<em>&gt;&gt; worse than average, for example, an AI that starts out with the same 
</em><br>
<em>&gt;&gt; emotional makeup as an autistic or a serial killer, makes the AI 
</em><br>
<em>&gt;&gt; researcher liable for both child abuse and for the consequences of the 
</em><br>
<em>&gt;&gt; AI's actions.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Much like human parents must take some responsibility for their 
</em><br>
<em>&gt; children's actions. Is there a point where the AI researcher is off the 
</em><br>
<em>&gt; moral hook, like we think human parents are after their children reach a 
</em><br>
<em>&gt; certain age,
</em><br>
<p>Providing the parents didn't abuse the child so greatly as to prevent his/her 
<br>
&quot;normal&quot; human growth.
<br>
<p>I presently see two incompatible views of this point, with only a slight overlap:
<br>
<p>1)  If you create an AI that is as good as an average human and provide a 
<br>
decent upbringing, you're off the hook after it grows up.  If you tilt the 
<br>
cognitive scales so hugely in favor of kindness and love that the outcome is 
<br>
deterministic, then you have deprived the offspring of moral autonomy (a sin) 
<br>
and you are never off the hook.
<br>
<p>2)  Creating an average human, if you have the opportunity to do better, 
<br>
constitutes child abuse (a sin).  You are obligated to do better than average 
<br>
- how much better not being specified.
<br>
<p>The slight overlap between these views is creating an AI who starts off as 
<br>
good as a really good human, but not with emotions skewed so greatly toward 
<br>
niceness as to be out of the human regime.
<br>
<p><em> &gt; or is there something fundamentally different about
</em><br>
<em> &gt; creating an AI?
</em><br>
<p>*Yes*, there is something fundamentally different about creating an AI!  There 
<br>
is something *hugely* different about creating an AI!  The decisions and moral 
<br>
responsibilities are those of creating a new sentient species, not those of 
<br>
raising a child.
<br>
<p>One who seeks to create a child of humankind is a higher-order parent, faced 
<br>
with a vastly greater space of options than a human mother caring for the 
<br>
product of her inscrutable womb.  A higher-order parent must possess far more 
<br>
knowledge and far deeper understanding than a conventional human parent just 
<br>
to be in the game.  Consequently I hold a higher-order parent to far higher 
<br>
standards; higher-order parents have far greater power and, I judge, far 
<br>
stricter responsibility.  That is why, contrary to my earlier aspirations, I 
<br>
no longer seek to create a child of humankind - not this century, not if I can 
<br>
avoid it.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11871.html">Marc Geddes: "Re: &quot;Objective&quot; Morality"</a>
<li><strong>Previous message:</strong> <a href="11869.html">Jeff Medina: "Re: [wta-talk] Re: On Our Duty to Not Be Responsible for ArtificialMinds"</a>
<li><strong>Maybe in reply to:</strong> <a href="11866.html">Eliezer S. Yudkowsky: "Re: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11872.html">Mark Walker: "Re: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<li><strong>Reply:</strong> <a href="11872.html">Mark Walker: "Re: On Our Duty to Not Be Responsible for Artificial Minds"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11870">[ date ]</a>
<a href="index.html#11870">[ thread ]</a>
<a href="subject.html#11870">[ subject ]</a>
<a href="author.html#11870">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
