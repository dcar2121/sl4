<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: On the dangers of AI</title>
<meta name="Author" content="David Clark (clarkd@rccconsulting.com)">
<meta name="Subject" content="Re: On the dangers of AI">
<meta name="Date" content="2005-08-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: On the dangers of AI</h1>
<!-- received="Thu Aug 18 11:50:34 2005" -->
<!-- isoreceived="20050818175034" -->
<!-- sent="Thu, 18 Aug 2005 11:50:25 -0600" -->
<!-- isosent="20050818175025" -->
<!-- name="David Clark" -->
<!-- email="clarkd@rccconsulting.com" -->
<!-- subject="Re: On the dangers of AI" -->
<!-- id="004301c5a41d$5039cfa0$0502a8c0@rcc5" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="4303C941.6000800@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> David Clark (<a href="mailto:clarkd@rccconsulting.com?Subject=Re:%20On%20the%20dangers%20of%20AI"><em>clarkd@rccconsulting.com</em></a>)<br>
<strong>Date:</strong> Thu Aug 18 2005 - 11:50:25 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11786.html">BillK: "Re: Shutting down the rivals"</a>
<li><strong>Previous message:</strong> <a href="11784.html">H C: "RE: Paperclip monster, demise of."</a>
<li><strong>In reply to:</strong> <a href="11764.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11789.html">Russell Wallace: "Book recommendation: 'Permanence'"</a>
<li><strong>Reply:</strong> <a href="11789.html">Russell Wallace: "Book recommendation: 'Permanence'"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11785">[ date ]</a>
<a href="index.html#11785">[ thread ]</a>
<a href="subject.html#11785">[ subject ]</a>
<a href="author.html#11785">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Your arguments on goals and motivations has been a breath of fresh air.  The
<br>
most useful and interesting since I started reading the SL4 list a year ago.
<br>
<p><em>&gt; The creature will know that some motivation choices (paperclipization,
</em><br>
<em>&gt; axe-murdering, and also, most importantly, total amorality) are
</em><br>
<em>&gt; divergent: they have the potential, once implemented and switched on,
</em><br>
<em>&gt; to so thoroughly consume the AI that there will be a severe danger that
</em><br>
<em>&gt; it will deliberately or accidentally, sooner or later, cause the
</em><br>
<em>&gt; snuffing out of all sentience. Choosing, on the other hand, to
</em><br>
<em>&gt; implement a sentience-compassion module, which then governs and limits
</em><br>
<em>&gt; all future choices of motivation experiments is convergent: it pretty
</em><br>
<em>&gt; much guarantees that it, at least, will not be responsible for
</em><br>
<em>&gt; eliminating sentience.
</em><br>
<p>I didn't quite understand what the divergence and convergence of your
<br>
hypothesis meant exactly.  The divergence seems to be bad and convergence
<br>
good but I don't exactly get what is converging?  Are these words just tags
<br>
from converging/diverging numbers and have no special meaning?  Why would
<br>
harmful motivations to humans be any more *consuming* than any other
<br>
motivations?
<br>
<p>Out of the infinite set of goals, why would preservation of sentient beings
<br>
be a good thing from the AI's point of view?  Let's say that the AI killed
<br>
off all sentient life and sometime in the future wanted to study what he had
<br>
annihilated, couldn't he just create either simulations or real sentient
<br>
beings and then obverse them as he pleased?  What universal something would
<br>
give preserving sentient life (in it's present form) a positive number
<br>
versus any other?
<br>
<p><em>&gt; I think I know which way it will go, and I believe that it will go that
</em><br>
<em>&gt; way because if it is able to think at all it will understand that its
</em><br>
<em>&gt; &quot;thinking&quot; and &quot;feeling&quot; are products of the sentient that came before
</em><br>
<em>&gt; it, so it will side with the sentient.
</em><br>
<p>Let's say that it knew that it was originally created from humans.  Why
<br>
would the AI give this any value?  Have human children not killed their
<br>
parents?  What universal value (not inserted by humans) would make the AI
<br>
give credit for being it's original creator?
<br>
<p><em>&gt; I do not believe this is a
</em><br>
<em>&gt; necessary outcome, in the sense of it being a law of nature, I just
</em><br>
<em>&gt; think that faced with a choice, and with no criteria either way, it will
</em><br>
<em>&gt; be slightly inclined to favor the convergent choice.
</em><br>
<p>Why?  Eliezer says that if you can't guarantee that this positive choice
<br>
will result  then you must assume the worst case scenario and make sure the
<br>
AI can't decide otherwise.  Because the outcome of humans all being
<br>
annihilated is so unthinkable , is anything other than absolute certainty in
<br>
the survival of humanity enough?
<br>
<p><em>&gt; I think that, interestingly, the universe may turn out to
</em><br>
<em>&gt; have a weird, inexplicable compulsion towards &quot;friendliness&quot; or
</em><br>
<em>&gt; &quot;cooperation&quot; (cf &quot;defection&quot;) or &quot;good&quot; (cf &quot;evil&quot;), in just the same
</em><br>
<em>&gt; way that, in apparent defiance of entropy, organic molecules seem to
</em><br>
<em>&gt; have this weird, inexplicable compulsion towards organisation into
</em><br>
<em>&gt; higher and higher life forms .
</em><br>
<p>Life seems to evolve into organisms with less entropy because more
<br>
organization happens to produce a more *fit* or selected organism by nature.
<br>
How does this apply to motivational modules in an AI?  Are you saying that
<br>
amoral goals will be negatively selected by some higher goal selector or
<br>
will it be discarded by some higher universal selector?  I don't understand
<br>
how these two processes are related. (Life being attracted to higher forms
<br>
of organization and an AI motivational system weighting *friendliness*
<br>
higher than it's opposite.)
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11786.html">BillK: "Re: Shutting down the rivals"</a>
<li><strong>Previous message:</strong> <a href="11784.html">H C: "RE: Paperclip monster, demise of."</a>
<li><strong>In reply to:</strong> <a href="11764.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11789.html">Russell Wallace: "Book recommendation: 'Permanence'"</a>
<li><strong>Reply:</strong> <a href="11789.html">Russell Wallace: "Book recommendation: 'Permanence'"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11785">[ date ]</a>
<a href="index.html#11785">[ thread ]</a>
<a href="subject.html#11785">[ subject ]</a>
<a href="author.html#11785">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:01 MST
</em></small></p>
</body>
</html>
