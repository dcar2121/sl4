<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: 6 points about Coherent Extrapolated Volition</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: 6 points about Coherent Extrapolated Volition">
<meta name="Date" content="2005-08-06">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: 6 points about Coherent Extrapolated Volition</h1>
<!-- received="Sat Aug  6 20:02:06 2005" -->
<!-- isoreceived="20050807020206" -->
<!-- sent="Sat, 06 Aug 2005 19:02:03 -0700" -->
<!-- isosent="20050807020203" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: 6 points about Coherent Extrapolated Volition" -->
<!-- id="42F56B9B.6020502@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="42E45A23.7060407@intelligence.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%206%20points%20about%20Coherent%20Extrapolated%20Volition"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Aug 06 2005 - 20:02:03 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11667.html">Jef Allbright: "Re: 6 points about Coherent Extrapolated Volition"</a>
<li><strong>Previous message:</strong> <a href="11665.html">William Chapin: "Re: Hiroshima Day"</a>
<li><strong>In reply to:</strong> <a href="../0507/11569.html">Michael Anissimov: "Re: 6 points about Coherent Extrapolated Volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11667.html">Jef Allbright: "Re: 6 points about Coherent Extrapolated Volition"</a>
<li><strong>Reply:</strong> <a href="11667.html">Jef Allbright: "Re: 6 points about Coherent Extrapolated Volition"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11666">[ date ]</a>
<a href="index.html#11666">[ thread ]</a>
<a href="subject.html#11666">[ subject ]</a>
<a href="author.html#11666">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michael Anissimov wrote:
<br>
<em>&gt; Hi Eliezer,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Few quick questions on the CEV post - notice that you've turned 
</em><br>
<em>&gt; &quot;Collective Extrapolated Volition&quot; into &quot;Coherent Extrapolated Volition&quot; 
</em><br>
<em>&gt; here - is this a permanent jargon change or are you just using the term 
</em><br>
<em>&gt; &quot;coherent&quot; to make some sort of point in this context?  Please explain.
</em><br>
<p>I think it will be a permanent jargon change, though perhaps not a final 
<br>
equilibrium; who knows but that there may be more in store.
<br>
<p><em>&gt; Eliezer S. Yudkowsky wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; 3.  The CEV writes an AI.  This AI may or may not work in any way 
</em><br>
<em>&gt;&gt; remotely resembling a volition-extrapolator.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ...though it's extremely likely it would, right?
</em><br>
<p>What?  No.  Possible, sure.  Where would be the justification for calling it 
<br>
'extremely likely'?  Asked what we want at the object-level, we may or may not 
<br>
want anything that treats with our wants at the meta-level.
<br>
<p><em>&gt; In the broadest sense, 
</em><br>
<em>&gt; &quot;volition extrapolation&quot; basically means &quot;guessing what people want&quot;, 
</em><br>
<em>&gt; right?
</em><br>
<p>Yes.
<br>
<p><em>&gt;&gt; 4.  The CEV returns one coherent answer.  The AI it returns may or may
</em><br>
<em>&gt;&gt; not display any given sort of coherence in how it treats different
</em><br>
<em>&gt;&gt; people, or create any given sort of coherent world.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Of course, if it doesn't display any sort of coherence in how it treats 
</em><br>
<em>&gt; different people, or doesn't create any sort of coherent world, that 
</em><br>
<em>&gt; would be a failure, right?
</em><br>
<p>This is only a licensable inference because many of our goals require 
<br>
coherence; not because coherence is a goal in itself.  Survival implies at 
<br>
least local continuity between past and future selves; challenge, success, and 
<br>
fun implies at least local continuity between past and future worlds.
<br>
<p>I think I would personally prefer that roughly the same thing happen to the 
<br>
whole human species, so that we are not split to go one way and another never 
<br>
to meet again.  But perhaps that will prove to be only a personal preference 
<br>
on my part, or only a transient delusion of morality.
<br>
<p><em>&gt; Is this statement being put forth to help 
</em><br>
<em>&gt; people distinguish the difference between the CEV and the AI it creates?
</em><br>
<p>Yes.
<br>
<p><em>&gt;&gt; 5.  The CEV runs for five minutes before producing an output.  It is
</em><br>
<em>&gt;&gt; not meant to govern for centuries.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Though of course, there could be substantial mutual information between 
</em><br>
<em>&gt; the CEV and the AI it creates - correct?
</em><br>
<p>Mutual information in the Shannon sense?  Absolutely!  Of course!
<br>
<p>On the other hand, I'd be really disturbed to see sections of code copied 
<br>
verbatim.  I would regard this as prima facie evidence of malfunction.
<br>
<p><em>&gt; Though such an AI (nor the CEV 
</em><br>
<em>&gt; which created it) would not &quot;govern&quot; in the anthropomorphic sense, it 
</em><br>
<em>&gt; would surely exert optimization pressure upon the world.  There are 
</em><br>
<em>&gt; probably some people out there who feel infinitely uncomfortable
</em><br>
<p>Wow, how does their brain pack in an infinite amount of uncomfort?  Up until 
<br>
this point I'd been an infinite set atheist, on the grounds that no reliable 
<br>
witness has ever reported encountering an infinite set.
<br>
<p><em>&gt; with 
</em><br>
<em>&gt; the idea of a superintelligent AI with initial conditions set by a human 
</em><br>
<em>&gt; programming team creating changes in the world, and will hence object to 
</em><br>
<em>&gt; any such proposals, but of course it seems like this event is basically 
</em><br>
<em>&gt; unavoidable...  I think it's important to distinguish between people who 
</em><br>
<em>&gt; are objecting to *any* FAI theory on the grounds that they haven't come 
</em><br>
<em>&gt; to terms with the reality of recursive self-improvement yet, and people 
</em><br>
<em>&gt; who have already accepted that superintelligent AI will eventually come 
</em><br>
<em>&gt; into existence whether we like it or not, and that it's merely our duty 
</em><br>
<em>&gt; to set the initial conditions as best we can.  It's sometimes difficult 
</em><br>
<em>&gt; to tell the difference between the two, people because it seems like 
</em><br>
<em>&gt; people in group #1 may occasionally pretend to be in group #2 for the 
</em><br>
<em>&gt; sake of argument (which ends up going nowhere).
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; 6.  The CEV by itself does not mess around with your life.  The CEV
</em><br>
<em>&gt;&gt; just decides which AI to replace itself with.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ...but the CEV isn't explicitly being programmed to create an AI output 
</em><br>
<em>&gt; - aye?
</em><br>
<p>Pretty much, although I may have to make certain assumptions about the class 
<br>
of thingydingies wo which the output belongs, in order to create a clearly 
<br>
defined CEV computation producing the output.  For example, one might require 
<br>
that the output be a computer program placed in charge of the existing RPOP 
<br>
infrastructure.  That computer program could be an AI; or it could clean up 
<br>
the infrastructure and delete itself; or it could execute a predefined set of 
<br>
actions and then clean up and delete itself.
<br>
<p><em>&gt; The AI output is based on the assumption that our wish if we 
</em><br>
<em>&gt; knew more, thought faster, were more the people we wished we were, had 
</em><br>
<em>&gt; grown up farther together; where the extrapolation converges rather than 
</em><br>
<em>&gt; diverges, where our wishes cohere rather than interfere; extrapolated as 
</em><br>
<em>&gt; we wish that extrapolated, interpreted as we wish that interpreted, we 
</em><br>
<em>&gt; would decide to construct an AI that exerts a sort of optimizing 
</em><br>
<em>&gt; pressure on the world such that it makes it a better place to live?
</em><br>
<p>No, that's exactly the sort of assumption you don't want to build into CEV. 
<br>
That's CEV as Nice Place To Live, which is a strong assumption about the sort 
<br>
of world humanity would enjoy inhabiting; quite distinct from CEV as Initial 
<br>
Dynamic.  For example, in the original &quot;Collective Volition&quot; I suggested that 
<br>
we might coherently extrapolatedly wish the CEV output to create a small set 
<br>
of understandable background rules for our new world, and &quot;hands off!&quot; for 
<br>
individual destinies.
<br>
<p><em>&gt; I 
</em><br>
<em>&gt; would agree with this assumption - I just think it's worthwhile to point 
</em><br>
<em>&gt; out explicitly for the sake of clarity.  Theoretically, the (extremely 
</em><br>
<em>&gt; improbable) output of CEV could merely be a single object, like a 
</em><br>
<em>&gt; banana, or something along those lines, yes?
</em><br>
<p>If a banana belongs to the class of possible outputs, then you're allowing the 
<br>
CEV to construct arbitrary physical devices as its output, rather than writing 
<br>
arbitrary computer programs.  That requires dynamic action and planning by CEV 
<br>
in the real world, in the process of producing its first-order output, which 
<br>
therefore occurs before the CEV's replacement by its first-order output.
<br>
<p>Perhaps the (extremely improbable) output of CEV would be a program that uses 
<br>
SI infrastructure to construct one banana, and then cleans up the SI 
<br>
infrastructure and thereby deletes itself.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11667.html">Jef Allbright: "Re: 6 points about Coherent Extrapolated Volition"</a>
<li><strong>Previous message:</strong> <a href="11665.html">William Chapin: "Re: Hiroshima Day"</a>
<li><strong>In reply to:</strong> <a href="../0507/11569.html">Michael Anissimov: "Re: 6 points about Coherent Extrapolated Volition"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11667.html">Jef Allbright: "Re: 6 points about Coherent Extrapolated Volition"</a>
<li><strong>Reply:</strong> <a href="11667.html">Jef Allbright: "Re: 6 points about Coherent Extrapolated Volition"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11666">[ date ]</a>
<a href="index.html#11666">[ thread ]</a>
<a href="subject.html#11666">[ subject ]</a>
<a href="author.html#11666">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:00 MST
</em></small></p>
</body>
</html>
