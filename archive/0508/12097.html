<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Complexity tells us to maybe not fear UFAI</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="Re: Complexity tells us to maybe not fear UFAI">
<meta name="Date" content="2005-08-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Complexity tells us to maybe not fear UFAI</h1>
<!-- received="Wed Aug 24 16:33:30 2005" -->
<!-- isoreceived="20050824223330" -->
<!-- sent="Wed, 24 Aug 2005 23:33:27 +0100 (BST)" -->
<!-- isosent="20050824223327" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="Re: Complexity tells us to maybe not fear UFAI" -->
<!-- id="20050824223327.75388.qmail@web26701.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20050824210227.95727.qmail@web54506.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20Complexity%20tells%20us%20to%20maybe%20not%20fear%20UFAI"><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Wed Aug 24 2005 - 16:33:27 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12098.html">Wei Dai: "uncertainty in mathematics"</a>
<li><strong>Previous message:</strong> <a href="12096.html">Thomas Buckner: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>In reply to:</strong> <a href="12095.html">Phil Goetz: "Complexity tells us to maybe not fear UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12101.html">Chris Paget: "Re: Complexity tells us to maybe not fear UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12097">[ date ]</a>
<a href="index.html#12097">[ thread ]</a>
<a href="subject.html#12097">[ subject ]</a>
<a href="author.html#12097">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; By limiting the computational power available to an AI to be
</em><br>
<em>&gt; one or two orders of magnitude less than that available to a
</em><br>
<em>&gt; human, we can guarantee that it won't outthink us - or, if it
</em><br>
<em>&gt; does, it will do so very, very slowly.
</em><br>
<p>AGIs have /many/ potential advantages over the brain, including
<br>
much less pressure to parallelise (with its corresponding
<br>
inefficiencies in problems that aren't embarassingly parallel),
<br>
reliability (removing the need for redundancy to achieve
<br>
accuracy) and much better reconfigurability. Plus to be fair
<br>
you'd have to count transistors, or the number of transistors
<br>
you'd need to simulate a neuron, not von-neumann ops/second (to
<br>
allow for the fact that the brain's hardware is mostly special
<br>
purpose).
<br>
<p>If you'd said 'nine or ten orders of magnitude', counting raw
<br>
FLOPs, this would be more reasonable but not terribly useful.
<br>
No one here is deliberately proposing to keep an AGI infrahuman,
<br>
and we've already been over how you can't prove that an AGI will
<br>
be safe when scaled to superhuman intelligence by performing
<br>
black box experiments on an infrahuman precursor. White box
<br>
experiments may help if the design is non-opaque and the reseacher
<br>
generally knows what they're doing. The problem here, aside from
<br>
the general difficultly of designing experiments that will
<br>
actually prove the scalable Friendliness of the design to a high
<br>
degree of confidence, is that most AGI designs tend to need a lot
<br>
more compute power at the start to get things rolling than they
<br>
do once reasonably efficient learned behaviours are in place. It's
<br>
entirely possible to write a throttleable AI driven by automatic
<br>
and/or manual assessment of the rate of progress, but that's
<br>
getting into takeoff prevention and layered safety architectures
<br>
beyond the (initially) charming simplicity of your original
<br>
proposal.
<br>
<p>In any case, it's not a reason not to be afraid of UFAI in
<br>
general, as regardless of whether you think this is a good
<br>
idea there are plenty of people out there who are going to
<br>
throw as much computing power as they can get their hands on
<br>
at their best-guess AGI architecture.
<br>
<p><em>&gt; but I don't think any algorithm will be found for general
</em><br>
<em>&gt; intelligence that doesn't have the property that exponential
</em><br>
<em>&gt; increases in resources are needed for a linear increase
</em><br>
<em>&gt; of some IQ-like measure.
</em><br>
<p>You could mean either of two things, that it takes exponentially
<br>
more resources to /run/ a more intelligent AI, or that it takes
<br>
exponentially more resources to /design/ a more intelligent AI.
<br>
<p>The first argument certainly doesn't hold for biological
<br>
intelligence. It doesn't take an exponentially greater amount
<br>
of brain tissue for a human to achieve an IQ score vastly in
<br>
excess of a chimps, nor does a chimp have a brain exponentially
<br>
larger than that of a wolf. I would say that the scaling is
<br>
better than linear, though my point is just that it's a lot
<br>
better than exponential. Does it take exponentially
<br>
more mass, or more DNA, or more cells to make a higher fitness
<br>
organism in general? Does building a supersonic fighter jet
<br>
instead of a subsonic one take exponentially more anything?
<br>
<p>The answer is no, because until you reach physical limits
<br>
performance is a question of organisation rather than raw
<br>
resources. The question of the difficultly of design scales
<br>
with intelligence isn't so clear cut, and indeed I asked it
<br>
myself in my first post to this list. There isn't a simple
<br>
answer to this one; I think the factor is a lot less that
<br>
exponential, but I don't have a concise argument I can use
<br>
to convince you. However I would note that once again
<br>
biological evolution sets an opposing precedent; if you plot
<br>
the intelligence of the brightest creature on the planet
<br>
over time you will have a graph that shows an exponential
<br>
/increase/, shooting up dramatically in the last million
<br>
years, despite steadily increasing generation times as brains
<br>
get bigger. Indeed this graph looks suspiciously like that of
<br>
the Singularity theory, extending back into time instead of
<br>
forwards.
<br>
<p><em>&gt; If the AI gets out and is able to harness the computational
</em><br>
<em>&gt; power on the internet, that would be different.  But within
</em><br>
<em>&gt; its box, it's going to remain at or less than the order of
</em><br>
<em>&gt; magnitude of intelligence dictated by its computational
</em><br>
<em>&gt; capacity.
</em><br>
<p>Better make sure it's an air gap, as human software security
<br>
will look pathetic to an AI that understands the causal
<br>
structure of computer programs as easily as we understand
<br>
navigation in 3 dimensional space.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p><p><p><p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
Yahoo! Messenger - NEW crystal clear PC to PC calling worldwide with voicemail <a href="http://uk.messenger.yahoo.com">http://uk.messenger.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12098.html">Wei Dai: "uncertainty in mathematics"</a>
<li><strong>Previous message:</strong> <a href="12096.html">Thomas Buckner: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>In reply to:</strong> <a href="12095.html">Phil Goetz: "Complexity tells us to maybe not fear UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12101.html">Chris Paget: "Re: Complexity tells us to maybe not fear UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12097">[ date ]</a>
<a href="index.html#12097">[ thread ]</a>
<a href="subject.html#12097">[ subject ]</a>
<a href="author.html#12097">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
