<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Paperclip monster, demise of.</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="Re: Paperclip monster, demise of.">
<meta name="Date" content="2005-08-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Paperclip monster, demise of.</h1>
<!-- received="Thu Aug 18 00:09:25 2005" -->
<!-- isoreceived="20050818060925" -->
<!-- sent="Thu, 18 Aug 2005 07:09:21 +0100 (BST)" -->
<!-- isosent="20050818060921" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="Re: Paperclip monster, demise of." -->
<!-- id="20050818060921.83875.qmail@web26703.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="430408AE.70200@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20Paperclip%20monster,%20demise%20of."><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Thu Aug 18 2005 - 00:09:21 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11970.html">Mikko Särelä: "Re: Paperclip monster, demise of."</a>
<li><strong>Previous message:</strong> <a href="11968.html">Michael Roy Ames: "Re: Paperclip monster, demise of."</a>
<li><strong>In reply to:</strong> <a href="11963.html">Richard Loosemore: "Re: Paperclip monster, demise of."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11961.html">Michael Roy Ames: "Re: Paperclip monster, demise of."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11969">[ date ]</a>
<a href="index.html#11969">[ thread ]</a>
<a href="subject.html#11969">[ subject ]</a>
<a href="author.html#11969">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Richard Loosemore wrote:
<br>
<em>&gt; Your comment above, about my not understanding &quot;how reasoning about
</em><br>
<em>&gt; goals works when the goals are all open to reflective examination and
</em><br>
<em>&gt; modification&quot; and the other comments about &quot;goal systems&quot; that appear in 
</em><br>
<em>&gt; your reply, all come from a very particular, narrow conception of how an 
</em><br>
<em>&gt; AI should be structured.
</em><br>
<p>Correct. They come from a examination of how a normative AI would be
<br>
structed; an AI that contains no unnecessary opacity or counterproductive
<br>
internal inconsistencies, and that reasons using probabilistic logic or
<br>
appropriate approximations thereof. 
<br>
<p><em>&gt; How can I characterize it?  It is a symbolic-AI, goal-hierarchical,
</em><br>
<em>&gt; planning-system approach to AI that is a direct descendant of good
</em><br>
<em>&gt; old Winograd et al.
</em><br>
<p>Symbolic, no. The fatal flaws of 'Good Old Fashioned AI' are often
<br>
mentioned by the SIAI (from 'Creating a Transhuman AI' onwards) and
<br>
indeed most of the serious AGI researchers on this list. There is some 
<br>
disagreement as to what they are and how to correct them, but I don't
<br>
think anyone is advocating SOAR and its bretheren as a model for an AGI.
<br>
<p>Goal-hierarchical, yes. Goal-hierachical systems behave in relatively
<br>
predictable ways, including having relatively predictable forms of goal
<br>
system stability. They are also efficient to evaluate and effective as
<br>
reality-optimisers. The term 'causally clean' denotes a superset of
<br>
AGIs that includes 'goal-hierarchical' in the classic sense of a tree
<br>
of goals and implied subgoals. Systems that are not causally clean are
<br>
extremely difficult to predict once capable of self-modification,
<br>
subject to counterproductive goal inteference and 'subgoal stomps', and
<br>
as such unsuitable as a basis for Friendly seed AI. It is arguable that
<br>
non-'goal-hierarchical' systems will inevitably self-modify into such
<br>
systems; humans are more goal-hierarchical than animals and you yourself
<br>
pointed out that you'd prefer to self-modify to have a more consistent
<br>
goal system. However this is an unproven hypothesis at this time.
<br>
<p>'Planning-system', not particularly. Plans are clearly highly useful,
<br>
but they are not always required nor are they the be-all and end-all
<br>
of inference. Again I don't know of any AGI researchers here who are
<br>
working on anything like a classic 'planning system'.
<br>
<p><em>&gt; Just for the record, I know perfectly well what kind of goal system you 
</em><br>
<em>&gt; are referring to. (I have written such systems, and taught postgrads 
</em><br>
<em>&gt; how to write them).
</em><br>
<p>You've written systems that choose actions based on expected utility,
<br>
as evaluated by a utility function fed by probabilistic inference?
<br>
<p><em>&gt; But I have also just spent 6000 words trying to communicate to
</em><br>
<em>&gt; various posters that the world of AI research has moved on a little
</em><br>
<em>&gt; since the early 1980s, and that there are now some very much more
</em><br>
<em>&gt; subtle kinds of motivational systems out there.
</em><br>
<p>Most of which are pointless inefficiency and obfuscation, or simply
<br>
don't work at all.
<br>
<p><em>&gt; I guess I made the mistake, from the outset, of assuming that the
</em><br>
<em>&gt; level of sophistication here was not just deep, but also broad.
</em><br>
<p>On the contrary, there are plenty of people with deep knowledge of
<br>
the various approaches to AI that are or have been popular in
<br>
academia. I personally am a tireless advocate of studying the
<br>
mistakes of the past in order to learn from them and avoid being so
<br>
foolish in the future. Pointless obfuscation of the goal system is
<br>
a classic one.
<br>
<p><em>&gt; Do you know about the difference between (1)  quasi-deterministic 
</em><br>
<em>&gt; symbol-system that keeps stacks of goals, (2) a Complex assemblage of 
</em><br>
<em>&gt; neurons, (3) Complex systems with partly symbolic, partly neuron-like 
</em><br>
<em>&gt; properties?
</em><br>
<p>I can't speak for anyone else, but I've critiqued all of these at
<br>
length (and the folly of opaque, emergence-based approaches to AI in
<br>
general). Symbolic systems may be dysfunctional, but they at least
<br>
tend to have a clear idea of how their functional components are
<br>
supposed to contribute to intelligence. This property tends to
<br>
dissapear the futher one goes down the connectionist spectrum. The
<br>
limited successes of connectionism can largely be attributed to the
<br>
fact that having no idea of how intelligence works at a global
<br>
level is often better than having an actively wrong idea of how
<br>
intelligence works.
<br>
<p><em>&gt; Do you understand the distinction between a set of goals and a set
</em><br>
<em>&gt; of motivations
</em><br>
<p>Only if you're talking about humans. 'Motivation' is not a well-defined
<br>
term in AI (hell, 'goal' doesn't have a rigorous consensus definition,
<br>
but it's a lot better than 'motivation'). By comparison 'expected
<br>
utility' is such a rigorous term.
<br>
<p><em>&gt; About the way that motivational systems can be the result of 
</em><br>
<em>&gt; interacting, tangled mechanisms that allow the entire system to be 
</em><br>
<em>&gt; sensitive to small influences, rendering it virtually non-deterministic?
</em><br>
<p>Of course they /can/ be; it's obvious that humans use a motivational
<br>
system of this type. You were arguing that general intelligence /must/
<br>
be like this, which is simply incorrect ('complete nonsense', to borrow
<br>
your phrase). The argument that I am making is that such a nondeterministic
<br>
system will (almost) inevitably fall into a stable, deterministic attractor
<br>
after some time spent traversing the space of possible goal systems via
<br>
self-modification.
<br>
&nbsp;
<br>
<em>&gt;&gt; Whether a system will actually 'think about' any given subjunctive goal
</em><br>
<em>&gt;&gt; system depends on whether its existing goal system makes it desireable
</em><br>
<em>&gt;&gt; to do so. 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; In general, an AI could use a goal system and motivation system that
</em><br>
<em>&gt; caused it to shoot off and consider all kinds of goals,
</em><br>
<p>Goal systems 'cause' an AI to do desireable things, pretty much by
<br>
definition. If it is considering something, either the act of consideration
<br>
is desireable or the action was generated by some internal process that is
<br>
not strongly linked to the goal system, but was set in motion because doing
<br>
so was considered a desireable action as a whole (a common term for such
<br>
processes is 'subdeliberative'). The goal or action does not have to be
<br>
'desireable' to be eligable for consideration, only the act of considering
<br>
it. But it must be 'desireable' to be eligable for implementation. If you
<br>
have any causation going on in your AGI that isn't the result of the goal
<br>
system, then you have build an unstable, unreliable and inefficient system
<br>
for no good reason.
<br>
<p><em>&gt; The AI says &quot;I could insert inside myself ANY motivation module in the
</em><br>
<em>&gt; universe, today.  Think I'll toss a coin [tosses coin]: Now I am
</em><br>
<em>&gt; passionately devoted to collecting crimson brocade furniture, and my
</em><br>
<em>&gt; goal for today is to get down to that wonderful antique store over in
</em><br>
<em>&gt; Chelsea.&quot;
</em><br>
<p>Where did the desire to toss a coin and create a random goal come from?
<br>
Alternately, /why/ did the AI create a random goal? What causal process
<br>
selected and initiated this action? If you have an answer to that, please
<br>
explain why you would possibly want to build an AI that used such a
<br>
process?
<br>
<p><em>&gt; Guess what, it *really* wants to do this, and it genuinely adopted the
</em><br>
<em>&gt; antique-hunting goal, but are you going to say that its previous goal
</em><br>
<em>&gt; system made it desirable to do so?
</em><br>
<p>You could have simply hardcoded a random goal generator into your AGI
<br>
independently from the structure that you have labeled the 'goal system'.
<br>
I would say that the so-called 'goal system' is now a goal system in
<br>
name only, as the effective goal system of the AI (the root sources of
<br>
cognitive causation that we would specify if we were modelling your AI
<br>
as a generally intelligent agent) now include the non-explicit 'insert
<br>
random goals' goal.
<br>
<p><em>&gt; That this new motivation/goal was actually determined by the previous
</em><br>
<em>&gt; goal, which was something like pure curiosity? The hell it did!
</em><br>
<p>/You/ are specifying the AI, /you/ are specifying what determines what
<br>
in the design. Since in this universe events have causes, something
<br>
caused the goal to be created. If you have implemented a 'curiosity'
<br>
goal that tries to infer what a version of the AI with new goal X
<br>
would do by self-modification, then so be it. If you have implemented
<br>
a bit of code that randomises the goal system without itself being
<br>
explicitly represented as a goal (going against your own 'AGI
<br>
understands its own entire codebase' statement earlier), then you
<br>
have created an implicit goal rather than an explicit one and gained
<br>
nothing but a new source of potential confusion.
<br>
<p><em>&gt;&gt; A goal system is simply a compact function defining a preference
</em><br>
<em>&gt;&gt; order over actions, or universe states, or something else that can
</em><br>
<em>&gt;&gt; be used to rank actions.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; A goal system *is* a compact function defining a preference order over 
</em><br>
<em>&gt; actions/universe states?  Who says so?
</em><br>
<p>/All/ decision functions generate a sequence of prefered actions. I
<br>
challenge you to name any decision function (i.e. any AI) that doesn't.
<br>
It is not quite true to say that all decision functions generate a
<br>
preference order over all possible actions, because forcing a
<br>
single-selection decision function to produce a ranked sequence by
<br>
progressive elimination of options may not produce a consistent sequence
<br>
due to preference intransitivity (still, you can average). It is true
<br>
that all rational systems will produce a single ranking sequence given
<br>
indefinite computing power.
<br>
<p>Decision functions are a well-defined concept. 'Goal systems' aren't,
<br>
so I'll need to be more specific. The 'goal-hierarchical' systems you
<br>
speak of usually have an overall decision function that consists of
<br>
a variable 'goal system', which a smaller fixed decision function
<br>
combines with the fruits of inference to select a preffered action.
<br>
For expected utility, the goal system is the utility function and the
<br>
fixed decision function is the expected utility equation instantiated
<br>
to evaluate a particular class of entities (classically universe
<br>
states or substates, although there are other possibilities). It is
<br>
perfectly possible to implement your decision function in a
<br>
distributed fashion, mash together goals and goal evaluators, even
<br>
mash together desirability and certainty (humans unfortunately suffer
<br>
from this; AI designers trying to perpetuate the mistake is inexcuable).
<br>
But you are simply obscuring your intent and complicating analysis and
<br>
prediction. Ultimately we can always analyse the causal relations
<br>
within your AI, isolate (and abstract if necessary) the set of root
<br>
selection mechanisms and hence infer the decision function. Whether
<br>
there will be a clean separation of fixed and dynamic components, and
<br>
indeed whether the whole thing will be tractable for humans, depends
<br>
on how mangled your architecture is. For Friendly AI design, the
<br>
researcher must endeavour to keep things as straightforward as possible
<br>
for there to be any hope of predictability at all. Fortunately on
<br>
analysis there don't seem to be any good reasons for such obfuscation.
<br>
<p><p><em>&gt; Many people would say it is not: this is just a particular way of
</em><br>
<em>&gt; construing a goal system.  It is possible to construct (Complex, as
</em><br>
<em>&gt; in Complex System) goal systems that work quite well, but which
</em><br>
<em>&gt; implicitly define a nondeterministic function over the space of
</em><br>
<em>&gt; possible actions, where that function is deeply nonlinear, non-analytic
</em><br>
<em>&gt; and probably noncomputable.
</em><br>
<p>Non-linear, certainly; we don't call thermostats AIs. Nondeterministic,
<br>
only if you incorporate a source of quantum noise into the system and
<br>
make its actions dependent on that noise. There may or may not be good
<br>
reasons for doing this; generally I subscribe to the philosophy that
<br>
injecting randomness into an AI is a last resort indicative of the
<br>
designer's failure to think of anything better, but there are specific
<br>
scenarious where it would be desireable to be nondeterministic. That
<br>
said, AFAIK no current AGI projects make use of a truely nondetermistic
<br>
RNG. Noncomputable, certainly not, since the AI is running on a computer!
<br>
You could argue that human cognition is noncomputable, but that's a
<br>
seperate argument.
<br>
<p><em>&gt; And, yes, it may be unstable and it may spend the entire lifetime
</em><br>
<em>&gt; of the universe heading towards a nice stable attractor *and never
</em><br>
<em>&gt; get there*....
</em><br>
<p>Now this is an interested question, how fast arbitrary (or randomly
<br>
selected) unstable AI goal systems will stabilise. Right now, the
<br>
rigorous theory doesn't exist to answer this question for anything but
<br>
a few trivial cases. My personal guess, based on my own research, is
<br>
'pretty fast', but I don't make any claim of reliability about that
<br>
guess. If you can give a formal description of such a system, please
<br>
do so, otherwise your statement is similarily pure speculation.
<br>
<p><em>&gt; What?  Did nobody here ever read Hofstadter? I'd bet good money that 
</em><br>
<em>&gt; every one of you did,
</em><br>
<p>You win. For one thing, Eliezer was recommending GEB as the one book
<br>
everyone must read for years.
<br>
<p><em>&gt; so what is so difficult about remembering his discussion of tangled
</em><br>
<em>&gt; hierarchies,
</em><br>
<p>They seemed like a good (nigh revolutionary) idea at the time. They
<br>
continued to seem like a good idea for inference for some time after
<br>
it became apparent that having a distributed representation of the
<br>
goal system was foolish for predictability, transparency (to the AI
<br>
and the programmers), internal stability and efficiency reasons.
<br>
Finally it became apparent that getting active symbol networks to
<br>
actually do anything relied on a loose set of weak inductive
<br>
mechanisms that sounded broadly reasonable to the implementers.
<br>
But 'tangled heirarchy' representational structure does not have
<br>
to imply a similarly tangled causal structure, and indeed once the
<br>
latter is removed the former becomes a lot more useful.
<br>
<p><em>&gt; and about how global system behavior can be determined by
</em><br>
<em>&gt; self-referential or recursive feedback loops within the system, in
</em><br>
<em>&gt; such a way that ascribing global behavior to particular local
</em><br>
<em>&gt; causes is nonsense?
</em><br>
<p>This is not news. Human goals aren't dependent on specific neurons,
<br>
goals and decision functions can have distributed representations.
<br>
The fact that human goals can still be concisely described underscores
<br>
the point that distributed representations are an implementation
<br>
strategy forced by sucky human hardware, not a cognitive necessity
<br>
nor indeed a good idea. Feel free to list perceived real-world
<br>
benefits of having distributed goal represnetations (or indeed,
<br>
distributed decision functions with no clear goal/evaluator
<br>
separation, e.g. 'motivational mechanisms' that influence behavoir
<br>
in a brain-global way), and I will tell you how those benefits are
<br>
achievable without the attendant drawbacks by competent design of
<br>
compact, explicit goals and decision functions.
<br>
<p><em>&gt; Why, in all of this discussion, are so many people implying that
</em><br>
<em>&gt; all goal systems must be one-level, quasi- deterministic
</em><br>
<em>&gt; mechanisms, with no feedback loops and no nonlinearity?
</em><br>
<p>They aren't. Self-modification is a feedback loop, albeit a specific
<br>
class of one in the narrow definition (I think people are using a
<br>
more general definition covering any drift in optimisation target(s)
<br>
here). Deterministic yes because nondeterministic (by which I assume
<br>
you mean 'generate non-transitive preference orderings' or 'affected
<br>
by seemingly irrelevant AI internal state') goals are a bad thing,
<br>
as you yourself seem to claim when explaining your own self-modification
<br>
desires. Nonlinearity, no, see above. Utility functions are often
<br>
mentioned, but they have no prohibition against including complex
<br>
conditional logic and/or nonlinear functions.
<br>
&nbsp;
<br>
<em>&gt; And why are the same people, rather patronizingly, I must say,
</em><br>
<em>&gt; treating me as if I am too dumb to understand what a goal system is?
</em><br>
<p>A lot of AI researchers are mired in irrelevant fluff which they
<br>
believe to be vitally important. To be honest, it looks like you may
<br>
be in this situation, because you are stressing things that many here
<br>
see as irrelevancies (since they can be compressed/abstracted away
<br>
with little loss of predictive power) that a seed AI would rapidly
<br>
discard. Feel free to try to prove otherwise.
<br>
<p><em>&gt; I keep trying to talk about motivational systems and goal hierarchies
</em><br>
<em>&gt; with tangled loops in them [what tangled loop? the one that occurs
</em><br>
<em>&gt; when the system realises that it can swap motivational modules in
</em><br>
<em>&gt; and out, and that this swapping process could have profound
</em><br>
<em>&gt; consequences for the entire observable  universe]
</em><br>
<p>Yes, this is self-modification. We have all been talking about the
<br>
consequences of self-modification for years. You will note that 'loop'
<br>
implies goals modifying goals, or more generally decision functions
<br>
modifying decision functions, possibly through some variable number
<br>
of intermediate layers. Your argument is quite the opposite; you
<br>
claim that new goals can come from somewhere other than existing
<br>
goals. What this other mechanism is is something you have not
<br>
clearly stated.
<br>
<p><em>&gt; If you really insist on characterizing it as &quot;my&quot; type of AGI vs 
</em><br>
<em>&gt; everyone else's type of AGI, that is fine:  but I am talking about a 
</em><br>
<em>&gt; more general type of AGI, as I have been [ranting] on about in this
</em><br>
<em>&gt; message.
</em><br>
<p>...which the people 'patronising' you consider to be the result of
<br>
layering some pointless obfuscation on a corresponding rational,
<br>
transparent AGI design.
<br>
&nbsp;
<br>
&nbsp;* Michael Wilson
<br>
<p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
To help you stay safe and secure online, we've developed the all new Yahoo! Security Centre. <a href="http://uk.security.yahoo.com">http://uk.security.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11970.html">Mikko Särelä: "Re: Paperclip monster, demise of."</a>
<li><strong>Previous message:</strong> <a href="11968.html">Michael Roy Ames: "Re: Paperclip monster, demise of."</a>
<li><strong>In reply to:</strong> <a href="11963.html">Richard Loosemore: "Re: Paperclip monster, demise of."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11961.html">Michael Roy Ames: "Re: Paperclip monster, demise of."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11969">[ date ]</a>
<a href="index.html#11969">[ thread ]</a>
<a href="subject.html#11969">[ subject ]</a>
<a href="author.html#11969">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
