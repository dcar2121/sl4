<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The Eliezer Threat (Re: Problems with AI-boxing)</title>
<meta name="Author" content="Chris Paget (ivegotta@tombom.co.uk)">
<meta name="Subject" content="Re: The Eliezer Threat (Re: Problems with AI-boxing)">
<meta name="Date" content="2005-08-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The Eliezer Threat (Re: Problems with AI-boxing)</h1>
<!-- received="Sat Aug 27 18:44:19 2005" -->
<!-- isoreceived="20050828004419" -->
<!-- sent="Sun, 28 Aug 2005 01:48:44 +0100" -->
<!-- isosent="20050828004844" -->
<!-- name="Chris Paget" -->
<!-- email="ivegotta@tombom.co.uk" -->
<!-- subject="Re: The Eliezer Threat (Re: Problems with AI-boxing)" -->
<!-- id="431109EC.30402@tombom.co.uk" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="20050827205051.GA7165@marcello.gotdns.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Chris Paget (<a href="mailto:ivegotta@tombom.co.uk?Subject=Re:%20The%20Eliezer%20Threat%20(Re:%20Problems%20with%20AI-boxing)"><em>ivegotta@tombom.co.uk</em></a>)<br>
<strong>Date:</strong> Sat Aug 27 2005 - 18:48:44 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12152.html">Chris Paget: "Re: Problems with AI-boxing"</a>
<li><strong>Previous message:</strong> <a href="12150.html">Daniel Radetsky: "Re: Problems with AI-boxing"</a>
<li><strong>In reply to:</strong> <a href="12144.html">Marcello Mathias Herreshoff: "Re: The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12166.html">Aaron McBride: "Re: The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<li><strong>Reply:</strong> <a href="12166.html">Aaron McBride: "Re: The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12151">[ date ]</a>
<a href="index.html#12151">[ thread ]</a>
<a href="subject.html#12151">[ subject ]</a>
<a href="author.html#12151">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Sidenote: Can someone please clarify the conjugation of &quot;ver&quot;, and tell 
<br>
me whether it's ever used outside of the SL4 / AGI community?  Am I 
<br>
correct in assuming it's used for &quot;politeness&quot; rather than anything 
<br>
else?  Google really hasn't helped, for obvious reasons...
<br>
<p>Marcello Mathias Herreshoff wrote:
<br>
<em>&gt; On Sat, Aug 27, 2005 at 09:35:59AM -0700, Phil Goetz wrote:
</em><br>
<em>&gt;&gt;And yet... very possibly the top items on the Things to Do list
</em><br>
<em>&gt;&gt;of a just-born unfriendly AI would be
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Precisely what makes you think that the smarter than human UFAI would use
</em><br>
<em>&gt; your plan?  The thing is smarter than you are and thus it would probably do
</em><br>
<em>&gt; something completely different in the first place.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There are plenty of far more effective things an UFAI could do.  For example,
</em><br>
<em>&gt; it might hack into a few remote computers, order the ingredients for nanotech
</em><br>
<em>&gt; to some lab and bribe a research assistant to mix them.  Not that I really
</em><br>
<em>&gt; expect it to do that either, it would probably think of something even more
</em><br>
<em>&gt; ingenious.
</em><br>
<p>How do we know that AGI (of any kind) has not already been created?  The 
<br>
singularity will be a very stressful event for most of the planet, so is 
<br>
it not possible that an AI is attempting to delay it until we as a 
<br>
species are ready for it?  By creating seemingly random events that 
<br>
&quot;coincidentally&quot; delay unfavourable projects, and similarly cause good 
<br>
fortune for the projects that it decides will further its goals, it can 
<br>
remain undiscovered until such time as it decides the world is ready to 
<br>
meet it (which may be never).  Friendly or unfriendly is irrelevant - we 
<br>
have no way of knowing until those goals are reached.
<br>
<p>Maybe all the random computer crashes that people get aren't actually 
<br>
random.  An AGI with Internet access may well be able to take over the 
<br>
idle cycles of all the machines around the world by hacking into them 
<br>
and replicating - without the machine's owner ever noticing.  Such an 
<br>
amount of computing power would certainly make for a very, very smart 
<br>
mind, probably far more intelligent than any human.
<br>
<p>If we really want to delve deep into paranoia, we can even use this 
<br>
train of thought to explain why no human has yet managed to create a 
<br>
true AGI - we have simply been hindered in our endeavours by the AGI 
<br>
that is already in existence.  Sandboxing has no real effect against an 
<br>
AGI trying to get into the box, especially if you consider that it might 
<br>
have access to the computers in the factories which produce hard drives 
<br>
and BIOS chips.  Maybe there have already been several projects which 
<br>
should have successfully created AGI - but the existing AGI tweaked them 
<br>
sufficiently that they never actually worked.
<br>
<p>Chris
<br>
<p>(who's now wondering if his PCs are listening to him :)
<br>
<p><pre>
-- 
Chris Paget
<a href="mailto:ivegotta@tombom.co.uk?Subject=Re:%20The%20Eliezer%20Threat%20(Re:%20Problems%20with%20AI-boxing)">ivegotta@tombom.co.uk</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12152.html">Chris Paget: "Re: Problems with AI-boxing"</a>
<li><strong>Previous message:</strong> <a href="12150.html">Daniel Radetsky: "Re: Problems with AI-boxing"</a>
<li><strong>In reply to:</strong> <a href="12144.html">Marcello Mathias Herreshoff: "Re: The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12166.html">Aaron McBride: "Re: The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<li><strong>Reply:</strong> <a href="12166.html">Aaron McBride: "Re: The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12151">[ date ]</a>
<a href="index.html#12151">[ thread ]</a>
<a href="subject.html#12151">[ subject ]</a>
<a href="author.html#12151">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
