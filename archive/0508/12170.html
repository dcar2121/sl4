<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: remove</title>
<meta name="Author" content="Joshua Amy (josh_amy@hotmail.com)">
<meta name="Subject" content="remove">
<meta name="Date" content="2005-08-31">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>remove</h1>
<!-- received="Wed Aug 31 01:26:26 2005" -->
<!-- isoreceived="20050831072626" -->
<!-- sent="Wed, 31 Aug 2005 07:26:23 +0000" -->
<!-- isosent="20050831072623" -->
<!-- name="Joshua Amy" -->
<!-- email="josh_amy@hotmail.com" -->
<!-- subject="remove" -->
<!-- id="BAY108-F13C5E0A83CC6C4ECB43F11FBA10@phx.gbl" -->
<!-- inreplyto="BAY101-F901A1101D2B47619A35A0ACA10@phx.gbl" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Joshua Amy (<a href="mailto:josh_amy@hotmail.com?Subject=Re:%20remove"><em>josh_amy@hotmail.com</em></a>)<br>
<strong>Date:</strong> Wed Aug 31 2005 - 01:26:23 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12171.html">Ben Goertzel: "hmmmm"</a>
<li><strong>Previous message:</strong> <a href="12169.html">Michael Wilson: "Re: drives ABC &gt; XYZ"</a>
<li><strong>In reply to:</strong> <a href="12168.html">Michael Vassar: "Re: drives ABC &gt; XYZ"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0509/12198.html">jogdendavis@mail.utexas.edu: "Remove"</a>
<li><strong>Reply:</strong> <a href="../0509/12198.html">jogdendavis@mail.utexas.edu: "Remove"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12170">[ date ]</a>
<a href="index.html#12170">[ thread ]</a>
<a href="subject.html#12170">[ subject ]</a>
<a href="author.html#12170">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt;From: &quot;Michael Vassar&quot; &lt;<a href="mailto:michaelvassar@hotmail.com?Subject=Re:%20remove">michaelvassar@hotmail.com</a>&gt;
</em><br>
<em>&gt;Reply-To: <a href="mailto:sl4@sl4.org?Subject=Re:%20remove">sl4@sl4.org</a>
</em><br>
<em>&gt;To: <a href="mailto:sl4@sl4.org?Subject=Re:%20remove">sl4@sl4.org</a>
</em><br>
<em>&gt;Subject: Re: drives ABC &gt; XYZ
</em><br>
<em>&gt;Date: Tue, 30 Aug 2005 22:06:14 -0400
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;We're already
</em><br>
<em>&gt;&gt;assuming that.  The A B C -&gt; X Y Z example shows how, one step at
</em><br>
<em>&gt;&gt;a time, the system can take actions that provide greater utility
</em><br>
<em>&gt;&gt;from the perspective of its top-level goals, that nonetheless end
</em><br>
<em>&gt;&gt;up replacing all those top-level goals.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Well then, so long as the ultimate goals are higher utility, from the 
</em><br>
<em>&gt;perspective of the original goals, than the original goals were, why is 
</em><br>
<em>&gt;this a problem?  A human would typically not be able to predict the long 
</em><br>
<em>&gt;term expected utility of a change to its top level goals, but a FAI 
</em><br>
<em>&gt;wouldn't make such changes unless it could.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;Another question entirely is whether, if the AI is told to maximize
</em><br>
<em>&gt;&gt;a score relating to the attainment of its top-level goals, and is
</em><br>
<em>&gt;&gt;given write access to those goals, it will rewrite those goals into
</em><br>
<em>&gt;&gt;ones more easily attainable?  (We could call this the &quot;Buddhist AI&quot;,
</em><br>
<em>&gt;&gt;perhaps?)  The REAL top-level goal in that case
</em><br>
<em>&gt;&gt;is &quot;maximize a score defined by the contents of memory locations X&quot;,
</em><br>
<em>&gt;&gt;but it doesn't help us to say that &quot;maximization&quot; won't be replaced.
</em><br>
<em>&gt;&gt;The kinds of goals we don't want to be replaced have referents
</em><br>
<em>&gt;&gt;in the real world.
</em><br>
<em>&gt;
</em><br>
<em>&gt;This really is a very very old insight for this list.  Try to familiarize 
</em><br>
<em>&gt;yourself with the list archive or at least with the major articles.  That 
</em><br>
<em>&gt;really applies to everyone who hasn't done so.  Suffice it to say that such 
</em><br>
<em>&gt;concerns were addressed very thoroughly years ago.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;You seem to be proposing that an AI will never make mistakes.
</em><br>
<em>&gt;
</em><br>
<em>&gt;In the human sense, yes.  If an AI is superintelligent and Friendly for any 
</em><br>
<em>&gt;significant time it will reach a state from which it will not ever make the 
</em><br>
<em>&gt;sort of errors of reasoning which humans mean by mistakes.  In fact, any 
</em><br>
<em>&gt;well calibrated Bayesian built on a sufficiently redundant substrate should 
</em><br>
<em>&gt;never make mistakes in the sense of either acting on implicit beliefs other 
</em><br>
<em>&gt;than its explicit beliefs or holding a belief with unjustified confidence.  
</em><br>
<em>&gt;Obviously, computing power, architectural details, and knowledge will 
</em><br>
<em>&gt;determine the degree to which it will or will not act in the manner which 
</em><br>
<em>&gt;actually maximized its utility function, but that is not what we humans 
</em><br>
<em>&gt;mean by a mistake.  We are used to constantly taking actions which we have 
</em><br>
<em>&gt;every reason to expect to regret.  A FAI shouldn't do that.  This is an 
</em><br>
<em>&gt;important distinction and not at all a natural one.  It shouldn't be 
</em><br>
<em>&gt;terribly shocking, but is.  But by now we should be used to the idea that 
</em><br>
<em>&gt;computers can perform long series of mathematical operations without error, 
</em><br>
<em>&gt;and that performing the right long series of mathematical operations is 
</em><br>
<em>&gt;equivalent to making a decision under uncertainty, so they should be able 
</em><br>
<em>&gt;to make decisions under uncertainty without error, though due to the 
</em><br>
<em>&gt;uncertainty such decisions will usually be less optimal that the decisions 
</em><br>
<em>&gt;that would have been available given more information.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;Making mistakes is a second way in which top-level goals can
</em><br>
<em>&gt;&gt;drift away from where they started.
</em><br>
<em>&gt;
</em><br>
<em>&gt;Making sub-optimal decisions can cause top-level goals to drift, but this 
</em><br>
<em>&gt;problem is absolutely unoavoidable, but should not be critical (and if it 
</em><br>
<em>&gt;is critical, that is, fundamental to the way reason works, we will just 
</em><br>
<em>&gt;have to do as well as we can).  Account must be taken of it when designing 
</em><br>
<em>&gt;an FAI, but this only requires an incremental development beyond that 
</em><br>
<em>&gt;needed to protect it from Pascal's Wagers.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12171.html">Ben Goertzel: "hmmmm"</a>
<li><strong>Previous message:</strong> <a href="12169.html">Michael Wilson: "Re: drives ABC &gt; XYZ"</a>
<li><strong>In reply to:</strong> <a href="12168.html">Michael Vassar: "Re: drives ABC &gt; XYZ"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0509/12198.html">jogdendavis@mail.utexas.edu: "Remove"</a>
<li><strong>Reply:</strong> <a href="../0509/12198.html">jogdendavis@mail.utexas.edu: "Remove"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12170">[ date ]</a>
<a href="index.html#12170">[ thread ]</a>
<a href="subject.html#12170">[ subject ]</a>
<a href="author.html#12170">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
