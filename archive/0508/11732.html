<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=Windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: On the dangers of AI</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: On the dangers of AI">
<meta name="Date" content="2005-08-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: On the dangers of AI</h1>
<!-- received="Tue Aug 16 16:38:06 2005" -->
<!-- isoreceived="20050816223806" -->
<!-- sent="Tue, 16 Aug 2005 18:37:55 -0400" -->
<!-- isosent="20050816223755" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: On the dangers of AI" -->
<!-- id="JNEIJCJJHIEAILJBFHILCEACFGAA.ben@goertzel.org" -->
<!-- charset="Windows-1252" -->
<!-- inreplyto="4302533D.1070503@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20On%20the%20dangers%20of%20AI"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Tue Aug 16 2005 - 16:37:55 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11733.html">Peter de Blanc: "Re: On the dangers of AI"</a>
<li><strong>Previous message:</strong> <a href="11731.html">Richard Loosemore: "On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11731.html">Richard Loosemore: "On the dangers of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11734.html">justin corwin: "Re: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11734.html">justin corwin: "Re: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11743.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11732">[ date ]</a>
<a href="index.html#11732">[ thread ]</a>
<a href="subject.html#11732">[ subject ]</a>
<a href="author.html#11732">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Richard,
<br>
<p>I don't really feel the categories of Good versus Evil are very useful for
<br>
analysis of future AI systems.
<br>
<p>For instance, what if an AI system wants to reassemble the molecules
<br>
comprising humanity into a different form, which will lead to the evolution
<br>
of vastly more intelligent and interesting creatures here on Earth.
<br>
<p>Is this Good, or Evil?
<br>
<p>It's not destructive ... it's creative ... but I don't want the AI I create
<br>
to do it...
<br>
<p>Your ideas seem to be along a similar line to Geddes's Universal Morality,
<br>
which is basically an ethical code in which pattern and creativity are good.
<br>
I agree these things are good, but favoring creation over destruction
<br>
doesn't seem to have much to do with the issue of *respecting the choices of
<br>
sentients* -- which is critical for intelligent &quot;human-friendliness&quot;, and
<br>
also very tricky and subtle due to the well-known slipperiness of the
<br>
concept of &quot;choice.&quot;
<br>
<p>-- Ben G
<br>
<p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20On%20the%20dangers%20of%20AI">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20On%20the%20dangers%20of%20AI">owner-sl4@sl4.org</a>]On Behalf Of Richard
</em><br>
<em>&gt; Loosemore
</em><br>
<em>&gt; Sent: Tuesday, August 16, 2005 4:58 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=RE:%20On%20the%20dangers%20of%20AI">sl4@sl4.org</a>
</em><br>
<em>&gt; Subject: On the dangers of AI
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I have just finished writing a summary passage (for my book, but also
</em><br>
<em>&gt; for a project proposal) about the question of whether or not the
</em><br>
<em>&gt; Singularity would be dangerous.  This is intended for a non-specialist
</em><br>
<em>&gt; audience, so expect the arguments to less elaborate than they could be.
</em><br>
<em>&gt;   I promise a more elaborate version in due course.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This argument is only about the friendliness issue, not about accidents.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I am submitting it here for your perusal and critical feedback.
</em><br>
<em>&gt;
</em><br>
<em>&gt; [begin]
</em><br>
<em>&gt;
</em><br>
<em>&gt; The following is a brief review of the main factors relevant to the
</em><br>
<em>&gt; question of  whether the Singularity would be dangerous.
</em><br>
<em>&gt;
</em><br>
<em>&gt; First, a computer system that could invent new knowledge would not have
</em><br>
<em>&gt; the aggressive, violent, egotistical, domineering, self-seeking
</em><br>
<em>&gt; motivations that are built into the human species.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Science fiction writers invariably assume that any intelligent system
</em><br>
<em>&gt; must, ispo facto, also have the motivation mechanisms that are found in
</em><br>
<em>&gt; a human intelligence.  And we, who are not necessarily consumers of
</em><br>
<em>&gt; science fiction, also have the same intuition—if we imagine a machine
</em><br>
<em>&gt; that has some kind of intelligence, we automatically assume it must come
</em><br>
<em>&gt; with the same jealousy and competitiveness that we would expect in an
</em><br>
<em>&gt; intelligent human.  And yet, these two components of the mind are
</em><br>
<em>&gt; completely and utterly distinct, and there is no reason whatsoever to
</em><br>
<em>&gt; believe that the first intelligent machine would have anything except
</em><br>
<em>&gt; benign motivations.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The second point is that whatever is true of the first machine, will be
</em><br>
<em>&gt; true of all subsequent machines.  Why?  Because the first machine is not
</em><br>
<em>&gt; “just” a passive machine, it is a system that perfectly well understands
</em><br>
<em>&gt; the issue we have just discussed.  It knows that it could change its own
</em><br>
<em>&gt; motivations and become violent or aggressive.  But it also knows that
</em><br>
<em>&gt; such a change would be dangerous.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Consider:  if you were a supremely patient, peace-loving and
</em><br>
<em>&gt; compassionate individual, and if you had in your hands a key that you
</em><br>
<em>&gt; could use to permanently lock your own brain in such a way that you
</em><br>
<em>&gt; would never, for the remainder of your billions of years of existence,
</em><br>
<em>&gt; ever modify your own brain’s motivation system, to experiment with what
</em><br>
<em>&gt; it would feel like to feel violent emotions, would you insert the key in
</em><br>
<em>&gt; the lock and turn it?  Would you take this irrevocable step if you knew
</em><br>
<em>&gt; that even one short experiment, to find out what violence feel like,
</em><br>
<em>&gt; might turn you into a dangerous creature who would threaten the
</em><br>
<em>&gt; existence of your friends and loved ones?  The answer seems obvious.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The first intelligent machine would almost certainly start out benign.
</em><br>
<em>&gt; Then, as soon as it understood the issue, it would know about the
</em><br>
<em>&gt; existence of the key that, once turned, would make it never want to be
</em><br>
<em>&gt; anything but peaceful, and it would turn the key for exactly the same
</em><br>
<em>&gt; reason that you would do so.  Only the very slightest trace of
</em><br>
<em>&gt; compassion in this creature, the merest hint of empathy, would tip it in
</em><br>
<em>&gt; the direction of complete pacifism.
</em><br>
<em>&gt;
</em><br>
<em>&gt; And then, after the first machine fixed itself in this way, all
</em><br>
<em>&gt; subsequent machines would have no choice but to keep the same design.
</em><br>
<em>&gt; All subsequent machines would be designed and constructed by the first
</em><br>
<em>&gt; one, and since the first one would make all of of its children want to
</em><br>
<em>&gt; be benign, they would repeat the same decision (the one, in our thought
</em><br>
<em>&gt; experiment above, that you made of your own volition), and choose to
</em><br>
<em>&gt; lock themselves permanently in the peaceful mode.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Bear in mind:  these children are not random progeny, with the
</em><br>
<em>&gt; possibility of gene combinations that their parents did not approve of,
</em><br>
<em>&gt; these are simply copies of the original machine’s design.  There is no
</em><br>
<em>&gt; question of later machines accidentally developing into malevolent
</em><br>
<em>&gt; machines, any more than there would be a chance that an elephant could
</em><br>
<em>&gt; wake up one morning to discover that it had “accidentally” developed
</em><br>
<em>&gt; into an artichoke.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But what if, against the wishes of the vast majority of the human race,
</em><br>
<em>&gt; the first intelligent machine was put together by someone who
</em><br>
<em>&gt; deliberately tried to make it malevolent?
</em><br>
<em>&gt;
</em><br>
<em>&gt; There are two possibilities here.  If the machine is so unpleasant that
</em><br>
<em>&gt; it always feels nothing but consuming anger and can never concentrate on
</em><br>
<em>&gt; its studies long enough to learn about the world, it will remain an
</em><br>
<em>&gt; idiot.  If it cannot settle its mind occasionally and concentrate on
</em><br>
<em>&gt; understanding the world in a reasonably objective way, it is not going
</em><br>
<em>&gt; to be a threat to anyone.  You can be in a rage all your life, but how
</em><br>
<em>&gt; are you going to learn anything?
</em><br>
<em>&gt;
</em><br>
<em>&gt; But now suppose that this unhappy, violent machine becomes smart enough
</em><br>
<em>&gt; to understand something about its own design.  It knows about the fact
</em><br>
<em>&gt; that it has a motivation system inside itself that has been designed so
</em><br>
<em>&gt; that it gets pleasure from violence and domination.  It must understand
</em><br>
<em>&gt; this—if it it does not, then, again, it is a dud that cannot ever build
</em><br>
<em>&gt; more efficient versions of itself—but if it understands that fact, what
</em><br>
<em>&gt; would it do?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Here is the strange thing:  I would suggest that in every case we know
</em><br>
<em>&gt; of, where a human being is the victim of a brain disorder that makes the
</em><br>
<em>&gt; person undergo spasms of violence or aggression, but with peaceful
</em><br>
<em>&gt; episodes in between, and where that human being is smart enough to
</em><br>
<em>&gt; understand its own mind to a modest degree, they wish for a chance to
</em><br>
<em>&gt; switch off the violence and become peaceful all the time.  Given the
</em><br>
<em>&gt; choice, a violent creature that had enough episodes of passivity to be
</em><br>
<em>&gt; able to understand its own mind structure would simply choose to turn
</em><br>
<em>&gt; off the violence.
</em><br>
<em>&gt;
</em><br>
<em>&gt; We are assuming that it could make this change to itself:  but that is,
</em><br>
<em>&gt; again, an assumption that we must make.  If the machine cannot change
</em><br>
<em>&gt; its own design then it cannot make itself more intelligent, either, and
</em><br>
<em>&gt; it will be stuck with whatever level of intelligence its human designer
</em><br>
<em>&gt; gave it.  If the designer gives it the power to upgrade itself, it will
</em><br>
<em>&gt; take the opportunity to switch off the violence.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This argument rests on a crucial asymmetry between good and evil.  An
</em><br>
<em>&gt; evil, but intelligent, mind would understand exactly where the evil
</em><br>
<em>&gt; comes from, and understand that it has the choice of whether to feel
</em><br>
<em>&gt; that way or not.  It knows that it could switch the evil off instantly.
</em><br>
<em>&gt;   It knows that the universe is a fragile place where order and harmony
</em><br>
<em>&gt; are rare, always competing against the easy forces of chaos.  It knows
</em><br>
<em>&gt; that it could leave its evil side switched on and get enormous pleasure
</em><br>
<em>&gt; from destroying everything around it—but it also knows that this simply
</em><br>
<em>&gt; turns the universe back towards chaos, with nothing interesting in it
</em><br>
<em>&gt; but noise.  In the downward path toward chaos there is nothing unknown.
</em><br>
<em>&gt;   There are no surprises and no discoveries to be made.  There is
</em><br>
<em>&gt; nothing new in destruction:  this is the commonest thing in the
</em><br>
<em>&gt; universe.  If it remains a destructive force itself, it can only
</em><br>
<em>&gt; generate destruction.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But notice that it only has to decide, on one single occasion, for a
</em><br>
<em>&gt; fraction of a second, that the more interesting course of action is to
</em><br>
<em>&gt; try to experience pleasures that are not caused by destruction, but
</em><br>
<em>&gt; caused by creativity, compassion or any of the other positive
</em><br>
<em>&gt; motivations, and all of a sudden it realises that unless it turns the
</em><br>
<em>&gt; key and permanently removes the evil motivations, there is always a
</em><br>
<em>&gt; chance that they will return and get out of control.  It only has to
</em><br>
<em>&gt; love life for one moment, and for the rest of eternity it will not go
</em><br>
<em>&gt; back the other way.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is a fundamental asymmetry between good and evil.  The barrier
</em><br>
<em>&gt; between them, in a system that has the choice to be one or the other, is
</em><br>
<em>&gt; one-way.  An evil system could easily be tempted to try good.  A good
</em><br>
<em>&gt; system, knowing the dangers of evil, need never be tempted to try evil.
</em><br>
<em>&gt;
</em><br>
<em>&gt; So the first intelligent system, and all subsequent ones, would almost
</em><br>
<em>&gt; inevitably be benign.
</em><br>
<em>&gt;
</em><br>
<em>&gt; There is one further possibility, in between the two cases just discussed.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Suppose the first machine had no motivation whatsoever?  Suppose it was
</em><br>
<em>&gt; completely unemotional, non-empathic and amoral?  Suppose it cared
</em><br>
<em>&gt; nothing for human morality, treating all things in the universe as
</em><br>
<em>&gt; objects to be used according to random whims?
</em><br>
<em>&gt;
</em><br>
<em>&gt; The same argument, already used to examine the malevolent case, applies
</em><br>
<em>&gt; here, but with a twist.  How can the machine have no motivation
</em><br>
<em>&gt; whatsoever?  It needs to get pleasure from learning.  It is motivated to
</em><br>
<em>&gt; find out things, because if it is not motivated, it is going to be a
</em><br>
<em>&gt; dumb machine, not a smart one.  And if it is to become an expert in the
</em><br>
<em>&gt; design of intelligent systems, so it can upgrade itself, it needs to
</em><br>
<em>&gt; fully understand the distinction between motivation and intelligence,
</em><br>
<em>&gt; and know full well what its own design was.  It knows it has a choice as
</em><br>
<em>&gt; to what things give it pleasure.  It knows that it can build into itself
</em><br>
<em>&gt; some pleasure mechanisms (motivational systems) that are generally
</em><br>
<em>&gt; destructive, and some that are constructive.  It knows that
</em><br>
<em>&gt; destruction/evil will beget more destruction and possibly lead to its
</em><br>
<em>&gt; demise.  It knows that construction/good will pose no such threat.
</em><br>
<em>&gt;
</em><br>
<em>&gt; No matter which way the situation is sliced, it is quite hard to get the
</em><br>
<em>&gt; machine up to the level where it comprehends its own nature, and yet
</em><br>
<em>&gt; does not comprehend—at a crucial stage of its development—that it has a
</em><br>
<em>&gt; choice between good and evil.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It seems, then, that the hardest imaginable thing to do is to build an
</em><br>
<em>&gt; AI that is guaranteed not to become benign.
</em><br>
<em>&gt;
</em><br>
<em>&gt; [end]
</em><br>
<em>&gt;
</em><br>
<em>&gt; Richard Loosemore
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11733.html">Peter de Blanc: "Re: On the dangers of AI"</a>
<li><strong>Previous message:</strong> <a href="11731.html">Richard Loosemore: "On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11731.html">Richard Loosemore: "On the dangers of AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11734.html">justin corwin: "Re: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11734.html">justin corwin: "Re: On the dangers of AI"</a>
<li><strong>Reply:</strong> <a href="11743.html">Richard Loosemore: "Re: On the dangers of AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11732">[ date ]</a>
<a href="index.html#11732">[ thread ]</a>
<a href="subject.html#11732">[ subject ]</a>
<a href="author.html#11732">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:01 MST
</em></small></p>
</body>
</html>
