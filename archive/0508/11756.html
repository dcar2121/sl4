<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: On the dangers of AI (Phase 2)</title>
<meta name="Author" content="justin corwin (outlawpoet@gmail.com)">
<meta name="Subject" content="Re: On the dangers of AI (Phase 2)">
<meta name="Date" content="2005-08-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: On the dangers of AI (Phase 2)</h1>
<!-- received="Wed Aug 17 12:12:53 2005" -->
<!-- isoreceived="20050817181253" -->
<!-- sent="Wed, 17 Aug 2005 11:12:50 -0700" -->
<!-- isosent="20050817181250" -->
<!-- name="justin corwin" -->
<!-- email="outlawpoet@gmail.com" -->
<!-- subject="Re: On the dangers of AI (Phase 2)" -->
<!-- id="3ad827f305081711122c080fa1@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="4302EE08.3030806@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> justin corwin (<a href="mailto:outlawpoet@gmail.com?Subject=Re:%20On%20the%20dangers%20of%20AI%20(Phase%202)"><em>outlawpoet@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Aug 17 2005 - 12:12:50 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11757.html">Brian Atkins: "Re: On the dangers of AI"</a>
<li><strong>Previous message:</strong> <a href="11755.html">Eliezer S. Yudkowsky: "Re: Shock Level 5 (SL5) - 'The Theory Of Everything'"</a>
<li><strong>In reply to:</strong> <a href="11747.html">Richard Loosemore: "On the dangers of AI (Phase 2)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11759.html">Richard Loosemore: "Re: On the dangers of AI (Phase 2)"</a>
<li><strong>Reply:</strong> <a href="11759.html">Richard Loosemore: "Re: On the dangers of AI (Phase 2)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11756">[ date ]</a>
<a href="index.html#11756">[ thread ]</a>
<a href="subject.html#11756">[ subject ]</a>
<a href="author.html#11756">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On 8/17/05, Richard Loosemore &lt;<a href="mailto:rpwl@lightlink.com?Subject=Re:%20On%20the%20dangers%20of%20AI%20(Phase%202)">rpwl@lightlink.com</a>&gt; wrote:
<br>
<em>&gt; Allow me to illustrate.  Under stress, I sometimes lose patience with my
</em><br>
<em>&gt; son and shout.  Afterwards, I regret it.  I regret the existence of an
</em><br>
<em>&gt; anger module that kicks in under stress.  Given the choice, I would
</em><br>
<em>&gt; switch that anger module off permanently.  But when I expressed that
</em><br>
<em>&gt; desire to excise it, did I develop a new motivation module that became
</em><br>
<em>&gt; the cause for my desire to reform my system?  No.  The desire for reform
</em><br>
<em>&gt; came from pure self-knowledge.  That is what I mean by a threshold of
</em><br>
<em>&gt; understanding, beyond which the motivations of an AI are no longer
</em><br>
<em>&gt; purely governed by its initial, hardwired motivations.
</em><br>
<p>You are misunderstanding here. You *already have* desires to reform
<br>
yourself. Humans are inconsistent, with multiple sources of
<br>
motivation. You presumably love your son, and desire to be a good
<br>
person. These motivations come from a different source than your
<br>
temporary limbic rage, and are unaffected in intensity and
<br>
directionality. Hence, those motivations view anger as orthogonal to
<br>
your goals of loving your son, and being a good person.
<br>
<p>&quot;Pure self-knowledge&quot; doesn't change anything about your total
<br>
motivations. You already wanted to be a more consistent person, which
<br>
includes revising some of your inconsistent, less powerful human
<br>
motivations. You'll notice, if you examine yourself carefully, that
<br>
you have little desire to reform your most important, cherished
<br>
beliefs. This is probably not because they are objectively the best,
<br>
but rather because they are the things that are important to you, they
<br>
comprise central portions of your motivations.
<br>
<p>A properly designed goal system of any kind does not include
<br>
overlapping independent motivation sources, unless you're trying to
<br>
recapitulate human failures of wisdom.
<br>
<p><em>&gt; This understanding of motivation, coupled with the ability to flip
</em><br>
<em>&gt; switches in the cognitive system (an ability available to an AI, though
</em><br>
<em>&gt; not yet to me) means that the final state of motivation of an AI is
</em><br>
<em>&gt; actually governed by a subtle feedback loop (via deep understanding and
</em><br>
<em>&gt; those switches I mentioned), and the final state is not at all obvious,
</em><br>
<em>&gt; and quite probably not determined by the motivations it starts with.
</em><br>
<p>It is very possible, given open, loosely defined goals, that a large
<br>
portion of the eventual motivational structure of a reflective system
<br>
will take it's shape from environmental and universal factors, like
<br>
the local environment, whether it's grown up with dangerous peers, the
<br>
optimal utility calculation in your universe, etc etc.
<br>
<p>I don't think that it's obvious at all that niceness is the
<br>
conservative assumption in a meandering goal system. Being nice is a
<br>
very small space of goals and actions. Just in a volumetric sense,
<br>
being not nice is much more likely, unless there is something special
<br>
about being nice, as Marc Geddes suggests.
<br>
<p><p><pre>
-- 
Justin Corwin
<a href="mailto:outlawpoet@hell.com?Subject=Re:%20On%20the%20dangers%20of%20AI%20(Phase%202)">outlawpoet@hell.com</a>
<a href="http://outlawpoet.blogspot.com">http://outlawpoet.blogspot.com</a>
<a href="http://www.adaptiveai.com">http://www.adaptiveai.com</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11757.html">Brian Atkins: "Re: On the dangers of AI"</a>
<li><strong>Previous message:</strong> <a href="11755.html">Eliezer S. Yudkowsky: "Re: Shock Level 5 (SL5) - 'The Theory Of Everything'"</a>
<li><strong>In reply to:</strong> <a href="11747.html">Richard Loosemore: "On the dangers of AI (Phase 2)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11759.html">Richard Loosemore: "Re: On the dangers of AI (Phase 2)"</a>
<li><strong>Reply:</strong> <a href="11759.html">Richard Loosemore: "Re: On the dangers of AI (Phase 2)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11756">[ date ]</a>
<a href="index.html#11756">[ thread ]</a>
<a href="subject.html#11756">[ subject ]</a>
<a href="author.html#11756">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:01 MST
</em></small></p>
</body>
</html>
