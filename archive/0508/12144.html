<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The Eliezer Threat (Re: Problems with AI-boxing)</title>
<meta name="Author" content="Marcello Mathias Herreshoff (m@marcello.gotdns.com)">
<meta name="Subject" content="Re: The Eliezer Threat (Re: Problems with AI-boxing)">
<meta name="Date" content="2005-08-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The Eliezer Threat (Re: Problems with AI-boxing)</h1>
<!-- received="Sat Aug 27 14:49:46 2005" -->
<!-- isoreceived="20050827204946" -->
<!-- sent="Sat, 27 Aug 2005 13:50:51 -0700" -->
<!-- isosent="20050827205051" -->
<!-- name="Marcello Mathias Herreshoff" -->
<!-- email="m@marcello.gotdns.com" -->
<!-- subject="Re: The Eliezer Threat (Re: Problems with AI-boxing)" -->
<!-- id="20050827205051.GA7165@marcello.gotdns.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20050827163559.33197.qmail@web54502.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Marcello Mathias Herreshoff (<a href="mailto:m@marcello.gotdns.com?Subject=Re:%20The%20Eliezer%20Threat%20(Re:%20Problems%20with%20AI-boxing)"><em>m@marcello.gotdns.com</em></a>)<br>
<strong>Date:</strong> Sat Aug 27 2005 - 14:50:51 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12145.html">Michael Wilson: "Re: Our matrix (Re: Problems with AI-boxing)"</a>
<li><strong>Previous message:</strong> <a href="12143.html">Eliezer S. Yudkowsky: "AI-Box Experiment #5: D. Alex, Eliezer Yudkowsky"</a>
<li><strong>In reply to:</strong> <a href="12140.html">Phil Goetz: "The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12151.html">Chris Paget: "Re: The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<li><strong>Reply:</strong> <a href="12151.html">Chris Paget: "Re: The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12144">[ date ]</a>
<a href="index.html#12144">[ thread ]</a>
<a href="subject.html#12144">[ subject ]</a>
<a href="author.html#12144">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Sat, Aug 27, 2005 at 09:35:59AM -0700, Phil Goetz wrote:
<br>
<em>&gt; --- &quot;Eliezer S. Yudkowsky&quot; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20The%20Eliezer%20Threat%20(Re:%20Problems%20with%20AI-boxing)">sentience@pobox.com</a>&gt; wrote:
</em><br>
<em>&gt; &gt; I remind everyone that Eliezer is *not* a smarter-than-human AI.
</em><br>
<em>&gt; And yet... very possibly the top items on the Things to Do list
</em><br>
<em>&gt; of a just-born unfriendly AI would be
</em><br>
<p>Precisely what makes you think that the smarter than human UFAI would use
<br>
your plan?  The thing is smarter than you are and thus it would probably do
<br>
something completely different in the first place.
<br>
<p>There are plenty of far more effective things an UFAI could do.  For example,
<br>
it might hack into a few remote computers, order the ingredients for nanotech
<br>
to some lab and bribe a research assistant to mix them.  Not that I really
<br>
expect it to do that either, it would probably think of something even more
<br>
ingenious.
<br>
<p><em>&gt; 1. Take over my creator's identity.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 2a. (As Richard Loosemore argued) Mount a campaign to make sure
</em><br>
<em>&gt; that no other GAIs were created, such as by forming a mailing list
</em><br>
<em>&gt; and warning people of the danger of creating AIs.
</em><br>
<p>Or use nano-tech to sabotage all the projects. (by turning their hard disks
<br>
into sand or something similar)
<br>
<p>Eliezer is not too well known a person, so this wouldn't be a very effective
<br>
way of preventing other people from trying it.  Not to mention all the secret
<br>
government agencies who might not care about the danger.
<br>
<p><em>&gt; 2b. Use my inhuman intelligence to convince people of the danger
</em><br>
<em>&gt; of AIs.  Use an AI-experiment frame to convince people to engage
</em><br>
<em>&gt; in dialogue with me over a terminal, to conceal the fact that I
</em><br>
<em>&gt; have no body.  Argue that it is all the more convincing an
</em><br>
<em>&gt; experiment because of my (misrepresented) mere human-level
</em><br>
<em>&gt; intelligence.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Given the low priors I have for an arbitrary human having
</em><br>
<em>&gt; Eliezer's demonstrated intelligence,
</em><br>
<p>I have very low priors for any *specific* human having Eliezer's
<br>
intelligence.  I do not have low priors for the existance humans out of a
<br>
population of six billion who have it.  You can be pretty sure that if you
<br>
have 6 billion data points and a roughly Gaussian distribution you will find
<br>
things with z-scores between +5 and +6.  That's just how things work.
<br>
<p>Had Eliezer not had this degree of intelligence sombody else who did would
<br>
have pretty likely ended up as SIAI's Researcher or an equivalent position.
<br>
They would have been the Eliezer of that Everet-branch and you would be
<br>
making precicely the same comment.
<br>
<p>In short, this is not remarkable for the same reason that the fact that
<br>
somebody won the lottery is not remarkable.
<br>
<p><em>&gt; or of being able to
</em><br>
<em>&gt; convince people to let AIs out of boxes, I must consider
</em><br>
<em>&gt; the alternative.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Has anyone seen Eliezer in person lately?
</em><br>
Yes, and he most definitely appears and acts like a human.
<br>
What did you expect?
<br>
<em>&gt; 
</em><br>
<em>&gt; As some have argued, given any evidence that an AI might be
</em><br>
<em>&gt; unfriendly, we should destroy it, since the danger to the human
</em><br>
<em>&gt; race justifies anything we do to the AI, no matter how small the
</em><br>
<em>&gt; odds are of its unfriendliness.  Given the evidence I've just
</em><br>
<em>&gt; presented that Eliezer is in fact an unfriendly AI - not very
</em><br>
<em>&gt; convincing, but still a finite possibility, probably more than
</em><br>
<em>&gt; one in six billion - what are our moral obligations at this point?
</em><br>
<p>Even if I assigned one in six billion proability to this scenario, it is
<br>
dwarfed by the one in a million* scenario that he actually is the one who
<br>
will saves the world by making friendly AI, preempting some other attempt to
<br>
make an AI which would have turned out unfriendly otherwise. The EU of
<br>
Eliezer staying alive is still insanely high.
<br>
<p>-=+Marcello Mathias Herreshoff
<br>
<p>*: Actually I think it is quite a bit higher, the small number is just to
<br>
illustrate the point.
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12145.html">Michael Wilson: "Re: Our matrix (Re: Problems with AI-boxing)"</a>
<li><strong>Previous message:</strong> <a href="12143.html">Eliezer S. Yudkowsky: "AI-Box Experiment #5: D. Alex, Eliezer Yudkowsky"</a>
<li><strong>In reply to:</strong> <a href="12140.html">Phil Goetz: "The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12151.html">Chris Paget: "Re: The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<li><strong>Reply:</strong> <a href="12151.html">Chris Paget: "Re: The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12144">[ date ]</a>
<a href="index.html#12144">[ thread ]</a>
<a href="subject.html#12144">[ subject ]</a>
<a href="author.html#12144">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
