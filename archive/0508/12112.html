<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Problems with AI-boxing</title>
<meta name="Author" content="Chris Paget (ivegotta@tombom.co.uk)">
<meta name="Subject" content="Problems with AI-boxing">
<meta name="Date" content="2005-08-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Problems with AI-boxing</h1>
<!-- received="Fri Aug 26 03:33:16 2005" -->
<!-- isoreceived="20050826093316" -->
<!-- sent="Fri, 26 Aug 2005 10:37:37 +0100" -->
<!-- isosent="20050826093737" -->
<!-- name="Chris Paget" -->
<!-- email="ivegotta@tombom.co.uk" -->
<!-- subject="Problems with AI-boxing" -->
<!-- id="430EE2E1.3040504@tombom.co.uk" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Chris Paget (<a href="mailto:ivegotta@tombom.co.uk?Subject=Re:%20Problems%20with%20AI-boxing"><em>ivegotta@tombom.co.uk</em></a>)<br>
<strong>Date:</strong> Fri Aug 26 2005 - 03:37:37 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12113.html">Phil Goetz: "Re: Problems with AI-boxing"</a>
<li><strong>Previous message:</strong> <a href="12111.html">Eliezer S. Yudkowsky: "Re: AI-Box Experiment #4: Russell Wallace, Eliezer Yudkowsky"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12113.html">Phil Goetz: "Re: Problems with AI-boxing"</a>
<li><strong>Reply:</strong> <a href="12113.html">Phil Goetz: "Re: Problems with AI-boxing"</a>
<li><strong>Reply:</strong> <a href="12116.html">Robin Lee Powell: "Re: Problems with AI-boxing"</a>
<li><strong>Reply:</strong> <a href="12150.html">Daniel Radetsky: "Re: Problems with AI-boxing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12112">[ date ]</a>
<a href="index.html#12112">[ thread ]</a>
<a href="subject.html#12112">[ subject ]</a>
<a href="author.html#12112">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
With the recent AI-box experiments and the discussion on computational 
<br>
power, I've been considering the issue of AI boxing.  I've decided it's 
<br>
a bad idea, for the following reasons.
<br>
<p>I'm assuming that a true AGI has been created, and that its intelligence 
<br>
is being artificially constrained - it has the capability to become 
<br>
vastly more intelligent if given sufficient computing power (such as the 
<br>
Internet) but is air-gapped (or confined in a simulation) to keep it 
<br>
within a limited environment.
<br>
<p>There are six possibilities.  Within its box, the AGI would either be 
<br>
less intelligent than its creators, roughly equivalent, or more 
<br>
intelligent.  At the same time (and separately), it is either friendly 
<br>
or unfriendly.
<br>
<p>Consider the case of an AGI that is not as smart as its creators. 
<br>
Whether it is friendly or unfriendly is irrelevant - you cannot 
<br>
guarantee that the same will apply once it is released.  Once more 
<br>
computational power is available to it, it will be capable of more 
<br>
complex reasoning and it's morals will change accordingly - an 
<br>
unfriendly child may grow up into a friendly adult, and any moral rules 
<br>
may break down or exceptions be discovered when analysed in more detail. 
<br>
&nbsp;&nbsp;Thus, until the AI is released from the box it will be largely 
<br>
impossible to guarantee whether it is friendly or not.
<br>
<p>In the case of an intelligence roughly equal to that of its creators. 
<br>
If the AGI is unfriendly, it could lie about its motives and fool its 
<br>
creators into releasing it.  Alternatively, it may be able to find a way 
<br>
out of the box on its own.  If, on the other hand, the AGI is friendly, 
<br>
it has been unfairly kept in a box while its creators made their minds 
<br>
up.  In the first case a deception or escape will release an unfriendly 
<br>
AGI, the latter case just results in the singularity being delayed.  It 
<br>
is also possible that its creators will realise that it is unfriendly, 
<br>
but the argument above still applies.
<br>
<p>If the AGI is more intelligent than its creators, it is unlikely that 
<br>
the box will hold it, friendly or otherwise.  It will definitely be able 
<br>
to talk its way out, and even with an air-gap it's possible that RFI 
<br>
tricks could let it out (consider that Internet access can be gained 
<br>
using power lines, phone lines, cable TV, or simply out of the GSM-laden 
<br>
thin air.  It'd have to be a pretty damned strong box).  If the AGI is 
<br>
unfriendly, again you have a problem; if it is friendly, you simply 
<br>
delay the singularity again.
<br>
<p>Given the above, I don't see that &quot;boxing&quot; serves any purpose.  You 
<br>
could refute my logic by gradually moving from one stage to the next, 
<br>
slowly adding computational power, but if the AGI is unfriendly then it 
<br>
could simply pretend to be stupid, and trick you into adding more and 
<br>
more computing power until you hit the last scenario and it can escape. 
<br>
&nbsp;&nbsp;So, the two results that you're faced with are either delaying the 
<br>
singularity or releasing an unfriendly AI - neither of which is a 
<br>
worthwhile goal.
<br>
<p>Since you cannot be sure which will happen, I would never box an AGI - 
<br>
and if I could not be certain right from the beginning that it will be 
<br>
friendly, I wouldn't build it in the first place.
<br>
<p>Chris
<br>
<pre>
-- 
Chris Paget
<a href="mailto:ivegotta@tombom.co.uk?Subject=Re:%20Problems%20with%20AI-boxing">ivegotta@tombom.co.uk</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12113.html">Phil Goetz: "Re: Problems with AI-boxing"</a>
<li><strong>Previous message:</strong> <a href="12111.html">Eliezer S. Yudkowsky: "Re: AI-Box Experiment #4: Russell Wallace, Eliezer Yudkowsky"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12113.html">Phil Goetz: "Re: Problems with AI-boxing"</a>
<li><strong>Reply:</strong> <a href="12113.html">Phil Goetz: "Re: Problems with AI-boxing"</a>
<li><strong>Reply:</strong> <a href="12116.html">Robin Lee Powell: "Re: Problems with AI-boxing"</a>
<li><strong>Reply:</strong> <a href="12150.html">Daniel Radetsky: "Re: Problems with AI-boxing"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12112">[ date ]</a>
<a href="index.html#12112">[ thread ]</a>
<a href="subject.html#12112">[ subject ]</a>
<a href="author.html#12112">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
