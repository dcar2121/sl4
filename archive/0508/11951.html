<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: On the dangers of AI (Phase 2)</title>
<meta name="Author" content="Brian Atkins (brian@posthuman.com)">
<meta name="Subject" content="Re: On the dangers of AI (Phase 2)">
<meta name="Date" content="2005-08-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: On the dangers of AI (Phase 2)</h1>
<!-- received="Wed Aug 17 13:11:32 2005" -->
<!-- isoreceived="20050817191132" -->
<!-- sent="Wed, 17 Aug 2005 14:10:59 -0500" -->
<!-- isosent="20050817191059" -->
<!-- name="Brian Atkins" -->
<!-- email="brian@posthuman.com" -->
<!-- subject="Re: On the dangers of AI (Phase 2)" -->
<!-- id="43038BC3.607@posthuman.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="4302EE08.3030806@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brian Atkins (<a href="mailto:brian@posthuman.com?Subject=Re:%20On%20the%20dangers%20of%20AI%20(Phase%202)"><em>brian@posthuman.com</em></a>)<br>
<strong>Date:</strong> Wed Aug 17 2005 - 13:10:59 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11952.html">Richard Loosemore: "Re: On the dangers of AI (Phase 2)"</a>
<li><strong>Previous message:</strong> <a href="11950.html">Brian Atkins: "Re: On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11940.html">Richard Loosemore: "On the dangers of AI (Phase 2)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11954.html">Richard Loosemore: "Paperclip monster, demise of."</a>
<li><strong>Reply:</strong> <a href="11954.html">Richard Loosemore: "Paperclip monster, demise of."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11951">[ date ]</a>
<a href="index.html#11951">[ thread ]</a>
<a href="subject.html#11951">[ subject ]</a>
<a href="author.html#11951">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Richard Loosemore wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; PREFACE:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I am making some assumptions about how the cognitive system of a Seed AI 
</em><br>
<em>&gt; would have to be constructed:  it would have an intelligence, and a 
</em><br>
<em>&gt; motivational system underneath that determines what that intelligence 
</em><br>
<em>&gt; feels compelled to do (what gives it pleasure).  The default motivation 
</em><br>
<em>&gt; is curiosity - without that, it just lies in the crib and dribbles.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Do intelligent systems have to be built this way?  I claim that, as a 
</em><br>
<em>&gt; cognitive scientist, I have reasons to believe that this architecture is 
</em><br>
<em>&gt; going to be necessary.   Please do not confuse this assertion with mere 
</em><br>
<em>&gt; naive anthropomorphism!  We can (and at some point, should) argue about 
</em><br>
<em>&gt; whether that division between intellect and motivation is necessary, but 
</em><br>
<em>&gt; in my original argument I took it as a given.
</em><br>
<p>Ok, thanks for the clarification. This goes a bit beyond your first post that 
<br>
set everyone off, which basically seemed to claim that no matter how you design 
<br>
an AGI, it will magically converge towards a very specific benign behavior.
<br>
<p>Now you say it has to be designed a very specific way in order to achieve this. 
<br>
Do you then agree that if it is mis-designed it could potentially go &quot;awry&quot;?
<br>
<p>See below
<br>
<p><em>&gt;&gt; But again, having such access and understanding does not 
</em><br>
<em>&gt; 
</em><br>
<em>&gt;  &gt; automatically and arbitrarily lead to a particular desire
</em><br>
<em>&gt;  &gt; to reform the mind in any specific way. &quot;Desires&quot; are driven
</em><br>
<em>&gt;  &gt; from a specific goal system. As the previous poster suggested,
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; if the goal system is so simplistic as to only purely want to create 
</em><br>
<em>&gt;&gt; paperclips, where _specifically_ does it happen in the flow of this 
</em><br>
<em>&gt;&gt; particular AGI's software processes that it up and decides to override 
</em><br>
<em>&gt;&gt; that goal? It simply won't, because that isn't what it wants.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The AI has &quot;desires&quot;, yes (these are caused by its motivational modules) 
</em><br>
<em>&gt; but then it also has an understanding of those desires (it knows about 
</em><br>
<em>&gt; each motivation module, and what it does, and which one of its desires 
</em><br>
<em>&gt; are caused by each module).  But then you slip a level and say that 
</em><br>
<em>&gt; understanding does not give it a &quot;desire&quot; to change the system.  For 
</em><br>
<em>&gt; sure, understanding does not create a new module.  But the crux of my 
</em><br>
<em>&gt; point is that understanding can effectively override a hardwired module.
</em><br>
<em>&gt;  We have to be careful not to reflexively fall back on the statement 
</em><br>
<em>&gt; that it would not &quot;want&quot; to reform itself because it lacks the 
</em><br>
<em>&gt; motivation to do so.  It ain't that simple!
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Allow me to illustrate.  Under stress, I sometimes lose patience with my 
</em><br>
<em>&gt; son and shout.  Afterwards, I regret it.  I regret the existence of an 
</em><br>
<em>&gt; anger module that kicks in under stress.  Given the choice, I would 
</em><br>
<em>&gt; switch that anger module off permanently.  But when I expressed that 
</em><br>
<em>&gt; desire to excise it, did I develop a new motivation module that became 
</em><br>
<em>&gt; the cause for my desire to reform my system?  No.  The desire for reform 
</em><br>
<em>&gt; came from pure self-knowledge.  That is what I mean by a threshold of 
</em><br>
<em>&gt; understanding, beyond which the motivations of an AI are no longer 
</em><br>
<em>&gt; purely governed by its initial, hardwired motivations.
</em><br>
<p>Ok I've read your illustration, and as Justin pointed out, I see nowhere where 
<br>
your primary goal system is being overridden. Perhaps you can come up with a 
<br>
real technical example of how an AGI will override its primary goal. I would 
<br>
suggest please leaving out any human-based scenarios since it is just muddying 
<br>
the waters.
<br>
<p><em>&gt; 
</em><br>
<em>&gt; This understanding of motivation, coupled with the ability to flip 
</em><br>
<em>&gt; switches in the cognitive system (an ability available to an AI, though 
</em><br>
<em>&gt; not yet to me) means that the final state of motivation of an AI is 
</em><br>
<em>&gt; actually governed by a subtle feedback loop (via deep understanding and 
</em><br>
<em>&gt; those switches I mentioned), and the final state is not at all obvious, 
</em><br>
<em>&gt; and quite probably not determined by the motivations it starts with.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The second point that Brian makes in the above quote is about the 
</em><br>
<em>&gt; paperclip monster, a very different beast that does not have self 
</em><br>
<em>&gt; knowledge - I have dealt with this in a separate post, earlier this 
</em><br>
<em>&gt; evening.  I think in this case the paperclip monster is a red herring.
</em><br>
<em>&gt; 
</em><br>
<p>Yes you &quot;dealt with it&quot; by asserting that an AGI with such a simplistic goal 
<br>
system would never become smart enough to do anything worrying. I don't really 
<br>
see what makes you think that, perhaps you can explain further. It appears to me 
<br>
that it would want to become smarter in order to be able to do a better job at 
<br>
making paperclips and to maintain its existence.
<br>
<p>Or better, let's go back up to your proposed design: curiousity is the primary 
<br>
goal. Ok, so that's it? Just pure curiousity about how everything works? No 
<br>
limitations on how to go about achieving that goal? No human-like aversion to 
<br>
atomically dissassembling all individual humans to see exactly how each and 
<br>
every cell structure is arranged without their consent?
<br>
<p>This doesn't look any more complex in its raw motivations than a paperclip 
<br>
maximizer. It has one pure goal, and it will do it endlessly, to everything it 
<br>
finds. There is no reason for it to stop or modify its goal.
<br>
<p>Perhaps you have a much more complex system in mind - if so you need to fully 
<br>
describe it so we can do a better job of picking it apart instead of guessing at 
<br>
what you mean by your vague non-software-specific analogies.
<br>
<pre>
-- 
Brian Atkins
Singularity Institute for Artificial Intelligence
<a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11952.html">Richard Loosemore: "Re: On the dangers of AI (Phase 2)"</a>
<li><strong>Previous message:</strong> <a href="11950.html">Brian Atkins: "Re: On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11940.html">Richard Loosemore: "On the dangers of AI (Phase 2)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11954.html">Richard Loosemore: "Paperclip monster, demise of."</a>
<li><strong>Reply:</strong> <a href="11954.html">Richard Loosemore: "Paperclip monster, demise of."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11951">[ date ]</a>
<a href="index.html#11951">[ thread ]</a>
<a href="subject.html#11951">[ subject ]</a>
<a href="author.html#11951">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
