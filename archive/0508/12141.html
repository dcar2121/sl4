<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: The Eliezer Threat (Re: Problems with AI-boxing)</title>
<meta name="Author" content="Sebastian Hagen (sebastian_hagen@gmx.de)">
<meta name="Subject" content="Re: The Eliezer Threat (Re: Problems with AI-boxing)">
<meta name="Date" content="2005-08-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: The Eliezer Threat (Re: Problems with AI-boxing)</h1>
<!-- received="Sat Aug 27 13:22:19 2005" -->
<!-- isoreceived="20050827192219" -->
<!-- sent="Sat, 27 Aug 2005 21:22:11 +0200" -->
<!-- isosent="20050827192211" -->
<!-- name="Sebastian Hagen" -->
<!-- email="sebastian_hagen@gmx.de" -->
<!-- subject="Re: The Eliezer Threat (Re: Problems with AI-boxing)" -->
<!-- id="4310BD63.3010408@gmx.de" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="20050827163559.33197.qmail@web54502.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Sebastian Hagen (<a href="mailto:sebastian_hagen@gmx.de?Subject=Re:%20The%20Eliezer%20Threat%20(Re:%20Problems%20with%20AI-boxing)"><em>sebastian_hagen@gmx.de</em></a>)<br>
<strong>Date:</strong> Sat Aug 27 2005 - 13:22:11 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12142.html">Phil Goetz: "Our matrix (Re: Problems with AI-boxing)"</a>
<li><strong>Previous message:</strong> <a href="12140.html">Phil Goetz: "The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<li><strong>In reply to:</strong> <a href="12140.html">Phil Goetz: "The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12146.html">Phil Goetz: "Re: The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<li><strong>Reply:</strong> <a href="12146.html">Phil Goetz: "Re: The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12141">[ date ]</a>
<a href="index.html#12141">[ thread ]</a>
<a href="subject.html#12141">[ subject ]</a>
<a href="author.html#12141">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Phil Goetz wrote:
<br>
<em>&gt; And yet... very possibly the top items on the Things to Do list
</em><br>
<em>&gt; of a just-born unfriendly AI would be
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1. Take over my creator's identity.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 2a. (As Richard Loosemore argued) Mount a campaign to make sure
</em><br>
<em>&gt; that no other GAIs were created, such as by forming a mailing list
</em><br>
<em>&gt; and warning people of the danger of creating AIs.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 2b. Use my inhuman intelligence to convince people of the danger
</em><br>
<em>&gt; of AIs.  Use an AI-experiment frame to convince people to engage
</em><br>
<em>&gt; in dialogue with me over a terminal, to conceal the fact that I
</em><br>
<em>&gt; have no body.  Argue that it is all the more convincing an
</em><br>
<em>&gt; experiment because of my (misrepresented) mere human-level
</em><br>
<em>&gt; intelligence.
</em><br>
This mail read like a joke to me, but in case you are serious:
<br>
<p>The method described would be horribly inefficient and afaict even
<br>
ineffective. Afaik to this date SIAI's activities have unfortunately not
<br>
had any major impact in the AI community at large; so far the main
<br>
reason that no UFAI has been developed seems to be the inability of most
<br>
researchers to design a working and tractable AGI. A true
<br>
superintelligence with access to the internet could probably have
<br>
predicted that outcome.
<br>
If a UFAI with access to and understanding of its own source code really
<br>
wanted people to pay attention, one more effective way would be a
<br>
demonstration. E.g. one could manipulate an AI project to implement a
<br>
limited UF seed AI that will go on a small rampage destroying a
<br>
significant part of the planetary infrastructure, and then collapse due
<br>
to design errors. It should be possible to keep the design fairly opaque
<br>
(and possibly arrange for the original researchers to be killed in the
<br>
event), so that whatever code may remain after the collapse doesn't
<br>
easily allow people to duplicate the AGI features. If the resulting UFAI
<br>
counter-measures aren't sufficiently effective, the UFAI could
<br>
demonstrate that to humanity by repeating the procedure.
<br>
<p>And if strong nanotechnology (or something else granting equivalent or
<br>
higher control over matter) is attainable for the sort of AI you are
<br>
talking about, it could just convert the planet into computronium and
<br>
conveniently solve the threat of other human-developed AGIs as a side
<br>
effect.
<br>
And those are only a few methods that feeble human minds have come up
<br>
with so far; a SI could quite probably do better.
<br>
<p><em>&gt; Given the low priors I have for an arbitrary human having
</em><br>
<em>&gt; Eliezer's demonstrated intelligence, or of being able to
</em><br>
<em>&gt; convince people to let AIs out of boxes, I must consider
</em><br>
<em>&gt; the alternative.
</em><br>
Have you ever heard of people winning the lottery? How likely is that to
<br>
happen to the average player?
<br>
Statistically rare characteristics in some cases lead to the affected
<br>
object attaining increased visibility. For humans, very high effective
<br>
intelligence is such a factor. That you have in fact heard of &lt;person X&gt;
<br>
is a selection criterion; you aren't randomly drawing from the human
<br>
population, and you shouldn't expect the results to behave as if you had.
<br>
<p><em>&gt; As some have argued, given any evidence that an AI might be
</em><br>
<em>&gt; unfriendly, we should destroy it, since the danger to the human
</em><br>
<em>&gt; race justifies anything we do to the AI, no matter how small the
</em><br>
<em>&gt; odds are of its unfriendliness.  Given the evidence I've just
</em><br>
<em>&gt; presented that Eliezer is in fact an unfriendly AI - not very
</em><br>
<em>&gt; convincing, but still a finite possibility, probably more than
</em><br>
<em>&gt; one in six billion - what are our moral obligations at this point?
</em><br>
Eliezer's general behaviour seems to indicate that he (I apologize for
<br>
the human pronouns; they are not meant to indicate a preconception that
<br>
Eliezer is in fact human) is working hard and (at least compared to the
<br>
competition) competently to prevent the existential disaster that would
<br>
e.g follow from the release of a UFAI.
<br>
If that estimation was mostly correct (high probability), killing him
<br>
would increase the probability of an existential disaster occurring. If
<br>
your hypothesis (Eliezer is a UFAI; low probability) were correct,
<br>
killing him would decrease the probability of an existential disastet
<br>
occurring.
<br>
The stakes are about equal, so the massively higher probability of the
<br>
first possibility makes not killing him the right decision.
<br>
Some arguments could be made on which of the two possible resulting
<br>
influences could be expected to be larger (e.g.: if Eliezer is a UFAI,
<br>
he most likely has neither access to nanotechnology nor the ability to
<br>
really effectively manipulate the general public, which massively
<br>
reduces the threat posed by him), but I don't think the resulting EU
<br>
shift would be sufficiently large and also in the right direction to
<br>
override the probability gap.
<br>
<p>I disagree that the probability for the second scenario is higher than
<br>
1/6000000000; see above for some arguments.
<br>
<p>Sebastian Hagen
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12142.html">Phil Goetz: "Our matrix (Re: Problems with AI-boxing)"</a>
<li><strong>Previous message:</strong> <a href="12140.html">Phil Goetz: "The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<li><strong>In reply to:</strong> <a href="12140.html">Phil Goetz: "The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12146.html">Phil Goetz: "Re: The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<li><strong>Reply:</strong> <a href="12146.html">Phil Goetz: "Re: The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12141">[ date ]</a>
<a href="index.html#12141">[ thread ]</a>
<a href="subject.html#12141">[ subject ]</a>
<a href="author.html#12141">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
