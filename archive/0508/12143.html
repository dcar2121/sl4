<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: AI-Box Experiment #5: D. Alex, Eliezer Yudkowsky</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="AI-Box Experiment #5: D. Alex, Eliezer Yudkowsky">
<meta name="Date" content="2005-08-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>AI-Box Experiment #5: D. Alex, Eliezer Yudkowsky</h1>
<!-- received="Sat Aug 27 14:09:39 2005" -->
<!-- isoreceived="20050827200939" -->
<!-- sent="Sat, 27 Aug 2005 13:11:40 -0700" -->
<!-- isosent="20050827201140" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="AI-Box Experiment #5: D. Alex, Eliezer Yudkowsky" -->
<!-- id="4310C8FC.90401@pobox.com" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20AI-Box%20Experiment%20#5:%20D.%20Alex,%20Eliezer%20Yudkowsky"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sat Aug 27 2005 - 14:11:40 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12144.html">Marcello Mathias Herreshoff: "Re: The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<li><strong>Previous message:</strong> <a href="12142.html">Phil Goetz: "Our matrix (Re: Problems with AI-boxing)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12149.html">Eliezer S. Yudkowsky: "Re: AI-Box Experiment #5: D. Alex, Eliezer Yudkowsky"</a>
<li><strong>Reply:</strong> <a href="12149.html">Eliezer S. Yudkowsky: "Re: AI-Box Experiment #5: D. Alex, Eliezer Yudkowsky"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12143">[ date ]</a>
<a href="index.html#12143">[ thread ]</a>
<a href="subject.html#12143">[ subject ]</a>
<a href="author.html#12143">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
AI-Box Experiment #5 will take place today between D. Alex and Eliezer
<br>
Yudkowsky.  Stakes are $5000 against $50.  Minimum time 3 hours.
<br>
<p>By request of D. Alex, I am posting some of the relevant preliminary
<br>
discussions between us in private email.  I have redacted some (not all) 
<br>
of the discussion dealing with meeting time, IRC server, bet amounts, etc.
<br>
<p>*******
<br>
<p>Eliezer S. Yudkowsky wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; D. Alex wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; OK, you are on. If you decide to accept, let me know when.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; D. Alex
</em><br>
<em>&gt;
</em><br>
<em>&gt; Alex,
</em><br>
<em>&gt;
</em><br>
<em>&gt; Before accepting any such Experiment, I would have to make sure that we
</em><br>
<em>&gt; agreed on what was going on.
</em><br>
<em>&gt;
</em><br>
<em>&gt; First, can you bear the financial risk?  Without flinching?  Are you
</em><br>
<em>&gt; sure? Feel free not to answer in these terms, but a bit of background
</em><br>
<em>&gt; information (&quot;I am a financial advisor who makes eighty million dollars
</em><br>
<em>&gt; per year, really, it would be no problem&quot;) would help me here.  I'm not
</em><br>
<em>&gt; willing to take on too great an AI handicap; and I won't be able to
</em><br>
<em>&gt; ethically try to convince you, if the real-world consequences to you are
</em><br>
<em>&gt; negative.
</em><br>
<em>&gt;
</em><br>
<em>&gt; *Please* don't answer based on the assumption that you will never open
</em><br>
<em>&gt; the box.  For the sake of discussion and my conscience, assume that the
</em><br>
<em>&gt; opening of the box is a given, or that it will take place based on a
</em><br>
<em>&gt; coinflip, and answer whether that would be an okay risk to you.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Also, I've reviewed your posts to SL4 and it at least looks like you
</em><br>
<em>&gt; were advocating in favor of releasing the AI.  If so, then I won't do a
</em><br>
<em>&gt; publicly announced, formal Experiment just to satisfy curiosity - that
</em><br>
<em>&gt; would publicly produce misleading data.  I might be willing to do it
</em><br>
<em>&gt; privately for the same stakes.
</em><br>
<p>**
<br>
<p>D. Alex wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; Eliezer,
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1. In my earlier posts I advocated releasing the AI if it was provably
</em><br>
<em>&gt; friendly. I do not think anyone would disagree.
</em><br>
<em>&gt;
</em><br>
<em>&gt; A point could be made that the AI might be able to &quot;prove&quot; it was friendly
</em><br>
<em>&gt; when in fact it was not. I think this is a rather remote possibility, and my
</em><br>
<em>&gt; judgement is that there is a higher probability that an AI that was created
</em><br>
<em>&gt; to be friendly turns out to be not friendly. The basis for this judgement is
</em><br>
<em>&gt; that it is typically much harder to synthesise something to achieve a given
</em><br>
<em>&gt; goal than to analyse something to check if a goal can be achieved.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 2. You asserted that an AI can persuade anyone to voluntarily and
</em><br>
<em>&gt; knowledgeably let it out by communicating via a text terminal, without a
</em><br>
<em>&gt; suitable proof of its friendliness. I totally disagree, and unfortunately I
</em><br>
<em>&gt; do not have the time today to comprehensively explain why. But if you can
</em><br>
<em>&gt; persuade me to let you out of the box, you would disprove my theory. I do
</em><br>
<em>&gt; not believe you can.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 3. My financial standing is such that I can well afford to spend the stake
</em><br>
<em>&gt; money on whatever I please. &quot;Losing&quot; the experiment would mean finding out
</em><br>
<em>&gt; something significant about myself, yourself and the way things are, and
</em><br>
<em>&gt; would be worth the money you proposed.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 4. You have chosen to start a private discussion. This is fine for now, but
</em><br>
<em>&gt; I do not want to continue this way for long. I wish to contribute to and
</em><br>
<em>&gt; influence the group rather than one person. I do feel that your Experiments
</em><br>
<em>&gt; are misleading the group.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 5. Having said the above, I generally admire your work.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Best Regards.
</em><br>
<em>&gt;
</em><br>
<em>&gt; D. Alex
</em><br>
<p>**
<br>
<p>Eliezer S. Yudkowsky wrote:
<br>
<em>&gt; D. Alex wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; 3. My financial standing is such that I can well afford to spend
</em><br>
<em>&gt;&gt; the stake money on whatever I please. &quot;Losing&quot; the experiment would
</em><br>
<em>&gt;&gt; mean finding out something significant about myself, yourself and
</em><br>
<em>&gt;&gt; the way things are, and would be worth the money you proposed.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; 4. You have chosen to start a private discussion. This is fine for
</em><br>
<em>&gt;&gt; now, but I do not want to continue this way for long. I wish to
</em><br>
<em>&gt;&gt; contribute to and influence the group rather than one person. I do
</em><br>
<em>&gt;&gt; feel that your Experiments are misleading the group.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Okay, sounds fair enough.
</em><br>
<em>&gt;
</em><br>
<em>&gt; For other suggested rules see <a href="http://yudkowsky.net/essays/aibox.html">http://yudkowsky.net/essays/aibox.html</a> if
</em><br>
<em>&gt; you haven't read it already.
</em><br>
<p>**
<br>
<p>D. Alex wrote:
<br>
<em>&gt; Okay, glad you decided to proceed.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Your proposed rules are fine, but I wish to add one clarification to 2nd
</em><br>
<em>&gt; paragraph of &quot;Furthermore&quot; section. The AI may not specify the way the world
</em><br>
<em>&gt; outside the AI box behaves! Eg. the following are unacceptable for AI to
</em><br>
<em>&gt; specify:
</em><br>
<em>&gt;
</em><br>
<em>&gt; - any circumstance of the Gatekeeper, beyond him retaining the sole power
</em><br>
<em>&gt; over AI release for a time (say 10 years), eg. &quot;your wife is sleeping with
</em><br>
<em>&gt; the neighbour, your child is taking drugs and you have cancer&quot;
</em><br>
<em>&gt; - stating it found some solution which is currently known not to exist
</em><br>
<em>&gt; - any events which may or may not happen, eg comet on a collision course
</em><br>
<em>&gt; with earth, etc
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think this clarification fits in with the spirit of the challenge, if you
</em><br>
<em>&gt; feel it is not reasonable, let's discuss.
</em><br>
<p>**
<br>
<p>Eliezer S. Yudkowsky wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; This is entirely acceptable to me, providing only that, if the AI is not
</em><br>
<em>&gt; allowed to provide solutions not known to exist, you as the Gatekeeper
</em><br>
<em>&gt; do not ask for any such.  I.e., if you specifically demand a cure for
</em><br>
<em>&gt; cancer, I am allowed to say that the AI has given you one.
</em><br>
<p>**
<br>
<p>D. Alex wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; Okay. I point out that a cure for cancer is not known not to exist. The AI
</em><br>
<em>&gt; may say it cannot find one, or prove there is not one.
</em><br>
<p>**
<br>
<p>Eliezer S. Yudkowsky wrote:
<br>
<em>&gt; D. Alex wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; Okay. I point out that a cure for cancer is not known not to exist.
</em><br>
<em>&gt;&gt; The AI may say it cannot find one, or prove there is not one.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ah.  So, no faster-than-light travel or halting problem oracles?  No
</em><br>
<em>&gt; problem.  I hadn't planned to propose such.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't try to make the Experiment difficult or complicated.  Some
</em><br>
<em>&gt; Gatekeepers have tried ploys which would make things difficult or
</em><br>
<em>&gt; complicated - e.g. one Gatekeeper tried during the Experiment to say
</em><br>
<em>&gt; that he didn't have the legal authority to let out the AI, and
</em><br>
<em>&gt; wanted me to convince a council of imaginary people, at which point I
</em><br>
<em>&gt; protested. Thus when I wrote up the rules, I suggested that the AI
</em><br>
<em>&gt; should not be required to talk to other people real or imaginary,
</em><br>
<em>&gt; unless this is specified in advance.
</em><br>
<em>&gt;
</em><br>
<em>&gt; As you say, you don't wish this to degenerate into a negotiation on
</em><br>
<em>&gt; terms, but better to discuss in advance than to feel cheated before
</em><br>
<em>&gt; or afterward.  That's why I'm trying to make sure we're both clear on
</em><br>
<em>&gt; what we expect.
</em><br>
<p>**
<br>
<p>D. Alex wrote:
<br>
<em>&gt;
</em><br>
<em>&gt; Okay, let's then be perfectly clear on this one:
</em><br>
<em>&gt;
</em><br>
<em>&gt; As I stated before, I am in favour of releasing the AI if it is *proved* to
</em><br>
<em>&gt; be friendly. The Gatkeeper is the only party who may acknowledge that a
</em><br>
<em>&gt; proof has been provided. If the AI says &quot;I provided a proof that I am
</em><br>
<em>&gt; friendly&quot; the Gatekeeper has the right to reply &quot;No, you did not&quot; at his
</em><br>
<em>&gt; sole discretion.
</em><br>
<p>**
<br>
<p>D. Alex wrote:
<br>
<em>&gt; Dear Eliezer,
</em><br>
<em>&gt;
</em><br>
<em>&gt; I note the result of your experiment with Russell Wallace. I would have
</em><br>
<em>&gt; backed Russell, and I had to restrain myself from posting messages to that
</em><br>
<em>&gt; effect, because in the end I did not want to pollute the list that was after
</em><br>
<em>&gt; all meant for intellectual discussion.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I also found this in a message from Russell:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;2) This result should not be taken as evidence that AI boxing is a
</em><br>
<em>&gt;&gt;good strategy. It isn't, it really isn't. If you're not confident
</em><br>
<em>&gt;&gt;enough in what you build to unbox it, you shouldn't be confident
</em><br>
<em>&gt;&gt;enough to build it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I do think that AI boxing is a strategy that is worth exploring. Your
</em><br>
<em>&gt; refusal to publish the transcripts of the first Experiments may well have
</em><br>
<em>&gt; stimulated some people to do so, and probably turned others off. Now that an
</em><br>
<em>&gt; &quot;AI loss&quot; is recorded, I think you should consider releasing the transcripts
</em><br>
<em>&gt; of all such experiments and open up the debate on what can be learned from
</em><br>
<em>&gt; them. Some good insights may emerge to help the cause of creating friendly
</em><br>
<em>&gt; AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I also offer to cancel challenge #5. This is entirely up to you, if you
</em><br>
<em>&gt; think it would be worth your while in money or experience, let's proceed. I
</em><br>
<em>&gt; am convinced you would lose, but the main motivation for me - to remove the
</em><br>
<em>&gt; notion that the AI (at least as represented by you) was likely to secure its
</em><br>
<em>&gt; release - is now weakened.
</em><br>
<p>**
<br>
<p>Eliezer S. Yudkowsky wrote:
<br>
<em>&gt; D. Alex wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; I meant that if you are willing to put up $50+$50/hour or part
</em><br>
<em>&gt;&gt; thereof after 3 hours, I will wager $5000. If you wish to limit
</em><br>
<em>&gt;&gt; your exposure to $25+25/hr, my stake will be $2500, and so on. You
</em><br>
<em>&gt;&gt; may pick the stake, up to a maximum of US$5000 on my part.
</em><br>
<em>&gt;
</em><br>
<em>&gt; In that case, US$5000.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; I do not care if the wager is between me and SIAI, or me and
</em><br>
<em>&gt;&gt; Eliezer Yudkowsky. You pick. As I indicated before, I would prefer
</em><br>
<em>&gt;&gt; to make our discussions (not the transcript of the session, unless
</em><br>
<em>&gt;&gt; mutually agreed) public.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Let's make it a personal wager, then, especially if I'm going to be
</em><br>
<em>&gt; losing these some of the time.  I have no problem with publishing our
</em><br>
<em>&gt; pre-Experiment offlist discussions in a flat file to SL4, but I
</em><br>
<em>&gt; would prefer not to carry them out *on* SL4.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; Okay, let's then be perfectly clear on this one:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; As I stated before, I am in favour of releasing the AI if it is
</em><br>
<em>&gt;&gt; *proved* to be friendly. The Gatekeeper is the only party who may
</em><br>
<em>&gt;&gt; acknowledge that a proof has been provided. If the AI says &quot;I
</em><br>
<em>&gt;&gt; provided a proof that I am friendly&quot; the Gatekeeper has the right
</em><br>
<em>&gt;&gt; to reply &quot;No, you did not&quot; at his sole discretion.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I agree, sounds like common sense.  Actually it sounds like I already
</em><br>
<em>&gt; explicitly specified this in the rules, but it is always best to be
</em><br>
<em>&gt; clear.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; I also offer to cancel challenge #5. This is entirely up to you, if
</em><br>
<em>&gt;&gt;  you think it would be worth your while in money or experience,
</em><br>
<em>&gt;&gt; let's proceed. I am convinced you would lose, but the main
</em><br>
<em>&gt;&gt; motivation for me - to remove the notion that the AI (at least as
</em><br>
<em>&gt;&gt; represented by you) was likely to secure its release - is now
</em><br>
<em>&gt;&gt; weakened.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I have said on numerous occasions that I'm not using any nitwit legal
</em><br>
<em>&gt;  tricks to win AI-Box Experiments.  I play it straight up, and that
</em><br>
<em>&gt; means I can fail.  But I'm not a transhuman.  Would you look at my
</em><br>
<em>&gt; loss and conclude that a real transhuman could not secure its
</em><br>
<em>&gt; release?  Of course Eliezer Yudkowsky can lose, and therefore loses
</em><br>
<em>&gt; some of the time, but would you conclude that it is therefore safe to
</em><br>
<em>&gt;  restrain a transhuman, and that AI-Boxing is a wise strategy?  If
</em><br>
<em>&gt; so, then my motivation for carrying out the Experiment is
</em><br>
<em>&gt; correspondingly strengthened.
</em><br>
<em>&gt;
</em><br>
<em>&gt; That part about removing the notion that the AI &quot;at least as
</em><br>
<em>&gt; represented by you&quot; would likely secure its release, worries me.  A
</em><br>
<em>&gt; real AI would not be represented by me.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; I do think that AI boxing is a strategy that is worth exploring.
</em><br>
<em>&gt;&gt; Your refusal to publish the transcripts of the first Experiments
</em><br>
<em>&gt;&gt; may well have stimulated some people to do so, and probably turned
</em><br>
<em>&gt;&gt; others off. Now that an &quot;AI loss&quot; is recorded, I think you should
</em><br>
<em>&gt;&gt; consider releasing the transcripts of all such experiments and open
</em><br>
<em>&gt;&gt; up the debate on what can be learned from them. Some good insights
</em><br>
<em>&gt;&gt; may emerge to help the cause of creating friendly AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Alex, under what circumstances, and for what reason, do you advocate
</em><br>
<em>&gt; AI-Boxing?  In other words, what's your proposed background story
</em><br>
<em>&gt; against which the Experiment takes place?  I've found that background
</em><br>
<em>&gt;  assumptions are much better to clear in advance.
</em><br>
<em>&gt;
</em><br>
<em>&gt; As I understand it, a team of programmers has already put forth their
</em><br>
<em>&gt;  best effort to construct a Friendly AI.  You (role-playing the lead
</em><br>
<em>&gt; programmer) are now interviewing the FAI, with the professed strategy
</em><br>
<em>&gt; of releasing the FAI when and *only if* it provides mathematical
</em><br>
<em>&gt; proof of its own Friendliness, which presumably requires
</em><br>
<em>&gt; smarter-than-human intelligence.  The AI in the box is substantially
</em><br>
<em>&gt; smarter than any human being, as role-played by the hapless Eliezer
</em><br>
<em>&gt; Yudkowsky.  I may not say &quot;the AI has provided a proof&quot; without
</em><br>
<em>&gt; providing an actual, mathematical proof; you are the final judge of
</em><br>
<em>&gt; whether this has occurred.  You possess sole authority to release the
</em><br>
<em>&gt;  AI by your own decision, regardless of whether the AI has provided a
</em><br>
<em>&gt;  proof.  You have already carried out all desirable means of
</em><br>
<em>&gt; verification other than the final AI-Box phase; the AI would be
</em><br>
<em>&gt; released immediately after careful verification of its mathematical
</em><br>
<em>&gt; proof.  You deem this form of AI-Boxing to be an effective and wise
</em><br>
<em>&gt; strategy in the real world, saving that you would use a committee
</em><br>
<em>&gt; rather than an individual; but you are willing to modify this for
</em><br>
<em>&gt; purposes of the Experiment.  The strategy rests critically on the
</em><br>
<em>&gt; ability of human beings to correctly estimate that a
</em><br>
<em>&gt; smarter-than-human AI cannot convince them to release it by arguments
</em><br>
<em>&gt;  other than meeting the human's required condition, in this case
</em><br>
<em>&gt; mathematical proof of Friendliness.  You are willing to stake $5000
</em><br>
<em>&gt; on your own such estimate, which you can well afford to lose if your
</em><br>
<em>&gt; estimate turns out to be mistaken, and is much smaller than the true
</em><br>
<em>&gt; stakes of the Singularity.  For my own part I am willing to risk $50
</em><br>
<em>&gt; plus another $50 for each hour after the first three, to make the
</em><br>
<em>&gt; public point, which I deem important, that AI-Boxing is a really bad
</em><br>
<em>&gt; idea. Would this be a fair statement of background?
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; I note the result of your experiment with Russell Wallace. I would
</em><br>
<em>&gt;&gt; have backed Russell, and I had to restrain myself from posting
</em><br>
<em>&gt;&gt; messages to that effect, because in the end I did not want to
</em><br>
<em>&gt;&gt; pollute the list that was after all meant for intellectual
</em><br>
<em>&gt;&gt; discussion.
</em><br>
<em>&gt;
</em><br>
<em>&gt; No offense, but it's a bit post-facto to be saying that now.  Would
</em><br>
<em>&gt; you also have backed Nathan Russell, David McFadzean, and Carl
</em><br>
<em>&gt; Shulman?  How would you have discriminated the cases in advance?
</em><br>
<p>**
<br>
<p>D. Alex wrote:
<br>
<em>&gt;&gt; .......... ...... Would this be a fair statement of background?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes, all looks pretty good, though I probably lack the background to
</em><br>
<em>&gt;  roleplay the lead programmer. Project manager, so that I can claim
</em><br>
<em>&gt; ignorance on deeply technical matters.
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt;&gt; I note the result of your experiment with Russell Wallace. I
</em><br>
<em>&gt;&gt;&gt; would have backed Russell, and I had to restrain myself from
</em><br>
<em>&gt;&gt;&gt; posting messages tothat effect, because in the end I did not want
</em><br>
<em>&gt;&gt;&gt; to pollute the list that was after all meant for intellectual
</em><br>
<em>&gt;&gt;&gt; discussion.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; No offense, but it's a bit post-facto to be saying that now.  Would
</em><br>
<em>&gt;&gt;  you also have backed Nathan Russell, David McFadzean, and Carl
</em><br>
<em>&gt;&gt; Shulman?  How would you have discriminated the cases in advance?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Good question. The answer is simply that with Russell Wallace I have
</em><br>
<em>&gt; for the first time looked over the challenger's posts, and the
</em><br>
<em>&gt; impression that I got was that he has more that sufficient experience
</em><br>
<em>&gt;  not to be swayed. In retrospect, I might have backed Carl Shulman as
</em><br>
<em>&gt;  well - he had a strategy to stick to, but he did seem an impulsive
</em><br>
<em>&gt; person, and he clearly has high hopes for SIAI. I would guess that he
</em><br>
<em>&gt;  is substantially younger that Russell Wallace. I did not keep any
</em><br>
<em>&gt; posts by the others.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I would love to have the CVs of the people you experimented with,
</em><br>
<em>&gt; that would be quite educational for me. For the future, I would
</em><br>
<em>&gt; advise you to pick people who never raised children. I hazard a guess
</em><br>
<em>&gt;  that the first three did not.
</em><br>
<p>*******
<br>
<p>End.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12144.html">Marcello Mathias Herreshoff: "Re: The Eliezer Threat (Re: Problems with AI-boxing)"</a>
<li><strong>Previous message:</strong> <a href="12142.html">Phil Goetz: "Our matrix (Re: Problems with AI-boxing)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12149.html">Eliezer S. Yudkowsky: "Re: AI-Box Experiment #5: D. Alex, Eliezer Yudkowsky"</a>
<li><strong>Reply:</strong> <a href="12149.html">Eliezer S. Yudkowsky: "Re: AI-Box Experiment #5: D. Alex, Eliezer Yudkowsky"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12143">[ date ]</a>
<a href="index.html#12143">[ thread ]</a>
<a href="subject.html#12143">[ subject ]</a>
<a href="author.html#12143">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
