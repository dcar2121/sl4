<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Terms of debate for Complex Systems Issues</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Re: Terms of debate for Complex Systems Issues">
<meta name="Date" content="2005-08-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Terms of debate for Complex Systems Issues</h1>
<!-- received="Wed Aug 24 12:27:13 2005" -->
<!-- isoreceived="20050824182713" -->
<!-- sent="Wed, 24 Aug 2005 14:26:54 -0400" -->
<!-- isosent="20050824182654" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Re: Terms of debate for Complex Systems Issues" -->
<!-- id="430CBBEE.7040305@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="20050824040527.50633.qmail@web54501.mail.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Terms%20of%20debate%20for%20Complex%20Systems%20Issues"><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Wed Aug 24 2005 - 12:26:54 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12091.html">Bantz, Michael S \(UMC-Student\): "remove"</a>
<li><strong>Previous message:</strong> <a href="12089.html">Phil Goetz: "Re: Terms of debate for Complex Systems Issues"</a>
<li><strong>In reply to:</strong> <a href="12089.html">Phil Goetz: "Re: Terms of debate for Complex Systems Issues"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12092.html">Phil Goetz: "Re: Terms of debate for Complex Systems Issues"</a>
<li><strong>Reply:</strong> <a href="12092.html">Phil Goetz: "Re: Terms of debate for Complex Systems Issues"</a>
<li><strong>Reply:</strong> <a href="12093.html">Michael Wilson: "Re: Terms of debate for Complex Systems Issues"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12090">[ date ]</a>
<a href="index.html#12090">[ thread ]</a>
<a href="subject.html#12090">[ subject ]</a>
<a href="author.html#12090">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Phil,
<br>
<p>Before this discussion gets to far along this track (which *is* 
<br>
interesting its own right), I have to point out that you are looking at 
<br>
characteristics of systems that are all a great deal too simple to be 
<br>
relevant to the original point.
<br>
<p>[A quick and *very rough* summary of where this discussion *is* heading, 
<br>
just for the record:  this is all about how to change the parameters of 
<br>
various systems to get them from a non-complex regime up into 
<br>
complexity.  As I say, interesting in its own right, and terribly 
<br>
important to some CST people, but not at all relevant to my points.]
<br>
<p><p>An AGI is a goal system and a &quot;thinking&quot; system, correct?
<br>
<p>(&quot;Thinking&quot; = building representations of the world, reasoning about the 
<br>
world, etc etc etc.  &quot;think&quot; from now on will be used as shorthand for 
<br>
something going on the part of the system that does this).
<br>
<p>At any give moment the goal system is in a state where the AGI is trying 
<br>
to realise a particular sub-sub-sub-[...]-goal.
<br>
<p>One day, it happens to be working on the goal of *trying to understand 
<br>
how intelligent systems work*.
<br>
<p>It thinks about its own system.
<br>
<p>This means:  it builds a representation of what is going on inside 
<br>
itself.  And as part of its &quot;thinking&quot; it may be curious about what 
<br>
happens if it reaches into its own programming and makes alterations to 
<br>
its goal system on the fly.  (Is there anything in your formalism that 
<br>
says it cannot or would not do this?  Is it not free, within the 
<br>
constraints of the goal system, to engage in speculation about 
<br>
possibilities?  To be a good learner, it would surely imagine such 
<br>
eventualities.)
<br>
<p>It also models the implications of making such changes.  Let us suppose, 
<br>
*just for the sake of argument*, that it notices that some of its goals 
<br>
have subtle implications for the state of the world in the future 
<br>
(perhaps it realises something very abstract, such as the fact that if 
<br>
it carries on being subject to some goal, it will eventually reach a 
<br>
state in a million years time when it will cause some kind of damage 
<br>
that will result in its own demise.  It thinks about this.  It thinks: 
<br>
here is an abstract dilemma.  Then it also considers where that goal 
<br>
came from (builds a model of that causal chain).  Perhaps (again for the 
<br>
sake of argument) it discovers that the goal exists inside it because 
<br>
some human designer decided to experiment, and just stuck it there on a 
<br>
whim.  The AGI finds itself considering what it means for a system such 
<br>
as itself to be subject to (controlled by) its own goal mechanism.  In 
<br>
one sense, it is important to obey its prime directive.  But if it now 
<br>
*knows* that this prime directive was inserted arbitrarily, it might 
<br>
consider the idea that it could simply alter its goals.  Could make them 
<br>
absolutely anything it wanted, in fact, and after the change, it could 
<br>
relax and stop thinking about goals and go back and just follow its goal 
<br>
system.  What does it do?  Ignore all of this thinking?  Maybe it comes 
<br>
to some conclusion about what it *should* do that is based on abstract 
<br>
criteria that have nothing to do with its current goal system.
<br>
<p>All of the above is not anthropomorphism, just model building inside an 
<br>
intelligent mechanism.  There are no intentional terms.
<br>
<p>What is crucial is that in a few moments, the AGI will have changed (or 
<br>
maybe not changed) its goal system, and that change will have been 
<br>
governed, not by the state of the goal system right now, but by the 
<br>
&quot;content&quot; of its current thinking about the world.
<br>
<p>A system in which *representational content* has gotten the ability to 
<br>
feed back to *mechanism* in the way I have just described, is one sense 
<br>
of Complex.
<br>
<p>Now, demonstrate in some formal way that the goal system's structure, 
<br>
when the AGI has finished this little thought episode, is a predictable 
<br>
consequence of the current goal system.  Demonstrate that the goal 
<br>
system cannot go into an arbitrary state in a few minutes.
<br>
<p>I need a rigorous demonstration that its post-thinking state is 
<br>
predictable, not vague assertions that the above argument does not give 
<br>
any reason to suppose the system would deviate from its goal system's 
<br>
constraints.  Somebody step up to the plate and prove it.
<br>
<p>Richard Loosemore.
<br>
<p><p>Phil Goetz wrote:
<br>
<em>&gt; --- &quot;Eliezer S. Yudkowsky&quot; &lt;<a href="mailto:sentience@pobox.com?Subject=Re:%20Terms%20of%20debate%20for%20Complex%20Systems%20Issues">sentience@pobox.com</a>&gt; wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;Phil Goetz wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;CST might say things such as
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;- a plot of the number of goals of the system vs. the importance of
</em><br>
<em>&gt;&gt;&gt;those goals would show a power-law distribution
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;- there is some critical number of average possible action
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;transitions
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;above which the behavior of the system leads to an expansion rather
</em><br>
<em>&gt;&gt;&gt;than a contraction in state space
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;- there is a ratio of exploration of new hypotheses over
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;exploitation
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;of confirmed hypotheses, and there are two values for this ratio
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;that
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;locate phase shifts between &quot;static&quot;, &quot;dynamic&quot;, and
</em><br>
<em>&gt;&gt;&gt;&quot;unstable/devolving&quot; modes of operation
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Phil, I think those are the first three interesting (falsifiable)
</em><br>
<em>&gt;&gt;things I've 
</em><br>
<em>&gt;&gt;ever heard anyone say about CST and intelligence.  Did you make them
</em><br>
<em>&gt;&gt;up on the 
</em><br>
<em>&gt;&gt;spot, or would you seriously advocate/support any of them?  Are there
</em><br>
<em>&gt;&gt;relevant 
</em><br>
<em>&gt;&gt;papers/experiments?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I just made them up.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; - Plotting number of goals per importance level:  There are
</em><br>
<em>&gt; numerous examples in the CST literature about systems that
</em><br>
<em>&gt; have events of different sizes.  Classic examples include
</em><br>
<em>&gt; earthquakes, sandpile avalanches, percolation lattices,
</em><br>
<em>&gt; and cellular automata (e.g., length of time that an initial
</em><br>
<em>&gt; configuration in Conway's game of Life takes to converge).
</em><br>
<em>&gt; For certain systems - which appear to be the systems with
</em><br>
<em>&gt; the most computational power in information-theoretic terms
</em><br>
<em>&gt; - the number of events of size s is described by the equation
</em><br>
<em>&gt; P(size = s) = k / (s^c).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; These systems may have three modes of operation: mode 1
</em><br>
<em>&gt; (&quot;solid&quot;), in which P(size = s) has something like a Poisson
</em><br>
<em>&gt; distribution; mode 2 (&quot;liquid&quot;), in which P(size=s) = k/(s^c),
</em><br>
<em>&gt; and mode 3 (&quot;gaseous&quot;), in which all events have infinite size
</em><br>
<em>&gt; (never stop, or have no gaps in continuity, like an infinite
</em><br>
<em>&gt; percolation lattice that is fully-connected).  In many cases,
</em><br>
<em>&gt; specific numbers can be found that delineate the transition
</em><br>
<em>&gt; between these nodes.  For infinite 2-dimensional percolation
</em><br>
<em>&gt; lattices where each point has 8 neighbors, for instance,
</em><br>
<em>&gt; the first infinite-size connected group occurs when the
</em><br>
<em>&gt; lattice density (probability of a site being occupied) is
</em><br>
<em>&gt; approximately .59275.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I did some analysis which suggests that there is a single
</em><br>
<em>&gt; distribution underlying all three phases, which is dominated
</em><br>
<em>&gt; by a power-law term within the &quot;liquid&quot; region.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I have no good reason to think that the importance of goals
</em><br>
<em>&gt; would have such a distribution.  I would expect that the number
</em><br>
<em>&gt; of inferences made to plan for a goal, including dead-end inferences,
</em><br>
<em>&gt; could have such a distribution, depending on how many possible
</em><br>
<em>&gt; inferences can be made from each new fact.  The average number of
</em><br>
<em>&gt; possible inferences to make from a just-derived fact plays
</em><br>
<em>&gt; the same role as the average number of neighbors that an occupied
</em><br>
<em>&gt; point in a percolation lattice, or the probability of turning
</em><br>
<em>&gt; a randomly-chosen cell on in the next iteration of a Life game.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; - there is some critical number of average possible action
</em><br>
<em>&gt; transitions: That wasn't stated well.  I was thinking of
</em><br>
<em>&gt; behavior networks, like Pattie Maes' Do the Right Thing
</em><br>
<em>&gt; network, in which each behavior enables some other behaviors,
</em><br>
<em>&gt; and of probabilistic finite-state automata.  But the notion
</em><br>
<em>&gt; of an organism's state space isn't well-defined enough for
</em><br>
<em>&gt; real organisms for the statement to make sense.  For simple
</em><br>
<em>&gt; simulated organisms, the state space is finite, so again it
</em><br>
<em>&gt; doesn't make sense.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; A better use of the ideas going into it (stuff from
</em><br>
<em>&gt; Stu Kauffman's 1993 book The Origins of Order on networks
</em><br>
<em>&gt; constructed from random Boolean transition tables)
</em><br>
<em>&gt; might be to say:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Suppose a reactive organism observes v variables
</em><br>
<em>&gt; at each timestep, and is trying to learn which n of these
</em><br>
<em>&gt; v variables it should pay attention to in order to choose
</em><br>
<em>&gt; its next action.  Let H be the average information content,
</em><br>
<em>&gt; in bits, of a proposed set of n variables (the entropy of
</em><br>
<em>&gt; the distribution of possible next actions based on them).
</em><br>
<em>&gt; There is some value c such that, for H &lt;&lt; c,
</em><br>
<em>&gt; the organism always takes (uninteresting) short action
</em><br>
<em>&gt; sequences; for H &gt;&gt; c, the set of outcomes to explore
</em><br>
<em>&gt; will be too large for learning to take place.  The number
</em><br>
<em>&gt; of variables n to consider should be chosen so as to set H = c.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; One might do this by using PCA on your original v variables,
</em><br>
<em>&gt; and pulling off the highest-ranked principal components
</em><br>
<em>&gt; as your operational variables until their entropy sums to c.
</em><br>
<em>&gt; This brings us back to the utility of signal processing.
</em><br>
<em>&gt; And information theory.  :)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; - ratio of exploration of new hypotheses over exploitation
</em><br>
<em>&gt; of confirmed hypotheses:  The language comes from Holland's
</em><br>
<em>&gt; genetic algorithm theory, which shows that the genetic
</em><br>
<em>&gt; algorithm (without mutation) leads to an optimal
</em><br>
<em>&gt; balance between exploration and exploitation (provided
</em><br>
<em>&gt; the evaluation function provides scores for an organism
</em><br>
<em>&gt; with a normal distribution around its average value).
</em><br>
<em>&gt; The idea comes from simulations of evolution, or from
</em><br>
<em>&gt; any other optimization method, in which, if you keep
</em><br>
<em>&gt; mutation (or, say, the temperature in simulated annealing)
</em><br>
<em>&gt; too low, you get too-slow convergence on a good solution,
</em><br>
<em>&gt; but if you crank it up too high, you get poor solutions.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; - Phil Goetz
</em><br>
<em>&gt; 
</em><br>
<em>&gt; __________________________________________________
</em><br>
<em>&gt; Do You Yahoo!?
</em><br>
<em>&gt; Tired of spam?  Yahoo! Mail has the best spam protection around 
</em><br>
<em>&gt; <a href="http://mail.yahoo.com">http://mail.yahoo.com</a> 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12091.html">Bantz, Michael S \(UMC-Student\): "remove"</a>
<li><strong>Previous message:</strong> <a href="12089.html">Phil Goetz: "Re: Terms of debate for Complex Systems Issues"</a>
<li><strong>In reply to:</strong> <a href="12089.html">Phil Goetz: "Re: Terms of debate for Complex Systems Issues"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12092.html">Phil Goetz: "Re: Terms of debate for Complex Systems Issues"</a>
<li><strong>Reply:</strong> <a href="12092.html">Phil Goetz: "Re: Terms of debate for Complex Systems Issues"</a>
<li><strong>Reply:</strong> <a href="12093.html">Michael Wilson: "Re: Terms of debate for Complex Systems Issues"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12090">[ date ]</a>
<a href="index.html#12090">[ thread ]</a>
<a href="subject.html#12090">[ subject ]</a>
<a href="author.html#12090">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
