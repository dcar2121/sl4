<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Paperclip monster, demise of.</title>
<meta name="Author" content="Michael Wilson (mwdestinystar@yahoo.co.uk)">
<meta name="Subject" content="Re: Paperclip monster, demise of.">
<meta name="Date" content="2005-08-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Paperclip monster, demise of.</h1>
<!-- received="Wed Aug 17 18:33:06 2005" -->
<!-- isoreceived="20050818003306" -->
<!-- sent="Thu, 18 Aug 2005 01:33:03 +0100 (BST)" -->
<!-- isosent="20050818003303" -->
<!-- name="Michael Wilson" -->
<!-- email="mwdestinystar@yahoo.co.uk" -->
<!-- subject="Re: Paperclip monster, demise of." -->
<!-- id="20050818003303.51158.qmail@web26706.mail.ukl.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="4303C16A.30909@lightlink.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Michael Wilson (<a href="mailto:mwdestinystar@yahoo.co.uk?Subject=Re:%20Paperclip%20monster,%20demise%20of."><em>mwdestinystar@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Wed Aug 17 2005 - 18:33:03 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11768.html">Michael Roy Ames: "Re: Paperclip monster, demise of."</a>
<li><strong>Previous message:</strong> <a href="11766.html">Chris Capel: "Re: On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11761.html">Richard Loosemore: "Paperclip monster, demise of."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11770.html">Richard Loosemore: "Re: Paperclip monster, demise of."</a>
<li><strong>Reply:</strong> <a href="11770.html">Richard Loosemore: "Re: Paperclip monster, demise of."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11767">[ date ]</a>
<a href="index.html#11767">[ thread ]</a>
<a href="subject.html#11767">[ subject ]</a>
<a href="author.html#11767">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; This hypothetical paperclip monster is being used in ways that are 
</em><br>
<em>&gt; incoherent, which interferes with the clarity of our arguments.
</em><br>
<p>The problem is not that we don't understand your position. It is a
<br>
common position that has been put forward by numerous people with
<br>
anthropic expectations of how AGI cognition will work. The problem is
<br>
that you do not understand the opposing position; how reasoning about
<br>
goals works when the goals are all open to reflective examination and
<br>
modification. You are incorrectly postulating that various quirks of
<br>
human cognition, which most readers are well aware of, apply to
<br>
intelligences in general.
<br>
<p><em>&gt; It is supposed to be so obsessed that it cannot even conceive of other 
</em><br>
<em>&gt; goals, or it cannot understand them, or it is too busy to stop and think 
</em><br>
<em>&gt; of them, or maybe it is incapable of even representing anything except 
</em><br>
<em>&gt; the task of paperclipization...... or something like that.
</em><br>
<p>No one has claimed this; in fact the opposite has been stated repeatadly.
<br>
A general intelligence is perfectly capable or representing any goal
<br>
system it likes, as well as modelling alternate cognitive architectures
<br>
and external agents of any stripe. Indeed the potential impressiveness of
<br>
this capability is the source of arguments such as 'an AGI could convince
<br>
any human to let it out of containment with the right choice of arguments
<br>
to make over a text terminal' - an achievement that relies on very good
<br>
subjunctive goal system and cognitive architecture modelling.
<br>
<p>Whether a system will actually 'think about' any given subjunctive goal
<br>
system depends on whether its existing goal system makes it desireable
<br>
to do so. Building a model of a human's goal system and then using it
<br>
to infer an argument that is likely to result in that human's obedience
<br>
would be goal-relevant for an AGI that wanted to get out of an 'AI box'.
<br>
Curiosity is a logical subgoal of virtually any goal system, as new
<br>
information almost always has some chance of being useful in some future
<br>
goal-relevant decision.
<br>
<p><em>&gt; Anyhow, the obsession is so complete that the paperclip monster is 
</em><br>
<em>&gt; somehow exempt from the constraints that might apply to a less 
</em><br>
<em>&gt; monomaniacal AI.
</em><br>
<p>What are these 'constraints'? An attempt to design a goal system with
<br>
'checks and-balances'? A goal system is simply a compact function
<br>
defining a preference order over actions, or universe states, or
<br>
something else that can be used to rank actions. If such a function
<br>
is not stable under self-modification, then it will traverse the space
<br>
of unstable goal systems (as it self-modifies) until it falls into a
<br>
stable attractor. It is possible to concieve exceptions to this rule,
<br>
such as a transhuman who treats avoidance of 'stagnation' as their
<br>
supergoal and thus has a perpetually changing goal system, but this
<br>
is an incredibly tiny area of the space of possible goal systems which
<br>
is correspondingly unlikely to be entered by 'accident'.
<br>
&nbsp;
<br>
<em>&gt; I submit that the concept is grossly inconsistent.  If it is a
</em><br>
<em>&gt; *general* AI, it must have a flexible, adaptive representation system
</em><br>
<em>&gt; that lets it model all kinds of things in the universe, including itself.
</em><br>
<p>Of course. You still haven't explained why a paperclip maximiser would
<br>
find anything wrong with its self model that would require alteration of
<br>
its supergoal to fix.
<br>
<p><em>&gt; But whenever the Paperclip Monster is cited, it comes across as too
</em><br>
<em>&gt; dumb to be a GAI ...
</em><br>
<p>You keep claiming that there is a connection between intelligence and
<br>
desires, in general not just for humans, yet you have not made a single
<br>
convincing argument why this is so. Frankly the only possible
<br>
justification I can see is an effectively religious belief in an
<br>
'objective morality' that transhumans will inevitably discover and that
<br>
will magically reverse the causal relationship between goals and
<br>
inference.
<br>
<p><em>&gt; and it does perceive within itself a strong compulsion to make
</em><br>
<em>&gt; paperclips, and it does understand the fact that this compulsion is
</em><br>
<em>&gt; somewhat arbitrary .... and so on.
</em><br>
<p>Ah, here we go. You presumably believe 'arbitrariness' is a bad thing
<br>
(most humans do). Why would an AGI believe this?
<br>
&nbsp;
<br>
<em>&gt; we sometimes make the mistake of positing general intelligence, and
</em><br>
<em>&gt; then selectively withdrawing that intelligence in specific scenarios,
</em><br>
<em>&gt; as it suits us,
</em><br>
<p>Intelligence is a tool to achieve goals (evolution produced it for exactly
<br>
that reason). If the goal is not there, an agent can be all-knowing and 
<br>
all-powerful and will still do nothing to achieve the missing goal.
<br>
<p><em>&gt; I am not saying that anyone is doing this deliberately or deceiptfully, 
</em><br>
<em>&gt; of course, just that we have to be vary wary of that trap, because it is 
</em><br>
<em>&gt; an easy mistake to make, and sometimes it is very subtle.
</em><br>
<p>There are a lots of easy mistakes and subtle traps in AGI. In this case
<br>
you're the one that's fallen into the trap.
<br>
<p><em>&gt; Does anyone else except me understand what I am driving at here?
</em><br>
<p>Understand your mistake and why you made it, yes.
<br>
<p>&nbsp;* Michael Wilson
<br>
<p>P.S. I actually quite like the term 'paperclip maximiser', because it
<br>
sounds both familiar and bizarre, even ridiculous, at the same time. No
<br>
one is claiming that the Singularity is actually likely to consist of
<br>
us being turned into paperclips. But it conveys the fact that nature is
<br>
not obliged to conform to our intuitions of 'common sense', and that the
<br>
future is not oblidged to look like science fiction. The boundaries of
<br>
science frequently return results that seem ridiculous to the layman, and
<br>
the future often turns out to be counter-intuitive. It is unfortunate
<br>
that people are particularly prone to jump in and use their 'intuition'
<br>
in cognitive science, compared to other areas of science. The point has
<br>
been rammed home by now that human intuitive physics is broken and is
<br>
worse than useless for inventing useful theories of how those parts of
<br>
the universe distant from everyday experience actually work. However
<br>
many people still think that human intuitive other-mind-modelling is
<br>
useful in cognitive science simply because we haven't had enough
<br>
widely publicised progress in the field to discourage them yet.
<br>
<p><p><p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
___________________________________________________________ 
<br>
To help you stay safe and secure online, we've developed the all new Yahoo! Security Centre. <a href="http://uk.security.yahoo.com">http://uk.security.yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11768.html">Michael Roy Ames: "Re: Paperclip monster, demise of."</a>
<li><strong>Previous message:</strong> <a href="11766.html">Chris Capel: "Re: On the dangers of AI"</a>
<li><strong>In reply to:</strong> <a href="11761.html">Richard Loosemore: "Paperclip monster, demise of."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11770.html">Richard Loosemore: "Re: Paperclip monster, demise of."</a>
<li><strong>Reply:</strong> <a href="11770.html">Richard Loosemore: "Re: Paperclip monster, demise of."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11767">[ date ]</a>
<a href="index.html#11767">[ thread ]</a>
<a href="subject.html#11767">[ subject ]</a>
<a href="author.html#11767">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Tue Feb 21 2006 - 04:23:01 MST
</em></small></p>
</body>
</html>
