<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Paperclip monster, demise of.</title>
<meta name="Author" content="Richard Loosemore (rpwl@lightlink.com)">
<meta name="Subject" content="Re: Paperclip monster, demise of.">
<meta name="Date" content="2005-08-17">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Paperclip monster, demise of.</h1>
<!-- received="Wed Aug 17 22:04:27 2005" -->
<!-- isoreceived="20050818040427" -->
<!-- sent="Thu, 18 Aug 2005 00:03:58 -0400" -->
<!-- isosent="20050818040358" -->
<!-- name="Richard Loosemore" -->
<!-- email="rpwl@lightlink.com" -->
<!-- subject="Re: Paperclip monster, demise of." -->
<!-- id="430408AE.70200@lightlink.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="20050818003303.51158.qmail@web26706.mail.ukl.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Richard Loosemore (<a href="mailto:rpwl@lightlink.com?Subject=Re:%20Paperclip%20monster,%20demise%20of."><em>rpwl@lightlink.com</em></a>)<br>
<strong>Date:</strong> Wed Aug 17 2005 - 22:03:58 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="11964.html">Samantha Atkins: "Re: Shock Level 5 (SL5) - 'The Theory Of Everything'"</a>
<li><strong>Previous message:</strong> <a href="11962.html">Mitchell Porter: "RE: Paperclip monster, demise of."</a>
<li><strong>In reply to:</strong> <a href="11960.html">Michael Wilson: "Re: Paperclip monster, demise of."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11968.html">Michael Roy Ames: "Re: Paperclip monster, demise of."</a>
<li><strong>Reply:</strong> <a href="11968.html">Michael Roy Ames: "Re: Paperclip monster, demise of."</a>
<li><strong>Reply:</strong> <a href="11969.html">Michael Wilson: "Re: Paperclip monster, demise of."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11963">[ date ]</a>
<a href="index.html#11963">[ thread ]</a>
<a href="subject.html#11963">[ subject ]</a>
<a href="author.html#11963">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Michael Wilson wrote:
<br>
<em>&gt;&gt;Richard Loosemore wrote:
</em><br>
<em>&gt;&gt;This hypothetical paperclip monster is being used in ways that are 
</em><br>
<em>&gt;&gt;incoherent, which interferes with the clarity of our arguments.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The problem is not that we don't understand your position. It is a
</em><br>
<em>&gt; common position that has been put forward by numerous people with
</em><br>
<em>&gt; anthropic expectations of how AGI cognition will work. The problem is
</em><br>
<em>&gt; that you do not understand the opposing position; how reasoning about
</em><br>
<em>&gt; goals works when the goals are all open to reflective examination and
</em><br>
<em>&gt; modification. You are incorrectly postulating that various quirks of
</em><br>
<em>&gt; human cognition, which most readers are well aware of, apply to
</em><br>
<em>&gt; intelligences in general.
</em><br>
<p>This comment, like the other ones in your reply, is not related to the 
<br>
quoted text that precedes it, nor related to the overall intent of the 
<br>
original message that I sent.  This is a little frustrating.  It is 
<br>
almost as if you did not read or try to understand the simple point I 
<br>
was making in this post, but instead launched a series of arguments that 
<br>
are generally directed at all of my arguments in the other posts.  Many 
<br>
of your comments about my original &quot;Paperclip monster, demise of&quot; post 
<br>
are wild misinterpretations of the letter and spirit of what I was 
<br>
talking about, like this glaring example:
<br>
<p><em> &gt;&gt;&gt; [Loosemore:]
</em><br>
<em>&gt;&gt;&gt; and it does perceive within itself a strong compulsion to make
</em><br>
<em>&gt;&gt;&gt; paperclips, and it does understand the fact that this compulsion is
</em><br>
<em>&gt;&gt;&gt; somewhat arbitrary .... and so on.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Ah, here we go. You presumably believe 'arbitrariness' is a bad thing
</em><br>
<em>&gt; (most humans do). Why would an AGI believe this?
</em><br>
<p>Not only is the &quot;Ah, here we go&quot; pretty insulting, but you then 
<br>
interpret &quot;arbitrary&quot; to be about something that has nothing to do with 
<br>
what I was talking about, then ascribe to me a puerile value judgement, 
<br>
then criticise me for it!
<br>
<p>****************************************************************
<br>
<p><p>For the rest of this reply I will ignore non-sequiteurs like the one 
<br>
above and just address the core issues.  The following is directed not 
<br>
just at you, Brian, but at that part of the group that expresses a 
<br>
similar position.
<br>
<p>Your comment above, about my not understanding &quot;how reasoning about
<br>
goals works when the goals are all open to reflective examination and
<br>
modification&quot; and the other comments about &quot;goal systems&quot; that appear in 
<br>
your reply, all come from a very particular, narrow conception of how an 
<br>
AI should be structured.  How can I characterize it?  It is a 
<br>
symbolic-AI, goal-hierarchical, planning-system approach to AI that is a 
<br>
direct descendant of good old Winograd et al.
<br>
<p>Just for the record, I know perfectly well what kind of goal system you 
<br>
are referring to.  (I have written such systems, and taught postgrads 
<br>
how to write them).  But I have also just spent 6000 words trying to 
<br>
communicate to various posters that the world of AI research has moved 
<br>
on a little since the early 1980s, and that there are now some very much 
<br>
more subtle kinds of motivational systems out there.  I guess I made the 
<br>
mistake, from the outset, of assuming that the level of sophistication 
<br>
here was not just deep, but also broad.
<br>
<p>Do you know about the difference between (1)  quasi-deterministic 
<br>
symbol-system that keeps stacks of goals, (2) a Complex assemblage of 
<br>
neurons, (3) Complex systems with partly symbolic, partly neuron-like 
<br>
properties?  Do you understand the distinction between a set of goals 
<br>
and a set of motivations, and why I have been talking about the latter 
<br>
while you persist in changing the subject to a particular interpretation 
<br>
of the former?
<br>
<p>Do you know about cognitive science?  About concept development? The 
<br>
problems with classical, feature and prototype models of concept 
<br>
structure?  About the binding problem in the context of neural systems? 
<br>
&nbsp;&nbsp;About the way that motivational systems can be the result of 
<br>
interacting, tangled mechanisms that allow the entire system to be 
<br>
sensitive to small influences, rendering it virtually non-deterministic?
<br>
<p>For example:
<br>
<p><em>&gt; Whether a system will actually 'think about' any given subjunctive goal
</em><br>
<em>&gt; system depends on whether its existing goal system makes it desireable
</em><br>
<em>&gt; to do so. 
</em><br>
<p>Complete nonsense.  In general, an AI could use a goal system and 
<br>
motivation system that caused it to shoot off and consider all kinds of 
<br>
goals, in ways that are exquisitely sensitive to small influences.  I 
<br>
have already made this point elsewhere:  The AI says &quot;I could insert 
<br>
inside myself ANY motivation module in the universe, today.  Think I'll 
<br>
toss a coin [tosses coin]: Now I am passionately devoted to collecting 
<br>
crimson brocade furniture, and my goal for today is to get down to that 
<br>
wonderful antique store over in Chelsea.&quot;  Guess what, it *really* wants 
<br>
to do this, and it genuinely adopted the antique-hunting goal, but are 
<br>
you going to say that its previous goal system made it desirable to do 
<br>
so?  That this new motivation/goal was actually determined by the 
<br>
previous goal, which was something like pure curiosity?  The hell it did!
<br>
<p>In the context of an example like this, your assertion above makes no sense.
<br>
<p><p><p>Or, another example:
<br>
<p><em>&gt;                       A goal system is simply a compact function
</em><br>
<em>&gt; defining a preference order over actions, or universe states, or
</em><br>
<em>&gt; something else that can be used to rank actions. If such a function
</em><br>
<em>&gt; is not stable under self-modification, then it will traverse the space
</em><br>
<em>&gt; of unstable goal systems (as it self-modifies) until it falls into a
</em><br>
<em>&gt; stable attractor. 
</em><br>
<p>A goal system *is* a compact function defining a preference order over 
<br>
actions/universe states?  Who says so?  Many people would say it is not: 
<br>
&nbsp;&nbsp;this is just a particular way of construing a goal system.  It is 
<br>
possible to construct (Complex, as in Complex System) goal systems that 
<br>
work quite well, but which implicitly define a nondeterministic function 
<br>
over the space of possible actions, where that function is deeply 
<br>
nonlinear, non-analytic and probably noncomputable.  And, yes, it may be 
<br>
unstable and it may spend the entire lifetime of the universe heading 
<br>
towards a nice stable attractor *and never get there*....  and still, 
<br>
meanwhile, it might working prtty damn well as a goal mechanism in a 
<br>
real cognitive system.
<br>
<p>And still, this talks about goal system and not motivational system.
<br>
<p>What you have done is to go from the general to a restrictively 
<br>
particular non-sequiteur.
<br>
<p><p><p>Or this, from Randall Randall:
<br>
<p><em>&gt;                                                         &quot;Pure thought&quot; is
</em><br>
<em>&gt; only useful as a tool to examine outcomes against goals.  In order to make
</em><br>
<em>&gt; a choice, you have to have some method for measuring &quot;better&quot; outcomes
</em><br>
<em>&gt; internally.  Whatever direction or scale you use to measure &quot;better&quot; is
</em><br>
<em>&gt; what other people here are calling your &quot;goal&quot;.  It may be that you have
</em><br>
<em>&gt; more than one goal, or that you have a (large) set of conflicting goals
</em><br>
<em>&gt; held by various subsystems of you, but each decision you make, each time
</em><br>
<em>&gt; you choose between &quot;better&quot; and &quot;not as good&quot;, the measurement is a
</em><br>
<em>&gt; reflection of the goal or goals involved in the decision.
</em><br>
<p>What?  Did nobody here ever read Hofstadter?  I'd bet good money that 
<br>
every one of you did, so what is so difficult about remembering his 
<br>
discussion of tangled hierarchies, and about how global system behavior 
<br>
can be determined by self-referential or recursive feedback loops within 
<br>
the system, in such a way that ascribing global behavior to particular 
<br>
local causes is nonsense?  Why, in all of this discussion, are so many 
<br>
people implying that all goal systems must be one-level, quasi- 
<br>
deterministic mechanisms, with no feedback loops and no nonlinearity?
<br>
<p>And why are the same people, rather patronizingly, I must say, treating 
<br>
me as if I am too dumb to understand what a goal system is?  When, in 
<br>
fact, I am trying to point out a subtle property of that more general 
<br>
class of goal (actually, motivational) systems?  I keep trying to talk 
<br>
about motivational systems and goal hierarchies with tangled loops in 
<br>
them [what tangled loop? the one that occurs when the system realises 
<br>
that it can swap motivational modules in and out, and that this swapping 
<br>
process could have profound consequences for the entire observable 
<br>
universe], but while I am trying to drag the discussion up to this 
<br>
subtle level, I find people trying to gently explain to me that I need 
<br>
to do more work to understand the properties of simple, non-tangled goal 
<br>
hierarchies, or that I seem to be making the even more stupid mistake of 
<br>
anthropomorphizing and confusing the true behavior of a real AI with the 
<br>
silly quirks of a human mind.
<br>
<p>Like the following comment from Michael Roy Ames:
<br>
<p><em>&gt;                    You are positing one type of AGI architecture, and the 
</em><br>
<em>&gt; other posters are positing a different type.  In your type the AGI's action 
</em><br>
<em>&gt; of &quot;thinking about&quot; its goals results in changing those goals to be quite 
</em><br>
<em>&gt; different.  In the other type this does not occur.  You suggest that such a 
</em><br>
<em>&gt; change must occur, or perhaps is very likely to occur.  You have provided 
</em><br>
<em>&gt; some arguments to support your suggestion but, so far, they have all had big 
</em><br>
<em>&gt; holes blown in them.  Got any other arguments to support your suggestion?
</em><br>
<p>Patronizing BS.  I have watched holes get blown in arguments I never 
<br>
made, about systems that I was not referring to (and which are probably 
<br>
too trivial to be worth investigating, but that is a side issue), by 
<br>
people who persistently fail to read what I have actually said, or make 
<br>
an effort to understand that what I have said.
<br>
<p>If you really insist on characterizing it as &quot;my&quot; type of AGI vs 
<br>
everyone else's type of AGI, that is fine:  but I am talking about a 
<br>
more general type of AGI, as I have been [ranting] on about in this message.
<br>
<p><p><p><p><p>Or, finally, this example from Chris Capel, which is certainly not 
<br>
patronizing, but includes the same misunderstanding that keeps occuring 
<br>
over and over:
<br>
<p><em>&gt; The AI doesn't have a meta-utility-function by which to judge its
</em><br>
<em>&gt; utility function. It has a single utility function by which to judge
</em><br>
<em>&gt; all potential actions, which is by definition the standard of good.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The only reason the act of reflecting on one's goals produces change
</em><br>
<em>&gt; in humans is that humans have multiple ways of evaluating the goodness
</em><br>
<em>&gt; of ideas and actions, and different standards are used depending on
</em><br>
<em>&gt; the mental state of the human. An AI would be designed only have one
</em><br>
<em>&gt; such standard, a single, unitary utility function, and thus no amount
</em><br>
<em>&gt; of reflection could ever, except by error, lead to the changing of the
</em><br>
<em>&gt; content of its goal system.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The best interpretation I can give your words (and I confess, I
</em><br>
<em>&gt; haven't read all of them) is that you're saying any AI would by
</em><br>
<em>&gt; necessity have multiple levels of goals that could potentially
</em><br>
<em>&gt; conflict. But this is just bad design, and I don't think it would
</em><br>
<em>&gt; happen. If you want to make a case for its necessity, perhaps that
</em><br>
<em>&gt; would progress this thread along a bit more. 
</em><br>
<p>All of what you say would be try of SHRDLU.  But it is a pitiably weak 
<br>
conception of what a goal system could be, or (even more so) what a 
<br>
motivational system could be.  You have so narrowly defined the meaning 
<br>
of goals and utility functions that there is nothing tangled in there. 
<br>
Why are all those recursive, tangled possibilities excluded?
<br>
<p><p><p><p><p><p><p>Richard Loosemore
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11964.html">Samantha Atkins: "Re: Shock Level 5 (SL5) - 'The Theory Of Everything'"</a>
<li><strong>Previous message:</strong> <a href="11962.html">Mitchell Porter: "RE: Paperclip monster, demise of."</a>
<li><strong>In reply to:</strong> <a href="11960.html">Michael Wilson: "Re: Paperclip monster, demise of."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11968.html">Michael Roy Ames: "Re: Paperclip monster, demise of."</a>
<li><strong>Reply:</strong> <a href="11968.html">Michael Roy Ames: "Re: Paperclip monster, demise of."</a>
<li><strong>Reply:</strong> <a href="11969.html">Michael Wilson: "Re: Paperclip monster, demise of."</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#11963">[ date ]</a>
<a href="index.html#11963">[ thread ]</a>
<a href="subject.html#11963">[ subject ]</a>
<a href="author.html#11963">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:51 MDT
</em></small></p>
</body>
</html>
