<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Complexity tells us to maybe not fear UFAI</title>
<meta name="Author" content="Phil Goetz (philgoetz@yahoo.com)">
<meta name="Subject" content="Re: Complexity tells us to maybe not fear UFAI">
<meta name="Date" content="2005-08-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Complexity tells us to maybe not fear UFAI</h1>
<!-- received="Thu Aug 25 09:57:37 2005" -->
<!-- isoreceived="20050825155737" -->
<!-- sent="Thu, 25 Aug 2005 08:57:19 -0700 (PDT)" -->
<!-- isosent="20050825155719" -->
<!-- name="Phil Goetz" -->
<!-- email="philgoetz@yahoo.com" -->
<!-- subject="Re: Complexity tells us to maybe not fear UFAI" -->
<!-- id="20050825155720.46352.qmail@web54513.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="Pine.OSF.4.61.0508251302340.246993@kosh.hut.fi" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Phil Goetz (<a href="mailto:philgoetz@yahoo.com?Subject=Re:%20Complexity%20tells%20us%20to%20maybe%20not%20fear%20UFAI"><em>philgoetz@yahoo.com</em></a>)<br>
<strong>Date:</strong> Thu Aug 25 2005 - 09:57:19 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="12104.html">Phil Goetz: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Previous message:</strong> <a href="12102.html">Mikko Särelä: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>In reply to:</strong> <a href="12102.html">Mikko Särelä: "Re: Complexity tells us to maybe not fear UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12105.html">Peter de Blanc: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Reply:</strong> <a href="12105.html">Peter de Blanc: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Reply:</strong> <a href="12106.html">Martin Striz: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Reply:</strong> <a href="12108.html">Mikko Särelä: "Re: Complexity tells us to maybe not fear UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12103">[ date ]</a>
<a href="index.html#12103">[ thread ]</a>
<a href="subject.html#12103">[ subject ]</a>
<a href="author.html#12103">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Mikko Särelä &lt;<a href="mailto:msarela@cc.hut.fi?Subject=Re:%20Complexity%20tells%20us%20to%20maybe%20not%20fear%20UFAI">msarela@cc.hut.fi</a>&gt; wrote:
<br>
<em>&gt; On Thu, 25 Aug 2005, Chris Paget wrote:
</em><br>
<em>&gt; &gt; Phil Goetz wrote:
</em><br>
<em>&gt; &gt; &gt; By limiting the computational power available to an AI to be one
</em><br>
or 
<br>
<em>&gt; &gt; &gt; two orders of magnitude less than that available to a human, we
</em><br>
can 
<br>
<em>&gt; &gt; &gt; guarantee that it won't outthink us - or, if it does, it will do
</em><br>
so 
<br>
<em>&gt; &gt; &gt; very, very slowly.
</em><br>
<em>&gt; &gt; 
</em><br>
<em>&gt; &gt; You're assuming that the human brain is operating at more than 1%
</em><br>
<em>&gt; of its 
</em><br>
<em>&gt; &gt; theoretical computational power here (and I'd be interested to see
</em><br>
<em>&gt; how 
</em><br>
<em>&gt; &gt; you plan to calculate or prove that).
</em><br>
<p>Evolution doesn't construct machines that operate at &lt; 1% of the
<br>
efficiency possible with the given materials.  Photosynthesis is
<br>
about 12% efficient, comparable to solar cells, which are made
<br>
out of human-selected materials rather than proteins.
<br>
Fat is more efficient than our best power storage cells.
<br>
Birds are probably much more efficient flyers than airplanes?
<br>
Bicycles enable a human to be more efficient than walking
<br>
animals, but only on roads.  (There is a case to be made that
<br>
a wheel could not evolve, except at microscopic sizes.)
<br>
<p><em>&gt; It is at least possible that the 
</em><br>
<em>&gt; &gt; AI will be able to self-optimise to such a degree that it could
</em><br>
<em>&gt; function effectively within any computational limits.
</em><br>
<p>No.  That is exactly what I was claiming is not possible.
<br>
I'm not proving it, but I think I made a pretty good argument.
<br>
Provide me with a counter-argument, not a mere denial.
<br>
<p><em>&gt; And you are assuming that many of the problems the AGI needs to solve
</em><br>
<em>&gt; have computationally tractable solutions. This makes the problem
</em><br>
<em>&gt;P=NP? highly relevant to such hypothetical situation.
</em><br>
<p>(I think the latest &quot;you&quot; also refers to me?)
<br>
<p><em>&gt; If P=NP and the AGI is the first to discover this, then he will be
</em><br>
<em>&gt; able to do things a lot faster than otherwise would be expected.
</em><br>
<em>&gt; Also if the truly 
</em><br>
<em>&gt; interesting problems have good polynomial (or rather linear, or
</em><br>
<em>&gt; sublinear) approximation algorithms, then taking away
</em><br>
<em>&gt; computational power does not really help that much.
</em><br>
<p>This is a point worth making.  I don't think it ultimately
<br>
matters, unless P = NP.
<br>
<p>The problems that may have polynomial, or even linear
<br>
algorithms, are specialized problems.  An AGI could construct
<br>
a subroutine that it could call to solve these problems for it.
<br>
In exactly the same way, a human could write a program to solve
<br>
these problems for him/her in polynomial or linear time.  This
<br>
might enable said human to make a lot of money, say by cracking
<br>
Internet commerce traffic, or by simulating protein folding,
<br>
but SL4 is not worried about that person being a threat to humanity.
<br>
In exactly the same way, an AGI might write all sorts of
<br>
high-speed subroutines that can solve problems at higher rates
<br>
than we expect, but the AGI's &quot;conscious&quot; general-purpose
<br>
intelligence is NOT going to be one of those things that can
<br>
be converted into a polynomial algorithm.  Unless P = NP,
<br>
the AGI will be of the same order of magnitude of threat as
<br>
as a human who develops some surprisingly new efficient
<br>
algorithms, and considerably less of a threat than a human who
<br>
develops a quantum computer.
<br>
<p>Not to say that a human with a quantum computer isn't a
<br>
considerable threat.  But that doesn't invoke the same fear
<br>
factor here on SL4.
<br>
<p>BTW, I suspect that the NSA will be the
<br>
first organization to develop quantum computers, and that it
<br>
will have them for several years before anyone finds out about it.
<br>
I have no inside information about this, but it makes sense.
<br>
They were the original main client for supercomputers, along
<br>
with Los Alamos, and nowadays Los Alamos is much less important,
<br>
and quantum computing is much more relevant to NSA than to
<br>
Los Alamos AFAIK.
<br>
<p>Am I willing to bet the future of humanity that P = NP?
<br>
I'm confident enough that P != NP to consider the expected
<br>
benefits of the gamble.
<br>
<p><em>&gt; Final note, I am not speaking for AGI-boxing, nor do I consider it a
</em><br>
<em>&gt; good strategy.
</em><br>
<p>[Digression:
<br>
The debate about AI-boxing is useless unless you have criteria
<br>
for when an AI is smart enough that it needs to be boxed.
<br>
We aren't going to even try boxing the AIs that we're working
<br>
on for a very long time, because we believe they're irreparably
<br>
stupid.  Even getting to the point where people believe
<br>
that AI-boxing is a better idea than just building and running
<br>
the thing on a computer attached to the internet with no
<br>
security precautions will take decades.
<br>
I myself take no precautions.
<br>
Telling the general public that AI-boxing is a bad idea
<br>
skips one or two shock levels.
<br>
/Digression]
<br>
<p><em>&gt; Then going to another topic I've been thinking for a while. If I've 
</em><br>
<em>&gt; understood correctly, one of the reasons a spike, a singularity, is 
</em><br>
<em>&gt; predicted soon after the development of AGI is that it could device
</em><br>
<em>&gt; itself 
</em><br>
<em>&gt; a better hardware in consecutive cycles and thus each time halve the
</em><br>
<em>&gt; time it takes to develope the next generation.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I would like to counter argue against this proposition.
</em><br>
<em>&gt; The whole proposition assumes 
</em><br>
<em>&gt; that developing next generation hardware is computationally as
</em><br>
<em>&gt; complex as 
</em><br>
<em>&gt; was developing next generation hardware. Or that at least the
</em><br>
<em>&gt; complexity does not go up fast.
</em><br>
<p>Yes, and this proposition is wrong; we already know that our
<br>
increase in computational power requires an exponential increase
<br>
(I think; pretty close, anyway) in monetary investment.  Plot
<br>
dollars invested vs. transistors per square cm, and Moore's law
<br>
looks a lot less impressive.  This relationship between scientific
<br>
investment in a field, and payback, is a general rule, expressed
<br>
in a law in the 1980s by a man whose name I can't remember, but that
<br>
starts with an 'R' and was in my Transvision 2004 presentation on
<br>
the myth of accelerating change.
<br>
<p><em>&gt; For the past decades we have lived with approximately exponential
</em><br>
<em>&gt; growth 
</em><br>
<em>&gt; of doubling computational capacity of a chip each two years. At the
</em><br>
<em>&gt; same 
</em><br>
<em>&gt; time, the computational effort we have put into generating each next 
</em><br>
<em>&gt; generation has also grown exponentially in two ways. Firstly, we
</em><br>
<em>&gt; spend 
</em><br>
<em>&gt; more computer time designing the next generation chip, and secondly,
</em><br>
<em>&gt; we 
</em><br>
<em>&gt; spend much, much more brainpower to solve the problems each new chip 
</em><br>
<em>&gt; generation brings. As there are several problem fields in computer 
</em><br>
<em>&gt; hardware design that can be run parallelly, lots of humans working on
</em><br>
<em>&gt; the 
</em><br>
<em>&gt; problems does not seem like a solution that looses much to the
</em><br>
<em>&gt; overhead.
</em><br>
<p>Right, exactly!
<br>
<p>- Phil Goetz
<br>
<p>__________________________________________________
<br>
Do You Yahoo!?
<br>
Tired of spam?  Yahoo! Mail has the best spam protection around 
<br>
<a href="http://mail.yahoo.com">http://mail.yahoo.com</a> 
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="12104.html">Phil Goetz: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Previous message:</strong> <a href="12102.html">Mikko Särelä: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>In reply to:</strong> <a href="12102.html">Mikko Särelä: "Re: Complexity tells us to maybe not fear UFAI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="12105.html">Peter de Blanc: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Reply:</strong> <a href="12105.html">Peter de Blanc: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Reply:</strong> <a href="12106.html">Martin Striz: "Re: Complexity tells us to maybe not fear UFAI"</a>
<li><strong>Reply:</strong> <a href="12108.html">Mikko Särelä: "Re: Complexity tells us to maybe not fear UFAI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#12103">[ date ]</a>
<a href="index.html#12103">[ thread ]</a>
<a href="subject.html#12103">[ subject ]</a>
<a href="author.html#12103">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:52 MDT
</em></small></p>
</body>
</html>
