<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Singularity Institute - update</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Singularity Institute - update">
<meta name="Date" content="2003-04-30">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Singularity Institute - update</h1>
<!-- received="Wed Apr 30 15:03:25 2003" -->
<!-- isoreceived="20030430210325" -->
<!-- sent="Wed, 30 Apr 2003 17:03:21 -0400" -->
<!-- isosent="20030430210321" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Singularity Institute - update" -->
<!-- id="3EB03A19.8070100@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20Singularity%20Institute%20-%20update"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Wed Apr 30 2003 - 15:03:21 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6526.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<li><strong>Previous message:</strong> <a href="6524.html">Simon Gordon: "Re: Infinite Hells/Infinite Apotheosis' was: Infinite universe"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6526.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="6526.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="6531.html">Mark Waser: "Fw: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="6540.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="6550.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="../0305/6554.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="../0305/6555.html">Spudboy100@aol.com: "Re: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="../0305/6566.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="../0305/6573.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="../0305/6577.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6525">[ date ]</a>
<a href="index.html#6525">[ thread ]</a>
<a href="subject.html#6525">[ subject ]</a>
<a href="author.html#6525">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Singularity Institute - Update
<br>
<p>I've been drawn farther and farther into Friendly AI theory recently, and 
<br>
I haven't been able to concentrate much on promoting SIAI - our last major 
<br>
addition of site content was in October 2002.  It's become clear that 
<br>
there are some major unfinished issues in Friendly AI, and I may need to 
<br>
end up concentrating exclusively on the Friendly AI side of things.  This 
<br>
is starting to drive home to me yet again that there's rather a *lot* of 
<br>
work involved in carrying off a successful Singularity.  I can't do all of 
<br>
it, or even most of it.  In the long run I doubt that I can successfully 
<br>
split my focus of attention between AI and SIAI - I haven't been having 
<br>
much luck so far.
<br>
<p>There is a certain amount of effort that needs to be exerted and there is 
<br>
a limit to how few people can exert it.  SIAI is here to improve 
<br>
humanity's chances in the superintelligent transition.  Surprisingly and 
<br>
counterintuitively for such a huge event, it turns out that there are 
<br>
points of leverage such that significant things can be accomplished by a 
<br>
small number of people - SIAI, which needs to exist in any case, may be 
<br>
able to accomplish its purpose without needing thousands of people.  This 
<br>
is not the same as being able to get by on a shoestring.  There's more 
<br>
work here than I can do.  For the AI project to start up with a realistic 
<br>
chance of success, it needs, I think, at least six extremely bright 
<br>
programmers.  As the AI theory has developed, the number of people needed 
<br>
has gone down, but the minimum necessary intelligence has gone up.
<br>
<p>We also need an Executive Director who can immediately take over the 
<br>
administrative work and the job of writing site content - someone who 
<br>
already has the competencies needed.  Everyone I can think of with the 
<br>
dedication required to fill this position is unfortunately too 
<br>
inexperienced to do so.
<br>
<p>What SIAI has, at this point, is a people problem.  Even if we had all the 
<br>
funders we needed, I don't think we'd be able to start up the primary AI 
<br>
project because we wouldn't have the programmers.  Perhaps, by the time we 
<br>
have the funders, we will have those programmers - but we do not have the 
<br>
people we need right at this moment.  Also, we wouldn't be able to start 
<br>
up because we wouldn't have an Executive Director to handle the 
<br>
administrative end.  We need people who are willing to step up and 
<br>
allocate their lives to this, and these people need to have specific 
<br>
competencies or abilities at very high levels.  That's what it takes to 
<br>
get the job done.
<br>
<p>Since the lack of people is a blocker problem, I think I may have to split 
<br>
my attention one more time, hopefully the last, and write something to 
<br>
attract the people we need.  My current thought is a book on the 
<br>
underlying theory and specific human practice of rationality, which is 
<br>
something I'd been considering for a while.  It has at least three major 
<br>
virtues to recommend it.  (1):  The Singularity movement is a very precise 
<br>
set of ideas that can be easily and dangerously misinterpreted in any 
<br>
number of emotionally attractive, rationally repugnant directions, and we 
<br>
need something like an introductory course in rationality for new members. 
<br>
&nbsp;&nbsp;(2):  Only a few people seem to have understood the AI papers already 
<br>
online, and the more recent theory is substantially deeper than what is 
<br>
currently online; I have been considering that I need to go back to the 
<br>
basics in order to convey a real understanding of these topics. 
<br>
Furthermore, much of the theory needed to give a consilient description of 
<br>
rationality is also prerequisite to correctly framing the task of building 
<br>
a seed AI.  (3):  People of the level SIAI needs are almost certainly 
<br>
already rationalists; this is the book they would be interested in.  I 
<br>
don't think we'll find the people we need by posting a job opening. 
<br>
Movements often start around books; we don't have our book yet.
<br>
<p>It would be much more convenient if the people we needed just walked up. 
<br>
They might.  But they have not done so yet.  There is no reason to believe 
<br>
they will do so.  And we cannot proceed without them.  The idea of writing 
<br>
a book on rationality is a roundabout approach, and I strongly dislike 
<br>
that, but it's the only method I can think of that might prove reliable.
<br>
<p>The recent changes in my thinking about Friendly AI are more difficult to 
<br>
explain.  Roughly, I'm holding myself to a higher standard and trying to 
<br>
accomplish more work in advance.  The theory has progressed considerably 
<br>
beyond &quot;Creating Friendly AI&quot;, but the current theory is in a state where 
<br>
further improvement is clearly possible, and is, at this time, still 
<br>
improving.  It has, in fact, improved to the point where it can describe 
<br>
certain problems which the programmers must solve, at some point, for a 
<br>
Friendly AI to be built.  I think it might prove very wise to have those 
<br>
solutions in hand before work begins, or a complete theoretical 
<br>
description of what the solution should look like, such that it is very 
<br>
clear which unfinished problems are unfinished.  I'm worried about whether 
<br>
I'll be able to simultaneously solve undone problems in Friendly AI theory 
<br>
while also trying to teach a group of AI programmers and building an AI. 
<br>
If the theory gets to a certain point, which looks doable in the near 
<br>
future, we will at the very least have a very solid description of the 
<br>
things we still need to know.  That way, even if FAI work goes slower than 
<br>
expected, or AI work progresses faster than expected, we are not setting 
<br>
ourselves up for future problems.  It should be remembered that it is far 
<br>
better to fail at AI than to succeed at AI and fail at Friendliness.
<br>
<p>The upshot is that there are at least the following simultaneous 
<br>
conditions that must be met before work can begin:
<br>
<p>1)  A programming team comprising, for all tasks required to build a 
<br>
complete seed AI, persons capable of completing those tasks, and with 
<br>
adequate time and energy to do so.  This will require a certain minimum 
<br>
number of brilliant people, who we must find, who will be very hard to find.
<br>
<p>2)  An Executive Director capable of taking over all nontechnical 
<br>
functions presently performed by Eliezer Yudkowsky, particularly that 
<br>
whole &quot;community leader&quot; thing, but also including content writing for the 
<br>
SIAI website, staying in touch with people, doing all administrative 
<br>
paperwork, and providing the motive &quot;push&quot; behind the Singularity Institute.
<br>
<p>3)  Sufficient funding, at any given point in time, to fund the 
<br>
programming team plus the Executive Director plus any ongoing activities 
<br>
relating to fundraising and Singularity education.
<br>
<p>4)  More advanced Friendly AI theory.  My job, but I'm becoming unsure of 
<br>
my ability to do this and other things simultaneously.
<br>
<p>Bear in mind the size of the goal we're trying to accomplish.  It can be 
<br>
very hard to take genuinely useful steps in that direction.  The 
<br>
establishment of SIAI is one such step; it allows for resources to be 
<br>
focused on the Singularity, for the development of professional 
<br>
specialists in Singularity matters, for volunteer efforts to be 
<br>
coordinated.  The establishment of SIAI is a genuinely useful step, but 
<br>
it's only one step - it doesn't solve the whole problem all by itself. 
<br>
Next we need an Executive Director.  Then the AI programmers.  Then enough 
<br>
funding to launch the project.  It's hard but not impossible.  We can do 
<br>
this, one genuinely useful step at a time, if we avoid distraction.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6526.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<li><strong>Previous message:</strong> <a href="6524.html">Simon Gordon: "Re: Infinite Hells/Infinite Apotheosis' was: Infinite universe"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6526.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="6526.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="6531.html">Mark Waser: "Fw: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="6540.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="6550.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="../0305/6554.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="../0305/6555.html">Spudboy100@aol.com: "Re: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="../0305/6566.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="../0305/6573.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<li><strong>Maybe reply:</strong> <a href="../0305/6577.html">Ramez Naam: "RE: Singularity Institute - update"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6525">[ date ]</a>
<a href="index.html#6525">[ thread ]</a>
<a href="subject.html#6525">[ subject ]</a>
<a href="author.html#6525">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
