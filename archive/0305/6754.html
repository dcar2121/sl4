<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)</title>
<meta name="Author" content="Bill Hibbard (test@demedici.ssec.wisc.edu)">
<meta name="Subject" content="Re: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)">
<meta name="Date" content="2003-05-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)</h1>
<!-- received="Tue May 20 15:20:49 2003" -->
<!-- isoreceived="20030520212049" -->
<!-- sent="Tue, 20 May 2003 16:20:38 -0500 (CDT)" -->
<!-- isosent="20030520212038" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@demedici.ssec.wisc.edu" -->
<!-- subject="Re: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)" -->
<!-- id="Pine.GSO.4.44.0305201619500.15294-100000@demedici.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="200305190744.h4J7ixN20979@sophia.objectent.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20Flawed%20Risk%20Analysis%20(was%20Re:%20SIAI's%20flawed%20friendliness%20analysis)"><em>test@demedici.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Tue May 20 2003 - 15:20:38 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6755.html">James Rogers: "RE: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>Previous message:</strong> <a href="6753.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6736.html">Samantha: "Re: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6755.html">James Rogers: "RE: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>Reply:</strong> <a href="6755.html">James Rogers: "RE: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6754">[ date ]</a>
<a href="index.html#6754">[ thread ]</a>
<a href="subject.html#6754">[ subject ]</a>
<a href="author.html#6754">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Mon, 19 May 2003, Samantha wrote:
<br>
<p><em>&gt; On Sunday 18 May 2003 02:31 pm, Bill Hibbard wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; We don't have a way to inspect human brains in the way we will
</em><br>
<em>&gt; &gt; inspect AI brains. And even if we could, we don't have the same
</em><br>
<em>&gt; &gt; level of motivation to inspect humans. With AIs, inspection is
</em><br>
<em>&gt; &gt; critical to the future survival of humanity.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Uh, no.  An AI worth the bother of inspecting would have code, even
</em><br>
<em>&gt; if it was in human readable form, complex and volumnious enough that
</em><br>
<em>&gt; no human or team of humans would be able to understand it that well.
</em><br>
<em>&gt; Certainly not well enough to determine motivation, behavior and
</em><br>
<em>&gt; likely future behavior to a sufficiently fine level as to satisfy
</em><br>
<em>&gt; you.    In actuality a true AI will be much more difficult to
</em><br>
<em>&gt; understand than a member of your own species.
</em><br>
<p>Humans will create the first AIs, and certainly will
<br>
understand their designs and code. If human designers
<br>
can understand it, then human regulators will also be
<br>
able to understand it. Once we have trusted AIs, we can
<br>
let them handle the details of regulation. I would trust
<br>
an AI with reinforcement values for human happiness more
<br>
than I would trust any individual human.
<br>
<p><em>&gt; &gt; I should make it clear that even with a strong effort to
</em><br>
<em>&gt; &gt; detect and inspect AIs, there is no guarantee that all AIs
</em><br>
<em>&gt; &gt; will be safe. But without that effort, unsafe AIs will be
</em><br>
<em>&gt; &gt; guaranteed.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; You are not competent to inspect AIs themselves in a way that will do
</em><br>
<em>&gt; anything to guarantee safety.  This says nothing about your personal
</em><br>
<em>&gt; abilities.  It is the nature of the problem.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; &gt; b) inspecting an AI will be an incredibly complex and difficult
</em><br>
<em>&gt; &gt; &gt; task requiring the intelligence and tracking abilities of a
</em><br>
<em>&gt; &gt; &gt; phlanx of highly tallented people with computer support, so it
</em><br>
<em>&gt; &gt; &gt; will take a lot of time to complete, rendering such inpections
</em><br>
<em>&gt; &gt; &gt; out of date and therefore of little value.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It is worse than that.  It is actually impossible for all but the
</em><br>
<em>&gt; simplest AIs.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I never said it would be easy. We must take the time and
</em><br>
<em>&gt; &gt; effort to inspect every AI to make sure its design
</em><br>
<em>&gt; &gt; conforms to regulations. Regulation certainly slowed down
</em><br>
<em>&gt; &gt; construction of nuclear power plants (before construction
</em><br>
<em>&gt; &gt; stopped altogether), and it will slow down AI development.
</em><br>
<em>&gt; &gt; But there's no reason to rush.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; There is every reason to rush.  Nuclear power was killed by poorly
</em><br>
<em>&gt; informed and politicized implementation of regulations.  I am not
</em><br>
<em>&gt; willing to have AI be stillborn on some quest to insure our safety
</em><br>
<em>&gt; from beings more complex and brighter than ourselves.
</em><br>
<p>I should have said that the reasons to go carefully
<br>
outweight the reasons to hurry. Intelligence is the
<br>
ultimate source of power, and super-intelligent AIs
<br>
will be the most dangerous human creations. As the
<br>
creators of AIs, humans have every right to design
<br>
them in a way that protects human interests. I know
<br>
some people who think it is right that humans become
<br>
extinct, replaced by the superior AIs they create (I
<br>
don't know whether you feel that way). I don't feel
<br>
that way, but recognize that this difference is
<br>
really a difference in basic values and so cannot be
<br>
resolved by logic. This difference will have to be
<br>
resolved by politics.
<br>
<p><em>&gt; &gt; But inspecting designs is different than inspecting operations.
</em><br>
<em>&gt; &gt; I'll grant that those inspecting designs may want to estimate
</em><br>
<em>&gt; &gt; the intentions of the designers, but the ultimate judgement
</em><br>
<em>&gt; &gt; about the design must come from an inspection of the design
</em><br>
<em>&gt; &gt; itself.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; So you believe that the ethics/behavior of an AI is a matter of the
</em><br>
<em>&gt; initial design?   If so, why?
</em><br>
<p>I think that behavior is learned through life experiences
<br>
to satisfy reinforcement values. Complex networks of values,
<br>
such as ethics and morals, are derived from initial values
<br>
via reinforcement learning (derived values are what
<br>
the SIAI analysis calls subgoals). Assuming a mind has
<br>
accurate simulation and learning algorithms, its behavior
<br>
is determined by its initial values, combined with its life
<br>
experiences.
<br>
<p>Humans create game playing programs that they cannot beat
<br>
(I've done it myself). Those humans can't trace the detailed
<br>
steps of their programs, but do understand their logic for
<br>
simulating the game and for learning. The exact details of
<br>
a game playing program's behavior are not pre-determined,
<br>
but if its reinforcement learning values are for winning,
<br>
then it is predictable that it will play to win. The
<br>
extent to which it wins will depend on the accuracy and
<br>
efficiency of its algorithms for simulating the game and
<br>
for learning.
<br>
<p>Of course intelligent behavior is much more complex than
<br>
game playing programs. But the same idea applies. It is
<br>
enough to know that the simulation and learning algorithms
<br>
are accurate and efficient, and to know the reinforcement
<br>
values.
<br>
<p><em>&gt; &gt; &gt; Correct.  But your statement seems to imply that the 'artifact'
</em><br>
<em>&gt; &gt; &gt; is unchanging.  This is untrue for any of the mind designs I have
</em><br>
<em>&gt; &gt; &gt; seen so far, including the human mind.  Minds change, and an AI
</em><br>
<em>&gt; &gt; &gt; is going to be faster and more capable at changing its mind than
</em><br>
<em>&gt; &gt; &gt; humans are.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; We cannot let it outrun our ability to inspect. There will be
</em><br>
<em>&gt; &gt; no rush.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; That is the most foolish statement I have ever heard.
</em><br>
<p>That's just name calling.
<br>
<p><em>&gt; Do you limit
</em><br>
<em>&gt; humans to not developing and changing faster than your ability to
</em><br>
<em>&gt; sufficiently inspect their thinking processes and psychology?   You
</em><br>
<em>&gt; are talking about new minds here, minds vastly more capable in
</em><br>
<em>&gt; potential than our own.  To insist they be limited to what your (no
</em><br>
<em>&gt; offense as this applies to all of us) pea brain can encompass and
</em><br>
<em>&gt; predict is to oppose the creation of such intelligences utterly.
</em><br>
<em>&gt; Hell, teams of very bright humans can barely keep something as
</em><br>
<em>&gt; uninspiring as Windows XP running and plug its security holes.  The
</em><br>
<em>&gt; notion that similar teams can analyze and judge the safety of an AI
</em><br>
<em>&gt; beyond a very rudimentary stage is ludicrous.
</em><br>
<p>OpenBSD illustrates that operating systems can be much more
<br>
secure than Windows XP. I think the difference between
<br>
OpenBSD and Windows XP security has a lot to do with the
<br>
different values of their creators.
<br>
<p>Humans do not have to predict the detailed thoughts of their AIs,
<br>
just the mechanisms of their thoughts. This is similar to the
<br>
situation of game playing programs that I described earlier.
<br>
Humans can write game programs that they cannot beat, because
<br>
humans don't have to trace the detailed steps of their programs
<br>
to know that they will play the game legally and well.
<br>
<p>Humans will design and code the first intelligent AIs, and
<br>
other humans will be able to verify the safety of those
<br>
designs. Once we have trusted AIs, the details of regulating
<br>
AIs can be delegated to them.
<br>
<p><em>&gt; There is no hurry only if we are sufficiently capable to solve the
</em><br>
<em>&gt; problems that face us and face us with more complexity and detail day
</em><br>
<em>&gt; by day. I do not believe that is the case.
</em><br>
<p>As I said before, intelligence is the real source of
<br>
power in this world and AIs will be the greatest source
<br>
of danger. Thus AIs must be created carefully.
<br>
<p><em>&gt; &gt; In the early days, AI technology won't be widely available
</em><br>
<em>&gt; &gt; so inspection efforts can focus on the few successful groups.
</em><br>
<em>&gt; &gt; No rush. Lets do the first strong AIs slowly, with the public
</em><br>
<em>&gt; &gt; insisting on an intensive effort to formulate and enforce
</em><br>
<em>&gt; &gt; regulations. Humanity can afford to take its time. It cannot
</em><br>
<em>&gt; &gt; afford to get it wrong because of some imagined need to rush.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; No!  I will become an &quot;outlaw&quot; myself before I will sit back and
</em><br>
<em>&gt; watch such a farce unfold.
</em><br>
<p>If you want to be an AI outlaw, go for it.
<br>
<p>It really comes down to who you trust. I favor a broad
<br>
political process because I trust the general public more
<br>
than any individual or small group. Of course, democratic
<br>
goverement does enlist the help of experts on technical
<br>
questions, but ultimate authority is with the public.
<br>
<p>Bill
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6755.html">James Rogers: "RE: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>Previous message:</strong> <a href="6753.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6736.html">Samantha: "Re: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6755.html">James Rogers: "RE: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>Reply:</strong> <a href="6755.html">James Rogers: "RE: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6754">[ date ]</a>
<a href="index.html#6754">[ thread ]</a>
<a href="subject.html#6754">[ subject ]</a>
<a href="author.html#6754">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
