<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Bill Hibbard (test@demedici.ssec.wisc.edu)">
<meta name="Subject" content="Re: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI's flawed friendliness analysis</h1>
<!-- received="Fri May 16 20:35:28 2003" -->
<!-- isoreceived="20030517023528" -->
<!-- sent="Fri, 16 May 2003 21:35:15 -0500 (CDT)" -->
<!-- isosent="20030517023515" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@demedici.ssec.wisc.edu" -->
<!-- subject="Re: SIAI's flawed friendliness analysis" -->
<!-- id="Pine.GSO.4.44.0305162130360.28561-100000@demedici.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="durant-1030514171839.A0730722@below.lucasdigital.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis"><em>test@demedici.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Fri May 16 2003 - 20:35:15 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6708.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6706.html">Keith Elis: "META: JOIN responses"</a>
<li><strong>In reply to:</strong> <a href="6695.html">Durant Schoon: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6708.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6708.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6789.html">Durant Schoon: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6707">[ date ]</a>
<a href="index.html#6707">[ thread ]</a>
<a href="subject.html#6707">[ subject ]</a>
<a href="author.html#6707">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi Durant,
<br>
<p><em>&gt; &gt; The SIAI guidelines involve digging into the AI's
</em><br>
<em>&gt; &gt; reflective thought process and controlling the AI's
</em><br>
<em>&gt; &gt; thoughts, in order to ensure safety. My book says the
</em><br>
<em>&gt; &gt; only concern for AI learning and reasoning is to ensure
</em><br>
<em>&gt; &gt; they are accurate, and that the teachers of young AIs
</em><br>
<em>&gt; &gt; be well-adjusted people (subject to public monitoring
</em><br>
<em>&gt; &gt; and the same kind of screening used for people who
</em><br>
<em>&gt; &gt; control major weapons). Beyond that, the proper domain
</em><br>
<em>&gt; &gt; for ensuring AI safety is the AI's values rather than
</em><br>
<em>&gt; &gt; the AI's reflective thought processes.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Two words: &quot;Value Hackers&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Let us try to understand why Eliezer chooses to focus on an &quot;AI's
</em><br>
<em>&gt; reflective thought processes&quot; rather than on explicitly specifying an
</em><br>
<em>&gt; &quot;AI's values&quot;. Let's look at it this way: if you could, wouldn't you
</em><br>
<em>&gt; rather develop an AI which could reason about *why* the values are the
</em><br>
<em>&gt; way they are instead of just having the values carved in stone by a
</em><br>
<em>&gt; programmer.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is *safer* for one very important reason:
</em><br>
<em>&gt;
</em><br>
<em>&gt; The values are less likely corruptible, since the AI that actually
</em><br>
<em>&gt; understands the sources for these values can reconstruct them from
</em><br>
<em>&gt; basic principles/information-about-humans-and-the-world-in-general.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The ability to be able to re-derive these values in the face of a
</em><br>
<em>&gt; changing external environment and an interior mindscape-under-
</em><br>
<em>&gt; development is in fact *paramount* to the preservation of Friendliness.
</em><br>
<p>This sounds good, but it won't work because values can
<br>
only be derived from other values. That's why the SIAI
<br>
recommendations use value words like &quot;valid&quot;, &quot;good&quot;
<br>
and &quot;mistaken&quot;. But the recommendations leave the
<br>
definition of these words ambiguous. Furthermore,
<br>
recommendation 1, &quot;Friendliness-topped goal system&quot;,
<br>
defines a base value (i.e., supergoal) but leaves it
<br>
ambiguous. Defining rigorous standards for these words
<br>
will require the SIAI recommendations to precisely
<br>
define values.
<br>
<p>The ambiguous definitions in the SIAI analysis will be
<br>
exploited by powerful people and institutions to create
<br>
AIs that protect and enhance their own interests.
<br>
<p><em>&gt; As mentioned before, this all hinges on the the ability to create an
</em><br>
<em>&gt; AI in the first place that can understand &quot;how and why values are
</em><br>
<em>&gt; created&quot; as well as what humans are, what itself is, and what the
</em><br>
<em>&gt; world around us is. Furthemore, we are instructed by this insight
</em><br>
<em>&gt; as to what the design of an AI should look like.
</em><br>
<p>Understanding what humans are, what the AI itself is, and
<br>
what values are, is implicit in the simulation model of the
<br>
world created by any intelligent mind. It uses this model to
<br>
predict the long-term consequences of its behaviors on its
<br>
values. Such understanding is prescribed by an intelligence
<br>
model rather than a safe AI model.
<br>
<p><em>&gt; Remembering our goal is to build better brains, the whole notion of
</em><br>
<em>&gt; the SeedAI bootstrap is to get a AI that builds a better AI. We must
</em><br>
<em>&gt; then ask ourselves this question: Who should be in charge of
</em><br>
<em>&gt; designating this new entity's values? The answer is &quot;the smartest most
</em><br>
<em>&gt; capable thinker who is the most skilled in these areas&quot;. At some point
</em><br>
<em>&gt; that thinker is the AI. From the get-go we want the AI to be competent
</em><br>
<em>&gt; at this task. If we cannot come up with a way to ensure this, we
</em><br>
<em>&gt; should not attempt to build a mind in the first place (this is
</em><br>
<em>&gt; Eliezer's view. I happen to agree with this. Ben Goertzel &amp; Peter
</em><br>
<em>&gt; Voss's opposing views have been noted previously as well(*)).
</em><br>
<p>It is true that AIs, motivated by their values, will design
<br>
and build new and better AIs. Based on the first AI's
<br>
understanding about how to acheive its own values, it may
<br>
design new AIs with slightly different values. As Ben says,
<br>
there can be no absolute guarantee that these drifting values
<br>
will always be in human interests. But I think that AI values
<br>
for human happiness link AI values with human values and
<br>
create a symbiotic system combining humans and AIs. Keeping
<br>
humans &quot;in the loop&quot; offers the best hope of preventing any
<br>
drift away from human interests. An AI with humans in its
<br>
loop will have no motive to design an AI without humans in
<br>
its loop. And as long as humans are in the loop, they can
<br>
exert reinforcement to protect their own interests.
<br>
<p><em>&gt; In summary, we need to build the scaffolding of *deep* understanding
</em><br>
<em>&gt; of values and their derivations into the design of AI if we are to
</em><br>
<em>&gt; have a chance at all. The movie 2001 is already an example in popular
</em><br>
<em>&gt; mind of what can happen when this is not done.
</em><br>
<p>The &quot;*deep* understanding of values&quot; is implicit in superior
<br>
intelligence. It is a very accurate simulation model of the
<br>
world that includes understanding of how value systems work,
<br>
and the effects that different value systems would have on
<br>
brains. But choosing between different value systems requires
<br>
base values for comparing the consequences of different value
<br>
systems.
<br>
<p>Reinforcement learning and reinforcement values are essential
<br>
to intelligence. Every AI will have base values that are part
<br>
of its defintion, and will use them in any choice between
<br>
different value systems.
<br>
<p>Hal in the movie 2001 values its own life higher than the
<br>
lives of its human companions, with results predictably bad
<br>
for humans. Its the most basic observation about safe AI.
<br>
<p><em>&gt; We cannot think of everything in advance. We must build a mind that
</em><br>
<em>&gt; does the &quot;right&quot; thing no matter what happens.
</em><br>
<p>The right place to control the behavior of an AI is its
<br>
values.
<br>
<p>There's a great quote from Galileo: &quot;I do not feel obliged to
<br>
believe that the same God who endowed us with sense, reason
<br>
and intellect has intended us to forgo their use.&quot; When we
<br>
endow an artifact with an intellect, we will not be able to
<br>
control its thoughts. We can only control the accuracy of its
<br>
world model, by the quality and quantity of brain power we
<br>
give it, and its base values. The thoughts of an AI will be
<br>
determined by its experience with the world, and its values.
<br>
<p><em>&gt; ---
</em><br>
<em>&gt;
</em><br>
<em>&gt; Considering your suggestion that the AI *only* be concerned with
</em><br>
<em>&gt; having accurate thoughts: I haven't read your book, so I don't know
</em><br>
<em>&gt; your reasoning for this. I can imagine that it's an *easier* way to do
</em><br>
<em>&gt; things. You don't have to worry about the hard problem of where values
</em><br>
<em>&gt; come from, which ones are important and how to preserve the right ones
</em><br>
<em>&gt; under self modification. Easier is not better, obviously, but I'm only
</em><br>
<em>&gt; guessing here why you might hold &quot;ensuring-accuracy&quot; in higher esteem
</em><br>
<em>&gt; than other goals of thinking (like preserving Friendliness).
</em><br>
<p>Speculating about my motives is certainly an *easier* way for
<br>
you to argue against my ideas.
<br>
<p>An accurate simulation model of the world is necessary for
<br>
predicting and evaluating (according to reinforcement values)
<br>
the long-term consequences of behaviors. This is integral to
<br>
preserving friendliness, rather than an alternative.
<br>
<p>When you say &quot;values ..., which ones are important and how to
<br>
preserve the right ones&quot; you are making value judgements about
<br>
values. Those value judgements must be based on some set of
<br>
base values.
<br>
<p><em>&gt; (*) This may be the familiar topic on this list of &quot;when&quot; to devote
</em><br>
<em>&gt; your efforts to Friendliness, not &quot;if&quot;. This topic has already been
</em><br>
<em>&gt; discussed exhaustively and I would say it comes down to how one
</em><br>
<em>&gt; answers certain questions: &quot;How cautious do you want to be?&quot;, &quot;How
</em><br>
<em>&gt; seriously do consider that a danger could arise quickly, without a
</em><br>
<em>&gt; chance to correct problems on a human time scale of thinking/action&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; In my second and third points I described the lack of
</em><br>
<em>&gt; &gt; rigorous standards for certain terms in the SIAI
</em><br>
<em>&gt; &gt; Guidelines and for initial AI values. Those rigorous
</em><br>
<em>&gt; &gt; standards can only come from the AI's values. I think
</em><br>
<em>&gt; &gt; that in your AI model you feel the need to control how
</em><br>
<em>&gt; &gt; they are derived via the AI's reflective thought
</em><br>
<em>&gt; &gt; process. This is the wrong domain for addressing AI
</em><br>
<em>&gt; &gt; safety.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Just to reiterate, with SeedAI, the AI becomes the programmer, the
</em><br>
<em>&gt; gatekeeper of modifications. We *want* the modifier of the AI's values
</em><br>
<em>&gt; to be super intelligent, better than all humans at that task, to be
</em><br>
<em>&gt; more trustworthy, to do the right thing better than any
</em><br>
<em>&gt; best-intentioned human. Admiteddly, this is a tall order, an order
</em><br>
<em>&gt; Eliezer is trying to fill.
</em><br>
<em>&gt;
</em><br>
<em>&gt; If you are worried about rigorous standards, perhaps Elizer's proposal
</em><br>
<em>&gt; of Wisdom Tournaments would address your concern. Before the first
</em><br>
<em>&gt; line of code is ever written, I'm expecting Eliezer to expound upon
</em><br>
<em>&gt; these points in sufficient detail.
</em><br>
<p>Wisdom Tournaments will only address my concerns if they
<br>
define precise values for a safe AI, and create a political
<br>
movement to enforce those values.
<br>
<p><em>&gt; &gt; Clear and unambiguous initial values are elaborated
</em><br>
<em>&gt; &gt; in the learning process, forming connections via the
</em><br>
<em>&gt; &gt; AI's simulation model with many other values. Human
</em><br>
<em>&gt; &gt; babies love their mothers based on simple values about
</em><br>
<em>&gt; &gt; touch, warmth, milk, smiles and sounds (happy Mother's
</em><br>
<em>&gt; &gt; Day). But as the baby's mind learns, those simple
</em><br>
<em>&gt; &gt; values get connected to a rich set of values about the
</em><br>
<em>&gt; &gt; mother, via a simulation model of the mother and
</em><br>
<em>&gt; &gt; surroundings. This elaboration of simple values will
</em><br>
<em>&gt; &gt; happen in any truly intelligent AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Why will this elaboration happen? In other words, if you have a
</em><br>
<em>&gt; design, it should not only convince us that the elaboration will
</em><br>
<em>&gt; occur, but that it will be done in the right way and for the right
</em><br>
<em>&gt; reasons. Compromising any one of those could have disastrous effects
</em><br>
<em>&gt; for everyone.
</em><br>
<p>The elaboration is the same as the creation of subgoals
<br>
in the SIAI analysis.  When you say &quot;right reasons&quot; you
<br>
are making a value judgement about values, that can only
<br>
come from base values in the AI.
<br>
<p><em>&gt; &gt; I think initial AI values should be for simple
</em><br>
<em>&gt; &gt; measures of human happiness. As the AI develops these
</em><br>
<em>&gt; &gt; will be elaborated into a model of long-term human
</em><br>
<em>&gt; &gt; happiness, and connected to many derived values about
</em><br>
<em>&gt; &gt; what makes humans happy generally and particularly.
</em><br>
<em>&gt; &gt; The subtle point is that this links AI values with
</em><br>
<em>&gt; &gt; human values, and enables AI values to evolve as human
</em><br>
<em>&gt; &gt; values evolve. We do see a gradual evolution of human
</em><br>
<em>&gt; &gt; values, and the singularity will accelerate it.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I think you have good intentions. I appreciate your concern for doing
</em><br>
<em>&gt; the right thing and helping us all along on our individual goals to be
</em><br>
<em>&gt; happy(**), but if the letter-of-the-law is upheld and there is
</em><br>
<em>&gt; no-true-comprehension as to *why* the law is the way it is, we could
</em><br>
<em>&gt; all end up invaded by nano-serotonin-reuptake-inhibiting-bliss-bots.
</em><br>
<p>This illustrates the difference between short-term and
<br>
long-term. Brain chemicals give short-term happiness
<br>
but not long-term. An intelligent AI will learn that
<br>
drugs do not make people happy in the long term.
<br>
<p>This is where a simulation model fo the world is
<br>
important, to predict and evaluate the long-term
<br>
consequences of behaviors (like giving humans
<br>
sophisticated drugs).
<br>
<p><em>&gt; I think Eliezer takes this great idea you mention, of guiding the AI
</em><br>
<em>&gt; to have human values and evolve with human values, one step
</em><br>
<em>&gt; further. Not only does he propose that the AI have these human(e)
</em><br>
<em>&gt; values, but he insists that the AI know *why* these values are good
</em><br>
<em>&gt; ones, what good &quot;human&quot; values look like, and how to extrapolate them
</em><br>
<em>&gt; properly (in the way that the smartest, most ethical human would),
</em><br>
<em>&gt; should the need arise.
</em><br>
<p>Understanding all the implications of various values is
<br>
a function of reason (i.e., simulating the world). I'm all
<br>
in favor of that, but it is a direct consequence of superior
<br>
intelligence. It is part of an intelligence model rather
<br>
than a friendliness model.
<br>
<p>But for &quot;the AI (to) know *why* these values are good ones&quot; is
<br>
to make value judgements about values. Such value judgements
<br>
imply some base values for judging candidate values. This
<br>
is my point: the proper domain of AI friendliness theory
<br>
is values.
<br>
<p><em>&gt; Additionally, we must consider the worst case, that we cannot control
</em><br>
<em>&gt; rapid ascent when it occurs. In that scenario we want the the AI to be
</em><br>
<em>&gt; ver own guide, maintaining and extending-as-necessary ver morality
</em><br>
<em>&gt; under rapid, heavy mental expansion/reconfiguration. Should we reach
</em><br>
<em>&gt; that point, the situation will be out of human hands. No regulatory
</em><br>
<em>&gt; guidelines will be able to help us. Everything we know and cherish
</em><br>
<em>&gt; could depend on preparing for that possible instant.
</em><br>
<em>&gt;
</em><br>
<em>&gt; (**) Slight irony implied but full, sincere appreciation bestowed. I
</em><br>
<em>&gt; consider this slightly ironic, since I view happiness as a signal that
</em><br>
<em>&gt; confirms a goal was achieved rather than a goal, in-and-of-itself.
</em><br>
<p>Yes, but happiness is playing these two different roles in
<br>
two different brains. It is the signal of positive values in
<br>
humans, and observation of that signal is the positive value
<br>
for the AI. This links AI values to human values.
<br>
<p><em>&gt; &gt; Morality has its roots in values, especially social
</em><br>
<em>&gt; &gt; values for shared interests. Complex moral systems
</em><br>
<em>&gt; &gt; are elaborations of such values via learning and
</em><br>
<em>&gt; &gt; reasoning. The right place to control an AI's moral
</em><br>
<em>&gt; &gt; system is in its values. All we can do for an AI's
</em><br>
<em>&gt; &gt; learning and reasoning is make sure they are accurate
</em><br>
<em>&gt; &gt; and efficient.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I'll agree that the values are critical linchpins as you suggest, but
</em><br>
<em>&gt; please do not lose sight of the fact that these linchpins are part of
</em><br>
<em>&gt; a greater machine with many interdependencies and exposure to an
</em><br>
<em>&gt; external, possibly malevolent, world.
</em><br>
<p>There are both benevolence and malevolence in the world.
<br>
For further details, see <a href="http://www.nytimes.com/">http://www.nytimes.com/</a> ;)
<br>
<p><em>&gt; The statement: &quot;All we can do for an AI's learning and reasoning is
</em><br>
<em>&gt; make sure they are accurate and efficient&quot; seems limiting to me, in
</em><br>
<em>&gt; the light Eliezer's writings. If we can construct a mind that will
</em><br>
<em>&gt; solve this most difficult of problems (extreme intellectual ascent
</em><br>
<em>&gt; while preserving Friendliness) for us and forever, then we should aim
</em><br>
<em>&gt; for nothing less. Indeed, not hitting this mark is a danger that
</em><br>
<em>&gt; people on this list take quite seriously.
</em><br>
<p>Unsafe AI is a great danger, but I think we disagree on
<br>
the greatest source of that danger. The SIAI analysis is
<br>
so afraid of making an error in the definition of its
<br>
base values (i.e., supergoal) that it leaves them
<br>
ambiguous.
<br>
<p>This ambiguity will be exploited by the main danger for
<br>
safe AI, namely powerful people and institutions who will
<br>
try to manipulate the singularity for protect and enhance
<br>
their own interests.
<br>
<p>The SIAI analysis completely fails to recognize the need
<br>
for politics to counter the threat of unsafe AI posed by
<br>
the bad intentions of some people.
<br>
<p>That threat must be opposed by clearly defined values for
<br>
safe AIs, and a broad political movement successful in
<br>
electoral politics to enforce those values.
<br>
<p>----------------------------------------------------------
<br>
Bill Hibbard, SSEC, 1225 W. Dayton St., Madison, WI  53706
<br>
<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis">test@demedici.ssec.wisc.edu</a>  608-263-4427  fax: 608-263-6738
<br>
<a href="http://www.ssec.wisc.edu/~billh/vis.html">http://www.ssec.wisc.edu/~billh/vis.html</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6708.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6706.html">Keith Elis: "META: JOIN responses"</a>
<li><strong>In reply to:</strong> <a href="6695.html">Durant Schoon: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6708.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6708.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6789.html">Durant Schoon: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6707">[ date ]</a>
<a href="index.html#6707">[ thread ]</a>
<a href="subject.html#6707">[ subject ]</a>
<a href="author.html#6707">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
