<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI's flawed friendliness analysis</h1>
<!-- received="Thu May 29 11:14:48 2003" -->
<!-- isoreceived="20030529171448" -->
<!-- sent="Thu, 29 May 2003 13:14:34 -0400" -->
<!-- isosent="20030529171434" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: SIAI's flawed friendliness analysis" -->
<!-- id="3ED63FFA.1020600@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="Pine.GSO.4.44.0305291037360.2056-100000@demedici.ssec.wisc.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Thu May 29 2003 - 11:14:34 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6841.html">Rafal Smigrodzki: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6839.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6839.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6844.html">Philip Sutton: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6844.html">Philip Sutton: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6879.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6840">[ date ]</a>
<a href="index.html#6840">[ thread ]</a>
<a href="subject.html#6840">[ subject ]</a>
<a href="author.html#6840">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Bill Hibbard wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; Nevertheless, its an interesting question and I'll try to answer it. I
</em><br>
<em>&gt; think the answer divides into two parts: the regulation itself, and how
</em><br>
<em>&gt; to enforce it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1. The regulation.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Here's my initial crack at it.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Any artifact implementing &quot;learning&quot; and capable of at least N
</em><br>
<em>&gt; mathematical operations per second must have &quot;human happiness&quot; as its
</em><br>
<em>&gt; only initial reinforcement value. Here &quot;learning&quot; means that system
</em><br>
<em>&gt; responses to inputs change over time, and &quot;human happiness&quot; values are
</em><br>
<em>&gt; produced by an algorithm produced by supervised learning, to recognize 
</em><br>
<em>&gt; happiness in human facial expressions, voices and body language, as
</em><br>
<em>&gt; trained by human behavior experts.
</em><br>
<p>Congratulations.  You've just ruled out SIAI's Friendly AI architecture 
<br>
and mandated one that is basically, fundamentally flawed.  You have tried 
<br>
to impose your own, flawed understanding of the nature of human 
<br>
intelligence and the structure of morality upon every AI researcher on 
<br>
Earth.  You are speaking from within an 80s-era understanding of AI; 
<br>
supervised learning, reinforcement values, training by human behavior 
<br>
experts.  You have walked into exactly the trap that I expect real-life 
<br>
regulators to walk into.  I oppose attempted regulation of AI morality for 
<br>
exactly this reason; there is no guarantee that researchers can solve the 
<br>
problem, but government regulators are guaranteed to fail.  Government 
<br>
regulators don't know how much they don't know.
<br>
<p>The one thing I have observed about AI is that everyone believes they 
<br>
understand it.  Government regulators will be no exception to this. 
<br>
Except that where real AI researchers must actually implement their ideas 
<br>
successfully in order to have any impact, government regulators have no 
<br>
check upon them, no experimental test to tell them about their own 
<br>
ignorance; they will happily regulate everyone into oblivion based on 
<br>
their private theories.  You ruled out SIAI's Friendliness architecture, 
<br>
and indeed any Friendliness architecture that could possibly work, and you 
<br>
did so without blinking an eye, without realizing the danger of what you 
<br>
were saying, without thinking that maybe you ought to spend a few more 
<br>
years thinking about AI morality before imposing your unfinished ideas on 
<br>
every researcher on Earth.  If you actually had the power to put your 
<br>
ideas into practice, if your suggestion had been implemented, you would 
<br>
have, just now, wiped out the Earth.  Oh, we'd have staggered along for a 
<br>
while, but in the end we would have died; you ruled out every possible 
<br>
architecture that could achieve FAI, and imposed one that inevitably 
<br>
results in UFAI.  You now have one subjunctive planetary kill on your 
<br>
record.  This is exactly the predictable disaster that my opposition of 
<br>
government regulation is based on.
<br>
<p><em>&gt; Since this is so much shorter than most government regulations, I
</em><br>
<em>&gt; suspect that a real regulation, produced after input from many experts,
</em><br>
<em>&gt; would be much longer.
</em><br>
<p>But, of course, just as unworkable - again, a quite predictable 
<br>
catastrophe.  If you cannot convene a panel of government experts to tell 
<br>
you exactly what human intelligence is, or a panel of government experts 
<br>
to tell you how to build an AGI, then why do you think a panel of 
<br>
government experts can understand FAI?  The answer is that they cannot. 
<br>
It is not because they are stupid.  A government panel of smart people 
<br>
also could not tell you how to build AGI, and this is not because they are 
<br>
stupid, but because AGI is hard.  The difference is that if a government 
<br>
panel of experts comes up with an unworkable AI theory, as of course they 
<br>
would, and the public spends a few billion bucks trying to implement the 
<br>
unworkable theory, nothing happens - the experts visibly fail, get some 
<br>
egg on their face, and democracy lurches on.  With FAI, the experts will 
<br>
quite happily impose their unworkable theory on everyone, since it is not 
<br>
subject to test until too late.  There's a reason why science tests 
<br>
theories using experiments instead of having panels of government experts 
<br>
vote on them, and it's not because government experts are stupid.
<br>
<p><em>&gt; 2. How the regulation can be enforced.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Enforcement is a hard problem. It helps that enforcement is not
</em><br>
<em>&gt; necessary indefinitely. It is only necessary until the singularity, at
</em><br>
<em>&gt; which time it becomes the worry of the (hopefully safe) singularity
</em><br>
<em>&gt; AIs. There is a spectrum of possible approaches of varying strictness.
</em><br>
<p>Of course, enforcement is far, far, far easier, as a conceptual problem, 
<br>
than anything of or relating to Friendly AI.  Unfortunately.
<br>
<p><em>&gt; AI and the singularity will be so much better if the public is informed
</em><br>
<em>&gt; and is in control via their elected governments. It is human nature for
</em><br>
<em>&gt; people to resist changes that are forced on them. If we respect
</em><br>
<em>&gt; humanity enough to want a safe singularity for them, then we should
</em><br>
<em>&gt; also respect them enough to get the public involved and consenting to
</em><br>
<em>&gt; what is happening.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Whether or not you think my regulation ideas can work, my basic point
</em><br>
<em>&gt; is that the singularity will be created by some wealthy and powerful
</em><br>
<em>&gt; institution, and its values will reflect the values of the institution.
</em><br>
<em>&gt; The only chance for a safe singularity will be if that institution is
</em><br>
<em>&gt; democratic government under the control of an aggressive public
</em><br>
<em>&gt; movement for safe AI, similar to the consumer, environmental and social
</em><br>
<em>&gt; justice movements.
</em><br>
<p>So far, you've just demonstrated one planetary kill for regulation.  There 
<br>
is no divine right of democracy; it does not confer infallibility.  What 
<br>
it does confer is faith and the illusion of infallibility.  Congress is 
<br>
not capable of understanding how little it knows, which is what makes it 
<br>
dangerous.  Democracy has known bugs; and those bugs, applied to 
<br>
Singularity scenarios, result in predictable kills - one of which you have 
<br>
just demonstrated.  People who have been invested with the divine right of 
<br>
democracy and the holy indignation of the voters do not have enough 
<br>
humility, in the face of Nature, to confront the Singularity and survive. 
<br>
&nbsp;&nbsp;Full of the righteous anger that politics brings, they will take one 
<br>
step in the wrong direction and die.  You will shrug off everything I say 
<br>
about your theory of AI failing, because, why, how can you have democracy 
<br>
if the experts are allowed to run everything?  Who died and left *me* in 
<br>
charge?  Only an elected representative can have the right to say what 
<br>
Nature will do in such-and-such a situation, which is, of course, a 
<br>
political matter, since public policy depends on it.  Politics, including 
<br>
democratic politics, is about who gets to be in charge.  Whoever wins the 
<br>
fight, and gets to be in charge, is far too flush with victory to listen 
<br>
to some mere unelected expert warning them they're walking into a trap. 
<br>
And that, too, is a predictable failure.
<br>
<p><em> &gt; My argument for regulation is based on the high probability of unsafe
</em><br>
<em> &gt; AI without regulation, rather than any confidence that I have all the
</em><br>
<em> &gt; answers about how to regulate. I have no practical experience with
</em><br>
<em> &gt; politics, regulation, security or law enforcement, and so my ideas on
</em><br>
<em> &gt; this would certainly need to be refined by professionals.
</em><br>
<p>I don't believe this is a valid argument for choosing a strategy.  &quot;I 
<br>
believe there is a high probability of unsafe AI without chocolate 
<br>
brownies, so I would like chocolate brownies to be involved.  Of course I 
<br>
have no experience with baking brownies, so my ideas will need to be 
<br>
refined by professional cooks...&quot;
<br>
<p>It is *hard* to survive the Singularity.  You saw a problem, and searched 
<br>
through your mental library of heuristics, until you found one that looked 
<br>
like it would solve it - your faith in the processes of democracy. 
<br>
Democracy is a good thing, isn't it?  Everyone loves democracy.  If you 
<br>
say bad things about democracy you must be a bad person, and to suggest 
<br>
that Congress is not capable of solving this scientific problem is, of 
<br>
course, very disrespectful of our elected representatives and democratic 
<br>
institutions.  But the Singularity is not solved.  It has not gotten any 
<br>
safer.  It has just gotten worse.  All you did was appeal to a solution 
<br>
sufficiently vague that you could no longer foresee how it would fail, and 
<br>
one with plenty of positive emotional hooks to draw in your faith and 
<br>
banish your anxiety.  This, of course, is how everyone seems to handle 
<br>
problems in Friendly AI as well.
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6841.html">Rafal Smigrodzki: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6839.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6839.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6844.html">Philip Sutton: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6844.html">Philip Sutton: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6879.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6840">[ date ]</a>
<a href="index.html#6840">[ thread ]</a>
<a href="subject.html#6840">[ subject ]</a>
<a href="author.html#6840">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
