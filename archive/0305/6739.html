<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Gary Miller (garymiller@starband.net)">
<meta name="Subject" content="RE: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: SIAI's flawed friendliness analysis</h1>
<!-- received="Mon May 19 10:34:34 2003" -->
<!-- isoreceived="20030519163434" -->
<!-- sent="Mon, 19 May 2003 12:33:50 -0400" -->
<!-- isosent="20030519163350" -->
<!-- name="Gary Miller" -->
<!-- email="garymiller@starband.net" -->
<!-- subject="RE: SIAI's flawed friendliness analysis" -->
<!-- id="000001c31e24$83adfea0$3e553f94@GaryMiller01" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3EC86CB3.4050006@access.net.ec" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Gary Miller (<a href="mailto:garymiller@starband.net?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis"><em>garymiller@starband.net</em></a>)<br>
<strong>Date:</strong> Mon May 19 2003 - 10:33:50 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6740.html">Gary Miller: "RE: Security overkill"</a>
<li><strong>Previous message:</strong> <a href="6738.html">Philip Sutton: "Re: No need to rush AGI development?"</a>
<li><strong>In reply to:</strong> <a href="6734.html">Leonardo Wild: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6742.html">Cliff Stabbert: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6739">[ date ]</a>
<a href="index.html#6739">[ thread ]</a>
<a href="subject.html#6739">[ subject ]</a>
<a href="author.html#6739">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Leonard Wild said:
<br>
<p><em>&gt;&gt; And I disagree with Gary Miller that his post of 5/17/03 ...
</em><br>
<em>&gt;&gt; ... is really a solution to the &quot;friendliness problem.&quot; His proposed 
</em><br>
<em>&gt;&gt; solution is one that deals with the &quot;safety&quot; of a FAI project.
</em><br>
<p>While a large part of my posting did deal with those issues...
<br>
<p>I also described a logging and testing process where the FAI could be 
<br>
re-examined on a continual basis to ensure no inadvertent anti-social 
<br>
training crept in or that any training was interpreted in a way that 
<br>
led to an anti-social inference.  By replaying existing training the
<br>
code can be evaluated for errors in the reasoning process itself. By 
<br>
performing personality tests and setting up ethical challenges for the 
<br>
AI at the end of each training run.  Any log containing antisocial 
<br>
inferences can traced back assuming complete causal inferencing is
<br>
recorded
<br>
to those inputs or insufficient inputs which triggered the antisocial 
<br>
inferences.  This allows for a development process where the experts can
<br>
<p>experiment with the order and prioritization of moral concepts and don't
<br>
have 
<br>
to right the first time!  If on Tuesday they have a paranoid psychotic
<br>
AI they
<br>
restore him to Friday and try to determine what made him bonkers!  Maybe
<br>
they 
<br>
Should have left him read Gandhi's autobiography instead of of Richard
<br>
Nixon's.
<br>
Or maybe they just forgot to assert Gandi good, Nixon bad!
<br>
<p>I did not of course try to define all the inputs that are required to
<br>
make
<br>
a normal, social, and mentally healthy FAI or even attempt to define
<br>
what one of those
<br>
is.  I leave that to the behavioral psychologists and knowledge
<br>
engineers.  
<br>
Most healthy normal parents who bother to make the time seem to do a
<br>
fairly 
<br>
decent job of creating well-adjusted humans even with all the negative
<br>
outside 
<br>
influences of television, peer pressure and raging hormones.  I dare say
<br>
if it
<br>
weren't for all those raging hormones I had, my parents would have
<br>
probably done a 
<br>
pretty good job.
<br>
<p><p>-----Original Message-----
<br>
From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis">owner-sl4@sl4.org</a>] On Behalf Of Leonardo
<br>
Wild
<br>
Sent: Monday, May 19, 2003 1:34 AM
<br>
To: <a href="mailto:sl4@sl4.org?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis">sl4@sl4.org</a>
<br>
Subject: Re: SIAI's flawed friendliness analysis
<br>
<p><p><p>Hello,
<br>
<p>I agree with Bill Hibbard on the following:
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Understanding what humans are, what the AI itself is, and what values 
</em><br>
<em>&gt; are, is implicit in the simulation model of the world created by any 
</em><br>
<em>&gt; intelligent mind. It uses this model to predict the long-term 
</em><br>
<em>&gt; consequences of its behaviors on its values. Such understanding is 
</em><br>
<em>&gt; prescribed by an intelligence model rather than a safe AI model.
</em><br>
<em>&gt; 
</em><br>
<p>And I disagree with Gary Miller that his post of 5/17/03 ...
<br>
<p>Gary Miller wrote:
<br>
<em>&gt; My proposed solution to friendliness problem.
</em><br>
<em>&gt;  
</em><br>
<em>&gt; Note some of you will laugh this off as overkill.  But believe me 
</em><br>
<em>&gt; having
</em><br>
<em>&gt; worked as a
</em><br>
<em>&gt; consultant for the government for a number of years, this is just 
</em><br>
<em>&gt; business as
</em><br>
<em>&gt; usual for NSA.  It is a very expensive but very secure development
</em><br>
process.
<br>
<em>&gt; It is based upon separation and balance of power.  No one person has
</em><br>
the 
<br>
<em>&gt; access
</em><br>
<em>&gt; and knowledge to compromise the system.  Relationships between team
</em><br>
members
<br>
<em>&gt; must be prohibited to prevent possibility of collusion.
</em><br>
<p>&lt;(snip)&gt;
<br>
<p>... is really a solution to the &quot;friendliness problem.&quot; His proposed 
<br>
solution is one that deals with the &quot;safety&quot; of a FAI project.
<br>
<p>Naturally, the safety is necessary to make sure that &quot;intended 
<br>
friendliness programming&quot; doesn't or can't get meddled with. In other 
<br>
words, its a solution (partial) as to how to go about making sure that 
<br>
intended outcomes are not changed by those working on such a project or 
<br>
by those who would have some interest in infiltrating such a project. 
<br>
Yet it does not define what &quot;friendliness&quot; is nor what core values are. 
<br>
Is is one more way to make sure that the following does happen (as 
<br>
expressed by Bill Hibbard):
<br>
<p><em>&gt; BH: The ambiguous definitions in the SIAI analysis will be exploited 
</em><br>
<em>&gt; by powerful people and institutions to create AIs that protect and 
</em><br>
<em>&gt; enhance their own interests.
</em><br>
<p>In his posting he also writes:
<br>
<p><em>&gt; BH: But I think that AI values
</em><br>
<em>&gt; for human happiness link AI values with human values and create a 
</em><br>
<em>&gt; symbiotic system combining humans and AIs. Keeping humans &quot;in the 
</em><br>
<em>&gt; loop&quot; offers the best hope of preventing any drift away from human 
</em><br>
<em>&gt; interests. An AI with humans in its loop will have no motive to design
</em><br>
<p><em>&gt; an AI without humans in its loop. And as long as humans are in the 
</em><br>
<em>&gt; loop, they can exert reinforcement to protect their own interests.
</em><br>
<em>&gt; 
</em><br>
(...)
<br>
<em>&gt; 
</em><br>
<em>&gt; The &quot;*deep* understanding of values&quot; is implicit in superior 
</em><br>
<em>&gt; intelligence. It is a very accurate simulation model of the world that
</em><br>
<p><em>&gt; includes understanding of how value systems work, and the effects that
</em><br>
<p><em>&gt; different value systems would have on brains. But choosing between 
</em><br>
<em>&gt; different value systems requires base values for comparing the 
</em><br>
<em>&gt; consequences of different value systems.
</em><br>
<p>Which is the core of the problem, which is also found in the partial or 
<br>
&nbsp;&nbsp;qualitatively limited definitions of &quot;intelligence,&quot; &quot;intuition,&quot; 
<br>
&quot;smartness,&quot; &quot;awareness,&quot; &quot;values,&quot; &quot;understanding,&quot; &quot;knowledge,&quot; etc.
<br>
<p>We can speak of intelligence like we can speak of the wind, but &quot;wind&quot; 
<br>
is a concept that is implicit to &quot;superior intelligence&quot; as long as the 
<br>
&quot;organism&quot; that has the concept has experienced &quot;wind.&quot; But the concept 
<br>
of wind, for someone whose life (or livelihood) depends on it, will have
<br>
<p>qualitatively different and relatively precise definitions about the 
<br>
kinds of &quot;wind.&quot;  The more you depend on it, the more precise (and 
<br>
diferentiating) will be your definition of wind (or any other subject).
<br>
<p>&quot;Yes, it is wind, but what _kind_ of wind?&quot;
<br>
<p>or &quot;What kind of snow?&quot;
<br>
or &quot;What kind of fish?&quot;
<br>
or &quot;What kind of intelligence?&quot;
<br>
<p>etc.
<br>
<p>So, too, friendliness is implicit to superior intelligence, yes (though 
<br>
not only then, according to studies presented in a book called GOOD 
<br>
NATURED -I can check on the author another time, for anyone interested).
<br>
<p>But what _is_ it that makes someone behave in a friendly way? What kind 
<br>
of values create the context in which it is &quot;intelligent&quot; to behave in a
<br>
<p>friendly way? Sometimes, friendliness can be the wrong type of attitude 
<br>
or behavior in a given context in order to survive; in fact, it may even
<br>
<p>be &quot;unintelligent&quot; to behave in a friendly way.
<br>
<p>Friendliness is directly related with &quot;values,&quot; just as values are 
<br>
directly related to &quot;needs.&quot; But there are different kinds of needs 
<br>
which reflect in different kinds of values. So, certain values can be 
<br>
viewed as positive or &quot;good&quot; in a certain context, and completely the 
<br>
opposite in another. The bottom line is to find the kind of need for 
<br>
values that will make an AI &quot;friendly&quot; towards humanity at large (though
<br>
<p>perhaps not necessarily towards particular individual human beings) or, 
<br>
rather, towards &quot;Life,&quot; in the most general sense. (?)
<br>
<p>As Bill Hibbard wrote:
<br>
<p><em>&gt; Keeping humans &quot;in the loop&quot; offers the best hope of preventing any 
</em><br>
<em>&gt; drift away from human interests.
</em><br>
<p>Meaning, how can we make sure that the needs of an AI for the &quot;respected
<br>
<p>and respectful&quot; presence and existence of human beings (and the context 
<br>
they need for survival and evolution) gets reflected in and AI's value 
<br>
system or structure? And how can such a &quot;set of values&quot; be programmed 
<br>
into the core or kernel of an AI regardless of future development and 
<br>
autopoietic growth? Because, for programming's sake, it's not enough for
<br>
<p>&nbsp;&nbsp;types of needs, types of values, types of friendliness to be 
<br>
&quot;implicit&quot; in superior intelligence (human or AI or n-versions of it), 
<br>
but it must be made &quot;explicit,&quot; which means the creation of an UML 
<br>
(Universal Modelling Language)-type diagram or sets of diagrams that 
<br>
enable programmers and project designers and all those involved in the 
<br>
FAI project to &quot;explicitly agree&quot; on the &quot;ingredients&quot; of concepts (such
<br>
<p>as friendliness) ... which is but a very broad ideal for a given type of
<br>
<p>behavior that appears to be as implicit as when we speak of &quot;happiness&quot; 
<br>
or &quot;anger.&quot;
<br>
<p>The thing to consider is that the ambiguous definitions of _any_ 
<br>
analysis will be exploited by powerful people and institutions to 
<br>
protect and enhance their own interests.
<br>
<p>I read a while ago posts on the &quot;search for money&quot; to fund SIAI, but 
<br>
never was there any questioning as to why money (which is a man made 
<br>
technology) is scarce (not just for funding deep science projects), and 
<br>
why money appears to flow only into certain type of projects, and why it
<br>
<p>is necessary for someone (like Eliezer) to _prove_ that the money is 
<br>
&quot;well invested&quot; ... it even remains ambigous what a &quot;good investment&quot; 
<br>
means in monetary terms. A lot of assumptions that do not question the 
<br>
&quot;technology money&quot; at all, which becomes, once again, a problem similar 
<br>
to what Bill Hibbard already wrote about finding the base values to the 
<br>
values you wish to work with. If you wish to work with &quot;funding&quot; then 
<br>
you must necessarily work with money, but if money's &quot;inner workings&quot; 
<br>
are implicit or ambigous in our understanding, then we can forever chase
<br>
<p>after it without really knowing what &quot;it&quot; is nor why it is not available
<br>
<p>for &quot;good&quot; projects. We may be intelligent, but our intelligence doesn't
<br>
<p>seem to be willing to create an awareness and hopefully understanding of
<br>
<p>one of the most widely used man-made contraptions (money).
<br>
<p>The flaw, if I may say so (and not only in relation to &quot;friendliness 
<br>
analysis&quot; but to many apparently implicitly understood concepts, 
<br>
including something as illusorily clear as &quot;money&quot;) is to avoid creating
<br>
<p>UML-diagramable versions of the concepts so we can all agree on them 
<br>
rather than spending our efforts disagreeing about them or going off on 
<br>
tangents.
<br>
<p>What this means is that certain concepts must be re-analyzed in this new
<br>
<p>light (with a different goal in mind, one of programmability, one of 
<br>
consensus agreements) rather than saying &quot;that's already a closed 
<br>
issue.&quot; It was, more or less, a closed issue tha the world is &quot;flat.&quot; It
<br>
<p>is, more or less, an assumed concept that it isn't possible to make 
<br>
triangles* that have three inner angles each of 90 degrees, or squares**
<br>
<p>with four inner angles each of 105 degrees (to give but two examples).
<br>
<p>Best,
<br>
<p>Leonardo Wild
<br>
<p>PS: Crocker's Rules apply ...
<br>
<p>... considering the fact that I was 'ousted' for a month for being 
<br>
un-scientific; yet my email created an activity of discussion on the 
<br>
subject of multiple universes and infinite universes of over 140 emails 
<br>
even though that subject had not previously been (according to the 
<br>
archives) discussed on this list, or at least not under such a heading. 
<br>
Paradoxical, isn't it?
<br>
<p>***
<br>
<p>*Triangle = 1. in geometry, a figure bounded by three lines, and 
<br>
containing three angles.
<br>
<p>**Square = 1. b) more or less cubical; rectangular and 
<br>
three-dimensional, as a box.
<br>
<p><p><p><p><p><p><p><p><p><p><p><p><em>&gt; 
</em><br>
<p><p><p><p><p>
<br><p>
<p><p><p><hr>
<ul>
<li>text/x-vcard attachment: <a href="../att-6739/01-Gary_A._Miller__garymiller_starband.net___garymiller_starband.net_.vcf">Gary_A._Miller__garymiller_starband.net___garymiller_starband.net_.vcf</a>
</ul>
<!-- attachment="01-Gary_A._Miller__garymiller_starband.net___garymiller_starband.net_.vcf" -->
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6740.html">Gary Miller: "RE: Security overkill"</a>
<li><strong>Previous message:</strong> <a href="6738.html">Philip Sutton: "Re: No need to rush AGI development?"</a>
<li><strong>In reply to:</strong> <a href="6734.html">Leonardo Wild: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6742.html">Cliff Stabbert: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6739">[ date ]</a>
<a href="index.html#6739">[ thread ]</a>
<a href="subject.html#6739">[ subject ]</a>
<a href="author.html#6739">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
