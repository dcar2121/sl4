<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI's flawed friendliness analysis</h1>
<!-- received="Fri May  9 22:14:23 2003" -->
<!-- isoreceived="20030510041423" -->
<!-- sent="Sat, 10 May 2003 00:14:27 -0400" -->
<!-- isosent="20030510041427" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: SIAI's flawed friendliness analysis" -->
<!-- id="3EBC7CA3.8000801@pobox.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="Pine.GSO.4.44.0305091747370.23415-100000@demedici.ssec.wisc.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Fri May 09 2003 - 22:14:27 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6655.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6653.html">Eliezer S. Yudkowsky: "META: Mail loop dealt with (or so I hope)"</a>
<li><strong>In reply to:</strong> <a href="6646.html">Bill Hibbard: "SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6655.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6655.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6673.html">Cliff Stabbert: "Re: Why FAI Theory is both Necessary and Hard (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>Reply:</strong> <a href="6681.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6654">[ date ]</a>
<a href="index.html#6654">[ thread ]</a>
<a href="subject.html#6654">[ subject ]</a>
<a href="author.html#6654">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Bill Hibbard wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; This critique refers to the following documents:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;   GUIDELINES:  <a href="http://www.intelligence.org/friendly/guidelines.html">http://www.intelligence.org/friendly/guidelines.html</a>
</em><br>
<em>&gt;   FEATURES:    <a href="http://www.intelligence.org/friendly/features.html">http://www.intelligence.org/friendly/features.html</a>
</em><br>
<em>&gt;   CFAI:        <a href="http://www.intelligence.org/CFAI/index.html">http://www.intelligence.org/CFAI/index.html</a>
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 1. The SIAI analysis fails to recognize the importance of
</em><br>
<em>&gt; the political process in creating safe AI.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is a fundamental error in the SIAI analysis. CFAI 4.2.1
</em><br>
<em>&gt; says &quot;If an effort to get Congress to enforce any set of
</em><br>
<em>&gt; regulations were launched, I would expect the final set of
</em><br>
<em>&gt; regulations adopted to be completely unworkable.&quot; It further
</em><br>
<em>&gt; says that government regulation of AI is unnecessary because
</em><br>
<em>&gt; &quot;The existing force tending to ensure Friendliness is that
</em><br>
<em>&gt; the most advanced projects will have the brightest AI
</em><br>
<em>&gt; researchers, who are most likely to be able to handle the
</em><br>
<em>&gt; problems of Friendly AI.&quot; History vividly teaches the danger
</em><br>
<em>&gt; of trusting the good intentions of individuals.
</em><br>
<p>...and, of course, the good intentions and competence of governments.
<br>
<p>There are few people who want to destroy the world on purpose.  The 
<br>
problem is people doing it by accident.
<br>
<p><em>&gt; The singularity will completely change power relations in
</em><br>
<em>&gt; human society. People and institutions that currently have
</em><br>
<em>&gt; great power and wealth will know this, and will try to
</em><br>
<em>&gt; manipulate the singularity to protect and enhance their
</em><br>
<em>&gt; positions. The public generally protects its own interests
</em><br>
<em>&gt; against the narrow interests of such powerful people and
</em><br>
<em>&gt; institutions via widespread political movements and the
</em><br>
<em>&gt; actions of democratically elected government. Such
</em><br>
<em>&gt; political action has never been more important than it
</em><br>
<em>&gt; will be in the singularity.
</em><br>
<p>There is nothing to fight over.
<br>
<p>The human species is fifty thousand years old.  We've come a long way 
<br>
during that time.  We've come a long way during the last century.  At the 
<br>
start of the twentieth century, neither women nor blacks could vote in the 
<br>
US.  The twentienth century was, by the standards of today's world, 
<br>
barbaric.  Are we, ourselves, raving barbarians by any future perspective? 
<br>
&nbsp;&nbsp;Almost certainly.  For one civilization to leave a permanent impress on 
<br>
the next billion years is no more fair than for one individual to do so, 
<br>
and just as deadly.  Individuals grow in their moralities, as do 
<br>
civilizations.  That fundamental capability is what needs to be 
<br>
transferred into a Friendly AI, not a morality frozen in time.  A human is 
<br>
capable of understanding the concept of moral improvement; a true FAI, a 
<br>
real mind in the humane frame of reference, must be able to do the same, 
<br>
or something vital is missing.
<br>
<p>If you want to make an FAI that is capable of moral improvement, 
<br>
constructed via a fair, species-representative method, then the only 
<br>
question is whether you have the knowledge to do that - to build an AI 
<br>
that fully understands the concept of moral failure and moral improvement 
<br>
as well as we do ourselves, and that symbolizes a species rather than any 
<br>
one individual or civilization.
<br>
<p>If you commit to that standard, then there are no conflicts of interest to 
<br>
fight over, no individually controllable variables that knowably correlate 
<br>
with the outcome in a way that creates conflicts of interest.
<br>
<p>What is dangerous is that someone who believes that &quot;AIs just do what 
<br>
they're told&quot; will think that the big issue is who gets to tell AIs what 
<br>
to do.  Such people will not, of course, succeed in taking over the world; 
<br>
I find it extremely implausible that any human expressing such a goal has 
<br>
the depth of understanding required to build anything that is not a 
<br>
thermostat AI.  The problem is that these people might succeed in 
<br>
destroying the world, given enough computing power to brute-force AI with 
<br>
no real understanding of it.
<br>
<p>Anyone fighting over what values the AI ought to have is simply fighting 
<br>
over who gets to commit suicide.  If you know how to give an AI any set of 
<br>
values, you know how to give it a humanly representative set of values. 
<br>
It is really not that hard to come up with fair strategies for any given 
<br>
model of FAI.  Coming up with an FAI model that works is very hard.
<br>
<p>It is not a trivial thing, to create a mind that embodies the full human 
<br>
understanding of morality.  There is a high and beautiful sorcery to it. 
<br>
I find it hard to believe that any human truly capable of learning and 
<br>
understanding that art would use it to do something so small and mean. 
<br>
And anyone else would be effectively committing suicide, whether they 
<br>
realized it or not, because the AI would not hear what they thought they 
<br>
had said.
<br>
<p><em>&gt; The reinforcement learning values of the largest (and hence
</em><br>
<em>&gt; most dangerous) AIs
</em><br>
<p>Indeed the largest computers are the most dangerous, but not in the way 
<br>
that you mean.  They are dangerous because even people who don't 
<br>
understand what they're doing may be able to brute-force AI given truly 
<br>
insane amounts of computing power.  Friendliness, of course, cannot be 
<br>
brute-forced.
<br>
<p><em> &gt; will be defined by the corporations and
</em><br>
<em>&gt; governments that build them, not the AI researchers working
</em><br>
<em>&gt; for those orgnaizations. Those organizations will give their
</em><br>
<em>&gt; AIs values that reflect the organizations' values: profits in
</em><br>
<em>&gt; the case of corporations, and political and military power
</em><br>
<em>&gt; in the case of governments.
</em><br>
<p>Yes, this is a good example of what I mean by &quot;small and mean&quot;.  Anyone 
<br>
trying to implement a goal system like that is a threat, not because they 
<br>
could take over the world, but because they haven't mastered the 
<br>
understanding necessary to avoid destroying the world.
<br>
<p><em> &gt; Only a strong public movement
</em><br>
<em>&gt; driving government regulation will be able to coerce these
</em><br>
<em>&gt; organizations to design AI values to protect the interests
</em><br>
<em>&gt; of all humans. This government regulation must include an
</em><br>
<em>&gt; agency to monitor AI development and enforce regulations.
</em><br>
<p>In theory, I can imagine a set of AI development rules, perhaps handed 
<br>
down from after the Singularity, so simple and obvious that even someone 
<br>
who doesn't really understand intelligence could follow those rules and be 
<br>
safe.  Or such a thing could turn out to be impossible.  It is certainly 
<br>
beyond my present ability to write such a set of rules.  Currently, there 
<br>
is no set of guidelines I can write that will make an AI project safe if 
<br>
the researcher does not have a sufficiently deep understanding to have 
<br>
written the same guidelines from scratch.
<br>
<p><em>&gt; The breakthrough ideas for achieving AI will come from
</em><br>
<em>&gt; individual researchers, many of whom will want their AI to
</em><br>
<em>&gt; serve the broad human interest. But their breakthrough ideas
</em><br>
<em>&gt; will become known to wealthy organizations. Their research
</em><br>
<em>&gt; will either be in the public domain, done for hire by wealthy
</em><br>
<em>&gt; organizations, or will be sold to such organizations.
</em><br>
<p>SIAI is a nonprofit, and I am not for sale at any price.  Such are my own 
<br>
choices, which are all that I control.
<br>
<p><em>&gt; Breakthrough research may simply be seized by governments and
</em><br>
<em>&gt; the researchers prohibited from publishing, as was done for
</em><br>
<em>&gt; research on effective cryptography during the 1970s. The most
</em><br>
<em>&gt; powerful AIs won't exist on the $5,000 computers on
</em><br>
<em>&gt; researchers' desktops, but on the $5,000,000,000 computers
</em><br>
<em>&gt; owned by wealthy organizations.  The dangerous AIs will be the
</em><br>
<em>&gt; ones capable of developing close personal relations with huge
</em><br>
<em>&gt; numbers of people. Such AIs will be operated by wealthy
</em><br>
<em>&gt; organizations, not individuals.
</em><br>
<p>Your political recommendations appear to be based on an extremely 
<br>
different model of AI.  Specifically:
<br>
<p>1)  &quot;AIs&quot; are just very powerful tools that amplify the short-term goals 
<br>
of their users, like any other technology.
<br>
<p>2)  AIs have power proportional to the computing resources invested in 
<br>
them, and everyone has access to pretty much the same theoretical model 
<br>
and class of AI.
<br>
<p>3)  There is no seed AI, no rapid recursive self-improvement, no hard 
<br>
takeoff, no &quot;first&quot; AI.  AIs are just new forces in existing society, 
<br>
coming into play a bit at a time, as everyone's AI technology improves at 
<br>
roughly the same rate.
<br>
<p>4)  Anyone can make an AI that does anything.  AI morality is an easy 
<br>
problem with fully specifiable arbitrary solutions that are reliable and 
<br>
humanly comprehensible.
<br>
<p>5)  Government workers can look at an AI design and tell what the AI's 
<br>
morality does and whether it's safe.
<br>
<p>6)  There are variables whose different values correlate to socially 
<br>
important differences in outcomes, such that government workers can 
<br>
understand the variables and their correlation to the outcomes, and such 
<br>
that society expects to have a conflict of interest with individuals or 
<br>
organizations as to the values of those variables, with the value to 
<br>
society of this conflict of interest exceeding the value to society of the 
<br>
outcome differentials that depend on the greater competence of those 
<br>
individuals or organizations.  Otherwise there's nothing worth voting on.
<br>
<p>I disagree with all six points, due to a different model of AI.
<br>
<p><em>&gt; Individuals working toward the singularity may resist
</em><br>
<em>&gt; regulation as interference with their research, as was
</em><br>
<em>&gt; evident in the SL4 discussion of testimony before
</em><br>
<em>&gt; Congressman Brad Sherman's committee. But such regulation
</em><br>
<em>&gt; will be necessary to coerce the wealthy organizations
</em><br>
<em>&gt; that will own the most powerful AIs. These will be much
</em><br>
<em>&gt; like the regulations that restrain powerful organizations
</em><br>
<em>&gt; from building dangerous products (cars, household
</em><br>
<em>&gt; chemicals, etc), polluting the environment, and abusing
</em><br>
<em>&gt; citizens.
</em><br>
<p>Hm.  I think all I can do here is point to Part III of LOGI and say that 
<br>
my concern is with FOOMgoing AIs (AIs that go FOOM, as in a hard takeoff). 
<br>
&nbsp;&nbsp;Computer programs with major social effects, owned by powerful 
<br>
organizations, that are *not* capable of rapid recursive self-improvement 
<br>
and sparking a superintelligent transition, are not the kind of AI I worry 
<br>
about.  If there are governmentally understandable variables that 
<br>
correlate to democratically disputed social outcomes, and so on, then I 
<br>
might indeed write it off as ordinary politics.
<br>
<p><em>&gt; 2. The design recommendations in GUIDELINES 3 fail to
</em><br>
<em>&gt; define rigorous standards for &quot;changes to supergoal
</em><br>
<em>&gt; content&quot; in recommendation 3, for &quot;valid&quot; and &quot;good&quot; in
</em><br>
<em>&gt; recommendation 4, for &quot;programmers' intentions&quot; in
</em><br>
<em>&gt; recommendation 5, and for &quot;mistaken&quot; in recommendation 7.
</em><br>
<p>Yes, I know.  CFAI is confessedly incomplete.
<br>
<p>To work, the theory of FAI is going to have to dig down to a point where 
<br>
the theory is described *entirely* in terms of:
<br>
<p>a) things that physically exist in external reality
<br>
b) incoming sensory information available to the AI
<br>
c) computations the AI knows how to perform
<br>
<p>In short, what's needed is a naturalistic description of moral systems 
<br>
building moral systems.  I agree it's alarming that I don't have the full 
<br>
specification of the entire pathway in hand at this instant, and I'm 
<br>
working to remedy that.  But you surely would not find it in a small set 
<br>
of guidelines.
<br>
<p><em>&gt; These recommendations are about the AI learning its own
</em><br>
<em>&gt; supergoal. But even digging into corresponding sections
</em><br>
<em>&gt; of CFAI and FEATURES fails to find rigorus standards for
</em><br>
<em>&gt; defining critical terms in these recommendations.
</em><br>
<em>&gt; Determination of their meanings is left to &quot;programmers&quot;
</em><br>
<em>&gt; or the AI itself. Without rigorous standards for these
</em><br>
<em>&gt; terms, wealthy organizations constructing AIs will be
</em><br>
<em>&gt; free to define them in any way that serves their purposes
</em><br>
<em>&gt; and hence to construct AIs that serve their narrow
</em><br>
<em>&gt; interests rather than the general public interest.
</em><br>
<p>The guidelines are not intended as a means of making AI programmers do 
<br>
something against their will.  I'll be astonished if I can get people to 
<br>
understand the method with their wholehearted cooperation and willingness 
<br>
to devote substantial amounts of time.  I see little or no hope for people 
<br>
who are vaguely interested and casually agreeable, unless they can be 
<br>
transformed into the former class.  Successfully *enforce* the creation of 
<br>
Friendly AI on someone who is actually *opposed* to it?  No way.  Not a 
<br>
chance.  It's asking enough that this happen on purpose.
<br>
<p>It's not conflicts of interest you need to worry about; any human who 
<br>
deeply understands the Prisoner's Dilemna can find a way to cooperate in 
<br>
almost any real-world circumstance, and you won't find someone capable of 
<br>
creating FAI who doesn't deeply understand the Prisoner's Dilemna.  The 
<br>
scary part is someone getting the definitions *wrong*, as in &quot;not doing 
<br>
what they thought it would&quot;.
<br>
<p><em>&gt; 3. CFAI defines &quot;friendliness&quot; in a way that can only
</em><br>
<em>&gt; be determined by an AI after it has developed super-
</em><br>
<em>&gt; intelligence, and fails to define rigorous standards
</em><br>
<em>&gt; for the values that guide its learning until it reaches
</em><br>
<em>&gt; super-intelligence
</em><br>
<em> &gt;
</em><br>
<em>&gt; The actual definition of &quot;friendliness&quot; in CFAI 3.4.4
</em><br>
<em>&gt; requires the AI to know most humans sufficiently well
</em><br>
<em>&gt; to decompose their minds into &quot;panhuman&quot;, &quot;gaussian&quot; and
</em><br>
<em>&gt; &quot;personality&quot; layers, and to &quot;converge to normative
</em><br>
<em>&gt; altruism&quot; based on collective content of the &quot;panhuman&quot;
</em><br>
<em>&gt; and &quot;gaussian&quot; layers. This will require the development
</em><br>
<em>&gt; of super-intelligence over a large amount of learning.
</em><br>
<em>&gt; The definition of friendliness values to reinforce that
</em><br>
<em>&gt; learning is left to &quot;programmers&quot;. As in the previous
</em><br>
<em>&gt; point, this will allow wealthy organizations to define
</em><br>
<em>&gt; intial learning values for their AIs as they like.
</em><br>
<p>I don't believe a young Friendly AI should be meddling in the real world 
<br>
at all.  If for some reason this becomes necessary, it might as well do 
<br>
what the programmer says, maybe with its own humane veto.  I'd trust a 
<br>
programmer more than I'd trust an infant Friendly AI, because regardless 
<br>
of its long-term purpose, during infancy the FAI is likely to have neither 
<br>
a better approximation to humaneness, nor a better understanding of the 
<br>
real world.
<br>
<p>You are correct that an FAI theory is not finished until there is, in 
<br>
hand, a specification of how the FAI is to be taught.
<br>
<p><em>&gt; 4. The CFAI analysis is based on a Bayesian reasoning
</em><br>
<em>&gt; model of intelligence, which is not a sufficient model
</em><br>
<em>&gt; for producing intelligence.
</em><br>
<em>&gt;
</em><br>
<em>&gt; While Bayesian reasoning has an important role in
</em><br>
<em>&gt; intelligence, it is not sufficient. Sensory experience
</em><br>
<em>&gt; and reinforcement learning are fundamental to
</em><br>
<em>&gt; intelligence. Just as symbols must be grounded in
</em><br>
<em>&gt; sensory experience, reasoning must be grounded in
</em><br>
<em>&gt; learning and emerges from it because of the need to
</em><br>
<em>&gt; solve the credit assignment problem, as discussed at:
</em><br>
<em>&gt; 
</em><br>
<em>&gt;   <a href="http://www.mail-archive.com/<a href="mailto:agi@v2.listbox.com?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis">agi@v2.listbox.com</a>/msg00390.html">http://www.mail-archive.com/<a href="mailto:agi@v2.listbox.com?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis">agi@v2.listbox.com</a>/msg00390.html</a>
</em><br>
<p>Non-Bayesian?  I don't think you're going to find much backing on this 
<br>
one.  If you've really discovered a non-Bayesian form of reasoning, write 
<br>
it up and collect your everlasting fame.  Personally I consider such a 
<br>
thing almost exactly analogous to a perpetual motion machine.  Except that 
<br>
a perpetual motion machine is merely physically impossible, while 
<br>
&quot;non-Bayesian reasoning&quot; appears to be mathematically impossible.  Though 
<br>
of course I could be wrong.
<br>
<p>Reinforcement learning emerges from Bayesian reasoning, not the other way 
<br>
around.  Sensory experience likewise.
<br>
<p>For more about Bayesian reasoning, see:
<br>
&nbsp;&nbsp;<a href="http://yudkowsky.net/bayes/bayes.html">http://yudkowsky.net/bayes/bayes.html</a>
<br>
&nbsp;&nbsp;<a href="http://bayes.wustl.edu/etj/science.pdf.html">http://bayes.wustl.edu/etj/science.pdf.html</a>
<br>
<p>Reinforcement, specifically, emerges in a Bayesian decision system:
<br>
&nbsp;&nbsp;<a href="http://intelligence.org/CFAI/design/clean.html#reinforcement">http://intelligence.org/CFAI/design/clean.html#reinforcement</a>
<br>
<p><pre>
-- 
Eliezer S. Yudkowsky                          <a href="http://intelligence.org/">http://intelligence.org/</a>
Research Fellow, Singularity Institute for Artificial Intelligence
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6655.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6653.html">Eliezer S. Yudkowsky: "META: Mail loop dealt with (or so I hope)"</a>
<li><strong>In reply to:</strong> <a href="6646.html">Bill Hibbard: "SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6655.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6655.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6673.html">Cliff Stabbert: "Re: Why FAI Theory is both Necessary and Hard (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>Reply:</strong> <a href="6681.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6654">[ date ]</a>
<a href="index.html#6654">[ thread ]</a>
<a href="subject.html#6654">[ subject ]</a>
<a href="author.html#6654">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
