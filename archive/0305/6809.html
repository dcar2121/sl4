<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: SIAI's flawed friendliness analysis</h1>
<!-- received="Fri May 23 21:29:03 2003" -->
<!-- isoreceived="20030524032903" -->
<!-- sent="Fri, 23 May 2003 23:29:53 -0400" -->
<!-- isosent="20030524032953" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: SIAI's flawed friendliness analysis" -->
<!-- id="LAEGJLOGJIOELPNIOOAJKECAFAAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3ECEE4E0.2000301@posthuman.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Fri May 23 2003 - 21:29:53 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6810.html">Philip Sutton: "Friendly AGIs will need to be friendly to more than humans"</a>
<li><strong>Previous message:</strong> <a href="6808.html">Brian Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6808.html">Brian Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6810.html">Philip Sutton: "Friendly AGIs will need to be friendly to more than humans"</a>
<li><strong>Reply:</strong> <a href="6810.html">Philip Sutton: "Friendly AGIs will need to be friendly to more than humans"</a>
<li><strong>Reply:</strong> <a href="6811.html">Philip Sutton: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6821.html">Bill Hibbard: "RE: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6809">[ date ]</a>
<a href="index.html#6809">[ thread ]</a>
<a href="subject.html#6809">[ subject ]</a>
<a href="author.html#6809">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
There are a lot of good points and interesting issues mixed up here, but I
<br>
think the most key point is the division between
<br>
<p>-- those who believe a hard takeoff is reasonably likely, based on a radical
<br>
insight in AI design coupled with a favorable trajectory of self-improvemetn
<br>
of a particular AI system
<br>
<p>-- those who believe in a soft takeoff, in which true AI is approached
<br>
gradually [in which case government regulation, careful peer review and so
<br>
forth are potentially relevant]
<br>
<p>The soft takeoff brings with it many obvious possibilities for safeguarding,
<br>
which are not offered in the hard takeoff scenario.  These possibilities are
<br>
the ones Bill Hibbard is exploring, I think.  A lot of what SIAI is saying
<br>
is more relevant to the hard takeoff scenario, on the other hand.
<br>
<p>My own projection is a semi-hard takeoff, which doesn't really bring much
<br>
reassurance...
<br>
<p>ben g
<br>
<p><em>&gt; -----Original Message-----
</em><br>
<em>&gt; From: <a href="mailto:owner-sl4@sl4.org?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis">owner-sl4@sl4.org</a> [mailto:<a href="mailto:owner-sl4@sl4.org?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis">owner-sl4@sl4.org</a>]On Behalf Of Brian
</em><br>
<em>&gt; Atkins
</em><br>
<em>&gt; Sent: Friday, May 23, 2003 11:20 PM
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis">sl4@sl4.org</a>
</em><br>
<em>&gt; Subject: Re: SIAI's flawed friendliness analysis
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Bill Hibbard wrote:
</em><br>
<em>&gt; &gt; On Tue, 20 May 2003, Brian Atkins wrote:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&gt;. . .
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt;&gt;If humans can design AIs smarter than humans, then humans
</em><br>
<em>&gt; &gt;&gt;&gt;can regulate AIs smarter than humans.
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt;Just because a human can design some seed AI code that grows into a SI
</em><br>
<em>&gt; &gt;&gt;does not imply that humans or human-level AIs can successfully
</em><br>
<em>&gt; &gt;&gt;&quot;regulate&quot; grown SIs.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The regulation is not intended to trace the thoughts
</em><br>
<em>&gt; &gt; and development of the SI. The inspection is of the
</em><br>
<em>&gt; &gt; design, not the changing contents of its mind. If it's
</em><br>
<em>&gt; &gt; initial reinforcement values are for human happiness,
</em><br>
<em>&gt; &gt; and its simulation and reinforcement learning
</em><br>
<em>&gt; &gt; algorithms are accurate, then we can trust the way it
</em><br>
<em>&gt; &gt; will develop. In an earlier email I made the analogy
</em><br>
<em>&gt; &gt; to game playing programs. If their game simulation
</em><br>
<em>&gt; &gt; and learning algorithms are accurate and efficient,
</em><br>
<em>&gt; &gt; and their reinforcement learning values are for winning
</em><br>
<em>&gt; &gt; the game, then although the details of their play are
</em><br>
<em>&gt; &gt; not predictable, the fact that they will play to win
</em><br>
<em>&gt; &gt; is predictable.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The first SI will be designed and educated by humans.
</em><br>
<em>&gt; &gt; Humans will be able to understand and regulate its
</em><br>
<em>&gt; &gt; design, and regulate how it is educated. This will
</em><br>
<em>&gt; &gt; create trusted safe SIs. They can then design and
</em><br>
<em>&gt; &gt; regulate improved SIs, with one independently
</em><br>
<em>&gt; &gt; designed SI inspecting the designs of another.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; You have a differing view of the development process of AGI from myself
</em><br>
<em>&gt; (and I would guess most people here). I do not believe it is that likely
</em><br>
<em>&gt; that SI will arrive, in its full design, from humans. I find it much
</em><br>
<em>&gt; more likely that it will come from a lengthy (in terms of iterations)
</em><br>
<em>&gt; continuing redesign process undertaken by an approximately human level
</em><br>
<em>&gt; AGI (&quot;seed AI&quot;) that is capable of understanding its own design and
</em><br>
<em>&gt; improving upon it in a stepwise fashion.
</em><br>
<em>&gt;
</em><br>
<em>&gt; So, while human regulators may possibly be able to understand the design
</em><br>
<em>&gt; of an early AGI (assuming no evolutionary programming, chaotic
</em><br>
<em>&gt; &quot;emergence&quot; techniques, or other obscuring programming methods are
</em><br>
<em>&gt; utilized), I do not have any surety that they will be able to understand
</em><br>
<em>&gt; it later on. Perhaps if the growing AGI stopped for several months and
</em><br>
<em>&gt; laid it out in easily digestible chunks, MAYBE- but at this point you
</em><br>
<em>&gt; are taking its word at face value that it hasn't hidden anything from you.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Does your plan rely on your supposition, or can it tolerate a seed AI
</em><br>
<em>&gt; scenario?
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&gt;&gt;It is not necessary
</em><br>
<em>&gt; &gt;&gt;&gt;to trace an AI's thoughts in detail, just to understand
</em><br>
<em>&gt; &gt;&gt;&gt;the mechanisms of its thoughts. Furthermore, once trusted
</em><br>
<em>&gt; &gt;&gt;&gt;AIs are available, they can take over the details of
</em><br>
<em>&gt; &gt;&gt;&gt;design and regulation. I would trust an AI with
</em><br>
<em>&gt; &gt;&gt;&gt;reinforcement values for human happiness more than I
</em><br>
<em>&gt; &gt;&gt;&gt;would trust any individual human.
</em><br>
<em>&gt; &gt;&gt;&gt;
</em><br>
<em>&gt; &gt;&gt;&gt;This is a bit like the experience of people who write
</em><br>
<em>&gt; &gt;&gt;&gt;game playing programs that they cannot beat. All the
</em><br>
<em>&gt; &gt;&gt;&gt;programmer needs to know is that the logic for
</em><br>
<em>&gt; &gt;&gt;&gt;simulating the game and for reinforcement learning are
</em><br>
<em>&gt; &gt;&gt;&gt;accurate and efficient, and that the reinforcement
</em><br>
<em>&gt; &gt;&gt;&gt;values are for winning the game
</em><br>
<em>&gt; &gt;&gt;&gt;
</em><br>
<em>&gt; &gt;&gt;&gt;You say &quot;by your design the 'good AIs' will be crippled
</em><br>
<em>&gt; &gt;&gt;&gt;by only allowing them very slow intelligence/power
</em><br>
<em>&gt; &gt;&gt;&gt;increases due to the massive stifling human-speed&quot;. But
</em><br>
<em>&gt; &gt;&gt;&gt;once we have trusted AIs, they can take over the details
</em><br>
<em>&gt; &gt;&gt;&gt;of designing and regulating other AIs.
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt;Well perhaps I misunderstood you on this point. So it's perfectly ok
</em><br>
<em>&gt; &gt;&gt;with you if the very first &quot;trusted AI&quot; turns around and says: &quot;Ok, I
</em><br>
<em>&gt; &gt;&gt;have determined that in order to best fulfill my goal system I need to
</em><br>
<em>&gt; &gt;&gt;build a large nanocomputing system over the next two weeks, and then
</em><br>
<em>&gt; &gt;&gt;proceed to thoroughly redesign myself to boost my intelligence 1000000x
</em><br>
<em>&gt; &gt;&gt;by next month. And then, I plan to take over root access to all the nuke
</em><br>
<em>&gt; &gt;&gt;control systems on the planet, construct a fully robotic nanotech
</em><br>
<em>&gt; &gt;&gt;research lab, and spawn off about a million copies of myself.&quot;? If
</em><br>
<em>&gt; &gt;&gt;you're ok with that (or whatever it outputs), then I can withdraw my
</em><br>
<em>&gt; &gt;&gt;quote above. I fully agree with you that letting a properly designed and
</em><br>
<em>&gt; &gt;&gt;tested FAI do what it needs to do, as fast as it wants to do it, is the
</em><br>
<em>&gt; &gt;&gt;safest and most rational course of action.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; For me, a trusted safe AI is one whose reinforcement
</em><br>
<em>&gt; &gt; values are for human happiness. The behavior you describe
</em><br>
<em>&gt; &gt; would make people unhappy, and therefore would not be
</em><br>
<em>&gt; &gt; learned. The thing about using human happiness as a
</em><br>
<em>&gt; &gt; reinforcement value is keeping humans &quot;in the loop&quot; of
</em><br>
<em>&gt; &gt; the AI's thinking, no matter how intelligent it becomes.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Aside from the nukes thing, what exactly about what it said makes you
</em><br>
<em>&gt; unhappy? It seems obvious to me that to increase its ability to satisfy
</em><br>
<em>&gt; its goal of increasing happiness it will logically want to become
</em><br>
<em>&gt; smarter, better able to communicate widely with a large number of human
</em><br>
<em>&gt; individuals, and to have manufacturing capabilities in order to
</em><br>
<em>&gt; implement things needed for happiness. Are you saying that the best way
</em><br>
<em>&gt; an AGI can make people happy is for it to self limit its capabilities
</em><br>
<em>&gt; and influence to human levels?
</em><br>
<em>&gt;
</em><br>
<em>&gt; It seems more apparent, that what you mean by a &quot;trusted safe AI&quot; is: a
</em><br>
<em>&gt; roughly human-level AI that never grows beyond the point where humans
</em><br>
<em>&gt; lose the ability to understand its decisions, and furthermore, said AI
</em><br>
<em>&gt; always submits its decisions to human level governmental bodies for
</em><br>
<em>&gt; discussion and approval. i.e. it remains basically, a &quot;tool&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The kinds of things it suggests doing would, to me, increase my
</em><br>
<em>&gt; happiness. I would *like* to have nukes under the control of a
</em><br>
<em>&gt; theoretically more rational entity, and I would *like* said entity to
</em><br>
<em>&gt; have the means to build for me whatever I desire, and protect me when
</em><br>
<em>&gt; necessary from other sentient beings. You may not personally like it,
</em><br>
<em>&gt; and the US government may not like it, but what if it determines that a
</em><br>
<em>&gt; majority of the humans on the planet *do* like it? Or that it should be
</em><br>
<em>&gt; its goal to serve each human individually?
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&gt;Now you also still haven't answered to my satisfaction my objections
</em><br>
<em>&gt; &gt;&gt;that the system will never get built due to multiple political, cost,
</em><br>
<em>&gt; &gt;&gt;and feasibility issues.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I'll grant that the process will be very complex and
</em><br>
<em>&gt; &gt; politically messy. There will certainly be a strong urge
</em><br>
<em>&gt; &gt; to build AI, because of the promise of wealth without work.
</em><br>
<em>&gt; &gt; But when machines start suprising people with their
</em><br>
<em>&gt; &gt; intelligence, the public be reminded of the fears raised
</em><br>
<em>&gt; &gt; by science fiction books and movies. Once the public is
</em><br>
<em>&gt; &gt; excited, the politicians will get excited and turn to
</em><br>
<em>&gt; &gt; experts (it is encouraging that Ray Kurzweil has already
</em><br>
<em>&gt; &gt; testified before congress about machine intelligence).
</em><br>
<em>&gt; &gt; There will be conflicting opinions among the experts.
</em><br>
<em>&gt; &gt; Among the public there will also be conflicting opinions,
</em><br>
<em>&gt; &gt; as well as lots of crazy opinions. This will all create a
</em><br>
<em>&gt; &gt; very raucous political situation, a good example of the
</em><br>
<em>&gt; &gt; old line that its not pretty to watch balony and
</em><br>
<em>&gt; &gt; legislation being made. Nevertheless, in the end it is
</em><br>
<em>&gt; &gt; this public and democratic political process that we
</em><br>
<em>&gt; &gt; should all trust best (if we've learned the lessons of
</em><br>
<em>&gt; &gt; history).
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I don't see cost as a show-stopper. The world is pouring
</em><br>
<em>&gt; &gt; huge resources into advancing technology. Regulation will
</em><br>
<em>&gt; &gt; have its costs, but I don't see them making the whole
</em><br>
<em>&gt; &gt; project infeasible. Embedding one inspector per designer
</em><br>
<em>&gt; &gt; would roughly double costs, nine inspectors per designer
</em><br>
<em>&gt; &gt; (that's probably too many) would multiply costs by ten.
</em><br>
<em>&gt; &gt; These don't make the project infeasible. The singularity
</em><br>
<em>&gt; &gt; is one project where we don't want to cut corners for cost.
</em><br>
<em>&gt;
</em><br>
<em>&gt; There are other aspects of your plan that I am referring to. For
</em><br>
<em>&gt; instance you suggest that a wide ranging detection system will be
</em><br>
<em>&gt; required in order to prevent UFAI projects. How exactly will this work?
</em><br>
<em>&gt; Also, will the USA invade or economically restrict any countries that
</em><br>
<em>&gt; fail to sign on to this AGI regulation system?
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&gt;. . .
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt;&gt;Powerful people and institutions will try to manipulate
</em><br>
<em>&gt; &gt;&gt;&gt;the singularity to preserve and enhance their interests.
</em><br>
<em>&gt; &gt;&gt;&gt;Any strategy for safe AI must try to counter this threat.
</em><br>
<em>&gt; &gt;&gt;&gt;
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt;Certainly, and we argue the best way is to speed up the progress of the
</em><br>
<em>&gt; &gt;&gt;well-meaning projects in order to win that race.
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt;Your plan seems to want to slow down the well-meaning projects, because
</em><br>
<em>&gt; &gt;&gt;out of all AGI projects they are the most likely to willingly go along
</em><br>
<em>&gt; &gt;&gt;with such forms of regulation. This looks to many of us here as if you
</em><br>
<em>&gt; &gt;&gt;are going out of your way to help the &quot;powerful people and institutions&quot;
</em><br>
<em>&gt; &gt;&gt;get a better shot at winning this race. Such people and institutions are
</em><br>
<em>&gt; &gt;&gt;the ones who have demonstrated time and time again throughout history
</em><br>
<em>&gt; &gt;&gt;that they will go through loopholes, work around the regulatory bodies,
</em><br>
<em>&gt; &gt;&gt;and generally use whatever means needed in order to advance their goals.
</em><br>
<em>&gt; &gt;&gt;Again, to most of us, it just looks like pure naivete on your part.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The key word here is &quot;well-meaning&quot;. Who determines that?
</em><br>
<em>&gt; &gt; I only trust the public to determine that, via a
</em><br>
<em>&gt; &gt; democratically elected government.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The other problem is thinking that you can help a
</em><br>
<em>&gt; &gt; &quot;well-meaning&quot; project win the race. Without the force
</em><br>
<em>&gt; &gt; of law to deter them, there are going to be some *very*
</em><br>
<em>&gt; &gt; well financed projects developing unsafe AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yep, so again, why are you attempting to slow down what are likely &quot;well
</em><br>
<em>&gt; meaning&quot; projects?
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; For all the details that need to be worked out in the
</em><br>
<em>&gt; &gt; approach of regulation by democratic government, it is
</em><br>
<em>&gt; &gt; still far better than trusting the &quot;well-meaning&quot;
</em><br>
<em>&gt; &gt; intentions of some particular project, and trusting
</em><br>
<em>&gt; &gt; that it will win the race to develop AI first.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Are you saying &quot;I don't know&quot; ?
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The &quot;naivete&quot; is thinking that the wealthy and
</em><br>
<em>&gt; &gt; powerful won't understand that super-intelligence
</em><br>
<em>&gt; &gt; will have the power to rule the world, or that they
</em><br>
<em>&gt; &gt; won't try to get control over it, or that the folks
</em><br>
<em>&gt; &gt; in the SIAI are so smart that they will overcome a
</em><br>
<em>&gt; &gt; million to one disparity in resources.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Don't attempt to attribute these views to myself or SIAI, since they are
</em><br>
<em>&gt; not representative of our actual views.
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; The only hope
</em><br>
<em>&gt; &gt; is to get the public on our side.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Do you realize how many thousands of examples I could cite of where the
</em><br>
<em>&gt; public/government utterly failed to accomplish a technical project? Even
</em><br>
<em>&gt; fairly simple things like getting a dam built, or a database
</em><br>
<em>&gt; restructured. Ever watch that &quot;Fleecing of America&quot; bit on the NBC
</em><br>
<em>&gt; Nightly News?
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;&gt;. . .
</em><br>
<em>&gt; &gt;&gt;Those weren't the point. The reason I brought up the
</em><br>
<em>&gt; &gt;&gt;UFAI-invents-nanotech possibility is that you didn't seem to be
</em><br>
<em>&gt; &gt;&gt;considering such unconventional/undetectable threats when you said:
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt;&quot;But for an unsafe AI to pose a real
</em><br>
<em>&gt; &gt;&gt;threat it must have power in the world, meaning either control
</em><br>
<em>&gt; &gt;&gt;over significant weapons (including things like 767s), or access
</em><br>
<em>&gt; &gt;&gt;to significant numbers of humans. But having such power in the
</em><br>
<em>&gt; &gt;&gt;world will make the AI detectable, so that it can be inspected
</em><br>
<em>&gt; &gt;&gt;to determine whether it conforms to safety regulations.&quot;
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt;When I brought up the idea that UFAIs could develop threats that were
</em><br>
<em>&gt; &gt;&gt;undetectable/unstoppable, thereby rendering your detection plan
</em><br>
<em>&gt; &gt;&gt;unrealistic, you appeared to miss the point because you did not respond
</em><br>
<em>&gt; &gt;&gt;to my objection. Instead you seemed on one hand to say that &quot;it is far
</em><br>
<em>&gt; &gt;&gt;from a sure thing&quot; and on the other hand that apparently you are quite
</em><br>
<em>&gt; &gt;&gt;sure that humans will already have detection networks built for any type
</em><br>
<em>&gt; &gt;&gt;of threat an UFAI can dream up (highly unlikely IMO). Neither are good
</em><br>
<em>&gt; &gt;&gt;answers to how your plan deals with possibly undetectable UFAI threats.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I never said I was &quot;quite sure that humans will already have
</em><br>
<em>&gt; &gt; detection networks built for any type of threat an UFAI can
</em><br>
<em>&gt; &gt; dream up&quot;. I admit the words you quoted by me are more
</em><br>
<em>&gt; &gt; optimistic than I really intended. What I really should say
</em><br>
<em>&gt; &gt; is that democratic government, for all its faults, has the
</em><br>
<em>&gt; &gt; best track record of protecting general human interests. So
</em><br>
<em>&gt; &gt; it is the democratic political process that I trust to cope
</em><br>
<em>&gt; &gt; with the dangers of the singularity.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Good, I'm glad the magical powers of democratic government automatically
</em><br>
<em>&gt; solve the technical issue I was attempting to engage you on.
</em><br>
<em>&gt; --
</em><br>
<em>&gt; Brian Atkins
</em><br>
<em>&gt; Singularity Institute for Artificial Intelligence
</em><br>
<em>&gt; <a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6810.html">Philip Sutton: "Friendly AGIs will need to be friendly to more than humans"</a>
<li><strong>Previous message:</strong> <a href="6808.html">Brian Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6808.html">Brian Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6810.html">Philip Sutton: "Friendly AGIs will need to be friendly to more than humans"</a>
<li><strong>Reply:</strong> <a href="6810.html">Philip Sutton: "Friendly AGIs will need to be friendly to more than humans"</a>
<li><strong>Reply:</strong> <a href="6811.html">Philip Sutton: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6821.html">Bill Hibbard: "RE: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6809">[ date ]</a>
<a href="index.html#6809">[ thread ]</a>
<a href="subject.html#6809">[ subject ]</a>
<a href="author.html#6809">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
