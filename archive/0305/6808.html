<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Brian Atkins (brian@posthuman.com)">
<meta name="Subject" content="Re: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI's flawed friendliness analysis</h1>
<!-- received="Fri May 23 21:20:43 2003" -->
<!-- isoreceived="20030524032043" -->
<!-- sent="Fri, 23 May 2003 22:20:00 -0500" -->
<!-- isosent="20030524032000" -->
<!-- name="Brian Atkins" -->
<!-- email="brian@posthuman.com" -->
<!-- subject="Re: SIAI's flawed friendliness analysis" -->
<!-- id="3ECEE4E0.2000301@posthuman.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="Pine.GSO.4.44.0305231116170.969-100000@demedici.ssec.wisc.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brian Atkins (<a href="mailto:brian@posthuman.com?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis"><em>brian@posthuman.com</em></a>)<br>
<strong>Date:</strong> Fri May 23 2003 - 21:20:00 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6809.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6807.html">Mark Waser: "Fw: Failure of AI so far"</a>
<li><strong>In reply to:</strong> <a href="6797.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6809.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6809.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6808">[ date ]</a>
<a href="index.html#6808">[ thread ]</a>
<a href="subject.html#6808">[ subject ]</a>
<a href="author.html#6808">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Bill Hibbard wrote:
<br>
<em>&gt; On Tue, 20 May 2003, Brian Atkins wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;. . .
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;If humans can design AIs smarter than humans, then humans
</em><br>
<em>&gt;&gt;&gt;can regulate AIs smarter than humans.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Just because a human can design some seed AI code that grows into a SI
</em><br>
<em>&gt;&gt;does not imply that humans or human-level AIs can successfully
</em><br>
<em>&gt;&gt;&quot;regulate&quot; grown SIs.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The regulation is not intended to trace the thoughts
</em><br>
<em>&gt; and development of the SI. The inspection is of the
</em><br>
<em>&gt; design, not the changing contents of its mind. If it's
</em><br>
<em>&gt; initial reinforcement values are for human happiness,
</em><br>
<em>&gt; and its simulation and reinforcement learning
</em><br>
<em>&gt; algorithms are accurate, then we can trust the way it
</em><br>
<em>&gt; will develop. In an earlier email I made the analogy
</em><br>
<em>&gt; to game playing programs. If their game simulation
</em><br>
<em>&gt; and learning algorithms are accurate and efficient,
</em><br>
<em>&gt; and their reinforcement learning values are for winning
</em><br>
<em>&gt; the game, then although the details of their play are
</em><br>
<em>&gt; not predictable, the fact that they will play to win
</em><br>
<em>&gt; is predictable.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The first SI will be designed and educated by humans.
</em><br>
<em>&gt; Humans will be able to understand and regulate its
</em><br>
<em>&gt; design, and regulate how it is educated. This will
</em><br>
<em>&gt; create trusted safe SIs. They can then design and
</em><br>
<em>&gt; regulate improved SIs, with one independently
</em><br>
<em>&gt; designed SI inspecting the designs of another.
</em><br>
<em>&gt; 
</em><br>
<p>You have a differing view of the development process of AGI from myself 
<br>
(and I would guess most people here). I do not believe it is that likely 
<br>
that SI will arrive, in its full design, from humans. I find it much 
<br>
more likely that it will come from a lengthy (in terms of iterations) 
<br>
continuing redesign process undertaken by an approximately human level 
<br>
AGI (&quot;seed AI&quot;) that is capable of understanding its own design and 
<br>
improving upon it in a stepwise fashion.
<br>
<p>So, while human regulators may possibly be able to understand the design 
<br>
of an early AGI (assuming no evolutionary programming, chaotic 
<br>
&quot;emergence&quot; techniques, or other obscuring programming methods are 
<br>
utilized), I do not have any surety that they will be able to understand 
<br>
it later on. Perhaps if the growing AGI stopped for several months and 
<br>
laid it out in easily digestible chunks, MAYBE- but at this point you 
<br>
are taking its word at face value that it hasn't hidden anything from you.
<br>
<p>Does your plan rely on your supposition, or can it tolerate a seed AI 
<br>
scenario?
<br>
<p><em>&gt; 
</em><br>
<em>&gt;&gt;&gt;It is not necessary
</em><br>
<em>&gt;&gt;&gt;to trace an AI's thoughts in detail, just to understand
</em><br>
<em>&gt;&gt;&gt;the mechanisms of its thoughts. Furthermore, once trusted
</em><br>
<em>&gt;&gt;&gt;AIs are available, they can take over the details of
</em><br>
<em>&gt;&gt;&gt;design and regulation. I would trust an AI with
</em><br>
<em>&gt;&gt;&gt;reinforcement values for human happiness more than I
</em><br>
<em>&gt;&gt;&gt;would trust any individual human.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;This is a bit like the experience of people who write
</em><br>
<em>&gt;&gt;&gt;game playing programs that they cannot beat. All the
</em><br>
<em>&gt;&gt;&gt;programmer needs to know is that the logic for
</em><br>
<em>&gt;&gt;&gt;simulating the game and for reinforcement learning are
</em><br>
<em>&gt;&gt;&gt;accurate and efficient, and that the reinforcement
</em><br>
<em>&gt;&gt;&gt;values are for winning the game
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;You say &quot;by your design the 'good AIs' will be crippled
</em><br>
<em>&gt;&gt;&gt;by only allowing them very slow intelligence/power
</em><br>
<em>&gt;&gt;&gt;increases due to the massive stifling human-speed&quot;. But
</em><br>
<em>&gt;&gt;&gt;once we have trusted AIs, they can take over the details
</em><br>
<em>&gt;&gt;&gt;of designing and regulating other AIs.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Well perhaps I misunderstood you on this point. So it's perfectly ok
</em><br>
<em>&gt;&gt;with you if the very first &quot;trusted AI&quot; turns around and says: &quot;Ok, I
</em><br>
<em>&gt;&gt;have determined that in order to best fulfill my goal system I need to
</em><br>
<em>&gt;&gt;build a large nanocomputing system over the next two weeks, and then
</em><br>
<em>&gt;&gt;proceed to thoroughly redesign myself to boost my intelligence 1000000x
</em><br>
<em>&gt;&gt;by next month. And then, I plan to take over root access to all the nuke
</em><br>
<em>&gt;&gt;control systems on the planet, construct a fully robotic nanotech
</em><br>
<em>&gt;&gt;research lab, and spawn off about a million copies of myself.&quot;? If
</em><br>
<em>&gt;&gt;you're ok with that (or whatever it outputs), then I can withdraw my
</em><br>
<em>&gt;&gt;quote above. I fully agree with you that letting a properly designed and
</em><br>
<em>&gt;&gt;tested FAI do what it needs to do, as fast as it wants to do it, is the
</em><br>
<em>&gt;&gt;safest and most rational course of action.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; For me, a trusted safe AI is one whose reinforcement
</em><br>
<em>&gt; values are for human happiness. The behavior you describe
</em><br>
<em>&gt; would make people unhappy, and therefore would not be
</em><br>
<em>&gt; learned. The thing about using human happiness as a
</em><br>
<em>&gt; reinforcement value is keeping humans &quot;in the loop&quot; of
</em><br>
<em>&gt; the AI's thinking, no matter how intelligent it becomes.
</em><br>
<p>Aside from the nukes thing, what exactly about what it said makes you 
<br>
unhappy? It seems obvious to me that to increase its ability to satisfy 
<br>
its goal of increasing happiness it will logically want to become 
<br>
smarter, better able to communicate widely with a large number of human 
<br>
individuals, and to have manufacturing capabilities in order to 
<br>
implement things needed for happiness. Are you saying that the best way 
<br>
an AGI can make people happy is for it to self limit its capabilities 
<br>
and influence to human levels?
<br>
<p>It seems more apparent, that what you mean by a &quot;trusted safe AI&quot; is: a 
<br>
roughly human-level AI that never grows beyond the point where humans 
<br>
lose the ability to understand its decisions, and furthermore, said AI 
<br>
always submits its decisions to human level governmental bodies for 
<br>
discussion and approval. i.e. it remains basically, a &quot;tool&quot;.
<br>
<p>The kinds of things it suggests doing would, to me, increase my 
<br>
happiness. I would *like* to have nukes under the control of a 
<br>
theoretically more rational entity, and I would *like* said entity to 
<br>
have the means to build for me whatever I desire, and protect me when 
<br>
necessary from other sentient beings. You may not personally like it, 
<br>
and the US government may not like it, but what if it determines that a 
<br>
majority of the humans on the planet *do* like it? Or that it should be 
<br>
its goal to serve each human individually?
<br>
<p><em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;Now you also still haven't answered to my satisfaction my objections
</em><br>
<em>&gt;&gt;that the system will never get built due to multiple political, cost,
</em><br>
<em>&gt;&gt;and feasibility issues.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I'll grant that the process will be very complex and
</em><br>
<em>&gt; politically messy. There will certainly be a strong urge
</em><br>
<em>&gt; to build AI, because of the promise of wealth without work.
</em><br>
<em>&gt; But when machines start suprising people with their
</em><br>
<em>&gt; intelligence, the public be reminded of the fears raised
</em><br>
<em>&gt; by science fiction books and movies. Once the public is
</em><br>
<em>&gt; excited, the politicians will get excited and turn to
</em><br>
<em>&gt; experts (it is encouraging that Ray Kurzweil has already
</em><br>
<em>&gt; testified before congress about machine intelligence).
</em><br>
<em>&gt; There will be conflicting opinions among the experts.
</em><br>
<em>&gt; Among the public there will also be conflicting opinions,
</em><br>
<em>&gt; as well as lots of crazy opinions. This will all create a
</em><br>
<em>&gt; very raucous political situation, a good example of the
</em><br>
<em>&gt; old line that its not pretty to watch balony and
</em><br>
<em>&gt; legislation being made. Nevertheless, in the end it is
</em><br>
<em>&gt; this public and democratic political process that we
</em><br>
<em>&gt; should all trust best (if we've learned the lessons of
</em><br>
<em>&gt; history).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I don't see cost as a show-stopper. The world is pouring
</em><br>
<em>&gt; huge resources into advancing technology. Regulation will
</em><br>
<em>&gt; have its costs, but I don't see them making the whole
</em><br>
<em>&gt; project infeasible. Embedding one inspector per designer
</em><br>
<em>&gt; would roughly double costs, nine inspectors per designer
</em><br>
<em>&gt; (that's probably too many) would multiply costs by ten.
</em><br>
<em>&gt; These don't make the project infeasible. The singularity
</em><br>
<em>&gt; is one project where we don't want to cut corners for cost.
</em><br>
<p>There are other aspects of your plan that I am referring to. For 
<br>
instance you suggest that a wide ranging detection system will be 
<br>
required in order to prevent UFAI projects. How exactly will this work?
<br>
Also, will the USA invade or economically restrict any countries that 
<br>
fail to sign on to this AGI regulation system?
<br>
<p><em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;. . .
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;Powerful people and institutions will try to manipulate
</em><br>
<em>&gt;&gt;&gt;the singularity to preserve and enhance their interests.
</em><br>
<em>&gt;&gt;&gt;Any strategy for safe AI must try to counter this threat.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Certainly, and we argue the best way is to speed up the progress of the
</em><br>
<em>&gt;&gt;well-meaning projects in order to win that race.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Your plan seems to want to slow down the well-meaning projects, because
</em><br>
<em>&gt;&gt;out of all AGI projects they are the most likely to willingly go along
</em><br>
<em>&gt;&gt;with such forms of regulation. This looks to many of us here as if you
</em><br>
<em>&gt;&gt;are going out of your way to help the &quot;powerful people and institutions&quot;
</em><br>
<em>&gt;&gt;get a better shot at winning this race. Such people and institutions are
</em><br>
<em>&gt;&gt;the ones who have demonstrated time and time again throughout history
</em><br>
<em>&gt;&gt;that they will go through loopholes, work around the regulatory bodies,
</em><br>
<em>&gt;&gt;and generally use whatever means needed in order to advance their goals.
</em><br>
<em>&gt;&gt;Again, to most of us, it just looks like pure naivete on your part.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The key word here is &quot;well-meaning&quot;. Who determines that?
</em><br>
<em>&gt; I only trust the public to determine that, via a
</em><br>
<em>&gt; democratically elected government.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The other problem is thinking that you can help a
</em><br>
<em>&gt; &quot;well-meaning&quot; project win the race. Without the force
</em><br>
<em>&gt; of law to deter them, there are going to be some *very*
</em><br>
<em>&gt; well financed projects developing unsafe AI.
</em><br>
<p>Yep, so again, why are you attempting to slow down what are likely &quot;well 
<br>
meaning&quot; projects?
<br>
<p><em>&gt; 
</em><br>
<em>&gt; For all the details that need to be worked out in the
</em><br>
<em>&gt; approach of regulation by democratic government, it is
</em><br>
<em>&gt; still far better than trusting the &quot;well-meaning&quot;
</em><br>
<em>&gt; intentions of some particular project, and trusting
</em><br>
<em>&gt; that it will win the race to develop AI first.
</em><br>
<p>Are you saying &quot;I don't know&quot; ?
<br>
<p><em>&gt; 
</em><br>
<em>&gt; The &quot;naivete&quot; is thinking that the wealthy and
</em><br>
<em>&gt; powerful won't understand that super-intelligence
</em><br>
<em>&gt; will have the power to rule the world, or that they
</em><br>
<em>&gt; won't try to get control over it, or that the folks
</em><br>
<em>&gt; in the SIAI are so smart that they will overcome a
</em><br>
<em>&gt; million to one disparity in resources.
</em><br>
<p>Don't attempt to attribute these views to myself or SIAI, since they are 
<br>
not representative of our actual views.
<br>
<p><em>&gt; The only hope
</em><br>
<em>&gt; is to get the public on our side.
</em><br>
<p>Do you realize how many thousands of examples I could cite of where the 
<br>
public/government utterly failed to accomplish a technical project? Even 
<br>
fairly simple things like getting a dam built, or a database 
<br>
restructured. Ever watch that &quot;Fleecing of America&quot; bit on the NBC 
<br>
Nightly News?
<br>
<p><em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;. . .
</em><br>
<em>&gt;&gt;Those weren't the point. The reason I brought up the
</em><br>
<em>&gt;&gt;UFAI-invents-nanotech possibility is that you didn't seem to be
</em><br>
<em>&gt;&gt;considering such unconventional/undetectable threats when you said:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&quot;But for an unsafe AI to pose a real
</em><br>
<em>&gt;&gt;threat it must have power in the world, meaning either control
</em><br>
<em>&gt;&gt;over significant weapons (including things like 767s), or access
</em><br>
<em>&gt;&gt;to significant numbers of humans. But having such power in the
</em><br>
<em>&gt;&gt;world will make the AI detectable, so that it can be inspected
</em><br>
<em>&gt;&gt;to determine whether it conforms to safety regulations.&quot;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;When I brought up the idea that UFAIs could develop threats that were
</em><br>
<em>&gt;&gt;undetectable/unstoppable, thereby rendering your detection plan
</em><br>
<em>&gt;&gt;unrealistic, you appeared to miss the point because you did not respond
</em><br>
<em>&gt;&gt;to my objection. Instead you seemed on one hand to say that &quot;it is far
</em><br>
<em>&gt;&gt;from a sure thing&quot; and on the other hand that apparently you are quite
</em><br>
<em>&gt;&gt;sure that humans will already have detection networks built for any type
</em><br>
<em>&gt;&gt;of threat an UFAI can dream up (highly unlikely IMO). Neither are good
</em><br>
<em>&gt;&gt;answers to how your plan deals with possibly undetectable UFAI threats.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I never said I was &quot;quite sure that humans will already have
</em><br>
<em>&gt; detection networks built for any type of threat an UFAI can
</em><br>
<em>&gt; dream up&quot;. I admit the words you quoted by me are more
</em><br>
<em>&gt; optimistic than I really intended. What I really should say
</em><br>
<em>&gt; is that democratic government, for all its faults, has the
</em><br>
<em>&gt; best track record of protecting general human interests. So
</em><br>
<em>&gt; it is the democratic political process that I trust to cope
</em><br>
<em>&gt; with the dangers of the singularity.
</em><br>
<p>Good, I'm glad the magical powers of democratic government automatically 
<br>
solve the technical issue I was attempting to engage you on.
<br>
<pre>
-- 
Brian Atkins
Singularity Institute for Artificial Intelligence
<a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6809.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6807.html">Mark Waser: "Fw: Failure of AI so far"</a>
<li><strong>In reply to:</strong> <a href="6797.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6809.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6809.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6808">[ date ]</a>
<a href="index.html#6808">[ thread ]</a>
<a href="subject.html#6808">[ subject ]</a>
<a href="author.html#6808">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
