<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Durant Schoon (durant@ilm.com)">
<meta name="Subject" content="Re: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI's flawed friendliness analysis</h1>
<!-- received="Wed May 14 18:27:03 2003" -->
<!-- isoreceived="20030515002703" -->
<!-- sent="Wed, 14 May 2003 17:18:39 -0700" -->
<!-- isosent="20030515001839" -->
<!-- name="Durant Schoon" -->
<!-- email="durant@ilm.com" -->
<!-- subject="Re: SIAI's flawed friendliness analysis" -->
<!-- id="durant-1030514171839.A0730722@below.lucasdigital.com" -->
<!-- inreplyto="Pine.GSO.4.44.0305111811040.2416-100000@demedici.ssec.wisc.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Durant Schoon (<a href="mailto:durant@ilm.com?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis"><em>durant@ilm.com</em></a>)<br>
<strong>Date:</strong> Wed May 14 2003 - 18:18:39 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6696.html">Durant Schoon: "Re: Foresight Recon?"</a>
<li><strong>Previous message:</strong> <a href="6694.html">Samantha: "Re: Understanding morality (was: SIAI's flawed friendliness analysis)"</a>
<li><strong>In reply to:</strong> <a href="6681.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6707.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6707.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6695">[ date ]</a>
<a href="index.html#6695">[ thread ]</a>
<a href="subject.html#6695">[ subject ]</a>
<a href="author.html#6695">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hi Bill,
<br>
<p>...
<br>
<p><em>&gt; The SIAI guidelines involve digging into the AI's
</em><br>
<em>&gt; reflective thought process and controlling the AI's
</em><br>
<em>&gt; thoughts, in order to ensure safety. My book says the
</em><br>
<em>&gt; only concern for AI learning and reasoning is to ensure
</em><br>
<em>&gt; they are accurate, and that the teachers of young AIs
</em><br>
<em>&gt; be well-adjusted people (subject to public monitoring
</em><br>
<em>&gt; and the same kind of screening used for people who
</em><br>
<em>&gt; control major weapons). Beyond that, the proper domain
</em><br>
<em>&gt; for ensuring AI safety is the AI's values rather than
</em><br>
<em>&gt; the AI's reflective thought processes.
</em><br>
<p>Two words: &quot;Value Hackers&quot;
<br>
<p>Let us try to understand why Eliezer chooses to focus on an &quot;AI's
<br>
reflective thought processes&quot; rather than on explicitly specifying an
<br>
&quot;AI's values&quot;. Let's look at it this way: if you could, wouldn't you
<br>
rather develop an AI which could reason about *why* the values are the
<br>
way they are instead of just having the values carved in stone by a
<br>
programmer. 
<br>
<p>This is *safer* for one very important reason:
<br>
<p>The values are less likely corruptible, since the AI that actually
<br>
understands the sources for these values can reconstruct them from
<br>
basic principles/information-about-humans-and-the-world-in-general. 
<br>
<p>The ability to be able to re-derive these values in the face of a
<br>
changing external environment and an interior mindscape-under-
<br>
development is in fact *paramount* to the preservation of Friendliness.
<br>
<p>As mentioned before, this all hinges on the the ability to create an
<br>
AI in the first place that can understand &quot;how and why values are
<br>
created&quot; as well as what humans are, what itself is, and what the
<br>
world around us is. Furthemore, we are instructed by this insight
<br>
as to what the design of an AI should look like.
<br>
<p>Remembering our goal is to build better brains, the whole notion of
<br>
the SeedAI bootstrap is to get a AI that builds a better AI. We must
<br>
then ask ourselves this question: Who should be in charge of
<br>
designating this new entity's values? The answer is &quot;the smartest most
<br>
capable thinker who is the most skilled in these areas&quot;. At some point
<br>
that thinker is the AI. From the get-go we want the AI to be competent
<br>
at this task. If we cannot come up with a way to ensure this, we
<br>
should not attempt to build a mind in the first place (this is
<br>
Eliezer's view. I happen to agree with this. Ben Goertzel &amp; Peter
<br>
Voss's opposing views have been noted previously as well(*)).
<br>
<p>In summary, we need to build the scaffolding of *deep* understanding
<br>
of values and their derivations into the design of AI if we are to
<br>
have a chance at all. The movie 2001 is already an example in popular
<br>
mind of what can happen when this is not done.
<br>
<p>We cannot think of everything in advance. We must build a mind that
<br>
does the &quot;right&quot; thing no matter what happens.
<br>
<p><pre>
---
Considering your suggestion that the AI *only* be concerned with
having accurate thoughts: I haven't read your book, so I don't know
your reasoning for this. I can imagine that it's an *easier* way to do
things. You don't have to worry about the hard problem of where values
come from, which ones are important and how to preserve the right ones
under self modification. Easier is not better, obviously, but I'm only
guessing here why you might hold &quot;ensuring-accuracy&quot; in higher esteem
than other goals of thinking (like preserving Friendliness). 
(*) This may be the familiar topic on this list of &quot;when&quot; to devote
your efforts to Friendliness, not &quot;if&quot;. This topic has already been
discussed exhaustively and I would say it comes down to how one
answers certain questions: &quot;How cautious do you want to be?&quot;, &quot;How
seriously do consider that a danger could arise quickly, without a
chance to correct problems on a human time scale of thinking/action&quot;.
&gt; In my second and third points I described the lack of
&gt; rigorous standards for certain terms in the SIAI
&gt; Guidelines and for initial AI values. Those rigorous
&gt; standards can only come from the AI's values. I think
&gt; that in your AI model you feel the need to control how
&gt; they are derived via the AI's reflective thought
&gt; process. This is the wrong domain for addressing AI
&gt; safety.
Just to reiterate, with SeedAI, the AI becomes the programmer, the
gatekeeper of modifications. We *want* the modifier of the AI's values
to be super intelligent, better than all humans at that task, to be
more trustworthy, to do the right thing better than any
best-intentioned human. Admiteddly, this is a tall order, an order
Eliezer is trying to fill. 
If you are worried about rigorous standards, perhaps Elizer's proposal
of Wisdom Tournaments would address your concern. Before the first
line of code is ever written, I'm expecting Eliezer to expound upon
these points in sufficient detail.
&gt; Clear and unambiguous initial values are elaborated
&gt; in the learning process, forming connections via the
&gt; AI's simulation model with many other values. Human
&gt; babies love their mothers based on simple values about
&gt; touch, warmth, milk, smiles and sounds (happy Mother's
&gt; Day). But as the baby's mind learns, those simple
&gt; values get connected to a rich set of values about the
&gt; mother, via a simulation model of the mother and
&gt; surroundings. This elaboration of simple values will
&gt; happen in any truly intelligent AI.
Why will this elaboration happen? In other words, if you have a
design, it should not only convince us that the elaboration will
occur, but that it will be done in the right way and for the right
reasons. Compromising any one of those could have disastrous effects
for everyone.
&gt; I think initial AI values should be for simple
&gt; measures of human happiness. As the AI develops these
&gt; will be elaborated into a model of long-term human
&gt; happiness, and connected to many derived values about
&gt; what makes humans happy generally and particularly.
&gt; The subtle point is that this links AI values with
&gt; human values, and enables AI values to evolve as human
&gt; values evolve. We do see a gradual evolution of human
&gt; values, and the singularity will accelerate it.
I think you have good intentions. I appreciate your concern for doing
the right thing and helping us all along on our individual goals to be
happy(**), but if the letter-of-the-law is upheld and there is
no-true-comprehension as to *why* the law is the way it is, we could
all end up invaded by nano-serotonin-reuptake-inhibiting-bliss-bots.
I think Eliezer takes this great idea you mention, of guiding the AI
to have human values and evolve with human values, one step
further. Not only does he propose that the AI have these human(e)
values, but he insists that the AI know *why* these values are good
ones, what good &quot;human&quot; values look like, and how to extrapolate them
properly (in the way that the smartest, most ethical human would),
should the need arise.
Additionally, we must consider the worst case, that we cannot control
rapid ascent when it occurs. In that scenario we want the the AI to be
ver own guide, maintaining and extending-as-necessary ver morality
under rapid, heavy mental expansion/reconfiguration. Should we reach
that point, the situation will be out of human hands. No regulatory
guidelines will be able to help us. Everything we know and cherish
could depend on preparing for that possible instant.
(**) Slight irony implied but full, sincere appreciation bestowed. I
consider this slightly ironic, since I view happiness as a signal that
confirms a goal was achieved rather than a goal, in-and-of-itself.
&gt; Morality has its roots in values, especially social
&gt; values for shared interests. Complex moral systems
&gt; are elaborations of such values via learning and
&gt; reasoning. The right place to control an AI's moral
&gt; system is in its values. All we can do for an AI's
&gt; learning and reasoning is make sure they are accurate
&gt; and efficient.
I'll agree that the values are critical linchpins as you suggest, but
please do not lose sight of the fact that these linchpins are part of
a greater machine with many interdependencies and exposure to an
external, possibly malevolent, world. 
The statement: &quot;All we can do for an AI's learning and reasoning is
make sure they are accurate and efficient&quot; seems limiting to me, in
the light Eliezer's writings. If we can construct a mind that will
solve this most difficult of problems (extreme intellectual ascent
while preserving Friendliness) for us and forever, then we should aim
for nothing less. Indeed, not hitting this mark is a danger that
people on this list take quite seriously.
Thank you for your interest and for joining the discussion.
--
Durant Schoon
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6696.html">Durant Schoon: "Re: Foresight Recon?"</a>
<li><strong>Previous message:</strong> <a href="6694.html">Samantha: "Re: Understanding morality (was: SIAI's flawed friendliness analysis)"</a>
<li><strong>In reply to:</strong> <a href="6681.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6707.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6707.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6695">[ date ]</a>
<a href="index.html#6695">[ thread ]</a>
<a href="subject.html#6695">[ subject ]</a>
<a href="author.html#6695">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
