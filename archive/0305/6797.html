<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Bill Hibbard (test@demedici.ssec.wisc.edu)">
<meta name="Subject" content="Re: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI's flawed friendliness analysis</h1>
<!-- received="Fri May 23 10:16:54 2003" -->
<!-- isoreceived="20030523161654" -->
<!-- sent="Fri, 23 May 2003 11:16:44 -0500 (CDT)" -->
<!-- isosent="20030523161644" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@demedici.ssec.wisc.edu" -->
<!-- subject="Re: SIAI's flawed friendliness analysis" -->
<!-- id="Pine.GSO.4.44.0305231116170.969-100000@demedici.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3ECAC893.7070100@posthuman.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis"><em>test@demedici.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Fri May 23 2003 - 10:16:44 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6798.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6796.html">Bill Hibbard: "Re: AGI Policy (was RE: SIAI's flawed friendliness analysis)"</a>
<li><strong>In reply to:</strong> <a href="6758.html">Brian Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6808.html">Brian Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6808.html">Brian Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6797">[ date ]</a>
<a href="index.html#6797">[ thread ]</a>
<a href="subject.html#6797">[ subject ]</a>
<a href="author.html#6797">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Tue, 20 May 2003, Brian Atkins wrote:
<br>
<p><em>&gt; . . .
</em><br>
<em>&gt; &gt; If humans can design AIs smarter than humans, then humans
</em><br>
<em>&gt; &gt; can regulate AIs smarter than humans.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Just because a human can design some seed AI code that grows into a SI
</em><br>
<em>&gt; does not imply that humans or human-level AIs can successfully
</em><br>
<em>&gt; &quot;regulate&quot; grown SIs.
</em><br>
<p>The regulation is not intended to trace the thoughts
<br>
and development of the SI. The inspection is of the
<br>
design, not the changing contents of its mind. If it's
<br>
initial reinforcement values are for human happiness,
<br>
and its simulation and reinforcement learning
<br>
algorithms are accurate, then we can trust the way it
<br>
will develop. In an earlier email I made the analogy
<br>
to game playing programs. If their game simulation
<br>
and learning algorithms are accurate and efficient,
<br>
and their reinforcement learning values are for winning
<br>
the game, then although the details of their play are
<br>
not predictable, the fact that they will play to win
<br>
is predictable.
<br>
<p>The first SI will be designed and educated by humans.
<br>
Humans will be able to understand and regulate its
<br>
design, and regulate how it is educated. This will
<br>
create trusted safe SIs. They can then design and
<br>
regulate improved SIs, with one independently
<br>
designed SI inspecting the designs of another.
<br>
<p><em>&gt; &gt; It is not necessary
</em><br>
<em>&gt; &gt; to trace an AI's thoughts in detail, just to understand
</em><br>
<em>&gt; &gt; the mechanisms of its thoughts. Furthermore, once trusted
</em><br>
<em>&gt; &gt; AIs are available, they can take over the details of
</em><br>
<em>&gt; &gt; design and regulation. I would trust an AI with
</em><br>
<em>&gt; &gt; reinforcement values for human happiness more than I
</em><br>
<em>&gt; &gt; would trust any individual human.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; This is a bit like the experience of people who write
</em><br>
<em>&gt; &gt; game playing programs that they cannot beat. All the
</em><br>
<em>&gt; &gt; programmer needs to know is that the logic for
</em><br>
<em>&gt; &gt; simulating the game and for reinforcement learning are
</em><br>
<em>&gt; &gt; accurate and efficient, and that the reinforcement
</em><br>
<em>&gt; &gt; values are for winning the game
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; You say &quot;by your design the 'good AIs' will be crippled
</em><br>
<em>&gt; &gt; by only allowing them very slow intelligence/power
</em><br>
<em>&gt; &gt; increases due to the massive stifling human-speed&quot;. But
</em><br>
<em>&gt; &gt; once we have trusted AIs, they can take over the details
</em><br>
<em>&gt; &gt; of designing and regulating other AIs.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Well perhaps I misunderstood you on this point. So it's perfectly ok
</em><br>
<em>&gt; with you if the very first &quot;trusted AI&quot; turns around and says: &quot;Ok, I
</em><br>
<em>&gt; have determined that in order to best fulfill my goal system I need to
</em><br>
<em>&gt; build a large nanocomputing system over the next two weeks, and then
</em><br>
<em>&gt; proceed to thoroughly redesign myself to boost my intelligence 1000000x
</em><br>
<em>&gt; by next month. And then, I plan to take over root access to all the nuke
</em><br>
<em>&gt; control systems on the planet, construct a fully robotic nanotech
</em><br>
<em>&gt; research lab, and spawn off about a million copies of myself.&quot;? If
</em><br>
<em>&gt; you're ok with that (or whatever it outputs), then I can withdraw my
</em><br>
<em>&gt; quote above. I fully agree with you that letting a properly designed and
</em><br>
<em>&gt; tested FAI do what it needs to do, as fast as it wants to do it, is the
</em><br>
<em>&gt; safest and most rational course of action.
</em><br>
<p>For me, a trusted safe AI is one whose reinforcement
<br>
values are for human happiness. The behavior you describe
<br>
would make people unhappy, and therefore would not be
<br>
learned. The thing about using human happiness as a
<br>
reinforcement value is keeping humans &quot;in the loop&quot; of
<br>
the AI's thinking, no matter how intelligent it becomes.
<br>
<p><em>&gt; Now you also still haven't answered to my satisfaction my objections
</em><br>
<em>&gt; that the system will never get built due to multiple political, cost,
</em><br>
<em>&gt; and feasibility issues.
</em><br>
<p>I'll grant that the process will be very complex and
<br>
politically messy. There will certainly be a strong urge
<br>
to build AI, because of the promise of wealth without work.
<br>
But when machines start suprising people with their
<br>
intelligence, the public be reminded of the fears raised
<br>
by science fiction books and movies. Once the public is
<br>
excited, the politicians will get excited and turn to
<br>
experts (it is encouraging that Ray Kurzweil has already
<br>
testified before congress about machine intelligence).
<br>
There will be conflicting opinions among the experts.
<br>
Among the public there will also be conflicting opinions,
<br>
as well as lots of crazy opinions. This will all create a
<br>
very raucous political situation, a good example of the
<br>
old line that its not pretty to watch balony and
<br>
legislation being made. Nevertheless, in the end it is
<br>
this public and democratic political process that we
<br>
should all trust best (if we've learned the lessons of
<br>
history).
<br>
<p>I don't see cost as a show-stopper. The world is pouring
<br>
huge resources into advancing technology. Regulation will
<br>
have its costs, but I don't see them making the whole
<br>
project infeasible. Embedding one inspector per designer
<br>
would roughly double costs, nine inspectors per designer
<br>
(that's probably too many) would multiply costs by ten.
<br>
These don't make the project infeasible. The singularity
<br>
is one project where we don't want to cut corners for cost.
<br>
<p><em>&gt; . . .
</em><br>
<em>&gt; &gt; Powerful people and institutions will try to manipulate
</em><br>
<em>&gt; &gt; the singularity to preserve and enhance their interests.
</em><br>
<em>&gt; &gt; Any strategy for safe AI must try to counter this threat.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Certainly, and we argue the best way is to speed up the progress of the
</em><br>
<em>&gt; well-meaning projects in order to win that race.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Your plan seems to want to slow down the well-meaning projects, because
</em><br>
<em>&gt; out of all AGI projects they are the most likely to willingly go along
</em><br>
<em>&gt; with such forms of regulation. This looks to many of us here as if you
</em><br>
<em>&gt; are going out of your way to help the &quot;powerful people and institutions&quot;
</em><br>
<em>&gt; get a better shot at winning this race. Such people and institutions are
</em><br>
<em>&gt; the ones who have demonstrated time and time again throughout history
</em><br>
<em>&gt; that they will go through loopholes, work around the regulatory bodies,
</em><br>
<em>&gt; and generally use whatever means needed in order to advance their goals.
</em><br>
<em>&gt; Again, to most of us, it just looks like pure naivete on your part.
</em><br>
<p>The key word here is &quot;well-meaning&quot;. Who determines that?
<br>
I only trust the public to determine that, via a
<br>
democratically elected government.
<br>
<p>The other problem is thinking that you can help a
<br>
&quot;well-meaning&quot; project win the race. Without the force
<br>
of law to deter them, there are going to be some *very*
<br>
well financed projects developing unsafe AI.
<br>
<p>For all the details that need to be worked out in the
<br>
approach of regulation by democratic government, it is
<br>
still far better than trusting the &quot;well-meaning&quot;
<br>
intentions of some particular project, and trusting
<br>
that it will win the race to develop AI first.
<br>
<p>The &quot;naivete&quot; is thinking that the wealthy and
<br>
powerful won't understand that super-intelligence
<br>
will have the power to rule the world, or that they
<br>
won't try to get control over it, or that the folks
<br>
in the SIAI are so smart that they will overcome a
<br>
million to one disparity in resources. The only hope
<br>
is to get the public on our side.
<br>
<p><em>&gt; . . .
</em><br>
<em>&gt; Those weren't the point. The reason I brought up the
</em><br>
<em>&gt; UFAI-invents-nanotech possibility is that you didn't seem to be
</em><br>
<em>&gt; considering such unconventional/undetectable threats when you said:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;But for an unsafe AI to pose a real
</em><br>
<em>&gt; threat it must have power in the world, meaning either control
</em><br>
<em>&gt; over significant weapons (including things like 767s), or access
</em><br>
<em>&gt; to significant numbers of humans. But having such power in the
</em><br>
<em>&gt; world will make the AI detectable, so that it can be inspected
</em><br>
<em>&gt; to determine whether it conforms to safety regulations.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; When I brought up the idea that UFAIs could develop threats that were
</em><br>
<em>&gt; undetectable/unstoppable, thereby rendering your detection plan
</em><br>
<em>&gt; unrealistic, you appeared to miss the point because you did not respond
</em><br>
<em>&gt; to my objection. Instead you seemed on one hand to say that &quot;it is far
</em><br>
<em>&gt; from a sure thing&quot; and on the other hand that apparently you are quite
</em><br>
<em>&gt; sure that humans will already have detection networks built for any type
</em><br>
<em>&gt; of threat an UFAI can dream up (highly unlikely IMO). Neither are good
</em><br>
<em>&gt; answers to how your plan deals with possibly undetectable UFAI threats.
</em><br>
<p>I never said I was &quot;quite sure that humans will already have
<br>
detection networks built for any type of threat an UFAI can
<br>
dream up&quot;. I admit the words you quoted by me are more
<br>
optimistic than I really intended. What I really should say
<br>
is that democratic government, for all its faults, has the
<br>
best track record of protecting general human interests. So
<br>
it is the democratic political process that I trust to cope
<br>
with the dangers of the singularity.
<br>
<p><em>&gt; &gt; The way to counter the threat of micro-organisms has been
</em><br>
<em>&gt; &gt; detection networks, isolation of affected people and
</em><br>
<em>&gt; &gt; regions, and urgent efforts to analyze the organisms and
</em><br>
<em>&gt; &gt; find counter measures. There are also efforts to monitor
</em><br>
<em>&gt; &gt; the humans with the knowledge to create new micro-organisms.
</em><br>
<em>&gt; &gt; These measures all have the force of law and the resources
</em><br>
<em>&gt; &gt; of government behind them. Similar measures will apply to
</em><br>
<em>&gt; &gt; the threat of nanotech. When safe AIs are available, they
</em><br>
<em>&gt; &gt; will certainly be enlisted to help. With such huge threats
</em><br>
<em>&gt; &gt; as nanotech the pipe dream is to think that they can be
</em><br>
<em>&gt; &gt; countered without the force of law and the resources of
</em><br>
<em>&gt; &gt; government. Or to think that government won't get involved.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Oh, I don't disagree that some form of &quot;government&quot; will be required, I
</em><br>
<em>&gt; just think it will be a post-Singularity form of governance that will
</em><br>
<em>&gt; have no relation to our current system.
</em><br>
<p>I agree that the singularity will cause a complete change
<br>
in form of government. I see AIs with values for human
<br>
happiness, that keep humans &quot;in the loop&quot; of their thought
<br>
processes, as the natural evolution of democracy.
<br>
<p>But there will still be traditional government as the
<br>
signularity starts to develop, and that will be the
<br>
critical time for determining what kind of singularity
<br>
we get.
<br>
<p><em>&gt; At any rate, I believe you will grant me my point that &quot;safe AIs&quot; can
</em><br>
<em>&gt; only defend us if they stay ahead of any possible UFAI's intelligence level.
</em><br>
<p>Sure. I see the force of law as a way to slow down those
<br>
who want to develop unsafe AIs, and the resources of
<br>
government as a way to help the development of safe AI.
<br>
<p>----------------------------------------------------------
<br>
Bill Hibbard, SSEC, 1225 W. Dayton St., Madison, WI  53706
<br>
<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis">test@demedici.ssec.wisc.edu</a>  608-263-4427  fax: 608-263-6738
<br>
<a href="http://www.ssec.wisc.edu/~billh/vis.html">http://www.ssec.wisc.edu/~billh/vis.html</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6798.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6796.html">Bill Hibbard: "Re: AGI Policy (was RE: SIAI's flawed friendliness analysis)"</a>
<li><strong>In reply to:</strong> <a href="6758.html">Brian Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6808.html">Brian Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6808.html">Brian Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6797">[ date ]</a>
<a href="index.html#6797">[ thread ]</a>
<a href="subject.html#6797">[ subject ]</a>
<a href="author.html#6797">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
