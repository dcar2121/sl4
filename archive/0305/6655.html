<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: SIAI's flawed friendliness analysis</h1>
<!-- received="Sat May 10 06:23:19 2003" -->
<!-- isoreceived="20030510122319" -->
<!-- sent="Sat, 10 May 2003 08:23:12 -0400" -->
<!-- isosent="20030510122312" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: SIAI's flawed friendliness analysis" -->
<!-- id="LAEGJLOGJIOELPNIOOAJMEOLEOAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3EBC7CA3.8000801@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sat May 10 2003 - 06:23:12 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6656.html">Mark Waser: "Understanding morality (was: SIAI's flawed friendliness analysis)"</a>
<li><strong>Previous message:</strong> <a href="6654.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6654.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6656.html">Mark Waser: "Understanding morality (was: SIAI's flawed friendliness analysis)"</a>
<li><strong>Reply:</strong> <a href="6656.html">Mark Waser: "Understanding morality (was: SIAI's flawed friendliness analysis)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6655">[ date ]</a>
<a href="index.html#6655">[ thread ]</a>
<a href="subject.html#6655">[ subject ]</a>
<a href="author.html#6655">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt; What is dangerous is that someone who believes that &quot;AIs just do what
</em><br>
<em>&gt; they're told&quot; will think that the big issue is who gets to tell AIs what
</em><br>
<em>&gt; to do.  Such people will not, of course, succeed in taking over
</em><br>
<em>&gt; the world;
</em><br>
<em>&gt; I find it extremely implausible that any human expressing such a goal has
</em><br>
<em>&gt; the depth of understanding required to build anything that is not a
</em><br>
<em>&gt; thermostat AI.  The problem is that these people might succeed in
</em><br>
<em>&gt; destroying the world, given enough computing power to brute-force AI with
</em><br>
<em>&gt; no real understanding of it.
</em><br>
<p>To me, this paragraph indicates a potentially dangerous naivete' about human
<br>
individual and group psychology.
<br>
<p>My concerns include:
<br>
<p>1)
<br>
There are many less extreme and more rational views in the conceptual
<br>
vicinity of &quot;AI's just do what they're told&quot; -- such as &quot;real AI's will be
<br>
autonomous yet heavily influencable, just like human beings.&quot;
<br>
<p>2)
<br>
Human nature is complex and contradictory.  Certainly, a deep understanding
<br>
of cognition and AI design could plausibly be compatible in the same human
<br>
with a shallowness on moral and futurist issues.
<br>
<p>3)
<br>
What about a group of people, where there's a team of scientists who
<br>
understand mind and AI but have decided to cede moral and futurist judgments
<br>
to their leaders, who do not understand mind and AI but do want to create an
<br>
AI that is under their influence (though not absolute control).
<br>
<p><p>I don't share your view that anyone with the the intuition and knowledge to
<br>
create AI, is necessarily likely to have strong moral standards and a wisdom
<br>
about morality.  I understand that creating a workable AI design is likely
<br>
to involve a deep understanding of the human mind [the Novamente design, for
<br>
example, certainly involved integration of information from neuroscience,
<br>
empirical psychology and introspective psychology, as well as math, CS, etc.
<br>
etc.], but ... well, so what?  Abstract understanding of mind structures and
<br>
dynamics does not imply moral wisdom, or the ability to make sound judgments
<br>
in very difficult situations.
<br>
<p><em>&gt; Anyone fighting over what values the AI ought to have is simply fighting
</em><br>
<em>&gt; over who gets to commit suicide.  If you know how to give an AI
</em><br>
<em>&gt; any set of
</em><br>
<em>&gt; values, you know how to give it a humanly representative set of values.
</em><br>
<p>This doesn't sound right to me either.  Maybe there is another non-human-ish
<br>
set of values that is easier to inculcate into an AI, than any set of
<br>
&quot;humanly representative&quot; values.  My intuition is that this IS in fact the
<br>
case.
<br>
<p><em>&gt; It is not a trivial thing, to create a mind that embodies the full human
</em><br>
<em>&gt; understanding of morality.
</em><br>
<p>I am not sure it is a desirable or necessary goal, either.  The human
<br>
understanding of morality has a lot of weaknesses, and a lot of ties to
<br>
confusing aspects of human biology.  I believe that ultimately an AI can be
<br>
*more* moral than nearly any human or human group.
<br>
<p><em>&gt; There is a high and beautiful sorcery to it.
</em><br>
<em>&gt; I find it hard to believe that any human truly capable of learning and
</em><br>
<em>&gt; understanding that art would use it to do something so small and mean.
</em><br>
<p>I am afraid you underestimate the complex, convoluted, and often
<br>
self-contradictory, perverse and self-defeating aspects of human nature --
<br>
qualities which occur in *very smart, insightful* people, along with many of
<br>
the others...
<br>
<p><em>&gt; Indeed the largest computers are the most dangerous, but not in the way
</em><br>
<em>&gt; that you mean.  They are dangerous because even people who don't
</em><br>
<em>&gt; understand what they're doing may be able to brute-force AI given truly
</em><br>
<em>&gt; insane amounts of computing power.  Friendliness, of course, cannot be
</em><br>
<em>&gt; brute-forced.
</em><br>
<p>I consider it reasonably possible that scientists without any deep
<br>
understanding of Ai morality, could produce a highly clever and functional
<br>
seed-AI system by methods far more sophisticated than &quot;brute force.&quot;
<br>
<p>Of course, I hope to beat them to it with Novamente ;-)
<br>
<p><em>&gt; Your political recommendations appear to be based on an extremely
</em><br>
<em>&gt; different model of AI.  Specifically:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1)  &quot;AIs&quot; are just very powerful tools that amplify the short-term goals
</em><br>
<em>&gt; of their users, like any other technology.
</em><br>
<p>AGIs may start out this way, and then grow into being autonomous beings and
<br>
seed AIs
<br>
<p><em>&gt; 2)  AIs have power proportional to the computing resources invested in
</em><br>
<em>&gt; them, and everyone has access to pretty much the same theoretical model
</em><br>
<em>&gt; and class of AI.
</em><br>
<p>Of course, this is not true.  But depending on how the future of AI
<br>
development goes, it may be roughly true at some point in the future.
<br>
<p>If someone figures out a new, workable theory of AGI, and it ends up taking
<br>
5-10 years to refine this theory into a viable seed AI, then it's likely
<br>
that within this 5-10 year period, said workable AI theory will propagate
<br>
through the scientific community, so that computing power will play a major
<br>
role (though far from being the only factor) in the relative intelligence
<br>
levels of in-development AI's using said workable theory.
<br>
<p><em>&gt; 3)  There is no seed AI, no rapid recursive self-improvement, no hard
</em><br>
<em>&gt; takeoff, no &quot;first&quot; AI.  AIs are just new forces in existing society,
</em><br>
<em>&gt; coming into play a bit at a time, as everyone's AI technology improves at
</em><br>
<em>&gt; roughly the same rate.
</em><br>
<p>I don't think Bill assumes that; I think his comments are sensible if one
<br>
assumes a soft takeoff that takes several years.  In the hard takeoff
<br>
scenario his comments are less relevant, I agree.
<br>
<p><em>&gt; 4)  Anyone can make an AI that does anything.  AI morality is an easy
</em><br>
<em>&gt; problem with fully specifiable arbitrary solutions that are reliable and
</em><br>
<em>&gt; humanly comprehensible.
</em><br>
<p>I don't think Bill said AI morality was an easy problem... I did not get
<br>
that impression from his book.
<br>
<p>Heck, even raising a human child effectively is not an easy problem !!!
<br>
<p><em>&gt; 5)  Government workers can look at an AI design and tell what the AI's
</em><br>
<em>&gt; morality does and whether it's safe.
</em><br>
<p>Maybe some government workers could do this about as well as anyone else.
<br>
There are a lot of mighty bright, knowledgeable people in gov't research
<br>
labs, for example.
<br>
<p><em>&gt; Hm.  I think all I can do here is point to Part III of LOGI and say that
</em><br>
<em>&gt; my concern is with FOOMgoing AIs (AIs that go FOOM, as in a hard
</em><br>
<em>&gt; takeoff).
</em><br>
<em>&gt;   Computer programs with major social effects, owned by powerful
</em><br>
<em>&gt; organizations, that are *not* capable of rapid recursive self-improvement
</em><br>
<em>&gt; and sparking a superintelligent transition, are not the kind of
</em><br>
<em>&gt; AI I worry
</em><br>
<em>&gt; about.
</em><br>
<p>But the risk is that one of these socially-pertinent non-seed-AI AI's will
<br>
then be grown into a seed AI through repeated iterative re-engineering...
<br>
<p><em>&gt;If there are governmentally understandable variables that
</em><br>
<em>&gt; correlate to democratically disputed social outcomes, and so on, then I
</em><br>
<em>&gt; might indeed write it off as ordinary politics.
</em><br>
<p>I don't understand your attitude toward &quot;governmental understandability.&quot;
<br>
The government has many scientists on its payroll who are as intelligent and
<br>
knowledgeable as anyone on this list.  I don't doubt that many employees of
<br>
national labs could contribute substantially to the design and evaluation of
<br>
seed AI's.  I have specific friends at Los Alamos Labs who would be
<br>
incredibly helpful in this role.
<br>
<p><em>&gt; The guidelines are not intended as a means of making AI programmers do
</em><br>
<em>&gt; something against their will.  I'll be astonished if I can get people to
</em><br>
<em>&gt; understand the method with their wholehearted cooperation and willingness
</em><br>
<em>&gt; to devote substantial amounts of time.  I see little or no hope
</em><br>
<em>&gt; for people
</em><br>
<em>&gt; who are vaguely interested and casually agreeable, unless they can be
</em><br>
<em>&gt; transformed into the former class.
</em><br>
<p>I believe that once there are impressive proto-seed-AI systems in existence,
<br>
a lot more people will become *very strongly interested* in this area....
<br>
The reason so many scientists are just vaguely interested in AI Friendliness
<br>
is that they reckon any kind of serious seed-AI is minimally decades away.
<br>
<p><em>&gt; Non-Bayesian?  I don't think you're going to find much backing on this
</em><br>
<em>&gt; one.  If you've really discovered a non-Bayesian form of reasoning, write
</em><br>
<em>&gt; it up and collect your everlasting fame.  Personally I consider such a
</em><br>
<em>&gt; thing almost exactly analogous to a perpetual motion machine.
</em><br>
<em>&gt; Except that
</em><br>
<em>&gt; a perpetual motion machine is merely physically impossible, while
</em><br>
<em>&gt; &quot;non-Bayesian reasoning&quot; appears to be mathematically impossible.  Though
</em><br>
<em>&gt; of course I could be wrong.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Reinforcement learning emerges from Bayesian reasoning, not the other way
</em><br>
<em>&gt; around.
</em><br>
<p>Well, this is a whole other topic!
<br>
<p>Bayesian modeling provides a way to understand cognitive systems.  But there
<br>
are also other, complementary ways to understand aspects of mental dynamics.
<br>
And, of course, the actual dynamic processes carrying out cognition -- even
<br>
if modelable using probability theory -- may be more easily describable
<br>
using some other formalism.
<br>
<p>In short, I think there can be &quot;non-Bayesian reasoning&quot; in the sense that
<br>
there can be reasoning whose most concise and useful explanation involves
<br>
concepts other than Bayes' rule and elementary probability theory in
<br>
general.  Probability theory can still be used to explain such reasoning,
<br>
but in an irritatingly cumbersome way.
<br>
<p>Novamente's cognition involves both explicitly probabilistic (Bayesian)
<br>
aspects, and other aspects.  The behavior of the
<br>
non-explicitly-probabilistic aspects can still be analyzed in terms of
<br>
probability theory, if one so wishes; but this is not necessarily the most
<br>
incisive analysis.
<br>
<p>I think that Bill's comments on reinforcement learning should be taken in
<br>
this light.  yeah, any RL scheme can be analyzed using probability theory,
<br>
and viewed as an approximate way to make some probabilistic calculations.
<br>
But for some RL schemes this is a really useful and productive perspective;
<br>
for others it isn't.
<br>
<p>-- Ben G
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6656.html">Mark Waser: "Understanding morality (was: SIAI's flawed friendliness analysis)"</a>
<li><strong>Previous message:</strong> <a href="6654.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6654.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6656.html">Mark Waser: "Understanding morality (was: SIAI's flawed friendliness analysis)"</a>
<li><strong>Reply:</strong> <a href="6656.html">Mark Waser: "Understanding morality (was: SIAI's flawed friendliness analysis)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6655">[ date ]</a>
<a href="index.html#6655">[ thread ]</a>
<a href="subject.html#6655">[ subject ]</a>
<a href="author.html#6655">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
