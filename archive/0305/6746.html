<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: No need to rush AGI development?</title>
<meta name="Author" content="Tommeteor@aol.com (Tommeteor@aol.com)">
<meta name="Subject" content="Re: No need to rush AGI development?">
<meta name="Date" content="2003-05-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: No need to rush AGI development?</h1>
<!-- received="Mon May 19 14:50:19 2003" -->
<!-- isoreceived="20030519205019" -->
<!-- sent="Mon, 19 May 2003 16:50:00 -0400" -->
<!-- isosent="20030519205000" -->
<!-- name="Tommeteor@aol.com" -->
<!-- email="Tommeteor@aol.com" -->
<!-- subject="Re: No need to rush AGI development?" -->
<!-- id="7E55767A.051477DC.02996B70@aol.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="No need to rush AGI development?" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> <a href="mailto:Tommeteor@aol.com?Subject=Re:%20No%20need%20to%20rush%20AGI%20development?"><em>Tommeteor@aol.com</em></a><br>
<strong>Date:</strong> Mon May 19 2003 - 14:50:00 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6747.html">Simon Gordon: "Re: AI Plurality"</a>
<li><strong>Previous message:</strong> <a href="6745.html">Robin Lee Powell: "Re: Security overkill"</a>
<li><strong>Maybe in reply to:</strong> <a href="6729.html">Philip Sutton: "Re: No need to rush AGI development?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6751.html">Bill Hibbard: "Re: No need to rush AGI development?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6746">[ date ]</a>
<a href="index.html#6746">[ thread ]</a>
<a href="subject.html#6746">[ subject ]</a>
<a href="author.html#6746">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I agree- humans cannot be trusted with sufficently advanced technology, and some alternative (BETTER alternative) to humans must be found if we are to handle such technology.
<br>
<p><em>&gt;Humans are certainly bright enough - with adequate &gt;education -it's what we apply our cleverness to or fail to &gt;apply it to that causes most of the problems.
</em><br>
<p>What do you mean, exactly, by &quot;education&quot;? No amount of &quot;education&quot; is going to chage our mental hardware. What, exactly, do you mean by &quot;cleverness&quot;? It seems to me that you are suggesting that what causes most of the problems is that we are failing to apply our &quot;cleverness&quot; - a very vague term -to the problem of making good decisions. In other words, we're not applying as much effort as is needed to make good decisions.  The problem is evolution and the way humans work. If the people who are making the decisions are not putting in enough effort to make GOOD decisions, then either we are hiring people who do a lackluster job, or the human brain doesn't have enough capacity to make decisions adequately on that level. No amount of education is going to fix the problem that, because of the inefficency of evolution, we only have a certain amount of effort, or as you called it, &quot;cleverness&quot;, that we can possibly apply. An analogy - you are basically saying that with enough body - building effort, we could push a<br>
&nbsp;100 - ton boulder up a hill, and the reason we can't without machines is because we fail to apply our full strength. Same limits - because of the way inefficent evolution designed us, we only have a certain amount of possible strength, and no amount of body - building is going to fix that. Same principle of enchancement - both body - building and education help to sharpen our skills, but they can't break evolution's limits. Same very difficult (impossible for ordinary humans) task - an impossibly heavy boulder and an impossibly hard set of decisions. Conclusion - ordinary humans cannot handle sufficently hard (post-Singularirty type) decisions, and we're going to have to have enhanced humans or AIs help them (if they don't take over completely). Yes, I recognize there always is that &quot;failure of Friendliness&quot; possibility, but that's why we need to design Friendly AI's properly, and any AI or enhanced human handling difficult decisions will have to be sufficently Friendly to handle such decisions without taki<br>
ng
<br>
&nbsp;a risk at blowing humanity up. 
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6747.html">Simon Gordon: "Re: AI Plurality"</a>
<li><strong>Previous message:</strong> <a href="6745.html">Robin Lee Powell: "Re: Security overkill"</a>
<li><strong>Maybe in reply to:</strong> <a href="6729.html">Philip Sutton: "Re: No need to rush AGI development?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6751.html">Bill Hibbard: "Re: No need to rush AGI development?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6746">[ date ]</a>
<a href="index.html#6746">[ thread ]</a>
<a href="subject.html#6746">[ subject ]</a>
<a href="author.html#6746">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
