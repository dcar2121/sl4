<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Brian Atkins (brian@posthuman.com)">
<meta name="Subject" content="Re: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI's flawed friendliness analysis</h1>
<!-- received="Sun May 18 18:40:52 2003" -->
<!-- isoreceived="20030519004052" -->
<!-- sent="Sun, 18 May 2003 19:40:16 -0500" -->
<!-- isosent="20030519004016" -->
<!-- name="Brian Atkins" -->
<!-- email="brian@posthuman.com" -->
<!-- subject="Re: SIAI's flawed friendliness analysis" -->
<!-- id="3EC827F0.4070700@posthuman.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="Pine.GSO.4.44.0305181632180.6365-100000@demedici.ssec.wisc.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brian Atkins (<a href="mailto:brian@posthuman.com?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis"><em>brian@posthuman.com</em></a>)<br>
<strong>Date:</strong> Sun May 18 2003 - 18:40:16 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6731.html">Gordon Worley: "Re: No need to rush AGI development?"</a>
<li><strong>Previous message:</strong> <a href="6729.html">Philip Sutton: "Re: No need to rush AGI development?"</a>
<li><strong>In reply to:</strong> <a href="6726.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6753.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6753.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6730">[ date ]</a>
<a href="index.html#6730">[ thread ]</a>
<a href="subject.html#6730">[ subject ]</a>
<a href="author.html#6730">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Bill Hibbard wrote:
<br>
<em>&gt; On Sat, 17 May 2003, Brian Atkins wrote:
</em><br>
<em>&gt;&gt;Bill Hibbard wrote:
</em><br>
<em>&gt;&gt;&gt;The danger of outlaws will increase as the technology for
</em><br>
<em>&gt;&gt;&gt;intelligent artifacts becomes easier. But as time passes we
</em><br>
<em>&gt;&gt;&gt;will also have the help of safe AIs to help detect and
</em><br>
<em>&gt;&gt;&gt;inspect other AIs.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Even in such fictional books as Neuromancer, we see that such Turing
</em><br>
<em>&gt;&gt;Police do not function well enough to stop a determined superior
</em><br>
<em>&gt;&gt;intelligence. Realistically, such a police force will only have any real
</em><br>
<em>&gt;&gt;chance of success at all if we have a very transparent society... it
</em><br>
<em>&gt;&gt;would require societal changes on a very grand scale, and not just in
</em><br>
<em>&gt;&gt;one country. It all seems rather unlikely... I think we need to focus on
</em><br>
<em>&gt;&gt;solutions that have a chance at actual implementation.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I never said that safe AI is a sure thing. It will require
</em><br>
<em>&gt; a broad political movement that is successful in electoral
</em><br>
<em>&gt; politics. It will require whatever commitment and resources
</em><br>
<em>&gt; are needed to regulate AIs. It will require the patience to
</em><br>
<em>&gt; not rush.
</em><br>
<p>Bill, I'll just come out and state my opinion that what you are 
<br>
describing is a pipe dream. I see no way that the things you speak of 
<br>
have any chance of happening within the next few decades. Governments 
<br>
won't even spend money on properly tracking potential asteroid threats, 
<br>
and you honestly believe they will commit to the VAST amount of both 
<br>
political willpower and real world resource expenditures required to 
<br>
implement an AI detection and inspection system that has even a low 
<br>
percentage shot at actually accomplishing anything?
<br>
<p>And that is not even getting into the fact that by your design the &quot;good 
<br>
AIs&quot; will be crippled by only allowing them very slow intelligence/power 
<br>
increases due to the massive stifling human-speed 
<br>
design/inspection/control regime... they will have zero chance to 
<br>
scale/keep up as computing power further spreads and enables vastly more 
<br>
powerful uncontrolled UFAIs to begin popping up. The result is seemingly 
<br>
a virtual guarantee that eventually an UFAI will get out of control (as 
<br>
you state, your plan is not a &quot;sure thing&quot;) and easily &quot;win&quot; over the 
<br>
regulated other AIs in existence. So what does it accomplish in the end, 
<br>
other than eliminating any chance that a &quot;regulated AI&quot; could &quot;win&quot;?
<br>
<p>Finally, how does your human-centric regulation and design system cope 
<br>
with AIs that need to grow to be smarter than human? Are you proposing 
<br>
to simply keep them limited indefinitely to this level of intelligence, 
<br>
or will the &quot;trusted&quot; AIs themselves eventually take over the process of 
<br>
writing design specs and inspecting each other?
<br>
<p><em>&gt; 
</em><br>
<em>&gt; By pointing out all these difficulties you are helping
</em><br>
<em>&gt; me make my case about the flaws in the SIAI friendliness
</em><br>
<em>&gt; analysis, which simply dismisses the importance of
</em><br>
<em>&gt; politics and regulation in eliminating unsafe AI.
</em><br>
<em>&gt; 
</em><br>
<p>This is a rather nonsensical mantra... everyone is pointing out the 
<br>
obvious flaws in your system- this does not help your idea that politics 
<br>
and regulation are important pieces to the solution of this problem. 
<br>
Tip: drop the mantras, and actually come up with some plausible answers 
<br>
to the objections being raised.
<br>
<p>SIAI's analysis, as already explained by Eliezer, is not attempting at 
<br>
all to completely eliminate the possibility of UFAI. As he said, we 
<br>
don't expect to be able to have any control over someone who sets out to 
<br>
deliberately construct such an UFAI, and we admit this reality rather 
<br>
than attempt to concoct world-spanning pipe dreams.
<br>
<p>P.S. You completely missed my point on the nanotech... I was suggesting 
<br>
a smart enough UFAI could develop in secret some working nanotech long 
<br>
before humans have even figured out how to do such things. There would 
<br>
be no human nanotech defense system. Or, even if you believe that the 
<br>
sequence of technology development will give humans molecular nanotech 
<br>
before AI, my point still stands that a smart enough UFAI will ALWAYS be 
<br>
able to do something that we have not prepared for. The only way to 
<br>
defend against a malevolent superior intelligence in the wild is to be 
<br>
(or have working for you) yourself an even more superior intelligence.
<br>
<pre>
-- 
Brian Atkins
Singularity Institute for Artificial Intelligence
<a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6731.html">Gordon Worley: "Re: No need to rush AGI development?"</a>
<li><strong>Previous message:</strong> <a href="6729.html">Philip Sutton: "Re: No need to rush AGI development?"</a>
<li><strong>In reply to:</strong> <a href="6726.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6753.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6753.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6730">[ date ]</a>
<a href="index.html#6730">[ thread ]</a>
<a href="subject.html#6730">[ subject ]</a>
<a href="author.html#6730">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
