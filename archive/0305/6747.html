<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AI Plurality</title>
<meta name="Author" content="Simon Gordon (sim_dizzy@yahoo.com)">
<meta name="Subject" content="Re: AI Plurality">
<meta name="Date" content="2003-05-19">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AI Plurality</h1>
<!-- received="Mon May 19 17:50:45 2003" -->
<!-- isoreceived="20030519235045" -->
<!-- sent="Tue, 20 May 2003 00:24:02 +0100 (BST)" -->
<!-- isosent="20030519232402" -->
<!-- name="Simon Gordon" -->
<!-- email="sim_dizzy@yahoo.com" -->
<!-- subject="Re: AI Plurality" -->
<!-- id="20030519232402.33245.qmail@web13609.mail.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20030519111419.A13496@sec.sprint.net" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Simon Gordon (<a href="mailto:sim_dizzy@yahoo.com?Subject=Re:%20AI%20Plurality"><em>sim_dizzy@yahoo.com</em></a>)<br>
<strong>Date:</strong> Mon May 19 2003 - 17:24:02 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6748.html">SMcClenahan@ATTBI.com: "Re: Security overkill"</a>
<li><strong>Previous message:</strong> <a href="6746.html">Tommeteor@aol.com: "Re: No need to rush AGI development?"</a>
<li><strong>Maybe in reply to:</strong> <a href="6715.html">Simon Gordon: "AI Plurality"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6747">[ date ]</a>
<a href="index.html#6747">[ thread ]</a>
<a href="subject.html#6747">[ subject ]</a>
<a href="author.html#6747">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;--- <a href="mailto:polysync@pobox.com?Subject=Re:%20AI%20Plurality">polysync@pobox.com</a> wrote: 
<br>
<em>&gt; &gt; Ironic splutter aside....the &quot;real problem&quot; seems
</em><br>
<em>&gt; to sort itself out if we
</em><br>
<em>&gt; &gt; release a society of FAIs into the physical world
</em><br>
<em>&gt; at exactly the same time.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;  That doesn't seem to work for all humans.
</em><br>
<p>Firstly humans are not all equal, they are born at
<br>
different times in different generations and their
<br>
social organisations are hierarchical.
<br>
<p>Humans are impure intelligences for many reasons. They
<br>
began in an environment of scarcity and so evolved to
<br>
become fiercely competitive, unfortunately this
<br>
translates into corruption in many adult humans in the
<br>
modern urban environment we are now living in. AIs of
<br>
course can be brought up in any environment we like
<br>
and they do not need to evolve in the same way our
<br>
society has; in the absense of scarcity there in no
<br>
need to be super competitive. It may seem unlikely
<br>
that AIs will naturally learn to become intrinsically
<br>
friendly in a societal structure, but they will at
<br>
least have to &quot;appear&quot; friendly since if they are all
<br>
of equal power and status there would be a stalemate
<br>
scenario, a standoff, like two huge nucleur powers
<br>
aware that they cannot attack each other without
<br>
themselves getting destroyed, thus the AIs would be
<br>
bred into an environment of cooperation. If they were
<br>
subsequently brought into the physical world (together
<br>
as a collective at the same time) then they would have
<br>
to continue their cooperation because each would gain
<br>
their own sysop-like control over the physical
<br>
environment and eventually become superpowers.
<br>
&nbsp;&nbsp;
<br>
<em>&gt; &gt; then they would naturally evolve to cooperate with
</em><br>
<em>&gt; each other having played
</em><br>
<em>&gt; &gt; lots of Prisoner's Dilemma type games and learned
</em><br>
<em>&gt; that overall the best
</em><br>
<em>&gt; &gt; strategy was friendly cooperation.
</em><br>
<em>&gt; 
</em><br>
<em>&gt;  The larger the group of cooperaters, the bigger the
</em><br>
<em>&gt; potential payoff for a
</em><br>
<em>&gt; lone defector. 
</em><br>
<p>It depends how the network of players is organised, on
<br>
an individual one-to-one basis the optimal strategy is
<br>
tit for tat with cooperation as the first perogative.
<br>
In a larger group, in order to secure the overall
<br>
good, the majority of AIs will cooperate, but this
<br>
type of behaviour has to be learnt through an extended
<br>
period of societal interaction. From this &quot;moral good&quot;
<br>
comes about, in a kind of holistic fashion.
<br>
<p>The ideal of course is that ALL the AIs or members in
<br>
the group learn the &quot;moral good&quot; to the extent that
<br>
they have become resistent to the idea of personal
<br>
defection and are able to ignore the potential
<br>
payoffs. It is certainly true that our human society
<br>
is nowhere near this ideal....but AIs can run faster
<br>
than us, they could outpace us in a few years perhaps
<br>
and reach this ideal much quicker. When this happens
<br>
they will have matured to a sociological level far
<br>
more advanced than humans and so by then we would be
<br>
able to safely call them FAIs , they would have become
<br>
advanced socially responsible agents, and it seems
<br>
plausible that then we would be able to put greater
<br>
trust in an FAI than we could in any human being, even
<br>
our closest and dearest. 
<br>
<p>Trust no-one - except the FAIs.
<br>
<p>Simon.
<br>
<p><p><p><p>__________________________________________________
<br>
Yahoo! Plus
<br>
For a better Internet experience
<br>
<a href="http://www.yahoo.co.uk/btoffer">http://www.yahoo.co.uk/btoffer</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6748.html">SMcClenahan@ATTBI.com: "Re: Security overkill"</a>
<li><strong>Previous message:</strong> <a href="6746.html">Tommeteor@aol.com: "Re: No need to rush AGI development?"</a>
<li><strong>Maybe in reply to:</strong> <a href="6715.html">Simon Gordon: "AI Plurality"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6747">[ date ]</a>
<a href="index.html#6747">[ thread ]</a>
<a href="subject.html#6747">[ subject ]</a>
<a href="author.html#6747">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
