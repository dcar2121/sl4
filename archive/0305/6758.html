<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Brian Atkins (brian@posthuman.com)">
<meta name="Subject" content="Re: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI's flawed friendliness analysis</h1>
<!-- received="Tue May 20 18:30:50 2003" -->
<!-- isoreceived="20030521003050" -->
<!-- sent="Tue, 20 May 2003 19:30:11 -0500" -->
<!-- isosent="20030521003011" -->
<!-- name="Brian Atkins" -->
<!-- email="brian@posthuman.com" -->
<!-- subject="Re: SIAI's flawed friendliness analysis" -->
<!-- id="3ECAC893.7070100@posthuman.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="Pine.GSO.4.44.0305201618460.15294-100000@demedici.ssec.wisc.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Brian Atkins (<a href="mailto:brian@posthuman.com?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis"><em>brian@posthuman.com</em></a>)<br>
<strong>Date:</strong> Tue May 20 2003 - 18:30:11 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6759.html">Leonardo Wild: "Contemplating Money &amp; Values (Was: Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>Previous message:</strong> <a href="6757.html">Keith Elis: "AGI Policy (was RE: SIAI's flawed friendliness analysis)"</a>
<li><strong>In reply to:</strong> <a href="6753.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6797.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6797.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6758">[ date ]</a>
<a href="index.html#6758">[ thread ]</a>
<a href="subject.html#6758">[ subject ]</a>
<a href="author.html#6758">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Bill Hibbard wrote:
<br>
<em>&gt; On Sun, 18 May 2003, Brian Atkins wrote:
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt;Bill Hibbard wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;On Sat, 17 May 2003, Brian Atkins wrote:
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;&gt;Bill Hibbard wrote:
</em><br>
<em>&gt;&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;&gt;&gt;The danger of outlaws will increase as the technology for
</em><br>
<em>&gt;&gt;&gt;&gt;&gt;intelligent artifacts becomes easier. But as time passes we
</em><br>
<em>&gt;&gt;&gt;&gt;&gt;will also have the help of safe AIs to help detect and
</em><br>
<em>&gt;&gt;&gt;&gt;&gt;inspect other AIs.
</em><br>
<em>&gt;&gt;&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;&gt;Even in such fictional books as Neuromancer, we see that such Turing
</em><br>
<em>&gt;&gt;&gt;&gt;Police do not function well enough to stop a determined superior
</em><br>
<em>&gt;&gt;&gt;&gt;intelligence. Realistically, such a police force will only have any real
</em><br>
<em>&gt;&gt;&gt;&gt;chance of success at all if we have a very transparent society... it
</em><br>
<em>&gt;&gt;&gt;&gt;would require societal changes on a very grand scale, and not just in
</em><br>
<em>&gt;&gt;&gt;&gt;one country. It all seems rather unlikely... I think we need to focus on
</em><br>
<em>&gt;&gt;&gt;&gt;solutions that have a chance at actual implementation.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;&gt;I never said that safe AI is a sure thing. It will require
</em><br>
<em>&gt;&gt;&gt;a broad political movement that is successful in electoral
</em><br>
<em>&gt;&gt;&gt;politics. It will require whatever commitment and resources
</em><br>
<em>&gt;&gt;&gt;are needed to regulate AIs. It will require the patience to
</em><br>
<em>&gt;&gt;&gt;not rush.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Bill, I'll just come out and state my opinion that what you are
</em><br>
<em>&gt;&gt;describing is a pipe dream. I see no way that the things you speak of
</em><br>
<em>&gt;&gt;have any chance of happening within the next few decades. Governments
</em><br>
<em>&gt;&gt;won't even spend money on properly tracking potential asteroid threats,
</em><br>
<em>&gt;&gt;and you honestly believe they will commit to the VAST amount of both
</em><br>
<em>&gt;&gt;political willpower and real world resource expenditures required to
</em><br>
<em>&gt;&gt;implement an AI detection and inspection system that has even a low
</em><br>
<em>&gt;&gt;percentage shot at actually accomplishing anything?
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;And that is not even getting into the fact that by your design the &quot;good
</em><br>
<em>&gt;&gt;AIs&quot; will be crippled by only allowing them very slow intelligence/power
</em><br>
<em>&gt;&gt;increases due to the massive stifling human-speed
</em><br>
<em>&gt;&gt;design/inspection/control regime... they will have zero chance to
</em><br>
<em>&gt;&gt;scale/keep up as computing power further spreads and enables vastly more
</em><br>
<em>&gt;&gt;powerful uncontrolled UFAIs to begin popping up. The result is seemingly
</em><br>
<em>&gt;&gt;a virtual guarantee that eventually an UFAI will get out of control (as
</em><br>
<em>&gt;&gt;you state, your plan is not a &quot;sure thing&quot;) and easily &quot;win&quot; over the
</em><br>
<em>&gt;&gt;regulated other AIs in existence. So what does it accomplish in the end,
</em><br>
<em>&gt;&gt;other than eliminating any chance that a &quot;regulated AI&quot; could &quot;win&quot;?
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;Finally, how does your human-centric regulation and design system cope
</em><br>
<em>&gt;&gt;with AIs that need to grow to be smarter than human? Are you proposing
</em><br>
<em>&gt;&gt;to simply keep them limited indefinitely to this level of intelligence,
</em><br>
<em>&gt;&gt;or will the &quot;trusted&quot; AIs themselves eventually take over the process of
</em><br>
<em>&gt;&gt;writing design specs and inspecting each other?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If humans can design AIs smarter than humans, then humans
</em><br>
<em>&gt; can regulate AIs smarter than humans.
</em><br>
<p>Just because a human can design some seed AI code that grows into a SI 
<br>
does not imply that humans or human-level AIs can successfully 
<br>
&quot;regulate&quot; grown SIs.
<br>
<p><em>&gt; It is not necessary
</em><br>
<em>&gt; to trace an AI's thoughts in detail, just to understand
</em><br>
<em>&gt; the mechanisms of its thoughts. Furthermore, once trusted
</em><br>
<em>&gt; AIs are available, they can take over the details of
</em><br>
<em>&gt; design and regulation. I would trust an AI with
</em><br>
<em>&gt; reinforcement values for human happiness more than I
</em><br>
<em>&gt; would trust any individual human.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is a bit like the experience of people who write
</em><br>
<em>&gt; game playing programs that they cannot beat. All the
</em><br>
<em>&gt; programmer needs to know is that the logic for
</em><br>
<em>&gt; simulating the game and for reinforcement learning are
</em><br>
<em>&gt; accurate and efficient, and that the reinforcement
</em><br>
<em>&gt; values are for winning the game
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You say &quot;by your design the 'good AIs' will be crippled
</em><br>
<em>&gt; by only allowing them very slow intelligence/power
</em><br>
<em>&gt; increases due to the massive stifling human-speed&quot;. But
</em><br>
<em>&gt; once we have trusted AIs, they can take over the details
</em><br>
<em>&gt; of designing and regulating other AIs. 
</em><br>
<p>Well perhaps I misunderstood you on this point. So it's perfectly ok 
<br>
with you if the very first &quot;trusted AI&quot; turns around and says: &quot;Ok, I 
<br>
have determined that in order to best fulfill my goal system I need to 
<br>
build a large nanocomputing system over the next two weeks, and then 
<br>
proceed to thoroughly redesign myself to boost my intelligence 1000000x 
<br>
by next month. And then, I plan to take over root access to all the nuke 
<br>
control systems on the planet, construct a fully robotic nanotech 
<br>
research lab, and spawn off about a million copies of myself.&quot;? If 
<br>
you're ok with that (or whatever it outputs), then I can withdraw my 
<br>
quote above. I fully agree with you that letting a properly designed and 
<br>
tested FAI do what it needs to do, as fast as it wants to do it, is the 
<br>
safest and most rational course of action.
<br>
<p>Now you also still haven't answered to my satisfaction my objections 
<br>
that the system will never get built due to multiple political, cost, 
<br>
and feasibility issues.
<br>
<p><em>&gt;&gt;&gt;By pointing out all these difficulties you are helping
</em><br>
<em>&gt;&gt;&gt;me make my case about the flaws in the SIAI friendliness
</em><br>
<em>&gt;&gt;&gt;analysis, which simply dismisses the importance of
</em><br>
<em>&gt;&gt;&gt;politics and regulation in eliminating unsafe AI.
</em><br>
<em>&gt;&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;This is a rather nonsensical mantra... everyone is pointing out the
</em><br>
<em>&gt;&gt;obvious flaws in your system- this does not help your idea that politics
</em><br>
<em>&gt;&gt;and regulation are important pieces to the solution of this problem.
</em><br>
<em>&gt;&gt;Tip: drop the mantras, and actually come up with some plausible answers
</em><br>
<em>&gt;&gt;to the objections being raised.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Calling this a &quot;nonsensical mantra&quot; does not answer it.
</em><br>
<em>&gt; The objections are just possible ways that a political
</em><br>
<em>&gt; solution may fail. Of course it may fail. But its the
</em><br>
<em>&gt; best chance of success.
</em><br>
<p>No, calling a proposed solution with many raised and unanswered 
<br>
objections, that you yourself admit could easily fail, the &quot;best chance 
<br>
of success&quot; is not something I will agree with. In order to convince me 
<br>
of that rather large claim, you will have to go much farther and into 
<br>
much more detail.
<br>
<p><em>&gt; 
</em><br>
<em>&gt;&gt;SIAI's analysis, as already explained by Eliezer, is not attempting at
</em><br>
<em>&gt;&gt;all to completely eliminate the possibility of UFAI. As he said, we
</em><br>
<em>&gt;&gt;don't expect to be able to have any control over someone who sets out to
</em><br>
<em>&gt;&gt;deliberately construct such an UFAI, and we admit this reality rather
</em><br>
<em>&gt;&gt;than attempt to concoct world-spanning pipe dreams.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Powerful people and institutions will try to manipulate
</em><br>
<em>&gt; the singularity to preserve and enhance their interests.
</em><br>
<em>&gt; Any strategy for safe AI must try to counter this threat.
</em><br>
<em>&gt; 
</em><br>
<p>Certainly, and we argue the best way is to speed up the progress of the 
<br>
well-meaning projects in order to win that race.
<br>
<p>Your plan seems to want to slow down the well-meaning projects, because 
<br>
out of all AGI projects they are the most likely to willingly go along 
<br>
with such forms of regulation. This looks to many of us here as if you 
<br>
are going out of your way to help the &quot;powerful people and institutions&quot; 
<br>
get a better shot at winning this race. Such people and institutions are 
<br>
the ones who have demonstrated time and time again throughout history 
<br>
that they will go through loopholes, work around the regulatory bodies, 
<br>
and generally use whatever means needed in order to advance their goals. 
<br>
Again, to most of us, it just looks like pure naivete on your part.
<br>
<p><em>&gt; 
</em><br>
<em>&gt;&gt;P.S. You completely missed my point on the nanotech... I was suggesting
</em><br>
<em>&gt;&gt;a smart enough UFAI could develop in secret some working nanotech long
</em><br>
<em>&gt;&gt;before humans have even figured out how to do such things. There would
</em><br>
<em>&gt;&gt;be no human nanotech defense system. Or, even if you believe that the
</em><br>
<em>&gt;&gt;sequence of technology development will give humans molecular nanotech
</em><br>
<em>&gt;&gt;before AI, my point still stands that a smart enough UFAI will ALWAYS be
</em><br>
<em>&gt;&gt;able to do something that we have not prepared for. The only way to
</em><br>
<em>&gt;&gt;defend against a malevolent superior intelligence in the wild is to be
</em><br>
<em>&gt;&gt;(or have working for you) yourself an even more superior intelligence.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I didn't miss your point. I accepted that nanotech is a big
</em><br>
<em>&gt; threat, along with genetic engineering of micro-organisms.
</em><br>
<em>&gt; I added that nanotech will be a threat with or without AI.
</em><br>
<p>Those weren't the point. The reason I brought up the 
<br>
UFAI-invents-nanotech possibility is that you didn't seem to be 
<br>
considering such unconventional/undetectable threats when you said:
<br>
<p>&quot;But for an unsafe AI to pose a real
<br>
threat it must have power in the world, meaning either control
<br>
over significant weapons (including things like 767s), or access
<br>
to significant numbers of humans. But having such power in the
<br>
world will make the AI detectable, so that it can be inspected
<br>
to determine whether it conforms to safety regulations.&quot;
<br>
<p>When I brought up the idea that UFAIs could develop threats that were 
<br>
undetectable/unstoppable, thereby rendering your detection plan 
<br>
unrealistic, you appeared to miss the point because you did not respond 
<br>
to my objection. Instead you seemed on one hand to say that &quot;it is far 
<br>
from a sure thing&quot; and on the other hand that apparently you are quite 
<br>
sure that humans will already have detection networks built for any type 
<br>
of threat an UFAI can dream up (highly unlikely IMO). Neither are good 
<br>
answers to how your plan deals with possibly undetectable UFAI threats.
<br>
<p><em>&gt; The way to counter the threat of micro-organisms has been
</em><br>
<em>&gt; detection networks, isolation of affected people and
</em><br>
<em>&gt; regions, and urgent efforts to analyze the organisms and
</em><br>
<em>&gt; find counter measures. There are also efforts to monitor
</em><br>
<em>&gt; the humans with the knowledge to create new micro-organisms.
</em><br>
<em>&gt; These measures all have the force of law and the resources
</em><br>
<em>&gt; of government behind them. Similar measures will apply to
</em><br>
<em>&gt; the threat of nanotech. When safe AIs are available, they
</em><br>
<em>&gt; will certainly be enlisted to help. With such huge threats
</em><br>
<em>&gt; as nanotech the pipe dream is to think that they can be
</em><br>
<em>&gt; countered without the force of law and the resources of
</em><br>
<em>&gt; government. Or to think that government won't get involved.
</em><br>
<p>Oh, I don't disagree that some form of &quot;government&quot; will be required, I 
<br>
just think it will be a post-Singularity form of governance that will 
<br>
have no relation to our current system.
<br>
<p>At any rate, I believe you will grant me my point that &quot;safe AIs&quot; can 
<br>
only defend us if they stay ahead of any possible UFAI's intelligence level.
<br>
<pre>
-- 
Brian Atkins
Singularity Institute for Artificial Intelligence
<a href="http://www.intelligence.org/">http://www.intelligence.org/</a>
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6759.html">Leonardo Wild: "Contemplating Money &amp; Values (Was: Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>Previous message:</strong> <a href="6757.html">Keith Elis: "AGI Policy (was RE: SIAI's flawed friendliness analysis)"</a>
<li><strong>In reply to:</strong> <a href="6753.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6797.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6797.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6758">[ date ]</a>
<a href="index.html#6758">[ thread ]</a>
<a href="subject.html#6758">[ subject ]</a>
<a href="author.html#6758">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
