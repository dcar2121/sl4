<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Leonardo Wild (dlwild@access.net.ec)">
<meta name="Subject" content="Re: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI's flawed friendliness analysis</h1>
<!-- received="Sun May 18 23:36:16 2003" -->
<!-- isoreceived="20030519053616" -->
<!-- sent="Mon, 19 May 2003 00:33:39 -0500" -->
<!-- isosent="20030519053339" -->
<!-- name="Leonardo Wild" -->
<!-- email="dlwild@access.net.ec" -->
<!-- subject="Re: SIAI's flawed friendliness analysis" -->
<!-- id="3EC86CB3.4050006@access.net.ec" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="00b101c31ceb$5b37c620$3e553f94@GaryMiller01" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Leonardo Wild (<a href="mailto:dlwild@access.net.ec?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis"><em>dlwild@access.net.ec</em></a>)<br>
<strong>Date:</strong> Sun May 18 2003 - 23:33:39 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6735.html">Samantha: "Re: No need to rush AGI development?"</a>
<li><strong>Previous message:</strong> <a href="6733.html">Mike Williams: "RE: No need to rush AGI development?"</a>
<li><strong>In reply to:</strong> <a href="6721.html">Gary Miller: "RE: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6737.html">Philip Sutton: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6737.html">Philip Sutton: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6739.html">Gary Miller: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6742.html">Cliff Stabbert: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6734">[ date ]</a>
<a href="index.html#6734">[ thread ]</a>
<a href="subject.html#6734">[ subject ]</a>
<a href="author.html#6734">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Hello,
<br>
<p>I agree with Bill Hibbard on the following:
<br>
<p><em>&gt; 
</em><br>
<em>&gt; Understanding what humans are, what the AI itself is, and
</em><br>
<em>&gt; what values are, is implicit in the simulation model of the
</em><br>
<em>&gt; world created by any intelligent mind. It uses this model to
</em><br>
<em>&gt; predict the long-term consequences of its behaviors on its
</em><br>
<em>&gt; values. Such understanding is prescribed by an intelligence
</em><br>
<em>&gt; model rather than a safe AI model.
</em><br>
<em>&gt; 
</em><br>
<p>And I disagree with Gary Miller that his post of 5/17/03 ...
<br>
<p>Gary Miller wrote:
<br>
<em>&gt; My proposed solution to friendliness problem.
</em><br>
<em>&gt;  
</em><br>
<em>&gt; Note some of you will laugh this off as overkill.  But believe me having 
</em><br>
<em>&gt; worked as a
</em><br>
<em>&gt; consultant for the government for a number of years, this is just 
</em><br>
<em>&gt; business as
</em><br>
<em>&gt; usual for NSA.  It is a very expensive but very secure development process.
</em><br>
<em>&gt; It is based upon separation and balance of power.  No one person has the 
</em><br>
<em>&gt; access
</em><br>
<em>&gt; and knowledge to compromise the system.  Relationships between team members
</em><br>
<em>&gt; must be prohibited to prevent possibility of collusion.
</em><br>
<p>&lt;(snip)&gt;
<br>
<p>... is really a solution to the &quot;friendliness problem.&quot; His proposed 
<br>
solution is one that deals with the &quot;safety&quot; of a FAI project.
<br>
<p>Naturally, the safety is necessary to make sure that &quot;intended 
<br>
friendliness programming&quot; doesn't or can't get meddled with. In other 
<br>
words, its a solution (partial) as to how to go about making sure that 
<br>
intended outcomes are not changed by those working on such a project or 
<br>
by those who would have some interest in infiltrating such a project. 
<br>
Yet it does not define what &quot;friendliness&quot; is nor what core values are. 
<br>
Is is one more way to make sure that the following does happen (as 
<br>
expressed by Bill Hibbard):
<br>
<p><em>&gt; BH: The ambiguous definitions in the SIAI analysis will be
</em><br>
<em>&gt; exploited by powerful people and institutions to create
</em><br>
<em>&gt; AIs that protect and enhance their own interests.
</em><br>
<p>In his posting he also writes:
<br>
<p><em>&gt; BH: But I think that AI values
</em><br>
<em>&gt; for human happiness link AI values with human values and
</em><br>
<em>&gt; create a symbiotic system combining humans and AIs. Keeping
</em><br>
<em>&gt; humans &quot;in the loop&quot; offers the best hope of preventing any
</em><br>
<em>&gt; drift away from human interests. An AI with humans in its
</em><br>
<em>&gt; loop will have no motive to design an AI without humans in
</em><br>
<em>&gt; its loop. And as long as humans are in the loop, they can
</em><br>
<em>&gt; exert reinforcement to protect their own interests.
</em><br>
<em>&gt; 
</em><br>
(...)
<br>
<em>&gt; 
</em><br>
<em>&gt; The &quot;*deep* understanding of values&quot; is implicit in superior
</em><br>
<em>&gt; intelligence. It is a very accurate simulation model of the
</em><br>
<em>&gt; world that includes understanding of how value systems work,
</em><br>
<em>&gt; and the effects that different value systems would have on
</em><br>
<em>&gt; brains. But choosing between different value systems requires
</em><br>
<em>&gt; base values for comparing the consequences of different value
</em><br>
<em>&gt; systems.
</em><br>
<p>Which is the core of the problem, which is also found in the partial or 
<br>
&nbsp;&nbsp;qualitatively limited definitions of &quot;intelligence,&quot; &quot;intuition,&quot; 
<br>
&quot;smartness,&quot; &quot;awareness,&quot; &quot;values,&quot; &quot;understanding,&quot; &quot;knowledge,&quot; etc.
<br>
<p>We can speak of intelligence like we can speak of the wind, but &quot;wind&quot; 
<br>
is a concept that is implicit to &quot;superior intelligence&quot; as long as the 
<br>
&quot;organism&quot; that has the concept has experienced &quot;wind.&quot; But the concept 
<br>
of wind, for someone whose life (or livelihood) depends on it, will have 
<br>
qualitatively different and relatively precise definitions about the 
<br>
kinds of &quot;wind.&quot;  The more you depend on it, the more precise (and 
<br>
diferentiating) will be your definition of wind (or any other subject).
<br>
<p>&quot;Yes, it is wind, but what _kind_ of wind?&quot;
<br>
<p>or &quot;What kind of snow?&quot;
<br>
or &quot;What kind of fish?&quot;
<br>
or &quot;What kind of intelligence?&quot;
<br>
<p>etc.
<br>
<p>So, too, friendliness is implicit to superior intelligence, yes (though 
<br>
not only then, according to studies presented in a book called GOOD 
<br>
NATURED -I can check on the author another time, for anyone interested). 
<br>
But what _is_ it that makes someone behave in a friendly way? What kind 
<br>
of values create the context in which it is &quot;intelligent&quot; to behave in a 
<br>
friendly way? Sometimes, friendliness can be the wrong type of attitude 
<br>
or behavior in a given context in order to survive; in fact, it may even 
<br>
be &quot;unintelligent&quot; to behave in a friendly way.
<br>
<p>Friendliness is directly related with &quot;values,&quot; just as values are 
<br>
directly related to &quot;needs.&quot; But there are different kinds of needs 
<br>
which reflect in different kinds of values. So, certain values can be 
<br>
viewed as positive or &quot;good&quot; in a certain context, and completely the 
<br>
opposite in another. The bottom line is to find the kind of need for 
<br>
values that will make an AI &quot;friendly&quot; towards humanity at large (though 
<br>
perhaps not necessarily towards particular individual human beings) or, 
<br>
rather, towards &quot;Life,&quot; in the most general sense. (?)
<br>
<p>As Bill Hibbard wrote:
<br>
<p><em>&gt; Keeping humans &quot;in the loop&quot; offers the best hope of preventing any
</em><br>
<em>&gt; drift away from human interests.
</em><br>
<p>Meaning, how can we make sure that the needs of an AI for the &quot;respected 
<br>
and respectful&quot; presence and existence of human beings (and the context 
<br>
they need for survival and evolution) gets reflected in and AI's value 
<br>
system or structure? And how can such a &quot;set of values&quot; be programmed 
<br>
into the core or kernel of an AI regardless of future development and 
<br>
autopoietic growth? Because, for programming's sake, it's not enough for 
<br>
&nbsp;&nbsp;types of needs, types of values, types of friendliness to be 
<br>
&quot;implicit&quot; in superior intelligence (human or AI or n-versions of it), 
<br>
but it must be made &quot;explicit,&quot; which means the creation of an UML 
<br>
(Universal Modelling Language)-type diagram or sets of diagrams that 
<br>
enable programmers and project designers and all those involved in the 
<br>
FAI project to &quot;explicitly agree&quot; on the &quot;ingredients&quot; of concepts (such 
<br>
as friendliness) ... which is but a very broad ideal for a given type of 
<br>
behavior that appears to be as implicit as when we speak of &quot;happiness&quot; 
<br>
or &quot;anger.&quot;
<br>
<p>The thing to consider is that the ambiguous definitions of _any_ 
<br>
analysis will be exploited by powerful people and institutions to 
<br>
protect and enhance their own interests.
<br>
<p>I read a while ago posts on the &quot;search for money&quot; to fund SIAI, but 
<br>
never was there any questioning as to why money (which is a man made 
<br>
technology) is scarce (not just for funding deep science projects), and 
<br>
why money appears to flow only into certain type of projects, and why it 
<br>
is necessary for someone (like Eliezer) to _prove_ that the money is 
<br>
&quot;well invested&quot; ... it even remains ambigous what a &quot;good investment&quot; 
<br>
means in monetary terms. A lot of assumptions that do not question the 
<br>
&quot;technology money&quot; at all, which becomes, once again, a problem similar 
<br>
to what Bill Hibbard already wrote about finding the base values to the 
<br>
values you wish to work with. If you wish to work with &quot;funding&quot; then 
<br>
you must necessarily work with money, but if money's &quot;inner workings&quot; 
<br>
are implicit or ambigous in our understanding, then we can forever chase 
<br>
after it without really knowing what &quot;it&quot; is nor why it is not available 
<br>
for &quot;good&quot; projects. We may be intelligent, but our intelligence doesn't 
<br>
seem to be willing to create an awareness and hopefully understanding of 
<br>
one of the most widely used man-made contraptions (money).
<br>
<p>The flaw, if I may say so (and not only in relation to &quot;friendliness 
<br>
analysis&quot; but to many apparently implicitly understood concepts, 
<br>
including something as illusorily clear as &quot;money&quot;) is to avoid creating 
<br>
UML-diagramable versions of the concepts so we can all agree on them 
<br>
rather than spending our efforts disagreeing about them or going off on 
<br>
tangents.
<br>
<p>What this means is that certain concepts must be re-analyzed in this new 
<br>
light (with a different goal in mind, one of programmability, one of 
<br>
consensus agreements) rather than saying &quot;that's already a closed 
<br>
issue.&quot; It was, more or less, a closed issue tha the world is &quot;flat.&quot; It 
<br>
is, more or less, an assumed concept that it isn't possible to make 
<br>
triangles* that have three inner angles each of 90 degrees, or squares** 
<br>
with four inner angles each of 105 degrees (to give but two examples).
<br>
<p>Best,
<br>
<p>Leonardo Wild
<br>
<p>PS: Crocker's Rules apply ...
<br>
<p>... considering the fact that I was 'ousted' for a month for being 
<br>
un-scientific; yet my email created an activity of discussion on the 
<br>
subject of multiple universes and infinite universes of over 140 emails 
<br>
even though that subject had not previously been (according to the 
<br>
archives) discussed on this list, or at least not under such a heading. 
<br>
Paradoxical, isn't it?
<br>
<p>***
<br>
<p>*Triangle = 1. in geometry, a figure bounded by three lines, and 
<br>
containing three angles.
<br>
<p>**Square = 1. b) more or less cubical; rectangular and 
<br>
three-dimensional, as a box.
<br>
<p><p><p><p><p><p><p><p><p><p><p><p><em>&gt; 
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6735.html">Samantha: "Re: No need to rush AGI development?"</a>
<li><strong>Previous message:</strong> <a href="6733.html">Mike Williams: "RE: No need to rush AGI development?"</a>
<li><strong>In reply to:</strong> <a href="6721.html">Gary Miller: "RE: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6737.html">Philip Sutton: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6737.html">Philip Sutton: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6739.html">Gary Miller: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6742.html">Cliff Stabbert: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6734">[ date ]</a>
<a href="index.html#6734">[ thread ]</a>
<a href="subject.html#6734">[ subject ]</a>
<a href="author.html#6734">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
