<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Ben Goertzel (ben@goertzel.org)">
<meta name="Subject" content="RE: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: SIAI's flawed friendliness analysis</h1>
<!-- received="Sat May 24 09:00:34 2003" -->
<!-- isoreceived="20030524150034" -->
<!-- sent="Sat, 24 May 2003 11:01:27 -0400" -->
<!-- isosent="20030524150127" -->
<!-- name="Ben Goertzel" -->
<!-- email="ben@goertzel.org" -->
<!-- subject="RE: SIAI's flawed friendliness analysis" -->
<!-- id="LAEGJLOGJIOELPNIOOAJIECJFAAA.ben@goertzel.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3ED012A0.3855.7FA70B@localhost" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Ben Goertzel (<a href="mailto:ben@goertzel.org?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis"><em>ben@goertzel.org</em></a>)<br>
<strong>Date:</strong> Sat May 24 2003 - 09:01:27 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6813.html">Samantha Atkins: "Re: Failure of AI so far"</a>
<li><strong>Previous message:</strong> <a href="6811.html">Philip Sutton: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6811.html">Philip Sutton: "RE: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6816.html">Philip Sutton: "Collating views on friendliness (or whatever it is we want)"</a>
<li><strong>Reply:</strong> <a href="6816.html">Philip Sutton: "Collating views on friendliness (or whatever it is we want)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6812">[ date ]</a>
<a href="index.html#6812">[ thread ]</a>
<a href="subject.html#6812">[ subject ]</a>
<a href="author.html#6812">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
hi,
<br>
<p><em>&gt; OK.  Let's look at the semi-hard take off scenario.  I assume that what
</em><br>
<em>&gt; you mean is that for a period after the creation of a baby-AGI the
</em><br>
<em>&gt; humans around it will have to do a lot of work to build it up to a
</em><br>
<em>&gt; reasonable level of competence in the real world (lots of
</em><br>
<em>&gt; training/lots of
</em><br>
<em>&gt; new code development/lots of hand holding) But at some stage all this
</em><br>
<em>&gt; hard work will come together and the AGI (or a group of AGIs) will be
</em><br>
<em>&gt; able to drive it's/their own self-improvement without much input from
</em><br>
<em>&gt; humans.  At that point we will get a hard take-off.  Ben, have I
</em><br>
<em>&gt; interpreted your views accurately?
</em><br>
<p>Yep.
<br>
<p>There are different scenarios regarding mainstream acceptance of the
<br>
potential of the baby AI.
<br>
<p>One case is where the world realizes a baby AGI is present and a takeoff is
<br>
imminent, and governments become involved.
<br>
<p>Another case is where the state of progress is intentionally kept secret by
<br>
the AGI developers.
<br>
<p>Another case is where the developers go public and almost nobody believes
<br>
them, or takes them fully seriously about the real potential of their work.
<br>
<p><em>&gt; If this is a reasonable summary, then it seems to me that we have to
</em><br>
<em>&gt; have a very, very reliable guess as to when to expect the transition to
</em><br>
<em>&gt; take-off to begin.
</em><br>
<p>I'm not sure this will be possible.  I think we will be able to say when
<br>
it's *remotely possible* versus when it's just too early-stage.  But
<br>
distinguishing remotely possible from reasonably likely is going to be
<br>
hard -- the first time around ... which is the most important time of
<br>
course...
<br>
<p><em>&gt; My understanding of things is that SIAI feels that we cannot know when
</em><br>
<em>&gt; we are one the safe side of take-off so friendliness work should be done
</em><br>
<em>&gt; now.  Ben on the other hand (I think) thinks that it would be 100% safe
</em><br>
<em>&gt; to have an early-model baby AGI in existence before much work was
</em><br>
<em>&gt; done on introducing life-compassion.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Whether Ben is right I think depends on whether the lead time to go
</em><br>
<em>&gt; from an early-model baby AGI to the point of hard take-off is longer
</em><br>
<em>&gt; than the lead-time for AGI development teams and/or society to go
</em><br>
<em>&gt; from a vague idea of what we want in the way of AGI morality to the
</em><br>
<em>&gt; point where we can introduce it tangibly and securely into real AGIs.
</em><br>
<p>I think that the appropriate manner of moving from vagueness to precision in
<br>
this matter is not clear right now, and will be much clearer once we have a
<br>
baby AGI doing the equivalent of crawling around and going &quot;Goo goo goo.&quot;
<br>
<p><em>&gt; My own feeling is that there are lots of issues about what sort of
</em><br>
<em>&gt; morality we think AGIs should have that are not hardware/software
</em><br>
<em>&gt; dependent and that have, most likley, a longer leadtime than the early-
</em><br>
<em>&gt; model baby AGI to the point of hard take-off leadtime.
</em><br>
<p>I don't deny that this is a possibility, but I don't clearly see this to be
<br>
the case.
<br>
<p>I have chosen not to focus my own efforts in this direction, but I am
<br>
supportive of such efforts and will participate in pertinent discussions or
<br>
projects led by others.
<br>
<p><em>&gt; If that's so then we need to redouble the effort on AGI morality.  The
</em><br>
<em>&gt; Singularity Institute has done very valuable work in the area which
</em><br>
<em>&gt; needs to be developed further.  But I think there are aspects of the AGI
</em><br>
<em>&gt; morality issue that the Institute itself hasn't even flagged.
</em><br>
<p>Such as?
<br>
<p><em>&gt; It would be quite interesting to conduct some sort of collaborative
</em><br>
<em>&gt; scoping exercise to identify what issues different people think we need
</em><br>
<em>&gt; to look at. If we could produce a single document that had all the big
</em><br>
<em>&gt; issues that each one of us thinks should be considered in the course of
</em><br>
<em>&gt; tackling AGI morality then we might be able to avoid talking past each
</em><br>
<em>&gt; other and from this document we might be able to generate an R&amp;D
</em><br>
<em>&gt; agenda - moving in several direction at once as I don't anticipate that
</em><br>
<em>&gt; we will all be of one mind..
</em><br>
<em>&gt;
</em><br>
<em>&gt; What do you think of this suggestion?
</em><br>
<p>It sounds like a fine idea to me -- except that I don't have time to write
<br>
the document at the moment, because I've prioritized other things ahead of
<br>
it.  I'll be happy to contribute ideas to such a document and to review and
<br>
discuss such a document.
<br>
<p>-- Ben Goertzel
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6813.html">Samantha Atkins: "Re: Failure of AI so far"</a>
<li><strong>Previous message:</strong> <a href="6811.html">Philip Sutton: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6811.html">Philip Sutton: "RE: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6816.html">Philip Sutton: "Collating views on friendliness (or whatever it is we want)"</a>
<li><strong>Reply:</strong> <a href="6816.html">Philip Sutton: "Collating views on friendliness (or whatever it is we want)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6812">[ date ]</a>
<a href="index.html#6812">[ thread ]</a>
<a href="subject.html#6812">[ subject ]</a>
<a href="author.html#6812">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
