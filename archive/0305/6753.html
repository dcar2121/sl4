<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Bill Hibbard (test@demedici.ssec.wisc.edu)">
<meta name="Subject" content="Re: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI's flawed friendliness analysis</h1>
<!-- received="Tue May 20 15:19:39 2003" -->
<!-- isoreceived="20030520211939" -->
<!-- sent="Tue, 20 May 2003 16:19:29 -0500 (CDT)" -->
<!-- isosent="20030520211929" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@demedici.ssec.wisc.edu" -->
<!-- subject="Re: SIAI's flawed friendliness analysis" -->
<!-- id="Pine.GSO.4.44.0305201618460.15294-100000@demedici.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3EC827F0.4070700@posthuman.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis"><em>test@demedici.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Tue May 20 2003 - 15:19:29 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6754.html">Bill Hibbard: "Re: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>Previous message:</strong> <a href="6752.html">Bill Hibbard: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6730.html">Brian Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6758.html">Brian Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6758.html">Brian Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6753">[ date ]</a>
<a href="index.html#6753">[ thread ]</a>
<a href="subject.html#6753">[ subject ]</a>
<a href="author.html#6753">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Sun, 18 May 2003, Brian Atkins wrote:
<br>
<p><em>&gt; Bill Hibbard wrote:
</em><br>
<em>&gt; &gt; On Sat, 17 May 2003, Brian Atkins wrote:
</em><br>
<em>&gt; &gt;&gt;Bill Hibbard wrote:
</em><br>
<em>&gt; &gt;&gt;&gt;The danger of outlaws will increase as the technology for
</em><br>
<em>&gt; &gt;&gt;&gt;intelligent artifacts becomes easier. But as time passes we
</em><br>
<em>&gt; &gt;&gt;&gt;will also have the help of safe AIs to help detect and
</em><br>
<em>&gt; &gt;&gt;&gt;inspect other AIs.
</em><br>
<em>&gt; &gt;&gt;&gt;
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt;Even in such fictional books as Neuromancer, we see that such Turing
</em><br>
<em>&gt; &gt;&gt;Police do not function well enough to stop a determined superior
</em><br>
<em>&gt; &gt;&gt;intelligence. Realistically, such a police force will only have any real
</em><br>
<em>&gt; &gt;&gt;chance of success at all if we have a very transparent society... it
</em><br>
<em>&gt; &gt;&gt;would require societal changes on a very grand scale, and not just in
</em><br>
<em>&gt; &gt;&gt;one country. It all seems rather unlikely... I think we need to focus on
</em><br>
<em>&gt; &gt;&gt;solutions that have a chance at actual implementation.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I never said that safe AI is a sure thing. It will require
</em><br>
<em>&gt; &gt; a broad political movement that is successful in electoral
</em><br>
<em>&gt; &gt; politics. It will require whatever commitment and resources
</em><br>
<em>&gt; &gt; are needed to regulate AIs. It will require the patience to
</em><br>
<em>&gt; &gt; not rush.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Bill, I'll just come out and state my opinion that what you are
</em><br>
<em>&gt; describing is a pipe dream. I see no way that the things you speak of
</em><br>
<em>&gt; have any chance of happening within the next few decades. Governments
</em><br>
<em>&gt; won't even spend money on properly tracking potential asteroid threats,
</em><br>
<em>&gt; and you honestly believe they will commit to the VAST amount of both
</em><br>
<em>&gt; political willpower and real world resource expenditures required to
</em><br>
<em>&gt; implement an AI detection and inspection system that has even a low
</em><br>
<em>&gt; percentage shot at actually accomplishing anything?
</em><br>
<em>&gt;
</em><br>
<em>&gt; And that is not even getting into the fact that by your design the &quot;good
</em><br>
<em>&gt; AIs&quot; will be crippled by only allowing them very slow intelligence/power
</em><br>
<em>&gt; increases due to the massive stifling human-speed
</em><br>
<em>&gt; design/inspection/control regime... they will have zero chance to
</em><br>
<em>&gt; scale/keep up as computing power further spreads and enables vastly more
</em><br>
<em>&gt; powerful uncontrolled UFAIs to begin popping up. The result is seemingly
</em><br>
<em>&gt; a virtual guarantee that eventually an UFAI will get out of control (as
</em><br>
<em>&gt; you state, your plan is not a &quot;sure thing&quot;) and easily &quot;win&quot; over the
</em><br>
<em>&gt; regulated other AIs in existence. So what does it accomplish in the end,
</em><br>
<em>&gt; other than eliminating any chance that a &quot;regulated AI&quot; could &quot;win&quot;?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Finally, how does your human-centric regulation and design system cope
</em><br>
<em>&gt; with AIs that need to grow to be smarter than human? Are you proposing
</em><br>
<em>&gt; to simply keep them limited indefinitely to this level of intelligence,
</em><br>
<em>&gt; or will the &quot;trusted&quot; AIs themselves eventually take over the process of
</em><br>
<em>&gt; writing design specs and inspecting each other?
</em><br>
<p>If humans can design AIs smarter than humans, then humans
<br>
can regulate AIs smarter than humans. It is not necessary
<br>
to trace an AI's thoughts in detail, just to understand
<br>
the mechanisms of its thoughts. Furthermore, once trusted
<br>
AIs are available, they can take over the details of
<br>
design and regulation. I would trust an AI with
<br>
reinforcement values for human happiness more than I
<br>
would trust any individual human.
<br>
<p>This is a bit like the experience of people who write
<br>
game playing programs that they cannot beat. All the
<br>
programmer needs to know is that the logic for
<br>
simulating the game and for reinforcement learning are
<br>
accurate and efficient, and that the reinforcement
<br>
values are for winning the game.
<br>
<p>You say &quot;by your design the 'good AIs' will be crippled
<br>
by only allowing them very slow intelligence/power
<br>
increases due to the massive stifling human-speed&quot;. But
<br>
once we have trusted AIs, they can take over the details
<br>
of designing and regulating other AIs. The real crippling
<br>
effect will be the inability of developers of unregulated
<br>
AIs to come out in the open for resources. Cooperating
<br>
corporations and government will have much larger
<br>
resources available to them for developing regulated AIs.
<br>
Don't misinterpret this: I am not saying that it is sure
<br>
to succeed (nothing is). But it is much better to use
<br>
the force of law and resources of government to help
<br>
solve the problem.
<br>
<p><em>&gt; &gt; By pointing out all these difficulties you are helping
</em><br>
<em>&gt; &gt; me make my case about the flaws in the SIAI friendliness
</em><br>
<em>&gt; &gt; analysis, which simply dismisses the importance of
</em><br>
<em>&gt; &gt; politics and regulation in eliminating unsafe AI.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is a rather nonsensical mantra... everyone is pointing out the
</em><br>
<em>&gt; obvious flaws in your system- this does not help your idea that politics
</em><br>
<em>&gt; and regulation are important pieces to the solution of this problem.
</em><br>
<em>&gt; Tip: drop the mantras, and actually come up with some plausible answers
</em><br>
<em>&gt; to the objections being raised.
</em><br>
<p>Calling this a &quot;nonsensical mantra&quot; does not answer it.
<br>
The objections are just possible ways that a political
<br>
solution may fail. Of course it may fail. But its the
<br>
best chance of success. It really comes down to who you
<br>
trust. I favor a broad political process because I trust
<br>
the general public more than any individual or small
<br>
group. Of course, democratic goverement does enlist the
<br>
help of experts on technical questions, but ultimate
<br>
authority is with the public.
<br>
<p><em>&gt; SIAI's analysis, as already explained by Eliezer, is not attempting at
</em><br>
<em>&gt; all to completely eliminate the possibility of UFAI. As he said, we
</em><br>
<em>&gt; don't expect to be able to have any control over someone who sets out to
</em><br>
<em>&gt; deliberately construct such an UFAI, and we admit this reality rather
</em><br>
<em>&gt; than attempt to concoct world-spanning pipe dreams.
</em><br>
<p>Powerful people and institutions will try to manipulate
<br>
the singularity to preserve and enhance their interests.
<br>
Any strategy for safe AI must try to counter this threat.
<br>
<p><em>&gt; P.S. You completely missed my point on the nanotech... I was suggesting
</em><br>
<em>&gt; a smart enough UFAI could develop in secret some working nanotech long
</em><br>
<em>&gt; before humans have even figured out how to do such things. There would
</em><br>
<em>&gt; be no human nanotech defense system. Or, even if you believe that the
</em><br>
<em>&gt; sequence of technology development will give humans molecular nanotech
</em><br>
<em>&gt; before AI, my point still stands that a smart enough UFAI will ALWAYS be
</em><br>
<em>&gt; able to do something that we have not prepared for. The only way to
</em><br>
<em>&gt; defend against a malevolent superior intelligence in the wild is to be
</em><br>
<em>&gt; (or have working for you) yourself an even more superior intelligence.
</em><br>
<p>I didn't miss your point. I accepted that nanotech is a big
<br>
threat, along with genetic engineering of micro-organisms.
<br>
I added that nanotech will be a threat with or without AI.
<br>
The way to counter the threat of micro-organisms has been
<br>
detection networks, isolation of affected people and
<br>
regions, and urgent efforts to analyze the organisms and
<br>
find counter measures. There are also efforts to monitor
<br>
the humans with the knowledge to create new micro-organisms.
<br>
These measures all have the force of law and the resources
<br>
of government behind them. Similar measures will apply to
<br>
the threat of nanotech. When safe AIs are available, they
<br>
will certainly be enlisted to help. With such huge threats
<br>
as nanotech the pipe dream is to think that they can be
<br>
countered without the force of law and the resources of
<br>
government. Or to think that government won't get involved.
<br>
<p>It really comes down to who you trust. I favor a broad
<br>
political process because I trust the general public more
<br>
than any individual or small group. Of course, democratic
<br>
goverement does enlist the help of experts of technical
<br>
questions, but ultimate authority is with the public.
<br>
<p>----------------------------------------------------------
<br>
Bill Hibbard, SSEC, 1225 W. Dayton St., Madison, WI  53706
<br>
<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis">test@demedici.ssec.wisc.edu</a>  608-263-4427  fax: 608-263-6738
<br>
<a href="http://www.ssec.wisc.edu/~billh/vis.html">http://www.ssec.wisc.edu/~billh/vis.html</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6754.html">Bill Hibbard: "Re: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>Previous message:</strong> <a href="6752.html">Bill Hibbard: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6730.html">Brian Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6758.html">Brian Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6758.html">Brian Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6753">[ date ]</a>
<a href="index.html#6753">[ thread ]</a>
<a href="subject.html#6753">[ subject ]</a>
<a href="author.html#6753">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
