<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Philip Sutton (Philip.Sutton@green-innovations.asn.au)">
<meta name="Subject" content="RE: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-24">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: SIAI's flawed friendliness analysis</h1>
<!-- received="Sat May 24 08:43:31 2003" -->
<!-- isoreceived="20030524144331" -->
<!-- sent="Sun, 25 May 2003 00:47:28 +1000" -->
<!-- isosent="20030524144728" -->
<!-- name="Philip Sutton" -->
<!-- email="Philip.Sutton@green-innovations.asn.au" -->
<!-- subject="RE: SIAI's flawed friendliness analysis" -->
<!-- id="3ED012A0.3855.7FA70B@localhost" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="LAEGJLOGJIOELPNIOOAJKECAFAAA.ben@goertzel.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Philip Sutton (<a href="mailto:Philip.Sutton@green-innovations.asn.au?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis"><em>Philip.Sutton@green-innovations.asn.au</em></a>)<br>
<strong>Date:</strong> Sat May 24 2003 - 08:47:28 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6812.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6810.html">Philip Sutton: "Friendly AGIs will need to be friendly to more than humans"</a>
<li><strong>In reply to:</strong> <a href="6809.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6812.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6812.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6811">[ date ]</a>
<a href="index.html#6811">[ thread ]</a>
<a href="subject.html#6811">[ subject ]</a>
<a href="author.html#6811">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Ben said:
<br>
<p><em>&gt; the most key point is the division between
</em><br>
<em>&gt; 
</em><br>
<em>&gt; -- those who believe a hard takeoff is reasonably likely, based on a
</em><br>
<em>&gt; radical insight in AI design coupled with a favorable trajectory of
</em><br>
<em>&gt; self-improvemetn of a particular AI system
</em><br>
<em>&gt; 
</em><br>
<em>&gt; -- those who believe in a soft takeoff, in which true AI is approached
</em><br>
<em>&gt; gradually [in which case government regulation, careful peer review and so
</em><br>
<em>&gt; forth are potentially relevant]
</em><br>
<em>&gt; 
</em><br>
<em>&gt; My own projection is a semi-hard takeoff, which doesn't really bring much
</em><br>
<em>&gt; reassurance...
</em><br>
<p>OK.  Let's look at the semi-hard take off scenario.  I assume that what 
<br>
you mean is that for a period after the creation of a baby-AGI the 
<br>
humans around it will have to do a lot of work to build it up to a 
<br>
reasonable level of competence in the real world (lots of training/lots of 
<br>
new code development/lots of hand holding) But at some stage all this 
<br>
hard work will come together and the AGI (or a group of AGIs) will be 
<br>
able to drive it's/their own self-improvement without much input from 
<br>
humans.  At that point we will get a hard take-off.  Ben, have I 
<br>
interpreted your views accurately?
<br>
<p>If this is a reasonable summary, then it seems to me that we have to 
<br>
have a very, very reliable guess as to when to expect the transition to 
<br>
take-off to begin.  (By the way it's like the birth process - which has 
<br>
three phases - pre-transition, transition and then post-transition. In the 
<br>
pre-transition phase there's lots of pushing, pushing, pushing but not 
<br>
much moving. The pre-transition period has a fairly regular rhythm to it.  
<br>
In post-transition the baby shoots down the birth canal (all being well) 
<br>
very fast - like a wet bar of soap firing out of a squeezing fist!) But 
<br>
transition is a very strange, uncomfortable phase where the old rhythm 
<br>
of contraction becomes very irregular but the new pattern of post 
<br>
transition fast movement hasn't taken hold yet. If this analogy has any 
<br>
real value for understanding AGI development then AGIs (as a class) 
<br>
might be considered to be born twice - once as non-self improving 
<br>
intelligences and then again as self-improving intellgences [post take-
<br>
off].)
<br>
<p>And it seems to me that all AGI-development projects need to ensure 
<br>
that they have introduced powerful life-compassion capability into their 
<br>
AGIs *before* the hard take off can possibly begin.
<br>
<p>My understanding of things is that SIAI feels that we cannot know when 
<br>
we are one the safe side of take-off so friendliness work should be done 
<br>
now.  Ben on the other hand (I think) thinks that it would be 100% safe 
<br>
to have an early-model baby AGI in existence before much work was 
<br>
done on introducing life-compassion.
<br>
<p>Whether Ben is right I think depends on whether the lead time to go 
<br>
from an early-model baby AGI to the point of hard take-off is longer 
<br>
than the lead-time for AGI development teams and/or society to go 
<br>
from a vague idea of what we want in the way of AGI morality to the 
<br>
point where we can introduce it tangibly and securely into real AGIs.
<br>
<p>My own feeling is that there are lots of issues about what sort of 
<br>
morality we think AGIs should have that are not hardware/software 
<br>
dependent and that have, most likley, a longer leadtime than the early-
<br>
model baby AGI to the point of hard take-off leadtime.
<br>
<p>If that's so then we need to redouble the effort on AGI morality.  The 
<br>
Singularity Institute has done very valuable work in the area which 
<br>
needs to be developed further.  But I think there are aspects of the AGI 
<br>
morality issue that the Institute itself hasn't even flagged.
<br>
<p>It would be quite interesting to conduct some sort of collaborative 
<br>
scoping exercise to identify what issues different people think we need 
<br>
to look at. If we could produce a single document that had all the big 
<br>
issues that each one of us thinks should be considered in the course of 
<br>
tackling AGI morality then we might be able to avoid talking past each 
<br>
other and from this document we might be able to generate an R&amp;D 
<br>
agenda - moving in several direction at once as I don't anticipate that 
<br>
we will all be of one mind..
<br>
<p>What do you think of this suggestion?
<br>
<p>I don't have a lot of time to put into this but I do have *some* time to 
<br>
contribute.
<br>
<p>Cheers, Philip
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6812.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6810.html">Philip Sutton: "Friendly AGIs will need to be friendly to more than humans"</a>
<li><strong>In reply to:</strong> <a href="6809.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6812.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6812.html">Ben Goertzel: "RE: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6811">[ date ]</a>
<a href="index.html#6811">[ thread ]</a>
<a href="subject.html#6811">[ subject ]</a>
<a href="author.html#6811">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
