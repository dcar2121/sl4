<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Bill Hibbard (test@demedici.ssec.wisc.edu)">
<meta name="Subject" content="Re: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-11">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI's flawed friendliness analysis</h1>
<!-- received="Sun May 11 17:15:25 2003" -->
<!-- isoreceived="20030511231525" -->
<!-- sent="Sun, 11 May 2003 18:14:37 -0500 (CDT)" -->
<!-- isosent="20030511231437" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@demedici.ssec.wisc.edu" -->
<!-- subject="Re: SIAI's flawed friendliness analysis" -->
<!-- id="Pine.GSO.4.44.0305111811040.2416-100000@demedici.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3EBC7CA3.8000801@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis"><em>test@demedici.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Sun May 11 2003 - 17:14:37 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6682.html">Cliff Stabbert: "Re: Why FAI Theory is both Necessary and Hard (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>Previous message:</strong> <a href="6680.html">Christian Rovner: "RE: Why FAI Theory is both Necessary and Hard (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>In reply to:</strong> <a href="6654.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6695.html">Durant Schoon: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6695.html">Durant Schoon: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6681">[ date ]</a>
<a href="index.html#6681">[ thread ]</a>
<a href="subject.html#6681">[ subject ]</a>
<a href="author.html#6681">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Sat, 10 May 2003, Eliezer S. Yudkowsky wrote:
<br>
<p><em>&gt; Bill Hibbard wrote:
</em><br>
<em>&gt; &gt; This critique refers to the following documents:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;   GUIDELINES:  <a href="http://www.intelligence.org/friendly/guidelines.html">http://www.intelligence.org/friendly/guidelines.html</a>
</em><br>
<em>&gt; &gt;   FEATURES:    <a href="http://www.intelligence.org/friendly/features.html">http://www.intelligence.org/friendly/features.html</a>
</em><br>
<em>&gt; &gt;   CFAI:        <a href="http://www.intelligence.org/CFAI/index.html">http://www.intelligence.org/CFAI/index.html</a>
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; 1. The SIAI analysis fails to recognize the importance of
</em><br>
<em>&gt; &gt; the political process in creating safe AI.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; This is a fundamental error in the SIAI analysis. CFAI 4.2.1
</em><br>
<em>&gt; &gt; says &quot;If an effort to get Congress to enforce any set of
</em><br>
<em>&gt; &gt; regulations were launched, I would expect the final set of
</em><br>
<em>&gt; &gt; regulations adopted to be completely unworkable.&quot; It further
</em><br>
<em>&gt; &gt; says that government regulation of AI is unnecessary because
</em><br>
<em>&gt; &gt; &quot;The existing force tending to ensure Friendliness is that
</em><br>
<em>&gt; &gt; the most advanced projects will have the brightest AI
</em><br>
<em>&gt; &gt; researchers, who are most likely to be able to handle the
</em><br>
<em>&gt; &gt; problems of Friendly AI.&quot; History vividly teaches the danger
</em><br>
<em>&gt; &gt; of trusting the good intentions of individuals.
</em><br>
<em>&gt;
</em><br>
<em>&gt; ...and, of course, the good intentions and competence of governments.
</em><br>
<p>Absolutely. I never claimed that AI safety is a sure thing.
<br>
But without a broad political movement for safe AI and its
<br>
success in elective democratic government, unsafe AI is a
<br>
sure thing.
<br>
<p><em>&gt; . . .
</em><br>
<em>&gt; Your political recommendations appear to be based on an extremely
</em><br>
<em>&gt; different model of AI.  Specifically:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1)  &quot;AIs&quot; are just very powerful tools that amplify the short-term goals
</em><br>
<em>&gt; of their users, like any other technology.
</em><br>
<p>I never said &quot;short-term&quot; - you are putting words into my mouth.
<br>
A key property of intelligence is understanding the long-term
<br>
effects of behavior on satisfying values (goals).
<br>
<p><em>&gt; 2)  AIs have power proportional to the computing resources invested in
</em><br>
<em>&gt; them, and everyone has access to pretty much the same theoretical model
</em><br>
<em>&gt; and class of AI.
</em><br>
<p>AI power does depend on computing resources and efficiency of
<br>
algorithms. Important algorithms have proved impossible to keep
<br>
secret for any length of time. Whether or not this continues in
<br>
the future, essential algorithms for intelligence will not be
<br>
secret from powerful organizations.
<br>
<p><em>&gt; 3)  There is no seed AI, no rapid recursive self-improvement, no hard
</em><br>
<em>&gt; takeoff, no &quot;first&quot; AI.  AIs are just new forces in existing society,
</em><br>
<em>&gt; coming into play a bit at a time, as everyone's AI technology improves at
</em><br>
<em>&gt; roughly the same rate.
</em><br>
<p>Where did this come from? I am very clear in my book about the
<br>
importance of proper training for young AIs, and the issues
<br>
involved in AI evolution.
<br>
<p><em>&gt; 4)  Anyone can make an AI that does anything.  AI morality is an easy
</em><br>
<em>&gt; problem with fully specifiable arbitrary solutions that are reliable and
</em><br>
<em>&gt; humanly comprehensible.
</em><br>
<p>I never said building safe AI is easy.
<br>
<p><em>&gt; 5)  Government workers can look at an AI design and tell what the AI's
</em><br>
<em>&gt; morality does and whether it's safe.
</em><br>
<p>We certainly expect government workers regulate nuclear energy
<br>
designs and operation to ensure their safety. And because of
<br>
their doubts about safety, people in the US have decided through
<br>
their democratic political process to stop building new nuclear
<br>
energy plants.
<br>
<p>Nowhere do I claim that regulation of safe AI will be simple.
<br>
But if we don't have government workers implementing regulation
<br>
of AI under democratic political control, then we will have
<br>
unsafe AIs.
<br>
<p><em>&gt; 6)  There are variables whose different values correlate to socially
</em><br>
<em>&gt; important differences in outcomes, such that government workers can
</em><br>
<em>&gt; understand the variables and their correlation to the outcomes, and such
</em><br>
<em>&gt; that society expects to have a conflict of interest with individuals or
</em><br>
<em>&gt; organizations as to the values of those variables, with the value to
</em><br>
<em>&gt; society of this conflict of interest exceeding the value to society of the
</em><br>
<em>&gt; outcome differentials that depend on the greater competence of those
</em><br>
<em>&gt; individuals or organizations.  Otherwise there's nothing worth voting on.
</em><br>
<p>There will be organizations with motives to build AIs
<br>
with values that will correlate with important social
<br>
differences in outcome. AIs with values to maximize
<br>
profits may end up empowering their owners at everyone
<br>
else's expense. AIs with values for military victory
<br>
may end up killing lots of people.
<br>
<p><em>&gt; I disagree with all six points, due to a different model of AI.
</em><br>
<p>I think we do have different models of AI. I think an AI
<br>
is an information process that has some values that it
<br>
tries to satisfy (positive values) and avoid (negative
<br>
values). It does this via reinforcement learning and a
<br>
simulation model of the world that it uses to solve the
<br>
credit assignment problem (i.e., to understand the long
<br>
term consequences of its behaviors on its values). Of
<br>
course, actually doing this in general circumstances is
<br>
very difficult, requiring pattern recognition to greatly
<br>
reduce the volume of sensory information, and the
<br>
equivalent to human conscious thought to reflect on
<br>
situations and find analogies.
<br>
<p>The SIAI guidelines involve digging into the AI's
<br>
reflective thought process and controlling the AI's
<br>
thoughts, in order to ensure safety. My book says the
<br>
only concern for AI learning and reasoning is to ensure
<br>
they are accurate, and that the teachers of young AIs
<br>
be well-adjusted people (subject to public monitoring
<br>
and the same kind of screening used for people who
<br>
control major weapons). Beyond that, the proper domain
<br>
for ensuring AI safety is the AI's values rather than
<br>
the AI's reflective thought processes.
<br>
<p>In my second and third points I described the lack of
<br>
rigorous standards for certain terms in the SIAI
<br>
Guidelines and for initial AI values. Those rigorous
<br>
standards can only come from the AI's values. I think
<br>
that in your AI model you feel the need to control how
<br>
they are derived via the AI's reflective thought
<br>
process. This is the wrong domain for addressing AI
<br>
safety.
<br>
<p>Clear and unambiguous initial values are elaborated
<br>
in the learning process, forming connections via the
<br>
AI's simulation model with many other values. Human
<br>
babies love their mothers based on simple values about
<br>
touch, warmth, milk, smiles and sounds (happy Mother's
<br>
Day). But as the baby's mind learns, those simple
<br>
values get connected to a rich set of values about the
<br>
mother, via a simulation model of the mother and
<br>
surroundings. This elaboration of simple values will
<br>
happen in any truly intelligent AI.
<br>
<p>I think initial AI values should be for simple
<br>
measures of human happiness. As the AI develops these
<br>
will be elaborated into a model of long-term human
<br>
happiness, and connected to many derived values about
<br>
what makes humans happy generally and particularly.
<br>
The subtle point is that this links AI values with
<br>
human values, and enables AI values to evolve as human
<br>
values evolve. We do see a gradual evolution of human
<br>
values, and the singularity will accelerate it.
<br>
<p>Morality has its roots in values, especially social
<br>
values for shared interests. Complex moral systems
<br>
are elaborations of such values via learning and
<br>
reasoning. The right place to control an AI's moral
<br>
system is in its values. All we can do for an AI's
<br>
learning and reasoning is make sure they are accurate
<br>
and efficient.
<br>
<p><em>&gt; . . .
</em><br>
<em>&gt; &gt; 3. CFAI defines &quot;friendliness&quot; in a way that can only
</em><br>
<em>&gt; &gt; be determined by an AI after it has developed super-
</em><br>
<em>&gt; &gt; intelligence, and fails to define rigorous standards
</em><br>
<em>&gt; &gt; for the values that guide its learning until it reaches
</em><br>
<em>&gt; &gt; super-intelligence
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The actual definition of &quot;friendliness&quot; in CFAI 3.4.4
</em><br>
<em>&gt; &gt; requires the AI to know most humans sufficiently well
</em><br>
<em>&gt; &gt; to decompose their minds into &quot;panhuman&quot;, &quot;gaussian&quot; and
</em><br>
<em>&gt; &gt; &quot;personality&quot; layers, and to &quot;converge to normative
</em><br>
<em>&gt; &gt; altruism&quot; based on collective content of the &quot;panhuman&quot;
</em><br>
<em>&gt; &gt; and &quot;gaussian&quot; layers. This will require the development
</em><br>
<em>&gt; &gt; of super-intelligence over a large amount of learning.
</em><br>
<em>&gt; &gt; The definition of friendliness values to reinforce that
</em><br>
<em>&gt; &gt; learning is left to &quot;programmers&quot;. As in the previous
</em><br>
<em>&gt; &gt; point, this will allow wealthy organizations to define
</em><br>
<em>&gt; &gt; intial learning values for their AIs as they like.
</em><br>
<em>&gt;
</em><br>
<em>&gt; I don't believe a young Friendly AI should be meddling in the real world
</em><br>
<em>&gt; at all.  If for some reason this becomes necessary, it might as well do
</em><br>
<em>&gt; what the programmer says, maybe with its own humane veto.  I'd trust a
</em><br>
<em>&gt; programmer more than I'd trust an infant Friendly AI, because regardless
</em><br>
<em>&gt; of its long-term purpose, during infancy the FAI is likely to have neither
</em><br>
<em>&gt; a better approximation to humaneness, nor a better understanding of the
</em><br>
<em>&gt; real world.
</em><br>
<p>I agree that young AIs should have limited access to senses
<br>
and actions. But in order to &quot;converge to normative altruism&quot;
<br>
based on collective content of the &quot;panhuman&quot; and &quot;gaussian&quot;
<br>
layers as described in CFAI 3.4.4, the AI is going to need
<br>
access to large numbers of humans.
<br>
<p><em>&gt; . . .
</em><br>
<em>&gt; &gt; 4. The CFAI analysis is based on a Bayesian reasoning
</em><br>
<em>&gt; &gt; model of intelligence, which is not a sufficient model
</em><br>
<em>&gt; &gt; for producing intelligence.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; While Bayesian reasoning has an important role in
</em><br>
<em>&gt; &gt; intelligence, it is not sufficient. Sensory experience
</em><br>
<em>&gt; &gt; and reinforcement learning are fundamental to
</em><br>
<em>&gt; &gt; intelligence. Just as symbols must be grounded in
</em><br>
<em>&gt; &gt; sensory experience, reasoning must be grounded in
</em><br>
<em>&gt; &gt; learning and emerges from it because of the need to
</em><br>
<em>&gt; &gt; solve the credit assignment problem, as discussed at:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt;   <a href="http://www.mail-archive.com/<a href="mailto:agi@v2.listbox.com?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis">agi@v2.listbox.com</a>/msg00390.html">http://www.mail-archive.com/<a href="mailto:agi@v2.listbox.com?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis">agi@v2.listbox.com</a>/msg00390.html</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; Non-Bayesian?  I don't think you're going to find much backing on this
</em><br>
<em>&gt; one.  If you've really discovered a non-Bayesian form of reasoning, write
</em><br>
<em>&gt; it up and collect your everlasting fame.  Personally I consider such a
</em><br>
<em>&gt; thing almost exactly analogous to a perpetual motion machine.  Except that
</em><br>
<em>&gt; a perpetual motion machine is merely physically impossible, while
</em><br>
<em>&gt; &quot;non-Bayesian reasoning&quot; appears to be mathematically impossible.  Though
</em><br>
<em>&gt; of course I could be wrong.
</em><br>
<p>I never said &quot;Non-Bayesian&quot;, although I find Pei's and Ben's
<br>
examples of Non-Bayesian logic in their systems interesting.
<br>
<p>What I really meant by my fourth point is that because your
<br>
model of intelligence is incomplete, there are things in your
<br>
model of friendliness that really belong in your model of
<br>
intelligence. For example, recommendation 5 from GUIDELINES 3
<br>
&quot;requires that the AI model the causal process that led to
<br>
the AI's creation and that the AI use its existing cognitive
<br>
complexity (or programmer assistance) to make judgements
<br>
about the validity or invalidity of factors in that causal
<br>
process.&quot; Any sufficiently intelligent brain will have a
<br>
simulation model of the world that includes the events that
<br>
led to its creation, and will make value judgements about
<br>
those events. The failure to do so would be a failure of
<br>
intelligence rather than a failure of safety.
<br>
<p>I think this confusion between model of intelligence and
<br>
model of safety leads to the difficulty of finding rigorous
<br>
standards for terms described in my second point, and the
<br>
difficulty of finding initial values described in my third
<br>
point.
<br>
<p><em>&gt; Reinforcement learning emerges from Bayesian reasoning, not the other way
</em><br>
<em>&gt; around.  Sensory experience likewise.
</em><br>
<em>&gt;
</em><br>
<em>&gt; For more about Bayesian reasoning, see:
</em><br>
<em>&gt;   <a href="http://yudkowsky.net/bayes/bayes.html">http://yudkowsky.net/bayes/bayes.html</a>
</em><br>
<em>&gt;   <a href="http://bayes.wustl.edu/etj/science.pdf.html">http://bayes.wustl.edu/etj/science.pdf.html</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; Reinforcement, specifically, emerges in a Bayesian decision system:
</em><br>
<em>&gt;   <a href="http://intelligence.org/CFAI/design/clean.html#reinforcement">http://intelligence.org/CFAI/design/clean.html#reinforcement</a>
</em><br>
<p>This describes a Bayesian mechanism for reinforcement learning,
<br>
but does not show that reinforcement learning emerges from
<br>
Bayesian reasoning. In fact, learning precedes reasoning in
<br>
brain evolution. Reasoning (i.e., a simulation model of the
<br>
world) evolved to solve the credit assignment problem of
<br>
learning.
<br>
<p>----------------------------------------------------------
<br>
Bill Hibbard, SSEC, 1225 W. Dayton St., Madison, WI  53706
<br>
<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis">test@demedici.ssec.wisc.edu</a>  608-263-4427  fax: 608-263-6738
<br>
<a href="http://www.ssec.wisc.edu/~billh/vis.html">http://www.ssec.wisc.edu/~billh/vis.html</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6682.html">Cliff Stabbert: "Re: Why FAI Theory is both Necessary and Hard (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>Previous message:</strong> <a href="6680.html">Christian Rovner: "RE: Why FAI Theory is both Necessary and Hard (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>In reply to:</strong> <a href="6654.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6695.html">Durant Schoon: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6695.html">Durant Schoon: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6681">[ date ]</a>
<a href="index.html#6681">[ thread ]</a>
<a href="subject.html#6681">[ subject ]</a>
<a href="author.html#6681">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
