<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Why FAI Theory is both Necessary and Hard (was Re: SIAI's flawed friendliness analysis)</title>
<meta name="Author" content="Cliff Stabbert (cstabbert@the-beach.net)">
<meta name="Subject" content="Re: Why FAI Theory is both Necessary and Hard (was Re: SIAI's flawed friendliness analysis)">
<meta name="Date" content="2003-05-11">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Why FAI Theory is both Necessary and Hard (was Re: SIAI's flawed friendliness analysis)</h1>
<!-- received="Sun May 11 07:57:58 2003" -->
<!-- isoreceived="20030511135758" -->
<!-- sent="Sun, 11 May 2003 09:58:18 -0400" -->
<!-- isosent="20030511135818" -->
<!-- name="Cliff Stabbert" -->
<!-- email="cstabbert@the-beach.net" -->
<!-- subject="Re: Why FAI Theory is both Necessary and Hard (was Re: SIAI's flawed friendliness analysis)" -->
<!-- id="1372438967.20030511095818@the-beach.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3EBC7CA3.8000801@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Cliff Stabbert (<a href="mailto:cstabbert@the-beach.net?Subject=Re:%20Why%20FAI%20Theory%20is%20both%20Necessary%20and%20Hard%20(was%20Re:%20SIAI's%20flawed%20friendliness%20analysis)"><em>cstabbert@the-beach.net</em></a>)<br>
<strong>Date:</strong> Sun May 11 2003 - 07:58:18 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6674.html">Ben Goertzel: "RE: Why FAI Theory is both Necessary and Hard (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>Previous message:</strong> <a href="6672.html">Aleksei Riikonen: "Luddism vs. singularitarianism"</a>
<li><strong>In reply to:</strong> <a href="6654.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6674.html">Ben Goertzel: "RE: Why FAI Theory is both Necessary and Hard (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>Reply:</strong> <a href="6674.html">Ben Goertzel: "RE: Why FAI Theory is both Necessary and Hard (was Re: SIAI's flawed friendliness analysis)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6673">[ date ]</a>
<a href="index.html#6673">[ thread ]</a>
<a href="subject.html#6673">[ subject ]</a>
<a href="author.html#6673">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer wrote:
<br>
<p>ESY&gt; The human species is fifty thousand years old.  We've come a long
<br>
ESY&gt; way during that time.  We've come a long way during the last
<br>
ESY&gt; century.  At the start of the twentieth century, neither women
<br>
ESY&gt; nor blacks could vote in the US.  The twentienth century was, by
<br>
ESY&gt; the standards of today's world, barbaric.  Are we, ourselves,
<br>
ESY&gt; raving barbarians by any future perspective?  Almost certainly.
<br>
ESY&gt; For one civilization to leave a permanent impress on the next
<br>
ESY&gt; billion years is no more fair than for one individual to do so,
<br>
ESY&gt; and just as deadly.  Individuals grow in their moralities, as do
<br>
ESY&gt; civilizations.  That fundamental capability is what needs to be
<br>
ESY&gt; transferred into a Friendly AI, not a morality frozen in time.  A
<br>
ESY&gt; human is capable of understanding the concept of moral
<br>
ESY&gt; improvement; a true FAI, a real mind in the humane frame of
<br>
ESY&gt; reference, must be able to do the same, or something vital is
<br>
ESY&gt; missing.
<br>
<p>ESY&gt; If you want to make an FAI that is capable of moral improvement,
<br>
ESY&gt; constructed via a fair, species-representative method, then the
<br>
ESY&gt; only question is whether you have the knowledge to do that - to
<br>
ESY&gt; build an AI that fully understands the concept of moral failure
<br>
ESY&gt; and moral improvement as well as we do ourselves, and that
<br>
ESY&gt; symbolizes a species rather than any one individual or
<br>
ESY&gt; civilization.
<br>
<p>ESY&gt; If you commit to that standard, then there are no conflicts of
<br>
ESY&gt; interest to fight over, no individually controllable variables
<br>
ESY&gt; that knowably correlate with the outcome in a way that creates
<br>
ESY&gt; conflicts of interest. 
<br>
<p>ESY&gt; What is dangerous is that someone who believes that &quot;AIs just do
<br>
ESY&gt; what they're told&quot; will think that the big issue is who gets to
<br>
ESY&gt; tell AIs what to do.  Such people will not, of course, succeed in
<br>
ESY&gt; taking over the world; I find it extremely implausible that any
<br>
ESY&gt; human expressing such a goal has the depth of understanding
<br>
ESY&gt; required to build anything that is not a thermostat AI.  The
<br>
ESY&gt; problem is that these people might succeed in destroying the
<br>
ESY&gt; world, given enough computing power to brute-force AI with no
<br>
ESY&gt; real understanding of it.
<br>
<p>ESY&gt; Anyone fighting over what values the AI ought to have is simply
<br>
ESY&gt; fighting over who gets to commit suicide. If you know how to give
<br>
ESY&gt; an AI any set of values, you know how to give it a humanly
<br>
ESY&gt; representative set of values. It is really not that hard to come
<br>
ESY&gt; up with fair strategies for any given model of FAI.  Coming up
<br>
ESY&gt; with an FAI model that works is very hard.
<br>
<p>ESY&gt; It is not a trivial thing, to create a mind that embodies the
<br>
ESY&gt; full human understanding of morality.  There is a high and
<br>
ESY&gt; beautiful sorcery to it. I find it hard to believe that any human
<br>
ESY&gt; truly capable of learning and understanding that art would use it
<br>
ESY&gt; to do something so small and mean. And anyone else would be
<br>
ESY&gt; effectively committing suicide, whether they realized it or not,
<br>
ESY&gt; because the AI would not hear what they thought they had said. 
<br>
<p>I think these six paragraphs could be productively mined for a
<br>
superb &quot;Why We Need FAI Theory&quot; / &quot;Why FAI is Hard&quot; intro, something
<br>
I've long felt was missing from SIAI's materials.
<br>
<p>Not to say I agree with every detail of the above -- I think Ben
<br>
raises some valid questions re para 4.  For me, this sentence in para
<br>
6 jumped out: 
<br>
<p>ESY&gt; I find it hard to believe that any human truly capable of
<br>
ESY&gt; learning and understanding that art would use it to do something
<br>
ESY&gt; so small and mean.
<br>
<p>You &quot;find it hard to believe&quot;?  _I_ find it hard to believe you would
<br>
use such a phrase casually or unintentionally, without awareness
<br>
of the implications.  *Especially* when such a statement is made about
<br>
a probability of the form P(small &amp; mean | capable)...
<br>
<p><pre>
--
Cliff
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6674.html">Ben Goertzel: "RE: Why FAI Theory is both Necessary and Hard (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>Previous message:</strong> <a href="6672.html">Aleksei Riikonen: "Luddism vs. singularitarianism"</a>
<li><strong>In reply to:</strong> <a href="6654.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6674.html">Ben Goertzel: "RE: Why FAI Theory is both Necessary and Hard (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>Reply:</strong> <a href="6674.html">Ben Goertzel: "RE: Why FAI Theory is both Necessary and Hard (was Re: SIAI's flawed friendliness analysis)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6673">[ date ]</a>
<a href="index.html#6673">[ thread ]</a>
<a href="subject.html#6673">[ subject ]</a>
<a href="author.html#6673">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
