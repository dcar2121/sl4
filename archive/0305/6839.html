<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Bill Hibbard (test@demedici.ssec.wisc.edu)">
<meta name="Subject" content="Re: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-29">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI's flawed friendliness analysis</h1>
<!-- received="Thu May 29 09:39:19 2003" -->
<!-- isoreceived="20030529153919" -->
<!-- sent="Thu, 29 May 2003 10:38:26 -0500 (CDT)" -->
<!-- isosent="20030529153826" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@demedici.ssec.wisc.edu" -->
<!-- subject="Re: SIAI's flawed friendliness analysis" -->
<!-- id="Pine.GSO.4.44.0305291037360.2056-100000@demedici.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3ED29971.4030904@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis"><em>test@demedici.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Thu May 29 2003 - 09:38:26 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6840.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6838.html">Ben Goertzel: "RE: The Bayesian philosophy of probability"</a>
<li><strong>In reply to:</strong> <a href="6823.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6840.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6840.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="../0306/6910.html">Samantha Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6839">[ date ]</a>
<a href="index.html#6839">[ thread ]</a>
<a href="subject.html#6839">[ subject ]</a>
<a href="author.html#6839">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Mon, 26 May 2003, Eliezer S. Yudkowsky wrote:
<br>
<p><em>&gt; Bill, I've had conversations similar to these before.  I'll give the
</em><br>
<em>&gt; challenge that has so far defeated every single proponent of AI regulation:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &quot;Name one specific regulation, in enough detail to make it enforceable,
</em><br>
<em>&gt; that you believe would improve the chances of a safe Singularity if the
</em><br>
<em>&gt; government attempted to enforce it.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; It is easy enough to call for &quot;regulation&quot;.  I have never yet heard anyone
</em><br>
<em>&gt; call for some specific regulation.
</em><br>
<p>My argument for regulation is based on the high probability
<br>
of unsafe AI without regulation, rather than any confidence
<br>
that I have all the answers about how to regulate. I have
<br>
no practical experience with politics, regulation, security
<br>
or law enforcement, and so my ideas on this would certainly
<br>
need to be refined by professionals.
<br>
<p>Nevertheless, its an interesting question and I'll try to
<br>
answer it. I think the answer divides into two parts: the
<br>
regulation itself, and how to enforce it.
<br>
<p>1. The regulation.
<br>
<p>Here's my initial crack at it.
<br>
<p>&nbsp;&nbsp;Any artifact implementing &quot;learning&quot; and capable of at
<br>
&nbsp;&nbsp;least N mathematical operations per second must have &quot;human
<br>
&nbsp;&nbsp;happiness&quot; as its only initial reinforcement value. Here
<br>
&nbsp;&nbsp;&quot;learning&quot; means that system responses to inputs change
<br>
&nbsp;&nbsp;over time, and &quot;human happiness&quot; values are produced by an
<br>
&nbsp;&nbsp;algorithm produced by supervised learning, to recognize
<br>
&nbsp;&nbsp;happiness in human facial expressions, voices and body
<br>
&nbsp;&nbsp;language, as trained by human behavior experts.
<br>
<p>Since this is so much shorter than most government
<br>
regulations, I suspect that a real regulation, produced
<br>
after input from many experts, would be much longer.
<br>
<p>The N mathematical operations per second number is picked
<br>
to be high enough to allow non-intelligent applications like
<br>
weather prediction (actually, most weather models don't learn
<br>
and so would be exempt from the N limit), and low enough to
<br>
exclude intelligence significantly greater than human
<br>
intelligence. Based on the opinions of various experts, a
<br>
guess at the value of N might be 10^15. There may be
<br>
&quot;mundane&quot; (i.e., no danger they will become intelligent)
<br>
learning applications that need more than N operations per
<br>
second, that can get case-by-case exemptions (with inspection
<br>
to verify how they are being used).
<br>
<p>As with any law, disputes would be settled before a court
<br>
with judges, lawyers representing both parties, and expert
<br>
witnesses.
<br>
<p>2. How the regulation can be enforced.
<br>
<p>Enforcement is a hard problem. It helps that enforcement is
<br>
not necessary indefinitely. It is only necessary until the
<br>
singularity, at which time it becomes the worry of the
<br>
(hopefully safe) singularity AIs. There is a spectrum of
<br>
possible approaches of varying strictness. I'll describe
<br>
two:
<br>
<p>a. A strict approach.
<br>
<p>Disallow all development of &quot;learning&quot; machines capable of
<br>
at least N operations per second, except for a government
<br>
safe AI project (and exempt &quot;mundane&quot; learning applications).
<br>
This would be something like the Manhattan Project (only the
<br>
government is allowed to build nuclear weapons, although
<br>
contractors are involved).
<br>
<p>The project could include people working for the government
<br>
and for private corporations. There could be multiple competing
<br>
designs in the project (e.g. &quot;Fat Man&quot; and &quot;Little Boy&quot;). The
<br>
project would have huge resources, which would have the side
<br>
effect of attracting talented AI designers away from the
<br>
temptation of outlaw AI projects. All designs would be
<br>
inspected and reviewed for compliance with the regulation,
<br>
overseen by the National Academies of Engineering and
<br>
Science.
<br>
<p>The focus for detecting illegal projects could be on computing
<br>
resources and on expert designers. Computing chips are widely
<br>
available, but chip factories aren't. There is already talk of
<br>
using the concentration of ownership of chip manufacturing to
<br>
implant copyright protection in every chip. Its called TCPA
<br>
and I'm against it - see my article at:
<br>
<p>&nbsp;&nbsp;<a href="http://www.ssec.wisc.edu/~billh/roads.html">http://www.ssec.wisc.edu/~billh/roads.html</a>
<br>
<p>Something very much like TCPA could be implanted in every chip
<br>
over a certain power (N/M where M = 1000 or 10000), to detect
<br>
when they are being used in sufficiently large clusters on
<br>
tightly coupled problems, and cease to operate unless they
<br>
have an inspection certificate.
<br>
<p>Another tool of strict enforcement could be to prohibit open
<br>
sales of chips with power greater than N/M. Chips with greater
<br>
than this power would only be available to certified inspected
<br>
server centers. The primary need for computing power close to
<br>
users is visuals and sound. Chips at 10^12 operations per
<br>
second (just about where the current technology driving Moore's
<br>
Law is predicted to run out) should be plenty for these needs,
<br>
especially in small clusters (anything less than M would be
<br>
legal). Otherwise the trend is to put most computing power in
<br>
central server sites anyways, so restricting the most powerful
<br>
chips to secure central sites should not distort the computing
<br>
world too much (I don't pretend there would be no distortion).
<br>
<p>Illegal projects could also be detected through their need for
<br>
expert designers. As long as the police are not corrupt or lazy
<br>
(hence the need for an aggressive public movement driving
<br>
aggressive enforcement), they can develop and exploit informers
<br>
among any outlaw community. Its hard to do an ambitous project
<br>
like creating AI without a lot of people knowing something
<br>
about it. They are vulnerable to bribes, and they get into
<br>
feuds and turn each other in.
<br>
<p>Although we all love to root for the little garage-shop
<br>
operations, the overwhelming probability is that machine
<br>
intelligence will first appear in facilities that look like
<br>
this (4x10^13 operations per second):
<br>
<p>&nbsp;&nbsp;<a href="http://www.es.jamstec.go.jp/esc/eng/GC/b_photo/esc11.jpg">http://www.es.jamstec.go.jp/esc/eng/GC/b_photo/esc11.jpg</a>
<br>
<p>Such projects are detectable by the enormous resources they
<br>
consume and the numbers of people involved.
<br>
<p>Internationally, there could be treaties analogous to those
<br>
controlling certain types of weapons. These would prohibit
<br>
military use of learning machines capable of more than N
<br>
operations per second, and would set up international bodies
<br>
analogous to the IAEA for coordinating regulation and
<br>
inspection.
<br>
<p>b. A less strict approach.
<br>
<p>This would be like the strict approach, except that safe AI
<br>
projects outside the government could be licensed, in
<br>
addition to the government project. These projects would
<br>
have inspectors embedded in their design teams. The burden
<br>
of proof would be on the designers to convince the
<br>
inspectors that their designs comply with the regulation.
<br>
As with the government project, all designs would be
<br>
reviewed for compliance with the regulation, overseen by
<br>
the National Academies of Engineering and Science.
<br>
<p>3. Wild cards.
<br>
<p>There are all sorts of wild cards that could change the
<br>
scenario for regulation considerably:
<br>
<p>a. Some new technology, such as quantum computing, enables
<br>
anyone with $100 million to fabricate computing devices
<br>
capable of 10^30 operations per second.
<br>
<p>b. Novamente (just to pick an AI project) demonstrates
<br>
human-level intelligence using just 10^11 operations per
<br>
second.
<br>
<p>c. Saddam Hussein uses his 4 semi loads of $100 bills to
<br>
buy a million Playstation 2's and hire AI design geniuses
<br>
to create an unsafe singularity in a remote province of
<br>
Kazakstan.
<br>
<p>There is no way to come up with a regulation plan that will
<br>
meet every contingency. The government games out a lot of
<br>
contingencies in issues it cares about, which is a lot of
<br>
work and usually fails to anticipate what really happens.
<br>
In any issue as complex as the singularity, it is
<br>
inevitable that strategy must be adaptable.
<br>
<p>The other thing to realize is that a lot of scenarios for
<br>
the singularity could result in violent human conflict.
<br>
If an AI grows fast but does not instantly eliminate human
<br>
governments, then the public may be frightened and the
<br>
governments may react defensively in a sort of &quot;national
<br>
security war over AI&quot;. It is impossible to game all these
<br>
scenarios out, but the important point is that some pretty
<br>
bad scenarios are possible. Which leads to my last point ...
<br>
<p>4. The consent of the governed.
<br>
<p>AI and the singularity will be so much better if the public
<br>
is informed and is in control via their elected governments.
<br>
It is human nature for people to resist changes that are
<br>
forced on them. If we respect humanity enough to want a safe
<br>
singularity for them, then we should also respect them
<br>
enough to get the public involved and consenting to what is
<br>
happening.
<br>
<p>Whether or not you think my regulation ideas can work, my
<br>
basic point is that the singularity will be created by some
<br>
wealthy and powerful institution, and its values will reflect
<br>
the values of the institution. The only chance for a safe
<br>
singularity will be if that institution is democratic
<br>
government under the control of an aggressive public movement
<br>
for safe AI, similar to the consumer, environmental and
<br>
social justice movements.
<br>
<p>----------------------------------------------------------
<br>
Bill Hibbard, SSEC, 1225 W. Dayton St., Madison, WI  53706
<br>
<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis">test@demedici.ssec.wisc.edu</a>  608-263-4427  fax: 608-263-6738
<br>
<a href="http://www.ssec.wisc.edu/~billh/vis.html">http://www.ssec.wisc.edu/~billh/vis.html</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6840.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6838.html">Ben Goertzel: "RE: The Bayesian philosophy of probability"</a>
<li><strong>In reply to:</strong> <a href="6823.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6840.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6840.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="../0306/6910.html">Samantha Atkins: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6839">[ date ]</a>
<a href="index.html#6839">[ thread ]</a>
<a href="subject.html#6839">[ subject ]</a>
<a href="author.html#6839">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
