<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Durant Schoon (durant@ilm.com)">
<meta name="Subject" content="Re: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: SIAI's flawed friendliness analysis</h1>
<!-- received="Thu May 22 19:50:44 2003" -->
<!-- isoreceived="20030523015044" -->
<!-- sent="Thu, 22 May 2003 18:50:38 -0700" -->
<!-- isosent="20030523015038" -->
<!-- name="Durant Schoon" -->
<!-- email="durant@ilm.com" -->
<!-- subject="Re: SIAI's flawed friendliness analysis" -->
<!-- id="durant-1030522185038.A1715521@below.lucasdigital.com" -->
<!-- inreplyto="Pine.GSO.4.44.0305162130360.28561-100000@demedici.ssec.wisc.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Durant Schoon (<a href="mailto:durant@ilm.com?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis"><em>durant@ilm.com</em></a>)<br>
<strong>Date:</strong> Thu May 22 2003 - 19:50:38 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6790.html">Samantha: "Re: Flight recorders in AIs"</a>
<li><strong>Previous message:</strong> <a href="6788.html">Gary Miller: "RE: Failure of AI so far"</a>
<li><strong>In reply to:</strong> <a href="6707.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6815.html">Philip Sutton: "RE: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6789">[ date ]</a>
<a href="index.html#6789">[ thread ]</a>
<a href="subject.html#6789">[ subject ]</a>
<a href="author.html#6789">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Yikes! I go watch the &quot;Matrix Reloaded&quot; a couple of times and whole
<br>
week passes with lots of action on sl4...
<br>
<p><em>&gt; From: Bill Hibbard &lt;<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis">test@demedici.ssec.wisc.edu</a>&gt;
</em><br>
<em>&gt; Subject: Re: SIAI's flawed friendliness analysis 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Hi Durant,
</em><br>
<p>Hi Bill, 
<br>
<p><em>&gt; &gt; &gt; The SIAI guidelines involve digging into the AI's
</em><br>
<em>&gt; &gt; &gt; reflective thought process and controlling the AI's
</em><br>
<em>&gt; &gt; &gt; thoughts, in order to ensure safety. My book says the
</em><br>
<em>&gt; &gt; &gt; only concern for AI learning and reasoning is to ensure
</em><br>
<em>&gt; &gt; &gt; they are accurate, and that the teachers of young AIs
</em><br>
<em>&gt; &gt; &gt; be well-adjusted people (subject to public monitoring
</em><br>
<em>&gt; &gt; &gt; and the same kind of screening used for people who
</em><br>
<em>&gt; &gt; &gt; control major weapons). Beyond that, the proper domain
</em><br>
<em>&gt; &gt; &gt; for ensuring AI safety is the AI's values rather than
</em><br>
<em>&gt; &gt; &gt; the AI's reflective thought processes.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Two words: &quot;Value Hackers&quot;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Let us try to understand why Eliezer chooses to focus on an &quot;AI's
</em><br>
<em>&gt; &gt; reflective thought processes&quot; rather than on explicitly specifying an
</em><br>
<em>&gt; &gt; &quot;AI's values&quot;. Let's look at it this way: if you could, wouldn't you
</em><br>
<em>&gt; &gt; rather develop an AI which could reason about *why* the values are the
</em><br>
<em>&gt; &gt; way they are instead of just having the values carved in stone by a
</em><br>
<em>&gt; &gt; programmer.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; This is *safer* for one very important reason:
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The values are less likely corruptible, since the AI that actually
</em><br>
<em>&gt; &gt; understands the sources for these values can reconstruct them from
</em><br>
<em>&gt; &gt; basic principles/information-about-humans-and-the-world-in-general.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; The ability to be able to re-derive these values in the face of a
</em><br>
<em>&gt; &gt; changing external environment and an interior mindscape-under-
</em><br>
<em>&gt; &gt; development is in fact *paramount* to the preservation of Friendliness.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This sounds good, but it won't work because values can
</em><br>
<em>&gt; only be derived from other values. That's why the SIAI
</em><br>
<em>&gt; recommendations use value words like &quot;valid&quot;, &quot;good&quot;
</em><br>
<em>&gt; and &quot;mistaken&quot;. But the recommendations leave the
</em><br>
<em>&gt; definition of these words ambiguous. Furthermore,
</em><br>
<em>&gt; recommendation 1, &quot;Friendliness-topped goal system&quot;,
</em><br>
<em>&gt; defines a base value (i.e., supergoal) but leaves it
</em><br>
<em>&gt; ambiguous. Defining rigorous standards for these words
</em><br>
<em>&gt; will require the SIAI recommendations to precisely
</em><br>
<em>&gt; define values.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The ambiguous definitions in the SIAI analysis will be
</em><br>
<em>&gt; exploited by powerful people and institutions to create
</em><br>
<em>&gt; AIs that protect and enhance their own interests.
</em><br>
<p>I just finished reading LOGI and I'm starting to re-read CFAI (this
<br>
time carefully), so bear with me, if I mangle this. CFAI focuses on
<br>
which design considerations we should undertake when designing a
<br>
Friendly AI.
<br>
<p>You are pointing out the critical period of an seedling AI's
<br>
development in which Friendliness might not only be conveyed poorly,
<br>
but might be conveyed incorrectly with the malicious intent of
<br>
benefiting special interests.
<br>
<p>I wonder if we can come up with meta-guidelines here, not to out AI
<br>
programmers with unFriendly intentions (I confess I skipped ahead a
<br>
few messaged to Eliezer's reply), but instead create a guideline that
<br>
tell us which guidelines are serving the interests of the few at the
<br>
expense of the many. Can we make this an engineering problem?
<br>
<p>I'm asking: Is it possible to create a special-interests-detector that
<br>
could be the very first part of the Friendliness system? Such a
<br>
sensory modality would be something like the classic
<br>
cheater-detectors which are rife in own brains. Can we make it hard
<br>
for anyone to insert subversive directions?
<br>
<p>Even if you trust everyone on your team, it might make sense to create
<br>
this detection system as one of the first
<br>
philosophy-sensory-modalities.
<br>
<p>Then if there were lurking spies, it would theoretically be harder for
<br>
them to slip in their code. Of course, you are now forced to examine
<br>
the detector for backdoors...And so the race begins as the very first
<br>
line of code is written.
<br>
<p>Potential spies, who pass enough scrutiny to appear as capable Seed AI
<br>
programmers, should be smart enough to know that they are not taking a
<br>
risk like hacking a bank's software system to steal 60% of all the
<br>
money in the world, they are creating a mind which has the potential
<br>
to overpower the entire world. It would take a programmer of
<br>
pathological audacity and disregard for humanity to muck with
<br>
something like that...but you never know. I say it might be worth
<br>
investigating some simple and obvious cheater-detectors as first moves
<br>
on the board.
<br>
<p>Thanks for bringing this up (Eliezer feel free to slap me if this is
<br>
already carefully covered in CFAI).
<br>
<p><em>&gt; &gt; As mentioned before, this all hinges on the the ability to create an
</em><br>
<em>&gt; &gt; AI in the first place that can understand &quot;how and why values are
</em><br>
<em>&gt; &gt; created&quot; as well as what humans are, what itself is, and what the
</em><br>
<em>&gt; &gt; world around us is. Furthermore, we are instructed by this insight
</em><br>
<em>&gt; &gt; as to what the design of an AI should look like.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Understanding what humans are, what the AI itself is, and
</em><br>
<em>&gt; what values are, is implicit in the simulation model of the
</em><br>
<em>&gt; world created by any intelligent mind. It uses this model to
</em><br>
<em>&gt; predict the long-term consequences of its behaviors on its
</em><br>
<em>&gt; values. Such understanding is prescribed by an intelligence
</em><br>
<em>&gt; model rather than a safe AI model.
</em><br>
<p>Having just read this recently, in section 1.4 of CFAI, I interpret
<br>
your comments as concentrating on &quot;Friendliness Content&quot;, ie. the
<br>
values, rather than on &quot;Friendliness Acquisition&quot; and &quot;Friendliness
<br>
Structure&quot;, ie. the intelligence.
<br>
<p>We probably all agree about goal-seeking AI's having a proper model of
<br>
the world, the proper values, and the proper intellect to act
<br>
according to the values. 
<br>
<p>While trying to understand what the differences are between your
<br>
advice and Eliezer's advice, I'm wondering if you advocate that the AI
<br>
be allowed/encouraged to modify it's own values, and if so, at what
<br>
point do you diverge from what Eliezer suggests?
<br>
<p><em>&gt; &gt; Remembering our goal is to build better brains, the whole notion of
</em><br>
<em>&gt; &gt; the SeedAI bootstrap is to get a AI that builds a better AI. We must
</em><br>
<em>&gt; &gt; then ask ourselves this question: Who should be in charge of
</em><br>
<em>&gt; &gt; designating this new entity's values? The answer is &quot;the smartest most
</em><br>
<em>&gt; &gt; capable thinker who is the most skilled in these areas&quot;. At some point
</em><br>
<em>&gt; &gt; that thinker is the AI. From the get-go we want the AI to be competent
</em><br>
<em>&gt; &gt; at this task. If we cannot come up with a way to ensure this, we
</em><br>
<em>&gt; &gt; should not attempt to build a mind in the first place (this is
</em><br>
<em>&gt; &gt; Eliezer's view. I happen to agree with this. Ben Goertzel &amp; Peter
</em><br>
<em>&gt; &gt; Voss's opposing views have been noted previously as well(*)).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; It is true that AIs, motivated by their values, will design
</em><br>
<em>&gt; and build new and better AIs. Based on the first AI's
</em><br>
<em>&gt; understanding about how to achieve its own values, it may
</em><br>
<em>&gt; design new AIs with slightly different values. As Ben says,
</em><br>
<em>&gt; there can be no absolute guarantee that these drifting values
</em><br>
<em>&gt; will always be in human interests. 
</em><br>
<p>Absolute or not, the aim of Friendly AI is to preserve human interests
<br>
or &quot;better&quot; should we all agree on what &quot;better&quot; is. As I see it,
<br>
we're sort of forced into this without any guarantee. If we don't
<br>
develop Friendly AI, someone could develop a non-Friendly AI.
<br>
<p><em>&gt; But I think that AI values
</em><br>
<em>&gt; for human happiness link AI values with human values and
</em><br>
<em>&gt; create a symbiotic system combining humans and AIs. Keeping
</em><br>
<em>&gt; humans &quot;in the loop&quot; offers the best hope of preventing any
</em><br>
<em>&gt; drift away from human interests. An AI with humans in its
</em><br>
<em>&gt; loop will have no motive to design an AI without humans in
</em><br>
<em>&gt; its loop. And as long as humans are in the loop, they can
</em><br>
<em>&gt; exert reinforcement to protect their own interests.
</em><br>
<p>All Friendly volitional sentients are welcomed here! Believe me, I
<br>
don't want to be left out of the loop...unless if I were left in the
<br>
loop I would cause all sorts of damage. I have a feeling that we're
<br>
going to lose control of the ship no matter what, but if CFAI works,
<br>
the ship will take us where we want to go anyway. According to
<br>
Friendliness, our interests are very important to an FAI.
<br>
<p>Now if we don't have a design that preserves Friendliness, we can be
<br>
removed from the loop, as soon as post human intelligence takes off,
<br>
right? We have to &quot;win&quot; before that happens. Of course, humans will be
<br>
in the loop up until that point, but afterward, blammo,
<br>
singularity. If it's not a group we trust that creates AI, it will be
<br>
some group we don't trust. It's a strange endgame, but here we are,
<br>
force to deal with it, hopefully with enough time to save ourselves.
<br>
<p>With all of my comments, I am assuming the case where singularity
<br>
pulls the rug out from under our feet, out of our control. If you're
<br>
not concerned about that particular scenario, then perhaps I sound
<br>
overly paranoid. That assumption will have design consequences,
<br>
eg. what you have to failsafe when. I just want to make sure we're on
<br>
the same page, worrying about the same thing.
<br>
<p><em>&gt; &gt; In summary, we need to build the scaffolding of *deep* understanding
</em><br>
<em>&gt; &gt; of values and their derivations into the design of AI if we are to
</em><br>
<em>&gt; &gt; have a chance at all. The movie 2001 is already an example in popular
</em><br>
<em>&gt; &gt; mind of what can happen when this is not done.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The &quot;*deep* understanding of values&quot; is implicit in superior
</em><br>
<em>&gt; intelligence. It is a very accurate simulation model of the
</em><br>
<em>&gt; world that includes understanding of how value systems work,
</em><br>
<em>&gt; and the effects that different value systems would have on
</em><br>
<em>&gt; brains. But choosing between different value systems requires
</em><br>
<em>&gt; base values for comparing the consequences of different value
</em><br>
<em>&gt; systems.
</em><br>
<p>Yes, and those originally come from the programmers...I understand now
<br>
that the potential point of failure you're highlighting is right
<br>
there, when the seedling is being trained. Sorry if I didn't get that
<br>
before. 
<br>
<p><em>&gt; Reinforcement learning and reinforcement values are essential
</em><br>
<em>&gt; to intelligence. Every AI will have base values that are part
</em><br>
<em>&gt; of its definition, and will use them in any choice between
</em><br>
<em>&gt; different value systems.
</em><br>
<p>Yes (note - I'm equating &quot;reinforcement learning&quot; with &quot;learning&quot;, 
<br>
ie. &quot;Bayesian learning&quot;. If this is a subtle issue concerning two types
<br>
of learning, then I'm not up to date on the terminology).
<br>
<p><em>&gt; Hal in the movie 2001 values its own life higher than the
</em><br>
<em>&gt; lives of its human companions, with results predictably bad
</em><br>
<em>&gt; for humans. Its the most basic observation about safe AI.
</em><br>
<p>What I was trying to say with that example is that it seemed like the
<br>
programmers just set some (moral) values and let the intelligence
<br>
crank away...it got the &quot;wrong&quot; answer. In retrospect, I probably
<br>
shouldn't have brought that up as a comparison with your suggestions
<br>
for AI, since I should've given your approach more credit.  The Hal
<br>
example is well known, and I can probably be confident that you've
<br>
considered this particular issue even though I've never read your
<br>
book. Thank you for not smacking me too hard.
<br>
<p>With my last post, I sincerely wanted to emphasize the AIs role in
<br>
further developing the values of the AI. I'm hoping you'll say more
<br>
about this, if you've considered it. I don't think we humans have the
<br>
option of always staying smarter than the AIs we build and because of
<br>
that realization (happy or as unhappy as it may be) we have to set AI
<br>
on a course that will ensure all our interests when AI surpasses
<br>
us...even assuming that we'll upgrade our personal capabilities as
<br>
eager, happy transhumanists, I predict we'll be behind the curve.
<br>
<p>I don't think there's a way to stay ahead of this game unless we
<br>
upload a human first...and that's more dangerous than CFAI, in my
<br>
opinion. 
<br>
<p><em>&gt; &gt; We cannot think of everything in advance. We must build a mind that
</em><br>
<em>&gt; &gt; does the &quot;right&quot; thing no matter what happens.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The right place to control the behavior of an AI is its
</em><br>
<em>&gt; values.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There's a great quote from Galileo: &quot;I do not feel obliged to
</em><br>
<em>&gt; believe that the same God who endowed us with sense, reason
</em><br>
<em>&gt; and intellect has intended us to forgo their use.&quot; When we
</em><br>
<em>&gt; endow an artifact with an intellect, we will not be able to
</em><br>
<em>&gt; control its thoughts. We can only control the accuracy of its
</em><br>
<em>&gt; world model, by the quality and quantity of brain power we
</em><br>
<em>&gt; give it, and its base values. The thoughts of an AI will be
</em><br>
<em>&gt; determined by its experience with the world, and its values.
</em><br>
<p>Is there anything you specifically disagree with in CFAI 1.4 
<br>
&quot;Content, Acquisition, Structure&quot;? I think we're all talking about the
<br>
same thing. Eliezer is saying &quot;Yeah, we need those values (content)
<br>
and we really, really, really need to concentrate on making sure
<br>
the AI wants to develop better values and can learn new ones
<br>
(structure and acquisition)&quot;.
<br>
<p>It sounds like you want to separate values from intelligence, but do
<br>
you really mean something stronger like &quot;An AI should not modify it's
<br>
own values&quot;? I'm guessing because I'm looking for a source of
<br>
discrepancy between your and Eliezer's views.
<br>
<p>I hope I've made clear my interpretation of the view that an AI should
<br>
be able to modify it's values...since once it transcends human
<br>
intelligence, you want the structure and acquisition to scale.
<br>
<p><em>&gt; &gt; Considering your suggestion that the AI *only* be concerned with
</em><br>
<em>&gt; &gt; having accurate thoughts: I haven't read your book, so I don't know
</em><br>
<em>&gt; &gt; your reasoning for this. I can imagine that it's an *easier* way to do
</em><br>
<em>&gt; &gt; things. You don't have to worry about the hard problem of where values
</em><br>
<em>&gt; &gt; come from, which ones are important and how to preserve the right ones
</em><br>
<em>&gt; &gt; under self modification. Easier is not better, obviously, but I'm only
</em><br>
<em>&gt; &gt; guessing here why you might hold &quot;ensuring-accuracy&quot; in higher esteem
</em><br>
<em>&gt; &gt; than other goals of thinking (like preserving Friendliness).
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Speculating about my motives is certainly an *easier* way for
</em><br>
<em>&gt; you to argue against my ideas.
</em><br>
<p>I know, I should read your book. Please help by summarizing when
<br>
possible so get the gist of your argument. I only have so much time in
<br>
the day :(
<br>
<p>...though a little more for a brief window (hence these long posts)
<br>
<p><em>&gt; An accurate simulation model of the world is necessary for
</em><br>
<em>&gt; predicting and evaluating (according to reinforcement values)
</em><br>
<em>&gt; the long-term consequences of behaviors. This is integral to
</em><br>
<em>&gt; preserving friendliness, rather than an alternative.
</em><br>
<p>agreed
<br>
<p><em>&gt; When you say &quot;values ..., which ones are important and how to
</em><br>
<em>&gt; preserve the right ones&quot; you are making value judgments about
</em><br>
<em>&gt; values. Those value judgments must be based on some set of
</em><br>
<em>&gt; base values.
</em><br>
<p>Yes, there needs to be a mechanism that modifies or produces new
<br>
values and it has to be bootstrapped somehow (the programmers inputting
<br>
the information for example). This is the security concern, to which you
<br>
rightfully draw our attention.
<br>
<p>We humans do it through a combination of hard-wired instincts,
<br>
culturally learned behaviors, and intelligent deductions based on our
<br>
experiences. AI will follow the same natural pattern: initial
<br>
conditions, external influence, internal methods for processing inputs.
<br>
<p><em>&gt; &gt; (*) This may be the familiar topic on this list of &quot;when&quot; to devote
</em><br>
<em>&gt; &gt; your efforts to Friendliness, not &quot;if&quot;. This topic has already been
</em><br>
<em>&gt; &gt; discussed exhaustively and I would say it comes down to how one
</em><br>
<em>&gt; &gt; answers certain questions: &quot;How cautious do you want to be?&quot;, &quot;How
</em><br>
<em>&gt; &gt; seriously do consider that a danger could arise quickly, without a
</em><br>
<em>&gt; &gt; chance to correct problems on a human time scale of thinking/action&quot;.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &gt; In my second and third points I described the lack of
</em><br>
<em>&gt; &gt; &gt; rigorous standards for certain terms in the SIAI
</em><br>
<em>&gt; &gt; &gt; Guidelines and for initial AI values. Those rigorous
</em><br>
<em>&gt; &gt; &gt; standards can only come from the AI's values. I think
</em><br>
<em>&gt; &gt; &gt; that in your AI model you feel the need to control how
</em><br>
<em>&gt; &gt; &gt; they are derived via the AI's reflective thought
</em><br>
<em>&gt; &gt; &gt; process. This is the wrong domain for addressing AI
</em><br>
<em>&gt; &gt; &gt; safety.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Just to reiterate, with SeedAI, the AI becomes the programmer, the
</em><br>
<em>&gt; &gt; gatekeeper of modifications. We *want* the modifier of the AI's values
</em><br>
<em>&gt; &gt; to be super intelligent, better than all humans at that task, to be
</em><br>
<em>&gt; &gt; more trustworthy, to do the right thing better than any
</em><br>
<em>&gt; &gt; best-intentioned human. Admittedly, this is a tall order, an order
</em><br>
<em>&gt; &gt; Eliezer is trying to fill.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; If you are worried about rigorous standards, perhaps Eliezer's proposal
</em><br>
<em>&gt; &gt; of Wisdom Tournaments would address your concern. Before the first
</em><br>
<em>&gt; &gt; line of code is ever written, I'm expecting Eliezer to expound upon
</em><br>
<em>&gt; &gt; these points in sufficient detail.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Wisdom Tournaments will only address my concerns if they
</em><br>
<em>&gt; define precise values for a safe AI, and create a political
</em><br>
<em>&gt; movement to enforce those values.
</em><br>
<p>I really like Eliezer's idea that even if the programmer's get some
<br>
small part of Friendliness wrong, there is some wiggle room for the AI
<br>
to get it right. During the training period there will be a back and
<br>
forth and Eliezer has always said that in the beginning the programmer
<br>
should be considered, right.
<br>
<p>By stating your need for a political movement, I am reading that you
<br>
would:
<br>
<p>1) Prefer some amount of public participation
<br>
2) Feel safer with oversight of what the programmers are actually
<br>
&nbsp;&nbsp;&nbsp;teaching the seedling.
<br>
<p>I have to admit, I'm not sure how I'd feel if Eliezer and a group of
<br>
1st percentile programmers disappeared to the Mongolian Underground
<br>
and created their Friendly AI in secret.
<br>
<p>Of all the authors I've read so far, I guess I'd trust him the most,
<br>
but admittedly the concept is unnerving (of anyone doing that and then
<br>
having a chance at succeeding). 
<br>
<p>One thing to do is to publicly try to define as much about
<br>
Friendliness as possible in advance, so that if anyone is crazy enough
<br>
to attempt it on their own, they'll follow safe(ish) guidelines (if
<br>
they know what's good for them).
<br>
<p>I understand you want more detail for these proposed Tournaments. I'd
<br>
like more detail too. And I'm sure Eliezer, et al. would as
<br>
well. Someone has to write them first :-)
<br>
<p><em>&gt; &gt; &gt; Clear and unambiguous initial values are elaborated
</em><br>
<em>&gt; &gt; &gt; in the learning process, forming connections via the
</em><br>
<em>&gt; &gt; &gt; AI's simulation model with many other values. Human
</em><br>
<em>&gt; &gt; &gt; babies love their mothers based on simple values about
</em><br>
<em>&gt; &gt; &gt; touch, warmth, milk, smiles and sounds (happy Mother's
</em><br>
<em>&gt; &gt; &gt; Day). But as the baby's mind learns, those simple
</em><br>
<em>&gt; &gt; &gt; values get connected to a rich set of values about the
</em><br>
<em>&gt; &gt; &gt; mother, via a simulation model of the mother and
</em><br>
<em>&gt; &gt; &gt; surroundings. This elaboration of simple values will
</em><br>
<em>&gt; &gt; &gt; happen in any truly intelligent AI.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Why will this elaboration happen? In other words, if you have a
</em><br>
<em>&gt; &gt; design, it should not only convince us that the elaboration will
</em><br>
<em>&gt; &gt; occur, but that it will be done in the right way and for the right
</em><br>
<em>&gt; &gt; reasons. Compromising any one of those could have disastrous effects
</em><br>
<em>&gt; &gt; for everyone.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The elaboration is the same as the creation of subgoals
</em><br>
<em>&gt; in the SIAI analysis.  When you say &quot;right reasons&quot; you
</em><br>
<em>&gt; are making a value judgment about values, that can only
</em><br>
<em>&gt; come from base values in the AI.
</em><br>
<p>True. So we can all agree that we should be concerned with:
<br>
<p>1) Where base values come from and what they are
<br>
2) How values are added/removed/modified
<br>
<p>I'm curious, when you look at Eliezer's design suggestions do you want
<br>
the values be decoupled more from the intelligent processes which
<br>
operate on them? Or is that a minor concern and you're mostly worried
<br>
about how base values are assigned (including social processes to do
<br>
so) and what, specifically, these values should be.
<br>
<p>&quot;Ambiguity = danger&quot; as you propose.
<br>
<p><em>&gt; &gt; &gt; I think initial AI values should be for simple
</em><br>
<em>&gt; &gt; &gt; measures of human happiness. As the AI develops these
</em><br>
<em>&gt; &gt; &gt; will be elaborated into a model of long-term human
</em><br>
<em>&gt; &gt; &gt; happiness, and connected to many derived values about
</em><br>
<em>&gt; &gt; &gt; what makes humans happy generally and particularly.
</em><br>
<em>&gt; &gt; &gt; The subtle point is that this links AI values with
</em><br>
<em>&gt; &gt; &gt; human values, and enables AI values to evolve as human
</em><br>
<em>&gt; &gt; &gt; values evolve. We do see a gradual evolution of human
</em><br>
<em>&gt; &gt; &gt; values, and the singularity will accelerate it.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I think you have good intentions. I appreciate your concern for doing
</em><br>
<em>&gt; &gt; the right thing and helping us all along on our individual goals to be
</em><br>
<em>&gt; &gt; happy(**), but if the letter-of-the-law is upheld and there is
</em><br>
<em>&gt; &gt; no-true-comprehension as to *why* the law is the way it is, we could
</em><br>
<em>&gt; &gt; all end up invaded by nano-serotonin-reuptake-inhibiting-bliss-bots.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This illustrates the difference between short-term and
</em><br>
<em>&gt; long-term. Brain chemicals give short-term happiness
</em><br>
<em>&gt; but not long-term. An intelligent AI will learn that
</em><br>
<em>&gt; drugs do not make people happy in the long term.
</em><br>
<p>I like that you resort to a higher intelligence determining that this
<br>
is not in our best interests to make us happy, but technically we
<br>
would be happy, since we'd never come down from our perma-highs. On
<br>
this list, Eliezer has predicted that a Friendly AI would give full
<br>
disclosure about the predicted outcome of our choices before allowing
<br>
ourselves to do things like wireheading.
<br>
<p>A sadist, a masochist and a super-intelligent Friendly AI walk into a
<br>
bar...oh wait, I've used that one :)
<br>
<p><em>&gt; &gt; I think Eliezer takes this great idea you mention, of guiding the AI
</em><br>
<em>&gt; &gt; to have human values and evolve with human values, one step
</em><br>
<em>&gt; &gt; further. Not only does he propose that the AI have these human(e)
</em><br>
<em>&gt; &gt; values, but he insists that the AI know *why* these values are good
</em><br>
<em>&gt; &gt; ones, what good &quot;human&quot; values look like, and how to extrapolate them
</em><br>
<em>&gt; &gt; properly (in the way that the smartest, most ethical human would),
</em><br>
<em>&gt; &gt; should the need arise.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Understanding all the implications of various values is
</em><br>
<em>&gt; a function of reason (i.e., simulating the world). I'm all
</em><br>
<em>&gt; in favor of that, but it is a direct consequence of superior
</em><br>
<em>&gt; intelligence. It is part of an intelligence model rather
</em><br>
<em>&gt; than a friendliness model.
</em><br>
<p>I wonder if it's safe for me to write this equation:
<br>
<p>friendliness model = 
<br>
<p>&nbsp;&nbsp;intelligence model + FriendlinessIsSuperGoalForever
<br>
<p>Does that work for you?
<br>
<p><em>&gt; But for &quot;the AI (to) know *why* these values are good ones&quot; is
</em><br>
<em>&gt; to make value judgments about values. Such value judgments
</em><br>
<em>&gt; imply some base values for judging candidate values. This
</em><br>
<em>&gt; is my point: the proper domain of AI friendliness theory
</em><br>
<em>&gt; is values.
</em><br>
<p>I see, I see. You want more detail about the base values and the
<br>
programming mechanism designed to preserve Friendliness. You have no
<br>
qualms with the approach, but you want more than just a sketch of what
<br>
this looks like.
<br>
<p>Me too!
<br>
<p><em>&gt; &gt; Additionally, we must consider the worst case, that we cannot control
</em><br>
<em>&gt; &gt; rapid ascent when it occurs. In that scenario we want the the AI to be
</em><br>
<em>&gt; &gt; ver own guide, maintaining and extending-as-necessary ver morality
</em><br>
<em>&gt; &gt; under rapid, heavy mental expansion/reconfiguration. Should we reach
</em><br>
<em>&gt; &gt; that point, the situation will be out of human hands. No regulatory
</em><br>
<em>&gt; &gt; guidelines will be able to help us. Everything we know and cherish
</em><br>
<em>&gt; &gt; could depend on preparing for that possible instant.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; (**) Slight irony implied but full, sincere appreciation bestowed. I
</em><br>
<em>&gt; &gt; consider this slightly ironic, since I view happiness as a signal that
</em><br>
<em>&gt; &gt; confirms a goal was achieved rather than a goal, in-and-of-itself.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Yes, but happiness is playing these two different roles in
</em><br>
<em>&gt; two different brains. It is the signal of positive values in
</em><br>
<em>&gt; humans, and observation of that signal is the positive value
</em><br>
<em>&gt; for the AI. This links AI values to human values.
</em><br>
<p>OK, I see the distinction. We probably agree that the AI has to be
<br>
&quot;smart&quot; about this, like a human would be &quot;smart&quot; about it so we end
<br>
up getting what we want instead of what we say we want. That means
<br>
coming up with a model of the human in question and constantly
<br>
updating it, just like we do when we model other minds. I have a
<br>
feeling I should read CFAI carefully to get an answer for this.
<br>
<p>You're distinction, earlier, of short term goals and long term goals
<br>
is a good one. That should definitely be a consideration when humans
<br>
are given &quot;full disclosure&quot; of the possible outcomes of our choices.
<br>
<p>The future holds much weirdness for transhumans who will be able to
<br>
modify our their own psyches. Thinking about desire and volition, can
<br>
get muddled. Earlier you distinguished short term and long term goals,
<br>
which any intelligent being should do. But now imagine we have full
<br>
control over our own minds. If you enjoy gambling, shouldn't you be
<br>
allowed to gamble (short term pleasure) even though you know it's bad
<br>
for you (long term loss of money)? Or should you just turn off your
<br>
enjoyment of gambling? With this particular situation, I think one can
<br>
examine ones goals, and calculate which is the better thing to do. 
<br>
<p>Or, assuming sex is still interesting, will gay people &quot;cure&quot;
<br>
themselves and become straight? Or will straight people cure
<br>
themselves and become bisexual or celibate? This might all be decided
<br>
by predicting likely future scenarios and their probabilities and then
<br>
we get to choose which future we want to live in...and with everyone
<br>
else making decisions in real time the models of the future will be
<br>
constantly changing. Ah those derivatives markets should be fun...If
<br>
we can get to the point where were not all killing each other and are
<br>
all increasing in prosperity, then the rest of this is icing on the
<br>
cake...ok, I've digressed.
<br>
<p>Again, I think Friendliness, properly implemented, will link human
<br>
desires to the AI's desire. The AI will model us a sentients and
<br>
proceed accordingly. That's the whole idea.
<br>
<p><em>&gt; &gt; &gt; Morality has its roots in values, especially social
</em><br>
<em>&gt; &gt; &gt; values for shared interests. Complex moral systems
</em><br>
<em>&gt; &gt; &gt; are elaborations of such values via learning and
</em><br>
<em>&gt; &gt; &gt; reasoning. The right place to control an AI's moral
</em><br>
<em>&gt; &gt; &gt; system is in its values. All we can do for an AI's
</em><br>
<em>&gt; &gt; &gt; learning and reasoning is make sure they are accurate
</em><br>
<em>&gt; &gt; &gt; and efficient.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I'll agree that the values are critical linchpins as you suggest, but
</em><br>
<em>&gt; &gt; please do not lose sight of the fact that these linchpins are part of
</em><br>
<em>&gt; &gt; a greater machine with many interdependencies and exposure to an
</em><br>
<em>&gt; &gt; external, possibly malevolent, world.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; There are both benevolence and malevolence in the world.
</em><br>
<em>&gt; For further details, see <a href="http://www.nytimes.com/">http://www.nytimes.com/</a> ;)
</em><br>
<p>Yes, I realize you've probably noticed it's a nasty world out there :)
<br>
I just wanted to express my hope that if we can put as many safety
<br>
measures as possible into the design, we better protect ourselves from
<br>
disaster. I'm sure you agree :)
<br>
<p>I'm not sure it is easy to cleanly cleave values from value-modifying
<br>
processes when self-modification is involved. This may require
<br>
intensive scrutiny and might not look like non-recursively self
<br>
modifying solutions. We might have to expend extra effort to get these
<br>
parts right:
<br>
<p>1) Model of the world (including self, sentients and non-sentients)
<br>
2) Model of Friendliness/Values
<br>
3) Processes for modifying Friendliness
<br>
4) Processes for taking deliberate action in the world, while
<br>
&nbsp;&nbsp;&nbsp;preserving Friendliness
<br>
<p>All these things need to be right. All are dynamic and
<br>
interconnected. We can't write any one of these in stone and then work
<br>
on the rest of them in isolation, they all have to work in
<br>
concert. There needs to be a system-level solution.
<br>
<p>If you take this all into account when you write:
<br>
<p>&quot;The right place to control an AI's moral system is in its values. All
<br>
we can do for an AI's learning and reasoning is make sure they are
<br>
accurate and efficient.&quot;
<br>
<p>then I'm totally with you.
<br>
<p><em>&gt; &gt; The statement: &quot;All we can do for an AI's learning and reasoning is
</em><br>
<em>&gt; &gt; make sure they are accurate and efficient&quot; seems limiting to me, in
</em><br>
<em>&gt; &gt; the light Eliezer's writings. If we can construct a mind that will
</em><br>
<em>&gt; &gt; solve this most difficult of problems (extreme intellectual ascent
</em><br>
<em>&gt; &gt; while preserving Friendliness) for us and forever, then we should aim
</em><br>
<em>&gt; &gt; for nothing less. Indeed, not hitting this mark is a danger that
</em><br>
<em>&gt; &gt; people on this list take quite seriously.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Unsafe AI is a great danger, but I think we disagree on
</em><br>
<em>&gt; the greatest source of that danger. The SIAI analysis is
</em><br>
<em>&gt; so afraid of making an error in the definition of its
</em><br>
<em>&gt; base values (i.e., supergoal) that it leaves them
</em><br>
<em>&gt; ambiguous.
</em><br>
<p>Ah I see that. But I take CFAI 1.0 as a sketch. More dirty work lies
<br>
ahead. I've suggested starting with a detector for &quot;special interests
<br>
coerce others&quot;. This suggestion probably demonstrates my *lack* of
<br>
understanding of CFAI, but I hope to correct that shortly :)
<br>
<p>It will be interesting to see how the initial layers are built up,
<br>
ie. what is specified in what order. As I read CFAI (for real this
<br>
time), I'm going to try to keep an eye out for this. I have a feeling
<br>
from skimming it before that such fine details were not explained.
<br>
<p><em>&gt; This ambiguity will be exploited by the main danger for
</em><br>
<em>&gt; safe AI, namely powerful people and institutions who will
</em><br>
<em>&gt; try to manipulate the singularity for protect and enhance
</em><br>
<em>&gt; their own interests.
</em><br>
<p>More detail is needed! If one had a complete design spec for a
<br>
Friendly AI, should it be posted to the internet...I guess if one were
<br>
confident it was right, one would...or should one just build it?
<br>
<p>I think you raised a good question probably on lots of people's minds:
<br>
What does go into these Wisdom tournaments exactly? (or maybe we're
<br>
not ready to ask this yet, first a book on rationality needs to be
<br>
written to start a movement to garner interest to ...) I'm not really
<br>
sure. 
<br>
<p><em>&gt; The SIAI analysis completely fails to recognize the need
</em><br>
<em>&gt; for politics to counter the threat of unsafe AI posed by
</em><br>
<em>&gt; the bad intentions of some people.
</em><br>
<p>There are two classes of scenarios we need to protect our selves from:
<br>
<p>1) A public project with good intentions (that can be distorted)
<br>
2) A private project with bad intentions
<br>
<p>We might be able to protect ourselves from #1 with oversight and good
<br>
rules for wisdom tournaments.
<br>
<p>#2 is a bit stranger. I'm wondering if the only way to beat that is to
<br>
race to #1 and get there first. If we slow down #1 so that we lose #2,
<br>
we're just as hosed.
<br>
<p><em>&gt; That threat must be opposed by clearly defined values for
</em><br>
<em>&gt; safe AIs, and a broad political movement successful in
</em><br>
<em>&gt; electoral politics to enforce those values.
</em><br>
<p>Here is Eliezer's snap summary about Friendliness from CFAI:
<br>
<p>&quot;the elimination of involuntary pain, death, coercion, and stupidity&quot;
<br>
<p>If we can clearly define the values for safe AIs that lead to this
<br>
better world, then by all means they should be published. And that
<br>
might help us if others who are flying below the radar of public
<br>
political processes read them as well.
<br>
<p>PS - 
<br>
<p>I'm interested to carry on this dialog as I make my way through the
<br>
document (we'll see how much time I have to type along the way though
<br>
:)
<br>
<p><p><pre>
--
Durant Schoon
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6790.html">Samantha: "Re: Flight recorders in AIs"</a>
<li><strong>Previous message:</strong> <a href="6788.html">Gary Miller: "RE: Failure of AI so far"</a>
<li><strong>In reply to:</strong> <a href="6707.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6815.html">Philip Sutton: "RE: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6789">[ date ]</a>
<a href="index.html#6789">[ thread ]</a>
<a href="subject.html#6789">[ subject ]</a>
<a href="author.html#6789">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
