<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Bill Hibbard (test@demedici.ssec.wisc.edu)">
<meta name="Subject" content="SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-09">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>SIAI's flawed friendliness analysis</h1>
<!-- received="Fri May  9 16:51:18 2003" -->
<!-- isoreceived="20030509225118" -->
<!-- sent="Fri, 9 May 2003 17:51:08 -0500 (CDT)" -->
<!-- isosent="20030509225108" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@demedici.ssec.wisc.edu" -->
<!-- subject="SIAI's flawed friendliness analysis" -->
<!-- id="Pine.GSO.4.44.0305091747370.23415-100000@demedici.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis"><em>test@demedici.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Fri May 09 2003 - 16:51:08 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6647.html">Michael Roy Ames: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6645.html">D. Goel: "Re: many worlds (was RE: Einstein)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6647.html">Michael Roy Ames: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6647.html">Michael Roy Ames: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6654.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Maybe reply:</strong> <a href="6815.html">Philip Sutton: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Maybe reply:</strong> <a href="6824.html">Tommeteor@aol.com: "RE: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6646">[ date ]</a>
<a href="index.html#6646">[ thread ]</a>
<a href="subject.html#6646">[ subject ]</a>
<a href="author.html#6646">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<a href="http://www.ssec.wisc.edu/~billh/g/SIAI_critique.html">http://www.ssec.wisc.edu/~billh/g/SIAI_critique.html</a>
<br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Critique of the SIAI Guidelines on Friendly AI
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bill Hibbard 9 May 2003
<br>
<p><p>This critique refers to the following documents:
<br>
<p>&nbsp;&nbsp;GUIDELINES:  <a href="http://www.intelligence.org/friendly/guidelines.html">http://www.intelligence.org/friendly/guidelines.html</a>
<br>
&nbsp;&nbsp;FEATURES:    <a href="http://www.intelligence.org/friendly/features.html">http://www.intelligence.org/friendly/features.html</a>
<br>
&nbsp;&nbsp;CFAI:        <a href="http://www.intelligence.org/CFAI/index.html">http://www.intelligence.org/CFAI/index.html</a>
<br>
<p><p>1. The SIAI analysis fails to recognize the importance of
<br>
the political process in creating safe AI.
<br>
<p>This is a fundamental error in the SIAI analysis. CFAI 4.2.1
<br>
says &quot;If an effort to get Congress to enforce any set of
<br>
regulations were launched, I would expect the final set of
<br>
regulations adopted to be completely unworkable.&quot; It further
<br>
says that government regulation of AI is unnecessary because
<br>
&quot;The existing force tending to ensure Friendliness is that
<br>
the most advanced projects will have the brightest AI
<br>
researchers, who are most likely to be able to handle the
<br>
problems of Friendly AI.&quot; History vividly teaches the danger
<br>
of trusting the good intentions of individuals.
<br>
<p>The singularity will completely change power relations in
<br>
human society. People and institutions that currently have
<br>
great power and wealth will know this, and will try to
<br>
manipulate the singularity to protect and enhance their
<br>
positions. The public generally protects its own interests
<br>
against the narrow interests of such powerful people and
<br>
institutions via widespread political movements and the
<br>
actions of democratically elected government. Such
<br>
political action has never been more important than it
<br>
will be in the singularity.
<br>
<p>The reinforcement learning values of the largest (and hence
<br>
most dangerous) AIs will be defined by the corporations and
<br>
governments that build them, not the AI researchers working
<br>
for those orgnaizations. Those organizations will give their
<br>
AIs values that reflect the organizations' values: profits in
<br>
the case of corporations, and political and military power
<br>
in the case of governments. Only a strong public movement
<br>
driving government regulation will be able to coerce these
<br>
organizations to design AI values to protect the interests
<br>
of all humans. This government regulation must include an
<br>
agency to monitor AI development and enforce regulations.
<br>
<p>The breakthrough ideas for achieving AI will come from
<br>
individual researchers, many of whom will want their AI to
<br>
serve the broad human interest. But their breakthrough ideas
<br>
will become known to wealthy organizations. Their research
<br>
will either be in the public domain, done for hire by wealthy
<br>
organizations, or will be sold to such organizations.
<br>
Breakthrough research may simply be seized by governments and
<br>
the researchers prohibited from publishing, as was done for
<br>
research on effective cryptography during the 1970s. The most
<br>
powerful AIs won't exist on the $5,000 computers on
<br>
researchers' desktops, but on the $5,000,000,000 computers
<br>
owned by wealthy organizations. The dangerous AIs will be the
<br>
ones capable of developing close personal relations with huge
<br>
numbers of people. Such AIs will be operated by wealthy
<br>
organizations, not individuals.
<br>
<p>Individuals working toward the singularity may resist
<br>
regulation as interference with their research, as was
<br>
evident in the SL4 discussion of testimony before
<br>
Congressman Brad Sherman's committee. But such regulation
<br>
will be necessary to coerce the wealthy organizations
<br>
that will own the most powerful AIs. These will be much
<br>
like the regulations that restrain powerful organizations
<br>
from building dangerous products (cars, household
<br>
chemicals, etc), polluting the environment, and abusing
<br>
citizens.
<br>
<p><p>2. The design recommendations in GUIDELINES 3 fail to
<br>
define rigorous standards for &quot;changes to supergoal
<br>
content&quot; in recommendation 3, for &quot;valid&quot; and &quot;good&quot; in
<br>
recommendation 4, for &quot;programmers' intentions&quot; in
<br>
recommendation 5, and for &quot;mistaken&quot; in recommendation 7.
<br>
<p>These recommendations are about the AI learning its own
<br>
supergoal. But even digging into corresponding sections
<br>
of CFAI and FEATURES fails to find rigorus standards for
<br>
defining critical terms in these recommendations.
<br>
Determination of their meanings is left to &quot;programmers&quot;
<br>
or the AI itself. Without rigorous standards for these
<br>
terms, wealthy organizations constructing AIs will be
<br>
free to define them in any way that serves their purposes
<br>
and hence to construct AIs that serve their narrow
<br>
interests rather than the general public interest.
<br>
<p><p>3. CFAI defines &quot;friendliness&quot; in a way that can only
<br>
be determined by an AI after it has developed super-
<br>
intelligence, and fails to define rigorous standards
<br>
for the values that guide its learning until it reaches
<br>
super-intelligence.
<br>
<p>The actual definition of &quot;friendliness&quot; in CFAI 3.4.4
<br>
requires the AI to know most humans sufficiently well
<br>
to decompose their minds into &quot;panhuman&quot;, &quot;gaussian&quot; and
<br>
&quot;personality&quot; layers, and to &quot;converge to normative
<br>
altruism&quot; based on collective content of the &quot;panhuman&quot;
<br>
and &quot;gaussian&quot; layers. This will require the development
<br>
of super-intelligence over a large amount of learning.
<br>
The definition of friendliness values to reinforce that
<br>
learning is left to &quot;programmers&quot;. As in the previous
<br>
point, this will allow wealthy organizations to define
<br>
intial learning values for their AIs as they like.
<br>
<p><p>4. The CFAI analysis is based on a Bayesian reasoning
<br>
model of intelligence, which is not a sufficient model
<br>
for producing intelligence.
<br>
<p>While Bayesian reasoning has an important role in
<br>
intelligence, it is not sufficient. Sensory experience
<br>
and reinforcement learning are fundamental to
<br>
intelligence. Just as symbols must be grounded in
<br>
sensory experience, reasoning must be grounded in
<br>
learning and emerges from it because of the need to
<br>
solve the credit assignment problem, as discussed at:
<br>
<p>&nbsp;&nbsp;<a href="http://www.mail-archive.com/<a href="mailto:agi@v2.listbox.com?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis">agi@v2.listbox.com</a>/msg00390.html">http://www.mail-archive.com/<a href="mailto:agi@v2.listbox.com?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis">agi@v2.listbox.com</a>/msg00390.html</a>
<br>
<p>Effective and general reinforcement learning requires
<br>
simulation models of the world, and sets of competing
<br>
agents. Furthermore, intelligence requires a general
<br>
ability to extract patterns from sense data and
<br>
internal information. An analysis of safe AI should be
<br>
based on a sufficient model of intelligence.
<br>
<p><p>I offer an alternative analysis of producing safe AI in
<br>
my book at <a href="http://www.ssec.wisc.edu/~billh/super.html">http://www.ssec.wisc.edu/~billh/super.html</a>.
<br>
<p>----------------------------------------------------------
<br>
Bill Hibbard, SSEC, 1225 W. Dayton St., Madison, WI  53706
<br>
<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20SIAI's%20flawed%20friendliness%20analysis">test@demedici.ssec.wisc.edu</a>  608-263-4427  fax: 608-263-6738
<br>
<a href="http://www.ssec.wisc.edu/~billh/vis.html">http://www.ssec.wisc.edu/~billh/vis.html</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6647.html">Michael Roy Ames: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6645.html">D. Goel: "Re: many worlds (was RE: Einstein)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6647.html">Michael Roy Ames: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6647.html">Michael Roy Ames: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6654.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Maybe reply:</strong> <a href="6815.html">Philip Sutton: "RE: SIAI's flawed friendliness analysis"</a>
<li><strong>Maybe reply:</strong> <a href="6824.html">Tommeteor@aol.com: "RE: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6646">[ date ]</a>
<a href="index.html#6646">[ thread ]</a>
<a href="subject.html#6646">[ subject ]</a>
<a href="author.html#6646">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
