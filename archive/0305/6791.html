<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)</title>
<meta name="Author" content="Samantha (samantha@objectent.com)">
<meta name="Subject" content="Re: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)">
<meta name="Date" content="2003-05-22">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)</h1>
<!-- received="Thu May 22 22:56:37 2003" -->
<!-- isoreceived="20030523045637" -->
<!-- sent="Thu, 22 May 2003 22:04:04 -0700" -->
<!-- isosent="20030523050404" -->
<!-- name="Samantha" -->
<!-- email="samantha@objectent.com" -->
<!-- subject="Re: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)" -->
<!-- id="200305230504.h4N545M03496@sophia.objectent.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="021e01c32080$dc4afaf0$3e553f94@GaryMiller01" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Samantha (<a href="mailto:samantha@objectent.com?Subject=Re:%20Flawed%20Risk%20Analysis%20(was%20Re:%20SIAI's%20flawed%20friendliness%20analysis)"><em>samantha@objectent.com</em></a>)<br>
<strong>Date:</strong> Thu May 22 2003 - 23:04:04 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6792.html">Samantha: "Re: Failure of AI so far"</a>
<li><strong>Previous message:</strong> <a href="6790.html">Samantha: "Re: Flight recorders in AIs"</a>
<li><strong>In reply to:</strong> <a href="6785.html">Gary Miller: "RE: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6800.html">Bill Hibbard: "Re: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6791">[ date ]</a>
<a href="index.html#6791">[ thread ]</a>
<a href="subject.html#6791">[ subject ]</a>
<a href="author.html#6791">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Thursday 22 May 2003 09:40 am, Gary Miller wrote:
<br>
<p><em>&gt; The point is that you teach the FAI morality, ethics, and let it
</em><br>
<em>&gt; develop it's moral compass early on before it is ten times
</em><br>
<em>&gt; smarter than you.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Once it's character has been established I don't believe it's going
</em><br>
<em>&gt; to turn evil on you at that point.  
</em><br>
<p>Please point out the foolproof version of ethics and morality, 
<br>
absolutely ironclad and logical for all minds of sufficient capacity, 
<br>
that this sort of trust is based upon.  Please point out what you 
<br>
mean by &quot;evil&quot;.   Would any future decision that does away with all 
<br>
human beings be considered &quot;evil&quot; for instance?   That you or I might 
<br>
say so (not a given on this list by past dialogue)  does not 
<br>
guarantee that the AI would consider it so indefinitely.    So where 
<br>
is the solid bedrock upon which we can predict the evil-ness 
<br>
(whatever that is to us) of any AI's future decisions based merely 
<br>
upon our understanding and comfortableness with its moral/ethical 
<br>
reasoning and decisions to date?
<br>
<p><p><em>&gt; The source of most criminal  and antisocial behavior is readily
</em><br>
<em>&gt; apparent when you examine the childhoods and upbringing the
</em><br>
<em>&gt; criminals had up to their teen years.
</em><br>
<em>&gt;
</em><br>
<p>No, I don't think so.   An AI will not be socialized as humans are.  
<br>
It will not have our evolutionary bounds on aggression and basis of 
<br>
compassion (limited as these may be).  This will be a loner (unless 
<br>
there are others of comparable ability) intelligence, not a sentient 
<br>
socialized into the human community.   So drawing parallels from 
<br>
human upbringing is strikingly unconvincing even if one believes the 
<br>
overplayed explanation for human criminality.
<br>
<p>- samantha
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6792.html">Samantha: "Re: Failure of AI so far"</a>
<li><strong>Previous message:</strong> <a href="6790.html">Samantha: "Re: Flight recorders in AIs"</a>
<li><strong>In reply to:</strong> <a href="6785.html">Gary Miller: "RE: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6800.html">Bill Hibbard: "Re: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6791">[ date ]</a>
<a href="index.html#6791">[ thread ]</a>
<a href="subject.html#6791">[ subject ]</a>
<a href="author.html#6791">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
