<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Rafal Smigrodzki (rafal@smigrodzki.org)">
<meta name="Subject" content="RE: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: SIAI's flawed friendliness analysis</h1>
<!-- received="Wed May 21 14:36:48 2003" -->
<!-- isoreceived="20030521203648" -->
<!-- sent="Wed, 21 May 2003 16:37:07 -0700" -->
<!-- isosent="20030521233707" -->
<!-- name="Rafal Smigrodzki" -->
<!-- email="rafal@smigrodzki.org" -->
<!-- subject="RE: SIAI's flawed friendliness analysis" -->
<!-- id="OJEHKDIANIFPAJPDBDGLGEJJCDAA.rafal@smigrodzki.org" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="3ECBC241.8040401@pobox.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Rafal Smigrodzki (<a href="mailto:rafal@smigrodzki.org?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis"><em>rafal@smigrodzki.org</em></a>)<br>
<strong>Date:</strong> Wed May 21 2003 - 17:37:07 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6775.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6773.html">Robin Lee Powell: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6767.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6799.html">Bill Hibbard: "RE: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6774">[ date ]</a>
<a href="index.html#6774">[ thread ]</a>
<a href="subject.html#6774">[ subject ]</a>
<a href="author.html#6774">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Eliezer wrote:
<br>
<em>&gt; Rafal Smigrodzki wrote:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; ### Here we definitely agree - a huge government-funded AI program
</em><br>
<em>&gt;&gt; would be a great idea. Since the FAI may be interpreted as the
</em><br>
<em>&gt;&gt; ultimate public good, a boon for all, yet profit for nobody, a good
</em><br>
<em>&gt;&gt; case can be made for public funding of this endeavor, just like the
</em><br>
<em>&gt;&gt; basic science that gave us modern medicine. This program, if open to
</em><br>
<em>&gt;&gt; all competent researchers, with results directly available to all
</em><br>
<em>&gt;&gt; humans, could vastly accelerate the building of the FAI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; How can you get FAI by throwing money at the problem?
</em><br>
<em>&gt; *Unfortunately* I can see how throwing sufficiently vast amounts of
</em><br>
<em>&gt; money at AI might result in advances on AI.  But how do you get
</em><br>
<em>&gt; advances in Friendliness?  It seems to me that the optimal political
</em><br>
<em>&gt; scenario for ensuring FAI over UFAI calls for all researchers to have
</em><br>
<em>&gt; *exactly* equal resources and funding, so that the smartest
</em><br>
<em>&gt; researchers have an advantage.  Why is this the optimal political
</em><br>
<em>&gt; scenario?  Because there is absolutely no way in heaven or Earth that
</em><br>
<em>&gt; the political process is capable of distinguishing competent
</em><br>
<em>&gt; Friendliness researchers from noncompetent ones.  Any non-maxentropy
</em><br>
<em>&gt; *political* resource distribution will probably be pragmatically
</em><br>
<em>&gt; worse than an even distribution.  Furthermore, you don't want absurd
</em><br>
<em>&gt; quantities of resources, either, as otherwise you may push research
</em><br>
<em>&gt; into the territory where brute-forcing AI becomes possible.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The more you look into the problem, the more you realize how hard it
</em><br>
<em>&gt; is to find forces that genuinely *improve* our prospects, rather than
</em><br>
<em>&gt; making things even worse.
</em><br>
<p><p>### This is a very interesting point of view. I think you might overestimate
<br>
the amount of correlation between AI-applicable smartness and
<br>
Friendliness-competence. There is clearly a positive correlation between
<br>
high IQ and lack of meanness, measured by criminality, marital fidelity,
<br>
etc. but if the correlation is not high  enough, having the smartest
<br>
programmers might not always be sufficient to minimize the risk of UnFAI, in
<br>
the absence of additional character analysis.
<br>
<p>The optimal scenario would be for all Friendliness-competent and -willing
<br>
researchers to have ridiculously good funding, with nothing left for the
<br>
UnFriendly ones. Since we do not have a validated measure of
<br>
Friendliness-competence, you are using smartness as a substitute. This would
<br>
be reasonable, if Friendliness theory predicted that it would be impossible
<br>
for a programmer to build an AI which would be stably Friendly to him alone,
<br>
because then the only rational behavior for both the cynical egoist and the
<br>
altruist, would be to work on true FAI, and the smartest programmers would
<br>
be most likely to see it. However, if there was an objectively existing
<br>
chance to build an AI with a special love for its maker alone, then some of
<br>
the smartest programmers might be tempted to try for personal godhood.
<br>
<p>As long as the political funder is not trying to micromanage the situation,
<br>
support would be offered to people with the ideas judged most promising by
<br>
their peers who had shown some results before. This is not likely to put the
<br>
best FAI programmers at a systemic disadvantage. The correlation between
<br>
being a good FAI programmer and getting funded probably won't be very high -
<br>
but it will be better than zero and is unlikely to be negative (unless of
<br>
course the peer reviewers are in their majority totally wrong).
<br>
<p>But the condition for would is definitely an absence of direct control by
<br>
non-experts, such as politicians and activists of all sorts. I share your
<br>
skepticism as to the likelihood of this condition being satisfied, but one
<br>
can always try to give good advice to the government guys. Maybe they'll
<br>
listen.
<br>
<p>Rafal
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6775.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6773.html">Robin Lee Powell: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6767.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6799.html">Bill Hibbard: "RE: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6774">[ date ]</a>
<a href="index.html#6774">[ thread ]</a>
<a href="subject.html#6774">[ subject ]</a>
<a href="author.html#6774">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
