<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: SIAI's flawed friendliness analysis</title>
<meta name="Author" content="Rafal Smigrodzki (rafal@smigrodzki.org)">
<meta name="Subject" content="RE: SIAI's flawed friendliness analysis">
<meta name="Date" content="2003-05-21">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: SIAI's flawed friendliness analysis</h1>
<!-- received="Wed May 21 10:58:40 2003" -->
<!-- isoreceived="20030521165840" -->
<!-- sent="Wed, 21 May 2003 12:59:19 -0700" -->
<!-- isosent="20030521195919" -->
<!-- name="Rafal Smigrodzki" -->
<!-- email="rafal@smigrodzki.org" -->
<!-- subject="RE: SIAI's flawed friendliness analysis" -->
<!-- id="OJEHKDIANIFPAJPDBDGLCEJBCDAA.rafal@smigrodzki.org" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="Pine.GSO.4.44.0305201617550.15294-100000@demedici.ssec.wisc.edu" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Rafal Smigrodzki (<a href="mailto:rafal@smigrodzki.org?Subject=RE:%20SIAI's%20flawed%20friendliness%20analysis"><em>rafal@smigrodzki.org</em></a>)<br>
<strong>Date:</strong> Wed May 21 2003 - 13:59:19 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6766.html">Ben Goertzel: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6764.html">Philip Sutton: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6752.html">Bill Hibbard: "RE: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6766.html">Ben Goertzel: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6766.html">Ben Goertzel: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6767.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6799.html">Bill Hibbard: "RE: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6765">[ date ]</a>
<a href="index.html#6765">[ thread ]</a>
<a href="subject.html#6765">[ subject ]</a>
<a href="author.html#6765">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Bill Hibbard wrote:
<br>
<em>&gt; On Sun, 18 May 2003, Rafal Smigrodzki wrote:
</em><br>
<p><em>&gt;&gt;
</em><br>
<em>&gt;&gt; If independent development of AI was unsafe, a political process
</em><br>
<em>&gt;&gt; would not make it any less so.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Pointing out the difficulties does not justify not even
</em><br>
<em>&gt; trying. Independent development of AI will be unsafe. A
</em><br>
<em>&gt; political process is not guaranteed to solve the problem,
</em><br>
<em>&gt; but it is necessary to at least try to stop humans who
</em><br>
<em>&gt; will purposely build unsafe AIs for their own imagined
</em><br>
<em>&gt; benefit.
</em><br>
<p>### But pointing out difficulties may focus attention on exactly what should
<br>
be tried, instead of merely hinted at in pleasantly general terms without
<br>
any concrete plan, with salutary affects on the outcomes.
<br>
<p>Now, good intentions alone are not sufficient to claim necessity of an
<br>
action, necessity appears only if a reasonable chance of success exists.
<br>
<p>----------------------------
<br>
<p><em>&gt;
</em><br>
<em>&gt; Regulation will make it more difficult for those who want
</em><br>
<em>&gt; to develop unsafe AI to succeed. The legal and trusted AIs
</em><br>
<em>&gt; will have much greater resources available to them and thus
</em><br>
<em>&gt; will probably be more intelligent than the unregulated AIs.
</em><br>
<em>&gt; The trusted AIs will be able to help with the regulation
</em><br>
<em>&gt; effort. I would trust an AI with reinforcement values for
</em><br>
<em>&gt; human happiness more than I would trust any individual
</em><br>
<em>&gt; human.
</em><br>
<p>### Here we definitely agree - a huge government-funded AI program would be
<br>
a great idea. Since the FAI may be interpreted as the ultimate public good,
<br>
a boon for all, yet profit for nobody, a good case can be made for public
<br>
funding of this endeavor, just like the basic science that gave us modern
<br>
medicine. This program, if open to all competent researchers, with results
<br>
directly available to all humans, could vastly accelerate the building of
<br>
the FAI.
<br>
<p>Now, of course if you are familiar with the way research progresses, you
<br>
know that the power of public research comes from the lack of bureaucratic
<br>
regulation and the reliance on peer review, collaboration and public
<br>
disclosure. This is why AI's from this program would be better than AI's
<br>
built in little closeted groups, and since only peer-reviewed programs (as
<br>
opposed to bureaucrat-reviewed ones) would be funded, there would be the
<br>
best balance between safety and efficiency. The government program could
<br>
have the &quot;security overkill&quot; measures described here recently, while still
<br>
running circles around the Islamic programmer trying to build his own phone
<br>
line to Allah. In this sense &quot;regulation&quot; would mean the best that human
<br>
minds can come up with, as opposed to the contrived schemes produced by
<br>
legislatures which try to micromanage. The only input from the political
<br>
process is the recognition of the importance of the problem, and the
<br>
appropriation of funds, with the actual implementation left to publicly
<br>
operating experts with good track records of achievement, operating in the
<br>
competitive and yet collaborative, democratic but multicentric fashion
<br>
typical of all successful innovative projects.
<br>
<p>The positive outcome would be the result of faster development of FAI,
<br>
rather than direct human-mediated retardation of the UnFAI. The FAI might
<br>
perhaps decide to act as a regulator of other AIs, if this were the smartest
<br>
move, in agreement with ideas previously presented by Eliezer. We could
<br>
trust this FAI as much or more than any other AI, because the open
<br>
competitive process of science, whether privately or publicly funded, is the
<br>
best institutional approximation of rationality humans so far have come up
<br>
with.
<br>
<p>I have a feeling, though, that you are less interested in using the
<br>
government's carrot, but rather you would rely on the stick. You mention
<br>
making it more difficult for those who want to develop unsafe AI. There are
<br>
some methods which seem to be the first to come to the mind of
<br>
government-oriented people - burly men with guns, codes, statutes, secrecy
<br>
and prisons. They seem easy, but their long-term effects are complex, and
<br>
frequently counterproductive, which is why I want to use them only if I have
<br>
absolutely no inkling of any better ideas.
<br>
<p>What exact means do you want to use for the purpose of &quot;making things
<br>
difficult&quot;, without interfering with the efforts I described above?
<br>
<p>An approach that slows FAI more than UnFAI, should *not* be tried no matter
<br>
how easy it appears.
<br>
<p>--------------------------------------
<br>
<p><em>&gt;
</em><br>
<em>&gt; It really comes down to who you trust.
</em><br>
<p>### Yes, it comes down to whether you trust the stick, or the carrot. I
<br>
prefer the latter, with only very limited uses for the former, and not when
<br>
applied to FAI.
<br>
<p>Rafal
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6766.html">Ben Goertzel: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6764.html">Philip Sutton: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>In reply to:</strong> <a href="6752.html">Bill Hibbard: "RE: SIAI's flawed friendliness analysis"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6766.html">Ben Goertzel: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6766.html">Ben Goertzel: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6767.html">Eliezer S. Yudkowsky: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Reply:</strong> <a href="6799.html">Bill Hibbard: "RE: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6765">[ date ]</a>
<a href="index.html#6765">[ thread ]</a>
<a href="subject.html#6765">[ subject ]</a>
<a href="author.html#6765">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
