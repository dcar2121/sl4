<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: AGI Policy (was RE: SIAI's flawed friendliness analysis)</title>
<meta name="Author" content="Bill Hibbard (test@demedici.ssec.wisc.edu)">
<meta name="Subject" content="Re: AGI Policy (was RE: SIAI's flawed friendliness analysis)">
<meta name="Date" content="2003-05-23">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: AGI Policy (was RE: SIAI's flawed friendliness analysis)</h1>
<!-- received="Fri May 23 10:15:52 2003" -->
<!-- isoreceived="20030523161552" -->
<!-- sent="Fri, 23 May 2003 11:15:41 -0500 (CDT)" -->
<!-- isosent="20030523161541" -->
<!-- name="Bill Hibbard" -->
<!-- email="test@demedici.ssec.wisc.edu" -->
<!-- subject="Re: AGI Policy (was RE: SIAI's flawed friendliness analysis)" -->
<!-- id="Pine.GSO.4.44.0305231115120.969-100000@demedici.ssec.wisc.edu" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="000501c31f1c$f70d5fa0$6ca6fea9@netcom.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bill Hibbard (<a href="mailto:test@demedici.ssec.wisc.edu?Subject=Re:%20AGI%20Policy%20(was%20RE:%20SIAI's%20flawed%20friendliness%20analysis)"><em>test@demedici.ssec.wisc.edu</em></a>)<br>
<strong>Date:</strong> Fri May 23 2003 - 10:15:41 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="6797.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6795.html">Bill Hibbard: "RE: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>In reply to:</strong> <a href="6757.html">Keith Elis: "AGI Policy (was RE: SIAI's flawed friendliness analysis)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6762.html">Samantha: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6796">[ date ]</a>
<a href="index.html#6796">[ thread ]</a>
<a href="subject.html#6796">[ subject ]</a>
<a href="author.html#6796">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Tue, 20 May 2003, Keith Elis wrote:
<br>
<p><em>&gt; This post is not directed to me, but I'm jumping in anyway.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Bill Hibbard:
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; Pointing out the difficulties does not justify not even
</em><br>
<em>&gt; &gt; trying. Independent development of AI will be unsafe. A
</em><br>
<em>&gt; &gt; political process is not guaranteed to solve the problem, but
</em><br>
<em>&gt; &gt; it is necessary to at least try to stop humans who will
</em><br>
<em>&gt; &gt; purposely build unsafe AIs for their own imagined benefit.
</em><br>
<em>&gt;
</em><br>
<em>&gt; You need to be far more specific about this. What do you mean by 'a
</em><br>
<em>&gt; political process'? Do you mean each line of code is subject to
</em><br>
<em>&gt; referendum? Surely not.
</em><br>
<p>No more than every hose, fitting and wire in a nuclear
<br>
plant is subject to a referendum. Why even ask this
<br>
question?
<br>
<p><em>&gt; Perhaps the design should be agreed upon by a
</em><br>
<em>&gt; Senate subcommitee? Your insistence on this unknown process doesn't
</em><br>
<em>&gt; really take a position.
</em><br>
<p>To a first approximation, this could work the way
<br>
government oversees other technology. Technical decisions
<br>
would be made by experts, reporting to elected officials.
<br>
Some elected officials would become semi-expert themselves,
<br>
and competing politicians would keep each other honest.
<br>
The critical thing in government oversight is always
<br>
that the public cares - otherwise policy is largely set
<br>
by lobbyists for corporations and other special
<br>
interests. As machines increasingly exhibit intelligence,
<br>
the public will remember all the books and movies about
<br>
the dangers of intelligent machines and care.
<br>
<p><em>&gt; A broad governmental policy with research guidelines that encourage
</em><br>
<em>&gt; Friendly AI (perhaps coupled with an offer of Manhattan Project funding
</em><br>
<em>&gt; to the most ethical researchers) *might* help. You admit that a
</em><br>
<em>&gt; political process is not guaranteed to help Friendly AI. It probably
</em><br>
<em>&gt; won't even come close. Friendly AI and compromise do not co-exist.
</em><br>
<p>Technology in general is friendlier in democracies than
<br>
in non-democracies. Politics without compromise are
<br>
totalitarian.
<br>
<p><em>&gt; &gt; Regulation will make it more difficult for those who want
</em><br>
<em>&gt; &gt; to develop unsafe AI to succeed.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Please be more specific. The regulatory process is cumbersome and slow,
</em><br>
<em>&gt; and mostly reactive. Good ideas are rarely implemented except as
</em><br>
<em>&gt; solutions to mature problems. The mature problems of this domain are the
</em><br>
<em>&gt; ones you just don't have time to react to.
</em><br>
<p>As I say in my book, one challenge of the singularity
<br>
is to get the public engaged in the issue early. They
<br>
have been somewhat primed by science fiction stories
<br>
about intelligent machines, and see computers play an
<br>
increasing role in their lives. There will be a
<br>
period of years from the time when machines start
<br>
surprising people with their intelligence until the
<br>
singularity. That will be the critical time to inform
<br>
the public and politicians about the issues. When the
<br>
public gets excited, politician get excited, and
<br>
politicians naturally reach out to experts. It is
<br>
encouraging that Ray Kurzweil has already testified
<br>
before congress about machine intelligence.
<br>
<p>I think there will be considerable concern among the
<br>
public and politicians about the dangers of machine
<br>
intelligence. There will be a debate with a wide
<br>
spectrum of opinions. The key will be to get a good
<br>
policy and regulatory mechanism in place before the
<br>
singularity really takes off. Not all government
<br>
agencies are slow. For example the Centers for
<br>
Disease Control generally do a good job in
<br>
containing disease outbreaks.
<br>
<p><em>&gt; &gt; The legal and trusted AIs
</em><br>
<em>&gt; &gt; will have much greater resources available to them and thus
</em><br>
<em>&gt; &gt; will probably be more intelligent than the unregulated AIs.
</em><br>
<em>&gt; &gt; The trusted AIs will be able to help with the regulation
</em><br>
<em>&gt; &gt; effort. I would trust an AI with reinforcement values for
</em><br>
<em>&gt; &gt; human happiness more than I would trust any individual human.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Are you talking about tool-level AGIs or &gt;H AGIs? In the latter case, do
</em><br>
<em>&gt; you really think a &gt;H AGI would make laws the way we do? It's possible,
</em><br>
<em>&gt; but even I can think of ways to establish a much larger degree of
</em><br>
<em>&gt; control over the things I would need control over.
</em><br>
<p>What do you mean by &quot;&gt;H AGI&quot;? Google couldn't find it.
<br>
<p>I am not suggesting that AGIs make laws any more than
<br>
nuclear plant inspectors make laws. Of course, at some
<br>
stage of the singularity the whole concept of law will
<br>
change radically.
<br>
<p><em>&gt; &gt; It really comes down to who you trust. I favor a broad
</em><br>
<em>&gt; &gt; political process because I trust the general public more
</em><br>
<em>&gt; &gt; than any individual or small group.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Most people aren't geniuses. What's even worse, most people deduce
</em><br>
<em>&gt; ethics from qualia. Average intelligence and utilitarian ethics might
</em><br>
<em>&gt; get you a business degree, but this is not the caliber of people that
</em><br>
<em>&gt; need to be working on AGI.
</em><br>
<p>Democracies have a good track record of employing
<br>
their best scientists and technologists on their
<br>
hardest problems. Harvard, Yale, Stanford and the
<br>
other great universities are in fierce competition
<br>
to enroll smart poor kids, and this situation was
<br>
created by pressure from citizens and their elected
<br>
government. It is in non-democracies where you find
<br>
the dingbat relatives of politicians screwing up
<br>
science policies.
<br>
<p><em>&gt; &gt; Of course, democratic
</em><br>
<em>&gt; &gt; goverement does enlist the help of experts on technical
</em><br>
<em>&gt; &gt; questions, but ultimate authority is with the public.
</em><br>
<em>&gt;
</em><br>
<em>&gt; What public? Do you mean the 50% of American citizens of voting age that
</em><br>
<em>&gt; actually cast a ballot? Or do you mean the rest of the world, too?
</em><br>
<p>The fact that we live in a world of multple nations
<br>
and great disparities in wealth poses some interesting
<br>
questions for how society approaches the singularity.
<br>
I discuss this a bit in my book. The reality is that
<br>
AGI will first appear in wealthy countries. Hopefully
<br>
the wealth that AGI creates will motivate some
<br>
generosity toward poorer countries (wealthy countries
<br>
already provide aid to poorer countries). I am also
<br>
hopeful that the issue of banning weapons based on
<br>
intelligent machines will have wide public support,
<br>
which will help motivate international cooperation on
<br>
AI safety and help the public understand the broader
<br>
issues of AI safety.
<br>
<p><em>&gt; &gt; When
</em><br>
<em>&gt; &gt; you say &quot;AI would be incomprehensible to the vast majority of
</em><br>
<em>&gt; &gt; persons involved in the political process&quot; I think you are
</em><br>
<em>&gt; &gt; not giving them enough credit. Democratic politics have
</em><br>
<em>&gt; &gt; managed to cope with some pretty complex and difficult problems.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is not directed to me, but can you name some of them that approach
</em><br>
<em>&gt; the complexity and difficulty of AGI?
</em><br>
<p>There has never been a problem as complex as AI, but the
<br>
ability of society to cope is constantly increasing. For
<br>
its time in history, the problem of defeating the nazis
<br>
and fascists was pretty complex and difficult. The
<br>
democracies have done a good job in the fight against
<br>
disease during the 20th century, which is certainly a
<br>
complex and difficult problem. They have also reduced
<br>
deaths from the other two mass killers: famine and war
<br>
(of course these killers are still around, but
<br>
percentage-wise they kill many fewer people than they
<br>
used to - Stephen Pinker made a point of this during
<br>
his talk at UW last year).
<br>
<p>And I can't leave out one of my favorite contributions of
<br>
democratic politics: development and free distribution to
<br>
the world of Vis5D and VisAD (thanks to support from the
<br>
US, Europe and Australia) ;)
<br>
<p>Bill
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="6797.html">Bill Hibbard: "Re: SIAI's flawed friendliness analysis"</a>
<li><strong>Previous message:</strong> <a href="6795.html">Bill Hibbard: "RE: Flawed Risk Analysis (was Re: SIAI's flawed friendliness analysis)"</a>
<li><strong>In reply to:</strong> <a href="6757.html">Keith Elis: "AGI Policy (was RE: SIAI's flawed friendliness analysis)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="6762.html">Samantha: "Re: SIAI's flawed friendliness analysis"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#6796">[ date ]</a>
<a href="index.html#6796">[ thread ]</a>
<a href="subject.html#6796">[ subject ]</a>
<a href="author.html#6796">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:42 MDT
</em></small></p>
</body>
</html>
