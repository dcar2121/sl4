<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: [SL4] JOIN: James Barton plus Re: Programmed morality</title>
<meta name="Author" content="James.Barton@sweetandmaxwell.co.uk (James.Barton@sweetandmaxwell.co.uk)">
<meta name="Subject" content="[SL4] JOIN: James Barton plus Re: Programmed morality">
<meta name="Date" content="2000-07-18">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>[SL4] JOIN: James Barton plus Re: Programmed morality</h1>
<!-- received="Tue Jul 18 10:48:30 2000" -->
<!-- isoreceived="20000718164830" -->
<!-- sent="Tue, 18 Jul 2000 12:25:46 -0000" -->
<!-- isosent="20000718122546" -->
<!-- name="James.Barton@sweetandmaxwell.co.uk" -->
<!-- email="James.Barton@sweetandmaxwell.co.uk" -->
<!-- subject="[SL4] JOIN: James Barton plus Re: Programmed morality" -->
<!-- id="8l1ica+ithe@eGroups.com" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="3968D841.36A139C2@intelligence.org" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> <a href="mailto:James.Barton@sweetandmaxwell.co.uk?Subject=Re:%20[SL4]%20JOIN:%20James%20Barton%20plus%20Re:%20Programmed%20morality"><em>James.Barton@sweetandmaxwell.co.uk</em></a><br>
<strong>Date:</strong> Tue Jul 18 2000 - 06:25:46 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="0019.html">Dale Johnstone: "[SL4] Re: AI testing and containment Re: Programmed morality"</a>
<li><strong>Previous message:</strong> <a href="0017.html">Eliezer S. Yudkowsky: "Re: [SL4] Programmed morality"</a>
<li><strong>In reply to:</strong> <a href="0017.html">Eliezer S. Yudkowsky: "Re: [SL4] Programmed morality"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18">[ date ]</a>
<a href="index.html#18">[ thread ]</a>
<a href="subject.html#18">[ subject ]</a>
<a href="author.html#18">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<body>
<tt>
Hi all.<BR>
Not really sure what's relevant about me, but:<BR>
born 21 September '76, living in London.<BR>
university background in Physics &amp; Philosophy, but hoping to do an <BR>
MSc in Computing.<BR>
lifelong neophile, and intellectual omnivore / dilettante.<BR>
apologies if I ramble a bit - I haven't assimilated all the <BR>
background on this topic yet.<BR>
Attitude towards Singularity: sceptical but very hopeful.<BR>
<BR>
<BR>
&gt; &gt; You may be correct in that only one will reach the singularity.<BR>
&gt; &gt; Exponential growth means whoever is in the lead should win. <BR>
However<BR>
&gt; &gt; the AI may decide to make a billion+ copies of itself on the way &amp;<BR>
&gt; &gt; coordinate as a society, or group mind. By that time it's already <BR>
out<BR>
&gt; &gt; of our hands. I expect we'll be uploaded into an archive &amp; our <BR>
atoms<BR>
&gt; &gt; used more efficiently.<BR>
&gt; <BR>
&gt; Um, a couple of disagreements here.&nbsp; One, I don't see why it would <BR>
make<BR>
&gt; copies of itself.&nbsp; Just because you and I grew up in a &quot;society&quot; <BR>
full of<BR>
&gt; vaguely similar people doesn't mean that's the best way to do <BR>
things.<BR>
<BR>
Two reasons an AI might want to make a copy of itself:<BR>
1) It places a high value on itself or its goals, and figures a <BR>
backup somewhere is good insurance. And lots of backups make better <BR>
insurance.<BR>
2) If it places a value on answers sooner rather than later, it might <BR>
want to create not merely backups but running copies of itself. These <BR>
may be nearly identical, or configured to work on specified sub-<BR>
problems.<BR>
<BR>
These courses of action depend on the AI knowing enough about its <BR>
substrate, and learning or deducing what's on the other end of the <BR>
network connections.<BR>
<BR>
Now, re: morality.<BR>
Are any of you familiar with the Naturalistic Fallacy? Essentially, <BR>
it says that no physical fact leads to a moral fact - e.g. nothing <BR>
about a cute defenceless infant makes it wrong to torture it. <BR>
Obviously, we all think it is wrong, but that's because of a separate <BR>
moral fact, something perhaps like, &quot;Torture is never justified&quot;.<BR>
No convincing system of ethics has been developed to my mind, and I <BR>
think we're left with Russell's comments in talking about Nietzsche: <BR>
we can only decide whether something is right by examining it with <BR>
our own internal sense of right and wrong, rather than comparing it <BR>
with some external, verifiable set of rules.<BR>
I suggest that an AI will have no internal sense of right and wrong, <BR>
as this in a product of our evolution. If we give it one, it may <BR>
choose to overwrite it in subsequent redesigns. Perhaps no &quot;bad&quot; <BR>
thing.<BR>
<BR>
<BR>
Eliezer, your demonstration that there are significant goals, even <BR>
starting from tabula rasa, in <a href="http://intelligence.org/tmol-faq/logic.html">http://intelligence.org/tmol-faq/logic.html</a> <BR>
is interesting.<BR>
Can you get point 11 to read &quot;All done: G2.desirability &gt; 0 &quot;?<BR>
Without knowing that something is positively desirable, how can an AI <BR>
make an informed choice to act towards it? I mean, assuming that G1 <BR>
is negatable, how can the AI decide between G1 and -G1?<BR>
And can G2 really be specified on such little information about G1?<BR>
<BR>
Anyway, just a few opening thoughts.<BR>
James<BR>
<BR>
<BR>
</tt>

<!-- |**|begin egp html banner|**| -->


<hr>
<!-- |@|begin eGroups banner|@| runid: 6992 crid: 3567 -->
<a target="_blank" href="http://click.egroups.com/1/6992/12/_/626675/_/963923152/"><center>
<img width="468" height="60"
  border="0"
  alt=""
  src="http://adimg.egroups.com/img/6992/12/_/626675/_/963923152/new468x60.gif"></center><center><font color="black"></font></center></a>
<!-- |@|end eGroups banner|@| -->
<hr>

<!-- |**|end egp html banner|**| -->


</body>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0019.html">Dale Johnstone: "[SL4] Re: AI testing and containment Re: Programmed morality"</a>
<li><strong>Previous message:</strong> <a href="0017.html">Eliezer S. Yudkowsky: "Re: [SL4] Programmed morality"</a>
<li><strong>In reply to:</strong> <a href="0017.html">Eliezer S. Yudkowsky: "Re: [SL4] Programmed morality"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18">[ date ]</a>
<a href="index.html#18">[ thread ]</a>
<a href="subject.html#18">[ subject ]</a>
<a href="author.html#18">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:00:35 MDT
</em></small></p>
</body>
</html>
