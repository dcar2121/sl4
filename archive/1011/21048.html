<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] Simple friendliness: plan B for AI</title>
<meta name="Author" content="Alexei Turchin (alexeiturchin@gmail.com)">
<meta name="Subject" content="Re: [sl4] Simple friendliness: plan B for AI">
<meta name="Date" content="2010-11-12">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] Simple friendliness: plan B for AI</h1>
<!-- received="Fri Nov 12 14:21:14 2010" -->
<!-- isoreceived="20101112212114" -->
<!-- sent="Sat, 13 Nov 2010 00:21:06 +0300" -->
<!-- isosent="20101112212106" -->
<!-- name="Alexei Turchin" -->
<!-- email="alexeiturchin@gmail.com" -->
<!-- subject="Re: [sl4] Simple friendliness: plan B for AI" -->
<!-- id="AANLkTi=1v3DHBdHx8+fue8QvjurnPa6oKKXZacniDN3O@mail.gmail.com" -->
<!-- charset="UTF-8" -->
<!-- inreplyto="BLU137-W36F7115136935BBA6E6531B9330@phx.gbl" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Alexei Turchin (<a href="mailto:alexeiturchin@gmail.com?Subject=Re:%20[sl4]%20Simple%20friendliness:%20plan%20B%20for%20AI"><em>alexeiturchin@gmail.com</em></a>)<br>
<strong>Date:</strong> Fri Nov 12 2010 - 14:21:06 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="21049.html">Matt Mahoney: "Re: [sl4] Simple friendliness: plan B for AI"</a>
<li><strong>Previous message:</strong> <a href="21047.html">Piaget Modeler: "RE: [sl4] Simple friendliness: plan B for AI"</a>
<li><strong>In reply to:</strong> <a href="21047.html">Piaget Modeler: "RE: [sl4] Simple friendliness: plan B for AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21049.html">Matt Mahoney: "Re: [sl4] Simple friendliness: plan B for AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21048">[ date ]</a>
<a href="index.html#21048">[ thread ]</a>
<a href="subject.html#21048">[ subject ]</a>
<a href="author.html#21048">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I think that the first rule for military robots is that they intelligence
<br>
should not evolve and its interests should be limited to their body and,
<br>
say, 100 meter neighborhood. In this case no global risk is associated with
<br>
such robotic AI.
<br>
<p>On Fri, Nov 12, 2010 at 9:47 PM, Piaget Modeler
<br>
&lt;<a href="mailto:piagetmodeler@hotmail.com?Subject=Re:%20[sl4]%20Simple%20friendliness:%20plan%20B%20for%20AI">piagetmodeler@hotmail.com</a>&gt;wrote:
<br>
<p><em>&gt;  Robots in Iraq and Afghanistan
</em><br>
<em>&gt;
</em><br>
<em>&gt; <a href="http://www.pbs.org/newshour/bb/science/jan-june09/robots_04-23.html">http://www.pbs.org/newshour/bb/science/jan-june09/robots_04-23.html</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; What do we do about Asimov's three laws where military AI is concerned?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; ------------------------------
</em><br>
<em>&gt; Date: Tue, 9 Nov 2010 22:07:20 +0300
</em><br>
<em>&gt; Subject: [sl4] Simple friendliness: plan B for AI
</em><br>
<em>&gt; From: <a href="mailto:alexeiturchin@gmail.com?Subject=Re:%20[sl4]%20Simple%20friendliness:%20plan%20B%20for%20AI">alexeiturchin@gmail.com</a>
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=Re:%20[sl4]%20Simple%20friendliness:%20plan%20B%20for%20AI">sl4@sl4.org</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Simple friendliness
</em><br>
<em>&gt;
</em><br>
<em>&gt; Friendly AI, as believes by Hanson, is doomed to failure, since if the
</em><br>
<em>&gt; friendliness system is too complicated, the other AI projects generally will
</em><br>
<em>&gt; not apply it. In addition, any system of friendliness may still be doomed to
</em><br>
<em>&gt; failure - and more unclear it is, the more chances it has to fail.  By fail
</em><br>
<em>&gt; I mean that it will not ne accepted by most succseful AI project.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Thus, the friendliness system should be simple and clear, so it can be
</em><br>
<em>&gt; spread as widely as possible.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; I roughly figured, what principles could form the basis of a simple
</em><br>
<em>&gt; friendliness:
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; 0) Any one should understood that AI can be global risks and the
</em><br>
<em>&gt; friendliness of the system is needed. This basic understanding should be
</em><br>
<em>&gt; shared by maximum number of AI-groups (I think this is alrready done)
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1) Architecture of AI should be such that it would use rules explicitly.
</em><br>
<em>&gt; (I.e. no genetic algorithms or neural networks)
</em><br>
<em>&gt;
</em><br>
<em>&gt; 2) the AI should obey commands of its creator, and clearly understand who
</em><br>
<em>&gt; is the creator and what is the format of commands.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 3) AI must comply with all existing CRIMINAL an CIVIL laws. These laws are
</em><br>
<em>&gt; the first attempt to create a friendly AI â€“ in the form of state. That is an
</em><br>
<em>&gt; attempt to describe good, safe human life using a system of rules. (Or
</em><br>
<em>&gt; system of precedents). And the number of volumes of laws and their
</em><br>
<em>&gt; interpretation speaks about complexity of this problem - but it has already
</em><br>
<em>&gt; been solved and it is not a sin to use the solution.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 4) the AI should not have secrets from their creator. Moreover, he is
</em><br>
<em>&gt; obliged to inform him of all his thoughts. This avoids rebel of AI.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 5) Each seldoptimizing of AI should be dosed in portions, under the control
</em><br>
<em>&gt; of the creator. And after each step mustbe run a full scan of system goals
</em><br>
<em>&gt; and effectivness.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 6) the AI should be tested in a virtual environment (such as Secnod Life)
</em><br>
<em>&gt; for safety and adequacy.
</em><br>
<em>&gt;
</em><br>
<em>&gt; 7) AI projects should be registrated by centralized oversight bodies and
</em><br>
<em>&gt; receive safety certification from it.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Such obvious steps do not create absolutely safe AI (you can figure out how
</em><br>
<em>&gt; to bypass it out), but they make it much safer. In addition, they look quite
</em><br>
<em>&gt; natural and reasonable so they could be use by any AI project with different
</em><br>
<em>&gt; variations.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;  Most of this steps are fallable. But without them the situation would be
</em><br>
<em>&gt; even worse. If each steps increase safety two times, 8 steps will increase
</em><br>
<em>&gt; it 256 times, which is good. Simple friendliness is plan B if mathematical
</em><br>
<em>&gt; FAI fails.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="21049.html">Matt Mahoney: "Re: [sl4] Simple friendliness: plan B for AI"</a>
<li><strong>Previous message:</strong> <a href="21047.html">Piaget Modeler: "RE: [sl4] Simple friendliness: plan B for AI"</a>
<li><strong>In reply to:</strong> <a href="21047.html">Piaget Modeler: "RE: [sl4] Simple friendliness: plan B for AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21049.html">Matt Mahoney: "Re: [sl4] Simple friendliness: plan B for AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21048">[ date ]</a>
<a href="index.html#21048">[ thread ]</a>
<a href="subject.html#21048">[ subject ]</a>
<a href="author.html#21048">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
