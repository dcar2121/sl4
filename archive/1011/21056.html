<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=Windows-1252">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: RE: [sl4] Simple friendliness: plan B for AI</title>
<meta name="Author" content="Piaget Modeler (piagetmodeler@hotmail.com)">
<meta name="Subject" content="RE: [sl4] Simple friendliness: plan B for AI">
<meta name="Date" content="2010-11-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>RE: [sl4] Simple friendliness: plan B for AI</h1>
<!-- received="Sun Nov 14 21:09:45 2010" -->
<!-- isoreceived="20101115040945" -->
<!-- sent="Sun, 14 Nov 2010 20:09:39 -0800" -->
<!-- isosent="20101115040939" -->
<!-- name="Piaget Modeler" -->
<!-- email="piagetmodeler@hotmail.com" -->
<!-- subject="RE: [sl4] Simple friendliness: plan B for AI" -->
<!-- id="blu137-w178DC2EABF84D4556C0BF6B9360@phx.gbl" -->
<!-- charset="Windows-1252" -->
<!-- inreplyto="20101115020526.CF2CAD2897@fungible.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Piaget Modeler (<a href="mailto:piagetmodeler@hotmail.com?Subject=RE:%20[sl4]%20Simple%20friendliness:%20plan%20B%20for%20AI"><em>piagetmodeler@hotmail.com</em></a>)<br>
<strong>Date:</strong> Sun Nov 14 2010 - 21:09:39 MST
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="21057.html">Luke Griffiths: "Re: [sl4] Simple friendliness: plan B for AI"</a>
<li><strong>Previous message:</strong> <a href="21055.html">Tim Freeman: "Re: [sl4] Simple friendliness: plan B for AI"</a>
<li><strong>In reply to:</strong> <a href="21055.html">Tim Freeman: "Re: [sl4] Simple friendliness: plan B for AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21057.html">Luke Griffiths: "Re: [sl4] Simple friendliness: plan B for AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21056">[ date ]</a>
<a href="index.html#21056">[ thread ]</a>
<a href="subject.html#21056">[ subject ]</a>
<a href="author.html#21056">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
There isn't just one notion of free-will morality. Obviously Gordon Worley has never been to prison.
<br>
Consequences must be defined for what society deems &quot;anti-social&quot; behavior, otherwise people will 
<br>
run amok.  We have devices such as religion, or the penal system to keep people in line.  For nation
<br>
states that decide to redefine the notion of morality in their favor, we have wars.  What will we use 
<br>
for robots who determine that they can do as they please? 
<br>
<p>Eliezer Yudkowski's message is that we need to design friendly AI's, that will solve our potential 
<br>
problem.  This is optimistic thinking. He says,  &quot;It doesn’t make sense to ask whether “AIs” will be 
<br>
friendly or hostile.&quot;  Since we have the creative power, we can design them to be friendly.  
<br>
Unfortunately this view neglects evolution.  No matter how we design systems initially, an 
<br>
evolutionary system may evolve (positively or negatively) well beyond the scope and expectations 
<br>
of its creators. 
<br>
<p>In any reasonable risk analysis, one should look at best-case, worst-case, and average-case 
<br>
scenarios, expecting the average case to occur. If today our average case is that governments
<br>
maintain armies and maintain scientists who devise more and more sophisticated weapons 
<br>
including robots and AI systems for the purposes of annihilating their &quot;enemies&quot;, the average
<br>
case projection then is to expect that this will continue into our future.  The best case is that 
<br>
we as a species learns to forego war before the Singularity.  This is unlikely. The worst case 
<br>
is that the AI we develop that becomes super intelligent is military AI.
<br>
<p>My original question remains unanswered.  If Asimov's three laws are insufficient, or are considered fictional and hence irrelevant for building / creating real robots and AI.  Then 
<br>
how do we address the question of military AI, even now as more sophisticated robots are
<br>
being deployed to the battlefield?
<br>
<p>PM.
<br>
<p><p><em>&gt; Date: Sun, 14 Nov 2010 17:56:37 -0700
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=RE:%20[sl4]%20Simple%20friendliness:%20plan%20B%20for%20AI">sl4@sl4.org</a>
</em><br>
<em>&gt; Subject: Re: [sl4] Simple friendliness: plan B for AI
</em><br>
<em>&gt; From: <a href="mailto:tim@fungible.com?Subject=RE:%20[sl4]%20Simple%20friendliness:%20plan%20B%20for%20AI">tim@fungible.com</a>
</em><br>
<em>&gt; 
</em><br>
<em>&gt; From: Piaget Modeler &lt;<a href="mailto:piagetmodeler@hotmail.com?Subject=RE:%20[sl4]%20Simple%20friendliness:%20plan%20B%20for%20AI">piagetmodeler@hotmail.com</a>&gt;
</em><br>
<em>&gt; &gt;What do we do about Asimov's three laws where military AI is concerned?
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Ignore them.  They were contrived to give Asimov interesting conflict
</em><br>
<em>&gt; he could write about, not to solve any real-world problems.  This is
</em><br>
<em>&gt; discussed at <a href="http://www.asimovlaws.com/">http://www.asimovlaws.com/</a>.  
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Eliezer says not to generalize from fiction, and I agree.  See
</em><br>
<em>&gt; <a href="http://www.imminst.org/forum/index.php?s=&amp;act=ST&amp;f=67&amp;t=1097&amp;st=0">http://www.imminst.org/forum/index.php?s=&amp;act=ST&amp;f=67&amp;t=1097&amp;st=0</a> [sl4] Simple friendliness: plan B for AI</em><br>
<em>&gt; -- 
</em><br>
<em>&gt; Tim Freeman               <a href="http://www.fungible.com">http://www.fungible.com</a>           <a href="mailto:tim@fungible.com?Subject=RE:%20[sl4]%20Simple%20friendliness:%20plan%20B%20for%20AI">tim@fungible.com</a>
</em><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="21057.html">Luke Griffiths: "Re: [sl4] Simple friendliness: plan B for AI"</a>
<li><strong>Previous message:</strong> <a href="21055.html">Tim Freeman: "Re: [sl4] Simple friendliness: plan B for AI"</a>
<li><strong>In reply to:</strong> <a href="21055.html">Tim Freeman: "Re: [sl4] Simple friendliness: plan B for AI"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="21057.html">Luke Griffiths: "Re: [sl4] Simple friendliness: plan B for AI"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#21056">[ date ]</a>
<a href="index.html#21056">[ thread ]</a>
<a href="subject.html#21056">[ subject ]</a>
<a href="author.html#21056">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:05 MDT
</em></small></p>
</body>
</html>
