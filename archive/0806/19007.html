<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] Is there a model for RSI?</title>
<meta name="Author" content="Mark Nuzzolilo (nuzz604@gmail.com)">
<meta name="Subject" content="Re: [sl4] Is there a model for RSI?">
<meta name="Date" content="2008-06-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] Is there a model for RSI?</h1>
<!-- received="Mon Jun 16 11:22:11 2008" -->
<!-- isoreceived="20080616172211" -->
<!-- sent="Mon, 16 Jun 2008 10:19:24 -0700" -->
<!-- isosent="20080616171924" -->
<!-- name="Mark Nuzzolilo" -->
<!-- email="nuzz604@gmail.com" -->
<!-- subject="Re: [sl4] Is there a model for RSI?" -->
<!-- id="ef87e2440806161019g7a89ec0cg355f9a3b7af585ea@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="769972.39804.qm@web51906.mail.re2.yahoo.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Mark Nuzzolilo (<a href="mailto:nuzz604@gmail.com?Subject=Re:%20[sl4]%20Is%20there%20a%20model%20for%20RSI?"><em>nuzz604@gmail.com</em></a>)<br>
<strong>Date:</strong> Mon Jun 16 2008 - 11:19:24 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19008.html">William Pearson: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>Previous message:</strong> <a href="19006.html">Stathis Papaioannou: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>In reply to:</strong> <a href="18997.html">Matt Mahoney: "[sl4] Is there a model for RSI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19008.html">William Pearson: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>Reply:</strong> <a href="19008.html">William Pearson: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>Reply:</strong> <a href="19011.html">Krekoski Ross: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>Reply:</strong> <a href="19031.html">Peter de Blanc: "Re: [sl4] Is there a model for RSI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19007">[ date ]</a>
<a href="index.html#19007">[ thread ]</a>
<a href="subject.html#19007">[ subject ]</a>
<a href="author.html#19007">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
I'll take a swing at this.
<br>
<p>Let's start with the assumption that a machine cannot output a machine of
<br>
greater algorithmic complexity.
<br>
Now for a thought experiment put humans in that same category.  A single
<br>
human would not be able to produce something &quot;greater&quot; than itself.  The
<br>
details of this are unimportant.  The point is that when you take a larger
<br>
group of humans, the complexity increases and you can now produce a machine
<br>
potentially greater than a single human.  This machine could then improve
<br>
the intelligence or ability of single humans at a time, and then those
<br>
humans could then create a greater machine.
<br>
<p>This is obviously not a &quot;typical&quot; RSI scenario but if my reasoning is
<br>
correct here (correct me if I am wrong), then in theory RSI would be
<br>
possible even by taking this concept and abstracting it to specific (and
<br>
properly designed) AGI components rather than specific components of a group
<br>
of humans (the humans themselves).
<br>
<p>Mark Nuzzolilo
<br>
<p><p>On Sun, Jun 15, 2008 at 1:18 PM, Matt Mahoney &lt;<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20Is%20there%20a%20model%20for%20RSI?">matmahoney@yahoo.com</a>&gt; wrote:
<br>
<p><em>&gt; Is there a model of recursive self improvement? A model would be a
</em><br>
<em>&gt; simulated environment in which agents improve themselves in terms of
</em><br>
<em>&gt; intelligence or some appropriate measure. This would not include genetic
</em><br>
<em>&gt; algorithms, i.e. agents make random changes to themselves or copies,
</em><br>
<em>&gt; followed by selection by an external fitness function not of the agent's
</em><br>
<em>&gt; choosing. It would also not include simulations where agents receiving
</em><br>
<em>&gt; external information on how to improve themselves. They have to figure it
</em><br>
<em>&gt; out for themselves.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The premise of the singularity is that humans will soon reach the point
</em><br>
<em>&gt; where we can enhance our own intelligence or make machines that are more
</em><br>
<em>&gt; intelligent than us. For example, we could genetically engineer humans for
</em><br>
<em>&gt; bigger brains, faster neurons, more synapses, etc. Alternatively, we could
</em><br>
<em>&gt; upload to computers, then upgrade them with more memory, more and faster
</em><br>
<em>&gt; processors, more I/O, more efficient software, etc. Or we could simply build
</em><br>
<em>&gt; intelligent machines or robots that would do the same.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Arguments in favor of RSI:
</em><br>
<em>&gt; - Humans can improve themselves by going to school, practicing skills,
</em><br>
<em>&gt; reading, etc. (arguably not RSI).
</em><br>
<em>&gt; - Moore's Law predicts computers will have as much computing power as human
</em><br>
<em>&gt; brains in a few decades, or sooner if we figure out more efficient
</em><br>
<em>&gt; algorithms for AI.
</em><br>
<em>&gt; - Increasing machine intelligence should be a straightforward hardware
</em><br>
<em>&gt; upgrade.
</em><br>
<em>&gt; - Evolution produced human brains capable of learning 10^9 bits of
</em><br>
<em>&gt; knowledge (stored using 10^15 synapses) with only 10^7 bits of genetic
</em><br>
<em>&gt; information. Therefore we are not cognitively limited from understanding our
</em><br>
<em>&gt; own code.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Arguments against RSI:
</em><br>
<em>&gt; - A Turing machine cannot output a machine of greater algorithmic
</em><br>
<em>&gt; complexity.
</em><br>
<em>&gt; - If an agent could reliably produce or test a more intelligent agent, it
</em><br>
<em>&gt; would already be that smart.
</em><br>
<em>&gt; - We do not know how to test for IQs above 200.
</em><br>
<em>&gt; - There are currently no non-evolutionary models of RSI in humans, animals,
</em><br>
<em>&gt; machines, or software (AFAIK, that is my question).
</em><br>
<em>&gt;
</em><br>
<em>&gt; If RSI is possible, then we should be able to model simple environments
</em><br>
<em>&gt; with agents (with less than human intelligence) that could self improve (up
</em><br>
<em>&gt; to the computational limits of the model) without relying on an external
</em><br>
<em>&gt; intelligence test or fitness function. The agents must figure out for
</em><br>
<em>&gt; themselves how to improve their intelligence. How could this be done? We
</em><br>
<em>&gt; already have genetic algorithms in simulated environments that are much
</em><br>
<em>&gt; simpler than biology. Perhaps agents could modify their own code in some
</em><br>
<em>&gt; simplified or abstract language of the designer's choosing. If no such model
</em><br>
<em>&gt; exists, then why should we believe that humans are on the threshold of RSI?
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; -- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20Is%20there%20a%20model%20for%20RSI?">matmahoney@yahoo.com</a>
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19008.html">William Pearson: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>Previous message:</strong> <a href="19006.html">Stathis Papaioannou: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>In reply to:</strong> <a href="18997.html">Matt Mahoney: "[sl4] Is there a model for RSI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19008.html">William Pearson: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>Reply:</strong> <a href="19008.html">William Pearson: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>Reply:</strong> <a href="19011.html">Krekoski Ross: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>Reply:</strong> <a href="19031.html">Peter de Blanc: "Re: [sl4] Is there a model for RSI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19007">[ date ]</a>
<a href="index.html#19007">[ thread ]</a>
<a href="subject.html#19007">[ subject ]</a>
<a href="author.html#19007">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
