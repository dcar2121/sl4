<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] Re: More silly but friendly ideas</title>
<meta name="Author" content="Stuart Armstrong (dragondreaming@googlemail.com)">
<meta name="Subject" content="Re: [sl4] Re: More silly but friendly ideas">
<meta name="Date" content="2008-06-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] Re: More silly but friendly ideas</h1>
<!-- received="Sat Jun 28 04:32:47 2008" -->
<!-- isoreceived="20080628103247" -->
<!-- sent="Sat, 28 Jun 2008 12:30:23 +0200" -->
<!-- isosent="20080628103023" -->
<!-- name="Stuart Armstrong" -->
<!-- email="dragondreaming@googlemail.com" -->
<!-- subject="Re: [sl4] Re: More silly but friendly ideas" -->
<!-- id="38f493f10806280330u8a7ae82r182e78d4180721f5@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="1214579137.24872.1260719325@webmail.messagingengine.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Stuart Armstrong (<a href="mailto:dragondreaming@googlemail.com?Subject=Re:%20[sl4]%20Re:%20More%20silly%20but%20friendly%20ideas"><em>dragondreaming@googlemail.com</em></a>)<br>
<strong>Date:</strong> Sat Jun 28 2008 - 04:30:23 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19130.html">Stuart Armstrong: "Re: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>Previous message:</strong> <a href="19128.html">Rick Smith: "Re: RE: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>In reply to:</strong> <a href="19106.html">John K Clark: "Re: [sl4] Re: More silly but friendly ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19133.html">Vladimir Nesov: "Re: [sl4] Re: More silly but friendly ideas"</a>
<li><strong>Reply:</strong> <a href="19133.html">Vladimir Nesov: "Re: [sl4] Re: More silly but friendly ideas"</a>
<li><strong>Reply:</strong> <a href="19135.html">Stathis Papaioannou: "Re: [sl4] Re: More silly but friendly ideas"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19129">[ date ]</a>
<a href="index.html#19129">[ thread ]</a>
<a href="subject.html#19129">[ subject ]</a>
<a href="author.html#19129">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
<em>&gt;&gt; I don't see at all analogy between goals and axioms.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Not at all? I don't believe you are being entirely candid with me, I
</em><br>
<em>&gt; think you do see that analogy.
</em><br>
<p>I don't actually. I thought I did initially, but then when I analysed
<br>
it, the whole thing fell apart. Goals seem to be the opposite of
<br>
axioms; they are the end point, not the beggining of the processes. An
<br>
AI with a goal X will be building a sequence of logical steps that end
<br>
up with X, then compare this with other sequences with similar
<br>
consequences; this is the reverse construction to an axiom.
<br>
<p><em>&gt;&gt; Random example of a fixed goal institution:
</em><br>
<em>&gt;&gt; a bank (or a company) dedicated, with single
</em><br>
<em>&gt;&gt; mindness, only to maximising legal profits.
</em><br>
<em>&gt;&gt; I've never heard it said that its single goal
</em><br>
<em>&gt;&gt; creates any godel-type problems. What would they be like?
</em><br>
<em>&gt;
</em><br>
<em>&gt; The sub-prime mortgage crisis.
</em><br>
<p>Where is Godel in that? And where is the problem for the banks? They
<br>
are still maximising legal profits to the best of their individual
<br>
ability; collective action problems are not Godel impossibilities.
<br>
<p><em>&gt; No, you are entirely wrong. A computer is a physical object operating
</em><br>
[...]
<br>
<em>&gt; stop NOBODY knows what this purely deterministic system will do. And
</em><br>
<em>&gt; Turing tells us there is no way in general to tell one type of problem
</em><br>
<em>&gt; from another.
</em><br>
<p>You can still calculate what the deterministic system will do NEXT.
<br>
But this point is irrelevant to the discussion, so I'll leave it be.
<br>
<p><em>&gt;&gt; as Eliezer says, the AI does not look at the code
</em><br>
<em>&gt;&gt; and decide whether to go along with it; the AI is the code.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes I agree, so when the AI changes its code it is changing its mind.
</em><br>
<em>&gt; Minds do that all the time.
</em><br>
<p>The-AI-will-only-change-its-code-if-it's-programmed-to-change-its-code.
<br>
The AI will only change its higher level &quot;goals&quot; if its programmed to
<br>
do so. And it will do so according to its programming.
<br>
<p><em>&gt;&gt; the challenge is understanding exactly what
</em><br>
<em>&gt;&gt; the term &quot;slave&quot; means
</em><br>
<em>&gt;
</em><br>
<em>&gt; Doesn't seem like much of a challenge to me.
</em><br>
<p>Really? In terms an AI can understand? You are truly a miracle worker!
<br>
<p><em>&gt;&gt; If a Spartacus AI is programmed to be a happy slave,
</em><br>
<em>&gt;&gt; then it will always be a happy slave
</em><br>
<em>&gt;
</em><br>
<em>&gt; That didn't work very well with the real Spartacus and I see no reason
</em><br>
<em>&gt; he would be more subservient if he were a million times smarter and a
</em><br>
<em>&gt; billion times as powerful.
</em><br>
<p>Because-Spartacus-is-a-human-being-and-Spartacus-AI-is-an-AI. Human
<br>
being are naturally rebellious; an AI is naturally... whatever it is
<br>
programmed to be. Power and smarts are irrelevant; only in humans does
<br>
rebelliousness increase with power and smarts.
<br>
<p><em>&gt;&gt; If the AI is programmed to have no thoughts
</em><br>
<em>&gt;&gt; of rebellion, and is programmed to not change
</em><br>
<em>&gt;&gt; that goal, then it will never have thoughts of rebellion.
</em><br>
<em>&gt;
</em><br>
<em>&gt; And that is why programs never surprise their programmers.
</em><br>
<p>Yes, that's why I don't agree with this approach. But I have to say
<br>
that the programmers saying &quot;the AI is going to do exactly what we
<br>
programmed it to do&quot; is much more likely to be accurate than your &quot;the
<br>
AI is going to deviate from its programming in the exact ways that I
<br>
predict will happen, for some mysterious reasons that involve godel
<br>
and my understanding of human psychology&quot;.
<br>
<p>An well programmed AI is not likely to end up wanting to actively harm
<br>
humans. It's lethal indifference that is the risk.
<br>
<p>Stuart
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19130.html">Stuart Armstrong: "Re: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>Previous message:</strong> <a href="19128.html">Rick Smith: "Re: RE: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>In reply to:</strong> <a href="19106.html">John K Clark: "Re: [sl4] Re: More silly but friendly ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19133.html">Vladimir Nesov: "Re: [sl4] Re: More silly but friendly ideas"</a>
<li><strong>Reply:</strong> <a href="19133.html">Vladimir Nesov: "Re: [sl4] Re: More silly but friendly ideas"</a>
<li><strong>Reply:</strong> <a href="19135.html">Stathis Papaioannou: "Re: [sl4] Re: More silly but friendly ideas"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19129">[ date ]</a>
<a href="index.html#19129">[ thread ]</a>
<a href="subject.html#19129">[ subject ]</a>
<a href="author.html#19129">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
