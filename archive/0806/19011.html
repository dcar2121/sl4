<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] Is there a model for RSI?</title>
<meta name="Author" content="Krekoski Ross (rosskrekoski@gmail.com)">
<meta name="Subject" content="Re: [sl4] Is there a model for RSI?">
<meta name="Date" content="2008-06-16">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] Is there a model for RSI?</h1>
<!-- received="Mon Jun 16 21:18:38 2008" -->
<!-- isoreceived="20080617031838" -->
<!-- sent="Tue, 17 Jun 2008 11:16:06 +0800" -->
<!-- isosent="20080617031606" -->
<!-- name="Krekoski Ross" -->
<!-- email="rosskrekoski@gmail.com" -->
<!-- subject="Re: [sl4] Is there a model for RSI?" -->
<!-- id="87478b5d0806162016l34a4e2d6odfd67acd5eb13507@mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="ef87e2440806161019g7a89ec0cg355f9a3b7af585ea@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Krekoski Ross (<a href="mailto:rosskrekoski@gmail.com?Subject=Re:%20[sl4]%20Is%20there%20a%20model%20for%20RSI?"><em>rosskrekoski@gmail.com</em></a>)<br>
<strong>Date:</strong> Mon Jun 16 2008 - 21:16:06 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19012.html">Mark Nuzzolilo: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>Previous message:</strong> <a href="19010.html">Anthony Berglas: "Re: [sl4] Re: Paper: Artificial Intelligence will Kill our Grandchildren"</a>
<li><strong>In reply to:</strong> <a href="19007.html">Mark Nuzzolilo: "Re: [sl4] Is there a model for RSI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19012.html">Mark Nuzzolilo: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>Reply:</strong> <a href="19012.html">Mark Nuzzolilo: "Re: [sl4] Is there a model for RSI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19011">[ date ]</a>
<a href="index.html#19011">[ thread ]</a>
<a href="subject.html#19011">[ subject ]</a>
<a href="author.html#19011">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
You would run into a complexity ceiling if it was machines improving
<br>
machines without external input.
<br>
<p>Ross
<br>
<p>On Tue, Jun 17, 2008 at 1:19 AM, Mark Nuzzolilo &lt;<a href="mailto:nuzz604@gmail.com?Subject=Re:%20[sl4]%20Is%20there%20a%20model%20for%20RSI?">nuzz604@gmail.com</a>&gt; wrote:
<br>
<p><em>&gt; I'll take a swing at this.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Let's start with the assumption that a machine cannot output a machine of
</em><br>
<em>&gt; greater algorithmic complexity.
</em><br>
<em>&gt; Now for a thought experiment put humans in that same category.  A single
</em><br>
<em>&gt; human would not be able to produce something &quot;greater&quot; than itself.  The
</em><br>
<em>&gt; details of this are unimportant.  The point is that when you take a larger
</em><br>
<em>&gt; group of humans, the complexity increases and you can now produce a machine
</em><br>
<em>&gt; potentially greater than a single human.  This machine could then improve
</em><br>
<em>&gt; the intelligence or ability of single humans at a time, and then those
</em><br>
<em>&gt; humans could then create a greater machine.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This is obviously not a &quot;typical&quot; RSI scenario but if my reasoning is
</em><br>
<em>&gt; correct here (correct me if I am wrong), then in theory RSI would be
</em><br>
<em>&gt; possible even by taking this concept and abstracting it to specific (and
</em><br>
<em>&gt; properly designed) AGI components rather than specific components of a group
</em><br>
<em>&gt; of humans (the humans themselves).
</em><br>
<em>&gt;
</em><br>
<em>&gt; Mark Nuzzolilo
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; On Sun, Jun 15, 2008 at 1:18 PM, Matt Mahoney &lt;<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20Is%20there%20a%20model%20for%20RSI?">matmahoney@yahoo.com</a>&gt;
</em><br>
<em>&gt; wrote:
</em><br>
<em>&gt;
</em><br>
<em>&gt;&gt; Is there a model of recursive self improvement? A model would be a
</em><br>
<em>&gt;&gt; simulated environment in which agents improve themselves in terms of
</em><br>
<em>&gt;&gt; intelligence or some appropriate measure. This would not include genetic
</em><br>
<em>&gt;&gt; algorithms, i.e. agents make random changes to themselves or copies,
</em><br>
<em>&gt;&gt; followed by selection by an external fitness function not of the agent's
</em><br>
<em>&gt;&gt; choosing. It would also not include simulations where agents receiving
</em><br>
<em>&gt;&gt; external information on how to improve themselves. They have to figure it
</em><br>
<em>&gt;&gt; out for themselves.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; The premise of the singularity is that humans will soon reach the point
</em><br>
<em>&gt;&gt; where we can enhance our own intelligence or make machines that are more
</em><br>
<em>&gt;&gt; intelligent than us. For example, we could genetically engineer humans for
</em><br>
<em>&gt;&gt; bigger brains, faster neurons, more synapses, etc. Alternatively, we could
</em><br>
<em>&gt;&gt; upload to computers, then upgrade them with more memory, more and faster
</em><br>
<em>&gt;&gt; processors, more I/O, more efficient software, etc. Or we could simply build
</em><br>
<em>&gt;&gt; intelligent machines or robots that would do the same.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Arguments in favor of RSI:
</em><br>
<em>&gt;&gt; - Humans can improve themselves by going to school, practicing skills,
</em><br>
<em>&gt;&gt; reading, etc. (arguably not RSI).
</em><br>
<em>&gt;&gt; - Moore's Law predicts computers will have as much computing power as
</em><br>
<em>&gt;&gt; human brains in a few decades, or sooner if we figure out more efficient
</em><br>
<em>&gt;&gt; algorithms for AI.
</em><br>
<em>&gt;&gt; - Increasing machine intelligence should be a straightforward hardware
</em><br>
<em>&gt;&gt; upgrade.
</em><br>
<em>&gt;&gt; - Evolution produced human brains capable of learning 10^9 bits of
</em><br>
<em>&gt;&gt; knowledge (stored using 10^15 synapses) with only 10^7 bits of genetic
</em><br>
<em>&gt;&gt; information. Therefore we are not cognitively limited from understanding our
</em><br>
<em>&gt;&gt; own code.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Arguments against RSI:
</em><br>
<em>&gt;&gt; - A Turing machine cannot output a machine of greater algorithmic
</em><br>
<em>&gt;&gt; complexity.
</em><br>
<em>&gt;&gt; - If an agent could reliably produce or test a more intelligent agent, it
</em><br>
<em>&gt;&gt; would already be that smart.
</em><br>
<em>&gt;&gt; - We do not know how to test for IQs above 200.
</em><br>
<em>&gt;&gt; - There are currently no non-evolutionary models of RSI in humans,
</em><br>
<em>&gt;&gt; animals, machines, or software (AFAIK, that is my question).
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; If RSI is possible, then we should be able to model simple environments
</em><br>
<em>&gt;&gt; with agents (with less than human intelligence) that could self improve (up
</em><br>
<em>&gt;&gt; to the computational limits of the model) without relying on an external
</em><br>
<em>&gt;&gt; intelligence test or fitness function. The agents must figure out for
</em><br>
<em>&gt;&gt; themselves how to improve their intelligence. How could this be done? We
</em><br>
<em>&gt;&gt; already have genetic algorithms in simulated environments that are much
</em><br>
<em>&gt;&gt; simpler than biology. Perhaps agents could modify their own code in some
</em><br>
<em>&gt;&gt; simplified or abstract language of the designer's choosing. If no such model
</em><br>
<em>&gt;&gt; exists, then why should we believe that humans are on the threshold of RSI?
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; -- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20Is%20there%20a%20model%20for%20RSI?">matmahoney@yahoo.com</a>
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19012.html">Mark Nuzzolilo: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>Previous message:</strong> <a href="19010.html">Anthony Berglas: "Re: [sl4] Re: Paper: Artificial Intelligence will Kill our Grandchildren"</a>
<li><strong>In reply to:</strong> <a href="19007.html">Mark Nuzzolilo: "Re: [sl4] Is there a model for RSI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19012.html">Mark Nuzzolilo: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>Reply:</strong> <a href="19012.html">Mark Nuzzolilo: "Re: [sl4] Is there a model for RSI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19011">[ date ]</a>
<a href="index.html#19011">[ thread ]</a>
<a href="subject.html#19011">[ subject ]</a>
<a href="author.html#19011">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
