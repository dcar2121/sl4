<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] End to violence and government [Was:Signaling after a singularity]</title>
<meta name="Author" content="Bryan Bishop (kanzure@gmail.com)">
<meta name="Subject" content="Re: [sl4] End to violence and government [Was:Signaling after a singularity]">
<meta name="Date" content="2008-06-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] End to violence and government [Was:Signaling after a singularity]</h1>
<!-- received="Fri Jun 27 09:00:21 2008" -->
<!-- isoreceived="20080627150021" -->
<!-- sent="Fri, 27 Jun 2008 10:02:24 -0500" -->
<!-- isosent="20080627150224" -->
<!-- name="Bryan Bishop" -->
<!-- email="kanzure@gmail.com" -->
<!-- subject="Re: [sl4] End to violence and government [Was:Signaling after a singularity]" -->
<!-- id="200806271002.24649.kanzure@gmail.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="38f493f10806270655s4bffa085t964aad86fb3a3f96@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bryan Bishop (<a href="mailto:kanzure@gmail.com?Subject=Re:%20[sl4]%20End%20to%20violence%20and%20government%20[Was:Signaling%20after%20a%20singularity]"><em>kanzure@gmail.com</em></a>)<br>
<strong>Date:</strong> Fri Jun 27 2008 - 09:02:24 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19106.html">John K Clark: "Re: [sl4] Re: More silly but friendly ideas"</a>
<li><strong>Previous message:</strong> <a href="19104.html">Stuart Armstrong: "Re: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>In reply to:</strong> <a href="19103.html">Stuart Armstrong: "[sl4] End to violence and government [Was:Signaling after a singularity]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19132.html">Stuart Armstrong: "Re: [sl4] End to violence and government [Was:Signaling after a singularity]"</a>
<li><strong>Reply:</strong> <a href="19132.html">Stuart Armstrong: "Re: [sl4] End to violence and government [Was:Signaling after a singularity]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19105">[ date ]</a>
<a href="index.html#19105">[ thread ]</a>
<a href="subject.html#19105">[ subject ]</a>
<a href="author.html#19105">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Friday 27 June 2008, Stuart Armstrong wrote:
<br>
<em>&gt; Sometime in the past, Bryan Bishop wrote:
</em><br>
<em>&gt; &gt; No, disuasion is not the point. Let's engineer the problems out of
</em><br>
<em>&gt; &gt; the system, the problems that violent tactics are exploiting.
</em><br>
<p>Some of the problems that are killing us:
<br>
<p>1) No backups. You are your only working copy. And if you die, your only 
<br>
bet at the moment is your published genome -- whether published over 
<br>
the net or published within a woman.
<br>
<p><em>&gt; Well, the problems seem rather fundamental. I'll try and break it
</em><br>
<em>&gt; down into assumptions, see if there's a way of getting round the
</em><br>
<em>&gt; problem.
</em><br>
<p>Yes, this is fundamental.
<br>
<p><em>&gt; Assumptions:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 1) Semi-coherent entities will continue to exist
</em><br>
<p>Just that we're supposing separate agents in this hypothetical context?
<br>
<p><em>&gt; 2) These entities will want ressources to do stuff
</em><br>
<p>Odd assumption. I think that the ability to use resources, seek them out 
<br>
and acquire them is more fundamental than any entity 'wanting' it. It's 
<br>
very hard to identify a 'want'. I'd label that as folk psychology 
<br>
actually. I'm not saying that I don't want food (for instance), or that 
<br>
I don't want to die, but rather that I /know/ that you can't actually 
<br>
identify within me my 'wants'. Because it's not quantified. So I don't 
<br>
know if this assumption is a good one to make.
<br>
<p><em>&gt; 3) Ressources are finite at any one time
</em><br>
<p>Caveat: current resources can be used to acquire more resources.
<br>
<p><em>&gt; 4) Demands on ressources will increase, absent an agreement between
</em><br>
<em>&gt; the entities, until it reaches the finite limit
</em><br>
<p>I'd like you to elaborate on this assumption. For instance, what is a 
<br>
demand? Are you talking about a drain on a system? Perhaps how much 
<br>
resources (1 kg H20 per hr?) a system is drawing from its connectivity? 
<br>
So in my physical-demand formulation, demand doesn't increase unless 
<br>
the design of the entities change or somesuch.
<br>
<p><em>&gt; 5) There exists entities that can make credible threats of violence
</em><br>
<p>Don't know what credible means here. Does it mean &quot;I'll shoot, I swear 
<br>
I'll do it,&quot; or something more certain?
<br>
<p><em>&gt; 6) There exist entities that will prefer to give up part of their
</em><br>
<em>&gt; ressources than to suffer the violence; these entities can be
</em><br>
<em>&gt; distinguished to some extent from those that do not
</em><br>
<p>Ok, like using energy to go to the next neighborly star system.
<br>
<p><em>&gt; and the big one:
</em><br>
<em>&gt;
</em><br>
<em>&gt; 7) The ressources gained by a threatning entity will be worth more to
</em><br>
<em>&gt; it than what it lost through threatning and occasionally carrying out
</em><br>
<em>&gt; its violence
</em><br>
<em>&gt;
</em><br>
<em>&gt; Let us add a singularity to the mix. Only 3) is guaranteed to stay
</em><br>
<em>&gt; true. If we assume that something like human beings continue to
</em><br>
<em>&gt; exist, then 1) and 2) will remain true. If these beings are free to
</em><br>
<em>&gt; do what they want, then 5) remains true. Now whether 4) is true is a
</em><br>
<em>&gt; judegment call (especially as ressources may be increasing all the
</em><br>
<em>&gt; time). But since it would only require one entity to want to get more
</em><br>
<em>&gt; and more and more ressources, and since we have an upper limit on how
</em><br>
<em>&gt; fast ressources can expand (and this limit is polynomial), then 4)
</em><br>
<p>Hm. I'd like to see a proof that resource acquisition is polynomial at 
<br>
best. I have a suspicion that it is actually exponential.
<br>
<p><em>&gt; will probably remain true.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Remains 6) and 7). The first part of 6) is probably true; you only
</em><br>
<em>&gt; need one &quot;coward&quot;, and the laws of thermodynamics imply that it's
</em><br>
<em>&gt; easier destroy something than to defend it. (I'm not thinking of
</em><br>
<em>&gt; threatning people's lives necessarily; something along the lines of
</em><br>
<em>&gt; &quot;give me half your house or I blow up all of it&quot; is enough).
</em><br>
<p>Hrm. So in after a singularity, wouldn't those people in that house have 
<br>
a few backups stored in various data centers in multiply redundant 
<br>
locations? It's not like that sort of equipment is going to be hard to 
<br>
come by. I'm not talking about a full instantation of everything that 
<br>
they are, but rather 'enough' -- it is indeed tragic to see any life 
<br>
die, but there _are_ ways that we can minimize the overall damage.
<br>
<p><em>&gt; The second half of 6) opens some fascinating possibilities. What if
</em><br>
<em>&gt; humanity was seeded with random quasi-humans who are similar to us in
</em><br>
<em>&gt; every way, but never give in to threatened violence? This is
</em><br>
<em>&gt; interesting, and would increase the cost of threatning violence.
</em><br>
<em>&gt; Maybe there's a idea here.
</em><br>
<p>Cost of threatening violence?
<br>
<p><em>&gt; Now 7), the usual point of these discussions. The whole question
</em><br>
<p>I disagree completely. But I'll explain this below/later.
<br>
<p><em>&gt; turns on the value of &quot;worth&quot;. There is the physical value of the
</em><br>
<em>&gt; ressources, the value of a reputation and other social factors, the
</em><br>
<em>&gt; feelings of the entity carrying out the threat, and the possible
</em><br>
<em>&gt; defenses and retaliations (before or after the event).
</em><br>
<p>Since after a singularity it would be easy to deploy entirely new 
<br>
civilizations (von Neumann probe, Bokov's civilization-in-a-box, etc.), 
<br>
I'll ignore all of those social factors since those can be either 
<br>
engineered, changed, hacked, whatever they want to do. As for the 
<br>
physical 'value' of resources. Arguably the only time that the value 
<br>
issue would come into play is when you do not have enough matter/energy 
<br>
or of the right type (etc) to get to a location where you can access 
<br>
more of what you need. So, in that case, you're going to have to start 
<br>
looking at the repositories around you and poking your nose abouts. 
<br>
This status of minimal matter/energy reserves implies that you messed 
<br>
up in your mat/ener management strategies, but that's not a big deal, 
<br>
why wouldn't there still be charities that might be interested in 
<br>
helping you along? And so on.
<br>
<p>* I'm also working on a hypothetical framework where systems can opt to 
<br>
share matter/energy communally in a manner that still maximizes 
<br>
individual use while fixing the scheduling problem. This would be an 
<br>
interesting alternative to &quot;every man for himself&quot;. I'm not suggesting 
<br>
this would be suitable for all systems, but I'm pretty sure such a 
<br>
design can work. Not communal sharing, but something different than 
<br>
that, since supposedly we'd be able to integrate or share intelligences 
<br>
and ideas and recombine them with significantly less groundwork that we 
<br>
have to do just now ...
<br>
<p><em>&gt; The feelings of the entity don't seem something we can relly on, even
</em><br>
<em>&gt; if there is increased empathy and understanding; some people are
</em><br>
<em>&gt; suicidal or self-harmers, so we can't trust that everyone will feel
</em><br>
<em>&gt; tremendously bad about using violence, all of the time.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Social factors seem unstable; if all law enforcement was removed from
</em><br>
<em>&gt; a country, you wouldn't see an immediate explosion of violence
</em><br>
<p>You're thinking pre-singularity. After a singularity, there wouldn't 
<br>
necessarily be a country, and there wouldn't necessarily be Social Law 
<br>
enforcement since there's no country. And let's be honest, it's not 
<br>
really Law Enforcement, but rather it's &quot;these are the guys that can 
<br>
press the buttons that can stop you&quot; more than anything else. We all 
<br>
have studied physics and know what a real, true law might look like. 
<br>
Gravity is a pretty good example. ;-)
<br>
<p><em>&gt; everywhere, as the social factors and norms hold it in check.
</em><br>
<em>&gt; However, as those who do resort to violence prosper, it will become
</em><br>
<em>&gt; normalised, and more and more will resort to it (if only in
</em><br>
<em>&gt; &quot;self-defense&quot;).
</em><br>
<em>&gt;
</em><br>
<em>&gt; What about reputation and the physical value of the ressources? In
</em><br>
<em>&gt; today's positive sum world, the physical value of a ressource is
</em><br>
<em>&gt; generally less important than a reputation (eg: countries that
</em><br>
<em>&gt; default don't easily get loans again). But reputation is not
</em><br>
<em>&gt; reliable; a mafia boss may only practice extortion to people he
</em><br>
<em>&gt; doesn't trade with, meaning that there is no drawback to trading with
</em><br>
<em>&gt; him, even if he's nasty apple. Maybe the values of the society will
</em><br>
<em>&gt; preclude trading with him? But these value are unstable, especially
</em><br>
<em>&gt; if he prospers through threatened violence. And &quot;trade with me or I
</em><br>
<em>&gt; will hurt you&quot; seems like a credible threat.
</em><br>
<p>I don't understand what you're talking about. The topic is the 
<br>
methodology of engineering fundamental problems out of our systems, 
<br>
within the context of those assumptions you presented, and within the 
<br>
context of a singularity. Who cares about trading with a mafia boss? 
<br>
Just go download the tech to acquire the resources on your own. I don't 
<br>
know why you still assume countries. I don't know why values of a 
<br>
society would matter .. if you really want to go trade with him, fork 
<br>
the society and go use one of those societies-in-a-box and be done with 
<br>
the problem.
<br>
<p><em>&gt; There remains one possibility: maybe the AI's who control most of the
</em><br>
<em>&gt; ressources will refuse to trade with him.
</em><br>
<p>Remains one ? How many did you consider ? I am confused. But I can agree 
<br>
that it is possible for nonhumans to maintain resources and caches and 
<br>
keep others out of it etc.
<br>
<p><em>&gt; Remaining is defense or retaliation. By definition, neither of them
</em><br>
<em>&gt; is enough just from the threatened entities. So, asent a government,
</em><br>
<em>&gt; what is needed is some system of militias, idealy temporary one (as
</em><br>
<em>&gt; permanent ones lead to competition for ressources between the groups,
</em><br>
<em>&gt; rather than between the individuals; MAD might work then, but that
</em><br>
<em>&gt; makes the militia into a geovernment, with a monopoly on outwards
</em><br>
<em>&gt; directed violence). This might work, if contracts are respected. So
</em><br>
<em>&gt; solving the problem of violence can be done, if contracts are always
</em><br>
<p>No, you're solving the problem of hiring protection. You need to look at 
<br>
the actual problem of the system -- your death and the possibility of 
<br>
injury that prohibits your advance in whatever it is that you do -- and 
<br>
not that others might throw a few eggs at your windows. Instead, get 
<br>
rid of the windows, or install windshield wipers to your windows, or 
<br>
build laser cannons that specifically target incoming eggs. Might 
<br>
misidentify other flying objects (birds?) for eggs, but it's the 
<br>
singularity, so I'm sure that can be fixed. ;-)
<br>
<p><em>&gt; respected. But this is not progress; if it were, getting everyone to
</em><br>
<em>&gt; sign a &quot;no violence&quot; contract would be enough. So this &quot;solution&quot; is
</em><br>
<em>&gt; strictly harder to implement than getting rid of violence in the
</em><br>
<em>&gt; first place.
</em><br>
<p>Your solution is completely bogus and isn't actually working on the 
<br>
fundamental problems of the human body or the agency that other 
<br>
entities might represent. Ooh. Let me try another method of explaining 
<br>
something. Let's engineer a new problem /into/ the system that is being 
<br>
exploited (the human person, or possibly another entity). This is 
<br>
actually, in practice, fairly easy. Just splice in a gene that provides 
<br>
a very specific disease, raise the human tissue culture, and you have 
<br>
your evidence that a new exploit has been established. In many cases, 
<br>
hereditary diseases are 'automatic', and in many other cases they can 
<br>
be induced due to environmental stimuli etc. So, therefore, the human 
<br>
is the system that we need to be working on. Okay, let's change the 
<br>
scenario up a little bit, and let's say that it's been a few weeks 
<br>
after the onset of a noted singularity (bleh), and for some reason we 
<br>
have robots that are running around with brains-in-a-jar or something. 
<br>
Okay, so let's short-circuit them. Run to them, open up their case, cut 
<br>
a few wires. Hey. Look at that, an exploit. Let's fix that sort of 
<br>
thing.
<br>
<p><em>&gt; So, apart from changing human nature, having only &quot;nice&quot; AIs, or some
</em><br>
<em>&gt; interesting manipulation of the second part of 6), I see no realistic
</em><br>
<em>&gt; way of doing away with governments, even after a singularity. And
</em><br>
<em>&gt; &quot;nice&quot; AI's would be a government in the soft sense.
</em><br>
<p>I don't see how you are able to get to those conclusions. Remember, 
<br>
governments are not magical entities. Let's replace the idea of a 
<br>
government with a more computational definition of it in this 
<br>
discussion. So, instead of it being just a government, it's really 
<br>
actually a communication coordination system for a number of people, 
<br>
no? What else is it? I suppose we could also model it with some email 
<br>
accounts flying around, some buildings and architectural infrastructure 
<br>
like that. So I don't see how this makes them magical or anything like 
<br>
that. It's not like the technological implementation of the government 
<br>
is anything magical, it's exactly technology that has been ruthlessly 
<br>
engineered. :-) So we could &quot;do away&quot; with governments that are 
<br>
completely failing at the human liberation project, or we could even 
<br>
spawn off as many as we want (civilization in a box) or really maybe 
<br>
consider the society-in-a-box. Implementation details are totally up to 
<br>
you when you find the available resources to implement it. Uh, but they 
<br>
are in fact somewhat restrained by your ability to program, but in the 
<br>
case of a singularity I don't see how that would be a Big Deal.
<br>
<p>I mentioned above that I wanted to explain that the issue of &quot;valued 
<br>
resources&quot; isn't the fundamental problem. The fundamental problem is 
<br>
the possibility of exploitation and not having redundancy or good 
<br>
backup strategies or good strategies at all. For instance, the majority 
<br>
of current human life, the 'strategy' of dying at the end. Holy hell 
<br>
man, that's going to end up with you _dead_.
<br>
<p>- Bryan
<br>
________________________________________
<br>
<a href="http://heybryan.org/">http://heybryan.org/</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19106.html">John K Clark: "Re: [sl4] Re: More silly but friendly ideas"</a>
<li><strong>Previous message:</strong> <a href="19104.html">Stuart Armstrong: "Re: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>In reply to:</strong> <a href="19103.html">Stuart Armstrong: "[sl4] End to violence and government [Was:Signaling after a singularity]"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19132.html">Stuart Armstrong: "Re: [sl4] End to violence and government [Was:Signaling after a singularity]"</a>
<li><strong>Reply:</strong> <a href="19132.html">Stuart Armstrong: "Re: [sl4] End to violence and government [Was:Signaling after a singularity]"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19105">[ date ]</a>
<a href="index.html#19105">[ thread ]</a>
<a href="subject.html#19105">[ subject ]</a>
<a href="author.html#19105">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
