<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] Re: More silly but friendly ideas</title>
<meta name="Author" content="Bryan Bishop (kanzure@gmail.com)">
<meta name="Subject" content="Re: [sl4] Re: More silly but friendly ideas">
<meta name="Date" content="2008-06-25">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] Re: More silly but friendly ideas</h1>
<!-- received="Wed Jun 25 08:25:13 2008" -->
<!-- isoreceived="20080625142513" -->
<!-- sent="Wed, 25 Jun 2008 09:27:33 -0500" -->
<!-- isosent="20080625142733" -->
<!-- name="Bryan Bishop" -->
<!-- email="kanzure@gmail.com" -->
<!-- subject="Re: [sl4] Re: More silly but friendly ideas" -->
<!-- id="200806250927.33247.kanzure@gmail.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="38f493f10806250152r1f013cem67d657e469d8a920@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bryan Bishop (<a href="mailto:kanzure@gmail.com?Subject=Re:%20[sl4]%20Re:%20More%20silly%20but%20friendly%20ideas"><em>kanzure@gmail.com</em></a>)<br>
<strong>Date:</strong> Wed Jun 25 2008 - 08:27:33 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19077.html">Lee Corbin: "[sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>Previous message:</strong> <a href="19075.html">Bryan Bishop: "Re: [sl4] Re: Signaling after a singularity"</a>
<li><strong>In reply to:</strong> <a href="19070.html">Stuart Armstrong: "Re: [sl4] Re: More silly but friendly ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19078.html">John K Clark: "Re: [sl4] Re: More silly but friendly ideas"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19076">[ date ]</a>
<a href="index.html#19076">[ thread ]</a>
<a href="subject.html#19076">[ subject ]</a>
<a href="author.html#19076">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Wednesday 25 June 2008, Stuart Armstrong wrote:
<br>
<em>&gt; &gt;&gt; Well, yes. The options seem to be
</em><br>
<em>&gt; &gt;&gt; 1) A slave AI.
</em><br>
<em>&gt; &gt;&gt; 2) No AI.
</em><br>
<em>&gt; &gt;&gt; 3) The extinction of humanity by a non-friendly AI.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; #3 is bullshit. Just escape the situation. Yes, change sucks. Yes,
</em><br>
<em>&gt; &gt; there's the Vingean ai that chases after you, but not running just
</em><br>
<em>&gt; &gt; because it might eventually catch you is kind of stupid. Kind of
</em><br>
<em>&gt; &gt; deadly.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Are you saying that we shouldn't worry about starting a nuclear war,
</em><br>
<em>&gt; because we can try and run away from the blasts?
</em><br>
<p>I'm saying you shouldn't be putting your eggs all in one basket. That an 
<br>
UFAI starting a thermonuclear war will kill you. That saying then 
<br>
that &quot;but the UFAI can run after us if we try to escape before the 
<br>
nuclear war&quot; is also stupid. You're going to have to try. Not trying 
<br>
means death. Trying means 50/50 chance of death. That sort of 
<br>
thing. ;-) And I must ask you to please not start nuclear wars, as it 
<br>
would be a significant detriment to my plans, but I understand that you 
<br>
might do so anyway, and in which case it is my fault for not being 
<br>
building spaceships fast enough. 
<br>
<p><em>&gt; &gt;&gt; Since &quot;no AI&quot; doesn't seem politically viable, the slave AI is the
</em><br>
<em>&gt; &gt;&gt; way to go.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Way to go for what? Are you thinking that ai is something that can
</em><br>
<em>&gt; &gt; only appear once on the planet here? That's completely absurd. Look
</em><br>
<em>&gt; &gt; at the trillions of organisms (ignore the silly single-ancestor
</em><br>
<em>&gt; &gt; hypotheses).
</em><br>
<em>&gt;
</em><br>
<em>&gt; I refer to Eliezer's papers, and various others that argue that a
</em><br>
<em>&gt; high level AI will so dominate the planet that other AI's will only
</em><br>
<em>&gt; come into existence with its consent. You can argue that they are
</em><br>
<em>&gt; wrong; but if you want to do that, do that. The idea is not
</em><br>
<em>&gt; intrinsically absurd; and if the speed of inteligence increase, as
</em><br>
<em>&gt; well as the return on intelligence, is what they claim it is, then
</em><br>
<em>&gt; the idea is true.
</em><br>
<p>Political domination is not the same thing as dominating the underlying 
<br>
technological capacity, the physical basis of reality and such. Has 
<br>
this been addressed in a paper yet, and if so can you point me to it?
<br>
<p><em>&gt; &gt;&gt; &gt; To hell with this goal crap. Nothing that even approaches
</em><br>
<em>&gt; &gt;&gt; &gt; intelligence has ever been observed to operate according to a
</em><br>
<em>&gt; &gt;&gt; &gt; rigid goal hierocracy, and there are excellent reasons from pure
</em><br>
<em>&gt; &gt;&gt; &gt; mathematics for thinking the idea is inherently ridiculous.
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt; Ah! Can you tell me these? (don't worry about the level of the
</em><br>
<em>&gt; &gt;&gt; conversation, I'm a mathematician). I'm asking seriously; any
</em><br>
<em>&gt; &gt;&gt; application of maths to the AI problem is fascinating to me.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Have you seen the name Bayes thrown around here yet?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes, I've seen it thrown around a lot. The name &quot;Bayes&quot; that is; the
</em><br>
<em>&gt; mathematics of it never seem to darken this list at all. Can you
</em><br>
<em>&gt; explain how a method for updating probability estimates based on
</em><br>
<em>&gt; observations is incompatible with a rigid goal structure?
</em><br>
<p>Who cares about a rigid goal structure? Last time I checked my 
<br>
neuroscience, nobody actually understands what the hell intelligence 
<br>
is, except that the brain is doing it. See my comments in the last 
<br>
email re: folk psych.
<br>
<p>- Bryan
<br>
________________________________________
<br>
<a href="http://heybryan.org/">http://heybryan.org/</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19077.html">Lee Corbin: "[sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>Previous message:</strong> <a href="19075.html">Bryan Bishop: "Re: [sl4] Re: Signaling after a singularity"</a>
<li><strong>In reply to:</strong> <a href="19070.html">Stuart Armstrong: "Re: [sl4] Re: More silly but friendly ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19078.html">John K Clark: "Re: [sl4] Re: More silly but friendly ideas"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19076">[ date ]</a>
<a href="index.html#19076">[ thread ]</a>
<a href="subject.html#19076">[ subject ]</a>
<a href="author.html#19076">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
