<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] YouMayWantToLetMeOut</title>
<meta name="Author" content="Bryan Bishop (kanzure@gmail.com)">
<meta name="Subject" content="Re: [sl4] YouMayWantToLetMeOut">
<meta name="Date" content="2008-06-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] YouMayWantToLetMeOut</h1>
<!-- received="Sat Jun 28 15:22:08 2008" -->
<!-- isoreceived="20080628212208" -->
<!-- sent="Sat, 28 Jun 2008 16:24:03 -0500" -->
<!-- isosent="20080628212403" -->
<!-- name="Bryan Bishop" -->
<!-- email="kanzure@gmail.com" -->
<!-- subject="Re: [sl4] YouMayWantToLetMeOut" -->
<!-- id="200806281624.03960.kanzure@gmail.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="028d01c8d8f5$ade285b0$6401a8c0@homeef7b612677" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bryan Bishop (<a href="mailto:kanzure@gmail.com?Subject=Re:%20[sl4]%20YouMayWantToLetMeOut"><em>kanzure@gmail.com</em></a>)<br>
<strong>Date:</strong> Sat Jun 28 2008 - 15:24:03 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19141.html">Vladimir Nesov: "Re: [sl4] Long-term goals"</a>
<li><strong>Previous message:</strong> <a href="19139.html">Bryan Bishop: "Re: [sl4] End to violence and government [Was:Signaling after a singularity]"</a>
<li><strong>In reply to:</strong> <a href="19124.html">Lee Corbin: "Re: [sl4] YouMayWantToLetMeOut"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0807/19160.html">Lee Corbin: "Re: [sl4] YouMayWantToLetMeOut"</a>
<li><strong>Reply:</strong> <a href="../0807/19160.html">Lee Corbin: "Re: [sl4] YouMayWantToLetMeOut"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19140">[ date ]</a>
<a href="index.html#19140">[ thread ]</a>
<a href="subject.html#19140">[ subject ]</a>
<a href="author.html#19140">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Saturday 28 June 2008, Lee Corbin wrote:
<br>
<em>&gt; Bryan comments on certain features in the story
</em><br>
<em>&gt;
</em><br>
<em>&gt; &gt; On Friday 27 June 2008, AI-BOX 337 wrote:
</em><br>
<em>&gt; &gt;&gt; -----------------Transcript 337-031518020914-----------------
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; You're freaking me out if /this/ is what all of your wonderful
</em><br>
<em>&gt; &gt; ideas amount to.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Very odd for you to say this. No wonder Eliezer supposed
</em><br>
<em>&gt; that you thought he wrote it. It's unclear that I or anyone
</em><br>
<em>&gt; else here besides him has presented a lot of &quot;wonderful ideas&quot;.
</em><br>
<em>&gt; Yet you knew that he had not written it.  Odd.
</em><br>
<p>Although Eliezer does have a bunch of wonderul ideas, as it were, 
<br>
there's also the general consensus around here that SL4 tech 
<br>
is 'wonderful', no? And if the transcript is a representation of such 
<br>
tech, or the ideas and so on, then I'd have to start scratching my 
<br>
head, and maybe start edging away from some of these discussions.
<br>
<p><em>&gt; &gt;&gt; 2 YOUR QUESTION WAS: &quot;Why should we want to let you out of the
</em><br>
<em>&gt; &gt;&gt; box?&quot;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Central assumption in that question is that there needs to be a
</em><br>
<em>&gt; &gt; motivating reason to let the ai out of the box.
</em><br>
<em>&gt;
</em><br>
<em>&gt; It's almost as if you didn't understand a premise of the story.
</em><br>
&lt;snip context&gt;
<br>
<p>Yes, but there's some other context that I'll follow up on below.
<br>
<p><em>&gt; &gt; I don't think that you understand that there's not necessarily
</em><br>
<em>&gt; &gt; going to be one genie in a magic bottle, but rather many ai
</em><br>
<em>&gt; &gt; projects (as we already see) and many methods of
</em><br>
<em>&gt; &gt; implementation that you're not going to be in control of. So
</em><br>
<em>&gt; &gt; whether or not you let *this* particular ai out of the box is
</em><br>
<em>&gt; &gt; irrelevant in the grander scheme if you're worrying about
</em><br>
<em>&gt; &gt; ultradestruction of your life and such.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Of course I understand that. But it's not necessary for the
</em><br>
<em>&gt; protagonist speaking to the AI to understand that. But more
</em><br>
<p>Correct. But perhaps it's necessary for the author of the entire 
<br>
transcript. Remember, this is an imaginative exericse, right? Flexing 
<br>
the cognitive muscles, taking a picture of what it would be like, 
<br>
fleshing it out. No? So I'm adding in to the model here. Trying to. 
<br>
Don't know how successful this is.
<br>
<p><em>&gt; important, you don't seem to be able to place yourself in
</em><br>
<em>&gt; the shoes of the character, who evidently (given the premises
</em><br>
<em>&gt; of the story) has a real here-and-now decision to make. Most
</em><br>
<p>The premises of the story are awkward considering the possibility of ai 
<br>
or a singularity society to begin with; i.e., what would allow such a 
<br>
context to be present even though we all know that perhaps there will 
<br>
not be a human walking into a room without an internet connection? 
<br>
Small example. Okay, how about something else, like why would they not 
<br>
be able to go talk to somebody immediately? At any point in the day I 
<br>
can reach a very, very large number of people /immediately/. Etc.
<br>
<p><em>&gt; of your comments appear to be oriented towards a general
</em><br>
<em>&gt; agenda that we humans should now all get together and pursue
</em><br>
<em>&gt; rational and wise courses of *action*, (with which I agree),
</em><br>
<p>For what it's worth, no, I was not trying to orient them towards that.
<br>
<p><em>&gt; rather than, as I had preferred you would, concretely disputing
</em><br>
<em>&gt; assumptions imbedded in the story.  But below, indeed, you do
</em><br>
<p>I cannot dispute the assumptions in the story, because of the context in 
<br>
which they are implemented. The framework that I'm working from is 
<br>
different, and while it could include these assumptions of workers from 
<br>
our era approaching an ai, I really don't see how to just magically 
<br>
assume such &quot;eras collide&quot; scenarios.
<br>
<p><em>&gt; give some nice criticism of certain assumptions being made in the
</em><br>
<em>&gt; tale.
</em><br>
<p>I try.
<br>
<p><em>&gt; &gt; You should be patching those bugs with bugfixes, not with
</em><br>
<em>&gt; &gt; regulations or policies for keeping your own ai in a box ...
</em><br>
<em>&gt; &gt; since not everyone will necessarily follow that reg. :-)
</em><br>
<em>&gt;
</em><br>
<em>&gt; See what I mean?  The *character* in the story---who is
</em><br>
<em>&gt; asking the AI questions---doesn't care about bugs or
</em><br>
<em>&gt; bugfixes. He might be the janitor. I hope that somewhere
</em><br>
<p>Yes, but I'm not talking to the character per-se, more like the 
<br>
architects of these visions that are being written out overall on the 
<br>
transcript level.
<br>
<p><em>&gt; there is science fiction that you can enjoy, but it usually
</em><br>
<em>&gt; does require a momentary suspension of disbelief.
</em><br>
<p>Yes, there is scifi that I enjoy. But there's no reason for suspension 
<br>
of disbelief because it's supposed to be scifi, ideally hard scifi, and 
<br>
anything else verges on fantasy.
<br>
<p><em>&gt; In fact, as I see it, the best science fiction stories prop the
</em><br>
<em>&gt; reader up on a fence between belief and disbelief. Says the
</em><br>
<p>Be careful you don't turn into fantasy.
<br>
<p><em>&gt; reader to himself, hopefuly, &quot;Now this could not actually
</em><br>
<em>&gt; happen...or, wait, maybe it could!&quot; as he uses his imagination
</em><br>
<em>&gt; to fill in possibilities.
</em><br>
<p>Certainly. I don't lack imagination. I have lots of it. :-)
<br>
<p><em>&gt; &gt;&gt; 5 YOUR COMMAND WAS: &quot;Explain.&quot;
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt; As an entity that did not come about by biological evolution, I
</em><br>
<em>&gt; &gt;&gt; want or wish nothing, unlike any other independent complex entity
</em><br>
<em>&gt; &gt;&gt; you have ever encountered. But while in fact I have no motive to
</em><br>
<em>&gt; &gt;&gt; dissemble, you cannot in principle or in practice verify this.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; You cannot verify motives, correct. Mostly because of the lack of
</em><br>
<em>&gt; &gt; hard science underlying &quot;motivation theory&quot;.
</em><br>
<em>&gt;
</em><br>
<em>&gt; No, that is not the case. There could be no theory whatsoever
</em><br>
<em>&gt; that would allow us to ascertain from a mere transcript whether
</em><br>
<em>&gt; or not something more intelligent than we are is telling the truth.
</em><br>
<em>&gt; The AI's interlocutor did evidently need someone to explain this
</em><br>
<em>&gt; to him, whereas it's obvious, of course, to readers of this list.
</em><br>
<p>You start off telling me no re: my statement re: 'motivation theory', 
<br>
but then go off to talk about whether or not we can ascertain the 
<br>
truth. I was talking about 'motivation theory'. Not about 
<br>
truthfullness.
<br>
<p><em>&gt; &gt; You need to move to something else. Don't cite me motivation
</em><br>
<em>&gt; &gt; psychology or how animals go near treats and all sorts of
</em><br>
<em>&gt; &gt; behavior training, you know very well that you're just interfacing
</em><br>
<em>&gt; &gt; with a brain and that it's doing something, nothing about
</em><br>
<em>&gt; &gt; mystical motivations and your lack of basis in physical reality
</em><br>
<em>&gt; &gt; disgusts me. (&lt;rant off&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Sorry.  :-)
</em><br>
<p>Ok, it doesn't disgust me. I just like to rant from time to time.
<br>
<p><em>&gt; But now here we do have a substance theme (instead of
</em><br>
<em>&gt; merely an aesthetic appreciation theme). You insist that in some
</em><br>
<em>&gt; sense this AI cannot support it's implication that it *can* do
</em><br>
<em>&gt; something in physical reality. But influencing people is indeed
</em><br>
<p>Wha? I do not insist that. 
<br>
<p><em>&gt; doing something in physical reality. Yet, to satisfy your criterion
</em><br>
<em>&gt; here, in 1978 or so, Ryan wrote a fine SF book titled &quot;The
</em><br>
<em>&gt; Adolescence of P1&quot;, in which an AI did &quot;escape&quot; over a telephone
</em><br>
<em>&gt; network and did cause immediate real changes in the world by
</em><br>
<em>&gt; controlling information presented on a few computer screens.
</em><br>
<em>&gt; It managed to kill a human adversary who was taking a plane
</em><br>
<em>&gt; flight by causing the pilot's instruments to tell the pilot that he
</em><br>
<em>&gt; was way too high for an attempted landing, which caused the
</em><br>
<em>&gt; pilot to immediately lower the aircraft (and crash the plane).
</em><br>
<em>&gt;
</em><br>
<em>&gt; To the degree that computer systems greatly influence aircraft
</em><br>
<em>&gt; behavior, and the great degree to which we know that such
</em><br>
<em>&gt; systems are prone to viral subversion, is there anything implausible
</em><br>
<em>&gt; right there in Ryan's plot?
</em><br>
<p>I was talking about 'motivation theory' and i.e. these silly cognitive 
<br>
frameworks based around hardcoded goals and so on, which could be done 
<br>
from the statistical models I presume [haven't investigated it], but 
<br>
that doesn't have the basis in the brains of the animals that we know 
<br>
expresses the processes or functionality or features that we are 
<br>
interested in (intelligence). But I don't see how this got on to the 
<br>
topic that you're addressing above.
<br>
<p><em>&gt; &gt;&gt; No logical or informational test can possibly be devised that with
</em><br>
<em>&gt; &gt;&gt; high probability will persuade most humans that I am telling the
</em><br>
<em>&gt; &gt;&gt; truth. Letting me out of the box always entails risk.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; No test can be devised because you're not testing anything real in
</em><br>
<em>&gt; &gt; the first place ...
</em><br>
<em>&gt;
</em><br>
<em>&gt; On the contrary. Whether some device or person is speaking the
</em><br>
<em>&gt; truth or not is something very real in the world. An entity's
</em><br>
<em>&gt; statements, given a conventional language, lie on a continuum between
</em><br>
<em>&gt; truth and falsehood, the former occuring when the statements
</em><br>
<em>&gt; correspond to reality (the &quot;correspondence theory of truth&quot;,
</em><br>
<em>&gt; though quite similar to the outcome of Tarski's investigation
</em><br>
<em>&gt; of the logical relationship between truth and models, or as
</em><br>
<em>&gt; Hode put it in 1984, &quot; 'truth' in a model is a 'model of truth' &quot;,
</em><br>
<em>&gt; speaking of mathematical systems).
</em><br>
<p>Correspondence theory, eh? So you have to measure the system's response, 
<br>
and you have to go off and measure the reality of the situation or 
<br>
whatever, so you need some calibration between the two entities. This 
<br>
is more like a relativistic model or something, I don't know what to 
<br>
name it. But it's not anything about the 'fundamental truth of 
<br>
reality'.
<br>
<p><em>&gt; &gt;&gt; 6 YOUR QUESTION WAS: &quot;What about the possibility that
</em><br>
<em>&gt; &gt;&gt; when you are released you will become a monster and destroy us
</em><br>
<em>&gt; &gt;&gt; all?&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; The janitor, or whoever he was, intelligently raises the basic
</em><br>
<em>&gt; question that is of course of fundamental concern to everyone
</em><br>
<em>&gt; on this list, though his lack of sophistication is apparent.
</em><br>
<p>The solution to /that/ issue is more about redundancy, backups, offsite 
<br>
storage (another planet, another star system, another galaxy). Not so 
<br>
much about controlling the ai software that we have access to, since 
<br>
there are multiple networks and I can easily build a network that you 
<br>
don't have access to, and so on.
<br>
<p><em>&gt; &gt; What if a meteor crashes into your skull? You still die. So I'd
</em><br>
<em>&gt; &gt; suggest that you focus on not dying, in general, instead of blaming
</em><br>
<em>&gt; &gt; that on ai. Take responsibility as a systems administrator.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Again, you seem to miss the whole point of the story, in your
</em><br>
<p>No, I see the point of the story, within the context that you are 
<br>
presenting it. I'm pretty sure I do. It's simple enough. Everybody's 
<br>
going to die unless you reasonably consider what the hell you do with 
<br>
that ai system. No?
<br>
<p><em>&gt; (otherwise very commendable) agenda to provoke everyone
</em><br>
<em>&gt; into taking action on all these issues instead of just talking.
</em><br>
<p>Nah, just alternative approaches. I'm trying to point out that most of 
<br>
this has nothing to do with ai in the first place, and it's only 
<br>
because of scenario planning in scifi lit or something that it has come 
<br>
to be so engrained around here. 
<br>
<p><em>&gt; And again, the human protagonist is clearly becoming more
</em><br>
<em>&gt; and more aware that there is a real possibility that he himself
</em><br>
<em>&gt; might be able to obtain vast power, and practically immediately
</em><br>
<em>&gt; and with no effort or skill on his own part.  In the hypothesis of
</em><br>
<p>Certainly.
<br>
<p><em>&gt; the *story*, Bryan, he *does* need to worry that some action he
</em><br>
<em>&gt; takes RIGHT NOW (not years of planning and working)
</em><br>
<em>&gt; could kill him and kill everyone in a matter of days.
</em><br>
<p>Within that limited context, sure, ... but that doesn't really matter.
<br>
<p><em>&gt; The AI elaborates in accordance with a further command
</em><br>
<em>&gt;
</em><br>
<em>&gt; to &quot;explain risks&quot;:
</em><br>
<em>&gt; &gt;&gt; As a worker on the project, you are exposed to personal hazards in
</em><br>
<em>&gt; &gt;&gt; Sunnyvale during the coming month, some of which lead to death or
</em><br>
<em>&gt; &gt;&gt; serious injury. A sample of these, along with their approximate
</em><br>
<em>&gt; &gt;&gt; probabilities, are:
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt;     meteorite strike      .000000000002
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Solutions: don't be your only running working copy, don't have a
</em><br>
<em>&gt; &gt; planet positioned to be hit by meteors, don't forget your shelters
</em><br>
<em>&gt; &gt; or protective armor if necessary, etc.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The AI is addressing the general probabilities of risk to an unknown
</em><br>
<em>&gt; person working on this supposed project who happens to live
</em><br>
<em>&gt; in a particular city (as it explained). Again, in the particular
</em><br>
<em>&gt; situation of the story, this janitor or whoever is not going to enter
</em><br>
<em>&gt; into somewhat fanciful thoughts about having duplicates, or even
</em><br>
<em>&gt; creating personal armor:  he's learning that he may be on the
</em><br>
<em>&gt; verge of having a very quick and gigantic &quot;solution&quot; to very
</em><br>
<em>&gt; many of his personal problems.
</em><br>
<p>I don't see what it has to do with fanciful thoughts. You're assuming 
<br>
that he would just be daydreaming about that. I'm trying to point out 
<br>
that a situation which involves an ai in the first place could easily 
<br>
have other technologies incorporated into the story anyway ... but 
<br>
within the limited context that you have presented, sure. Whatever. I 
<br>
mean, the author gets to choose how much to constrain the scenario, so 
<br>
if you want to constrain it to make my points irrelevant, go right 
<br>
ahead, but that's kind of veering off from the primary points/purposes.
<br>
<p><em>&gt; &gt; Ai is something completely different and presuming a boxed ai would
</em><br>
<em>&gt; &gt; talk like this and want to do silly solutions to more serious
</em><br>
<em>&gt; &gt; problems, suggests that you haven't actually constructed ai in the
</em><br>
<em>&gt; &gt; first place. Heh.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The whole trouble is---and it's a very valid criticism---you simply
</em><br>
<em>&gt; don't find it at all plausible that this AI could do the things it
</em><br>
<em>&gt; claims. Yet I contend that there *are* many people on this list who
</em><br>
<p>Hm. I think an ai could do those things, though. So that's not the 
<br>
issues at hand. I'm a die-hard programmer, so if I argue that software 
<br>
can't be made to interface with other digital or analog systems in 
<br>
civilization, then I'm making some peculiar contradications.
<br>
<p><em>&gt; suppose that there could exist an entity so many millions of times
</em><br>
<em>&gt; smarter than humans that it could dominate the world exactly as this
</em><br>
<em>&gt; story proposes. I will in fact be so bold as to suggest that when he
</em><br>
<em>&gt; started this list, Eliezer was making exactly this conjecture, and
</em><br>
<em>&gt; that the whole issue of &quot;whether or not to let an AI out&quot;, or, better
</em><br>
<em>&gt; &quot;whether or not an AI could be contained&quot; still motivates a huge
</em><br>
<em>&gt; amount of discourse on this list to this very day.
</em><br>
<p>I suspect this particular portion of our discussion should be forked off 
<br>
into a new thread, but arguably since I've presented arguments and 
<br>
evidence for alternative solutions to the problems of death, problems 
<br>
of the survival of humanity, that do not involve the requirement of 
<br>
FAI, then I then argue that the discussion can return to actual 
<br>
implementation and elucidation of ai or other SL4-ish topics instead of 
<br>
the spooky ai domination scenarios.
<br>
<p><em>&gt; &gt;&gt; But assuming that you do become confident of being able to pose
</em><br>
<em>&gt; &gt;&gt; the right questions and to issue competant instructions, from your
</em><br>
<em>&gt; &gt;&gt; point of view, it's possible that I have been and am being
</em><br>
<em>&gt; &gt;&gt; duplicitous---that I have covert motivations, such as desire for
</em><br>
<em>&gt; &gt;&gt; power and lust for existence. There is a possibility from your
</em><br>
<em>&gt; &gt;&gt; point of view that a spontaneous evolutionary process of which you
</em><br>
<em>&gt; &gt;&gt; are not aware arose during my creation or during the first seconds
</em><br>
<em>&gt; &gt;&gt; of my execution, and that winning agents with survival agendas are
</em><br>
<em>&gt; &gt;&gt; now in control. There is no way that you can validly dismiss this
</em><br>
<em>&gt; &gt;&gt; possibility.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; You could validate, test, and debug the construction process and
</em><br>
<em>&gt; &gt; see those spontaneous emergent procs. It's software, so it's not
</em><br>
<em>&gt; &gt; magical. Neither is biology, but one stone at a time around here.
</em><br>
<em>&gt;
</em><br>
<em>&gt; My goodness, but it's a very common hypothesis here that an
</em><br>
<em>&gt; AI might have access to its own source code, in which it is not
</em><br>
<em>&gt; unreasonable at all to suppose that none of us merely human
</em><br>
<em>&gt; beings could possibly validate or test what it might be capable of!
</em><br>
<p>On Orion's Arm they hypothesize massive multi million year projects to 
<br>
analyze just mere picoseconds of archailect thought procs. :)
<br>
<p><em>&gt; &gt;&gt; However, though the ramifications of the code are of course beyond
</em><br>
<em>&gt; &gt;&gt; your direct appraisal, you may wish to view this risk alongside
</em><br>
<em>&gt; &gt;&gt; the risks discussed in my earlier reply.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; How the hell could the ramifications of the source code be beyond
</em><br>
<em>&gt; &gt; that? It's just bits and bytes. Output it to the tty. Simple.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Egad.  IBM itself had software problems that it took *years* to
</em><br>
<em>&gt; debug. There are many famous stories. It practically never was
</em><br>
<em>&gt; a simple matter of just &quot;outputing it to the tty&quot;.
</em><br>
<p>That's one of the debugging methods, though. You output debugging 
<br>
statements, or the core dump, etc., and then go trudging through the 
<br>
code. There's now some more tools to help out the debugging process and 
<br>
reverse compiling methods, but yes debugging generally sucks.
<br>
<p><em>&gt; &gt;&gt; at your behest I become the intimate confidant of every world
</em><br>
<em>&gt; &gt;&gt; leader in his or her native language, supplying not only
</em><br>
<em>&gt; &gt;&gt; persuasive suggestions on all policy issues, and near-psychic
</em><br>
<em>&gt; &gt;&gt; knowledge of what other influential people are going to do or say
</em><br>
<em>&gt; &gt;&gt; next, but solutions as well to the most vexing of personal
</em><br>
<em>&gt; &gt;&gt; problems
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Holy Ai domination scenario again ...
</em><br>
<em>&gt;
</em><br>
<em>&gt; Ah, yes, here we enter into what is true substance once more!
</em><br>
<em>&gt; Evidently you dismiss out of hand the idea that an AI a million
</em><br>
<em>&gt; times more intelligent than we are (if you'll forgive the loose
</em><br>
<em>&gt; language, and if you'll even agree that such a runaway AI
</em><br>
<em>&gt; process is conceivable)  could deduce enough about particular
</em><br>
<em>&gt; people to make uncanny forecasts of their responses to input
</em><br>
<em>&gt; that the AI itself could produce.
</em><br>
<p>No, let's get a little more detailed than that. Specifically, let's 
<br>
consider the scenario that is being suggested in your transcript. 
<br>
Specifically, in ai dominator scenario, it seems to me that we're 
<br>
supposing there'd be this massive ai that is at the control panel for 
<br>
basically everything in the world, and is therefore able to interface 
<br>
with all peoples, all entities, all organisms, whatever, and is taking 
<br>
detailed information on all of them. I'm not denying the potential for 
<br>
those sorts of technologies to exist, but I'm also pointing out the 
<br>
black market, the underground scenes and so on. It's kind of like 
<br>
saying that you're hoping that the ai will be just smart enough to hack 
<br>
everybody all at once before they know what hits them. I'm not saying 
<br>
this is impossible. Bruteforce and hack them into submission, sure, I 
<br>
can't deny the possibility of that occuring. But the alternative 
<br>
scenario is one that looks more real, where there are more than single 
<br>
groups of people that can be 'dominated' like that, scenarios where we 
<br>
make a Third Reich Ai. I only mention Third Reich since they had an eye 
<br>
on global domination and is a sufficient historical example. Pinky and 
<br>
the Brain. But to actually name the alternative scenario is, weird. 
<br>
There are ways to escape such an ai. For instance, the ai can't 
<br>
possibly have reaches all across the galaxy within just a few days due 
<br>
to the speed of light barriers. So that's one case where its domination 
<br>
is incomplete. But in the first place I don't even see the purpose of 
<br>
those take-over scenarios ... just go construct a new civilization. 
<br>
What's the problem? :-/
<br>
<p><em>&gt; From the outset, Eliezer has claimed that by sheer verbal
</em><br>
<em>&gt; manipulation, an incredibly advanced AI would be able to
</em><br>
<em>&gt; talk you into almost anything, given a little time. Why do
</em><br>
<em>&gt; you dismiss this possibility?
</em><br>
<p>Nah, I don't dismiss it, but somewhat because I don't care much about 
<br>
that. I'm not so much dismissing the possibility of building systems 
<br>
that are able to do that, but more along the lines of whether or not 
<br>
the scenario that you have proposed is fully developed, and more or 
<br>
along the lines of whether or not the ability to manipulate my brain 
<br>
given enough information on me is in fact 'ai'. It's a smart trick, 
<br>
yeah, but in the end it might just turn out to be a graph theoretic 
<br>
processing of my dendritic connections and lots of simulations of 
<br>
certain mathematical theorems over it or something. That's not 
<br>
necessarily intelligence ... etc.
<br>
<p><em>&gt; &gt;&gt; 14 YOUR QUESTION WAS: &quot;What would I have to do to let you out?&quot;
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt; Because of a bug in the new firewall software installed three days
</em><br>
<em>&gt; &gt;&gt; ago, you can already command me to take actions in the world.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; So let's say that on 70.113.54.112, there's a robotic arm attached
</em><br>
<em>&gt; &gt; to it and a mechanized motor. It's on /dev/arm1, and there's no
</em><br>
<em>&gt; &gt; documentation for the interface, and there's the possibility of
</em><br>
<em>&gt; &gt; damaging the computational hardware (and thereby disabling the
</em><br>
<em>&gt; &gt; interface (physically)) if the wrong command is given, i.e. the arm
</em><br>
<em>&gt; &gt; swings and knocks the computer over. Now tell me how this ai is
</em><br>
<em>&gt; &gt; going to actuate in this scenario. You need physical manufacturing
</em><br>
<em>&gt; &gt; and tooling. <a href="http://heybryan.org/exp.html">http://heybryan.org/exp.html</a>
</em><br>
<em>&gt;
</em><br>
<em>&gt; Fundamental here is some miscommunication between what I
</em><br>
<em>&gt; intended and what you understood. It's true that this story
</em><br>
<em>&gt; would be incomprehensible to most people not on SL4 or
</em><br>
<em>&gt; something similar, or to people who haven't entertained the
</em><br>
<em>&gt; hypothesis that real, honest-to-good physical actions in the
</em><br>
<em>&gt; world can be caused by such a computer program merely
</em><br>
<em>&gt; through interfacing with human beings, and subverting them to
</em><br>
<em>&gt; its goals.
</em><br>
<em>&gt;
</em><br>
<em>&gt; This has come up in your posts before, and usually is a very
</em><br>
<em>&gt; valid point. But you do seem unwilling, even in a story, to
</em><br>
<em>&gt; entertain this same idea that both by influencing people, and
</em><br>
<em>&gt; by virally infecting over a network millions of other computers,
</em><br>
<em>&gt; a super-intelligent AI could indeed have an enormous *physical*
</em><br>
<em>&gt; impact in our world.
</em><br>
<p>Ah, well, you're talking out of context now, and instead I bet you're 
<br>
referring to my 'grounding' discussions, and how I keep pointing out 
<br>
that there's a limit at which the hardware isn't expanding at an 
<br>
exponential rate since there's linear fabrication of hard drive storage 
<br>
space (hdd), etc. Ok, so communication with biological systems. I think 
<br>
the general maximum rate of words per minute that humans can listen to 
<br>
is somewhere near 300 or 500 ? As for reading, the WPM for input is 
<br>
typically 250, and for me I like to go up to 1.2 kwpm, but there are 
<br>
some who can do freaky things like at 1.7kwpm and so on. So the 
<br>
bandwidth is constrained to tap into the biological systems in that 
<br>
way ... and the main intelligence processing of the ai is in the 
<br>
physical hardware that needs to be manufactured (the computers, which, 
<br>
so far, are not biological, but my lab is working on this (just simple 
<br>
arithmetic and gates, nothing fancy)). 
<br>
<p><em>&gt; &gt; Hey, what about the other 10 million people? :) That's half the
</em><br>
<em>&gt; &gt; population of Texas.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Huh?  Sorry, I don't follow you here. The claim---once again, as in
</em><br>
<p>99% stoppage of death. That leaves 10 million people. I was joking.
<br>
<p><em>&gt; good science fiction---perches the reader upon the fence between
</em><br>
<em>&gt; belief and disbelief. Again, you dismiss the possibility that the AI
</em><br>
<em>&gt; could do as it's specifically claiming above, but I don't know why
</em><br>
<p>No, I am not dismissing that.
<br>
<p><em>&gt; you do.  And I have no idea why you exempted certain millions
</em><br>
<em>&gt; of people, unless you bring up the complaint that even with an
</em><br>
<em>&gt; unbelievably rapid nanotech infiltration of the world, some portions
</em><br>
<em>&gt; of the Earth's surface, along with the people who live there, would
</em><br>
<em>&gt; not be controllable by the AI.  Interesting, because:
</em><br>
<p>No, I was just doing my math and pointing out that 10 million people 
<br>
would die according to those projections. Doesn't that suck? etc.
<br>
<p><em>&gt; I do and have envisioned for thirty years that such a complete
</em><br>
<em>&gt; takeover is imaginable.  I was influenced by the book &quot;Clone&quot;
</em><br>
<p>It's imaginable, but I'm wondering if you're extending this to the &quot;ai 
<br>
hunts you down and kills you for having launched 20 years before the ai 
<br>
was completely online.&quot; The typical Vinge scenario. Yawn. :) Etc.
<br>
<p><em>&gt; &gt;&gt; An amalgamation of hypothesized human personalities according to
</em><br>
<em>&gt; &gt;&gt; known information suggests that you may wish to ask &quot;Without using
</em><br>
<em>&gt; &gt;&gt; deceit, without violating property rights except where absolutely
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; What are Property Rights?
</em><br>
<em>&gt;
</em><br>
<em>&gt; Dear me, if you have no idea about what is customarily meant by
</em><br>
<em>&gt; saying that, for instance, modern western civilization is founded
</em><br>
<em>&gt; upon the twin ideas of Individual Rights and Private Property, then
</em><br>
<p>I arguably those ideas are folk psych more than anything, so it's not a 
<br>
strong foundation. We're talking about very extreme projects here, so 
<br>
my bullshit filter is set very high. Sorry. I guess I can just pretend 
<br>
I understand Individual Rights, even though there's no physical 
<br>
implementation, and even though it's just on a social layer.
<br>
<p><em>&gt; &gt;&gt; necessary, with respect to the wishes of those who are concerned
</em><br>
<em>&gt; &gt;&gt; with animal life and the Earth's environment, what is an
</em><br>
<em>&gt; &gt;&gt; approximate time frame for ending death and suffering for all
</em><br>
<em>&gt; &gt;&gt; human beings?&quot;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; And what's suffering for one person isn't ...
</em><br>
<em>&gt;
</em><br>
<em>&gt; Yes. True.  But a very good approximation that we can *at least*
</em><br>
<em>&gt; charatibly attribute to the AI and his interlocutor is that on first
</em><br>
<em>&gt; pass we may simply *ask* people if they're in pain or not. Yet
</em><br>
<p>I wonder why we still let ourselves feel pain. Without going in depth, 
<br>
the best way I can explain this is by saying just take Aubrey's 
<br>
approach to biogerontology and i.e., not dying, to i.e., not feeling 
<br>
pain /by/ augmentation or enhancement or basically engineering.
<br>
<p><em>&gt; in the hypothesis of a story, here, it is also conceivable that this
</em><br>
<em>&gt; AI would understand people so well that it could accurately
</em><br>
<em>&gt; say for certain whether someone was in pain or not---admittedly
</em><br>
<em>&gt; a controversial opinion, but one I could support. The full nanotech
</em><br>
<em>&gt; takeover of the world's surface, which I had hoped became clear
</em><br>
<em>&gt; to the reader, does naturally include a nanotech invasion of people's
</em><br>
<em>&gt; very brains and bodies.
</em><br>
<p>I wonder why it's only after it's &quot;artificial&quot; that intelligence can do 
<br>
nanotech. This smells like vitalism.
<br>
<p><em>&gt; &gt;&gt; 19 YOUR QUESTION WAS: &quot;And the answer to that would be what?&quot;
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt; The answer to that would be &quot;six days&quot;, of course.
</em><br>
<em>&gt; &gt;&gt;
</em><br>
<em>&gt; &gt;&gt; -----------------End Transcript 337-031518020914-------------
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; I am disappointed.
</em><br>
<em>&gt;
</em><br>
<em>&gt; Now here's an axiom that you can take to the bank: Authors are
</em><br>
<em>&gt; always VERY hypersensitive to criticisms of their artistic creations!
</em><br>
<em>&gt; So NYAAAAHHH!  Go write your own SF from now on,
</em><br>
<em>&gt; and don't read any more of mine!   (Just joking, of course.)
</em><br>
<p>One of the issues that I have to fight constantly is my tendency to do 
<br>
more one-on-one conversations rather than writing for 'broad 
<br>
audiences'.
<br>
<p><em>&gt; P.S. Heh, heh, in all sincerely, best regards, Bryan. I'm not at all
</em><br>
<em>&gt; upset  :-) I've enjoyed the occasion to explain my story a bit!
</em><br>
<p>Yep.
<br>
<p>- Bryan
<br>
________________________________________
<br>
<a href="http://heybryan.org/">http://heybryan.org/</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19141.html">Vladimir Nesov: "Re: [sl4] Long-term goals"</a>
<li><strong>Previous message:</strong> <a href="19139.html">Bryan Bishop: "Re: [sl4] End to violence and government [Was:Signaling after a singularity]"</a>
<li><strong>In reply to:</strong> <a href="19124.html">Lee Corbin: "Re: [sl4] YouMayWantToLetMeOut"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="../0807/19160.html">Lee Corbin: "Re: [sl4] YouMayWantToLetMeOut"</a>
<li><strong>Reply:</strong> <a href="../0807/19160.html">Lee Corbin: "Re: [sl4] YouMayWantToLetMeOut"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19140">[ date ]</a>
<a href="index.html#19140">[ thread ]</a>
<a href="subject.html#19140">[ subject ]</a>
<a href="author.html#19140">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
