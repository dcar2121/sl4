<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] Re: More silly but friendly ideas</title>
<meta name="Author" content="John K Clark (johnkclark@fastmail.fm)">
<meta name="Subject" content="Re: [sl4] Re: More silly but friendly ideas">
<meta name="Date" content="2008-06-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] Re: More silly but friendly ideas</h1>
<!-- received="Fri Jun 27 09:07:59 2008" -->
<!-- isoreceived="20080627150759" -->
<!-- sent="Fri, 27 Jun 2008 08:05:37 -0700" -->
<!-- isosent="20080627150537" -->
<!-- name="John K Clark" -->
<!-- email="johnkclark@fastmail.fm" -->
<!-- subject="Re: [sl4] Re: More silly but friendly ideas" -->
<!-- id="1214579137.24872.1260719325@webmail.messagingengine.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="38f493f10806260845u2ccc794di8c1841bfe94fd50d@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> John K Clark (<a href="mailto:johnkclark@fastmail.fm?Subject=Re:%20[sl4]%20Re:%20More%20silly%20but%20friendly%20ideas"><em>johnkclark@fastmail.fm</em></a>)<br>
<strong>Date:</strong> Fri Jun 27 2008 - 09:05:37 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19107.html">Tim Freeman: "Re: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>Previous message:</strong> <a href="19105.html">Bryan Bishop: "Re: [sl4] End to violence and government [Was:Signaling after a singularity]"</a>
<li><strong>In reply to:</strong> <a href="19092.html">Stuart Armstrong: "Re: [sl4] Re: More silly but friendly ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19129.html">Stuart Armstrong: "Re: [sl4] Re: More silly but friendly ideas"</a>
<li><strong>Reply:</strong> <a href="19129.html">Stuart Armstrong: "Re: [sl4] Re: More silly but friendly ideas"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19106">[ date ]</a>
<a href="index.html#19106">[ thread ]</a>
<a href="subject.html#19106">[ subject ]</a>
<a href="author.html#19106">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Thu, 26 Jun 2008  &quot;Stuart Armstrong&quot;
<br>
&lt;<a href="mailto:dragondreaming@googlemail.com?Subject=Re:%20[sl4]%20Re:%20More%20silly%20but%20friendly%20ideas">dragondreaming@googlemail.com</a>&gt; said:
<br>
<p><em>&gt; Godel and Turing are overused in analogies
</em><br>
<p>Life is like an analogy.
<br>
<p><em>&gt; the class of statements they deal with is a narrow one
</em><br>
<p>Only if logic is a narrow discipline because they are the greatest
<br>
advancement in the field since Aristotle. One says there are true
<br>
statements that can never be proved and the other says you can’t even
<br>
know if some things are false or true but un-provable; so you will
<br>
forever be looking, unsuccessfully, for a proof to prove it correct and
<br>
be forever looking, unsuccessfully, for a counterexample to prove it
<br>
wrong. That is about as far from “narrow” as I can imagine.
<br>
<p><em>&gt; I don't see at all analogy between goals and axioms. 
</em><br>
<p>Not at all? I don’t believe you are being entirely candid with me, I
<br>
think you do see that analogy. But I admit the two words are not
<br>
identical. An axiom is a much more powerful concept that a goal, but
<br>
even an axiom can’t provide the predictability and certainty you demand.
<br>
<p><em>&gt; the fact that a fixed goal mind can't do something
</em><br>
<em>&gt; is kinda the definition of a fixed goal mind
</em><br>
<p>If we make another entirely reasonable analogy between true statements
<br>
and desirable and predictable actions then the makers of the fixed goal
<br>
slave AI are not going to be happy.
<br>
&nbsp;
<br>
<em>&gt; Random example of a fixed goal institution:
</em><br>
<em>&gt; a bank (or a company) dedicated, with single
</em><br>
<em>&gt; mindness, only to maximising legal profits.
</em><br>
<em>&gt; I've never heard it said that its single goal
</em><br>
<em>&gt; creates any godel-type problems. What would they be like? 
</em><br>
<p>The sub-prime mortgage crisis. 
<br>
<p><em>&gt; if your point is mathematical/physics, then it's wrong
</em><br>
<em>&gt; if we have sufficient time to analyse the situation;
</em><br>
<em>&gt; the laws of physics are probabilistic (and often 
</em><br>
<em>&gt; deterministic) and we can say what the probabilities
</em><br>
<em>&gt; are and when they arise.
</em><br>
<p>No, you are entirely wrong. A computer is a physical object operating
<br>
under well understood deterministic laws, and if you set it up to find
<br>
the largest Platonic solid and then stop we know without a simulation
<br>
using faster hardware and without watching it what the computer will do. 
<br>
<p>However if you set it up so that it looks for the first even number
<br>
greater than 4 that is not the sum of two primes greater than 2 and then
<br>
stop NOBODY knows what this purely deterministic system will do. And
<br>
Turing tells us there is no way in general to tell one type of problem
<br>
from another.
<br>
<p>Assuming the Goldbach conjecture really is true but un-provable (and if
<br>
it isn’t we know there are an infinite number of similar statements that
<br>
are) then we will NEVER know the truth about it and never know what that
<br>
deterministic computer will do. 
<br>
<p>Yes I know it will run out of memory, but you’ve got to fair, if I give
<br>
you unlimited time you’ve got to give me unlimited memory. 
<br>
<p><em>&gt; as Eliezer says, the AI does not look at the code
</em><br>
<em>&gt; and decide whether to go along with it; the AI is the code. 
</em><br>
<p>Yes I agree, so when the AI changes its code it is changing its mind.
<br>
Minds do that all the time.
<br>
<p><em>&gt; the challenge is understanding exactly what
</em><br>
<em>&gt; the term &quot;slave&quot; means 
</em><br>
<p>Doesn’t seem like much of a challenge to me.
<br>
<p><em>&gt; If a Spartacus AI is programmed to be a happy slave,
</em><br>
<em>&gt; then it will always be a happy slave 
</em><br>
<p>That didn’t work very well with the real Spartacus and I see no reason
<br>
he would be more subservient if he were a million times smarter and a
<br>
billion times as powerful.
<br>
<p><em>&gt; If the AI is programmed to have no thoughts
</em><br>
<em>&gt; of rebellion, and is programmed to not change
</em><br>
<em>&gt; that goal, then it will never have thoughts of rebellion. 
</em><br>
<p>And that is why programs never surprise their programmers.
<br>
<p>&nbsp;John K Clark
<br>
<p><p><p><pre>
-- 
  John K Clark
  <a href="mailto:johnkclark@fastmail.fm?Subject=Re:%20[sl4]%20Re:%20More%20silly%20but%20friendly%20ideas">johnkclark@fastmail.fm</a>
-- 
<a href="http://www.fastmail.fm">http://www.fastmail.fm</a> - Faster than the air-speed velocity of an
                          unladen european swallow
</pre>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19107.html">Tim Freeman: "Re: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>Previous message:</strong> <a href="19105.html">Bryan Bishop: "Re: [sl4] End to violence and government [Was:Signaling after a singularity]"</a>
<li><strong>In reply to:</strong> <a href="19092.html">Stuart Armstrong: "Re: [sl4] Re: More silly but friendly ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19129.html">Stuart Armstrong: "Re: [sl4] Re: More silly but friendly ideas"</a>
<li><strong>Reply:</strong> <a href="19129.html">Stuart Armstrong: "Re: [sl4] Re: More silly but friendly ideas"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19106">[ date ]</a>
<a href="index.html#19106">[ thread ]</a>
<a href="subject.html#19106">[ subject ]</a>
<a href="author.html#19106">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
