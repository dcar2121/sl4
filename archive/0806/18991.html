<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: Artificial Intelligence will Kill our Grandchildren</title>
<meta name="Author" content="john ellis (john_f_ellis@yahoo.co.uk)">
<meta name="Subject" content="Re: Artificial Intelligence will Kill our Grandchildren">
<meta name="Date" content="2008-06-14">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: Artificial Intelligence will Kill our Grandchildren</h1>
<!-- received="Sat Jun 14 15:53:10 2008" -->
<!-- isoreceived="20080614215310" -->
<!-- sent="Sat, 14 Jun 2008 21:50:34 +0000 (GMT)" -->
<!-- isosent="20080614215034" -->
<!-- name="john ellis" -->
<!-- email="john_f_ellis@yahoo.co.uk" -->
<!-- subject="Re: Artificial Intelligence will Kill our Grandchildren" -->
<!-- id="785928.75238.qm@web27203.mail.ukl.yahoo.com" -->
<!-- charset="utf-8" -->
<!-- inreplyto="1db0b2da0806140541y3f6eb899k12e94496871af15@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> john ellis (<a href="mailto:john_f_ellis@yahoo.co.uk?Subject=Re:%20Artificial%20Intelligence%20will%20Kill%20our%20Grandchildren"><em>john_f_ellis@yahoo.co.uk</em></a>)<br>
<strong>Date:</strong> Sat Jun 14 2008 - 15:50:34 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="18992.html">justin corwin: "Re: Artificial Intelligence will Kill our Grandchildren"</a>
<li><strong>Previous message:</strong> <a href="18990.html">John K Clark: "Re: More silly but friendly ideas"</a>
<li><strong>In reply to:</strong> <a href="18988.html">Aleksei Riikonen: "Re: Artificial Intelligence will Kill our Grandchildren"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18992.html">justin corwin: "Re: Artificial Intelligence will Kill our Grandchildren"</a>
<li><strong>Reply:</strong> <a href="18992.html">justin corwin: "Re: Artificial Intelligence will Kill our Grandchildren"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18991">[ date ]</a>
<a href="index.html#18991">[ thread ]</a>
<a href="subject.html#18991">[ subject ]</a>
<a href="author.html#18991">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Terry Wallace, associate director for Science, Technology and Engineering at Los Alamos. “Just a week after formal introduction of the machine to the world, we are already doing computational tasks that existed only in the realm of imagination a year ago.”
<br>
<p>You'd better get a move on. Roadrunner's ability to program the entire visual cortex, which is 50% of the intelligent part of the human brain does, exists now.
<br>
<p>Human intelligence is at most 2-3 years away in machines by this route, and post human intelligence by IJ Good's dictum, the day after that. 
<br>
<p>There is but one way to guarantee safety now, which is to build containable AGI first and neutralise all other attempts as humanely as possible, which luckily for me I'm working on and expect to complete before Roadrunner does.
<br>
<p><p>--- On Sat, 14/6/08, Aleksei Riikonen &lt;<a href="mailto:aleksei@iki.fi?Subject=Re:%20Artificial%20Intelligence%20will%20Kill%20our%20Grandchildren">aleksei@iki.fi</a>&gt; wrote:
<br>
<p><em>&gt; From: Aleksei Riikonen &lt;<a href="mailto:aleksei@iki.fi?Subject=Re:%20Artificial%20Intelligence%20will%20Kill%20our%20Grandchildren">aleksei@iki.fi</a>&gt;
</em><br>
<em>&gt; Subject: Re: Artificial Intelligence will Kill our Grandchildren
</em><br>
<em>&gt; To: <a href="mailto:sl4@sl4.org?Subject=Re:%20Artificial%20Intelligence%20will%20Kill%20our%20Grandchildren">sl4@sl4.org</a>
</em><br>
<em>&gt; Date: Saturday, 14 June, 2008, 5:41 AM
</em><br>
<em>&gt; On Sat, Jun 14, 2008 at 11:22 AM, Vladimir Nesov
</em><br>
<em>&gt; &lt;<a href="mailto:robotact@gmail.com?Subject=Re:%20Artificial%20Intelligence%20will%20Kill%20our%20Grandchildren">robotact@gmail.com</a>&gt; wrote:
</em><br>
<em>&gt; &gt; On Sat, Jun 14, 2008 at 6:11 AM, Anthony Berglas
</em><br>
<em>&gt; &lt;<a href="mailto:anthony@berglas.org?Subject=Re:%20Artificial%20Intelligence%20will%20Kill%20our%20Grandchildren">anthony@berglas.org</a>&gt; wrote:
</em><br>
<em>&gt; &gt;&gt; So all comments most welcome, especially as to
</em><br>
<em>&gt; what the paper does not need
</em><br>
<em>&gt; &gt;&gt; to say.
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; &quot;So this paper proposes a moratorium on producing
</em><br>
<em>&gt; faster computers. &quot;
</em><br>
<em>&gt; &gt;
</em><br>
<em>&gt; &gt; Even if it works now (it won't), in the future,
</em><br>
<em>&gt; when nanotechnology
</em><br>
<em>&gt; &gt; matures, you won't be able to ban it anyway, just
</em><br>
<em>&gt; as you can't ban
</em><br>
<em>&gt; &gt; information now.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; One can't ban information right now -- technical and
</em><br>
<em>&gt; political
</em><br>
<em>&gt; structures that would be required are not in place -- but
</em><br>
<em>&gt; high-tech
</em><br>
<em>&gt; societies where the flow of information is strictly
</em><br>
<em>&gt; controlled and/or
</em><br>
<em>&gt; monitored are actually quite feasible. Mostly just a matter
</em><br>
<em>&gt; of having
</em><br>
<em>&gt; a huge amount of surveillance. A society where the use of
</em><br>
<em>&gt; advanced
</em><br>
<em>&gt; technology, such as freely programmable computers, is
</em><br>
<em>&gt; banned
</em><br>
<em>&gt; everywhere except in a relatively few strictly controlled
</em><br>
<em>&gt; facilities
</em><br>
<em>&gt; is also a non-impossibility.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; If in the next few decades we get destructive near-human
</em><br>
<em>&gt; intelligence
</em><br>
<em>&gt; computer viruses running around on the net, or something
</em><br>
<em>&gt; like that, it
</em><br>
<em>&gt; is also politically realistic that we'll see stuff like
</em><br>
<em>&gt; the outlawing
</em><br>
<em>&gt; of all computers that don't have advanced government
</em><br>
<em>&gt; spyware on them,
</em><br>
<em>&gt; including e.g. the feature to send a live screen capture to
</em><br>
<em>&gt; government
</em><br>
<em>&gt; agents whenever they feel like it. Administrations such as
</em><br>
<em>&gt; currently
</em><br>
<em>&gt; in power in major countries such as China, U.S. and Russia
</em><br>
<em>&gt; would rub
</em><br>
<em>&gt; their hands with glee at any political excuse to implement
</em><br>
<em>&gt; such
</em><br>
<em>&gt; systems of control. And I for one would not even oppose if
</em><br>
<em>&gt; they
</em><br>
<em>&gt; proposed such things, if the only alternative was to have
</em><br>
<em>&gt; no defenses
</em><br>
<em>&gt; against powerful AI technologies. (In reality, there is the
</em><br>
<em>&gt; alternative of a Transparent Society, where all
</em><br>
<em>&gt; surveillance
</em><br>
<em>&gt; information is available to everyone and not just
</em><br>
<em>&gt; government
</em><br>
<em>&gt; agencies.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; This is actually what I consider the highest-probability
</em><br>
<em>&gt; scenario for
</em><br>
<em>&gt; the pre-singularity future. We will see extremely pervasive
</em><br>
<em>&gt; surveillance, either of the Transparent Society or the Big
</em><br>
<em>&gt; Brother
</em><br>
<em>&gt; variation. Friendly AI advocates will probably start to
</em><br>
<em>&gt; lobby for that
</em><br>
<em>&gt; as the necessary Plan A, once the difficulty of solving FAI
</em><br>
<em>&gt; has become
</em><br>
<em>&gt; sufficiently apparent. I certainly expect a solution to FAI
</em><br>
<em>&gt; to take
</em><br>
<em>&gt; more time than we would have if we didn't build
</em><br>
<em>&gt; pervasive
</em><br>
<em>&gt; surveillance.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 
</em><br>
<em>&gt; (On the paper that started this thread, I'll also make
</em><br>
<em>&gt; note here of
</em><br>
<em>&gt; the core error that earlier comments also touched on, that
</em><br>
<em>&gt; it is
</em><br>
<em>&gt; assumed in the paper that AIs would necessarily have
</em><br>
<em>&gt; human-unfriendly
</em><br>
<em>&gt; motivations. Friendly AI is very difficult, but not
</em><br>
<em>&gt; impossible.)
</em><br>
<em>&gt; 
</em><br>
<em>&gt; -- 
</em><br>
<em>&gt; Aleksei Riikonen - <a href="http://www.iki.fi/aleksei">http://www.iki.fi/aleksei</a>
</em><br>
<p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__________________________________________________________
<br>
Sent from Yahoo! Mail.
<br>
A Smarter Email <a href="http://uk.docs.yahoo.com/nowyoucan.html">http://uk.docs.yahoo.com/nowyoucan.html</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18992.html">justin corwin: "Re: Artificial Intelligence will Kill our Grandchildren"</a>
<li><strong>Previous message:</strong> <a href="18990.html">John K Clark: "Re: More silly but friendly ideas"</a>
<li><strong>In reply to:</strong> <a href="18988.html">Aleksei Riikonen: "Re: Artificial Intelligence will Kill our Grandchildren"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18992.html">justin corwin: "Re: Artificial Intelligence will Kill our Grandchildren"</a>
<li><strong>Reply:</strong> <a href="18992.html">justin corwin: "Re: Artificial Intelligence will Kill our Grandchildren"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18991">[ date ]</a>
<a href="index.html#18991">[ thread ]</a>
<a href="subject.html#18991">[ subject ]</a>
<a href="author.html#18991">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
