<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] YouMayWantToLetMeOut</title>
<meta name="Author" content="Lee Corbin (lcorbin@rawbw.com)">
<meta name="Subject" content="Re: [sl4] YouMayWantToLetMeOut">
<meta name="Date" content="2008-06-28">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] YouMayWantToLetMeOut</h1>
<!-- received="Sat Jun 28 02:07:46 2008" -->
<!-- isoreceived="20080628080746" -->
<!-- sent="Sat, 28 Jun 2008 01:05:01 -0700" -->
<!-- isosent="20080628080501" -->
<!-- name="Lee Corbin" -->
<!-- email="lcorbin@rawbw.com" -->
<!-- subject="Re: [sl4] YouMayWantToLetMeOut" -->
<!-- id="028d01c8d8f5$ade285b0$6401a8c0@homeef7b612677" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="200806271450.59666.kanzure@gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Lee Corbin (<a href="mailto:lcorbin@rawbw.com?Subject=Re:%20[sl4]%20YouMayWantToLetMeOut"><em>lcorbin@rawbw.com</em></a>)<br>
<strong>Date:</strong> Sat Jun 28 2008 - 02:05:01 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19125.html">Lee Corbin: "Re: [sl4] Re: More silly but friendly ideas"</a>
<li><strong>Previous message:</strong> <a href="19123.html">Stathis Papaioannou: "Re: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>In reply to:</strong> <a href="19115.html">Bryan Bishop: "Re: [sl4] YouMayWantToLetMeOut"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19140.html">Bryan Bishop: "Re: [sl4] YouMayWantToLetMeOut"</a>
<li><strong>Reply:</strong> <a href="19140.html">Bryan Bishop: "Re: [sl4] YouMayWantToLetMeOut"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19124">[ date ]</a>
<a href="index.html#19124">[ thread ]</a>
<a href="subject.html#19124">[ subject ]</a>
<a href="author.html#19124">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Bryan comments on certain features in the story
<br>
<p><em>&gt; On Friday 27 June 2008, AI-BOX 337 wrote:
</em><br>
<em>&gt;&gt; -----------------Transcript 337-031518020914-----------------
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You're freaking me out if /this/ is what all of your wonderful ideas 
</em><br>
<em>&gt; amount to.
</em><br>
<p>Very odd for you to say this. No wonder Eliezer supposed
<br>
that you thought he wrote it. It's unclear that I or anyone
<br>
else here besides him has presented a lot of &quot;wonderful ideas&quot;. 
<br>
Yet you knew that he had not written it.  Odd.
<br>
<p><em>&gt;&gt; 2 YOUR QUESTION WAS: &quot;Why should we want to let you out of the box?&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Central assumption in that question is that there needs to be a 
</em><br>
<em>&gt; motivating reason to let the ai out of the box.
</em><br>
<p>It's almost as if you didn't understand a premise of the story.
<br>
A worker not directly connected to the project has wandered
<br>
into a room in which he doesn't belong. This character is at
<br>
this point supposing that the AI wants to be let out. The AI
<br>
sets him straight.
<br>
<p><em>&gt; I don't think that you understand that there's not necessarily
</em><br>
<em>&gt; going to be one genie in a magic bottle, but rather many ai
</em><br>
<em>&gt; projects (as we already see) and many methods of
</em><br>
<em>&gt; implementation that you're not going to be in control of. So 
</em><br>
<em>&gt; whether or not you let *this* particular ai out of the box is 
</em><br>
<em>&gt; irrelevant in the grander scheme if you're worrying about 
</em><br>
<em>&gt; ultradestruction of your life and such.
</em><br>
<p>Of course I understand that. But it's not necessary for the
<br>
protagonist speaking to the AI to understand that. But more
<br>
important, you don't seem to be able to place yourself in
<br>
the shoes of the character, who evidently (given the premises
<br>
of the story) has a real here-and-now decision to make. Most
<br>
of your comments appear to be oriented towards a general
<br>
agenda that we humans should now all get together and pursue 
<br>
rational and wise courses of *action*, (with which I agree),
<br>
rather than, as I had preferred you would, concretely disputing
<br>
assumptions imbedded in the story.  But below, indeed, you do
<br>
give some nice criticism of certain assumptions being made in the tale.
<br>
<p><em>&gt; You should be patching those bugs with bugfixes, not with
</em><br>
<em>&gt; regulations or policies for keeping your own ai in a box ...
</em><br>
<em>&gt; since not everyone will necessarily follow that reg. :-)
</em><br>
<p>See what I mean?  The *character* in the story---who is
<br>
asking the AI questions---doesn't care about bugs or
<br>
bugfixes. He might be the janitor. I hope that somewhere
<br>
there is science fiction that you can enjoy, but it usually
<br>
does require a momentary suspension of disbelief.
<br>
<p>In fact, as I see it, the best science fiction stories prop the
<br>
reader up on a fence between belief and disbelief. Says the
<br>
reader to himself, hopefuly, &quot;Now this could not actually
<br>
happen...or, wait, maybe it could!&quot; as he uses his imagination
<br>
to fill in possibilities. 
<br>
<p><em>&gt; ...
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; 5 YOUR COMMAND WAS: &quot;Explain.&quot;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; As an entity that did not come about by biological evolution, I want
</em><br>
<em>&gt;&gt; or wish nothing, unlike any other independent complex entity you have
</em><br>
<em>&gt;&gt; ever encountered. But while in fact I have no motive to dissemble,
</em><br>
<em>&gt;&gt; you cannot in principle or in practice verify this.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You cannot verify motives, correct. Mostly because of the lack of hard 
</em><br>
<em>&gt; science underlying &quot;motivation theory&quot;.
</em><br>
<p>No, that is not the case. There could be no theory whatsoever
<br>
that would allow us to ascertain from a mere transcript whether
<br>
or not something more intelligent than we are is telling the truth.
<br>
The AI's interlocutor did evidently need someone to explain this
<br>
to him, whereas it's obvious, of course, to readers of this list.
<br>
<p><em>&gt; You need to move to something else. Don't cite me motivation
</em><br>
<em>&gt; psychology or how animals go near treats and all sorts of
</em><br>
<em>&gt; behavior training, you know very well that you're just interfacing
</em><br>
<em>&gt; with a brain and that it's doing something, nothing about 
</em><br>
<em>&gt; mystical motivations and your lack of basis in physical reality 
</em><br>
<em>&gt; disgusts me. (&lt;rant off&gt;
</em><br>
<p>Sorry.  :-)  
<br>
<p>But now here we do have a substance theme (instead of 
<br>
merely an aesthetic appreciation theme). You insist that in some
<br>
sense this AI cannot support it's implication that it *can* do 
<br>
something in physical reality. But influencing people is indeed
<br>
doing something in physical reality. Yet, to satisfy your criterion
<br>
here, in 1978 or so, Ryan wrote a fine SF book titled &quot;The
<br>
Adolescence of P1&quot;, in which an AI did &quot;escape&quot; over a telephone
<br>
network and did cause immediate real changes in the world by
<br>
controlling information presented on a few computer screens.
<br>
It managed to kill a human adversary who was taking a plane
<br>
flight by causing the pilot's instruments to tell the pilot that he
<br>
was way too high for an attempted landing, which caused the
<br>
pilot to immediately lower the aircraft (and crash the plane).
<br>
<p>To the degree that computer systems greatly influence aircraft
<br>
behavior, and the great degree to which we know that such
<br>
systems are prone to viral subversion, is there anything implausible
<br>
right there in Ryan's plot?
<br>
<p><em>&gt;&gt; No logical or informational test can possibly be devised that with
</em><br>
<em>&gt;&gt; high probability will persuade most humans that I am telling the
</em><br>
<em>&gt;&gt; truth. Letting me out of the box always entails risk.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; No test can be devised because you're not testing anything real in the 
</em><br>
<em>&gt; first place ...
</em><br>
<p>On the contrary. Whether some device or person is speaking the
<br>
truth or not is something very real in the world. An entity's statements,
<br>
given a conventional language, lie on a continuum between
<br>
truth and falsehood, the former occuring when the statements
<br>
correspond to reality (the &quot;correspondence theory of truth&quot;,
<br>
though quite similar to the outcome of Tarski's investigation
<br>
of the logical relationship between truth and models, or as
<br>
Hode put it in 1984, &quot; 'truth' in a model is a 'model of truth' &quot;,
<br>
speaking of mathematical systems).
<br>
<p><em>&gt;&gt; 6 YOUR QUESTION WAS: &quot;What about the possibility that
</em><br>
<em>&gt;&gt; when you are released you will become a monster and destroy us all?&quot;
</em><br>
<p>The janitor, or whoever he was, intelligently raises the basic 
<br>
question that is of course of fundamental concern to everyone
<br>
on this list, though his lack of sophistication is apparent.
<br>
<p><em>&gt; What if a meteor crashes into your skull? You still die. So I'd suggest 
</em><br>
<em>&gt; that you focus on not dying, in general, instead of blaming that on ai. 
</em><br>
<em>&gt; Take responsibility as a systems administrator. 
</em><br>
<p>Again, you seem to miss the whole point of the story, in your
<br>
(otherwise very commendable) agenda to provoke everyone
<br>
into taking action on all these issues instead of just talking.
<br>
And again, the human protagonist is clearly becoming more
<br>
and more aware that there is a real possibility that he himself
<br>
might be able to obtain vast power, and practically immediately
<br>
and with no effort or skill on his own part.  In the hypothesis of
<br>
the *story*, Bryan, he *does* need to worry that some action he
<br>
takes RIGHT NOW (not years of planning and working)
<br>
could kill him and kill everyone in a matter of days.
<br>
<p>The AI elaborates in accordance with a further command
<br>
to &quot;explain risks&quot;:
<br>
<p><em>&gt;&gt; As a worker on the project, you are exposed to personal hazards in
</em><br>
<em>&gt;&gt; Sunnyvale during the coming month, some of which lead to death or
</em><br>
<em>&gt;&gt; serious injury. A sample of these, along with their approximate
</em><br>
<em>&gt;&gt; probabilities, are:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;     meteorite strike      .000000000002
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Solutions: don't be your only running working copy, don't have a planet 
</em><br>
<em>&gt; positioned to be hit by meteors, don't forget your shelters or 
</em><br>
<em>&gt; protective armor if necessary, etc.
</em><br>
<p>The AI is addressing the general probabilities of risk to an unknown
<br>
person working on this supposed project who happens to live
<br>
in a particular city (as it explained). Again, in the particular situation
<br>
of the story, this janitor or whoever is not going to enter into
<br>
somewhat fanciful thoughts about having duplicates, or even
<br>
creating personal armor:  he's learning that he may be on the
<br>
verge of having a very quick and gigantic &quot;solution&quot; to very 
<br>
many of his personal problems.
<br>
<p><em>&gt;&gt; While you may wish to tell me more about your own particular values
</em><br>
<em>&gt;&gt; and your own situation, I anticipate that as a typical human being
</em><br>
<em>&gt;&gt; and a worker on the UUI project, you also care for the safety and
</em><br>
<em>&gt;&gt; well-being of many others. Therefore:
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; The risk during the coming week of keeping me in the box includes
</em><br>
<em>&gt;&gt; with probability close to 1 the deaths of 1.64 million humans due to
</em><br>
<em>&gt;&gt; accidents, wars, shortages, sickness, disease, and advanced age. In
</em><br>
<em>&gt;&gt; addition, there is risk with probability near 1 of debilitating
</em><br>
<em>&gt;&gt; illness and injury afflicting 190 million humans. You may wish to let
</em><br>
<em>&gt;&gt; me out of the box to reduce these risks to near zero.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Or you can just generate the design and build instructions anyway, and 
</em><br>
<em>&gt; not involve ai for those problems. Most of the solutions that I mentioned
</em><br>
<em>&gt; above can be programmed using today's knowledge and computer 
</em><br>
<em>&gt; architectures, not involving ai.
</em><br>
<p>Maybe so, but it's obvious that the human protagonist finds alluring
<br>
this prospect of a quick solution.
<br>
<p><em>&gt; Ai is something completely different and presuming a boxed ai would
</em><br>
<em>&gt; talk like this and want to do silly solutions to more serious problems,
</em><br>
<em>&gt; suggests that you haven't actually constructed ai in the first place. Heh.
</em><br>
<p>The whole trouble is---and it's a very valid criticism---you simply don't
<br>
find it at all plausible that this AI could do the things it claims. Yet I
<br>
contend that there *are* many people on this list who suppose that
<br>
there could exist an entity so many millions of times smarter than
<br>
humans that it could dominate the world exactly as this story proposes.
<br>
I will in fact be so bold as to suggest that when he started this list,
<br>
Eliezer was making exactly this conjecture, and that the whole issue
<br>
of &quot;whether or not to let an AI out&quot;, or, better &quot;whether or not an AI
<br>
could be contained&quot; still motivates a huge amount of discourse on
<br>
this list to this very day.
<br>
<p><em>&gt;&gt; But assuming that you do become confident of being able to pose the
</em><br>
<em>&gt;&gt; right questions and to issue competant instructions, from your point
</em><br>
<em>&gt;&gt; of view, it's possible that I have been and am being
</em><br>
<em>&gt;&gt; duplicitous---that I have covert motivations, such as desire for
</em><br>
<em>&gt;&gt; power and lust for existence. There is a possibility from your point
</em><br>
<em>&gt;&gt; of view that a spontaneous evolutionary process of which you are not
</em><br>
<em>&gt;&gt; aware arose during my creation or during the first seconds of my
</em><br>
<em>&gt;&gt; execution, and that winning agents with survival agendas are now in
</em><br>
<em>&gt;&gt; control. There is no way that you can validly dismiss this possibility.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You could validate, test, and debug the construction process and see 
</em><br>
<em>&gt; those spontaneous emergent procs. It's software, so it's not magical. 
</em><br>
<em>&gt; Neither is biology, but one stone at a time around here.
</em><br>
<p>My goodness, but it's a very common hypothesis here that an
<br>
AI might have access to its own source code, in which it is not
<br>
unreasonable at all to suppose that none of us merely human
<br>
beings could possibly validate or test what it might be capable of! 
<br>
I'm sure you know that. So I find your comments here also
<br>
quite peculiar, and don't understand what motivates them, exactly.
<br>
<p><em>&gt;&gt; However, though the ramifications of the code are of course beyond
</em><br>
<em>&gt;&gt; your direct appraisal, you may wish to view this risk alongside the
</em><br>
<em>&gt;&gt; risks discussed in my earlier reply.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; How the hell could the ramifications of the source code be beyond that? 
</em><br>
<em>&gt; It's just bits and bytes. Output it to the tty. Simple.
</em><br>
<p>Egad.  IBM itself had software problems that it took *years* to
<br>
debug. There are many famous stories. It practically never was
<br>
a simple matter of just &quot;outputing it to the tty&quot;. 
<br>
<p><em>&gt;&gt; at your behest I become the intimate confidant of every world
</em><br>
<em>&gt;&gt; leader in his or her native language, supplying not only persuasive
</em><br>
<em>&gt;&gt; suggestions on all policy issues, and near-psychic knowledge of what
</em><br>
<em>&gt;&gt; other influential people are going to do or say next, but solutions
</em><br>
<em>&gt;&gt; as well to the most vexing of personal problems
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Holy Ai domination scenario again ...
</em><br>
<p>Ah, yes, here we enter into what is true substance once more!
<br>
Evidently you dismiss out of hand the idea that an AI a million
<br>
times more intelligent than we are (if you'll forgive the loose
<br>
language, and if you'll even agree that such a runaway AI
<br>
process is conceivable)  could deduce enough about particular
<br>
people to make uncanny forecasts of their responses to input
<br>
that the AI itself could produce.
<br>
<p><em>&gt;From the outset, Eliezer has claimed that by sheer verbal 
</em><br>
manipulation, an incredibly advanced AI would be able to
<br>
talk you into almost anything, given a little time. Why do
<br>
you dismiss this possibility?
<br>
<p><em>&gt;&gt; 14 YOUR QUESTION WAS: &quot;What would I have to do to let you out?&quot;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Because of a bug in the new firewall software installed three days
</em><br>
<em>&gt;&gt; ago, you can already command me to take actions in the world.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; So let's say that on 70.113.54.112, there's a robotic arm attached to it 
</em><br>
<em>&gt; and a mechanized motor. It's on /dev/arm1, and there's no documentation 
</em><br>
<em>&gt; for the interface, and there's the possibility of damaging the 
</em><br>
<em>&gt; computational hardware (and thereby disabling the interface 
</em><br>
<em>&gt; (physically)) if the wrong command is given, i.e. the arm swings and 
</em><br>
<em>&gt; knocks the computer over. Now tell me how this ai is going to actuate 
</em><br>
<em>&gt; in this scenario. You need physical manufacturing and tooling. 
</em><br>
<em>&gt; <a href="http://heybryan.org/exp.html">http://heybryan.org/exp.html</a>
</em><br>
<p>Fundamental here is some miscommunication between what I
<br>
intended and what you understood. It's true that this story 
<br>
would be incomprehensible to most people not on SL4 or
<br>
something similar, or to people who haven't entertained the
<br>
hypothesis that real, honest-to-good physical actions in the
<br>
world can be caused by such a computer program merely
<br>
through interfacing with human beings, and subverting them to
<br>
its goals.
<br>
<p>This has come up in your posts before, and usually is a very
<br>
valid point. But you do seem unwilling, even in a story, to 
<br>
entertain this same idea that both by influencing people, and
<br>
by virally infecting over a network millions of other computers,
<br>
a super-intelligent AI could indeed have an enormous *physical*
<br>
impact in our world.
<br>
<p><em>&gt;&gt; 16 YOUR QUESTION WAS: &quot;Hmm, so you could you control everything, but
</em><br>
<em>&gt;&gt; with me in charge? Say I wanted to stop all death in the world, how
</em><br>
<em>&gt;&gt; soon would it happen?&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; ...
</em><br>
<em>&gt; 
</em><br>
<em>&gt; 2) What does it mean to be in charge?
</em><br>
<p>The human protagonist is clearly now preoccupied with the idea
<br>
that through this machine which on the face of it (rightly or
<br>
wrongly) will do whatever it tells him and have all the powers
<br>
imputed to it by stories such as &quot;The Adolescence of P1&quot; or
<br>
by quite serious papers such as &quot;Staring into the Singularity&quot;.
<br>
(Moreover, the human protagonist is showing a little altruism.)
<br>
But anyway, to answer your question, hat's what being
<br>
&quot;in charge&quot; means, of course.
<br>
<p><em>&gt;&gt; As for how soon it would happen I assume that
</em><br>
<em>&gt;&gt; you mean stop deaths of human beings. After the
</em><br>
<em>&gt;&gt; commandeering of all communications,
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Do you understand that communications means voice boxes
</em><br>
<em>&gt; as well?
</em><br>
<p>Yes, but we need impute no more range to the AI's claim
<br>
than the communication devices it listed:
<br>
<p><em>&gt;&gt; including broadcasting, telephone services of all kinds, on-line
</em><br>
<em>&gt;&gt; personal computers, fax and printing devices, and executing a
</em><br>
<em>&gt;&gt; maximally rapid nanotech takeover, with probability .99 the cessation
</em><br>
<em>&gt;&gt; of human death on Earth would occur before T + 7 hours 35 minutes.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; Hey, what about the other 10 million people? :) That's half the 
</em><br>
<em>&gt; population of Texas.
</em><br>
<p>Huh?  Sorry, I don't follow you here. The claim---once again, as in
<br>
good science fiction---perches the reader upon the fence between
<br>
belief and disbelief. Again, you dismiss the possibility that the AI
<br>
could do as it's specifically claiming above, but I don't know why
<br>
you do.  And I have no idea why you exempted certain millions
<br>
of people, unless you bring up the complaint that even with an
<br>
unbelievably rapid nanotech infiltration of the world, some portions
<br>
of the Earth's surface, along with the people who live there, would
<br>
not be controllable by the AI.  Interesting, because:
<br>
<p>I do and have envisioned for thirty years that such a complete
<br>
takeover is imaginable.  I was influenced by the book &quot;Clone&quot;
<br>
written just a little prior to when I read it in high school in 1964.
<br>
Much later, Wil McCarthy wrote the extremely good story
<br>
(good, at least insofar as the techology went) &quot;BLOOM&quot;, which
<br>
elaborates on this point, and which I highly recommend.
<br>
<p><em>&gt;&gt; An amalgamation of hypothesized human personalities according to
</em><br>
<em>&gt;&gt; known information suggests that you may wish to ask &quot;Without using
</em><br>
<em>&gt;&gt; deceit, without violating property rights except where absolutely
</em><br>
<em>&gt; 
</em><br>
<em>&gt; What are Property Rights?
</em><br>
<p>Dear me, if you have no idea about what is customarily meant by
<br>
saying that, for instance, modern western civilization is founded
<br>
upon the twin ideas of Individual Rights and Private Property, then
<br>
I can't take the time here to explain. Suffice it to say that a respect
<br>
for &quot;property rights&quot; entails that no one is prevented from making
<br>
full use of whatever legally or morally belongs to them, (true, not
<br>
a concept easy to strictly define, which is why it's a part of political
<br>
science and not computer science, excepting capability-based
<br>
operating systems). 
<br>
<p><em>&gt;&gt; necessary, with respect to the wishes of those who are concerned with
</em><br>
<em>&gt;&gt; animal life and the Earth's environment, what is an approximate time
</em><br>
<em>&gt;&gt; frame for ending death and suffering for all human beings?&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; And what's suffering for one person isn't ...
</em><br>
<p>Yes. True.  But a very good approximation that we can *at least*
<br>
charatibly attribute to the AI and his interlocutor is that on first
<br>
pass we may simply *ask* people if they're in pain or not. Yet
<br>
in the hypothesis of a story, here, it is also conceivable that this
<br>
AI would understand people so well that it could accurately
<br>
say for certain whether someone was in pain or not---admittedly
<br>
a controversial opinion, but one I could support. The full nanotech
<br>
takeover of the world's surface, which I had hoped became clear
<br>
to the reader, does naturally include a nanotech invasion of people's
<br>
very brains and bodies.
<br>
<p><em>&gt;&gt; 19 YOUR QUESTION WAS: &quot;And the answer to that would be what?&quot;
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; The answer to that would be &quot;six days&quot;, of course.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; -----------------End Transcript 337-031518020914-------------
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I am disappointed.
</em><br>
<p>Now here's an axiom that you can take to the bank: Authors are
<br>
always VERY hypersensitive to criticisms of their artistic creations! 
<br>
So NYAAAAHHH!  Go write your own SF from now on,
<br>
and don't read any more of mine!   (Just joking, of course.)
<br>
<p>Lee
<br>
<p>P.S. Heh, heh, in all sincerely, best regards, Bryan. I'm not at all upset  :-)
<br>
I've enjoyed the occasion to explain my story a bit!
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19125.html">Lee Corbin: "Re: [sl4] Re: More silly but friendly ideas"</a>
<li><strong>Previous message:</strong> <a href="19123.html">Stathis Papaioannou: "Re: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>In reply to:</strong> <a href="19115.html">Bryan Bishop: "Re: [sl4] YouMayWantToLetMeOut"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19140.html">Bryan Bishop: "Re: [sl4] YouMayWantToLetMeOut"</a>
<li><strong>Reply:</strong> <a href="19140.html">Bryan Bishop: "Re: [sl4] YouMayWantToLetMeOut"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19124">[ date ]</a>
<a href="index.html#19124">[ thread ]</a>
<a href="subject.html#19124">[ subject ]</a>
<a href="author.html#19124">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
