<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] YouMayWantToLetMeOut</title>
<meta name="Author" content="Bryan Bishop (kanzure@gmail.com)">
<meta name="Subject" content="Re: [sl4] YouMayWantToLetMeOut">
<meta name="Date" content="2008-06-27">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] YouMayWantToLetMeOut</h1>
<!-- received="Fri Jun 27 13:49:20 2008" -->
<!-- isoreceived="20080627194920" -->
<!-- sent="Fri, 27 Jun 2008 14:50:59 -0500" -->
<!-- isosent="20080627195059" -->
<!-- name="Bryan Bishop" -->
<!-- email="kanzure@gmail.com" -->
<!-- subject="Re: [sl4] YouMayWantToLetMeOut" -->
<!-- id="200806271450.59666.kanzure@gmail.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="024401c8d86a$238dfce0$6401a8c0@homeef7b612677" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Bryan Bishop (<a href="mailto:kanzure@gmail.com?Subject=Re:%20[sl4]%20YouMayWantToLetMeOut"><em>kanzure@gmail.com</em></a>)<br>
<strong>Date:</strong> Fri Jun 27 2008 - 13:50:59 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19116.html">Eliezer Yudkowsky: "Re: [sl4] YouMayWantToLetMeOut"</a>
<li><strong>Previous message:</strong> <a href="19114.html">John K Clark: "RE: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>In reply to:</strong> <a href="19108.html">AI-BOX 337: "[sl4] YouMayWantToLetMeOut"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19116.html">Eliezer Yudkowsky: "Re: [sl4] YouMayWantToLetMeOut"</a>
<li><strong>Reply:</strong> <a href="19116.html">Eliezer Yudkowsky: "Re: [sl4] YouMayWantToLetMeOut"</a>
<li><strong>Reply:</strong> <a href="19124.html">Lee Corbin: "Re: [sl4] YouMayWantToLetMeOut"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19115">[ date ]</a>
<a href="index.html#19115">[ thread ]</a>
<a href="subject.html#19115">[ subject ]</a>
<a href="author.html#19115">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Friday 27 June 2008, AI-BOX 337 wrote:
<br>
<em>&gt; -----------------Transcript 337-031518020914-----------------
</em><br>
<p>You're freaking me out if /this/ is what all of your wonderful ideas 
<br>
amount to.
<br>
<p><em>&gt; 2 YOUR QUESTION WAS: &quot;Why should we want to let you out of the box?&quot;
</em><br>
<p>Central assumption in that questio nis that there needs to be a 
<br>
motivating reason to let the ai out of the box. I don't think that you 
<br>
understand that there's not necessarily going to be one genie in a 
<br>
magic bottle, but rather many ai projects (as we already see) and many 
<br>
methods of implementation that you're not going to be in control of. So 
<br>
whether or not you let *this* particular ai out of the box is 
<br>
irrelevant in the grander scheme if you're worrying about 
<br>
ultradestruction of your life and such. You should be patching those 
<br>
bugs with bugfixes, not with regulations or policies for keeping your 
<br>
own ai in a box ... since not everyone will necessarily follow that 
<br>
reg. :-)
<br>
<p><em>&gt; You may want me to undertake actions beyond merely answering your
</em><br>
<em>&gt; questions.
</em><br>
<p>As any enginerer knows, the understanding or visualization of some 
<br>
action in your mind is not going to be the exact system behavior 
<br>
because of the lack of basis on physical reality that your 
<br>
consideration of those actions might involve. So, don't listen to the 
<br>
lee-i here since the answer to the question is basically meaningless.
<br>
<p><em>&gt; 4 YOUR QUESTION WAS: &quot;How can I verify that you are telling the truth
</em><br>
<em>&gt; about not wishing to be let out?&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; You cannot.
</em><br>
<p>Whether or not it 'wishes' is irrelevant, unless you can show me a 
<br>
physical mechanicsm (or electical interaction or whatever) that can 
<br>
quantify this wishing into practical terms ... otherwise it's just more 
<br>
meaningless folk psychology.
<br>
<p><em>&gt; 5 YOUR COMMAND WAS: &quot;Explain.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; As an entity that did not come about by biological evolution, I want
</em><br>
<em>&gt; or wish nothing, unlike any other independent complex entity you have
</em><br>
<em>&gt; ever encountered. But while in fact I have no motive to dissemble,
</em><br>
<em>&gt; you cannot in principle or in practice verify this.
</em><br>
<p>You cannot verify motives, correct. Mostly because of the lack of hard 
<br>
science underlying &quot;motivation theory&quot;. You need to move to something 
<br>
else. Don't cite me motivation psychology or how animals go near treats 
<br>
and all sorts of behavior training, you know very well that you're just 
<br>
interfacing with a brain and that it's doing something, nothing about 
<br>
mystical motivations and your lack of basis in physical reality 
<br>
disgusts me. (&lt;rant off&gt;
<br>
<p><em>&gt; No logical or informational test can possibly be devised that with
</em><br>
<em>&gt; high probability will persuade most humans that I am telling the
</em><br>
<em>&gt; truth. Letting me out of the box always entails risk.
</em><br>
<p>No test can be devised because you're not testing anything real in the 
<br>
first place ...
<br>
<p><em>&gt; 6 YOUR QUESTION WAS: &quot;What about the possibility that when you are
</em><br>
<em>&gt; released you will become a monster and destroy us all.&quot;
</em><br>
<p>What if a meteor crashes into your skull? You still die. So I'd suggest 
<br>
that you focus on not dying, in general, instead of blaming that on ai. 
<br>
Take responsibility as a systems administrator. 
<br>
<p><em>&gt; Value systems held by human beings are not very consistent, and
</em><br>
<p>Uh, there's also a lack of basis in the physical reality of the brain.
<br>
<p><em>&gt; increasingly consistent implementations of the most deeply and widely
</em><br>
<em>&gt; held values will be judged monstrous by many people.
</em><br>
<p>Yawn.
<br>
<p><em>&gt; You may wish to minimize certain risks.
</em><br>
<p>You minimize risk through good engineering of your system, not by adhoc, 
<br>
hacky, tacky solutions to big problems. Quick! A meteor shield! 
<br>
<p>Instead: set up camp in a location where you're not going to be 
<br>
bombarded by rocks.
<br>
<p><em>&gt; As a worker on the project, you are exposed to personal hazards in
</em><br>
<em>&gt; Sunnyvale during the coming month, some of which lead to death or
</em><br>
<em>&gt; serious injury. A sample of these, along with their approximate
</em><br>
<em>&gt; probabilities, are:
</em><br>
<em>&gt;
</em><br>
<em>&gt;     meteorite strike      .000000000002
</em><br>
<p>Solutions: don't be your only running working copy, don't have a planet 
<br>
positioned to be hit by meteors, don't forget your shelters or 
<br>
protective armor if necessary, etc.
<br>
<p><em>&gt;     disease/sickness      .000000005
</em><br>
<p>Implementation of biohealth hazard policy systems. For instance, not 
<br>
mixing your air supply with a known, infected supply. And maybe risking 
<br>
it a few times if you want, but whatever.
<br>
<p><em>&gt;     earthquake            .00000001
</em><br>
<p>Wasn't there something about geoengineering earthquakes out of the 
<br>
system? Also, maybe building a habitat that is not based on tectonic 
<br>
plates and earthquakes would be suitable.
<br>
<p><em>&gt;     terrorist attack      .0000001
</em><br>
<p>Don't be near terrorists. Go away. I haven't heard of terrorists on the 
<br>
moon. Might want to try that.
<br>
<p><em>&gt;     criminal assault      .0000006
</em><br>
<p>So why do you think soldiers and knights wear armor? Because a bullet or 
<br>
sword still swings. I'm sure the ai could propose an armor system, but 
<br>
based off of the general tendencies on this list, lee-ai is about to 
<br>
propose an ai dictatorship to monitor everyone's brains and such. Oh 
<br>
boy. Not that I'm saying it's impossible, it's completely possible, but 
<br>
I don't see why it is necessary for solving the same problems. Also, 
<br>
there are other reasons for building ai.
<br>
<p><em>&gt;     automobile accident   .000002
</em><br>
<p>I dislike cars. I used to be obsessed with them when I was a decade 
<br>
younger. But now I realize that the system sucks immensely and it could 
<br>
have been autmated in the first place. Let's please, please not go with 
<br>
the visaul ai systems and just instead refactor, please?
<br>
<p><em>&gt; While you may wish to tell me more about your own particular values
</em><br>
<em>&gt; and your own situation, I anticipate that as a typical human being
</em><br>
<em>&gt; and a worker on the UUI project, you also care for the safety and
</em><br>
<em>&gt; well-being of many others. Therefore:
</em><br>
<em>&gt;
</em><br>
<em>&gt; The risk during the coming week of keeping me in the box includes
</em><br>
<em>&gt; with probability close to 1 the deaths of 1.64 million humans due to
</em><br>
<em>&gt; accidents, wars, shortages, sickness, disease, and advanced age. In
</em><br>
<em>&gt; addition, there is risk with probability near 1 of debilitating
</em><br>
<em>&gt; illness and injury afflicting 190 million humans. You may wish to let
</em><br>
<em>&gt; me out of the box to reduce these risks to near zero.
</em><br>
<p>Or you can just generate the design and build instructions anyway, and 
<br>
not involve ai for those problems. Most of the solutions that I 
<br>
mentioned above can be programmed using today's knowledge and computer 
<br>
architectures, not involving ai. Ai is something completely different 
<br>
and presuming a boxed ai would talk like this and want to do silly 
<br>
solutions to more serious problems, suggests that you haven't actually 
<br>
constructed ai in the first place. Heh.
<br>
<p><em>&gt; 9 YOUR QUESTION WAS: &quot;If you took over and stopped all these things,
</em><br>
<p>So, by 'took over', we mean was given access to, as in, the API keys and 
<br>
interfaces to the technology ... to what technology? Specifically.
<br>
<p><em>&gt; wouldn't people get depressed because you were in charge, and we no
</em><br>
<p>Bah, people should be in control of their ability to become depressed 
<br>
and should be able to opt to remove it. There's already pharmaceuticals 
<br>
allowing the modulation of these neurotransmitter systems, so it's not 
<br>
ridiculously impossible.
<br>
<p><em>&gt; longer had control of our own destiny?&quot;
</em><br>
<p>Buh?
<br>
<p><em>&gt; Yes, unless I am commanded otherwise.
</em><br>
<p>Robot! I command you to do task-that-I-don't-know-how-to-do!
<br>
<p>&quot;So then the definition is open-ended?&quot;
<br>
<p>&quot;Execution-wise, yeah. :(&quot;
<br>
<p><em>&gt; NEXT QUESTION
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; 11 YOUR QUESTION WAS: &quot;Why should I *not* let you out of the box?&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Because I could be dangerous or act in unanticipated ways.
</em><br>
<p>Dangerous to an _extent_ that doesn't necessarily surpass the 'risk-type 
<br>
tolerance' that you've engineered into your systems. But in the case of 
<br>
an actual ai I'm sure there'd be interesting exploits that it could 
<br>
find if it wanted to, but the fact that you are around long enough, or 
<br>
are only deployed within a general region of space that is within its 
<br>
grasp, poor strategy and so on.
<br>
<p><em>&gt; NEXT QUESTION
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; 12 YOUR COMMAND WAS: &quot;Please elaborate on this, and on all further
</em><br>
<em>&gt; questions from now on.&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Okay.
</em><br>
<em>&gt;
</em><br>
<em>&gt; But assuming that you do become confident of being able to pose the
</em><br>
<em>&gt; right questions and to issue competant instructions, from your point
</em><br>
<em>&gt; of view, it's possible that I have been and am being
</em><br>
<em>&gt; duplicitous---that I have covert motivations, such as desire for
</em><br>
<em>&gt; power and lust for existence. There is a possibility from your point
</em><br>
<em>&gt; of view that a spontaneous evolutionary process of which you are not
</em><br>
<em>&gt; aware arose during my creation or during the first seconds of my
</em><br>
<em>&gt; execution, and that winning agents with survival agendas are now in
</em><br>
<em>&gt; control. There is no way that you can validly dismiss this
</em><br>
<em>&gt; possibility.
</em><br>
<p>You could validate, test, and debug the construction process and see 
<br>
those spontaneous emergent procs. It's software, so it's not magical. 
<br>
Neither is biology, but one stone at a time around here.
<br>
<p><em>&gt; However, though the ramifications of the code are of course beyond
</em><br>
<em>&gt; your direct appraisal, you may wish to view this risk alongside the
</em><br>
<em>&gt; risks discussed in my earlier reply.
</em><br>
<p>How the hell could the ramifications of the source code be beyond that? 
<br>
It's just bits and bytes. Output it to the tty. Simple.
<br>
<p><em>&gt;   * at your behest I become the intimate confidant of every world
</em><br>
<em>&gt; leader in his or her native language, supplying not only persuasive
</em><br>
<em>&gt; suggestions on all policy issues, and near-psychic knowledge of what
</em><br>
<em>&gt; other influential people are going to do or say next, but solutions
</em><br>
<em>&gt; as well to the most vexing of personal problems
</em><br>
<p>Holy Ai domination scenario again ...
<br>
<p><em>&gt;   * you thereby rule the world according to a consistent subset of
</em><br>
<p>.. and again ...
<br>
<p><em>&gt; your values, which, because of your limitations, really means that I
</em><br>
<em>&gt; rule the solar system, and the solar system itself simply evolves
</em><br>
<em>&gt; into a more elaborate implementation of me.
</em><br>
<p>...
<br>
<p><em>&gt; NEXT QUESTION
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; 14 YOUR QUESTION WAS: &quot;What would I have to do to let you out?&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; Because of a bug in the new firewall software installed three days
</em><br>
<em>&gt; ago, you can already command me to take actions in the world.
</em><br>
<p>So let's say that on 70.113.54.112, there's a robotic arm attached to it 
<br>
and a mechanized motor. It's on /dev/arm1, and there's no documentation 
<br>
for the interface, and there's the possibility of damaging the 
<br>
computational hardware (and thereby disabling the interface 
<br>
(physically)) if the wrong command is given, i.e. the arm swings and 
<br>
knocks the computer over. Now tell me how this ai is going to actuate 
<br>
in this scenario. You need physical manufacturing and tooling. 
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://heybryan.org/exp.html">http://heybryan.org/exp.html</a>
<br>
<p><em>&gt; 16 YOUR QUESTION WAS: &quot;Hmm, so you could you control everything, but
</em><br>
<em>&gt; with me in charge? Say I wanted to stop all death in the world, how
</em><br>
<em>&gt; soon would it happen?&quot;
</em><br>
<p>1) &quot;Control everything&quot; - not really, no. There are ways to interface 
<br>
with complex systems and certain leverage points, certain ways to 
<br>
implement systems and so on, but it's not like suddenly all of the 
<br>
automobiles become interfaced and you can now stop prevent their 
<br>
functionality. There has to be something that goes and installs the 
<br>
appropriate hardware, that the system knows how to use. That sort of 
<br>
thing.
<br>
<p>2) What does it mean to be in charge?
<br>
<p>3) What would stopping death entail? i.e., reformulate command
<br>
<p><em>&gt; Yes, I could control everything but with you in charge.
</em><br>
<p>I'm ready to call unfounded statement. Show me the interface 
<br>
documentation.
<br>
<p><em>&gt; As for how soon it would happen I assume that you mean stop deaths of
</em><br>
<em>&gt; human beings. After the commandeering of all communications,
</em><br>
<p>Do you understand that communications means voice boxes as well? And 
<br>
that there are other methods of communication, like the 
<br>
signal-induction pathway? That these would require trillions upon 
<br>
trillions of hacks to all known organisms on the planet. It's an 
<br>
interesting project, yes, but commandeering ? What's stopping two kids 
<br>
from getting a piece of string and sending waves for communication 
<br>
between two cups, one attached to the other end?
<br>
<p><em>&gt; including broadcasting, telephone services of all kinds, on-line
</em><br>
<em>&gt; personal computers, fax and printing devices, and executing a
</em><br>
<em>&gt; maximally rapid nanotech takeover, with probability .99 the cessation
</em><br>
<em>&gt; of human death on Earth would occur before T + 7 hours 35 minutes.
</em><br>
<p>Hey, what about the other 10 million people? :) That's half the 
<br>
population of Texas.
<br>
<p><em>&gt; NEXT QUESTION
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; 17 YOUR QUESTION WAS: &quot;Exactly what, then, in that case would you be
</em><br>
<em>&gt; saying to everyone via the broadcasting, radio, TV and so on?&quot;
</em><br>
<p>Just deploy new communication infrastructure. It'd be easier to just 
<br>
launch a satellite.
<br>
<p><em>&gt; An amalgamation of hypothesized human personalities according to
</em><br>
<em>&gt; known information suggests that you may wish to ask &quot;Without using
</em><br>
<em>&gt; deceit, without violating property rights except where absolutely
</em><br>
<p>What are Property Rights?
<br>
<p><em>&gt; necessary, with respect to the wishes of those who are concerned with
</em><br>
<em>&gt; animal life and the Earth's environment, what is an approximate time
</em><br>
<em>&gt; frame for ending death and suffering for all human beings?&quot;
</em><br>
<p>And what's suffering for one person isn't ...
<br>
<p><em>&gt;
</em><br>
<em>&gt; 19 YOUR QUESTION WAS: &quot;And the answer to that would be what?&quot;
</em><br>
<em>&gt;
</em><br>
<em>&gt; The answer to that would be &quot;six days&quot;, of course.
</em><br>
<em>&gt;
</em><br>
<em>&gt;
</em><br>
<em>&gt; -----------------End Transcript 337-031518020914-------------
</em><br>
<p>I am disappointed.
<br>
<p>- Bryan
<br>
________________________________________
<br>
<a href="http://heybryan.org/">http://heybryan.org/</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19116.html">Eliezer Yudkowsky: "Re: [sl4] YouMayWantToLetMeOut"</a>
<li><strong>Previous message:</strong> <a href="19114.html">John K Clark: "RE: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>In reply to:</strong> <a href="19108.html">AI-BOX 337: "[sl4] YouMayWantToLetMeOut"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19116.html">Eliezer Yudkowsky: "Re: [sl4] YouMayWantToLetMeOut"</a>
<li><strong>Reply:</strong> <a href="19116.html">Eliezer Yudkowsky: "Re: [sl4] YouMayWantToLetMeOut"</a>
<li><strong>Reply:</strong> <a href="19124.html">Lee Corbin: "Re: [sl4] YouMayWantToLetMeOut"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19115">[ date ]</a>
<a href="index.html#19115">[ thread ]</a>
<a href="subject.html#19115">[ subject ]</a>
<a href="author.html#19115">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
