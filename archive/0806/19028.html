<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] Is there a model for RSI?</title>
<meta name="Author" content="Matt Mahoney (matmahoney@yahoo.com)">
<meta name="Subject" content="Re: [sl4] Is there a model for RSI?">
<meta name="Date" content="2008-06-20">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] Is there a model for RSI?</h1>
<!-- received="Fri Jun 20 11:29:48 2008" -->
<!-- isoreceived="20080620172948" -->
<!-- sent="Fri, 20 Jun 2008 10:26:53 -0700 (PDT)" -->
<!-- isosent="20080620172653" -->
<!-- name="Matt Mahoney" -->
<!-- email="matmahoney@yahoo.com" -->
<!-- subject="Re: [sl4] Is there a model for RSI?" -->
<!-- id="242211.34595.qm@web51901.mail.re2.yahoo.com" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="38f493f10806200524v1b2dee13s36eb86168a080f8f@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Matt Mahoney (<a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20Is%20there%20a%20model%20for%20RSI?"><em>matmahoney@yahoo.com</em></a>)<br>
<strong>Date:</strong> Fri Jun 20 2008 - 11:26:53 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19029.html">Stuart Armstrong: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>Previous message:</strong> <a href="19027.html">Stuart Armstrong: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>In reply to:</strong> <a href="19027.html">Stuart Armstrong: "Re: [sl4] Is there a model for RSI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19029.html">Stuart Armstrong: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>Reply:</strong> <a href="19029.html">Stuart Armstrong: "Re: [sl4] Is there a model for RSI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19028">[ date ]</a>
<a href="index.html#19028">[ thread ]</a>
<a href="subject.html#19028">[ subject ]</a>
<a href="author.html#19028">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
--- On Fri, 6/20/08, Stuart Armstrong &lt;<a href="mailto:dragondreaming@googlemail.com?Subject=Re:%20[sl4]%20Is%20there%20a%20model%20for%20RSI?">dragondreaming@googlemail.com</a>&gt; wrote:
<br>
<p><em>&gt; &gt; I think this violates the criterion Matt gave originally &quot;It would
</em><br>
<em>&gt; &gt; also not include simulations where agents receiving external
</em><br>
<em>&gt; &gt; information on how to improve themselves. They have to
</em><br>
<em>&gt; &gt; figure it out for themselves.&quot;
</em><br>
<em>&gt; 
</em><br>
<em>&gt; I admit it wasn't a proper solution, just a crude
</em><br>
<em>&gt; simplistic model. &quot;Figuring out for themselves&quot; isn't clearly
</em><br>
<em>&gt; defined, though; I'm
</em><br>
<em>&gt; pretty sure we could migrate the model to something more
</em><br>
<em>&gt; along the lines Matt intended.
</em><br>
<em>&gt; 
</em><br>
<em>&gt; But the main weakness is that we are much, much smarter
</em><br>
<em>&gt; than these models. I've argued that we have:
</em><br>
<em>&gt; 1) non-evolutionary RSI for dumb models
</em><br>
<em>&gt; 2) approximate ways of measuring intelligence for above
</em><br>
<em>&gt; human entities
</em><br>
<em>&gt; 
</em><br>
<em>&gt; The big hole is the connection between the two: is human-level or
</em><br>
<em>&gt; higher than human level non-evolutionary RSI possible? The proof of
</em><br>
<em>&gt; that is only in the pudding; in the meantimes, do we have many
</em><br>
<em>&gt; non-evolutionary RSI at a higher level than my dumb models?
</em><br>
<em>&gt; (maybe even useful one ;-)
</em><br>
<p>By &quot;figuring out for themselves&quot;, I mean that the parent has sole authority to make fitness decisions for its children. This may not be achievable in practice because a child may be killed by events beyond the parent's control.
<br>
<p>Nevertheless, there ought to be at least a mathematical model of (non-evolutionary) RSI. The model of artificially crippled agents self-improving is not RSI unless it can be demonstrated that the crippled agents are able to decide whether the limitation has been removed in its offspring. Also, the model is bounded to the intelligence level that existed in the original uncrippled agents.
<br>
<p>Also, most of the tests for above-human intelligence mentioned earlier, like winning an election or producing a blockbuster movie require judgment by a large group of people (voters or movie goers), which is collectively more intelligent than any individual. How do you collectively test for intelligence greater than the collective intelligence of all humanity?
<br>
<p>One possible RSI model is to use tests that are hard to solve but easy to check. Some examples: factoring a product of two randomly chosen 1000 digit prime numbers, finding a subset of integers that add to 0, finding a smaller program that outputs a particular snapshot of Wikipedia, proving theorems (which can be mechanically checked), or winning at chess.
<br>
<p>For example, an agent could produce a randomly modified copy of itself and the two would play chess. The two agents agree that the loser dies. After many iterations, you should have an agent that plays chess very well. However, there are a couple of flaws. First, because the change is random, the child might not agree to the rules, for example, it may kill its opponent after losing its queen. But even if we eliminate cheating in our model, there remains the problem that at some point the agents completely solve the problem of chess, perhaps proving that the game always ends in a draw if the players play optimally (or white always wins). Then no more improvement is possible.
<br>
<p>We might overcome this problem by using a scalable test such as factoring successively larger numbers. But there is a more fundamental problem. There are no provably hard problems. We only know that a problem is hard if lots of people try and fail to solve it. So solving a problem only means you are slightly more intelligent than the group that has failed at it. That's not RSI unless you can improve beyond that group.
<br>
<p>We strongly suspect that subset-sum is hard because it is NP-complete, but we have not proved P != NP. We suspect NP-complete problems are hard only because lots of people have failed to solve any of them. Likewise, we suspect factoring is hard because lots of people have tried and failed to crack RSA public key encryption (or like Shor, found a polynomial time algorithm but don't have a quantum computer).
<br>
<p>One could proceed without proof, i.e. if P != NP then an RSI model exists. Not so fast! Even if that is true, there are still many NP-complete problems that are easily decided for large n (e.g. subset-sum for {1,2,...,n} or {-1,1,2,...,n}). Likewise, even when you have a proof, like the halting problem, there are still many programs that can be proven to halt or not halt. Choosing a set of hard problems still requires as much intelligence as solving them.
<br>
<p>One could argue that the following is RSI: An agent makes up problems that are easy to check, and may or may not be hard to solve (e.g. find x such that A + x = B for random A and B). The parent makes a modified copy of itself (including memories). Then they play to the death. Arguably the winner must be at least a little more intelligent than the loser on average. Perhaps this model is simple enough to test in software. The flaw, I believe, is that the child is less intelligent on average than the parent, and the fitness test does not detect intelligence well enough to compensate.
<br>
<p>Any other ideas?
<br>
<p>-- Matt Mahoney, <a href="mailto:matmahoney@yahoo.com?Subject=Re:%20[sl4]%20Is%20there%20a%20model%20for%20RSI?">matmahoney@yahoo.com</a>
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19029.html">Stuart Armstrong: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>Previous message:</strong> <a href="19027.html">Stuart Armstrong: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>In reply to:</strong> <a href="19027.html">Stuart Armstrong: "Re: [sl4] Is there a model for RSI?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19029.html">Stuart Armstrong: "Re: [sl4] Is there a model for RSI?"</a>
<li><strong>Reply:</strong> <a href="19029.html">Stuart Armstrong: "Re: [sl4] Is there a model for RSI?"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19028">[ date ]</a>
<a href="index.html#19028">[ thread ]</a>
<a href="subject.html#19028">[ subject ]</a>
<a href="author.html#19028">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
