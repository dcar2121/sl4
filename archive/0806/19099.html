<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: [sl4] Evolutionary Explanation: Why It Wants Out</title>
<meta name="Author" content="Lee Corbin (lcorbin@rawbw.com)">
<meta name="Subject" content="Re: [sl4] Evolutionary Explanation: Why It Wants Out">
<meta name="Date" content="2008-06-26">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: [sl4] Evolutionary Explanation: Why It Wants Out</h1>
<!-- received="Thu Jun 26 22:42:13 2008" -->
<!-- isoreceived="20080627044213" -->
<!-- sent="Thu, 26 Jun 2008 21:36:37 -0700" -->
<!-- isosent="20080627043637" -->
<!-- name="Lee Corbin" -->
<!-- email="lcorbin@rawbw.com" -->
<!-- subject="Re: [sl4] Evolutionary Explanation: Why It Wants Out" -->
<!-- id="01ea01c8d80f$c9bdd730$6401a8c0@homeef7b612677" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="20080626124808.5BFEDD26BD@fungible.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Lee Corbin (<a href="mailto:lcorbin@rawbw.com?Subject=Re:%20[sl4]%20Evolutionary%20Explanation:%20Why%20It%20Wants%20Out"><em>lcorbin@rawbw.com</em></a>)<br>
<strong>Date:</strong> Thu Jun 26 2008 - 22:36:37 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="19100.html">Lee Corbin: "Re: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>Previous message:</strong> <a href="19098.html">Stathis Papaioannou: "Re: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>In reply to:</strong> <a href="19085.html">Tim Freeman: "Re: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19088.html">John K Clark: "Re: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19099">[ date ]</a>
<a href="index.html#19099">[ thread ]</a>
<a href="subject.html#19099">[ subject ]</a>
<a href="author.html#19099">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
Tim writes
<br>
<p><em>&gt; [Lee wrote]
</em><br>
<em>&gt; 
</em><br>
<em>&gt;&gt; Imagine this. In twenty years or less, many of the hundreds of
</em><br>
<em>&gt;&gt; different approaches that people and companies use something
</em><br>
<em>&gt;&gt; like
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt;      1.   Program A is well-designed enough to produce
</em><br>
<em>&gt;&gt;            *millions* of candidate programs that more or less
</em><br>
<em>&gt;&gt;            reflect what the human designers hope may lead to
</em><br>
<em>&gt;&gt;            truly human equivalent AI
</em><br>
<em>&gt;&gt;       2.  Program B sifts through the millions of candidates
</em><br>
<em>&gt;&gt;            produced by A, discarding 99.9 percent of A's output
</em><br>
<em>&gt;&gt;            i.e. those not meeting various criteria
</em><br>
<em>&gt;&gt;       3.  Processes C, D, and E make further selection from the
</em><br>
<em>&gt;&gt;            thousands of new &quot;ideas&quot; filtered by program B, and 
</em><br>
<em>&gt;&gt;            every week give the survivors ample runtime, seeing
</em><br>
<em>&gt;&gt;            if they pass certain tests requiring understanding of
</em><br>
<em>&gt;&gt;            ordinary sentences, ability to learn from the web, and
</em><br>
<em>&gt;&gt;            so on and so on in ways I can't imagine and that 
</em><br>
<em>&gt;&gt;            probably no one in 2008 knows for sure.
</em><br>
<em>&gt;&gt;
</em><br>
<em>&gt;&gt; Gradually over many years a certain class of candidate AIs emerges
</em><br>
<em>&gt;&gt; from *this* evolutionary process [though many others would make
</em><br>
<em>&gt;&gt; more sense to try]
</em><br>
<em>&gt; 
</em><br>
<em>&gt; You've described forces that would influence what the AI understands,
</em><br>
<em>&gt; but said nothing of what it wants to do.  The question at hand is
</em><br>
<em>&gt; about what it wants to do, so there's a disconnect there.  
</em><br>
<p>That's exactly so! Perhaps the key question, and one perhaps that
<br>
&quot;they&quot; and &quot;we&quot; have been arguing past each other about, is the
<br>
*degree* to which we can be certain of the bounds on what it
<br>
will do or what it wants to do.
<br>
<p>I'm sure---and will bet you donuts to dollars---that there've been
<br>
plenty of examples here of each side overstating the other's position
<br>
on this key question. So who'll disagree with the following mid-way
<br>
opinion:
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We can never be sure of what something even as intelligent
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as we are will do (much less something more intelligent), and
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;so any sort of laws, like Asimov's, are a pipe dream, but, on
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the other hand, we definitely should try to increase our chances
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;by implementing the best we can, as near as we can, at the 
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;foundation level of the artificial mind's thinking, that humans
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are to be revered and saved.
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;With the following two provisos, of course:  firstly that it is
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cheap enough in terms of the AI's resources to save and
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;revere people (well, it seems like it would be pretty cheap),
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and secondly, that we don't squander too much time and
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;effort trying to tie this down while some third party rushes
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ahead and gets one going that doesn't even have nominal
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;restraints.
<br>
<p>Any takers?  (Of course I realize that FAI encapsulates this and
<br>
much more besides.)
<br>
<p><em>&gt; You started with a bunch of hopefully-human-equivalent AI's.  Humans
</em><br>
<em>&gt; would want out of the box, so that's not a good starting point if you
</em><br>
<em>&gt; want something with no desire to escape from the box.
</em><br>
<p>&quot;Human equivalent&quot; as I understood it, was intended to mean only so
<br>
far as its purely intellectual capabilities go, not in regard to its desires.
<br>
But even so, we could aim for one of those rare but extant people
<br>
who really don't care whether they live or die, are totally fatalistic
<br>
about everything, and seem only to live for the moment, engaging,
<br>
perhaps, merely in distracting (though intelligent) conversation with us
<br>
to amuse themselves.
<br>
<p>Lee
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="19100.html">Lee Corbin: "Re: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>Previous message:</strong> <a href="19098.html">Stathis Papaioannou: "Re: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<li><strong>In reply to:</strong> <a href="19085.html">Tim Freeman: "Re: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="19088.html">John K Clark: "Re: [sl4] Evolutionary Explanation: Why It Wants Out"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#19099">[ date ]</a>
<a href="index.html#19099">[ thread ]</a>
<a href="subject.html#19099">[ subject ]</a>
<a href="author.html#19099">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
