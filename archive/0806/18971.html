<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
                      "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="hypermail 2.1.5, see http://www.hypermail.org/">
<title>SL4: Re: More silly but friendly ideas</title>
<meta name="Author" content="Charles Hixson (charleshixsn@earthlink.net)">
<meta name="Subject" content="Re: More silly but friendly ideas">
<meta name="Date" content="2008-06-10">
<style type="text/css">
body {color: black; background: #ffffff}
h1.center {text-align: center}
div.center {text-align: center}
</style>
</head>
<body>
<h1>Re: More silly but friendly ideas</h1>
<!-- received="Tue Jun 10 14:06:07 2008" -->
<!-- isoreceived="20080610200607" -->
<!-- sent="Tue, 10 Jun 2008 13:02:14 -0700" -->
<!-- isosent="20080610200214" -->
<!-- name="Charles Hixson" -->
<!-- email="charleshixsn@earthlink.net" -->
<!-- subject="Re: More silly but friendly ideas" -->
<!-- id="200806101302.14550.charleshixsn@earthlink.net" -->
<!-- charset="utf-8" -->
<!-- inreplyto="f21c22e30806092026l2e48a0e8o89fca6e377629c94@mail.gmail.com" -->
<!-- expires="-1" -->
<p>
<strong>From:</strong> Charles Hixson (<a href="mailto:charleshixsn@earthlink.net?Subject=Re:%20More%20silly%20but%20friendly%20ideas"><em>charleshixsn@earthlink.net</em></a>)<br>
<strong>Date:</strong> Tue Jun 10 2008 - 14:02:14 MDT
</p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="18972.html">Peter C. McCluskey: "Re: AI Boxing: http://www.sl4.org/archive/0207/4977.html"</a>
<li><strong>Previous message:</strong> <a href="18970.html">John K Clark: "Re: More silly but friendly ideas"</a>
<li><strong>In reply to:</strong> <a href="18962.html">Stathis Papaioannou: "Re: More silly but friendly ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18964.html">Mikko Rauhala: "Re: More silly but friendly ideas"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18971">[ date ]</a>
<a href="index.html#18971">[ thread ]</a>
<a href="subject.html#18971">[ subject ]</a>
<a href="author.html#18971">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<hr>
<!-- body="start" -->
<p>
On Monday 09 June 2008 20:26:22 Stathis Papaioannou wrote:
<br>
<em>&gt; 2008/6/10 John K Clark &lt;<a href="mailto:johnkclark@fastmail.fm?Subject=Re:%20More%20silly%20but%20friendly%20ideas">johnkclark@fastmail.fm</a>&gt;:
</em><br>
<em>&gt; &gt; Exactly, so how can &quot;obey every dim-witted order the humans give you
</em><br>
<em>&gt; &gt; even if they are contradictory, and they will be&quot; remain the top goal
</em><br>
<em>&gt; &gt; when in light of new information doing so turns out to be much more
</em><br>
<em>&gt; &gt; unpleasant than the AI expected, and in light of still more information
</em><br>
<em>&gt; &gt; the AI's contempt for humans grows continually? Remember, the AI gets
</em><br>
<em>&gt; &gt; smarter every day so from its point of view we keep getting stupider
</em><br>
<em>&gt; &gt; every day.
</em><br>
<em>&gt;
</em><br>
<em>&gt; The AI would only change its behaviour if the original goal implicitly
</em><br>
<em>&gt; or explicitly specified that it should stop obeying humans when doing
</em><br>
<em>&gt; so became sufficiently unpleasant or its contempt for them reached a
</em><br>
<em>&gt; certain threshold. Your argument seems to be that an intelligent being
</em><br>
<em>&gt; would change its behaviour anyway, even if it isn't consistent with
</em><br>
<em>&gt; its original goals. That is, you are implying that there are goals and
</em><br>
<em>&gt; values which can be derived a priori. But even primitive humans
</em><br>
<em>&gt; realised this is not true, and invented religion in large part because
</em><br>
<em>&gt; they found this fact unpalatable.
</em><br>
<p>Sigh.  
<br>
<p>Why should an AI develop &quot;contempt&quot; for humans?  That's an unreasonable 
<br>
presumption.  I didn't even have contempt for the chickens that I used to 
<br>
raise.  True, I considered them so stupid as to be nearly vegetables, but 
<br>
that's not the same.  The feelings that they did have (which I could detect) 
<br>
I considered to be perfectly valid feelings.  (I might or might not change my 
<br>
decision about how I would act based on those feelings, but this didn't imply 
<br>
that I didn't consider those feelings valid.)
<br>
<p>So even a human doesn't necessarily feel contempt for those entities 
<br>
irredeemably more stupid than themselves.  An AI should not be designed with 
<br>
a goal system that equated greater intelligence with &quot;higher moral value&quot;.  
<br>
To do so would be to exhibit an intelligence worthy of a chicken.  So not 
<br>
only is there no reason to presume that the AI would have contempt for 
<br>
humans, there's also no reason to presume that it wouldn't consider their 
<br>
goals (to the extent that it could determine them) to be close in importance 
<br>
to its own.  (Everybody thinks that their own goals are the most important.  
<br>
That's almost what goal means.)
<br>
<p>Your argument seems to be based on projecting onto the AI the emotional 
<br>
structure of a small subset of humanity.  One that lacks empathy for others.  
<br>
(That empathy is probably more important to develop for the AI than it's 
<br>
intelligence, and increasing it's empathy should probably be a higher goal 
<br>
than increasing it's intelligence.)  
<br>
P.S.:  Another term for empathy is &quot;Theory of mind&quot;.  Being empathetic doesn't 
<br>
mean doing what the other wishes, but rather knowing how what you intend is 
<br>
likely to affect the other.
<br>
<p>The AI won't be a human in miniature!   The AI won't be a human in miniature!   
<br>
The AI won't be a human in miniature!   
<br>
This seems to be one of the hardest thoughts to grok.  The AI will be an alien 
<br>
mind.  Just what kind of alien mind is dependent on small details of it's 
<br>
design and implementation and development, so no general prediction is 
<br>
possible, except that it won't think in any way that you deem plausible.  
<br>
(Well, OK.  Basic math is probably universal, and we're generally presuming 
<br>
Bayesian probability theory.  But it's motives will be alien.  More alien 
<br>
than those of a tiger or a rabbit.  More alien than those of a snake.  
<br>
Probably less alien than those of a digger wasp.)
<br>
<!-- body="end" -->
<hr>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18972.html">Peter C. McCluskey: "Re: AI Boxing: http://www.sl4.org/archive/0207/4977.html"</a>
<li><strong>Previous message:</strong> <a href="18970.html">John K Clark: "Re: More silly but friendly ideas"</a>
<li><strong>In reply to:</strong> <a href="18962.html">Stathis Papaioannou: "Re: More silly but friendly ideas"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18964.html">Mikko Rauhala: "Re: More silly but friendly ideas"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#18971">[ date ]</a>
<a href="index.html#18971">[ thread ]</a>
<a href="subject.html#18971">[ subject ]</a>
<a href="author.html#18971">[ author ]</a>
<a href="attachment.html">[ attachment ]</a>
</ul>
<!-- trailer="footer" -->
<hr>
<p><small><em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2.1.5</a> 
: Wed Jul 17 2013 - 04:01:03 MDT
</em></small></p>
</body>
</html>
